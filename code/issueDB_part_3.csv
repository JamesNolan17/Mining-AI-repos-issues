Title,Name_Repo,Date_Created,Num_Comment,Label_Issue,Identity_Repo,Identity_Global,Body
(ppvehicle)通过export_model导出的模型配置文件与开源的模型配置文件不符,PaddlePaddle/PaddleDetection,2022-11-08 13:49:07,1,,7289,1440239250,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

_No response_

### Bug描述 Describe the Bug


![1667914571764](https://user-images.githubusercontent.com/21285692/200578913-6856d1f2-207e-480a-b536-30021f72f682.png)

以[ppvehicle](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.5/configs/ppvehicle)提供的[mot_ppyoloe_l_36e_ppvehicle.pdparams](https://paddledet.bj.bcebos.com/models/mot_ppyoloe_l_36e_ppvehicle.pdparams)为例：虽然文档声称**类别数为1**，但通过`python tools/export_model.py`方式导出的模型配置文件**infer_cfg.yml**如下：
```
mode: paddle
draw_threshold: 0.5
metric: COCO
use_dynamic_shape: false
arch: YOLO
min_subgraph_size: 3
Preprocess:
- interp: 2
  keep_ratio: false
  target_size:
  - 640
  - 640
  type: Resize
- is_scale: true
  mean:
  - 0.485
  - 0.456
  - 0.406
  std:
  - 0.229
  - 0.224
  - 0.225
  type: NormalizeImage
- type: Permute
label_list:
- person
- bicycle
- car
- motorcycle
- airplane
- bus
- train
- truck
- boat
- traffic light
- fire hydrant
- stop sign
- parking meter
- bench
- bird
- cat
- dog
- horse
- sheep
- cow
- elephant
- bear
- zebra
- giraffe
- backpack
- umbrella
- handbag
- tie
- suitcase
- frisbee
- skis
- snowboard
- sports ball
- kite
- baseball bat
- baseball glove
- skateboard
- surfboard
- tennis racket
- bottle
- wine glass
- cup
- fork
- knife
- spoon
- bowl
- banana
- apple
- sandwich
- orange
- broccoli
- carrot
- hot dog
- pizza
- donut
- cake
- chair
- couch
- potted plant
- bed
- dining table
- toilet
- tv
- laptop
- mouse
- remote
- keyboard
- cell phone
- microwave
- oven
- toaster
- sink
- refrigerator
- book
- clock
- vase
- scissors
- teddy bear
- hair drier
- toothbrush
```
========

而官方在[PPVehicle_QUICK_STARTED](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.5/deploy/pipeline/docs/tutorials/PPVehicle_QUICK_STARTED.md#%E8%BD%A6%E8%BE%86%E6%A3%80%E6%B5%8B)提供的[车辆检测(高精度)](https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_ppvehicle.zip)中的模型配置文件**infer_cfg.yml**：
```
mode: paddle
draw_threshold: 0.5
metric: COCO
use_dynamic_shape: false
arch: YOLO
min_subgraph_size: 3
Preprocess:
- interp: 2
  keep_ratio: false
  target_size:
  - 640
  - 640
  type: Resize
- type: Permute
label_list:
- vehicle
```
 
========
二者除类别数不一致外，还存在图像归一化操作的区别，请确认该问题，是否模型上传错误？

### 复现环境 Environment

-OS: Linux
-Python: 3.7
-PaddleDetection: release/2.5
-PaddlePaddle: '2.3.2'

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [X] 我愿意提交PR！I'd like to help by submitting a PR!"
cascase_rcnn模型导出部署后召回变差好多,PaddlePaddle/PaddleDetection,2022-11-08 03:07:56,0,,7286,1439386443,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

我是使用cascade_rcnn_r50_vd_fpn_ssld_2x_coco.yml训练模型后用
python tools/export_model.py -c configs/cascade_rcnn/cascade_rcnn_r50_vd_fpn_ssld_2x_coco.yml --output_dir=./inference_model \
 -o weights=./output/cascade_rcnn_r50_vd_fpn_ssld_2x_coco//best_model.pdparams 
导出模型后部署效果和直接用
python3 tools/infer.py -c configs/cascade_rcnn/cascade_rcnn_r50_vd_fpn_ssld_2x_coco.yml  -o use_gpu=true weights=./output/cascade_rcnn_r50_vd_fpn_ssld_2x_coco/best_model.pdparams 
效果差了好多。
请问下可能是那个地方处理的有问题"
换一台设备运行相同代码和模型，结果错误,PaddlePaddle/PaddleDetection,2022-11-07 09:38:37,0,,7284,1438052953,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

PaddleDetection的C++代码。使用训练的Picodet-m模型，本地测试没问题，然后代码和模型在另一台设备上运行，结果确出来很多框，位置也不对。     代码和模型已经paddle_inference都一致，请问为什么换了设备结果就错误了呢
本地没问题的是2080，换到3060上效果错误"
PicoDet-M 导出报错,PaddlePaddle/PaddleDetection,2022-11-07 09:36:40,0,,7283,1438050524,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

我在训练代码的后面加了一段导出的代码，想要在训练晚后直接导出，不用再去执行tools\export_model.py

添加的代码：
```
    # FIXME: Temporarily solve the priority problem of FLAGS.opt
    merge_config(FLAGS.opt)
    check.check_config(cfg)
    check.check_gpu(cfg.use_gpu)
    check.check_npu(cfg.use_npu)
    check.check_version()

    run(FLAGS, cfg, logger, infer_output)
    export_model(FLAGS, cfg)         #<---------------- add code here
```

```
def export_model(FLAGS, cfg):
    #export_model
    print(""Export Model..."")
    paddle.set_device(""cpu"")
    best_model = os.path.join(cfg[""save_dir""], ""best_model.pdparams"")
    final_model = os.path.join(cfg[""save_dir""], ""model_final.pdparams"")

    if os.path.exists(best_model):
        cfg['pretrain_weights'] = "" ""
        cfg['weights'] = best_model
    else:
        cfg['pretrain_weights'] = "" ""
        cfg['weights'] = final_model

    exporter = Trainer(cfg, mode='test', logger=setup_logger('exporter'))
    # load weights
    if cfg.architecture in ['DeepSORT', 'ByteTrack']:
        exporter.load_weights_sde(cfg.det_weights, cfg.reid_weights)
    else:
        exporter.load_weights(cfg.weights)
    # export model
    exporter.export(""D:\\Output_Model"")
```

执行后报错：
```
AssertionError: In transformed code:

    File ""I:\DeepLearningSystem\PaddleDetection\ppdet\modeling\architectures\meta_arch.py"", line 74, in forward
        self.inputs = inp
    File ""I:\DeepLearningSystem\PaddleDetection\ppdet\modeling\architectures\picodet.py"", line 89, in get_pred
        bbox_pred, bbox_num = self._forward()
    File ""I:\DeepLearningSystem\PaddleDetection\ppdet\modeling\architectures\picodet.py"", line 65, in _forward
        def _forward(self):
            body_feats = self.backbone(self.inputs)
            fpn_feats = self.neck(body_feats)
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
            head_outs = self.head(fpn_feats, self.export_post_process)
            if self.training or not self.export_post_process:

    File ""I:\DeepLearningSystem\PaddleDetection\paddle\fluid\dygraph\layers.py"", line 917, in __call__
        return self._dygraph_call_func(*inputs, **kwargs)
    File ""I:\DeepLearningSystem\PaddleDetection\paddle\fluid\dygraph\layers.py"", line 907, in _dygraph_call_func
        outputs = self.forward(*inputs, **kwargs)
    File ""C:\Users\admin\AppData\Local\Temp\tmp8k9cxdor.py"", line 12, in forward
        paddle.jit.dy2static.convert_assert(paddle.jit.dy2static.convert_call(
    File ""I:\DeepLearningSystem\PaddleDetection\paddle\fluid\dygraph\dygraph_to_static\convert_operators.py"", line 478, in convert_assert
        assert cond, message

    AssertionError
```
请问这个有办法解决吗"
问下，paddlepaddle-gpu版本要求在哪看呀,PaddlePaddle/PaddleDetection,2022-11-07 08:24:31,2,,7281,1437956770,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

paddlepaddle-gpu版本要求在哪看呀，是安装最新的？"
JDE多目标跟踪,PaddlePaddle/PaddleDetection,2022-11-07 08:02:24,3,,7280,1437925820,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

想问一下咋们JDE那里相较于原论文进行了哪些优化？"
车辆跨境跟踪方案配置文件中的参数如何使用,PaddlePaddle/PaddleDetection,2022-11-06 11:25:43,1,,7278,1437371824,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

车辆跨境跟踪方案的配置文件mtmct_cfg.yml中有四类参数，运行demo时默认关闭，注释中有表示开启部分参数后可能可以提升跨境跟踪的精确率，但并未对其含义和使用方法有更多的解释，想请教一下1-4这四类参数的具体含义和使用方法，谢谢！
![微信截图_20221106191641](https://user-images.githubusercontent.com/98454125/200167840-33f93a97-d714-4192-9356-dd1d51c2edb5.png)"
目标检测中的fps,PaddlePaddle/PaddleDetection,2022-11-06 06:07:24,1,,7276,1437299194,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

您好，我在使用voc数据集训练，请问怎么打印出来fps呢？"
请问在MOT中有没有评价HOTA这一个指标的方法？我看到只有关于MOTA、IDF1等指标,PaddlePaddle/PaddleDetection,2022-11-06 02:57:15,1,,7275,1437265651,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

请问在MOT中有没有评价HOTA这一个指标的方法？我看到只有关于MOTA、IDF1等指标"
paddle trt下载链接BUG：跳转bug,PaddlePaddle/PaddleDetection,2022-11-05 03:56:09,2,,7273,1436797089,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

_No response_

### Bug描述 Describe the Bug

bug地址：
https://paddleinference.paddlepaddle.org.cn/user_guides/download_lib.html#python
bug描述：
![image](https://user-images.githubusercontent.com/110536580/200099704-51c6615b-9bd1-491f-8b65-e6274fd5eaab.png)

bug现象：
![image](https://user-images.githubusercontent.com/110536580/200099713-13e2509c-5be8-4694-882a-26a7775b1459.png)


### 复现环境 Environment

普通网页bug

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
车辆跨境跟踪算法检测不到视频中的车辆轨迹,PaddlePaddle/PaddleDetection,2022-11-04 13:37:36,0,,7272,1436085677,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Deploy

### Bug描述 Describe the Bug

在运行deploy/pptracking/python/mot_sde_infer.py时报错，显示没有在视频中找到车辆轨迹
![0](https://user-images.githubusercontent.com/98454125/199984428-33ae689f-9d09-4f1c-837c-9b2049f659dc.png)
在postprocess.py文件中添加了一行print([idx])，打印出了[0]至[24]，同时添加了数组的越界处理
#6649 
![1](https://user-images.githubusercontent.com/98454125/199984865-81aa8d5a-eec4-4960-8c39-3b67bf6da151.png)



### 复现环境 Environment

- OS: Windows
- PaddlePaddle: 2.3.2
- PaddleDetection: release/2.5
- Python: 3.9.13
- CUDA: 11.7
- cuDNN: 8.4 

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [x] 我愿意提交PR！I'd like to help by submitting a PR!"
Salient Object Detection,PaddlePaddle/PaddleDetection,2022-11-04 08:31:42,0,feature request,7271,1435726832,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

1. 任务目标（请描述你正在做的项目是什么，如模型、论文、项目是什么？）
进行画幅中心显著性目标的检测/分割

2. 需求场景（请描述你的项目中为什么需要用此功能）
显著性目标检测的好处是，无论目标具体是什么类别，都可以将最主要的那个提取出来。这样就可以一个模型适用多个场景，而不用针对每个场景单独训练这个场景的数据。
具体来说，一个好的显著性目标检测模型，既可以适用于仪表盘识别中提取仪表盘，又适用于人像拍照时提取被摄人物，而不需要收集数据以及重新训练。
SOD也一直是视觉领域的重要课题，包括ECCV等顶会也有专门的SOD赛道。

3. 功能描述（请简单描述或设计这个功能）
在PaddleDetection中加入一个Salient Object Detection的分支，复现以及提供一些SOTA模型可供大家试用

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
如何将多类算法的PR曲线画在一张图上呢？,PaddlePaddle/PaddleDetection,2022-11-04 08:24:25,0,,7270,1435719726,
如何在评估代码输出percison和recall的值呢？谢谢了,PaddlePaddle/PaddleDetection,2022-11-04 06:12:11,0,,7268,1435610491,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

我训练单分类的目标检测，在评估过程中，怎么才能输出percision和recall的数值呢，我看评估代码里只有画出p和R的曲线，没有输出具体的数值，谢谢！"
8显卡安装报错,PaddlePaddle/PaddleDetection,2022-11-04 04:40:58,2,,7267,1435543169,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

报错什么原因？
Error: Your machine doesn't support AVX, but the installed PaddlePaddle is avx core, you should reinstall paddlepaddle with no-avx core.
Traceback (most recent call last):
  File ""/root/pyenv/paddel_env/test.py"", line 1, in <module>
    import paddle
  File ""/root/anaconda3/envs/paddle_env/lib/python3.8/site-packages/paddle/__init__.py"", line 25, in <module>
linux系统
8张p106显卡

还有
你们的回答问题太敷衍了！ 之前的问题你们问了一下就没有再跟进了！ "
如何修改车辆跨境跟踪方案输出文本信息的顺序？,PaddlePaddle/PaddleDetection,2022-11-03 12:29:45,2,,7264,1434581486,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

官方文档中说明跨镜头跟踪结果输出的txt文件每行信息分别是是camera_id,frame,id,x1,y1,w,h,-1,-1
目前我想替换一下frame和id两者的顺序，改为camera_id, id, frame的形式，请问该在何处进行修改？"
使用：python tools/anchor_cluster.py 报AttributeError: 'SchemaDict' object has no attribute 'parse_dataset',PaddlePaddle/PaddleDetection,2022-11-03 10:58:30,4,,7262,1434466479,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

使用：python tools/anchor_cluster.py 报AttributeError: 'SchemaDict' object has no attribute 'parse_dataset'"
STGCN的数据准备报错,PaddlePaddle/PaddleDetection,2022-11-03 10:04:37,0,,7260,1434396582,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

_No response_

### Bug描述 Describe the Bug

我想解析json文件内容、并保存为PaddleVideo可用的文件格式但跑了提供的脚本报错。
跑的脚本是：PaddleVideo/applications/PPHuman/datasets下的prepare_dataset.py 。

以下是报错的信息：
  File ""prepare_dataset.py"", line 88, in <module>
    video_anno, score = decode_json_path(os.path.join(""annotations"", path))
  File ""prepare_dataset.py"", line 77, in decode_json_path
    video_anno, scores = convert_to_ppvideo(all_kpts_np, all_score_np,
  File ""prepare_dataset.py"", line 25, in convert_to_ppvideo
    keypoint = np.expand_dims(np.transpose(all_kpts, [2, 0, 1]),
  File ""<__array_function__ internals>"", line 180, in transpose
  File ""/home/anaconda3/envs/paddle/lib/python3.8/site-packages/numpy/core/fromnumeric.py"", line 660, in transpose
    return _wrapfunc(a, 'transpose', axes)
  File ""/home/anaconda3/envs/paddle/lib/python3.8/site-packages/numpy/core/fromnumeric.py"", line 57, in _wrapfunc
    return bound(*args, **kwds)
ValueError: axes don't match array


### 复现环境 Environment

-PaddlePaddle: 2.2.2
-PAddleDetection: develop
-Python: 3.8
-CUDA: 11.2
-CUDNN: 8.1

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
评估如何打印precision和recall,PaddlePaddle/PaddleDetection,2022-11-03 07:17:02,2,,7258,1434212847,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

您好，我在训练voc格式的数据集的时候，我看在评估模型的时候只打印了map的值，请问如何打印出来precision和recall的值呢？谢谢了。"
picodet 评估报错,PaddlePaddle/PaddleDetection,2022-11-03 06:16:28,4,,7257,1434166652,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

paddleDet: release/2.4
paddle version: 2.2.2
Picodet eval时报错，报错信息如下：
![image](https://user-images.githubusercontent.com/37332773/199657883-df87c078-d07d-4255-9947-0af1a5ed26b2.png)
"
openvino使用gpu加载模型报RuntimeError: Operation: multiclass_nms3_0.tmp_1 of type MulticlassNms(op::v0) is not supported,PaddlePaddle/PaddleDetection,2022-11-03 02:59:57,1,,7256,1434032212,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

openvino使用gpu加载模型报RuntimeError: Operation: multiclass_nms3_0.tmp_1 of type MulticlassNms(op::v0) is not supported，如何解决？"
yolov3_mobilenetv3训练自己数据集损失一直降不下来,PaddlePaddle/PaddleDetection,2022-11-03 02:11:27,9,,7255,1434007213,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

yolov3_mobilenetv3训练自己数据集损失一直降不下来，数据集大概6800张，两个类别，anchors重新计算过了（貌似没啥影响），但是损失一直在30左右，做一下预测，效果很差，基本上原数据集的图都框不出来，
architecture: YOLOv3
use_gpu: true
max_iters: 500000
log_iter: 20
save_dir: output
snapshot_iter: 10000
metric: COCO
pretrain_weights: weights/mobel_fire.pdparams
weights: output/yolov3_mobilenet_v1_fire/best_model
num_classes: 2
use_fine_grained_loss: false

YOLOv3:
  backbone: MobileNetV3
  yolo_head: YOLOv3Head

MobileNetV3:
  norm_type: sync_bn
  norm_decay: 0.
  model_name: large
  scale: 1.
  extra_block_filters: []
  feature_maps: [1, 2, 3, 4, 6]

YOLOv3Head:
  anchor_masks: [[6, 7, 8], [3, 4, 5], [0, 1, 2]]
  anchors: [[24, 33], [52, 69], [73, 139],
          [148, 84], [99, 265], [167, 163],
          [341, 137], [221, 305], [446, 385]]
  norm_decay: 0.
  yolo_loss: YOLOv3Loss
  nms:
    background_label: -1
    keep_top_k: 100
    nms_threshold: 0.45
    nms_top_k: 1000
    normalized: false
    score_threshold: 0.01

YOLOv3Loss:
  ignore_thresh: 0.7
  label_smooth: false

LearningRate:
  base_lr: 0.00001
  schedulers:
  - !PiecewiseDecay
    gamma: 0.1
    milestones:
    - 400000
    - 450000
  - !LinearWarmup
    start_factor: 0.
    steps: 4000

OptimizerBuilder:
  optimizer:
    momentum: 0.9
    type: Momentum
  regularizer:
    factor: 0.0005
    type: L2

_READER_: 'yolov3_reader.yml'
"
关于在pd2.4下训练的目标检测模型不能在pd2.5下使用trt加速报错的问题描述,PaddlePaddle/PaddleDetection,2022-11-03 00:49:18,8,,7254,1433960135,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Deploy

### Bug描述 Describe the Bug

情况描述：用pd2.4的代码训练了一个目标检测模型，在pd2.5里使用trt加速失败的现象描述。

cuda10.2
cudnn7.6.5
tensorRT7.0.0.11
windows环境

![image](https://user-images.githubusercontent.com/104408880/199628142-977eafd7-4632-4995-be92-005142ac4b3e.png)

报错提示：
![Uploading image.png…]()





### 复现环境 Environment

Windows
python3.8


### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [X] 我愿意提交PR！I'd like to help by submitting a PR!"
"ERROR 2022-11-02 08:42:32,186 launch_utils.py:604] ABORT!!! Out of all 1 trainers, the trainer process with rank=[0] was aborted. Please check its log.",PaddlePaddle/PaddleDetection,2022-11-02 09:46:56,1,,7252,1432816563,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

使用paddledetection下的ppyoloe进行训练，后出现这个问题

> Traceback (most recent call last):
  File ""tools/train.py"", line 172, in <module>
    main()
  File ""tools/train.py"", line 168, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 132, in run
    trainer.train(FLAGS.eval)
  File ""/mnt/PaddleDetection/ppdet/engine/trainer.py"", line 453, in train
    for step_id, data in enumerate(self.loader):
  File ""/mnt/PaddleDetection/ppdet/data/reader.py"", line 209, in __next__
    return next(self.loader)
  File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 697, in __next__
    data = self._reader.read_next_var_list()
  File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/multiprocess_utils.py"", line 134, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 1104) exited is killed by signal: Killed. (at /paddle/paddle/fluid/imperative/data_loader.cc:181)

> INFO 2022-11-02 08:42:32,186 launch_utils.py:341] terminate all the procs

> ERROR 2022-11-02 08:42:32,186 launch_utils.py:604] ABORT!!! Out of all 1 trainers, the trainer process with rank=[0] was aborted. Please check its log.
> INFO 2022-11-02 08:42:36,191 launch_utils.py:341] terminate all the procs
> INFO 2022-11-02 08:42:36,191 launch.py:311] Local processes completed.

rank是代表显卡出现问题嘛？我是用的docker，只有一颗gpu，请问该怎么解决呢？"
人体骨骼点行为识别的二次开发,PaddlePaddle/PaddleDetection,2022-11-02 09:43:58,4,,7251,1432809753,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

你好，我想二次开发PP-Human下的基于骨骼点的摔倒识别。请问如果我想在摔倒识别上新增新的行为，那在准备新的数据集时，需要把摔跤的数据也添加进新的数据集一起训练吗？"
yolov7 训练自定义数据集，卡主不动，且不报错,PaddlePaddle/PaddleDetection,2022-11-02 03:32:02,5,,7249,1432425568,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Training

### Bug描述 Describe the Bug

1，构建了一个自定义数据集，COCO格式的；
2，PPyoloe上可以正常训练；
3，yolov7训练完第一个epoch后，第二个epoch训练到中间，然后卡住不动；

### 复现环境 Environment

- OS: Linux, 4.15.0-163-generic
- PaddlePaddle: 2.3.2
- PaddleDetection: release/2.5
- Python: 3.7.5
- CUDA: 11.2.152
- CUDNN: 8.1.1
- GCC: 7.5.0

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
s2anet 训练旋转矩形检测速度很慢,PaddlePaddle/PaddleDetection,2022-11-02 01:18:49,2,,7248,1432319091,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

数据集约1万张图，4张16G显存v100 batch_size 10 ，lr0.05 一个batch 要差不多3分钟，看了下

[11/02 09:14:21] ppdet.engine INFO: Epoch: [0] [  0/200] learning_rate: 0.016667 loss: 4.026768 fam_cls_loss: 1.191281 fam_reg_loss: 1.182869 odm_cls_loss: 1.165921 odm_reg_loss: 0.486696 eta: 1 day, 22:42:04 batch_cost: 10.5078 data_cost: 1.3552 ips: 0.9517 images/s
[11/02 09:17:41] ppdet.engine INFO: Epoch: [0] [ 20/200] learning_rate: 0.017333 loss: 2.455547 fam_cls_loss: 0.776772 fam_reg_loss: 0.681920 odm_cls_loss: 0.615343 odm_reg_loss: 0.256558 eta: 1 day, 20:27:41 batch_cost: 9.9918 data_cost: 0.0006 ips: 1.0008 images/s
"
复现pphuman模型部署serving不能检测到目标对象,PaddlePaddle/PaddleDetection,2022-11-02 01:11:06,15,,7247,1432313950,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

_No response_

### Bug描述 Describe the Bug

你好，我正在部署pphuman下的[ppyoloe_crn_s_36e_pphuman.yml]
(https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.5/configs/pphuman/ppyoloe_crn_s_36e_pphuman.yml)模型，
[我按照部署步骤](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.5/deploy/serving/python)进行部署，只是将模型.yml进行了替换，服务能够成功启动，但是客户端测试一个有人的图片检测不到结果。
image

### 复现环境 Environment

-os:linux
-python:3.6.13
-paddle-serving-app        0.9.0
-paddle-serving-client     0.9.0
-paddle-serving-server-gpu 0.9.0.post112
-paddle2onnx               1.0.1
-paddleclas                2.4.3
-cuda  :11.6


### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
PaddlePaddle配不上cuda环境,PaddlePaddle/PaddleDetection,2022-11-01 13:01:23,2,,7246,1431387959,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

_No response_

### Bug描述 Describe the Bug

PaddlePaddle配不上cuda环境，调用不了GPU
CUDA11.2 & 11.6版本皆安装失败
<img width=""1235"" alt=""2c8abd7d6563490d1ec903ad3102026"" src=""https://user-images.githubusercontent.com/98454125/199237193-d24e1ab5-67b0-436f-8d9b-505b1bfd74aa.png"">


### 复现环境 Environment

- OS: Linux
- PaddlePaddle : 2.3.2
- PaddleYOLO : release/2.5
- Python : 3.9.13
- CUDA : 11.4
- cuDNN Version : 8.4
- GUP : V100  


### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
车辆跨境跟踪mtmct方案是否支持在线跟踪？,PaddlePaddle/PaddleDetection,2022-11-01 12:10:33,2,,7245,1431327635,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

目前已经完成了车辆跨境跟踪算法框架的复现，实现了两个及以上视频输入得到跨境跟踪的视频和文本结果。请问现有的跨境跟踪方案是否支持多个视频的在线跟踪或是得到第一个视频的全部数据后再对第二个视频进行在线的逐帧跟踪？"
paddledetection，eval评估,PaddlePaddle/PaddleDetection,2022-11-01 12:09:33,2,,7244,1431326177,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

eval.py评估多类，如何打印每一类的Precision Recall  mAP."
Using a pre-trained picodet with different number of classes,PaddlePaddle/PaddleDetection,2022-11-01 10:15:44,1,,7242,1431190649,"
Hi Everyone,
I'm using PicoDet for the layout analysis task
There's a pre-trained model already trained in English for 5 classes

I want to fine-tune this model using my dataset which contains only 2 classes

How can I do that using the configuration file for PicoDet?

Would only changing `num_classes: 2` work and adding the pre-trained model?"
多类检测召回率如何打印，如何打印指定IOU的AP,PaddlePaddle/PaddleDetection,2022-11-01 09:42:30,2,,7241,1431148671,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

![image](https://user-images.githubusercontent.com/67177370/199205347-9b59ede0-4635-47a5-aee7-36173986d021.png)
AP是IoU=0.50:0.95，如何打印IoU=0.50"
部署模型检测不到目标,PaddlePaddle/PaddleDetection,2022-11-01 09:36:29,1,,7240,1431139936,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

你好，我正在部署pphuman下的[ppyoloe_crn_s_36e_pphuman.yml](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.5/configs/pphuman/ppyoloe_crn_s_36e_pphuman.yml)模型，但是检测不到结果。
![image](https://user-images.githubusercontent.com/51695758/199204794-d8ce8704-3236-4eae-ad90-2dd04f22720b.png)
"
Batch_norm 替换 bug！,PaddlePaddle/PaddleDetection,2022-11-01 08:50:59,2,,7239,1431087774,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

hi，
   请问下，TestReader.fuse_normalize=true 为啥我转换模型的时候不管怎么设置前面这个参数，我导出的模型都有 batch_normalxxx 这个模块呢？ 如图 
![9a822ba8c75d01b116810c2f6bcce2e](https://user-images.githubusercontent.com/8407513/199194914-c6e3a06f-d21c-4344-a302-d5d12a3c3cee.jpg)
 怎样才能在导出模型的时候去掉呢？把这个 batch norm 模块换成 下图 这两个op呢？  
![2bb29a5e2d2e07fdbf0cb3deb0fff1f](https://user-images.githubusercontent.com/8407513/199194957-fdc3dd75-35e0-43d0-923d-a695ee928342.jpg)
  期待你的答复，多谢~
BR
"
关于percison和recall,PaddlePaddle/PaddleDetection,2022-11-01 07:03:51,0,,7237,1430977877,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

您好，我训练完成以后发现模型只有计算map的值，没有看到P和R在哪里计算，请问如果我想要统计出P和R，这个要怎么改呀，谢谢！
![image](https://user-images.githubusercontent.com/43515926/199177558-161e753b-c443-44f4-a79c-029180552f35.png)
"
loss_l1: 0.000000,PaddlePaddle/PaddleDetection,2022-11-01 04:55:08,6,,7236,1430868972,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

您好，我在训练过程中loss_l1: 0.000000一直是0，请问这个是怎么回事呢？
![image](https://user-images.githubusercontent.com/43515926/199161955-e309d7bd-608a-48f5-9e87-ebcee06370d1.png)

"
tinypose qat quant missing,PaddlePaddle/PaddleDetection,2022-11-01 03:26:30,0,,7235,1430807446,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

hi，
[PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection)/[configs](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.5/configs)/[slim](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.5/configs/slim)/[quant](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.5/configs/slim/quant)/tinypose_qat.yml
请问下上述链接里 tinypose的 qat 全量化是否有demo发布，如果没有年底计划有安排了吗？
我自己尝试量化训练后结果如下: loss都是nan 明显不对
image
![image](https://user-images.githubusercontent.com/8407513/199154060-fd18b0a1-b3c6-44b4-ad9e-adc130b62f80.png)

BR

复现环境 Environment
paddlepaddle 2.4rc
paddleslim 最新release
ppdet 2.4 最新release

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
tinypose qat quant missing！,PaddlePaddle/PaddleDetection,2022-11-01 03:23:53,0,,7234,1430805739,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

_No response_

### Bug描述 Describe the Bug

hi，

[PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection)/[configs](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.5/configs)/[slim](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.5/configs/slim)/[quant](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.5/configs/slim/quant)/tinypose_qat.yml 
请问下上述链接里 tinypose的 qat 全量化是否有demo发布，如果没有年底计划有安排了吗？
我自己尝试量化训练后结果如下: loss都是nan 明显不对
![image](https://user-images.githubusercontent.com/8407513/199151903-6e76062e-93d6-4304-86ca-7b6cd18ca3fd.png)
![image](https://user-images.githubusercontent.com/8407513/199154042-73902e73-61a6-4828-b309-4842fca80f34.png)


BR

### 复现环境 Environment

paddlepaddle 2.4rc
paddleslim 最新release
ppdet 2.4 最新release

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [X] 我愿意提交PR！I'd like to help by submitting a PR!"
ppyoloe多类检测如何计算每一类的map，召回率,PaddlePaddle/PaddleDetection,2022-11-01 03:05:28,1,,7233,1430788561,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

eval.py只能计算平均map，召回率"
mot数据集 det.txt如何生成,PaddlePaddle/PaddleDetection,2022-11-01 01:45:50,8,,7232,1430726598,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

现已生成MOT相关的标注文件。不过我这里有一个新的问题：我使用darklabel工具标记自己的数据集，可以生成gt.txt文件，没有det.txt文件，导致无法生成相应的标注文件，det文件是如何制作的呢，期待您的回答！"
怎么计算map50:95,PaddlePaddle/PaddleDetection,2022-10-31 12:25:11,0,,7231,1429761649,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

mAP(0.50, 11point)已经很高了，想要计算mAP50:95，怎么在ppdet/metrics/map_utils.py中修改？"
关于voc数据评估,PaddlePaddle/PaddleDetection,2022-10-31 10:24:33,1,,7228,1429607239,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

请问有没有评估voc的方法呢？我把我的标签文件都弄成了voc格式的，谢谢
![image](https://user-images.githubusercontent.com/43515926/198986588-8263e826-c2b0-46df-bff5-f6f229c0a4d3.png)
"
MOT  使用JDE算法，自定义数据集的配置文件应该怎么写呢,PaddlePaddle/PaddleDetection,2022-10-31 10:20:08,1,,7227,1429601347,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

期待您的解答"
MOT   选择SDE方案 可以使用检测标注的配置文件/configs/datasets/coco_detection.yml吗,PaddlePaddle/PaddleDetection,2022-10-31 10:15:04,1,,7226,1429594541,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

如果用户选择SDE系列方案，是准备准检测标注的自定义数据集，则可以参照DET数据准备文档准备。"
MOT16数据集可以通过tools/x2coco生成annotation_train、annotation_val吗，如果不可以，有推荐的生成脚本吗,PaddlePaddle/PaddleDetection,2022-10-31 08:51:07,2,,7225,1429483732,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

MOT16数据集可以通过tools/x2coco生成annotation_train、annotation_val吗，如果不可以，有推荐的生成脚本吗"
关于对应Paddle_inference库版本的问题,PaddlePaddle/PaddleDetection,2022-10-31 06:50:31,1,,7224,1429347100,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

当前PaddleDetection中C++的paddle_inference库下载，最高版本是2.2。   而PaddleOCR的paddle_inference已经支持2.4了。 
请问这两个的paddle_inference-2.2都是一样的吧？    如果想把两个程序联调起来使用，Paddledetectioon可以用大于2.2的推理库么？还是说两个都只能用paddle_inference-2.2的版本？"
能不能直接将opencv的cv2.cuda_GpuMat 变量，直接转为 模型输入tensor,PaddlePaddle/PaddleDetection,2022-10-29 01:58:16,1,,7222,1428018063,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

能不能直接将opencv的cv2.cuda_GpuMat 变量，直接转为 模型输入tensor，我使用 飞桨目标检测的infer.py ， 里面提供的demo是使用opencv做前处理，但都是使用的CPU，能不能使用opencv的 cuda_GpuMat 做前处理，然后直接转入tensor做模型预测？

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
gpu检测不到目标，使用cpu能检测到目标有标注，gpu没有标注,PaddlePaddle/PaddleDetection,2022-10-28 14:20:35,0,,7221,1427338015,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

cpu和gpu输出的过程是一样的，用gpu检测不到目标，没有标注。

pc配置：
os：Linux h-H61MLC2 5.15.0-52-generic #58~20.04.1-Ubuntu SMP Thu Oct 13 13:09:46 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
cuda: CUDA Version: 11.6 
cudnn:8.4
paddlepaddle-gpu==2.3.2

#在您的Python解释器中确认PaddlePaddle安装成功
Python 3.8.10 (default, Jun 22 2022, 20:18:18) 
[GCC 9.4.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import paddle
>>> paddle.utils.run_check()
Running verify PaddlePaddle program ... 
W1029 01:11:56.998651  9970 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 5.2, Driver API Version: 11.6, Runtime API Version: 11.6
W1029 01:11:57.004197  9970 gpu_resources.cc:91] device: 0, cuDNN Version: 8.4.
PaddlePaddle works well on 1 GPU.
PaddlePaddle works well on 1 GPUs.
PaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.

# 确认PaddlePaddle版本
h@h-H61MLC2:~$ python3 -c ""import paddle; print(paddle.__version__)""
2.3.2



命令，使用use_gpu=false，能预测出图片：
python3 tools/infer.py -c configs/ppyolo/ppyolo_r50vd_dcn_1x_coco.yml -o use_gpu=false weights=https://paddledet.bj.bcebos.com/models/ppyolo_r50vd_dcn_1x_coco.pdparams --infer_img=demo/000000014439.jpg
运行过程：
[10/28 21:37:41] ppdet.utils.checkpoint INFO: Finish loading model weights: /home/h/.cache/paddle/weights/ppyolo_r50vd_dcn_1x_coco.pdparams
[10/28 21:37:41] ppdet.data.source.category WARNING: anno_file 'dataset/coco/annotations/instances_val2017.json' is None or not set or not exist, please recheck TrainDataset/EvalDataset/TestDataset.anno_path, otherwise the default categories will be used by metric_type.
[10/28 21:37:41] ppdet.data.source.category WARNING: metric_type: COCO, load default categories of COCO.

100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.11s/it]
[10/28 21:37:44] ppdet.engine INFO: Detection bbox results save in output/000000014439.jpg
结果：
**预测图片有标注，ok**

命令，使用use_gpu=true，不能预测出图片：
export CUDA_VISIBLE_DEVICES=0
python3 tools/infer.py -c configs/ppyolo/ppyolo_r50vd_dcn_1x_coco.yml -o use_gpu=true weights=https://paddledet.bj.bcebos.com/models/ppyolo_r50vd_dcn_1x_coco.pdparams --infer_img=demo/000000014439.jpg
过程：
W1028 21:39:15.508200  5394 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 5.2, Driver API Version: 11.6, Runtime API Version: 11.6
W1028 21:39:15.526571  5394 gpu_resources.cc:91] device: 0, cuDNN Version: 8.4.

[10/28 21:39:17] ppdet.utils.checkpoint INFO: Finish loading model weights: /home/h/.cache/paddle/weights/ppyolo_r50vd_dcn_1x_coco.pdparams
[10/28 21:39:17] ppdet.data.source.category WARNING: anno_file 'dataset/coco/annotations/instances_val2017.json' is None or not set or not exist, please recheck TrainDataset/EvalDataset/TestDataset.anno_path, otherwise the default categories will be used by metric_type.
[10/28 21:39:17] ppdet.data.source.category WARNING: metric_type: COCO, load default categories of COCO.
100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.07s/it]
[10/28 21:39:19] ppdet.engine INFO: Detection bbox results save in output/000000014439.jpg
结果：
**预测的图片没有标注，没有检测到目标**


"
ppyoloe+，训练使用normalize，但是预测和推理为什么都不用normalize,PaddlePaddle/PaddleDetection,2022-10-28 10:10:36,4,,7220,1427028365,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

请问，ppyoloe+的升级其中一点，说是为了加快推理速度，去掉了normalize的操作
![image](https://user-images.githubusercontent.com/16351823/198561246-fbeb87ea-ec9c-48d8-bc5b-ccabab659e3d.png)
为什么训练过程要normalize，但推理和预测的过程不用，加上反而结果是错的呢？
我这边是训练了小目标检测中的configs\smalldet\ppyoloe_crn_s_300e_sliced_visdrone_640_025_text_ico.yml，
![image](https://user-images.githubusercontent.com/16351823/198561793-0f5acecb-c974-42d3-8869-0f22df3311ac.png)
其中数据读取模块，使用的ppyoloe_reader
![image](https://user-images.githubusercontent.com/16351823/198561924-8c6d75d0-e6cc-4d0e-aecc-d3c5347547ee.png)
而ppyoloe_reader中的训练验证和测试，都是有normalize这一步的：
![image](https://user-images.githubusercontent.com/16351823/198562233-e435202d-576b-4aed-87b7-f77e490271e5.png)
但是后面仔细看，在ppyoloe_crn_s_300e_sliced_visdrone_640_025_text_ico.yml中的TestReader中多了一个fuse_normalize的参数，经过调试，发现这个参数就是在测试过程中不使用normalize，刚开始没看到，加上了normalize，结果一片空白，什么都没检测出来，去掉后就正常了。
所以我有点疑惑，为什么训练时候加了，测试的时候加上反而结果错误，去掉反而结果正常了？不应该训练和测试时的预处理要保持一致吗？"
layout picodet_lcnet_x1_0_fgd_layout_cdla模型,PaddlePaddle/PaddleDetection,2022-10-28 07:49:25,0,,7219,1426840582,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

（推理模型、训练模型）测试layout.jpg标签类别不一致，
![layout](https://user-images.githubusercontent.com/67177370/198533343-6f0f2bc7-1eaf-44ed-8e6a-f42baced849f.jpg)
![企业微信截图_16669417029184](https://user-images.githubusercontent.com/67177370/198533406-d82ce330-5187-497a-b8e1-7d066c0f9f02.png)
"
关于ppyoloe训练命令的一些疑问,PaddlePaddle/PaddleDetection,2022-10-28 04:35:34,2,,7217,1426657326,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

1、官网的训练命令如下，我用完之后报了如下错误请问是什么原因？
![image](https://user-images.githubusercontent.com/102035987/198503122-160ab92d-cddc-47e8-8e07-3c02fad3b16f.png)
![image](https://user-images.githubusercontent.com/102035987/198503146-a117afcc-7fae-4e7e-a8ee-1904a630be5b.png)
2、但是用下边的就可以
![image](https://user-images.githubusercontent.com/102035987/198503207-fa3ecb84-c281-4325-8023-850a865933cc.png)
请问第一种该如何解决，第二种是否真的使用到8张卡，怎么查看当前环境有几张卡可以用？"
复现S2ANet出现的精度问题,PaddlePaddle/PaddleDetection,2022-10-28 02:02:45,1,,7215,1426496196,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

Env：
4块11GB的2080Ti
按照官方教程复现S2ANet
数据集也是按照官方给出的进行剪切处理的

config没有改 直接跑的 s2anet_alignconv 2x的  在DOTA服务器上eval的结果mAP 59.07%
修改backbone 从resnetvd50->resnetvb50 mAP 71.19%

官方给出的结果应该为：
<img width=""960"" alt=""image"" src=""https://user-images.githubusercontent.com/30338877/198454323-8456dc5e-5116-401b-b7b6-57656bc94cfe.png"">

能给出训练过程的log 我排查一下问题

"
ppyole+ infer 的时候存储的box.json 后期处理时无法直接对应到文件名。建议self._imid2path 也存储下来。,PaddlePaddle/PaddleDetection,2022-10-27 11:58:55,3,,7213,1425517585,"### 问题描述 Please describe your issue

ppyole+ infer 的时候存储的box.json 后期处理时无法直接对应到文件名。建议self._imid2path 也存储下来。"
部署lite时报错：Makefile.def: No such file or directory,PaddlePaddle/PaddleDetection,2022-10-27 06:46:31,0,,7209,1425119081,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

x86编译arm linux
```
./lite/tools/build_linux.sh --arch=armv8 --with_python=ON --with_cv=ON --with_extra=ON --with_opencl=ON
```
照着deploy/lite下的文档整理
![Snip20221027_32](https://user-images.githubusercontent.com/7626261/198210677-042ba5c4-d59d-45c5-9304-69057b59fdbb.png)

如下是我执行后的目录结构
![Snip20221027_31](https://user-images.githubusercontent.com/7626261/198210701-4134d2ac-7691-49d7-be8f-cf3530f0df95.png)

执行make ARM_ABI=arm8后报错
![Snip20221027_33](https://user-images.githubusercontent.com/7626261/198210864-dbead034-b74f-4362-956e-9c7b225a5444.png)

"
human_attribution,PaddlePaddle/PaddleDetection,2022-10-27 05:55:10,0,,7208,1425074439,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

推理示例里  对原图先检测 再把检测送入属性模型。
我想只使用属性模块  输入为检测裁剪后的原图 输出属性类型结果，但给的pipeline也太麻烦了 有更简洁的推理代码吗"
ppyolo系列的蒸馏配置文件写法,PaddlePaddle/PaddleDetection,2022-10-27 05:31:50,0,,7207,1425057073,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

对于蒸馏的使用，只给出了yolov3的蒸馏配置文件，在configs/slim/distill/yolov3_mobilenet_v1_coco_distill.yml：
```
_BASE_: [
  '../../yolov3/yolov3_r34_270e_coco.yml',
]

pretrain_weights: https://paddledet.bj.bcebos.com/models/yolov3_r34_270e_coco.pdparams


slim: Distill
distill_loss: DistillYOLOv3Loss

DistillYOLOv3Loss:
  weight: 1000
```

而我需要用在ppyolo系列模型，从ppyolo_r50vd_dcn模型蒸馏到ppyolo_r18vd模型
为此我对于上面这个蒸馏配置文件做了两处改动：
1. 将_BASE_替换为configs/ppyolo/ppyolo_r50vd_dcn_xx.yml，即我用自己数据集训练ppyolo_r50vd_dcn模型时的配置文件
2. 将pretrain_weights替换为output/ppyolo_r50vd_dcn_xx/best_model.pdparams，即我用自己数据集训练的ppyolo_r50vd_dcn模型
保存为configs/slim/distill/ppyolo_r18vd_xx_distill.yml

然后执行蒸馏训练：
python3 tools/train.py -c configs/ppyolo/ppyolo_r18vd_xx.yml --slim_config configs/slim/distill/ppyolo_r18vd_xx_distill.yml --eval
但是map只用0.14%，明显存在问题

对比直接训练小模型：
python3 tools/train.py -c configs/ppyolo/ppyolo_r18vd_xx.yml
map有53.37%，就很正常

这里有两个问题：
1. 是否是因为没有支持从ppyolo_r50vd_dcn向ppyolo_r18vd蒸馏，才导致蒸馏效果很差？
2. 在修改蒸馏配置文件为ppyolo系列时，我仍然沿用了DistillYOLOv3Loss，这一处是否需要修改？

感谢！"
报错：InvalidArgumentError: The 'shape' attribute in ReshapeOp is invalid.,PaddlePaddle/PaddleDetection,2022-10-27 04:09:49,1,,7206,1424998965,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

_No response_

### Bug描述 Describe the Bug

pp-human的video action recognition 用自己训练的模型以下的错误:    
'''
InvalidArgumentError: The 'shape' attribute in ReshapeOp is invalid. The input tensor X'size must be divisible by known capacity of 'shape'. But received X's shape = [1, 1, 1, 8, 3, 320, 320], X's size = 2457600, 'shape' is [-1, 3, 224, 224], known capacity of 'shape' is -150528.
      [Hint: Expected output_shape[unk_dim_idx] * capacity == -in_size, but received output_shape[unk_dim_idx] * capacity:-2408448 != -in_size:-2457600.] (at /paddle/paddle/fluid/operators/reshape_op.cc:210)
      [operator < reshape2 > error]
'''


### 复现环境 Environment

- PaddlePaddle: 2.2.2
- PaddleDetection: develop

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
pp-human v2抽烟检测报错 KeyError: 'DET',PaddlePaddle/PaddleDetection,2022-10-27 02:21:13,0,,7203,1424926425,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Deploy

### Bug描述 Describe the Bug

执行
!python deploy/pipeline/pipeline.py --config deploy/pipeline/config/examples/infer_cfg_smoking.yml --image_file=demo/000000014439_640x640.jpg --device=gpu

报错如下：

deploy/pipeline/pipeline.py:24: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  from collections import Sequence, defaultdict
-----------  Running Arguments -----------
ID_BASED_DETACTION:
  batch_size: 8
  display_frames: 80
  enable: true
  model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/ppyoloe_crn_s_80e_smoking_visdrone.zip
  skip_frame_num: 2
  threshold: 0.6
MOT:
  batch_size: 1
  enable: true
  model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip
  tracker_config: deploy/pipeline/config/tracker_config.yml
crop_thresh: 0.5
visual: true
warmup_frame: 50

------------------------------------------
Multi-Object Tracking enabled
IDBASED Detection Action Recognition enabled
100%|████████████████████████████████| 186349/186349 [00:07<00:00, 25003.16KB/s]
MOT  model dir:  /home/aistudio/.cache/paddle/infer_weights/mot_ppyoloe_l_36e_pipeline
100%|██████████████████████████████████| 27194/27194 [00:00<00:00, 31835.57KB/s]
ID_BASED_DETACTION  model dir:  /home/aistudio/.cache/paddle/infer_weights/ppyoloe_crn_s_80e_smoking_visdrone
Traceback (most recent call last):
  File ""deploy/pipeline/pipeline.py"", line 1103, in <module>
    main()
  File ""deploy/pipeline/pipeline.py"", line 1088, in main
    pipeline = Pipeline(FLAGS, cfg)
  File ""deploy/pipeline/pipeline.py"", line 87, in __init__
    self.predictor = PipePredictor(args, cfg, self.is_video)
  File ""deploy/pipeline/pipeline.py"", line 369, in __init__
    det_cfg = self.cfg['DET']
KeyError: 'DET'

### 复现环境 Environment

拷贝自：https://aistudio.baidu.com/aistudio/projectdetail/4834683?channelType=0&channel=0
算力卡试过几个配置都一样。

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
PaddleDetection 目标检测微调 ,PaddlePaddle/PaddleDetection,2022-10-27 01:11:20,1,,7202,1424881619,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

你好
PPOCRLabelv2 标注的目标检测数据，是否可以转换为VOC/COCO 格式，放到PaddleDetection中微调？"
PP-TinyPose自然监控场景下的pose识别效果不佳,PaddlePaddle/PaddleDetection,2022-10-26 10:18:50,2,,7197,1423784022,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

您好，
我按照 [https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.5/configs/keypoint/tiny_pose](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.5/configs/keypoint/tiny_pose ) 最新发布的模型尝试了一下python 推理。

模型采用 单人模型配置 PicoDet-S-Lcnet-Pedestrian-192*192 + PP-TinyPose-128*96，并下好了模型。

推理命令是
`python deploy/python/det_keypoint_unite_infer.py --det_model_dir=output_inference/picodet_v2_s_192_pedestrian --keypoint_model_dir=output_inference/tinypose_128x96 --image_dir=./test --device=GPU`

部分图效果如下：
![hand_00026_vis](https://user-images.githubusercontent.com/19179730/198000413-a699ef05-8bdc-43a9-bf99-38a826573a28.jpg)

![SELF jiejing-1one_R6204_11_vis](https://user-images.githubusercontent.com/19179730/198000560-445970cb-5e28-47a2-ba71-cd353fb977c8.jpg)

![hand_00007_vis](https://user-images.githubusercontent.com/19179730/198000817-45247720-5d5f-4608-b058-555dc6b34048.jpg)


![MARKET 0613_c6s2_019293_01_vis](https://user-images.githubusercontent.com/19179730/198001015-10aa648e-9650-45f3-9bda-70eb0e129b48.jpg)

![hand_00004_vis](https://user-images.githubusercontent.com/19179730/198001143-c17e06e9-892d-4d2a-a023-236cce426497.jpg)

因此我想咨询一下，是不是测试某些细节没注意到？
谢谢"
"paddleX划分的基于rle格式的COCO数据集在paddledetection训练时报错""not found any coco record""",PaddlePaddle/PaddleDetection,2022-10-26 09:43:32,20,status/close,7195,1423737481,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Training, DataProcess

### Bug描述 Describe the Bug

使用paddleX进行数据集划分：
```bash
paddlex --split_dataset --format COCO --dataset_dir ./texture_dataset/ --val_value 0.2 --test_value 0.1
```
打印信息为：
```bash
2022-10-26 11:38:46,751-WARNING: type object 'QuantizationTransformPass' has no attribute '_supported_quantizable_op_type'
2022-10-26 11:38:46,751-WARNING: If you want to use training-aware and post-training quantization, please use Paddle >= 1.8.4 or develop version
2022-10-26 11:38:48 [INFO]	Dataset split starts...
loading annotations into memory...
Done (t=6.63s)
creating index...
index created!
2022-10-26 11:38:55 [INFO]	Dataset split done.
2022-10-26 11:38:55 [INFO]	Train samples: 700
2022-10-26 11:38:55 [INFO]	Eval samples: 200
2022-10-26 11:38:55 [INFO]	Test samples: 100
2022-10-26 11:38:55 [INFO]	Split files saved in ./texture_dataset/
```

数据集中的实例分割以rle编码，使用COCO可视化结果如下，显示标注文件无错误：
![image](https://user-images.githubusercontent.com/38804949/197993131-32ca8b34-722c-4fcf-b778-fefd1f8e38fd.png)


在使用paddledetection训练目标检测任务时报错：
```bash
python3 tools/train.py -c ./configs/yolov3/yolov3_darknet53_270e_obj1texture_coco.yml --use_vdl=true --vdl_log_dir=vdl_dir/scalar


loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Traceback (most recent call last):
  File ""tools/train.py"", line 172, in <module>
    main()
  File ""tools/train.py"", line 168, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 123, in run
    trainer = Trainer(cfg, mode='train')
  File ""/data-r10/kb/Projects/SynDataGen/PaddleDetection/ppdet/engine/trainer.py"", line 95, in __init__
    self.dataset, cfg.worker_num)
  File ""/data-r10/kb/Projects/SynDataGen/PaddleDetection/ppdet/data/reader.py"", line 163, in __call__
    self.dataset.parse_dataset()
  File ""/data-r10/kb/Projects/SynDataGen/PaddleDetection/ppdet/data/source/coco.py"", line 225, in parse_dataset
    assert ct > 0, 'not found any coco record in %s' % (anno_path)
AssertionError: not found any coco record in ../datasets/obj1/texture_dataset/train.json
```

### 复现环境 Environment

- ubuntu 18.04
- paddle-bfloat       0.1.7
- paddlepaddle-gpu    2.3.2
- paddleslim          2.2.1
- paddlex             2.1.0

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [X] 我愿意提交PR！I'd like to help by submitting a PR!"
如何修改train log的保存路径？,PaddlePaddle/PaddleDetection,2022-10-26 08:27:21,1,,7193,1423632740,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

记得以前好像还有一个--log_dir的参数，paddleDetection2.5 train的时候不知道怎么改默认路径参数，文件总是保存在根目录的log；如何修改？"
使用Mask_rcnn训练模型之后infer运行失败内存溢出,PaddlePaddle/PaddleDetection,2022-10-26 01:27:11,21,,7191,1423303015,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Inference

### Bug描述 Describe the Bug

模型训练之后进行infer内存溢出，显卡并无其他任务运行,预测图片总数约500Mb
```shell
python deploy/python/infer.py --model_dir=./inference_model/mask_rcnn_r50_vd_fpn_2x_coco/ --image_dir=output/500A --output_dir=output/500A_TEST --device=gpu
```
log日志如下
```shell
Traceback (most recent call last):
  File ""deploy/python/infer.py"", line 909, in <module>
    main()
  File ""deploy/python/infer.py"", line 882, in main
    img_list, FLAGS.run_benchmark, repeats=100, save_file=save_file)
  File ""deploy/python/infer.py"", line 278, in predict_image
    result = self.predict()
  File ""deploy/python/infer.py"", line 206, in predict
    self.predictor.run()
RuntimeError: ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 2.324581GB memory on GPU 0, 6.075684GB memory has been allocated and available memory is only 1.711487GB.
```

### 复现环境 Environment

- OS:Ubuntu20
- DockerImage:paddlecloud/paddledetection:2.4-gpu-cuda11.2-cudnn8-latest
- Cuda:11.2
- paddlepaddle-gpu:2.3.0.post112

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
PaddleDetection/Paddle-Lite-目标跟踪问题,PaddlePaddle/PaddleDetection,2022-10-25 15:38:48,1,,7190,1422677827,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

您好，我想问一下PaddleLite或者是PaddleDetection是否有目标检测+目标跟踪（例如YOLOv5+Deepsort）部署在Android移动端设备实现行人计数或者车辆计数或者钢筋计数等等的功能或者是实现样例呢？

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
没有YoloV4模型？,PaddlePaddle/PaddleDetection,2022-10-25 10:56:39,1,,7188,1422272176,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

我没有看到YoloV4模型的代码, PaddleYolo也没有

https://paddlelite-demo.bj.bcebos.com/NNAdapter/models/PaddleDetection/yolov4_cspdarknet.tgz
这个yolov4的官方模型从何而来"
C++部署PaddleDetection输出路径出错？,PaddlePaddle/PaddleDetection,2022-10-25 08:44:17,2,,7185,1422093654,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

完全按照github上提供的[编译指南]操作(https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.5/deploy/cpp/docs/windows_vs2019_build.md)编译：
不过在使用可执行文件进行预测时输出路径总是输出图片路径
![image](https://user-images.githubusercontent.com/51957420/197725403-4f6adc37-d4de-49b0-84fc-0728492e800d.png)
![image](https://user-images.githubusercontent.com/51957420/197725652-90f72adc-df1c-4892-894d-cfa48bd2ce49.png)
最终把需要预测的图片放入根目录，根据程序显示的输出路径调整图片路径。才让预测结果在根目录的output文件夹成功输出
![image](https://user-images.githubusercontent.com/51957420/197726765-3516bfb2-4e1d-431f-a068-29fc70644c36.png)
![image](https://user-images.githubusercontent.com/51957420/197726821-2c11f8a7-01d4-48a1-bf8c-5592fc139008.png)
编译的代码没有修改，为什么output部分会出现问题呢？
个人的环境：
window10 vs2019 cuda11.2 cudnn8.2 tensorrt8.0.1.6 opencv3.4.6 [预测库使用官网对应预编译版本](https://paddleinference.paddlepaddle.org.cn/master/user_guides/download_lib.html#windows)


"
mot_ppyoloe_l_36e_ppvehicle模型服务部署,PaddlePaddle/PaddleDetection,2022-10-25 07:42:08,26,,7183,1422012047,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

你好，我参考https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.5/deploy/serving
部署mot_ppyoloe_l_36e_ppvehicle模型，我看官方给的样例是yolov3，我使用的是mot_ppyoloe_l_36e_ppvehicle预训练模型，已经成功启动服务，但是在test_client出错：
ValueError: Failed to fetch, maybe the type of [multiclass_nms3_0.tmp_0] is wrong, please check the model file
下面两个是我的模型文件
[serving_server_conf.prototxt.txt](https://github.com/PaddlePaddle/PaddleDetection/files/9858153/serving_server_conf.prototxt.txt)
[serving_client_conf.prototxt.txt](https://github.com/PaddlePaddle/PaddleDetection/files/9858154/serving_client_conf.prototxt.txt)
"
pp-human  中的人体摔倒检测 支持tensorrt  加速吗？,PaddlePaddle/PaddleDetection,2022-10-25 07:38:21,1,,7182,1422007628,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

pp-human  中的人体摔倒检测 支持tensorrt  加速吗？"
关于layout ppyolov2_r50vd_dcn_365e_publaynet config文件配置,PaddlePaddle/PaddleDetection,2022-10-25 06:08:11,1,,7180,1421912334,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

1. 任务目标（请描述你正在做的项目是什么，如模型、论文、项目是什么？）; 2. 需求场景（请描述你的项目中为什么需要用此功能）; 3. 功能描述（请简单描述或设计这个功能）

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
训练时出错,PaddlePaddle/PaddleDetection,2022-10-25 03:34:42,1,,7178,1421789435,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

![图片](https://user-images.githubusercontent.com/93699131/197676226-dfc99057-d92e-4827-be12-0728f02601eb.png)
训练时出现：Target 784 is out of upper bound.的问题，应该如何解决"
车辆的跨境跟踪方案是否有论文出处？,PaddlePaddle/PaddleDetection,2022-10-24 10:40:55,2,,7177,1420618999,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

请问一下飞桨提供的车俩跨境跟踪demo是否有发表过相关论文?如果有是否可以提供下载链接？"
"如何在PP-YOLOE大规模数据集obj365预训练模型的基础上,增加标签，又保持标签",PaddlePaddle/PaddleDetection,2022-10-24 10:17:55,3,,7176,1420588374,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

如何在PP-YOLOE大规模数据集obj365预训练模型的基础上,
增加标签，又保持obj365标签"
多gpu训练报错,PaddlePaddle/PaddleDetection,2022-10-24 07:00:53,3,,7175,1420320562,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

使用 2张显卡训练提示错误


PS D:\python\envs\paddle2\paddledetection> python -m paddle.distributed.launch --gpus 0,1 tools/train.py -c configs/yolov3/yolov3_mobilenet_v1_roadsign.yml
LAUNCH INFO 2022-10-24 14:53:51,624 -----------  Configuration  ----------------------
LAUNCH INFO 2022-10-24 14:53:51,625 devices: None
LAUNCH INFO 2022-10-24 14:53:51,625 elastic_level: -1
LAUNCH INFO 2022-10-24 14:53:51,626 elastic_timeout: 30
LAUNCH INFO 2022-10-24 14:53:51,626 gloo_port: 6767
LAUNCH INFO 2022-10-24 14:53:51,626 host: None
LAUNCH INFO 2022-10-24 14:53:51,626 job_id: default
LAUNCH INFO 2022-10-24 14:53:51,627 legacy: False
LAUNCH INFO 2022-10-24 14:53:51,627 log_dir: log
LAUNCH INFO 2022-10-24 14:53:51,627 log_level: INFO
LAUNCH INFO 2022-10-24 14:53:51,627 master: None
LAUNCH INFO 2022-10-24 14:53:51,627 max_restart: 3
LAUNCH INFO 2022-10-24 14:53:51,627 nnodes: 1
LAUNCH INFO 2022-10-24 14:53:51,628 nproc_per_node: None
LAUNCH INFO 2022-10-24 14:53:51,628 rank: -1
LAUNCH INFO 2022-10-24 14:53:51,628 run_mode: collective
LAUNCH INFO 2022-10-24 14:53:51,628 server_num: None
LAUNCH INFO 2022-10-24 14:53:51,628 servers:
LAUNCH INFO 2022-10-24 14:53:51,629 trainer_num: None
LAUNCH INFO 2022-10-24 14:53:51,629 trainers:
LAUNCH INFO 2022-10-24 14:53:51,629 training_script: 0,1
LAUNCH INFO 2022-10-24 14:53:51,629 training_script_args: ['tools/train.py', '-c', 'configs/yolov3/yolov3_mobilenet_v1_roadsign.yml']
LAUNCH INFO 2022-10-24 14:53:51,629 with_gloo: 0
LAUNCH INFO 2022-10-24 14:53:51,629 --------------------------------------------------
LAUNCH WARNING 2022-10-24 14:53:51,630 Compatible mode enable with args ['--gpus']
-----------  Configuration Arguments -----------
backend: auto
cluster_topo_path: None
elastic_pre_hook: None
elastic_server: None
enable_auto_mapping: False
force: False
gpus: 0,1
heter_devices:
heter_worker_num: None
heter_workers:
host: None
http_port: None
ips: 127.0.0.1
job_id: None
log_dir: log
np: None
nproc_per_node: None
rank_mapping_path: None
run_mode: None
scale: 0
server_num: None
servers:
training_script: tools/train.py
training_script_args: ['-c', 'configs/yolov3/yolov3_mobilenet_v1_roadsign.yml']
worker_num: None
workers:
------------------------------------------------
WARNING 2022-10-24 14:53:51,634 launch.py:518] Not found distinct arguments and compiled with cuda or xpu or npu or mlu. Default use collective mode
WARNING 2022-10-24 14:53:51,634 launch.py:518] Not found distinct arguments and compiled with cuda or xpu or npu or mlu. Default use collective mode
launch train in GPU mode!
INFO 2022-10-24 14:53:51,635 launch_utils.py:558] Local start 2 processes. First process distributed environment info (Only For Debug):
    +=======================================================================================+
    |                        Distributed Envs                      Value                    |
    +---------------------------------------------------------------------------------------+
    |                       PADDLE_TRAINER_ID                        0                      |
    |                 PADDLE_CURRENT_ENDPOINT                 127.0.0.1:1635                |
    |                     PADDLE_TRAINERS_NUM                        2                      |
    |                PADDLE_TRAINER_ENDPOINTS          127.0.0.1:1635,127.0.0.1:1636        |
    |                     PADDLE_RANK_IN_NODE                        0                      |
    |                 PADDLE_LOCAL_DEVICE_IDS                        0                      |
    |                 PADDLE_WORLD_DEVICE_IDS                       0,1                     |
    |                     FLAGS_selected_gpus                        0                      |
    |             FLAGS_selected_accelerators                        0                      |
    +=======================================================================================+

INFO 2022-10-24 14:53:51,635 launch_utils.py:558] Local start 2 processes. First process distributed environment info (Only For Debug):
    +=======================================================================================+
    |                        Distributed Envs                      Value                    |
    +---------------------------------------------------------------------------------------+
    |                       PADDLE_TRAINER_ID                        0                      |
    |                 PADDLE_CURRENT_ENDPOINT                 127.0.0.1:1635                |
    |                     PADDLE_TRAINERS_NUM                        2                      |
    |                PADDLE_TRAINER_ENDPOINTS          127.0.0.1:1635,127.0.0.1:1636        |
    |                     PADDLE_RANK_IN_NODE                        0                      |
    |                 PADDLE_LOCAL_DEVICE_IDS                        0                      |
    |                 PADDLE_WORLD_DEVICE_IDS                       0,1                     |
    |                     FLAGS_selected_gpus                        0                      |
    |             FLAGS_selected_accelerators                        0                      |
    +=======================================================================================+

INFO 2022-10-24 14:53:51,636 launch_utils.py:563] details about PADDLE_TRAINER_ENDPOINTS can be found in log/endpoints.log, and detail running logs maybe found in log/workerlog.0
INFO 2022-10-24 14:53:51,636 launch_utils.py:563] details about PADDLE_TRAINER_ENDPOINTS can be found in log/endpoints.log, and detail running logs maybe found in log/workerlog.0
子目录或文件 -p 已经存在。
处理: -p 时出错。
子目录或文件 log 已经存在。
处理: log 时出错。
'rm' 不是内部或外部命令，也不是可运行的程序
或批处理文件。
子目录或文件 -p 已经存在。
处理: -p 时出错。
子目录或文件 log 已经存在。
处理: log 时出错。
'rm' 不是内部或外部命令，也不是可运行的程序
或批处理文件。
launch proc_id:2984 idx:0
launch proc_id:4072 idx:1
Traceback (most recent call last):
  File ""tools/train.py"", line 172, in <module>
    main()
  File ""tools/train.py"", line 168, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 117, in run
    init_parallel_env()
  File ""D:\python\envs\paddle2\paddledetection\ppdet\engine\env.py"", line 44, in init_parallel_env
    paddle.distributed.init_parallel_env()
  File ""C:\Users\62845\.conda\envs\paddle2\lib\site-packages\paddle\distributed\parallel.py"", line 300, in init_parallel_env
    core.NCCLParallelContext(strategy, place))
AttributeError: module 'paddle.fluid.core_avx' has no attribute 'NCCLParallelContext'
INFO 2022-10-24 14:53:57,702 launch_utils.py:343] terminate all the procs
INFO 2022-10-24 14:53:57,702 launch_utils.py:343] terminate all the procs
ERROR 2022-10-24 14:53:57,705 launch_utils.py:640] ABORT!!! Out of all 2 trainers, the trainer process with rank=[0, 1] was aborted. Please check its log.
ERROR 2022-10-24 14:53:57,705 launch_utils.py:640] ABORT!!! Out of all 2 trainers, the trainer process with rank=[0, 1] was aborted. Please check its log.
INFO 2022-10-24 14:54:00,709 launch_utils.py:343] terminate all the procs
INFO 2022-10-24 14:54:00,709 launch_utils.py:343] terminate all the procs
INFO 2022-10-24 14:54:00,713 launch.py:402] Local processes completed.
INFO 2022-10-24 14:54:00,713 launch.py:402] Local processes completed.
"
热力图绘制,PaddlePaddle/PaddleDetection,2022-10-24 00:59:42,3,,7172,1420019751,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

请问在PaddleDetection下怎么实现热力图的绘制？"
关于paddlevideo的配置文件pptsm_fight_frames_dense.yaml和main.py里train_model_multigrid()模型评估方法,PaddlePaddle/PaddleDetection,2022-10-21 09:49:44,1,,7168,1418069391,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

请问这个模型评估代码的配置文件参数和代码是不是匹配不上
![image](https://user-images.githubusercontent.com/104432024/197166696-77953505-85ec-4838-923a-a8474dc250a0.png)
这个cfg.MULTIGRID.LONG_CYCLE在pptsm_fight_frames_dense.yaml根本不存在，还是我的yml选择错了？"
"convert ppyoloe_plus_crn_s_80e_coco  to onnx is ok, but from ppyoloe_plus_crn_s_80e_coco.onnx to ppyoloe_plus_crn_s_80e_coco.rknn is error",PaddlePaddle/PaddleDetection,2022-10-21 07:37:24,1,,7166,1417897145,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

_No response_

### Bug描述 Describe the Bug

convert ppyoloe_plus_crn_s_80e_coco  to onnx is ok, but from ppyoloe_plus_crn_s_80e_coco.onnx to ppyoloe_plus_crn_s_80e_coco.rknn is error:
W The follow op is not support : ['Resize']
W Not match tensor Reshape_p2o.Reshape.19:out0 
E Try match layer: Reshape_p2o.Reshape.19:out0 failed!

### 复现环境 Environment

linux
paddle:2.2.3

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [X] 我愿意提交PR！I'd like to help by submitting a PR!"
请问怎么在paddle里面实现torch.cdist?,PaddlePaddle/PaddleDetection,2022-10-21 07:17:03,1,,7163,1417871967,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

1. 任务目标（请描述你正在做的项目是什么，如模型、论文、项目是什么？）; 2. 需求场景（请描述你的项目中为什么需要用此功能）; 3. 功能描述（请简单描述或设计这个功能）

任务目标：计算predict anshor 与 target anchor的L1距离。

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
关键点模型速度测试，自训练模型与预训练模型时间差很多,PaddlePaddle/PaddleDetection,2022-10-21 02:58:49,4,,7162,1417629410,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

以下是模型测试的时间
![image](https://user-images.githubusercontent.com/35481439/197100398-4da4b91a-d05b-4e51-8b23-f51b1f6f8959.png)
为什么自己训练的模型比预训练模型慢2倍？
以上测试均基于python deploy/python/det_keypoint_unite_infer.py. 
自训练模型只用来测试时间，所以仅训练了1个epoch, 数据集大小也只有20张"
训练自己的coco关键点数据集，执行infer.py报错,PaddlePaddle/PaddleDetection,2022-10-20 08:00:34,2,,7159,1416154083,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

在colab中执行：!CUDA_VISIBLE_DEVICES=0 python3 tools/infer.py \                 
 -c /content/PaddleDetection/configs/keypoint/hrnet/hrnet_w32_384x288.yml \                  
-o weights=/content/PaddleDetection/output/hrnet_w32_384x288/model_final.pdparams \                 
 --infer_dir=/content/drive/MyDrive/Colab-Notebooks/coco-7/val/ \                  
--draw_threshold=0.5 \                  
--save_results=True
报错

W1020 07:56:11.114243  6820 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.2, Runtime API Version: 11.2
W1020 07:56:11.122241  6820 device_context.cc:465] device: 0, cuDNN Version: 8.1.
[10/20 07:56:15] ppdet.utils.checkpoint INFO: Finish loading model weights: /content/PaddleDetection/output/hrnet_w32_384x288/model_final.pdparams
[10/20 07:56:15] train INFO: Found 84 inference images in total.
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
[10/20 07:56:15] ppdet.data.source.category WARNING: anno_file 'dataset/coco/keypoint_imagelist.txt' is None or not set or not exist, please recheck TrainDataset/EvalDataset/TestDataset.anno_path, otherwise the default categories will be used by metric_type.
  0% 0/84 [00:01<?, ?it/s]
Traceback (most recent call last):
  File ""tools/infer.py"", line 226, in <module>
    main()
  File ""tools/infer.py"", line 222, in main
    run(FLAGS, cfg)
  File ""tools/infer.py"", line 187, in run
    visualize=FLAGS.visualize)
  File ""/content/PaddleDetection/ppdet/engine/trainer.py"", line 882, in predict
    _m.update(data, outs)
  File ""/content/PaddleDetection/ppdet/metrics/keypoint_metrics.py"", line 75, in update
    inputs['center'], paddle.Tensor) else inputs['center'][:, 0:2]
KeyError: 'center'"
PD安装文档快速体验报错 KeyError:36 output没有预测图像输出,PaddlePaddle/PaddleDetection,2022-10-20 07:43:46,7,,7158,1416132264,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

_No response_

### Bug描述 Describe the Bug



python tools/infer.py -c configs/ppyolo/ppyolo_r50vd_dcn_1x_coco.yml -o use_gpu=true weights=https://paddledet.bj.bcebos.com/models/ppyolo_r50vd_dcn_1x_coco.pdparams --infer_img=demo/000000014439.jpg
W1020 14:29:32.354683 10416 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 11.6, Runtime API Version: 10.2
W1020 14:29:32.360683 10416 gpu_resources.cc:91] device: 0, cuDNN Version: 7.6.
[10/20 14:29:32] ppdet.utils.download INFO: Downloading ppyolo_r50vd_dcn_1x_coco.pdparams from https://paddledet.bj.bcebos.com/models/ppyolo_r50vd_dcn_1x_coco.pdparams
100%|███████████████████████████████| 277591/277591 [00:28<00:00, 9576.04KB/s]
[10/20 14:30:03] ppdet.utils.checkpoint INFO: Finish loading model weights: C:\Users\NINGMEI/.cache/paddle/weights\ppyolo_r50vd_dcn_1x_coco.pdparams
loading annotations into memory...
Done (t=0.05s)
creating index...
index created!
100%|██████████████████████████████████████████| 1/1 [09:11<00:00, 551.03s/it]
Traceback (most recent call last):
  File ""tools/infer.py"", line 226, in <module>
    main()
  File ""tools/infer.py"", line 222, in main
    run(FLAGS, cfg)
  File ""tools/infer.py"", line 187, in run
    visualize=FLAGS.visualize)
  File ""D:\pkg\PaddleDetection\ppdet\engine\trainer.py"", line 905, in predict
    batch_res = get_infer_results(outs, clsid2catid)
  File ""D:\pkg\PaddleDetection\ppdet\metrics\coco_utils.py"", line 53, in get_infer_results
    outs['bbox'], outs['bbox_num'], im_id, catid, bias=bias)
  File ""D:\pkg\PaddleDetection\ppdet\metrics\json_results.py"", line 30, in get_det_res
    category_id = label_to_cat_id_map[int(num_id)]
KeyError: 36

### 复现环境 Environment

windows:10
paddlepaddlegpu:2.2.2
paddleDetection: 2.5.0
python: 3.7.13
cuda:10.2
cudnn:7.6.5.32 


### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
PP-Tracking中的车辆跨境跟踪方案是否支持三个及以上的视频输入？,PaddlePaddle/PaddleDetection,2022-10-20 03:31:49,1,,7157,1415880486,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

已成功复现mtmct的车辆跨境跟踪方案，也已经在自己录制的demo视频上运行成功，但目前模型的输入仅为两个视频，请问该算法框架是否支持三个及以上的视频输入，从而进行车辆的连续跨境跟踪？"
中文引号的问题,PaddlePaddle/PaddleDetection,2022-10-20 03:17:22,2,,7156,1415868379,"### 文档链接&描述 Document Links & Description

![image](https://user-images.githubusercontent.com/104432024/196848262-b596510c-c986-458f-8e4c-c61e316b3c4b.png)


### 请提出你的建议 Please give your suggestion

_No response_"
Tinypose augumentation detailed！！！,PaddlePaddle/PaddleDetection,2022-10-20 02:21:56,3,,7155,1415823005,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

Hi,
请问下
1 tinypose 训练的时候，采用了哪些数据增广呢？是否有针对不同的物体或者人 检测框box 尺寸和大小随机调整的数据增广呢？ 具体文档链接是？
2 就是说假如前面输入的人检测出来的box 比人要大的多而且尺寸 宽高比也很异常，那么这种情况的数据增广是怎么处理的呢？ 具体代码体现在哪个地方呢？ 具体代码位置是？
3 gt框 一般比较准确，万一检测box不准确的时候，怎样去学习检测框box 偏差大 尺寸和宽高比都异常的情况，怎么去做相应的数据增广呢？多谢


BR"
🚶‍♀️PP-Human v2全面升级！五大行为识别、视频流功能上新！,PaddlePaddle/PaddleDetection,2022-10-19 16:19:04,2,,7154,1415260835,"### Discussed in https://github.com/PaddlePaddle/PaddleDetection/discussions/6281

<div type='discussions-op-text'>

<sup>Originally posted by **YixinKristy** June 28, 2022</sup>
# 🔔[PP-Human v2](https://github.com/PaddlePaddle/PaddleDetection/tree/develop/deploy/pipeline)强势来袭！
- 五大异常行为一键识别
- 10余种预训练模型一站下载
- 10分钟快速新增识别类型
- 多路视频流部署高性能支持
- 全流程保姆级教程，从技术选型、数据准备到模型部署全覆盖

<div align=""center"">
<img src=""https://user-images.githubusercontent.com/48054808/179469496-c757b432-41d6-4ef7-b932-8f1a08f12219.gif""  width = ""800"" />  
</div>

教程文档：https://github.com/PaddlePaddle/PaddleDetection/tree/develop/deploy/pipeline

欢迎大家扫码进群，方便相互交流沟通

<img width=""300"" alt=""ppyoloe_map_fps"" src=""https://user-images.githubusercontent.com/22989727/196743046-d2527cd1-4c46-4a6c-8779-1921a28aca48.jpg"">"
🚙 PP-Vehicle重磅发新！车牌识别、交通违章核心场景全覆盖！,PaddlePaddle/PaddleDetection,2022-10-19 16:17:37,0,,7153,1415258943,"PP-Vehicle首发，提供车牌识别、车辆属性分析（颜色、车型）、车流量统计以及违章检测四大功能，完善的文档教程支持高效完成二次开发与模型优化

<img src=""https://user-images.githubusercontent.com/22989727/196746975-3a7032bb-ce22-41ee-8b40-f44e006a827a.png""  width = ""800"" />  

教程文档：https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.5/deploy/pipeline

**- 实用性**
    - 针对车辆分析场景共性的底层模型进行优选迭代；
    - 针对高频场景进行详细的后处理策略设计，可以满足业务的快速上线需求;
    - 提供丰富的二次开发教程，方便用户根据自己的业务场景进行私有化开发;
    
**- 泛化性**
    - 公开数据集以及自采数据集上进行充分训练，并且提供预训练模型
    - 覆盖车辆分析中监控视角、驾驶员视角、俯拍视角等常见相机视角。

**- 低代码**
    - 实现1行代码快速部署，支持图片、视频、单路/多路rtsp视频流输入
    - 修改配置文件即可快速实现策略修改以及pipeline的组合。

欢迎大家扫码进群，方便相互交流沟通
<img width=""300"" src=""https://user-images.githubusercontent.com/22989727/196746768-6ff54b1a-57b7-42bc-a328-7d198f619a12.jpg"">"
🚀 PP-YOLOE+高精度云边一体模型已经发布，欢迎大家试用讨论,PaddlePaddle/PaddleDetection,2022-10-19 15:46:23,0,,7152,1415215908,"PP-YOLOE+是基于飞桨云边一体高精度模型PP-YOLOE迭代优化升级版本，具备以下特点

**1. 超强性能**

使用Objects365预训练模型，升级版骨干网络等改动大幅提升模型精度，最高精度提升2.4% mAP，达到54.9% mAP

**2. 训练收敛加速**

基于Objects365预训练模型，减少训练轮数，训练收敛速度提升3.75倍

**3. 下游任务泛化性显著提升**

在农业、夜间安防、工业等不同场景数据集上验证，精度最高提升8.1%

**4. 高性能部署能力**

本次升级PP-YOLOE+支持多种部署方式，包括Python/C++、Serving、ONNX Runtime、ONNX-TRT、INT8量化等部署能力。

PP-YOLOE+教程文档：https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.5/configs/ppyoloe/README_cn.md

为了方便大家相互交流沟通，欢迎大家扫码加入微信群，相关意见、建议和使用中的疑问可以在微信群中交流或者在github中提issue

<img width=""300"" alt=""ppyoloe_map_fps"" src=""https://user-images.githubusercontent.com/22989727/196740057-c5dddc49-34b3-47dc-b159-36d2691a63a2.jpg"">

"
pp-human支持C++部署吗,PaddlePaddle/PaddleDetection,2022-10-19 12:14:08,1,,7151,1414864538,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

pp-human支持C++部署吗"
PPYOLOE在导出的时候不想带post_process和nms，应该如何操作？,PaddlePaddle/PaddleDetection,2022-10-19 09:18:33,3,status/close,7150,1414608316,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

如题，需要把PPYOLOE的模型部署在嵌入式设备上，paddle-lite的性能不理想，所以需要转换成其他框架，但是head中的post_process和nms在转换过程中不支持，如果才能导出不带post_process和nms的模型？"
tinypose训练模型转化为deploy模型异常！！！,PaddlePaddle/PaddleDetection,2022-10-19 08:47:40,3,,7149,1414562707,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

_No response_

### Bug描述 Describe the Bug


按照官方链接和文档 https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.5/configs/keypoint/tiny_pose
进行tinypose模型的转换，如下图所示 从花圈的1 转化为 2 ，发现使用网页推荐的命令 
python3 tools/export_model.py -c configs/keypoint/tiny_pose/tinypose_128x96.yml --output_dir=outut_inference -o weights=output/tinypose_128x96/model_final TestReader.fuse_normalize=true
 和 python3 tools/export_model.py -c configs/keypoint/tiny_pose/tinypose_128x96.yml --output_dir=outut_inference -o weights=output/tinypose_128x96/model_final TestReader.fuse_normalize=false
从pdparams和yml转化出来的 pdmodel 跟文档中给出的模型 https://bj.bcebos.com/v1/paddledet/models/keypoint/tinypose_256x192.pdparams 结果不同。
![4fb2a0038598d905749464a62d72d5f](https://user-images.githubusercontent.com/8407513/196641628-3b1d9fa6-969d-4780-8aa5-7b434368821a.jpg)

请问下，是否可以帮忙检查下如何才能顺利的导出与https://bj.bcebos.com/v1/paddledet/models/keypoint/tinypose_256x192.pdparams 结果一致的模型呢？


### 复现环境 Environment

paddle2onnx                   1.0.1
paddledet                     2.5.0
paddlepaddle-gpu              2.3.1.post112
Python 3.8.10 
GCC 9.4.0
CUDA Version: 11.7

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
PaddleYOLO模型库中导出的部署模型是否可以用于pptracking中的目标跨境跟踪？,PaddlePaddle/PaddleDetection,2022-10-19 07:00:07,4,status/close,7146,1414415750,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

最近在复现PP-Tracking中的车辆跨境跟踪项目，然而跟踪效果很大程度上依赖检测结果，请问PaddleYOLO模型库中例如PP-YOLOE+, YOLOv5的部署模型是否可以直接用于跨境跟踪的目标检测？"
layout analysis 版面分析模型无法训练,PaddlePaddle/PaddleDetection,2022-10-19 03:59:06,5,,7144,1414242079,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Training

### Bug描述 Describe the Bug

版面分析模型无法训练，总是在下载好预训练模型后就卡住无法往下运行然后程序就自动推出。没有任何结果返回。数据集采用的是个人制作的coco数据集。
![record3](https://user-images.githubusercontent.com/62826242/196593711-7a6ccf7b-57a7-43e9-8a1b-a9d0f55e669d.PNG)
经过调试发现trainer.py中的self.loader这个数据加载器无法从里面正常读取数据，读取数据的for循环无法正常跳出。在读取最后一个数据后依旧执行for循环从loader中读取数据。调整batchsize也没有效果
![record](https://user-images.githubusercontent.com/62826242/196594004-179cfe22-dfc6-40a3-8d10-28bf3b99481d.PNG)
对配置文件picodet_lcnet_x1_0_layout.yml的配置如下
![record1](https://user-images.githubusercontent.com/62826242/196594129-ded3cb20-b44d-4396-923a-768e9c396235.PNG)
![record2](https://user-images.githubusercontent.com/62826242/196594134-fd272dbe-6fd3-4208-aaa3-01da8e0f6858.PNG)


### 复现环境 Environment

-OS: Windows
-paddlepaddle: 2.2.2
-python:3.7
-cuda:10.2
-cudnn：7.6

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
PicoDet转成onnx后推理偶尔会出错,PaddlePaddle/PaddleDetection,2022-10-19 03:00:26,1,,7143,1414191322,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

我基于自己的数据集训练了一个picodet_l_320模型，按照教程，先转成静态模型，再转成onnx模型，然后在用onnx推理的时候，遇到某些图，就会出现如下错误：
![image](https://user-images.githubusercontent.com/16351823/196586517-ff55a9ef-46e5-44a7-80ee-c4bd6d94749c.png)
看样子是某个Gather层出错了，用netron查看结构，这个节点在NMS的位置，
![image](https://user-images.githubusercontent.com/16351823/196586649-ab9aaef2-93ce-462b-9e62-53b335b21051.png)
我检查了这张图的输入尺寸和其他图没有什么差别，为什么会报维度错误呢？
我查了一些资料，说是可以用onnxsim做简化后，就不会有这些层了，但是简化后的模型还是会有这些层，还是报同样的错误
![image](https://user-images.githubusercontent.com/16351823/196587059-1fa333ea-ac88-4914-9e51-e92b41d1107c.png)
麻烦帮我解答一下谢谢，猜测可能是将nms一起导出有时候会有问题"
"2448*370,我应该怎么设置默认的800*1333,分辨率图片大小的提问",PaddlePaddle/PaddleDetection,2022-10-19 02:27:07,2,,7142,1414157634,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

原图
2448*370

最小的框为1*1

我应该怎么设置默认的800*1333"
paddle支持人脸检测加人脸识别加活体检测吗,PaddlePaddle/PaddleDetection,2022-10-19 02:09:20,1,,7141,1414137060,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

paddle支持人脸检测加人脸识别加活体检测吗"
end2end_ppyoloe生成onnx模型报错,PaddlePaddle/PaddleDetection,2022-10-18 13:39:49,0,,7139,1413250659,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

![image](https://user-images.githubusercontent.com/21285692/196445015-f012cb55-ab9f-47d4-9495-916ec1b79777.png)
`paddle.__version__ ：2.3.0 `
`PaddleDetection ： release/2.5`
`Python 3.6.13`
`onnx                 1.11.0
onnx-graphsurgeon    0.3.24
onnxruntime          1.10.0
onnxsim              0.4.8
paddle2onnx          1.0.1
`
参照官方ReadeMe文档 : `https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.5/deploy/end2end_ppyoloe`
执行命令：`python ./deploy/end2end_ppyoloe/end2end.py --model-dir output_inference/ppyoloe_crn_s_300e_coco --save-file output_inference/ppyoloe_crn_s_300e_coco.onnx --opset 11 --batch-size 1 --topk-all 100 --iou-thres 0.6 --conf-thres 0.4`
报错信息：
`2022-10-18 21:39:10 [INFO]	ONNX model saved in output_inference/ppyoloe_crn_s_300e_coco.onnx
Traceback (most recent call last):
  File ""./deploy/end2end_ppyoloe/end2end.py"", line 97, in <module>
    main(opt)
  File ""./deploy/end2end_ppyoloe/end2end.py"", line 39, in main
    if node.op == 'Concat' and node.o().op == 'Reshape' and node.o().o().op == 'ReduceSum':
  File ""/home/robot/anaconda3/envs/pp_env/lib/python3.6/site-packages/onnx_graphsurgeon/ir/node.py"", line 89, in o
    return self.outputs[tensor_idx].outputs[consumer_idx]
IndexError: list index out of range
`"
转换出的tinypose openvino模型demo运行异常！！！,PaddlePaddle/PaddleDetection,2022-10-18 08:29:55,0,,7138,1412794295,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

hi，
    请问下paddledetection官方文档里面有个openvino的模型，tinypose256x192 是怎么生成的呢？ 
https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.5/deploy/third_engine/demo_openvino_kpts
![1666081412741](https://user-images.githubusercontent.com/8407513/196377066-9278d03e-72c0-42fe-a81c-b35b886c576f.jpg)
    我尝试了下 用https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.5/deploy/third_engine/demo_openvino_kpts 下面的    python3 mo_onnx.py --input_model <ONNX_MODEL> --mean_values [103.53,116.28,123.675] --scale_values [57.375,57.12,58.395]  去转化，生成了openvino模型后，再次运行tinypose demo发现 idx_data也就是关键点的坐标矩阵都没有正常的结果。 说明模型转化后有问题。  我的原始模型来自于 https://github.com/PaddlePaddle/PaddleDetection/tree/v2.5.0/configs/keypoint/tiny_pose  中的tinypose256x192  
     麻烦帮忙看下到底是哪里的问题呢？  如果想自己生成tinypose256x192的openvino，并且保证程序运行正常  ./tinypose_demo 2  XXX.MP4
，那么如何处理呢？ 多谢 

 "
tinypose openvino 节点缺失，运行异常！,PaddlePaddle/PaddleDetection,2022-10-18 04:04:18,0,,7136,1412536967,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component
_No response_
### Bug描述 Describe the Bug

Hi，

按照 https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.5/deploy/third_engine/demo_openvino_kpts 运行 tinypose_demo 2 VIDEO_PATH  发现对于tinypose2022的模型 和 2021的模型 输出节点名字不同，于是分别做了实验。
从paddle模型转化到onnx 然后再转化为openvino

对于2021的模型能够正常运行，不需要改动git发布的源码，结果关键点的位置虽然有偏差但是大体上算是跑起来了
但是对于2022的模型，必须更改源码里面的输出节点名称，之后可以运行，但是运行到获取输出节点的keypoints index的时候 系统崩掉了，看起来index超出了64x48的范围，这属于模型输出结果异常吧？

补充内容: 
paddle detection 最新release版本里面的openvino  tinypose  demo，发现自己从原始的tinypose 256x192的模型转化到 openvino的模型，运行时候缺少       infer_request_.GetBlob(""save_infer_model/scale_0.tmp_1""); 和      infer_request_.GetBlob(""save_infer_model/scale_1.tmp_1"");
 两个节点信息，导致无法正常运行tinypose模型，如下图所示，麻烦帮忙检查下，多谢

![image](https://user-images.githubusercontent.com/8407513/196332664-d40ab44b-07e1-4b30-90a9-523b9f7663e0.png)




### 复现环境 Environment

![image](https://user-images.githubusercontent.com/8407513/196332871-bd5d0059-40ae-4109-8c2f-3ef69654f1c3.png)


### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
Can slowfast model be integrated into pphuman ,PaddlePaddle/PaddleDetection,2022-10-17 21:32:52,1,,7135,1412256333,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

1. 任务目标（请描述你正在做的项目是什么，如模型、论文、项目是什么？）; 2. 需求场景（请描述你的项目中为什么需要用此功能）; 3. 功能描述（请简单描述或设计这个功能）
I would like to deploy the pphuman module in security monitoring, but the skeleton based action detection provided here can only detect falling. I found the the slowfast model pretrained on AVA dataset is provided in paddlevideo, are there any chance slowfast can be integrated into the pphuman pipeline as well?

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
目标跟踪算法能否支持2d box是不规则多边形呢？,PaddlePaddle/PaddleDetection,2022-10-17 10:30:28,6,,7134,1411327472,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

这边目标跟踪算法能否支持2d box是不规则多边形呢？例如2d框是菱形或者梯形，能否用这边的目标跟踪算法进行跟踪？如果暂时不支持的话，我能否自己修改呢？"
tools/box_distribution.py,PaddlePaddle/PaddleDetection,2022-10-17 09:34:21,8,,7133,1411237569,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

tools/box_distribution.py 是有小bug没有修复吗？

![image](https://user-images.githubusercontent.com/35171131/196143165-715e0cec-79a3-4a28-8d66-66fb84b6e23d.png)

![image](https://user-images.githubusercontent.com/35171131/196143095-ff3bf2f1-bd0f-48b9-bda8-7cd1da9e5ffe.png)

期待大佬指点！感谢！"
车辆跨境跟踪模型MTMCT模型是否包含了评测脚本？,PaddlePaddle/PaddleDetection,2022-10-17 07:50:27,2,,7130,1411082740,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

已经使用ppyolov2检测模型和PPLCNet车辆ReID模型基于python在自己录制的demo视频上实现了跨境跟踪的功能，但是再看输出视频的时候感觉跟踪效果不太好，模型输出的是一个mtmct_result.txt文件，里面包含了视频帧号，目标ID和标定框在图像中的位置，请问是否有评测脚本可以计算模型的IDF1, IDP, IDR ID Switch这些评价指标？"
det的C++部署(CPU版本)在centos8上报错，新旧版本的det我都用过了，还是同样的报错。日志如下,PaddlePaddle/PaddleDetection,2022-10-17 06:41:23,4,,7129,1410998567,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

_No response_

### Bug描述 Describe the Bug

以下是报错，不知道怎么生成不了`CMakeFiles/main.dir/src/main.cc.o`这个文件，已经卡了很久了希望大佬们帮忙解决一下这个部署问题。

 ```
-- Build files have been written to: /root/deploy_water/cpp/build
[  7%] Creating directories for 'ext-yaml-cpp'
[ 14%] Performing download step (download, verify and extract) for 'ext-yaml-cpp'
-- ext-yaml-cpp download command succeeded.  See also /root/deploy_water/cpp/build/ext/yaml-cpp/src/ext-yaml-cpp-stamp/ext-yaml-cpp-download-*.log
[ 21%] No update step for 'ext-yaml-cpp'
[ 28%] No patch step for 'ext-yaml-cpp'
[ 35%] Performing configure step for 'ext-yaml-cpp'
-- The C compiler identification is GNU 8.5.0
-- The CXX compiler identification is GNU 8.5.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Performing Test FLAG_WEXTRA
-- Performing Test FLAG_WEXTRA - Success
-- Configuring done
-- Generating done
-- Build files have been written to: /root/deploy_water/cpp/build/ext/yaml-cpp/src/ext-yaml-cpp-build
[ 42%] Performing build step for 'ext-yaml-cpp'
-- ext-yaml-cpp build command succeeded.  See also /root/deploy_water/cpp/build/ext/yaml-cpp/src/ext-yaml-cpp-stamp/ext-yaml-cpp-build-*.log
[ 50%] No install step for 'ext-yaml-cpp'
[ 57%] Completed 'ext-yaml-cpp'
[ 57%] Built target ext-yaml-cpp
[ 64%] Building CXX object CMakeFiles/main.dir/src/main.cc.o
cc1plus: 错误：给定了太多文件名。试用 cc1plus --help 以了解用法
cc1plus: 致命错误：CMakeFiles/main.dir/src/main.cc.d：没有那个文件或目录
编译中断。
make[2]: *** [CMakeFiles/main.dir/build.make:76：CMakeFiles/main.dir/src/main.cc.o] 错误 1
make[1]: *** [CMakeFiles/Makefile2:111：CMakeFiles/main.dir/all] 错误 2
make: *** [Makefile:91：all] 错误 2
make finished!
 ```


### 复现环境 Environment

- OS: Linux-centos8
- paddle: paddle_inference/manylinux_cpu_avx_mkl_gcc8.2- [今天更新的paddle_inference我也用过了](https://www.paddlepaddle.org.cn/inference/v2.4/guides/install/download_lib.html#linux)
- paddleDet: release/2.4  ---  release/2.5最新的我也在前天试过了
- GCC:  8.5.0 
- cmake: 3.20.2
- cpu推理   ！！！

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [X] 我愿意提交PR！I'd like to help by submitting a PR!"
关于picodet主体检测的输出,PaddlePaddle/PaddleDetection,2022-10-15 10:16:08,0,,7124,1410119189,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

你好，picodet的主体检测不知道为什么检测不出来，静态图和动态图都试了也不行，权重是下载的官方提供的。

脚本如下
```bash
## 动态图预测
python tools/infer.py -c configs\picodet\application\mainbody_detection\picodet_lcnet_x2_5_640_mainbody.yml --infer_dir=demo --output_dir=output  -o weights=models\picodet\picodet_PPLCNet_x2_5_mainbody_lite_v1.0_pretrained.pdparams

## 动态图导出静态图
python tools/export_model.py -c configs\picodet\application\mainbody_detection\picodet_lcnet_x2_5_640_mainbody.yml -o weights=models\picodet\picodet_PPLCNet_x2_5_mainbody_lite_v1.0_pretrained.pdparams --output_dir models

## 静态图预测
python deploy/python/infer.py --model_dir=models\picodet_lcnet_x2_5_640_mainbody --image_file=D:\Data\Retail_Product_Identification\test_image\recognition_2.jpg --device=cpu
```

输出的bbox.json结果如下，但是图片上没有任何标注。

```json
[{""image_id"": 0, ""category_id"": 1, ""bbox"": [-128.05091857910156, -107.67440032958984, 300.4485626220703, 249.3084487915039], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [-40.85092544555664, -107.67440032958984, 300.44858169555664, 249.3084487915039], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [46.34907531738281, -107.67440032958984, 300.4485626220703, 249.3084487915039], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [133.549072265625, -107.67440032958984, 300.4485778808594, 249.3084487915039], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [220.7490692138672, -107.67440032958984, 300.44862365722656, 249.3084487915039], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [-84.450927734375, -72.87439727783203, 300.4485778808594, 249.3084487915039], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [2.749074935913086, -72.87439727783203, 300.4485569000244, 249.3084487915039], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [89.9490737915039, -72.87439727783203, 300.44860076904297, 249.3084487915039], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [177.14907836914062, -72.87439727783203, 300.4485778808594, 249.3084487915039], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [264.3490905761719, -72.87439727783203, 300.4485778808594, 249.3084487915039], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [-128.05091857910156, -38.074398040771484, 300.4485626220703, 249.30845260620117], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [-40.85092544555664, -38.074398040771484, 300.44858169555664, 249.30845260620117], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [46.34907531738281, -38.074398040771484, 300.4485626220703, 249.30845260620117], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [133.549072265625, -38.074398040771484, 300.4485778808594, 249.30845260620117], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [220.7490692138672, -38.074398040771484, 300.44862365722656, 249.30845260620117], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [-84.450927734375, -3.274397850036621, 300.4485778808594, 249.30844020843506], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [2.749074935913086, -3.274397850036621, 300.4485569000244, 249.30844020843506], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [89.9490737915039, -3.274397850036621, 300.44860076904297, 249.30844020843506], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [177.14907836914062, -3.274397850036621, 300.4485778808594, 249.30844020843506], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [264.3490905761719, -3.274397850036621, 300.4485778808594, 249.30844020843506], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [-128.05091857910156, 31.525602340698242, 300.4485626220703, 249.30847358703613], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [-40.85092544555664, 31.525602340698242, 300.44858169555664, 249.30847358703613], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [46.34907531738281, 31.525602340698242, 300.4485626220703, 249.30847358703613], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [133.549072265625, 31.525602340698242, 300.4485778808594, 249.30847358703613], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [220.7490692138672, 31.525602340698242, 300.44862365722656, 249.30847358703613], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [-84.450927734375, 66.32559967041016, 300.4485778808594, 249.30846405029297], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [2.749074935913086, 66.32559967041016, 300.4485569000244, 249.30846405029297], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [89.9490737915039, 66.32559967041016, 300.44860076904297, 249.30846405029297], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [177.14907836914062, 66.32559967041016, 300.4485778808594, 249.30846405029297], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [264.3490905761719, 66.32559967041016, 300.4485778808594, 249.30846405029297], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [-128.05091857910156, 101.12560272216797, 300.4485626220703, 249.3084487915039], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [-40.85092544555664, 101.12560272216797, 300.44858169555664, 249.3084487915039], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [46.34907531738281, 101.12560272216797, 300.4485626220703, 249.3084487915039], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [133.549072265625, 101.12560272216797, 300.4485778808594, 249.3084487915039], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [220.7490692138672, 101.12560272216797, 300.44862365722656, 249.3084487915039], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [-84.450927734375, 135.92559814453125, 300.4485778808594, 249.3084716796875], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [2.749074935913086, 135.92559814453125, 300.4485569000244, 249.3084716796875], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [89.9490737915039, 135.92559814453125, 300.44860076904297, 249.3084716796875], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [177.14907836914062, 135.92559814453125, 300.4485778808594, 249.3084716796875], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [264.3490905761719, 135.92559814453125, 300.4485778808594, 249.3084716796875], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [-128.05091857910156, 170.72560119628906, 300.4485626220703, 249.30845642089844], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [-40.85092544555664, 170.72560119628906, 300.44858169555664, 249.30845642089844], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [46.34907531738281, 170.72560119628906, 300.4485626220703, 249.30845642089844], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [133.549072265625, 170.72560119628906, 300.4485778808594, 249.30845642089844], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [220.7490692138672, 170.72560119628906, 300.44862365722656, 249.30845642089844], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [-84.450927734375, 205.52560424804688, 300.4485778808594, 249.3084716796875], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [2.749074935913086, 205.52560424804688, 300.4485569000244, 249.3084716796875], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [89.9490737915039, 205.52560424804688, 300.44860076904297, 249.3084716796875], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [177.14907836914062, 205.52560424804688, 300.4485778808594, 249.3084716796875], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [264.3490905761719, 205.52560424804688, 300.4485778808594, 249.3084716796875], ""score"": 0.21851232647895813}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [-60.45991516113281, -52.44614791870117, 143.66524505615234, 122.17841720581055], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [-16.859912872314453, -52.44614791870117, 143.66524124145508, 122.17841720581055], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [26.74008560180664, -52.44614791870117, 143.6652488708496, 122.17841720581055], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [70.340087890625, -52.44614791870117, 143.6652374267578, 122.17841720581055], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [113.94007873535156, -52.44614791870117, 143.6652374267578, 122.17841720581055], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [157.5400848388672, -52.44614791870117, 143.6652374267578, 122.17841720581055], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [201.14007568359375, -52.44614791870117, 143.66525268554688, 122.17841720581055], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [244.74008178710938, -52.44614791870117, 143.66525268554688, 122.17841720581055], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [288.340087890625, -52.44614791870117, 143.66525268554688, 122.17841720581055], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [331.9400939941406, -52.44614791870117, 143.66525268554688, 122.17841720581055], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [-38.659912109375, -35.046146392822266, 143.6652374267578, 122.17841720581055], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [4.940086364746094, -35.046146392822266, 143.66524505615234, 122.17841720581055], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [48.54008483886719, -35.046146392822266, 143.6652374267578, 122.17841720581055], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [92.14008331298828, -35.046146392822266, 143.66524505615234, 122.17841720581055], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [135.74008178710938, -35.046146392822266, 143.66525268554688, 122.17841720581055], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [179.340087890625, -35.046146392822266, 143.66525268554688, 122.17841720581055], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [222.94007873535156, -35.046146392822266, 143.66526794433594, 122.17841720581055], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [266.5400695800781, -35.046146392822266, 143.665283203125, 122.17841720581055], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [310.14007568359375, -35.046146392822266, 143.665283203125, 122.17841720581055], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [353.7401123046875, -35.046146392822266, 143.66522216796875, 122.17841720581055], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [-60.45991516113281, -17.646146774291992, 143.66524505615234, 122.17841911315918], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [-16.859912872314453, -17.646146774291992, 143.66524124145508, 122.17841911315918], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [26.74008560180664, -17.646146774291992, 143.6652488708496, 122.17841911315918], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [70.340087890625, -17.646146774291992, 143.6652374267578, 122.17841911315918], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [113.94007873535156, -17.646146774291992, 143.6652374267578, 122.17841911315918], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [157.5400848388672, -17.646146774291992, 143.6652374267578, 122.17841911315918], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [201.14007568359375, -17.646146774291992, 143.66525268554688, 122.17841911315918], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [244.74008178710938, -17.646146774291992, 143.66525268554688, 122.17841911315918], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [288.340087890625, -17.646146774291992, 143.66525268554688, 122.17841911315918], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [331.9400939941406, -17.646146774291992, 143.66525268554688, 122.17841911315918], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [-38.659912109375, -0.24614611268043518, 143.6652374267578, 122.17841997742653], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [4.940086364746094, -0.24614611268043518, 143.66524505615234, 122.17841997742653], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [48.54008483886719, -0.24614611268043518, 143.6652374267578, 122.17841997742653], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [92.14008331298828, -0.24614611268043518, 143.66524505615234, 122.17841997742653], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [135.74008178710938, -0.24614611268043518, 143.66525268554688, 122.17841997742653], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [179.340087890625, -0.24614611268043518, 143.66525268554688, 122.17841997742653], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [222.94007873535156, -0.24614611268043518, 143.66526794433594, 122.17841997742653], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [266.5400695800781, -0.24614611268043518, 143.665283203125, 122.17841997742653], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [310.14007568359375, -0.24614611268043518, 143.665283203125, 122.17841997742653], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [353.7401123046875, -0.24614611268043518, 143.66522216796875, 122.17841997742653], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [-60.45991516113281, 17.153854370117188, 143.66524505615234, 122.17842102050781], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [-16.859912872314453, 17.153854370117188, 143.66524124145508, 122.17842102050781], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [26.74008560180664, 17.153854370117188, 143.6652488708496, 122.17842102050781], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [70.340087890625, 17.153854370117188, 143.6652374267578, 122.17842102050781], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [113.94007873535156, 17.153854370117188, 143.6652374267578, 122.17842102050781], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [157.5400848388672, 17.153854370117188, 143.6652374267578, 122.17842102050781], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [201.14007568359375, 17.153854370117188, 143.66525268554688, 122.17842102050781], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [244.74008178710938, 17.153854370117188, 143.66525268554688, 122.17842102050781], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [288.340087890625, 17.153854370117188, 143.66525268554688, 122.17842102050781], ""score"": 0.1752346009016037}, {""image_id"": 0, ""category_id"": 1, ""bbox"": [331.9400939941406, 17.153854370117188, 143.66525268554688, 122.17842102050781], ""score"": 0.1752346009016037}]
```"
PaddleDetection C++部署不开TensorRT结果正确 使用TensorRT锚框结果会变成NaN,PaddlePaddle/PaddleDetection,2022-10-15 08:40:57,0,,7123,1410096503,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

版本：Cuda 11.6
Cudnn 8.4
TensorRT 8.4.1.5
代码：
![image](https://user-images.githubusercontent.com/109460192/195977550-a9b84905-974a-4dfc-8fea-5fcfbef85ce7.png)
我使用的是PaddleDetection训练的PPyolov2模型，在不使用TensorRT的情况下结果正常，使用TensorRT后结果变成Nan
![image](https://user-images.githubusercontent.com/109460192/195977613-f3c537f5-2ba7-4a95-9c2d-795c82c318d2.png)
是不是图片中提示信息的原因，请问该如何修改"
请问：使用paddledetection并使用TensorRT加速时，每次都要重新生成一次优化序列文件，导致加载速度慢能有方法解决吗？,PaddlePaddle/PaddleDetection,2022-10-14 02:56:48,2,,7117,1408679920,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

使用paddledetection训练并转换PPYOLO_r18vd模型，在使用TensorRT加速部署时，每次都要加载很久
"
基于Paddle有没有自监督或者半自监督的目标检测模型？,PaddlePaddle/PaddleDetection,2022-10-12 13:29:19,1,,7110,1406221169,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

如题"
decord的GPU版本怎么编译啊，有没有可参考的方法。因为总是报这个错,PaddlePaddle/PaddleDetection,2022-10-12 09:51:44,1,,7109,1405913428,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

decord的GPU版本怎么编译啊，有没有可参考的方法。因为总是报这个错
![image](https://user-images.githubusercontent.com/32315802/195311117-628e1969-cc19-4c54-940d-4a650860507f.png)
pip install 只能在无GPU环境下安装，有GPU时没办法安装"
部署c++版本，跟踪代码报错,PaddlePaddle/PaddleDetection,2022-10-12 03:24:02,1,,7100,1405496508,"### 问题描述 Please describe your issue

编译main_jde.cc主程序，代码运行报错。
![image](https://user-images.githubusercontent.com/38750407/195242574-da2ec6c2-3614-4856-946c-09cb426b7362.png)

经过定位，报错位置在tracker.cc中：
![image](https://user-images.githubusercontent.com/38750407/195242165-a0ff3c9a-8c6d-424e-91fc-e2344be21f3c.png)
请问该如何修改？"
各位大佬好，我在利用PPHuman进行吸烟检测时，在pipeline上运行一段视频，为什么程序会卡到第一帧人体检测处，并不会报错？,PaddlePaddle/PaddleDetection,2022-10-12 03:11:40,2,,7099,1405488428,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

我在利用PPHuman进行吸烟检测时，在pipeline上运行一段视频，为什么程序会卡到第一帧人体检测处，并不会报错？"
PPYOLOE+在infer.py使用save_results生成json文件，我想在json文件中添加一个文件名字段，在哪里修改代码？,PaddlePaddle/PaddleDetection,2022-10-12 02:49:08,6,,7098,1405472984,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

PPYOLOE+在infer.py使用save_results生成json文件，我想在json文件中添加一个文件名字段，在哪里修改代码？"
Missing `init_anno_cropper` method in `COCODataSet` class,PaddlePaddle/PaddleDetection,2022-10-11 09:30:42,2,,7097,1404319466,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Training

### Bug描述 Describe the Bug

I'm trying default training command for picodet model with a custom coco-format data. It doesn't let me run my data in `COCODataSet` object:

`Traceback (most recent call last):
  File ""tools/train.py"", line 172, in <module>
    main()
  File ""tools/train.py"", line 168, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 123, in run
    trainer = Trainer(cfg, mode='train')
  File ""/home/PaddleDetection/ppdet/engine/trainer.py"", line 191, in __init__
    self._init_callbacks()
  File ""/home/PaddleDetection/ppdet/engine/trainer.py"", line 203, in _init_callbacks
    self._callbacks.append(SniperProposalsGenerator(self))
  File ""/home/PaddleDetection/ppdet/engine/callbacks.py"", line 441, in __init__
    self.dataset = self._create_new_dataset(ori_dataset)
  File ""/home/PaddleDetection/ppdet/engine/callbacks.py"", line 449, in _create_new_dataset
    dataset.init_anno_cropper()
AttributeError: 'COCODataSet' object has no attribute 'init_anno_cropper'`

### 复现环境 Environment

Docker image: registry.baidubce.com/paddlepaddle/paddle:2.3.2-gpu-cuda11.2-cudnn8
local cuda 11.4

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
PP-YOLOE 后处理的部分源码在哪里,PaddlePaddle/PaddleDetection,2022-10-11 08:18:18,5,,7095,1404209478,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

您好，请问相似与YOLOV4/v5/v6的后处理（exp计算，nms之前）的部分在源码的哪里呢？
感谢！"
PicoDet转onnx的结果与静态图推理结果差异较大,PaddlePaddle/PaddleDetection,2022-10-11 06:19:09,1,,7093,1404065767,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Inference

### Bug描述 Describe the Bug

deploy\third_engine\demo_onnxruntime\infer_demo.py
将PicoDet转成静态图和onnx模型后，推理的结果差异较大，静态图推理结果是正确的，onnx的推理结果较差，经过调试，发现是在onnx的数据预处理时，使用的是BGR模式，没有做BGR2RGB的操作，而静态图的推理是有的，对比如下图所示：
静态图推理预处理：
![image](https://user-images.githubusercontent.com/16351823/195008098-eb5a683f-eb0c-474b-9e12-e41432535e8d.png)
onnx推理预处理：
![image](https://user-images.githubusercontent.com/16351823/195008247-b9eaecca-5971-42bf-b7c4-6323aa6f3de5.png)
此外，两种推理的resize方法也不一样，
静态图推理预处理resize，self,interp = 2
![image](https://user-images.githubusercontent.com/16351823/195014220-50f9db64-0234-4d98-9ed8-6bc3195dc1e5.png)
onnx推理预处理resize，interpolation=3
![image](https://user-images.githubusercontent.com/16351823/195014809-bd81d49b-d382-4828-ae4e-dc236f279f58.png)
这两个地方差异较大，导致最终的输出结果差异较大。

### 复现环境 Environment

-OS:windows
-PaddlePaddle 2.2.2.post112
-PaddleDetection: release/2.4
-python:3.8


### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
2.5版本 s2anet训练报错,PaddlePaddle/PaddleDetection,2022-10-10 07:51:32,4,,7091,1402743593,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

2.5版本 s2anet训练报错：

python tools\train.py -c configs\rotate\s2anet\test_s2anet_alignconv_2x_dota.yml --eval  -o use_gpu=true
Warning: Unable to use JDE/FairMOT/ByteTrack, please install lap, for example: `pip install lap`, see https://github.com/gatagat/lap
Warning: Unable to use OC-SORT, please install filterpy, for example: `pip install filterpy`, see https://github.com/rlabbe/filterpy
Warning: Unable to use MOT metric, please install motmetrics, for example: `pip install motmetrics`, see https://github.com/longcw/py-motmetrics
Warning: Unable to use MCMOT metric, please install motmetrics, for example: `pip install motmetrics`, see https://github.com/longcw/py-motmetrics
Warning: import ppdet from source directory without installing, run 'python setup.py install' to install ppdet firstly
loading annotations into memory...
Done (t=0.30s)
creating index...
index created!
W1010 10:41:12.869786 18520 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 11.6, Runtime API Version: 11.2
W1010 10:41:12.871786 18520 gpu_resources.cc:91] device: 0, cuDNN Version: 8.4.
[10/10 10:41:14] ppdet.utils.checkpoint INFO: ['backbone.conv1.conv1.conv.weight', 'backbone.conv1.conv1.norm._mean', 'backbone.conv1.conv1.norm._variance', 'backbone.conv1.conv1.norm.bias', 'backbone.conv1.conv1.norm.weight', 'backbone.res3.res3a.short.conv.weight', 'backbone.res3.res3a.short.norm._mean', 'backbone.res3.res3a.short.norm._variance', 'backbone.res3.res3a.short.norm.bias', 'backbone.res3.res3a.short.norm.weight', 'backbone.res4.res4a.short.conv.weight', 'backbone.res4.res4a.short.norm._mean', 'backbone.res4.res4a.short.norm._variance', 'backbone.res4.res4a.short.norm.bias', 'backbone.res4.res4a.short.norm.weight', 'backbone.res5.res5a.short.conv.weight', 'backbone.res5.res5a.short.norm._mean', 'backbone.res5.res5a.short.norm._variance', 'backbone.res5.res5a.short.norm.bias', 'backbone.res5.res5a.short.norm.weight', 's2anet_head.align_conv.bias', 's2anet_head.align_conv.weight'] in pretrained weight is not used in the model, and its will not be loaded
[10/10 10:41:14] ppdet.utils.checkpoint INFO: The shape (15,) in pretrained weight s2anet_head.fam_cls.bias is unmatched with the shape [16] in model s2anet_head.fam_cls.bias. And the weight s2anet_head.fam_cls.bias will not be loaded
[10/10 10:41:14] ppdet.utils.checkpoint INFO: The shape (15, 256, 1, 1) in pretrained weight s2anet_head.fam_cls.weight is unmatched with the shape [16, 256, 1, 1] in model s2anet_head.fam_cls.weight. And the weight s2anet_head.fam_cls.weight will not be loaded
[10/10 10:41:14] ppdet.utils.checkpoint INFO: The shape (15,) in pretrained weight s2anet_head.odm_cls.bias is unmatched with the shape [16] in model s2anet_head.odm_cls.bias. And the weight s2anet_head.odm_cls.bias will not be loaded
[10/10 10:41:14] ppdet.utils.checkpoint INFO: The shape (15, 256, 3, 3) in pretrained weight s2anet_head.odm_cls.weight is unmatched with the shape [16, 256, 3, 3] in model s2anet_head.odm_cls.weight. And the weight s2anet_head.odm_cls.weight will not be loaded
[10/10 10:41:14] ppdet.utils.checkpoint INFO: Finish loading model weights: I:/DeepLearningSystem/PaddleDetection/weights/s2anet_conv_1x_dota.pdparams
Traceback (most recent call last):
  File ""I:\DeepLearningSystem\PaddleDetection\tools\train.py"", line 172, in <module>
    main()
  File ""I:\DeepLearningSystem\PaddleDetection\tools\train.py"", line 168, in main
    run(FLAGS, cfg)
  File ""I:\DeepLearningSystem\PaddleDetection\tools\train.py"", line 132, in run
    trainer.train(FLAGS.eval)
  File ""I:\DeepLearningSystem\PaddleDetection\ppdet\engine\trainer.py"", line 506, in train
    outputs = model(data)
  File ""C:\Program Files\Python\Python39\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""C:\Program Files\Python\Python39\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""I:\DeepLearningSystem\PaddleDetection\ppdet\modeling\architectures\meta_arch.py"", line 59, in forward
    out = self.get_loss()
  File ""I:\DeepLearningSystem\PaddleDetection\ppdet\modeling\architectures\s2anet.py"", line 78, in get_loss
    loss = self._forward()
  File ""I:\DeepLearningSystem\PaddleDetection\ppdet\modeling\architectures\s2anet.py"", line 62, in _forward
    loss = self.s2anet_head(body_feats, self.inputs)
  File ""C:\Program Files\Python\Python39\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""C:\Program Files\Python\Python39\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""I:\DeepLearningSystem\PaddleDetection\ppdet\modeling\heads\s2anet_head.py"", line 313, in forward
    return self.get_loss([
  File ""I:\DeepLearningSystem\PaddleDetection\ppdet\modeling\heads\s2anet_head.py"", line 630, in get_loss
    gt_bboxes = paddle.gather(inputs['gt_rbox'][i], gt_idx).numpy()
OSError: (External) CUDA error(700), an illegal memory access was encountered.
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ..\paddle\phi\backends\gpu\cuda\cuda_info.cc:258)

使用DOTA数据集，用[DOTA2COCO](https://github.com/CAPTAIN-WHU/DOTA_devkit/blob/master/DOTA2COCO.py)进行转换
报错信息最后提示的
  File ""I:\DeepLearningSystem\PaddleDetection\ppdet\modeling\heads\s2anet_head.py"", line 630, in get_loss
    gt_bboxes = paddle.gather(inputs['gt_rbox'][i], gt_idx).numpy()
但是根据 configs\datasets\dota.yml，数据里好像没有'gt_rbox'这一项，dota.yml如下：
```
metric: RBOX
num_classes: 16

TrainDataset:
  !COCODataSet
    image_dir: images/train
    anno_path: labels/train.json
    dataset_dir: F:/DOTA_dataset/data_obb/
    data_fields: ['image', 'gt_bbox', 'gt_class', 'is_crowd', 'gt_poly']

EvalDataset:
  !COCODataSet
    image_dir: images/vali
    anno_path: labels/vali.json
    dataset_dir: F:/DOTA_dataset/data_obb/
    data_fields: ['image', 'gt_bbox', 'gt_class', 'is_crowd', 'gt_poly']

TestDataset:
  !ImageFolder
    anno_path: labels/vali.json
    dataset_dir: F:/DOTA_dataset/data_obb/
```

请问怎么处理这个报错"
PP-YOLOE中的检测头为什么没有背景类？如何区分该区域是不是背景类？,PaddlePaddle/PaddleDetection,2022-10-10 06:39:22,3,,7090,1402669960,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

PP-YOLOE中的检测头为什么没有背景类？如何区分该区域是不是背景类？在YOLOX中是（80+4+1）*8400，而PP-YOLOE的检测头确是（80+4）*8400，PP-YOLOE如何区别正负样本？
![image](https://user-images.githubusercontent.com/78945582/194810027-f30cd957-1cc2-49a0-b108-89e18096922b.png)
"
多目标跟踪时，报错,PaddlePaddle/PaddleDetection,2022-10-10 03:44:12,1,windows,7088,1402550392,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

环境：
windows11
python 3.8.13
paddlepaddle 2.3.2
W1010 11:40:49.623167  3004 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 11.7, Runtime API Version: 11.6
W1010 11:40:49.623167  3004 gpu_resources.cc:91] device: 0, cuDNN Version: 8.4.


问题：
使用mot模型做评估或推理时提示错误，请问什么问题？
报错如下：
PS D:\python\envs\paddle2\paddledetection> python tools/infer_mot.py -c configs/mot/fairmot/fairmot_dla34_30e_1088x608.yml -o weights=output/mot/fairmot_dla34_30e_1088x608.pdparams --video_file ..\image\ped2.mp4  --save_videos
W1010 11:27:51.778409  7436 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 11.7, Runtime API Version: 11.6
W1010 11:27:51.794034  7436 gpu_resources.cc:91] device: 0, cuDNN Version: 8.4.
[10/10 11:27:53] ppdet.utils.checkpoint INFO: Finish resuming model weights: output/mot/fairmot_dla34_30e_1088x608.pdparams
[10/10 11:27:55] ppdet.data.source.mot INFO: Length of the video: 587 frames.
[10/10 11:27:55] ppdet.engine.tracker INFO: Starting tracking video ..\image\ped2.mp4
  0%|                                                                                                                                                                                                                                                                                          | 0/587 [00:00<?, ?it/s]E
rror: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
  0%|                                                                                                                                                                                                                                                                                          | 0/587 [00:02<?, ?it/s]
Traceback (most recent call last):
  File ""tools/infer_mot.py"", line 146, in <module>
    main()
  File ""tools/infer_mot.py"", line 142, in main
    run(FLAGS, cfg)
  File ""tools/infer_mot.py"", line 99, in run
    tracker.mot_predict_seq(
  File ""D:\python\envs\paddle2\paddledetection\ppdet\engine\tracker.py"", line 573, in mot_predict_seq
    results, nf, ta, tc = self._eval_seq_jde(
  File ""D:\python\envs\paddle2\paddledetection\ppdet\engine\tracker.py"", line 154, in _eval_seq_jde
    pred_dets, pred_embs = self.model(data)
  File ""C:\Users\62845\.conda\envs\paddle2\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""C:\Users\62845\.conda\envs\paddle2\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""D:\python\envs\paddle2\paddledetection\ppdet\modeling\architectures\meta_arch.py"", line 75, in forward
    outs.append(self.get_pred())
  File ""D:\python\envs\paddle2\paddledetection\ppdet\modeling\architectures\fairmot.py"", line 95, in get_pred
    output = self._forward()
  File ""D:\python\envs\paddle2\paddledetection\ppdet\modeling\architectures\fairmot.py"", line 75, in _forward
    det_outs = self.detector(self.inputs)
  File ""C:\Users\62845\.conda\envs\paddle2\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""C:\Users\62845\.conda\envs\paddle2\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""D:\python\envs\paddle2\paddledetection\ppdet\modeling\architectures\meta_arch.py"", line 75, in forward
    outs.append(self.get_pred())
  File ""D:\python\envs\paddle2\paddledetection\ppdet\modeling\architectures\centernet.py"", line 82, in get_pred
    bbox, bbox_inds, topk_clses = self.post_process(
  File ""D:\python\envs\paddle2\paddledetection\ppdet\modeling\post_process.py"", line 409, in __call__
    scores, inds, topk_clses, ys, xs = self._topk(heat)
  File ""D:\python\envs\paddle2\paddledetection\ppdet\modeling\layers.py"", line 829, in _topk
    topk_xs = topk_inds % width
  File ""C:\Users\62845\.conda\envs\paddle2\lib\site-packages\paddle\fluid\dygraph\math_op_patch.py"", line 299, in __impl__
    return math_op(self, other_var, 'axis', axis)
OSError: (External) CUDA error(719), unspecified launch failure.
  [Hint: 'cudaErrorLaunchFailure'. An exception occurred on the device while executing a kernel. Common causes include dereferencing an invalid device pointerand accessing out of bounds shared memory. Less common cases can be system specific - more information about these cases canbe found in the system specifi
c user guide. This leaves the process in an inconsistent state and any further CUDA work willreturn the same error. To continue using CUDA, the process must be terminated and relaunched.] (at ..\paddle\phi\backends\gpu\gpu_context.cc:435)
  [operator < elementwise_mod > error]
"
PPYOLOE里面的Backbone（cspresnet-s/-m/-l/-x）有什么区别？,PaddlePaddle/PaddleDetection,2022-10-10 03:42:48,1,,7087,1402549545,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

![image](https://user-images.githubusercontent.com/78945582/194796372-895cd71a-5215-493b-9689-0b823a3c9389.png)
PPYOLOE里面的Backbone（cspresnet-s/cspresnet-m/cspresnet-l/cspresnet-x）有什么区别？具体哪个地方不一样？"
demo_openvino_kpts明显精度损失,PaddlePaddle/PaddleDetection,2022-10-10 00:51:29,14,,7082,1402457324,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Deploy

### Bug描述 Describe the Bug

完整运行demo, 发现关键点预测结果不符合预期。因xml输出节点名称有变化，修改输出节点信息如下：
      {""save_infer_model/scale_0.tmp_1"", ""save_infer_model/scale_1.tmp_1"", 8},
      {""save_infer_model/scale_2.tmp_1"", ""save_infer_model/scale_3.tmp_1"", 16},
      {""save_infer_model/scale_4.tmp_1"", ""save_infer_model/scale_5.tmp_1"", 32},
      {""save_infer_model/scale_6.tmp_1"", ""save_infer_model/scale_7.tmp_1"", 64},



尝试自己的手动转模型以及用py代码复现，也发现关键点模型输出与paddle不一致，坐标有明显的偏移。对模型仅仅输入和输出也会发现无法精度对齐。

### 复现环境 Environment

-CUDA 10.2
- openvino： l_openvino_toolkit_ubuntu18_2022.2.0.7713

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [X] 我愿意提交PR！I'd like to help by submitting a PR!"
每次训练到到100个epoch 就报错 ,PaddlePaddle/PaddleDetection,2022-10-09 15:40:45,5,windows,7081,1402312494,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

报错信息如下：
[10/09 23:28:44] ppdet.engine INFO: Epoch: [96] [0/7] learning_rate: 0.008464 loss: 0.000080 loss_cls: 0.000080 loss_iou: 0.000000 loss_dfl: 0.000000 loss_l1: 0.000000 eta: 3:35:08 batch_cost: 9.1311 data_cost: 8.3457 ips: 0.8761 images/s
[10/09 23:29:48] ppdet.engine INFO: Epoch: [97] [0/7] learning_rate: 0.008432 loss: 0.000078 loss_cls: 0.000078 loss_iou: 0.000000 loss_dfl: 0.000000 loss_l1: 0.000000 eta: 3:34:06 batch_cost: 9.1472 data_cost: 8.3619 ips: 0.8746 images/s
[10/09 23:30:53] ppdet.engine INFO: Epoch: [98] [0/7] learning_rate: 0.008400 loss: 0.000078 loss_cls: 0.000078 loss_iou: 0.000000 loss_dfl: 0.000000 loss_l1: 0.000000 eta: 3:33:06 batch_cost: 9.1677 data_cost: 8.3838 ips: 0.8726 images/s
[10/09 23:31:44] ppdet.engine INFO: Epoch: [99] [0/7] learning_rate: 0.008368 loss: 0.000078 loss_cls: 0.000078 loss_iou: 0.000000 loss_dfl: 0.000000 loss_l1: 0.000000 eta: 3:31:35 batch_cost: 9.0041 data_cost: 8.2190 ips: 0.8885 images/s
[10/09 23:32:33] ppdet.utils.checkpoint INFO: Save checkpoint: output\ppyoloe_crn_l_300e_coco
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [1] and greater than or equal to 0, but received [0]
Traceback (most recent call last):
  File ""D:\ai\PaddleDetection\tools\train.py"", line 172, in <module>
    main()
  File ""D:\ai\PaddleDetection\tools\train.py"", line 168, in main
    run(FLAGS, cfg)
  File ""D:\ai\PaddleDetection\tools\train.py"", line 132, in run
    trainer.train(FLAGS.eval)
  File ""D:\ai\PaddleDetection\ppdet\engine\trainer.py"", line 485, in train
    outputs = model(data)
  File ""C:\Users\Lenovo\miniconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""C:\Users\Lenovo\miniconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""D:\ai\PaddleDetection\ppdet\modeling\architectures\meta_arch.py"", line 59, in forward
    out = self.get_loss()
  File ""D:\ai\PaddleDetection\ppdet\modeling\architectures\yolo.py"", line 124, in get_loss
    return self._forward()
  File ""D:\ai\PaddleDetection\ppdet\modeling\architectures\yolo.py"", line 88, in _forward
    yolo_losses = self.yolo_head(neck_feats, self.inputs)
  File ""C:\Users\Lenovo\miniconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""C:\Users\Lenovo\miniconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""D:\ai\PaddleDetection\ppdet\modeling\heads\ppyoloe_head.py"", line 219, in forward
    return self.forward_train(feats, targets)
  File ""D:\ai\PaddleDetection\ppdet\modeling\heads\ppyoloe_head.py"", line 161, in forward_train
    return self.get_loss([
  File ""D:\ai\PaddleDetection\ppdet\modeling\heads\ppyoloe_head.py"", line 325, in get_loss
    self.assigner(
  File ""C:\Users\Lenovo\miniconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""C:\Users\Lenovo\miniconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""C:\Users\Lenovo\miniconda3\envs\paddle\lib\site-packages\decorator.py"", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File ""C:\Users\Lenovo\miniconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\base.py"", line 354, in _decorate_function
    return func(*args, **kwargs)
  File ""D:\ai\PaddleDetection\ppdet\modeling\assigners\task_aligned_assigner.py"", line 114, in forward
    is_in_topk = gather_topk_anchors(
  File ""D:\ai\PaddleDetection\ppdet\modeling\assigners\utils.py"", line 105, in gather_topk_anchors
    return is_in_topk * topk_mask
  File ""C:\Users\Lenovo\miniconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\math_op_patch.py"", line 299, in __impl__
    return math_op(self, other_var, 'axis', axis)
OSError: (External) CUDA error(719), unspecified launch failure.
  [Hint: 'cudaErrorLaunchFailure'. An exception occurred on the device while executing a kernel. Common causes include dereferencing an invalid device pointerand accessing out of bounds shared memory. Less common cases can be system specific - more information about these cases canbe found in the system specific user guide. This leaves the process in an inconsistent state and any further CUDA work willreturn the same error. To continue using CUDA, the process must be terminated and relaunched.] (at ..\paddle\phi\backends\gpu\cuda\cuda_info.cc:258)
  [operator < elementwise_mul > error]

系统环境：win11 ,cuda 11.6 paddle 2.3.2  coco 数据集
python tools/train.py -c configs/ppyoloe/ppyoloe_crn_l_300e_coco.yml --use_vdl=True --vdl_log_dir=./ppyoloe/ --amp
"
pp-tracking cpp 推理结果错误,PaddlePaddle/PaddleDetection,2022-10-09 10:04:07,1,,7080,1402223425,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Inference

### Bug描述 Describe the Bug

你好：
在使用[PP-Tracking之手把手玩转多目标跟踪](https://aistudio.baidu.com/aistudio/projectdetail/3022582)案例应用过程中，跟踪结果正常，图片如下：
<img width=""1062"" alt=""image"" src=""https://user-images.githubusercontent.com/44053467/194750132-f073a751-da17-41b5-98e5-24feb4d528fb.png"">
从aistudio下载inference模型和视频后，在编译的pptracking cpp上运行结果不正常，预测命令：
```
./main --video_file=person.mp4 --track_model_dir ./models
# 其中./models存放的是aistudio下载下来的inference模型
```
结果如图：
<img width=""1301"" alt=""image"" src=""https://user-images.githubusercontent.com/44053467/194750182-f7707103-e0e5-4278-91db-a2e04624e12a.png"">
请问一下，是什么原因导致的，谢谢！
在编译过程中，CMakeLists.txt文件貌似对mac支持不太好，编译会报错，所以改了部分编译选项，文件内容如下：
```cmake
cmake_minimum_required(VERSION 3.0)
project(PaddleObjectDetector CXX C)

option(WITH_MKL        ""Compile demo with MKL/OpenBlas support,defaultuseMKL.""          ON)
option(WITH_GPU        ""Compile demo with GPU/CPU, default use CPU.""                    ON)
option(WITH_TENSORRT   ""Compile demo with TensorRT.""                                    OFF)

SET(PADDLE_DIR """" CACHE PATH ""Location of libraries"")
SET(PADDLE_LIB_NAME """" CACHE STRING ""libpaddle_inference"")
SET(OPENCV_DIR """" CACHE PATH ""Location of libraries"")
SET(CUDA_LIB """" CACHE PATH ""Location of libraries"")
SET(CUDNN_LIB """" CACHE PATH ""Location of libraries"")
SET(TENSORRT_INC_DIR """" CACHE PATH ""Compile demo with TensorRT"")
SET(TENSORRT_LIB_DIR """" CACHE PATH ""Compile demo with TensorRT"")

include(cmake/yaml-cpp.cmake)

include_directories(""${CMAKE_SOURCE_DIR}/"")
include_directories(""${CMAKE_CURRENT_BINARY_DIR}/ext/yaml-cpp/src/ext-yaml-cpp/include"")
link_directories(""${CMAKE_CURRENT_BINARY_DIR}/ext/yaml-cpp/lib"")

set(SRCS src/main.cc src/preprocess_op.cc src/pipeline.cc src/jde_predictor.cc src/sde_predictor.cc src/tracker.cc src/trajectory.cc src/lapjv.cpp src/postprocess.cc)

macro(safe_set_static_flag)
    foreach(flag_var
        CMAKE_CXX_FLAGS CMAKE_CXX_FLAGS_DEBUG CMAKE_CXX_FLAGS_RELEASE
        CMAKE_CXX_FLAGS_MINSIZEREL CMAKE_CXX_FLAGS_RELWITHDEBINFO)
      if(${flag_var} MATCHES ""/MD"")
        string(REGEX REPLACE ""/MD"" ""/MT"" ${flag_var} ""${${flag_var}}"")
      endif(${flag_var} MATCHES ""/MD"")
    endforeach(flag_var)
endmacro()

if (WITH_MKL)
    ADD_DEFINITIONS(-DUSE_MKL)
endif()

if (NOT DEFINED PADDLE_DIR OR ${PADDLE_DIR} STREQUAL """")
    message(FATAL_ERROR ""please set PADDLE_DIR with -DPADDLE_DIR=/path/paddle_influence_dir"")
endif()
message(""PADDLE_DIR IS:"" ${PADDLE_DIR})

if (NOT DEFINED OPENCV_DIR OR ${OPENCV_DIR} STREQUAL """")
    set(OpenCV_DIR ${OPENCV_DIR})
    message(FATAL_ERROR ""please set OPENCV_DIR with -DOPENCV_DIR=/path/opencv"")
endif()

include_directories(""${CMAKE_SOURCE_DIR}/"")
include_directories(""${PADDLE_DIR}/"")
include_directories(""${PADDLE_DIR}/third_party/install/protobuf/include"")
include_directories(""${PADDLE_DIR}/third_party/install/glog/include"")
include_directories(""${PADDLE_DIR}/third_party/install/gflags/include"")
include_directories(""${PADDLE_DIR}/third_party/install/xxhash/include"")
if (EXISTS ""${PADDLE_DIR}/third_party/install/snappy/include"")
    include_directories(""${PADDLE_DIR}/third_party/install/snappy/include"")
endif()
if(EXISTS ""${PADDLE_DIR}/third_party/install/snappystream/include"")
    include_directories(""${PADDLE_DIR}/third_party/install/snappystream/include"")
endif()
include_directories(""${PADDLE_DIR}/third_party/boost"")
include_directories(""${PADDLE_DIR}/third_party/eigen3"")

if (EXISTS ""${PADDLE_DIR}/third_party/install/snappy/lib"")
    link_directories(""${PADDLE_DIR}/third_party/install/snappy/lib"")
endif()
if(EXISTS ""${PADDLE_DIR}/third_party/install/snappystream/lib"")
    link_directories(""${PADDLE_DIR}/third_party/install/snappystream/lib"")
endif()

link_directories(""${PADDLE_DIR}/third_party/install/protobuf/lib"")
link_directories(""${PADDLE_DIR}/third_party/install/glog/lib"")
link_directories(""${PADDLE_DIR}/third_party/install/gflags/lib"")
link_directories(""${PADDLE_DIR}/third_party/install/xxhash/lib"")
link_directories(""${PADDLE_DIR}/paddle/lib/"")
link_directories(""${CMAKE_CURRENT_BINARY_DIR}"")



if (WIN32)
  include_directories(""${PADDLE_DIR}/paddle/fluid/inference"")
  include_directories(""${PADDLE_DIR}/paddle/include"")
  link_directories(""${PADDLE_DIR}/paddle/fluid/inference"")
  find_package(OpenCV REQUIRED PATHS ${OPENCV_DIR} NO_DEFAULT_PATH)

else ()
  find_package(OpenCV REQUIRED PATHS ${OPENCV_DIR} NO_DEFAULT_PATH)
  include_directories(""${PADDLE_DIR}/paddle/include"")
  link_directories(""${PADDLE_DIR}/paddle/lib"")
endif ()
include_directories(${OpenCV_INCLUDE_DIRS})

if (WIN32)
    add_definitions(""/DGOOGLE_GLOG_DLL_DECL="")
    set(CMAKE_C_FLAGS_DEBUG   ""${CMAKE_C_FLAGS_DEBUG} /bigobj /MTd"")
    set(CMAKE_C_FLAGS_RELEASE  ""${CMAKE_C_FLAGS_RELEASE} /bigobj /MT"")
    set(CMAKE_CXX_FLAGS_DEBUG  ""${CMAKE_CXX_FLAGS_DEBUG} /bigobj /MTd"")
    set(CMAKE_CXX_FLAGS_RELEASE   ""${CMAKE_CXX_FLAGS_RELEASE} /bigobj /MT"")
else()
    set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -g -o2 -std=c++11 -Wno-format -Wunused-value"")
    set(CMAKE_STATIC_LIBRARY_PREFIX """")
endif()

# TODO let users define cuda lib path
if (WITH_GPU)
    if (NOT DEFINED CUDA_LIB OR ${CUDA_LIB} STREQUAL """")
        message(FATAL_ERROR ""please set CUDA_LIB with -DCUDA_LIB=/path/cuda-8.0/lib64"")
    endif()
    if (NOT WIN32)
        if (NOT DEFINED CUDNN_LIB)
            message(FATAL_ERROR ""please set CUDNN_LIB with -DCUDNN_LIB=/path/cudnn_v7.4/cuda/lib64"")
        endif()
    endif(NOT WIN32)
endif()


if (NOT WIN32)
  if (WITH_TENSORRT AND WITH_GPU)
	  include_directories(""${TENSORRT_INC_DIR}/"")
	  link_directories(""${TENSORRT_LIB_DIR}/"")
  endif()
endif(NOT WIN32)

if (NOT WIN32)
    set(NGRAPH_PATH ""${PADDLE_DIR}/third_party/install/ngraph"")
    if(EXISTS ${NGRAPH_PATH})
        include(GNUInstallDirs)
        include_directories(""${NGRAPH_PATH}/include"")
        link_directories(""${NGRAPH_PATH}/${CMAKE_INSTALL_LIBDIR}"")
        set(NGRAPH_LIB ${NGRAPH_PATH}/${CMAKE_INSTALL_LIBDIR}/libngraph${CMAKE_SHARED_LIBRARY_SUFFIX})
    endif()
endif()

if(WITH_MKL)
  include_directories(""${PADDLE_DIR}/third_party/install/mklml/include"")
  if (WIN32)
    set(MATH_LIB ${PADDLE_DIR}/third_party/install/mklml/lib/mklml.lib
            ${PADDLE_DIR}/third_party/install/mklml/lib/libiomp5md.lib)
  else ()
    set(MATH_LIB ${PADDLE_DIR}/third_party/install/mklml/lib/libmklml_intel${CMAKE_SHARED_LIBRARY_SUFFIX}
            ${PADDLE_DIR}/third_party/install/mklml/lib/libiomp5${CMAKE_SHARED_LIBRARY_SUFFIX})
    execute_process(COMMAND cp -r ${PADDLE_DIR}/third_party/install/mklml/lib/libmklml_intel${CMAKE_SHARED_LIBRARY_SUFFIX} /usr/lib)
  endif ()
  set(MKLDNN_PATH ""${PADDLE_DIR}/third_party/install/mkldnn"")
  if(EXISTS ${MKLDNN_PATH})
    include_directories(""${MKLDNN_PATH}/include"")
    if (WIN32)
      set(MKLDNN_LIB ${MKLDNN_PATH}/lib/mkldnn.lib)
    else ()
      set(MKLDNN_LIB ${MKLDNN_PATH}/lib/libmkldnn.so.0)
    endif ()
  endif()
else()
  set(MATH_LIB ${PADDLE_DIR}/third_party/install/openblas/lib/libopenblas${CMAKE_STATIC_LIBRARY_SUFFIX})
endif()


if (WIN32)
    if(EXISTS ""${PADDLE_DIR}/paddle/fluid/inference/${PADDLE_LIB_NAME}${CMAKE_STATIC_LIBRARY_SUFFIX}"")
        set(DEPS
            ${PADDLE_DIR}/paddle/fluid/inference/${PADDLE_LIB_NAME}${CMAKE_STATIC_LIBRARY_SUFFIX})
    else()
        set(DEPS
            ${PADDLE_DIR}/paddle/lib/${PADDLE_LIB_NAME}${CMAKE_STATIC_LIBRARY_SUFFIX})
    endif()
endif()


if (WIN32)
    set(DEPS ${PADDLE_DIR}/paddle/lib/${PADDLE_LIB_NAME}${CMAKE_STATIC_LIBRARY_SUFFIX})
else()
    set(DEPS ${PADDLE_DIR}/paddle/lib/${PADDLE_LIB_NAME}${CMAKE_SHARED_LIBRARY_SUFFIX})
endif()

message(""PADDLE_LIB_NAME:"" ${PADDLE_LIB_NAME})
message(""DEPS:"" $DEPS)

if (NOT WIN32)
    set(DEPS ${DEPS}
        ${MATH_LIB} ${MKLDNN_LIB}
        glog gflags protobuf z xxhash yaml-cpp
        )
    if(EXISTS ""${PADDLE_DIR}/third_party/install/snappystream/lib"")
        set(DEPS ${DEPS} snappystream)
    endif()
    if (EXISTS ""${PADDLE_DIR}/third_party/install/snappy/lib"")
        set(DEPS ${DEPS} snappy)
    endif()
else()
    set(DEPS ${DEPS}
        ${MATH_LIB} ${MKLDNN_LIB}
        glog gflags_static libprotobuf xxhash libyaml-cppmt)
    set(DEPS ${DEPS} libcmt shlwapi)
    if (EXISTS ""${PADDLE_DIR}/third_party/install/snappy/lib"")
        set(DEPS ${DEPS} snappy)
    endif()
    if(EXISTS ""${PADDLE_DIR}/third_party/install/snappystream/lib"")
        set(DEPS ${DEPS} snappystream)
    endif()
endif(NOT WIN32)

if(WITH_GPU)
  if(NOT WIN32)
    if (WITH_TENSORRT)
	    set(DEPS ${DEPS} ${TENSORRT_LIB_DIR}/libnvinfer${CMAKE_SHARED_LIBRARY_SUFFIX})
	    set(DEPS ${DEPS} ${TENSORRT_LIB_DIR}/libnvinfer_plugin${CMAKE_SHARED_LIBRARY_SUFFIX})
    endif()
    set(DEPS ${DEPS} ${CUDA_LIB}/libcudart${CMAKE_SHARED_LIBRARY_SUFFIX})
    set(DEPS ${DEPS} ${CUDNN_LIB}/libcudnn${CMAKE_SHARED_LIBRARY_SUFFIX})
  else()
    set(DEPS ${DEPS} ${CUDA_LIB}/cudart${CMAKE_STATIC_LIBRARY_SUFFIX} )
    set(DEPS ${DEPS} ${CUDA_LIB}/cublas${CMAKE_STATIC_LIBRARY_SUFFIX} )
    set(DEPS ${DEPS} ${CUDNN_LIB}/cudnn${CMAKE_STATIC_LIBRARY_SUFFIX})
  endif()
endif()

if (NOT WIN32)
    set(EXTERNAL_LIB ""-ldl  -lz -lm -lpthread"")
    set(DEPS ${DEPS} ${EXTERNAL_LIB})
endif()

set(DEPS ${DEPS} ${OpenCV_LIBS})
add_executable(main ${SRCS})
ADD_DEPENDENCIES(main ext-yaml-cpp)
message(""DEPS:"" ${DEPS})
target_link_libraries(main ${DEPS})

if (WIN32 AND WITH_MKL)
    add_custom_command(TARGET main POST_BUILD
        COMMAND ${CMAKE_COMMAND} -E copy_if_different ${PADDLE_DIR}/third_party/install/mklml/lib/mklml.dll ./mklml.dll
        COMMAND ${CMAKE_COMMAND} -E copy_if_different ${PADDLE_DIR}/third_party/install/mklml/lib/libiomp5md.dll ./libiomp5md.dll
        COMMAND ${CMAKE_COMMAND} -E copy_if_different ${PADDLE_DIR}/third_party/install/mkldnn/lib/mkldnn.dll ./mkldnn.dll
        COMMAND ${CMAKE_COMMAND} -E copy_if_different ${PADDLE_DIR}/third_party/install/mklml/lib/mklml.dll ./release/mklml.dll
        COMMAND ${CMAKE_COMMAND} -E copy_if_different ${PADDLE_DIR}/third_party/install/mklml/lib/libiomp5md.dll ./release/libiomp5md.dll
        COMMAND ${CMAKE_COMMAND} -E copy_if_different ${PADDLE_DIR}/third_party/install/mkldnn/lib/mkldnn.dll ./release/mkldnn.dll
        COMMAND ${CMAKE_COMMAND} -E copy_if_different ${PADDLE_DIR}/paddle/lib/${PADDLE_LIB_NAME}.dll ./release/${PADDLE_LIB_NAME}.dll
    )
endif()

if (WIN32)
    add_custom_command(TARGET main POST_BUILD
        COMMAND ${CMAKE_COMMAND} -E copy_if_different ${PADDLE_DIR}/paddle/lib/${PADDLE_LIB_NAME}.dll ./release/${PADDLE_LIB_NAME}.dll
    )
endif()
```
其中主要对如下进行修改：
1. `set(EXTERNAL_LIB ""-ldl  -lz -lm -lpthread"")`
2. `set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -g -o2 -std=c++11 -Wno-format -Wunused-value"")`
麻烦官方大佬可以用macos复现并优化一下cmake文件么（本人太菜，暂时没法pr），谢谢！！！


### 复现环境 Environment

- OS:macos 12.2 intel x86架构
- paddlepaddle：2.3.2（下载paddle_nference的cpp库）
- cpu推理
- PaddleDetection  最新拉取release/2.5分支

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
pp-tracking  bytetrack模型支持问题,PaddlePaddle/PaddleDetection,2022-10-09 08:10:54,2,deploy#mot,7079,1402193213,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

请问一下，目前pp-tracking c++部署支持bytetrack 不需要reid的模型部署么？"
最新版使用save_results推理时生成的json文件如何处理？,PaddlePaddle/PaddleDetection,2022-10-09 07:58:08,1,,7077,1402189916,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

1. 任务目标：使用ppyoloe+参加paddle学习赛《钢铁缺陷检测》; 
2. 模型都训练好了，使用infer.py推理时，以前版本的save_txt选项没了，新版本save_results生成的json文件明显不好处理，不明白为啥要改了。txt不是更通俗易处理吗？
3. 新生成的json几百个bbox序列，每个序列几千个bbox，是所有有概率的目标都预测出来了吗？该怎么处理得到最终的预测信息呢？（每张图预测框类别、预测框坐标、置信度）
4. 看版本更新有《基于PP-YOLOE+的PCB电路板缺陷检测》的产业实践范例教程，怎么也找不到，能给个地址么？
5. 产品更新那么快，是不是说明文档和教程也应该同步更新呢？找资料都找不到，岂不是将很多人拒之门外？

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
labelme标注关键点无法通过x2coco转换为coco数据格式,PaddlePaddle/PaddleDetection,2022-10-09 07:51:52,1,,7076,1402188467,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

DataProcess

### Bug描述 Describe the Bug

在教程文档[KeyPointAnnoTools](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.5/docs/tutorials/data/KeyPointAnnoTools.md)中。按照gif图新建类型为point的关键点后通过x2coco.py转换发现没有生成数据集。如下图所示。
进一步分析，x2coco.py代码中并未对point类型进行keypoint数组生成。实际上根本没有进行关键点处理。
<img width=""326"" alt=""image"" src=""https://user-images.githubusercontent.com/20356658/194744566-d6ad2871-5d23-48b5-a963-7cca96b95b8e.png"">


### 复现环境 Environment

- OS: Linux
- PaddlePaddle: 2.2.2
- PaddleDetection: release/2.5
- Python: 3.8
- CUDA: 10.2
- CUDNN: 7.6
- GCC: 8.2.0

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
摔倒识别模型,PaddlePaddle/PaddleDetection,2022-10-09 03:15:07,0,,7075,1402133453,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

如果传入h264编码的rtsp视频流，每次运行大概30秒程序就会断掉，并且在output下生成一个损坏的视频文件
操作系统：windows10
显卡：NVIDIA 3070
cuda11.0
cudnn 8
paddlepaddle-gpu 2.3.0 post110
谢谢解答~

C:\Users\14548\Anaconda3\python.exe D:/PycharmProjects/Projects/action_detection/Falling_Detection2.5/pipeline/pipeline.py
-----------  Running Arguments -----------
ATTR:
  batch_size: 8
  enable: false
  model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/PPLCNet_x1_0_person_attribute_945_infer.zip
DET:
  batch_size: 1
  model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip
ID_BASED_CLSACTION:
  batch_size: 8
  display_frames: 80
  enable: false
  model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/PPHGNet_tiny_calling_halfbody.zip
  skip_frame_num: 2
  threshold: 0.8
ID_BASED_DETACTION:
  batch_size: 8
  display_frames: 80
  enable: false
  model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/ppyoloe_crn_s_80e_smoking_visdrone.zip
  skip_frame_num: 2
  threshold: 0.6
KPT:
  batch_size: 8
  model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/dark_hrnet_w32_256x192.zip
MOT:
  batch_size: 1
  enable: false
  model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip
  skip_frame_num: -1
  tracker_config: D:\PycharmProjects\Projects\action_detection\Falling_Detection2.5\pipeline\config\tracker_config.yml
REID:
  batch_size: 16
  enable: false
  model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/reid_model.zip
SKELETON_ACTION:
  batch_size: 1
  coord_size:
  - 384
  - 512
  display_frames: 80
  enable: true
  max_frames: 50
  model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/STGCN.zip
VIDEO_ACTION:
  batch_size: 1
  enable: false
  frame_len: 8
  model_dir: https://videotag.bj.bcebos.com/PaddleVideo-release2.3/ppTSM_fight.zip
  sample_freq: 7
  short_size: 340
  target_size: 320
attr_thresh: 0.5
crop_thresh: 0.5
kpt_thresh: 0.2
visual: true
warmup_frame: 50

------------------------------------------
SkeletonAction Recognition enabled
DET  model dir:  C:\Users\14548/.cache/paddle/infer_weights\mot_ppyoloe_l_36e_pipeline
mot_model_dir model_dir:  C:\Users\14548/.cache/paddle/infer_weights\mot_ppyoloe_l_36e_pipeline
KPT  model dir:  C:\Users\14548/.cache/paddle/infer_weights\dark_hrnet_w32_256x192
SKELETON_ACTION  model dir:  C:\Users\14548/.cache/paddle/infer_weights\STGCN
-----------  Model Configuration -----------
Model Arch: STGCN
Transform Order: 
--transform op: AutoPadding
--------------------------------------------
-----------  Model Configuration -----------
Model Arch: HRNet
Transform Order: 
--transform op: TopDownEvalAffine
--transform op: Permute
--------------------------------------------
-----------  Model Configuration -----------
Model Arch: YOLO
Transform Order: 
--transform op: Resize
--transform op: Permute
--------------------------------------------
video fps: 25, frame_count: -2562047788015215
Thread: 0; frame id: 0
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 10
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 20
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 30
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 40
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 50
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 60
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 70
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 80
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 90
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 100
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 110
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 120
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 130
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 140
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 150
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 160
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 170
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 180
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 190
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 200
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 210
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 220
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 230
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 240
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 250
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 260
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 270
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 280
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 290
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 300
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 310
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 320
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 330
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 340
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 350
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 360
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 370
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 380
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 390
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 400
[WARNNING] No object detected.
Thread: 0; trackid number: 0
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
[WARNNING] No object detected.
Thread: 0; frame id: 410
Thread: 0; trackid number: 0
Thread: 0; frame id: 420
Thread: 0; trackid number: 1
Thread: 0; frame id: 430
Thread: 0; trackid number: 1
Thread: 0; frame id: 440
Thread: 0; trackid number: 1
Thread: 0; frame id: 450
Thread: 0; trackid number: 1
Thread: 0; frame id: 460
Thread: 0; trackid number: 2
Thread: 0; frame id: 470
Thread: 0; trackid number: 1
save result to output\main_t00_rtsp.mp4
------------------ Inference Time Info ----------------------
total_time(ms): 1665285115235.3, img_num: 428
mot time(ms): 30255.9; per frame average time(ms): 70.69135514018691
kpt time(ms): 7829.1; per trackid average time(ms): 110.26901408450705
skeleton_action time(ms): 20.9; per trackid average time(ms): 0.2943661971830986
average latency time(ms): 3890853072.98, QPS: 0.000000
[hevc @ 000001d0950b3e40] Could not find ref with POC 2
[hevc @ 000001d1477bcf80] The cu_qp_delta 48 is outside the valid range [-26, 25].

Process finished with exit code 0
"
yolov6请问一下yaml的问题,PaddlePaddle/PaddleDetection,2022-10-08 17:48:10,1,,7073,1402015945,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

yaml.constructor.ConstructorError: could not determine a constructor for the tag '!YOLOv5LRDecay'
  in ""configs/yolov6/yolov6_n_400e_coco.yml"", line 27, column 5
请问这个报错是什么原因 怎么解决呢"
部署det训练好的模型，deploy/cpp,PaddlePaddle/PaddleDetection,2022-10-08 14:35:41,15,,7072,1401966107,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

_No response_

### Bug描述 Describe the Bug

以下是报错，不知道怎么生成不了`CMakeFiles/main.dir/src/main.cc.o`这个文件，已经卡了很久了希望大佬们帮忙解决一下这个部署问题。

 ```
-- Build files have been written to: /root/deploy_water/cpp/build
[  7%] Creating directories for 'ext-yaml-cpp'
[ 14%] Performing download step (download, verify and extract) for 'ext-yaml-cpp'
-- ext-yaml-cpp download command succeeded.  See also /root/deploy_water/cpp/build/ext/yaml-cpp/src/ext-yaml-cpp-stamp/ext-yaml-cpp-download-*.log
[ 21%] No update step for 'ext-yaml-cpp'
[ 28%] No patch step for 'ext-yaml-cpp'
[ 35%] Performing configure step for 'ext-yaml-cpp'
-- The C compiler identification is GNU 8.5.0
-- The CXX compiler identification is GNU 8.5.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Performing Test FLAG_WEXTRA
-- Performing Test FLAG_WEXTRA - Success
-- Configuring done
-- Generating done
-- Build files have been written to: /root/deploy_water/cpp/build/ext/yaml-cpp/src/ext-yaml-cpp-build
[ 42%] Performing build step for 'ext-yaml-cpp'
-- ext-yaml-cpp build command succeeded.  See also /root/deploy_water/cpp/build/ext/yaml-cpp/src/ext-yaml-cpp-stamp/ext-yaml-cpp-build-*.log
[ 50%] No install step for 'ext-yaml-cpp'
[ 57%] Completed 'ext-yaml-cpp'
[ 57%] Built target ext-yaml-cpp
[ 64%] Building CXX object CMakeFiles/main.dir/src/main.cc.o
cc1plus: 错误：给定了太多文件名。试用 cc1plus --help 以了解用法
cc1plus: 致命错误：CMakeFiles/main.dir/src/main.cc.d：没有那个文件或目录
编译中断。
make[2]: *** [CMakeFiles/main.dir/build.make:76：CMakeFiles/main.dir/src/main.cc.o] 错误 1
make[1]: *** [CMakeFiles/Makefile2:111：CMakeFiles/main.dir/all] 错误 2
make: *** [Makefile:91：all] 错误 2
make finished!
 ```


### 复现环境 Environment

- OS: Linux
- paddle: paddle_inference/manylinux_cpu_avx_mkl_gcc8.2
- paddleDet: release/2.4
- GCC:  8.5.0 
- cmake: 3.20.2
- cpu推理

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [X] 我愿意提交PR！I'd like to help by submitting a PR!"
int8 use cache for cpp detection,PaddlePaddle/PaddleDetection,2022-10-08 14:04:19,5,,7071,1401957493,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

Hi,

https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.5/deploy/cpp/docs/linux_build.md

I compiled and cpp version working perfectly . 

But every time its converting the model again and again ? Is there any opton or way to tell use the cache file if there is before converting ?

Best

"
请教关于训练picodet算法时的负样本选取问题,PaddlePaddle/PaddleDetection,2022-10-08 09:11:19,1,,7070,1401881942,"### 问题确认 Search before asking

- [ ] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

想知道picodet算法训练时的负样本是随机选取还是其他办法？因为我的训练数据中有很多遗漏的目标（即未标注的正样本），这对训练和预测产生多大影响？"
"OSError: (External) CUDA error(700), an illegal memory access was encountered",PaddlePaddle/PaddleDetection,2022-10-08 06:45:18,1,,7068,1401845865,"### 问题描述 Please describe your issue

- 使用paddle detection，用自己的coco数据集跑faster rcnn模型出现的问题，只有使用gpu的时候出现该报错，cpu下可以正常训练
### 报错信息
```python
(venv) PS D:\Project\paddle\PaddleDetection> python .\tools\train.py -c .\configs\faster_rcnn\faster_rcnn_r34_fpn_1x_coco.yml --eval
Warning: Unable to use JDE/FairMOT/ByteTrack, please install lap, for example: `pip install lap`, see https://github.com/gatagat/lap
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
W1008 14:41:32.427362 11228 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 11.6, Runtime API Version: 11.6
W1008 14:41:32.430361 11228 gpu_resources.cc:91] device: 0, cuDNN Version: 8.6.
[10/08 14:41:33] ppdet.utils.checkpoint INFO: Finish loading model weights: C:\Users\ps/.cache/paddle/weights\ResNet34_pretrained.pdparams
Traceback (most recent call last):
  File "".\tools\train.py"", line 172, in <module>
    main()
  File "".\tools\train.py"", line 168, in main
    run(FLAGS, cfg)
  File "".\tools\train.py"", line 132, in run
    trainer.train(FLAGS.eval)
  File ""D:\Project\paddle\PaddleDetection\ppdet\engine\trainer.py"", line 506, in train
    outputs = model(data)
  File ""D:\Project\paddle\PaddleDetection\venv\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""D:\Project\paddle\PaddleDetection\venv\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""D:\Project\paddle\PaddleDetection\ppdet\modeling\architectures\meta_arch.py"", line 59, in forward
    out = self.get_loss()
  File ""D:\Project\paddle\PaddleDetection\ppdet\modeling\architectures\faster_rcnn.py"", line 95, in get_loss
    rpn_loss, bbox_loss = self._forward()
  File ""D:\Project\paddle\PaddleDetection\ppdet\modeling\architectures\faster_rcnn.py"", line 76, in _forward
    rois, rois_num, rpn_loss = self.rpn_head(body_feats, self.inputs)
  File ""D:\Project\paddle\PaddleDetection\venv\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""D:\Project\paddle\PaddleDetection\venv\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""D:\Project\paddle\PaddleDetection\ppdet\modeling\proposal_generator\rpn_head.py"", line 143, in forward
    loss = self.get_loss(scores, deltas, anchors, inputs)
  File ""D:\Project\paddle\PaddleDetection\ppdet\modeling\proposal_generator\rpn_head.py"", line 281, in get_loss
    pos_ind = paddle.nonzero(pos_mask)
  File ""D:\Project\paddle\PaddleDetection\venv\lib\site-packages\paddle\tensor\search.py"", line 402, in nonzero
    outs = _C_ops.where_index(x)
OSError: (External) CUDA error(700), an illegal memory access was encountered.
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue u
sing CUDA, the process must be terminated and relaunched. ] (at ..\paddle\phi\backends\gpu\cuda\cuda_info.cc:251)
  [operator < where_index > error]
```
### 虚拟环境如下
```python
Package            Version
------------------ -------------
astor              0.8.1
Babel              2.10.3
bce-python-sdk     0.8.74
certifi            2022.9.24
chardet            5.0.0
charset-normalizer 2.1.1
click              8.1.3
colorama           0.4.5
cycler             0.11.0
Cython             0.29.32
decorator          5.1.1
dill               0.3.5.1
filterpy           1.4.5
Flask              2.2.2
Flask-Babel        2.0.0
fonttools          4.37.3
future             0.18.2
idna               3.4
importlib-metadata 4.12.0
itsdangerous       2.1.2
Jinja2             3.1.2
joblib             1.2.0
kiwisolver         1.4.4
lap                0.4.0
MarkupSafe         2.1.1
matplotlib         3.5.3
motmetrics         1.2.5
multiprocess       0.70.13
numpy              1.19.3
opencv-python      4.6.0.66
opt-einsum         3.3.0
packaging          21.3
paddle-bfloat      0.1.7
paddledet          2.5.0
paddlepaddle-gpu   2.3.2.post116
paddleslim         2.1.1
paddlex            2.0.0
pandas             1.3.5
Pillow             9.2.0
pip                22.2.2
protobuf           3.20.0
pyclipper          1.3.0.post3
pycocotools        2.0.5
pycryptodome       3.15.0
pyparsing          3.0.9
python-dateutil    2.8.2
pytz               2022.2.1
PyYAML             6.0
pyzmq              24.0.1
requests           2.28.1
scikit-learn       0.23.2
scipy              1.7.3
setuptools         47.1.0
Shapely            1.8.4
six                1.16.0
sklearn            0.0
terminaltables     3.1.10
threadpoolctl      3.1.0
tqdm               4.64.1
typeguard          2.13.3
typing_extensions  4.3.0
urllib3            1.26.12
visualdl           2.4.1
Werkzeug           2.2.2
xmltodict          0.13.0
zipp               3.8.1
```
- cuda: 11.6 cudnn: 8.6.0"
部署模型导出ONNX格式，支持onnxruntime-gpu 1.4.0嘛？,PaddlePaddle/PaddleDetection,2022-10-08 06:41:27,1,,7067,1401844990,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

# 运行环境
cuda: 10.1 
cudnn: 7.6.5
python: 3.6
onnxruntime-gpu 1.4.0

# ONNX导出命令
![image](https://user-images.githubusercontent.com/55420158/194693757-11a60824-10d9-4659-9ec2-cfaca895ec27.png)

# 使用python推理报错
onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: Unknown model file format version."
根据paddle detection的faster rcnn里面的文件修改训练voc数据集报错,PaddlePaddle/PaddleDetection,2022-10-08 03:12:05,2,,7066,1401802125,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

- 我想使用voc格式的数据集跑faster rcnn测试，只把coco数据集改成了voc数据集，其他没动，无法运行，也没见过这种报错
### 我的训练配置文件
```python
_BASE_: [
  'coco_demo.yml',
]
pretrain_weights: https://paddledet.bj.bcebos.com/models/pretrained/ResNet50_vd_pretrained.pdparams
weights: output/faster_rcnn_r50_vd_fpn_1x_coco/model_final

ResNet:
  # index 0 stands for res2
  depth: 50
  variant: d
  norm_type: bn
  freeze_at: 0
  return_idx: [0,1,2,3]
  num_stages: 4
```
### coco_demo的配置文件
```python
_BASE_: [
  '../datasets/voc_demo.yml',
  '../runtime.yml',
  '_base_/optimizer_1x.yml',
  '_base_/faster_rcnn_r50.yml',
  '_base_/faster_reader.yml',
]
- weights: output/faster_rcnn_r50_1x_coco/model_final
```
### voc_demo的配置文件
```python
metric: VOC
map_type: 11point
num_classes: 6

TrainDataset:
  !VOCDataSet
    dataset_dir: dataset/coco_demo
    anno_path: train_list.txt
    label_list: labels.txt
    data_fields: ['image', 'gt_bbox', 'gt_class', 'difficult']

EvalDataset:
  !VOCDataSet
    dataset_dir: dataset/coco_demo
    anno_path: val_list.txt
    label_list: labels.txt
    data_fields: ['image', 'gt_bbox', 'gt_class', 'difficult']

TestDataset:
  !ImageFolder
    anno_path: dataset/coco_demo/val_list.txt
```
- 这个数据集跑voc的是没问题的，没用过coco数据集的配置，还有什么地方需要修改的吗，我用官方的x2coco把voc格式的数据集转成coco的也不对
### 报错信息如下
```python
W1008 11:02:16.883631  3996 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 11.6, Runtime API Version: 11.6
W1008 11:02:16.888631  3996 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
Traceback (most recent call last):
  File "".\tools\train.py"", line 172, in <module>
    main()
  File "".\tools\train.py"", line 168, in main
    run(FLAGS, cfg)
  File "".\tools\train.py"", line 129, in run
    trainer.load_weights(cfg.pretrain_weights)
  File ""D:\Project\paddle\PaddleDetection\ppdet\engine\trainer.py"", line 375, in load_weights
    load_pretrain_weight(self.model, weights)
  File ""D:\Project\paddle\PaddleDetection\ppdet\utils\checkpoint.py"", line 215, in load_pretrain_weight
    param_state_dict = match_state_dict(model_dict, param_state_dict)
  File ""D:\Project\paddle\PaddleDetection\ppdet\utils\checkpoint.py"", line 194, in match_state_dict
    weight_key]))
ValueError: Ambiguity weight backbone.res5.res5a.branch2a.conv.weight loaded, it matches at least bbox_head.head.res5.res5a.branch2a.conv.weight and backbone.res5.res5a.branch2a.conv.weight in the model
```
## 使用官方的x2coco.py把voc格式的数据集转成coco格式的数据集也有问题
### 数据集的配置文件如下
```python
metric: COCO
num_classes: 6

TrainDataset:
  !COCODataSet
    image_dir: JPEGImages
    anno_path: train.json
    dataset_dir: dataset/coco_demo
    data_fields: ['image', 'gt_bbox', 'gt_class', 'is_crowd']

EvalDataset:
  !COCODataSet
    image_dir: JPEGImages
    anno_path: val.json
    dataset_dir: dataset/coco_demo

TestDataset:
  !ImageFolder
    anno_path: val.json # also support txt (like VOC's label_list.txt)
    dataset_dir: dataset/coco_demo # if set, anno_path will be 'dataset_dir/anno_path'
```
- coco_demo数据集的格式如下
- --coco_demo
- ----Annotations
- ----JPEGImages
- ----train.json
- ----val.json
- json内容中的file_name为'{""file_name"": ""_202207261730340524.jpg"", ""height"": 1024.0, ""width"": 2048.0, ""id"": 0}',我不清楚是不是路径不对，除了数据集的地方进行修改，其他的配置文件为faster_rcnn_r34_fpn_1x_coco.yml，只修改了datasets里的配置文件，报错信息为:
```python
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
W1008 11:08:24.253907   356 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 11.6, Runtime API Version: 11.6
W1008 11:08:24.258906   356 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[10/08 11:08:25] ppdet.utils.checkpoint INFO: Finish loading model weights: C:\Users\ps/.cache/paddle/weights\ResNet34_pretrained.pdparams
W1008 11:08:25.682456   356 gpu_resources.cc:201] WARNING: device: . The installed Paddle is compiled with CUDNN 8.4, but CUDNN version in your machine is 8.1, which may cause serious incompatible bug. Please recompile or reinstall 
Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "".\tools\train.py"", line 172, in <module>
    main()
  File "".\tools\train.py"", line 168, in main
    run(FLAGS, cfg)
  File "".\tools\train.py"", line 132, in run
    trainer.train(FLAGS.eval)
  File ""D:\Project\paddle\PaddleDetection\ppdet\engine\trainer.py"", line 506, in train
    outputs = model(data)
  File ""D:\Project\paddle\PaddleDetection\venv\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""D:\Project\paddle\PaddleDetection\venv\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""D:\Project\paddle\PaddleDetection\ppdet\modeling\architectures\meta_arch.py"", line 59, in forward
    out = self.get_loss()
  File ""D:\Project\paddle\PaddleDetection\ppdet\modeling\architectures\faster_rcnn.py"", line 95, in get_loss
    rpn_loss, bbox_loss = self._forward()
  File ""D:\Project\paddle\PaddleDetection\ppdet\modeling\architectures\faster_rcnn.py"", line 76, in _forward
    rois, rois_num, rpn_loss = self.rpn_head(body_feats, self.inputs)
  File ""D:\Project\paddle\PaddleDetection\venv\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""D:\Project\paddle\PaddleDetection\venv\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""D:\Project\paddle\PaddleDetection\ppdet\modeling\proposal_generator\rpn_head.py"", line 143, in forward
    loss = self.get_loss(scores, deltas, anchors, inputs)
  File ""D:\Project\paddle\PaddleDetection\ppdet\modeling\proposal_generator\rpn_head.py"", line 281, in get_loss
    pos_ind = paddle.nonzero(pos_mask)
  File ""D:\Project\paddle\PaddleDetection\venv\lib\site-packages\paddle\tensor\search.py"", line 402, in nonzero
    outs = _C_ops.where_index(x)
OSError: (External) CUDA error(700), an illegal memory access was encountered.
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue u
sing CUDA, the process must be terminated and relaunched. ] (at ..\paddle\phi\backends\gpu\cuda\cuda_info.cc:251)
  [operator < where_index > error]
```"
"ppyoloe替换一些部分导致，map(29epoch)=70%,map(39epoch)=0.09%",PaddlePaddle/PaddleDetection,2022-10-06 01:05:49,1,,7063,1398573611,"### 问题描述 Please describe your issue

1.ppyoloe-m的模型配置文件中的act修改为silu
--------------------------
CustomCSPPAN:
  out_channels: [768, 384, 192]
  stage_num: 1
  block_num: 3
  act: 'silu'
  spp: true
-------------------------------
2.替换了backbone中的注意力机制，由EffectiveSELayer替换成Att
-------------------------------
        if attn:
            # self.attn = EffectiveSELayer(ch_mid, act='hardsigmoid')
            self.attn = Att(ch_mid)
        else:
            self.attn = None

-------------------------------


以上俩种情况是跑的俩次实验，都存在精度突然趋于0的情况，这是啥原因呢？
![image](https://user-images.githubusercontent.com/71055342/194190981-e3ecb5fc-abbf-46ea-956f-a59cbd54ec75.png)

![image](https://user-images.githubusercontent.com/71055342/194191142-791a13d1-5aa4-42f1-ab7c-ed0e85e0fc7a.png)




"
速度测速时报错,PaddlePaddle/PaddleDetection,2022-10-05 17:32:45,6,,7062,1398133617,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

_No response_

### Bug描述 Describe the Bug

在使用Paddle Inference且使用TensorRT进行测速时，报这个错误是什么意思呀

```
-----------  Model Configuration -----------
Model Arch: YOLO
Transform Order:
--transform op: Resize
--transform op: Permute
--------------------------------------------
Traceback (most recent call last):
  File ""deploy/python/infer.py"", line 1032, in <module>
    main()
  File ""deploy/python/infer.py"", line 979, in main
    output_dir=FLAGS.output_dir)
  File ""deploy/python/infer.py"", line 118, in __init__
    delete_shuffle_pass=delete_shuffle_pass)
  File ""deploy/python/infer.py"", line 880, in load_predictor
    predictor = create_predictor(config)
ValueError: (InvalidArgument) Pass preln_embedding_eltwise_layernorm_fuse_pass has not been registered.
  [Hint: Expected Has(pass_type) == true, but received Has(pass_type):0 != true:1.] (at /paddle/paddle/fluid/framework/ir/pass.h:242)

```

### 复现环境 Environment

- Linux
- paddle:2.3.1
- python:3.7.13

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
ppyoloe 训练损失函数为nan,PaddlePaddle/PaddleDetection,2022-10-04 13:09:33,2,,7061,1396251588,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Training

### Bug描述 Describe the Bug

ppdet\modeling\heads\ppyoloe_head.py
def _bbox_loss(self, pred_dist, pred_bboxes, anchor_points, assigned_labels,assigned_bboxes, assigned_scores, assigned_scores_sum):
函数中的代码loss_cls /= assigned_scores_sum
当assigned_scores_sum为0时候，得到loss_cls 的值为inf

### 复现环境 Environment

linux 

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [X] 我愿意提交PR！I'd like to help by submitting a PR!"
paddledetection ppylooe转tensort模型后，可视化结果错误,PaddlePaddle/PaddleDetection,2022-10-04 13:06:29,4,,7060,1396247136,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.5/configs/ppyoloe

1.Using Paddle Inference with TensorRT to test speed, run following command
1.1 python tools/export_model.py -c configs/ppyoloe/ppyoloe_plus_crn_l_80e_coco.yml -o weights=https://paddledet.bj.bcebos.com/models/ppyoloe_plus_crn_l_80e_coco.pdparams exclude_nms=True trt=True

1.2 CUDA_VISIBLE_DEVICES=0 python deploy/python/infer.py --model_dir=output_inference/ppyoloe_plus_crn_l_80e_coco --image_file=demo/000000014439_640x640.jpg --run_mode=trt_fp16 --device=gpu --run_benchmark=True
用coco的val集合测试发现保存在output文件夹的检测结果是错误的，模型转换过程中没有报错





### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
训练时老是莫名中断，是什么原因,PaddlePaddle/PaddleDetection,2022-10-01 23:56:06,1,,7059,1393591915,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

运行环境如下：
![image](https://user-images.githubusercontent.com/102035987/193432276-b72b2c59-1e20-4cef-93cf-612e50243a05.png)
1.当训练到中途时老是显示内存溢出，之前训练时都没有这个情况，请问是什么原因？
![252d120a0d44d5ce9a3d6186216f5cf](https://user-images.githubusercontent.com/102035987/193432295-8abb38d4-797c-4d39-9239-2798fb6478ac.png)
实验语句如下：
![image](https://user-images.githubusercontent.com/102035987/193432309-4132e12a-f84e-45ff-90cb-bcd0745eccef.png)
2.有时候运行时浏览器又会莫名崩溃，这和BML有关吗？
![image](https://user-images.githubusercontent.com/102035987/193432331-c1e78a70-a03f-409c-b041-b66c41967c07.png)
"
paddledetec部署在centos8出现的问题,PaddlePaddle/PaddleDetection,2022-10-01 06:53:14,1,,7058,1393272319,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

_No response_

### Bug描述 Describe the Bug

在centos8上部署paddledetection中ppyolov2_r50vd_dcn_voc的时运行build.sh出现的错误
![QQ图片20221001143627](https://user-images.githubusercontent.com/94736950/193396331-b2793336-cf70-4653-809f-fab7418f2548.png)
这是打印出的日志
![image](https://user-images.githubusercontent.com/94736950/193396407-95544388-052b-423d-8d50-9203e28d35c4.png)
也曾自己手动添加yaml-cpp没有用。paddle预测库是https://paddleinference.paddlepaddle.org.cn/user_guides/download_lib.html
第一个。

### 复现环境 Environment

系统：centos8
gcc：8.2
cmake：3.15+
使用CPU没有下载CUDA和cudnn
opencv：4.6.0

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [X] 我愿意提交PR！I'd like to help by submitting a PR!"
报错：'latin-1' codec can't encode characters in position 0-1: ordinal not in range(256),PaddlePaddle/PaddleDetection,2022-09-29 13:45:53,1,,7055,1390904829,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

_No response_

### Bug描述 Describe the Bug

我的labels 就是英文的  也没有改其他脚本


### 复现环境 Environment

Window
paddlepaddle 2.2.2

python 3.7

CUDA 11.2

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [X] 我愿意提交PR！I'd like to help by submitting a PR!"
训练数据裁剪,PaddlePaddle/PaddleDetection,2022-09-29 06:00:08,1,,7054,1390301131,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

注意 ： 在实际使用的预测过程中，使用的是单人图像进行预测，**因此在训练过程中建议将图像裁剪为单人图像**，再进行烟头检测框的标注，以提升准确率。
你好，这是《基于人体id的检测模型开发》中的一句话，我想咨询下这个图像裁剪是直接根据人的box裁剪下来吗？还是说需要有一些tricks；
比如我想识别是否佩戴安全帽这个场景，我是否需要将人先检测出来，然后裁剪出来一个个识别是否佩戴安全帽。"
PPYOLOE基于COCO在V100、3090和2080显卡上训练，mAP指标能差多大？,PaddlePaddle/PaddleDetection,2022-09-29 03:58:22,2,,7052,1390216504,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

PPYOLOE基于COCO在V100、3090和2080显卡上训练，mAP指标能差多大？当前只有3090显卡和2080显卡进行训练，不知道最后mAP指标能差别多大"
paddle export to hailo-8 tappas,PaddlePaddle/PaddleDetection,2022-09-28 20:16:41,1,,7050,1389897929,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

Hi,

there is a growing number of AI chips on the market. is there any work with this Hailo-8 chip with paddle detections ?

What is the best way to export visdrone model to hailo-8 ?

as ie : https://github.com/hailo-ai/hailo_model_zoo/blob/master/hailo_models/vehicle_detection/docs/TRAINING_GUIDE.md "
export trt int8 for visdroen,PaddlePaddle/PaddleDetection,2022-09-28 15:47:33,3,,7049,1389587196,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

Hi ,

Can we export to trt / int8 :

CUDA_VISIBLE_DEVICES=0 python tools/export_model.py -c configs/smalldet/ppyoloe_crn_l_80e_sliced_visdrone_640_025.yml -o weights=https://paddledet.bj.bcebos.com/models/ppyoloe_crn_l_80e_sliced_visdrone_640_025.pdparams --trt=True

it gives error ? 

Best

"
An IndexError in running mtmct demo,PaddlePaddle/PaddleDetection,2022-09-28 12:42:29,0,,7048,1389292242,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Deploy

### Bug描述 Describe the Bug

按照官方文档下载导出的检测模型和REID模型，用导出的模型进行车辆的跨摄像头跟踪
https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.5/deploy/pptracking/python
在命令行中输入以下指令，运行demo，出现报错，布尔索引报错
python deploy/pptracking/python/mot_sde_infer.py --model_dir=picodet_l_640_aic21mtmct_vehicle/ --reid_model_dir=deepsort_pplcnet_vehicle --mtmct_dir=mtmct-demo --mtmct_cfg=deploy/pptracking/python/mtmct_cfg.yml --tracker_config=deploy/pptracking/python/tracker_config.yml --device=GPU --scaled=True --threshold=0.5 --save_mot_txts --save_images
![1664367788519](https://user-images.githubusercontent.com/98454125/192778939-f6932c8d-8806-41e1-bf12-920aa09bd71a.png)
the running arguments and model configuration as follow：
![1664368792898](https://user-images.githubusercontent.com/98454125/192781169-845c435f-4707-45fa-8587-59b17f5c9577.png)
![微信截图_20220928204012](https://user-images.githubusercontent.com/98454125/192781177-f5df2322-188f-4ecd-892f-3228fe92a80c.png)
 



### 复现环境 Environment

- OS: Windows
- PaddlePaddle: 2.3.2
- PaddleDetection: release/2.5
- Python: 3.9.13
- CUDA: 11.6
- cuDNN Version: 8.4


### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [x] 我愿意提交PR！I'd like to help by submitting a PR!"
IndexError in running pptracking mtmct demo,PaddlePaddle/PaddleDetection,2022-09-28 09:05:54,1,,7046,1388999850,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

-----------  Running Arguments -----------
batch_size: 1
camera_id: -1
cpu_threads: 1
device: GPU
do_break_in_counting: False
do_entrance_counting: False
draw_center_traj: False
enable_mkldnn: False
image_dir: None
image_file: None
model_dir: picodet_l_640_aic21mtmct_vehicle/
mtmct_cfg: deploy/pptracking/python/mtmct_cfg.yml
mtmct_dir: mtmct-demo
output_dir: output
region_polygon: []
region_type: horizontal
reid_batch_size: 50
reid_model_dir: deepsort_pplcnet_vehicle
run_benchmark: False
run_mode: paddle
save_images: True
save_mot_txt_per_img: False
save_mot_txts: True
scaled: True
secs_interval: 2
skip_frame_num: -1
threshold: 0.5
tracker_config: deploy/pptracking/python/tracker_config.yml
trt_calib_mode: False
trt_max_shape: 1280
trt_min_shape: 1
trt_opt_shape: 640
use_dark: True
use_gpu: False
video_file: None
------------------------------------------
-----------  Model Configuration -----------
Model Arch: PicoDet
Transform Order:
--transform op: Resize
--transform op: NormalizeImage
--transform op: Permute
--transform op: PadStride
--------------------------------------------
-----------  Model Configuration -----------
Model Arch: DeepSORT
Transform Order:
--transform op: LetterBoxResize
--transform op: NormalizeImage
--transform op: Permute
--------------------------------------------
start tracking seq: c003
Tracking frame: 0
Traceback (most recent call last):
  File ""D:\THICV\Project\PaddleDetection\deploy\pptracking\python\mot_sde_infer.py"", line 882, in <module>
    main()
  File ""D:\THICV\Project\PaddleDetection\deploy\pptracking\python\mot_sde_infer.py"", line 851, in main
    detector.predict_mtmct(FLAGS.mtmct_dir, mtmct_cfg)
  File ""D:\THICV\Project\PaddleDetection\deploy\pptracking\python\mot_sde_infer.py"", line 761, in predict_mtmct
    mot_features_dict = self.predict_image(
  File ""D:\THICV\Project\PaddleDetection\deploy\pptracking\python\mot_sde_infer.py"", line 488, in predict_image
    det_result = self.postprocess(inputs, result)
  File ""D:\THICV\Project\PaddleDetection\deploy\pptracking\python\mot_sde_infer.py"", line 238, in postprocess
    result['boxes'] = result['boxes'][keep_idx]
IndexError: boolean index did not match indexed array along dimension 1; dimension is 6400 but corresponding boolean dimension is 80

There is an IndexError that the boolean index didn't match.
I don't know how to fix this index bug."
export导出的模型，使用paddle.inference加载Config出现错误,PaddlePaddle/PaddleDetection,2022-09-27 09:02:52,1,,7039,1387393302,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

InvalidArgumentError: The split Op's Input Variable `X` contains uninitialized Tensor.
      [Hint: Expected t->IsInitialized() == true, but received t->IsInitialized():0 != true:1.] (at /paddle/paddle/fluid/framework/operator.cc:2094)
      [operator < split > error]

在使用paddleinference推理的过程中，没有使用到infer_cfg.yml文件，请问该文件应该如何使用？
看了文档里的写法，还是没能将推理过程写入一个py文件，请问官方大大，能否给个不解耦的py部署教程呢？"
在做头盔检测训练是出现loss nan，什么原因？,PaddlePaddle/PaddleDetection,2022-09-27 08:09:21,1,,7035,1387315933,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

在做头盔检测训练是出现loss变为空值，什么原因？
![1921664262683_ pic](https://user-images.githubusercontent.com/55121158/192470374-2c740812-70ab-41c6-b1d0-c0a15d667e1e.jpg)
"
onnx infer.py not worked ; request ORT provider,PaddlePaddle/PaddleDetection,2022-09-27 07:36:14,5,,7034,1387269991,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

_No response_

### Bug描述 Describe the Bug

CUDA_VISIBLE_DEVICES=0 python3 deploy/third_engine/onnx/infer.py  --infer_cfg output_inference/ppyoloe_crn_l_80e_sliced_visdrone_640_025/infer_cfg.yml  --onnx_file sliced_visdrone.onnx --image_file demo/000000014439.jpg

""onnxruntime.InferenceSession(..., providers={}, ...)"".format(available_providers)
ValueError: This ORT build has ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'] enabled. Since ORT 1.9, you are required to explicitly set the providers parameter when instantiating InferenceSession. For example, onnxruntime.InferenceSession(..., providers=['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'], ...)

### 复现环境 Environment

linux
paddle 2.2
onnxruntime-gpu

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
请问是否有CDLA数据集上训练的lcnet + FGD的预训练模型,PaddlePaddle/PaddleDetection,2022-09-27 07:08:57,0,,7033,1387234319,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

想进行FGD蒸馏训练，但是看到https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.5/configs/picodet/legacy_model/application/layout_analysis 中的模型是在PubLayNet数据集上训练的，请问是否有提供CDLA数据集上的预训练的模型？"
请问有没有无监督目标检测的案例,PaddlePaddle/PaddleDetection,2022-09-27 06:20:01,1,,7032,1387173726,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

rt:   请问有没有无监督目标检测的案例"
TensorRT 部署效率太差的BUG！,PaddlePaddle/PaddleDetection,2022-09-27 03:20:23,6,,7027,1387015001,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

hi，
   请问下， https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.5/deploy/TENSOR_RT.md 关于tensorRT的部署里面，调用下图中的程序段，用来开启和配置N卡gpu，以及模型转化
![1664248271371](https://user-images.githubusercontent.com/8407513/192423230-79639ce7-fe28-4428-91c3-8953ef6931fe.jpg)
关于以上有几个问题
1 是不是每次运行一个模型，只需要最开始调用一次上述的程序段 进行trt模型转化
2 该程序段耗时很大，有时候甚至需要几分钟，是这样的吗？ 为什么在gpu上也这么慢呢？
3 假如只运行一次上述程序段，那为何不可以提前把这个模型转化并保存到一个新的模型格式，然后启动程序时候直接加载这个新的模型，加载速度肯定比用上述程序段转化更快吧？
4 是否有相应的参考链接可用呢？能发下吗 多谢
5 关于picodet和tinypose的 tensor RT模型使用的demo有相应链接吗？
BR"
picodet openvino demo failed！,PaddlePaddle/PaddleDetection,2022-09-27 02:55:55,5,,7026,1386998146,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

_No response_

### Bug描述 Describe the Bug

根据
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.5/deploy/third_engine/demo_openvino_kpts/picodet_openvino.cpp
和
https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.5/deploy/third_engine/demo_openvino 的指导部署picodet 模型，成功编译后，运行demo程序，出现bug 如下:
![image](https://user-images.githubusercontent.com/8407513/192418297-e039531c-3dbc-4c6d-8ef1-e5fc682fb605.png)
怀疑是加载的模型不支持导致的。是否可以修复下picodet 加载模型失败的这个bug呢？   (貌似tinypose模型可以成功加载,这个需要picodet修复好)
期待你的回复~~~

### 复现环境 Environment

ubuntu linux 20.04
paddledet                          2.5.0
paddlepaddle-gpu                   2.3.2
python3.9.7
cuda11.7  (尝试过10.1 结果一样)
GCC 9.4.0

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
CPP deploy build error missing libraries,PaddlePaddle/PaddleDetection,2022-09-26 13:03:06,2,,7023,1386047309,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Deploy

### Bug描述 Describe the Bug

I am trying to build deploy CPP with downloaded paddle_inference.tgz . 
It gives a bunch of errors and missing library.

>  Configuring done
-- Generating done
CMake Warning:
  Manually-specified variables were not used by the project:

    WITH_STATIC_LIB


-- Build files have been written to: /data/dProjects/PaddleDetection/deploy/cpp/build
Scanning dependencies of target ext-yaml-cpp
[  6%] Creating directories for 'ext-yaml-cpp'
[ 12%] Performing download step (download, verify and extract) for 'ext-yaml-cpp'
-- ext-yaml-cpp download command succeeded.  See also /data/dProjects/PaddleDetection/deploy/cpp/build/ext/yaml-cpp/src/ext-yaml-cpp-stamp/ext-yaml-cpp-download-*.log
[ 18%] No patch step for 'ext-yaml-cpp'
[ 25%] No update step for 'ext-yaml-cpp'
[ 31%] Performing configure step for 'ext-yaml-cpp'
-- The C compiler identification is GNU 8.4.0
-- The CXX compiler identification is GNU 8.4.0
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Performing Test FLAG_WEXTRA
-- Performing Test FLAG_WEXTRA - Success
-- Configuring done
-- Generating done
-- Build files have been written to: /data/dProjects/PaddleDetection/deploy/cpp/build/ext/yaml-cpp/src/ext-yaml-cpp-build
[ 37%] Performing build step for 'ext-yaml-cpp'
-- ext-yaml-cpp build command succeeded.  See also /data/dProjects/PaddleDetection/deploy/cpp/build/ext/yaml-cpp/src/ext-yaml-cpp-stamp/ext-yaml-cpp-build-*.log
[ 43%] No install step for 'ext-yaml-cpp'
[ 50%] Completed 'ext-yaml-cpp'
[ 50%] Built target ext-yaml-cpp
Scanning dependencies of target main
[ 56%] Building CXX object CMakeFiles/main.dir/src/main_keypoint.cc.o
/data/dProjects/PaddleDetection/deploy/cpp/src/main_keypoint.cc: In function ‘void PredictImage(std::vector<std::__cxx11::basic_string<char> >, int, double, bool, PaddleDetection::ObjectDetector*, PaddleDetection::KeyPointDetector*, const string&)’:
/data/dProjects/PaddleDetection/deploy/cpp/src/main_keypoint.cc:345:10: warning: format ‘%d’ expects argument of type ‘int’, but argument 2 has type ‘std::vector<std::__cxx11::basic_string<char> >::size_type’ {aka ‘long unsigned int’} [-Wformat=]
   printf(""total images = %d, batch_size = %d, total steps = %d\n"",
          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
          all_img_paths.size(),
          ~~~~~~~~~~~~~~~~~~~~
[ 62%] Building CXX object CMakeFiles/main.dir/src/preprocess_op.cc.o
[ 68%] Building CXX object CMakeFiles/main.dir/src/object_detector.cc.o
[ 75%] Building CXX object CMakeFiles/main.dir/src/picodet_postprocess.cc.o
[ 81%] Building CXX object CMakeFiles/main.dir/src/utils.cc.o
[ 87%] Building CXX object CMakeFiles/main.dir/src/keypoint_detector.cc.o
[ 93%] Building CXX object CMakeFiles/main.dir/src/keypoint_postprocess.cc.o
[100%] Linking CXX executable main
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_dnn.so.3.4.16: .dynsym local symbol at index 1 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_dnn.so.3.4.16: .dynsym local symbol at index 2 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_dnn.so.3.4.16: .dynsym local symbol at index 3 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_highgui.so.3.4.16: .dynsym local symbol at index 1 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_highgui.so.3.4.16: .dynsym local symbol at index 2 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_highgui.so.3.4.16: .dynsym local symbol at index 3 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_objdetect.so.3.4.16: .dynsym local symbol at index 1 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_objdetect.so.3.4.16: .dynsym local symbol at index 2 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_objdetect.so.3.4.16: .dynsym local symbol at index 3 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_shape.so.3.4.16: .dynsym local symbol at index 1 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_shape.so.3.4.16: .dynsym local symbol at index 2 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_shape.so.3.4.16: .dynsym local symbol at index 3 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_stitching.so.3.4.16: .dynsym local symbol at index 1 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_stitching.so.3.4.16: .dynsym local symbol at index 2 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_stitching.so.3.4.16: .dynsym local symbol at index 3 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_superres.so.3.4.16: .dynsym local symbol at index 1 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_superres.so.3.4.16: .dynsym local symbol at index 2 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_superres.so.3.4.16: .dynsym local symbol at index 3 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videostab.so.3.4.16: .dynsym local symbol at index 1 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videostab.so.3.4.16: .dynsym local symbol at index 2 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videostab.so.3.4.16: .dynsym local symbol at index 3 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_calib3d.so.3.4.16: .dynsym local symbol at index 1 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_calib3d.so.3.4.16: .dynsym local symbol at index 2 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_calib3d.so.3.4.16: .dynsym local symbol at index 3 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_features2d.so.3.4.16: .dynsym local symbol at index 1 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_features2d.so.3.4.16: .dynsym local symbol at index 2 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_features2d.so.3.4.16: .dynsym local symbol at index 3 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_photo.so.3.4.16: .dynsym local symbol at index 1 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_photo.so.3.4.16: .dynsym local symbol at index 2 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_photo.so.3.4.16: .dynsym local symbol at index 3 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_video.so.3.4.16: .dynsym local symbol at index 1 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_video.so.3.4.16: .dynsym local symbol at index 2 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_video.so.3.4.16: .dynsym local symbol at index 3 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: .dynsym local symbol at index 1 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: .dynsym local symbol at index 2 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: .dynsym local symbol at index 3 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: .dynsym local symbol at index 1 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: .dynsym local symbol at index 2 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: .dynsym local symbol at index 3 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_core.so.3.4.16: .dynsym local symbol at index 1 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_core.so.3.4.16: .dynsym local symbol at index 2 (>= sh_info of 1)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_core.so.3.4.16: .dynsym local symbol at index 3 (>= sh_info of 1)
/usr/bin/ld: warning: libonnxruntime.so.1.11.1, needed by ../paddle_inference/paddle/lib/libpaddle_inference.so, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libpaddle2onnx.so.1.0.0rc2, needed by ../paddle_inference/paddle/lib/libpaddle_inference.so, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libavcodec-ffmpeg.so.56, needed by ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libavformat-ffmpeg.so.56, needed by ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libavutil-ffmpeg.so.54, needed by ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libswscale-ffmpeg.so.3, needed by ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libpng12.so.0, needed by ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libjasper.so.1, needed by ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16, not found (try using -rpath or -rpath-link)
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `avformat_close_input@LIBAVFORMAT_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `sws_scale@LIBSWSCALE_FFMPEG_3'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `jas_image_writecmpt'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_create_write_struct@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_guess_codec@LIBAVFORMAT_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_guess_format@LIBAVFORMAT_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_write_end@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_guess_sample_aspect_ratio@LIBAVFORMAT_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `avcodec_get_context_defaults3@LIBAVCODEC_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `jas_image_encode'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_set_compression_level@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_get_IHDR@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_get_tRNS@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `jas_image_decode'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `avcodec_flush_buffers@LIBAVCODEC_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `jas_cleanup'
/usr/bin/ld: ../paddle_inference/paddle/lib/libpaddle_inference.so: undefined reference to `OrtGetApiBase@VERS_1.11.1'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_set_palette_to_rgb@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_codec_get_tag@LIBAVFORMAT_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_write_image@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `avformat_get_mov_video_tags@LIBAVFORMAT_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `avformat_open_input@LIBAVFORMAT_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `jas_image_strtofmt'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `avformat_alloc_context@LIBAVFORMAT_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_read_update_info@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_set_write_fn@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_set_strip_16@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `avcodec_decode_video2@LIBAVCODEC_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_set_filter@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_bitstream_filter_close@LIBAVCODEC_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_image_fill_arrays@LIBAVUTIL_FFMPEG_54'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `jas_image_create'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_find_input_format@LIBAVFORMAT_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_create_info_struct@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_rescale_q@LIBAVUTIL_FFMPEG_54'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `avcodec_find_decoder_by_name@LIBAVCODEC_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_frame_get_buffer@LIBAVUTIL_FFMPEG_54'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `sws_getCachedContext@LIBSWSCALE_FFMPEG_3'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_bitstream_filter_init@LIBAVCODEC_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `jas_init'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `sws_getContext@LIBSWSCALE_FFMPEG_3'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `avio_open@LIBAVFORMAT_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `avformat_get_riff_video_tags@LIBAVFORMAT_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_bitstream_filter_filter@LIBAVCODEC_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `jas_cmprof_createfromclrspc'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `jas_stream_fopen'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_init_io@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_set_read_fn@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_freep@LIBAVUTIL_FFMPEG_54'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_set_swap@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_write_frame@LIBAVFORMAT_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_set_tRNS_to_alpha@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `jas_matrix_destroy'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_mallocz@LIBAVUTIL_FFMPEG_54'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `avio_close@LIBAVFORMAT_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `jas_image_destroy'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_image_get_buffer_size@LIBAVUTIL_FFMPEG_54'
/usr/bin/ld: ../paddle_inference/paddle/lib/libpaddle_inference.so: undefined reference to `paddle2onnx::IsExportable(void const*, int, void const*, int, int, bool, bool, bool, bool, bool, paddle2onnx::CustomOp*, int, char const*)'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_set_interlace_handling@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_write_trailer@LIBAVFORMAT_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `jas_stream_close'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_dict_parse_string@LIBAVUTIL_FFMPEG_54'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_set_bgr@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `avcodec_open2@LIBAVCODEC_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `jas_image_readcmpt'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `sws_freeContext@LIBSWSCALE_FFMPEG_3'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `avcodec_find_decoder@LIBAVCODEC_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_read_image@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `avformat_new_stream@LIBAVFORMAT_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `avformat_find_stream_info@LIBAVFORMAT_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `avcodec_get_name@LIBAVCODEC_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_lockmgr_register@LIBAVCODEC_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_read_end@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_dict_free@LIBAVUTIL_FFMPEG_54'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_write_info@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_set_compression_strategy@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `avcodec_encode_video2@LIBAVCODEC_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `avcodec_pix_fmt_to_codec_tag@LIBAVCODEC_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_frame_alloc@LIBAVUTIL_FFMPEG_54'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_free@LIBAVUTIL_FFMPEG_54'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_read_frame@LIBAVFORMAT_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_set_packing@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_dict_get@LIBAVUTIL_FFMPEG_54'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_seek_frame@LIBAVFORMAT_FFMPEG_56'
/usr/bin/ld: ../paddle_inference/paddle/lib/libpaddle_inference.so: undefined reference to `paddle2onnx::Export(void const*, int, void const*, int, char**, int*, int, bool, bool, bool, bool, bool, paddle2onnx::CustomOp*, int, char const*)'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_frame_free@LIBAVUTIL_FFMPEG_54'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `jas_image_getcmptbytype'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `jas_cmprof_destroy'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `avformat_write_header@LIBAVFORMAT_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_get_io_ptr@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `avformat_free_context@LIBAVFORMAT_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_frame_unref@LIBAVUTIL_FFMPEG_54'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_codec_get_id@LIBAVFORMAT_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_set_expand_gray_1_2_4_to_8@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_destroy_write_struct@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_set_strip_alpha@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `jas_matrix_create'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_log_set_callback@LIBAVUTIL_FFMPEG_54'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_sub_q@LIBAVUTIL_FFMPEG_54'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `avformat_network_init@LIBAVFORMAT_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_destroy_read_struct@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_opt_set@LIBAVUTIL_FFMPEG_54'
/usr/bin/ld: ../paddle_inference/paddle/lib/libpaddle_inference.so: undefined reference to `paddle2onnx::IsExportable(char const*, char const*, int, bool, bool, bool, bool, bool, paddle2onnx::CustomOp*, int, char const*)'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_create_read_struct@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_error@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_packet_unref@LIBAVCODEC_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `avcodec_find_encoder@LIBAVCODEC_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_set_rgb_to_gray@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_init_packet@LIBAVCODEC_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_log_set_level@LIBAVUTIL_FFMPEG_54'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_dict_set@LIBAVUTIL_FFMPEG_54'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_set_gray_to_rgb@PNG12_0'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `jas_image_chclrspc'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_malloc@LIBAVUTIL_FFMPEG_54'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `av_register_all@LIBAVFORMAT_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_set_IHDR@PNG12_0'
/usr/bin/ld: ../paddle_inference/paddle/lib/libpaddle_inference.so: undefined reference to `paddle2onnx::Export(char const*, char const*, char**, int*, int, bool, bool, bool, bool, bool, paddle2onnx::CustomOp*, int, char const*)'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16: undefined reference to `avcodec_close@LIBAVCODEC_FFMPEG_56'
/usr/bin/ld: ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_imgcodecs.so.3.4.16: undefined reference to `png_read_info@PNG12_0'
collect2: error: ld returned 1 exit status
make[2]: *** [CMakeFiles/main.dir/build.make:197: main] Error 1
make[1]: *** [CMakeFiles/Makefile2:78: CMakeFiles/main.dir/all] Error 2
make: *** [Makefile:84: all] Error 2
make finished!

### 复现环境 Environment

Linux
paddle 2.2

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
使用【PP-YOLOE】训练300epoch没有有效结果,PaddlePaddle/PaddleDetection,2022-09-26 12:12:56,7,,7022,1385973502,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

### 使用8卡训练 ppyoloe s 300个epoch，日志中显示训练期间从未显示eval过精度，训练结束后进行eval发现best model精度为0
这是什么原因呢？
**训练命令**：python -m paddle.distributed.launch --gpus 0,1,2,3,4,5,6,7 tools/train.py -c configs/ppyoloe/ppyoloe_crn_s_300e_coco.yml --amp

**测试命令**： CUDA_VISIBLE_DEVICES=0 python tools/eval.py -c configs/ppyoloe/ppyoloe_crn_s_300e_coco.yml -o weights=output/ppyoloe_crn_s_300e_coco/best_model.pdparams

**训练部分日志**：
-----------  Configuration Arguments -----------
backend: auto
elastic_server: None
force: False
gpus: 0,1,2,3,4,5,6,7
heter_devices: 
heter_worker_num: None
heter_workers: 
host: None
http_port: None
ips: 127.0.0.1
job_id: None
log_dir: log
np: None
nproc_per_node: None
run_mode: None
scale: 0
server_num: None
servers: 
training_script: tools/train.py
training_script_args: ['-c', 'configs/ppyoloe/ppyoloe_crn_s_300e_coco.yml', '--amp']
worker_num: None
workers: 
------------------------------------------------
WARNING 2022-09-24 10:43:08,980 launch.py:423] Not found distinct arguments and compiled with cuda or xpu. Default use collective mode
launch train in GPU mode!
INFO 2022-09-24 10:43:08,981 launch_utils.py:528] Local start 8 processes. First process distributed environment info (Only For Debug): 
    +=======================================================================================+
    |                        Distributed Envs                      Value                    |
    +---------------------------------------------------------------------------------------+
    |                       PADDLE_TRAINER_ID                        0                      |
    |                 PADDLE_CURRENT_ENDPOINT                 127.0.0.1:43114               |
    |                     PADDLE_TRAINERS_NUM                        8                      |
    |                PADDLE_TRAINER_ENDPOINTS  ... 0.1:42298,127.0.0.1:48123,127.0.0.1:35710|
    |                     PADDLE_RANK_IN_NODE                        0                      |
    |                 PADDLE_LOCAL_DEVICE_IDS                        0                      |
    |                 PADDLE_WORLD_DEVICE_IDS                 0,1,2,3,4,5,6,7               |
    |                     FLAGS_selected_gpus                        0                      |
    |             FLAGS_selected_accelerators                        0                      |
    +=======================================================================================+

INFO 2022-09-24 10:43:08,982 launch_utils.py:532] details abouts PADDLE_TRAINER_ENDPOINTS can be found in log/endpoints.log, and detail running logs maybe found in log/workerlog.0
launch proc_id:2825 idx:0
launch proc_id:2828 idx:1
launch proc_id:2831 idx:2
launch proc_id:2836 idx:3
launch proc_id:2841 idx:4
launch proc_id:2846 idx:5
launch proc_id:2851 idx:6
launch proc_id:2856 idx:7
Warning: import ppdet from source directory without installing, run 'python setup.py install' to install ppdet firstly
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:42362', '127.0.0.1:48123']
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Bootstrap : Using eth0:172.16.0.90<0>
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO NET/Plugin : Plugin load returned 17 : libnccl-net.so: cannot open shared object file: No such file or directory.
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB ; OOB eth0:172.16.0.90<0>
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Using network IB
I0924 10:43:15.805625  2825 nccl_context.cc:74] init nccl context nranks: 8 local rank: 0 gpu id: 0 ring id: 0
NCCL version 2.10.3+cuda10.2
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO NCCL_IB_TIMEOUT set by environment to 22.
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 00/12 :    0   1   3   7   5   4   6   2
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 01/12 :    0   1   3   7   5   4   6   2
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 02/12 :    0   2   6   4   5   7   3   1
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 03/12 :    0   2   6   4   5   7   3   1
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 04/12 :    0   3   2   1   5   6   7   4
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 05/12 :    0   4   7   6   5   1   2   3
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 06/12 :    0   1   3   7   5   4   6   2
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 07/12 :    0   1   3   7   5   4   6   2
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 08/12 :    0   2   6   4   5   7   3   1
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 09/12 :    0   2   6   4   5   7   3   1
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 10/12 :    0   3   2   1   5   6   7   4
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 11/12 :    0   4   7   6   5   1   2   3
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 2/-1/-1->0->-1 [3] 2/-1/-1->0->-1 [4] 3/-1/-1->0->-1 [5] 4/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 2/-1/-1->0->-1 [9] 2/-1/-1->0->-1 [10] 3/-1/-1->0->-1 [11] 4/-1/-1->0->-1
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Setting affinity for GPU 0 to 3ffff0,0003ffff
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 00 : 0[2d000] -> 1[32000] via P2P/IPC
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 01 : 0[2d000] -> 1[32000] via P2P/IPC
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 06 : 0[2d000] -> 1[32000] via P2P/IPC
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 07 : 0[2d000] -> 1[32000] via P2P/IPC
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 02 : 0[2d000] -> 2[5b000] via P2P/IPC
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 03 : 0[2d000] -> 2[5b000] via P2P/IPC
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 08 : 0[2d000] -> 2[5b000] via P2P/IPC
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 09 : 0[2d000] -> 2[5b000] via P2P/IPC
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 04 : 0[2d000] -> 3[5f000] via P2P/IPC
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 10 : 0[2d000] -> 3[5f000] via P2P/IPC
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 05 : 0[2d000] -> 4[b5000] via P2P/IPC
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 11 : 0[2d000] -> 4[b5000] via P2P/IPC
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Connected all rings
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Connected all trees
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 05 : 0[2d000] -> 5[be000] via P2P/indirect/4[b5000]
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 13 : 0[2d000] -> 5[be000] via P2P/indirect/4[b5000]
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 06 : 0[2d000] -> 6[e1000] via P2P/indirect/2[5b000]
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 14 : 0[2d000] -> 6[e1000] via P2P/indirect/2[5b000]
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 07 : 0[2d000] -> 7[e9000] via P2P/indirect/3[5f000]
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Channel 15 : 0[2d000] -> 7[e9000] via P2P/indirect/3[5f000]
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO comm 0x562defcb6fe0 rank 0 nranks 8 cudaDev 0 busId 2d000 - Init COMPLETE
W0924 10:43:17.685346  2825 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0924 10:43:17.710189  2825 device_context.cc:465] device: 0, cuDNN Version: 7.6.
loading annotations into memory...
Done (t=18.56s)
creating index...
index created!
[09/24 10:43:49] ppdet.data.source.coco WARNING: Found an invalid bbox in annotations: im_id: 200365, area: 0.0 x1: 296.65, y1: 388.33, x2: 297.67999999999995, y2: 388.33.
[09/24 10:43:58] ppdet.data.source.coco WARNING: Found an invalid bbox in annotations: im_id: 550395, area: 0.0 x1: 9.98, y1: 188.56, x2: 15.52, y2: 188.56.
[09/24 10:44:05] ppdet.utils.checkpoint INFO: Finish loading model weights: /cache/data/CSPResNetb_s_pretrained.pdparams
modelarts-job-e797d2a2-9b34-4e84-afc9-31291c55a1cc-worker-0:2825:2825 [0] NCCL INFO Launch mode Parallel
[09/24 10:44:13] ppdet.engine INFO: Epoch: [0] [   0/1832] learning_rate: 0.000000 loss: 1.973711 loss_cls: 0.342441 loss_iou: 0.392251 loss_dfl: 1.301286 loss_l1: 5.916676 eta: 51 days, 10:04:25 batch_cost: 8.0835 data_cost: 0.0003 ips: 0.9897 images/s
[09/24 10:44:44] ppdet.engine INFO: Epoch: [0] [ 100/1832] learning_rate: 0.000109 loss: 4.863109 loss_cls: 1.841195 loss_iou: 0.658263 loss_dfl: 2.889247 loss_l1: 4.796049 eta: 2 days, 5:22:41 batch_cost: 0.2724 data_cost: 0.0019 ips: 29.3723 images/s
[09/24 10:45:16] ppdet.engine INFO: Epoch: [0] [ 200/1832] learning_rate: 0.000218 loss: 4.330437 loss_cls: 1.628571 loss_iou: 0.605775 loss_dfl: 2.546664 loss_l1: 3.969674 eta: 1 day, 23:07:38 batch_cost: 0.2675 data_cost: 0.0020 ips: 29.9064 images/s
[09/24 10:45:47] ppdet.engine INFO: Epoch: [0] [ 300/1832] learning_rate: 0.000328 loss: 4.623132 loss_cls: 1.767785 loss_iou: 0.616231 loss_dfl: 2.416162 loss_l1: 3.077706 eta: 1 day, 20:52:52 batch_cost: 0.2647 data_cost: 0.0014 ips: 30.2265 images/s
[09/24 10:46:18] ppdet.engine INFO: Epoch: [0] [ 400/1832] learning_rate: 0.000437 loss: 4.124754 loss_cls: 1.635101 loss_iou: 0.549718 loss_dfl: 2.066106 loss_l1: 2.655838 eta: 1 day, 19:48:20 batch_cost: 0.2661 data_cost: 0.0015 ips: 30.0653 images/s
[09/24 10:46:49] ppdet.engine INFO: Epoch: [0] [ 500/1832] learning_rate: 0.000546 loss: 3.798736 loss_cls: 1.566531 loss_iou: 0.507754 loss_dfl: 1.980544 loss_l1: 2.213470 eta: 1 day, 19:06:09 batch_cost: 0.2643 data_cost: 0.0017 ips: 30.2675 images/s
[09/24 10:47:20] ppdet.engine INFO: Epoch: [0] [ 600/1832] learning_rate: 0.000655 loss: 3.923654 loss_cls: 1.624911 loss_iou: 0.525020 loss_dfl: 1.964137 loss_l1: 1.901668 eta: 1 day, 18:33:21 batch_cost: 0.2614 data_cost: 0.0016 ips: 30.6091 images/s
[09/24 10:47:51] ppdet.engine INFO: Epoch: [0] [ 700/1832] learning_rate: 0.000764 loss: 4.028397 loss_cls: 1.690205 loss_iou: 0.546071 loss_dfl: 1.941340 loss_l1: 1.677113 eta: 1 day, 18:14:30 batch_cost: 0.2650 data_cost: 0.0013 ips: 30.1934 images/s
[09/24 10:48:22] ppdet.engine INFO: Epoch: [0] [ 800/1832] learning_rate: 0.000873 loss: 3.570728 loss_cls: 1.491613 loss_iou: 0.495842 loss_dfl: 1.740766 loss_l1: 1.565649 eta: 1 day, 18:00:03 batch_cost: 0.2648 data_cost: 0.0011 ips: 30.2120 images/s
[09/24 10:48:52] ppdet.engine INFO: Epoch: [0] [ 900/1832] learning_rate: 0.000983 loss: 3.906841 loss_cls: 1.625144 loss_iou: 0.499656 loss_dfl: 1.875912 loss_l1: 1.535909 eta: 1 day, 17:45:08 batch_cost: 0.2613 data_cost: 0.0011 ips: 30.6181 images/s
[09/24 10:49:23] ppdet.engine INFO: Epoch: [0] [1000/1832] learning_rate: 0.001092 loss: 3.599799 loss_cls: 1.508436 loss_iou: 0.486658 loss_dfl: 1.748408 loss_l1: 1.545722 eta: 1 day, 17:35:04 batch_cost: 0.2634 data_cost: 0.0014 ips: 30.3702 images/s
[09/24 10:49:54] ppdet.engine INFO: Epoch: [0] [1100/1832] learning_rate: 0.001201 loss: 3.472381 loss_cls: 1.548673 loss_iou: 0.456917 loss_dfl: 1.609280 loss_l1: 1.429778 eta: 1 day, 17:29:29 batch_cost: 0.2667 data_cost: 0.0009 ips: 29.9968 images/s
[09/24 10:50:26] ppdet.engine INFO: Epoch: [0] [1200/1832] learning_rate: 0.001310 loss: 3.740201 loss_cls: 1.635180 loss_iou: 0.490231 loss_dfl: 1.770468 loss_l1: 1.323303 eta: 1 day, 17:22:39 batch_cost: 0.2640 data_cost: 0.0015 ips: 30.3081 images/s
[09/24 10:50:57] ppdet.engine INFO: Epoch: [0] [1300/1832] learning_rate: 0.001419 loss: 3.897837 loss_cls: 1.787888 loss_iou: 0.502100 loss_dfl: 1.701340 loss_l1: 1.233154 eta: 1 day, 17:19:22 batch_cost: 0.2676 data_cost: 0.0012 ips: 29.8975 images/s
[09/24 10:51:28] ppdet.engine INFO: Epoch: [0] [1400/1832] learning_rate: 0.001528 loss: 3.523293 loss_cls: 1.630071 loss_iou: 0.439430 loss_dfl: 1.576932 loss_l1: 1.269268 eta: 1 day, 17:15:49 batch_cost: 0.2666 data_cost: 0.0014 ips: 30.0117 images/s
[09/24 10:51:59] ppdet.engine INFO: Epoch: [0] [1500/1832] learning_rate: 0.001638 loss: 3.739848 loss_cls: 1.802097 loss_iou: 0.458117 loss_dfl: 1.717125 loss_l1: 1.234737 eta: 1 day, 17:09:00 batch_cost: 0.2605 data_cost: 0.0012 ips: 30.7065 images/s
[09/24 10:52:30] ppdet.engine INFO: Epoch: [0] [1600/1832] learning_rate: 0.001747 loss: 3.706899 loss_cls: 1.731808 loss_iou: 0.456640 loss_dfl: 1.641941 loss_l1: 1.162927 eta: 1 day, 17:06:44 batch_cost: 0.2671 data_cost: 0.0010 ips: 29.9510 images/s
[09/24 10:53:01] ppdet.engine INFO: Epoch: [0] [1700/1832] learning_rate: 0.001856 loss: 3.372004 loss_cls: 1.552045 loss_iou: 0.417739 loss_dfl: 1.524120 loss_l1: 1.124117 eta: 1 day, 17:01:48 batch_cost: 0.2617 data_cost: 0.0019 ips: 30.5674 images/s
[09/24 10:53:32] ppdet.engine INFO: Epoch: [0] [1800/1832] learning_rate: 0.001965 loss: 3.515841 loss_cls: 1.663327 loss_iou: 0.420909 loss_dfl: 1.541853 loss_l1: 1.075212 eta: 1 day, 16:57:03 batch_cost: 0.2611 data_cost: 0.0008 ips: 30.6393 images/s
[09/24 10:53:43] ppdet.engine INFO: Epoch: [1] [   0/1832] learning_rate: 0.002000 loss: 3.343108 loss_cls: 1.628906 loss_iou: 0.407236 loss_dfl: 1.499181 loss_l1: 1.063801 eta: 1 day, 17:01:33 batch_cost: 0.2738 data_cost: 0.0106 ips: 29.2235 images/s
[09/24 10:54:14] ppdet.engine INFO: Epoch: [1] [ 100/1832] learning_rate: 0.002109 loss: 3.376305 loss_cls: 1.647739 loss_iou: 0.405162 loss_dfl: 1.473208 loss_l1: 1.012852 eta: 1 day, 17:00:25 batch_cost: 0.2682 data_cost: 0.0015 ips: 29.8286 images/s
[09/24 10:54:46] ppdet.engine INFO: Epoch: [1] [ 200/1832] learning_rate: 0.002218 loss: 3.132732 loss_cls: 1.514112 loss_iou: 0.383811 loss_dfl: 1.378531 loss_l1: 0.989075 eta: 1 day, 16:59:58 batch_cost: 0.2695 data_cost: 0.0015 ips: 29.6818 images/s
[09/24 10:55:17] ppdet.engine INFO: Epoch: [1] [ 300/1832] learning_rate: 0.002328 loss: 2.999316 loss_cls: 1.461869 loss_iou: 0.353373 loss_dfl: 1.359534 loss_l1: 0.981827 eta: 1 day, 16:55:05 batch_cost: 0.2592 data_cost: 0.0015 ips: 30.8656 images/s
[09/24 10:55:48] ppdet.engine INFO: Epoch: [1] [ 400/1832] learning_rate: 0.002437 loss: 3.138419 loss_cls: 1.489123 loss_iou: 0.371690 loss_dfl: 1.397101 loss_l1: 0.954392 eta: 1 day, 16:54:42 batch_cost: 0.2692 data_cost: 0.0011 ips: 29.7124 images/s
[09/24 10:56:19] ppdet.engine INFO: Epoch: [1] [ 500/1832] learning_rate: 0.002546 loss: 3.226414 loss_cls: 1.614958 loss_iou: 0.378989 loss_dfl: 1.407963 loss_l1: 0.917337 eta: 1 day, 16:52:22 batch_cost: 0.2643 data_cost: 0.0012 ips: 30.2740 images/s
[09/24 10:56:50] ppdet.engine INFO: Epoch: [1] [ 600/1832] learning_rate: 0.002655 loss: 3.131895 loss_cls: 1.506892 loss_iou: 0.367339 loss_dfl: 1.366505 loss_l1: 0.915300 eta: 1 day, 16:49:40 batch_cost: 0.2628 data_cost: 0.0016 ips: 30.4370 images/s
[09/24 10:57:21] ppdet.engine INFO: Epoch: [1] [ 700/1832] learning_rate: 0.002764 loss: 3.251610 loss_cls: 1.589792 loss_iou: 0.384414 loss_dfl: 1.435535 loss_l1: 0.908442 eta: 1 day, 16:45:51 batch_cost: 0.2593 data_cost: 0.0009 ips: 30.8532 images/s
[09/24 10:57:52] ppdet.engine INFO: Epoch: [1] [ 800/1832] learning_rate: 0.002873 loss: 3.232834 loss_cls: 1.566636 loss_iou: 0.380846 loss_dfl: 1.454808 loss_l1: 0.932477 eta: 1 day, 16:43:32 batch_cost: 0.2628 data_cost: 0.0015 ips: 30.4375 images/s
[09/24 10:58:23] ppdet.engine INFO: Epoch: [1] [ 900/1832] learning_rate: 0.002983 loss: 3.231883 loss_cls: 1.605464 loss_iou: 0.375802 loss_dfl: 1.426023 loss_l1: 0.849997 eta: 1 day, 16:42:24 batch_cost: 0.2660 data_cost: 0.0013 ips: 30.0782 images/s
[09/24 10:58:54] ppdet.engine INFO: Epoch: [1] [1000/1832] learning_rate: 0.003092 loss: 2.942536 loss_cls: 1.466311 loss_iou: 0.335384 loss_dfl: 1.310007 loss_l1: 0.864348 eta: 1 day, 16:41:00 batch_cost: 0.2650 data_cost: 0.0011 ips: 30.1894 images/s
[09/24 10:59:25] ppdet.engine INFO: Epoch: [1] [1100/1832] learning_rate: 0.003201 loss: 3.277115 loss_cls: 1.628841 loss_iou: 0.370649 loss_dfl: 1.397699 loss_l1: 0.858561 eta: 1 day, 16:39:54 batch_cost: 0.2658 data_cost: 0.0003 ips: 30.0993 images/s
[09/24 10:59:56] ppdet.engine INFO: Epoch: [1] [1200/1832] learning_rate: 0.003310 loss: 3.071394 loss_cls: 1.543086 loss_iou: 0.356368 loss_dfl: 1.343150 loss_l1: 0.836658 eta: 1 day, 16:38:38 batch_cost: 0.2651 data_cost: 0.0008 ips: 30.1829 images/s
[09/24 11:00:27] ppdet.engine INFO: Epoch: [1] [1300/1832] learning_rate: 0.003419 loss: 3.054913 loss_cls: 1.490325 loss_iou: 0.351002 loss_dfl: 1.320646 loss_l1: 0.837058 eta: 1 day, 16:36:51 batch_cost: 0.2631 data_cost: 0.0011 ips: 30.4037 images/s
[09/24 11:00:59] ppdet.engine INFO: Epoch: [1] [1400/1832] learning_rate: 0.003528 loss: 3.111308 loss_cls: 1.576360 loss_iou: 0.345202 loss_dfl: 1.332696 loss_l1: 0.826022 eta: 1 day, 16:36:37 batch_cost: 0.2683 data_cost: 0.0012 ips: 29.8141 images/s
[09/24 11:01:30] ppdet.engine INFO: Epoch: [1] [1500/1832] learning_rate: 0.003638 loss: 3.100139 loss_cls: 1.549652 loss_iou: 0.332128 loss_dfl: 1.307026 loss_l1: 0.826304 eta: 1 day, 16:36:28 batch_cost: 0.2686 data_cost: 0.0010 ips: 29.7824 images/s
[09/24 11:02:01] ppdet.engine INFO: Epoch: [1] [1600/1832] learning_rate: 0.003747 loss: 2.989034 loss_cls: 1.516252 loss_iou: 0.314211 loss_dfl: 1.273464 loss_l1: 0.805247 eta: 1 day, 16:34:43 batch_cost: 0.2627 data_cost: 0.0016 ips: 30.4511 images/s
[09/24 11:02:32] ppdet.engine INFO: Epoch: [1] [1700/1832] learning_rate: 0.003856 loss: 3.045382 loss_cls: 1.486809 loss_iou: 0.337575 loss_dfl: 1.295458 loss_l1: 0.784472 eta: 1 day, 16:34:02 batch_cost: 0.2665 data_cost: 0.0015 ips: 30.0151 images/s
[09/24 11:03:04] ppdet.engine INFO: Epoch: [1] [1800/1832] learning_rate: 0.003965 loss: 2.947834 loss_cls: 1.478187 loss_iou: 0.320757 loss_dfl: 1.215965 loss_l1: 0.748821 eta: 1 day, 16:33:23 batch_cost: 0.2667 data_cost: 0.0009 ips: 30.0004 images/s



**测试日志**：
loading annotations into memory...
Done (t=0.56s)
creating index...
index created!
[09/26 19:06:36] ppdet.utils.checkpoint INFO: Finish loading model weights: output/ppyoloe_crn_s_300e_coco/best_model.pdparams
[09/26 19:06:36] ppdet.engine INFO: Eval iter: 0
[09/26 19:07:03] ppdet.engine INFO: Eval iter: 100
[09/26 19:07:28] ppdet.engine INFO: Eval iter: 200
[09/26 19:07:54] ppdet.engine INFO: Eval iter: 300
[09/26 19:08:20] ppdet.engine INFO: Eval iter: 400
[09/26 19:08:45] ppdet.engine INFO: Eval iter: 500
[09/26 19:09:11] ppdet.engine INFO: Eval iter: 600
[09/26 19:09:37] ppdet.engine INFO: Eval iter: 700
[09/26 19:10:03] ppdet.engine INFO: Eval iter: 800
[09/26 19:10:29] ppdet.engine INFO: Eval iter: 900
[09/26 19:10:54] ppdet.engine INFO: Eval iter: 1000
[09/26 19:11:20] ppdet.engine INFO: Eval iter: 1100
[09/26 19:11:46] ppdet.engine INFO: Eval iter: 1200
[09/26 19:12:12] ppdet.engine INFO: Eval iter: 1300
[09/26 19:12:38] ppdet.engine INFO: Eval iter: 1400
[09/26 19:13:03] ppdet.engine INFO: Eval iter: 1500
[09/26 19:13:29] ppdet.engine INFO: Eval iter: 1600
[09/26 19:13:54] ppdet.engine INFO: Eval iter: 1700
[09/26 19:14:20] ppdet.engine INFO: Eval iter: 1800
[09/26 19:14:46] ppdet.engine INFO: Eval iter: 1900
[09/26 19:15:12] ppdet.engine INFO: Eval iter: 2000
[09/26 19:15:38] ppdet.engine INFO: Eval iter: 2100
[09/26 19:16:04] ppdet.engine INFO: Eval iter: 2200
[09/26 19:16:29] ppdet.engine INFO: Eval iter: 2300
[09/26 19:16:54] ppdet.engine INFO: Eval iter: 2400
[09/26 19:17:34] ppdet.metrics.metrics INFO: The bbox result is saved to bbox.json.
loading annotations into memory...
Done (t=0.41s)
creating index...
index created!
[09/26 19:17:34] ppdet.metrics.coco_utils INFO: Start evaluate...
Loading and preparing results...
DONE (t=11.19s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=46.79s).
Accumulating evaluation results...
DONE (t=18.12s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.003
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.007
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.012
[09/26 19:18:52] ppdet.engine INFO: Total sample number: 4952, averge FPS: 7.773512684163935

-------------------------------------------------------------------------------------------------------------------------------
直接test你们官方提供的模型， 结果如下：
命令：CUDA_VISIBLE_DEVICES=0 python tools/eval.py -c configs/ppyoloe/ppyoloe_plus_crn_s_80e_coco.yml -o weights=ppyoloe_plus_crn_s_80e_coco.pdparams

日志：
W0926 19:39:06.501822 14883 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0926 19:39:06.506700 14883 device_context.cc:465] device: 0, cuDNN Version: 7.6.
loading annotations into memory...
Done (t=0.53s)
creating index...
index created!
[09/26 19:39:11] ppdet.utils.checkpoint INFO: Finish loading model weights: ppyoloe_plus_crn_s_80e_coco.pdparams
[09/26 19:39:11] ppdet.engine INFO: Eval iter: 0
[09/26 19:39:17] ppdet.engine INFO: Eval iter: 100
[09/26 19:39:23] ppdet.engine INFO: Eval iter: 200
[09/26 19:39:28] ppdet.engine INFO: Eval iter: 300
[09/26 19:39:34] ppdet.engine INFO: Eval iter: 400
[09/26 19:39:40] ppdet.engine INFO: Eval iter: 500
[09/26 19:39:46] ppdet.engine INFO: Eval iter: 600
[09/26 19:39:52] ppdet.engine INFO: Eval iter: 700
[09/26 19:39:58] ppdet.engine INFO: Eval iter: 800
[09/26 19:40:04] ppdet.engine INFO: Eval iter: 900
[09/26 19:40:09] ppdet.engine INFO: Eval iter: 1000
[09/26 19:40:15] ppdet.engine INFO: Eval iter: 1100
[09/26 19:40:21] ppdet.engine INFO: Eval iter: 1200
[09/26 19:40:27] ppdet.engine INFO: Eval iter: 1300
[09/26 19:40:33] ppdet.engine INFO: Eval iter: 1400
[09/26 19:40:39] ppdet.engine INFO: Eval iter: 1500
[09/26 19:40:45] ppdet.engine INFO: Eval iter: 1600
[09/26 19:40:51] ppdet.engine INFO: Eval iter: 1700
[09/26 19:40:57] ppdet.engine INFO: Eval iter: 1800
[09/26 19:41:03] ppdet.engine INFO: Eval iter: 1900
[09/26 19:41:09] ppdet.engine INFO: Eval iter: 2000
[09/26 19:41:15] ppdet.engine INFO: Eval iter: 2100
[09/26 19:41:21] ppdet.engine INFO: Eval iter: 2200
[09/26 19:41:27] ppdet.engine INFO: Eval iter: 2300
[09/26 19:41:32] ppdet.engine INFO: Eval iter: 2400
[09/26 19:41:57] ppdet.metrics.metrics INFO: The bbox result is saved to bbox.json.
loading annotations into memory...
Done (t=0.40s)
creating index...
index created!
[09/26 19:41:58] ppdet.metrics.coco_utils INFO: Start evaluate...
Loading and preparing results...
DONE (t=11.04s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=107.40s).
Accumulating evaluation results...
DONE (t=20.70s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.437
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.606
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.479
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.265
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.475
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.590
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.352
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.590
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.653
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.467
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.715
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.817
[09/26 19:44:20] ppdet.engine INFO: Total sample number: 4952, averge FPS: 33.932165302691274"
使用自定义数据集训练yolo7模型报错,PaddlePaddle/PaddleDetection,2022-09-26 09:44:11,5,,7021,1385759196,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

![image](https://user-images.githubusercontent.com/66579493/192245587-d35bf988-6b2b-4e3b-a3f9-1bcd4a102ab8.png)
"
autoaugment算子疑似存在问题,PaddlePaddle/PaddleDetection,2022-09-26 08:34:08,1,,7019,1385658982,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

DataProcess

### Bug描述 Describe the Bug

报错代码如下：
fail to map sample transform [AutoAugment_e4cecf] with error: index can't contain negative values and stack:
Traceback (most recent call last):
  File ""/home/vis/zhaoyilin/code/01_work/01_weizhang_detection/PaddleDetection/ppdet/data/reader.py"", line 54, in __call__
    data = f(data)
  File ""/home/vis/zhaoyilin/code/01_work/01_weizhang_detection/PaddleDetection/ppdet/data/transform/operators.py"", line 105, in __call__
    sample = self.apply(sample, context)
  File ""/home/vis/zhaoyilin/code/01_work/01_weizhang_detection/PaddleDetection/ppdet/data/transform/operators.py"", line 601, in apply
    self.autoaug_type)
  File ""/home/vis/zhaoyilin/code/01_work/01_weizhang_detection/PaddleDetection/ppdet/data/transform/autoaugment_utils.py"", line 1586, in distort_image_with_autoaugment
    augmentation_hparams)
  File ""/home/vis/zhaoyilin/code/01_work/01_weizhang_detection/PaddleDetection/ppdet/data/transform/autoaugment_utils.py"", line 1548, in build_and_apply_nas_policy
    tf_policies, image, bboxes)
  File ""/home/vis/zhaoyilin/code/01_work/01_weizhang_detection/PaddleDetection/ppdet/data/transform/autoaugment_utils.py"", line 1496, in select_and_apply_random_policy
    image, bboxes = policy(image, bboxes)
  File ""/home/vis/zhaoyilin/code/01_work/01_weizhang_detection/PaddleDetection/ppdet/data/transform/autoaugment_utils.py"", line 1540, in final_policy
    prob, bboxes_)
  File ""/home/vis/zhaoyilin/code/01_work/01_weizhang_detection/PaddleDetection/ppdet/data/transform/autoaugment_utils.py"", line 1484, in _apply_func_with_prob
    augmented_image, augmented_bboxes = func(image, bboxes, *args)
  File ""/home/vis/zhaoyilin/code/01_work/01_weizhang_detection/PaddleDetection/ppdet/data/transform/autoaugment_utils.py"", line 747, in translate_y_only_bboxes
    image, bboxes, prob, translate_y, func_changes_bbox, pixels, replace)
  File ""/home/vis/zhaoyilin/code/01_work/01_weizhang_detection/PaddleDetection/ppdet/data/transform/autoaugment_utils.py"", line 706, in _apply_multi_bbox_augmentation_wrapper
    new_image, new_bboxes, prob, aug_func, func_changes_bbox, *args)
  File ""/home/vis/zhaoyilin/code/01_work/01_weizhang_detection/PaddleDetection/ppdet/data/transform/autoaugment_utils.py"", line 687, in _apply_multi_bbox_augmentation
    idx, (image, new_bboxes) = body(idx, (image, new_bboxes))
  File ""/home/vis/zhaoyilin/code/01_work/01_weizhang_detection/PaddleDetection/ppdet/data/transform/autoaugment_utils.py"", line 685, in <lambda>
    _images_and_bboxes[1])]
  File ""/home/vis/zhaoyilin/code/01_work/01_weizhang_detection/PaddleDetection/ppdet/data/transform/autoaugment_utils.py"", line 655, in <lambda>
    wrapped_aug_func = lambda _image, bbox, _new_bboxes: _apply_bbox_augmentation_wrapper(_image, bbox, _new_bboxes, prob, aug_func, func_changes_bbox, *args)
  File ""/home/vis/zhaoyilin/code/01_work/01_weizhang_detection/PaddleDetection/ppdet/data/transform/autoaugment_utils.py"", line 611, in _apply_bbox_augmentation_wrapper
    augmentation_func, *args)
  File ""/home/vis/zhaoyilin/code/01_work/01_weizhang_detection/PaddleDetection/ppdet/data/transform/autoaugment_utils.py"", line 543, in _apply_bbox_augmentation
    constant_values=1)
  File ""<__array_function__ internals>"", line 6, in pad
  File ""/home/vis/zhaoyilin/anaconda3/envs/paddle2.3/lib/python3.7/site-packages/numpy/lib/arraypad.py"", line 743, in pad
    pad_width = _as_pairs(pad_width, array.ndim, as_index=True)
  File ""/home/vis/zhaoyilin/anaconda3/envs/paddle2.3/lib/python3.7/site-packages/numpy/lib/arraypad.py"", line 514, in _as_pairs
    raise ValueError(""index can't contain negative values"")
ValueError: index can't contain negative values

### 复现环境 Environment

paddle 2.3.1
release 2.4

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
基于ONNXRuntime预测库的c++样例代码，希望能尽快提供,PaddlePaddle/PaddleDetection,2022-09-26 02:28:04,2,,7016,1385316139,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

![image](https://user-images.githubusercontent.com/31306616/192181841-1f7307c3-76d2-43eb-a14b-bd2df8caf548.png)

基于ONNXRuntime预测库的c++样例代码，一直是Coming soon，请尽快提供一个实际例子。

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
模型评价以及数据集的问题,PaddlePaddle/PaddleDetection,2022-09-25 03:36:28,0,,7015,1384884040,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

当我使用yolov3、yolov5系列中。
自建数据集没有进行test数据集的划分，在dataset/coco_detection中没有存放正确的test路径，但是一切训练都很正常，最后eval也是有结果（map），这个结果应该是没有用到test数据集的，**这个结果是用val数据集的吧，那这个结果应该是对模型的拟合效果的评价吧？**
如果我设置了test数据集，才能体现模型的泛化能力，这个时候在训练过程应该会有test的损失过程吧，重点是，**对泛化能力的评价是不是也是用map来计算（还是说有另外体现泛化能力的标准？）**，**这里的map应该是用test数据集而不是val集，那我如果设置了test数据集，如何调整参数是最后的map是对泛化能力的体现。**

理论上明白val、test的作用，但是我看代码里没有test图片以及标签的设置位置，那对模型泛化能力的map怎么看。希望可以有一个回答 谢谢"
为什么PPYOLOE配置文件NormalizaImage参数影响对训练效果极大,PaddlePaddle/PaddleDetection,2022-09-24 13:39:00,5,,7014,1384677207,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

您好，我在使用ppyoloe_plus_crn_l_largesize_80e_visdrone.yml在自定义数据集上进行训练时，尽管配置文件中batch_transforms中的NormalizaImage的norm_type配置为none，但在实际训练过程中发现NormalizaImage中的mean和std对训练结果依然具有非常大的影响。查看ppdet/data/operator.py文件中的NormalizaImage源码后发现norm_type=None时，mean和std对训练应当不起作用才对。"
Faster video Processing using the all power ,PaddlePaddle/PaddleDetection,2022-09-23 16:29:55,0,,7013,1384041398,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

Hi,

How we can increase the video processing speed for small object detection : 

`CUDA_VISIBLE_DEVICES=0 python deploy/python/infer.py --model_dir=output_inference/ppyoloe_crn_l_80e_sliced_visdrone_640_025   --device=GPU  --threshold=0.40 --video_file=/data/Videos/2Khiv00131.mp4 --cpu_thread=8 --output_dir=output/_hivALL_1   --use_gpu=0 --enable_mkldnn=True --save_mot_txt_per_img`

increains the batch_size doesnt affecting the speed. GPU only using %33 and only  2 cores cpu.

in 2080Ti I assume I should process the small file detection (above) in realtime. 

Where is the pieces that we an gain extra performance ? 
* Decoding video ? (Opencv read from video)
* Is there any multithreaded way to process video to use all horse power both Gpu and cpu ? 
* Is the paddleServing has this capability ? 

"
solov2 推断标签错误和部署问题,PaddlePaddle/PaddleDetection,2022-09-23 09:14:38,2,,7011,1383515590,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

1、我用自己的数据集训练后，infer的结果标签是coco数据集的标签，我应该修改什么部分能够让标签正确显示

2、我希望把solo推断的掩膜部分单独作为图像结果输出，如何修改代码

3、请问有没有solov2模型在安卓端部署的教程或者流程，官方模型导出的代码，导出后我没找到继续进行部署的方法"
【YOLO Vision】Sep 27 PaddleDetection,PaddlePaddle/PaddleDetection,2022-09-23 07:45:12,0,,7010,1383418265,"### 问题描述 Please describe your issue

## 🎊YOLO Vision世界学术交流大会🎊
PaddleDetection受邀参与首个以YOLO为主题的YOLO Vision世界大会，与全球AI领先开发者学习交流，欢迎大家报名参加！

⏰时间：9月27日
👨‍🏫演讲主题：PaddleDetection Toolkit and PP-YOLO Series
💎圆桌论坛：Open Source Projects Enabling the Future of Computer Vision AI

**⛓报名链接：https://ultralytics.com/yolo-vision**

🔮彩蛋：8月26日 PaddleDetection发布YOLO系列全家族，包括YOLOv5/X/v6/v7与自研的PP-YOLOE、PP-YOLOE+
🗳项目链接：https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.5/docs/feature_models/YOLOSERIES_MODEL.md

---

## 🎊YOLO Vision Event🎊
PaddleDetection is invited to attend the world-class conference in YOLO field--YOLO Vision Event. 
Join the experts of Ultralytics as well as leaders in the space on September 27th, 2022 to explore the technical and business insights shaping the future of Vision AI!

⏰Time：Sep 27th
👨‍🏫Tech Talk：PaddleDetection Toolkit and PP-YOLO Series
💎Panel Topic：Open Source Projects Enabling the Future of Computer Vision AI

**⛓Register Now：https://ultralytics.com/yolo-vision**

🔮Easter eggs：PaddleDetection has released the YOLO Family model zoo on August 26th, including YOLOv3/YOLOv5/YOLOX/YOLOv7 and PP-YOLOE/PP-YOLOE+, feel free to check out: https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.5/docs/feature_models/YOLOSERIES_MODEL.md"
"Evaluation results empty, this may be due to training iterations being too few or not loading the correct weights",PaddlePaddle/PaddleDetection,2022-09-23 07:15:22,2,,7009,1383389595,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

_No response_

### Bug描述 Describe the Bug

请问把ppyoloe backbone中的一个conv3*3换成dcnv2之后，为什么会出现这样的情况呀
![5aefa503cae3cd508c4ab4466089499](https://user-images.githubusercontent.com/99952437/191909178-0f456292-ac3c-4302-85f0-c37ae2546933.jpg)
![5decda566221c889a0b4eec0dc7199d](https://user-images.githubusercontent.com/99952437/191909191-b5dbefdd-4c57-4f83-a2fa-bacccf4782b8.jpg)


### 复现环境 Environment

-PaddlePaddle:2.3.1
-Python:3.7.13

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
飞行器跟踪案例,PaddlePaddle/PaddleDetection,2022-09-22 14:18:23,2,,7007,1382535537,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

在页面介绍中看到与飞行器跟踪的案例动态图，请问在哪里可以找到相应的数据集和跟踪设置代码呢，非常感谢。"
/deploy/auto_compression/run.py 对yolox_s自动压缩报错,PaddlePaddle/PaddleDetection,2022-09-22 09:19:59,0,,7004,1382120452,"### 问题确认 Search before asking

- [x] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

ppyoloe-l可以压缩，但对yolox_s压缩时出错
1、执行以下代码得到转换后的模型
python tools/export_model.py \
       -c configs/yolox/yolox_s_300e_coco.yml \
       -o weights=https://paddledet.bj.bcebos.com/models/yolox_s_300e_coco.pdparams \
       trt=True \

2、进行压缩
export CUDA_VISIBLE_DEVICES=0
python run.py --config_path=./configs/yolox_s_qat_dis.yaml --save_dir='./output_yolox/'

报错了：
loading annotations into memory...
Done (t=0.58s)
creating index...
index created!
W0922 09:13:05.484802 43786 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.4, Runtime API Version: 10.2
W0922 09:13:05.487728 43786 gpu_context.cc:306] device: 0, cuDNN Version: 8.0.
loading annotations into memory...
Done (t=0.86s)
creating index...
index created!
Traceback (most recent call last):
  File ""run.py"", line 189, in <module>
    main()
  File ""run.py"", line 178, in main
    eval_callback=eval_func)
  File ""/home/PaddleDetection/venv/lib/python3.6/site-packages/paddleslim/auto_compression/compressor.py"", line 146, in __init__
    self.feed_vars)
  File ""/home/PaddleDetection/venv/lib/python3.6/site-packages/paddleslim/common/dataloader.py"", line 38, in wrap_dataloader
    data = next(dataloader())
  File ""run.py"", line 57, in gen
    in_dict[input_name] = data[input_name]
KeyError: 'image'


要怎么解决？"
ppyoloe 在导出模型时，如何去掉scale_factor相关分支？,PaddlePaddle/PaddleDetection,2022-09-22 03:20:44,1,,6996,1381766600,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

我将ppyoloe模型导出转为onnx模型在某平台进行测试，该平台支持的onnx输入仅有图片一个参数，因此由于yoloe的输入包括图片image和scale_factor两个参数，现在我需要将scale_factor相关的计算分支从导出的模型中去掉，放在模型外进行后处理;请问应该如何操作？需要修改哪些代码？"
pp-human一用上gpu就报错,PaddlePaddle/PaddleDetection,2022-09-22 02:09:44,3,,6995,1381719214,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

跑pp-human给的模型，进行推理，只要加上gpu就报错，跑的命令是
![image](https://user-images.githubusercontent.com/32315802/191642689-ebd4a543-9d5d-4653-9fc9-989df9e9f3aa.png)
，去掉device=gpu就可以顺利进行，报错如下：
![image](https://user-images.githubusercontent.com/32315802/191642807-47a3afbf-392b-4a00-8f67-6cbfe1391652.png)
"
"InvalidArgumentError: The axes should be less than or equal to input tensor's rank.But received 2 of axes[0], input tensor shape [2]",PaddlePaddle/PaddleDetection,2022-09-21 13:40:19,2,,6999,1381953185,"### 请提出你的问题 Please ask your question

用 paddle 官方的 infer_yolov3.py 做推理，yolov3的模型就没问题，yolox 就会报错下面的日志，请问下是哪块需要专门配置吗？搜了各种答案都找不到

## 堆栈如下
Traceback (most recent call last):
  File ""infer_yolov3.py"", line 132, in <module>
    result = run(pred, [im_shape, data, scale_factor])
  File ""infer_yolov3.py"", line 73, in run
    predictor.run()
ValueError: In user code:

    File ""tools/export_model.py"", line 108, in <module>
      main()
    File ""tools/export_model.py"", line 104, in main
      run(FLAGS, cfg)
    File ""tools/export_model.py"", line 73, in run
      trainer.export(FLAGS.output_dir)
    File ""/paddle/ppdet/engine/trainer.py"", line 1060, in export
      save_dir)
    File ""/paddle/ppdet/engine/trainer.py"", line 1019, in _get_infer_cfg_and_input_spec
      input_spec, static_model.forward.main_program,
    File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 580, in main_program
      concrete_program = self.concrete_program
    File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 488, in concrete_program
      return self.concrete_program_specify_input_spec(input_spec=None)
    File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 528, in concrete_program_specify_input_spec
      *desired_input_spec, with_hook=with_hook)
    File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 436, in get_concrete_program
      concrete_program, partial_program_layer = self._program_cache[cache_key]
    File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 801, in __getitem__
      self._caches[item_id] = self._build_once(item)
    File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 790, in _build_once
      **cache_key.kwargs)
    File ""/usr/local/python3.7.0/lib/python3.7/site-packages/decorator.py"", line 232, in fun
      return caller(func, *(extras + args), **kw)
    File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/wrapped_decorator.py"", line 25, in __impl__
      return wrapped_func(*args, **kwargs)
    File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/dygraph/base.py"", line 51, in __impl__
      return func(*args, **kwargs)
    File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 733, in from_func_spec
      outputs = static_func(*inputs)
    File ""/tmp/tmpd54zgkce.py"", line 101, in forward
      false_fn_5, (), (inputs, self), (out,))
    File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 211, in convert_ifelse
      out = _run_py_ifelse(pred, true_fn, false_fn, true_args, false_args)
    File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 257, in _run_py_ifelse
      return true_fn(*true_args) if pred else false_fn(*false_args)
    File ""/tmp/tmpd54zgkce.py"", line 84, in false_fn_5
      for_loop_body_0, [__for_loop_var_index_0, inputs_list])
    File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 45, in convert_while_loop
      loop_vars = _run_py_while(cond, body, loop_vars)
    File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 59, in _run_py_while
      loop_vars = body(*loop_vars)
    File ""/tmp/tmpd54zgkce.py"", line 79, in for_loop_body_0
      dy2static.convert_call(self.get_pred)())
    File ""/paddle/ppdet/modeling/architectures/yolox.py"", line 108, in get_pred
      return self._forward()
    File ""/paddle/ppdet/modeling/architectures/yolox.py"", line 91, in _forward
      body_feats = self.backbone(self.inputs)
    File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 930, in __call__
      return self._dygraph_call_func(*inputs, **kwargs)
    File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 915, in _dygraph_call_func
      outputs = self.forward(*inputs, **kwargs)
    File ""/paddle/ppdet/modeling/backbones/csp_darknet.py"", line 391, in forward
      x = self.stem(x)
    File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 930, in __call__
      return self._dygraph_call_func(*inputs, **kwargs)
    File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 915, in _dygraph_call_func
      outputs = self.forward(*inputs, **kwargs)
    File ""/paddle/ppdet/modeling/backbones/csp_darknet.py"", line 117, in forward
      top_left = inputs[:, :, 0::2, 0::2]
    File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/framework.py"", line 2186, in __getitem__
      return _getitem_impl_(self, item)
    File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/variable_index.py"", line 489, in _getitem_impl_
      attrs=attrs)
    File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/framework.py"", line 3621, in append_op
      attrs=kwargs.get(""attrs"", None))
    File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/framework.py"", line 2635, in __init__
      for frame in traceback.extract_stack():

    InvalidArgumentError: The axes should be less than or equal to input tensor's rank.But received 2 of axes[0], input tensor shape [2]
      [Hint: Expected axes[i] < in_dims.size(), but received axes[i]:2 >= in_dims.size():2.] (at /home/Paddle/paddle/fluid/operators/strided_slice_op.cc:76)
      [operator < strided_slice > error]
"
"转换模型报错,AttributeError: module 'paddle.vision.ops' has no attribute 'generate_proposals'",PaddlePaddle/PaddleDetection,2022-09-21 11:33:41,1,,6993,1380779122,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

Python 3.8.13
paddledet          2.5.0
paddlepaddle-gpu   2.2.2.post112

```
(PaddleDetection) PS I:\Paddle\PaddleDetection> python tools/export_model.py -c configs\mask_rcnn\mask_rcnn_r50_vd_fpn_2x_coco.yml  --output_dir=I:\Paddle\PaddleDetection\output\01_XIA  -o weights=I:\Paddle\PaddleDetection\output\01_XIA_mask_rcnn_r50_vd_fpn_2x_coco\19.pdparams

D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\vision\transforms\functional_pil.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': Image.NEAREST,
D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\vision\transforms\functional_pil.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': Image.BILINEAR,
D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\vision\transforms\functional_pil.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': Image.BICUBIC,
D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\vision\transforms\functional_pil.py:39: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  'box': Image.BOX,
D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\vision\transforms\functional_pil.py:40: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  'lanczos': Image.LANCZOS,
D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\vision\transforms\functional_pil.py:41: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  'hamming': Image.HAMMING
Warning: Unable to use JDE/FairMOT/ByteTrack, please install lap, for example: `pip install lap`, see https://github.com/gatagat/lap
[09/21 19:30:43] ppdet.utils.checkpoint INFO: Finish loading model weights: I:\Paddle\PaddleDetection\output\01_XIA_mask_rcnn_r50_vd_fpn_2x_coco\19.pdparams
loading annotations into memory...
Done (t=0.09s)
creating index...
index created!
[09/21 19:30:44] ppdet.engine INFO: Export inference config file to I:\Paddle\PaddleDetection\output\01_XIA\mask_rcnn_r50_vd_fpn_2x_coco\infer_cfg.yml
Traceback (most recent call last):
  File ""tools/export_model.py"", line 108, in <module>
    main()
  File ""tools/export_model.py"", line 104, in main
    run(FLAGS, cfg)
  File ""tools/export_model.py"", line 73, in run
    trainer.export(FLAGS.output_dir)
  File ""I:\Paddle\PaddleDetection\ppdet\engine\trainer.py"", line 1059, in export
    static_model, pruned_input_spec = self._get_infer_cfg_and_input_spec(
  File ""I:\Paddle\PaddleDetection\ppdet\engine\trainer.py"", line 1019, in _get_infer_cfg_and_input_spec
    input_spec, static_model.forward.main_program,
  File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 563, in main_program
    concrete_program = self.concrete_program
  File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 479, in concrete_program
    return self.concrete_program_specify_input_spec(input_spec=None)
  File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 516, in concrete_program_specify_input_spec
    concrete_program, _ = self.get_concrete_program(
  File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 427, in get_concrete_program
    concrete_program, partial_program_layer = self._program_cache[cache_key]
  File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 723, in __getitem__
    self._caches[item] = self._build_once(item)
  File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 709, in _build_once
    concrete_program = ConcreteProgram.from_func_spec(
  File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\decorator.py"", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\wrapped_decorator.py"", line 25, in __impl__
    return wrapped_func(*args, **kwargs)
  File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\base.py"", line 51, in __impl__
    return func(*args, **kwargs)
  File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 668, in from_func_spec
    error_data.raise_new_exception()
  File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\error.py"", line 336, in raise_new_exception
    six.exec_(""raise new_exception from None"")
  File ""<string>"", line 1, in <module>
AttributeError: In transformed code:

    File ""I:\Paddle\PaddleDetection\ppdet\modeling\architectures\meta_arch.py"", line 74, in forward
        self.inputs = inp
    File ""I:\Paddle\PaddleDetection\ppdet\modeling\architectures\mask_rcnn.py"", line 133, in get_pred
        bbox_pred, bbox_num, mask_pred = self._forward()
    File ""I:\Paddle\PaddleDetection\ppdet\modeling\architectures\mask_rcnn.py"", line 102, in _forward
        else:
    File ""I:\Paddle\PaddleDetection\ppdet\modeling\proposal_generator\rpn_head.py"", line 141, in forward
        rois, rois_num = self._gen_proposal(scores, deltas, anchors, inputs)
    File ""I:\Paddle\PaddleDetection\ppdet\modeling\proposal_generator\rpn_head.py"", line 197, in _gen_proposal
        batch_size = paddle.slice(paddle.shape(im_shape), [0], [0], [1])

        # Generate proposals for each level and each batch.
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
        # Discard batch-computing to avoid sorting bbox cross different batches.
        for i in range(batch_size):

    File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\convert_call_func.py"", line 258, in convert_call
        converted_call = convert_to_static(call_func)
    File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 140, in convert_to_static
        static_func = _FUNCTION_CACHE.convert_with_cache(function)
    File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 77, in convert_with_cache
        static_func = self._convert(func)
    File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 115, in _convert
        root_wrapper = self._dygraph_to_static.get_static_ast(root)
    File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\ast_transformer.py"", line 58, in get_static_ast
        self.static_analysis_visitor = StaticAnalysisVisitor(root)
    File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\static_analysis.py"", line 207, in __init__
        self.run(ast_root)
    File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\static_analysis.py"", line 215, in run
        self.dfs_visit(ast_root)
    File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\static_analysis.py"", line 241, in dfs_visit
        func_type = self.dfs_visit(child)
    File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\static_analysis.py"", line 244, in dfs_visit
        self.dfs_visit(child)
    File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\static_analysis.py"", line 244, in dfs_visit
        self.dfs_visit(child)
    File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\static_analysis.py"", line 244, in dfs_visit
        self.dfs_visit(child)
    File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\static_analysis.py"", line 247, in dfs_visit
        cur_wrapper.node_var_type = self._get_node_var_type(cur_wrapper)
    File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\static_analysis.py"", line 359, in _get_node_var_type
        if is_dygraph_api(node):
    File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\utils.py"", line 201, in is_dygraph_api
        if is_api_in_module(node, DYGRAPH_TO_STATIC_MODULE_PREFIX):
    File ""D:\anaconda308\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\utils.py"", line 192, in is_api_in_module
        return eval(""_is_api_in_module_helper({}, '{}')"".format(func_str,
    File ""<string>"", line 1, in <module>


    AttributeError: module 'paddle.vision.ops' has no attribute 'generate_proposals'

```"
将推理的结果按类名进行切割分类,PaddlePaddle/PaddleDetection,2022-09-21 07:35:39,1,,6990,1380467001,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

例如，我的模型是检测猫和狗的，在我预测一张图片后，能否将猫和狗的部分单独切分出来，并且放入两个不同的文件夹？"
训练PaddleDetection仓库的ppyolo网络，loss=nan,PaddlePaddle/PaddleDetection,2022-09-21 07:03:20,9,,6998,1381946132,"### bug描述 Describe the Bug

基于Paddle develop分支 - PaddleDetection develop分支， 以及Paddle release2.3分支 - PaddleDetection release2.4分支，跑ppyolo网络，无论是4卡还是8卡，均会有50%的概率出现 loss=nan。环境和超参数修改如下：

**原生参数8卡参数为（base_lr: 0.01，batch_size: 24）**

 - **出现loss=nan的8卡参数和脚本**

参数为（base_lr: 0.005，batch_size: 12），学习率和batch减半，训练脚本如下：
`python -m paddle.distributed.launch --gpus 0,1,2,3,4,5,6,7 tools/train.py -c configs/ppyolo/ppyolo_r50vd_dcn_1x_coco.yml`

git diff 如下：
```bash
diff --git a/configs/ppyolo/_base_/optimizer_1x.yml b/configs/ppyolo/_base_/optimizer_1x.yml
index 8e6301e..a419c52 100644
--- a/configs/ppyolo/_base_/optimizer_1x.yml
+++ b/configs/ppyolo/_base_/optimizer_1x.yml
@@ -1,7 +1,7 @@
-epoch: 405
+epoch: 50
 
 LearningRate:
-  base_lr: 0.01
+  base_lr: 0.005
   schedulers:
   - !PiecewiseDecay
     gamma: 0.1
diff --git a/configs/ppyolo/_base_/ppyolo_reader.yml b/configs/ppyolo/_base_/ppyolo_reader.yml
index 1698539..2814cfa 100644
--- a/configs/ppyolo/_base_/ppyolo_reader.yml
+++ b/configs/ppyolo/_base_/ppyolo_reader.yml
@@ -17,7 +17,7 @@ TrainReader:
     - NormalizeImage: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225], is_scale: True}
     - Permute: {}
     - Gt2YoloTarget: {anchor_masks: [[6, 7, 8], [3, 4, 5], [0, 1, 2]], anchors: [[10, 13], [16, 30], [33, 23], [30, 61], [62, 45], [59, 119], [116, 90], [156, 198], [373, 326]], downsample_ratios: [32, 16, 8]}
-  batch_size: 24
+  batch_size: 12
   shuffle: true
   drop_last: true
   mixup_epoch: 25000
```
----
- **出现loss=nan的4卡参数和脚本**

参数为（base_lr: 0.0025，batch_size: 12），学习率和batch减半，训练脚本如下：
`python -m paddle.distributed.launch  --gpus 0,1,2,3 tools/train.py -c configs/ppyolo/ppyolo_r50vd_dcn_1x_coco.yml`

git diff 如下：
```bash
diff --git a/configs/ppyolo/_base_/optimizer_1x.yml b/configs/ppyolo/_base_/optimizer_1x.yml
index 8e6301e32..f96dff742 100644
--- a/configs/ppyolo/_base_/optimizer_1x.yml
+++ b/configs/ppyolo/_base_/optimizer_1x.yml
@@ -1,7 +1,7 @@
-epoch: 405
+epoch: 15
 
 LearningRate:
-  base_lr: 0.01
+  base_lr: 0.0025
   schedulers:
   - !PiecewiseDecay
     gamma: 0.1
diff --git a/configs/ppyolo/_base_/ppyolo_reader.yml b/configs/ppyolo/_base_/ppyolo_reader.yml
index 1698539af..2814cfa9f 100644
--- a/configs/ppyolo/_base_/ppyolo_reader.yml
+++ b/configs/ppyolo/_base_/ppyolo_reader.yml
@@ -17,7 +17,7 @@ TrainReader:
     - NormalizeImage: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225], is_scale: True}
     - Permute: {}
     - Gt2YoloTarget: {anchor_masks: [[6, 7, 8], [3, 4, 5], [0, 1, 2]], anchors: [[10, 13], [16, 30], [33, 23], [30, 61], [62, 45], [59, 119], [116, 90], [156, 198], [373, 326]], downsample_ratios: [32, 16, 8]}
-  batch_size: 24
+  batch_size: 12
   shuffle: true
   drop_last: true
   mixup_epoch: 25000
```

报错信息如下：
```python
[09/20 20:33:24] ppdet.engine INFO: Epoch: [4] [1960/2443] learning_rate: 0.002500 loss_xy: 4.929123 loss_wh: 6.653260 loss_iou: 17.435518 loss_iou_aware: 3.315290 loss_obj: 37.247818 loss_cls: 20.935398 loss: 89.923317 eta: 6:41:38 batch_cost: 0.9275 data_cost: 0.0547 ips: 12.9386 images/s
[09/20 20:33:44] ppdet.engine INFO: Epoch: [4] [1980/2443] learning_rate: 0.002500 loss_xy: 5.024055 loss_wh: 6.127202 loss_iou: 17.634968 loss_iou_aware: 3.545506 loss_obj: 38.444519 loss_cls: 22.425531 loss: 97.338821 eta: 6:41:19 batch_cost: 0.9750 data_cost: 0.0003 ips: 12.3075 images/s
[09/20 20:34:05] ppdet.engine INFO: Epoch: [4] [2000/2443] learning_rate: 0.002500 loss_xy: 4.801731 loss_wh: 6.628194 loss_iou: 17.139820 loss_iou_aware: 3.319241 loss_obj: 38.700241 loss_cls: 22.693611 loss: 93.608757 eta: 6:40:59 batch_cost: 0.9674 data_cost: 0.0006 ips: 12.4038 images/s
[09/20 20:34:26] ppdet.engine INFO: Epoch: [4] [2020/2443] learning_rate: 0.002500 loss_xy: 4.418971 loss_wh: 5.848875 loss_iou: 15.264145 loss_iou_aware: 2.959143 loss_obj: 34.033241 loss_cls: 18.832973 loss: 79.911163 eta: 6:40:41 batch_cost: 0.9842 data_cost: 0.0003 ips: 12.1927 images/s
[09/20 20:34:46] ppdet.engine INFO: Epoch: [4] [2040/2443] learning_rate: 0.002500 loss_xy: 4.705611 loss_wh: 5.466923 loss_iou: 15.579102 loss_iou_aware: 3.252701 loss_obj: 36.642715 loss_cls: 22.718971 loss: 90.127289 eta: 6:40:20 batch_cost: 0.9489 data_cost: 0.0003 ips: 12.6465 images/s
[09/20 20:35:05] ppdet.engine INFO: Epoch: [4] [2060/2443] learning_rate: 0.002500 loss_xy: nan loss_wh: nan loss_iou: nan loss_iou_aware: nan loss_obj: nan loss_cls: nan loss: nan eta: 6:39:59 batch_cost: 0.9237 data_cost: 0.2108 ips: 12.9917 images/s
[09/20 20:35:26] ppdet.engine INFO: Epoch: [4] [2080/2443] learning_rate: 0.002500 loss_xy: nan loss_wh: nan loss_iou: nan loss_iou_aware: nan loss_obj: nan loss_cls: nan loss: nan eta: 6:39:41 batch_cost: 0.9857 data_cost: 0.2855 ips: 12.1743 images/s
```

### 其他补充信息 Additional Supplementary Information

_No response_"
基于Paddle有没有自监督或者半自监督的目标检测模型？,PaddlePaddle/PaddleDetection,2022-09-21 01:18:51,1,feature request,6986,1380176219,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

基于Paddle有没有自监督或者半自监督的目标检测模型？"
batch_size怎么设置才合理,PaddlePaddle/PaddleDetection,2022-09-20 12:13:35,2,,6984,1379320906,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

比如我的是3060显卡 12GB显存

TrainReader
  batch_size: 1

EvalReader
  batch_size: 1

TestReader:
  batch_size: 1

比如
yolo模型
maskrcnn v50 模型

需要怎么设置才比较合理"
准备数据路径失效,PaddlePaddle/PaddleDetection,2022-09-20 11:39:11,1,,6981,1379274314,"### 文档链接&描述 Document Links & Description

https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.5/docs/tutorials/data/PrepareDataSet.md#%E5%87%86%E5%A4%87%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE
这里的路径失效

### 请提出你的建议 Please give your suggestion

_No response_"
评估时出现错误,PaddlePaddle/PaddleDetection,2022-09-20 11:05:54,1,,6980,1379229700,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Validation

### Bug描述 Describe the Bug

```
Traceback (most recent call last):
  File ""/home/aistudio/PaddleDetection/tools/eval.py"", line 193, in <module>
    main()
  File ""/home/aistudio/PaddleDetection/tools/eval.py"", line 189, in main
    run(FLAGS, cfg)
  File ""/home/aistudio/PaddleDetection/tools/eval.py"", line 154, in run
    trainer.evaluate()
  File ""/home/aistudio/PaddleDetection/ppdet/engine/trainer.py"", line 621, in evaluate
    self._eval_with_loader(self.loader)
  File ""/home/aistudio/PaddleDetection/ppdet/engine/trainer.py"", line 595, in _eval_with_loader
    outs = self.model(data)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""/home/aistudio/PaddleDetection/ppdet/modeling/architectures/meta_arch.py"", line 75, in forward
    outs.append(self.get_pred())
  File ""/home/aistudio/PaddleDetection/ppdet/modeling/architectures/cascade_rcnn.py"", line 136, in get_pred
    bbox_pred, bbox_num, mask_pred = self._forward()
  File ""/home/aistudio/PaddleDetection/ppdet/modeling/architectures/cascade_rcnn.py"", line 112, in _forward
    preds, (refined_rois, rois_num), im_shape, scale_factor)
  File ""/home/aistudio/PaddleDetection/ppdet/modeling/post_process.py"", line 68, in __call__
    bbox_pred, bbox_num, _ = self.nms(bboxes, score, self.num_classes)
TypeError: 'dict' object is not callable
```

### 复现环境 Environment

- PaddlePaddle: 2.3.2
- PaddleDetection: 2.5


### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
det_keypoint_unite_infer 部署集成问题：NameError: name 'Detector' is not defined,PaddlePaddle/PaddleDetection,2022-09-20 05:42:47,1,,6977,1378849762,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

![image](https://user-images.githubusercontent.com/74706568/191173531-20a8b06e-2578-4a48-8ce4-e44ba26d9268.png)

由于是集成到项目中，不采用命令行的方式，是否和未设置FLAGS参数有关？"
param_groups在paddle中的api,PaddlePaddle/PaddleDetection,2022-09-20 03:56:36,1,,6976,1378781071,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

请问torch优化器中的optimizer.param_groups，在paddle对应哪个api呀？"
跨镜头目标追踪，如何接入rtsp 视频流,PaddlePaddle/PaddleDetection,2022-09-19 15:28:58,2,,6973,1378141023,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

例子里面只能配置多个视频文件，请问如何接入多个视频流"
"assert ct > 0, 'not found any coco record in %s' % (anno_path)",PaddlePaddle/PaddleDetection,2022-09-19 14:15:51,3,,6972,1378033925,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

```
```
[09/19 22:12:31] ppdet.data.source.coco WARNING: Illegal image file: I:\2022\MsCOCO_dataset\Train\MM_3_A_22.jpg, and it will be ignored
[09/19 22:12:31] ppdet.data.source.coco WARNING: Illegal image file: I:\2022\MsCOCO_dataset\Train\MM_3_A_23.jpg, and it will be ignored
[09/19 22:12:31] ppdet.data.source.coco WARNING: Illegal image file: I:\2022\MsCOCO_dataset\Train\MM_3_A_24.jpg, and it will be ignored
[09/19 22:12:31] ppdet.data.source.coco WARNING: Illegal image file: I:\2022\MsCOCO_dataset\Train\MM_3_A_25.jpg, and it will be ignored
[09/19 22:12:31] ppdet.data.source.coco WARNING: Illegal image file: I:\2022\MsCOCO_dataset\Train\MM_3_A_26.jpg, and it will be ignored
[09/19 22:12:31] ppdet.data.source.coco WARNING: Illegal image file: I:\2022\MsCOCO_dataset\Train\MM_3_A_27.jpg, and it will be ignored
[09/19 22:12:31] ppdet.data.source.coco WARNING: Illegal image file: I:\2022\MsCOCO_dataset\Train\MM_3_A_28.jpg, and it will be ignored
[09/19 22:12:31] ppdet.data.source.coco WARNING: Illegal image file: I:\2022\MsCOCO_dataset\Train\MM_3_A_29.jpg, and it will be ignored
[09/19 22:12:31] ppdet.data.source.coco WARNING: Illegal image file: I:\2022\MsCOCO_dataset\Train\MM_3_A_30.jpg, and it will be ignored
Traceback (most recent call last):
  File ""tools/train.py"", line 172, in <module>
    main()
  File ""tools/train.py"", line 168, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 123, in run
    trainer = Trainer(cfg, mode='train')
  File ""I:\Paddle\PaddleDetection\ppdet\engine\trainer.py"", line 94, in __init__
    self.loader = create('{}Reader'.format(capital_mode))(
  File ""I:\Paddle\PaddleDetection\ppdet\data\reader.py"", line 163, in __call__
    self.dataset.parse_dataset()
  File ""I:\Paddle\PaddleDetection\ppdet\data\source\coco.py"", line 225, in parse_dataset
    assert ct > 0, 'not found any coco record in %s' % (anno_path)
AssertionError: not found any coco record in I:\2022\MsCOCO_dataset\Annotation\Train.json
```

```
metric: COCO
num_classes: 1

TrainDataset:
  !COCODataSet
    image_dir: Train
    anno_path: I:\2022\MsCOCO_dataset\Annotation\Train.json
    dataset_dir: I:\2022\MsCOCO_dataset\
    data_fields: ['image', 'gt_bbox', 'gt_class', 'gt_poly', 'is_crowd']

EvalDataset:
  !COCODataSet
    image_dir: Val
    anno_path: I:\2022\MsCOCO_dataset\Annotation\Val.json
    dataset_dir: I:\2022\MsCOCO_dataset\Val

TestDataset:
  !ImageFolder
    anno_path: I:\2022\MsCOCO_dataset\Annotation\Test.json # also support txt (like VOC's label_list.txt)
    dataset_dir: I:\2022\MsCOCO_dataset\Test # if set, anno_path will be 'dataset_dir/anno_path'

```"
基于Paddle的YOLOX的backbone有没有替换成Hornet进行实验，与YOLOX-ConvNeXt-s相比精度如何？,PaddlePaddle/PaddleDetection,2022-09-19 09:58:06,1,feature request,6970,1377699802,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

基于Paddle的YOLOX的backbone有没有替换成Hornet进行实验，与YOLOX-ConvNeXt-s相比精度如何？"
"集群Docker训练报错：Compiled with WITH_GPU, but no GPU found in runtime.",PaddlePaddle/PaddleDetection,2022-09-19 03:53:39,1,,6968,1377358026,"### 问题确认 Search before asking

- [x] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

在集群中使用同一个镜像，申请交互式调试任务的时候运行paddledet没有问题。
![image](https://user-images.githubusercontent.com/54058611/190946656-c3f62371-290b-4ca5-abe1-5399385299ad.png)
但是在申请训练任务的时候，会出现
![image](https://user-images.githubusercontent.com/54058611/190946841-fa5d91fa-a657-4eb3-b962-d6d4def8e136.png)
但是系统是存在gpu环境的
![image](https://user-images.githubusercontent.com/54058611/190947092-50f2bfa6-34e1-4fbf-8496-962fc613d97a.png)
按照以往使用其它深度学习的框架经验，申请交互式调试任务能够正常运行，申请任务训练时试可以正常运行的，请问是怎么回事？（我也试过cudnn8）
"
单目标追踪,PaddlePaddle/PaddleDetection,2022-09-19 02:31:05,2,feature request,6967,1377304102,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

paddledetection 可以做单目标追踪吗？有相关示例吗？包括样本格式怎么组织之类的？"
windows上执行tools/infer_mot.py报错,PaddlePaddle/PaddleDetection,2022-09-18 15:15:00,2,windows,6966,1377091148,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

我在windows上执行python tools/infer_mot.py -c configs/mot/fairmot/fairmot_dla34_30e_1088x608.yml -o weights=https://paddledet.bj.bcebos.com/models/mot/fairmot_dla34_30e_1088x608.pdparams --video_file ..\image\ped.mp4 --frame_rate=20 --save_videos
我的paddldpaddle2.3.2 cuda 11.6 显卡3070
为什么会报错？如何解决？
报错内容如下：
PS D:\python\envs\paddle2\paddledetection> python tools/infer_mot.py -c configs/mot/fairmot/fairmot_dla34_30e_1088x608.yml -o weights=https://paddledet.bj.bcebos.com/models/mot/fairmot_dla34_30e_1088x608.pdparams --video_file ..\image\ped.mp4 --fra
me_rate=20 --save_videos
W0918 23:06:19.119932 20496 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 11.6, Runtime API Version: 11.6
W0918 23:06:19.135466 20496 gpu_resources.cc:91] device: 0, cuDNN Version: 8.4.
[09/18 23:06:21] ppdet.utils.checkpoint INFO: Finish resuming model weights: C:\Users\62845/.cache/paddle/weights\fairmot_dla34_30e_1088x608.pdparams
[09/18 23:06:23] ppdet.data.source.mot INFO: Length of the video: 1202 frames.
[09/18 23:06:23] ppdet.engine.tracker INFO: Starting tracking video ..\image\ped.mp4
  0%|                                                                                                                                                                                                                         | 0/1202 [00:00<?, ?it/s]E
rror: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
  0%|                                                                                                                                                                                                                         | 0/1202 [00:03<?, ?it/s]
Traceback (most recent call last):
  File ""tools/infer_mot.py"", line 146, in <module>
    main()
  File ""tools/infer_mot.py"", line 142, in main
    run(FLAGS, cfg)
  File ""tools/infer_mot.py"", line 99, in run
    tracker.mot_predict_seq(
  File ""D:\python\envs\paddle2\paddledetection\ppdet\engine\tracker.py"", line 573, in mot_predict_seq
    results, nf, ta, tc = self._eval_seq_jde(
  File ""D:\python\envs\paddle2\paddledetection\ppdet\engine\tracker.py"", line 154, in _eval_seq_jde
    pred_dets, pred_embs = self.model(data)
  File ""C:\Users\62845\.conda\envs\paddle2\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""C:\Users\62845\.conda\envs\paddle2\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""D:\python\envs\paddle2\paddledetection\ppdet\modeling\architectures\meta_arch.py"", line 75, in forward
    outs.append(self.get_pred())
  File ""D:\python\envs\paddle2\paddledetection\ppdet\modeling\architectures\fairmot.py"", line 95, in get_pred
    output = self._forward()
  File ""D:\python\envs\paddle2\paddledetection\ppdet\modeling\architectures\fairmot.py"", line 75, in _forward
    det_outs = self.detector(self.inputs)
  File ""C:\Users\62845\.conda\envs\paddle2\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""C:\Users\62845\.conda\envs\paddle2\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""D:\python\envs\paddle2\paddledetection\ppdet\modeling\architectures\meta_arch.py"", line 75, in forward
    outs.append(self.get_pred())
  File ""D:\python\envs\paddle2\paddledetection\ppdet\modeling\architectures\centernet.py"", line 82, in get_pred
    bbox, bbox_inds, topk_clses = self.post_process(
  File ""D:\python\envs\paddle2\paddledetection\ppdet\modeling\post_process.py"", line 409, in __call__
    scores, inds, topk_clses, ys, xs = self._topk(heat)
  File ""D:\python\envs\paddle2\paddledetection\ppdet\modeling\layers.py"", line 829, in _topk
    topk_xs = topk_inds % width
  File ""C:\Users\62845\.conda\envs\paddle2\lib\site-packages\paddle\fluid\dygraph\math_op_patch.py"", line 299, in __impl__
    return math_op(self, other_var, 'axis', axis)
OSError: (External) CUDA error(719), unspecified launch failure.
  [Hint: 'cudaErrorLaunchFailure'. An exception occurred on the device while executing a kernel. Common causes include dereferencing an invalid device pointerand accessing out of bounds shared memory. Less common cases can be system specific - more
 information about these cases canbe found in the system specific user guide. This leaves the process in an inconsistent state and any further CUDA work willreturn the same error. To continue using CUDA, the process must be terminated and relaunche
d.] (at ..\paddle\phi\backends\gpu\gpu_context.cc:435)
  [operator < elementwise_mod > error]

"
ppyoloe_plus: the images are not normalized during network training ?,PaddlePaddle/PaddleDetection,2022-09-18 06:50:41,1,,6965,1376964624,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

When inferring, I used the following code,but there is no results on the image

    def preprocess(self, srcimg):
        img = cv2.cvtColor(srcimg, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, self.input_size, interpolation=cv2.INTER_LINEAR)
        img = img.astype(np.float32)
        img = img / 255.
        img -= self.mean_[None, None, :]
        img /= self.std_[None, None, :]
        img = np.transpose(img, [2, 0, 1])

        return img

 However, when I remove the code below, the result is normal

        img = img / 255.
        img -= self.mean_[None, None, :]
        img /= self.std_[None, None, :]
"
PaddleDetection是否有计划提供GUI界面，类似于PaddleX的GUI界面,PaddlePaddle/PaddleDetection,2022-09-18 03:23:54,2,,6964,1376932510,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

每次训练需要修改配置 麻烦
每次还要导出对应数据集

如果有PaddleX的GUI界面训练模型
那就方便多了

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
batch size affect to the inference speed,PaddlePaddle/PaddleDetection,2022-09-17 20:34:50,0,,6963,1376867658,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

Hi,

I am using : 

`������CUDA_VISIBLE_DEVICES=0 python deploy/python/infer.py --model_dir=output_inference/ppyoloe_crn_l_80e_sliced_visdrone_640_025   --device=GPU  --threshold=0.40 --video_file=/data/Videos/hiv00045.mp4 --cpu_thread=8 --output_dir=output/hiv038 --batch_size=16`

only %30 of the gpu is utilized. , cpu %10 of total cpus/cores

How I can improve the speed ? giving batch doesnt affected the gpu utilization 

Best
"
exported model give FatalError: `Segmentation fault` is detected by the operating system.,PaddlePaddle/PaddleDetection,2022-09-17 16:30:17,0,,6962,1376815807,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Inference

### Bug描述 Describe the Bug

When use the inference without exported model it is working fine with 👍 

`CUDA_VISIBLE_DEVICES=0 python tools/infer.py -c configs/smalldet/ppyoloe_crn_l_80e_sliced_visdrone_640_025.yml -o weights=https://paddledet.bj.bcebos.com/models/ppyoloe_crn_l_80e_sliced_visdrone_640_025.pdparams --infer_img=demo.jpg --draw_threshold=0.25
`

is ok.


but when I exported as referred i the related page : 

`CUDA_VISIBLE_DEVICES=0 python tools/export_model.py -c configs/smalldet/ppyoloe_crn_l_80e_sliced_visdrone_640_025.yml -o weights=https://paddledet.bj.bcebos.com/models/ppyoloe_crn_l_80e_sliced_visdrone_640_025.pdparams
`

then use the exported model : 

`CUDA_VISIBLE_DEVICES=0 python deploy/python/infer.py --model_dir=output_inference/ppyoloe_crn_l_80e_sliced_visdrone_640_025 --image_file=demo.jpg --device=GPU --save_images --threshold=0.25
`

it gives below error

> CUDA_VISIBLE_DEVICES=0 python deploy/python/infer.py --model_dir=output_inference/ppyoloe_crn_l_80e_sliced_visdrone_640_025 --image_file=inImages/250/thumb0002.jpg  --device=GPU  --threshold=0.25
-----------  Running Arguments -----------
action_file: None
batch_size: 1
camera_id: -1
combine_method: nms
cpu_threads: 1
device: GPU
enable_mkldnn: False
enable_mkldnn_bfloat16: False
image_dir: None
image_file: inImages/250/thumb0002.jpg
match_metric: ios
match_threshold: 0.6
model_dir: output_inference/ppyoloe_crn_l_80e_sliced_visdrone_640_025
output_dir: output
overlap_ratio: [0.25, 0.25]
random_pad: False
reid_batch_size: 50
reid_model_dir: None
run_benchmark: False
run_mode: paddle
save_images: True
save_mot_txt_per_img: False
save_mot_txts: False
save_results: False
scaled: False
slice_infer: False
slice_size: [640, 640]
threshold: 0.25
tracker_config: None
trt_calib_mode: False
trt_max_shape: 1280
trt_min_shape: 1
trt_opt_shape: 640
use_coco_category: False
use_dark: True
use_gpu: False
video_file: None
window_size: 50
------------------------------------------
-----------  Model Configuration -----------
Model Arch: YOLO
Transform Order:
--transform op: Resize
--transform op: Permute
--------------------------------------------


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1663431912 (unix time) try ""date -d @1663431912"" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 1933867 (TID 0x7f3fde260740) from PID 0 ***]

Segmentation fault (core dumped)

### 复现环境 Environment

Linux ubuntu
latest 2.3 paddle
NVIDIA-SMI 460.27.04    Driver Version: 460.27.04    CUDA Version: 11.2

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
FPN网络训练时报错,PaddlePaddle/PaddleDetection,2022-09-17 15:35:25,2,,6961,1376803358,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Training

### Bug描述 Describe the Bug

在rcnn包含fpn的网络中训练时报错
```
Traceback (most recent call last):
  File ""tools/train.py"", line 172, in <module>
    main()
  File ""tools/train.py"", line 168, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 132, in run
    trainer.train(FLAGS.eval)
  File ""/home/aistudio/PaddleDetection/ppdet/engine/trainer.py"", line 506, in train
    outputs = model(data)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""/home/aistudio/PaddleDetection/ppdet/modeling/architectures/meta_arch.py"", line 59, in forward
    out = self.get_loss()
  File ""/home/aistudio/PaddleDetection/ppdet/modeling/architectures/cascade_rcnn.py"", line 125, in get_loss
    rpn_loss, bbox_loss, mask_loss = self._forward()
  File ""/home/aistudio/PaddleDetection/ppdet/modeling/architectures/cascade_rcnn.py"", line 94, in _forward
    self.inputs)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""/home/aistudio/PaddleDetection/ppdet/modeling/heads/cascade_head.py"", line 252, in forward
    rois_feat = self.roi_extractor(body_feats, rois, rois_num)
  File ""/home/aistudio/PaddleDetection/ppdet/modeling/heads/roi_extractor.py"", line 112, in __call__
    boxes=rois_dist[lvl],
UnboundLocalError: local variable 'rois_dist' referenced before assignment
```

### 复现环境 Environment

PaddlePaddle 2.3.2
python: 3.7.4
环境为ai studio环境

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
pp-humanv2部署调用摄像头问题,PaddlePaddle/PaddleDetection,2022-09-17 13:15:43,10,,6960,1376770050,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

python deploy/pipeline/pipeline.py --config deploy/pipeline/config/infer_cfg_pphuman.yml --camera_id=0 --device=gpu
显示
![image](https://user-images.githubusercontent.com/78284628/190858865-8f847416-59b3-4cd4-b21c-ce9e9d57e3e0.png)
"
执行 tools/train.py训练提示错误 out of bound ,PaddlePaddle/PaddleDetection,2022-09-17 08:36:47,1,windows,6959,1376709184,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

已经下载了coco的数据集
执行tools/train.py训练提示错误。请问什么原因？
我的电脑gpu是3070显卡 cuda 11.6
![image](https://user-images.githubusercontent.com/55121158/190848152-04f1b552-d258-49ac-a86f-fb775b603369.png)
"
picodet_m_ofa无法复现，验证集精度为0,PaddlePaddle/PaddleDetection,2022-09-17 08:03:51,0,,6958,1376702202,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

_No response_

### Bug描述 Describe the Bug

问题如标题所示，想要使用OFA复现在picodet上的搜索。使用的slim配置就是ofa_picodet_demo.yml，检测配置为：picodet_m_320_coco。其他配置没有任何改动。

复现过程中发现，用最新版本：origin/release/2.5 的paddledetection，发现有诸多问题无法将带有OFA的检测方法跑起来。于是将paddledetection版本换成了：4ab7c7c079b9b4713c39e93e172e9e1475504bd6，也就是当初官方复现的时候的版本。但是发现ofa训练也无法收敛。

现有问题两个：
1. 当初官方复现的OFA对应的PaddleDetection、PaddleSlim、Paddle的版本分别为多少？
2. 复现的效果可否分享一下？以及搜索的subnet？

### 复现环境 Environment

paddle: 2.3.2
paddleDetection:  commit: 4ab7c7c079b9b4713c39e93e172e9e1475504bd6
PaddleSlim-2.3.4

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [x] 我愿意提交PR！I'd like to help by submitting a PR!"
solov2 C++推理结果跟python推理不一致,PaddlePaddle/PaddleDetection,2022-09-17 02:53:51,3,,6957,1376644385,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

环境如下：
PaddleDetection 2.5
win10
CPU推理
通过VS2019编译mkl加速的main.exe之后，通过CPU推理
得到的图片跟python的推理不一致，solov2使用的是solov2_r50_enhance_coco.yml配置 文件进行训练的
以下图片是C++推理的

![test](https://user-images.githubusercontent.com/52707280/190837804-9dc86872-f314-46b5-af20-89d7c1f4a31a.jpg)
以下图片是python推理的


![test](https://user-images.githubusercontent.com/52707280/190837879-eb12b613-fbe8-4b4f-a594-39137682b2b5.jpg)


"
Segmentaion Fault paddlepaddleDetectionDev ,PaddlePaddle/PaddleDetection,2022-09-16 12:57:46,1,,6956,1375915518,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Inference

### Bug描述 Describe the Bug

python3 deploy/pipeline/pipeline.py --config deploy/pipeline/config/infer_cfg_ppvehicle.yml    --video_file=/data/Videos/hiv00038.mp4  --device=gpu
deploy/pipeline/pipeline.py:24: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working
  from collections import Sequence, defaultdict
-----------  Running Arguments -----------
DET:
  batch_size: 1
  model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_ppvehicle.zip
MOT:
  batch_size: 1
  enable: true
  model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_ppvehicle.zip
  skip_frame_num: -1
  tracker_config: deploy/pipeline/config/tracker_config.yml
VEHICLE_ATTR:
  batch_size: 8
  color_threshold: 0.5
  enable: false
  model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/vehicle_attribute_model.zip
  type_threshold: 0.5
VEHICLE_PLATE:
  det_limit_side_len: 736
  det_limit_type: min
  det_model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/ch_PP-OCRv3_det_infer.tar.gz
  enable: false
  rec_batch_num: 6
  rec_image_shape:
  - 3
  - 48
  - 320
  rec_model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/ch_PP-OCRv3_rec_infer.tar.gz
  word_dict_path: deploy/pipeline/ppvehicle/rec_word_dict.txt
crop_thresh: 0.3
visual: true
warmup_frame: 50

------------------------------------------
Multi-Object Tracking enabled
DET  model dir:  /home/alp2080/.cache/paddle/infer_weights/mot_ppyoloe_l_36e_ppvehicle
MOT  model dir:  /home/alp2080/.cache/paddle/infer_weights/mot_ppyoloe_l_36e_ppvehicle
-----------  Model Configuration -----------
Model Arch: YOLO
Transform Order:
--transform op: Resize
--transform op: Permute
--------------------------------------------
video fps: 25, frame_count: 12525
Thread: 0; frame id: 0


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1663332777 (unix time) try ""date -d @1663332777"" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 1553976 (TID 0x7fe07e306740) from PID 0 ***]

Segmentation fault (core dumped)

### 复现环境 Environment

Linux
PaddleDetecton latest dev


docker conatiner  working  ,  but in  linux installed paddledetection its gives seg fault for tracking



### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
"Target(Tensor(shape=[1],dtype=int64,place=CUDAPlace(0),stop_gradient=True,[10])) is out of class_dimension's upper bound(9)",PaddlePaddle/PaddleDetection,2022-09-16 10:43:02,2,,6955,1375767896,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Training

### Bug描述 Describe the Bug

Target(Tensor(shape=[1],dtype=int64,place=CUDAPlace(0),stop_gradient=True,[10])) is out of class_dimension's upper bound(9)

视频目标追踪，单目标，我用了10个视频序列，就出现这样的错误

### 复现环境 Environment

Target(Tensor(shape=[1],dtype=int64,place=CUDAPlace(0),stop_gradient=True,[10])) is out of class_dimension's upper bound(9)

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [X] 我愿意提交PR！I'd like to help by submitting a PR!"
dataset/visdrone_sliced/val_640_025.json' is None or not set,PaddlePaddle/PaddleDetection,2022-09-16 09:14:08,2,,6954,1375666361,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

when run : 

`CUDA_VISIBLE_DEVICES=0 python tools/infer.py -c configs/smalldet/ppyoloe_crn_l_80e_sliced_visdrone_640_025.yml -o weights=https://paddledet.bj.bcebos.com/models/ppyoloe_crn_l_80e_sliced_visdrone_640_025.pdparams --infer_dir=inImages/ --draw_threshold=0.25`

I got warnings and labelling is incorrect: 

`Warning: import ppdet from source directory without installing, run 'python setup.py install' to install ppdet firstly
W0916 12:06:49.653527 1499607 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.2, Runtime API Version: 10.2
W0916 12:06:49.654078 1499607 gpu_resources.cc:91] device: 0, cuDNN Version: 8.3.
[09/16 12:06:53] ppdet.utils.checkpoint INFO: Finish loading model weights: /home/alp2080/.cache/paddle/weights/ppyoloe_crn_l_80e_sliced_visdrone_640_025.pdparams
[09/16 12:06:53] train INFO: Found 8220 inference images in total.
[09/16 12:06:53] ppdet.data.source.category WARNING: anno_file 'dataset/visdrone_sliced/val_640_025.json' is None or not set or not exist, please recheck TrainDataset/EvalDataset/TestDataset.anno_path, otherwise the default categories will be used by metric_type.
[09/16 12:06:53] ppdet.data.source.category WARNING: metric_type: COCO, load default categories of COCO.
  0%|▏                                                                                              | 11/8220 [00:01<13:22, 10.23it/s]`


where can I found this  dataset/visdrone_sliced/val_640_025.json  json file ?


"
"使用yolov7训练报错ValueError: (InvalidArgument) The dtype of Tensor in list must be int32 or int64, but received: 0 (at ..\paddle/fluid/operators/utils.h:85)   [operator < slice > error]",PaddlePaddle/PaddleDetection,2022-09-16 07:50:44,1,,6953,1375563497,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Training

### Bug描述 Describe the Bug

训练自己的数据集是出现```ValueError: (InvalidArgument) The dtype of Tensor in list must be int32 or int64, but received: 0 (at ..\paddle/fluid/operators/utils.h:85)```
根据提示找到代码部分```yolo_loss.py```文件，605行的```this_target = targets[b_idx]```，发现b_idx为布尔类型的Tensor，因此猜想是否这里有问题

### 复现环境 Environment

windows
paddlepaddle:2.2.2
paddlDetection:2.4
python: 3.7.6
CUDA: 10.2
CUDNN: 7.6

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
参数共享的实现,PaddlePaddle/PaddleDetection,2022-09-16 04:46:38,1,,6952,1375403017,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

请问参数共享是否可以这么实现，为什么会报这样的错误？
```
share1=self.create_parameter(
                shape=[c,c//2,1,1],
                attr=ParamAttr(initializer=Constant(value=1.)),
                dtype=""float32"")
        self.conv11 = ConvBNLayer(
            ch_in = c, 
            ch_out = c // 2,
            filter_size=1,
            stride=1,
            padding=0,
            weight_attr=share1)
```
![image](https://user-images.githubusercontent.com/99952437/190558528-ac4ef5eb-535a-4777-ae57-677afc9760e2.png)
"
自定义数据的行为识别教程,PaddlePaddle/PaddleDetection,2022-09-16 03:54:55,1,,6951,1375369409,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

我想对人体骨骼点的行为识别进行二次开发但教程有点迷糊。
比如我有100视频。是要对每个视频分别获取序列的骨骼点坐标，再保存为PaddleVideo可用的文件格式？"
二维码的微信群邀请过期了,PaddlePaddle/PaddleDetection,2022-09-16 03:31:44,0,,6950,1375355512,"### 问题描述 Please describe your issue

需要礼包链接，进不了群"
模型文件无法下载,PaddlePaddle/PaddleDetection,2022-09-15 11:39:17,1,,6948,1374390536,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

https://paddledet.bj.bcebos.com/deploy/third_engine/picodet_s_320_coco_lcnet.onnx"
大佬们帮忙看看，采用visdrone/ppyoloe_crn_l_alpha_largesize_80e_visdrone.yml模型报错，但是ppyoloe/ppyoloe_plus_crn_x_80e_coco.yml 正常 ,PaddlePaddle/PaddleDetection,2022-09-15 07:27:57,2,,6947,1374061530,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Training

### Bug描述 Describe the Bug

**### 大佬们帮忙看看，采用visdrone/ppyoloe_crn_l_alpha_largesize_80e_visdrone.yml模型报错，但是采用ppyoloe/ppyoloe_plus_crn_x_80e_coco.yml 可以正常 运行。我希望检测小目标因此还是希望把visdrone/ppyoloe_crn_l_alpha_largesize_80e_visdrone.yml 模型用起来。**
代码没有修改直接用的是百度的https://aistudio.baidu.com/aistudio/ 下的开发平台
运行!python tools/train.py -c configs/visdrone/ppyoloe_crn_l_alpha_largesize_80e_visdrone.yml --use_vdl=True --vdl_log_dir=./visdrone/ --eval 命令时候报错。  报错如下：
loading annotations into memory...
Done (t=0.20s)
creating index...
index created!
W0911 11:41:50.710125 4053 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0911 11:41:50.714862 4053 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.
[09/11 11:41:53] ppdet.utils.checkpoint INFO: The shape [80] in pretrained weight yolo_head.pred_cls.0.bias is unmatched with the shape [3] in model yolo_head.pred_cls.0.bias. And the weight yolo_head.pred_cls.0.bias will not be loaded
[09/11 11:41:53] ppdet.utils.checkpoint INFO: The shape [80, 768, 3, 3] in pretrained weight yolo_head.pred_cls.0.weight is unmatched with the shape [3, 768, 3, 3] in model yolo_head.pred_cls.0.weight. And the weight yolo_head.pred_cls.0.weight will not be loaded
[09/11 11:41:53] ppdet.utils.checkpoint INFO: The shape [80] in pretrained weight yolo_head.pred_cls.1.bias is unmatched with the shape [3] in model yolo_head.pred_cls.1.bias. And the weight yolo_head.pred_cls.1.bias will not be loaded
[09/11 11:41:53] ppdet.utils.checkpoint INFO: The shape [80, 384, 3, 3] in pretrained weight yolo_head.pred_cls.1.weight is unmatched with the shape [3, 384, 3, 3] in model yolo_head.pred_cls.1.weight. And the weight yolo_head.pred_cls.1.weight will not be loaded
[09/11 11:41:53] ppdet.utils.checkpoint INFO: The shape [80] in pretrained weight yolo_head.pred_cls.2.bias is unmatched with the shape [3] in model yolo_head.pred_cls.2.bias. And the weight yolo_head.pred_cls.2.bias will not be loaded
[09/11 11:41:53] ppdet.utils.checkpoint INFO: The shape [80, 192, 3, 3] in pretrained weight yolo_head.pred_cls.2.weight is unmatched with the shape [3, 192, 3, 3] in model yolo_head.pred_cls.2.weight. And the weight yolo_head.pred_cls.2.weight will not be loaded
[09/11 11:41:53] ppdet.utils.checkpoint INFO: Finish loading model weights: models/ppyoloe_plus_crn_l_80e_coco.pdparams
[09/11 11:41:55] ppdet.engine INFO: Epoch: [0] [ 0/5564] learning_rate: 0.000000 loss: 5.066185 loss_cls: 1.017579 loss_iou: 0.979015 loss_dfl: 3.202135 loss_l1: 6.590621 eta: 4 days, 4:15:48 batch_cost: 2.1624 data_cost: 0.0044 ips: 0.9249 images/s
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion p_in_data[idx] >= 0 && p_in_data[idx] < depth failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [24276], but received [140098075152656].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion p_in_data[idx] >= 0 && p_in_data[idx] < depth failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [24276], but received [140098075152656].



### 复现环境 Environment

项目“工业品表面缺陷检测”共享链接(有效期三天)：https://aistudio.baidu.com/studio/project/partial/verify/4422217/8805b66ed7964a969d3721ed18a0bc2e

操作系统: Linux.
paddlepaddle:2.2.2
paddledetectionc:2.5
python:3.7
cuda:10.2
cudnn:7.6
gcc:8.2

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [X] 我愿意提交PR！I'd like to help by submitting a PR!"
PPYoloE-Plus的预测时间,PaddlePaddle/PaddleDetection,2022-09-15 07:00:14,1,,6946,1374029391,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

命令：CUDA_VISIBLE_DEVICES=0 python python/infer.py --model_dir=ppyoloe_plus_crn_x_80e_coco --image_file=demo/000000014439_640x640.jpg --run_mode=paddle --device=gpu --save_images=True

------------------ Inference Time Info ----------------------
total_time(ms): 1358.8, img_num: 1
average latency time(ms): 1358.80, QPS: 0.735943
preprocess_time(ms): 22.30, inference_time(ms): 1336.40, postprocess_time(ms): 0.10


 inference_time(ms)为1336ms,这时间为什么这么长

显卡为3090"
YOLOX-ConvNeXt-s的推理速度是多少？官网好像没有公布,PaddlePaddle/PaddleDetection,2022-09-15 04:38:47,7,,6944,1373908562,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

YOLOX-ConvNeXt-s的推理速度是多少？官网好像没有公布
![image](https://user-images.githubusercontent.com/78945582/190315785-1c55ad12-181f-47df-942b-0c910f7e454a.png)
"
Linux下用官方给的c++部署教程报错,PaddlePaddle/PaddleDetection,2022-09-15 04:06:40,1,,6943,1373884549,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Deploy

### Bug描述 Describe the Bug

在Linux下，用C++调用pp—tsm模型
按照官方给的教程，执行到最后一步（./build/ppvideo rec \
--rec_model_dir=../../inference/ppTSM \
--inference_model_name=ppTSM \
--video_dir=./example_video_dir \
--num_seg=8 \
--seg_len=1）
的时候抛出了一个错误（terminate called after throwing an instance of char const*）

### 复现环境 Environment

linux
paddlepaddle 2.3.2
python3.9

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [x] 我愿意提交PR！I'd like to help by submitting a PR!"
使用PaddleDetection2.5报错：AttributeError: module 'paddle.vision.ops' has no attribute 'generate_proposals',PaddlePaddle/PaddleDetection,2022-09-15 01:42:03,4,,6940,1373784004,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question


Load load_state_dict....
[09/15 09:15:48] ppdet.utils.checkpoint INFO: Finish loading model weights: /lzl/PaddleDetection-release-2.5/output/cascade_rcnn_vit_base_hrfpn_cae_1x_coco/best_model.pdparams
loading annotations into memory...
Done (t=0.04s)
creating index...
index created!
[09/15 09:15:48] ppdet.engine INFO: Export inference config file to ./output_inference/cascade_rcnn_vit_base_hrfpn_cae_1x_coco/infer_cfg.yml
Traceback (most recent call last):
  File ""tools/export_model.py"", line 108, in <module>
    main()
  File ""tools/export_model.py"", line 104, in main
    run(FLAGS, cfg)
  File ""tools/export_model.py"", line 73, in run
    trainer.export(FLAGS.output_dir)
  File ""/lzl/PaddleDetection-release-2.5/ppdet/engine/trainer.py"", line 1060, in export
    save_dir)
  File ""/lzl/PaddleDetection-release-2.5/ppdet/engine/trainer.py"", line 1019, in _get_infer_cfg_and_input_spec
    input_spec, static_model.forward.main_program,
  File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 563, in main_program
    concrete_program = self.concrete_program
  File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 479, in concrete_program
    return self.concrete_program_specify_input_spec(input_spec=None)
  File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 517, in concrete_program_specify_input_spec
    *desired_input_spec)
  File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 427, in get_concrete_program
    concrete_program, partial_program_layer = self._program_cache[cache_key]
  File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 723, in __getitem__
    self._caches[item] = self._build_once(item)
  File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 714, in _build_once
    **cache_key.kwargs)
  File ""/Envs/lzl/lib/python3.6/site-packages/decorator.py"", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/wrapped_decorator.py"", line 25, in __impl__
    return wrapped_func(*args, **kwargs)
  File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/base.py"", line 51, in __impl__
    return func(*args, **kwargs)
  File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 668, in from_func_spec
    error_data.raise_new_exception()
  File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/dygraph_to_static/error.py"", line 336, in raise_new_exception
    six.exec_(""raise new_exception from None"")
  File ""<string>"", line 1, in <module>
AttributeError: In transformed code:

    File ""/lzl/PaddleDetection-release-2.5/ppdet/modeling/architectures/cascade_rcnn.py"", line 136, in get_pred
        bbox_pred, bbox_num, mask_pred = self._forward()
    File ""/lzl/PaddleDetection-release-2.5/ppdet/modeling/architectures/cascade_rcnn.py"", line 104, in _forward
        rois, rois_num, _ = self.rpn_head(body_feats, self.inputs)
    File ""/lzl/PaddleDetection-release-2.5/ppdet/modeling/proposal_generator/rpn_head.py"", line 141, in forward
        rois, rois_num = self._gen_proposal(scores, deltas, anchors, inputs)
    File ""/lzl/PaddleDetection-release-2.5/ppdet/modeling/proposal_generator/rpn_head.py"", line 198, in _gen_proposal
        # Generate proposals for each level and each batch.
        # Discard batch-computing to avoid sorting bbox cross different batches.
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
        for i in range(batch_size):
            rpn_rois_list = []

    File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_call_func.py"", line 258, in convert_call
        converted_call = convert_to_static(call_func)
    File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 140, in convert_to_static
        static_func = _FUNCTION_CACHE.convert_with_cache(function)
    File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 77, in convert_with_cache
        static_func = self._convert(func)
    File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 115, in _convert
        root_wrapper = self._dygraph_to_static.get_static_ast(root)
    File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/dygraph_to_static/ast_transformer.py"", line 58, in get_static_ast
        self.static_analysis_visitor = StaticAnalysisVisitor(root)
    File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/dygraph_to_static/static_analysis.py"", line 207, in __init__
        self.run(ast_root)
    File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/dygraph_to_static/static_analysis.py"", line 215, in run
        self.dfs_visit(ast_root)
    File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/dygraph_to_static/static_analysis.py"", line 241, in dfs_visit
        func_type = self.dfs_visit(child)
    File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/dygraph_to_static/static_analysis.py"", line 244, in dfs_visit
        self.dfs_visit(child)
    File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/dygraph_to_static/static_analysis.py"", line 244, in dfs_visit
        self.dfs_visit(child)
    File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/dygraph_to_static/static_analysis.py"", line 244, in dfs_visit
        self.dfs_visit(child)
    File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/dygraph_to_static/static_analysis.py"", line 247, in dfs_visit
        cur_wrapper.node_var_type = self._get_node_var_type(cur_wrapper)
    File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/dygraph_to_static/static_analysis.py"", line 359, in _get_node_var_type
        if is_dygraph_api(node):
    File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/dygraph_to_static/utils.py"", line 201, in is_dygraph_api
        if is_api_in_module(node, DYGRAPH_TO_STATIC_MODULE_PREFIX):
    File ""/Envs/lzl/lib/python3.6/site-packages/paddle/fluid/dygraph/dygraph_to_static/utils.py"", line 193, in is_api_in_module
        module_prefix))
    File ""<string>"", line 1, in <module>


    AttributeError: module 'paddle.vision.ops' has no attribute 'generate_proposals'
+++++++++++++++++++++
在使用[PaddleDetection2.5版本的时候 使用cascade_rcnn_vit_base_hrfpn_cae_1x_coco进行训练然后 进行导出模型的时候报错。使用环境paddle2.2.2 python3.6.8   命令使用：python tools/export_model.py -c configs/vitdet/cascade_rcnn_vit_base_hrfpn_cae_1x_coco.yml --output_dir=./output_inference  -o weights=/lzl/PaddleDetection-release-2.5/output/cascade_rcnn_vit_base_hrfpn_cae_1x_coco/best_model.pdparams  请问是什么原因"
vehicle tracking video not worked,PaddlePaddle/PaddleDetection,2022-09-14 15:03:39,5,,6939,1373153314,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

Hi,

I ran :

```
python deploy/pipeline/pipeline.py --config deploy/pipeline/config/infer_cfg_ppvehicle.yml \
                                                   --video_file=myVideo.mp4 \
                                                   --device=gpu
```

without changing anything in infer_cfg_ppvehicle.yml , the resulting video is the same as input video. No boxes and trajectories ?  additionally how I can export Box and related vehichle_id ?

Best

"
Triton部署yolov3推理异常,PaddlePaddle/PaddleDetection,2022-09-13 11:51:38,1,,6932,1371326192,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

将训练的yolov3导出为推理模式，再转换为onnx格式，使用Triton部署。推理时报错：
```
Non-zero status code returned while running Gather node. Name:'p2o.Gather.12' Status Message: /workspace/onnxruntime/onnxruntime/core/providers/common.h:23 int64_t onnxruntime::HandleNegativeAxis(int64_t, int64_t) axis >= -tensor_rank && axis <= tensor_rank - 1 was false. axis 0 is not in valid range [-0,-1]
```
模型输入图像大小为[1, 3, 608, 608], float32数据类型   
如果将图像数据归一化，则报上面的错。   
如果不归一化，直接将原始数据由uint8转换为float32，则可以正常运行推理，但是返回的bboxes的坐标点错误。   

首先，是不是需要归一化？yolov3的mobilenet ssl模型。   
其次，如果不需要归一化，为什么模型返回值和paddlepaddle返回的不相同（后者返回正确），需要后处理吗？   
最后，如果需要归一化，为什么会出现上面运行时错误？   

提前谢谢"
SOLOV2支持裁剪和蒸馏吗？,PaddlePaddle/PaddleDetection,2022-09-13 11:40:25,1,,6931,1371311385,"SOLOV2支持裁剪和蒸馏吗？

如果支持的话，麻烦提供一下参考配置文件"
about deploy/pipeline/pipeline.py use camera_id,PaddlePaddle/PaddleDetection,2022-09-13 10:07:22,2,,6930,1371194774,"python deploy/pipeline/pipeline.py --config deploy/pipeline/config/infer_cfg_pphuman.yml --camera_id=0 --device=gpu  --draw_center_traj --do_break_in_counting --region_type=custom --region_polygon 
200 200 400 200 300 400 100 400

deploy\pipeline\pipeline.py:24: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import Sequence, defaultdict
Warning: Unable to use JDE/FairMOT/ByteTrack, please install lap, for example: `pip install lap`, see https://github.com/gatagat/lap
Warning: Unable to use OC-SORT, please install filterpy, for example: `pip install filterpy`, see https://github.com/rlabbe/filterpy
Warning: Unable to use motmetrics in MTMCT in PP-Tracking, please install motmetrics, for example: `pip install motmetrics`, see https://github.com/longcw/py-motmetrics
Warning: Unable to use JDE/FairMOT/ByteTrack, please install lap, for example: `pip install lap`, see https://github.com/gatagat/lap
Warning: Unable to use OC-SORT, please install filterpy, for example: `pip install filterpy`, see https://github.com/rlabbe/filterpy
Warning: Unable to use motmetrics in MTMCT in PP-Tracking, please install motmetrics, for example: `pip install motmetrics`, see https://github.com/longcw/py-motmetrics
-----------  Running Arguments -----------
ATTR:
  batch_size: 8
  enable: false
  model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/PPLCNet_x1_0_person_attribute_945_infer.zip
DET:
  batch_size: 1
  model_dir: output_inference\mot_ppyoloe_l_36e_pipeline
ID_BASED_CLSACTION:
  batch_size: 8
  display_frames: 80
  enable: false
  model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/PPHGNet_tiny_calling_halfbody.zip
  skip_frame_num: 2
  threshold: 0.8
ID_BASED_DETACTION:
  batch_size: 8
  display_frames: 80
  enable: false
  model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/ppyoloe_crn_s_80e_smoking_visdrone.zip
  skip_frame_num: 2
  threshold: 0.6
KPT:
  batch_size: 8
  model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/dark_hrnet_w32_256x192.zip
MOT:
  batch_size: 1
  enable: true
  model_dir: output_inference\mot_ppyoloe_l_36e_pipeline
  skip_frame_num: -1
  tracker_config: deploy/pipeline/config/tracker_config.yml
REID:
  batch_size: 16
  enable: false
  model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/reid_model.zip
SKELETON_ACTION:
  batch_size: 1
  coord_size:
  - 384
  - 512
  display_frames: 80
  enable: false
  max_frames: 50
  model_dir: https://bj.bcebos.com/v1/paddledet/models/pipeline/STGCN.zip
VIDEO_ACTION:
  batch_size: 1
  enable: false
  frame_len: 8
  model_dir: https://videotag.bj.bcebos.com/PaddleVideo-release2.3/ppTSM_fight.zip
  sample_freq: 7
  short_size: 340
  target_size: 320
attr_thresh: 0.5
crop_thresh: 0.5
kpt_thresh: 0.2
visual: true
warmup_frame: 50

------------------------------------------
Multi-Object Tracking enabled
DET  model dir:  output_inference\mot_ppyoloe_l_36e_pipeline
MOT  model dir:  output_inference\mot_ppyoloe_l_36e_pipeline
KPT  model dir:  .cache/paddle/infer_weights\dark_hrnet_w32_256x192
-----------  Model Configuration -----------
Model Arch: YOLO
Transform Order:
--transform op: Resize
--transform op: Permute
--------------------------------------------
Traceback (most recent call last):
  File ""deploy\pipeline\pipeline.py"", line 1084, in <module>
    main()
  File ""deploy\pipeline\pipeline.py"", line 1069, in main
    pipeline = Pipeline(FLAGS, cfg)
  File ""deploy\pipeline\pipeline.py"", line 88, in __init__
    self.predictor.set_file_name(self.input)
  File ""deploy\pipeline\pipeline.py"", line 473, in set_file_name
    self.file_name = os.path.split(path)[-1]
  File ""anaconda3\envs\paddlehub\lib\ntpath.py"", line 185, in split
    p = os.fspath(p)
TypeError: expected str, bytes or os.PathLike object, not int"
faster_rcnn_swin_tiny_fpn_3x_coco无法导出ONNX,PaddlePaddle/PaddleDetection,2022-09-13 03:20:00,7,,6925,1370793570,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

在以往的Issues中看到了一些类似的问题，可是我还是没有解决。
我的导出流程如下：
1.设置配置文件 faster_rcnn_swin_tiny_fpn_3x_coco.yml
export_onnx: True
batch_size: 1
2.运行paddleDetection的export_model.py
!python tools/export_model.py -c configs/faster_rcnn/faster_rcnn_swin_tiny_fpn_3x_coco.yml \
                             -o weights=output/faster_rcnn_swin_tiny_fpn_3x_coco/model_final.pdparams \
                             --output_dir inference_model
3.安装paddle2onnx
pip install paddle2onnx  版本为1.0.0
4.导出ONNX
!paddle2onnx --model_dir inference_model/faster_rcnn_swin_tiny_fpn_3x_coco \
            --model_filename model.pdmodel \
            --params_filename model.pdiparams \
            --opset_version 16 \
            --enable_dev_version False\
            --enable_onnx_checker True\
            --save_file faster_rcnn.onnx
5.结果报错
There's 7 ops are not supported yet
=========== lod_array_length ===========
=========== conditional_block ===========
=========== select_input ===========
=========== pad ===========
=========== tensor_array_to_tensor ===========
=========== while ===========
=========== write_to_array ===========

按照之前的Issue都是设置export_onnx: True就可以了，不知道我的为何依然缺少ops?
请大家帮帮我，Thanks!
"
YOLOX_nano自有数据集训练，loss_cls波动不收敛，AP几乎为0,PaddlePaddle/PaddleDetection,2022-09-13 01:43:13,3,,6924,1370727553,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Training

### Bug描述 Describe the Bug

我使用yolox-nano训练自有的数据集（COCO格式），是无人机的航拍图像，目标比较小，总共有10类。在配置文件里面修改了base_lr=0.001，batch_size=64，输入尺寸416，单卡训练，关闭了mosaic数据增强（之前使用base_lr=0.01并且开启mosaic就会出现这个问题，所以我尝试调小lr并关闭mosaic，但并没有解决）

出现loss_cls不收敛，且bbox-mAP几乎为0的情况
![8ce69a6e21ef662a99076a2e17bf2ee](https://user-images.githubusercontent.com/52685620/189788579-15a37e3c-aabd-4431-a5f5-790602485db1.jpg)
![61def37e29ab522aeed0713c0d31d1e](https://user-images.githubusercontent.com/52685620/189788596-280508a3-6530-4980-a61b-cb67d63695e9.jpg)
同样的数据集使用picodet模型训练是没有问题的，loss都能正常收敛


### 复现环境 Environment

-OS：Windows
-PaddlePaddle：2.2.2
-PaddleDetection：release/2.5
-Python：3.7
-CUDA：11.2
-CUDNN：8.5

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
关于恢复训练的训练语句问题,PaddlePaddle/PaddleDetection,2022-09-13 01:40:04,2,,6923,1370725952,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

请问下中途中断训练 想要重新训练 是从output里最大的那轮开始还是best_model开始
一开始的训练语句如下：
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python tools/train.py \
	-c configs/picodet/picodet_l_640_coco_lcnet_HiXray.yml \
	-o num_classes=8 LearningRate.base_lr=0.005 \
	--use_vdl=true \
	--vdl_log_dir=./output/vdl_dir/2022.9.12_picodet_l \
	--eval
如果最后训练到40轮，重新训练从40开始吗，那么语句如下对不对：
CUDA_VISIBLE_DEVICES=0,1,2,3 python tools/train.py \
	-c configs/picodet/picodet_s_320_coco_lcnet.yml \
	-r ./output/picodet_s_320_coco_lcnet/40.pdparams
其他参数还需不需要像开始训练时一样加上：
![image](https://user-images.githubusercontent.com/102035987/189788480-2c94ae22-dd06-4b63-aaae-d0f32d036fdb.png)
"
Color classification in human attributes model,PaddlePaddle/PaddleDetection,2022-09-12 07:07:48,2,,6922,1369385104,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

Hi PaddlePaddle team,

My team has been testing multiple solutions form PaddleDetection including body detector, key-points detector and human attributes model. The models are working well, but we would be really interested in color classification attributes as part of the human attributes model to be able to provide information about the color of clothes on the lower body, upper body and possibly head cover of the detected person.
Do you plan adding such a feature to human attributes model or maybe provide this a a separate model in future?
Would it be possible to add this feature in your backlog if it is not there yet?

Also, is there any public roadmap you can share so we can check what features and improvements you will add in  the upcoming releases?

Thank you in advance.
Best regards
Marek

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
有没有基于Paddle的CenterX的实现？,PaddlePaddle/PaddleDetection,2022-09-12 04:55:07,1,,6921,1369279751,"### 问题描述 Please describe your issue

有没有基于Paddle的CenterX的实现？
https://github.com/JDAI-CV/centerX"
ppyoloe_plus_crn_x_80e_coco 训练时候报错，请大佬帮忙看看啊,PaddlePaddle/PaddleDetection,2022-09-11 20:13:44,2,,6920,1369051851,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

**大佬们，帮忙看看
我采用的模型：visdrone/ppyoloe_crn_l_alpha_largesize_80e_visdrone.yml
刚刚开始训练就出现下面错误：**
loading annotations into memory...
Done (t=0.19s)
creating index...
index created!
W0911 16:54:24.747865 8921 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0911 16:54:24.752705 8921 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.
[09/11 16:54:26] ppdet.utils.download INFO: Downloading ppyoloe_crn_l_300e_coco.pdparams from [https://paddledet.bj.bcebos.com/models/ppyoloe_crn_l_300e_coco.pdparams](https://gitee.com/link?target=https%3A%2F%2Fpaddledet.bj.bcebos.com%2Fmodels%2Fppyoloe_crn_l_300e_coco.pdparams)
100%|█████████████████████████████████| 211693/211693 [00:21<00:00, 9961.79KB/s]
[09/11 16:54:49] ppdet.utils.checkpoint INFO: ['yolo_head.anchor_points', 'yolo_head.stride_tensor'] in pretrained weight is not used in the model, and its will not be loaded
[09/11 16:54:49] ppdet.utils.checkpoint INFO: The shape [80] in pretrained weight yolo_head.pred_cls.0.bias is unmatched with the shape [3] in model yolo_head.pred_cls.0.bias. And the weight yolo_head.pred_cls.0.bias will not be loaded
[09/11 16:54:49] ppdet.utils.checkpoint INFO: The shape [80, 768, 3, 3] in pretrained weight yolo_head.pred_cls.0.weight is unmatched with the shape [3, 768, 3, 3] in model yolo_head.pred_cls.0.weight. And the weight yolo_head.pred_cls.0.weight will not be loaded
[09/11 16:54:49] ppdet.utils.checkpoint INFO: The shape [80] in pretrained weight yolo_head.pred_cls.1.bias is unmatched with the shape [3] in model yolo_head.pred_cls.1.bias. And the weight yolo_head.pred_cls.1.bias will not be loaded
[09/11 16:54:49] ppdet.utils.checkpoint INFO: The shape [80, 384, 3, 3] in pretrained weight yolo_head.pred_cls.1.weight is unmatched with the shape [3, 384, 3, 3] in model yolo_head.pred_cls.1.weight. And the weight yolo_head.pred_cls.1.weight will not be loaded
[09/11 16:54:49] ppdet.utils.checkpoint INFO: The shape [80] in pretrained weight yolo_head.pred_cls.2.bias is unmatched with the shape [3] in model yolo_head.pred_cls.2.bias. And the weight yolo_head.pred_cls.2.bias will not be loaded
[09/11 16:54:49] ppdet.utils.checkpoint INFO: The shape [80, 192, 3, 3] in pretrained weight yolo_head.pred_cls.2.weight is unmatched with the shape [3, 192, 3, 3] in model yolo_head.pred_cls.2.weight. And the weight yolo_head.pred_cls.2.weight will not be loaded
[09/11 16:54:49] ppdet.utils.checkpoint INFO: Finish loading model weights: /home/aistudio/.cache/paddle/weights/ppyoloe_crn_l_300e_coco.pdparams
[09/11 16:54:51] ppdet.engine INFO: Epoch: [0] [ 0/5564] learning_rate: 0.000000 loss: 5.018298 loss_cls: 1.015438 loss_iou: 0.968017 loss_dfl: 3.165634 loss_l1: 5.039592 eta: 4 days, 12:22:53 batch_cost: 2.3375 data_cost: 0.0005 ips: 0.8556 images/s
Found inf or nan, current scale is: 1024.0, decrease to: 1024.00.5
Found inf or nan, current scale is: 512.0, decrease to: 512.00.5
Found inf or nan, current scale is: 256.0, decrease to: 256.00.5
Found inf or nan, current scale is: 128.0, decrease to: 128.00.5
Found inf or nan, current scale is: 64.0, decrease to: 64.00.5
Found inf or nan, current scale is: 32.0, decrease to: 32.00.5
Found inf or nan, current scale is: 16.0, decrease to: 16.00.5
Found inf or nan, current scale is: 8.0, decrease to: 8.00.5
Found inf or nan, current scale is: 4.0, decrease to: 4.00.5
Found inf or nan, current scale is: 2.0, decrease to: 2.00.5
Found inf or nan, current scale is: 1.0, decrease to: 1.00.5
Found inf or nan, current scale is: 0.5, decrease to: 0.50.5
Found inf or nan, current scale is: 0.25, decrease to: 0.250.5
Found inf or nan, current scale is: 0.125, decrease to: 0.1250.5
Found inf or nan, current scale is: 0.0625, decrease to: 0.06250.5
Found inf or nan, current scale is: 0.03125, decrease to: 0.031250.5
Found inf or nan, current scale is: 0.015625, decrease to: 0.0156250.5
Found inf or nan, current scale is: 0.0078125, decrease to: 0.00781250.5
Found inf or nan, current scale is: 0.00390625, decrease to: 0.003906250.5
[09/11 16:55:52] ppdet.engine INFO: Epoch: [0] [ 100/5564] learning_rate: 0.000001 loss: 4.645183 loss_cls: 1.564811 loss_iou: 0.728613 loss_dfl: 2.748396 loss_l1: 3.467703 eta: 1 day, 0:28:35 batch_cost: 0.5101 data_cost: 0.0003 ips: 3.9207 images/s
Found inf or nan, current scale is: 0.001953125, decrease to: 0.0019531250.5
Found inf or nan, current scale is: 0.0009765625, decrease to: 0.00097656250.5
Found inf or nan, current scale is: 0.00048828125, decrease to: 0.000488281250.5
Found inf or nan, current scale is: 0.000244140625, decrease to: 0.0002441406250.5
Found inf or nan, current scale is: 0.0001220703125, decrease to: 0.00012207031250.5
Found inf or nan, current scale is: 6.103515625e-05, decrease to: 6.103515625e-050.5
Found inf or nan, current scale is: 3.0517578125e-05, decrease to: 3.0517578125e-050.5
Found inf or nan, current scale is: 1.52587890625e-05, decrease to: 1.52587890625e-050.5
Found inf or nan, current scale is: 7.62939453125e-06, decrease to: 7.62939453125e-060.5
Found inf or nan, current scale is: 3.814697265625e-06, decrease to: 3.814697265625e-060.5
Found inf or nan, current scale is: 1.9073486328125e-06, decrease to: 1.9073486328125e-060.5
Found inf or nan, current scale is: 9.5367431640625e-07, decrease to: 9.5367431640625e-070.5
Found inf or nan, current scale is: 4.76837158203125e-07, decrease to: 4.76837158203125e-070.5
Found inf or nan, current scale is: 2.384185791015625e-07, decrease to: 2.384185791015625e-070.5
Found inf or nan, current scale is: 1.1920928955078125e-07, decrease to: 1.1920928955078125e-070.5
Found inf or nan, current scale is: 5.960464477539063e-08, decrease to: 5.960464477539063e-080.5
Found inf or nan, current scale is: 2.9802322387695312e-08, decrease to: 2.9802322387695312e-080.5
Found inf or nan, current scale is: 1.4901161193847656e-08, decrease to: 1.4901161193847656e-080.5

"
TTA还是不能普遍适用吗？,PaddlePaddle/PaddleDetection,2022-09-10 07:43:02,1,,6917,1368540239,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests."
"Invitation to PaddleDetection team to speak at YOLO VISION event on September 27th, 2022",PaddlePaddle/PaddleDetection,2022-09-09 21:20:28,1,,6916,1368340805,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

Hi PaddleDetection team,

I'm [Glenn Jocher](https://www.linkedin.com/in/glenn-jocher/), author of [YOLOv5](https://github.com/ultralytics/yolov5). I'd like to invite you to speak at the world's first-ever YOLO conference: [YOLO VISION](https://ultralytics.com/yolo-vision). This virtual event takes place on September 27th, 2022 with talks from the world's leading vision AI experts from Google, SenseTime's MMLabs, Meituan YOLOv6, Weight & Biases, Roboflow and of course Ultralytics and YOLOv5 and many others.

I've also extended invites to the YOLOv4 and YOLOv7 team and Megvii's YOLOX team to make this a truly YOLO-centric event. 

Please send us an email at glenn.jocher@ultralytics.com to get started.

Thanks!

<a align=""center"" href=""https://ultralytics.com/yolo-vision"" target=""_blank"">
      <img width=""850"" src=""https://user-images.githubusercontent.com/26833433/187986731-f749af08-6e2b-4e14-aeaa-dec2fa0b0113.jpg""></a>


"
基于Paddle的YOLOX疑问，紧急，多谢,PaddlePaddle/PaddleDetection,2022-09-09 08:47:00,3,,6913,1367496905,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

1  为什么基于Paddle的YOLOX-nano/ YOLOX-tiny/ YOLOX-m/ YOLOX-l/ YOLOX-x都有不同程度的涨点？具体修改了哪些部分？
2  为什么基于Paddle的YOLOX-s却比官方公布的降点0.1%？

https://github.com/PaddlePaddle/PaddleDetection/issues/new?assignees=&labels=type%2Fquestion%2Cstatus%2Fnew-issue&template=4_ask-a-question.yml"
目标检测数据集需要一个类似TrustAI来清洗数据,PaddlePaddle/PaddleDetection,2022-09-09 05:17:55,1,,6910,1367299368,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

1. 任务目标：当前手里有标注质量不是太好的目标检测数据集，当前需要类似Paddle出的TrustAI（https://github.com/PaddlePaddle/TrustAI）这样的工具来清洗数据集; 
2 需求场景：手里有很多数据集，每个数据集有部分标注质量不是很好，想用类似TrustAI这样的工具来清洗; 
3.功能描述：对目标检测数据集进行清洗，类似TrustAI，将不太好的数据删除，或者修正部分标注。


### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
推理的标签顺序问题,PaddlePaddle/PaddleDetection,2022-09-09 03:46:39,2,,6909,1367244881,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

我使用的是PaddleDetection2.4，使用ppyoloe进行训练，我的label_list.txt是`head plate`，每个标签占一行，在我顺利进行训练后，推理的时候，我发现在deploy/python/visualize.py中，在draw_box函数中，我将np_boxes打印出来，
发现clsid=0代表的是plate，clsid=1代表的是head，这与我标签文件完全相反，这个是按照什么顺序规定clsid的啊？"
ppyoloe训练完模型之后，如何个性化推理文件夹内的图片,PaddlePaddle/PaddleDetection,2022-09-08 10:13:45,2,,6905,1365971751,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

如何批量推理文件夹内的图片，想将图片文件夹、指定配置文件等写入py文件，然后把多余的infer.py代码删除，但是经过自己测试，总是会出现各种问题，请问官方大大能整一个不要在命令行指定配置和图片的infer文件嘛（纯小白...）"
paddledet环境下可以执行，使用pyinstaller编译后运行失败,PaddlePaddle/PaddleDetection,2022-09-08 03:49:34,4,,6899,1365484261,"### 问题描述 Please describe your issue

导入的是deploy/python/infer.py

Traceback (most recent call last):
  File ""pedestrain_infer.py"", line 7, in <module>
    from infer import Detector
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""PyInstaller\loader\pyimod02_importers.py"", line 493, in exec_module
  File ""infer.py"", line 25, in <module>
    import paddle
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""PyInstaller\loader\pyimod02_importers.py"", line 493, in exec_module
  File ""paddle\__init__.py"", line 25, in <module>
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""PyInstaller\loader\pyimod02_importers.py"", line 493, in exec_module
  File ""paddle\framework\__init__.py"", line 17, in <module>
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""PyInstaller\loader\pyimod02_importers.py"", line 493, in exec_module
  File ""paddle\framework\random.py"", line 16, in <module>
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""PyInstaller\loader\pyimod02_importers.py"", line 493, in exec_module
  File ""paddle\fluid\__init__.py"", line 45, in <module>
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""PyInstaller\loader\pyimod02_importers.py"", line 493, in exec_module
  File ""paddle\fluid\dataset.py"", line 19, in <module>
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""PyInstaller\loader\pyimod02_importers.py"", line 493, in exec_module
  File ""paddle\utils\__init__.py"", line 28, in <module>
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""PyInstaller\loader\pyimod02_importers.py"", line 493, in exec_module
  File ""paddle\utils\cpp_extension\__init__.py"", line 15, in <module>
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""PyInstaller\loader\pyimod02_importers.py"", line 493, in exec_module
  File ""paddle\utils\cpp_extension\cpp_extension.py"", line 20, in <module>
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""PyInstaller\loader\pyimod02_importers.py"", line 493, in exec_module
  File ""setuptools\__init__.py"", line 8, in <module>
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""PyInstaller\loader\pyimod02_importers.py"", line 493, in exec_module
  File ""_distutils_hack\override.py"", line 1, in <module>
  File ""_distutils_hack\__init__.py"", line 77, in do_override
  File ""_distutils_hack\__init__.py"", line 64, in ensure_local_distutils
AssertionError: C:\Users\xingyili\AppData\Local\Temp\_MEI230122\distutils\core.pyc
[10480] Failed to execute script 'pedestrain_infer' due to unhandled exception!"
PicoDet 无法使用 TensorRT 加速吗?,PaddlePaddle/PaddleDetection,2022-09-08 03:21:20,2,,6895,1365463162,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

PicoDet 用 TensorRT 加速之后推理的结果框都不正确."
picodet推理出现错误,PaddlePaddle/PaddleDetection,2022-09-07 09:24:09,0,,6894,1364363518,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

在使用 deploy 下的部署代码推理 Pico 结构的模型时,下图 object_detector.cc 中的 output_shape_list 并没有图中所示的数据,导致 num_class 是一个非法的值导致 postprocess 出现错误,请问这里该怎么解决?
![image](https://user-images.githubusercontent.com/50402380/188842350-ff0ffd55-ce18-41a9-b2bd-5a24a4de77f1.png)

"
JAVA和安卓如何利用模型来做预测？,PaddlePaddle/PaddleDetection,2022-09-07 08:34:15,0,,6893,1364298473,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

第一个问题：我如何导出一个轻量级的模型可以给没有GPU的服务器和移动终端做预测
第二个问题：JAVA和Android如何去根绝导出的模型来做预测，有参考教程吗？"
windows部署demo，报无法解析的外部符号 Yaml相关,PaddlePaddle/PaddleDetection,2022-09-07 08:26:44,0,,6892,1364289106,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

cuda 11.2 
infrence paddle_inference_avx_mkl_cuda11.2_cudnn8.2_avx_mkl-trt8.0.1.6
opencv3.4.6
trt 8.0.1.6

按照教程，报错如下
```
main.obj : error LNK2019: 无法解析的外部符号 ""__declspec(dllimport) public: __cdecl YAML::InvalidNode::InvalidNode(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)"" (__imp_??0InvalidNode@YAML@@QEAA@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)，函数 ""public: int __cdecl YAML::Node::as<int>(void)const "" (??$as@H@Node@YAML@@QEBAHXZ) 中引用了该符号
2>object_detector.obj : error LNK2001: 无法解析的外部符号 ""__declspec(dllimport) public: __cdecl YAML::InvalidNode::InvalidNode(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)"" (__imp_??0InvalidNode@YAML@@QEAA@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)
2>main.obj : error LNK2019: 无法解析的外部符号 ""__declspec(dllimport) private: void __cdecl YAML::detail::node_data::convert_to_map(class std::shared_ptr<class YAML::detail::memory_holder> const &)"" (__imp_?convert_to_map@node_data@detail@YAML@@AEAAXAEBV?$shared_ptr@Vmemory_holder@detail@YAML@@@std@@@Z)，函数 ""public: class YAML::detail::node & __cdecl YAML::detail::node_data::get<char [4]>(char const (&)[4],class std::shared_ptr<class YAML::detail::memory_holder>)"" (??$get@$$BY03D@node_data@detail@YAML@@QEAAAEAVnode@12@AEAY03$$CBDV?$shared_ptr@Vmemory_holder@detail@YAML@@@std@@@Z) 中引用了该符号
2>object_detector.obj : error LNK2001: 无法解析的外部符号 ""__declspec(dllimport) private: void __cdecl YAML::detail::node_data::convert_to_map(class std::shared_ptr<class YAML::detail::memory_holder> const &)"" (__imp_?convert_to_map@node_data@detail@YAML@@AEAAXAEBV?$shared_ptr@Vmemory_holder@detail@YAML@@@std@@@Z)
2>main.obj : error LNK2019: 无法解析的外部符号 ""__declspec(dllimport) public: static bool __cdecl YAML::convert<bool>::decode(class YAML::Node const &,bool &)"" (__imp_?decode@?$convert@_N@YAML@@SA_NAEBVNode@2@AEA_N@Z)，函数 ""public: bool __cdecl YAML::Node::as<bool>(void)const "" (??$as@_N@Node@YAML@@QEBA_NXZ) 中引用了该符号
2>D:\MyGithub\PaddleDetection\deploy\cpp\build\Release\main.exe : fatal error LNK1120: 3 个无法解析的外部命令
```
"
如何制作一个自己的多目标跟踪的数据集，有详细的文档吗,PaddlePaddle/PaddleDetection,2022-09-07 07:53:04,1,,6891,1364247209,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

如何制作一个自己的多目标跟踪的数据集，有详细的文档吗

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
MTMCT 跨境头追踪：  为啥教程中的demo视频可以预测，换成自己的视频就预测失败呢,PaddlePaddle/PaddleDetection,2022-09-07 06:58:43,6,,6889,1364186759,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

Tracking frame: 2700
Tracking frame: 2710
Tracking frame: 2720
Tracking frame: 2730
[WARNNING] No object detected.
Traceback (most recent call last):
  File ""deploy/pptracking/python/mot_sde_infer.py"", line 850, in <module>
    main()
  File ""deploy/pptracking/python/mot_sde_infer.py"", line 819, in main
    detector.predict_mtmct(FLAGS.mtmct_dir, mtmct_cfg)
  File ""deploy/pptracking/python/mot_sde_infer.py"", line 730, in predict_mtmct
    mot_features_dict = self.predict_image(
  File ""deploy/pptracking/python/mot_sde_infer.py"", line 489, in predict_image
    tracking_outs = self.tracking(det_result)
  File ""deploy/pptracking/python/mot_sde_infer.py"", line 288, in tracking
    online_targets = self.tracker.update(pred_dets, pred_embs)
  File ""/mnt/PaddleDetection/deploy/pptracking/python/mot/tracker/deepsort_tracker.py"", line 104, in update
    for tlwh, score, feat, cls_id in zip(pred_tlwhs, pred_scores,
TypeError: 'NoneType' object is not iterable"
行人分析工具,PaddlePaddle/PaddleDetection,2022-09-07 02:25:21,1,,6882,1364002768,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

请问 pp-human是可以支持多摄像头吗？我应该如何开启多摄像头呢？"
![image](https://user-images.githubusercontent.com/111327813/186877725-dcbf0a5a-7c85-4afe-86ba-85a23f55cb7f.png),PaddlePaddle/PaddleDetection,2022-09-07 00:11:20,3,,6881,1363933198,"![image](https://user-images.githubusercontent.com/111327813/186877725-dcbf0a5a-7c85-4afe-86ba-85a23f55cb7f.png)
跑完隔一段时间显示这个错误，难道是把数据集都处理了一起放进显存？

_Originally posted by @qinlihaoWork in https://github.com/PaddlePaddle/PaddleDetection/issues/6756#issuecomment-1228293595_
遇到同样的问题"
YOLOE+混合精度训练的疑问,PaddlePaddle/PaddleDetection,2022-09-06 14:54:39,1,,6880,1363417326,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

问题1：混合精度和正常多卡训练有什么区别吗
问题2：按照官网示例的混合精度语句，报了如下错误是什么原因
语句（train的bs由默认的8调整为4）：
python -m paddle.distributed.launch --gpus 0,1,2,3,4,5,6,7 tools/train.py \
	-c configs/ppyoloe/ppyoloe_plus_crn_s_80e_coco_HiXray.yml \
	-o num_classes=8 LearningRate.base_lr=0.005 \
	--use_vdl=true \
	--vdl_log_dir=./output/vdl_dir \
	--eval \
	--amp
![image](https://user-images.githubusercontent.com/102035987/188667524-dd7a7b20-831c-4363-8cd5-8a5b86bf8e61.png)
"
x2coco问题,PaddlePaddle/PaddleDetection,2022-09-06 13:06:56,0,,6878,1363259923,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

`https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.5/tools/x2coco.py`

```

python tools/x2coco.py \
                --dataset_type labelme \
                --json_input_dir ./labelme_annos/ \
                --image_input_dir ./labelme_imgs/ \
                --output_dir ./cocome/ \
                --train_proportion 0.8 \
                --val_proportion 0.2 \
                --test_proportion 0.0
```

生成coco格式之后  提示图片文件 不存在

因为文件名 和 json 的 imagePath 不一致

生成coco是以 imagePath  的文件名为标准
并不是以labelme_annos里面的文件名为标准"
有没有可以直接计算ap的方法,PaddlePaddle/PaddleDetection,2022-09-06 10:25:16,1,,6873,1363061565,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

自己的数据集，不是voc也不是coco格式的，eval的时候不能通过dataset的gt_box和gt_class简单的计算ap吗，

### 是否愿意提交PR Are you willing to submit a PR?

- [x] Yes I'd like to help by submitting a PR!"
terminate called after throwing an instance of 'InferenceEngine::GeneralError'   what():  Cannot find blob with name: transpose_1.tmp_0,PaddlePaddle/PaddleDetection,2022-09-06 08:40:07,1,,6872,1362916353,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Inference

### Bug描述 Describe the Bug

tinypose转openvino后无法使用terminate called after throwing an instance of 'InferenceEngine::GeneralError'
  what():  Cannot find blob with name: transpose_1.tmp_0


### 复现环境 Environment

paddleDetection 2.5

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
三个676类目标检测模型的对应配置文件无法下载,PaddlePaddle/PaddleDetection,2022-09-06 03:22:52,0,,6868,1362651406,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

您好，那三个676类目标检测模型对应的配置文件下载地址失效，想问一下在哪里可以获取？"
deploy/lite/include/preprocess_op.h文件中的 “json/json.h” 忘记上传了吗？,PaddlePaddle/PaddleDetection,2022-09-06 02:11:47,0,,6866,1362612877,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

json/json.h 文件你们忘记上传了吗？"
https://paddledet.bj.bcebos.com/deploy/third_engine/tinypose_256_openvino.zip 空文件,PaddlePaddle/PaddleDetection,2022-09-06 01:34:53,2,,6865,1362594508,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有发现相似的bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### Bug组件 Bug Component

Deploy

### Bug描述 Describe the Bug

https://paddledet.bj.bcebos.com/deploy/third_engine/tinypose_256_openvino.zip  空文件

Plateform: Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz x 24(核) Model: [Tinypose256_Openvino]
tinypose256_Openvino是空文件



### 复现环境 Environment

https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.4/deploy/third_engine/demo_openvino_kpts/README.md  

里面无法下载tinypose_256_openvino.zip是空文件

### Bug描述确认 Bug description confirmation

- [X] 我确认已经提供了Bug复现步骤、代码改动说明、以及环境信息，确认问题是可以复现的。I confirm that the bug replication steps, code change instructions, and environment information have been provided, and the problem can be reproduced.


### 是否愿意提交PR？ Are you willing to submit a PR?

- [ ] 我愿意提交PR！I'd like to help by submitting a PR!"
paddledetection中使用TensorRT的问题,PaddlePaddle/PaddleDetection,2022-09-04 14:00:32,9,,6858,1361158847,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

安装了几遍tensorrt和paddle，一直报RuntimeError: (NotFound) Tensor named layer_norm_3.w_0 is not found in TRT engine的错误（cuda10.1，cudnn7.6，python3.7。所有版本都是按照教程的，paddle也是带trt的2.3.1版本）

python deploy/python/infer.py --model_dir=XXX --image_dir=XXX --output_dir=XXX --device=gpu --run_benchmark=True --run_mode=trt_fp32
-----------  Running Arguments -----------
action_file: None
batch_size: 1
camera_id: -1
cpu_threads: 1
device: gpu
enable_mkldnn: False
enable_mkldnn_bfloat16: False
image_dir: demo/
image_file: None
model_dir: output/trt/yolox_l_convnext
output_dir: demo/outputs
random_pad: False
reid_batch_size: 50
reid_model_dir: None
run_benchmark: True
run_mode: trt_fp32
save_images: False
save_mot_txt_per_img: False
save_mot_txts: False
save_results: False
scaled: False
threshold: 0.5
tracker_config: None
trt_calib_mode: False
trt_max_shape: 1280
trt_min_shape: 1
trt_opt_shape: 640
use_dark: True
use_gpu: False
video_file: None
window_size: 50
------------------------------------------
-----------  Model Configuration -----------
Model Arch: YOLOX
Transform Order: 
--transform op: Resize
--transform op: Pad
--transform op: Permute
--------------------------------------------
Traceback (most recent call last):
  File ""deploy/python/infer.py"", line 920, in <module>
    main()
  File ""deploy/python/infer.py"", line 880, in main
    output_dir=FLAGS.output_dir)
  File ""deploy/python/infer.py"", line 136, in __init__
    delete_shuffle_pass=delete_shuffle_pass)
  File ""deploy/python/infer.py"", line 781, in load_predictor
    predictor = create_predictor(config)
RuntimeError: (NotFound) Tensor named layer_norm_3.w_0 is not found in TRT engine
  [Hint: Expected itensor_map_.count(name) == true, but received itensor_map_.count(name):0 != true:1.] (at /paddle/paddle/fluid/inference/tensorrt/engine.cc:391)"
在华为昇腾910环境下做预测任务，报argument 'bboxes' (position 0) must be Tensor 错误。,PaddlePaddle/PaddleDetection,2022-09-04 00:36:38,1,,6856,1360996876,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

用tools下的infer.py 执行预测命令，报以下的错误
报错：
[09/04 00:55:36] ppdet.utils.checkpoint INFO: Finish loading model weights: /home/nfs/appnfs/sgf/model/20220903-232033-0x12fc/v1/ckpt/model_final.pdparams

[09/04 00:55:36] train INFO: Found 4 inference images in total.

0%| | 0/4 [00:00<?, ?it/s] 0%| | 0/4 [03:30<?, ?it/s]

Traceback (most recent call last):

File ""/home/nfs/appnfs/sgf/code/702/paddle_back/PaddleDetection/tools/infer.py"", line 255, in <module>

main()

File ""/home/nfs/appnfs/sgf/code/702/paddle_back/PaddleDetection/tools/infer.py"", line 251, in main

run(FLAGS, cfg)

File ""/home/nfs/appnfs/sgf/code/702/paddle_back/PaddleDetection/tools/infer.py"", line 194, in run

visualize=FLAGS.visualize)

File ""/home/nfs/appnfs/sgf/code/702/paddle_back/PaddleDetection/ppdet/engine/trainer.py"", line 877, in predict

outs = self.model(data)

File ""/opt/conda/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 950, in __call__

return self._dygraph_call_func(*inputs, **kwargs)

File ""/opt/conda/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 935, in _dygraph_call_func

outputs = self.forward(*inputs, **kwargs)

File ""/home/nfs/appnfs/sgf/code/702/paddle_back/PaddleDetection/ppdet/modeling/architectures/meta_arch.py"", line 75, in forward

outs.append(self.get_pred())

File ""/home/nfs/appnfs/sgf/code/702/paddle_back/PaddleDetection/ppdet/modeling/architectures/yolo.py"", line 127, in get_pred

return self._forward()

File ""/home/nfs/appnfs/sgf/code/702/paddle_back/PaddleDetection/ppdet/modeling/architectures/yolo.py"", line 115, in _forward

self.inputs['im_shape'], self.inputs['scale_factor'])

File ""/home/nfs/appnfs/sgf/code/702/paddle_back/PaddleDetection/ppdet/modeling/post_process.py"", line 71, in __call__

bbox_pred, bbox_num, _ = self.nms(bboxes, score, self.num_classes)

File ""/home/nfs/appnfs/sgf/code/702/paddle_back/PaddleDetection/ppdet/modeling/layers.py"", line 564, in __call__

return ops.multiclass_nms(bboxes, score, **kwargs)

File ""/home/nfs/appnfs/sgf/code/702/paddle_back/PaddleDetection/ppdet/modeling/ops.py"", line 500, in multiclass_nms

rois_num, *attrs)

ValueError:

--------------------------------------

C++ Traceback (most recent call last):

--------------------------------------

0 phi::enforce::EnforceNotMet::EnforceNotMet(phi::ErrorSummary const&, char const*, int)

1 phi::enforce::GetCurrentTraceBackString(bool)

----------------------

Error Message Summary:

----------------------

InvalidArgumentError: multiclass_nms3(): argument 'bboxes' (position 0) must be Tensor, but got Tensor (at /home/nfs/appnfs/sgf/code/702/paddle/Paddle/paddle/fluid/pybind/eager_utils.cc:891)

请问这是什么原因造成的？该如何解决？感谢！

"
ImportError: cannot import name '_legacy_C_ops' from 'paddle' (/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/__init__.py),PaddlePaddle/PaddleDetection,2022-09-03 08:19:50,4,,6855,1360795004,"### 问题描述 Please describe your issue

!python tools/train.py -c configs/ppyoloe/ppyoloe_plus_crn_x_80e_coco.yml --use_vdl=True --vdl_log_dir=./ppyoloe-p-X/ --eval --amp

Traceback (most recent call last):
  File ""tools/train.py"", line 32, in <module>
    from ppdet.core.workspace import load_config, merge_config
  File ""/home/aistudio/PaddleDetection/ppdet/__init__.py"", line 15, in <module>
    from . import (core, data, engine, modeling, model_zoo, optimizer, metrics,
  File ""/home/aistudio/PaddleDetection/ppdet/data/__init__.py"", line 16, in <module>
    from . import transform
  File ""/home/aistudio/PaddleDetection/ppdet/data/transform/__init__.py"", line 15, in <module>
    from . import operators
  File ""/home/aistudio/PaddleDetection/ppdet/data/transform/operators.py"", line 53, in <module>
    from ppdet.modeling.keypoint_utils import get_affine_transform, affine_transform
  File ""/home/aistudio/PaddleDetection/ppdet/modeling/__init__.py"", line 19, in <module>
    from . import ops
  File ""/home/aistudio/PaddleDetection/ppdet/modeling/ops.py"", line 20, in <module>
    from paddle import _C_ops, _legacy_C_ops
ImportError: cannot import name '_legacy_C_ops' from 'paddle' (/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/__init__.py)"
预测如何才能传入图片对象信息或者视频流信息而不是路径？,PaddlePaddle/PaddleDetection,2022-09-03 04:25:23,2,,6852,1360752212,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

![image](https://user-images.githubusercontent.com/8224370/188255697-fa0deb54-ceea-4d48-93f2-bec4ed4edfa9.png)
![image](https://user-images.githubusercontent.com/8224370/188255704-17f41562-6b5c-4fde-a148-f70c46b61808.png)
这里的image是图片路径，我想传入程序的内存截图image该怎么做呢？还有如果我要接入视频摄像头的视频流改怎么做呢？"
找不到 ppyolo.yml,PaddlePaddle/PaddleDetection,2022-09-03 02:27:50,1,,6851,1360729345,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

python tools/infer.py -c configs/ppyolo/ppyolo.yml -o use_gpu=true weights=https://paddlemodels.bj.bcebos.com/object_detection/ppyolo.pdparams --infer_img=demo/000000014439.jpg

测试时找不到ppyolo.yml文件"
带量化信息的onnx模型missing！,PaddlePaddle/PaddleDetection,2022-09-02 10:53:00,0,,6847,1360028811,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

hi，
    请问下paddle是否支持将带int8量化信息的onnx模型转化为paddle detection支持的模型，进行finetue或者预测呢？
    什么时候会支持呢？是否可以提供一些其他解决方法，多谢
BR 

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
支持obj365训练测试,PaddlePaddle/PaddleDetection,2022-09-02 09:35:17,1,,6846,1359947081,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

看到ppyoloeplus用了基于obj365的预训练模型，是否能支持直接训练obj365？

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
paddledetection推理报错,PaddlePaddle/PaddleDetection,2022-09-02 07:34:30,4,,6842,1359817637,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

用paddledetection预测图片时报错
链接：https://aistudio.baidu.com/aistudio/projectdetail/4483911"
PicoDet 推理报错,PaddlePaddle/PaddleDetection,2022-09-02 01:16:50,3,,6840,1359591422,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

我在推理 PicoDet 结构的模型时遇到下面的问题,请问该如何解决?
![image](https://user-images.githubusercontent.com/50402380/188038353-f919f78d-bb3e-48a8-a6ad-f1a0097e3425.png)
"
PPOCRLabel四点标注的数据能用于s2anet网络训练吗,PaddlePaddle/PaddleDetection,2022-09-01 08:50:22,2,,6837,1358484412,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

PPOCRLabel四点标注的数据能用于s2anet网络训练吗"
maskRCNN量化导出的模型C++部署报错,PaddlePaddle/PaddleDetection,2022-09-01 05:05:27,0,,6833,1358255985,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

WARNING: Logging before InitGoogleLogging() is written to STDERR
E0901 13:04:19.000319  6368 paddle_pass_builder.cc:221] GPU not support MKLDNN yet
E0901 13:04:19.001315  6368 paddle_pass_builder.cc:221] GPU not support MKLDNN yet
E0901 13:04:19.881146  6368 paddle_pass_builder.cc:221] GPU not support MKLDNN yet
I0901 13:04:19.967123  6368 analysis_predictor.cc:964] MKLDNN is enabled
e[1me[35m--- Running analysis [ir_graph_build_pass]e[0m
e[1me[35m--- Running analysis [ir_graph_clean_pass]e[0m
e[1me[35m--- Running analysis [ir_analysis_pass]e[0m
e[32m--- Running IR pass [cudnn_placement_pass]e[0m
e[32m--- Running IR pass [is_test_pass]e[0m
e[32m--- Running IR pass [simplify_with_basic_ops_pass]e[0m
e[32m--- Running IR pass [conv_bn_fuse_pass]e[0m
e[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass]e[0m
e[32m--- Running IR pass [embedding_eltwise_layernorm_fuse_pass]e[0m
e[32m--- Running IR pass [multihead_matmul_fuse_pass_v2]e[0m
e[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass]e[0m
e[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass]e[0m
e[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass]e[0m
e[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass]e[0m
I0901 13:04:20.276324  6368 fuse_pass_base.cc:57] ---  detected 4 subgraphs
e[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass]e[0m
e[32m--- Running IR pass [matmul_scale_fuse_pass]e[0m
e[32m--- Running IR pass [multihead_matmul_fuse_pass_v3]e[0m
e[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass]e[0m
e[32m--- Running IR pass [fc_fuse_pass]e[0m
I0901 13:04:20.333146  6368 fuse_pass_base.cc:57] ---  detected 4 subgraphs
e[32m--- Running IR pass [fc_elementwise_layernorm_fuse_pass]e[0m
e[32m--- Running IR pass [conv_elementwise_add_act_fuse_pass]e[0m
e[32m--- Running IR pass [conv_elementwise_add2_act_fuse_pass]e[0m
e[32m--- Running IR pass [conv_elementwise_add_fuse_pass]e[0m
I0901 13:04:20.404953  6368 fuse_pass_base.cc:57] ---  detected 18 subgraphs
e[32m--- Running IR pass [transpose_flatten_concat_fuse_pass]e[0m
e[32m--- Running IR pass [runtime_context_cache_pass]e[0m
e[1me[35m--- Running analysis [ir_params_sync_among_devices_pass]e[0m
I0901 13:04:20.441882  6368 ir_params_sync_among_devices_pass.cc:100] Sync params from CPU to GPU
e[1me[35m--- Running analysis [adjust_cudnn_workspace_size_pass]e[0m
e[1me[35m--- Running analysis [inference_op_replace_pass]e[0m
e[1me[35m--- Running analysis [ir_graph_to_program_pass]e[0m
I0901 13:04:20.922325  6368 analysis_predictor.cc:1035] ======= optimize end =======
I0901 13:04:20.922785  6368 naive_executor.cc:102] ---  skip [feed], feed -> scale_factor
I0901 13:04:20.924187  6368 naive_executor.cc:102] ---  skip [feed], feed -> image
I0901 13:04:20.924187  6368 naive_executor.cc:102] ---  skip [feed], feed -> im_shape
I0901 13:04:20.936185  6368 naive_executor.cc:102] ---  skip [concat_13.tmp_0], fetch -> fetch
I0901 13:04:20.937153  6368 naive_executor.cc:102] ---  skip [concat_9.tmp_0], fetch -> fetch
I0901 13:04:20.937153  6368 naive_executor.cc:102] ---  skip [tmp_138], fetch -> fetch
data\22.bmp
input_data size:1228800
bs_im_shape size:2
bs_scale_factor size:2
bs_ori_size size:1
W0901 13:04:21.404117  6368 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 11.6, Runtime API Version: 10.1
W0901 13:04:21.405118  6368 gpu_resources.cc:91] device: 0, cuDNN Version: 7.5.
W0901 13:04:21.407136  6368 gpu_resources.cc:201] WARNING: device: . The installed Paddle is compiled with CUDNN 7.6, but CUDNN version in your machine is 7.5, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
Not support stack backtrace yet.

----------------------
Error Message Summary:
----------------------
InvalidArgumentError: The tensor Input (Input) of Slice op is not initialized.
  [Hint: Expected in_tensor.IsInitialized() == true, but received in_tensor.IsInitialized():0 != true:1.] (at ..\paddle/fluid/operators/slice_op.cc:137)

### 复现环境 Environment

_No response_

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
paddledetection是否支持单卡多进程推理？,PaddlePaddle/PaddleDetection,2022-09-01 02:21:49,3,,6830,1358154729,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

如题，手里只有一张卡，我想使用多进程并行推理，目前paddledetection是否支持？"
测试MTMCT-demo中打开draw_center_traj无法绘制轨迹,PaddlePaddle/PaddleDetection,2022-08-31 12:50:16,6,,6827,1357291237,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

**PP团队大神您好，我在测试官方MTMCT-demo中，打开draw_center_traj，但生成的视频中没有绘制轨迹：**

--model_dir=F:\PaddleDetection\output_inference\ppyolov2_r50vd_dcn_365e_aic21mtmct_vehicle
--reid_model_dir=F:\PaddleDetection\output_inference\deepsort_pplcnet_vehicle
--mtmct_dir=F:\PaddleDetection\output_inference\mtmct-demo
--mtmct_cfg=F:\PaddleDetection\deploy\pptracking\python\mtmct_cfg.yml
--tracker_config=F:\PaddleDetection\deploy\pptracking\python\tracker_config.yml
--device=GPU
--scaled=True
--save_mot_txts
--save_images
--draw_center_traj

这是我的运行形参，求解，谢谢！"
请问版面分析中picohead的ScaleReg相关代码在哪里,PaddlePaddle/PaddleDetection,2022-08-31 12:49:39,2,,6826,1357290512,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

在观察[模型](https://paddleocr.bj.bcebos.com/ppstructure/models/layout/picodet_lcnet_x1_0_fgd_layout_cdla.pdparams) 结构的时候发现picohead中有一个
![image](https://user-images.githubusercontent.com/32239722/187681819-685e03b1-ee20-46a5-a29e-7bb3cb2b50a3.png)
head.p3_feat.scale_reg的操作，可能是计算bbox的回归值，但是根据yml查看到picohead中没有相关操作，请问在哪里可以看到呢
"
关于PP-PicoDet的一个问题,PaddlePaddle/PaddleDetection,2022-08-31 10:58:03,1,,6824,1357165236,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

在picodet的[论文](https://arxiv.org/pdf/2111.00902.pdf)中，提到“Channel shuffle provides the information exchange of ShuffleNetV2 channels, but it causes the loss of fusion features.”
![image](https://user-images.githubusercontent.com/43233772/187662610-0537ee2f-e5e8-4d14-8921-97ddbde145a4.png)
我对这个“loss of fusion features”不是很理解，首先“fusion features”的定义是什么，然后怎么证明它loss了，有人能解释得更清楚一点吗
"
新需求：在一个paddledetection目录下，同时运行多个训练程序,PaddlePaddle/PaddleDetection,2022-08-31 02:19:19,3,,6814,1356652610,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

1. 任务目标：; 
2. 需求场景：有时不需要多卡并行去跑一个训练程序，而是希望一张卡跑1个训练程序;
3. 功能描述：在一个paddledetection目录下，同时运行多个训练程序，我试了一下，好像打印的精度指标会混起来

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
"如何获取TP, FP, FN, TN数值",PaddlePaddle/PaddleDetection,2022-08-30 08:02:26,1,,6805,1355342035,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

你好，想请问该如何获取TP, FP, FN, TN这些内容吗？谢谢"
实例分割算法的集成,PaddlePaddle/PaddleDetection,2022-08-30 05:50:38,1,,6801,1355205691,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

1. 任务目标（请描述你正在做的项目是什么，如模型、论文、项目是什么？）; 2. 需求场景（请描述你的项目中为什么需要用此功能）; 3. 功能描述（请简单描述或设计这个功能）

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
关于solov2的训练target_size问题,PaddlePaddle/PaddleDetection,2022-08-30 02:28:32,0,,6793,1355080458,请问solov2_light_reader中的randomresize的target_size是怎么得来的，怎么得到适合自己数据集的target_size？
"训练过程中出现报错OSError: (External) CUDA error(999), unknown error",PaddlePaddle/PaddleDetection,2022-08-30 01:17:19,2,windows,6792,1355040665,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

环境：
paddlepaddle-gpu              2.3.1
paddledet                     2.4.0

显卡：GTX 1070 8G
CUDA     10.2
cudnn     7.6.5

数据集：自定义数据集，COCO格式，图片格式为JPG

代码如下：
```
(python38) E:\AI_projecs\pac_identification\V2>python tools/train.py -c configs/ppyolo/ppyolov2_r101vd_dcn_365e_coco.yml
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
W0829 17:29:59.399045 15216 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 10.2, Runtime API Version: 10.2
W0829 17:29:59.439047 15216 gpu_resources.cc:91] device: 0, cuDNN Version: 7.6.
[08/29 17:30:03] ppdet.utils.checkpoint INFO: Finish loading model weights: C:\Users\yhy/.cache/paddle/weights\ResNet101_vd_ssld_pretrained.pdparams
[08/29 17:30:06] ppdet.engine INFO: Epoch: [0] [ 0/32] learning_rate: 0.000000 loss_xy: 2.398562 loss_wh: 13.457909 loss_iou: 9.439013 loss_iou_aware: 1.867091 loss_obj: 21850.132812 loss_cls: 5.447876 loss: 21882.742188 eta: 4:48:34 batch_cost: 3.6072 data_cost: 0.0000 ips: 0.5544 images/s
[08/29 17:30:27] ppdet.engine INFO: Epoch: [0] [10/32] learning_rate: 0.000008 loss_xy: 3.053678 loss_wh: 11.517223 loss_iou: 12.724132 loss_iou_aware: 2.116462 loss_obj: 16094.050781 loss_cls: 7.076260 loss: 16138.867188 eta: 2:02:04 batch_cost: 1.3213 data_cost: 0.0002 ips: 1.5137 images/s
[08/29 17:30:48] ppdet.engine INFO: Epoch: [0] [20/32] learning_rate: 0.000015 loss_xy: 2.776784 loss_wh: 9.722041 loss_iou: 10.544098 loss_iou_aware: 2.225593 loss_obj: 11327.802734 loss_cls: 7.315646 loss: 11361.541992 eta: 1:56:54 batch_cost: 1.3996 data_cost: 0.0003 ips: 1.4290 images/s
[08/29 17:31:07] ppdet.engine INFO: Epoch: [0] [30/32] learning_rate: 0.000023 loss_xy: 3.456446 loss_wh: 11.567091 loss_iou: 13.852726 loss_iou_aware: 2.336302 loss_obj: 16976.078125 loss_cls: 8.916719 loss: 17018.533203 eta: 1:50:30 batch_cost: 1.2274 data_cost: 0.0000 ips: 1.6295 images/s
[08/29 17:31:12] ppdet.engine INFO: Epoch: [1] [ 0/32] learning_rate: 0.000025 loss_xy: 3.210438 loss_wh: 10.256889 loss_iou: 12.561707 loss_iou_aware: 1.914630 loss_obj: 19656.015625 loss_cls: 8.410213 loss: 19692.189453 eta: 1:52:24 batch_cost: 1.3358 data_cost: 0.1189 ips: 1.4973 images/s
[08/29 17:31:32] ppdet.engine INFO: Epoch: [1] [10/32] learning_rate: 0.000032 loss_xy: 3.692497 loss_wh: 11.925137 loss_iou: 10.633288 loss_iou_aware: 1.640993 loss_obj: 12537.815430 loss_cls: 7.130577 loss: 12561.986328 eta: 1:49:52 batch_cost: 1.2893 data_cost: 0.0001 ips: 1.5513 images/s
Traceback (most recent call last):
  File ""tools/train.py"", line 177, in <module>
    main()
  File ""tools/train.py"", line 173, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 127, in run
    trainer.train(FLAGS.eval)
  File ""D:\Anaconda3\envs\python38\lib\site-packages\paddledet-2.4.0-py3.8.egg\ppdet\engine\trainer.py"", line 458, in train
    self.optimizer.step()
  File ""D:\Anaconda3\envs\python38\lib\site-packages\decorator.py"", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File ""D:\Anaconda3\envs\python38\lib\site-packages\paddle\fluid\dygraph\base.py"", line 299, in __impl__
    return func(*args, **kwargs)
  File ""D:\Anaconda3\envs\python38\lib\site-packages\decorator.py"", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File ""D:\Anaconda3\envs\python38\lib\site-packages\paddle\fluid\wrapped_decorator.py"", line 25, in __impl__
    return wrapped_func(*args, **kwargs)
  File ""D:\Anaconda3\envs\python38\lib\site-packages\paddle\fluid\framework.py"", line 434, in __impl__
    return func(*args, **kwargs)
  File ""D:\Anaconda3\envs\python38\lib\site-packages\paddle\optimizer\optimizer.py"", line 1219, in step
    self._apply_optimize(
  File ""D:\Anaconda3\envs\python38\lib\site-packages\paddle\optimizer\optimizer.py"", line 952, in _apply_optimize
    params_grads = self._grad_clip(params_grads)
  File ""D:\Anaconda3\envs\python38\lib\site-packages\paddle\fluid\clip.py"", line 193, in __call__
    return self._dygraph_clip(params_grads)
  File ""D:\Anaconda3\envs\python38\lib\site-packages\decorator.py"", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File ""D:\Anaconda3\envs\python38\lib\site-packages\paddle\fluid\dygraph\base.py"", line 299, in __impl__
    return func(*args, **kwargs)
  File ""D:\Anaconda3\envs\python38\lib\site-packages\paddle\fluid\clip.py"", line 500, in _dygraph_clip
    sum_square = _squared_l2_norm(merge_grad)
  File ""D:\Anaconda3\envs\python38\lib\site-packages\paddle\fluid\clip.py"", line 80, in _squared_l2_norm
    return _C_ops.squared_l2_norm(x)
OSError: (External) CUDA error(999), unknown error.
  [Hint: 'cudaErrorUnknown'. This indicates that an unknown internal error has occurred. ] (at ../paddle/fluid/operators/math/squared_l2_norm.h:77)
  [operator < squared_l2_norm > error]
```"
网络结构代码提问,PaddlePaddle/PaddleDetection,2022-08-29 09:41:11,0,,6789,1353994756,"麻烦问一下，这个代码导出来的网络为啥长这个样子呀，reshape和constant是哪里来的...

```
class DILATED(nn.Layer):
    def __init__(self, c, act='silu'):
        super(DIlATED, self).__init__()
        self.conv1 = nn.Conv2D(
            c,c//4,
            kernel_size=1,
            stride=1,
            padding=0)
        self.dilatedblocks = nn.LayerList( [
            nn.Conv2D(
                c//4,c//4,
                kernel_size=3,
                dilation=i+1,
                stride=1,
                padding=i+1) for i in range(3)])  
        self.conv2 = nn.Conv2D(
            c//4,c,
            kernel_size=1,
            stride=1,
            padding=0)
        self.conv3 = nn.AdaptiveAvgPool2D(output_size=40)

    def forward(self, x):
        out = []
        for layer in self.dilatedblocks:
            mid_layer = self.conv1(x)
            mid_layer = layer(mid_layer)
            mid_layer = self.conv2(mid_layer)
            mid_layer = mid_layer + x
            out.append(mid_layer)
        y = x + out[0] + out[1] + out[2]
        return self.conv3(y)
```


![%{HLVAT$)93GF1XBRD1GRYV](https://user-images.githubusercontent.com/99952437/187172083-29206419-7ffa-4064-9dd7-81fd4327be90.png)
"
ppyoloe自定义数据集模型敏感分析报错：ImportError: cannot import name 'create_reader' from 'ppdet.data.reader' (/home/hr/anaconda3/lib/python3.9/site-packages/paddledet-2.4.0-py3.9.egg/ppdet/data/reader.py,PaddlePaddle/PaddleDetection,2022-08-29 09:23:53,0,,6788,1353972083,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

使用自定义数据集训练完成模型后，想对模型进行裁剪，按照文档：https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/static/slim/sensitive 进行敏感度分析时报错：

```
(base) [hr@huabei-bj-d-gpu404tmp sensitive]$ python3 sensitive.py -c /home/hr/projects/ppdetection/baidu/personal-code/ppdetection/configs/ppyoloe/ppyoloe_crn_l_300e_voc_hr.yml --print_params
INFO 2022-08-29 16:42:03,812 utils.py:147] Note: NumExpr detected 14 cores but ""NUMEXPR_MAX_THREADS"" not set, so enforcing safe limit of 8.
Traceback (most recent call last):
  File ""/home/hr/projects/ppdetection/baidu/personal-code/ppdetection/static/slim/sensitive/sensitive.py"", line 30, in <module>
    from ppdet.data.reader import create_reader
ImportError: cannot import name 'create_reader' from 'ppdet.data.reader' (/home/hr/anaconda3/lib/python3.9/site-packages/paddledet-2.4.0-py3.9.egg/ppdet/data/reader.py)
```

根据issue：https://github.com/PaddlePaddle/PaddleDetection/issues/5572 卸载了ppdet，依然报错：
```
(ppdetection) [hr@huabei-bj-d-gpu404tmp sensitive]$ python3 sensitive.py -c /home/hr/projects/ppdetection/baidu/personal-code/ppdetection/configs/ppyoloe/ppyoloe_crn_l_300e_voc_hr.yml --print_params
/home/hr/anaconda3/envs/ppdetection/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn(""Setuptools is replacing distutils."")
FLAGS.config: /home/hr/projects/ppdetection/baidu/personal-code/ppdetection/configs/ppyoloe/ppyoloe_crn_l_300e_voc_hr_slim.yml
Traceback (most recent call last):
  File ""/home/hr/projects/ppdetection/baidu/personal-code/ppdetection/static/slim/sensitive/sensitive.py"", line 203, in <module>
    main()
  File ""/home/hr/projects/ppdetection/baidu/personal-code/ppdetection/static/slim/sensitive/sensitive.py"", line 49, in main
    check_config(cfg)
  File ""/home/hr/projects/ppdetection/baidu/personal-code/ppdetection/static/ppdet/utils/check.py"", line 141, in check_config
    train_dataset = cfg['TrainReader']['dataset']
KeyError: 'dataset'
```

其中配置文件的内容如下：
ppyoloe_crn_l_300e_voc_hr.yml：
```
_BASE_: [
  './dataset/zhang_voc_hr.yml',
  '../runtime.yml',
  './_base_/optimizer_300e.yml',
  './_base_/ppyoloe_crn.yml',
  './_base_/ppyoloe_reader.yml',
]

log_iter: 100
snapshot_epoch: 10
weights: output/ppyoloe_crn_l_300e_voc_hr/model_final

pretrain_weights: https://paddledet.bj.bcebos.com/models/pretrained/CSPResNetb_l_pretrained.pdparams
depth_mult: 1.0
width_mult: 1.0
```

'./dataset/zhang_voc_hr.yml'
```
metric: VOC
map_type: 11point
num_classes: 1

TrainDataset:
  !VOCDataSet
    dataset_dir: /home/hr/projects/ppdetection/baidu/personal-code/ppdetection/configs/ppyoloe/dataset/voc
    anno_path: /home/hr/projects/ppdetection/baidu/personal-code/ppdetection/configs/ppyoloe/dataset/voc/trainval.txt
    label_list: /home/hr/projects/ppdetection/baidu/personal-code/ppdetection/configs/ppyoloe/dataset/voc/label_list.txt
    data_fields: ['image', 'gt_bbox', 'gt_class', 'difficult']


EvalDataset:
  !VOCDataSet
    dataset_dir: /home/hr/projects/ppdetection/baidu/personal-code/ppdetection/configs/ppyoloe/dataset/voc
    anno_path: /home/hr/projects/ppdetection/baidu/personal-code/ppdetection/configs/ppyoloe/dataset/voc/eval.txt
    label_list: /home/hr/projects/ppdetection/baidu/personal-code/ppdetection/configs/ppyoloe/dataset/voc/label_list.txt
    data_fields: ['image', 'gt_bbox', 'gt_class', 'difficult']

TestDataset:
  !ImageFolder
    anno_path: /home/hr/projects/ppdetection/baidu/personal-code/ppdetection/configs/ppyoloe/dataset/voc/label_list.txt
```

'./_base_/ppyoloe_crn.yml',
```
architecture: YOLOv3
norm_type: sync_bn
use_ema: true
ema_decay: 0.9998

YOLOv3:
  backbone: CSPResNet
  neck: CustomCSPPAN
  yolo_head: PPYOLOEHead
  post_process: ~

CSPResNet:
  layers: [3, 6, 6, 3]
  channels: [64, 128, 256, 512, 1024]
  return_idx: [1, 2, 3]
  use_large_stem: True

CustomCSPPAN:
  out_channels: [768, 384, 192]
  stage_num: 1
  block_num: 3
  act: 'swish'
  spp: true

PPYOLOEHead:
  fpn_strides: [32, 16, 8]
  grid_cell_scale: 5.0
  grid_cell_offset: 0.5
  static_assigner_epoch: 100
  use_varifocal_loss: True
  loss_weight: {class: 1.0, iou: 2.5, dfl: 0.5}
  static_assigner:
    name: ATSSAssigner
    topk: 9
  assigner:
    name: TaskAlignedAssigner
    topk: 13
    alpha: 1.0
    beta: 6.0
  nms:
    name: MultiClassNMS
    nms_top_k: 1000
    keep_top_k: 100
    score_threshold: 0.01
    nms_threshold: 0.6
``` 

'./_base_/ppyoloe_reader.yml',
```
worker_num: 4
eval_height: &eval_height 640
eval_width: &eval_width 640
eval_size: &eval_size [*eval_height, *eval_width]

TrainReader:
  sample_transforms:
    - Decode: {}
    - RandomDistort: {}
    - RandomExpand: {fill_value: [123.675, 116.28, 103.53]}
    - RandomCrop: {}
    - RandomFlip: {}
  batch_transforms:
    - BatchRandomResize: {target_size: [320, 352, 384, 416, 448, 480, 512, 544, 576, 608, 640, 672, 704, 736, 768], random_size: True, random_interp: True, keep_ratio: False}
    - NormalizeImage: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225], is_scale: True}
    - Permute: {}
    - PadGT: {}
  batch_size: 32
  shuffle: true
  drop_last: true
  use_shared_memory: true
  collate_batch: true

EvalReader:
  sample_transforms:
    - Decode: {}
    - Resize: {target_size: *eval_size, keep_ratio: False, interp: 2}
    - NormalizeImage: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225], is_scale: True}
    - Permute: {}
  batch_size: 32

TestReader:
  inputs_def:
    image_shape: [3, *eval_height, *eval_width]
  sample_transforms:
    - Decode: {}
    - Resize: {target_size: *eval_size, keep_ratio: False, interp: 2}
    - NormalizeImage: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225], is_scale: True}
    - Permute: {}
  batch_size: 1
```

### 复现环境 Environment

- paddlepaddle-gpu : 2.3.1
- paddleslim : 2.3.4
- ppdetection : 2.4
- python : 3.9.12

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
参数文档中提到的rtsp流地址，配置上会报错,PaddlePaddle/PaddleDetection,2022-08-29 06:26:22,2,,6782,1353771321,"### 文档链接&描述 Document Links & Description

你好，参数文档中提到的rtsp流地址，配置上会报错
File ""PaddleDetection-develop/deploy/pipeline/pipeline.py"", line 124, in _parse_input
    assert os.path.exists(video_file), ""video_file not exists.""
AssertionError: video_file not exists.

文档链接：https://github.com/PaddlePaddle/PaddleDetection/blob/develop/deploy/pipeline/docs/tutorials/PPHuman_QUICK_STARTED.md
--video_file 需要预测的视频，或者rtsp流地址


### 请提出你的建议 Please give your suggestion

_No response_"
release/2.4分支代码训练的lcnet模型用paddleLite在安卓上部署时报错,PaddlePaddle/PaddleDetection,2022-08-29 01:36:01,4,,6776,1353560347,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

在release/v2.4分支下，用 `configs/picodet/picodet_xs_320_coco_lcnet.yml` 这个配置文件训练出来的模型，在用paddle-lite部署在安卓上时，报了如下错误，

![image](https://user-images.githubusercontent.com/20619835/187105400-b75322c8-a1db-48d0-ae18-50af7e3d635e.png)

看报错信息应该是模型的维度出现了错误，但我是完全按照步骤进行操作的，我也不知道给怎么修改，麻烦大佬指点



### 复现环境 Environment

- PaddlePaddle:2.2.1
- PaddleDetection:release/2.4
- Python:3.7.11
- CUDA:11.2
- CUDNN:8.1

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
PaddleDetection教程最后一个阶段图片没有检测框,PaddlePaddle/PaddleDetection,2022-08-27 18:29:29,2,windows,6774,1353113970,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

![image](https://user-images.githubusercontent.com/8224370/187043475-c79cf386-e2ce-4030-a504-0809e16fa071.png)
![image](https://user-images.githubusercontent.com/8224370/187043498-7a6312e5-2fac-49a5-91c2-6476f3398006.png)
尝试过自己训练，训练好了用测试图集也meiyou没有显示框
命令：python tools/infer.py -c configs/ppyolo/ppyolo_r50vd_dcn_1x_coco.yml -o use_gpu=true weights=https://paddledet.bj.bcebos.com/models/ppyolo_r50vd_dcn_1x_coco.pdparams --infer_img=demo/000000014439.jpg
如何才能显示被检测图让它显示检测框？检测框的代码在哪里？"
PPYOLOE C++部署,PaddlePaddle/PaddleDetection,2022-08-27 16:05:06,3,,6773,1353082070,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

采用PPYOLOE 进行C++部署，利用PaddleDetection/deploy/cpp中代码运行，出现CopyTocpu 报错，根据排查发现是embding 没有，但是本来模型就没有embding，如何更改才能使用呢。"
voc和coco格式是否,PaddlePaddle/PaddleDetection,2022-08-27 08:48:15,1,,6772,1352987602,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

PPYOLOE的配置后缀只有coco，是否代表只能用coco格式的数据集，VOC的可以吗
![11111111111](https://user-images.githubusercontent.com/86812421/187022875-513ef508-fd83-4650-96d6-1aa41e17bd72.jpg)
？"
PP-human可以部署到android设备上吗？,PaddlePaddle/PaddleDetection,2022-08-26 08:20:32,1,,6768,1351926534,
"PP-PicoDet用GPU检测,有没什么影响",PaddlePaddle/PaddleDetection,2022-08-25 13:56:53,1,,6761,1350945894,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

PP-PicoDet用GPU检测,有没什么影响

还是用CPU比较好？

速度 识别率有没什么影响"
吸烟检测精度较低,PaddlePaddle/PaddleDetection,2022-08-25 11:07:55,1,,6758,1350726180,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

测试环境：paddle detection 2.4
测试集：[Human detection and tracking using RGB-D camera]，服装店监控视频，行人就是店员和顾客
测试结果：吸烟检测精度较低。非吸烟动作误检为吸烟，视频中几乎80%的人都判定为吸烟，实际视频中没有人吸烟
参考文献：People detection and tracking using RGB-D cameras for mobile robots，2016

吸烟检测精度较低的问题会修复吗？大概什么时候？多谢解答

### 复现环境 Environment

_No response_

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
tinypose 新增关键点后eval评估异常！！！,PaddlePaddle/PaddleDetection,2022-08-25 07:32:56,2,,6750,1350454802,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

1 请问下， 如果我的关键点标注coco json 有27个关键点，比之前原始的17个多了10个，json的格式跟原始coco 一样，就是增加了keypoints的数量 [     ... 27个三元组]  ，请问下我只要把yml文件里面的关键点数量改为27  然后 skeleton关系改为27个 关联关系就行了 对吗？ 
2   更改为31个点后，训练可以跑起来了，但是eval 评估有些问题 ，请您看下错误信息， 能指点下吗？ 多谢
![image](https://user-images.githubusercontent.com/8407513/186603236-93336e2f-9e74-4afd-957b-4fd05813f01a.png)


### 复现环境 Environment

![image](https://user-images.githubusercontent.com/8407513/186603400-053fd495-b1fd-4000-b6d0-e93fd12069a5.png)


### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
模型剪枝后量化导出失败,PaddlePaddle/PaddleDetection,2022-08-25 02:35:04,2,,6748,1350236922,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

pretrain_weights: /root/autodl-tmp/pd/output/yolov3_mobilenet_v3_large_270e_coco/model_final.pdparams
slim: PrunerQAT

PrunerQAT:
criterion: fpgm
pruned_params: ['conv2d_52.w_0', 'conv2d_53.w_0', 'conv2d_54.w_0',
'conv2d_55.w_0', 'conv2d_56.w_0', 'conv2d_57.w_0',
'conv2d_59.w_0', 'conv2d_60.w_0', 'conv2d_61.w_0',
'conv2d_62.w_0', 'conv2d_63.w_0', 'conv2d_64.w_0',
'conv2d_66.w_0', 'conv2d_67.w_0', 'conv2d_68.w_0',
'conv2d_69.w_0', 'conv2d_70.w_0', 'conv2d_71.w_0']
pruned_ratios: [0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.875,0.875,0.875,0.875,0.875,0.875]
print_prune_params: False
quant_config: {
'weight_quantize_type': 'channel_wise_abs_max', 'activation_quantize_type': 'moving_average_abs_max',
'weight_bits': 8, 'activation_bits': 8, 'dtype': 'int8', 'window_size': 10000, 'moving_rate': 0.9,
'quantizable_layer_type': ['Conv2D', 'Linear']}
print_qat_model: False

改写的剪枝量化模型，训练正常。在导出模型时，出现了：
[08/15 12:21:06] ppdet.engine INFO: Export inference config file to /root/autodl-tmp/infer/yolov3_mobilenetv3_prune_qat/yolov3_mobilenetv3_prune_qat/infer_cfg.yml
Traceback (most recent call last):
File ""tools/export_model.py"", line 108, in
main()
File ""tools/export_model.py"", line 104, in main
run(FLAGS, cfg)
File ""tools/export_model.py"", line 73, in run
trainer.export(FLAGS.output_dir)
File ""/root/autodl-tmp/pd/ppdet/engine/trainer.py"", line 869, in export
self.cfg.slim.save_quantized_model(
File ""/root/autodl-tmp/pd/ppdet/slim/prune.py"", line 150, in save_quantized_model
self.quanter.save_quantized_model(
File ""/root/miniconda3/lib/python3.8/site-packages/paddleslim/dygraph/quant/qat.py"", line 289, in save_quantized_model
self.imperative_qat.save_quantized_model(
File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/contrib/slim/quantization/imperative/qat.py"", line 262, in save_quantized_model
self._quantize_outputs.save_quantized_model(layer, path, input_spec,
File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/contrib/slim/quantization/imperative/qat.py"", line 465, in save_quantized_model
paddle.jit.save(layer=model, path=path, input_spec=input_spec, **config)
File ""/root/miniconda3/lib/python3.8/site-packages/decorator.py"", line 232, in fun
return caller(func, *(extras + args), **kw)
File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/wrapped_decorator.py"", line 25, in impl
return wrapped_func(*args, **kwargs)
File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/base.py"", line 51, in impl
return func(*args, **kwargs)
File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/jit.py"", line 731, in save
configs = _parse_save_configs(configs)
File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/jit.py"", line 376, in _parse_save_configs
raise ValueError(
ValueError: The additional config (onnx_format) of paddle.jit.save is not supported.

导出代码如下：
!# 导出YOLOv3模型
!export CUDA_VISIBLE_DEVICES=0 #windows和Mac下不需要执行该命令
%cd /root/autodl-tmp/pd
!python tools/export_model.py -c configs/yolov3/yolov3_mobilenet_v3_large_270e_coco.yml
--slim_config configs/slim/extensions/yolov3_mobilenetv3_prune_qat.yml
--output_dir=/root/autodl-tmp/infer/yolov3_mobilenetv3_prune_qat
-o weights=output/yolov3_mobilenetv3_prune_qat/model_final.pdparams"
Paddle-Lite端侧部署fp16出现问题,PaddlePaddle/PaddleDetection,2022-08-24 06:46:22,2,,6742,1348950140,"### 问题描述 Please describe your issue

我是完全按照这份文档 https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/deploy/lite 说明步骤进行操作的，最终在android终端运行程序，出现`Illegal instruction`报错，如下所示，没有其他任何信息，有没有大佬遇到过，帮忙说一下哪里出错或需要注意的

![image](https://user-images.githubusercontent.com/20619835/186349240-1ea6ca36-dc9a-410b-87a8-805ed1dbbeca.png)
"
尝试测试跨境推理报错,PaddlePaddle/PaddleDetection,2022-08-24 06:27:03,1,,6740,1348932394,"[root@e5944951b2cb PaddleDetection]# python deploy/pptracking/python/mot_sde_infer.py --model_dir=output_inference/ppyolov2_r50vd_dcn_365e_aic21mtmct_vehicle/ --reid_model_dir=output_inference/deepsort_pplcnet_vehicle/ --mtmct_dir=./mtmct-demo --device=GPU --mtmct_cfg=deploy/pptracking/python/mtmct_cfg.yml --scaled=True --save_mot_txts --save_images
-----------  Running Arguments -----------
batch_size: 1
camera_id: -1
cpu_threads: 1
device: GPU
do_break_in_counting: False
do_entrance_counting: False
draw_center_traj: False
enable_mkldnn: False
image_dir: None
image_file: None
model_dir: output_inference/ppyolov2_r50vd_dcn_365e_aic21mtmct_vehicle/
mtmct_cfg: deploy/pptracking/python/mtmct_cfg.yml
mtmct_dir: ./mtmct-demo
output_dir: output
region_polygon: []
region_type: horizontal
reid_batch_size: 50
reid_model_dir: output_inference/deepsort_pplcnet_vehicle/
run_benchmark: False
run_mode: paddle
save_images: True
save_mot_txt_per_img: False
save_mot_txts: True
scaled: True
secs_interval: 2
threshold: 0.5
tracker_config: None
trt_calib_mode: False
trt_max_shape: 1280
trt_min_shape: 1
trt_opt_shape: 640
use_dark: True
use_gpu: False
video_file: None
------------------------------------------
-----------  Model Configuration -----------
Model Arch: YOLO
Transform Order: 
--transform op: Resize
--transform op: NormalizeImage
--transform op: Permute
--------------------------------------------
-----------  Model Configuration -----------
Model Arch: DeepSORT
Transform Order: 
--transform op: LetterBoxResize
--transform op: NormalizeImage
--transform op: Permute
--------------------------------------------
Traceback (most recent call last):
  File ""deploy/pptracking/python/mot_sde_infer.py"", line 850, in <module>
    main()
  File ""deploy/pptracking/python/mot_sde_infer.py"", line 788, in main
    detector = SDE_Detector(
  File ""deploy/pptracking/python/mot_sde_infer.py"", line 156, in __init__
    assert tracker_config is not None, 'Note that tracker_config should be set.'
AssertionError: Note that tracker_config should be set.
"
solov2中，paddle.linspace操作在openvino中生成失败,PaddlePaddle/PaddleDetection,2022-08-24 06:24:37,1,,6739,1348930191,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question


![MicrosoftTeams-image](https://user-images.githubusercontent.com/54624556/186345857-8505a470-8908-4c1f-a7a7-625e188b1a93.png)
在SOLOV2中，solov2_head.py里面有个paddle.linspace这个操作，openvino转换时报错，请问一下有什么好的方法可以替换这个操作，用来成功转换成openvino可以推理的吗?"
yolox训练问题,PaddlePaddle/PaddleDetection,2022-08-24 05:54:18,3,,6738,1348904464,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

1、训练时显存利用率不高
在私有数据集上训练yolox_m 在四卡p40上，平均每张卡bs=4，利用率为70%，显卡利用率不高，而且每张卡上的显存波动比较大。
2、eval私有数据集时，显存利用率绝大时间为0%，评测指令为
`export CUDA_VISIBLE_DEVICES=0,1 #windows和Mac下不需要执行该命令
python tools/eval.py -c configs/yolox/yolox_m_300e_coco.yml -o weights=59.pdparams`

这俩个问题有什么好的解决办法吗？
"
使用 python tools/infer 可以检测到东西，没什么问题。 但是使用部署步骤，export_model.py，再使用 deploy/python/infer，就直接报错,PaddlePaddle/PaddleDetection,2022-08-24 02:11:22,5,,6735,1348747205,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

使用 python tools/infer 可以检测到东西，没什么问题。 但是使用部署步骤，export_model.py，再使用 deploy/python/infer，就直接报错，
使用的环境：paddle2.2.2
                     python3.6
                     PaddleDetection 2.3
我的命令是：python tools/export_model.py -c configs/faster_rcnn/faster_rcnn_swin_tiny_fpn_3x_coco.yml --output_dir=./inference_model  -o weights=/lzl/PaddleDetection/output/faster_rcnn_swin_tiny_fpn_3x_coco/best_model.pdparams
报的错误是：
<img width=""727"" alt=""51faaa12d9ee920cd91a6dfda02480d"" src=""https://user-images.githubusercontent.com/50006064/186300980-ecd232d3-f911-4fb2-9e71-ad9b2a3c0c4c.png"">
"
支持将全部数据加载到内存以加速训练吗,PaddlePaddle/PaddleDetection,2022-08-24 01:55:57,3,,6734,1348738600,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

请问：是否支持将全部数据加载到内存中以加速训练？类似yolov5中的--cache功能？之前训练5万张图片就要训练好几天，现在要训练30多万张，用yolov5将所有图片一次性加载到内存中可以显著加速训练

### 是否愿意提交PR Are you willing to submit a PR?

- [x] Yes I'd like to help by submitting a PR!"
yolox的onnx部署推理,PaddlePaddle/PaddleDetection,2022-08-23 08:24:04,4,,6726,1347543304,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

能不能实现yolox的onnx推理？paddle的部署没有onnx方便

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
ValueError: Target 460 is out of upper bound.,PaddlePaddle/PaddleDetection,2022-08-23 05:03:46,5,windows,6723,1347341288,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

ppyoloe_crn_s_300e_coco
VOC 数据集

python  tools/train.py -c configs/ppyoloe/ppyoloe_crn_s_300e_coco.yml

```

W0823 14:30:26.446256  4452 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 11.6, Runtime API Version: 11.2
W0823 14:30:26.461884  4452 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.
[08/23 14:30:27] ppdet.utils.checkpoint INFO: Finish loading model weights: C:\Users\fujunnnn/.cache/paddle/weights\CSPResNetb_s_pretrained.pdparams
[08/23 14:30:30] ppdet.engine INFO: Epoch: [0] [  0/339] learning_rate: 0.000000 loss: 1931307253760.000000 loss_cls: 0.594841 loss_iou: 772522901504.000000 loss_dfl: 5885.125977 loss_l1: 0.105123 eta: 4 days, 9:30:32 batch_cost: 3.7348 data_cost: 0.2500 ips: 2.6775 images/s
Traceback (most recent call last):
  File ""tools/train.py"", line 177, in <module>
    main()
  File ""tools/train.py"", line 173, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 127, in run
    trainer.train(FLAGS.eval)
  File ""E:\PaddleX_GUI_2.1.0_win10\PaddleDetection\ppdet\engine\trainer.py"", line 454, in train
    outputs = model(data)
  File ""E:\anaconda3\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""E:\anaconda3\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""E:\PaddleX_GUI_2.1.0_win10\PaddleDetection\ppdet\modeling\architectures\meta_arch.py"", line 59, in forward
    out = self.get_loss()
  File ""E:\PaddleX_GUI_2.1.0_win10\PaddleDetection\ppdet\modeling\architectures\yolo.py"", line 125, in get_loss
    return self._forward()
  File ""E:\PaddleX_GUI_2.1.0_win10\PaddleDetection\ppdet\modeling\architectures\yolo.py"", line 88, in _forward
    yolo_losses = self.yolo_head(neck_feats, self.inputs)
  File ""E:\anaconda3\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""E:\anaconda3\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""E:\PaddleX_GUI_2.1.0_win10\PaddleDetection\ppdet\modeling\heads\ppyoloe_head.py"", line 217, in forward
    return self.forward_train(feats, targets)
  File ""E:\PaddleX_GUI_2.1.0_win10\PaddleDetection\ppdet\modeling\heads\ppyoloe_head.py"", line 160, in forward_train
    ], targets)
  File ""E:\PaddleX_GUI_2.1.0_win10\PaddleDetection\ppdet\modeling\heads\ppyoloe_head.py"", line 355, in get_loss
    assigned_scores_sum)
  File ""E:\PaddleX_GUI_2.1.0_win10\PaddleDetection\ppdet\modeling\heads\ppyoloe_head.py"", line 291, in _bbox_loss
    assigned_ltrb_pos) * bbox_weight
  File ""E:\PaddleX_GUI_2.1.0_win10\PaddleDetection\ppdet\modeling\heads\ppyoloe_head.py"", line 256, in _df_loss
    pred_dist, target_left, reduction='none') * weight_left
  File ""E:\anaconda3\envs\PaddleDetection\lib\site-packages\paddle\nn\functional\loss.py"", line 1723, in cross_entropy
    label_max.item()))
ValueError: Target 25479 is out of upper bound.
```


python tools/train.py -c configs/ppyoloe/ppyoloe_plus_crn_s_80e_coco.yml
```

W0823 21:31:38.730271 10200 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 11.6, Runtime API Version: 11.2
W0823 21:31:38.750262 10200 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.
[08/23 21:31:40] ppdet.utils.checkpoint INFO: The shape [365] in pretrained weight yolo_head.pred_cls.0.bias is unmatched with the shape [4] in model yolo_head.pred_cls.0.bias. And the weight yolo_head.pred_cls.0.bias will not be loaded
[08/23 21:31:40] ppdet.utils.checkpoint INFO: The shape [365, 384, 3, 3] in pretrained weight yolo_head.pred_cls.0.weight is unmatched with the shape [4, 384, 3, 3] in model yolo_head.pred_cls.0.weight. And the weight yolo_head.pred_cls.0.weight will not be loaded
[08/23 21:31:40] ppdet.utils.checkpoint INFO: The shape [365] in pretrained weight yolo_head.pred_cls.1.bias is unmatched with the shape [4] in model yolo_head.pred_cls.1.bias. And the weight yolo_head.pred_cls.1.bias will not be loaded
[08/23 21:31:40] ppdet.utils.checkpoint INFO: The shape [365, 192, 3, 3] in pretrained weight yolo_head.pred_cls.1.weight is unmatched with the shape [4, 192, 3, 3] in model yolo_head.pred_cls.1.weight. And the weight yolo_head.pred_cls.1.weight will not be loaded
[08/23 21:31:40] ppdet.utils.checkpoint INFO: The shape [365] in pretrained weight yolo_head.pred_cls.2.bias is unmatched with the shape [4] in model yolo_head.pred_cls.2.bias. And the weight yolo_head.pred_cls.2.bias will not be loaded
[08/23 21:31:40] ppdet.utils.checkpoint INFO: The shape [365, 96, 3, 3] in pretrained weight yolo_head.pred_cls.2.weight is unmatched with the shape [4, 96, 3, 3] in model yolo_head.pred_cls.2.weight. And the weight yolo_head.pred_cls.2.weight will not be loaded
[08/23 21:31:40] ppdet.utils.checkpoint INFO: Finish loading model weights: C:\Users\MM/.cache/paddle/weights\ppyoloe_crn_s_obj365_pretrained.pdparams
Traceback (most recent call last):
  File ""tools/train.py"", line 172, in <module>
    main()
  File ""tools/train.py"", line 168, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 132, in run
    trainer.train(FLAGS.eval)
  File ""D:\0SDXX\PaddleDetection\ppdet\engine\trainer.py"", line 504, in train
    outputs = model(data)
  File ""D:\Anaconda3\envs\PaddleSeg\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""D:\Anaconda3\envs\PaddleSeg\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""D:\0SDXX\PaddleDetection\ppdet\modeling\architectures\meta_arch.py"", line 59, in forward
    out = self.get_loss()
  File ""D:\0SDXX\PaddleDetection\ppdet\modeling\architectures\yolo.py"", line 124, in get_loss
    return self._forward()
  File ""D:\0SDXX\PaddleDetection\ppdet\modeling\architectures\yolo.py"", line 88, in _forward
    yolo_losses = self.yolo_head(neck_feats, self.inputs)
  File ""D:\Anaconda3\envs\PaddleSeg\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""D:\Anaconda3\envs\PaddleSeg\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""D:\0SDXX\PaddleDetection\ppdet\modeling\heads\ppyoloe_head.py"", line 216, in forward
    return self.forward_train(feats, targets)
  File ""D:\0SDXX\PaddleDetection\ppdet\modeling\heads\ppyoloe_head.py"", line 161, in forward_train
    ], targets)
  File ""D:\0SDXX\PaddleDetection\ppdet\modeling\heads\ppyoloe_head.py"", line 354, in get_loss
    assigned_scores_sum)
  File ""D:\0SDXX\PaddleDetection\ppdet\modeling\heads\ppyoloe_head.py"", line 290, in _bbox_loss
    assigned_ltrb_pos) * bbox_weight
  File ""D:\0SDXX\PaddleDetection\ppdet\modeling\heads\ppyoloe_head.py"", line 255, in _df_loss
    pred_dist, target_left, reduction='none') * weight_left
  File ""D:\Anaconda3\envs\PaddleSeg\lib\site-packages\paddle\nn\functional\loss.py"", line 1723, in cross_entropy
    label_max.item()))
ValueError: Target 28 is out of upper bound.

```"
可以多显卡预测吗,PaddlePaddle/PaddleDetection,2022-08-22 02:21:48,4,,6711,1345710103,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

图片太大 20000*20000
切割下来 也有二百多张图
PP-PicoDet模型
每一张检测 都要 50-60ms

大概要6-10秒

能否多显卡预测
在2秒之内全部识别完成"
yolov7训练报错,PaddlePaddle/PaddleDetection,2022-08-20 14:18:23,4,,6707,1345176310,"### 问题确认 Search before asking

### 麻烦问一下这个报错怎么解决，这个数据集在yolov5可以正常使用

![1661004008021](https://user-images.githubusercontent.com/99952437/185750789-a85be497-d35b-4717-ab90-b476ab013242.jpg)"
ppyoloe 多卡训练报错 nan,PaddlePaddle/PaddleDetection,2022-08-20 09:16:51,1,,6706,1345103542,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question


![472c9e8fc7429a178f382de91c469e1](https://user-images.githubusercontent.com/77281977/185738450-e886d088-fcbe-4ecb-916a-9441705754e1.png)
![6795b169612a4d95aa3f6896569a0b6](https://user-images.githubusercontent.com/77281977/185738454-6ca7a7be-162a-4283-ad6b-df9c7a9ee2b7.png)

训练过程中应该是验证的时候 nan  报错 
这是什么原因"
关于picodetV1训练效果问题,PaddlePaddle/PaddleDetection,2022-08-19 06:20:12,0,,6694,1343996478,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

<img width=""654"" alt=""企业微信截图_16608896695664"" src=""https://user-images.githubusercontent.com/48303408/185555179-8f56c8b1-08ca-46ed-9f99-101fb034d247.png"">
请问各位大佬，我训练的样本大概如图所示，使用的是picodetV1版本的m_416模型，如果只检测绿框这一个类，map可以达到0.9以上；如果同时检测这2个类的时候，map0.5只能达到0.6左右，测试效果红框的类基本检测不出，绿框类能检测出很多，请问大佬这是什么原因"
PaddleDetection新版本问题,PaddlePaddle/PaddleDetection,2022-08-19 02:13:56,1,,6692,1343842852,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

您好，我是按照aistudio上的【PaddleDetection2.0专项】新版本快速体验这个项目一步一步执行的，但是在执行
!python ppdet/modeling/tests/test_architectures.py
这行代码时，报错，说很多模块没有注册，错误部分如下图：
![image](https://user-images.githubusercontent.com/71055342/185527561-cb55b411-822f-45e5-ac88-0e4c413dcd87.png)
![image](https://user-images.githubusercontent.com/71055342/185527639-2cd50532-4784-488f-b5f7-07ae5227756e.png)

，请问我改如何解决？"
Multiple predictor objects in android detection demo,PaddlePaddle/PaddleDetection,2022-08-19 01:19:58,0,,6690,1343811732,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

This is in reference to `Picodet-detection-demo.`

I am trying to have multiple forwards passes run in parallel and I tried this by declaring multiple predictor objects and then calling them via `CoroutineScope` in the android implementation. But it looks like there is some internal lock on something as I am not observing any increase in throughput. Am I doing something wrong here or does the compiled paddle-lite library that is available with sample app not support multiple instantiations in parallel? "
摔倒推理有问题,PaddlePaddle/PaddleDetection,2022-08-18 10:27:59,1,,6688,1342897423,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

使用版本为develop，运行以下代码
python deploy/pipeline/pipeline.py --config deploy/pipeline/config/infer_cfg_pphuman.yml \
                                                   --video_file=test_video.mp4 \
                                                   --device=gpu \
                                                   --model_dir kpt=./dark_hrnet_w32_256x192 action=./STGCN

提出参数有问题：pipeline.py: error: unrecognized arguments: action=./STGCN

### 复现环境 Environment

_No response_

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
pphumen能高并发吗,PaddlePaddle/PaddleDetection,2022-08-18 09:04:59,1,,6687,1342795800,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

pphumen能高并发吗"
how to export txt file?(PP-YOLOE),PaddlePaddle/PaddleDetection,2022-08-18 05:12:20,2,,6683,1342568732,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

thanks for your work!!

I could inference my own images on PP-YOLOE.
But, I counldn't export txt file of deetection result.
now, I'm using ""--save_mot_txt_per_img"" or ""--save_mot_txts"" and arguments is True.

command
`python deploy/python/infer.py 
--model_dir=output_inference/ppyoloe_crn_x_300e_coco 
--image_file=demo/000000014439.jpg --device=GPU --save_mot_txt_per_img`

--save_mot_txt_per_img
![image](https://user-images.githubusercontent.com/56009331/185298359-fb205c60-86b3-41c5-b13d-d6afb21315d3.png)

--save_mot_txts
![image](https://user-images.githubusercontent.com/56009331/185298315-0b12ad8f-4fea-4f76-88a2-ea665e5da6c0.png)


Even this situation, the output is only image.
I need txt file to count object.
![image](https://user-images.githubusercontent.com/56009331/185298493-6ab60987-7e7d-426b-8dfc-80aeabec0041.png)



this issue is happen PP-YOLO too.
please help me."
C++编译失败，编译到100%然后报错,PaddlePaddle/PaddleDetection,2022-08-18 03:45:18,2,,6682,1342525834,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

[100%] Linking CXX executable main
/usr/bin/ld: warning: libonnxruntime.so.1.10.0, needed by /home/alex/Downloads/paddle_inference/paddle/lib/libpaddle_inference.so, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libpaddle2onnx.so, needed by /home/alex/Downloads/paddle_inference/paddle/lib/libpaddle_inference.so, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libavcodec-ffmpeg.so.56, needed by ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libavformat-ffmpeg.so.56, needed by ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libavutil-ffmpeg.so.54, needed by ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libswscale-ffmpeg.so.3, needed by ../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16, not found (try using -rpath or -rpath-link)
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘avformat_close_input@LIBAVFORMAT_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘sws_scale@LIBSWSCALE_FFMPEG_3’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_guess_codec@LIBAVFORMAT_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_guess_format@LIBAVFORMAT_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_guess_sample_aspect_ratio@LIBAVFORMAT_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘avcodec_get_context_defaults3@LIBAVCODEC_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘avcodec_flush_buffers@LIBAVCODEC_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_codec_get_tag@LIBAVFORMAT_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘avformat_get_mov_video_tags@LIBAVFORMAT_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘avformat_open_input@LIBAVFORMAT_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘avformat_alloc_context@LIBAVFORMAT_FFMPEG_56’未定义的引用
/home/alex/Downloads/paddle_inference/paddle/lib/libpaddle_inference.so：对‘OrtGetApiBase@VERS_1.10.0’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘avcodec_decode_video2@LIBAVCODEC_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_bitstream_filter_close@LIBAVCODEC_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_image_fill_arrays@LIBAVUTIL_FFMPEG_54’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_find_input_format@LIBAVFORMAT_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_rescale_q@LIBAVUTIL_FFMPEG_54’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘avcodec_find_decoder_by_name@LIBAVCODEC_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_frame_get_buffer@LIBAVUTIL_FFMPEG_54’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘sws_getCachedContext@LIBSWSCALE_FFMPEG_3’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_bitstream_filter_init@LIBAVCODEC_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘sws_getContext@LIBSWSCALE_FFMPEG_3’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘avio_open@LIBAVFORMAT_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘avformat_get_riff_video_tags@LIBAVFORMAT_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_bitstream_filter_filter@LIBAVCODEC_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_freep@LIBAVUTIL_FFMPEG_54’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_write_frame@LIBAVFORMAT_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_mallocz@LIBAVUTIL_FFMPEG_54’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘avio_close@LIBAVFORMAT_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_image_get_buffer_size@LIBAVUTIL_FFMPEG_54’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_write_trailer@LIBAVFORMAT_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_dict_parse_string@LIBAVUTIL_FFMPEG_54’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘avcodec_open2@LIBAVCODEC_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘sws_freeContext@LIBSWSCALE_FFMPEG_3’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘avcodec_find_decoder@LIBAVCODEC_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘avformat_new_stream@LIBAVFORMAT_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘avformat_find_stream_info@LIBAVFORMAT_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘avcodec_get_name@LIBAVCODEC_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_lockmgr_register@LIBAVCODEC_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_dict_free@LIBAVUTIL_FFMPEG_54’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘avcodec_encode_video2@LIBAVCODEC_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘avcodec_pix_fmt_to_codec_tag@LIBAVCODEC_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_frame_alloc@LIBAVUTIL_FFMPEG_54’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_free@LIBAVUTIL_FFMPEG_54’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_read_frame@LIBAVFORMAT_FFMPEG_56’未定义的引用
/home/alex/Downloads/paddle_inference/paddle/lib/libpaddle_inference.so：对‘paddle2onnx::Export(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, bool, int, bool, bool, bool, bool, bool)’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_dict_get@LIBAVUTIL_FFMPEG_54’未定义的引用
/home/alex/Downloads/paddle_inference/paddle/lib/libpaddle_inference.so：对‘paddle2onnx::IsExportable(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool, int, bool, bool, bool, bool, bool)’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_seek_frame@LIBAVFORMAT_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_frame_free@LIBAVUTIL_FFMPEG_54’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘avformat_write_header@LIBAVFORMAT_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘avformat_free_context@LIBAVFORMAT_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_frame_unref@LIBAVUTIL_FFMPEG_54’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_codec_get_id@LIBAVFORMAT_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_log_set_callback@LIBAVUTIL_FFMPEG_54’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_sub_q@LIBAVUTIL_FFMPEG_54’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘avformat_network_init@LIBAVFORMAT_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_opt_set@LIBAVUTIL_FFMPEG_54’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_packet_unref@LIBAVCODEC_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘avcodec_find_encoder@LIBAVCODEC_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_init_packet@LIBAVCODEC_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_log_set_level@LIBAVUTIL_FFMPEG_54’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_dict_set@LIBAVUTIL_FFMPEG_54’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_malloc@LIBAVUTIL_FFMPEG_54’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘av_register_all@LIBAVFORMAT_FFMPEG_56’未定义的引用
../deps/opencv-3.4.16_gcc8.2_ffmpeg/lib/libopencv_videoio.so.3.4.16：对‘avcodec_close@LIBAVCODEC_FFMPEG_56’未定义的引用
collect2: error: ld returned 1 exit status
CMakeFiles/main.dir/build.make:168: recipe for target 'main' failed
make[2]: *** [main] Error 1
CMakeFiles/Makefile2:108: recipe for target 'CMakeFiles/main.dir/all' failed
make[1]: *** [CMakeFiles/main.dir/all] Error 2
Makefile:83: recipe for target 'all' failed
make: *** [all] Error 2
make finished!

### 复现环境 Environment

CUDA-10.2
CUDNN-8.1.1
gcc-8.4.0
PaddleDetction-2.4.0

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
小白提问，教程里都是notebook的代码，有pycharm直接用的教程吗,PaddlePaddle/PaddleDetection,2022-08-18 03:25:43,1,,6681,1342515451,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

RT"
静态图多机多卡训练教程,PaddlePaddle/PaddleDetection,2022-08-18 03:18:14,7,status/close,6680,1342512012,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

静态图多机多卡训练教程"
"InvalidArgumentError: The rois_batch_size and imgs batch_size must be the same. But received rois_batch_size = 9, batch_size = 1       [Hint: Expected rois_batch_size == batch_size, but received rois_batch_size:9 != batch_size:1.] (at /paddle/paddle/fluid/operators/roi_align_op.cu:283)       [operator < roi_align > error]",PaddlePaddle/PaddleDetection,2022-08-17 10:04:50,3,,6678,1341513209,
Could you please provide the training logs of YOLOE-m models.,PaddlePaddle/PaddleDetection,2022-08-17 09:11:22,2,,6677,1341430743,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

Hi, YOLOE is a great work. I want to reproduce the YOLOE in PyTorch, howerer, my training result is lower than your results. So I ant to find the training logs of YOLOE-m model to check the difference.
If possible, please send the training logs to 157995010@qq.com. Thanks very much."
Mask_rcnn评估的时候显存一直再长直到内存溢出,PaddlePaddle/PaddleDetection,2022-08-17 08:41:29,4,,6676,1341394903,"### 问题描述 Please describe your issue

![image](https://user-images.githubusercontent.com/15188456/185074742-65f3abad-ba4f-4a63-963d-e6a86bcd6e12.png)
![image](https://user-images.githubusercontent.com/15188456/185074859-362b45bb-6750-4bef-acd6-f7d86d8e1b82.png)
"
coco数据集yolov5可以正常训练，yolox模型训练报错 cudaErrorIllegalAddress,PaddlePaddle/PaddleDetection,2022-08-17 06:49:50,10,,6673,1341270349,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

coco数据集yolov5可以正常训练，yolox模型训练报错无法定位错误原因，查看显存没有溢出

loading annotations into memory...
Done (t=14.63s)
creating index...
index created!
[08/17 14:27:03] ppdet.data.source.coco WARNING: Found an invalid bbox in annotations: im_id: 200365, area: 0.0 x1: 296.65, y1: 388.33, x2: 297.67999999999995, y2: 388.33.
[08/17 14:27:14] ppdet.data.source.coco WARNING: Found an invalid bbox in annotations: im_id: 550395, area: 0.0 x1: 9.98, y1: 188.56, x2: 15.52, y2: 188.56.
W0817 14:27:17.776957   512 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.6, Runtime API Version: 11.2
W0817 14:27:17.800897   512 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.
Traceback (most recent call last):
  File ""tools/train.py"", line 178, in <module>
    main()
  File ""tools/train.py"", line 174, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 136, in run
    trainer.train(FLAGS.eval)
  File ""E:\AI_Code\PaddleDetection_YOLOSeries-develop\ppdet\engine\trainer.py"", line 487, in train
    outputs = model(data)
  File ""D:\Users\Aorus\miniconda3\envs\paddle_env\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""D:\Users\Aorus\miniconda3\envs\paddle_env\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""E:\AI_Code\PaddleDetection_YOLOSeries-develop\ppdet\modeling\architectures\meta_arch.py"", line 59, in forward
    out = self.get_loss()
  File ""E:\AI_Code\PaddleDetection_YOLOSeries-develop\ppdet\modeling\architectures\yolox.py"", line 105, in get_loss
    return self._forward()
  File ""E:\AI_Code\PaddleDetection_YOLOSeries-develop\ppdet\modeling\architectures\yolox.py"", line 95, in _forward
    yolox_losses = self.head(neck_feats, self.inputs)
  File ""D:\Users\Aorus\miniconda3\envs\paddle_env\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""D:\Users\Aorus\miniconda3\envs\paddle_env\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""E:\AI_Code\PaddleDetection_YOLOSeries-develop\ppdet\modeling\heads\yolo_head.py"", line 301, in forward
    ], targets)
  File ""E:\AI_Code\PaddleDetection_YOLOSeries-develop\ppdet\modeling\heads\yolo_head.py"", line 321, in get_loss
    pred_score, center_and_strides, pred_bbox, gt_box, gt_label)
  File ""E:\AI_Code\PaddleDetection_YOLOSeries-develop\ppdet\modeling\assigners\simota_assigner.py"", line 189, in __call__
    gt_bboxes)  # [num_points,num_gts]
  File ""E:\AI_Code\PaddleDetection_YOLOSeries-develop\ppdet\modeling\bbox_utils.py"", line 227, in batch_bbox_overlaps
    eps = paddle.to_tensor([eps])
  File ""D:\Users\Aorus\miniconda3\envs\paddle_env\lib\site-packages\decorator.py"", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File ""D:\Users\Aorus\miniconda3\envs\paddle_env\lib\site-packages\paddle\fluid\wrapped_decorator.py"", line 25, in __impl__
    return wrapped_func(*args, **kwargs)
  File ""D:\Users\Aorus\miniconda3\envs\paddle_env\lib\site-packages\paddle\fluid\framework.py"", line 434, in __impl__
    return func(*args, **kwargs)
  File ""D:\Users\Aorus\miniconda3\envs\paddle_env\lib\site-packages\paddle\tensor\creation.py"", line 189, in to_tensor
    stop_gradient=stop_gradient)
OSError: (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ..\paddle\phi\backends\gpu\cuda\cuda_info.cc:258)
"
deploy/python/infer.py 出现错误,PaddlePaddle/PaddleDetection,2022-08-17 05:57:19,3,,6672,1341206686,"### 问题描述 Please describe your issue

 python tools/export_model.py -c configs/solov2/solov2_r50_fpn_3x_coco.yml -o weights=output/solov2_r50_fpn_3x_coco/best_model.pdparams
用
 python deploy/python/infer.py --model_dir=output_inference/solov2_r50_fpn_3x_coco/ --image_file=../../datas/images/20.jpg出现如下错误，
环境是paddledetection2.2,
Package            Version
------------------ -------------
astor              0.8.1
Babel              2.10.3
bce-python-sdk     0.8.74
certifi            2022.6.15
charset-normalizer 2.1.0
click              8.1.3
cycler             0.11.0
Cython             0.29.32
cython-bbox        0.1.3
decorator          5.1.1
et-xmlfile         1.1.0
Flask              2.2.2
Flask-Babel        2.0.0
fonttools          4.34.4
future             0.18.2
idna               3.3
importlib-metadata 4.12.0
itsdangerous       2.1.2
Jinja2             3.1.2
joblib             1.1.0
kiwisolver         1.4.4
lap                0.4.0
MarkupSafe         2.1.1
matplotlib         3.5.3
motmetrics         1.2.5
numpy              1.23.1
opencv-python      4.6.0.66
openpyxl           3.0.10
opt-einsum         3.3.0
packaging          21.3
paddle-bfloat      0.1.7
paddledet          2.2.0
paddlepaddle-gpu   2.3.1.post116
pandas             1.4.3
Pillow             9.2.0
pip                22.1.2
protobuf           3.20.0
pycocotools        2.0.4
pycryptodome       3.15.0
pyparsing          3.0.9
python-dateutil    2.8.2
pytz               2022.1
PyYAML             6.0
requests           2.28.1
scikit-learn       1.1.2
scipy              1.9.0
setuptools         61.2.0
Shapely            1.8.2
six                1.16.0
sklearn            0.0
terminaltables     3.1.10
threadpoolctl      3.1.0
tqdm               4.64.0
typeguard          2.13.3
urllib3            1.26.11
visualdl           2.3.0
Werkzeug           2.2.2
wheel              0.37.1
xmltodict          0.13.0
zipp               3.8.1

-----------  Model Configuration -----------
Model Arch: SOLOv2
Transform Order:
--transform op: NormalizeImage
--transform op: Resize
--transform op: Permute
--transform op: PadStride
--------------------------------------------
Traceback (most recent call last):
  File ""/home/88/project/ASR/test/2.2/PaddleDetection/deploy/python/infer.py"", line 667, in <module>
    main()
  File ""/home/88/project/ASR/test/2.2/PaddleDetection/deploy/python/infer.py"", line 630, in main
    predict_image(detector, img_list, FLAGS.batch_size)
  File ""/home/88/project/ASR/test/2.2/PaddleDetection/deploy/python/infer.py"", line 544, in predict_image
    results = detector.predict(batch_image_list, FLAGS.threshold)
  File ""/home/88/project/ASR/test/2.2/PaddleDetection/deploy/python/infer.py"", line 258, in predict
    self.predictor.run()
IndexError: In user code:
OutOfRangeError: The element of Index must be less than the size of input dim size of axis which is 3872, but received index element which is 3872 in the 14 index.
      [Hint: Expected p_index[i] < input_size, but received p_index[i]:3872 >= input_size:3872.] (at /paddle/paddle/phi/kernels/funcs/gather.h:83)
      [operator < gather > error]
"
需要一个op能够计算points和polygons的位置关系,PaddlePaddle/PaddleDetection,2022-08-17 01:54:51,1,,6667,1341065518,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

1. 任务目标：要求最终得到points是否在polygongs的矩阵表示，论文Oriented RepPoints for Aerial Object Detection。 2. 在进行spatial loss计算的时候，论文要求算出预测点中不在对应groundtruthbox中的点，然后拿这些点与loss中心算l1损失; 3. 用CUDA实现点在多边形内外的判断，采用的是射线法判定交点个数。参考代码https://github.com/LiWentomng/OrientedRepPoints/tree/main/mmdet/ops/point_justify

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
训练中加载数据异常中止,PaddlePaddle/PaddleDetection,2022-08-17 01:36:47,1,,6666,1341057206,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

训练到第160个epoch程序就中止了，报错信息如下，load数据那里，看不懂细节原因


loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
{'freighter': 0, 'fishermen': 1, 'nonMotor': 2}
ERROR: Unexpected segmentation fault encountered in DataLoader workers.
Exception in thread Thread-162:
Traceback (most recent call last):
  File ""/root/new_mount_2/lyg/conda/envs/paddle_env/lib/python3.8/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 620, in _get_data
    data = self._data_queue.get(timeout=self._timeout)
  File ""/root/new_mount_2/lyg/conda/envs/paddle_env/lib/python3.8/multiprocessing/queues.py"", line 108, in get
Traceback (most recent call last):
  File ""tools/train.py"", line 196, in <module>
    raise Empty
_queue    .main()Empty
  File ""tools/train.py"", line 192, in main


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/root/new_mount_2/lyg/conda/envs/paddle_env/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
    run(FLAGS, cfg)
  File ""tools/train.py"", line 145, in run
    trainer.train(FLAGS.eval)
      File ""/home/paddleflow/storage/mnt/lyg/Projects/PaddleDetection/ppdet/engine/incre_trainer.py"", line 433, in train
self.run()
  File ""/root/new_mount_2/lyg/conda/envs/paddle_env/lib/python3.8/threading.py"", line 870, in run
    self._target(*self._args, **self._kwargs)
  File ""/root/new_mount_2/lyg/conda/envs/paddle_env/lib/python3.8/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 534, in _thread_loop
    batch = self._get_data()    
for step_id, data in enumerate(self.loader):  File ""/root/new_mount_2/lyg/conda/envs/paddle_env/lib/python3.8/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 635, in _get_data

  File ""/home/paddleflow/storage/mnt/lyg/Projects/PaddleDetection/ppdet/data/reader.py"", line 209, in __next__
    raise RuntimeError(""DataLoader {} workers exit unexpectedly, "" \
RuntimeError: DataLoader 1 workers exit unexpectedly, pids: 25225
    return next(self.loader)
  File ""/root/new_mount_2/lyg/conda/envs/paddle_env/lib/python3.8/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 746, in __next__
    data = self._reader.read_next_var_list()
SystemError: (Fatal) Blocking queue is killed because the data reader raises an exception.
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] (at /paddle/paddle/fluid/operators/reader/blocking_queue.h:166)

"
基于PaddleDetection的人员摔倒识别,PaddlePaddle/PaddleDetection,2022-08-16 09:22:42,1,,6662,1340061521,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

您好，我目前正在尝试做一个基于关键点检测摔倒的项目。
在使用picodet_s_192_pedestrian进行行人检测和用tinypose_128x96进行关键点检测之后，我使用“基于PaddleDetection的人员摔倒识别”提供的方案进行添加摔倒字幕添加后处理。
在正常执行完读取关键点结果后放入判断文件这个步骤之后，使用自己的文件执行videovis函数时会报错

fps: 25, frame_count: 284
---------------------------------------------------------------------------IndexError                                Traceback (most recent call last)/tmp/ipykernel_97/2477555930.py in <module>
     12 
     13 #4）根据检测的摔倒帧在视频显示
---> 14 videovis(videof, kpts_data, fallframes)
~/source.py in videovis(videof, posedata, fallframes)
    172         if not ret:
    173             break
--> 174         frameid, rects, kpts_all = posedata[index]
    175         if index in fallframes:
    176             fallmark = True
IndexError: list index out of range

按说已经正常的生成了判断文件，放入字幕的函数里面又不涉及数组创建函数，是不应该报数组下标错误的。所以想请教一下我还应该修改哪些参数。
附上folk链接 

 基于关键点检测的摔倒识别_副本2：https://aistudio.baidu.com/aistudio/projectdetail/4435736?sUid=2498778&shared=1&ts=1660641679614

### 复现环境 Environment

_No response_

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
ppyoloe垂类模型训练报错,PaddlePaddle/PaddleDetection,2022-08-16 08:20:08,1,,6661,1339987419,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

使用ppyoloe垂类模型训练：
```python
python tools/train.py -c configs/visdrone/ppyoloe_crn_l_alpha_largesize_80e_visdrone.yml --use_vdl=True --vdl_log_dir=./visdrone/ --eval
```
一段时间后，报错如下信息：
```python
W0816 15:37:44.290258  8489 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1
W0816 15:37:44.294898  8489 gpu_resources.cc:91] device: 0, cuDNN Version: 7.6.
[08/16 15:37:47] ppdet.utils.checkpoint INFO: The shape [10] in pretrained weight yolo_head.pred_cls.0.bias is unmatched with the shape [3] in model yolo_head.pred_cls.0.bias. And the weight yolo_head.pred_cls.0.bias will not be loaded
[08/16 15:37:47] ppdet.utils.checkpoint INFO: The shape [10, 768, 3, 3] in pretrained weight yolo_head.pred_cls.0.weight is unmatched with the shape [3, 768, 3, 3] in model yolo_head.pred_cls.0.weight. And the weight yolo_head.pred_cls.0.weight will not be loaded
[08/16 15:37:47] ppdet.utils.checkpoint INFO: The shape [10] in pretrained weight yolo_head.pred_cls.1.bias is unmatched with the shape [3] in model yolo_head.pred_cls.1.bias. And the weight yolo_head.pred_cls.1.bias will not be loaded
[08/16 15:37:47] ppdet.utils.checkpoint INFO: The shape [10, 384, 3, 3] in pretrained weight yolo_head.pred_cls.1.weight is unmatched with the shape [3, 384, 3, 3] in model yolo_head.pred_cls.1.weight. And the weight yolo_head.pred_cls.1.weight will not be loaded
[08/16 15:37:47] ppdet.utils.checkpoint INFO: The shape [10] in pretrained weight yolo_head.pred_cls.2.bias is unmatched with the shape [3] in model yolo_head.pred_cls.2.bias. And the weight yolo_head.pred_cls.2.bias will not be loaded
[08/16 15:37:47] ppdet.utils.checkpoint INFO: The shape [10, 192, 3, 3] in pretrained weight yolo_head.pred_cls.2.weight is unmatched with the shape [3, 192, 3, 3] in model yolo_head.pred_cls.2.weight. And the weight yolo_head.pred_cls.2.weight will not be loaded
[08/16 15:37:47] ppdet.utils.checkpoint INFO: Finish loading model weights: /home/aistudio/.cache/paddle/weights/ppyoloe_crn_l_alpha_largesize_80e_visdrone.pdparams
[08/16 15:37:49] ppdet.engine INFO: Epoch: [0] [  0/622] learning_rate: 0.000000 loss: 3.933158 loss_cls: 1.887929 loss_iou: 0.569066 loss_dfl: 1.245128 loss_l1: 4.728155 eta: 1 day, 2:14:35 batch_cost: 1.8986 data_cost: 0.0037 ips: 1.0534 images/s
[08/16 15:38:42] ppdet.engine INFO: Epoch: [0] [100/622] learning_rate: 0.000402 loss: 3.658453 loss_cls: 1.755152 loss_iou: 0.403740 loss_dfl: 1.681514 loss_l1: 1.478678 eta: 6:15:43 batch_cost: 0.4395 data_cost: 0.0156 ips: 4.5505 images/s
[08/16 15:39:35] ppdet.engine INFO: Epoch: [0] [200/622] learning_rate: 0.000804 loss: 4.048609 loss_cls: 1.679811 loss_iou: 0.512568 loss_dfl: 2.193413 loss_l1: 2.195704 eta: 6:03:42 batch_cost: 0.4266 data_cost: 0.0003 ips: 4.6887 images/s
[08/16 15:40:30] ppdet.engine INFO: Epoch: [0] [300/622] learning_rate: 0.001206 loss: 4.098348 loss_cls: 1.700503 loss_iou: 0.534061 loss_dfl: 2.175586 loss_l1: 2.364273 eta: 6:06:31 batch_cost: 0.4533 data_cost: 0.0047 ips: 4.4121 images/s
[08/16 15:41:21] ppdet.engine INFO: Epoch: [0] [400/622] learning_rate: 0.001608 loss: 3.960057 loss_cls: 1.590578 loss_iou: 0.513343 loss_dfl: 2.123349 loss_l1: 2.231357 eta: 5:59:27 batch_cost: 0.4138 data_cost: 0.0147 ips: 4.8332 images/s
[08/16 15:42:14] ppdet.engine INFO: Epoch: [0] [500/622] learning_rate: 0.002010 loss: 4.493289 loss_cls: 1.991162 loss_iou: 0.536207 loss_dfl: 2.378226 loss_l1: 2.770939 eta: 5:57:47 batch_cost: 0.4312 data_cost: 0.0017 ips: 4.6384 images/s
[08/16 15:43:08] ppdet.engine INFO: Epoch: [0] [600/622] learning_rate: 0.002412 loss: 3.035724 loss_cls: 0.000746 loss_iou: 0.622485 loss_dfl: 2.449761 loss_l1: 3.216350 eta: 5:57:09 batch_cost: 0.4365 data_cost: 0.0062 ips: 4.5815 images/s
[08/16 15:43:20] ppdet.engine INFO: Epoch: [1] [  0/622] learning_rate: 0.002500 loss: 2.863246 loss_cls: 0.000187 loss_iou: 0.644130 loss_dfl: 2.454205 loss_l1: 3.360621 eta: 5:56:50 batch_cost: 0.4333 data_cost: 0.0109 ips: 4.6159 images/s
[08/16 15:44:15] ppdet.engine INFO: Epoch: [1] [100/622] learning_rate: 0.002500 loss: 2.525643 loss_cls: 0.000281 loss_iou: 0.598970 loss_dfl: 2.293157 loss_l1: 2.505533 eta: 5:57:27 batch_cost: 0.4476 data_cost: 0.0063 ips: 4.4686 images/s
[08/16 15:45:08] ppdet.engine INFO: Epoch: [1] [200/622] learning_rate: 0.002500 loss: 2.639079 loss_cls: 0.000320 loss_iou: 0.595111 loss_dfl: 2.137939 loss_l1: 2.533323 eta: 5:56:08 batch_cost: 0.4315 data_cost: 0.0148 ips: 4.6350 images/s
[08/16 15:46:02] ppdet.engine INFO: Epoch: [1] [300/622] learning_rate: 0.002500 loss: 2.396768 loss_cls: 0.000764 loss_iou: 0.577287 loss_dfl: 2.014158 loss_l1: 2.146335 eta: 5:56:06 batch_cost: 0.4445 data_cost: 0.0009 ips: 4.4993 images/s
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [30324], but received [94671165246424].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [30324], but received [94671165246424].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [30324], but received [94671165246424].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [30324], but received [94671165246424].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [30324], but received [94671165246424].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [30324], but received [94671165246424].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [30324], but received [94671165246424].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [30324], but received [94671165246424].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [30324], but received [94671165246424].
```
使用的数据集：
https://aistudio.baidu.com/aistudio/projectdetail/4408882?channelType=0&channel=0&sUid=90149&ts=1660637966846

### 复现环境 Environment

- PaddlePaddle 2.3
- PaddleDetection develop
- AI Studio高级版

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
python tools/x2coco.py 报错,PaddlePaddle/PaddleDetection,2022-08-16 07:45:03,2,,6659,1339948081,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

你好，我不知道为什么voc转coco脚本会报错。它只报错在有些图片上但我检查了标注好像没有问题。

Traceback (most recent call last):
  File ""tools/x2coco.py"", line 542, in <module>
    main()
  File ""tools/x2coco.py"", line 431, in main
    voc_xmls_to_cocojson(
  File ""tools/x2coco.py"", line 269, in voc_xmls_to_cocojson
    ann = voc_get_coco_annotation(obj=obj, label2id=label2id)
  File ""tools/x2coco.py"", line 237, in voc_get_coco_annotation
    assert xmax > xmin and ymax > ymin, ""Box size error.""
AssertionError: Box size error.


### 复现环境 Environment

_No response_

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
FatalError: `Erroneous arithmetic operation` is detected by the operating system.,PaddlePaddle/PaddleDetection,2022-08-16 02:22:12,1,,6657,1339724970,"### 问题描述 Please describe your issue

基于ppdet yolov3训练报错如下：

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::imperative::Tracer::TraceOp(std::string const&, paddle::imperative::NameVarBaseMap const&, paddle::imperative::NameVarBaseMap const&, paddle::framework::AttributeMap, std::map<std::string, std::string, std::less<std::string >, std::allocator<std::pair<std::string const, std::string > > > const&)
1   paddle::imperative::Tracer::TraceOp(std::string const&, paddle::imperative::NameVarBaseMap const&, paddle::imperative::NameVarBaseMap const&, paddle::framework::AttributeMap, paddle::platform::Place const&, bool, std::map<std::string, std::string, std::less<std::string >, std::allocator<std::pair<std::string const, std::string > > > const&)
2   paddle::imperative::PreparedOp::Run(paddle::imperative::NameVarBaseMap const&, paddle::imperative::NameVarBaseMap const&, paddle::framework::AttributeMap const&)
3   std::_Function_handler<void (paddle::framework::ExecutionContext const&), paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CUDAPlace, false, 1ul, paddle::operators::SplitOpKernel<paddle::platform::CUDADeviceContext, double>, paddle::operators::SplitOpKernel<paddle::platform::CUDADeviceContext, float>, paddle::operators::SplitOpKernel<paddle::platform::CUDADeviceContext, long>, paddle::operators::SplitOpKernel<paddle::platform::CUDADeviceContext, int>, paddle::operators::SplitOpKernel<paddle::platform::CUDADeviceContext, bool>, paddle::operators::SplitOpKernel<paddle::platform::CUDADeviceContext, paddle::platform::float16> >::operator()(char const*, char const*, int) const::{lambda(paddle::framework::ExecutionContext const&)#1}>::_M_invoke(std::_Any_data const&, paddle::framework::ExecutionContext const&)
4   paddle::operators::SplitOpKernel<paddle::platform::CUDADeviceContext, float>::Compute(paddle::framework::ExecutionContext const&) const
5   paddle::operators::math::SplitFunctor<paddle::platform::CUDADeviceContext, float>::operator()(paddle::platform::CUDADeviceContext const&, paddle::framework::Tensor const&, std::vector<paddle::framework::Tensor const*, std::allocator<paddle::framework::Tensor const*> > const&, int, std::vector<paddle::framework::Tensor*, std::allocator<paddle::framework::Tensor*> >*)
6   paddle::framework::SignalHandle(char const*, int)
7   paddle::platform::GetCurrentTraceBackString[abi:cxx11]()

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1660599751 (unix time) try ""date -d @1660599751"" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd7f781054f) received by PID 73816 (TID 0x7fd83a7ee740) from PID 18446744073567012175 ***]"
最新的2.4的readme.md中，PP-Human的链接是无效的，点击显示page not found,PaddlePaddle/PaddleDetection,2022-08-15 23:35:51,1,,6656,1339638955,"### 文档链接&描述 Document Links & Description

如题

### 请提出你的建议 Please give your suggestion

只能官方人员矫正链接"
在paddledetection中，自制数据集VOC格式转YOLO格式,PaddlePaddle/PaddleDetection,2022-08-15 12:08:00,2,,6655,1338920545,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

为什么在自制数据集最后一步运行官方提供的create_list.py文件，得到的是两个空白的txt文件"
训练卡住不动,PaddlePaddle/PaddleDetection,2022-08-15 07:41:40,6,,6650,1338652412,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

自定义一个数据集，3类，进行训练，训练到第9个epoch时，卡住不动，训练进程里面cpu占用为0，内存占用也为0，手动关掉后显示：
SystemError: (Fatal) Blocking queue is killed because the data reader raises an exception。
看报错，是读取数据有问题，如果说数据有问题，那为什么到第9个epoch才显示，另外，有没有什么建议，需要怎么调试，谢谢！

### 复现环境 Environment

_No response_

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
PPTracking 多相机追踪,PaddlePaddle/PaddleDetection,2022-08-15 07:36:38,8,,6649,1338648406,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

```
ffmpeg processing of video ./mtmct-demo/c004.mp4
start tracking seq: c004
Tracking frame: 0
Tracking frame: 10
Tracking frame: 20
Tracking frame: 30
Tracking frame: 40
Tracking frame: 50
Tracking frame: 60
Tracking frame: 70
Tracking frame: 80
Tracking frame: 90
Tracking frame: 100
Traceback (most recent call last):
  File ""deploy/pptracking/python/mot_sde_infer.py"", line 755, in <module>
    main()
  File ""deploy/pptracking/python/mot_sde_infer.py"", line 724, in main
    detector.predict_mtmct(FLAGS.mtmct_dir, mtmct_cfg)
  File ""deploy/pptracking/python/mot_sde_infer.py"", line 656, in predict_mtmct
    map_tid = sub_cluster(
  File ""/home/nodes/Database/deepleaning/gitclone/PaddleDetection-release-2.4/deploy/pptracking/python/mot/mtmct/postprocess.py"", line 114, in sub_cluster
    clu = get_labels(
  File ""/home/nodes/Database/deepleaning/gitclone/PaddleDetection-release-2.4/deploy/pptracking/python/mot/mtmct/utils.py"", line 532, in get_labels
    sim_matrix = get_sim_matrix(
  File ""/home/nodes/Database/deepleaning/gitclone/PaddleDetection-release-2.4/deploy/pptracking/python/mot/mtmct/utils.py"", line 509, in get_sim_matrix
    q_arr = normalize(q_arr, axis=1)
  File ""/home/nodes/Database/deepleaning/gitclone/PaddleDetection-release-2.4/deploy/pptracking/python/mot/mtmct/utils.py"", line 377, in normalize
    nparray = preprocessing.normalize(nparray, norm='l2', axis=axis)
  File ""/home/nodes/Database/deepleaning/gitclone/PaddleDetection-release-2.4/.venv/lib/python3.8/site-packages/sklearn/preprocessing/_data.py"", line 1786, in normalize
    X = check_array(
  File ""/home/nodes/Database/deepleaning/gitclone/PaddleDetection-release-2.4/.venv/lib/python3.8/site-packages/sklearn/utils/validation.py"", line 879, in check_array
    raise ValueError(
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
```

### 复现环境 Environment

_No response_

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
关于tingpose部署运行时报错,PaddlePaddle/PaddleDetection,2022-08-15 05:45:15,1,,6648,1338567382,"### 问题描述 Please describe your issue

进行关键点检测时，如果使用命令行运行（cmd）进行参数输入就没错，如果进行用函数运行就错了 
![Uploading image.png…]()
"
ValueError: The additional config (onnx_format) of `paddle.jit.save` is not supported.,PaddlePaddle/PaddleDetection,2022-08-15 04:26:11,1,,6647,1338523517,"### 问题描述 Please describe your issue

# Weights of yolov3_mobilenet_v1_voc
pretrain_weights: /root/autodl-tmp/pd/output/yolov3_mobilenet_v3_large_270e_coco/model_final.pdparams
slim: PrunerQAT

PrunerQAT:
  criterion: fpgm
  pruned_params: ['conv2d_52.w_0', 'conv2d_53.w_0', 'conv2d_54.w_0',
                  'conv2d_55.w_0', 'conv2d_56.w_0', 'conv2d_57.w_0',
                  'conv2d_59.w_0', 'conv2d_60.w_0', 'conv2d_61.w_0',
                  'conv2d_62.w_0', 'conv2d_63.w_0', 'conv2d_64.w_0',
                  'conv2d_66.w_0', 'conv2d_67.w_0', 'conv2d_68.w_0',
                  'conv2d_69.w_0', 'conv2d_70.w_0', 'conv2d_71.w_0']
  pruned_ratios: [0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.875,0.875,0.875,0.875,0.875,0.875]
  print_prune_params: False
  quant_config: {
    'weight_quantize_type': 'channel_wise_abs_max', 'activation_quantize_type': 'moving_average_abs_max',
    'weight_bits': 8, 'activation_bits': 8, 'dtype': 'int8', 'window_size': 10000, 'moving_rate': 0.9,
    'quantizable_layer_type': ['Conv2D', 'Linear']}
  print_qat_model: False

改写的剪枝量化模型，训练正常。在导出模型时，出现了：
[08/15 12:21:06] ppdet.engine INFO: Export inference config file to /root/autodl-tmp/infer/yolov3_mobilenetv3_prune_qat/yolov3_mobilenetv3_prune_qat/infer_cfg.yml
Traceback (most recent call last):
  File ""tools/export_model.py"", line 108, in <module>
    main()
  File ""tools/export_model.py"", line 104, in main
    run(FLAGS, cfg)
  File ""tools/export_model.py"", line 73, in run
    trainer.export(FLAGS.output_dir)
  File ""/root/autodl-tmp/pd/ppdet/engine/trainer.py"", line 869, in export
    self.cfg.slim.save_quantized_model(
  File ""/root/autodl-tmp/pd/ppdet/slim/prune.py"", line 150, in save_quantized_model
    self.quanter.save_quantized_model(
  File ""/root/miniconda3/lib/python3.8/site-packages/paddleslim/dygraph/quant/qat.py"", line 289, in save_quantized_model
    self.imperative_qat.save_quantized_model(
  File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/contrib/slim/quantization/imperative/qat.py"", line 262, in save_quantized_model
    self._quantize_outputs.save_quantized_model(layer, path, input_spec,
  File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/contrib/slim/quantization/imperative/qat.py"", line 465, in save_quantized_model
    paddle.jit.save(layer=model, path=path, input_spec=input_spec, **config)
  File ""/root/miniconda3/lib/python3.8/site-packages/decorator.py"", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/wrapped_decorator.py"", line 25, in __impl__
    return wrapped_func(*args, **kwargs)
  File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/base.py"", line 51, in __impl__
    return func(*args, **kwargs)
  File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/jit.py"", line 731, in save
    configs = _parse_save_configs(configs)
  File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/jit.py"", line 376, in _parse_save_configs
    raise ValueError(
ValueError: The additional config (onnx_format) of `paddle.jit.save` is not supported.



导出代码如下：
!# 导出YOLOv3模型
!export CUDA_VISIBLE_DEVICES=0 #windows和Mac下不需要执行该命令
%cd /root/autodl-tmp/pd
!python tools/export_model.py -c configs/yolov3/yolov3_mobilenet_v3_large_270e_coco.yml \
     --slim_config configs/slim/extensions/yolov3_mobilenetv3_prune_qat.yml\
     --output_dir=/root/autodl-tmp/infer/yolov3_mobilenetv3_prune_qat \
     -o weights=output/yolov3_mobilenetv3_prune_qat/model_final.pdparams"
能否提供一下官方PPYOLOE在COCO数据集上的训练日志？,PaddlePaddle/PaddleDetection,2022-08-15 04:00:43,4,,6646,1338505054,"### 问题描述 Please describe your issue

能否提供一下官方PPYOLOE在COCO数据集上的训练日志？最近在基于COCO训练时发现loss cls突然增加的问题，想和官方训练日志进行比对发现问题，如果能提供最好能包含s模型的训练日志。"
PP-Tracking预测报错AttributeError: 'OCSORTTracker' object has no attribute 'input_size',PaddlePaddle/PaddleDetection,2022-08-13 12:43:35,3,,6644,1337951146,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

用导出的PPYOLOe行人检测模型和PPLCNet ReID模型
python deploy/pptracking/python/mot_sde_infer.py --model_dir=output_inference/ppyoloe_crn_l_36e_640x640_mot17half/ --reid_model_dir=output_inference/deepsort_pplcnet/ --tracker_config=deploy/pptracking/python/tracker_config.yml --video_file=est_demo.mp4 --device=GPU --threshold=0.5 --save_mot_txts --save_images


Traceback (most recent call last):
  File ""mot_sde_infer.py"", line 848, in <module>
    main()
  File ""mot_sde_infer.py"", line 813, in main
    detector.predict_video(FLAGS.video_file, FLAGS.camera_id)
  File ""mot_sde_infer.py"", line 606, in predict_video
    [frame], visual=False, seq_name=seq_name)
  File ""mot_sde_infer.py"", line 488, in predict_image
    det_result = self.reidprocess(det_result)
  File ""mot_sde_infer.py"", line 257, in reidprocess
    w, h = self.tracker.input_size
AttributeError: 'OCSORTTracker' object has no attribute 'input_size'

### 复现环境 Environment

win10，python3.7，cuda10.1，PaddleDetection2.4

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
The FLOPs Difference of PP-YOLOE series,PaddlePaddle/PaddleDetection,2022-08-13 09:57:23,1,,6643,1337919117,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

I tried to calculate flops & params of ppyoloe models, regarding https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/configs/ppyoloe,

```shell
for m in 's' 'm' 'l' 'x';do
    python tools/eval.py -c configs/ppyoloe/ppyoloe_crn_${m}_300e_coco.yml -o weights=ppyoloe_crn_${m}_300e_coco.pdparams
done
```

With `paddleslim` installed and added `print_flops: true` to `configs/ppyoloe/_base_/ppyoloe_crn.yml`, I obtain only FLOPs of the above models, which have big gaps with those reported. Am I doing something wrong? Could you spare some time looking into this issue?

```
[08/13 17:21:46] ppdet.utils.checkpoint INFO: Finish loading model weights: ppyoloe_crn_s_300e_coco.pdparams
loading annotations into memory...
Done (t=0.66s)
creating index...
index created!
<class 'ppdet.modeling.architectures.yolo.YOLOv3'>
[08/13 17:22:19] ppdet.engine INFO:  Model FLOPs : 8.845220G. (image shape is [1, 3, 640, 640])

[08/13 17:30:18] ppdet.utils.checkpoint INFO: Finish loading model weights: ppyoloe_crn_m_300e_coco.pdparams
loading annotations into memory...
Done (t=0.60s)
creating index...
index created!
<class 'ppdet.modeling.architectures.yolo.YOLOv3'>
[08/13 17:30:49] ppdet.engine INFO:  Model FLOPs : 25.701527G. (image shape is [1, 3, 640, 640])

[08/13 17:38:55] ppdet.utils.checkpoint INFO: Finish loading model weights: ppyoloe_crn_l_300e_coco.pdparams
loading annotations into memory...
Done (t=0.60s)
creating index...
index created!
<class 'ppdet.modeling.architectures.yolo.YOLOv3'>
[08/13 17:39:27] ppdet.engine INFO:  Model FLOPs : 57.011041G. (image shape is [1, 3, 640, 640])

[08/13 17:48:14] ppdet.utils.checkpoint INFO: Finish loading model weights: ppyoloe_crn_x_300e_coco.pdparams
loading annotations into memory...
Done (t=0.51s)
creating index...
index created!
<class 'ppdet.modeling.architectures.yolo.YOLOv3'>
[08/13 17:48:46] ppdet.engine INFO:  Model FLOPs : 107.396507G. (image shape is [1, 3, 640, 640])
```

BTW, how to compute params?"
Paddle_Inference下C++推理PPHuman模型问题,PaddlePaddle/PaddleDetection,2022-08-13 06:37:24,6,,6642,1337876237,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

Paddle_Inference下C++推理PPHuman模型问题

1、采用PPHuman发布的模型mot_ppyoloe_l_36e_pipeline，下载地址：
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.4/deploy/pphuman/README_en.md

2、采用Paddle_Inference读取模型并分析图片，采用的代码是实例代码：
Paddle-Inference-Demo\c++\cpu\yolov3\yolov3_test.cc

3、采用的分析图片如下：
![15ac09e8821f8803dc2c927bf6326080](https://user-images.githubusercontent.com/62933135/184472014-0f3cd0d1-3f2a-42fc-a859-bd3ed4d14130.jpeg)


4、整个分析过程看不出问题，推理没有异常，output_names/shape都是正确的，但是到
int output_size = std::accumulate(output_shape.begin(), output_shape.end(), 1,
        std::multiplies<int>());
output_size == 0，这里是为什么是0 ？？？？？？？？？？？？？？？



"
metrics打印结果有误,PaddlePaddle/PaddleDetection,2022-08-12 17:00:35,2,,6641,1337470237,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

验证类别只有一个类，保存的json文件category_id全是2，nonMotor，
<img width=""1093"" alt=""image"" src=""https://user-images.githubusercontent.com/40205682/184407736-0cedfbf8-86c5-40a4-89d6-a535d3936f9a.png"">
但是指标在freighter上；
<img width=""426"" alt=""image"" src=""https://user-images.githubusercontent.com/40205682/184407493-ad792666-c681-4bd5-b370-5cf8960c9af7.png"">


### 复现环境 Environment

_No response_

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
背景类的索引？,PaddlePaddle/PaddleDetection,2022-08-12 15:52:37,1,,6640,1337376203,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

在Faster rcnn里面，背景类上放在最后一个filter上，还是第0个filter，代码看了好久找不到"
 .\main --model_dir=model --image_file --camera_id 0,PaddlePaddle/PaddleDetection,2022-08-12 07:25:08,1,,6639,1336851420,报错[ WARN:0@2.101] global c:\build\master_winpack-build-win64-vc15\opencv\modules\imgcodecs\src\loadsave.cpp (239) cv::findDecoder imread_('--camera_id'): can't open/read file: check file path/integrity是怎么回事？
在PP-Picodet的检测头中，为什么没有前景和背景信息？如下图所示,PaddlePaddle/PaddleDetection,2022-08-12 02:14:13,1,,6636,1336672210,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

在PP-Picodet的检测头中，为什么没有前景和背景信息？如下图所示
![image](https://user-images.githubusercontent.com/78945582/184272447-d60287a2-b389-4f37-b9f2-cb2a88f5675f.png)
"
基于PP-YOLOE的知识蒸馏方法和代码有吗？,PaddlePaddle/PaddleDetection,2022-08-12 02:10:39,1,,6635,1336670367,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

基于PP-YOLOE的知识蒸馏方法和代码有吗？多谢专家"
如何将两个onnx模型合并为1个onnx模型（均基于PP-YOLOE-S）,PaddlePaddle/PaddleDetection,2022-08-12 01:49:32,1,,6634,1336660697,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

当前有两个基于PP-YOLOE-S训练的模型，一个是检测玩手机的，一个是检测吸烟的，如何将这两个模型的onnx合并，从而得到一个既可以检测玩手机也可以检测吸烟的模型，避免重新训练"
Paddle + TensorRT 支持多线程吗,PaddlePaddle/PaddleDetection,2022-08-10 03:40:14,2,,6620,1334023385,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

在qt中使用多线程时，第一次开启线程推理可以正常执行，第二次开启线程会出现下面的错误。而在使用GPU推理时则完全没有问题
![image](https://user-images.githubusercontent.com/50402380/183807888-32fc16f9-8a03-42d7-a81d-754855893c01.png)


"
Target 4578971296541297966 is out of upper bound错误,PaddlePaddle/PaddleDetection,2022-08-10 03:09:23,2,,6619,1334000141,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

复现环境：
win10系统
cuda 11.6
cudnn 8.4
python 3.9
paddlepaddle-gpu==2.3.1.post116 windows版本
按照教程进行fairmot目标跟踪训练，发生如下报错：
Traceback (most recent call last):
  File ""D:\Project_pptracking\PaddleDetection-release-2.4\tools\train.py"", line 177, in <module>
    main()
  File ""D:\Project_pptracking\PaddleDetection-release-2.4\tools\train.py"", line 173, in main
    run(FLAGS, cfg)
  File ""D:\Project_pptracking\PaddleDetection-release-2.4\tools\train.py"", line 127, in run
    trainer.train(FLAGS.eval)
  File ""D:\Project_pptracking\PaddleDetection-release-2.4\ppdet\engine\trainer.py"", line 454, in train
    outputs = model(data)
  File ""D:\Anaconda3\envs\pp-tracking\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""D:\Anaconda3\envs\pp-tracking\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func    outputs = self.forward(*inputs, **kwargs)
  File ""D:\Project_pptracking\PaddleDetection-release-2.4\ppdet\modeling\architectures\meta_arch.py"", line 59, in forward
    out = self.get_loss()
  File ""D:\Project_pptracking\PaddleDetection-release-2.4\ppdet\modeling\architectures\fairmot.py"", line 99, in get_loss    loss = self._forward()
  File ""D:\Project_pptracking\PaddleDetection-release-2.4\ppdet\modeling\architectures\fairmot.py"", line 78, in _forward    reid_loss = self.reid(neck_feat, self.inputs)
  File ""D:\Anaconda3\envs\pp-tracking\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""D:\Anaconda3\envs\pp-tracking\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func    outputs = self.forward(*inputs, **kwargs)
  File ""D:\Project_pptracking\PaddleDetection-release-2.4\ppdet\modeling\reid\fairmot_embedding_head.py"", line 115, in forward
    loss = self.get_loss(reid_feat, inputs)
  File ""D:\Project_pptracking\PaddleDetection-release-2.4\ppdet\modeling\reid\fairmot_embedding_head.py"", line 163, in get_loss
    loss = self.reid_loss(logit, target)
  File ""D:\Anaconda3\envs\pp-tracking\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""D:\Anaconda3\envs\pp-tracking\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func    outputs = self.forward(*inputs, **kwargs)
  File ""D:\Anaconda3\envs\pp-tracking\lib\site-packages\paddle\nn\layer\loss.py"", line 397, in forward
    ret = paddle.nn.functional.cross_entropy(
  File ""D:\Anaconda3\envs\pp-tracking\lib\site-packages\paddle\nn\functional\loss.py"", line 1722, in cross_entropy
    raise ValueError(""Target {} is out of upper bound."".format(
ValueError: Target 4578971296541297966 is out of upper bound."
训练一直 ap为0，还有loss是nan的已经把lr改成很小了0.000125，单gpu训练,PaddlePaddle/PaddleDetection,2022-08-10 02:34:00,4,,6618,1333981112,"[08/10 10:30:54] ppdet.engine INFO: Epoch: [9] [360/437] learning_rate: 0.000125 loss_xy: nan loss_wh: nan loss_obj: nan loss_cls: nan loss: nan eta: 10:58:56 batch_cost: 0.2701 data_cost: 0.0811 ips: 29.6175 images/s
[08/10 10:30:59] ppdet.engine INFO: Epoch: [9] [380/437] learning_rate: 0.000125 loss_xy: nan loss_wh: nan loss_obj: nan loss_cls: nan loss: nan eta: 10:57:21 batch_cost: 0.2523 data_cost: 0.0597 ips: 31.7112 images/s
[08/10 10:31:04] ppdet.engine INFO: Epoch: [9] [400/437] learning_rate: 0.000125 loss_xy: nan loss_wh: nan loss_obj: nan loss_cls: nan loss: nan eta: 10:55:55 batch_cost: 0.2744 data_cost: 0.0550 ips: 29.1509 images/s
[08/10 10:31:10] ppdet.engine INFO: Epoch: [9] [420/437] learning_rate: 0.000125 loss_xy: nan loss_wh: nan loss_obj: nan loss_cls: nan loss: nan eta: 10:54:28 batch_cost: 0.2718 data_cost: 0.0697 ips: 29.4309 images/s
[08/10 10:31:17] ppdet.utils.checkpoint INFO: Save checkpoint: output/yolov3_darknet53_270e_voc
[08/10 10:31:17] ppdet.engine INFO: Eval iter: 0
[08/10 10:31:19] ppdet.engine INFO: Eval iter: 100
[08/10 10:31:22] ppdet.engine INFO: Eval iter: 200
[08/10 10:31:24] ppdet.engine INFO: Eval iter: 300
[08/10 10:31:27] ppdet.engine INFO: Eval iter: 400
[08/10 10:31:29] ppdet.engine INFO: Eval iter: 500
[08/10 10:31:32] ppdet.engine INFO: Eval iter: 600
[08/10 10:31:34] ppdet.engine INFO: Eval iter: 700
[08/10 10:31:37] ppdet.engine INFO: Eval iter: 800
[08/10 10:31:39] ppdet.engine INFO: Eval iter: 900
[08/10 10:31:42] ppdet.metrics.metrics INFO: Accumulating evaluatation results...
[08/10 10:31:42] ppdet.metrics.metrics INFO: mAP(0.50, 11point) = 0.00%
[08/10 10:31:42] ppdet.engine INFO: Total sample number: 1000, averge FPS: 40.33762743732353
[08/10 10:31:42] ppdet.engine INFO: Best test bbox ap is 0.000."
deploy/python/infer.py,PaddlePaddle/PaddleDetection,2022-08-10 01:51:54,2,,6617,1333959110,"python deploy/python/infer.py 出现下列错误
Traceback (most recent call last):
  File ""deploy/python/infer.py"", line 909, in <module>
    main()
  File ""deploy/python/infer.py"", line 856, in main
    detector = eval(detector_func)(
  File ""deploy/python/infer.py"", line 120, in __init__
    self.predictor, self.config = load_predictor(
  File ""deploy/python/infer.py"", line 770, in load_predictor
    predictor = create_predictor(config)
ValueError: (InvalidArgument) The inverse of Fused batch norm variance should be finite. Found nonfinite values! Please check batch_norm2d_0.w_2
  [Hint: Expected std::isfinite(variance_array[i]) == true, but received std::isfinite(variance_array[i]):0 != true:1.] (at /paddle/paddle/fluid/framework/ir/conv_bn_fuse_pass.cc:105)
"
win10无法编译CPU版本的C++推理代码,PaddlePaddle/PaddleDetection,2022-08-09 13:48:12,0,,6615,1333288541,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

我使用vs2019和opencv3.4.6 在2.4版本和2.3版本的c++推理代码上进行了编译。这两个版本均无法通过编译
复现方法：
    下载 https://paddle-inference-lib.bj.bcebos.com/2.3.1/cxx_c/Windows/CPU/x86-64_vs2017_avx_mkl/paddle_inference.zip 这个预测库，并解压源码后，使用  cmake . -G ""Visual Studio 16 2019"" -A x64 -T host=x64 -DWITH_GPU=OFF -DWITH_MKL=ON -DCMAKE_BUILD_TYPE=Release -DPADDLE_DIR=D:\cpp\project\paddle_inference -DPADDLE_LIB_NAME=paddle_inference -DOPENCV_DIR=D:\cpp\opencv3_4_6 -DWITH_KEYPOINT=ON 命令生成项目文件，结果如下：
![cmake](https://user-images.githubusercontent.com/56796761/183664689-0d35cae6-b219-4a28-8f7b-172310604007.JPG)


随后在vs2019下进行编译
在2.3版本上出现下面的错误
![cpp_make error2 3](https://user-images.githubusercontent.com/56796761/183663078-13969e4d-5be2-43d2-b7e7-4943aad239bd.JPG)
在2.4版本上出现下面的错误
![cpp_make error](https://user-images.githubusercontent.com/56796761/183663257-1ae82551-8fd5-450f-9c7c-72a00c210df0.JPG)


### 复现环境 Environment

PaddleDetection:2.3/2.4
CPU：i5-9300H
GPU:  无
CUDA：无
paddle_inference文件路径：D:\cpp\project\paddle_inference
PaddleDetection文件路径：D:\cpp\project\PaddleDetection
opencv文件路径：D:\cpp\opencv3_4_6
vs2019配置如下
![vs](https://user-images.githubusercontent.com/56796761/183665630-2b2bdf9b-83ca-4f53-ab8c-c607c335cf6a.JPG)


### 是否愿意提交PR Are you willing to submit a PR?

- [x] Yes I'd like to help by submitting a PR!"
KeyError: 'target0' ,PaddlePaddle/PaddleDetection,2022-08-08 02:29:25,7,,6601,1331238000,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

!export CUDA_VISIBLE_DEVICES=0 #windows和Mac下不需要执行该命令
%cd /home/aistudio/work/PaddleDetection
!python3.7 tools/post_quant.py -c configs/yolov3/yolov3_darknet53_270e_voc.yml --slim_config configs/slim/quant/yolov3_darknet_qat.yml
模型是按照官网教程训练的，离线量化不持支，结果在线量化也报错。
![image](https://user-images.githubusercontent.com/75561356/183326370-bcf4e975-925c-4753-9dd3-69df59a4fd6f.png)
"
剪枝后量化到底怎么做？,PaddlePaddle/PaddleDetection,2022-08-08 02:09:18,3,,6600,1331225124,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

看了一圈issue，发现还是不知道剪枝后量化要怎么做，目前detection支持剪枝后量化了吗？
"
剪枝不是应该剪完再重新训练吗,PaddlePaddle/PaddleDetection,2022-08-07 04:57:14,1,,6598,1330914096,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

正常剪枝不是直接剪去通道后再微调吗？
python tools/train.py -c configs/{MODEL.yml} --slim_config configs/slim/{SLIM_CONFIG.yml}
这个为什么是训练和剪枝放一起了呢"
Hrnet python推理与C推理结果不一致,PaddlePaddle/PaddleDetection,2022-08-05 12:46:29,0,,6596,1329893475,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

设备：NX
cuda:10.1
paddledetection:2.4.0
ppdet:2.3.1

使用python 推理躺下的人和C躺下人的关节点，C的效果非常差，都不在人身上。使用的都是同一个模型。"
使用openvino2022加载solov2模型出错,PaddlePaddle/PaddleDetection,2022-08-05 08:28:53,2,,6591,1329622187,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

环境最新版的ppdet和openvino，在加载模型的时候就报错，如下所示。而使用pp-yolo却能正常加载及显示效果。
![图片](https://user-images.githubusercontent.com/52130681/183036241-02a6b1c5-dce3-40a8-9f66-c3d3cc1982df.png)


### 复现环境 Environment

_No response_

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
检测等任务可行，但是目标跟踪会有报错,PaddlePaddle/PaddleDetection,2022-08-04 02:58:44,3,windows,6584,1328000266,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

复现环境：
win10系统
cuda 11.6
cudnn 8.4
python 3.9
paddlepaddle-gpu==2.3.1.post116 windows版本

检测任务都可以正常训练与推理，但是跟踪任务在训练和推理时都会报错。
以fairmot模型为例，按照官方流程进行MOT16的训练推理：
训练时会报错：ValueError: Target xxx(数值不定) is out of lower bound.
直接下载官方提供权重文件进行推理会报错：OSError: (External) CUDA error(719), unspecified launch failure."
PPyoloE针对长宽比很大的目标检测效果很差,PaddlePaddle/PaddleDetection,2022-08-03 02:54:43,4,,6572,1326611657,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

1. yoloe这种anchor free的方式，针对长宽比很大（比如1:100）的目标检测效果很差。但是工业场景存在大量的长宽比很大的目标。
2. 具体差的原因个人感觉有两个
- 标签分配策略的问题
- fcn处理方式。比如512:512的图像上的  5:500的目标，这种目标在哪个层处理都不太合适

### 是否愿意提交PR Are you willing to submit a PR?

- [x] Yes I'd like to help by submitting a PR!"
OpenVino Picodet runtime error,PaddlePaddle/PaddleDetection,2022-08-03 02:09:40,0,,6571,1326587251,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

I've built a C++ example project successfully in VS studio 2019 (`third_engine/demo_openvino/`). But when I ran the program, it raised a runtime error.
```
.\picodet_demo.exe 1 .\images\cat.jpg
start init model
success
.\images\cat.jpg
preprocessing...
Detecting...
exception: Cannot find blob with name: transpose_1.tmp_0
```
I've used the original openvino model from this: https://paddledet.bj.bcebos.com/deploy/third_engine/picodet_m_416_openvino.zip

### 复现环境 Environment

- PaddleDetection: release/2.4
- Python 3.8
- OpenVino 2021

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
python.preprocess找不到ShortSizeScale,PaddlePaddle/PaddleDetection,2022-08-02 09:31:46,1,,6566,1325574963,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

版本信息：release 2.4
复现步骤：
1、根据PaddleDetection/docs/tutorials/INSTALL_cn.md文档，安装PaddlePaddle和PaddleDetection
2、根据PaddleDetection/deploy/pipeline/docs/tutorials/mot.md文档，下载模型并解压到./output_inference
3、修改设置infer_cfg_pphuman.yml中的MOT配置的enable=True
4、执行python deploy/pipeline/pipeline.py --config deploy/pipeline/config/infer_cfg_pphuman.yml \
                                                   --video_file=test_video.mp4 \
                                                   --device=gpu
测试结果：
python.preprocess找不到ShortSizeScale，报错信息如下。
查看PaddleDetection/deploy/python/preprocess.py文件，的确没有ShortSizeScale

Traceback (most recent call last):
  File ""deploy/pipeline/pipeline.py"", line 37, in <module>
    from python.preprocess import decode_image, ShortSizeScale
ImportError: cannot import name 'ShortSizeScale'

出现概率：必现
修复方法：暂未发现


### 复现环境 Environment

_No response_

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
目标跟踪预测报错，按照教程运行报错了,PaddlePaddle/PaddleDetection,2022-08-02 09:31:41,8,,6565,1325574868,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

执行命令：CUDA_VISIBLE_DEVICES=0 python tools/infer_mot.py -c configs/mot/jde/jde_darknet53_30e_1088x608.yml -o weights=https://paddledet.bj.bcebos.com/models/mot/jde_darknet53_30e_1088x608.pdparams --video_file=E:/PaddleDetection/dataset/my_test_data/videos/MOT16.mp4  --save_videos

cpu执行就可以

Traceback (most recent call last):
  File ""E:/PaddleDetection/tools/infer_mot.py"", line 150, in <module>
    main()
  File ""E:/PaddleDetection/tools/infer_mot.py"", line 146, in main
    run(FLAGS, cfg)
  File ""E:/PaddleDetection/tools/infer_mot.py"", line 100, in run
    tracker.mot_predict_seq(
  File ""E:\PaddleDetection\ppdet\engine\tracker.py"", line 544, in mot_predict_seq
    results, nf, ta, tc = self._eval_seq_jde(
  File ""E:\PaddleDetection\ppdet\engine\tracker.py"", line 147, in _eval_seq_jde
    pred_dets, pred_embs = self.model(data)
  File ""D:\Miniconda\envs\Paddle\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""D:\Miniconda\envs\Paddle\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""E:\PaddleDetection\ppdet\modeling\architectures\meta_arch.py"", line 75, in forward
    outs.append(self.get_pred())
  File ""E:\PaddleDetection\ppdet\modeling\architectures\jde.py"", line 110, in get_pred
    return self._forward()
  File ""E:\PaddleDetection\ppdet\modeling\architectures\jde.py"", line 68, in _forward
    det_outs = self.detector(self.inputs)
  File ""D:\Miniconda\envs\Paddle\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""D:\Miniconda\envs\Paddle\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""E:\PaddleDetection\ppdet\modeling\architectures\meta_arch.py"", line 75, in forward
    outs.append(self.get_pred())
  File ""E:\PaddleDetection\ppdet\modeling\architectures\yolo.py"", line 128, in get_pred
    return self._forward()
  File ""E:\PaddleDetection\ppdet\modeling\architectures\yolo.py"", line 99, in _forward
    boxes_idx, bbox, bbox_num, nms_keep_idx = self.post_process(
  File ""D:\Miniconda\envs\Paddle\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""D:\Miniconda\envs\Paddle\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""E:\PaddleDetection\ppdet\modeling\post_process.py"", line 459, in forward
    bbox_pred, bbox_num, nms_keep_idx = self.nms(
  File ""E:\PaddleDetection\ppdet\modeling\layers.py"", line 488, in __call__
    return ops.multiclass_nms(bboxes, score, **kwargs)
  File ""E:\PaddleDetection\ppdet\modeling\ops.py"", line 725, in multiclass_nms
    helper.append_op(
  File ""D:\Miniconda\envs\Paddle\lib\site-packages\paddle\fluid\layer_helper.py"", line 44, in append_op
    return self.main_program.current_block().append_op(*args, **kwargs)
  File ""D:\Miniconda\envs\Paddle\lib\site-packages\paddle\fluid\framework.py"", line 3599, in append_op
    _dygraph_tracer().trace_op(type,
  File ""D:\Miniconda\envs\Paddle\lib\site-packages\paddle\fluid\dygraph\tracer.py"", line 307, in trace_op
    self.trace(type, inputs, outputs, attrs,
OSError: (External) CUDA error(719), unspecified launch failure. 
  [Hint: 'cudaErrorLaunchFailure'. An exception occurred on the device while executing a kernel. Common causes include dereferencing an invalid device pointerand accessing out of bounds shared memory. Less common cases can be system specific - more information about these cases canbe found in the system specific user guide. This leaves the process in an inconsistent state and any further CUDA work willreturn the same error. To continue using CUDA, the process must be terminated and relaunched.] (at ..\paddle\phi\backends\gpu\gpu_context.cc:435)
  [operator < multiclass_nms3 > error]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion `index_value >= 0 && index_value < input_dims[j]` failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [54264] and greater than or equal to 0, but received [0]


### 复现环境 Environment

- PaddlePaddle 2.31.post116
- cuda 11.6
- cudnn 8.4
- python 3.8

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
模型评估类别显示不全,PaddlePaddle/PaddleDetection,2022-08-01 12:44:45,2,,6555,1324359891,"### 问题确认 Search before asking

- [x] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

用yolov3 voc格式数据集训练了五个类别的目标，模型预测可以检测出来第五个类别，但是模型评估也就是查看ap的时候，只能显示前四个类别，没有第五个类别。"
训练COCO自定义数据集出现label_map索引异常问题,PaddlePaddle/PaddleDetection,2022-08-01 10:19:50,0,,6553,1324181228,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

将XML格式数据集转换为COCO数据集后，使用PaddleDetection中的picodet和ssdlite_mobilenet模型进行训练。
在训练到若干个Epoch即为出现以下错误：

```python
Traceback (most recent call last):
  File ""tools/train.py"", line 177, in <module>
    main()
  File ""tools/train.py"", line 173, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 127, in run
    trainer.train(FLAGS.eval)
  File ""/media/hxzh02/SB@home/hxzh/PaddleDetection/ppdet/engine/trainer.py"", line 506, in train
    self._eval_with_loader(self._eval_loader)
  File ""/media/hxzh02/SB@home/hxzh/PaddleDetection/ppdet/engine/trainer.py"", line 533, in _eval_with_loader
    metric.update(data, outs)
  File ""/media/hxzh02/SB@home/hxzh/PaddleDetection/ppdet/metrics/metrics.py"", line 113, in update
    outs, self.clsid2catid, bias=self.bias)
  File ""/media/hxzh02/SB@home/hxzh/PaddleDetection/ppdet/metrics/coco_utils.py"", line 53, in get_infer_results
    outs['bbox'], outs['bbox_num'], im_id, catid, bias=bias)
  File ""/media/hxzh02/SB@home/hxzh/PaddleDetection/ppdet/metrics/json_results.py"", line 32, in get_det_res
    category_id = label_to_cat_id_map[int(num_id)]
KeyError: 34
```

最开始怀疑是数据集格式转换出现问题，因此重新生成了几遍又继续去跑，但仍然无果，依旧出现以上错误。
接着把其他使用COCO格式的模型配置来跑当前数据集，运行正常，这里使用的是faster_rcnn。
那么这样问题就来了，有一个跑成功说明COCO格式本身是没问题的，会不会有可能是模型运行机制上存在不同，
于是顺着Error的报错位置和line来做定位，发现问题出现在label_to_cat_id_map这个变量的索引获取。
label_to_cat_id_map是一个dict结构,当前的用法是若dict获取不到key，那么就会产生Error。
抱着试一试的心态去底层代码修改调整和打印数据，将`PaddleDetection/ppdet/metrics/json_results.py`中 `category_id = label_to_cat_id_map[int(num_id)]`修改为`category_id = label_to_cat_id_map.get('num_id'`)。

这个修改的方法是利用dict内置的get(key[,default])方法，如果key存在，则返回其value,否则返回default;使用这个方法永远不会触发KeyError，如：

```python
t = {
    'a': '1',
    'b': '2',
    'c': '3',
}
print(t.get('d'))
```

会出现：None


接着重新运行一遍所有步骤，发现运行正常。


复现训练命令：
```shell
export CUDA_VISIBLE_DEVICES=0
python -u tools/train.py -c configs/picodet/picodet_m_320_coco_lcnet.yml -o use_gpu=True --use_vdl=True --vdl_log_dir=vdl_dir/scalar --eval
```

```shell
export CUDA_VISIBLE_DEVICES=0
python -u tools/train.py -c configs/ssd/ssdlite_mobilenet_v3_small_320_coco.yml -o use_gpu=True --use_vdl=True --vdl_log_dir=vdl_dir/scalar --eval
```

### 复现环境 Environment

- PaddlePaddle: paddlepaddle-gpu        2.3.1.post112
- CUDA: 11.2

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
S2Anet本地部署,PaddlePaddle/PaddleDetection,2022-08-01 05:54:28,28,,6552,1323864154,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/configs/dota
请问这个链接的镜像是不是没了。
我显卡3070 cuda11.1 paddle2.2  paddledetection 2.2 按照教程训练显示cudnn错误。会不会是目前这个只支持cuda10 cudnn7.可30显卡好像对cuda10不太友好。请问有没有办法能本地安装上这个模型阿"
关于picodet的一些超参数疑问,PaddlePaddle/PaddleDetection,2022-08-01 02:35:40,2,,6551,1323718477,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

问题1：log_iter设置的是什么，看过一些项目里对其的解释是显示训练信息的迭代间隔，有点不太理解什么意思，控制下图红框中的信息吗，是怎么算得的
![image](https://user-images.githubusercontent.com/102035987/182059927-38d537d7-1d4f-46b6-a79d-c2346938d616.png)
问题2：如图，没有CIouLoss吗
![image](https://user-images.githubusercontent.com/102035987/182060387-51d17e91-61bb-4605-9a82-6ab3ff881133.png)
![image](https://user-images.githubusercontent.com/102035987/182060509-9ceaeaf9-14ed-4cf8-833b-7bb6ebb4ba4e.png)
问题3：单图和batch数据增强最多可以加几种，有什么先后顺序吗
![image](https://user-images.githubusercontent.com/102035987/182061301-05fe8ebc-4aea-479d-a2f0-61d8ff8b9104.png)
麻烦大佬解答，万分感谢！"
YOLOX训练自己数据集，AP一直为0,PaddlePaddle/PaddleDetection,2022-07-29 04:38:32,4,,6541,1321751085,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

训练YOLOX MOT模型，采用COCO预训练模型，按原配置训练24个EPOCH，其中AP一直为0
数据格式不变，训练PPYOLOE，AP是能达到30左右的。
使用Aistudio脚本任务，

还未完全收敛，但是这样继续下去还有用吗
![image](https://user-images.githubusercontent.com/22713115/181683605-44f0922d-948e-4539-8ab3-96b31759357e.png)
![image](https://user-images.githubusercontent.com/22713115/181683692-a7a781b4-cade-4cca-af1f-cf86b95f85bf.png)


这个是预测出来的结果，是YOLOX多尺度的问题吗，还是其他原因，但是PPYOLOE不也有多尺度吗
![image](https://user-images.githubusercontent.com/22713115/181683837-bb7fca1b-bf1e-4e7c-8109-2142da210467.png)
"
在picodet_m_416_coco_lcnet默认配置文件中添加单图数据增强算子训练时报错,PaddlePaddle/PaddleDetection,2022-07-28 13:29:06,2,,6538,1320935544,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

在原有配置文件四个算子下，根据官方文档，另外加入随机擦除算子：
![image](https://user-images.githubusercontent.com/102035987/181516243-d54ff9f4-5d93-4e5e-a400-93e27e49d77e.png)
![image](https://user-images.githubusercontent.com/102035987/181516576-f46e9b7a-4e4d-4197-bbe2-ec69c16a3602.png)
所报错误如下：
![image](https://user-images.githubusercontent.com/102035987/181516929-5f5ea255-e145-4c72-8eb4-e74a28c536d3.png)
请问是什么原因，求大佬解答。"
MTMCT对AIC21-demo中的车辆跟踪可视化结果存在过多boundingbox和频繁的IDS，该调整那些参数。,PaddlePaddle/PaddleDetection,2022-07-28 08:35:53,3,,6536,1320588080,"### 问题确认 Search before asking

- [x] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

通过如下命令成功运行对两个demo视频的mtmct：
> python deploy/pptracking/python/mot_sde_infer.py --model_dir=output_inference/ppyolov2_r50vd_dcn_365e_aic21mtmct_vehicle/ --reid_model_dir=output_inference/deepsort_pplcnet_vehicle/ --mtmct_dir=mtmct-demo --mtmct_cfg=deploy/pptracking/python/mtmct_cfg.yml --device=GPU --tracker_config=deploy/pptracking/python/tracker_config.yml --scaled=True --save_mot_txts --save_images --cpu_threads -1

输出的跟踪结果：
![图片](https://user-images.githubusercontent.com/64450886/181460544-7e9cc3c4-3d2e-4a96-a9fa-02c5a453a64e.png)


我想为每个车辆仅展示一个boundingbox和ID，该调整那些参数，我尝试过修改如下超参数：
`output_inference/ppyolov2_r50vd_dcn_365e_aic21mtmct_vehicle/infer_cfg.yml`中的draw_threshold；
`output_inference/deepsort_pplcnet_vehicle/infer_cfg.yml`和`deploy/pptracking/python/tracker_config.yml`
中的draw_threshold，matching_threshold，和max_iou_distance。
均未在output/mtmct_vis得到满意的可视化效果。@[nemonameless](https://github.com/nemonameless)"
目标跟踪报错：cudaErrorLaunchFailure （719）,PaddlePaddle/PaddleDetection,2022-07-28 07:53:23,3,,6535,1320539932,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

预测所有的 目标跟踪 模型都会报：
OSError: (External) CUDA error(719), unspecified launch failure.
  [Hint: 'cudaErrorLaunchFailure'. An exception occurred on the device while executing a kernel. Common causes include dereferencing an invalid device pointerand accessing out of bound
s shared memory. Less common cases can be system specific - more information about these cases canbe found in the system specific user guide. This leaves the process in an inconsistent
 state and any further CUDA work willreturn the same error. To continue using CUDA, the process must be terminated and relaunched.] (at ..\paddle\phi\backends\gpu\gpu_context.cc:435)
  [operator < multiclass_nms3 > error]
"
PicoDet模型8个输出如何用python解析,PaddlePaddle/PaddleDetection,2022-07-28 04:15:08,0,,6534,1320367942,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

请问Picodet训练完毕后, 我的模型结构在最终输出时有8个输出部分, 请问这个些数据如何利用python解析出来并完成图片检测框的绘制呢?
![image](https://user-images.githubusercontent.com/61443763/181418975-c5fd207f-32eb-4366-a52c-c77c943efe66.png)
"
当使用DETR训练自己的数据集时，发现训练500个epoch以后，精度仍然非常低，mAP仍然非常低，只有5%,PaddlePaddle/PaddleDetection,2022-07-27 13:21:36,2,,6533,1319558538,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

当使用DETR训练自己的数据集时，发现训练500个epoch以后，精度仍然非常低，mAP只有5%，我已经检查过数据集，应该并不是数据集的问题，因为用YOLOV3、FCOS、CenterNet，mAP都可以达到80%，甚至是90%。虽然DETR收敛比较慢，但只有5%也太不正常了？？？
Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.052
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.125
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.038
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.052
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.100
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.117
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.219
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.280
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
[07/27 21:14:27] ppdet.engine INFO: Total sample number: 542, averge FPS: 15.35280900697572"
请问萌新如何进行关键点识别,PaddlePaddle/PaddleDetection,2022-07-27 08:35:09,4,,6528,1319211739,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

之前用paddlex进行目标检测，效果不错，但是一旦想要识别指针之类的计算角度的东西就好像不好用了，查了以下是不是可以用paddleDetection的关键点识别计算角度。目前需求是在一张图片合中先框出带检测物体然后计算他指针的转角，我用labelme标注时一块把框和关键点标了，请问接下来该如何转成detection需要的格式以及如何训练，貌似配置config挺麻烦的，文档看的不是很明白，因为不是人体关键点检测，也没有具体的例子，希望paddde能有个简单的例子。还有我看文档预测时参数里有检测模型和关键点模型，目标检测模型是单独训练的吗。"
训练时正常，评估时报错,PaddlePaddle/PaddleDetection,2022-07-27 05:09:00,7,,6524,1319028636,"### 问题描述 Please describe your issue

Traceback (most recent call last):
  File ""tools/infer.py"", line 178, in <module>
    main()
  File ""tools/infer.py"", line 174, in main
    run(FLAGS, cfg)
  File ""tools/infer.py"", line 135, in run
    save_results=FLAGS.save_results)
  File ""C:\Users\H1CE\Desktop\PaddleDetection-release-2.4\ppdet\engine\trainer.py"", line 641, in predict
    batch_res = get_infer_results(outs, clsid2catid)
  File ""C:\Users\H1CE\Desktop\PaddleDetection-release-2.4\ppdet\metrics\coco_utils.py"", line 53, in get_infer_results
    outs['bbox'], outs['bbox_num'], im_id, catid, bias=bias)
  File ""C:\Users\H1CE\Desktop\PaddleDetection-release-2.4\ppdet\metrics\json_results.py"", line 37, in get_det_res
    category_id = label_to_cat_id_map[int(num_id)]
KeyError: 1
通过自己打断点发现最后的abel_to_cat_id_map出现1时报错，应该是检测为另外不在标签内的物体，请问这个问题怎么解决。"
"scores = outputs['bbox'][:, 1].numpy() RuntimeError: (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received  Tensor's dimension is d4 memory's size is 0.   [Hint: Expected numel() * SizeOfType(type()) <= memory_size(), but received numel() * SizeOfType(type()):4 > memory_size():0.] (at ..\paddle\fluid\framework\tensor.cc:39)",PaddlePaddle/PaddleDetection,2022-07-27 02:14:06,2,,6523,1318933676,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

scores = outputs['bbox'][:, 1].numpy()
RuntimeError: (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received  Tensor's dimension is d4 memory's size is 0.
  [Hint: Expected numel() * SizeOfType(type()) <= memory_size(), but received numel() * SizeOfType(type()):4 > memory_size():0.] (at ..\paddle\fluid\framework\tensor.cc:39)

在用pp-yolo做目标检测的时候遇到了这个报错，不知道要怎么改动去解决"
为什么pphuman的model.pdmodel，不能使用Openvino2022推理使用？,PaddlePaddle/PaddleDetection,2022-07-27 01:53:08,1,,6522,1318922924,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

使用后报如下异常：

 

RuntimeError: Check 'creator_it != CREATORS_MAP.end()' failed at C:\j\workspace\private-ci\ie\build-windows-vs2019@3\b\repos\openvino\src\frontends\paddle\src\frontend.cpp:40:
FrontEnd API failed with OpConversionFailure: :
No creator found for swish node.

"
"/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [2], but received [5],CUDA error(719), unspecified launch failure.",PaddlePaddle/PaddleDetection,2022-07-26 15:35:12,3,,6521,1318416803,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

win11
anaconda python 3.7

paddledet                         2.4.0
paddlepaddle-gpu                  2.3.1.post112

模型
solov2_r50_fpn_1x_coco.yml

```
W0726 23:22:00.142284 13396 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 11.3, Runtime API Version: 11.2
W0726 23:22:00.157871 13396 gpu_context.cc:306] device: 0, cuDNN Version: 8.2.
[07/26 23:22:01] ppdet.utils.download INFO: Downloading ResNet50_cos_pretrained.pdparams from https://paddledet.bj.bcebos.com/models/pretrained/ResNet50_cos_pretrained.pdparams
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92063/92063 [00:14<00:00, 6295.36KB/s]
[07/26 23:22:17] ppdet.utils.checkpoint INFO: Finish loading model weights: C:\Users\aaa/.cache/paddle/weights\ResNet50_cos_pretrained.pdparams
Error: ../paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [2], but received [5].
Error: ../paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [2], but received [5].
Error: ../paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [2], but received [6].
Error: ../paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [2], but received [6].
Error: ../paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [2], but received [6].
Error: ../paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [2], but received [5].
Error: ../paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [2], but received [7].
Error: ../paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [2], but received [6].
Traceback (most recent call last):
  File ""tools/train.py"", line 177, in <module>
    main()
  File ""tools/train.py"", line 173, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 127, in run
    trainer.train(FLAGS.eval)
  File ""D:\PaddleDetection\ppdet\engine\trainer.py"", line 407, in train
    outputs = model(data)
  File ""D:\anaconda3\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""D:\anaconda3\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""D:\PaddleDetection\ppdet\modeling\architectures\meta_arch.py"", line 54, in forward
    out = self.get_loss()
  File ""D:\PaddleDetection\ppdet\modeling\architectures\solov2.py"", line 94, in get_loss
    gt_ins_labels, gt_cate_labels, gt_grid_orders, fg_num)
  File ""D:\PaddleDetection\ppdet\modeling\heads\solov2_head.py"", line 405, in get_loss
    ins_pred_list, ins_labels, flatten_cate_preds, cate_labels, num_ins)
  File ""D:\PaddleDetection\ppdet\modeling\losses\solov2_loss.py"", line 99, in __call__
    alpha=self.focal_loss_alpha)
  File ""D:\anaconda3\envs\PaddleDetection\lib\site-packages\paddle\nn\functional\loss.py"", line 2048, in sigmoid_focal_loss
    alpha = fluid.dygraph.base.to_variable([alpha], dtype=loss.dtype)
  File ""D:\anaconda3\envs\PaddleDetection\lib\site-packages\decorator.py"", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File ""D:\anaconda3\envs\PaddleDetection\lib\site-packages\paddle\fluid\wrapped_decorator.py"", line 25, in __impl__
    return wrapped_func(*args, **kwargs)
  File ""D:\anaconda3\envs\PaddleDetection\lib\site-packages\paddle\fluid\framework.py"", line 434, in __impl__
    return func(*args, **kwargs)
  File ""D:\anaconda3\envs\PaddleDetection\lib\site-packages\paddle\fluid\dygraph\base.py"", line 768, in to_variable
    name=name if name else '')
OSError: (External) CUDA error(719), unspecified launch failure.
  [Hint: 'cudaErrorLaunchFailure'. An exception occurred on the device while executing a kernel. Common causes include dereferencing an invalid device pointerand accessing out of bounds shared memory. Less common cases can be system specific - more information about these cases canbe found in the system specific user guide. This leaves the process in an inconsistent state and any further CUDA work willreturn the same error. To continue using CUDA, the process must be terminated and relaunched.] (at ..\paddle\phi\backends\gpu\cuda\cuda_info.cc:258)
```

如果是数据问题
也没提示是哪张   不然我直接删除就好了

有没工具  或者 代码哪里修改
才能显示  有问题的数据 
print 图片或者json地址

如果是cuda问题  怎么弄"
RuntimeError: ffmpeg process video: test.mp4 error,PaddlePaddle/PaddleDetection,2022-07-26 11:22:58,2,,6519,1318096594,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question


[07/26 19:18:47] ppdet.utils.checkpoint INFO: Finish resuming model weights: weights/fairmot_dla34_30e_1088x608.pdparams
'ffmpeg' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���
���������ļ���
Traceback (most recent call last):
  File ""D:/zhaohang/GitHub/PaddleDetection-release-2.4/infer_mot.py"", line 157, in <module>
    main()
  File ""D:/zhaohang/GitHub/PaddleDetection-release-2.4/infer_mot.py"", line 153, in main
    run(FLAGS, cfg)
  File ""D:/zhaohang/GitHub/PaddleDetection-release-2.4/infer_mot.py"", line 107, in run
    tracker.mot_predict_seq(
  File ""D:\zhaohang\GitHub\PaddleDetection-release-2.4\ppdet\engine\tracker.py"", line 518, in mot_predict_seq
    self.dataset.set_video(video_file, frame_rate)
  File ""D:\zhaohang\GitHub\PaddleDetection-release-2.4\ppdet\data\source\mot.py"", line 574, in set_video
    self.roidbs = self._load_video_images()
  File ""D:\zhaohang\GitHub\PaddleDetection-release-2.4\ppdet\data\source\mot.py"", line 506, in _load_video_images
    frames_path = video2frames(self.video_file, output_path,
  File ""D:\zhaohang\GitHub\PaddleDetection-release-2.4\ppdet\data\source\mot.py"", line 607, in video2frames
    raise RuntimeError('ffmpeg process video: {} error'.format(video_path))
RuntimeError: ffmpeg process video: test.mp4 error"
考虑yolov7模型不，加进来,PaddlePaddle/PaddleDetection,2022-07-26 09:55:16,2,,6516,1317990664,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

考虑 yolov7 模型不，加进来

`https://github.com/WongKinYiu/yolov7`

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
PP-YOLOE on DeepStream,PaddlePaddle/PaddleDetection,2022-07-25 16:53:48,0,feature request,6511,1317109347,"Repository to use PP-YOLOE model on NVIDIA DeepStream SDK

Link: https://github.com/marcoslucianops/DeepStream-Yolo

#### Config

```
board = NVIDIA Tesla V100 (AWS: p3.2xlarge)
batch-size = 1
eval = val2017 (COCO)
sample = 1920x1080 video
```

#### NMS config

- Eval

```
nms-iou-threshold=0.7
pre-cluster-threshold=0.001
topk=300
```

- Test

```
nms-iou-threshold=0.7
pre-cluster-threshold=0.25
topk=300
```

#### Results

| DeepStream         | Precision | Resolution | IoU=0.5:0.95 | IoU=0.5 | IoU=0.75 | FPS<br />(without display) |
|:------------------:|:---------:|:----------:|:------------:|:-------:|:--------:|:--------------------------:|
| PP-YOLOE-x         | FP16      | 640        | 0.506        | 0.681   | 0.551    | 116.54                     |
| PP-YOLOE-l         | FP16      | 640        | 0.498        | 0.674   | 0.545    | 187.93                     |
| PP-YOLOE-m         | FP16      | 640        | 0.476        | 0.646   | 0.522    | 257.42                     |
| PP-YOLOE-s (400)   | FP16      | 640        | 0.422        | 0.589   | 0.463    | 465.23                     |"
训练时报错，使用Windows平台正常，linuex报错,PaddlePaddle/PaddleDetection,2022-07-25 01:24:25,3,,6503,1316081513,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

Traceback (most recent call last):
  File ""tools/train.py"", line 177, in <module>
    main()
  File ""tools/train.py"", line 173, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 127, in run
    trainer.train(FLAGS.eval)
  File ""/home/haokun/h1ce/PaddleDetection-release-2.4/ppdet/engine/trainer.py"", line 506, in train
    self._eval_with_loader(self._eval_loader)
  File ""/home/haokun/h1ce/PaddleDetection-release-2.4/ppdet/engine/trainer.py"", line 533, in _eval_with_loader
    metric.update(data, outs)
  File ""/home/haokun/h1ce/PaddleDetection-release-2.4/ppdet/metrics/metrics.py"", line 113, in update
    outs, self.clsid2catid, bias=self.bias)
  File ""/home/haokun/h1ce/PaddleDetection-release-2.4/ppdet/metrics/coco_utils.py"", line 53, in get_infer_results
    outs['bbox'], outs['bbox_num'], im_id, catid, bias=bias)
  File ""/home/haokun/h1ce/PaddleDetection-release-2.4/ppdet/metrics/json_results.py"", line 30, in get_det_res
    category_id = label_to_cat_id_map[int(num_id)]
KeyError: 1
"
solov2训练后预测结果矩阵有问题,PaddlePaddle/PaddleDetection,2022-07-24 04:14:19,12,,6501,1315803767,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.
![2](https://user-images.githubusercontent.com/53806323/180641866-cb035324-b41b-4024-b1bf-031234cc5da2.jpg)


### bug描述 Describe the Bug

solov2训练后预测结果矩阵有问题

### 复现环境 Environment

![2](https://user-images.githubusercontent.com/53806323/180641866-cb035324-b41b-4024-b1bf-031234cc5da2.jpg)
这是为什么？ 下的都是最新版本，paddle也是最新的，训练完预测就这样子了
训练参数为下：
我是labelme所以先转换了样本
x2coco.py --dataset_type labelme --json_input_dir E:\python\PaddleDetection-release-2.4\sample\json --image_input_dir E:\python\PaddleDetection-release-2.4\sample\image --output_dir E:\python\PaddleDetection-release-2.4\sample\coco --train_proportion 0.9 --val_proportion 0.1

train.py -c configs/solov2/solov2_r50_enhance_coco.yml

export_model.py -c configs/solov2/solov2_r50_enhance_coco.yml --output_dir=./output_inference -o weights=./output/solov2_r50_enhance_coco/model_final

infer.py --model_dir=./output_inference/solov2_r50_enhance_coco --image_file=./2.jpg --threshold=0.5



### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
PicoDet 剪枝例子输出模型,PaddlePaddle/PaddleDetection,2022-07-22 06:12:52,0,,6490,1314380925,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

ubuntu 20.04
cuda 11.2
python3.9
paddleDetection 2.3
paddlelite 2.10
paddleslim  2.2.2

官方提供权重[非结构化稀疏在 PicoDet 上的应用教程](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/configs/picodet/legacy_model/pruner)
![image](https://user-images.githubusercontent.com/33504478/180373664-c656071d-b940-495d-91c2-75485f222804.png)
使用如下命令转换
'python tools/export_model.py -c configs/picodet/legacy_model/picodet_m_320_coco.yml -o weights=output/picodet_m_320_coco.pdparams --output_dir=inference_model/`
`paddle_lite_opt --valid_targets=arm --model_dir=inference_model/picodet_m_320_coco --optimize_out=export_model/picodet_m_320_coco`

输出的**nb文件大小为9.3MB**
picodet_m_320__coco_sparse_75，转换命令
'python tools/export_model.py -c configs/picodet/legacy_model/picodet_m_320_coco.yml -o weights=output/picodet_m_320__coco_sparse_75.pdparams --output_dir=inference_model/`
`paddle_lite_opt --valid_targets=arm --model_dir=inference_model/picodet_m_320__coco_sparse_75 --optimize_out=export_model/picodet_m_320__coco_sparse_75 --sparse_model true `
转换后**得到的模型也是9.3MB**

**且高通865，ARMv8测试时间相差0.3ms**

**问题1** PC上转换出来的模型与介绍的差的有些多，且时间差别不大，原因是什么？

**问题2**我在自己的数据集上剪枝与量化，转换出来的nb文件变小了，但是推测时间更长了？

"
关键点检测 LiteHRNet-18 联合部署推理报错,PaddlePaddle/PaddleDetection,2022-07-22 01:48:34,8,,6488,1314032489,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

用yolo 和 LiteHRNet-18 联合部署检测视频，刚开始正常预测，预测到700多帧时突然报错，不太理解为什么正常预测到中途会报错，不太清楚原因
```
-----------  Running Arguments -----------
camera_id: -1
cpu_threads: 1
det_model_dir: output_inference/ppyolov2_r50vd_dcn_365e_coco/
det_threshold: 0.5
device: gpu
enable_mkldnn: False
image_dir: None
image_file: None
keypoint_batch_size: 8
keypoint_model_dir: output_inference/lite_hrnet_18_rat_coco/
keypoint_threshold: 0.5
output_dir: output
run_benchmark: False
run_mode: paddle
save_res: False
trt_calib_mode: False
trt_max_shape: 1280
trt_min_shape: 1
trt_opt_shape: 640
use_dark: True
video_file: dataset/rat_temperature.mp4
------------------------------------------
-----------  Model Configuration -----------
Model Arch: YOLO
Transform Order: 
--transform op: Resize
--transform op: NormalizeImage
--transform op: Permute
--------------------------------------------
-----------  Model Configuration -----------
Model Arch: HRNet
Transform Order: 
--transform op: TopDownEvalAffine
--transform op: NormalizeImage
--transform op: Permute
--------------------------------------------
fps: 30, frame_count: 6760
detect frame: 1
detect frame: 2
detect frame: 3
detect frame: 4
...
detect frame: 707
detect frame: 708
detect frame: 709
detect frame: 710
detect frame: 711
Traceback (most recent call last):
  File ""deploy/python/det_keypoint_unite_infer.py"", line 270, in <module>
    main()
  File ""deploy/python/det_keypoint_unite_infer.py"", line 235, in main
    FLAGS.save_res)
  File ""deploy/python/det_keypoint_unite_infer.py"", line 163, in topdown_unite_predict_video
    FLAGS.run_benchmark)
  File ""deploy/python/det_keypoint_unite_infer.py"", line 47, in predict_with_given_det
    rec_images, run_benchmark, repeats=10, visual=False)
  File ""/home/chenmin/workspace/PaddleDetection/deploy/python/keypoint_infer.py"", line 240, in predict_image
    results = self.merge_batch_result(results)
  File ""/home/chenmin/workspace/PaddleDetection/deploy/python/infer.py"", line 221, in merge_batch_result
    res_key = batch_result[0].keys()
IndexError: list index out of range
```

### 复现环境 Environment

- PaddlePaddle : 2.3.0
- PaddleDetection : release/2.4
- Python : 3.6.13
- CUDA : 11.2.2
- CUDNN : 8.1.0.77

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
进行非结构化剪枝时不小心退出，之后无法进行恢复训练,PaddlePaddle/PaddleDetection,2022-07-19 12:39:05,0,,6477,1309461463,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

1. 这是我的运行代码
`%cd ~
%cd PaddleDetection
!python tools/train.py -c configs/picodet/legacy_model/pruner/picodet_s_416_coco_pruner.yml \
                                                 --slim_config configs/slim/prune/picodet_m_unstructured_prune_75.yml --eval \
                                                  --r output/picodet_m_unstructured_prune_75/best_model \
                                                  --use_vdl=true \
                                                 --vdl_log_dir=/home/aistudio/vdl_log_dir/scalar \`
2.这是报错
`Traceback (most recent call last):
  File ""tools/train.py"", line 177, in <module>
    main()
  File ""tools/train.py"", line 173, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 122, in run
    trainer.resume_weights(FLAGS.resume)
  File ""/home/aistudio/PaddleDetection/ppdet/engine/trainer.py"", line 370, in resume_weights
    self.ema if self.use_ema else None)
  File ""/home/aistudio/PaddleDetection/ppdet/utils/checkpoint.py"", line 99, in load_weight
    model.set_dict(model_weight)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py"", line 281, in wrapper
    return func(*args, **kwargs)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 1445, in set_state_dict
    param.set_value(state)
  File ""<decorator-gen-127>"", line 2, in set_value
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/wrapped_decorator.py"", line 25, in __impl__
    return wrapped_func(*args, **kwargs)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py"", line 229, in __impl__
    return func(*args, **kwargs)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/varbase_patch_methods.py"", line 174, in set_value
    self.name, self_tensor_np.dtype, value_np.dtype)
AssertionError: Variable dtype not match, Variable [ generated_tensor_52 ] need tensor with dtype float32  but load tensor with dtype bool`"
"只让maskrcnn实现目标检测功能 ，应该如何修改代码 ,能不能提供下文档",PaddlePaddle/PaddleDetection,2022-07-19 02:58:03,5,,6471,1308909405,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

使用2.4版本数据，只让maskrcnn实现目标检测功能 ，应该如何修改代码 "
Windows  S2Anet  编译,PaddlePaddle/PaddleDetection,2022-07-18 02:54:43,5,,6463,1307333719,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

编译S2ANet 
python setup.py install  报错，请问该怎么解决

环境:  CUDA10.2  Paddledetection2.4 


![image](https://user-images.githubusercontent.com/46549527/179438693-57b81f03-bcc1-4d97-9f19-e0d6898a8c6f.png)


![image](https://user-images.githubusercontent.com/46549527/179438834-7c868b21-6458-4305-a06a-020f0564668d.png)

"
ppyolo模型转化为onnx后infer_shapes失败,PaddlePaddle/PaddleDetection,2022-07-17 09:21:47,3,,6461,1307041262,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

ppyolo_mbv3_large_coco模型转化为onnx后infer_shapes失败，报如下错误：
`onnx.onnx_cpp2py_export.shape_inference.InferenceError: [ShapeInferenceError] (op_type:Gather, node name: Gather_12): [ShapeInferenceError] Inferred shape and existing shape differ in dimension 0: (1) vs (-1)
`
同时，使用TVM加载转化后onnx模型，也无法通过，报如下错误：
`Check failed: *axis_ptr == 1 (-1 vs. 1) : cannot squeeze axis with dimension not equal to 1`

但是，使用onnx的check_model对模型进行验证，一切正常。
所以不知道上述问题是什么原因造成的，还是说转化过程需要什么特别操作。
转化步骤参考了EXPORT_ONNX_MODEL.md中的步骤。
其余模型如ppyolov2_r50vd_dcn_365e_coco同样存在这个问题。
环境：
OS：ubuntu 18.04
paddle2onnx：0.9.8/1.0.0rc
onnx：1.11/1.12"
ssd_mobilenet_v1的静态图剪裁的模型评估时候这个问题，求大佬解答,PaddlePaddle/PaddleDetection,2022-07-15 21:51:40,4,,6453,1306564809,"大家好，我在做ssd_mobilenet_v1的静态图剪裁的时候出现这个问题，求教如何解决？
工作路径在PaddleDetection/static/下
![image](https://user-images.githubusercontent.com/109390344/179322261-1f75504a-df55-4109-92e6-ec31d98573b8.png)

![SWIE5~1 $S@B~8Y@%TW$7CC](https://user-images.githubusercontent.com/109390344/179315276-a9a10b70-7d36-4674-beb3-30830e1ab47b.png)

_Originally posted by @SunYF-0729 in https://github.com/PaddlePaddle/PaddleDetection/issues/6333#issuecomment-1185970346_"
最近的PPhuman V2 Reid 聚类阶段报错,PaddlePaddle/PaddleDetection,2022-07-15 07:57:54,2,,6449,1305710774,"### 问题确认 Search before asking

- [x] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

使用教程代码 多镜头文件夹里面放了多个视频


Traceback (most recent call last):
  File ""/snap/pycharm-professional/290/plugins/python/helpers/pydev/pydevd.py"", line 1491, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/snap/pycharm-professional/290/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/D/tools/PaddleDetection/deploy/pipeline/pipeline.py"", line 1021, in <module>
    main()
  File ""/D/tools/PaddleDetection/deploy/pipeline/pipeline.py"", line 1010, in main
    pipeline.run()
  File ""/D/tools/PaddleDetection/deploy/pipeline/pipeline.py"", line 159, in run
    mtmct_process(
  File ""/D/tools/PaddleDetection/deploy/pipeline/pphuman/mtmct.py"", line 335, in mtmct_process
    map_tid = sub_cluster(cid_tid_dict)
  File ""/D/tools/PaddleDetection/deploy/pipeline/pphuman/mtmct.py"", line 267, in sub_cluster
    clu = get_labels(cid_tid_dict, cid_tids)
  File ""/D/tools/PaddleDetection/deploy/pipeline/pphuman/mtmct.py"", line 244, in get_labels
    cost_matrix = get_sim_matrix_new(cid_tid_dict, cid_tids)
  File ""/D/tools/PaddleDetection/deploy/pipeline/pphuman/mtmct.py"", line 208, in get_sim_matrix_new
    distmat = get_dist_mat(q_arr, g_arr, func_name=""cosine"")
  File ""/D/tools/PaddleDetection/deploy/pipeline/pphuman/mtmct.py"", line 182, in get_dist_mat
    dist_mat = get_cosine(x, y)
  File ""/D/tools/PaddleDetection/deploy/pipeline/pphuman/mtmct.py"", line 176, in get_cosine
    sim_mt = cosine_similarity(x, y, eps)
  File ""/D/tools/PaddleDetection/deploy/pipeline/pphuman/mtmct.py"", line 160, in cosine_similarity
    x_n, y_n = np.linalg.norm(
  File ""<__array_function__ internals>"", line 5, in norm
  File ""/home/lgc/anaconda3/lib/python3.8/site-packages/numpy/linalg/linalg.py"", line 2560, in norm
    return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))
numpy.AxisError: axis 1 is out of bounds for array of dimension 1

### 复现环境 Environment

_No response_

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
PPYOLOE训练到360轮学习率变为0，正常吗就此需要停止训练了吗？loss为1.83左右,PaddlePaddle/PaddleDetection,2022-07-14 12:43:38,3,,6443,1304734005,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

PPYOLOE训练到360轮学习率变为0，正常吗就此需要停止训练了吗？loss为1.83左右
是不是配置文件里的optimizer_300e.yml里面maxepoch的问题，默认是360，可以随意更改吗"
cpp部署代码在windows端进行连续预测产生内存泄漏 #3795 被关闭 但未提供解决方案后续解决方案代码,PaddlePaddle/PaddleDetection,2022-07-14 03:35:55,4,,6429,1304206391,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

c++ 代码 PaddleDetection严重内存溢出 原因 内部  std::vector<const float*> output_data_list_; 指针未释放

det->Predict(imgs, FLAGS_threshold, 0, 1, &result, &bbox_num, &det_times); 严重内存溢出

修改 object_detector.cc
函数
void ObjectDetector::Predict
函数末尾增加
for (auto it = output_data_list_.begin(); it != output_data_list_.end(); it++)
  {
      if (*it != NULL)
      {
          delete* it;
          *it = NULL;
      }
  }
  output_data_list_.clear();

### 复现环境 Environment

所有c++版本

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
picoDet 量化问题,PaddlePaddle/PaddleDetection,2022-07-13 08:54:35,0,,6421,1303120428,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

1.请问picoDet 量化后转换问题解决了吗？ issue#6359 中说还在修复。
2.问题是出现再backbone, 还是neck 还是decode_head?"
Faster RCNN Window 预测 报错,PaddlePaddle/PaddleDetection,2022-07-12 11:04:21,2,,6410,1301899982,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

下载的已经编译好的paddle_inference库，支持本地电脑的版本以及cuda cudnn tensorRT版本
CUDA version: 10.2
CUDNN version: v7.6
CXX compiler version: 19.16.27045.0
WITH_TENSORRT: ON
TensorRT version: v7
显卡RTX2070Super

使用export_modle.py 导出导出faster_rcnn_r50_1x_voc.yml训练的模型
infer_cfg.yml如下：
mode: trt_fp32
draw_threshold: 0.5
metric: VOC
use_dynamic_shape: true
arch: RCNN
min_subgraph_size: 40
Preprocess:
- interp: 2
  keep_ratio: true
  target_size:
  - 2500
  - 2500
  type: Resize
- is_scale: true
  mean:
  - 0.6596901986291833
  - 0.6596901986291833
  std:
  - 0.10138360185013484
  - 0.10138360185013484
  type: NormalizeImage
- type: Permute
- stride: -1
  type: PadStride
label_list:
- ""\u5D29\u89D2""
- ""\u5212\u75D5""
- ""\u88C2\u7EB9""
- ""\u8868\u9762\u6742\u7269”

当mode:fluid时预测一切正常，但当将mode改为trt_fp16或者trt_fp32时 第一张图能够正常预测，但第二张（或者再次预测第一张图）预测会报错如下：
--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
Not support stack backtrace yet.

----------------------
Error Message Summary:
----------------------
InvalidArgumentError: decrease dim should be 1
  [Hint: Expected out_dims[decrease_axis[i]] == 1, but received out_dims[decrease_axis[i]]:0 != 1:1.] (at C:/home/workspace/Paddle_release5/paddle/fluid/operators/slice_op.cc:147)
  [operator < slice > error]


详细堆栈信息：
Compile Traceback (most recent call last):
    File ""/snap/pycharm-community/286/plugins/python-ce/helpers/pydev/pydevd.py""
, line 2181, in <module>
      main()
    File ""/snap/pycharm-community/286/plugins/python-ce/helpers/pydev/pydevd.py""
, line 2172, in main
      globals = debugger.run(setup['file'], None, None, is_module)
    File ""/snap/pycharm-community/286/plugins/python-ce/helpers/pydev/pydevd.py""
, line 1484, in run
      return self._exec(is_module, entry_point_fn, module_name, file, globals, l
ocals)
    File ""/snap/pycharm-community/286/plugins/python-ce/helpers/pydev/pydevd.py""
, line 1491, in _exec
      pydev_imports.execfile(file, globals, locals)  # execute the script
    File ""/snap/pycharm-community/286/plugins/python-ce/helpers/pydev/_pydev_imp
s/_pydev_execfile.py"", line 18, in execfile
      exec(compile(contents+""\n"", file, 'exec'), glob, loc)
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/tools/export_model.py
"", line 115, in <module>
      main()
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/tools/export_model.py
"", line 111, in main
      run(FLAGS, cfg)
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/tools/export_model.py
"", line 77, in run
      trainer.export(FLAGS.output_dir)
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/ppdet/engine/trainer.
py"", line 686, in export
      static_model, pruned_input_spec = self._get_infer_cfg_and_input_spec(
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/ppdet/engine/trainer.
py"", line 664, in _get_infer_cfg_and_input_spec
      input_spec, static_model.forward.main_program,
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
dygraph_to_static/program_translator.py"", line 563, in main_program
      concrete_program = self.concrete_program
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
dygraph_to_static/program_translator.py"", line 479, in concrete_program
      return self.concrete_program_specify_input_spec(input_spec=None)
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
dygraph_to_static/program_translator.py"", line 516, in concrete_program_specify_
input_spec
      concrete_program, _ = self.get_concrete_program(
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
dygraph_to_static/program_translator.py"", line 427, in get_concrete_program
      concrete_program, partial_program_layer = self._program_cache[cache_key]
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
dygraph_to_static/program_translator.py"", line 744, in __getitem__
      self._caches[item] = self._build_once(item)
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
dygraph_to_static/program_translator.py"", line 730, in _build_once
      concrete_program = ConcreteProgram.from_func_spec(
    File ""<decorator-gen-206>"", line 2, in from_func_spec

    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/wrapped_
decorator.py"", line 25, in __impl__
      return wrapped_func(*args, **kwargs)
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
base.py"", line 40, in __impl__
      return func(*args, **kwargs)
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
dygraph_to_static/program_translator.py"", line 683, in from_func_spec
      outputs = static_func(*inputs)
    File ""/tmp/tmpqpqzpt3g.py"", line 42, in forward
      out = paddle.jit.dy2static.convert_ifelse(self.training, true_fn_2,
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
dygraph_to_static/convert_operators.py"", line 211, in convert_ifelse
      out = _run_py_ifelse(pred, true_fn, false_fn, true_args, false_args)
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
dygraph_to_static/convert_operators.py"", line 271, in _run_py_ifelse
      return true_fn(*true_args) if pred else false_fn(*false_args)
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/ppdet/modeling/archit
ectures/meta_arch.py"", line 72, in forward
      out = self.get_pred()
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/ppdet/modeling/archit
ectures/faster_rcnn.py"", line 104, in get_pred
      bbox_pred, bbox_num = self._forward()
    File ""/tmp/tmp4dutrvx3.py"", line 53, in _forward
      _, __return_value_0, rois, rois_num = paddle.jit.dy2static.convert_ifelse(

    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
dygraph_to_static/convert_operators.py"", line 211, in convert_ifelse
      out = _run_py_ifelse(pred, true_fn, false_fn, true_args, false_args)
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
dygraph_to_static/convert_operators.py"", line 271, in _run_py_ifelse
      return true_fn(*true_args) if pred else false_fn(*false_args)
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/ppdet/modeling/archit
ectures/faster_rcnn.py"", line 81, in _forward
      rois, rois_num, _ = self.rpn_head(body_feats, self.inputs)
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
layers.py"", line 914, in __call__
      outputs = self.forward(*inputs, **kwargs)
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/ppdet/modeling/propos
al_generator/rpn_head.py"", line 131, in forward
      anchors = self.anchor_generator(rpn_feats)
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
layers.py"", line 914, in __call__
      outputs = self.forward(*inputs, **kwargs)
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/ppdet/modeling/propos
al_generator/anchor_generator.py"", line 125, in forward
      anchors_over_all_feature_maps = self._grid_anchors(grid_sizes)
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/ppdet/modeling/propos
al_generator/anchor_generator.py"", line 113, in _grid_anchors
      shift_x, shift_y = self._create_grid_offsets(size, stride,
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/ppdet/modeling/propos
al_generator/anchor_generator.py"", line 99, in _create_grid_offsets
      grid_height, grid_width = size[0], size[1]
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/framewor
k.py"", line 1783, in __getitem__
      return _getitem_impl_(self, item)
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/variable
_index.py"", line 459, in _getitem_impl_
      target_block.append_op(
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/framewor
k.py"", line 3178, in append_op
      op = Operator(
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/framewor
k.py"", line 2224, in __init__
      for frame in traceback.extract_stack():"
Release TinyPose qat全量化版本！,PaddlePaddle/PaddleDetection,2022-07-11 10:34:03,1,,6402,1300519692,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

Hi，
1 请问下 TinyPose模型的自动化压缩代码无bug版本的整理好了吗？
2 是否有文档，确保过程无误？
3 coco数据集qat全量化后掉点多少，业务最终掉点多少呢？
多谢

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
Paddle_inference 推理报错，CreatePredictor时间较长,PaddlePaddle/PaddleDetection,2022-07-11 03:01:08,0,,6396,1300138962,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

问题1：类ObjectDetector中LoadModel函数中predictor_ = std::move(CreatePredictor(config)); 该句话执行时间超过三分钟
问题2：在ObejectDetector::Predict函数中  predictor_->Run();执行第一张图时能够正常有结果，当执行第二张图时报如下错误
Compile Traceback (most recent call last):
    File ""/snap/pycharm-community/286/plugins/python-ce/helpers/pydev/pydevd.py""
, line 2181, in <module>
      main()
    File ""/snap/pycharm-community/286/plugins/python-ce/helpers/pydev/pydevd.py""
, line 2172, in main
      globals = debugger.run(setup['file'], None, None, is_module)
    File ""/snap/pycharm-community/286/plugins/python-ce/helpers/pydev/pydevd.py""
, line 1484, in run
      return self._exec(is_module, entry_point_fn, module_name, file, globals, l
ocals)
    File ""/snap/pycharm-community/286/plugins/python-ce/helpers/pydev/pydevd.py""
, line 1491, in _exec
      pydev_imports.execfile(file, globals, locals)  # execute the script
    File ""/snap/pycharm-community/286/plugins/python-ce/helpers/pydev/_pydev_imp
s/_pydev_execfile.py"", line 18, in execfile
      exec(compile(contents+""\n"", file, 'exec'), glob, loc)
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/tools/export_model.py
"", line 115, in <module>
      main()
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/tools/export_model.py
"", line 111, in main
      run(FLAGS, cfg)
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/tools/export_model.py
"", line 77, in run
      trainer.export(FLAGS.output_dir)
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/ppdet/engine/trainer.
py"", line 686, in export
      static_model, pruned_input_spec = self._get_infer_cfg_and_input_spec(
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/ppdet/engine/trainer.
py"", line 664, in _get_infer_cfg_and_input_spec
      input_spec, static_model.forward.main_program,
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
dygraph_to_static/program_translator.py"", line 563, in main_program
      concrete_program = self.concrete_program
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
dygraph_to_static/program_translator.py"", line 479, in concrete_program
      return self.concrete_program_specify_input_spec(input_spec=None)
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
dygraph_to_static/program_translator.py"", line 516, in concrete_program_specify_
input_spec
      concrete_program, _ = self.get_concrete_program(
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
dygraph_to_static/program_translator.py"", line 427, in get_concrete_program
      concrete_program, partial_program_layer = self._program_cache[cache_key]
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
dygraph_to_static/program_translator.py"", line 744, in __getitem__
      self._caches[item] = self._build_once(item)
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
dygraph_to_static/program_translator.py"", line 730, in _build_once
      concrete_program = ConcreteProgram.from_func_spec(
    File ""<decorator-gen-206>"", line 2, in from_func_spec

    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/wrapped_
decorator.py"", line 25, in __impl__
      return wrapped_func(*args, **kwargs)
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
base.py"", line 40, in __impl__
      return func(*args, **kwargs)
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
dygraph_to_static/program_translator.py"", line 683, in from_func_spec
      outputs = static_func(*inputs)
    File ""/tmp/tmpqpqzpt3g.py"", line 42, in forward
      out = paddle.jit.dy2static.convert_ifelse(self.training, true_fn_2,
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
dygraph_to_static/convert_operators.py"", line 211, in convert_ifelse
      out = _run_py_ifelse(pred, true_fn, false_fn, true_args, false_args)
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
dygraph_to_static/convert_operators.py"", line 271, in _run_py_ifelse
      return true_fn(*true_args) if pred else false_fn(*false_args)
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/ppdet/modeling/archit
ectures/meta_arch.py"", line 72, in forward
      out = self.get_pred()
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/ppdet/modeling/archit
ectures/faster_rcnn.py"", line 104, in get_pred
      bbox_pred, bbox_num = self._forward()
    File ""/tmp/tmp4dutrvx3.py"", line 53, in _forward
      _, __return_value_0, rois, rois_num = paddle.jit.dy2static.convert_ifelse(

    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
dygraph_to_static/convert_operators.py"", line 211, in convert_ifelse
      out = _run_py_ifelse(pred, true_fn, false_fn, true_args, false_args)
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
dygraph_to_static/convert_operators.py"", line 271, in _run_py_ifelse
      return true_fn(*true_args) if pred else false_fn(*false_args)
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/ppdet/modeling/archit
ectures/faster_rcnn.py"", line 81, in _forward
      rois, rois_num, _ = self.rpn_head(body_feats, self.inputs)
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
layers.py"", line 914, in __call__
      outputs = self.forward(*inputs, **kwargs)
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/ppdet/modeling/propos
al_generator/rpn_head.py"", line 131, in forward
      anchors = self.anchor_generator(rpn_feats)
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/
layers.py"", line 914, in __call__
      outputs = self.forward(*inputs, **kwargs)
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/ppdet/modeling/propos
al_generator/anchor_generator.py"", line 125, in forward
      anchors_over_all_feature_maps = self._grid_anchors(grid_sizes)
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/ppdet/modeling/propos
al_generator/anchor_generator.py"", line 113, in _grid_anchors
      shift_x, shift_y = self._create_grid_offsets(size, stride,
    File ""/home/tjzn/Downloads/PaddleDetection-release-2.3/ppdet/modeling/propos
al_generator/anchor_generator.py"", line 99, in _create_grid_offsets
      grid_height, grid_width = size[0], size[1]
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/framewor
k.py"", line 1783, in __getitem__
      return _getitem_impl_(self, item)
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/variable
_index.py"", line 459, in _getitem_impl_
      target_block.append_op(
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/framewor
k.py"", line 3178, in append_op
      op = Operator(
    File ""/home/tjzn/anaconda3/lib/python3.8/site-packages/paddle/fluid/framewor
k.py"", line 2224, in __init__
      for frame in traceback.extract_stack():

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
Not support stack backtrace yet.

----------------------
Error Message Summary:
----------------------
InvalidArgumentError: decrease dim should be 1
  [Hint: Expected out_dims[decrease_axis[i]] == 1, but received out_dims[decreas
e_axis[i]]:0 != 1:1.] (at C:/home/workspace/Paddle_release5/paddle/fluid/operato
rs/slice_op.cc:147)
  [operator < slice > error]

问题详细描述：
使用Ubuntu系统PaddleDetection2.3 release 版本 export_model.py 导出faster_rcnn_r50_1x_voc.yml训练的模型
在win7系统上进行部署测试。
导出模型的infer_cfg.yml
    mode: fluid
    draw_threshold: 0.5
    metric: VOC
    use_dynamic_shape: true
    arch: RCNN
    …..
    …..
当mode为“fluid”时上面的问题1与问题2都不会发生，当直接修改mode:为trt_fp16以后，问题1与问题2就会必现（是手动直接修改infer_cfg.yml中的mode:导出配置没有做任何修改，这样是否有问题？如果想使用TensorRT,是否export_model.py 时需要特殊修改参数？）。

两个系统配置如下：
1.Win7 paddle_inference的版本如下
GIT COMMIT ID: 1e62c239d323354eccfc974d4e2e6496f93d848e
WITH_MKL: ON
WITH_MKLDNN: ON
WITH_GPU: ON
WITH_ROCM: OFF
CUDA version: 10.2
CUDNN version: v7.6
CXX compiler version: 19.16.27045.0
WITH_TENSORRT: ON
TensorRT version: v7
显卡RTX2070Super

2.Ubuntu系统上使用的环境paddle版本：2.2.1
paddleDetection 版本2.3.0
操作系统Ubuntu18.04
python版本3.8.5
CUDA 版本11.1
cuDNN 版本 8.0.5
显卡为Geforce RTX3060 12G 显存


期待问题得到指导解决
"
ppyolo 预测list index out of range问题,PaddlePaddle/PaddleDetection,2022-07-11 02:41:26,2,,6395,1300129749,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

环境：
paddlepaddle 2.2
kylin v10
os：ft2000+
paddledetection：2.4

指令：python3 deploy/python/infer.py --model_dir=output_inference/ppyolo_r50vd_dcn_1x_coco --image_dir=demo  --device=CPU
报错日志：
-----------  Running Arguments -----------
batch_size: 1
camera_id: -1
cpu_threads: 1
device: CPU
enable_mkldnn: False
image_dir: demo
image_file: None
model_dir: output_inference/ppyolo_r50vd_dcn_1x_coco
output_dir: output
reid_batch_size: 50
reid_model_dir: None
run_benchmark: False
run_mode: paddle
save_images: False
save_mot_txt_per_img: False
save_mot_txts: False
scaled: False
threshold: 0.5
trt_calib_mode: False
trt_max_shape: 1280
trt_min_shape: 1
trt_opt_shape: 640
use_dark: True
use_gpu: False
video_file: None
------------------------------------------
-----------  Model Configuration -----------
Model Arch: YOLO
Transform Order: 
--transform op: Resize
--transform op: NormalizeImage
--transform op: Permute
--------------------------------------------
---    Fused 0 subgraphs into layer_norm op.
Found 10 inference images in total.
class_id:0, confidence:1.0000, left_top:[0.00,113.84],right_bottom:[639.00,113.84]
class_id:0, confidence:1.0000, left_top:[0.00,309.32],right_bottom:[639.00,309.32]
class_id:0, confidence:1.0000, left_top:[0.00,208.58],right_bottom:[639.00,208.58]
class_id:0, confidence:1.0000, left_top:[0.00,12.47],right_bottom:[639.00,12.47]
class_id:0, confidence:1.0000, left_top:[0.00,259.11],right_bottom:[639.00,259.11]
class_id:0, confidence:1.0000, left_top:[0.00,467.53],right_bottom:[639.00,467.53]
class_id:0, confidence:1.0000, left_top:[0.00,233.53],right_bottom:[639.00,233.53]
class_id:0, confidence:1.0000, left_top:[0.00,126.47],right_bottom:[639.00,126.47]
class_id:0, confidence:1.0000, left_top:[0.00,158.05],right_bottom:[639.00,158.05]
class_id:0, confidence:1.0000, left_top:[0.00,202.26],right_bottom:[639.00,202.26]
class_id:0, confidence:1.0000, left_top:[0.00,473.84],right_bottom:[639.00,473.84]
class_id:0, confidence:1.0000, left_top:[622.95,315.95],right_bottom:[622.95,315.95]
class_id:0, confidence:1.0000, left_top:[622.95,290.68],right_bottom:[622.95,290.68]
class_id:0, confidence:1.0000, left_top:[0.00,189.63],right_bottom:[639.00,189.63]
class_id:0, confidence:1.0000, left_top:[0.00,372.79],right_bottom:[639.00,372.79]
class_id:0, confidence:1.0000, left_top:[0.00,183.00],right_bottom:[639.00,183.00]
class_id:0, confidence:1.0000, left_top:[0.00,50.68],right_bottom:[639.00,50.68]
class_id:0, confidence:1.0000, left_top:[0.00,31.74],right_bottom:[639.00,31.74]
class_id:0, confidence:1.0000, left_top:[0.00,315.95],right_bottom:[639.00,315.95]
class_id:0, confidence:1.0000, left_top:[0.00,265.11],right_bottom:[639.00,265.11]
class_id:0, confidence:1.0000, left_top:[0.00,50.84],right_bottom:[639.00,50.84]
class_id:0, confidence:1.0000, left_top:[0.00,233.84],right_bottom:[639.00,233.84]
class_id:0, confidence:1.0000, left_top:[0.00,177.00],right_bottom:[639.00,177.00]
class_id:0, confidence:1.0000, left_top:[0.00,278.05],right_bottom:[639.00,278.05]
class_id:0, confidence:1.0000, left_top:[0.00,271.74],right_bottom:[639.00,271.74]
class_id:0, confidence:1.0000, left_top:[0.00,328.26],right_bottom:[639.00,328.26]
class_id:0, confidence:1.0000, left_top:[0.00,290.68],right_bottom:[639.00,290.68]
class_id:0, confidence:1.0000, left_top:[622.95,385.42],right_bottom:[622.95,385.42]
class_id:0, confidence:1.0000, left_top:[0.00,0.00],right_bottom:[639.00,-0.16]
class_id:0, confidence:1.0000, left_top:[622.95,94.89],right_bottom:[622.95,94.89]
class_id:0, confidence:1.0000, left_top:[622.95,50.68],right_bottom:[622.95,50.68]
class_id:0, confidence:1.0000, left_top:[0.00,221.21],right_bottom:[639.00,221.21]
class_id:0, confidence:1.0000, left_top:[0.00,353.37],right_bottom:[639.00,353.37]
class_id:0, confidence:1.0000, left_top:[622.95,158.05],right_bottom:[622.95,158.05]
class_id:0, confidence:1.0000, left_top:[0.00,119.84],right_bottom:[639.00,119.84]
class_id:0, confidence:1.0000, left_top:[0.00,284.37],right_bottom:[639.00,284.37]
class_id:0, confidence:1.0000, left_top:[0.00,151.74],right_bottom:[639.00,151.74]
class_id:0, confidence:1.0000, left_top:[0.00,170.68],right_bottom:[639.00,170.68]
class_id:0, confidence:1.0000, left_top:[0.00,195.63],right_bottom:[639.00,195.63]
class_id:0, confidence:1.0000, left_top:[0.00,82.26],right_bottom:[639.00,82.26]
class_id:0, confidence:1.0000, left_top:[0.00,214.58],right_bottom:[639.00,214.58]
class_id:0, confidence:1.0000, left_top:[0.00,360.16],right_bottom:[639.00,360.16]
class_id:0, confidence:1.0000, left_top:[0.00,145.42],right_bottom:[639.00,145.42]
class_id:0, confidence:1.0000, left_top:[0.00,120.16],right_bottom:[639.00,120.16]
class_id:0, confidence:1.0000, left_top:[34.11,189.16],right_bottom:[34.11,189.16]
class_id:0, confidence:1.0000, left_top:[0.00,461.21],right_bottom:[639.00,461.21]
class_id:0, confidence:1.0000, left_top:[622.95,139.11],right_bottom:[622.95,139.11]
class_id:0, confidence:1.0000, left_top:[0.00,435.84],right_bottom:[639.00,435.84]
class_id:0, confidence:1.0000, left_top:[0.00,467.05],right_bottom:[-0.29,467.05]
class_id:0, confidence:1.0000, left_top:[0.00,0.00],right_bottom:[639.00,-0.16]
class_id:0, confidence:1.0000, left_top:[0.00,265.42],right_bottom:[639.00,265.42]
class_id:0, confidence:1.0000, left_top:[0.00,227.53],right_bottom:[639.00,227.53]
class_id:0, confidence:1.0000, left_top:[0.00,321.95],right_bottom:[639.00,321.95]
class_id:0, confidence:1.0000, left_top:[0.00,233.84],right_bottom:[639.00,233.84]
class_id:0, confidence:1.0000, left_top:[0.00,427.51],right_bottom:[639.00,427.51]
class_id:0, confidence:1.0000, left_top:[0.00,94.58],right_bottom:[639.00,94.58]
class_id:0, confidence:1.0000, left_top:[0.00,214.89],right_bottom:[639.00,214.89]
class_id:0, confidence:1.0000, left_top:[0.00,322.26],right_bottom:[639.00,322.26]
class_id:0, confidence:1.0000, left_top:[0.00,164.37],right_bottom:[639.00,164.37]
class_id:0, confidence:1.0000, left_top:[622.95,435.95],right_bottom:[622.95,435.95]
class_id:0, confidence:1.0000, left_top:[0.00,195.95],right_bottom:[639.00,195.95]
class_id:0, confidence:1.0000, left_top:[0.00,0.00],right_bottom:[639.00,-0.16]
class_id:0, confidence:1.0000, left_top:[0.00,398.05],right_bottom:[639.00,398.05]
class_id:0, confidence:1.0000, left_top:[0.00,0.00],right_bottom:[639.00,-0.16]
class_id:0, confidence:1.0000, left_top:[622.95,404.37],right_bottom:[622.95,404.37]
class_id:0, confidence:1.0000, left_top:[0.00,57.00],right_bottom:[639.00,57.00]
class_id:0, confidence:1.0000, left_top:[0.00,403.89],right_bottom:[639.00,403.89]
class_id:0, confidence:1.0000, left_top:[0.00,63.32],right_bottom:[639.00,63.32]
class_id:0, confidence:1.0000, left_top:[622.95,391.74],right_bottom:[622.95,391.74]
class_id:0, confidence:1.0000, left_top:[0.00,252.79],right_bottom:[639.00,252.79]
class_id:0, confidence:1.0000, left_top:[0.00,341.21],right_bottom:[639.00,341.21]
class_id:0, confidence:1.0000, left_top:[0.00,94.89],right_bottom:[639.00,94.89]
class_id:0, confidence:1.0000, left_top:[0.00,391.26],right_bottom:[639.00,391.26]
class_id:0, confidence:1.0000, left_top:[622.95,410.68],right_bottom:[622.95,410.68]
class_id:0, confidence:1.0000, left_top:[0.00,353.84],right_bottom:[639.00,353.84]
class_id:0, confidence:1.0000, left_top:[0.00,0.00],right_bottom:[639.00,-0.16]
class_id:0, confidence:1.0000, left_top:[622.95,183.32],right_bottom:[622.95,183.32]
class_id:0, confidence:1.0000, left_top:[0.00,170.37],right_bottom:[639.00,170.37]
class_id:0, confidence:1.0000, left_top:[0.00,328.58],right_bottom:[639.00,328.58]
class_id:0, confidence:1.0000, left_top:[34.11,151.26],right_bottom:[34.11,151.26]
class_id:0, confidence:1.0000, left_top:[0.00,429.63],right_bottom:[639.00,429.63]
class_id:0, confidence:1.0000, left_top:[0.00,164.05],right_bottom:[639.00,164.05]
class_id:0, confidence:1.0000, left_top:[0.00,75.95],right_bottom:[639.00,75.95]
class_id:0, confidence:1.0000, left_top:[0.00,334.89],right_bottom:[639.00,334.89]
class_id:0, confidence:1.0000, left_top:[0.00,303.32],right_bottom:[639.00,303.32]
class_id:0, confidence:1.0000, left_top:[0.00,157.74],right_bottom:[639.00,157.74]
class_id:0, confidence:1.0000, left_top:[0.00,107.53],right_bottom:[639.00,107.53]
class_id:0, confidence:1.0000, left_top:[0.00,290.37],right_bottom:[639.00,290.37]
class_id:0, confidence:1.0000, left_top:[0.00,0.00],right_bottom:[639.00,-0.16]
class_id:0, confidence:1.0000, left_top:[0.00,0.00],right_bottom:[639.00,-0.16]
class_id:0, confidence:1.0000, left_top:[0.00,69.63],right_bottom:[639.00,69.63]
class_id:0, confidence:1.0000, left_top:[0.00,183.32],right_bottom:[639.00,183.32]
class_id:0, confidence:1.0000, left_top:[0.00,340.89],right_bottom:[639.00,340.89]
class_id:0, confidence:1.0000, left_top:[0.00,132.79],right_bottom:[639.00,132.79]
class_id:0, confidence:1.0000, left_top:[0.00,25.11],right_bottom:[639.00,25.11]
class_id:0, confidence:1.0000, left_top:[0.00,0.00],right_bottom:[639.00,-0.16]
class_id:0, confidence:1.0000, left_top:[0.00,0.00],right_bottom:[639.00,-0.16]
class_id:0, confidence:1.0000, left_top:[0.00,284.05],right_bottom:[639.00,284.05]
class_id:0, confidence:1.0000, left_top:[622.95,366.47],right_bottom:[622.95,366.47]
class_id:0, confidence:1.0000, left_top:[622.95,347.53],right_bottom:[622.95,347.53]
save result to: output/000000087038.jpg
class_id:0, confidence:1.0000, left_top:[0.00,0.00],right_bottom:[639.00,-0.16]
class_id:0, confidence:1.0000, left_top:[0.00,0.00],right_bottom:[639.00,-0.16]
class_id:0, confidence:1.0000, left_top:[0.00,0.00],right_bottom:[639.00,-0.16]
class_id:0, confidence:1.0000, left_top:[0.00,480.16],right_bottom:[639.00,479.00]
class_id:0, confidence:1.0000, left_top:[0.00,0.00],right_bottom:[639.00,-0.16]
class_id:0, confidence:1.0000, left_top:[0.00,0.00],right_bottom:[639.00,-0.16]
class_id:0, confidence:1.0000, left_top:[0.00,0.00],right_bottom:[639.00,-0.16]
class_id:0, confidence:1.0000, left_top:[0.00,0.00],right_bottom:[639.00,-0.16]
class_id:0, confidence:1.0000, left_top:[0.00,0.00],right_bottom:[639.00,-0.16]
class_id:0, confidence:1.0000, left_top:[0.00,0.00],right_bottom:[639.00,-0.16]
class_id:0, confidence:1.0000, left_top:[0.00,0.00],right_bottom:[639.00,-0.16]
class_id:0, confidence:1.0000, left_top:[0.00,0.00],right_bottom:[639.00,-0.16]
class_id:0, confidence:1.0000, left_top:[0.00,0.00],right_bottom:[639.00,-0.16]
class_id:0, confidence:1.0000, left_top:[0.00,480.16],right_bottom:[639.00,479.00]
Traceback (most recent call last):
  File ""deploy/python/infer.py"", line 778, in <module>
    main()
  File ""deploy/python/infer.py"", line 738, in main
    predict_image(detector, img_list, FLAGS.batch_size)
  File ""deploy/python/infer.py"", line 664, in predict_image
    threshold=FLAGS.threshold)
  File ""deploy/python/infer.py"", line 622, in visualize
    image_file, im_results, labels, threshold=threshold)
  File ""/workspace/PaddleDetection/deploy/python/visualize.py"", line 47, in visualize_box_mask
    im = draw_box(im, results['boxes'], labels, threshold=threshold)
  File ""/workspace/PaddleDetection/deploy/python/visualize.py"", line 136, in draw_box
    clsid2color[clsid] = color_list[clsid]
IndexError: list index out of range
"
关于ppyoloe在jetson nano2g上的推理速度,PaddlePaddle/PaddleDetection,2022-07-11 01:21:24,3,,6394,1300081166,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

设备是jetson nano 2g的设备。网络是ppyoloes_300e。图像输入大小是640.开启了tensorrt， 精度是fp16。
在该设备的推理速度平均160多毫秒，这样子是正常的吗？？"
Batched Inference using paddleLite,PaddlePaddle/PaddleDetection,2022-07-08 21:50:02,2,,6391,1299468482,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

Hi, is there a way that we can batch images for inference in the android demo of picodet?
"
ppyoloe训练报错ValueError: Target *** is out of upper/lower bound.,PaddlePaddle/PaddleDetection,2022-07-07 04:10:16,13,,6380,1296779769,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

ppyoloe训练报错ValueError: Target *** is out of upper bound.

训练配置：
**ppyoloe_crn_l_300e_coco.yml**
_BASE_: [
  '../datasets/coco_detection.yml',
  '../runtime.yml',
  './_base_/optimizer_300e.yml',
  './_base_/ppyoloe_crn.yml',
  './_base_/ppyoloe_reader.yml',
]

log_iter: 100
snapshot_epoch: 10
weights: output/ppyoloe_crn_l_300e_coco/model_final

pretrain_weights: https://paddledet.bj.bcebos.com/models/pretrained/CSPResNetb_l_pretrained.pdparams
depth_mult: 1.0
width_mult: 1.0


**coco_detection.yml**
metric: COCO
num_classes: 80

TrainDataset:
  !COCODataSet
    image_dir: train2017
    anno_path: annotations/instances_train2017.json
    dataset_dir: dataset/coco
    data_fields: ['image', 'gt_bbox', 'gt_class', 'is_crowd']

EvalDataset:
  !COCODataSet
    image_dir: val2017
    anno_path: annotations/instances_val2017.json
    dataset_dir: dataset/coco

TestDataset:
  !ImageFolder
    anno_path: annotations/instances_val2017.json # also support txt (like VOC's label_list.txt)
    dataset_dir: dataset/coco # if set, anno_path will be 'dataset_dir/anno_path'


**optimizer_300e.yml**
epoch: 300

LearningRate:
  base_lr: 0.0025
  #0.0025 = 0.025 * (16 * 1) / (20 * 8)
  #lrnew = lrdefault * (batch_sizenew * GPU_numbernew) / (batch_sizedefault * GPU_numberdefault).
  schedulers:
    - !CosineDecay
      max_epochs: 360
    - !LinearWarmup
      start_factor: 0.
      epochs: 5

OptimizerBuilder:
  optimizer:
    momentum: 0.9
    type: Momentum
  regularizer:
    factor: 0.0005
    type: L2


**ppyoloe_reader.yml**
worker_num: 4
eval_height: &eval_height 640
eval_width: &eval_width 640
eval_size: &eval_size [*eval_height, *eval_width]

TrainReader:
  sample_transforms:
    - Decode: {}
    - RandomDistort: {}
    - RandomExpand: {fill_value: [123.675, 116.28, 103.53]}
    - RandomCrop: {}
    - RandomFlip: {}
  batch_transforms:
    - BatchRandomResize: {target_size: [320, 352, 384, 416, 448, 480, 512, 544, 576, 608, 640, 672, 704, 736, 768], random_size: True, random_interp: True, keep_ratio: False}
    - NormalizeImage: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225], is_scale: True}
    - Permute: {}
    - PadGT: {}
  batch_size: 16
  shuffle: true
  drop_last: true
  use_shared_memory: true
  collate_batch: true

EvalReader:
  sample_transforms:
    - Decode: {}
    - Resize: {target_size: *eval_size, keep_ratio: False, interp: 2}
    - NormalizeImage: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225], is_scale: True}
    - Permute: {}
  batch_size: 2

TestReader:
  inputs_def:
    image_shape: [3, *eval_height, *eval_width]
  sample_transforms:
    - Decode: {}
    - Resize: {target_size: *eval_size, keep_ratio: False, interp: 2}
    - NormalizeImage: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225], is_scale: True}
    - Permute: {}
  batch_size: 1


综上，改动的配置有：
1.optimizer_300e.yml中base_lr改为0.0025；
2.ppyoloe_reader.yml中TrainReader的batch_size改为16
其他配置没动。

运行指令：
`python tools/train.py -c configs/ppyoloe/ppyoloe_crn_l_300e_coco.yml`

出现报错：

(paddle_env) H:\zhujunyu\PaddleDetection-develop1>python tools/train.py -c configs/ppyoloe/ppyoloe_crn_l_300e_coco.yml
Warning: import ppdet from source directory without installing, run 'python setup.py install' to install ppdet firstly
[07/07 11:40:31] ppdet.utils.download WARNING: Config annotation dataset/coco\annotations/instances_train2017.json is not a file, dataset config is not val
id
[07/07 11:40:31] ppdet.utils.download INFO: Dataset H:\zhujunyu\PaddleDetection-develop1\dataset\coco is not valid for reason above, try searching C:\Users
\Administrator/.cache/paddle/dataset or downloading dataset...
loading annotations into memory...
Done (t=12.40s)
creating index...
index created!
[07/07 11:40:49] ppdet.data.source.coco WARNING: Found an invalid bbox in annotations: im_id: 200365, area: 0.0 x1: 296.65, y1: 388.33, x2: 297.67999999999
995, y2: 388.33.
[07/07 11:41:00] ppdet.data.source.coco WARNING: Found an invalid bbox in annotations: im_id: 550395, area: 0.0 x1: 9.98, y1: 188.56, x2: 15.52, y2: 188.56
.
W0707 11:41:02.102504 16816 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 11.6, Runtime API Version: 11.2
W0707 11:41:02.107491 16816 gpu_context.cc:306] device: 0, cuDNN Version: 8.2.
[07/07 11:41:06] ppdet.utils.checkpoint INFO: Finish loading model weights: C:\Users\Administrator/.cache/paddle/weights\CSPResNetb_l_pretrained.pdparams
Traceback (most recent call last):
  File ""H:\zhujunyu\PaddleDetection-develop1\tools\train.py"", line 172, in <module>
    main()
  File ""H:\zhujunyu\PaddleDetection-develop1\tools\train.py"", line 168, in main
    run(FLAGS, cfg)
  File ""H:\zhujunyu\PaddleDetection-develop1\tools\train.py"", line 132, in run
    trainer.train(FLAGS.eval)
  File ""H:\zhujunyu\PaddleDetection-develop1\ppdet\engine\trainer.py"", line 454, in train
    outputs = model(data)
  File ""E:\ProgramData\Anaconda3\envs\paddle_env\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""E:\ProgramData\Anaconda3\envs\paddle_env\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""H:\zhujunyu\PaddleDetection-develop1\ppdet\modeling\architectures\meta_arch.py"", line 59, in forward
    out = self.get_loss()
  File ""H:\zhujunyu\PaddleDetection-develop1\ppdet\modeling\architectures\yolo.py"", line 124, in get_loss
    return self._forward()
  File ""H:\zhujunyu\PaddleDetection-develop1\ppdet\modeling\architectures\yolo.py"", line 88, in _forward
    yolo_losses = self.yolo_head(neck_feats, self.inputs)
  File ""E:\ProgramData\Anaconda3\envs\paddle_env\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""E:\ProgramData\Anaconda3\envs\paddle_env\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""H:\zhujunyu\PaddleDetection-develop1\ppdet\modeling\heads\ppyoloe_hea
d.py"", line 218, in forward
    return self.forward_train(feats, targets)
  File ""H:\zhujunyu\PaddleDetection-develop1\ppdet\modeling\heads\ppyoloe_hea
d.py"", line 158, in forward_train
    return self.get_loss([
  File ""H:\zhujunyu\PaddleDetection-develop1\ppdet\modeling\heads\ppyoloe_head.py"", line 354, in get_loss
    self._bbox_loss(pred_distri, pred_bboxes, anchor_points_s,
  File ""H:\zhujunyu\PaddleDetection-develop1\ppdet\modeling\heads\ppyoloe_head.py"", line 291, in _bbox_loss
    loss_dfl = self._df_loss(pred_dist_pos,
  File ""H:\zhujunyu\PaddleDetection-develop1\ppdet\modeling\heads\ppyoloe_head.py"", line 256, in _df_loss
    loss_left = F.cross_entropy(
  File ""E:\ProgramData\Anaconda3\envs\paddle_env\lib\site-packages\paddle\nn\functional\loss.py"", line 1719, in cross_entropy
    raise ValueError(""Target {} is out of upper bound."".format(
ValueError: Target 604 is out of upper bound.

若“Target *** ”中 ***为负数，则为ValueError: Target *** is out of lower bound.
例如：
    raise ValueError(""Target {} is out of lower bound."".format(
ValueError: Target -3 is out of lower bound.

请问该如何解决呢？"
PPYOLOE训练速度慢，lr 设置的是 0.005，但是训练几十个epoch后，变成0.024左右,PaddlePaddle/PaddleDetection,2022-07-07 03:14:41,11,,6379,1296739939,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

使用 ppyoloe_crn_s_400e_coco.yml   
训练8000多张1200*1600的图片, 单卡3090训练，base_lr: 0.005.    batch_size: 32
因为训练中出现一次错误，查了一下 issues 所以把ppyoloe_reader.yml里面的 num_works 设置为0；
 然后可以正常训练，但是训练时间提示为5天，而且lr变成0.024，不知道是什么情况；
![企业微信截图_79222c2d-b42f-4fd3-9404-345b394e0d30](https://user-images.githubusercontent.com/16578290/177682931-3af8aa2d-307e-410b-8402-932b0376f2fc.png)



### 复现环境 Environment

- PaddlePaddle: 2.2.2
- PaddleDetection: release/2.4
- Pyhton: 3.7.13
- CUDA: 11.1
- CUDNN: 8.0.2

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
模型修改问题,PaddlePaddle/PaddleDetection,2022-07-06 09:46:57,7,,6371,1295570848,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

您好，我最近想对于用export_model.py导出的模型文件中添加复制删除节点，但是不知道export_model是如何根据训练参数构造模型结构的，我们的模型是利用paddledetection的模型config文件训练的。利用export_model导出。"
"在使用COCO数据集训练RetinaNet时（并未修改任何参数），出现OSError: (External) CUDA error(719), unspecified launch failure.",PaddlePaddle/PaddleDetection,2022-07-06 02:41:49,8,,6366,1295040202,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

- python tools/train.py -c configs/retinanet/retinanet_r50_fpn_1x_coco.yml --eval -o use_gpu=ture

会报错：
[07/05 21:38:33] ppdet.utils.checkpoint INFO: Finish loading model weights: C:\Users\Hubery/.cache/paddle/weights\ResNet50_cos_pretrained.pdparams
Error: ../paddle/phi/kernels/funcs/gather.cu.h:67 Assertion index_value >= 0 && index_value < input_dims[j] failed. The index is out of bounds, please check whether the dimensions of index and input meet the requirements. It should be less than [182403] and greater than or equal to 0, but received [0]   有很多行跟这个一样的错误

Traceback (most recent call last):
File ""tools/train.py"", line 177, in
main()
File ""tools/train.py"", line 173, in main
run(FLAGS, cfg)
File ""tools/train.py"", line 127, in run
trainer.train(FLAGS.eval)
File ""D:\deep_learning\PaddleDetection\ppdet\engine\trainer.py"", line 448, in train
outputs = model(data)
File ""C:\Users\Hubery\anaconda3\envs\paddle_env\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in call
return self._dygraph_call_func(*inputs, **kwargs)
File ""C:\Users\Hubery\anaconda3\envs\paddle_env\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
outputs = self.forward(*inputs, **kwargs)
File ""D:\deep_learning\PaddleDetection\ppdet\modeling\architectures\meta_arch.py"", line 59, in forward
out = self.get_loss()
File ""D:\deep_learning\PaddleDetection\ppdet\modeling\architectures\retinanet.py"", line 65, in get_loss
return self._forward()
File ""D:\deep_learning\PaddleDetection\ppdet\modeling\architectures\retinanet.py"", line 57, in _forward
return self.head(neck_feats, self.inputs)
File ""C:\Users\Hubery\anaconda3\envs\paddle_env\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in call
return self._dygraph_call_func(*inputs, **kwargs)
File ""C:\Users\Hubery\anaconda3\envs\paddle_env\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
outputs = self.forward(*inputs, **kwargs)
File ""D:\deep_learning\PaddleDetection\ppdet\modeling\heads\retina_head.py"", line 105, in forward
return self.get_loss([cls_logits_list, bboxes_reg_list], targets)
File ""D:\deep_learning\PaddleDetection\ppdet\modeling\heads\retina_head.py"", line 160, in get_loss
cls_tar = gt_class[matches[chosen_mask]]
File ""C:\Users\Hubery\anaconda3\envs\paddle_env\lib\site-packages\paddle\fluid\dygraph\varbase_patch_methods.py"", line 735, in getitem
return getitem_impl(self, item)
File ""C:\Users\Hubery\anaconda3\envs\paddle_env\lib\site-packages\paddle\fluid\variable_index.py"", line 430, in getitem_impl
return get_value_for_bool_tensor(var, slice_item)
File ""C:\Users\Hubery\anaconda3\envs\paddle_env\lib\site-packages\paddle\fluid\variable_index.py"", line 310, in get_value_for_bool_tensor
lambda: idx_empty(var))
File ""C:\Users\Hubery\anaconda3\envs\paddle_env\lib\site-packages\paddle\fluid\layers\control_flow.py"", line 2452, in cond
pred = pred.numpy()[0]
OSError: (External) CUDA error(719), unspecified launch failure.
[Hint: 'cudaErrorLaunchFailure'. An exception occurred on the device while executing a kernel. Common causes include dereferencing an invalid device pointerand accessing out of bounds shared memory. Less common cases can be
system specific - more information about these cases canbe found in the system specific user guide. This leaves the process in an inconsistent state and any further CUDA work willreturn the same error. To continue using CUDA, the process must be terminated and relaunched.] (at ..\paddle\phi\backends\gpu\cuda\cuda_info.cc:258)

请问该如何解决呢？

### 复现环境 Environment

-PaddlePaddle   2.3.0.post112
-paddledet   2.4.0 
-python -3.7
-cudatoolkit      11.2.2 
-cudnn          8.2.1.32

- 显卡型号：RTX 3090

>nvidia-smi

Tue Jul 5 21:12:59 2022
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 472.12 Driver Version: 472.12 CUDA Version: 11.4 |
|-------------------------------+----------------------+----------------------+
| GPU Name TCC/WDDM | Bus-Id Disp.A | Volatile Uncorr. ECC |
| 0 N/A N/A 13168 C+G ...lPanel\SystemSettings.exe N/A |
| 0 N/A N/A 13228 C+G ...cw5n1h2txyewy\LockApp.exe N/A |
| 0 N/A N/A 13284 C+G ...bbwe\Microsoft.Photos.exe N/A |
| 0 N/A N/A 13572 C+G ...ge\Application\msedge.exe N/A |
| 0 N/A N/A 18156 C+G ...2txyewy\TextInputHost.exe N/A |
| 0 N/A N/A 18600 C+G ...mathpix-snipping-tool.exe N/A |
| 0 N/A N/A 19792 C+G ...y\ShellExperienceHost.exe N/A |
| 0 N/A N/A 20504 C+G ...1\jbr\bin\jcef_helper.exe N/A |
| 0 N/A N/A 20676 C+G ...264.44\msedgewebview2.exe N/A |
| 0 N/A N/A 20984 C+G ...8wekyb3d8bbwe\GameBar.exe N/A |
+-----------------------------------------------------------------------------+

>nvcc -V

nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2020 NVIDIA Corporation
Built on Mon_Nov_30_19:15:10_Pacific_Standard_Time_2020
Cuda compilation tools, release 11.2, V11.2.67
Build cuda_11.2.r11.2/compiler.29373293_0

并且test_architectures.py和python tools/infer.py -c configs/ppyolo/ppyolo_r50vd_dcn_1x_coco.yml -o use_gpu=true weights=https://paddledet.bj.bcebos.com/models/ppyolo_r50vd_dcn_1x_coco.pdparams --infer_img=demo/000000014439.jpg测试通过。
.......
Ran 7 tests in 2.265s

OK


Done (t=0.75s)
creating index...
index created!
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1.19it/s]
[07/06 10:08:15] ppdet.engine INFO: Detection bbox results save in output\000000014439.jpg


- 跑yolov3就可以正常跑

python tools/train.py -c configs/yolov3/yolov3_darknet53_270e_coco.yml --eval -o use_gpu=ture

[07/06 10:39:58] ppdet.utils.checkpoint INFO: Finish loading model weights: C:\Users\Hubery/.cache/paddle/weights\DarkNet53_pretrained.pdparams
[07/06 10:40:00] ppdet.engine INFO: Epoch: [0] [    0/14658] learning_rate: 0.000000 loss_xy: 8.961815 loss_wh: 10.346878 loss_obj: 10171.745117 loss_cls: 193.526794 loss: 10384.580078 eta: 89 days, 16:38:52 batch_cost: 1.9581 data_cost: 0.0000 ips: 4.0856 images/s
[07/06 10:40:11] ppdet.engine INFO: Epoch: [0] [   20/14658] learning_rate: 0.000005 loss_xy: 17.073185 loss_wh: 17.941078 loss_obj: 831.352783 loss_cls: 329.363922 loss: 1140.583252 eta: 27 days, 14:10:25 batch_cost: 0.5345 data_cost: 0.3638 ips: 14.9660 images/s
.................................
### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
PPTracking C++ not found bouding box when output video,PaddlePaddle/PaddleDetection,2022-07-05 09:18:41,11,,6362,1294024858,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

I want to detect  multiple object tracking in Jetson with example in folder PaddleDetection/deploy/pptracking/cpp/build everything it's good but the output video not show bounding box for each object tracking. It's just a video with show fps in the corner. I used C++ for that and opencv 4.1.1 default. Below this is a CMakeList file:

```shell
# 是否使用GPU(即是否使用 CUDA)
# WITH_GPU=ON
WITH_GPU=ON

# 是否使用MKL or openblas，TX2需要设置为OFF
WITH_MKL=OFF

# 是否集成 TensorRT(仅WITH_GPU=ON 有效)
WITH_TENSORRT=ON

# paddle 预测库lib名称，由于不同平台不同版本预测库lib名称不同，请查看所下载的预测库中`paddle_inference/lib/`文件夹下`lib`的名称
PADDLE_LIB_NAME=libpaddle_inference

# TensorRT 的include路径
# TENSORRT_INC_DIR=/path/to/tensorrt/include
TENSORRT_INC_DIR=/usr/src/tensorrt/include

# TensorRT 的lib路径
# TENSORRT_LIB_DIR=/path/to/tensorrt/lib
TENSORRT_LIB_DIR=/usr/lib/aarch64-linux-gnu

# Paddle 预测库路径
PADDLE_DIR=home/user/paddle_inference

# CUDA 的 lib 路径
# CUDA_LIB=/path/to/cuda/lib
CUDA_LIB=/usr/local/cuda-10.2/lib64/

# CUDNN 的 lib 路径
# CUDNN_LIB=/path/to/cudnn/lib
CUDNN_LIB=/usr/lib/aarch64-linux-gnu/

MACHINE_TYPE=`uname -m`
echo ""MACHINE_TYPE: ""${MACHINE_TYPE}


# if [ ""$MACHINE_TYPE"" = ""x86_64"" ]
# then
#   echo ""set OPENCV_DIR for x86_64""
#   # linux系统通过以下命令下载预编译的opencv
#   mkdir -p $(pwd)/deps && cd $(pwd)/deps
#   wget -c https://paddledet.bj.bcebos.com/data/opencv-3.4.16_gcc8.2_ffmpeg.tar.gz
#   tar -xvf opencv-3.4.16_gcc8.2_ffmpeg.tar.gz && cd ..

#   # set OPENCV_DIR
#   OPENCV_DIR=$(pwd)/deps/opencv-3.4.16_gcc8.2_ffmpeg

# elif [ ""$MACHINE_TYPE"" = ""aarch64"" ]
# then
#   echo ""set OPENCV_DIR for aarch64""
#   # TX2平台通过以下命令下载预编译的opencv
#   mkdir -p $(pwd)/deps && cd $(pwd)/deps
#   wget -c https://bj.bcebos.com/v1/paddledet/data/TX2_JetPack4.3_opencv_3.4.6_gcc7.5.0.tar.gz
#   tar -xvf TX2_JetPack4.3_opencv_3.4.6_gcc7.5.0.tar.gz && cd ..

#   # set OPENCV_DIR
#   OPENCV_DIR=$(pwd)/deps/TX2_JetPack4.3_opencv_3.4.6_gcc7.5.0/

# else
#   echo ""Please set OPENCV_DIR manually""
# fi

# echo ""OPENCV_DIR: ""$OPENCV_DIR

mkdir -p $(pwd)/deps && cd $(pwd)/deps && cd ..

# 以下无需改动
rm -rf build
mkdir -p build
cd build
cmake .. \
    -DWITH_GPU=${WITH_GPU} \
    -DWITH_MKL=${WITH_MKL} \
    -DWITH_TENSORRT=${WITH_TENSORRT} \
    -DTENSORRT_LIB_DIR=${TENSORRT_LIB_DIR} \
    -DTENSORRT_INC_DIR=${TENSORRT_INC_DIR} \
    -DPADDLE_DIR=${PADDLE_DIR} \
    -DWITH_STATIC_LIB=${WITH_STATIC_LIB} \
    -DCUDA_LIB=${CUDA_LIB} \
    -DCUDNN_LIB=${CUDNN_LIB} \
    -DOPENCV_DIR=${OPENCV_DIR} \
    -DPADDLE_LIB_NAME=${PADDLE_LIB_NAME} \

make -j6
echo ""make finished!""

``` 
```shell
cmake_minimum_required(VERSION 3.0)
project(PaddleObjectDetector CXX C)

option(WITH_MKL        ""Compile demo with MKL/OpenBlas support,defaultuseMKL.""          ON)
option(WITH_GPU        ""Compile demo with GPU/CPU, default use CPU.""                    ON)
option(WITH_TENSORRT   ""Compile demo with TensorRT.""                                    OFF)

SET(PADDLE_DIR """" CACHE PATH ""Location of libraries"")
SET(PADDLE_LIB_NAME """" CACHE STRING ""libpaddle_inference"")
# SET(OPENCV_DIR """" CACHE PATH ""Location of libraries"")
SET(CUDA_LIB """" CACHE PATH ""Location of libraries"")
SET(CUDNN_LIB """" CACHE PATH ""Location of libraries"")
SET(TENSORRT_INC_DIR """" CACHE PATH ""Compile demo with TensorRT"")
SET(TENSORRT_LIB_DIR """" CACHE PATH ""Compile demo with TensorRT"")

include(cmake/yaml-cpp.cmake)

include_directories(""${CMAKE_SOURCE_DIR}/"")
include_directories(""${CMAKE_CURRENT_BINARY_DIR}/ext/yaml-cpp/src/ext-yaml-cpp/include"")
link_directories(""${CMAKE_CURRENT_BINARY_DIR}/ext/yaml-cpp/lib"")

set(SRCS src/main.cc src/preprocess_op.cc src/pipeline.cc src/jde_predictor.cc src/sde_predictor.cc src/tracker.cc src/trajectory.cc src/lapjv.cpp src/postprocess.cc)

macro(safe_set_static_flag)
    foreach(flag_var
        CMAKE_CXX_FLAGS CMAKE_CXX_FLAGS_DEBUG CMAKE_CXX_FLAGS_RELEASE
        CMAKE_CXX_FLAGS_MINSIZEREL CMAKE_CXX_FLAGS_RELWITHDEBINFO)
      if(${flag_var} MATCHES ""/MD"")
        string(REGEX REPLACE ""/MD"" ""/MT"" ${flag_var} ""${${flag_var}}"")
      endif(${flag_var} MATCHES ""/MD"")
    endforeach(flag_var)
endmacro()

if (WITH_MKL)
    ADD_DEFINITIONS(-DUSE_MKL)
endif()

if (NOT DEFINED PADDLE_DIR OR ${PADDLE_DIR} STREQUAL """")
    message(FATAL_ERROR ""please set PADDLE_DIR with -DPADDLE_DIR=/path/paddle_influence_dir"")
endif()
message(""PADDLE_DIR IS:"" ${PADDLE_DIR})

# if (NOT DEFINED OPENCV_DIR OR ${OPENCV_DIR} STREQUAL """")
#     message(FATAL_ERROR ""please set OPENCV_DIR with -DOPENCV_DIR=/path/opencv"")
# endif()

include_directories(""${CMAKE_SOURCE_DIR}/"")
include_directories(""${PADDLE_DIR}/"")
include_directories(""${PADDLE_DIR}/third_party/install/protobuf/include"")
include_directories(""${PADDLE_DIR}/third_party/install/glog/include"")
include_directories(""${PADDLE_DIR}/third_party/install/gflags/include"")
include_directories(""${PADDLE_DIR}/third_party/install/xxhash/include"")
if (EXISTS ""${PADDLE_DIR}/third_party/install/snappy/include"")
    include_directories(""${PADDLE_DIR}/third_party/install/snappy/include"")
endif()
if(EXISTS ""${PADDLE_DIR}/third_party/install/snappystream/include"")
    include_directories(""${PADDLE_DIR}/third_party/install/snappystream/include"")
endif()
include_directories(""${PADDLE_DIR}/third_party/boost"")
include_directories(""${PADDLE_DIR}/third_party/eigen3"")

if (EXISTS ""${PADDLE_DIR}/third_party/install/snappy/lib"")
    link_directories(""${PADDLE_DIR}/third_party/install/snappy/lib"")
endif()
if(EXISTS ""${PADDLE_DIR}/third_party/install/snappystream/lib"")
    link_directories(""${PADDLE_DIR}/third_party/install/snappystream/lib"")
endif()

link_directories(""${PADDLE_DIR}/third_party/install/protobuf/lib"")
link_directories(""${PADDLE_DIR}/third_party/install/glog/lib"")
link_directories(""${PADDLE_DIR}/third_party/install/gflags/lib"")
link_directories(""${PADDLE_DIR}/third_party/install/xxhash/lib"")
link_directories(""${PADDLE_DIR}/paddle/lib/"")
link_directories(""${CMAKE_CURRENT_BINARY_DIR}"")



if (WIN32)
  include_directories(""${PADDLE_DIR}/paddle/fluid/inference"")
  include_directories(""${PADDLE_DIR}/paddle/include"")
  link_directories(""${PADDLE_DIR}/paddle/fluid/inference"")
  find_package(OpenCV REQUIRED PATHS ${OPENCV_DIR}/build/ NO_DEFAULT_PATH)

else ()
#   find_package(OpenCV REQUIRED PATHS ${OPENCV_DIR}/share/OpenCV NO_DEFAULT_PATH)
  include_directories(""${PADDLE_DIR}/paddle/include"")
  link_directories(""${PADDLE_DIR}/paddle/lib"")
endif ()

find_package(OpenCV REQUIRED)
if(OpenCV_FOUND)
   include_directories(${OpenCV_INCLUDE_DIRS})
   message(""Found OpenCV"")
   message(""Includes: "" ${OpenCV_INCLUDE_DIRS})
endif(OpenCV_FOUND)
# include_directories(${OpenCV_INCLUDE_DIRS})

if (WIN32)
    add_definitions(""/DGOOGLE_GLOG_DLL_DECL="")
    set(CMAKE_C_FLAGS_DEBUG   ""${CMAKE_C_FLAGS_DEBUG} /bigobj /MTd"")
    set(CMAKE_C_FLAGS_RELEASE  ""${CMAKE_C_FLAGS_RELEASE} /bigobj /MT"")
    set(CMAKE_CXX_FLAGS_DEBUG  ""${CMAKE_CXX_FLAGS_DEBUG} /bigobj /MTd"")
    set(CMAKE_CXX_FLAGS_RELEASE   ""${CMAKE_CXX_FLAGS_RELEASE} /bigobj /MT"")
else()
    set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -g -o2 -fopenmp -std=c++11"")
    set(CMAKE_STATIC_LIBRARY_PREFIX """")
endif()

# TODO let users define cuda lib path
if (WITH_GPU)
    if (NOT DEFINED CUDA_LIB OR ${CUDA_LIB} STREQUAL """")
        message(FATAL_ERROR ""please set CUDA_LIB with -DCUDA_LIB=/path/cuda-8.0/lib64"")
    endif()
    if (NOT WIN32)
        if (NOT DEFINED CUDNN_LIB)
            message(FATAL_ERROR ""please set CUDNN_LIB with -DCUDNN_LIB=/path/cudnn_v7.4/cuda/lib64"")
        endif()
    endif(NOT WIN32)
endif()


if (NOT WIN32)
  if (WITH_TENSORRT AND WITH_GPU)
	  include_directories(""${TENSORRT_INC_DIR}/"")
	  link_directories(""${TENSORRT_LIB_DIR}/"")
  endif()
endif(NOT WIN32)

if (NOT WIN32)
    set(NGRAPH_PATH ""${PADDLE_DIR}/third_party/install/ngraph"")
    if(EXISTS ${NGRAPH_PATH})
        include(GNUInstallDirs)
        include_directories(""${NGRAPH_PATH}/include"")
        link_directories(""${NGRAPH_PATH}/${CMAKE_INSTALL_LIBDIR}"")
        set(NGRAPH_LIB ${NGRAPH_PATH}/${CMAKE_INSTALL_LIBDIR}/libngraph${CMAKE_SHARED_LIBRARY_SUFFIX})
    endif()
endif()

if(WITH_MKL)
  include_directories(""${PADDLE_DIR}/third_party/install/mklml/include"")
  if (WIN32)
    set(MATH_LIB ${PADDLE_DIR}/third_party/install/mklml/lib/mklml.lib
            ${PADDLE_DIR}/third_party/install/mklml/lib/libiomp5md.lib)
  else ()
    set(MATH_LIB ${PADDLE_DIR}/third_party/install/mklml/lib/libmklml_intel${CMAKE_SHARED_LIBRARY_SUFFIX}
            ${PADDLE_DIR}/third_party/install/mklml/lib/libiomp5${CMAKE_SHARED_LIBRARY_SUFFIX})
    execute_process(COMMAND cp -r ${PADDLE_DIR}/third_party/install/mklml/lib/libmklml_intel${CMAKE_SHARED_LIBRARY_SUFFIX} /usr/lib)
  endif ()
  set(MKLDNN_PATH ""${PADDLE_DIR}/third_party/install/mkldnn"")
  if(EXISTS ${MKLDNN_PATH})
    include_directories(""${MKLDNN_PATH}/include"")
    if (WIN32)
      set(MKLDNN_LIB ${MKLDNN_PATH}/lib/mkldnn.lib)
    else ()
      set(MKLDNN_LIB ${MKLDNN_PATH}/lib/libmkldnn.so.0)
    endif ()
  endif()
else()
  set(MATH_LIB ${PADDLE_DIR}/third_party/install/openblas/lib/libopenblas${CMAKE_STATIC_LIBRARY_SUFFIX})
endif()


if (WIN32)
    if(EXISTS ""${PADDLE_DIR}/paddle/fluid/inference/${PADDLE_LIB_NAME}${CMAKE_STATIC_LIBRARY_SUFFIX}"")
        set(DEPS
            ${PADDLE_DIR}/paddle/fluid/inference/${PADDLE_LIB_NAME}${CMAKE_STATIC_LIBRARY_SUFFIX})
    else()
        set(DEPS
            ${PADDLE_DIR}/paddle/lib/${PADDLE_LIB_NAME}${CMAKE_STATIC_LIBRARY_SUFFIX})
    endif()
endif()


if (WIN32)
    set(DEPS ${PADDLE_DIR}/paddle/lib/${PADDLE_LIB_NAME}${CMAKE_STATIC_LIBRARY_SUFFIX})
else()
    set(DEPS ${PADDLE_DIR}/paddle/lib/${PADDLE_LIB_NAME}${CMAKE_SHARED_LIBRARY_SUFFIX})
endif()

message(""PADDLE_LIB_NAME:"" ${PADDLE_LIB_NAME})
message(""DEPS:"" ${DEPS})

if (NOT WIN32)
    set(DEPS ${DEPS}
        ${MATH_LIB} ${MKLDNN_LIB}
        glog gflags protobuf z xxhash yaml-cpp
        )
    if(EXISTS ""${PADDLE_DIR}/third_party/install/snappystream/lib"")
        set(DEPS ${DEPS} snappystream)
    endif()
    if (EXISTS ""${PADDLE_DIR}/third_party/install/snappy/lib"")
        set(DEPS ${DEPS} snappy)
    endif()
else()
    set(DEPS ${DEPS}
        ${MATH_LIB} ${MKLDNN_LIB}
        glog gflags_static libprotobuf xxhash libyaml-cppmt)
    set(DEPS ${DEPS} libcmt shlwapi)
    if (EXISTS ""${PADDLE_DIR}/third_party/install/snappy/lib"")
        set(DEPS ${DEPS} snappy)
    endif()
    if(EXISTS ""${PADDLE_DIR}/third_party/install/snappystream/lib"")
        set(DEPS ${DEPS} snappystream)
    endif()
endif(NOT WIN32)

if(WITH_GPU)
  if(NOT WIN32)
    if (WITH_TENSORRT)
	    set(DEPS ${DEPS} ${TENSORRT_LIB_DIR}/libnvinfer${CMAKE_SHARED_LIBRARY_SUFFIX})
	    set(DEPS ${DEPS} ${TENSORRT_LIB_DIR}/libnvinfer_plugin${CMAKE_SHARED_LIBRARY_SUFFIX})
    endif()
    set(DEPS ${DEPS} ${CUDA_LIB}/libcudart${CMAKE_SHARED_LIBRARY_SUFFIX})
    set(DEPS ${DEPS} ${CUDNN_LIB}/libcudnn${CMAKE_SHARED_LIBRARY_SUFFIX})
  else()
    set(DEPS ${DEPS} ${CUDA_LIB}/cudart${CMAKE_STATIC_LIBRARY_SUFFIX} )
    set(DEPS ${DEPS} ${CUDA_LIB}/cublas${CMAKE_STATIC_LIBRARY_SUFFIX} )
    set(DEPS ${DEPS} ${CUDNN_LIB}/cudnn${CMAKE_STATIC_LIBRARY_SUFFIX})
  endif()
endif()

if (NOT WIN32)
    set(EXTERNAL_LIB ""-ldl -lrt -lgomp -lz -lm -lpthread"")
    set(DEPS ${DEPS} ${EXTERNAL_LIB})
endif()

set(DEPS ${DEPS} ${OpenCV_LIBS})
add_executable(main ${SRCS})
ADD_DEPENDENCIES(main ext-yaml-cpp)
message(""DEPS:"" ${DEPS})
target_link_libraries(main ${DEPS})

if (WIN32 AND WITH_MKL)
    add_custom_command(TARGET main POST_BUILD
        COMMAND ${CMAKE_COMMAND} -E copy_if_different ${PADDLE_DIR}/third_party/install/mklml/lib/mklml.dll ./mklml.dll
        COMMAND ${CMAKE_COMMAND} -E copy_if_different ${PADDLE_DIR}/third_party/install/mklml/lib/libiomp5md.dll ./libiomp5md.dll
        COMMAND ${CMAKE_COMMAND} -E copy_if_different ${PADDLE_DIR}/third_party/install/mkldnn/lib/mkldnn.dll ./mkldnn.dll
        COMMAND ${CMAKE_COMMAND} -E copy_if_different ${PADDLE_DIR}/third_party/install/mklml/lib/mklml.dll ./release/mklml.dll
        COMMAND ${CMAKE_COMMAND} -E copy_if_different ${PADDLE_DIR}/third_party/install/mklml/lib/libiomp5md.dll ./release/libiomp5md.dll
        COMMAND ${CMAKE_COMMAND} -E copy_if_different ${PADDLE_DIR}/third_party/install/mkldnn/lib/mkldnn.dll ./release/mkldnn.dll
        COMMAND ${CMAKE_COMMAND} -E copy_if_different ${PADDLE_DIR}/paddle/lib/${PADDLE_LIB_NAME}.dll ./release/${PADDLE_LIB_NAME}.dll
    )
endif()

if (WIN32)
    add_custom_command(TARGET main POST_BUILD
        COMMAND ${CMAKE_COMMAND} -E copy_if_different ${PADDLE_DIR}/paddle/lib/${PADDLE_LIB_NAME}.dll ./release/${PADDLE_LIB_NAME}.dll
    )
endif()

```
I ran program with command:
```shell
./main -video_file=entrance_count_demo.mp4 --track_model_dir=output_inference/fairmot_hrnetv2_w18_dlafpn_30e_576x320 --device=gpu --do_entrance_counting=True --save_result=Tru
```
Please help me this problem! Have a nice day!


### 复现环境 Environment

-PaddlePaddle C++ : 2.2.2
-PaddleDetection: release/2.4
- Python: 3.6.9
- CUDA: 10.2.3
- CUDnn: 8.2.1.32
- TensorRT: 8.0.1.6
- Opencv 4.1.1 (default)
- Jetson Xavier NX 4.6 [L4T 32.6.1]

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
PicoDet 量化动态图验证精度正常，静态图推理异常,PaddlePaddle/PaddleDetection,2022-07-05 02:35:43,2,,6359,1293712486,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

QAT int8量化的picoDet-xs 动态图验证集上精度指标正常，但是转换成静态图和Lite模型在pc/端上推理结果异常"
jetson nx jetpack4.6 使用trt_fp16推理出现报错,PaddlePaddle/PaddleDetection,2022-07-04 09:16:20,0,,6356,1292874670,"### 问题确认 Search before asking

- [x] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

--run_mode=paddle 不开启trt的情况下可以正常运行输出，用命令“python3 tools/export_model.py -c configs/ppyoloe/ppyoloe_crn_s_300e_coco.yml -o weights=https://paddledet.bj.bcebos.com/models/ppyoloe_crn_s_300e_coco.pdparams trt=True”导出trt以后进行fp16推理出现异常，输出信息如下：
-----------  Running Arguments -----------
action_file: None
batch_size: 1
camera_id: -1
cpu_threads: 1
device: gpu
enable_mkldnn: False
enable_mkldnn_bfloat16: False
image_dir: None
image_file: demo/000000014439_640x640.jpg
model_dir: output_inference/ppyoloe_crn_s_300e_coco
output_dir: output
random_pad: False
reid_batch_size: 50
reid_model_dir: None
run_benchmark: False
run_mode: trt_fp16
save_images: False
save_mot_txt_per_img: False
save_mot_txts: False
save_results: False
scaled: False
threshold: 0.5
tracker_config: None
trt_calib_mode: False
trt_max_shape: 1280
trt_min_shape: 1
trt_opt_shape: 640
use_dark: True
use_gpu: False
video_file: None
window_size: 50
------------------------------------------
-----------  Model Configuration -----------
Model Arch: YOLO
Transform Order: 
--transform op: Resize
--transform op: NormalizeImage
--transform op: Permute
--------------------------------------------
E0704 17:15:40.951094  9144 helper.h:111] Invalid parameter: NMS topK (10000) exceeds limit (4096)
E0704 17:15:40.951248  9144 helper.h:111] Invalid parameter: NMS topK (10000) exceeds limit (4096)
E0704 17:15:40.953336  9144 helper.h:111] Invalid parameter: NMS topK (10000) exceeds limit (4096)
E0704 17:15:44.247506  9144 helper.h:111] Invalid parameter: NMS topK (10000) exceeds limit (4096)
E0704 17:15:45.962250  9144 helper.h:111] Invalid parameter: NMS topK (10000) exceeds limit (4096)
E0704 17:15:46.186007  9144 helper.h:111] Invalid parameter: NMS topK (10000) exceeds limit (4096)

### 复现环境 Environment

PaddlePaddle: 2.3.0 (飞浆官网下载的 paddlepaddle_gpu-2.3.0-cp36-cp36m-linux_aarch64.whl)
PaddleDetection: 2.4
Python: 3.6
CUDA: 10.2
CUDNN: 8.2.1.32
TensorRT: 8.0.1.6


### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
AutoAugment 的搜索策略,PaddlePaddle/PaddleDetection,2022-07-04 08:56:44,0,,6355,1292849211,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

查看了相关代码 ，发现ppdet所支持的AutoAugment只包含随机选择一种数据增强策略应用到数据上，并没有搜索的过程。麻烦问下，ppdet中是否实现了论文中的搜索策略，具体在哪？
原文：
https://arxiv.org/pdf/1906.11172.pdf"
有的conv2无法8bit量化,PaddlePaddle/PaddleDetection,2022-07-04 07:49:48,0,,6351,1292770076,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

## 如图最后的卷积无法做8bit量化是为什么呢
![image](https://user-images.githubusercontent.com/45120374/177107174-e04dd9c8-7acf-47f3-ae25-68d002bc2a54.png)
![image](https://user-images.githubusercontent.com/45120374/177107304-560d735e-ccc3-43d0-b70a-a6e5eeabeaef.png)

"
请问怎么评估coco格式数据集在IoU=0.5时的大、中和小尺度目标的IoU？,PaddlePaddle/PaddleDetection,2022-07-04 03:17:55,0,,6343,1292559863,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

请问怎么评估coco格式数据集在IoU=0.5时的大、中和小尺度目标的IoU？（或者其他指定IoU，在mmDetecion中评估时有相关的参数可以设置，paddledetecion中好像没有找到，有其他方法吗？）"
VOC格式数据集中的标注信息对训练的影响,PaddlePaddle/PaddleDetection,2022-07-02 14:26:03,2,,6336,1292067446,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

请在VOC格式数据集中的以下标注信息：
<truncated>0</truncated>
<difficult>0</difficult>
它们为0或1会对模型效果产生什么影响呢
PaddleDetection中有没有针对其取值的不同进行相关的处理"
求一个picodetV1版本035模型,PaddlePaddle/PaddleDetection,2022-07-02 09:24:00,0,,6335,1292012261,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

大佬们，有没有picodetV1版本ESNet：035模型，V1版本最小的是ESNet：075，请求一个035版本的coco预训练模型，谢谢大佬们。

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
OSError: [Errno 5] Input/output error,PaddlePaddle/PaddleDetection,2022-07-01 14:34:53,2,,6332,1291490536,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

I train the ppyolo_r50vd_dcn_1x_coco model using Google Colab environment. I use the python command:
`!python tools/train.py -c configs/ppyolo/ppyolo_r50vd_dcn_1x_coco.yml --eval`
And the error generates after that
```
/usr/local/lib/python3.7/dist-packages/paddle/tensor/creation.py:125: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if data.dtype == np.object:
/usr/local/lib/python3.7/dist-packages/scipy/fft/__init__.py:97: DeprecationWarning: The module numpy.dual is deprecated.  Instead of using dual, use the functions directly from numpy or scipy.
  from numpy.dual import register_func
/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.
  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]
/usr/local/lib/python3.7/dist-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,
W0701 12:14:48.330195 169235 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.2, Runtime API Version: 10.2
W0701 12:14:48.335217 169235 gpu_context.cc:306] device: 0, cuDNN Version: 7.6.
[07/01 12:14:55] ppdet.utils.checkpoint INFO: The shape (258,) in pretrained weight yolo_head.yolo_output.0.bias is unmatched with the shape [21] in model yolo_head.yolo_output.0.bias. And the weight yolo_head.yolo_output.0.bias will not be loaded
[07/01 12:14:55] ppdet.utils.checkpoint INFO: The shape (258, 1024, 1, 1) in pretrained weight yolo_head.yolo_output.0.weight is unmatched with the shape [21, 1024, 1, 1] in model yolo_head.yolo_output.0.weight. And the weight yolo_head.yolo_output.0.weight will not be loaded
[07/01 12:14:55] ppdet.utils.checkpoint INFO: The shape (258,) in pretrained weight yolo_head.yolo_output.1.bias is unmatched with the shape [21] in model yolo_head.yolo_output.1.bias. And the weight yolo_head.yolo_output.1.bias will not be loaded
[07/01 12:14:55] ppdet.utils.checkpoint INFO: The shape (258, 512, 1, 1) in pretrained weight yolo_head.yolo_output.1.weight is unmatched with the shape [21, 512, 1, 1] in model yolo_head.yolo_output.1.weight. And the weight yolo_head.yolo_output.1.weight will not be loaded
[07/01 12:14:55] ppdet.utils.checkpoint INFO: The shape (258,) in pretrained weight yolo_head.yolo_output.2.bias is unmatched with the shape [21] in model yolo_head.yolo_output.2.bias. And the weight yolo_head.yolo_output.2.bias will not be loaded
[07/01 12:14:55] ppdet.utils.checkpoint INFO: The shape (258, 256, 1, 1) in pretrained weight yolo_head.yolo_output.2.weight is unmatched with the shape [21, 256, 1, 1] in model yolo_head.yolo_output.2.weight. And the weight yolo_head.yolo_output.2.weight will not be loaded
[07/01 12:14:55] ppdet.utils.checkpoint INFO: Finish loading model weights: /content/drive/MyDrive/UIT/Four_Year/Term_2/DS505.M21/Code/PaddleDetection/pretrained_model/ppyolo_r50vd_dcn_1x_coco.pdparams
[07/01 12:15:02] ppdet.engine INFO: Epoch: [0] [  0/526] learning_rate: 0.000000 loss_xy: 1.268517 loss_wh: 2.687380 loss_iou: 4.231785 loss_iou_aware: 0.679625 loss_obj: 20271.953125 loss_cls: 0.852223 loss: 20281.671875 eta: 2 days, 1:15:46 batch_cost: 6.7432 data_cost: 5.5761 ips: 1.4830 images/s
[07/01 12:16:13] ppdet.engine INFO: Epoch: [0] [  5/526] learning_rate: 0.000000 loss_xy: 1.177958 loss_wh: 2.413409 loss_iou: 4.251762 loss_iou_aware: 0.641516 loss_obj: 14273.924805 loss_cls: 0.703213 loss: 14283.142578 eta: 3 days, 21:57:12 batch_cost: 14.0870 data_cost: 13.2695 ips: 0.7099 images/s
[07/01 12:17:13] ppdet.engine INFO: Epoch: [0] [ 10/526] learning_rate: 0.000000 loss_xy: 1.104466 loss_wh: 2.132365 loss_iou: 3.897967 loss_iou_aware: 0.656381 loss_obj: 12482.313477 loss_cls: 0.641516 loss: 12490.728516 eta: 3 days, 19:22:27 batch_cost: 12.0913 data_cost: 11.2510 ips: 0.8270 images/s
[07/01 12:19:00] ppdet.engine INFO: Epoch: [0] [ 15/526] learning_rate: 0.000000 loss_xy: 0.915604 loss_wh: 2.491956 loss_iou: 4.159617 loss_iou_aware: 0.622869 loss_obj: 10716.389648 loss_cls: 0.736629 loss: 10724.900391 eta: 4 days, 15:27:38 batch_cost: 21.3232 data_cost: 20.4098 ips: 0.4690 images/s
[07/01 12:20:46] ppdet.engine INFO: Epoch: [0] [ 20/526] learning_rate: 0.000001 loss_xy: 1.121335 loss_wh: 2.470541 loss_iou: 4.441169 loss_iou_aware: 0.629464 loss_obj: 4258.397461 loss_cls: 0.621566 loss: 4267.644531 eta: 5 days, 1:50:17 batch_cost: 21.2485 data_cost: 20.3849 ips: 0.4706 images/s
[07/01 12:22:31] ppdet.engine INFO: Epoch: [0] [ 25/526] learning_rate: 0.000001 loss_xy: 1.092964 loss_wh: 2.442391 loss_iou: 4.365379 loss_iou_aware: 0.611063 loss_obj: 1514.855469 loss_cls: 0.755561 loss: 1524.373047 eta: 5 days, 7:39:50 batch_cost: 20.8574 data_cost: 20.1473 ips: 0.4794 images/s
[07/01 12:24:06] ppdet.engine INFO: Epoch: [0] [ 30/526] learning_rate: 0.000001 loss_xy: 1.245981 loss_wh: 2.342381 loss_iou: 4.491904 loss_iou_aware: 0.638886 loss_obj: 1469.394043 loss_cls: 0.715071 loss: 1479.742065 eta: 5 days, 9:22:20 batch_cost: 18.9637 data_cost: 18.0708 ips: 0.5273 images/s
[07/01 12:25:42] ppdet.engine INFO: Epoch: [0] [ 35/526] learning_rate: 0.000001 loss_xy: 1.144956 loss_wh: 2.406286 loss_iou: 4.320916 loss_iou_aware: 0.617093 loss_obj: 838.997192 loss_cls: 0.665849 loss: 847.173401 eta: 5 days, 10:47:43 batch_cost: 19.1575 data_cost: 18.4439 ips: 0.5220 images/s
[07/01 12:27:12] ppdet.engine INFO: Epoch: [0] [ 40/526] learning_rate: 0.000001 loss_xy: 1.138049 loss_wh: 2.127945 loss_iou: 4.030950 loss_iou_aware: 0.582580 loss_obj: 642.472351 loss_cls: 0.709636 loss: 650.463745 eta: 5 days, 10:44:47 batch_cost: 17.9005 data_cost: 17.1293 ips: 0.5586 images/s
[07/01 12:29:12] ppdet.engine INFO: Epoch: [0] [ 45/526] learning_rate: 0.000001 loss_xy: 0.956402 loss_wh: 2.319241 loss_iou: 4.204791 loss_iou_aware: 0.631527 loss_obj: 312.325195 loss_cls: 0.753860 loss: 320.881866 eta: 5 days, 15:31:27 batch_cost: 23.9826 data_cost: 23.3344 ips: 0.4170 images/s
[07/01 12:30:50] ppdet.engine INFO: Epoch: [0] [ 50/526] learning_rate: 0.000001 loss_xy: 1.231448 loss_wh: 2.558268 loss_iou: 4.370479 loss_iou_aware: 0.621082 loss_obj: 340.638580 loss_cls: 0.765347 loss: 349.412964 eta: 5 days, 16:12:59 batch_cost: 19.5872 data_cost: 18.8506 ips: 0.5105 images/s
[07/01 12:32:17] ppdet.engine INFO: Epoch: [0] [ 55/526] learning_rate: 0.000001 loss_xy: 1.223972 loss_wh: 2.396241 loss_iou: 4.393015 loss_iou_aware: 0.676423 loss_obj: 289.965118 loss_cls: 0.804316 loss: 299.275024 eta: 5 days, 15:18:19 batch_cost: 17.3212 data_cost: 16.6357 ips: 0.5773 images/s
[07/01 12:33:49] ppdet.engine INFO: Epoch: [0] [ 60/526] learning_rate: 0.000002 loss_xy: 1.095244 loss_wh: 2.720826 loss_iou: 4.326620 loss_iou_aware: 0.667595 loss_obj: 378.453705 loss_cls: 0.702839 loss: 388.153412 eta: 5 days, 15:10:41 batch_cost: 18.3898 data_cost: 17.5443 ips: 0.5438 images/s
[07/01 12:35:23] ppdet.engine INFO: Epoch: [0] [ 65/526] learning_rate: 0.000002 loss_xy: 0.988449 loss_wh: 2.104736 loss_iou: 4.186148 loss_iou_aware: 0.600449 loss_obj: 215.139709 loss_cls: 0.613115 loss: 224.098312 eta: 5 days, 15:14:21 batch_cost: 18.7034 data_cost: 18.0916 ips: 0.5347 images/s
[07/01 12:36:55] ppdet.engine INFO: Epoch: [0] [ 70/526] learning_rate: 0.000002 loss_xy: 1.078253 loss_wh: 2.615308 loss_iou: 4.237937 loss_iou_aware: 0.630623 loss_obj: 320.503784 loss_cls: 0.783672 loss: 331.229492 eta: 5 days, 15:09:30 batch_cost: 18.4503 data_cost: 17.5756 ips: 0.5420 images/s
[07/01 12:38:19] ppdet.engine INFO: Epoch: [0] [ 75/526] learning_rate: 0.000002 loss_xy: 1.161517 loss_wh: 2.456941 loss_iou: 4.451501 loss_iou_aware: 0.640539 loss_obj: 285.971924 loss_cls: 0.775499 loss: 295.907166 eta: 5 days, 14:15:29 batch_cost: 16.7253 data_cost: 15.8633 ips: 0.5979 images/s
[07/01 12:39:48] ppdet.engine INFO: Epoch: [0] [ 80/526] learning_rate: 0.000002 loss_xy: 1.221983 loss_wh: 2.509572 loss_iou: 4.425272 loss_iou_aware: 0.666793 loss_obj: 224.867050 loss_cls: 0.701793 loss: 234.744431 eta: 5 days, 13:55:23 batch_cost: 17.7417 data_cost: 16.9488 ips: 0.5636 images/s
[07/01 12:41:11] ppdet.engine INFO: Epoch: [0] [ 85/526] learning_rate: 0.000002 loss_xy: 0.996946 loss_wh: 2.555164 loss_iou: 4.233212 loss_iou_aware: 0.582177 loss_obj: 128.914963 loss_cls: 0.564532 loss: 137.997147 eta: 5 days, 13:06:02 batch_cost: 16.5054 data_cost: 15.9178 ips: 0.6059 images/s
[07/01 12:42:29] ppdet.engine INFO: Epoch: [0] [ 90/526] learning_rate: 0.000002 loss_xy: 1.281924 loss_wh: 2.155412 loss_iou: 4.224259 loss_iou_aware: 0.642223 loss_obj: 201.118118 loss_cls: 0.733224 loss: 211.086273 eta: 5 days, 12:02:38 batch_cost: 15.7005 data_cost: 14.7922 ips: 0.6369 images/s
[07/01 12:43:47] ppdet.engine INFO: Epoch: [0] [ 95/526] learning_rate: 0.000002 loss_xy: 1.254092 loss_wh: 2.572512 loss_iou: 4.429868 loss_iou_aware: 0.659593 loss_obj: 152.828247 loss_cls: 0.804853 loss: 162.739441 eta: 5 days, 10:59:13 batch_cost: 15.4150 data_cost: 14.5565 ips: 0.6487 images/s
[07/01 12:45:14] ppdet.engine INFO: Epoch: [0] [100/526] learning_rate: 0.000003 loss_xy: 0.996635 loss_wh: 2.301072 loss_iou: 4.120408 loss_iou_aware: 0.593843 loss_obj: 99.766457 loss_cls: 0.643620 loss: 108.784454 eta: 5 days, 10:45:15 batch_cost: 17.4179 data_cost: 16.6638 ips: 0.5741 images/s
[07/01 12:46:34] ppdet.engine INFO: Epoch: [0] [105/526] learning_rate: 0.000003 loss_xy: 1.063770 loss_wh: 2.539247 loss_iou: 4.360949 loss_iou_aware: 0.655446 loss_obj: 118.341606 loss_cls: 0.721550 loss: 127.873985 eta: 5 days, 10:03:09 batch_cost: 15.9947 data_cost: 15.0714 ips: 0.6252 images/s
[07/01 12:47:56] ppdet.engine INFO: Epoch: [0] [110/526] learning_rate: 0.000003 loss_xy: 1.177744 loss_wh: 2.230473 loss_iou: 4.356344 loss_iou_aware: 0.686642 loss_obj: 91.067383 loss_cls: 0.780575 loss: 100.362160 eta: 5 days, 9:29:54 batch_cost: 16.2577 data_cost: 15.5967 ips: 0.6151 images/s
[07/01 12:49:13] ppdet.engine INFO: Epoch: [0] [115/526] learning_rate: 0.000003 loss_xy: 1.158705 loss_wh: 2.281775 loss_iou: 4.029062 loss_iou_aware: 0.637969 loss_obj: 125.738983 loss_cls: 0.792050 loss: 135.915512 eta: 5 days, 8:44:32 batch_cost: 15.4675 data_cost: 14.5713 ips: 0.6465 images/s
[07/01 12:50:27] ppdet.engine INFO: Epoch: [0] [120/526] learning_rate: 0.000003 loss_xy: 0.991264 loss_wh: 1.821931 loss_iou: 3.765229 loss_iou_aware: 0.558607 loss_obj: 68.910866 loss_cls: 0.650912 loss: 75.308769 eta: 5 days, 7:48:29 batch_cost: 14.6733 data_cost: 13.8942 ips: 0.6815 images/s
[07/01 12:51:40] ppdet.engine INFO: Epoch: [0] [125/526] learning_rate: 0.000003 loss_xy: 1.258148 loss_wh: 2.423700 loss_iou: 4.441479 loss_iou_aware: 0.638279 loss_obj: 73.731422 loss_cls: 0.759794 loss: 83.374672 eta: 5 days, 6:55:32 batch_cost: 14.6012 data_cost: 13.7802 ips: 0.6849 images/s
[07/01 12:52:56] ppdet.engine INFO: Epoch: [0] [130/526] learning_rate: 0.000003 loss_xy: 1.090885 loss_wh: 2.375309 loss_iou: 4.019869 loss_iou_aware: 0.594898 loss_obj: 84.786713 loss_cls: 0.664097 loss: 93.494270 eta: 5 days, 6:15:10 batch_cost: 15.1191 data_cost: 14.2524 ips: 0.6614 images/s
[07/01 12:54:03] ppdet.engine INFO: Epoch: [0] [135/526] learning_rate: 0.000003 loss_xy: 1.244282 loss_wh: 2.057654 loss_iou: 4.130521 loss_iou_aware: 0.716851 loss_obj: 83.640106 loss_cls: 0.823826 loss: 92.512581 eta: 5 days, 5:11:28 batch_cost: 13.4848 data_cost: 12.4651 ips: 0.7416 images/s
[07/01 12:55:10] ppdet.engine INFO: Epoch: [0] [140/526] learning_rate: 0.000004 loss_xy: 1.110622 loss_wh: 2.304725 loss_iou: 4.162155 loss_iou_aware: 0.617140 loss_obj: 50.154106 loss_cls: 0.641885 loss: 59.076149 eta: 5 days, 4:07:40 batch_cost: 13.1908 data_cost: 12.4731 ips: 0.7581 images/s
[07/01 12:56:15] ppdet.engine INFO: Epoch: [0] [145/526] learning_rate: 0.000004 loss_xy: 1.159231 loss_wh: 2.232166 loss_iou: 4.176077 loss_iou_aware: 0.683062 loss_obj: 71.293312 loss_cls: 0.781162 loss: 79.995094 eta: 5 days, 3:06:57 batch_cost: 13.1099 data_cost: 12.0606 ips: 0.7628 images/s
[07/01 12:57:24] ppdet.engine INFO: Epoch: [0] [150/526] learning_rate: 0.000004 loss_xy: 1.132923 loss_wh: 2.046457 loss_iou: 4.106165 loss_iou_aware: 0.610845 loss_obj: 69.038246 loss_cls: 0.639172 loss: 76.659317 eta: 5 days, 2:17:18 batch_cost: 13.6038 data_cost: 12.6718 ips: 0.7351 images/s
[07/01 12:58:29] ppdet.engine INFO: Epoch: [0] [155/526] learning_rate: 0.000004 loss_xy: 1.089540 loss_wh: 2.326177 loss_iou: 4.250847 loss_iou_aware: 0.608413 loss_obj: 29.434540 loss_cls: 0.747675 loss: 40.095325 eta: 5 days, 1:21:58 batch_cost: 12.9737 data_cost: 12.4263 ips: 0.7708 images/s
[07/01 12:59:36] ppdet.engine INFO: Epoch: [0] [160/526] learning_rate: 0.000004 loss_xy: 0.864582 loss_wh: 2.029992 loss_iou: 3.836189 loss_iou_aware: 0.579900 loss_obj: 27.774757 loss_cls: 0.591898 loss: 35.567009 eta: 5 days, 0:37:18 batch_cost: 13.5130 data_cost: 12.8749 ips: 0.7400 images/s
[07/01 13:01:02] ppdet.engine INFO: Epoch: [0] [165/526] learning_rate: 0.000004 loss_xy: 1.316992 loss_wh: 2.169061 loss_iou: 4.145683 loss_iou_aware: 0.624901 loss_obj: 39.394150 loss_cls: 0.706548 loss: 47.988468 eta: 5 days, 0:41:15 batch_cost: 17.0185 data_cost: 16.2318 ips: 0.5876 images/s
[07/01 13:02:31] ppdet.engine INFO: Epoch: [0] [170/526] learning_rate: 0.000004 loss_xy: 1.174806 loss_wh: 2.012739 loss_iou: 4.157804 loss_iou_aware: 0.668963 loss_obj: 44.314423 loss_cls: 0.718145 loss: 52.917553 eta: 5 days, 0:54:20 batch_cost: 17.7605 data_cost: 16.8871 ips: 0.5630 images/s
[07/01 13:03:45] ppdet.engine INFO: Epoch: [0] [175/526] learning_rate: 0.000004 loss_xy: 1.247828 loss_wh: 2.023144 loss_iou: 4.336152 loss_iou_aware: 0.656447 loss_obj: 29.839693 loss_cls: 0.680164 loss: 38.585308 eta: 5 days, 0:29:42 batch_cost: 14.7787 data_cost: 14.0798 ips: 0.6767 images/s
[07/01 13:04:51] ppdet.engine INFO: Epoch: [0] [180/526] learning_rate: 0.000005 loss_xy: 1.149554 loss_wh: 2.169506 loss_iou: 3.785664 loss_iou_aware: 0.601007 loss_obj: 34.015617 loss_cls: 0.713665 loss: 43.458797 eta: 4 days, 23:47:12 batch_cost: 13.1845 data_cost: 12.5146 ips: 0.7585 images/s
[07/01 13:05:52] ppdet.engine INFO: Epoch: [0] [185/526] learning_rate: 0.000005 loss_xy: 1.223797 loss_wh: 2.178720 loss_iou: 4.150827 loss_iou_aware: 0.598653 loss_obj: 35.078701 loss_cls: 0.671679 loss: 43.485683 eta: 4 days, 22:55:08 batch_cost: 12.1778 data_cost: 11.4247 ips: 0.8212 images/s
[07/01 13:07:03] ppdet.engine INFO: Epoch: [0] [190/526] learning_rate: 0.000005 loss_xy: 1.295334 loss_wh: 1.855066 loss_iou: 4.324129 loss_iou_aware: 0.661017 loss_obj: 34.514874 loss_cls: 0.631759 loss: 42.388554 eta: 4 days, 22:28:53 batch_cost: 14.2077 data_cost: 13.4654 ips: 0.7038 images/s
[07/01 13:08:19] ppdet.engine INFO: Epoch: [0] [195/526] learning_rate: 0.000005 loss_xy: 1.134431 loss_wh: 2.360353 loss_iou: 4.451344 loss_iou_aware: 0.638183 loss_obj: 24.914568 loss_cls: 0.690286 loss: 33.923691 eta: 4 days, 22:14:03 batch_cost: 15.1232 data_cost: 14.4812 ips: 0.6612 images/s
[07/01 13:09:29] ppdet.engine INFO: Epoch: [0] [200/526] learning_rate: 0.000005 loss_xy: 1.286161 loss_wh: 1.787248 loss_iou: 4.156811 loss_iou_aware: 0.640577 loss_obj: 24.339710 loss_cls: 0.664834 loss: 32.167515 eta: 4 days, 21:47:40 batch_cost: 13.9926 data_cost: 13.3090 ips: 0.7147 images/s
[07/01 13:10:36] ppdet.engine INFO: Epoch: [0] [205/526] learning_rate: 0.000005 loss_xy: 1.184417 loss_wh: 2.029136 loss_iou: 3.975405 loss_iou_aware: 0.668309 loss_obj: 36.605125 loss_cls: 0.602948 loss: 45.886314 eta: 4 days, 21:15:23 batch_cost: 13.3168 data_cost: 12.5310 ips: 0.7509 images/s
[07/01 13:12:02] ppdet.engine INFO: Epoch: [0] [210/526] learning_rate: 0.000005 loss_xy: 1.081506 loss_wh: 1.724030 loss_iou: 3.893903 loss_iou_aware: 0.739715 loss_obj: 34.875927 loss_cls: 0.555110 loss: 42.719929 eta: 4 days, 21:23:04 batch_cost: 17.0525 data_cost: 16.1985 ips: 0.5864 images/s
[07/01 13:13:15] ppdet.engine INFO: Epoch: [0] [215/526] learning_rate: 0.000005 loss_xy: 1.145555 loss_wh: 1.895893 loss_iou: 4.057085 loss_iou_aware: 0.727120 loss_obj: 34.658497 loss_cls: 0.718911 loss: 43.180389 eta: 4 days, 21:05:31 batch_cost: 14.5880 data_cost: 13.6649 ips: 0.6855 images/s
[07/01 13:14:21] ppdet.engine INFO: Epoch: [0] [220/526] learning_rate: 0.000006 loss_xy: 1.106895 loss_wh: 1.921716 loss_iou: 4.197306 loss_iou_aware: 0.623526 loss_obj: 23.560707 loss_cls: 0.628223 loss: 32.653717 eta: 4 days, 20:34:38 batch_cost: 13.1562 data_cost: 12.4561 ips: 0.7601 images/s
[07/01 13:15:31] ppdet.engine INFO: Epoch: [0] [225/526] learning_rate: 0.000006 loss_xy: 1.197099 loss_wh: 1.876569 loss_iou: 3.978829 loss_iou_aware: 0.706580 loss_obj: 27.415630 loss_cls: 0.699091 loss: 35.734127 eta: 4 days, 20:13:23 batch_cost: 14.0210 data_cost: 13.2424 ips: 0.7132 images/s
[07/01 13:16:45] ppdet.engine INFO: Epoch: [0] [230/526] learning_rate: 0.000006 loss_xy: 1.057469 loss_wh: 1.776332 loss_iou: 3.924106 loss_iou_aware: 0.649162 loss_obj: 35.021446 loss_cls: 0.760346 loss: 43.127289 eta: 4 days, 20:00:20 batch_cost: 14.8005 data_cost: 13.8679 ips: 0.6757 images/s
[07/01 13:18:00] ppdet.engine INFO: Epoch: [0] [235/526] learning_rate: 0.000006 loss_xy: 1.198717 loss_wh: 1.827318 loss_iou: 4.267738 loss_iou_aware: 0.699909 loss_obj: 23.384451 loss_cls: 0.753166 loss: 31.450428 eta: 4 days, 19:49:35 batch_cost: 14.9973 data_cost: 14.1241 ips: 0.6668 images/s
[07/01 13:19:03] ppdet.engine INFO: Epoch: [0] [240/526] learning_rate: 0.000006 loss_xy: 1.282138 loss_wh: 1.899866 loss_iou: 4.010087 loss_iou_aware: 0.672560 loss_obj: 32.506649 loss_cls: 0.627504 loss: 41.963383 eta: 4 days, 19:16:41 batch_cost: 12.4951 data_cost: 11.6089 ips: 0.8003 images/s
[07/01 13:20:06] ppdet.engine INFO: Epoch: [0] [245/526] learning_rate: 0.000006 loss_xy: 1.028864 loss_wh: 1.826583 loss_iou: 3.988883 loss_iou_aware: 0.664884 loss_obj: 18.929489 loss_cls: 0.670509 loss: 27.673689 eta: 4 days, 18:45:00 batch_cost: 12.4847 data_cost: 11.7054 ips: 0.8010 images/s
[07/01 13:21:25] ppdet.engine INFO: Epoch: [0] [250/526] learning_rate: 0.000006 loss_xy: 1.235800 loss_wh: 1.777964 loss_iou: 4.063594 loss_iou_aware: 0.667364 loss_obj: 18.668900 loss_cls: 0.572790 loss: 26.589201 eta: 4 days, 18:43:32 batch_cost: 15.8379 data_cost: 15.0809 ips: 0.6314 images/s
[07/01 13:22:27] ppdet.engine INFO: Epoch: [0] [255/526] learning_rate: 0.000006 loss_xy: 1.208367 loss_wh: 2.075184 loss_iou: 4.206319 loss_iou_aware: 0.669621 loss_obj: 17.611246 loss_cls: 0.677594 loss: 26.703337 eta: 4 days, 18:12:12 batch_cost: 12.3145 data_cost: 11.6690 ips: 0.8120 images/s
[07/01 13:23:41] ppdet.engine INFO: Epoch: [0] [260/526] learning_rate: 0.000007 loss_xy: 1.085346 loss_wh: 1.668180 loss_iou: 4.177268 loss_iou_aware: 0.697426 loss_obj: 30.483238 loss_cls: 0.630117 loss: 38.814762 eta: 4 days, 18:02:18 batch_cost: 14.7525 data_cost: 13.7259 ips: 0.6779 images/s
[07/01 13:24:49] ppdet.engine INFO: Epoch: [0] [265/526] learning_rate: 0.000007 loss_xy: 1.088767 loss_wh: 1.783628 loss_iou: 4.164545 loss_iou_aware: 0.736160 loss_obj: 22.592388 loss_cls: 0.655318 loss: 31.026768 eta: 4 days, 17:43:42 batch_cost: 13.6480 data_cost: 12.8042 ips: 0.7327 images/s
[07/01 13:25:54] ppdet.engine INFO: Epoch: [0] [270/526] learning_rate: 0.000007 loss_xy: 1.029153 loss_wh: 1.677392 loss_iou: 3.650034 loss_iou_aware: 0.585184 loss_obj: 13.997475 loss_cls: 0.601826 loss: 21.570038 eta: 4 days, 17:19:13 batch_cost: 12.8302 data_cost: 12.1707 ips: 0.7794 images/s
[07/01 13:26:59] ppdet.engine INFO: Epoch: [0] [275/526] learning_rate: 0.000007 loss_xy: 1.096723 loss_wh: 1.582747 loss_iou: 3.985842 loss_iou_aware: 0.640106 loss_obj: 15.957102 loss_cls: 0.634033 loss: 23.852882 eta: 4 days, 16:56:51 batch_cost: 12.9921 data_cost: 12.3699 ips: 0.7697 images/s
[07/01 13:28:10] ppdet.engine INFO: Epoch: [0] [280/526] learning_rate: 0.000007 loss_xy: 1.029367 loss_wh: 1.631019 loss_iou: 4.059268 loss_iou_aware: 0.738247 loss_obj: 19.005415 loss_cls: 0.754441 loss: 28.284607 eta: 4 days, 16:43:40 batch_cost: 14.0849 data_cost: 13.1892 ips: 0.7100 images/s
[07/01 13:29:24] ppdet.engine INFO: Epoch: [0] [285/526] learning_rate: 0.000007 loss_xy: 0.972801 loss_wh: 1.615446 loss_iou: 4.031400 loss_iou_aware: 0.711254 loss_obj: 19.575111 loss_cls: 0.687294 loss: 27.518564 eta: 4 days, 16:37:11 batch_cost: 14.9121 data_cost: 13.9654 ips: 0.6706 images/s
[07/01 13:30:36] ppdet.engine INFO: Epoch: [0] [290/526] learning_rate: 0.000007 loss_xy: 1.141120 loss_wh: 1.821583 loss_iou: 3.844513 loss_iou_aware: 0.712960 loss_obj: 14.678379 loss_cls: 0.594878 loss: 22.830244 eta: 4 days, 16:25:48 batch_cost: 14.2301 data_cost: 13.5185 ips: 0.7027 images/s
[07/01 13:32:01] ppdet.engine INFO: Epoch: [0] [295/526] learning_rate: 0.000007 loss_xy: 1.059708 loss_wh: 1.558759 loss_iou: 3.763732 loss_iou_aware: 0.694737 loss_obj: 19.689251 loss_cls: 0.740966 loss: 27.290422 eta: 4 days, 16:35:50 batch_cost: 17.1097 data_cost: 16.2212 ips: 0.5845 images/s
[07/01 13:33:07] ppdet.engine INFO: Epoch: [0] [300/526] learning_rate: 0.000008 loss_xy: 1.199921 loss_wh: 1.615327 loss_iou: 4.127220 loss_iou_aware: 0.639887 loss_obj: 13.675007 loss_cls: 0.637481 loss: 22.013250 eta: 4 days, 16:16:50 batch_cost: 13.1272 data_cost: 12.5675 ips: 0.7618 images/s
[07/01 13:34:16] ppdet.engine INFO: Epoch: [0] [305/526] learning_rate: 0.000008 loss_xy: 1.050035 loss_wh: 1.681755 loss_iou: 3.872050 loss_iou_aware: 0.572892 loss_obj: 18.434284 loss_cls: 0.667585 loss: 25.835394 eta: 4 days, 16:01:59 batch_cost: 13.6314 data_cost: 12.7587 ips: 0.7336 images/s
[07/01 13:35:26] ppdet.engine INFO: Epoch: [0] [310/526] learning_rate: 0.000008 loss_xy: 0.997568 loss_wh: 1.509685 loss_iou: 3.722756 loss_iou_aware: 0.584143 loss_obj: 15.210144 loss_cls: 0.677971 loss: 22.759336 eta: 4 days, 15:50:47 batch_cost: 14.0941 data_cost: 13.3666 ips: 0.7095 images/s
[07/01 13:36:30] ppdet.engine INFO: Epoch: [0] [315/526] learning_rate: 0.000008 loss_xy: 1.045465 loss_wh: 1.656418 loss_iou: 3.877389 loss_iou_aware: 0.716924 loss_obj: 16.369495 loss_cls: 0.655378 loss: 24.386274 eta: 4 days, 15:30:22 batch_cost: 12.7003 data_cost: 12.0041 ips: 0.7874 images/s
[07/01 13:37:37] ppdet.engine INFO: Epoch: [0] [320/526] learning_rate: 0.000008 loss_xy: 1.154737 loss_wh: 1.728677 loss_iou: 4.048987 loss_iou_aware: 0.679962 loss_obj: 21.279150 loss_cls: 0.690357 loss: 29.280508 eta: 4 days, 15:15:21 batch_cost: 13.4124 data_cost: 12.4195 ips: 0.7456 images/s
[07/01 13:38:38] ppdet.engine INFO: Epoch: [0] [325/526] learning_rate: 0.000008 loss_xy: 1.082633 loss_wh: 1.326666 loss_iou: 3.693600 loss_iou_aware: 0.665069 loss_obj: 13.832065 loss_cls: 0.617947 loss: 21.107273 eta: 4 days, 14:52:22 batch_cost: 12.1507 data_cost: 11.4775 ips: 0.8230 images/s
[07/01 13:40:06] ppdet.engine INFO: Epoch: [0] [330/526] learning_rate: 0.000008 loss_xy: 1.102530 loss_wh: 1.402024 loss_iou: 3.685415 loss_iou_aware: 0.695506 loss_obj: 14.423463 loss_cls: 0.626651 loss: 22.490555 eta: 4 days, 15:04:41 batch_cost: 17.4448 data_cost: 16.7416 ips: 0.5732 images/s
[07/01 13:41:16] ppdet.engine INFO: Epoch: [0] [335/526] learning_rate: 0.000008 loss_xy: 1.136834 loss_wh: 1.368255 loss_iou: 3.694318 loss_iou_aware: 0.656776 loss_obj: 12.866175 loss_cls: 0.684606 loss: 21.235811 eta: 4 days, 14:54:13 batch_cost: 13.9724 data_cost: 13.1803 ips: 0.7157 images/s
[07/01 13:42:26] ppdet.engine INFO: Epoch: [0] [340/526] learning_rate: 0.000009 loss_xy: 1.002786 loss_wh: 1.762086 loss_iou: 3.859673 loss_iou_aware: 0.691230 loss_obj: 16.758720 loss_cls: 0.673387 loss: 25.358444 eta: 4 days, 14:43:41 batch_cost: 13.9185 data_cost: 13.0802 ips: 0.7185 images/s
[07/01 13:43:29] ppdet.engine INFO: Epoch: [0] [345/526] learning_rate: 0.000009 loss_xy: 1.104222 loss_wh: 1.643591 loss_iou: 4.099178 loss_iou_aware: 0.664676 loss_obj: 18.918755 loss_cls: 0.557912 loss: 26.811903 eta: 4 days, 14:25:41 batch_cost: 12.6807 data_cost: 11.7806 ips: 0.7886 images/s
[07/01 13:44:36] ppdet.engine INFO: Epoch: [0] [350/526] learning_rate: 0.000009 loss_xy: 1.141841 loss_wh: 1.622595 loss_iou: 3.870060 loss_iou_aware: 0.679749 loss_obj: 17.151106 loss_cls: 0.593945 loss: 25.032406 eta: 4 days, 14:11:32 batch_cost: 13.2250 data_cost: 12.3910 ips: 0.7561 images/s
[07/01 13:45:37] ppdet.engine INFO: Epoch: [0] [355/526] learning_rate: 0.000009 loss_xy: 1.052077 loss_wh: 1.664710 loss_iou: 3.881771 loss_iou_aware: 0.661104 loss_obj: 14.782454 loss_cls: 0.469873 loss: 22.958609 eta: 4 days, 13:51:20 batch_cost: 12.1702 data_cost: 11.4402 ips: 0.8217 images/s
[07/01 13:46:43] ppdet.engine INFO: Epoch: [0] [360/526] learning_rate: 0.000009 loss_xy: 1.093885 loss_wh: 1.406798 loss_iou: 3.811672 loss_iou_aware: 0.694201 loss_obj: 14.363710 loss_cls: 0.596051 loss: 22.274965 eta: 4 days, 13:37:47 batch_cost: 13.1939 data_cost: 12.4711 ips: 0.7579 images/s
[07/01 13:47:51] ppdet.engine INFO: Epoch: [0] [365/526] learning_rate: 0.000009 loss_xy: 1.124093 loss_wh: 1.381487 loss_iou: 3.874315 loss_iou_aware: 0.699272 loss_obj: 17.813011 loss_cls: 0.612456 loss: 25.545204 eta: 4 days, 13:26:30 batch_cost: 13.5193 data_cost: 12.4458 ips: 0.7397 images/s
[07/01 13:49:03] ppdet.engine INFO: Epoch: [0] [370/526] learning_rate: 0.000009 loss_xy: 1.188190 loss_wh: 1.624373 loss_iou: 3.864948 loss_iou_aware: 0.702230 loss_obj: 14.380863 loss_cls: 0.686754 loss: 22.397690 eta: 4 days, 13:21:04 batch_cost: 14.4740 data_cost: 13.5806 ips: 0.6909 images/s
[07/01 13:50:05] ppdet.engine INFO: Epoch: [0] [375/526] learning_rate: 0.000009 loss_xy: 1.113057 loss_wh: 1.414446 loss_iou: 3.888521 loss_iou_aware: 0.698814 loss_obj: 11.986423 loss_cls: 0.579011 loss: 18.496145 eta: 4 days, 13:03:37 batch_cost: 12.3672 data_cost: 11.5277 ips: 0.8086 images/s
[07/01 13:51:29] ppdet.engine INFO: Epoch: [0] [380/526] learning_rate: 0.000010 loss_xy: 0.991184 loss_wh: 1.621124 loss_iou: 3.902881 loss_iou_aware: 0.680126 loss_obj: 11.574911 loss_cls: 0.570284 loss: 20.008980 eta: 4 days, 13:11:17 batch_cost: 16.7181 data_cost: 16.0548 ips: 0.5982 images/s
[07/01 13:52:33] ppdet.engine INFO: Epoch: [0] [385/526] learning_rate: 0.000010 loss_xy: 1.156179 loss_wh: 1.454666 loss_iou: 3.837860 loss_iou_aware: 0.697370 loss_obj: 15.354259 loss_cls: 0.557391 loss: 22.213127 eta: 4 days, 12:56:43 batch_cost: 12.7873 data_cost: 11.8790 ips: 0.7820 images/s
[07/01 13:53:37] ppdet.engine INFO: Epoch: [0] [390/526] learning_rate: 0.000010 loss_xy: 1.044631 loss_wh: 1.496924 loss_iou: 3.769402 loss_iou_aware: 0.666927 loss_obj: 11.900872 loss_cls: 0.625725 loss: 18.992374 eta: 4 days, 12:42:10 batch_cost: 12.7279 data_cost: 12.0840 ips: 0.7857 images/s
[07/01 13:54:42] ppdet.engine INFO: Epoch: [0] [395/526] learning_rate: 0.000010 loss_xy: 1.177679 loss_wh: 1.506815 loss_iou: 4.003256 loss_iou_aware: 0.688290 loss_obj: 13.409214 loss_cls: 0.672994 loss: 21.275620 eta: 4 days, 12:28:31 batch_cost: 12.8296 data_cost: 11.9984 ips: 0.7794 images/s
[07/01 13:55:45] ppdet.engine INFO: Epoch: [0] [400/526] learning_rate: 0.000010 loss_xy: 1.173628 loss_wh: 1.515467 loss_iou: 4.000115 loss_iou_aware: 0.690052 loss_obj: 14.598661 loss_cls: 0.626922 loss: 22.702097 eta: 4 days, 12:14:20 batch_cost: 12.6730 data_cost: 11.6656 ips: 0.7891 images/s
[07/01 13:56:43] ppdet.engine INFO: Epoch: [0] [405/526] learning_rate: 0.000010 loss_xy: 1.100368 loss_wh: 1.575760 loss_iou: 4.194187 loss_iou_aware: 0.671972 loss_obj: 9.648710 loss_cls: 0.603631 loss: 17.500242 eta: 4 days, 11:54:35 batch_cost: 11.5648 data_cost: 11.0840 ips: 0.8647 images/s
[07/01 13:57:44] ppdet.engine INFO: Epoch: [0] [410/526] learning_rate: 0.000010 loss_xy: 1.195703 loss_wh: 1.437436 loss_iou: 3.927448 loss_iou_aware: 0.744302 loss_obj: 14.254375 loss_cls: 0.612301 loss: 21.800064 eta: 4 days, 11:37:59 batch_cost: 12.0772 data_cost: 11.2205 ips: 0.8280 images/s
[07/01 13:58:52] ppdet.engine INFO: Epoch: [0] [415/526] learning_rate: 0.000010 loss_xy: 1.065183 loss_wh: 1.467490 loss_iou: 4.010128 loss_iou_aware: 0.646286 loss_obj: 10.328963 loss_cls: 0.642096 loss: 18.596519 eta: 4 days, 11:28:58 batch_cost: 13.4697 data_cost: 12.7611 ips: 0.7424 images/s
[07/01 14:00:10] ppdet.engine INFO: Epoch: [0] [420/526] learning_rate: 0.000010 loss_xy: 1.073382 loss_wh: 1.399643 loss_iou: 3.956860 loss_iou_aware: 0.691939 loss_obj: 14.661393 loss_cls: 0.647687 loss: 22.744785 eta: 4 days, 11:31:40 batch_cost: 15.7167 data_cost: 14.6602 ips: 0.6363 images/s
[07/01 14:01:11] ppdet.engine INFO: Epoch: [0] [425/526] learning_rate: 0.000011 loss_xy: 1.110812 loss_wh: 1.475729 loss_iou: 3.955990 loss_iou_aware: 0.702570 loss_obj: 12.199990 loss_cls: 0.642899 loss: 19.389687 eta: 4 days, 11:16:09 batch_cost: 12.1384 data_cost: 11.2846 ips: 0.8238 images/s
[07/01 14:02:20] ppdet.engine INFO: Epoch: [0] [430/526] learning_rate: 0.000011 loss_xy: 1.133572 loss_wh: 1.311864 loss_iou: 3.688827 loss_iou_aware: 0.692649 loss_obj: 10.177580 loss_cls: 0.530342 loss: 17.044258 eta: 4 days, 11:08:35 batch_cost: 13.6620 data_cost: 12.9042 ips: 0.7320 images/s
[07/01 14:03:34] ppdet.engine INFO: Epoch: [0] [435/526] learning_rate: 0.000011 loss_xy: 1.053666 loss_wh: 1.387102 loss_iou: 3.845040 loss_iou_aware: 0.705124 loss_obj: 12.000084 loss_cls: 0.618547 loss: 19.661043 eta: 4 days, 11:07:13 batch_cost: 14.8847 data_cost: 14.0828 ips: 0.6718 images/s
[07/01 14:04:40] ppdet.engine INFO: Epoch: [0] [440/526] learning_rate: 0.000011 loss_xy: 1.012944 loss_wh: 1.303181 loss_iou: 3.869767 loss_iou_aware: 0.657314 loss_obj: 10.116856 loss_cls: 0.617109 loss: 16.418688 eta: 4 days, 10:56:55 batch_cost: 13.0560 data_cost: 12.3919 ips: 0.7659 images/s
[07/01 14:05:46] ppdet.engine INFO: Epoch: [0] [445/526] learning_rate: 0.000011 loss_xy: 1.020963 loss_wh: 1.274895 loss_iou: 3.828473 loss_iou_aware: 0.703915 loss_obj: 10.813742 loss_cls: 0.580715 loss: 18.155323 eta: 4 days, 10:47:02 batch_cost: 13.0975 data_cost: 12.2879 ips: 0.7635 images/s
[07/01 14:06:46] ppdet.engine INFO: Epoch: [0] [450/526] learning_rate: 0.000011 loss_xy: 1.147226 loss_wh: 1.537148 loss_iou: 3.797288 loss_iou_aware: 0.686343 loss_obj: 11.856258 loss_cls: 0.604958 loss: 19.863937 eta: 4 days, 10:32:20 batch_cost: 12.0528 data_cost: 11.1376 ips: 0.8297 images/s
[07/01 14:07:56] ppdet.engine INFO: Epoch: [0] [455/526] learning_rate: 0.000011 loss_xy: 1.034053 loss_wh: 1.613674 loss_iou: 3.838971 loss_iou_aware: 0.658241 loss_obj: 9.643236 loss_cls: 0.544299 loss: 17.847387 eta: 4 days, 10:27:15 batch_cost: 14.0231 data_cost: 13.3004 ips: 0.7131 images/s
[07/01 14:09:07] ppdet.engine INFO: Epoch: [0] [460/526] learning_rate: 0.000012 loss_xy: 1.040160 loss_wh: 1.295907 loss_iou: 3.821574 loss_iou_aware: 0.717952 loss_obj: 13.428027 loss_cls: 0.582457 loss: 21.589560 eta: 4 days, 10:22:28 batch_cost: 14.0663 data_cost: 13.0924 ips: 0.7109 images/s
[07/01 14:10:13] ppdet.engine INFO: Epoch: [0] [465/526] learning_rate: 0.000012 loss_xy: 1.152655 loss_wh: 1.342875 loss_iou: 3.947140 loss_iou_aware: 0.691189 loss_obj: 9.812326 loss_cls: 0.619066 loss: 17.257215 eta: 4 days, 10:13:22 batch_cost: 13.1199 data_cost: 12.3495 ips: 0.7622 images/s
[07/01 14:11:20] ppdet.engine INFO: Epoch: [0] [470/526] learning_rate: 0.000012 loss_xy: 1.009960 loss_wh: 1.367424 loss_iou: 3.860856 loss_iou_aware: 0.641173 loss_obj: 8.767929 loss_cls: 0.530717 loss: 16.264814 eta: 4 days, 10:05:49 batch_cost: 13.4169 data_cost: 12.7390 ips: 0.7453 images/s
[07/01 14:12:26] ppdet.engine INFO: Epoch: [0] [475/526] learning_rate: 0.000012 loss_xy: 0.880614 loss_wh: 1.468517 loss_iou: 3.700959 loss_iou_aware: 0.720627 loss_obj: 11.092227 loss_cls: 0.627311 loss: 18.430204 eta: 4 days, 9:56:36 batch_cost: 13.0224 data_cost: 12.0082 ips: 0.7679 images/s
[07/01 14:13:27] ppdet.engine INFO: Epoch: [0] [480/526] learning_rate: 0.000012 loss_xy: 1.061072 loss_wh: 1.287019 loss_iou: 3.872210 loss_iou_aware: 0.684550 loss_obj: 8.296071 loss_cls: 0.556529 loss: 15.575493 eta: 4 days, 9:44:15 batch_cost: 12.2817 data_cost: 11.5879 ips: 0.8142 images/s
[07/01 14:14:25] ppdet.engine INFO: Epoch: [0] [485/526] learning_rate: 0.000012 loss_xy: 0.908903 loss_wh: 1.104860 loss_iou: 3.683180 loss_iou_aware: 0.719687 loss_obj: 11.253470 loss_cls: 0.624695 loss: 18.605165 eta: 4 days, 9:28:30 batch_cost: 11.4633 data_cost: 10.5807 ips: 0.8724 images/s
[07/01 14:15:29] ppdet.engine INFO: Epoch: [0] [490/526] learning_rate: 0.000012 loss_xy: 1.040438 loss_wh: 1.338577 loss_iou: 3.615209 loss_iou_aware: 0.749816 loss_obj: 10.571490 loss_cls: 0.549764 loss: 17.733517 eta: 4 days, 9:18:47 batch_cost: 12.7700 data_cost: 11.8489 ips: 0.7831 images/s
[07/01 14:16:34] ppdet.engine INFO: Epoch: [0] [495/526] learning_rate: 0.000012 loss_xy: 0.972936 loss_wh: 1.131723 loss_iou: 3.537634 loss_iou_aware: 0.662659 loss_obj: 10.013592 loss_cls: 0.520124 loss: 16.848978 eta: 4 days, 9:10:33 batch_cost: 13.0737 data_cost: 12.1537 ips: 0.7649 images/s
[07/01 14:17:38] ppdet.engine INFO: Epoch: [0] [500/526] learning_rate: 0.000013 loss_xy: 1.043951 loss_wh: 1.071104 loss_iou: 3.648298 loss_iou_aware: 0.709077 loss_obj: 8.897619 loss_cls: 0.664455 loss: 15.998529 eta: 4 days, 9:01:07 batch_cost: 12.7595 data_cost: 11.9775 ips: 0.7837 images/s
[07/01 14:18:31] ppdet.engine INFO: Epoch: [0] [505/526] learning_rate: 0.000013 loss_xy: 0.995084 loss_wh: 1.141770 loss_iou: 3.911056 loss_iou_aware: 0.675935 loss_obj: 7.697985 loss_cls: 0.607199 loss: 14.856952 eta: 4 days, 8:41:53 batch_cost: 10.4142 data_cost: 9.7294 ips: 0.9602 images/s
[07/01 14:19:31] ppdet.engine INFO: Epoch: [0] [510/526] learning_rate: 0.000013 loss_xy: 1.022732 loss_wh: 1.114027 loss_iou: 3.688476 loss_iou_aware: 0.752083 loss_obj: 8.362950 loss_cls: 0.512267 loss: 16.175888 eta: 4 days, 8:29:39 batch_cost: 11.9926 data_cost: 11.2365 ips: 0.8338 images/s
[07/01 14:20:40] ppdet.engine INFO: Epoch: [0] [515/526] learning_rate: 0.000013 loss_xy: 0.999646 loss_wh: 1.286703 loss_iou: 3.516223 loss_iou_aware: 0.709584 loss_obj: 10.304377 loss_cls: 0.590871 loss: 17.952936 eta: 4 days, 8:25:17 batch_cost: 13.8291 data_cost: 12.8221 ips: 0.7231 images/s
[07/01 14:21:42] ppdet.engine INFO: Epoch: [0] [520/526] learning_rate: 0.000013 loss_xy: 0.960961 loss_wh: 1.245511 loss_iou: 3.809340 loss_iou_aware: 0.699583 loss_obj: 6.763412 loss_cls: 0.518565 loss: 13.833389 eta: 4 days, 8:14:33 batch_cost: 12.2701 data_cost: 11.5657 ips: 0.8150 images/s
[07/01 14:22:43] ppdet.engine INFO: Epoch: [0] [525/526] learning_rate: 0.000013 loss_xy: 0.961485 loss_wh: 1.279465 loss_iou: 3.649951 loss_iou_aware: 0.640815 loss_obj: 6.676243 loss_cls: 0.543096 loss: 14.173296 eta: 4 days, 8:03:37 batch_cost: 12.1764 data_cost: 11.5398 ips: 0.8213 images/s
[07/01 14:22:50] reader WARNING: fail to map sample transform [Decode_5b4f7f] with error: [Errno 5] Input/output error and stack:
Traceback (most recent call last):
  File ""/content/drive/MyDrive/UIT/Four_Year/Term_2/DS505.M21/Code/PaddleDetection/ppdet/data/reader.py"", line 54, in __call__
    data = f(data)
  File ""/content/drive/MyDrive/UIT/Four_Year/Term_2/DS505.M21/Code/PaddleDetection/ppdet/data/transform/operators.py"", line 103, in __call__
    sample[i] = self.apply(sample[i], context)
  File ""/content/drive/MyDrive/UIT/Four_Year/Term_2/DS505.M21/Code/PaddleDetection/ppdet/data/transform/operators.py"", line 123, in apply
    sample['image'] = f.read()
OSError: [Errno 5] Input/output error

Exception in thread Thread-2:
Traceback (most recent call last):
  File ""/usr/lib/python3.7/threading.py"", line 926, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.7/threading.py"", line 870, in run
    self._target(*self._args, **self._kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 216, in _thread_loop
    self._thread_done_event)
  File ""/usr/local/lib/python3.7/dist-packages/paddle/fluid/dataloader/fetcher.py"", line 121, in fetch
    data.append(self.dataset[idx])
  File ""/content/drive/MyDrive/UIT/Four_Year/Term_2/DS505.M21/Code/PaddleDetection/ppdet/data/source/dataset.py"", line 91, in __getitem__
    return self.transform(roidb)
  File ""/content/drive/MyDrive/UIT/Four_Year/Term_2/DS505.M21/Code/PaddleDetection/ppdet/data/reader.py"", line 60, in __call__
    raise e
  File ""/content/drive/MyDrive/UIT/Four_Year/Term_2/DS505.M21/Code/PaddleDetection/ppdet/data/reader.py"", line 54, in __call__
    data = f(data)
  File ""/content/drive/MyDrive/UIT/Four_Year/Term_2/DS505.M21/Code/PaddleDetection/ppdet/data/transform/operators.py"", line 103, in __call__
    sample[i] = self.apply(sample[i], context)
  File ""/content/drive/MyDrive/UIT/Four_Year/Term_2/DS505.M21/Code/PaddleDetection/ppdet/data/transform/operators.py"", line 123, in apply
    sample['image'] = f.read()
OSError: [Errno 5] Input/output error

[07/01 14:23:06] ppdet.utils.checkpoint INFO: Save checkpoint: output/ppyolo_r50vd_dcn_1x_coco/ppyolo_r50vd_dcn_1x_coco

```
I use PASCAL VOC format to train model. Does the bug above have any effect on the model's training? 
Thank you for your time to support me.

### 复现环境 Environment

Paddlepaddle-gpu==2.3.0
PaddleDetection: Release/2.4
Python 3.7.13
CUDA Version: 11.2
cuDNN Version: 7.6.

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
FairMOT轻量级模型导出，使用官方c++代码推理报错,PaddlePaddle/PaddleDetection,2022-07-01 11:41:03,2,,6329,1291297196,"### 问题描述 Please describe your issue

我训练了一个FairMOT轻量级模型（HRNetV2-W18），导出推理模型后，使用官方的c++代码进行部署推理，出现错误。错误如下图所示：
![QQ截图20220701194003](https://user-images.githubusercontent.com/26791156/176887788-879a5d33-5831-4827-870f-501a61d6856d.png)

使用其他的FairMOT模型没有问题。怎么解决呢？
"
ssd_mobilenet_v1_relu_voc_int8_300_per_layer下这个模型怎么复现呢,PaddlePaddle/PaddleDetection,2022-07-01 03:29:02,2,,6323,1290860470,"### 问题描述 Please describe your issue

https://paddlelite-demo.bj.bcebos.com/models/ssd_mobilenet_v1_relu_voc_int8_300_per_layer.tar.gz
这个ssd_mobilenet_v1_relu_voc_int8_300_per_layer是用paddle-detection训练量化出来的吗，怎么复现呢"
infer.py: error: unrecognized arguments: -c,PaddlePaddle/PaddleDetection,2022-06-30 08:03:29,3,,6317,1289753422,"### 问题确认 Search before asking

- [x] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

使用2.3版本，突然无法进行推理，问题出现在用ppyolo进行训练后，之前用hrnet，hrnet等训练的模型均无法正常推理。只能导出以后进行推理，
![image](https://user-images.githubusercontent.com/35481439/176599713-0fa969ef-23e5-442a-b583-ca3dceca05ee.png)
提示中的usage里面的参数好像都不是tools/infer.py中需要的参数，所以感觉并没有真正调用tools/infer.py，是调用了其他的infer.py? 还有一点就是，调用怎么感觉好像是以客户端的方式在调用？不太明白，请问有什么思路或建议吗？
"
大规模行人跟踪 pathtrack数据集重训练,PaddlePaddle/PaddleDetection,2022-06-30 03:34:20,3,,6313,1289533736,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

想咨询一下：大规模行人跟踪 (Pedestrian Tracking) 中提供的pathtrack模型是否只使用pathtrack数据集重新训练了reID部分呢？ 如果想要重新训练检测网络是否有合适的数据集可以使用？"
Coco可以使用.txt取代.json作为label吗,PaddlePaddle/PaddleDetection,2022-06-28 07:51:59,2,,6294,1286939870,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

在coco数据集的基础上新增了两个类，然后创建的是 class, x, y, weight, height类型的.txt标签文件，可以不使用.json而使用.txt进行训练吗"
c++部署编译成功，链接错误,PaddlePaddle/PaddleDetection,2022-06-28 06:48:55,4,,6292,1286866842,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

c++部署编译成功，链接错误

![image](https://user-images.githubusercontent.com/46232202/176108083-27933f77-432f-4d04-9031-39cd4271be44.png)

环境：
ubuntu 20.04 
opencv opencv-3.4.16_gcc8.2_ffmpeg
cmake 3.2.0
gcc 9.4.0
paddleDetection 2.4"
MaskRCNN 开启mkldnn预测出错,PaddlePaddle/PaddleDetection,2022-06-28 05:50:34,8,,6289,1286811382,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

MaskRCNN infer时开启mkldnn后 结果矩阵混乱
关闭mkldnn:
![image](https://user-images.githubusercontent.com/25918643/176102847-1497e23b-883e-41bd-b011-a11ed95a62f5.png)
开启mkldnn：
![image](https://user-images.githubusercontent.com/25918643/176103174-6e3c0011-25e5-4a56-bc3a-8e2bf1bb022d.png)


### 复现环境 Environment

模型：
mask_rcnn_r50_vd_fpn_2x：
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.4/configs/mask_rcnn/mask_rcnn_r50_vd_fpn_2x_coco.yml
cascade_mask_rcnn_r50_fpn_1x：
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.4/configs/cascade_rcnn/cascade_mask_rcnn_r50_fpn_1x_coco.yml
复现代码：
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.4/deploy/python/infer.py
复现步骤：
下载官方提供的coco预训练模型，使用infer.py进行推理，参数中指定开启mkldnn，结果出错
paddle版本：2.2.2 和 2.3均出现同样的问题
语言：python
系统：linux：Linux version 3.10.0-1160.42.2.el7.x86_64(ubuntu 16.04)
windows：Windows Server 2019

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
maskrcnn训练的模型 用tensorRT加速有问题,PaddlePaddle/PaddleDetection,2022-06-28 04:40:43,2,,6287,1286760863,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

用paddlex训练了一个maskrcnn模型，然后用tensorRT加速，我用export_inference 转成tensorRT能用模型，但是运行的时候会有问题，报下面这个错误，
![image](https://user-images.githubusercontent.com/31894804/176094128-8e6ad4fb-2dfc-4181-9aa9-92940c35aae9.png)
然后我把输入图片尺寸改成[512，320]后,又报另一个错
![image](https://user-images.githubusercontent.com/31894804/176094305-015efdf3-b949-4dd6-b124-72107fa9e11a.png)
请问如何改正"
有用c#调用的么，net core,PaddlePaddle/PaddleDetection,2022-06-28 03:51:13,2,,6285,1286729491,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

求指点相关说明。"
> @zmdcs 可以指定--classwise，会输出PR，具体代码在： https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.3/ppdet/metrics/map_utils.py#L325-L326,PaddlePaddle/PaddleDetection,2022-06-28 02:07:20,4,,6280,1286662895,"> @zmdcs 可以指定--classwise，会输出PR，具体代码在： https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.3/ppdet/metrics/map_utils.py#L325-L326

我想取出具体iou=0.95或者0.5的precision和recall是可以的吗？

_Originally posted by @zmdcs in https://github.com/PaddlePaddle/PaddleDetection/issues/4826#issuecomment-988644004_"
third_engine--demo_openvino报错，模型是从paddle的inference导出的onnx格式，在demo_onnxruntime中测试没有问题,PaddlePaddle/PaddleDetection,2022-06-28 01:50:48,3,,6279,1286652503,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

Traceback (most recent call last):
  File ""D:/work/projects/PaddleDetection/deploy/third_engine/demo_openvino/python/openvino_infer.py"", line 265, in <module>
    compiled_model = ie.compile_model(net, 'CPU')
  File ""D:\work\apps\Anaconda3\lib\site-packages\openvino\runtime\ie_api.py"", line 266, in compile_model
    super().compile_model(model, device_name, {} if config is None else config)
RuntimeError: NonZero layer with name 'NonZero_0' has incorrect number of output edges: 2

### 复现环境 Environment

win10
paddlepaddle2.2
paddledetection2.4
openvino-dev2022.1.0

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
TensorRT没有NonZero插件，请问有替换PPYoLo中NonZero的方法吗?,PaddlePaddle/PaddleDetection,2022-06-27 14:29:22,2,,6278,1285895754,"[06/27/2022-12:10:38] [I] [TRT] Searching for plugin: NonZero, plugin_version: 1, plugin_namespace: 
[06/27/2022-12:10:38] [E] [TRT] ModelImporter.cpp:773: While parsing node number 1139 [NonZero -> ""NonZero_0""]:
[06/27/2022-12:10:38] [E] [TRT] ModelImporter.cpp:774: --- Begin node ---
[06/27/2022-12:10:38] [E] [TRT] ModelImporter.cpp:775: input: ""Sub_51""
output: ""NonZero_0""
name: ""NonZero_0""
op_type: ""NonZero""

[06/27/2022-12:10:38] [E] [TRT] ModelImporter.cpp:776: --- End node ---
[06/27/2022-12:10:38] [E] [TRT] ModelImporter.cpp:779: ERROR: builtin_op_importers.cpp:4871 In function importFallbackPluginImporter:
[8] Assertion failed: creator && ""Plugin not found, are the plugin name, version, and namespace correct?"""
FatalError: Build TensorRT cuda engine failed!,PaddlePaddle/PaddleDetection,2022-06-26 04:50:49,2,deploy,6270,1284802503,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

Thanks for your excellent work!  when I try FP16 inference,I run:
`python3 deploy/python/det_keypoint_unite_infer.py --det_model_dir=output_inference/picodet_s_320_pedestrian --keypoint_model_dir=output_inference/tinypose_128x96 --video_file={/home/Packages/PaddleDetection-release-2.4/mov.mp4} --device=GPU --run_mode=trt_fp16`
but I get:
`SystemError: C++ Traceback (most recent call last):
0   paddle_infer::Predictor::Predictor(paddle::AnalysisConfig const&)
1   std::unique_ptr<paddle::PaddlePredictor, std::default_delete<paddle::PaddlePredictor> > paddle::CreatePaddlePredictor<paddle::AnalysisConfig, (paddle::PaddleEngineKind)2>(paddle::AnalysisConfig const&)
2   paddle::AnalysisPredictor::Init(std::shared_ptr<paddle::framework::Scope> const&, std::shared_ptr<paddle::framework::ProgramDesc> const&)
3   paddle::AnalysisPredictor::PrepareProgram(std::shared_ptr<paddle::framework::ProgramDesc> const&)
4   paddle::AnalysisPredictor::OptimizeInferenceProgram()
5   paddle::inference::analysis::Analyzer::RunAnalysis(paddle::inference::analysis::Argument*)
6   paddle::inference::analysis::IrAnalysisPass::RunImpl(paddle::inference::analysis::Argument*)
7   paddle::inference::analysis::IRPassManager::Apply(std::unique_ptr<paddle::framework::ir::Graph, std::default_delete<paddle::framework::ir::Graph> >)
8   paddle::framework::ir::Pass::Apply(paddle::framework::ir::Graph*) const
9   paddle::inference::analysis::TensorRtSubgraphPass::ApplyImpl(paddle::framework::ir::Graph*) const
10  paddle::inference::analysis::TensorRtSubgraphPass::CreateTensorRTOp(paddle::framework::ir::Node*, paddle::framework::ir::Graph*, std::vector<std::string, std::allocator<std::string > > const&, std::vector<std::string, std::allocator<std::string > >*) const
11  paddle::inference::tensorrt::OpConverter::ConvertBlockToTRTEngine(paddle::framework::BlockDesc*, paddle::framework::Scope const&, std::vector<std::string, std::allocator<std::string > > const&, std::unordered_set<std::string, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::string > > const&, std::vector<std::string, std::allocator<std::string > > const&, paddle::inference::tensorrt::TensorRTEngine*)
12  paddle::inference::tensorrt::TensorRTEngine::FreezeNetwork()
13  paddle::platform::EnforceNotMet::EnforceNotMet(paddle::platform::ErrorSummary const&, char const*, int)
14  paddle::platform::GetCurrentTraceBackString[abi:cxx11]()

----------------------
Error Message Summary:
----------------------
FatalError: Build TensorRT cuda engine failed! Please recheck you configurations related to paddle-TensorRT.
  [Hint: infer_engine_ should not be null.] (at /home/shine/Packages/Paddle/paddle/fluid/inference/tensorrt/engine.cc:244)
`
I have  paddledet           2.4.0   paddlepaddle-gpu    0.0.0
thanks for your reply."
ppyoloe_crn_m_300e_coco 模型训练到100 epoch的时候报错,PaddlePaddle/PaddleDetection,2022-06-26 01:57:14,4,,6269,1284770731,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

有排查task_aligned_assigner中的操作，暂未发现有label异常，怀疑是这个操作之前的某些cuda操作出错导致backtrack中的函数调用出错
```
xxx@DESKTOP-HOET4AN:~/code/PaddleDetection$ python tools/train.py -c configs/ppyoloe/ppyoloe_crn_s_300e_coco.yml -r output/ppyoloe_crn_s_300e_coco/99.pdparams
/home/xxx/.local/lib/python3.8/site-packages/paddle/tensor/creation.py:125: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if data.dtype == np.object:
W0626 09:36:49.641674 15600 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 11.6, Runtime API Version: 11.2
W0626 09:36:49.647449 15600 gpu_context.cc:306] device: 0, cuDNN Version: 8.4.
[06/26 09:36:51] ppdet.utils.checkpoint INFO: Finish resuming model weights: output/ppyoloe_crn_s_300e_coco/99.pdparams
[06/26 09:36:53] ppdet.engine INFO: Epoch: [100] [  0/119] learning_rate: 0.033339 loss: 4.603765 loss_cls: 0.000000 loss_iou: 1.222221 loss_dfl: 3.096424 loss_l1: 5.899565 eta: 10:48:34 batch_cost: 1.6351 data_cost: 0.0002 ips: 7.3391 images/s
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Error: /paddle/paddle/phi/kernels/gpu/one_hot_kernel.cu:38 Assertion `p_in_data[idx] >= 0 && p_in_data[idx] < depth` failed. Illegal index value, Input(input) value should be greater than or equal to 0, and less than depth [6069], but received [140099021711266].
Traceback (most recent call last):
  File ""tools/train.py"", line 177, in <module>
    main()
  File ""tools/train.py"", line 173, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 127, in run
    trainer.train(FLAGS.eval)
  File ""/home/xxx/code/PaddleDetection/ppdet/engine/trainer.py"", line 448, in train
    outputs = model(data)
  File ""/home/xxx/.local/lib/python3.8/site-packages/paddle/fluid/dygraph/layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""/home/xxx/.local/lib/python3.8/site-packages/paddle/fluid/dygraph/layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""/home/xxx/code/PaddleDetection/ppdet/modeling/architectures/meta_arch.py"", line 59, in forward
    out = self.get_loss()
  File ""/home/xxx/code/PaddleDetection/ppdet/modeling/architectures/yolo.py"", line 125, in get_loss
    return self._forward()
  File ""/home/xxx/code/PaddleDetection/ppdet/modeling/architectures/yolo.py"", line 88, in _forward
    yolo_losses = self.yolo_head(neck_feats, self.inputs)
  File ""/home/xxx/.local/lib/python3.8/site-packages/paddle/fluid/dygraph/layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""/home/xxx/.local/lib/python3.8/site-packages/paddle/fluid/dygraph/layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""/home/xxx/code/PaddleDetection/ppdet/modeling/heads/ppyoloe_head.py"", line 217, in forward
    return self.forward_train(feats, targets)
  File ""/home/xxx/code/PaddleDetection/ppdet/modeling/heads/ppyoloe_head.py"", line 157, in forward_train
    return self.get_loss([
  File ""/home/xxx/code/PaddleDetection/ppdet/modeling/heads/ppyoloe_head.py"", line 323, in get_loss
    self.assigner(
  File ""/home/xxx/.local/lib/python3.8/site-packages/paddle/fluid/dygraph/layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""/home/xxx/.local/lib/python3.8/site-packages/paddle/fluid/dygraph/layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""/home/xxx/.local/lib/python3.8/site-packages/decorator.py"", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File ""/home/xxx/.local/lib/python3.8/site-packages/paddle/fluid/dygraph/base.py"", line 354, in _decorate_function
    return func(*args, **kwargs)
  File ""/home/xxx/code/PaddleDetection/ppdet/modeling/assigners/task_aligned_assigner.py"", line 125, in forward
    if mask_positive_sum.max() > 1:
  File ""/home/xxx/.local/lib/python3.8/site-packages/paddle/fluid/dygraph/varbase_patch_methods.py"", line 668, in __bool__
    return self.__nonzero__()
  File ""/home/xxx/.local/lib/python3.8/site-packages/paddle/fluid/dygraph/varbase_patch_methods.py"", line 665, in __nonzero__
    return bool(np.all(tensor.__array__() > 0))
OSError: (External) CUDA error(719), unspecified launch failure. 
  [Hint: 'cudaErrorLaunchFailure'. An exception occurred on the device while executing a kernel. Common causes include dereferencing an invalid device pointerand accessing out of bounds shared memory. Less common cases can be system specific - more information about these cases canbe found in the system specific user guide. This leaves the process in an inconsistent state and any further CUDA work willreturn the same error. To continue using CUDA, the process must be terminated and relaunched.] (at /paddle/paddle/phi/backends/gpu/cuda/cuda_info.cc:258)
```

### 复现环境 Environment

- paddledet:                  2.4.0
- paddlepaddle-gpu:    2.3.0.post112 
- Driver API Version:     11.6
- Runtime API Version: 11.2
- cuDNN Version:          8.4

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
solov2运行时返回对象result为空,PaddlePaddle/PaddleDetection,2022-06-24 15:22:53,2,,6268,1283880987,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

在使用paddle中solov2模型时，上传图片请求数据返回时的json对象中的result是空值导致无法继续进行，并且服务端没有报错，请问是什么原因还有如何解决
![QQ图片20220624153059](https://user-images.githubusercontent.com/86654889/175566751-b91f3d56-d882-4885-85c4-7579dce4e5b2.png)
![QQ图片20220624163923](https://user-images.githubusercontent.com/86654889/175566788-007e1874-44e9-4f33-884c-60b1cd99da11.png)
"
picoDet QAT训练突然精度大幅下降什么原因？,PaddlePaddle/PaddleDetection,2022-06-23 01:34:27,3,,6253,1281474410,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

picoDet 进行QAT量化训练，每10个epoch进行一次测试，前50epoch精度正常，到第60epoch 测试精度突然降低，map0.5-0.95接近0，map0.5 接近1. 请问这个是什么原因，有什么建议？然后具体修改比如轮数这些参数在哪设置？"
"Windows11+paddlepaddle-gpu==2.3.0+cuda11.2+cudnn8.2 出现""cudaErrorLaunchFailure""报错",PaddlePaddle/PaddleDetection,2022-06-22 12:56:54,2,,6252,1280102242,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

'''代码行执行
PS E:\项目文件\MOT\PaddleDetection-release-2.4\deploy\pptracking\python> python mot_jde_infer.py --model_dir=fairmot_dla34_30e_1088x608_visdrone_vehicle --video_file=videos/DJI_0061.MOV --device=GPU --threshold=0.4 --save_mot_txts --save_images --draw_center_traj

'''shell
-----------  Running Arguments -----------
batch_size: 1
camera_id: -1
cpu_threads: 1
device: GPU
do_entrance_counting: False
draw_center_traj: True
enable_mkldnn: False
image_dir: None
image_file: None
model_dir: fairmot_dla34_30e_1088x608_visdrone_vehicle
mtmct_cfg: None
mtmct_dir: None
output_dir: output
reid_batch_size: 50
reid_model_dir: None
run_benchmark: False
run_mode: paddle
save_images: True
save_mot_txt_per_img: False
save_mot_txts: True
scaled: False
secs_interval: 2
threshold: 0.4
tracker_config: None
trt_calib_mode: False
trt_max_shape: 1280
trt_min_shape: 1
trt_opt_shape: 640
use_dark: True
use_gpu: True
video_file: videos/DJI_0061.MOV
------------------------------------------
-----------  Model Configuration -----------
Model Arch: FairMOT
Transform Order:
--transform op: LetterBoxResize
--transform op: NormalizeImage
--transform op: Permute
--------------------------------------------
fps: 29, frame_count: 9015
preprocess
predict
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Traceback (most recent call last):
  File ""mot_jde_infer.py"", line 443, in <module>
    main()
  File ""mot_jde_infer.py"", line 414, in main
    detector.predict_video(FLAGS.video_file, FLAGS.camera_id)
  File ""mot_jde_infer.py"", line 332, in predict_video
    [frame], visual=False, seq_name=seq_name)
  File ""mot_jde_infer.py"", line 237, in predict_image
    result = self.predict()
  File ""mot_jde_infer.py"", line 175, in predict
    self.predictor.run()
OSError: In user code:

    File ""tools/export_model.py"", line 114, in <module>
      main()
    File ""tools/export_model.py"", line 110, in main
      run(FLAGS, cfg)
    File ""tools/export_model.py"", line 76, in run
      trainer.export(FLAGS.output_dir)
    File ""G:\学习\源码\PaddlePaddle\PaddleDetection-release-2.4\ppdet\engine\trainer.py"", line 775, in export
      save_dir)
    File ""G:\学习\源码\PaddlePaddle\PaddleDetection-release-2.4\ppdet\engine\trainer.py"", line 752, in _get_infer_cfg_and_input_spec
      input_spec, static_model.forward.main_program,
    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 580, in main_program
      concrete_program = self.concrete_program
    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 488, in concrete_program
      return self.concrete_program_specify_input_spec(input_spec=None)
    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 528, in concrete_program_specify_input_spec
      *desired_input_spec, with_hook=with_hook)
    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 436, in get_concrete_program
      concrete_program, partial_program_layer = self._program_cache[cache_key]
    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 801, in __getitem__
      self._caches[item_id] = self._build_once(item)
    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 790, in _build_once
      **cache_key.kwargs)
    File ""<decorator-gen-104>"", line 2, in from_func_spec

    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\wrapped_decorator.py"", line 25, in __impl__
      return wrapped_func(*args, **kwargs)
    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\dygraph\base.py"", line 51, in __impl__
      return func(*args, **kwargs)
    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 733, in from_func_spec
      outputs = static_func(*inputs)
    File ""C:\Users\ADMINI~1\AppData\Local\Temp\tmpkzhzwl8j.py"", line 101, in forward
      false_fn_5, (), (inputs, self), (out,))
    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\convert_operators.py"", line 211, in convert_ifelse
      out = _run_py_ifelse(pred, true_fn, false_fn, true_args, false_args)
    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\convert_operators.py"", line 257, in _run_py_ifelse
      return true_fn(*true_args) if pred else false_fn(*false_args)
    File ""C:\Users\ADMINI~1\AppData\Local\Temp\tmpkzhzwl8j.py"", line 84, in false_fn_5
      for_loop_body_0, [inputs_list, __for_loop_var_index_0])
    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\convert_operators.py"", line 45, in convert_while_loop
      loop_vars = _run_py_while(cond, body, loop_vars)
    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\convert_operators.py"", line 59, in _run_py_while
      loop_vars = body(*loop_vars)
    File ""C:\Users\ADMINI~1\AppData\Local\Temp\tmpkzhzwl8j.py"", line 79, in for_loop_body_0
      dy2static.convert_call(self.get_pred)())
    File ""G:\学习\源码\PaddlePaddle\PaddleDetection-release-2.4\ppdet\modeling\architectures\fairmot.py"", line 95, in get_pred
      output = self._forward()
    File ""G:\学习\源码\PaddlePaddle\PaddleDetection-release-2.4\ppdet\modeling\architectures\fairmot.py"", line 75, in _forward
      det_outs = self.detector(self.inputs)
    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
      return self._dygraph_call_func(*inputs, **kwargs)
    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
      outputs = self.forward(*inputs, **kwargs)
    File ""C:\Users\ADMINI~1\AppData\Local\Temp\tmp_u2pp8lm.py"", line 101, in forward
      false_fn_5, (), (inputs, self), (out,))
    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\convert_operators.py"", line 211, in convert_ifelse
      out = _run_py_ifelse(pred, true_fn, false_fn, true_args, false_args)
    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\convert_operators.py"", line 257, in _run_py_ifelse
      return true_fn(*true_args) if pred else false_fn(*false_args)
    File ""C:\Users\ADMINI~1\AppData\Local\Temp\tmp_u2pp8lm.py"", line 84, in false_fn_5
      for_loop_body_0, [inputs_list, __for_loop_var_index_0])
    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\convert_operators.py"", line 45, in convert_while_loop
      loop_vars = _run_py_while(cond, body, loop_vars)
    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\convert_operators.py"", line 59, in _run_py_while
      loop_vars = body(*loop_vars)
    File ""C:\Users\ADMINI~1\AppData\Local\Temp\tmp_u2pp8lm.py"", line 79, in for_loop_body_0
      dy2static.convert_call(self.get_pred)())
    File ""C:\Users\ADMINI~1\AppData\Local\Temp\tmpdoifpuaw.py"", line 31, in get_pred
      output))
    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\convert_operators.py"", line 211, in convert_ifelse
      out = _run_py_ifelse(pred, true_fn, false_fn, true_args, false_args)
    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\convert_operators.py"", line 257, in _run_py_ifelse
      return true_fn(*true_args) if pred else false_fn(*false_args)
    File ""G:\学习\源码\PaddlePaddle\PaddleDetection-release-2.4\ppdet\modeling\architectures\centernet.py"", line 87, in get_pred
      scale_factor=self.inputs['scale_factor'])
    File ""G:\学习\源码\PaddlePaddle\PaddleDetection-release-2.4\ppdet\modeling\post_process.py"", line 507, in __call__
      scores, inds, topk_clses, ys, xs = self._topk(heat)
    File ""G:\学习\源码\PaddlePaddle\PaddleDetection-release-2.4\ppdet\modeling\layers.py"", line 837, in _topk
      topk_xs = topk_inds % width
    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\layers\math_op_patch.py"", line 347, in __impl__
      attrs={'axis': axis})
    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\framework.py"", line 3621, in append_op
      attrs=kwargs.get(""attrs"", None))
    File ""C:\Program Files\Python36\lib\site-packages\paddle\fluid\framework.py"", line 2635, in __init__
      for frame in traceback.extract_stack():

    ExternalError: CUDA error(719), unspecified launch failure.
      [Hint: 'cudaErrorLaunchFailure'. An exception occurred on the device while executing a kernel. Common causes include dereferencing an invalid device pointerand accessing out of bounds shared memory. L
ess common cases can be system specific - more information about these cases canbe found in the system specific user guide. This leaves the process in an inconsistent state and any further CUDA work willreturn the same error. To continue using CUDA, the process must be terminated and relaunched.] (at ..\paddle\phi\backends\gpu\gpu_context.cc:624)
      [operator < elementwise_mod > error]

### 复现环境 Environment

- PaddlePaddle: GPU 2.3.0
- PaddleDetection: 2.4
- Windows: 11
- CUDA: 11.2
- CUDNN: 8.2

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
NAS  of Picodet really sota？,PaddlePaddle/PaddleDetection,2022-06-22 12:23:19,9,,6251,1280061400,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

Hi，
  这里有几个问题，尤其是: 
1  在检测数据集上训练one-shot超网 ， 这具体是指同时训练六个或者更多个不同的检测网络结构吗？
2  使用EA（evolutionary algorithm，进化算法）算法对训练好的超网络进行架构搜索。  这个是六个网络同时进行的，还是六个网络单独训练，结果出来后，再次使用EA算法演进出最优网络。或者更简单的可以粗略认为就是选出这多个网络中训练结果最好的一个呢？
3   请问是否有这个NAS具体的步骤说明和相关的源码部分，可以发下具体链接和代码位置吗？ 多谢
BR 
    "
我量化训练完成后，用python命令推断图片模型结果是正常的，然后用量化导出的paddlelite模型放在安卓上结果乱七八糟的，像不是一个模型一样，到底是怎么回事啊？,PaddlePaddle/PaddleDetection,2022-06-21 09:28:01,6,,6244,1278162887,"### 问题描述 Please describe your issue

python量化推断图片命令：
python tools/infer.py -c configs/picodet/picodet_xs_320_coco_lcnet.yml --slim_config configs/slim/quant/picodet_xs_320_lcnet_quant.yml -o weights=./output/picodet_xs_320_lcnet_quant/best_model  --infer_dir=./output/3

安卓量化导出命令：
python tools/export_model.py -c configs/picodet/picodet_xs_320_coco_lcnet.yml --slim_config configs/slim/quant/picodet_xs_320_lcnet_quant.yml -o weights=./output/picodet_xs_320_lcnet_quant/best_model --output_dir=inference_model

量化导出nb float16模型：
paddle_lite_opt  --model_dir=inference_model/picodet_xs_320_lcnet_quant --valid_targets=arm --optimize_out=inference_model/picodet_xs_320_lcnet_quant/model_xs_16 --enable_fp16=true

量化导出json：
python deploy/lite/convert_yml_to_json.py inference_model/picodet_xs_320_lcnet_quant/infer_cfg.yml

我量化训练完成后，用python命令推断图片模型结果是正常的，然后用量化导出的paddlelite模型放在安卓上结果乱七八糟的，像不是一个模型一样，到底是怎么回事啊？"
picoDet 量化训练相关问题,PaddlePaddle/PaddleDetection,2022-06-21 05:45:35,2,,6242,1277923133,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

1.picoDet 后量化是不是仍然不行，看2.3版本readme 提示精度有异常
2.picoDet 量化感知训练，是需要从头开始训练吗？不能使用训练好的模型进行训练？
3.picoDet QAT 只需要单卡就可以训练？具体的训练的轮数和模型输出路径在哪设置呢？"
windows C++部署报错,PaddlePaddle/PaddleDetection,2022-06-20 03:28:14,6,,6231,1276309999,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

在使用Paddledeteciton进行windows平台C++部署阶段，根据提供的编译教程--[Visual Studio 2019 Community CMake 编译指南](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.4/deploy/cpp/docs/windows_vs2019_build.md)已经完成了cmake和visual stuido生成项目，已经成功生成了对应的可执行文件（.exe）。但是在执行预测命令时发生如下的报错：![](https://pica.zhimg.com/80/v2-8774f7971720fe158c45656ea517d886_1440w.png)。
下面是尝试修改的方式：
1.将编译命令中```PADDLE_LIB_NAME=libpaddle_inference```，即
```
cmake . -G ""Visual Studio 16 2019"" -A x64 -T host=x64 -DWITH_GPU=ON -DWITH_MKL=ON -DCMAKE_BUILD_TYPE=Release -DCUDA_LIB=E:\Program\tools\CUDA\v10.2\lib\x64 -DCUDNN_LIB=E:\Program\tools\CUDA\v10.2\lib\x64 -DPADDLE_DIR=E:\Program\tools\paddle_inference -DPADDLE_LIB_NAME=libpaddle_inference -DOPENCV_DIR=E:\Program\tools\opencv
```
在cmake环节就会失败。
2. 将paddle_inference\third_party\install\paddle2onnx\lib下的paddle2onnx.dll复制到生成目录Release下
发生如下的报错：
![](https://pica.zhimg.com/80/v2-9d1bb3512f575c8aa031e388f192e3d2_1440w.png)

### 复现环境 Environment

- windows10家庭版
- Visual Studio 2019
- CUDA10.2
- cudnn 7.6.5
- opencv 3.4.6
- TensorRT 7.0.0.11 (下载的版本与cuda10.2和cudnn7.6.5对应)
- paddle_inference (最新版)
- CMake 3.23.2

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
python部署推理没问题，C++部署报错,PaddlePaddle/PaddleDetection,2022-06-17 06:45:51,2,,6220,1274638502,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

C++部署编译最后报错
![1](https://user-images.githubusercontent.com/101175483/174241779-9091b616-64b1-455f-bc30-4cc2c2f6676b.png)

尝试解决方法，将opencvdir换成自己编译的opencv，报错如下
![2](https://user-images.githubusercontent.com/101175483/174241832-aa059f19-44e4-4a58-b3be-f0046228c56a.png)

将cmake文件中报错行修改如下
![3](https://user-images.githubusercontent.com/101175483/174241850-5b997a93-7f2a-464e-ab4b-243187c87a0b.png)


最后编译报错如下
![4](https://user-images.githubusercontent.com/101175483/174241864-c605fbcb-efad-4d10-8b9a-88c0c218507a.png)


请问如何解决这个问题"
安装后测试test_architectures.py报错,PaddlePaddle/PaddleDetection,2022-06-17 04:08:52,2,,6219,1274541851,"### 问题描述 Please describe your issue

安装paddledetection前面的过程很顺利，但是测试时运行python test_architectures.py出现如下报错：（求大佬解答）

home/zhou/anaconda3/lib/python3.8/site-packages/paddle/tensor/creation.py:125: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if data.dtype == np.object:
EW0617 12:01:41.747937 361302 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.2, Runtime API Version: 11.2
W0617 12:01:41.751001 361302 gpu_context.cc:306] device: 0, cuDNN Version: 7.6.
EEEEEE
======================================================================
ERROR: test_trainer (__main__.TestCascadeRCNN)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""test_architectures.py"", line 34, in test_trainer
    cfg = ppdet.core.workspace.load_config(self.cfg_file)
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddledet-2.4.0-py3.8.egg/ppdet/core/workspace.py"", line 114, in load_config
    cfg = _load_config_with_base(file_path)
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddledet-2.4.0-py3.8.egg/ppdet/core/workspace.py"", line 78, in _load_config_with_base
    with open(file_path) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'configs/cascade_rcnn/cascade_rcnn_r50_fpn_1x_coco.yml'

======================================================================
ERROR: test_trainer (__main__.TestFasterRCNN)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""test_architectures.py"", line 35, in test_trainer
    trainer = ppdet.engine.Trainer(cfg, mode='test')
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddledet-2.4.0-py3.8.egg/ppdet/engine/trainer.py"", line 102, in __init__
    self.model = create(cfg.architecture)
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddledet-2.4.0-py3.8.egg/ppdet/core/workspace.py"", line 246, in create
    cls_kwargs.update(cls.from_config(config, **kwargs))
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddledet-2.4.0-py3.8.egg/ppdet/modeling/architectures/faster_rcnn.py"", line 56, in from_config
    backbone = create(cfg['backbone'])
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddledet-2.4.0-py3.8.egg/ppdet/core/workspace.py"", line 283, in create
    return cls(**cls_kwargs)
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddledet-2.4.0-py3.8.egg/ppdet/modeling/backbones/resnet.py"", line 514, in __init__
    ConvNormLayer(
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddledet-2.4.0-py3.8.egg/ppdet/modeling/backbones/resnet.py"", line 61, in __init__
    self.conv = nn.Conv2D(
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddle/nn/layer/conv.py"", line 644, in __init__
    super(Conv2D, self).__init__(
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddle/nn/layer/conv.py"", line 133, in __init__
    self.weight = self.create_parameter(
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/layers.py"", line 423, in create_parameter
    return self._helper.create_parameter(temp_attr, shape, dtype, is_bias,
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddle/fluid/layer_helper_base.py"", line 376, in create_parameter
    return self.main_program.global_block().create_parameter(
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddle/fluid/framework.py"", line 3572, in create_parameter
    initializer(param, self)
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddle/fluid/initializer.py"", line 365, in __call__
    out_var = _C_ops.gaussian_random(
OSError: (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at /paddle/paddle/phi/backends/gpu/gpu_context.cc:424)
  [operator < gaussian_random > error]

======================================================================
ERROR: test_trainer (__main__.TestGFL)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""test_architectures.py"", line 34, in test_trainer
    cfg = ppdet.core.workspace.load_config(self.cfg_file)
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddledet-2.4.0-py3.8.egg/ppdet/core/workspace.py"", line 114, in load_config
    cfg = _load_config_with_base(file_path)
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddledet-2.4.0-py3.8.egg/ppdet/core/workspace.py"", line 78, in _load_config_with_base
    with open(file_path) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'configs/gfl/gfl_r50_fpn_1x_coco.yml'

======================================================================
ERROR: test_trainer (__main__.TestMaskRCNN)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""test_architectures.py"", line 34, in test_trainer
    cfg = ppdet.core.workspace.load_config(self.cfg_file)
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddledet-2.4.0-py3.8.egg/ppdet/core/workspace.py"", line 114, in load_config
    cfg = _load_config_with_base(file_path)
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddledet-2.4.0-py3.8.egg/ppdet/core/workspace.py"", line 78, in _load_config_with_base
    with open(file_path) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.yml'

======================================================================
ERROR: test_trainer (__main__.TestPicoDet)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""test_architectures.py"", line 34, in test_trainer
    cfg = ppdet.core.workspace.load_config(self.cfg_file)
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddledet-2.4.0-py3.8.egg/ppdet/core/workspace.py"", line 114, in load_config
    cfg = _load_config_with_base(file_path)
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddledet-2.4.0-py3.8.egg/ppdet/core/workspace.py"", line 78, in _load_config_with_base
    with open(file_path) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'configs/picodet/picodet_s_320_coco_lcnet.yml'

======================================================================
ERROR: test_trainer (__main__.TestSSD)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""test_architectures.py"", line 34, in test_trainer
    cfg = ppdet.core.workspace.load_config(self.cfg_file)
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddledet-2.4.0-py3.8.egg/ppdet/core/workspace.py"", line 114, in load_config
    cfg = _load_config_with_base(file_path)
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddledet-2.4.0-py3.8.egg/ppdet/core/workspace.py"", line 78, in _load_config_with_base
    with open(file_path) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'configs/ssd/ssd_vgg16_300_240e_voc.yml'

======================================================================
ERROR: test_trainer (__main__.TestYolov3)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""test_architectures.py"", line 34, in test_trainer
    cfg = ppdet.core.workspace.load_config(self.cfg_file)
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddledet-2.4.0-py3.8.egg/ppdet/core/workspace.py"", line 114, in load_config
    cfg = _load_config_with_base(file_path)
  File ""/home/zhou/anaconda3/lib/python3.8/site-packages/paddledet-2.4.0-py3.8.egg/ppdet/core/workspace.py"", line 78, in _load_config_with_base
    with open(file_path) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'configs/yolov3/yolov3_darknet53_270e_coco.yml'

----------------------------------------------------------------------
Ran 7 tests in 40.179s
"
picoDet能不能导出动态batch的onnx文件,PaddlePaddle/PaddleDetection,2022-06-17 02:27:50,5,,6216,1274416065,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

picoDet能不能导出动态batch的onnx文件"
"solov2,  use_shared_memory=True时，报错",PaddlePaddle/PaddleDetection,2022-06-16 11:33:48,2,,6212,1273438162,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

1、代码
  self.dataloader = DataLoader(
      dataset=self.dataset,
      batch_sampler=self._batch_sampler,
      collate_fn=self._batch_transforms,
      num_workers=worker_num,
      return_list=return_list,
      use_shared_memory=True/False)  # 设置为True时，报错

2、详细报错文档
[use_shared_memory.log.tar.gz](https://github.com/PaddlePaddle/PaddleDetection/files/8918438/use_shared_memory.log.tar.gz)


### 复现环境 Environment

pp: 2.2.2
ppdet: 2.4
python: 3.7.6
cuda: 10.1
cudnn: 7

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
S2ANet  ext_op运行错误,PaddlePaddle/PaddleDetection,2022-06-16 09:12:16,2,,6209,1273281046,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

![image](https://user-images.githubusercontent.com/46549527/174035894-feae6745-5a32-4775-9df9-fcc17e23d928.png)

运行python3.7 setup.py install 出现上面的错误

环境:
paddledet                         2.2.0
paddlepaddle-gpu                  2.1.3
请问该如何解决"
pp-picodet 主体检测导出模型后 执行python deploy/python/infer.py 遇到问题 ValueError: cannot reshape array of size 1 into shape (4),PaddlePaddle/PaddleDetection,2022-06-16 08:57:35,2,,6208,1273264546,"### 问题确认 Search before asking

- [x] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

aistudio中使用pp-picodet训练了自己的数据，主体检测导出模型：
```python
python tools/export_model.py -c configs/picodet/picodet_lcnet_x2_5_640_mainbody_patrol_develop.yml --output_dir=./inference_model -o weights=output/picodet_lcnet_x2_5_640_mainbody_patrol_2.4/best_model.pdparams
```
导出后执行推理预测：
```python
python deploy/python/infer.py --model_dir=./inference_model/picodet_lcnet_x2_5_640_mainbody_patrol_develop --image_file=./coco/val/01.mp4#t=2.933333.jpg --device=GPU
```
结果出错：
```python
-----------  Running Arguments -----------
action_file: None
batch_size: 1
camera_id: -1
cpu_threads: 1
device: GPU
enable_mkldnn: False
enable_mkldnn_bfloat16: False
image_dir: None
image_file: ./coco/val/01.mp4#t=2.933333.jpg
model_dir: ./inference_model/picodet_lcnet_x2_5_640_mainbody_patrol_develop
output_dir: output
random_pad: False
reid_batch_size: 50
reid_model_dir: None
run_benchmark: False
run_mode: paddle
save_images: False
save_mot_txt_per_img: False
save_mot_txts: False
save_results: False
scaled: False
threshold: 0.5
tracker_config: None
trt_calib_mode: False
trt_max_shape: 1280
trt_min_shape: 1
trt_opt_shape: 640
use_dark: True
use_gpu: False
video_file: None
window_size: 50
------------------------------------------
-----------  Model Configuration -----------
Model Arch: PicoDet
Transform Order: 
--transform op: Resize
--transform op: NormalizeImage
--transform op: Permute
--transform op: PadStride
--------------------------------------------
Traceback (most recent call last):
  File ""deploy/python/infer.py"", line 918, in <module>
    main()
  File ""deploy/python/infer.py"", line 891, in main
    img_list, FLAGS.run_benchmark, repeats=100, save_file=save_file)
  File ""deploy/python/infer.py"", line 286, in predict_image
    result = self.postprocess(inputs, result)
  File ""deploy/python/infer.py"", line 550, in postprocess
    np_boxes, np_boxes_num = postprocessor(np_score_list, np_boxes_list)
  File ""/home/aistudio/work/PaddleDetection-develop/deploy/python/picodet_postprocess.py"", line 160, in __call__
    box_distance = np.sum(box_distance, axis=1).reshape((-1, 4))
ValueError: cannot reshape array of size 1 into shape (4)
```


### 复现环境 Environment

_No response_

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
"C++调用PPYOLOE的onnx模型，输出的类别ID和置信度正常，可是X1,Y1,X2,Y2的值不正常；",PaddlePaddle/PaddleDetection,2022-06-16 02:03:48,3,,6202,1272951664,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

C++调用PPYOLOE的onnx模型，输出的类别ID和置信度正常，可是X1,Y1,X2,Y2的值不正常（模型在python代码验证过没有问题）"
确认一下picodet在ARM Linux上的性能,PaddlePaddle/PaddleDetection,2022-06-15 08:37:12,0,deploy,6200,1271876904,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

RK3399， CPU，大核
Ubuntu 20.04
Paddle-Lite v2.11
测试416x416的输入，平均每个图片(原始尺寸640x480)的处理时间在400ms左右
想确认一下，这个性能是不是正常的。

如果性能偏低，请指导一下可能的调优方向。"
导出绑定NMS的模型，其NMS的参数如何调整？,PaddlePaddle/PaddleDetection,2022-06-15 07:33:29,4,deploy,6199,1271798560,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

1.导出绑定NMS的模型，但是不想用默认的NMS参数，请问如何设置？在.yml里面设置好像不能完全覆盖（picoDet）
2.导出带有NMS的模型，是否推理的时候性能会出现较大波动？我这边导出到paddleLIte，端上测试性能会有较大波动"
最后一层卷积层插入量化节点的问题,PaddlePaddle/PaddleDetection,2022-06-11 14:01:09,0,,6181,1268284478,
模型量化：模型fliter显示“Tensor data type 'int8' is not implemented.”,PaddlePaddle/PaddleDetection,2022-06-10 09:46:45,0,,6178,1267315539,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

![image](https://user-images.githubusercontent.com/87405313/173038978-2a3706e0-f22f-45ea-8e09-16c3c721da74.png)
模型现在这个Tensor data type 'int8' is not implemented是啥意思，是该卷积没有量化成功的意思吗？"
c++调用PPYOLOE的ONNX模型，无法正常输出BBOXS的值,PaddlePaddle/PaddleDetection,2022-06-10 08:30:45,3,,6177,1267232923,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

c++调用PPYOLO的ONNX模型，输出的结果class_id和置信度的结果输出正常，并且置信度的得分很高，但是X1,Y1,X2,Y2的值输出结果为inf"
自定义量化节点应该如何实现？包括节点的实现、原静态图的模型加载、保存以及组网？,PaddlePaddle/PaddleDetection,2022-06-10 03:13:25,0,,6174,1266954997,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

as titile"
请问能将prior_box设置成int8类型的输入吗？想要借此优化模型,PaddlePaddle/PaddleDetection,2022-06-10 01:33:09,3,,6172,1266879380,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

正在使用ssd_mobilenet_v1模型，想要修改prior_box的输入数据类型，使卷积全都变成8bit的卷积，实现卷积全量化

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
ubuntu下C++部署如何使用自己编译的opencv,PaddlePaddle/PaddleDetection,2022-06-09 07:03:59,3,,6165,1265688461,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

ubuntu下C++部署如何使用自己编译的opencv？，需要将build,sh文件中下载编译的opencv库代码注释 然后cmake文件也需要修改吗"
pphuman 可以压缩精简并转换成PaddleJS的格式么？,PaddlePaddle/PaddleDetection,2022-06-08 14:06:29,2,,6163,1264810715,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

我们打算在浏览器中运行模型推理，想问问pphuman可以压缩精简并转换成PaddleJS的格式么？或者压缩成lite格式，在树莓派这种类似的设备上运行？我们需要做视频实时推理"
训练过程中报错异常退出,PaddlePaddle/PaddleDetection,2022-06-08 11:18:36,3,status/close,6161,1264586340,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

使用命令 nohup /usr/local/bin/python3 tools/train.py -c configs/ppyolo/ppyolov2_r50vd_dcn_365e_coco.yml --eval >train0608.out 2>&1 &训练模型 
然后模型会异常退出

Traceback (most recent call last):
  File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 616, in _get_data
    data = self._data_queue.get(timeout=self._timeout)
  File ""/usr/local/python3.7.0/lib/python3.7/multiprocessing/queues.py"", line 105, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/python3.7.0/lib/python3.7/threading.py"", line 917, in _bootstrap_inner
    self.run()
  File ""/usr/local/python3.7.0/lib/python3.7/threading.py"", line 865, in run
    self._target(*self._args, **self._kwargs)
  File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 530, in _thread_loop
    batch = self._get_data()
  File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 632, in _get_data
    ""pids: {}"".format(len(failed_workers), pids))
RuntimeError: DataLoader 1 workers exit unexpectedly, pids: 461

Traceback (most recent call last):
  File ""tools/train.py"", line 177, in <module>
    main()
  File ""tools/train.py"", line 173, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 127, in run
    trainer.train(FLAGS.eval)
  File ""/paddle/PaddleDetection/ppdet/engine/trainer.py"", line 428, in train
    for step_id, data in enumerate(self.loader):
  File ""/paddle/PaddleDetection/ppdet/data/reader.py"", line 209, in __next__
    return next(self.loader)
  File ""/usr/local/python3.7.0/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 742, in __next__
    data = self._reader.read_next_var_list()
SystemError: (Fatal) Blocking queue is killed because the data reader raises an exception.
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] (at /paddle/paddle/fluid/operators/reader/blocking_queue.h:166)

我是使用的docker方式部署的paddleDetection，coco数据集"
python部署代码预处理进行标准化时可能会更改原有图片数据,PaddlePaddle/PaddleDetection,2022-06-07 08:55:14,4,,6148,1262959650,"https://github.com/PaddlePaddle/PaddleDetection/blob/405a953917a1ba8ca0559c143c44c27f4b22fc57/deploy/python/preprocess.py#L109-L140

如果`is_scale==False`，存储图像的numpy对象（`im`）在进行自减和自除之后会对传入的图像数据进行修改。
如果`is_scale==True`，由于`im = im / 255.0 `导致`im`变成一个新对象，则不会对传入的图像数据进行修改。

无论resize的初衷是改变原数据还是不改变原数据，由于`is_scale`导致函数行为不一致，我认为这是一个bug。"
PPtracking 运行fairmot模型报错cuda error 719,PaddlePaddle/PaddleDetection,2022-06-07 08:12:17,6,,6146,1262900427,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

GPU运行pptracking的fairmot模型报错CUDA Error (719),CPU版本运行无问题
尝试过多个cuda与cudnn版本，报错内容均一致

目前paddlepaddle版本为2.3.0，paddle detection版本为2.4，python为3.7.13
运行paddlepaddle的检测与paddledetection的检测均无问题
电脑配置为i712700h，rtx3060，16g rom

报错内容如下



'''
fps: 25, frame_count: 3020
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Error: ../paddle/phi/kernels/funcs/elementwise_functor.h:545 Assertion `b != 0` failed. InvalidArgumentError: Integer division by zero encountered in (floor) divide. Please check the input value.
Traceback (most recent call last):
  File ""deploy/pptracking/python/mot_jde_infer.py"", line 437, in <module>
    main()
  File ""deploy/pptracking/python/mot_jde_infer.py"", line 410, in main
    detector.predict_video(FLAGS.video_file, FLAGS.camera_id)
  File ""deploy/pptracking/python/mot_jde_infer.py"", line 328, in predict_video
    [frame], visual=False, seq_name=seq_name)
  File ""deploy/pptracking/python/mot_jde_infer.py"", line 235, in predict_image
    result = self.predict()
  File ""deploy/pptracking/python/mot_jde_infer.py"", line 174, in predict
    self.predictor.run()
OSError: In user code:

    File ""tools/export_model.py"", line 111, in <module>
      main()
    File ""tools/export_model.py"", line 107, in main
      run(FLAGS, cfg)
    File ""tools/export_model.py"", line 73, in run
      trainer.export(FLAGS.output_dir)
    File ""C:\Users\Misa\Project\PaddleDetection\ppdet\engine\trainer.py"", line 775, in export
      save_dir)
    File ""C:\Users\Misa\Project\PaddleDetection\ppdet\engine\trainer.py"", line 752, in _get_infer_cfg_and_input_spec
      input_spec, static_model.forward.main_program,
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 580, in main_program
      concrete_program = self.concrete_program
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 488, in concrete_program
      return self.concrete_program_specify_input_spec(input_spec=None)
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 528, in concrete_program_specify_input_spec
      *desired_input_spec, with_hook=with_hook)
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 436, in get_concrete_program
      concrete_program, partial_program_layer = self._program_cache[cache_key]
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 801, in __getitem__
      self._caches[item_id] = self._build_once(item)
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 790, in _build_once
      **cache_key.kwargs)
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\decorator.py"", line 232, in fun
      return caller(func, *(extras + args), **kw)
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\wrapped_decorator.py"", line 25, in __impl__
      return wrapped_func(*args, **kwargs)
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\base.py"", line 51, in __impl__
      return func(*args, **kwargs)
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 733, in from_func_spec
      outputs = static_func(*inputs)
    File ""C:\Users\Misa\AppData\Local\Temp\tmprixq875_.py"", line 101, in forward
      false_fn_5, (), (inputs, self), (out,))
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\convert_operators.py"", line 211, in convert_ifelse
      out = _run_py_ifelse(pred, true_fn, false_fn, true_args, false_args)
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\convert_operators.py"", line 257, in _run_py_ifelse
      return true_fn(*true_args) if pred else false_fn(*false_args)
    File ""C:\Users\Misa\AppData\Local\Temp\tmprixq875_.py"", line 84, in false_fn_5
      for_loop_body_0, [inputs_list, __for_loop_var_index_0])
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\convert_operators.py"", line 45, in convert_while_loop
      loop_vars = _run_py_while(cond, body, loop_vars)
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\convert_operators.py"", line 59, in _run_py_while
      loop_vars = body(*loop_vars)
    File ""C:\Users\Misa\AppData\Local\Temp\tmprixq875_.py"", line 79, in for_loop_body_0
      dy2static.convert_call(self.get_pred)())
    File ""C:\Users\Misa\Project\PaddleDetection\ppdet\modeling\architectures\fairmot.py"", line 95, in get_pred
      output = self._forward()
    File ""C:\Users\Misa\Project\PaddleDetection\ppdet\modeling\architectures\fairmot.py"", line 75, in _forward
      det_outs = self.detector(self.inputs)
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
      return self._dygraph_call_func(*inputs, **kwargs)
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
      outputs = self.forward(*inputs, **kwargs)
    File ""C:\Users\Misa\AppData\Local\Temp\tmpujiqlz4k.py"", line 101, in forward
      false_fn_5, (), (inputs, self), (out,))
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\convert_operators.py"", line 211, in convert_ifelse
      out = _run_py_ifelse(pred, true_fn, false_fn, true_args, false_args)
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\convert_operators.py"", line 257, in _run_py_ifelse
      return true_fn(*true_args) if pred else false_fn(*false_args)
    File ""C:\Users\Misa\AppData\Local\Temp\tmpujiqlz4k.py"", line 84, in false_fn_5
      for_loop_body_0, [inputs_list, __for_loop_var_index_0])
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\convert_operators.py"", line 45, in convert_while_loop
      loop_vars = _run_py_while(cond, body, loop_vars)
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\convert_operators.py"", line 59, in _run_py_while
      loop_vars = body(*loop_vars)
    File ""C:\Users\Misa\AppData\Local\Temp\tmpujiqlz4k.py"", line 79, in for_loop_body_0
      dy2static.convert_call(self.get_pred)())
    File ""C:\Users\Misa\AppData\Local\Temp\tmpg3hfxf38.py"", line 31, in get_pred
      output))
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\convert_operators.py"", line 211, in convert_ifelse
      out = _run_py_ifelse(pred, true_fn, false_fn, true_args, false_args)
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\convert_operators.py"", line 257, in _run_py_ifelse
      return true_fn(*true_args) if pred else false_fn(*false_args)
    File ""C:\Users\Misa\Project\PaddleDetection\ppdet\modeling\architectures\centernet.py"", line 87, in get_pred
      scale_factor=self.inputs['scale_factor'])
    File ""C:\Users\Misa\Project\PaddleDetection\ppdet\modeling\post_process.py"", line 507, in __call__
      scores, inds, topk_clses, ys, xs = self._topk(heat)
    File ""C:\Users\Misa\Project\PaddleDetection\ppdet\modeling\layers.py"", line 837, in _topk
      topk_xs = topk_inds % width
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\layers\math_op_patch.py"", line 347, in __impl__
      attrs={'axis': axis})
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\framework.py"", line 3621, in append_op
      attrs=kwargs.get(""attrs"", None))
    File ""C:\Users\Misa\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\framework.py"", line 2635, in __init__
      for frame in traceback.extract_stack():

    ExternalError: CUDA error(719), unspecified launch failure.
      [Hint: 'cudaErrorLaunchFailure'. An exception occurred on the device while executing a kernel. Common causes include dereferencing an invalid device pointerand accessing out of bounds shared memory. Less common cases can be system specific - more information about these cases canbe found in the system specific user guide. This leaves the process in an inconsistent state and any further CUDA work willreturn the same error. To continue using CUDA, the process must be terminated and relaunched.] (at ..\paddle\phi\backends\gpu\gpu_context.cc:624)
      [operator < elementwise_mod > error]
'''

### 复现环境 Environment

-paddlepaddle 2.3.0
-paddle detection release/2.4
-python 3.7.13
-cuda 11.2.2
-cudnn 8.2.1

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
Linux C++部署 最后一步报错，无法生成main文件,PaddlePaddle/PaddleDetection,2022-06-05 06:31:05,6,,6130,1260974145,"### 问题确认 Search before asking

- [x] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question


![1](https://user-images.githubusercontent.com/101175483/172038357-8a850c70-8c4a-4892-bb79-8cd454f226dc.png)
在C++部署，运行build.sh文件 最后100%时报错，build.sh文件中配置路径已经修改正确，环境要求也配置正确，这个问题不知道怎么解决
"
基于深度学习的目标模板跟踪（咨询学习方法和途径问题）,PaddlePaddle/PaddleDetection,2022-06-05 01:12:48,2,,6128,1260930859,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

paddledetection 中提供了基于数据驱动的目标检测解决方案，但类似的传统跟踪算法应用场景，如KCF，给定一个目标模板和一张图，得到目标在这张图的位置；类似的这类应用，运用深度学习的解决方案，paddledetection中似乎文档和例程不是很多，这篇是：基于孪生网络的目标跟踪网络SiamFC；https://aistudio.baidu.com/aistudio/projectdetail/3423489?channelType=0&channel=0，我想咨询：这些应用，如果在paddledetection中开发，到端侧应用，支持不？有没有好的学习思路？谢谢"
PPYOLOE loss_cl非常大,PaddlePaddle/PaddleDetection,2022-06-02 04:13:49,6,,6117,1257661436,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

[06/02 11:49:40] ppdet.utils.checkpoint INFO: Finish loading model weights: /root/.cache/paddle/weights/CSPResNetb_x_pretrained.pdparams
[06/02 11:49:42] ppdet.engine INFO: Epoch: [0] [  0/327] learning_rate: 0.000000 loss: 3.975326 loss_cls: 0.105371 loss_iou: 0.981339 loss_dfl: 2.833213 loss_l1: 7.127966 eta: 2 days, 5:15:06 batch_cost: 1.9542 data_cost: 0.0003 ips: 8.1875 images/s
[06/02 11:53:04] ppdet.engine INFO: Epoch: [0] [300/327] learning_rate: 0.003303 loss: 37.457855 loss_cls: 35.277142 loss_iou: 0.620637 loss_dfl: 1.260373 loss_l1: 0.674086 eta: 16:42:04 batch_cost: 0.6103 data_cost: 0.0001 ips: 26.2162 images/s
[06/02 11:53:27] ppdet.engine INFO: Epoch: [1] [  0/327] learning_rate: 0.003600 loss: 44.684166 loss_cls: 42.488293 loss_iou: 0.599753 loss_dfl: 1.204882 loss_l1: 0.606429 eta: 16:59:13 batch_cost: 0.6244 data_cost: 0.0123 ips: 25.6245 images/s
[06/02 11:56:43] ppdet.engine INFO: Epoch: [1] [300/327] learning_rate: 0.006903 loss: 57.273800 loss_cls: 55.637402 loss_iou: 0.469898 loss_dfl: 0.916840 loss_l1: 0.422670 eta: 16:30:10 batch_cost: 0.5921 data_cost: 0.0004 ips: 27.0246 images/s
[06/02 11:57:07] ppdet.engine INFO: Epoch: [2] [  0/327] learning_rate: 0.007200 loss: 57.396709 loss_cls: 55.745827 loss_iou: 0.467921 loss_dfl: 0.912606 loss_l1: 0.418009 eta: 16:41:23 batch_cost: 0.6147 data_cost: 0.0160 ips: 26.0311 images/s
[06/02 12:00:27] ppdet.engine INFO: Epoch: [2] [300/327] learning_rate: 0.010503 loss: 57.955795 loss_cls: 56.367802 loss_iou: 0.461489 loss_dfl: 0.912825 loss_l1: 0.432382 eta: 16:33:04 batch_cost: 0.6063 data_cost: 0.0001 ips: 26.3896 images/s
[06/02 12:00:47] ppdet.engine INFO: Epoch: [3] [  0/327] learning_rate: 0.010800 loss: 57.499207 loss_cls: 55.874950 loss_iou: 0.465653 loss_dfl: 0.918409 loss_l1: 0.439063 eta: 16:31:49 batch_cost: 0.6084 data_cost: 0.0068 ips: 26.2982 images/s
[06/02 12:04:10] ppdet.engine INFO: Epoch: [3] [300/327] learning_rate: 0.014103 loss: 53.663479 loss_cls: 51.858963 loss_iou: 0.507553 loss_dfl: 1.005520 loss_l1: 0.508582 eta: 16:30:19 batch_cost: 0.6169 data_cost: 0.0043 ips: 25.9350 images/s
[06/02 12:04:34] ppdet.engine INFO: Epoch: [4] [  0/327] learning_rate: 0.014400 loss: 53.663479 loss_cls: 51.858963 loss_iou: 0.509146 loss_dfl: 1.003465 loss_l1: 0.499418 eta: 16:34:53 batch_cost: 0.6175 data_cost: 0.0180 ips: 25.9
loss_cls: 已经达到50多了  昨晚我已经跑了80个批次 输出的map是0

### 复现环境 Environment

我用的版本是PPYOLOE的X版本 其中 S、L版本我都用过了 无这类情况发生
GPU:A5000 1个
PaddlePaddle  2.2.0
Python  3.8
Cuda  11.2
CPU  15核 AMD EPYC 7543 32-Core Processor


### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
ppyoloe没问题吗？,PaddlePaddle/PaddleDetection,2022-05-31 11:27:22,10,,6104,1253726007,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

### 错误信息
ValueError: (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 2100, 2] and the shape of Y = [8400, 2]. Received [2100] in X is not equal to [8400] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || x_dims_array[i] <= 1 || y_dims_array[i] <= 1 == true, but received x_dims_array[i] == y_dims_array[i] || x_dims_array[i] <= 1 || y_dims_array[i] <= 1:0 != true:1.] (at /paddle/paddle/phi/kernels/funcs/common_shape.h:84)
  [operator < elementwise_add > error]
### 前提
1.自己的数据集在其他配置文件下均可以训练，比如ppyolo,faster rcnn等
2.没有修改ppyoloe的配置文件，除了epoch改小成100"
x2coco.py可以支持随机划分训练集与验证集吗？而不是按着默认的图像顺序？,PaddlePaddle/PaddleDetection,2022-05-31 02:07:10,3,,6098,1253249182,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

x2coco.py可以支持随机划分训练集与验证集吗？而不是按着默认的图像顺序？

### 是否愿意提交PR Are you willing to submit a PR?

- [x] Yes I'd like to help by submitting a PR!"
Any configuraton of picoDet model about detecting small objects ？,PaddlePaddle/PaddleDetection,2022-05-31 01:18:29,5,,6097,1253228256,"hi，
     1 请问下，是否有针对小目标检测或者大目标和小目标混杂的场景，picoDet模型检测效果比较好的cfg 配置，能否发下链接呢？  另外是否有比价好的相应preweights呢？
     2 针对picoDet  针对小物体场景如何配置cfg, 才能增大对于小物体的检测能力？ 比如减少box的尺寸或者feature box 数量？

![image](https://user-images.githubusercontent.com/8407513/171074649-38fffea6-26e0-4bdc-9047-9f9d3cf6d927.png)
    3 上图中主要参数的意义，可否详细解释下，如果大物体和小物体混杂的情况下，如何调整哪些参数？多谢
  reg_max: 7
  cell_offset: 0.5
  grid_cell_scale: 5.0
    4 这些配置  RandomFlip Permute 等都是默认启动为True的吗？
![image](https://user-images.githubusercontent.com/8407513/171081396-e0cb55b8-fd00-4cbd-8336-5ef9e9325ff7.png)
   
BR "
PPYOLOE训练问题,PaddlePaddle/PaddleDetection,2022-05-30 10:21:31,18,,6093,1252521437,"使用PPYOLOE训练总会报错：ValueError: Target XXXX is out of upper bound.
但是标签用其他模型训练都能正常收敛而且mAP都很高。用的也是官方提供的x2coco.py做的标签转换。请问这个问题有人遇到过吗，该怎么解决
![image](https://user-images.githubusercontent.com/96117546/171074186-52bab63e-67e0-4f8d-b2b9-97a7c2f269f1.png)
"
python wheel预测接口,PaddlePaddle/PaddleDetection,2022-05-29 04:02:51,2,feature request,6087,1251780954,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有类似需求。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar feature requests.


### 需求描述 Feature Description

调用时有python wheel能够快速调用。
import ppdet
predictor = ppdet.deploy.Predictor('./inference_model')
result = predictor.predict(img_file='test.jpg')
像这种paadlex的调用方式。

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
"PicoDet训练报错：(External) CUDA error(700), an illegal memory access was encountered.",PaddlePaddle/PaddleDetection,2022-05-28 13:14:35,8,windows,6085,1251629418,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

[05/28 20:59:08] ppdet.utils.checkpoint INFO: Finish loading model weights: C:\Users\vision/.cache/paddle/weights\PPLCNet_x0_75_pretrained.pdparams
Traceback (most recent call last):
  File ""tools/train.py"", line 177, in <module>
    main()
  File ""tools/train.py"", line 173, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 127, in run
    trainer.train(FLAGS.eval)
  File ""D:\vision\PaddleDetection\ppdet\engine\trainer.py"", line 442, in train
    outputs = model(data)
  File ""D:\vision\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""D:\vision\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""D:\vision\PaddleDetection\ppdet\modeling\architectures\meta_arch.py"", line 59, in forward
    out = self.get_loss()
  File ""D:\vision\PaddleDetection\ppdet\modeling\architectures\picodet.py"", line 79, in get_loss
    loss_gfl = self.head.get_loss(head_outs, self.inputs)
  File ""D:\vision\PaddleDetection\ppdet\modeling\heads\pico_head.py"", line 723, in get_loss
    avg_factor=4.0)
  File ""D:\vision\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 930, in __call__
    return self._dygraph_call_func(*inputs, **kwargs)
  File ""D:\vision\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\layers.py"", line 915, in _dygraph_call_func
    outputs = self.forward(*inputs, **kwargs)
  File ""D:\vision\PaddleDetection\ppdet\modeling\losses\gfocal_loss.py"", line 199, in forward
    loss = self.loss_weight * distribution_focal_loss(pred, target)
  File ""D:\vision\PaddleDetection\ppdet\modeling\losses\gfocal_loss.py"", line 100, in distribution_focal_loss
    loss = F.cross_entropy(pred, dis_left, reduction='none') * weight_left \
  File ""D:\vision\anaconda3\envs\paddle\lib\site-packages\paddle\nn\functional\loss.py"", line 1714, in cross_entropy
    if label_min < 0:
  File ""D:\vision\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\varbase_patch_methods.py"", line 668, in __bool__
    return self.__nonzero__()
  File ""D:\vision\anaconda3\envs\paddle\lib\site-packages\paddle\fluid\dygraph\varbase_patch_methods.py"", line 665, in __nonzero__
    return bool(np.all(tensor.__array__() > 0))
OSError: (External) CUDA error(700), an illegal memory access was encountered.
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ..\paddle\phi\backends\gpu\cuda\cuda_info.cc:258)

### 复现环境 Environment

- PaddlePadle 2.3.0 + cudnn11.2（Conda方式安装）
- PaddleDetection 2.4/release
- Windows10 + RTX 3070 Ti 8G

按[30分钟快速上手PaddleDetection
](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.4/docs/tutorials/GETTING_STARTED_cn.md)可以正常训练yolov3_mobilenet_v1_roadsign

修改 picodet-s-416 lcnet 配置为roadsign_voc数据集训练则报错

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
剪枝量化联合策略的模型导出问题,PaddlePaddle/PaddleDetection,2022-05-28 03:36:49,2,model compression,6080,1251448564,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

我在ppdet/slim/_init_.py下构建了一个剪枝和量化的联合策略（先剪枝后量化），可以正常train，eval，但是不能导出，我看了trainer下的导出代码，剪枝或者量化的导出api不能同时实现，我现在想实现能正常导出经过剪枝量化后的模型，该怎么修改相关代码，谢谢！"
模型库是否有yolov4与yolov5的模型复现？,PaddlePaddle/PaddleDetection,2022-05-28 03:17:23,2,,6079,1251439555,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

请问目前是没有yolo4与yolo5的模型复现吗"
target_size问题,PaddlePaddle/PaddleDetection,2022-05-27 07:55:35,3,,6077,1250459412,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

你好，试用了一下paddledetection，个人感觉比较方便，从训练到部署，对于训练参数有个疑问。在训练中有个target_size参数设定，比如我们的训练尺度是320x320，在target_size中会设定基于这个数值加减32倍数的尺度，比如352这个数值，实际处理方式，是会将320的图像放置到352尺度的图像中，再对352的图像resize到320吗，同样，对于288这个尺度的图像，是将320x320的图像，从中截取出288x288大小的图像吗

补充说明：target_size参数，来自BatchRandomResize"
剪裁模型导出错误,PaddlePaddle/PaddleDetection,2022-05-24 12:55:59,0,,6052,1246512208,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

使用如下命令导出剪裁后的模型出错：

!python tools/export_model.py -c configs/ppyoloe/ppyoloe_crn_l_300e_coco.yml --slim_config configs/slim/prune/ppyolo_r50vd_prune_fpgm.yml -o weights=output/ppyolo_r50vd_prune_fpgm/1.pdparams

报错如下：

/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/tensor/creation.py:130: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if data.dtype == np.object:
[05-24 20:46:14 MainThread @logger.py:242] Argv: tools/export_model.py -c configs/ppyoloe/ppyoloe_crn_l_300e_coco.yml --slim_config configs/slim/prune/ppyolo_r50vd_prune_fpgm.yml -o weights=output/ppyolo_r50vd_prune_fpgm/1.pdparams
[05-24 20:46:14 MainThread @utils.py:79] WRN paddlepaddle version: 2.2.2. The dynamic graph version of PARL is under development, not fully tested and supported
<class 'ppdet.modeling.architectures.yolo.YOLOv3'>
[05/24 20:46:19] ppdet.slim.prune INFO: FLOPs before pruning: 55.59761528GFLOPs
<class 'ppdet.modeling.architectures.yolo.YOLOv3'>
2022-05-24 20:46:37,917-WARNING: Leaves ['t_999', 't_995', 't_993', 't_988', 't_57', 't_48', 't_44', 't_450', 't_182', 't_511', 't_25', 't_20', 't_19', 't_695', 't_15', 't_64', 't_8', 't_1', 'fetch_0', 't_420', 't_769', 't_37', 't_374', 't_825', 't_592', 't_505', 't_714', 't_928', 't_689', 't_346', 't_250', 't_281', 't_980', 't_861', 't_21', 't_444', 't_106', 't_792', 't_965', 't_943', 't_395', 't_246', 't_389', 't_306', 't_437', 'fetch_1', 't_833', 't_461', 't_82', 't_125', 't_673', 't_694', 't_495', 't_641', 't_193', 't_289', 't_751', 't_757', 't_376', 't_745', 't_633', 't_747', 't_632', 't_565', 't_132', 't_627', 't_50', 't_221', 't_187', 't_200', 't_336', 't_524', 't_26', 't_7', 't_819', 't_351', 't_739', 't_14', 't_219', 't_838', 't_226', 't_806', 't_942', 't_257', 't_986', 't_307', 't_654', 't_831', 't_45', 't_151', 't_62', 't_874', 't_621', 't_850', 't_560', 't_9', 't_672', 't_63', 't_483', 't_76', 't_956', 't_457', 't_49', 't_597', 't_135', 't_523', 't_3', 't_58', 't_771', 't_77', 't_220', 't_2', 't_276', 't_192', 't_820', 't_904', 't_677', 't_856', 't_867', 't_13', 't_137', 't_356', 't_43', 't_418', 't_701', 't_626', 't_647', 't_512', 't_32', 't_807', 't_401', 't_868', 't_425', 't_295', 't_1011', 't_31', 't_231', 't_136', 't_162', 't_646', 't_653', 't_206', 't_39', 't_667', 't_781', 't_941', 't_327', 't_365', 't_183', 't_288', 't_545', 't_258', 't_232', 't_602', 't_713', 't_676', 't_56', 't_628', 't_325', 't_487', 't_332', 't_67', 't_83', 't_68', 't_69', 't_75', 't_86', 't_88', 't_269', 't_499', 't_100', 't_863', 't_102', 't_108', 't_252', 't_112', 't_114', 't_118', 't_317', 't_855', 't_119', 't_120', 't_124', 't_126', 't_149', 't_130', 't_131', 't_143', 't_244', 't_836', 't_264', 't_144', 't_535', 't_150', 't_154', 't_155', 't_661', 't_881', 't_163', 't_164', 't_456', 't_168', 't_169', 't_170', 't_173', 't_174', 't_175', 't_33', 't_188', 't_331', 't_189', 't_337', 't_752', 't_194', 't_854', 't_201', 't_892', 't_207', 't_208', 't_481', 't_211', 't_212', 't_703', 't_225', 't_227', 't_113', 't_590', 't_519', 't_720', 't_412', 't_87', 't_685', 't_782', 't_38', 't_245', 't_251', 't_256', 't_262', 't_263', 't_396', 't_268', 't_564', 't_274', 't_355', 't_388', 't_275', 't_280', 't_287', 't_27', 't_708', 't_293', 't_270', 't_298', 't_299', 't_300', 't_308', 't_312', 't_313', 't_345', 't_979', 't_319', 't_712', 't_740', 't_333', 't_344', 't_352', 't_357', 't_101', 't_364', 't_181', 't_338', 't_369', 't_370', 't_371', 't_375', 't_458', 't_919', 't_390', 't_614', 't_81', 't_394', 't_522', 't_709', 't_402', 't_407', 't_408', 't_813', 't_615', 't_400', 't_413', 't_414', 't_891', 't_419', 't_423', 't_424', 't_431', 't_432', 't_433', 't_574', 't_438', 't_439', 't_442', 't_443', 't_451', 't_452', 't_462', 't_463', 't_476', 't_477', 't_482', 't_488', 't_489', 't_494', 't_659', 't_156', 't_500', 't_501', 't_504', 't_506', 't_475', 't_905', 't_513', 't_517', 't_758', 't_518', 't_533', 't_534', 't_539', 't_540', 't_541', 't_546', 't_547', 't_550', 't_552', 't_558', 't_559', 't_318', 't_566', 't_678', 't_735', 't_794', 't_572', 't_573', 't_579', 't_580', 't_584', 't_585', 't_586', 't_869', 't_591', 't_595', 't_596', 't_958', 't_603', 't_604', 't_608', 't_326', 't_609', 't_610', 't_756', 't_620', 't_832', 't_578', 't_213', 't_684', 't_631', 't_702', 't_639', 't_640', 't_107', 't_645', 't_776', 't_655', 't_660', 't_665', 't_666', 't_671', 't_350', 't_691', 't_722', 't_802', 't_493', 't_696', 't_622', 't_845', 't_707', 't_294', 't_721', 't_726', 't_727', 't_728', 't_279', 't_733', 't_734', 't_363', 't_314', 't_741', 't_746', 't_763', 't_753', 't_764', 't_765', 't_406', 't_690', 't_770', 't_774', 't_775', 't_783', 't_787', 't_788', 't_827', 't_789', 't_793', 't_800', 't_801', 't_808', 't_814', 't_815', 't_821', 't_826', 't_844', 't_202', 't_230', 't_837', 't_843', 't_849', 't_851', 't_862', 't_872', 't_873', 't_551', 't_880', 't_882', 't_906', 't_145', 't_912', 't_683', 't_914', 't_921', 't_929', 't_930', 't_613', 't_1013', 't_893', 't_949', 't_951', 't_966', 't_967', 't_978'] will be skipped when parsing graph.
2022-05-24 20:48:19,939-WARNING: ('Unsupported operator named reshape2',)
2022-05-24 20:48:19,940-WARNING: ('Unsupported operator named concat',)
2022-05-24 20:48:19,940-INFO: Found 63 collections.
[05/24 20:48:20] ppdet.slim.prune INFO: pruned params: ['conv2d_56.w_0', 'conv2d_57.w_0', 'conv2d_58.w_0', 'conv2d_59.w_0', 'conv2d_60.w_0', 'conv2d_61.w_0', 'conv2d_63.w_0', 'conv2d_64.w_0', 'conv2d_65.w_0', 'conv2d_66.w_0', 'conv2d_67.w_0', 'conv2d_68.w_0', 'conv2d_70.w_0', 'conv2d_71.w_0', 'conv2d_72.w_0', 'conv2d_73.w_0', 'conv2d_74.w_0', 'conv2d_75.w_0']
2022-05-24 20:48:20,141-INFO: Pruning variable [conv2d_58.w_0] and its relatives ['conv2d_58.w_0', 'batch_norm2d_56.w_0', 'batch_norm2d_56.b_0', 'batch_norm2d_56.w_1', 'batch_norm2d_56.w_2', 'conv2d_59.w_0', 'conv2d_60.w_0']
2022-05-24 20:48:20,249-INFO: Pruning variable [conv2d_63.w_0] and its relatives ['conv2d_63.w_0', 'batch_norm2d_60.w_0', 'batch_norm2d_60.b_0', 'batch_norm2d_60.w_1', 'batch_norm2d_60.w_2', 'conv2d_64.w_0', 'conv2d_65.w_0']
2022-05-24 20:48:31,705-INFO: Pruning variable [conv2d_66.w_0] and its relatives ['conv2d_66.w_0', 'batch_norm2d_63.w_0', 'batch_norm2d_63.b_0', 'batch_norm2d_63.w_1', 'batch_norm2d_63.w_2', 'conv2d_67.w_0', 'conv2d_68.w_0']
2022-05-24 20:48:32,989-INFO: Pruning variable [conv2d_72.w_0] and its relatives ['conv2d_72.w_0', 'batch_norm2d_69.w_0', 'batch_norm2d_69.b_0', 'batch_norm2d_69.w_1', 'batch_norm2d_69.w_2', 'conv2d_73.w_0', 'conv2d_74.w_0']
<class 'ppdet.modeling.architectures.yolo.YOLOv3'>
[05/24 20:48:37] ppdet.slim.prune INFO: FLOPs after pruning: 51.69669368GFLOPs; pruned ratio: 0.07016346978830343
[05/24 20:48:38] ppdet.utils.checkpoint INFO: Finish loading model weights: output/ppyolo_r50vd_prune_fpgm/1.pdparams
Traceback (most recent call last):
  File ""tools/export_model.py"", line 111, in <module>
    main()
  File ""tools/export_model.py"", line 107, in main
    run(FLAGS, cfg)
  File ""tools/export_model.py"", line 73, in run
    trainer.export(FLAGS.output_dir)
  File ""/home/aistudio/work/PaddleDetection/ppdet/engine/trainer.py"", line 771, in export
    save_dir)
  File ""/home/aistudio/work/PaddleDetection/ppdet/engine/trainer.py"", line 706, in _get_infer_cfg_and_input_spec
    layer.convert_to_deploy()
  File ""/home/aistudio/work/PaddleDetection/ppdet/modeling/backbones/cspresnet.py"", line 97, in convert_to_deploy
    self.conv.weight.set_value(kernel)
  File ""<decorator-gen-127>"", line 2, in set_value
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/wrapped_decorator.py"", line 25, in __impl__
    return wrapped_func(*args, **kwargs)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py"", line 229, in __impl__
    return func(*args, **kwargs)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/varbase_patch_methods.py"", line 170, in set_value
    self.name, self_tensor_np.shape, value_np.shape)
AssertionError: Variable Shape not match, Variable [ conv2d_175.w_0 ] need tensor with shape (192, 192, 3, 3) but load set tensor with shape (192, 48, 3, 3)
"
paddle.static.default_main_program() return block 0,PaddlePaddle/PaddleDetection,2022-05-24 12:34:57,0,,6051,1246483119,"### 问题确认 Search before asking

- [x] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

两个问题，我用的是paddlepaddle2.0
1、paddle.static.default_main_program() return block 0 按文档上说，应该当模型加载的时候就从主程序默认提取但返回是0
2、我主要是用于模型蒸馏，想通过这里得到模型特征图的矩阵，我看到蒸馏例子yolov3的输出model.yolo_head.pred_cls.distill_pairs可以得到矩阵，我的蒸馏的模型是ppyoloe的，但是我不知道如何得到pred_reg的矩阵，输出model.yolo_head.pred_reg.distill_pairs返回没有distill_pairs该属性，model.yolo_head.pred_reg返回如下图
    (0): Conv2D(384, 68, kernel_size=[3, 3], padding=1, data_format=NCHW)
    (1): Conv2D(192, 68, kernel_size=[3, 3], padding=1, data_format=NCHW)
    (2): Conv2D(96, 68, kernel_size=[3, 3], padding=1, data_format=NCHW)
我应该怎么做"
ppyoloe剪裁,PaddlePaddle/PaddleDetection,2022-05-23 08:03:16,2,,6039,1244740600,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

想对ppyoloe模型进行剪裁，但是PaddleDetection里面没有相应的配置文件，请问可以提供ppyoloe的剪裁配置吗？如果要自己配置的话，我应该进行哪些操作流程呢"
tinypose and lite hrne QAT 训练失败,PaddlePaddle/PaddleDetection,2022-05-23 04:53:08,7,,6037,1244578948,"

### bug描述 Describe the Bug

1 参考https://github.com/PaddlePaddle/Paddle/issues/42341 这个picodet的在线训练量化，完全搭建不出来一套tinypose或者litehrnet的qat在线训练版本啊，什么时候可以发布一套类似picodet的 tinypose的正式在线量化版本，多谢！
可以看到下面例子完全不能作为tinypos或者litehrnet的参考，必须paddle自己来做，这样失去了相当大的paddleslim这个框架研发的初衷
![image](https://user-images.githubusercontent.com/8407513/169747280-415dacf5-0ebf-4012-99b9-76f99527293e.png)


2 举个例子，pretrain_weights: https://paddledet.bj.bcebos.com/models/picodet_s_416_coco_lcnet.pdparams 
这个对于litehrnet 我的pretrain_weights 从哪里拿呢？不用也行？
![Uploading image.png…]()

3  #####data
TrainDataset:
  !KeypointTopDownCocoDataset
    image_dir: """"
    anno_path: aic_coco_train_cocoformat.json
    dataset_dir: dataset

这里的 dataset 数据集是哪个，具体子目录应该是啥样？aic_coco_train_cocoformat.json这个从哪里下载啊？

BR
"
ModuleNotFoundError: No module named 'pycocotools',PaddlePaddle/PaddleDetection,2022-05-22 15:03:49,4,,6035,1244280823,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

aistudio环境有,但运行就是显示这个错误"
 我在使用maskrcnn训练五分类任务结束后 在预测的时候 发现其中有个分类效果很差 这个分类数据量还是整个五类中最高的 请问有可能是什么原因造成的？,PaddlePaddle/PaddleDetection,2022-05-22 09:25:35,3,,6029,1244199366,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

 我在使用maskrcnn训练五分类任务结束后 在预测的时候 发现其中有个分类效果很差 这个分类数据量还是整个五类中最高的 请问有可能是什么原因造成的？"
pp-tracking跨镜头检测中，在哪里修改可以只显示检测类别为某一类或几类（bus和car.）,PaddlePaddle/PaddleDetection,2022-05-21 14:00:57,2,,6028,1243984244,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

pp-tracking跨镜头检测中，在哪里修改可以只显示检测类别为某一类或几类（bus和car.）"
使用PaddleDetection官方QAT配置对yolov3_mobilenet_v1_qat进行量化，量化前后无明显优化效果,PaddlePaddle/PaddleDetection,2022-05-21 08:37:05,0,,6026,1243917421,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

我使用yolov3_mobilenet_v1_qat这个配置进行QAT量化，并将该模型与无量化的yolov3_mobilenet_v1_270e_voc模型进行对比，发现量化后模型没有体现优化效果。

对比一：运行deploy/python/benchmark.sh和deploy/python/benchmark_quant.sh，分别用两模型在850张图片上进行推理。运行方式为：
bash deploy/benchmark/benchmark.sh ./inference_model/yolov3_mobilenet_v1_270e_voc_origin model  
bash deploy/benchmark/benchmark_quant.sh ./inference_model/yolov3_mobilenet_v1_270e_qat/yolov3_mobilenet_v1_qat model

得到的结果如下：
QAT版本
```
2022-05-19 17:54:27,443 - benchmark_utils - INFO - Paddle Inference benchmark log will be saved to /home/ubuntu/lxd-storage/xzy/PaddleCV/PaddleDetection/deploy/python/../../output/yolov3_mobilenet_v1_qat.log
2022-05-19 17:54:27,444 - benchmark_utils - INFO -
Fluid
2022-05-19 17:54:27,444 - benchmark_utils - INFO - ---------------------- Paddle info ----------------------
2022-05-19 17:54:27,444 - benchmark_utils - INFO - [Det] paddle_version: 2.3.0
2022-05-19 17:54:27,444 - benchmark_utils - INFO - [Det] paddle_commit: 590b4dbcdd989324089ce43c22ef151c746c92a3
2022-05-19 17:54:27,444 - benchmark_utils - INFO - [Det] paddle_branch: HEAD
2022-05-19 17:54:27,445 - benchmark_utils - INFO - [Det] log_api_version: 1.0.3
2022-05-19 17:54:27,445 - benchmark_utils - INFO - ----------------------- Conf info -----------------------
2022-05-19 17:54:27,445 - benchmark_utils - INFO - [Det] runtime_device: gpu
2022-05-19 17:54:27,445 - benchmark_utils - INFO - [Det] ir_optim: True
2022-05-19 17:54:27,445 - benchmark_utils - INFO - [Det] enable_memory_optim: True
2022-05-19 17:54:27,445 - benchmark_utils - INFO - [Det] enable_tensorrt: False
2022-05-19 17:54:27,445 - benchmark_utils - INFO - [Det] enable_mkldnn: False
2022-05-19 17:54:27,445 - benchmark_utils - INFO - [Det] cpu_math_library_num_threads: 1
2022-05-19 17:54:27,446 - benchmark_utils - INFO - ----------------------- Model info ----------------------
2022-05-19 17:54:27,446 - benchmark_utils - INFO - [Det] model_name: yolov3_mobilenet_v1_qat
2022-05-19 17:54:27,446 - benchmark_utils - INFO - [Det] precision: fluid
2022-05-19 17:54:27,446 - benchmark_utils - INFO - ----------------------- Data info -----------------------
2022-05-19 17:54:27,446 - benchmark_utils - INFO - [Det] batch_size: 1
2022-05-19 17:54:27,446 - benchmark_utils - INFO - [Det] input_shape: dynamic_shape
2022-05-19 17:54:27,446 - benchmark_utils - INFO - [Det] data_num: 850
2022-05-19 17:54:27,446 - benchmark_utils - INFO - ----------------------- Perf info -----------------------
2022-05-19 17:54:27,446 - benchmark_utils - INFO - [Det] cpu_rss(MB): 4161, cpu_vms: 0, cpu_shared_mb: 0, cpu_dirty_mb: 0, cpu_util: 0%
2022-05-19 17:54:27,447 - benchmark_utils - INFO - [Det] gpu_rss(MB): 10, gpu_util: 37.32%, gpu_mem_util: 0%
2022-05-19 17:54:27,447 - benchmark_utils - INFO - [Det] total time spent(s): 43.5324
2022-05-19 17:54:27,447 - benchmark_utils - INFO - [Det] preprocess_time(ms): 40.9, inference_time(ms): 10.3, postprocess_time(ms): 0.0

2022-05-19 18:02:33,875 - benchmark_utils - INFO - Paddle Inference benchmark log will be saved to /home/ubuntu/lxd-storage/xzy/PaddleCV/PaddleDetection/deploy/python/../../output/yolov3_mobilenet_v1_qat.log
2022-05-19 18:02:33,876 - benchmark_utils - INFO -
TRT_FP32
2022-05-19 18:02:33,876 - benchmark_utils - INFO - ---------------------- Paddle info ----------------------
2022-05-19 18:02:33,876 - benchmark_utils - INFO - [Det] paddle_version: 2.3.0
2022-05-19 18:02:33,876 - benchmark_utils - INFO - [Det] paddle_commit: 590b4dbcdd989324089ce43c22ef151c746c92a3
2022-05-19 18:02:33,876 - benchmark_utils - INFO - [Det] paddle_branch: HEAD
2022-05-19 18:02:33,876 - benchmark_utils - INFO - [Det] log_api_version: 1.0.3
2022-05-19 18:02:33,876 - benchmark_utils - INFO - ----------------------- Conf info -----------------------
2022-05-19 18:02:33,876 - benchmark_utils - INFO - [Det] runtime_device: gpu
2022-05-19 18:02:33,877 - benchmark_utils - INFO - [Det] ir_optim: True
2022-05-19 18:02:33,877 - benchmark_utils - INFO - [Det] enable_memory_optim: True
2022-05-19 18:02:33,877 - benchmark_utils - INFO - [Det] enable_tensorrt: True
2022-05-19 18:02:33,877 - benchmark_utils - INFO - [Det] enable_mkldnn: False
2022-05-19 18:02:33,877 - benchmark_utils - INFO - [Det] cpu_math_library_num_threads: 1
2022-05-19 18:02:33,877 - benchmark_utils - INFO - ----------------------- Model info ----------------------
2022-05-19 18:02:33,877 - benchmark_utils - INFO - [Det] model_name: yolov3_mobilenet_v1_qat
2022-05-19 18:02:33,877 - benchmark_utils - INFO - [Det] precision: fp32
2022-05-19 18:02:33,877 - benchmark_utils - INFO - ----------------------- Data info -----------------------
2022-05-19 18:02:33,878 - benchmark_utils - INFO - [Det] batch_size: 1
2022-05-19 18:02:33,878 - benchmark_utils - INFO - [Det] input_shape: dynamic_shape
2022-05-19 18:02:33,878 - benchmark_utils - INFO - [Det] data_num: 850
2022-05-19 18:02:33,878 - benchmark_utils - INFO - ----------------------- Perf info -----------------------
2022-05-19 18:02:33,878 - benchmark_utils - INFO - [Det] cpu_rss(MB): 5771, cpu_vms: 0, cpu_shared_mb: 0, cpu_dirty_mb: 0, cpu_util: 0%
2022-05-19 18:02:33,878 - benchmark_utils - INFO - [Det] gpu_rss(MB): 10, gpu_util: 21.04%, gpu_mem_util: 0%
2022-05-19 18:02:33,878 - benchmark_utils - INFO - [Det] total time spent(s): 38.6838
2022-05-19 18:02:33,878 - benchmark_utils - INFO - [Det] preprocess_time(ms): 40.0, inference_time(ms): 5.5, postprocess_time(ms): 0.0

2022-05-19 18:10:32,187 - benchmark_utils - INFO - Paddle Inference benchmark log will be saved to /home/ubuntu/lxd-storage/xzy/PaddleCV/PaddleDetection/deploy/python/../../output/yolov3_mobilenet_v1_qat.log
2022-05-19 18:10:32,187 - benchmark_utils - INFO -
TRT_FP16
2022-05-19 18:10:32,188 - benchmark_utils - INFO - ---------------------- Paddle info ----------------------
2022-05-19 18:10:32,188 - benchmark_utils - INFO - [Det] paddle_version: 2.3.0
2022-05-19 18:10:32,188 - benchmark_utils - INFO - [Det] paddle_commit: 590b4dbcdd989324089ce43c22ef151c746c92a3
2022-05-19 18:10:32,188 - benchmark_utils - INFO - [Det] paddle_branch: HEAD
2022-05-19 18:10:32,188 - benchmark_utils - INFO - [Det] log_api_version: 1.0.3
2022-05-19 18:10:32,188 - benchmark_utils - INFO - ----------------------- Conf info -----------------------
2022-05-19 18:10:32,189 - benchmark_utils - INFO - [Det] runtime_device: gpu
2022-05-19 18:10:32,189 - benchmark_utils - INFO - [Det] ir_optim: True
2022-05-19 18:10:32,189 - benchmark_utils - INFO - [Det] enable_memory_optim: True
2022-05-19 18:10:32,189 - benchmark_utils - INFO - [Det] enable_tensorrt: True
2022-05-19 18:10:32,189 - benchmark_utils - INFO - [Det] enable_mkldnn: False
2022-05-19 18:10:32,189 - benchmark_utils - INFO - [Det] cpu_math_library_num_threads: 1
2022-05-19 18:10:32,190 - benchmark_utils - INFO - ----------------------- Model info ----------------------
2022-05-19 18:10:32,190 - benchmark_utils - INFO - [Det] model_name: yolov3_mobilenet_v1_qat
2022-05-19 18:10:32,190 - benchmark_utils - INFO - [Det] precision: fp16
2022-05-19 18:10:32,190 - benchmark_utils - INFO - ----------------------- Data info -----------------------
2022-05-19 18:10:32,190 - benchmark_utils - INFO - [Det] batch_size: 1
2022-05-19 18:10:32,190 - benchmark_utils - INFO - [Det] input_shape: dynamic_shape
2022-05-19 18:10:32,190 - benchmark_utils - INFO - [Det] data_num: 850
2022-05-19 18:10:32,191 - benchmark_utils - INFO - ----------------------- Perf info -----------------------
2022-05-19 18:10:32,191 - benchmark_utils - INFO - [Det] cpu_rss(MB): 5811, cpu_vms: 0, cpu_shared_mb: 0, cpu_dirty_mb: 0, cpu_util: 0%
2022-05-19 18:10:32,191 - benchmark_utils - INFO - [Det] gpu_rss(MB): 10, gpu_util: 11.8%, gpu_mem_util: 0%
2022-05-19 18:10:32,191 - benchmark_utils - INFO - [Det] total time spent(s): 36.1927
2022-05-19 18:10:32,191 - benchmark_utils - INFO - [Det] preprocess_time(ms): 39.7, inference_time(ms): 2.8, postprocess_time(ms): 0.0

TRT_INT8
2022-05-19 15:30:53,786 - benchmark_utils - INFO - ---------------------- Paddle info ----------------------
2022-05-19 15:30:53,787 - benchmark_utils - INFO - [Det] paddle_version: 2.3.0
2022-05-19 15:30:53,787 - benchmark_utils - INFO - [Det] paddle_commit: 590b4dbcdd989324089ce43c22ef151c746c92a3
2022-05-19 15:30:53,787 - benchmark_utils - INFO - [Det] paddle_branch: HEAD
2022-05-19 15:30:53,787 - benchmark_utils - INFO - [Det] log_api_version: 1.0.3
2022-05-19 15:30:53,787 - benchmark_utils - INFO - ----------------------- Conf info -----------------------
2022-05-19 15:30:53,787 - benchmark_utils - INFO - [Det] runtime_device: gpu
2022-05-19 15:30:53,787 - benchmark_utils - INFO - [Det] ir_optim: True
2022-05-19 15:30:53,788 - benchmark_utils - INFO - [Det] enable_memory_optim: True
2022-05-19 15:30:53,788 - benchmark_utils - INFO - [Det] enable_tensorrt: True
2022-05-19 15:30:53,788 - benchmark_utils - INFO - [Det] enable_mkldnn: False
2022-05-19 15:30:53,788 - benchmark_utils - INFO - [Det] cpu_math_library_num_threads: 1
2022-05-19 15:30:53,788 - benchmark_utils - INFO - ----------------------- Model info ----------------------
2022-05-19 15:30:53,788 - benchmark_utils - INFO - [Det] model_name: yolov3_mobilenet_v1_qat
2022-05-19 15:30:53,788 - benchmark_utils - INFO - [Det] precision: int8
2022-05-19 15:30:53,789 - benchmark_utils - INFO - ----------------------- Data info -----------------------
2022-05-19 15:30:53,789 - benchmark_utils - INFO - [Det] batch_size: 1
2022-05-19 15:30:53,789 - benchmark_utils - INFO - [Det] input_shape: dynamic_shape
2022-05-19 15:30:53,789 - benchmark_utils - INFO - [Det] data_num: 850
2022-05-19 15:30:53,789 - benchmark_utils - INFO - ----------------------- Perf info -----------------------
2022-05-19 15:30:53,789 - benchmark_utils - INFO - [Det] cpu_rss(MB): 5814, cpu_vms: 0, cpu_shared_mb: 0, cpu_dirty_mb: 0, cpu_util: 0%
2022-05-19 15:30:53,789 - benchmark_utils - INFO - [Det] gpu_rss(MB): 9337, gpu_util: 26.3%, gpu_mem_util: 0%
2022-05-19 15:30:53,790 - benchmark_utils - INFO - [Det] total time spent(s): 31.9994
2022-05-19 15:30:53,790 - benchmark_utils - INFO - [Det] preprocess_time(ms): 34.9, inference_time(ms): 2.8, postprocess_time(ms): 0.0
```
未量化版本：
```
2022-05-20 11:05:36,368 - benchmark_utils - INFO - Paddle Inference benchmark log will be saved to /home/ubuntu/lxd-storage/xzy/PaddleCV/PaddleDetection/deploy/python/../../output/yolov3_mobilenet_v1_270e_voc_origin.log
2022-05-20 11:05:36,369 - benchmark_utils - INFO -
Fluid
2022-05-20 11:05:36,369 - benchmark_utils - INFO - ---------------------- Paddle info ----------------------
2022-05-20 11:05:36,369 - benchmark_utils - INFO - [Det] paddle_version: 2.3.0
2022-05-20 11:05:36,369 - benchmark_utils - INFO - [Det] paddle_commit: 590b4dbcdd989324089ce43c22ef151c746c92a3
2022-05-20 11:05:36,370 - benchmark_utils - INFO - [Det] paddle_branch: HEAD
2022-05-20 11:05:36,370 - benchmark_utils - INFO - [Det] log_api_version: 1.0.3
2022-05-20 11:05:36,370 - benchmark_utils - INFO - ----------------------- Conf info -----------------------
2022-05-20 11:05:36,370 - benchmark_utils - INFO - [Det] runtime_device: gpu
2022-05-20 11:05:36,370 - benchmark_utils - INFO - [Det] ir_optim: True
2022-05-20 11:05:36,370 - benchmark_utils - INFO - [Det] enable_memory_optim: True
2022-05-20 11:05:36,370 - benchmark_utils - INFO - [Det] enable_tensorrt: False
2022-05-20 11:05:36,371 - benchmark_utils - INFO - [Det] enable_mkldnn: False
2022-05-20 11:05:36,371 - benchmark_utils - INFO - [Det] cpu_math_library_num_threads: 1
2022-05-20 11:05:36,371 - benchmark_utils - INFO - ----------------------- Model info ----------------------
2022-05-20 11:05:36,371 - benchmark_utils - INFO - [Det] model_name: yolov3_mobilenet_v1_270e_voc_origin
2022-05-20 11:05:36,371 - benchmark_utils - INFO - [Det] precision: fluid
2022-05-20 11:05:36,371 - benchmark_utils - INFO - ----------------------- Data info -----------------------
2022-05-20 11:05:36,371 - benchmark_utils - INFO - [Det] batch_size: 1
2022-05-20 11:05:36,371 - benchmark_utils - INFO - [Det] input_shape: dynamic_shape
2022-05-20 11:05:36,372 - benchmark_utils - INFO - [Det] data_num: 850
2022-05-20 11:05:36,372 - benchmark_utils - INFO - ----------------------- Perf info -----------------------
2022-05-20 11:05:36,372 - benchmark_utils - INFO - [Det] cpu_rss(MB): 4128, cpu_vms: 0, cpu_shared_mb: 0, cpu_dirty_mb: 0, cpu_util: 0%
2022-05-20 11:05:36,372 - benchmark_utils - INFO - [Det] gpu_rss(MB): 2426, gpu_util: 38.5%, gpu_mem_util: 0%
2022-05-20 11:05:36,372 - benchmark_utils - INFO - [Det] total time spent(s): 40.5134
2022-05-20 11:05:36,372 - benchmark_utils - INFO - [Det] preprocess_time(ms): 40.3, inference_time(ms): 7.4, postprocess_time(ms): 0.0

2022-05-20 11:13:08,369 - benchmark_utils - INFO - Paddle Inference benchmark log will be saved to /home/ubuntu/lxd-storage/xzy/PaddleCV/PaddleDetection/deploy/python/../../output/yolov3_mobilenet_v1_270e_voc_origin.log
2022-05-20 11:13:08,370 - benchmark_utils - INFO -
TRT_FP32
2022-05-20 11:13:08,370 - benchmark_utils - INFO - ---------------------- Paddle info ----------------------
2022-05-20 11:13:08,370 - benchmark_utils - INFO - [Det] paddle_version: 2.3.0
2022-05-20 11:13:08,370 - benchmark_utils - INFO - [Det] paddle_commit: 590b4dbcdd989324089ce43c22ef151c746c92a3
2022-05-20 11:13:08,370 - benchmark_utils - INFO - [Det] paddle_branch: HEAD
2022-05-20 11:13:08,371 - benchmark_utils - INFO - [Det] log_api_version: 1.0.3
2022-05-20 11:13:08,371 - benchmark_utils - INFO - ----------------------- Conf info -----------------------
2022-05-20 11:13:08,371 - benchmark_utils - INFO - [Det] runtime_device: gpu
2022-05-20 11:13:08,371 - benchmark_utils - INFO - [Det] ir_optim: True
2022-05-20 11:13:08,371 - benchmark_utils - INFO - [Det] enable_memory_optim: True
2022-05-20 11:13:08,371 - benchmark_utils - INFO - [Det] enable_tensorrt: True
2022-05-20 11:13:08,371 - benchmark_utils - INFO - [Det] enable_mkldnn: False
2022-05-20 11:13:08,371 - benchmark_utils - INFO - [Det] cpu_math_library_num_threads: 1
2022-05-20 11:13:08,371 - benchmark_utils - INFO - ----------------------- Model info ----------------------
2022-05-20 11:13:08,372 - benchmark_utils - INFO - [Det] model_name: yolov3_mobilenet_v1_270e_voc_origin
2022-05-20 11:13:08,372 - benchmark_utils - INFO - [Det] precision: fp32
2022-05-20 11:13:08,372 - benchmark_utils - INFO - ----------------------- Data info -----------------------
2022-05-20 11:13:08,372 - benchmark_utils - INFO - [Det] batch_size: 1
2022-05-20 11:13:08,372 - benchmark_utils - INFO - [Det] input_shape: dynamic_shape
2022-05-20 11:13:08,372 - benchmark_utils - INFO - [Det] data_num: 850
2022-05-20 11:13:08,372 - benchmark_utils - INFO - ----------------------- Perf info -----------------------
2022-05-20 11:13:08,372 - benchmark_utils - INFO - [Det] cpu_rss(MB): 5750, cpu_vms: 0, cpu_shared_mb: 0, cpu_dirty_mb: 0, cpu_util: 0%
2022-05-20 11:13:08,372 - benchmark_utils - INFO - [Det] gpu_rss(MB): 3146, gpu_util: 20.38%, gpu_mem_util: 0%
2022-05-20 11:13:08,373 - benchmark_utils - INFO - [Det] total time spent(s): 40.178
2022-05-20 11:13:08,373 - benchmark_utils - INFO - [Det] preprocess_time(ms): 41.9, inference_time(ms): 5.3, postprocess_time(ms): 0.0

2022-05-20 11:20:30,255 - benchmark_utils - INFO - Paddle Inference benchmark log will be saved to /home/ubuntu/lxd-storage/xzy/PaddleCV/PaddleDetection/deploy/python/../../output/yolov3_mobilenet_v1_270e_voc_origin.log
2022-05-20 11:20:30,255 - benchmark_utils - INFO - 
TRT_FP16
2022-05-20 11:20:30,255 - benchmark_utils - INFO - ---------------------- Paddle info ----------------------
2022-05-20 11:20:30,255 - benchmark_utils - INFO - [Det] paddle_version: 2.3.0
2022-05-20 11:20:30,255 - benchmark_utils - INFO - [Det] paddle_commit: 590b4dbcdd989324089ce43c22ef151c746c92a3
2022-05-20 11:20:30,256 - benchmark_utils - INFO - [Det] paddle_branch: HEAD
2022-05-20 11:20:30,256 - benchmark_utils - INFO - [Det] log_api_version: 1.0.3
2022-05-20 11:20:30,256 - benchmark_utils - INFO - ----------------------- Conf info -----------------------
2022-05-20 11:20:30,256 - benchmark_utils - INFO - [Det] runtime_device: gpu
2022-05-20 11:20:30,256 - benchmark_utils - INFO - [Det] ir_optim: True
2022-05-20 11:20:30,256 - benchmark_utils - INFO - [Det] enable_memory_optim: True
2022-05-20 11:20:30,256 - benchmark_utils - INFO - [Det] enable_tensorrt: True
2022-05-20 11:20:30,256 - benchmark_utils - INFO - [Det] enable_mkldnn: False
2022-05-20 11:20:30,256 - benchmark_utils - INFO - [Det] cpu_math_library_num_threads: 1
2022-05-20 11:20:30,256 - benchmark_utils - INFO - ----------------------- Model info ----------------------
2022-05-20 11:20:30,256 - benchmark_utils - INFO - [Det] model_name: yolov3_mobilenet_v1_270e_voc_origin
2022-05-20 11:20:30,256 - benchmark_utils - INFO - [Det] precision: fp16
2022-05-20 11:20:30,257 - benchmark_utils - INFO - ----------------------- Data info -----------------------
2022-05-20 11:20:30,257 - benchmark_utils - INFO - [Det] batch_size: 1
2022-05-20 11:20:30,257 - benchmark_utils - INFO - [Det] input_shape: dynamic_shape
2022-05-20 11:20:30,257 - benchmark_utils - INFO - [Det] data_num: 850
2022-05-20 11:20:30,257 - benchmark_utils - INFO - ----------------------- Perf info -----------------------
2022-05-20 11:20:30,257 - benchmark_utils - INFO - [Det] cpu_rss(MB): 5799, cpu_vms: 0, cpu_shared_mb: 0, cpu_dirty_mb: 0, cpu_util: 0%
2022-05-20 11:20:30,257 - benchmark_utils - INFO - [Det] gpu_rss(MB): 3048, gpu_util: 10.82%, gpu_mem_util: 0%
2022-05-20 11:20:30,257 - benchmark_utils - INFO - [Det] total time spent(s): 37.8408
2022-05-20 11:20:30,257 - benchmark_utils - INFO - [Det] preprocess_time(ms): 41.8, inference_time(ms): 2.7, postprocess_time(ms): 0.0

2022-05-20 11:41:09,278 - benchmark_utils - INFO - Paddle Inference benchmark log will be saved to /home/ubuntu/lxd-storage/xzy/PaddleCV/PaddleDetection/deploy/python/../../output/yolov3_mobilenet_v1_270e_voc_origin.log
2022-05-20 11:41:09,278 - benchmark_utils - INFO -
TRT_INT8
2022-05-20 11:41:09,278 - benchmark_utils - INFO - ---------------------- Paddle info ----------------------
2022-05-20 11:41:09,279 - benchmark_utils - INFO - [Det] paddle_version: 2.3.0
2022-05-20 11:41:09,279 - benchmark_utils - INFO - [Det] paddle_commit: 590b4dbcdd989324089ce43c22ef151c746c92a3
2022-05-20 11:41:09,279 - benchmark_utils - INFO - [Det] paddle_branch: HEAD
2022-05-20 11:41:09,279 - benchmark_utils - INFO - [Det] log_api_version: 1.0.3
2022-05-20 11:41:09,279 - benchmark_utils - INFO - ----------------------- Conf info -----------------------
2022-05-20 11:41:09,279 - benchmark_utils - INFO - [Det] runtime_device: gpu
2022-05-20 11:41:09,279 - benchmark_utils - INFO - [Det] ir_optim: True
2022-05-20 11:41:09,279 - benchmark_utils - INFO - [Det] enable_memory_optim: True
2022-05-20 11:41:09,279 - benchmark_utils - INFO - [Det] enable_tensorrt: True
2022-05-20 11:41:09,279 - benchmark_utils - INFO - [Det] enable_mkldnn: False
2022-05-20 11:41:09,280 - benchmark_utils - INFO - [Det] cpu_math_library_num_threads: 1
2022-05-20 11:41:09,280 - benchmark_utils - INFO - ----------------------- Model info ----------------------
2022-05-20 11:41:09,280 - benchmark_utils - INFO - [Det] model_name: yolov3_mobilenet_v1_270e_voc_origin
2022-05-20 11:41:09,280 - benchmark_utils - INFO - [Det] precision: int8
2022-05-20 11:41:09,280 - benchmark_utils - INFO - ----------------------- Data info -----------------------
2022-05-20 11:41:09,280 - benchmark_utils - INFO - [Det] batch_size: 1
2022-05-20 11:41:09,280 - benchmark_utils - INFO - [Det] input_shape: dynamic_shape
2022-05-20 11:41:09,280 - benchmark_utils - INFO - [Det] data_num: 850
2022-05-20 11:41:09,280 - benchmark_utils - INFO - ----------------------- Perf info -----------------------
2022-05-20 11:41:09,280 - benchmark_utils - INFO - [Det] cpu_rss(MB): 5799, cpu_vms: 0, cpu_shared_mb: 0, cpu_dirty_mb: 0, cpu_util: 0%
2022-05-20 11:41:09,281 - benchmark_utils - INFO - [Det] gpu_rss(MB): 3048, gpu_util: 11.38%, gpu_mem_util: 0%
2022-05-20 11:41:09,281 - benchmark_utils - INFO - [Det] total time spent(s): 31.2766
2022-05-20 11:41:09,281 - benchmark_utils - INFO - [Det] preprocess_time(ms): 34.1, inference_time(ms): 2.7, postprocess_time(ms): 0.0
```
对比发现：
1. 在相同运行模式下，量化前后模型推理速度无明显表现差异，无法体现优化。
2. 在相同运行模式下，量化前后模型占用显存内存均变小。但INT8模式似乎有异常，甚至量化后gpu_rss不如未量化模型的。在运行时加载模型时间也明显过长。

之后，我又手动测试了两模型在850张图片上的推理。运行方式为：
python deploy/python/infer.py \
  --model_dir=./inference_model/yolov3_mobilenet_v1_270e_qat/yolov3_mobilenet_v1_qat \
  --image_dir=./demo/img_dir \
  --device=GPU \
  --run_mode=trt_fp32（或trt_int8等）

得到结果如下：
```
QAT TRT_FP32
------------------ Inference Time Info ----------------------
total_time(ms): 38458.1, img_num: 850
average latency time(ms): 45.24, QPS: 22.101976
preprocess_time(ms): 38.60, inference_time(ms): 6.60, postprocess_time(ms): 0.00

Origin TRT_FP32
------------------ Inference Time Info ----------------------
total_time(ms): 47660.4, img_num: 850
average latency time(ms): 56.07, QPS: 17.834513
preprocess_time(ms): 47.20, inference_time(ms): 8.90, postprocess_time(ms): 0.00

QAT TRT_INT8
------------------ Inference Time Info ----------------------
total_time(ms): 36080.8, img_num: 850
average latency time(ms): 42.45, QPS: 23.558236
preprocess_time(ms): 39.50, inference_time(ms): 2.90, postprocess_time(ms): 0.00

Origin TRT_INT8
------------------ Inference Time Info ----------------------
total_time(ms): 33595.2, img_num: 850
average latency time(ms): 39.52, QPS: 25.301234
preprocess_time(ms): 36.80, inference_time(ms): 2.70, postprocess_time(ms): 0.00
```
可以看出推理速度依然无差别。

问题：
1. 为何QAT量化后模型运行结果，与未量化模型对比，在相同模式下运行速度无差别？
2. 理论上来说，量化后模型占用应为原模型的1/2~1/4。原模型的.pdiparams文件只有92.3MB，但为何实际量化后的gpu_rss只有10MB？另外，为何trt_int8下，QAT模型的gpu_rss竟然会达到9337MB？
4. 相比于Fluid和TRT_FP32，TRT_FP16和TRT_INT8的确有加速。但为何TRT_FP16和TRT_INT8的推理时间相同？
5. 为何未量化模型也可以使用trt_fp16和trt_int8模式进行运行，并且同样能取得加速效果？原理是什么？

我不清楚是我的实验设置有问题，还是Paddle框架有问题，对于Paddle推理内部流程也不太了解，可否请官方工作人员解答一下疑惑？

补充：实验环境：
Ubuntu 18.04 RTS
Python 3.7
PaddleDetection 2.3
PaddlePaddle-gpu 2.3.0.post101 （带TensorRT预编译库版本）
CUDA 10.1
cudnn 7.6.5
显卡是Nvidia RTX 2080Ti"
PaddleDetection 用 tools/export_mode.py导出推理模型报错  出现段错误,PaddlePaddle/PaddleDetection,2022-05-21 02:48:12,2,,6025,1243845331,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

```
lcfc@lcfc-desktop:/home/zhaozhenshan/zhaozhenshan/PaddleDetection-release-2.4$ sudo python3 tools/export_model.py -c configs/yolov3/yolov3_darknet53_270e_coco.yml --output_dir=./inference_model \
>  -o weights=weights/yolov3_darknet53_270e_coco.pdparams


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   cuDevicePrimaryCtxRetain

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1653100828 (unix time) try ""date -d @1653100828"" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x7f807a9090) received by PID 1508 (TID 0x7f807f6010) from PID 18446744071570100368 ***]

Segmentation fault

```


### 复现环境 Environment

paddlepaddle 2.5
cuda 10.2
cudnn 8.0
python 3.6
jetson xavier

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
知识蒸馏,PaddlePaddle/PaddleDetection,2022-05-19 14:24:47,0,,6015,1241869733,
如何convert其他仓库训练的torch模型到paddledetection框架下,PaddlePaddle/PaddleDetection,2022-05-18 05:29:17,3,,6002,1239448282,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

如题，使用的是一个没有FPN的Faster-Rcnn torch模型，想要实现tensorrt的部署，能否通过convert模型进而load到paddledetection框架下进行部署呢，希望能给点指导，十分感谢～"
s2anet现在可支持eval吗，用的2.4貌似还是不支持,PaddlePaddle/PaddleDetection,2022-05-17 02:43:07,2,,5993,1237976742,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

s2anet在2.4版本无法支持eval

### 复现环境 Environment

_No response_

### 是否愿意提交PR Are you willing to submit a PR?

- [X] Yes I'd like to help by submitting a PR!"
静态图中没有fake_quant_dequant节点,PaddlePaddle/PaddleDetection,2022-05-16 06:59:37,2,model compression,5989,1236710342,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

使用paddle对模型进行在线量化，然后使用export_model.py将动态图转为静态图。因为量化的时候把所有的卷积算子都量化了，但是在静态图中有的卷积算子有fake_quant_dequant节点，而有的没有该节点，如下图所示
![image](https://user-images.githubusercontent.com/52658065/168536258-8311fb15-13c5-4ac8-83c3-235c816714bd.png)
请问这种情况是什么原因呢，如果想给所有静态图中所有的卷积算子都加上fake节点，该如何处理呢？"
SSD_mobilenet_V1的剪枝效果差,PaddlePaddle/PaddleDetection,2022-05-14 07:53:31,57,,5983,1235907716,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

之前已经根据静态图分析的敏感度调整pruned ratios，但是FLOPS 的pruned ratio很少只有0.02，相比官方给的yolo3demo 0.几的剪切率相差太多了"
为什么PPYOLOE单卡（8G显卡）训练特别慢？,PaddlePaddle/PaddleDetection,2022-05-14 04:12:46,3,,5981,1235863026,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

选small模型，图片1024，只允许batchsize=4，正常吗？ YOLOV5可以15, YOLOX也只能6. 
我看small的网络尺寸都是10M一下。 怎么训练速度差这么大？ 没有优化吗？ 
YOLOV5现在看来是既快又准确高"
maskrcnn小目标检测如何调参,PaddlePaddle/PaddleDetection,2022-05-14 02:24:19,2,training,5979,1235836366,"### 文档链接&描述 Document Links & Description

maskrcnn小目标检测如何调参，在一幅图像中，需要被提取的目标有的非常大，有的非常小，通过什么样的方法可以把大小目标都分割出来呢，目前看大目标效果较好，小目标漏检情况比较多，如何调参呢？

### 请提出你的建议 Please give your suggestion

maskrcnn小目标检测如何调参，在一幅图像中，需要被提取的目标有的非常大，有的非常小，通过什么样的方法可以把大小目标都分割出来呢，目前看大目标效果较好，小目标漏检情况比较多，如何调参呢？"
模型蒸馏时，学生的配置参数被教师的配置参数覆盖掉了,PaddlePaddle/PaddleDetection,2022-05-13 11:00:52,0,,5976,1235091596,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

在运行蒸馏命令时，配置自己的网络结构，设计损失运行时，发现传入的slim_config（教师参数）会覆盖初始的config：具体描述如下所示：
```python
# [PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection)/[ppdet](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/ppdet)/[slim](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/ppdet/slim)/distill.py
class DistillModel(nn.Layer):
    def __init__(self, cfg, slim_cfg): # 行32，传入的cfg是学生的配置
        super(DistillModel, self).__init__()

        self.student_model = create(cfg.architecture)
        logger.debug('Load student model pretrain_weights:{}'.format(
            cfg.pretrain_weights))
        load_pretrain_weight(self.student_model, cfg.pretrain_weights)

        slim_cfg = load_config(slim_cfg) # 行40，教师的配置覆盖学生的配置
        self.teacher_model = create(slim_cfg.architecture)
        self.distill_loss = create(slim_cfg.distill_loss)
        logger.debug('Load teacher model pretrain_weights:{}'.format(
            slim_cfg.pretrain_weights))
        load_pretrain_weight(self.teacher_model, slim_cfg.pretrain_weights)

        for param in self.teacher_model.parameters(): # 教师是不参与训练的
            param.trainable = False
```
40行这里返回了一个global_config，会更改上文32行构造函数中传入的cfg（学生参数），导致其实学生的训练是依照slim_config（教师的参数）进行。而教师在训练中通常是不进行可训练参数更新的。

个人认为既然在蒸馏中，学生的可训练参数会被更新，那么教师不能覆盖学生的训练设置。或者在进一步的开发中，教师和学生的可训练参数会对各自的损失求导更新，那么教师和学生的训练配置（如学习率，优化器等）应该是分离的，不能彼此覆盖。

为什么现有的蒸馏不会有问题？因为教师和学生均用的是yolov3结构蒸馏，配置参数相同，所以无需担心参数覆盖的问题。

### 复现环境 Environment

_No response_

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR! "
PPhuman 使用 --run_mode trt_fp16 参数，程序 运行到Model Configuration ，不再往下运行，也不报错，也不返回,PaddlePaddle/PaddleDetection,2022-05-13 09:41:59,5,,5975,1234997229,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

PPhuman 使用 --run_mode trt_fp16 参数，程序 运行到Model Configuration ，不再往下运行，也不报错，也不返回

如果不使用 TensorRT，即不加--run_mode trt_fp16 参数，则可正常运行

### 复现环境 Environment

-paddlepaddle-gpu   2.2.2.post101
-paddledet          2.4.0
-python               3.8.0
-CUDA                 10.1
-CUDNN              7.6.5
-OS                       WIN10



python deploy/pphuman/pipeline.py --config deploy/pphuman/config/infer_cfg.yml --video_file=G:\win_project\FairMOT\code\data_prepaired\2.mp4 --device=gpu --enable_action=True --run_mode
 trt_fp16
F:\python_project\paddle_detection_project\venv\lib\site-packages\paddle\vision\transforms\functional_pil.py:36: DeprecationWarning: N
EAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': Image.NEAREST,
F:\python_project\paddle_detection_project\venv\lib\site-packages\paddle\vision\transforms\functional_pil.py:37: DeprecationWarning: B
ILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': Image.BILINEAR,
F:\python_project\paddle_detection_project\venv\lib\site-packages\paddle\vision\transforms\functional_pil.py:38: DeprecationWarning: B
ICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': Image.BICUBIC,
F:\python_project\paddle_detection_project\venv\lib\site-packages\paddle\vision\transforms\functional_pil.py:39: DeprecationWarning: B
OX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  'box': Image.BOX,
F:\python_project\paddle_detection_project\venv\lib\site-packages\paddle\vision\transforms\functional_pil.py:40: DeprecationWarning: L
ANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  'lanczos': Image.LANCZOS,
F:\python_project\paddle_detection_project\venv\lib\site-packages\paddle\vision\transforms\functional_pil.py:41: DeprecationWarning: H
AMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  'hamming': Image.HAMMING
deploy/pphuman/pipeline.py:26: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is
 deprecated since Python 3.3, and in 3.9 it will stop working
  from collections import Sequence
-----------  Running Arguments -----------
ACTION:
  batch_size: 1
  coord_size:
  - 384
  - 512
  display_frames: 80
  max_frames: 50
  model_dir: output_inference/STGCN
ATTR:
  batch_size: 8
  model_dir: output_inference/strongbaseline_r50_30e_pa100k/
DET:
  batch_size: 1
  model_dir: output_inference/mot_ppyoloe_l_36e_pipeline/
KPT:
  batch_size: 8
  model_dir: output_inference/dark_hrnet_w32_256x192/
MOT:
  batch_size: 1
  model_dir: output_inference/mot_ppyoloe_l_36e_pipeline/
  tracker_config: deploy/pphuman/config/tracker_config.yml
REID:
  batch_size: 16
  model_dir: output_inference/reid_model/
attr_thresh: 0.5
crop_thresh: 0.5
kpt_thresh: 0.2
visual: true
warmup_frame: 50

------------------------------------------
Action Recognition enabled
-----------  Model Configuration -----------
Model Arch: YOLO
Transform Order:
--transform op: Resize
--transform op: Permute
--------------------------------------------
程序停止在此处，无报错，无返回

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
PaddleDetection使用TensorRT，对进行yolov3_mobilenet_v1_qat推理时报错,PaddlePaddle/PaddleDetection,2022-05-13 07:46:52,3,,5971,1234864498,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

该问题为此[Issue](https://github.com/PaddlePaddle/PaddleDetection/issues/5726)的后续，在那之后，由于环境和系统配置比较乱，我重新配置了一个新的系统环境，环境附后。
按照官方教程，在VOC数据集上训练了一个yolov3_mobilenet_v1_270e_voc模型，量化config使用默认的yolov3_mobilenet_v1_qat.yml。按照原Issue指示更新到PaddlePaddle 2.3.0.rc0后，使用TRT_INT8不再报tensorrt_subgraph_pass相关错误，但报另一个Pass错误。

PaddleInference命令和报错如下：
```
(pdconfig) ubuntu@sunyuke:~/lxd-storage/xzy/PaddleCV/PaddleDetection$ python deploy/python/infer.py \
>   --model_dir=./inference_model/yolov3_mobilenet_v1_270e_qat/yolov3_mobilenet_v1_qat \
>   --image_file=./dataset/fire_smoke_voc/images/00001.jpg \
>   --device=GPU \
>   --run_mode=trt_int8
/home/ubuntu/anaconda3/envs/pdconfig/lib/python3.7/site-packages/paddle/vision/transforms/functional_pil.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': Image.NEAREST,
/home/ubuntu/anaconda3/envs/pdconfig/lib/python3.7/site-packages/paddle/vision/transforms/functional_pil.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': Image.BILINEAR,
/home/ubuntu/anaconda3/envs/pdconfig/lib/python3.7/site-packages/paddle/vision/transforms/functional_pil.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': Image.BICUBIC,
/home/ubuntu/anaconda3/envs/pdconfig/lib/python3.7/site-packages/paddle/vision/transforms/functional_pil.py:39: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  'box': Image.BOX,
/home/ubuntu/anaconda3/envs/pdconfig/lib/python3.7/site-packages/paddle/vision/transforms/functional_pil.py:40: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  'lanczos': Image.LANCZOS,
/home/ubuntu/anaconda3/envs/pdconfig/lib/python3.7/site-packages/paddle/vision/transforms/functional_pil.py:41: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  'hamming': Image.HAMMING
-----------  Running Arguments -----------
batch_size: 1
camera_id: -1
cpu_threads: 1
device: GPU
enable_mkldnn: False
image_dir: None
image_file: ./dataset/fire_smoke_voc/images/00001.jpg
model_dir: ./inference_model/yolov3_mobilenet_v1_270e_qat/yolov3_mobilenet_v1_qat
output_dir: output
reid_batch_size: 50
reid_model_dir: None
run_benchmark: False
run_mode: trt_int8
save_images: False
save_mot_txt_per_img: False
save_mot_txts: False
scaled: False
threshold: 0.5
trt_calib_mode: False
trt_max_shape: 1280
trt_min_shape: 1
trt_opt_shape: 640
use_dark: True
use_gpu: False
video_file: None
------------------------------------------
-----------  Model Configuration -----------
Model Arch: YOLO
Transform Order:
--transform op: Resize
--transform op: NormalizeImage
--transform op: Permute
--------------------------------------------
Traceback (most recent call last):
  File ""deploy/python/infer.py"", line 773, in <module>
    main()
  File ""deploy/python/infer.py"", line 726, in main
    enable_mkldnn=FLAGS.enable_mkldnn)
  File ""deploy/python/infer.py"", line 94, in __init__
    enable_mkldnn=enable_mkldnn)
  File ""deploy/python/infer.py"", line 563, in load_predictor
    predictor = create_predictor(config)
ValueError: (InvalidArgument) Pass preln_embedding_eltwise_layernorm_fuse_pass has not been registered.
  [Hint: Expected Has(pass_type) == true, but received Has(pass_type):0 != true:1.] (at /paddle/paddle/fluid/framework/ir/pass.h:242)
```

PaddleServing命令和报错如下（会随机报以下两不同错误，出现前提条件不明）：
```
(pdconfig) ubuntu@sunyuke:~/lxd-storage/xzy/PaddleCV/PaddleDetection/inference_model/yolov3_mobilenet_v1_270e_qat_pdserving/yolov3_mobilenet_v1_qat$ python -m paddle_serving_server.serve --model serving_server --port 9393 --gpu_ids 1 --precision int8 --use_trt
/home/ubuntu/anaconda3/envs/pdconfig/lib/python3.7/runpy.py:125: RuntimeWarning: 'paddle_serving_server.serve' found in sys.modules after import of package 'paddle_serving_server', but prior to execution of 'paddle_serving_server.serve'; this may result in unpredictable behaviour
  warn(RuntimeWarning(msg))
Going to Run Comand
/home/ubuntu/anaconda3/envs/pdconfig/lib/python3.7/site-packages/paddle_serving_server/serving-gpu-101-0.8.3/serving -enable_model_toolkit -inferservice_path workdir_9393 -inferservice_file infer_service.prototxt -max_concurrency 0 -num_threads 4 -port 9393 -precision int8 -use_calib=False -reload_interval_s 10 -resource_path workdir_9393 -resource_file resource.prototxt -workflow_path workdir_9393 -workflow_file workflow.prototxt -bthread_concurrency 4 -max_body_size 536870912
I0100 00:00:00.000000 31581 op_repository.h:68] RAW: Succ regist op: GeneralDistKVInferOp
I0100 00:00:00.000000 31581 op_repository.h:68] RAW: Succ regist op: GeneralDistKVQuantInferOp
I0100 00:00:00.000000 31581 op_repository.h:68] RAW: Succ regist op: GeneralInferOp
I0100 00:00:00.000000 31581 op_repository.h:68] RAW: Succ regist op: GeneralReaderOp
I0100 00:00:00.000000 31581 op_repository.h:68] RAW: Succ regist op: GeneralRecOp
I0100 00:00:00.000000 31581 op_repository.h:68] RAW: Succ regist op: GeneralResponseOp
I0100 00:00:00.000000 31581 service_manager.h:79] RAW: Service[LoadGeneralModelService] insert successfully!
I0100 00:00:00.000000 31581 load_general_model_service.pb.h:333] RAW: Success regist service[LoadGeneralModelService][PN5baidu14paddle_serving9predictor26load_general_model_service27LoadGeneralModelServiceImplE]
I0100 00:00:00.000000 31581 service_manager.h:79] RAW: Service[GeneralModelService] insert successfully!
I0100 00:00:00.000000 31581 general_model_service.pb.h:1608] RAW: Success regist service[GeneralModelService][PN5baidu14paddle_serving9predictor13general_model23GeneralModelServiceImplE]
I0100 00:00:00.000000 31581 factory.h:155] RAW: Succ insert one factory, tag: PADDLE_INFER, base type N5baidu14paddle_serving9predictor11InferEngineE
W0100 00:00:00.000000 31581 paddle_engine.cpp:34] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<PaddleInferenceEngine>->::baidu::paddle_serving::predictor::InferEngine, tag: PADDLE_INFER in macro!
I0513 15:06:24.095415 31585 analysis_predictor.cc:576] TensorRT subgraph engine is enabled
--- Running analysis [ir_graph_build_pass]
--- Running analysis [ir_graph_clean_pass]
--- Running analysis [ir_analysis_pass]
--- Running IR pass [conv_affine_channel_fuse_pass]
--- Running IR pass [adaptive_pool2d_convert_global_pass]
--- Running IR pass [conv_eltwiseadd_affine_channel_fuse_pass]
--- Running IR pass [shuffle_channel_detect_pass]
--- Running IR pass [quant_conv2d_dequant_fuse_pass]
--- Running IR pass [delete_quant_dequant_op_pass]
I0513 15:06:24.263115 31585 fuse_pass_base.cc:57] ---  detected 47 subgraphs
--- Running IR pass [delete_quant_dequant_filter_op_pass]
I0513 15:06:24.300499 31585 fuse_pass_base.cc:57] ---  detected 47 subgraphs
--- Running IR pass [simplify_with_basic_ops_pass]
--- Running IR pass [embedding_eltwise_layernorm_fuse_pass]
--- Running IR pass [multihead_matmul_fuse_pass_v2]
--- Running IR pass [multihead_matmul_fuse_pass_v3]
--- Running IR pass [skip_layernorm_fuse_pass]
--- Running IR pass [unsqueeze2_eltwise_fuse_pass]
--- Running IR pass [squeeze2_matmul_fuse_pass]
--- Running IR pass [reshape2_matmul_fuse_pass]
--- Running IR pass [flatten2_matmul_fuse_pass]
--- Running IR pass [map_matmul_v2_to_mul_pass]
--- Running IR pass [map_matmul_v2_to_matmul_pass]
--- Running IR pass [map_matmul_to_mul_pass]
--- Running IR pass [fc_fuse_pass]
--- Running IR pass [conv_elementwise_add_fuse_pass]
--- Running IR pass [tensorrt_subgraph_pass]
I0513 15:06:24.358453 31585 tensorrt_subgraph_pass.cc:138] ---  detect a sub-graph with 145 nodes
I0513 15:06:24.391294 31585 tensorrt_subgraph_pass.cc:395] Prepare TRT engine (Optimize model structure, Select OP kernel etc). This process may cost a lot of time.
terminate called after throwing an instance of 'paddle::platform::EnforceNotMet'
  what():

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------

----------------------
Error Message Summary:
----------------------
UnimplementedError: no OpConverter for optype [nearest_interp_v2]
  [Hint: it should not be null.] (at /paddle/paddle/fluid/inference/tensorrt/convert/op_converter.h:142)

Aborted (core dumped)
```
```
(pdconfig) ubuntu@sunyuke:~/lxd-storage/xzy/PaddleCV/PaddleDetection/inference_model/yolov3_mobilenet_v1_270e_qat_pdserving/yolov3_mobilenet_v1_qat$ python -m paddle_serving_server.serve --model serving_server --port 9393 --gpu_ids 1 --precision int8 --use_trt
/home/ubuntu/anaconda3/envs/pdconfig/lib/python3.7/runpy.py:125: RuntimeWarning: 'paddle_serving_server.serve' found in sys.modules after import of package 'paddle_serving_server', but prior to execution of 'paddle_serving_server.serve'; this may result in unpredictable behaviour
  warn(RuntimeWarning(msg))
Going to Run Comand
/home/ubuntu/anaconda3/envs/pdconfig/lib/python3.7/site-packages/paddle_serving_server/serving-gpu-101-0.8.3/serving -enable_model_toolkit -inferservice_path workdir_9393 -inferservice_file infer_service.prototxt -max_concurrency 0 -num_threads 4 -port 9393 -precision int8 -use_calib=False -reload_interval_s 10 -resource_path workdir_9393 -resource_file resource.prototxt -workflow_path workdir_9393 -workflow_file workflow.prototxt -bthread_concurrency 4 -max_body_size 536870912
I0100 00:00:00.000000 31237 op_repository.h:68] RAW: Succ regist op: GeneralDistKVInferOp
I0100 00:00:00.000000 31237 op_repository.h:68] RAW: Succ regist op: GeneralDistKVQuantInferOp
I0100 00:00:00.000000 31237 op_repository.h:68] RAW: Succ regist op: GeneralInferOp
I0100 00:00:00.000000 31237 op_repository.h:68] RAW: Succ regist op: GeneralReaderOp
I0100 00:00:00.000000 31237 op_repository.h:68] RAW: Succ regist op: GeneralRecOp
I0100 00:00:00.000000 31237 op_repository.h:68] RAW: Succ regist op: GeneralResponseOp
I0100 00:00:00.000000 31237 service_manager.h:79] RAW: Service[LoadGeneralModelService] insert successfully!
I0100 00:00:00.000000 31237 load_general_model_service.pb.h:333] RAW: Success regist service[LoadGeneralModelService][PN5baidu14paddle_serving9predictor26load_general_model_service27LoadGeneralModelServiceImplE]
I0100 00:00:00.000000 31237 service_manager.h:79] RAW: Service[GeneralModelService] insert successfully!
I0100 00:00:00.000000 31237 general_model_service.pb.h:1608] RAW: Success regist service[GeneralModelService][PN5baidu14paddle_serving9predictor13general_model23GeneralModelServiceImplE]
I0100 00:00:00.000000 31237 factory.h:155] RAW: Succ insert one factory, tag: PADDLE_INFER, base type N5baidu14paddle_serving9predictor11InferEngineE
W0100 00:00:00.000000 31237 paddle_engine.cpp:34] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<PaddleInferenceEngine>->::baidu::paddle_serving::predictor::InferEngine, tag: PADDLE_INFER in macro!
I0513 12:57:24.111806 31240 analysis_predictor.cc:576] TensorRT subgraph engine is enabled
--- Running analysis [ir_graph_build_pass]
--- Running analysis [ir_graph_clean_pass]
--- Running analysis [ir_analysis_pass]
--- Running IR pass [conv_affine_channel_fuse_pass]
--- Running IR pass [adaptive_pool2d_convert_global_pass]
--- Running IR pass [conv_eltwiseadd_affine_channel_fuse_pass]
--- Running IR pass [shuffle_channel_detect_pass]
--- Running IR pass [quant_conv2d_dequant_fuse_pass]
--- Running IR pass [delete_quant_dequant_op_pass]
I0513 12:57:24.282593 31240 fuse_pass_base.cc:57] ---  detected 47 subgraphs
--- Running IR pass [delete_quant_dequant_filter_op_pass]
I0513 12:57:24.320891 31240 fuse_pass_base.cc:57] ---  detected 47 subgraphs
--- Running IR pass [simplify_with_basic_ops_pass]
--- Running IR pass [embedding_eltwise_layernorm_fuse_pass]
--- Running IR pass [multihead_matmul_fuse_pass_v2]
--- Running IR pass [multihead_matmul_fuse_pass_v3]
--- Running IR pass [skip_layernorm_fuse_pass]
--- Running IR pass [unsqueeze2_eltwise_fuse_pass]
--- Running IR pass [squeeze2_matmul_fuse_pass]
--- Running IR pass [reshape2_matmul_fuse_pass]
--- Running IR pass [flatten2_matmul_fuse_pass]
--- Running IR pass [map_matmul_v2_to_mul_pass]
--- Running IR pass [map_matmul_v2_to_matmul_pass]
--- Running IR pass [map_matmul_to_mul_pass]
--- Running IR pass [fc_fuse_pass]
--- Running IR pass [conv_elementwise_add_fuse_pass]
--- Running IR pass [tensorrt_subgraph_pass]
I0513 12:57:24.429746 31240 tensorrt_subgraph_pass.cc:138] ---  detect a sub-graph with 8 nodes
I0513 12:57:24.433738 31240 tensorrt_subgraph_pass.cc:395] Prepare TRT engine (Optimize model structure, Select OP kernel etc). This process may cost a lot of time.
W0513 12:57:24.989765 31240 helper.h:107] Calibrator is not being used. Users must provide dynamic range for all tensors that are not Int32.
E0513 12:57:24.990360 31240 helper.h:111] Calibration failure occurred with no scaling factors detected. This could be due to no int8 calibrator or insufficient custom scales for network layers. Please see int8 sample to setup calibration correctly.
E0513 12:57:24.990375 31240 helper.h:111] Builder failed while configuring INT8 mode.
terminate called after throwing an instance of 'paddle::platform::EnforceNotMet'
  what():

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------

----------------------
Error Message Summary:
----------------------
FatalError: Build TensorRT cuda engine failed! Please recheck you configurations related to paddle-TensorRT.
  [Hint: infer_engine_ should not be null.] (at /paddle/paddle/fluid/inference/tensorrt/engine.cc:252)

Aborted (core dumped)
```


### 复现环境 Environment

paddlepaddle-gpu=2.3.0.rc0.post101
paddledet=2.3.0
paddleslim=2.2.2
paddle-serving-server-gpu=0.8.3.post101

Ubuntu 18.04
Python 3.7

Nvidia Driver 430.64
CUDA 10.1
cudnn 7.6.5
TensorRT=6.0.1.5

### 是否愿意提交PR Are you willing to submit a PR?

- [x] Yes I'd like to help by submitting a PR!"
参考《PP-Tracking之手把手玩转多目标跟踪》运行多目标跟踪示例失败，报错AssertionError: MTMCT only support single class.,PaddlePaddle/PaddleDetection,2022-05-13 03:23:20,12,,5967,1234693203,"### 问题确认 Search before asking

- [X] 我已经查询[历史issue](https://github.com/PaddlePaddle/PaddleDetection/issues)，没有报过同样bug。I have searched the [issues](https://github.com/PaddlePaddle/PaddleDetection/issues) and found no similar bug report.


### bug描述 Describe the Bug

步骤一，下载预测部署模型：
```shell
!wget https://paddledet.bj.bcebos.com/models/mot/deepsort/ppyolov2_r50vd_dcn_365e_aic21mtmct_vehicle.tar
!wget https://paddledet.bj.bcebos.com/models/mot/deepsort/deepsort_pplcnet_vehicle.tar
!cd ~/PaddleDetection/ && mkdir -p output_inference
!mv ppyolov2_r50vd_dcn_365e_aic21mtmct_vehicle.tar ~/PaddleDetection/output_inference
!mv deepsort_pplcnet_vehicle.tar ~/PaddleDetection/output_inference
!cd ~/PaddleDetection/output_inference && tar -xvf ppyolov2_r50vd_dcn_365e_aic21mtmct_vehicle.tar && tar -xvf deepsort_pplcnet_vehicle.tar
```

步骤二，完成跨镜跟踪预测
在完成模型下载后，需要修改PaddleDetection/deploy/pptracking/python路径下的mtmct_cfg.yml，这份配置文件中包含了跨镜跟踪中轨迹融合的相关参数。首先需要确定cameras_bias中对应的名称与输入视频名称对应；其次，我们本次项目使用轨迹融合中的通用方法，将zone和camera相关的方法设置为False。修改后配置如下：
```shell
# config for MTMCT
MTMCT: True
cameras_bias:
  c003: 0
  c004: 0
# 1.zone releated parameters
use_zone: False #True
zone_path: dataset/mot/aic21mtmct_vehicle/S06/zone
# 2.tricks parameters, can be used for other mtmct dataset
use_ff: True
use_rerank: True
# 3.camera releated parameters
use_camera: False #True
use_st_filter: False
# 4.zone releated parameters
use_roi: False #True
roi_dir: dataset/mot/aic21mtmct_vehicle/S06

配置完成后即可运行如下命令，输入视频为c003.mp4和c004.mp4两个不同视角的摄像头拍摄结果。
```

```shell
!wget https://bj.bcebos.com/v1/paddledet/data/mot/demo/mtmct-demo.tar && mv mtmct-demo.tar ~/PaddleDetection && cd ~/PaddleDetection && tar xvf mtmct-demo.tar 
!cd ~/PaddleDetection && python deploy/pptracking/python/mot_sde_infer.py --model_dir=output_inference/ppyolov2_r50vd_dcn_365e_aic21mtmct_vehicle/ --reid_model_dir=output_inference/deepsort_pplcnet_vehicle/ --mtmct_dir=./mtmct-demo --device=GPU --mtmct_cfg=deploy/pptracking/python/mtmct_cfg.yml --scaled=True --save_mot_txts --save_images
```

运行报错：
```shell
-----------  Running Arguments -----------
batch_size: 1
camera_id: -1
cpu_threads: 1
device: GPU
do_entrance_counting: False
draw_center_traj: False
enable_mkldnn: False
image_dir: None
image_file: None
model_dir: output_inference/ppyolov2_r50vd_dcn_365e_aic21mtmct_vehicle/
mtmct_cfg: deploy/pptracking/python/mtmct_cfg.yml
mtmct_dir: ./mtmct-demo
output_dir: output
reid_batch_size: 50
reid_model_dir: output_inference/deepsort_pplcnet_vehicle/
run_benchmark: False
run_mode: paddle
save_images: True
save_mot_txt_per_img: False
save_mot_txts: True
scaled: True
secs_interval: 2
threshold: 0.5
tracker_config: deploy/pptracking/python/tracker_config.yml
trt_calib_mode: False
trt_max_shape: 1280
trt_min_shape: 1
trt_opt_shape: 640
use_dark: True
use_gpu: False
video_file: None
------------------------------------------
-----------  Model Configuration -----------
Model Arch: YOLO
Transform Order: 
--transform op: Resize
--transform op: NormalizeImage
--transform op: Permute
--------------------------------------------
-----------  Model Configuration -----------
Model Arch: DeepSORT
Transform Order: 
--transform op: LetterBoxResize
--transform op: NormalizeImage
--transform op: Permute
--------------------------------------------
ffmpeg processing of video ./mtmct-demo/c003.mp4
start tracking seq: c003
Tracking frame: 0
Traceback (most recent call last):
  File ""deploy/pptracking/python/mot_sde_infer.py"", line 755, in <module>
    main()
  File ""deploy/pptracking/python/mot_sde_infer.py"", line 724, in main
    detector.predict_mtmct(FLAGS.mtmct_dir, mtmct_cfg)
  File ""deploy/pptracking/python/mot_sde_infer.py"", line 638, in predict_mtmct
    mot_features_dict = self.predict_image(
  File ""deploy/pptracking/python/mot_sde_infer.py"", line 418, in predict_image
    tracking_outs = self.tracking(det_result)
  File ""deploy/pptracking/python/mot_sde_infer.py"", line 312, in tracking
    assert self.num_classes == 1, 'MTMCT only support single class.'
AssertionError: MTMCT only support single class.
```

### 复现环境 Environment

- paddlepaddle-gpu:2.2.2
- paddledet:2.4.2
- cuda:10.2

### 是否愿意提交PR Are you willing to submit a PR?

- [ ] Yes I'd like to help by submitting a PR!"
关键点coco oks评估结果都是0,PaddlePaddle/PaddleDetection,2022-05-13 02:10:47,4,,5966,1234656115,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

您好，我使用paddledetection 2.4版本训练hrnet32的关键点检测模型，训练的目标为自定义的目标24个关键点，迭代了400epoch，模型推理可视化是正确的，但是使用eval评估oks指标时，所有的ap都是0，请问这是为何？
![image](https://user-images.githubusercontent.com/728405/168197283-377b2958-226d-402f-b3a6-2c5bac71c37c.png)
"
ppyoloe训练coco格式自己的数据集精度不断下降,PaddlePaddle/PaddleDetection,2022-05-12 13:15:31,3,,5960,1233969686,"### 问题确认 Search before asking

- [X] 我已经搜索过问题，但是没有找到解答。I have searched the question and found no related answer.


### 请提出你的问题 Please ask your question

使用ppyoloe训练自己的coco数据集，loss_cls不断上升，单卡训练，batch_size为14，学习率为0.0021875
<img width=""853"" alt=""image"" src=""https://user-images.githubusercontent.com/5400347/168083116-fb5f25e1-6320-4ba3-bace-967bb4578c3d.png"">
<img width=""1891"" alt=""image"" src=""https://user-images.githubusercontent.com/5400347/168083207-6d6b5b68-3abf-40c9-8b8d-145ee9a6491c.png"">


`
_BASE_: [
  '../datasets/wood_corner.yml',
  '../runtime.yml',
  './_base_/optimizer_300e.yml',
  './_base_/ppyoloe_crn.yml',
  './_base_/ppyoloe_reader.yml',
]

log_iter: 100
snapshot_epoch: 10
weights: output/ppyoloe_crn_m_300e_coco/model_final

pretrain_weights: https://paddledet.bj.bcebos.com/models/pretrained/CSPResNetb_m_pretrained.pdparams
depth_mult: 0.67
width_mult: 0.75

TrainReader:
  batch_size: 14

LearningRate:
  #base_lr: 0.035
  base_lr: 0.0021875

`"
[Other General Issues] 导出模型时出错 (NotFound) No Input(X) found for Concat operator.,PaddlePaddle/PaddleDetection,2022-05-12 10:03:02,4,deploy,5957,1233752018,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.

自己修改了一下主干网络结构，训练没啥问题，导出模型时候出错了。
## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?

```none
 python tools/export_model.py -c configs/ppyolo/ppyolo_pnet_1x_drone.yml --output_dir=./inference_model  -o weights=output/ppyolo_pnet_1x_drone/1.pdparams
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.
更改了主干网络
```
from functools import reduce
class Block(nn.Layer):
    def __init__(self, c1, c2, s, img_size, patch_size):
        super().__init__()
        self.patch_size = patch_size
        self.attention = []
        assert img_size % patch_size == 0
        self.patch_num = img_size//patch_size
        for i in range(1, img_size//patch_size + 1):
            x = paddle.zeros((1, 1, img_size, img_size))
            x[:, :,(i-1) * patch_size : i * patch_size, (i-1) * patch_size : i* patch_size] = 1
            self.attention.append(x)
        self.cov = Conv(c1, c1, s=s)
        self.csp = BottleneckCSP(c1, c2)
        

    def forward(self, input):
        B, C, W, H = input.shape
        out = []
        for atten in self.attention:
            y = paddle.empty((1, C + 1, W, H))
            for i in range(B):
                x = paddle.concat((paddle.unsqueeze(input[i], 0), atten), axis=1)
                y = paddle.concat((y,x), 0) if i > 0 else x
            x = self.cov(y)
            x = self.csp(x)
            out.append(x)
        return reduce(lambda x, y: x+y, out)
```
3. 您使用的数据集是？/What dataset did you use?
    coco格式的数据集
4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.
```
    File ""/data1/hxy/nfs/hxy/work/PaddleDetection/ppdet/modeling/backbones/parallel_net.py"", line 169, in forward
        y = paddle.empty((1, C + 1, W, H))
        for i in range(B):
            x = paddle.concat((paddle.unsqueeze(input[i], 0), atten), axis=1)
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
            y = paddle.concat((y,x), 0) if i > 0 else x
        x = self.cov(y)

    File ""/home/ubuntu/anaconda3/lib/python3.8/site-packages/paddle/tensor/manipulation.py"", line 345, in concat
        return paddle.fluid.layers.concat(input=x, axis=axis, name=name)
    File ""/home/ubuntu/anaconda3/lib/python3.8/site-packages/paddle/fluid/layers/tensor.py"", line 375, in concat
        helper.append_op(
    File ""/home/ubuntu/anaconda3/lib/python3.8/site-packages/paddle/fluid/layer_helper.py"", line 43, in append_op
        return self.main_program.current_block().append_op(*args, **kwargs)
    File ""/home/ubuntu/anaconda3/lib/python3.8/site-packages/paddle/fluid/framework.py"", line 3178, in append_op
        op = Operator(
    File ""/home/ubuntu/anaconda3/lib/python3.8/site-packages/paddle/fluid/framework.py"", line 2344, in __init__
        self.desc.infer_shape(self.block.desc)

    RuntimeError: (NotFound) No Input(X) found for Concat operator.
  [Hint: Expected ctx->HasInputs(""X"") == true, but received ctx->HasInputs(""X""):0 != true:1.] (at /paddle/paddle/fluid/operators/concat_op.cc:35)
  [operator < concat > error]
```
## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：
    2.20
2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：
    无
3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：
    ubuntu20.04
4. 请问您使用的Python版本是？/ Please provide the version of Python you used.
    python3.8
5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.
    11.1 8.0

如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
[BUG] PicoDet-lcnet，use_align_head=False时训练报错,PaddlePaddle/PaddleDetection,2022-05-12 07:01:20,2,,5953,1233535809,"Paddle版本： v2.4
改动地方：
`picodet_l_640_coco.yaml
PicoHeadV2下增加use_align_head: False
`
主要是sqrt算子部署时转换不了，报错：
`ValueError: (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 11253, 2] and the shape of Y = [8500, 2]. Received [11253] in X is not equal to [8500] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || x_dims_array[i] <= 1 || y_dims_array[i] <= 1 == true, but received x_dims_array[i] == y_dims_array[i] || x_dims_array[i] <= 1 || y_dims_array[i] <= 1:0 != true:1.] (at /paddle/paddle/fluid/operators/elementwise/elementwise_op_function.h:240)
  [operator < elementwise_add > error]
`"
GPU运行代码后既没有报错产生，程序也不继续运行，ctrl+c也退出不了，只能用kill杀死进程，请问会是什么原因？,PaddlePaddle/PaddleDetection,2022-05-12 05:46:39,2,,5952,1233475627,"GPU运行代码后既没有报错产生，程序也不继续运行，ctrl+c也退出不了，只能用kill杀死进程，请问会是什么原因？

centos7
cuda10.2
cudnn7.6.5
3080的显卡



用的是picodet里面的infer.py 在别的机器上都可以


卡在了这一行
results = detector.predict([frame], FLAGS.threshold)"
yolov3 mobilenetv3 和 ppyolo-tiny有啥区别？,PaddlePaddle/PaddleDetection,2022-05-11 11:38:26,2,,5946,1232486960,"![image](https://user-images.githubusercontent.com/88072901/167840816-2a75f3c8-90e6-4d12-b13f-d4abd162e6cb.png)
我看这个 结构 感觉一样 啊 也就是fpn的通道数 scale 不一样 还有head loss用的不一样没看出来太大变化 感觉结构一样就是 参数做了调整似的"
运行代码后既没有报错产生，程序也不继续运行，ctrl+c也退出不了，只能用kill杀死进程,PaddlePaddle/PaddleDetection,2022-05-11 02:22:42,2,,5934,1231913981,"GPU运行代码后既没有报错产生，程序也不继续运行，ctrl+c也退出不了，只能用kill杀死进程，请问会是什么原因？

centos7
cuda10.2
cudnn7.6.5
3080的显卡"
ShuffleNetV2 结构配置小错误[BUG],PaddlePaddle/PaddleDetection,2022-05-10 10:13:39,7,,5928,1230929950,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.
3. 确认bug是否在新版本里还未修复/The bug has not been fixed in the latest version.
最新代码 https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.4/ppdet/modeling/backbones/shufflenet_v2.py

## 描述问题/Describe the bug
A clear and concise description of what the bug is.
shufflenet_v2.py  191行：
            stage_out_channels = [-1, 24, 2**2**4, 488, 976, 2048]
是否应为
            stage_out_channels = [-1, 24, 244, 488, 976, 2048]
参考其它实现。

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?

```none
请填写命令/A placeholder for the command.
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

3. 您使用的数据集是？/What dataset did you use?

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：

4. 请问您使用的Python版本是？/ Please provide the version of Python you used.

5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.


如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
请问pphuman的摔倒识别如何部署？,PaddlePaddle/PaddleDetection,2022-05-10 08:10:49,4,,5920,1230781744,"目前的想法是通过paddle-serving部署。

paddle-serving部署时支持pipeline模式，可以通过定义op来进行多个模型的串联。

但是将模型转换为paddle-serving支持的模型时，目标检测与追踪的模型（pp-yoloe）输出包含两个结果，维度与关键点检测模型（HRNet）的输入并不匹配。并且最终需要通过stgcn来判断是否摔倒，而stgcn需要50帧图像才能判断。

请问有关于pp-human部署的教程吗？

提前感谢您的回答。
"
[BUG] Picodet_xs 对于1280*720 或者1920*1080预测效果很差,PaddlePaddle/PaddleDetection,2022-05-09 10:35:38,2,,5910,1229508925,"训练的时候采用的320*320 的输入，图片训练时大小基本是640*460，460*640，648*352等这种比例形式，还有少部分1280*720的图片，但是预测是有些是1280*1920 和1920*1080的预测，在这种情况下推理预测效果，出现了明显的漏检和误检。

因为是采用红外的数据YUV格式，彩色的比较少，这也造成了红外数据预测不到，彩色数据效果更差，我想知道的是训练过程中是自适应缩放比例，不同尺度嘛，还是说我要将模型输入改为16：9的比例输入，例如256*160输入训练等才能达到想要的效果。
还是说有其他的方法？"
SSD剪枝问题 ,PaddlePaddle/PaddleDetection,2022-05-06 08:57:14,6,model compression,5893,1227597050,"
![166101578-9dd102a7-2bfb-46de-b841-90874148fdbd](https://user-images.githubusercontent.com/104298741/167100179-e311d0f6-356b-43a5-9b3f-b6f37198bfed.png)

为什么这个的pruned ratio 只有0.045左右啊，yolov3的这个都可以达到30%几。这个配置文件该怎么改呢？按照yolo的数据，最后的pruned ratio只有0.005，而改变参数和层数最多只有0.045，请问如何更改此配置文件使得在训练ssd时的pruned ratio达到0.1以上?
![167069608-86d7a5cd-85e3-4e64-9c49-a35f7d082b50](https://user-images.githubusercontent.com/104298741/167100458-b3357888-6ddd-4a0a-aa9f-ffef93d411ff.png)

"
About speed test,PaddlePaddle/PaddleDetection,2022-05-06 05:53:14,2,question,5889,1227447967,"First, I can reproduce the speed (4.8 ms) of ppyoloe-s model on V100 with fp32 precision and batch_size=1. 

Then, I try to test the speed of ppyoloe-s model on V100 with fp32 precision and batch_size=32, and the results show that the latency of per image is 2.2 ms. Is my testing results right?

"
PicoDet 使用COCOJson格式训练出现问题,PaddlePaddle/PaddleDetection,2022-05-06 02:51:11,2,custom dataset,5888,1227360518,"在picodet使用coco数据训练，出现了KeyError: 'pad_gt_mask'，在ppyolo中训练时并没有出现，我想知道的是我json标注数据缺少的某一部分嘛

[新建 文本文档 (2).txt](https://github.com/PaddlePaddle/PaddleDetection/files/8637259/2.txt)

voc数据也是可以正常训练的



这个问题怎么解决？"
训练时会卡住,PaddlePaddle/PaddleDetection,2022-05-04 12:42:51,5,training#PP-PicoDet,5879,1225318042,"训练了3次，一次卡在第3个epoch，一次卡在第13个epcoh，第三次卡在第15个epoch，卡住不动，1天都不动，手动ctrl c出现如下提示：

SystemError: (Fatal) Blocking queue is killed because the data reader raises an exception.
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] (at /paddle/paddle/fluid/operators/reader/blocking_queue.h:166)

补充：使用的是官网下载的coco原始数据，训练命令：
python tools/train.py -c configs/picodet/picodet_s_320_coco_lcnet.yml --eval
除了下载放置数据，没做过什么改动，请问有什么建议吗
"
 Bus error (core dumped),PaddlePaddle/PaddleDetection,2022-05-03 07:25:14,6,environment#training,5878,1223769747,"训练ppyoloe时，遇到错误
cuda10.2 paddle2.2

```
--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::imperative::Tracer::TraceOp(std::string const&, paddle::imperative::NameVarBaseMap const&, paddle::imperative::NameVarBaseMap const&, paddle::framework::AttributeMap, paddle::platform::Place const&, bool, std::map<std::string, std::string, std::less<std::string >, std::allocator<std::pair<std::string const, std::string > > > const&)
1   paddle::imperative::PreparedOp::Prepare(paddle::imperative::NameVarBaseMap const&, paddle::imperative::NameVarBaseMap const&, paddle::framework::OperatorWithKernel const&, paddle::platform::Place const&, paddle::framework::AttributeMap const&, paddle::framework::AttributeMap const&)
2   paddle::imperative::PreparedOp paddle::imperative::PrepareImpl<paddle::imperative::VarBase>(paddle::imperative::details::NameVarMapTrait<paddle::imperative::VarBase>::Type const&, paddle::imperative::details::NameVarMapTrait<paddle::imperative::VarBase>::Type const&, paddle::framework::OperatorWithKernel const&, paddle::platform::Place const&, paddle::framework::AttributeMap const&, paddle::framework::AttributeMap const&)
3   paddle::platform::DeviceContextPool::Get(paddle::platform::Place const&)
4   std::__future_base::_Deferred_state<std::_Bind_simple<paddle::platform::EmplaceDeviceContext<paddle::platform::CUDADeviceContext, paddle::platform::CUDAPlace>(std::map<paddle::platform::Place, std::shared_future<std::unique_ptr<paddle::platform::DeviceContext, std::default_delete<paddle::platform::DeviceContext> > >, std::less<paddle::platform::Place>, std::allocator<std::pair<paddle::platform::Place const, std::shared_future<std::unique_ptr<paddle::platform::DeviceContext, std::default_delete<paddle::platform::DeviceContext> > > > > >*, paddle::platform::Place)::{lambda()#1} ()>, std::unique_ptr<paddle::platform::DeviceContext, std::default_delete<paddle::platform::DeviceContext> > >::_M_complete_async()
5   std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
6   std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<std::unique_ptr<paddle::platform::DeviceContext, std::default_delete<paddle::platform::DeviceContext> > >, std::__future_base::_Result_base::_Deleter>, std::_Bind_simple<paddle::platform::EmplaceDeviceContext<paddle::platform::CUDADeviceContext, paddle::platform::CUDAPlace>(std::map<paddle::platform::Place, std::shared_future<std::unique_ptr<paddle::platform::DeviceContext, std::default_delete<paddle::platform::DeviceContext> > >, std::less<paddle::platform::Place>, std::allocator<std::pair<paddle::platform::Place const, std::shared_future<std::unique_ptr<paddle::platform::DeviceContext, std::default_delete<paddle::platform::DeviceContext> > > > > >*, paddle::platform::Place)::{lambda()#1} ()>, std::unique_ptr<paddle::platform::DeviceContext, std::default_delete<paddle::platform::DeviceContext> > > >::_M_invoke(std::_Any_data const&)
7   paddle::platform::CUDADeviceContext::CUDADeviceContext(paddle::platform::CUDAPlace)
8   paddle::platform::CUDAContext::CUDAContext(paddle::platform::CUDAPlace const&, paddle::platform::stream::Priority const&, paddle::platform::stream::StreamFlag const&)
9   paddle::platform::CUDAContext::InitCuSolverContext()
10  cusolverDnCreate

----------------------
Error Message Summary:
----------------------
FatalError: `Access to an undefined portion of a memory object` is detected by the operating system.
  [TimeInfo: *** Aborted at 1651560573 (unix time) try ""date -d @1651560573"" if you are using GNU date ***]
  [SignalInfo: *** SIGBUS (@0x7ff8d19f2000) received by PID 19918 (TID 0x7ffa49643700) from PID 18446744072931450880 ***]

Bus error (core dumped)
```

"
为什么picodet在backbone上用SE注意力模块却不用CA注意力模块？,PaddlePaddle/PaddleDetection,2022-04-28 08:30:31,3,question#algorithm,5855,1218384939,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.
专家您好，为什么picodet在backbone上用SE注意力模块却不用CA注意力模块？
![image](https://user-images.githubusercontent.com/78945582/165711369-bb67e133-ee5c-4909-a7d1-8f40a37dc3fd.png)

"
使用paddle inference 2.3rc仍然重复生成trt_serialized文件,PaddlePaddle/PaddleDetection,2022-04-28 07:13:07,5,,5852,1218303132,听说2.3rc已经解决了重复生成trt_serialized文件的问题，但我的使用中仍然会不断重复地生成engine文件，用的是detection中deploy的例程，请问会是什么原因呢？麻烦提供解决方案。
[BUG],PaddlePaddle/PaddleDetection,2022-04-26 16:42:02,1,,5837,1216217103,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.
3. 确认bug是否在新版本里还未修复/The bug has not been fixed in the latest version.

## 描述问题/Describe the bug
当我在使用pp yoloe这个模型训练时导出的模型在使用deploy预测时会报错显示No object detected，在查看源码后得知，其infer支持的模型不包括上述模型，但是在官方示例中所给出的导出模型以及推理的方案都是如此，在遵循官方示例操作后仍然有问题，所以希望能够得到准确的答复，谢谢各位大佬

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?

```
!CUDA_VISIBLE_DEVICES=0 python deploy/python/infer.py --model_dir=/home/aistudio/PaddleDetection/output_inference/ppyoloe_crn_l_300e_coco --image_file=demo/road554.png --device=gpu
请填写命令/A placeholder for the command.
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

 没有
3. 您使用的数据集是？/What dataset did you use?
官方示例数据集
4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.
```
-----------  Running Arguments -----------
action_file: None
batch_size: 1
camera_id: -1
cpu_threads: 1
device: gpu
enable_mkldnn: False
enable_mkldnn_bfloat16: False
image_dir: None
image_file: demo/road554.png
model_dir: /home/aistudio/PaddleDetection/output_inference/ppyoloe_crn_l_300e_coco
output_dir: output
random_pad: False
reid_batch_size: 50
reid_model_dir: None
run_benchmark: False
run_mode: paddle
save_images: False
save_mot_txt_per_img: False
save_mot_txts: False
scaled: False
threshold: 0.5
tracker_config: None
trt_calib_mode: False
trt_max_shape: 1280
trt_min_shape: 1
trt_opt_shape: 640
use_dark: True
use_gpu: False
video_file: None
window_size: 50
------------------------------------------
-----------  Model Configuration -----------
Model Arch: YOLO
Transform Order: 
--transform op: Resize
--transform op: NormalizeImage
--transform op: Permute
--------------------------------------------
[WARNNING] No object detected.
save result to: output/road554.png
Test iter 0
------------------ Inference Time Info ----------------------
total_time(ms): 2530.5, img_num: 1
average latency time(ms): 2530.50, QPS: 0.395179
preprocess_time(ms): 2508.00, inference_time(ms): 22.30, postprocess_time(ms): 0.20
```
## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：
!git clone https://gitee.com/paddlepaddle/PaddleDetection.git -b release/2.4
2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：
没有
3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：
AI studio
4. 请问您使用的Python版本是？/ Please provide the version of Python you used.
3.7
5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.
7.6

如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
[BUG]卡退Process finished with exit code -1073741819 (0xC0000005),PaddlePaddle/PaddleDetection,2022-04-24 09:08:45,1,,5822,1213594798,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.
3. 确认bug是否在新版本里还未修复/The bug has not been fixed in the latest version.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?

```none
请填写命令/A placeholder for the command.
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

3. 您使用的数据集是？/What dataset did you use?

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：

4. 请问您使用的Python版本是？/ Please provide the version of Python you used.

5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.


如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
在windows下debug 会在这个地方卡退，然后报那个错，函数能够执行完，但是不能返回值。
![image](https://user-images.githubusercontent.com/49071179/164969036-6c82219f-81ec-44de-a5be-bd3c23fd2869.png)




"
PP-Tracking训练时进行eval报错,PaddlePaddle/PaddleDetection,2022-04-24 08:50:49,3,,5821,1213590677,"运行命令：
CUDA_VISIBLE_DEVICES=3 python tools/train.py --config configs/mot/vehicle/fairmot_hrnetv2_w18_dlafpn_30e_576x320_bdd100kmot_vehicle.yml --use_vdl=True --vdl_log_dir=output/vdl_dir/scalar --eval

跑完一个epoch以后报错：
[04/24 08:52:26] ppdet.data.source.mot INFO: MOT dataset summary: 
[04/24 08:52:26] ppdet.data.source.mot INFO: OrderedDict([('car1.train', 33)])
[04/24 08:52:26] ppdet.data.source.mot INFO: Total images: 100
[04/24 08:52:26] ppdet.data.source.mot INFO: Image start index: OrderedDict([('car1.train', 0)])
[04/24 08:52:26] ppdet.data.source.mot INFO: Total identities: 34
[04/24 08:52:26] ppdet.data.source.mot INFO: Identity start index: OrderedDict([('car1.train', 0)])
[04/24 08:52:26] reader WARNING: Shared memory size is less than 1G, disable shared_memory in DataLoader
W0424 08:52:27.036528 22457 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.2, Runtime API Version: 10.2
W0424 08:52:27.043077 22457 device_context.cc:465] device: 0, cuDNN Version: 7.6.
[04/24 08:52:35] ppdet.utils.checkpoint INFO: Finish loading model weights: /root/.cache/paddle/weights/HRNet_W18_C_pretrained.pdparams
https://paddledet.bj.bcebos.com/models/pretrained/HRNet_W18_C_pretrained.pdparams
[04/24 08:52:35] ppdet.engine INFO: Epoch: [0] [0/8] learning_rate: 0.000000 loss: 15.519042 heatmap_loss: 2.984292 size_loss: 2.176330 offset_loss: 0.449540 det_loss: 3.651464 reid_loss: 3.749733 eta: 0:00:30 batch_cost: 0.7746 data_cost: 0.0004 ips: 15.4910 images/s
[04/24 08:52:46] ppdet.utils.checkpoint INFO: Save checkpoint: output/save_model_output
Traceback (most recent call last):
  File ""tools/train.py"", line 260, in <module>
    main()
  File ""tools/train.py"", line 254, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 192, in run
    trainer.train(FLAGS.eval)
  File ""/home/nfs/syj/PaddleDetection/ppdet/engine/trainer.py"", line 453, in train
    self._eval_dataset = self.cfg.EvalDataset
  File ""/home/nfs/syj/PaddleDetection/ppdet/core/workspace.py"", line 68, in __getattr__
    raise AttributeError(""object has no attribute '{}'"".format(key))
AttributeError: object has no attribute 'EvalDataset'"
yolov4 敏感度分析，如何确定分析的网络层,PaddlePaddle/PaddleDetection,2022-04-24 08:36:40,0,,5820,1213587650,"敏感度分析时，参数-pruned_params如何确定 ，是选择所有卷积层吗,-pruned_ratios 如何确定"
[k200 麒麟并行问题]AttributeError: module 'paddle.fluid.core_noavx' has no attribute 'BKCLParallelContext',PaddlePaddle/PaddleDetection,2022-04-22 08:15:37,2,,5807,1211997989,"#python3 -m paddle.distributed.launch --log_dir=./ppyolo_dygraph/ --xpus 0,1 tools/train.py -c configs/ppyolo/ppyolo_r50vd_dcn_1x_coco.yml
XPURT /usr/local/lib64/python3.7/site-packages/paddle/fluid/../libs/libxpurt.so loaded
/usr/lib/python3.7/site-packages/setuptools/depends.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/local/lib64/python3.7/site-packages/paddle/vision/transforms/functional_pil.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': Image.NEAREST,
/usr/local/lib64/python3.7/site-packages/paddle/vision/transforms/functional_pil.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': Image.BILINEAR,
/usr/local/lib64/python3.7/site-packages/paddle/vision/transforms/functional_pil.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': Image.BICUBIC,
/usr/local/lib64/python3.7/site-packages/paddle/vision/transforms/functional_pil.py:39: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  'box': Image.BOX,
/usr/local/lib64/python3.7/site-packages/paddle/vision/transforms/functional_pil.py:40: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  'lanczos': Image.LANCZOS,
/usr/local/lib64/python3.7/site-packages/paddle/vision/transforms/functional_pil.py:41: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  'hamming': Image.HAMMING
-----------  Configuration Arguments -----------
heter_worker_num: None
heter_workers: 
http_port: None
ips: 127.0.0.1
log_dir: ./ppyolo_dygraph/
nproc_per_node: None
run_mode: None
server_num: None
servers: 
training_script: tools/train.py
training_script_args: ['-c', 'configs/ppyolo/ppyolo_r50vd_dcn_1x_coco.yml']
worker_num: None
workers: 
xpus: 0,1
------------------------------------------------
WARNING 2022-04-22 16:04:55,019 launch.py:359] Not found distinct arguments and compiled with cuda or xpu. Default use collective mode
launch train in XPU mode
INFO 2022-04-22 16:04:55,037 launch_utils.py:510] Local start 2 processes. First process distributed environment info (Only For Debug): 
    +=======================================================================================+
    |                        Distributed Envs                      Value                    |
    +---------------------------------------------------------------------------------------+
    |                       PADDLE_TRAINER_ID                        0                      |
    |                 PADDLE_CURRENT_ENDPOINT                 127.0.0.1:51941               |
    |                     PADDLE_TRAINERS_NUM                        2                      |
    |                PADDLE_TRAINER_ENDPOINTS         127.0.0.1:51941,127.0.0.1:38935       |
    |                     PADDLE_RANK_IN_NODE                        0                      |
    |                 PADDLE_LOCAL_DEVICE_IDS                        0                      |
    |                 PADDLE_WORLD_DEVICE_IDS                       0,1                     |
    |             FLAGS_selected_accelerators                        0                      |
    |                     FLAGS_selected_xpus                        0                      |
    +=======================================================================================+

INFO 2022-04-22 16:04:55,038 launch_utils.py:514] details abouts PADDLE_TRAINER_ENDPOINTS can be found in ./ppyolo_dygraph//endpoints.log, and detail running logs maybe found in ./ppyolo_dygraph//workerlog.0
launch proc_id:4129 idx:0
launch proc_id:4132 idx:1
XPURT /usr/local/lib64/python3.7/site-packages/paddle/fluid/../libs/libxpurt.so loaded
/usr/lib/python3.7/site-packages/setuptools/depends.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/usr/local/lib64/python3.7/site-packages/paddle/vision/transforms/functional_pil.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': Image.NEAREST,
/usr/local/lib64/python3.7/site-packages/paddle/vision/transforms/functional_pil.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': Image.BILINEAR,
/usr/local/lib64/python3.7/site-packages/paddle/vision/transforms/functional_pil.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': Image.BICUBIC,
/usr/local/lib64/python3.7/site-packages/paddle/vision/transforms/functional_pil.py:39: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  'box': Image.BOX,
/usr/local/lib64/python3.7/site-packages/paddle/vision/transforms/functional_pil.py:40: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  'lanczos': Image.LANCZOS,
/usr/local/lib64/python3.7/site-packages/paddle/vision/transforms/functional_pil.py:41: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  'hamming': Image.HAMMING
/usr/local/lib64/python3.7/site-packages/paddle/tensor/creation.py:125: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if data.dtype == np.object:
Traceback (most recent call last):
  File ""tools/train.py"", line 177, in <module>
    main()
  File ""tools/train.py"", line 173, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 112, in run
    init_parallel_env()
  File ""/workspace/PaddleDetection/ppdet/engine/env.py"", line 44, in init_parallel_env
    paddle.distributed.init_parallel_env()
  File ""/usr/local/lib64/python3.7/site-packages/paddle/distributed/parallel.py"", line 195, in init_parallel_env
    core.BKCLParallelContext(strategy, place))
AttributeError: module 'paddle.fluid.core_noavx' has no attribute 'BKCLParallelContext'"
[Other General Issues]安装过程中出现的问题，是不是可能和cuda版本有关？,PaddlePaddle/PaddleDetection,2022-04-21 14:38:00,3,,5795,1211139293,"```
1 22:18:47.915304 58190 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 11.6, Runtime API Version: 10.2
W0421 22:18:47.915446 58190 dynamic_loader.cc:287] The third-party dynamic library (libcudnn.so) that Paddle depends on is not configured correctly. (error code is /usr/local/cuda/lib64/libcudnn.so: cannot open shared object file: No such file or directory)
  Suggestions:
  1. Check if the third-party dynamic library (e.g. CUDA, CUDNN) is installed correctly and its version is matched with paddlepaddle you installed.
  2. Configure third-party dynamic library environment variables as follows:
  - Linux: set LD_LIBRARY_PATH by `export LD_LIBRARY_PATH=...`
  - Windows: set PATH by `set PATH=XXX;

```
paddle.utils.run_check()后出现的上面的错误


本人系统ubuntu20.04 显卡是3060 是用的Conda的虚拟环境 cudnn是7.6.5 cudatoolkit=10.2 系统的cuda版本是11.6，是不是和cuda版本对不上有关呢
`conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch`"
[Other General Issues]PaddleDetection有没有评估敏感度特异性准确率ROC曲线等指标的评估,PaddlePaddle/PaddleDetection,2022-04-20 07:33:38,5,,5770,1209279588,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?

```none
请填写命令/A placeholder for the command.
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

3. 您使用的数据集是？/What dataset did you use?

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：

4. 请问您使用的Python版本是？/ Please provide the version of Python you used.

5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.


如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
能否对test_feed.dataset进行清空或是获取 loader 随机读取的图片路径,PaddlePaddle/PaddleDetection,2022-04-19 07:03:36,6,,5754,1207855454,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.

## 复现/Reproduction

1. 您使用的命令是？
python -u tools/infer_paddle.py -c configs/yolov3_mobilenet_v1_caih_car_220413.yml \
                    -o weights=output/yolov3_mobilenet_v1_caih_car_220413/best_model\
                    --infer_dir=dataset/caih_images/img/ \
                    --output_dir=infer_output/yolov3_mobilenet_v1_caih_car_220413/caih_images \
                    --save_txt=true \
                    --save_dir=save_img/ \


```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。**提供于后文**

3. 您使用的数据集是？自制数据集

4. 请提供您出现的报错信息及相关log。后文

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号 ：Paddle1.6   PaddleDetection0.1

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号:无

3. 请提供您使用的操作系统信息 ：   Linux

4. 请问您使用的Python版本是？

5. 请问您使用的CUDA/cuDNN的版本号是？


目前我们正在使用paddle1.6版本下的paddledetection-0.1的代码进行yolov3网络的检测。目前出现这样一个问题，0.1版的代码提供的infer.py使用的test_feed.dataset存储图片路径，在使用loader读取图片时是随机读取，但是由于项目的一些要求我们需要对图片进行按顺序的读取。

我们对infer.py进行了部分修改以适应自身项目需求

### 1、[将需要按一定的顺序的图片一个一个输入数据集中：此处展示的是输入一张]
croped_img = img[y_start:y_stop, x_start:x_stop, 0:3]
save = FLAGS.save_dir + img_rename + '_' + str(slice_idx) + '.jpg'
cv2.imwrite(save, croped_img)
test_images = []
test_images.append(save)
test_feed.dataset.add_images(test_images)
place = fluid.CUDAPlace(0) if cfg.use_gpu else fluid.CPUPlace()
exe = fluid.Executor(place)

### 2、[此处基本来自infer.py 初始化loader以读取数据]
startup_prog = fluid.Program()
                infer_prog = fluid.Program()
                with fluid.program_guard(infer_prog, startup_prog):
                    with fluid.unique_name.guard():
                        loader, feed_vars = create_feed(test_feed, iterable=False)
                        test_fetches = model.test(feed_vars)

 infer_prog = infer_prog.clone(True)
reader = create_reader(test_feed)
imid2path = reader.imid2path
loader.set_sample_list_generator(reader, place)
exe.run(startup_prog)
if cfg.weights:
        checkpoint.load_params(exe, infer_prog, cfg.weights)

### **3、[问题出在此处：我们在运行代码时发现loader对test_feed.dataset里的图片进行随机读取，同时由于无法对dataset进行清空，所以无法实现按输入顺序预测图片。我们对loader设置为不可迭代式，使得它可以一次性读取一张图片，但这张图片仍然是随机的]**

loader.start()
outs = exe.run(infer_prog,
                        fetch_list=values,
                         return_numpy=False)
res = {
            k: (np.array(v), v.recursive_sequence_lengths())
            for k, v in zip(keys, outs)
          }
 loader.reset()


所以想请问一下test_feed.dataset这个有没有清空这个功能，这样就能实现每次读取的都是刚输入的图片；又或者是否有办法获取loader读取的图片的名称呢？"
[BUG]dark postprocess container overflow,PaddlePaddle/PaddleDetection,2022-04-19 05:27:41,0,,5751,1207765171,dark postprocess vector container overflow
"ValueError: (InvalidArgument) Input(X) of GridSampleOp should be 4-D Tensor, but received X dimension size(5)[Other General Issues]",PaddlePaddle/PaddleDetection,2022-04-19 03:33:50,9,,5747,1207672338,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## 描述问题/Describe the bug
A clear and concise description of what the bug is.
訓練mask_rcnn_res2net50_vd_26w_4s_fpn_2x_coco時一切正常，但是在eval.py 和 infer.py時出現這個錯誤

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?
   python tools/eval.py -c configs/res2net/mask_rcnn_res2net50_vd_26w_4s_fpn_2x_coco.yml -o weights=output/mask_rcnn_res2net50_vd_26w_4s_fpn_2x_coco/model_final.pdparams --classwise

   python tools/infer.py -c configs/res2net/mask_rcnn_res2net50_vd_26w_4s_fpn_2x_coco.yml --infer_dir=ship_test_img/ --output_dir=infer_output/ -o weights=output/mask_rcnn_res2net50_vd_26w_4s_fpn_2x_coco/model_final.pdparams --save_txt=Ture

2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.
無

3. 您使用的数据集是？/What dataset did you use?
自製的COCO格式數據集，只有一類，包含polygon 和 bbox。

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.
![image](https://user-images.githubusercontent.com/58526756/163914662-ee63d2c0-9b0d-4527-b5fb-aaf7210aa5d2.png)


## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：

   paddledet            2.4.0
   paddlepaddle-gpu     2.2.2

3. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：
無

4. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：
Windows + conda

5. 请问您使用的Python版本是？/ Please provide the version of Python you used.
3.7.13

6. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.
CUDA 11.4
cuDNN 7.6
"
[BUG]win10安装requirements出现如图所示错误，麻烦专家帮忙解决一下,PaddlePaddle/PaddleDetection,2022-04-19 01:07:50,2,,5744,1207599574,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.
3. 确认bug是否在新版本里还未修复/The bug has not been fixed in the latest version.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.
win10安装requirements出现如图所示错误
![image](https://user-images.githubusercontent.com/78945582/163900184-c65a7e97-5bd0-4f30-a0fd-8424166b53ce.png)

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?
pip install -r requirements.txt
```none
请填写命令/A placeholder for the command.
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

3. 您使用的数据集是？/What dataset did you use?

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：

4. 请问您使用的Python版本是？/ Please provide the version of Python you used.

5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.


如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
[BUG] 训练ppyoloE时，VOC数据，只采用单个分类，那么在loss.backward()的时候会卡住不动，查看了loss是4.49，不报错。,PaddlePaddle/PaddleDetection,2022-04-18 09:13:27,15,training,5738,1206849426,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.
3. 确认bug是否在新版本里还未修复/The bug has not been fixed in the latest version.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.

## 复现/Reproduction



1. 您使用的命令是？/What command or script did you run?

```none
请填写命令/A placeholder for the command.
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

3. 您使用的数据集是？/What dataset did you use?

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：

4. 请问您使用的Python版本是？/ Please provide the version of Python you used.

5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.


如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
🚀  PP-YOLOE高精度云边一体模型已经发布，欢迎大家试用讨论,PaddlePaddle/PaddleDetection,2022-04-18 03:54:01,31,,5730,1206626803,"🚀  PP-YOLOE是基于PP-YOLOv2的卓越的单阶段Anchor-free模型，超越了多种流行的yolo模型，更多细节可以参考[**PP-YOLOE文档**](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.4/configs/ppyoloe/README_cn.md)以及技术报告[**PP-YOLOE论文**](https://arxiv.org/abs/2203.16250)。概括的说，PP-YOLOE具有以下特点：
1. **精度速度双高**
   - PP-YOLOE-l在COCO test-dev上精度可达**51.4 mAP**，速度可达**149FPS**
   - 精度速度性价比超越**YOLOv5, YOLOX**
2. **部署友好**
   - 具有**s/m/l/x**一系列的模型，可以适配不同算力的硬件，**V100，T4等服务端GPU**以及**Jetson系列和FPGA等边缘端芯片**
   - 避免使用DCN, Matrix NMS等特殊算子，方便部署到更多的硬件上
   - 支持**TensorRT部署**以及**ONNX导出**，提升部署易用性
3. **训练速度大幅提升**
   - 混合精度训练等优化大幅提升训练速度，相较于PP-YOLOv2**训练速度提升30%以上**
<img width=""500"" alt=""ppyoloe_map_fps"" src=""https://user-images.githubusercontent.com/69842442/165039851-f5d3eed0-0939-4a38-8ffc-7ddd7c801413.png"">
为了方便大家相互交流沟通，欢迎大家扫码加入微信群，相关意见、建议和使用中的疑问可以在微信群中交流或者在github中提issue

<img width=""400"" alt=""wechat"" src=""https://user-images.githubusercontent.com/69842442/177932227-30022865-fff3-4971-84a2-fdf06ec86b71.jpg"">
"
[Other General Issues]PaddleDetection使用TensorRT选项，对进行yolov3_mobilenet_v1_qat推理时报错,PaddlePaddle/PaddleDetection,2022-04-18 01:50:44,6,model compression,5726,1206568400,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.

## 描述问题/Describe the bug
按照官方教程，在VOC数据集上训练了一个yolov3_mobilenet_v1_270e_voc模型，量化config使用默认的yolov3_mobilenet_v1_qat.yml。导出为paddleserving模型后，不开启--use_trt选项时可以正常运行和检测，但无加速和压缩效果。开启--use_trt时报此错误。

在PaddleServing群里咨询管理后，管理称这是模型问题，建议联系PaddleDetection。

另外，用benchmark脚本测试时也同样报TensorRT相关错误，无法测试trt_fp32、trt_fp16和trt_int8，但use_cpu、use_gpu可以测试。

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?
paddleserving命令：
```
cd /home/ubuntu/lxd-storage/xzy/PaddleCV/PaddleDetection/inference_model/yolov3_mobilenet_v1_270e_qat_pdserving/yolov3_mobilenet_v1_qat
python -m paddle_serving_server.serve --model serving_server --port 9393 --gpu_ids 0 --precision int8 --use_trt
```
benchmark测试命令：
```
bash deploy/benchmark/benchmark.sh ./inference_model/yolov3_mobilenet_v1_270e_voc_origin model
bash deploy/benchmark/benchmark_quant.sh ./inference_model/yolov3_mobilenet_v1_270e_qat/yolov3_mobilenet_v1_qat model
```

2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

否

4. 您使用的数据集是？/What dataset did you use?

VOC数据集

5. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.
paddleserving报错信息和log：
```
/home/ubuntu/miniconda3/envs/paddle_env/lib/python3.7/runpy.py:125: RuntimeWarning: 'paddle_serving_server.serve' found in sys.modules after import of package 'paddle_serving_server', but prior to execution of 'paddle_serving_server.serve'; this may result in unpredictable behaviour
  warn(RuntimeWarning(msg))
Going to Run Comand
/home/ubuntu/miniconda3/envs/paddle_env/lib/python3.7/site-packages/paddle_serving_server/serving-gpu-101-0.8.3/serving -enable_model_toolkit -inferservice_path workdir_9393 -inferservice_file infer_service.prototxt -max_concurrency 0 -num_threads 4 -port 9393 -precision int8 -use_calib=False -reload_interval_s 10 -resource_path workdir_9393 -resource_file resource.prototxt -workflow_path workdir_9393 -workflow_file workflow.prototxt -bthread_concurrency 4 -max_body_size 536870912
I0100 00:00:00.000000 11926 op_repository.h:68] RAW: Succ regist op: GeneralDistKVInferOp
I0100 00:00:00.000000 11926 op_repository.h:68] RAW: Succ regist op: GeneralDistKVQuantInferOp
I0100 00:00:00.000000 11926 op_repository.h:68] RAW: Succ regist op: GeneralInferOp
I0100 00:00:00.000000 11926 op_repository.h:68] RAW: Succ regist op: GeneralReaderOp
I0100 00:00:00.000000 11926 op_repository.h:68] RAW: Succ regist op: GeneralRecOp
I0100 00:00:00.000000 11926 op_repository.h:68] RAW: Succ regist op: GeneralResponseOp
I0100 00:00:00.000000 11926 service_manager.h:79] RAW: Service[LoadGeneralModelService] insert successfully!
I0100 00:00:00.000000 11926 load_general_model_service.pb.h:333] RAW: Success regist service[LoadGeneralModelService][PN5baidu14paddle_serving9predictor26load_general_model_service27LoadGeneralModelServiceImplE]
I0100 00:00:00.000000 11926 service_manager.h:79] RAW: Service[GeneralModelService] insert successfully!
I0100 00:00:00.000000 11926 general_model_service.pb.h:1608] RAW: Success regist service[GeneralModelService][PN5baidu14paddle_serving9predictor13general_model23GeneralModelServiceImplE]
I0100 00:00:00.000000 11926 factory.h:155] RAW: Succ insert one factory, tag: PADDLE_INFER, base type N5baidu14paddle_serving9predictor11InferEngineE
W0100 00:00:00.000000 11926 paddle_engine.cpp:34] RAW: Succ regist factory: ::baidu::paddle_serving::predictor::FluidInferEngine<PaddleInferenceEngine>->::baidu::paddle_serving::predictor::InferEngine, tag: PADDLE_INFER in macro!
I0415 14:52:29.661229 11929 analysis_predictor.cc:576] TensorRT subgraph engine is enabled
--- Running analysis [ir_graph_build_pass]
--- Running analysis [ir_graph_clean_pass]
--- Running analysis [ir_analysis_pass]
--- Running IR pass [conv_affine_channel_fuse_pass]
--- Running IR pass [adaptive_pool2d_convert_global_pass]
--- Running IR pass [conv_eltwiseadd_affine_channel_fuse_pass]
--- Running IR pass [shuffle_channel_detect_pass]
--- Running IR pass [quant_conv2d_dequant_fuse_pass]
--- Running IR pass [delete_quant_dequant_op_pass]
I0415 14:52:29.881975 11929 fuse_pass_base.cc:57] ---  detected 47 subgraphs
--- Running IR pass [delete_quant_dequant_filter_op_pass]
I0415 14:52:29.944126 11929 fuse_pass_base.cc:57] ---  detected 47 subgraphs
--- Running IR pass [simplify_with_basic_ops_pass]
--- Running IR pass [embedding_eltwise_layernorm_fuse_pass]
--- Running IR pass [multihead_matmul_fuse_pass_v2]
--- Running IR pass [multihead_matmul_fuse_pass_v3]
--- Running IR pass [skip_layernorm_fuse_pass]
--- Running IR pass [unsqueeze2_eltwise_fuse_pass]
--- Running IR pass [squeeze2_matmul_fuse_pass]
--- Running IR pass [reshape2_matmul_fuse_pass]
--- Running IR pass [flatten2_matmul_fuse_pass]
--- Running IR pass [map_matmul_v2_to_mul_pass]
--- Running IR pass [map_matmul_v2_to_matmul_pass]
--- Running IR pass [map_matmul_to_mul_pass]
--- Running IR pass [fc_fuse_pass]
--- Running IR pass [conv_elementwise_add_fuse_pass]
--- Running IR pass [tensorrt_subgraph_pass]
I0415 14:52:30.001792 11929 tensorrt_subgraph_pass.cc:138] ---  detect a sub-graph with 145 nodes
I0415 14:52:30.034184 11929 tensorrt_subgraph_pass.cc:395] Prepare TRT engine (Optimize model structure, Select OP kernel etc). This process may cost a lot of time.
terminate called after throwing an instance of 'paddle::platform::EnforceNotMet'
  what():

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------

----------------------
Error Message Summary:
----------------------
UnimplementedError: no OpConverter for optype [nearest_interp_v2]
  [Hint: it should not be null.] (at /paddle/paddle/fluid/inference/tensorrt/convert/op_converter.h:142)

Aborted (core dumped)
```

Benchmark测试报错：
```
model_dir : ./inference_model/yolov3_mobilenet_v1_270e_qat/yolov3_mobilenet_v1_qat
img_dir: demo/fire_smoke_demo
model  ./inference_model/yolov3_mobilenet_v1_270e_qat/yolov3_mobilenet_v1_qat, run_mode: trt_int8
-----------  Running Arguments -----------
batch_size: 1
camera_id: -1
cpu_threads: 1
device: GPU
enable_mkldnn: False
image_dir: demo/fire_smoke_demo
image_file: None
model_dir: ./inference_model/yolov3_mobilenet_v1_270e_qat/yolov3_mobilenet_v1_qat
output_dir: output
reid_batch_size: 50
reid_model_dir: None
run_benchmark: True
run_mode: trt_int8
save_images: False
save_mot_txt_per_img: False
save_mot_txts: False
scaled: False
threshold: 0.5
trt_calib_mode: False
trt_max_shape: 1280
trt_min_shape: 1
trt_opt_shape: 640
use_dark: True
use_gpu: False
video_file: None
------------------------------------------
-----------  Model Configuration -----------
Model Arch: YOLO
Transform Order:
--transform op: Resize
--transform op: NormalizeImage
--transform op: Permute
--------------------------------------------
Traceback (most recent call last):
  File ""deploy/python/infer.py"", line 773, in <module>
    main()
  File ""deploy/python/infer.py"", line 726, in main
    enable_mkldnn=FLAGS.enable_mkldnn)
  File ""deploy/python/infer.py"", line 94, in __init__
    enable_mkldnn=enable_mkldnn)
  File ""deploy/python/infer.py"", line 563, in load_predictor
    predictor = create_predictor(config)
ValueError: (InvalidArgument) Pass tensorrt_subgraph_pass has not been registered. Please use the paddle inference library compiled with tensorrt or disable the tensorrt engine in inference configuration!
  [Hint: Expected Has(pass_type) == true, but received Has(pass_type):0 != true:1.] (at /paddle/paddle/fluid/framework/ir/pass.h:236)
```

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：
paddlepaddle-gpu=2.2.2.post101
paddledet=2.3.0

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：
paddleslim=2.2.2
paddle-serving-server-gpu=0.8.3.post101
tensorrt=6.0.1.5

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：
Ubuntu 16.04

4. 请问您使用的Python版本是？/ Please provide the version of Python you used.
Python 3.7

7. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.
CUDA 10.1"
Picodet更换注意力模块后mAP骤降,PaddlePaddle/PaddleDetection,2022-04-17 14:25:38,7,,5725,1206389397,"## 问题描述
在AIStudio上，搭配了paddlepaddle2.2，使用的picodet是backbone为esnet的版本，数据集是coco的验证集。

在esnet.py中增添了Coordinate Attention模块的代码，将原本的SEmodule更换为CAModule，进行训练，如图经过第一个10epoch后mAP比起原版本骤降

![p1](https://user-images.githubusercontent.com/53994773/163718297-29d669b9-61c5-4150-bbfc-d881958a55bc.png)
第一个10epoch原本第一个指标mAP(0.5, 0.95)应为约0.040，但这里仅有0.002。
更换注意力机制SEModule为CAModule应该效果会更好（根据Coordinate Attention论文中CA效果要比SE高出0.6%～0.7%指标）
![p2](https://user-images.githubusercontent.com/53994773/163718529-48d40254-b9dd-4063-838f-347bfc2eaacd.png)
但实际不增反而骤降

CA模块的代码为
```python
class CAModule(nn.Layer):
    def __init__(self, channel, groups=32):
        super(CAModule, self).__init__()
        self.pool_h = AdaptiveAvgPool2D((None, 1))
        self.pool_w = AdaptiveAvgPool2D((1, None))

        mip = max(8, channel // groups)

        self.conv1 = Conv2D(
            in_channels=channel,
            out_channels=mip,
            kernel_size=1,
            stride=1,
            padding=0
        )
        self.bn1 = nn.BatchNorm2D(mip)
        self.conv2 = Conv2D(
            in_channels=mip,
            out_channels=channel,
            kernel_size=1,
            stride=1,
            padding=0
        )
        self.conv3 = Conv2D(
            in_channels=mip,
            out_channels=channel,
            kernel_size=1,
            stride=1,
            padding=0
        )

    def forward(self, x):
        
        indentity = x
        n,c,h,w = x.shape
        x_h = self.pool_h(x)
        x_w = paddle.transpose(self.pool_w(x), (0, 1, 3, 2))

        y = paddle.concat([x_h, x_w], axis=2)
        y = self.conv1(y)
        y = self.bn1(y)
        y = F.hardswish(y)
        x_h, x_w = paddle.split(y, [h, w], axis=2)
        x_w = paddle.transpose(x_w, (0, 1, 3, 2))

        x_h = paddle.expand(F.hardsigmoid(self.conv2(x_h)), (-1, -1, h, w))
        x_w = paddle.expand(F.hardsigmoid(self.conv3(x_w)), (-1, -1, h, w))

        y = indentity * x_w * x_h

        return y
```
使用模块即把InvertedResidual和InvertedResidualDS中的SE均改为调用CA。

## 疑惑
请问这是为什么呢？应该怎么去解决这个问题，之前我也有把SEModule改为ECAModule（同为一种更优的注意力机制），但结果也是mAP骤降。训练时的loss是正常降低的，除了原先导入的预训练权重se部分没有导入外，没有其他警告信息"
🌟 PP-PicoDet发布增强版模型,PaddlePaddle/PaddleDetection,2022-04-17 09:54:53,13,,5723,1206332266,"https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/configs/picodet
## 升级点一：更高精度
- **Backbone**：选用LCNet，CPU与ARM等移动端硬件上更通用的网络，网络更加简洁，更多硬件支持。
- **FPN**：使用LC-PAN替换CSP-PAN，带来了1个点的精度收益；
- **Head**：引入SE和ETA head模块，精度提升0.5；
- **Label Assign**：使用TAL采样策略，精度提升0.5。

<div align=""center"">
  <img src=""https://user-images.githubusercontent.com/15628872/163708469-4f5d7437-5859-4ea3-aef7-5c652d73b649.png"" height=""250px"" >
</div>

## 升级点二：速度优化
- 训练速度：由于优化Label Assign等tensor操作，PicoDet-S模型在V100机器上4卡训练300epoch从48h缩短至24h左右，训练速度翻倍。
- CPU预测速度：由于网络更加精简，Intel CPU预测速度提升50%：

| 模型     | 输入尺寸 | ONNX  | 预测时延<sup><small>[CPU](#latency)|
| :-------- | :--------: | :---------------------: | :----------------: |
| PicoDet-XS |  320*320   | [( w/o 后处理)](https://paddledet.bj.bcebos.com/deploy/third_engine/picodet_xs_320_coco_lcnet.onnx) | 3.9ms |
| PicoDet-XS |  416*416   | [( w/o 后处理)](https://paddledet.bj.bcebos.com/deploy/third_engine/picodet_xs_416_coco_lcnet.onnx) | 6.1ms |
| PicoDet-S |  320*320   | [( w/o 后处理)](https://paddledet.bj.bcebos.com/deploy/third_engine/picodet_s_320_coco_lcnet.onnx) |     4.8ms |
| PicoDet-S |  416*416   |  [( w/o 后处理)](https://paddledet.bj.bcebos.com/deploy/third_engine/picodet_s_416_coco_lcnet.onnx) |     6.6ms |
| PicoDet-M |  320*320   | [( w/o 后处理)](https://paddledet.bj.bcebos.com/deploy/third_engine/picodet_m_320_coco_lcnet.onnx) | 8.2ms  |
| PicoDet-M |  416*416   |  [( w/o 后处理)](https://paddledet.bj.bcebos.com/deploy/third_engine/picodet_m_416_coco_lcnet.onnx) | 12.7ms |
| PicoDet-L |  320*320   | [( w/o 后处理)](https://paddledet.bj.bcebos.com/deploy/third_engine/picodet_l_320_coco_lcnet.onnx) | 11.5ms |
| PicoDet-L |  416*416   |  [( w/o 后处理)](https://paddledet.bj.bcebos.com/deploy/third_engine/picodet_l_416_coco_lcnet.onnx) |     20.7ms |
| PicoDet-L |  640*640   | [( w/o 后处理)](https://paddledet.bj.bcebos.com/deploy/third_engine/picodet_l_640_coco_lcnet.onnx) |     62.5ms |

- <a name=""latency"">测试环境：</a> 英特尔酷睿i7 10750H CPU。

## 升级点三：简化后处理
- 将后处理加入到网络中，部署时开发者无需二次开发；（现支持Paddle Lite、Paddle Inference、ONNXRuntime、OpenVINO）
- 优化后处理操作，后处理OP数从150+降到50，预测速度提升10%。
<div align=""center"">
  <img src=""https://user-images.githubusercontent.com/15628872/163708673-a1139d8d-fb46-4ff7-bf05-420a4f9d1577.png"" height=""200px""  width=""720"">
</div>

## 其他
- 支持量化训练，模型精度损失1个点之内，在低端机加速30%以上：https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/configs/picodet#%E9%87%8F%E5%8C%96

## Demo
| 预测库     | Python | C++  | 带后处理预测 |
| :-------- | :--------: | :---------------------: | :----------------: |
| OpenVINO | [Python](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/deploy/third_engine/demo_openvino/python) | [C++](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/deploy/third_engine/demo_openvino)（带后处理开发中） |  ✔︎ |
| Paddle Lite |  -    |  [C++](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/deploy/lite) | ✔︎ |
| Android Demo |  -  |  [Paddle Lite](https://github.com/PaddlePaddle/Paddle-Lite-Demo/tree/develop/object_detection/android/app/cxx/picodet_detection_demo) | ✔︎ |
| PaddleInference | [Python](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/deploy/python) |  [C++](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/deploy/cpp) | ✔︎ |
| ONNXRuntime  | [Python](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/deploy/third_engine/demo_onnxruntime) | Comming soon | ✔︎ |
| NCNN |  -  | [C++](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/deploy/third_engine/demo_ncnn) | ✘ |
| MNN  | - | [C++](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/deploy/third_engine/demo_mnn) |  ✘ |

"
"[BUG]转静态图报(InvalidArgument) end should greater than start, but received end = 5, start = 4.  求解决",PaddlePaddle/PaddleDetection,2022-04-16 16:10:42,2,,5720,1206153319,"通过PP-PicoDet 训练自己的数据集voc
对已经训练好的动态图转静态图，执行：python tools/export_model.py -c configs/picodet/legacy_model/picodet_s_320_voc.yml  -o weights=output/picodet_s_320_voc/best_model.pdparams TestReader.inputs_def.image_shape=[3,320,320] --output_dir output/picodet_s_320_voc

ValueError: In transformed code:

    File ""/tmp/tmpw2unlhke.py"", line 98, in forward
        false_fn_5, (), (inputs, self), (out,))
    File ""/home/user/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 210, in convert_ifelse
        return _run_py_ifelse(pred, true_fn, false_fn, true_args, false_args)
    File ""/home/user/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 235, in _run_py_ifelse
        return true_fn(*true_args) if pred else false_fn(*false_args)
    File ""/tmp/tmpw2unlhke.py"", line 81, in false_fn_5
        for_loop_body_0, [inputs_list, __for_loop_var_index_0])
    File ""/home/user/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 44, in convert_while_loop
        loop_vars = _run_py_while(cond, body, loop_vars)
    File ""/home/user/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 58, in _run_py_while
        loop_vars = body(*loop_vars)
    File ""/tmp/tmpw2unlhke.py"", line 76, in for_loop_body_0
        dy2static.convert_call(self.get_pred)())
    File ""/tmp/tmphqkmqzr4.py"", line 45, in get_pred
        __return_value_0,))
    File ""/home/user/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 210, in convert_ifelse
        return _run_py_ifelse(pred, true_fn, false_fn, true_args, false_args)
    File ""/home/user/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 235, in _run_py_ifelse
        return true_fn(*true_args) if pred else false_fn(*false_args)
    File ""/tmp/tmphqkmqzr4.py"", line 40, in false_fn_7
        __return_value_0, self), (__return_value_0, output))
    File ""/home/user/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 210, in convert_ifelse
        return _run_py_ifelse(pred, true_fn, false_fn, true_args, false_args)
    File ""/home/user/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 235, in _run_py_ifelse
        return true_fn(*true_args) if pred else false_fn(*false_args)
    File ""/home/user/sishuiruoxin/PaddleDetection/ppdet/modeling/architectures/picodet.py"", line 90, in get_pred (* user code *)
        bbox_pred, bbox_num = self._forward()
    File ""/home/user/sishuiruoxin/PaddleDetection/ppdet/modeling/architectures/picodet.py"", line 66, in _forward (* user code *)
        head_outs = self.head(fpn_feats, self.export_post_process)
    File ""/home/user/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 902, in __call__
        outputs = self.forward(*inputs, **kwargs)
    File ""/tmp/tmphqdrjxfz.py"", line 112, in forward
        for_loop_body_4, [i, __for_loop_var_index_5, fpn_feats])
    File ""/home/user/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 44, in convert_while_loop
        loop_vars = _run_py_while(cond, body, loop_vars)
    File ""/home/user/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 58, in _run_py_while
        loop_vars = body(*loop_vars)
    File ""/tmp/tmphqdrjxfz.py"", line 104, in for_loop_body_4
        cls_score, i, self), (bbox_pred, cls_score))
    File ""/home/user/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 210, in convert_ifelse
        return _run_py_ifelse(pred, true_fn, false_fn, true_args, false_args)
    File ""/home/user/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 235, in _run_py_ifelse
        return true_fn(*true_args) if pred else false_fn(*false_args)
    File ""/tmp/tmphqdrjxfz.py"", line 99, in false_fn_19
        bbox_pred, cls_score), (bbox_pred, cls_score))
    File ""/home/user/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 210, in convert_ifelse
        return _run_py_ifelse(pred, true_fn, false_fn, true_args, false_args)
    File ""/home/user/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 235, in _run_py_ifelse
        return true_fn(*true_args) if pred else false_fn(*false_args)
    File ""/home/user/sishuiruoxin/PaddleDetection/ppdet/modeling/heads/pico_head.py"", line 318, in forward (* user code *)
        b, cell_h, cell_w, _ = paddle.shape(cls_score)
    File ""/home/user/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/framework.py"", line 1683, in __getitem__
        return _getitem_impl_(self, item)
    File ""/home/user/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/variable_index.py"", line 225, in _getitem_impl_
        attrs=attrs)
    File ""/home/user/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/framework.py"", line 2967, in append_op
        attrs=kwargs.get(""attrs"", None))
    File ""/home/user/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/framework.py"", line 2154, in __init__
        self.desc.infer_shape(self.block.desc)
    ValueError: (InvalidArgument) end should greater than start, but received end = 5, start = 4.
  [Hint: Expected end > start, but received end:4 <= start:4.] (at /paddle/paddle/fluid/operators/slice_op.cc:135)
  [operator < slice > error]
报Hint: Expected end > start, but received end:4 <= start:4.] (at /paddle/paddle/fluid/operators/slice_op.cc:135出错，查找各种FAQ，均未知道原因。求解决，谢谢！"
jetsonAGX量化推理int8报错,PaddlePaddle/PaddleDetection,2022-04-16 08:55:28,3,model compression,5719,1206059590,"**环境：**
jetpack4.6
paddlepaddle2.2
使用的是官方预测库：https://www.paddlepaddle.org.cn/inference/v2.2/user_guides/download_lib.html
(Jetpack4.6：nv_jetson-cuda10.2-trt8-xavier)
ppdet2.1

模型为量化训练所得

**过程：**
fluid/trt_fp32/fp_16都可以正常运行，且trt_fp32/fp_16确实有小幅提升

trt_int8报错：
![微信图片_20220416165504](https://user-images.githubusercontent.com/74665518/163668804-7dd8f45d-62f4-48ca-adf1-444ea53bac4b.jpg)
"
PicoDet增强版有C++的部署教程吗？,PaddlePaddle/PaddleDetection,2022-04-16 06:12:44,4,PP-PicoDet,5716,1206027199,您好，请问PicoDet增强版有C++的部署教程吗？介绍上说各类硬件无需单独开发后处理模块，降低部署门槛，部署的代码还是以YOLO为例的。网站提供的onnx的模型有( w/ 后处理)( w/o 后处理)这两个的部署应该也是不一样的吧？不知有没有教程学习学习，谢谢
trouble on installing paddle detection  in paddle serving container,PaddlePaddle/PaddleDetection,2022-04-14 08:44:27,2,,5695,1204231919,"Hi, I try to install paddle detection in the paddle serving container. The steps are 

docker pull paddlepaddle/serving:0.7.0-devel
docker run -p 9292:9292 --name test -dit paddlepaddle/serving:0.7.0-devel bash
docker exec -it test bash
git clone https://github.com/PaddlePaddle/Serving
cd Serving
pip3 install -r python/requirements.txt
pip3 install paddlepaddle==2.2.0
git clone https://github.com/PaddlePaddle/PaddleDetection.git
# 安装其他依赖
cd PaddleDetection
pip3 install -r requirements.txt
# 编译安装paddledet
python3 setup.py install
However, I run python3 ppdet/modeling/tests/test_architectures.py, the results are as follows:

grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
Traceback (most recent call last):
  File ""ppdet/modeling/tests/test_architectures.py"", line 20, in <module>
    import ppdet
  File ""/usr/local/lib/python3.6/site-packages/paddledet-2.4.0-py3.6.egg/ppdet/__init__.py"", line 15, in <module>
    from . import (core, data, engine, modeling, model_zoo, optimizer, metrics,
  File ""/usr/local/lib/python3.6/site-packages/paddledet-2.4.0-py3.6.egg/ppdet/engine/__init__.py"", line 15, in <module>
    from . import trainer
  File ""/usr/local/lib/python3.6/site-packages/paddledet-2.4.0-py3.6.egg/ppdet/engine/trainer.py"", line 40, in <module>
    from ppdet.metrics import Metric, COCOMetric, VOCMetric, WiderFaceMetric, get_infer_results, KeyPointTopDownCOCOEval, KeyPointTopDownMPIIEval
  File ""/usr/local/lib/python3.6/site-packages/paddledet-2.4.0-py3.6.egg/ppdet/metrics/__init__.py"", line 27, in <module>
    from . import mcmot_metrics
  File ""/usr/local/lib/python3.6/site-packages/paddledet-2.4.0-py3.6.egg/ppdet/metrics/mcmot_metrics.py"", line 24, in <module>
    from motmetrics.math_util import quiet_divide
  File ""/usr/local/lib/python3.6/site-packages/motmetrics/__init__.py"", line 28, in <module>
    from motmetrics import io
  File ""/usr/local/lib/python3.6/site-packages/motmetrics/io.py"", line 18, in <module>
    import pandas as pd
  File ""/usr/local/lib/python3.6/site-packages/pandas/__init__.py"", line 55, in <module>
    from pandas.core.api import (
  File ""/usr/local/lib/python3.6/site-packages/pandas/core/api.py"", line 24, in <module>
    from pandas.core.groupby import Grouper, NamedAgg
  File ""/usr/local/lib/python3.6/site-packages/pandas/core/groupby/__init__.py"", line 1, in <module>
    from pandas.core.groupby.generic import (  # noqa: F401
  File ""/usr/local/lib/python3.6/site-packages/pandas/core/groupby/generic.py"", line 44, in <module>
    from pandas.core.frame import DataFrame
  File ""/usr/local/lib/python3.6/site-packages/pandas/core/frame.py"", line 115, in <module>
    from pandas.core.series import Series
  File ""/usr/local/lib/python3.6/site-packages/pandas/core/series.py"", line 84, in <module>
    import pandas.plotting
  File ""/usr/local/lib/python3.6/site-packages/pandas/plotting/__init__.py"", line 59, in <module>
    from pandas.plotting._core import (
  File ""/usr/local/lib/python3.6/site-packages/pandas/plotting/_core.py"", line 17, in <module>
    import pandas.plotting._matplotlib  # noqa
  File ""/usr/local/lib/python3.6/site-packages/pandas/plotting/_matplotlib/__init__.py"", line 3, in <module>
    from pandas.plotting._matplotlib.boxplot import (
  File ""/usr/local/lib/python3.6/site-packages/pandas/plotting/_matplotlib/boxplot.py"", line 4, in <module>
    from matplotlib.artist import setp
  File ""/usr/local/lib/python3.6/site-packages/matplotlib/__init__.py"", line 107, in <module>
    from . import cbook, rcsetup
  File ""/usr/local/lib/python3.6/site-packages/matplotlib/rcsetup.py"", line 28, in <module>
    from matplotlib.fontconfig_pattern import parse_fontconfig_pattern
  File ""/usr/local/lib/python3.6/site-packages/matplotlib/fontconfig_pattern.py"", line 15, in <module>
    from pyparsing import (Literal, ZeroOrMore, Optional, Regex, StringEnd,
  File ""/usr/local/lib/python3.6/site-packages/pyparsing/__init__.py"", line 130, in <module>
    __version__ = __version_info__.__version__"
AiStudio异常检测摔倒案例视频文件/模型参数/配置文件下载,PaddlePaddle/PaddleDetection,2022-04-14 07:43:38,2,,5693,1204155752,大家好，想问一下，基于PPVideo的异常检测案例中的 /home/aistudio/work/data/abnormal_action_videos/abnormal_action.mp4视频文件和AVA_SlowFast_FastRcnn_best.pdparams.zip 预训练模型以及/home/aistudio/work/abnoraml_action.yaml配置文件在哪里下载呢？谢谢。
训练后导出模型无法使用deploy部署正确推理,PaddlePaddle/PaddleDetection,2022-04-13 12:23:19,3,,5688,1203228213,"训练后导出模型无法使用deploy正确推理

你好，我也遇到了同样的问题
训练过程跟你也是一样。平台是Win 10，rtx2060，使用显卡训练
使用 python tools/infer 可以检测到东西，没什么问题。
但是使用部署步骤，export_model.py，再使用 deploy/python/infer，就识别不到了。

看上面的回复说缺tensorRT联编，现在我还没有做tensorRT联编，想先尝试使用CPU进行推理，但是结果相同，没有报错，但是同样检测不到任何东西。
问题1：用联编可以解决这个问题吗？
问题2：如果想用CPU进行部署的话，需要怎么调？


```
python tools/export_model.py -c configs/yolov3/yolov3_bamboo2.yml --output_dir=./inference_model -o weights=output/yolov3_bamboo2/best_model.pdparams

python deploy/python/infer.py --device=CPU --model_dir=./inference_model/yolov3_bamboo2 --image_file=./dataset/bamboo/testimg/1.jpg

Q:\paddletest\venv\lib\site-packages\paddle\vision\transforms\functional_pil.py:36: DeprecationWarning: NEAREST is deprecated and will be removed
in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': Image.NEAREST,
Q:\paddletest\venv\lib\site-packages\paddle\vision\transforms\functional_pil.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed
 in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': Image.BILINEAR,
Q:\paddletest\venv\lib\site-packages\paddle\vision\transforms\functional_pil.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed
in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': Image.BICUBIC,
Q:\paddletest\venv\lib\site-packages\paddle\vision\transforms\functional_pil.py:39: DeprecationWarning: BOX is deprecated and will be removed in P
illow 10 (2023-07-01). Use Resampling.BOX instead.
  'box': Image.BOX,
Q:\paddletest\venv\lib\site-packages\paddle\vision\transforms\functional_pil.py:40: DeprecationWarning: LANCZOS is deprecated and will be removed
in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  'lanczos': Image.LANCZOS,
Q:\paddletest\venv\lib\site-packages\paddle\vision\transforms\functional_pil.py:41: DeprecationWarning: HAMMING is deprecated and will be removed
in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  'hamming': Image.HAMMING
-----------  Running Arguments -----------
action_file: None
batch_size: 1
camera_id: -1
cpu_threads: 1
device: CPU
enable_mkldnn: False
enable_mkldnn_bfloat16: False
image_dir: None
image_file: ./dataset/bamboo/testimg/1.jpg
model_dir: ./inference_model/yolov3_bamboo2
output_dir: output
random_pad: False
reid_batch_size: 50
reid_model_dir: None
run_benchmark: False
run_mode: paddle
save_images: False
save_mot_txt_per_img: False
save_mot_txts: False
scaled: False
threshold: 0.5
tracker_config: None
trt_calib_mode: False
trt_max_shape: 1280
trt_min_shape: 1
trt_opt_shape: 640
use_dark: True
use_gpu: False
video_file: None
window_size: 50
------------------------------------------
-----------  Model Configuration -----------
Model Arch: YOLO
Transform Order:
--transform op: Resize
--transform op: NormalizeImage
--transform op: Permute
--------------------------------------------
[WARNNING] No object detected.
save result to: output\1.jpg
Test iter 0
------------------ Inference Time Info ----------------------
total_time(ms): 1339.2, img_num: 1
average latency time(ms): 1339.20, QPS: 0.746714
preprocess_time(ms): 120.50, inference_time(ms): 1218.70, postprocess_time(ms): 0.00
```

_Originally posted by @aileenfun in https://github.com/PaddlePaddle/PaddleDetection/issues/4205#issuecomment-1097595719_"
数据测试 异常,PaddlePaddle/PaddleDetection,2022-04-13 11:49:33,6,,5686,1203191697,"遇到一种情况 就是 使用模型 做测试的数据 一批测试数据效果 很好  然后又增加了新的一批测试数据测试发现 效果较差 这个正常么？ 是什么原因呢？
"
 How to solve download error ,PaddlePaddle/PaddleDetection,2022-04-13 05:18:51,2,,5680,1202771497,"`ppdet.utils.download ERROR: Get config /home/ubuntu/.cache/paddle/configs/faster_rcnn/faster_rcnn_r50#_fpn_1x_coco.yml failed after download, please contact us on https://github.com/PaddlePaddle/PaddleDetection/issues`

"
导出模型有误,PaddlePaddle/PaddleDetection,2022-04-12 08:22:47,0,,5669,1201400447,"![image](https://user-images.githubusercontent.com/34839719/162915260-164f5e1a-f575-4a2b-ac71-73a1518c0fa6.png)

我想要导出`https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.4/static/docs/featured_model/LARGE_SCALE_DET_MODEL.md`中的`ResNet50-vd-FPN-Dcnv2`模型，为什么导不出来呢？"
[Other General Issues] No kernel image is available for execution on the device. ,PaddlePaddle/PaddleDetection,2022-04-12 03:02:37,1,,5664,1200896562,"## 描述问题/Describe the bug

我把训练好的模型部署到一台win10系统的机器中，执行eval或者inference推断时都会报错：

cudaErrorNoKernelImageDevice: no kernel image is available for execution on the device.

个人感觉是cuda driver 版本、cudatoolkit版本和paddle版本出现了不兼容的问题，但我不太确定，

想把具体情况复述给大佬们，能否帮我看一下如何解决这个问题？ 谢谢！

## 复现/Reproduction
python tools/eval.py -c *** -o weights=***
使用模型:
picodet_l_640

## 环境/Environment

虚拟环境:
python: 3.7
paddle-gpu: 2.2.2(cudatoolkit 10.2)
paddledet : release-2.4

硬件信息:
win10
cpu i9
RTX 3060ti单卡

CUDA版本:
cuda driver: 512.15 cuda versionj: 11.6(driver API version: 11.6)
cudatoolkit: 10.2(runtime API version: 10.2)
cudnn: 7.6.5
-------------------------------------------------------------------------------------------------------------------
简要地说, 考虑到cudatoolkit 10.2版本在win10上更稳定，所以我选择了10.2版本，同时下载了对应cudatoolkit 10.2版本的paddle。
最新的cuda driver对应的cuda version是11.6，理论上来说driver API version >= runtime API version就可以启动程序。
但是现在还是出现了不兼容的问题，我就不明白了。
望解答， 多谢！"
[Other General Issues] 利用paddle.summary打印模型参数量和计算量,PaddlePaddle/PaddleDetection,2022-04-06 14:26:46,2,,5613,1194695360,"## 描述问题/Describe the bug
A clear and concise description of what the bug is.
利用paddle.summary打印模型参数量和计算量，但是打印失败，我的理解是在实例化模型之后使用该api，但是失败了，不知道应该把该代码插入到哪个位置处？

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?

```bash
python tools/train.py -c=configs/yolov3/yolov3_mobilenet_v3_large_270e_coco.yml --eval
```
3. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

```python
# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os
import sys

# add python path of PadleDetection to sys.path
parent_path = os.path.abspath(os.path.join(__file__, *(['..'] * 2)))
sys.path.insert(0, parent_path)

# ignore warning log
import warnings
warnings.filterwarnings('ignore')

import paddle

from ppdet.core.workspace import load_config, merge_config
from ppdet.engine import Trainer, init_parallel_env, set_random_seed, init_fleet_env
from ppdet.slim import build_slim_model

import ppdet.utils.cli as cli
import ppdet.utils.check as check
from ppdet.utils.logger import setup_logger
logger = setup_logger('train')


def parse_args():
    parser = cli.ArgsParser()
    parser.add_argument(
        ""--eval"",
        action='store_true',
        default=False,
        help=""Whether to perform evaluation in train"")
    parser.add_argument(
        ""-r"", ""--resume"", default=None, help=""weights path for resume"")
    parser.add_argument(
        ""--slim_config"",
        default=None,
        type=str,
        help=""Configuration file of slim method."")
    parser.add_argument(
        ""--enable_ce"",
        type=bool,
        default=False,
        help=""If set True, enable continuous evaluation job.""
        ""This flag is only used for internal test."")
    parser.add_argument(
        ""--fp16"",
        action='store_true',
        default=False,
        help=""Enable mixed precision training."")
    parser.add_argument(
        ""--fleet"", action='store_true', default=False, help=""Use fleet or not"")
    parser.add_argument(
        ""--use_vdl"",
        type=bool,
        default=False,
        help=""whether to record the data to VisualDL."")
    parser.add_argument(
        '--vdl_log_dir',
        type=str,
        default=""vdl_log_dir/scalar"",
        help='VisualDL logging directory for scalar.')
    parser.add_argument(
        '--save_prediction_only',
        action='store_true',
        default=False,
        help='Whether to save the evaluation results only')
    parser.add_argument(
        '--profiler_options',
        type=str,
        default=None,
        help=""The option of profiler, which should be in ""
        ""format \""key1=value1;key2=value2;key3=value3\"".""
        ""please see ppdet/utils/profiler.py for detail."")
    parser.add_argument(
        '--save_proposals',
        action='store_true',
        default=False,
        help='Whether to save the train proposals')
    parser.add_argument(
        '--proposals_path',
        type=str,
        default=""sniper/proposals.json"",
        help='Train proposals directory')

    args = parser.parse_args()
    return args


def run(FLAGS, cfg):
    # init fleet environment
    if cfg.fleet:
        init_fleet_env(cfg.get('find_unused_parameters', False))
    else:
        # init parallel environment if nranks > 1
        init_parallel_env()

    if FLAGS.enable_ce:
        set_random_seed(0)

    # build trainer
    trainer = Trainer(cfg, mode='train')
    params_info = paddle.summary(trainer, (1, 3, 320, 320))
    print(params_info)

    # load weights
    if FLAGS.resume is not None:
        trainer.resume_weights(FLAGS.resume)
    elif 'pretrain_weights' in cfg and cfg.pretrain_weights:
        trainer.load_weights(cfg.pretrain_weights)

    # training
    trainer.train(FLAGS.eval)


def main():
    FLAGS = parse_args()
    cfg = load_config(FLAGS.config)
    cfg['fp16'] = FLAGS.fp16
    cfg['fleet'] = FLAGS.fleet
    cfg['use_vdl'] = FLAGS.use_vdl
    cfg['vdl_log_dir'] = FLAGS.vdl_log_dir
    cfg['save_prediction_only'] = FLAGS.save_prediction_only
    cfg['profiler_options'] = FLAGS.profiler_options
    cfg['save_proposals'] = FLAGS.save_proposals
    cfg['proposals_path'] = FLAGS.proposals_path
    merge_config(FLAGS.opt)

    # disable npu in config by default
    if 'use_npu' not in cfg:
        cfg.use_npu = False

    if cfg.use_gpu:
        place = paddle.set_device('gpu')
    elif cfg.use_npu:
        place = paddle.set_device('npu')
    else:
        place = paddle.set_device('cpu')

    if 'norm_type' in cfg and cfg['norm_type'] == 'sync_bn' and not cfg.use_gpu:
        cfg['norm_type'] = 'bn'

    if FLAGS.slim_config:
        cfg = build_slim_model(cfg, FLAGS.slim_config)

    # FIXME: Temporarily solve the priority problem of FLAGS.opt
    merge_config(FLAGS.opt)
    check.check_config(cfg)
    check.check_gpu(cfg.use_gpu)
    check.check_npu(cfg.use_npu)
    check.check_version()

    run(FLAGS, cfg)


if __name__ == ""__main__"":
    main()

```
说明：    在# build trainer后面加了下面这些代码
```
    params_info = paddle.summary(trainer, (1, 3, 320, 320))
    print(params_info)
```

6. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.
```
Traceback (most recent call last):
  File ""tools/train.py"", line 173, in <module>
    main()
  File ""tools/train.py"", line 169, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 119, in run
    params_info = paddle.summary(trainer, (1, 3, 320, 320))
  File ""/home/xuzhi/anaconda3/envs/dxl/lib/python3.7/site-packages/paddle/hapi/model_summary.py"", line 184, in summary
    in_train_mode = net.training
AttributeError: 'Trainer' object has no attribute 'training'
```

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：
paddle: 2.2.0
paddledetection: 最新
"
在Jetson nano2g 推理时间慢,PaddlePaddle/PaddleDetection,2022-04-06 09:25:37,8,,5606,1194330392,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.

## 描述问题/  硬件是Jetson nano 2g
paddle version 是2.2.2  cuda版本10.2  TensorRT  8.0   cuDNN8.2
测试的模型  pico-Det ESnet-s   和Yolov3-mobilenetv1    
图片大小为640和608
前一个平均时间是0.19秒  后一个平均时间是0.18秒。开启了tensorRT加速。
这速度正常吗？是不是太慢了呀。


"
正常训练一个epoch后，不能保存模型，直接结束训练,PaddlePaddle/PaddleDetection,2022-04-06 05:28:40,2,,5600,1194054384,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?

```none
请填写命令/A placeholder for the command.
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

3. 您使用的数据集是？/What dataset did you use?

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：

4. 请问您使用的Python版本是？/ Please provide the version of Python you used.

5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.


如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
新版本PPYOLOe出现问题·1[Other General Issues],PaddlePaddle/PaddleDetection,2022-04-05 15:36:38,6,,5591,1193358316,"<img width=""1548"" alt=""image"" src=""https://user-images.githubusercontent.com/78856426/161791175-877e5ecf-5676-4335-9fa7-a9f57c57333e.png"">
我尝试使用ppyoloe改了一个backbone把CSPResNet换成ConvNExt，ConvNext是我自己按照ppdet Backbone要求写的，在2.3没出过这种情况，换成2.4后就不行了"
Process finished with exit code -1073741819 (0xC0000005),PaddlePaddle/PaddleDetection,2022-04-05 13:34:04,16,,5590,1193159521,"复现yolov4 在计算iou损失的时候  代码调试 什么也没做 内存暴涨 直到报 **Process finished with exit code -1073741819 (0xC0000005)** 的错  非常奇怪  调试时 运行完paddle.argmax(self.calculate_iou(gt_box, anchor_shapes), axis=-1)  找到iou最大下标后 for循环完 卡主 什么也没见到执行 内存一直上涨到爆  知道是内存溢出 也看了复现指南https://github.com/PaddlePaddle/models/blob/release%2F2.2/tutorials/article-implementation/ArticleReproduction_CV.md
里面的几个方法  试过了 都没用"
Object detector开启TRT模式反而更慢,PaddlePaddle/PaddleDetection,2022-04-02 02:42:23,0,,5573,1190470982,"设置ObjectDetector为GPU/TRT模式：PaddleDetection::ObjectDetector det(FLAGS_model_dir, ""GPU"", false, 1, ""trt_fp16"");

并将use_static设为true:     config.EnableTensorRtEngine(1 << 30,
                                  batch_size,
                                  this->min_subgraph_size_,
                                  precision,
                                  true, //false 改为 true
                                  this->trt_calib_mode_);

但应该还是每次转换trt模型，导致推理很慢，要20多分钟1次。有解决办法吗？

改回CPU模式，cpu_threads设为10，比GPU要快一些。应该怎样设置呢？"
[BUG]敏感度分析脚本运行报错求助,PaddlePaddle/PaddleDetection,2022-04-01 12:55:47,3,,5572,1189756479,"按照该文档进行卷积层敏感度分析：
https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/static/slim/sensitive

在执行文档中的以下命令时报错：ImportError: cannot import name 'create_reader' from 'ppdet.data.reader'
![1648817635(1)](https://user-images.githubusercontent.com/40461420/161267506-daec22fc-ca91-4bd3-939f-c0f590ba59e7.png)

备注：我是在 AI studio 中使用的 PaddleDetection2.4 版本"
调参指导，训练之初loss迅速趋近于0,PaddlePaddle/PaddleDetection,2022-03-31 06:15:10,4,,5546,1187569605,"使用hrnet关键点检测，自定义数据集（共38个关键点，6007张图像，训练集4802，测试集1201），原配置学习率为
![image](https://user-images.githubusercontent.com/35481439/160986373-a4398d8e-b692-46ca-ab8e-166061e5ebf9.png)
batchsize为64。

我训练过程中采用单卡训练，batchsize = 8, 学习率试过0.0001,  0.0005， （0.0005/32）,  steps也试过改成300， 3000但无论改大还是改小，每次训练一开始就出现loss迅速趋近于0的情况，batch_cost，data_cost也基本不变，但loss有看到在变小
![image](https://user-images.githubusercontent.com/35481439/160988065-46b2a2dd-2727-4cfc-a673-39a2143a2a14.png)

请大佬指导，这是什么原因，应该怎么调，谢谢！



"
[BUG],PaddlePaddle/PaddleDetection,2022-03-30 09:43:30,4,,5527,1186179131,"![image](https://user-images.githubusercontent.com/25811500/160802176-f0af35fa-9030-4479-8a0d-957a3dd4bb08.png)


 报错

Frame id: 170, Total count: 1
[[277.  44.]
 [277.  43.]
 [277.  42.]
 [277.  41.]
 [276.  41.]
 [275.  41.]
 [271.  42.]
 [266.  41.]
 [261.  42.]
 [257.  42.]
 [254.  42.]
 [251.  42.]
 [248.  41.]
 [245.  40.]
 [242.  39.]
 [240.  38.]
 [239.  37.]
 [237.  37.]
 [236.  37.]
 [234.  36.]
 [233.  36.]
 [231.  36.]
 [228.  35.]
 [224.  35.]
 [219.  35.]
 [215.  35.]
 [211.  36.]
 [207.  36.]
 [205.  36.]
 [202.  36.]
 [201.  36.]
 [200.  36.]
 [199.  35.]
 [198.  35.]
 [197.  35.]
 [196.  35.]
 [194.  34.]
 [193.  34.]
 [192.  33.]
 [191.  33.]
 [189.  33.]
 [188.  33.]
 [183.  33.]
 [179.  33.]
 [176.  33.]
 [173.  33.]
 [170.  33.]
 [168.  33.]
 [167.  33.]
 [166.  32.]]
Traceback (most recent call last):
  File ""deploy/pphuman/pipeline.py"", line 742, in <module>
    main()
  File ""deploy/pphuman/pipeline.py"", line 731, in main
    pipeline.run()
  File ""deploy/pphuman/pipeline.py"", line 206, in run
    self.predictor.run(self.input)
  File ""deploy/pphuman/pipeline.py"", line 410, in run
    self.predict_video(input)
  File ""deploy/pphuman/pipeline.py"", line 580, in predict_video
    self.coord_size)
  File ""F:\desk\PaddleDetection-release-2.4\deploy\pphuman\pipe_utils.py"", line 377, in parse_mot_keypoint
    skeleton.append(refine_keypoint_coordinary(kpts, bbox, coord_size))
  File ""F:\desk\PaddleDetection-release-2.4\deploy\pphuman\pipe_utils.py"", line 359, in refine_keypoint_coordinary
    tl = np.expand_dims(np.transpose(tl, (1, 0)), (2, 3))
  File ""G:\Anaconda\envs\py36\lib\site-packages\numpy\lib\shape_base.py"", line 579, in expand_dims
    if axis > a.ndim or axis < -a.ndim - 1:
TypeError: '>' not supported between instances of 'tuple' and 'int'

## 复现/Reproduction
(py36) F:\desk\PaddleDetection-release-2.4>python deploy/pphuman/pipeline.py --config deploy/pphuman/config/infer_cfg.yml --video_file=1.avi --device=gpu --enable_action=True

"
PaddleDetection如何开启GPU模式,PaddlePaddle/PaddleDetection,2022-03-30 07:23:08,2,,5523,1186004455,"将PaddleDetection::ObjectDetector det(FLAGS_model_dir, ""CPU"");中“CPU”改为“GPU”，推理速度并没有加快，GPU使用率也没有提高，是没有正常开启trtEngine吗？应该怎样开启GPU/TRT模式？"
pp-yoloe导出onnx问题[BUG],PaddlePaddle/PaddleDetection,2022-03-30 02:41:47,21,feature request#deploy,5511,1185785971,"您好。我在转ONNX模型的时候，提示了警告：Due to the operator:multiclass_nms3, the converted ONNX model will only supports input[batch_size] == 1；  我得到的onnx，我查看输入的维数为[-1,3,640,640]，下游环境对这个-1不能识别，比较悲催。请问可以在哪设置一下吗
"
picodet新旧版本推理速度问题,PaddlePaddle/PaddleDetection,2022-03-29 13:15:48,4,PP-PicoDet,5505,1184856744,"我在使用最新的2.4版本的picodet_xs_320_voc_lcnet模型(仅更换了成了voc数据集)和2.3版本的picodet_s_320_voc模型用c++ demo来推理相同数据集。

picodet_xs_320_coco_lcnet的处理结果如下：
I0329 20:55:39.949678 40395 main.cc:59] ----------------------- Config info -----------------------
I0329 20:55:39.949775 40395 main.cc:60] runtime_device: CPU
I0329 20:55:39.949779 40395 main.cc:61] ir_optim: True
I0329 20:55:39.949784 40395 main.cc:62] enable_memory_optim: True
I0329 20:55:39.949797 40395 main.cc:69] enable_tensorrt: False
I0329 20:55:39.949800 40395 main.cc:70] precision: fp32
I0329 20:55:39.949803 40395 main.cc:72] enable_mkldnn: False
I0329 20:55:39.949806 40395 main.cc:73] cpu_math_library_num_threads: 1
I0329 20:55:39.949810 40395 main.cc:74] ----------------------- Data info -----------------------
I0329 20:55:39.949813 40395 main.cc:75] batch_size: 1
I0329 20:55:39.949816 40395 main.cc:76] input_shape: dynamic shape
I0329 20:55:39.949820 40395 main.cc:77] ----------------------- Model info -----------------------
I0329 20:55:39.949836 40395 main.cc:79] model_name: picodet_xs_320_voc_lcnet
I0329 20:55:39.949841 40395 main.cc:80] ----------------------- Perf info ------------------------
I0329 20:55:39.949846 40395 main.cc:81] Total number of predicted data: 163 and total time spent(ms): 16101
I0329 20:55:39.949849 40395 main.cc:84] preproce_time(ms): 8.02807, inference_time(ms): 90.7461, postprocess_time(ms): 2.33119

picodet_s_320_voc的处理结果如下：
I0329 21:00:47.205097 40713 main.cc:76] ----------------------- Config info -----------------------
I0329 21:00:47.205195 40713 main.cc:77] runtime_device: CPU
I0329 21:00:47.205200 40713 main.cc:78] ir_optim: True
I0329 21:00:47.205204 40713 main.cc:80] enable_memory_optim: True
I0329 21:00:47.205219 40713 main.cc:89] enable_tensorrt: False
I0329 21:00:47.205221 40713 main.cc:91] precision: fp32
I0329 21:00:47.205225 40713 main.cc:94] enable_mkldnn: False
I0329 21:00:47.205229 40713 main.cc:95] cpu_math_library_num_threads: 1
I0329 21:00:47.205231 40713 main.cc:96] ----------------------- Data info -----------------------
I0329 21:00:47.205235 40713 main.cc:97] batch_size: 1
I0329 21:00:47.205238 40713 main.cc:98] input_shape: dynamic shape
I0329 21:00:47.205241 40713 main.cc:100] ----------------------- Model info -----------------------
I0329 21:00:47.205260 40713 main.cc:102] model_name: picodet_s_320_voc
I0329 21:00:47.205264 40713 main.cc:104] ----------------------- Perf info ------------------------
I0329 21:00:47.205268 40713 main.cc:105] Total number of predicted data: 163 and total time spent(ms): 9876
I0329 21:00:47.205272 40713 main.cc:108] preproce_time(ms): 7.82403, inference_time(ms): 52.6086, postprocess_time(ms): 0.16433

可以看到XS模型的耗时比S模型的耗时要接近多一倍。我XS模型是2.4release里导出的，使用2.2demo测试的；S模型是2.3release导出的，使用2.3demo测试的。

理论上benchmark下XS应该和S差不多速度的，是不是哪里处理得不太对"
win10下pptracking编译报错,PaddlePaddle/PaddleDetection,2022-03-28 08:23:52,0,,5479,1183073239,"## 描述问题
win10+VS2017+cuda10.1+cudnn7.6+opencv3.4.6+pp-tracking编译出错

## 复现

1. 操作步骤？
（1）下载opencv，预测库paddle_inference_notrt，paddleDetection2.3代码
（2）cmake生成PaddleObjectDetector.sln工程，deploy/cpp下编译无问题，deploy/pptracking/cpp下报错。

2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。
否

![image](https://user-images.githubusercontent.com/12136347/160356717-96a06008-968a-482d-984a-06780e05a9b7.png)

![image](https://user-images.githubusercontent.com/12136347/160356583-2a48e733-1ab1-4d1e-b6be-49660a303dd3.png)
"
[Other General Issues]picodet-l模型可以转换成pytorch模型吗？具体怎么操作？,PaddlePaddle/PaddleDetection,2022-03-25 08:57:05,0,,5462,1180515969,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://github.com/PaddlePaddle/PaddleDetection/tree/develop/docs/tutorials/FAQ) /I have read the [FAQ documentation](https://github.com/PaddlePaddle/PaddleDetection/tree/develop/docs/tutorials/FAQ) but cannot get the expected help.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.
picodet-l模型可以转换成pytorch模型吗？具体怎么操作？
## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?

```none
请填写命令/A placeholder for the command.
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

3. 您使用的数据集是？/What dataset did you use?

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：

4. 请问您使用的Python版本是？/ Please provide the version of Python you used.

5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.


如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
"ObjectDetector中device设置为""GPU“出现问题",PaddlePaddle/PaddleDetection,2022-03-25 06:25:24,2,,5460,1180396048,"提示”Please compile with gpu to EnableGpu()""
需要怎样才能使用GPU加速Paddle?
是需要安装tensorrt吗？"
如何标注负样本,PaddlePaddle/PaddleDetection,2022-03-24 14:52:19,2,,5455,1179630072,如何标注负样本？具体方法
[Other General Issues]我的这个问题是否应该用检测任务来实现,PaddlePaddle/PaddleDetection,2022-03-24 12:33:50,2,,5452,1179454567,"![6E225EEAAB1A7D8871C1F437043C72E4](https://user-images.githubusercontent.com/42019021/159916500-31b77605-d0f2-4672-92fb-c218839bd954.jpg)
假如有无数个不同的图片，每张图片里包含图示的矩形，位置不固定。
任务就是判断矩形里的X 和 Y 是否有1，最终输出对应的结果，最多就四种情况。这个问题是做目标检测好实现还是分类好实现，或者是OCR？

想了好几天，也没太想通..."
[Feature Request]在FPGA部署问题,PaddlePaddle/PaddleDetection,2022-03-24 08:22:27,2,,5450,1179166574,"我们参照PaddleLite开发文档，在使用paddleDetection的模型时，使用paddle1.8.4训练出来的模型格式时__model__,__param__但是新版的模型似乎发生了改变，原来版本的模型可以部署到FPGA，但是新版本的模型部署到FPGA会abortion，这是什么原因导致的？？另外paddlex训练的模型格式也与这个版本不一样，这里面的区别是什么？？"
"paddle中很多训练出的模型无效，只有yolov3, cascade mask rcnn, solo等有限的几个有效",PaddlePaddle/PaddleDetection,2022-03-23 09:42:57,11,enhancement,5442,1177863142,请问是什么原因？
使用多卡训练问题：训练卡住 [Other General Issues],PaddlePaddle/PaddleDetection,2022-03-23 03:26:42,2,,5435,1177547420,"
## 描述问题/Describe the bug
使用paddleDetection训练自己的数据集，使用同样的数据和配置文件，单卡可以训练，多卡训练在load pretrain model之后会卡住

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?

```bash
# 多卡
python -m paddle.distributed.launch --gpus 0,1,2,3 tools/train.py -c configs/ppyolo/ppyolov2_r50vd_dcn_coco_v20.yml --eval --use_vdl True 
# 单卡：可以成功
python tools/train.py -c configs/ppyolo/ppyolov2_r50vd_dcn_coco_v20.yml --eval --use_vdl True 
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

训练自己的数据集，只是修改了数据集路径、batch_size、和输入图片大小
configs/ppyolo/_base_/ppyolov2_reader.yml: 
```
worker_num: 2
TrainReader:
  inputs_def:
    num_max_boxes: 100
  sample_transforms:
    - Decode: {}
    - Mixup: {alpha: 1.5, beta: 1.5}
    - RandomDistort: {}
    - RandomExpand: {fill_value: [123.675, 116.28, 103.53]}
    - RandomCrop: {}
    - RandomFlip: {}
    - Resize: {target_size: [1024, 1024], keep_ratio: False, interp: 2} # modified
  batch_transforms:
    - BatchRandomResize: {target_size: [320, 352, 384, 416, 448, 480, 512, 544, 576, 608, 640, 672, 704, 736, 768], random_size: True, random_interp: True, keep_ratio: False}
    - NormalizeBox: {}
    - PadBox: {num_max_boxes: 100}
    - BboxXYXY2XYWH: {}
    - NormalizeImage: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225], is_scale: True}
    - Permute: {}
    - Gt2YoloTarget: {anchor_masks: [[6, 7, 8], [3, 4, 5], [0, 1, 2]], anchors: [[10, 13], [16, 30], [33, 23], [30, 61], [62, 45], [59, 119], [116, 90], [156, 198], [373, 326]], downsample_ratios: [32, 16, 8]}
  batch_size: 12 # modified
  shuffle: true
  drop_last: true
  mixup_epoch: 25000
  use_shared_memory: true

EvalReader:
  sample_transforms:
    - Decode: {}
    - Resize: {target_size: [1024, 1024], keep_ratio: False, interp: 2} # modified
    - NormalizeImage: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225], is_scale: True}
    - Permute: {}
  batch_size: 8 # modified

```

5. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.
1. 终端输出：
```
-----------  Configuration Arguments -----------
backend: auto
elastic_server: None
force: False
gpus: 0,1,2,3
heter_devices: 
heter_worker_num: None
heter_workers: 
host: None
http_port: None
ips: 127.0.0.1
job_id: None
log_dir: log
np: None
nproc_per_node: None
run_mode: None
scale: 0
server_num: None
servers: 
training_script: tools/train.py
training_script_args: ['-c', 'configs/ppyolo/ppyolov2_r50vd_dcn_spore_coco_v20.yml', '--eval', '--use_vdl', 'True']
worker_num: None
workers: 
------------------------------------------------
launch train in GPU mode!
launch proc_id:3735921 idx:0
launch proc_id:3735926 idx:1
launch proc_id:3735931 idx:2
launch proc_id:3735938 idx:3
/home/LAB/huzy/span/paddle/PaddleDetection/ppdet/modeling/ops.py:476: DeprecationWarning: invalid escape sequence \_
  """"""
/home/LAB/huzy/span/paddle/PaddleDetection/ppdet/modeling/ops.py:1277: DeprecationWarning: invalid escape sequence \l
  """"""
/home/LAB/huzy/.conda/envs/pd/lib/python3.8/site-packages/paddle/tensor/creation.py:130: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if data.dtype == np.object:
I0323 10:56:39.002599 3735921 nccl_context.cc:74] init nccl context nranks: 4 local rank: 0 gpu id: 0 ring id: 0
W0323 10:56:40.937754 3735921 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.6, Runtime API Version: 11.2
W0323 10:56:40.958221 3735921 device_context.cc:465] device: 0, cuDNN Version: 8.3.
loading annotations into memory...
Done (t=0.03s)
creating index...
index created!
/home/LAB/huzy/.conda/envs/pd/lib/python3.8/site-packages/paddle/tensor/creation.py:130: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if data.dtype == np.object:
[03/23 10:56:59] ppdet.utils.checkpoint INFO: ['backbone.res5.res5a.branch2b.norm._mean', 'backbone.res5.res5a.branch2b.norm._variance', 'backbone.res5.res5b.branch2b.norm.bias', 'backbone.res5.res5b.branch2b.norm.weight', 'backbone.res5.res5c.branch2c.conv.weight', 'backbone.res5.res5c.branch2c.norm._mean'] in pretrained weight is not used in the model, and its will not be loaded
[03/23 10:56:59] ppdet.utils.checkpoint INFO: Finish loading model weights: /home/LAB/huzy/.cache/paddle/weights/ResNet50_vd_ssld_pretrained.pdparams

```
2. workerlog.1
```
/home/LAB/huzy/span/paddle/PaddleDetection/ppdet/modeling/ops.py:476: DeprecationWarning: invalid escape sequence \_
  """"""
/home/LAB/huzy/span/paddle/PaddleDetection/ppdet/modeling/ops.py:1277: DeprecationWarning: invalid escape sequence \l
  """"""
/home/LAB/huzy/.conda/envs/pd/lib/python3.8/site-packages/paddle/tensor/creation.py:130: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if data.dtype == np.object:
I0323 10:56:39.002599 3735921 nccl_context.cc:74] init nccl context nranks: 4 local rank: 0 gpu id: 0 ring id: 0
W0323 10:56:40.937754 3735921 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.6, Runtime API Version: 11.2
W0323 10:56:40.958221 3735921 device_context.cc:465] device: 0, cuDNN Version: 8.3.
loading annotations into memory...
Done (t=0.03s)
creating index...
index created!
/home/LAB/huzy/.conda/envs/pd/lib/python3.8/site-packages/paddle/tensor/creation.py:130: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if data.dtype == np.object:
[03/23 10:56:59] ppdet.utils.checkpoint INFO: ['backbone.res5.res5a.branch2b.norm._mean', 'backbone.res5.res5a.branch2b.norm._variance', 'backbone.res5.res5b.branch2b.norm.bias', 'backbone.res5.res5b.branch2b.norm.weight', 'backbone.res5.res5c.branch2c.conv.weight', 'backbone.res5.res5c.branch2c.norm._mean'] in pretrained weight is not used in the model, and its will not be loaded
[03/23 10:56:59] ppdet.utils.checkpoint INFO: Finish loading model weights: /home/LAB/huzy/.cache/paddle/weights/ResNet50_vd_ssld_pretrained.pdparams


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::imperative::Tracer::TraceOp(std::string const&, paddle::imperative::NameVarBaseMap const&, paddle::imperative::NameVarBaseMap const&, paddle::framework::AttributeMap, std::map<std::string, std::string, std::less<std::string >, std::allocator<std::pair<std::string const, std::string > > > const&)
1   paddle::imperative::Tracer::TraceOp(std::string const&, paddle::imperative::NameVarBaseMap const&, paddle::imperative::NameVarBaseMap const&, paddle::framework::AttributeMap, paddle::platform::Place const&, bool, std::map<std::string, std::string, std::less<std::string >, std::allocator<std::pair<std::string const, std::string > > > const&)
2   std::shared_ptr<paddle::imperative::details::NameVarMapTrait<paddle::imperative::VarBase>::Type> paddle::imperative::PrepareData<paddle::imperative::VarBase>(paddle::framework::OperatorWithKernel const&, paddle::imperative::details::NameVarMapTrait<paddle::imperative::VarBase>::Type const&, paddle::framework::OpKernelType const&)
3   paddle::framework::TransformData(paddle::framework::OpKernelType const&, paddle::framework::OpKernelType const&, paddle::framework::Tensor const&, paddle::framework::Tensor*)
4   paddle::framework::TransDataDevice(paddle::framework::Tensor const&, paddle::platform::Place const&, paddle::framework::Tensor*)
5   paddle::framework::TensorCopySync(paddle::framework::Tensor const&, paddle::platform::Place const&, paddle::framework::Tensor*)
6   void paddle::memory::Copy<paddle::platform::CUDAPlace, paddle::platform::CUDAPinnedPlace>(paddle::platform::CUDAPlace, void*, paddle::platform::CUDAPinnedPlace, void const*, unsigned long, CUstream_st*)
7   paddle::platform::GpuMemcpySync(void*, void const*, unsigned long, cudaMemcpyKind)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1648005016 (unix time) try ""date -d @1648005016"" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x386700390158) received by PID 3735921 (TID 0x7f5d4e6794c0) from PID 3735896 ***]


```

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：
paddlepaddle-gpu=2.2.2.post112, PaddleDetection is default, cuDNN=8.3, nccl checked:

```

```
Python 3.8.12 (default, Oct 12 2021, 13:49:34) 
[GCC 7.5.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import paddle
>>> paddle.utils.run_check()
Running verify PaddlePaddle program ... 
W0323 11:10:39.810568 3739188 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.6, Runtime API Version: 11.2
W0323 11:10:39.820714 3739188 device_context.cc:465] device: 0, cuDNN Version: 8.3.
PaddlePaddle works well on 1 GPU.
```

"
Clarifying ppbytemot: PaddleDetection bytetrack model,PaddlePaddle/PaddleDetection,2022-03-22 16:12:41,4,,5432,1176992271,"Hi,

Couild I check if https://github.com/PaddlePaddle/PaddleDetection/blob/develop/configs/mot/bytetrack/README_cn.md is the ppbytemot model submitted to MOT17 Challenge with 79.4 MOT score? It says Bytetrack but was not able to identify the MOT score on the page."
windows下ppyoloe tensorrt部署,PaddlePaddle/PaddleDetection,2022-03-22 12:14:06,17,,5428,1176694473,"训练好了ppyoloe_s模型，通过命令
`python tools/export_model.py -c configs/ppyoloe/ppyoloe_crn_s_300e_voc.yml -o weights=output/ppyoloe_crn_s_300e_voc/best_model exclude_nms=True  trt=True`
导出了trt模型，通过命令
`python deploy/python/infer.py --model_dir=output_inference/ppyoloe_crn_s_300e_voc --image_dir=pics/ --run_mode=trt_fp16 --device=gpu --run_benchmark=True`
预测，加载模型后需要很长的时间优化，五分钟左右，最后成功输出结果，预测速度明显加快，但是每次执行上一行命令都需要等待五六分钟转化模型。怎么样解决第一次优化模型，之后不需要优化转化模型时间？
使用paddle预测库是官网编译的cuda10.2_cudnn7.6.5_avx_mkl-trt7.0.0.11。"
[BUG] deploy程序 在设置batchsize=2时后，程序崩溃,PaddlePaddle/PaddleDetection,2022-03-22 11:22:22,3,,5427,1176638799,"在使用deploy 下的 CPP代码测试时，设置batchSize=1 正常运行；但是设置batchSize=2时，无法正常运行；

使用的目标检测：ObjectDetector::Predict 在    predictor_->Run();时崩溃；
环境：win10 + CPU + VS2017 
paddle_version = version: 2.2.1 官网直接下载
使用的模型：std::string model_dir = ""./output_inference/picodet_s_320_pedestrian""; 官网下载

请问：1、在初始化是就设置了batchsize =2 ；2、对2个图像同时推理，程序异常。 如何正确使用呢？

![image](https://user-images.githubusercontent.com/56465628/159470650-8ec92926-60c0-4d1a-80a5-d03656523e60.png)


![image](https://user-images.githubusercontent.com/56465628/159470746-8a081f7d-49b9-4bb3-b5ab-cbcad459f44e.png)

"
集成的visualDL使用问题,PaddlePaddle/PaddleDetection,2022-03-22 08:27:39,3,,5423,1176434207,"`class VisualDLWriter(Callback):
    """"""
    Use VisualDL to log data or image
    """"""

    def __init__(self, model):
        super(VisualDLWriter, self).__init__(model)

        assert six.PY3, ""VisualDL requires Python >= 3.5""
        try:
            from visualdl import LogWriter
        except Exception as e:
            logger.error('visualdl not found, plaese install visualdl. '
                         'for example: `pip install visualdl`.')
            raise e
        self.vdl_writer = LogWriter(
            lodir=model.cfg.get('vdl_log_dir', 'vdl_log_dir/scalar'),
            file_name='vdlrecords.111')  # 修改file_name实现续写
        self.vdl_loss_step = 0
        self.vdl_mAP_step = 0
        self.vdl_image_step = 0
        self.vdl_image_frame = 0

    def on_step_end(self, status):
        mode = status['mode']
        if dist.get_world_size() < 2 or dist.get_rank() == 0:
            if mode == 'train':
                training_staus = status['training_staus']
                for loss_name, loss_value in training_staus.get().items():
                    # 折线图LOSS
                    self.vdl_writer.add_scalar(loss_name, loss_value,
                                               self.vdl_loss_step)
                    self.vdl_loss_step += 1
            elif mode == 'test':
                ori_image = status['original_image']
                result_image = status['result_image']
                # 图片可视化
                self.vdl_writer.add_image(
                    ""original/frame_{}"".format(self.vdl_image_frame), ori_image,
                    self.vdl_image_step)
                self.vdl_writer.add_image(
                    ""result/frame_{}"".format(self.vdl_image_frame),
                    result_image, self.vdl_image_step)
                self.vdl_image_step += 1
                # each frame can display ten pictures at most.
                if self.vdl_image_step % 10 == 0:
                    self.vdl_image_step = 0
                    self.vdl_image_frame += 1

    def on_epoch_end(self, status):
        mode = status['mode']
        if dist.get_world_size() < 2 or dist.get_rank() == 0:
            if mode == 'eval':
                for metric in self.model._metrics:
                    for key, map_value in metric.get_results().items():
                        # 折线图mAP
                        self.vdl_writer.add_scalar(""{}-mAP"".format(key),
                                                   map_value[0],
                                                   self.vdl_mAP_step)
                self.vdl_mAP_step += 1`


想通过修改callbacks.py中的Use VisualDL to log data or image部分实现恢复训练后的折线图的续写，修改LogWriter（）方法内的file_name变量没有效果，训练产生的log文件仍是默认名称。"
mask-rcnn和yolo的模型能检测到结果，而faster cnn，detr和ppyolo的模型检测不到结果,PaddlePaddle/PaddleDetection,2022-03-18 01:40:09,2,,5394,1173092539,是不是有什么配置文件有不同，没改对呢？
main.cc中函数PredictImage()不能对mask-rcnn结果可视化,PaddlePaddle/PaddleDetection,2022-03-17 08:11:51,3,deploy,5390,1172058987,函数PredictImage()只能对yolo模型的检测结果可视化，不能对mask-rcnn和cascade-rcnn的结果可视化，能提供可视化的函数吗？
每次训练后模型准确率 不同,PaddlePaddle/PaddleDetection,2022-03-14 14:48:55,2,,5365,1168488465,我使用的是pp-yolo_tiny 发现 在同样的配置环境下 第一次训练和 第二次 训练后 测试结果有大约2%左右的浮动 什么情况 是正常么？
MOTA 计算含义,PaddlePaddle/PaddleDetection,2022-03-11 03:40:35,2,question#mot,5346,1165963387,"mota = 1. - quiet_divide(
            (overall_dic['num_misses'] + overall_dic['num_switches'] +
             overall_dic['num_false_positives']), overall_dic['num_objects'])

您好，能帮忙解释下，这个MOTA是怎么计算出来的吗？overall_dic['num_misses']  overall_dic['num_switches']  overall_dic['num_false_positives']  overall_dic['num_objects'] 这几个分别代表什么意思？"
[Other General Issues]训练自己训练集，推理时图片完全检测不出来,PaddlePaddle/PaddleDetection,2022-03-09 14:24:54,3,,5341,1164006583,"数据集是kaggle上的70张的 手枪和剑 目标检测数据集

训练代码：
!python -m paddle.distributed.launch tools/train.py -c configs/yolov3/yolov3_mobilenet_v1_ssld_270e_voc.yml \
                            -o weights=output/yolov3_mobilenet_v1_ssld_270e_voc/model_final.pdparams \
                            --eval
训练结果：
[03/09 14:09:36] ppdet.engine INFO: Epoch: [269] [0/6] learning_rate: 0.000010 loss_xy: 2.230445 loss_wh: 0.576769 loss_obj: 3.632374 loss_cls: 0.339784 loss: 6.829461 eta: 0:00:08 batch_cost: 1.3080 data_cost: 0.5845 ips: 6.1163 images/s
[03/09 14:09:42] ppdet.utils.checkpoint INFO: Save checkpoint: output/yolov3_mobilenet_v1_ssld_270e_voc
[03/09 14:09:42] ppdet.engine INFO: Eval iter: 0
[03/09 14:09:44] ppdet.metrics.metrics INFO: Accumulating evaluatation results...
[03/09 14:09:44] ppdet.metrics.metrics INFO: mAP(0.50, 11point) = 71.03%
[03/09 14:09:44] ppdet.engine INFO: Total sample number: 24, averge FPS: 9.418394009625983
[03/09 14:09:44] ppdet.engine INFO: Best test bbox ap is 0.710.
[03/09 14:09:46] ppdet.utils.checkpoint INFO: Save checkpoint: output/yolov3_mobilenet_v1_ssld_270e_voc
INFO 2022-03-09 14:09:50,245 launch.py:311] Local processes completed.

推理代码：
!python tools/infer.py -c configs/yolov3/yolov3_mobilenet_v1_ssld_270e_voc.yml \
                    --infer_dir=dataset/my/JPEGImages \
                    --output_dir=infer_output/ \
                    --draw_threshold=0.7 \
                    -o weights=output/yolov3_mobilenet_v1_ssld_270e_voc/best_model

但是推理图片里完全没有框出来

"
PicoDet在树莓派上运行的fps是多少啊,PaddlePaddle/PaddleDetection,2022-03-09 10:29:49,2,PP-PicoDet,5340,1163758832,请教大家 PicoDet在树莓派上运行的fps是多少啊
C++部署怎样使用摄像头的数据并显示,PaddlePaddle/PaddleDetection,2022-03-09 08:50:03,2,,5338,1163654239,"您好，我按照教程编译了C++的部署，但是运行main.exe的时候出现了提示，看起来只能使用当前的一个图片进行推理，如果读摄像头的话怎么办呢？谢谢指点
Usage: ./main --model_dir=/PATH/TO/INFERENCE_MODEL/ --image_file=/PATH/TO/INPUT/IMAGE/"
使用 ppyolo 导出模型提示错误,PaddlePaddle/PaddleDetection,2022-03-08 16:48:43,2,,5331,1162886340,"![image](https://user-images.githubusercontent.com/88072901/157285133-3d1cb593-8176-4882-8c6c-c5f8e33ac855.png)
这个什么情况呢？ 我就是把neck换成 bifpn 训练后 想导出训练的 模型 就提示这个错误 "
visual studio 2019编译问题,PaddlePaddle/PaddleDetection,2022-03-08 00:39:17,3,deploy,5321,1162086004,"我将deploy/cpp目录下的object_detector文件添加进我的工程中，出现找不到函数定义的问题，是不是需要添加额外的链接库？
错误如下：

LNK2019：无法解析的外部符号 ""class std::vector<class cv::Mat,class std::allocator<class cv::Mat> > __cdecl PaddleDetection::PadBatch(class std::vector<class cv::Mat,class std::allocator<class cv::Mat> > const &)"" (?PadBatch@PaddleDetection@@YA?AV?$vector@VMat@cv@@V?$allocator@VMat@cv@@@std@@@std@@AEBV23@@Z)，函数 ""public: void __cdecl PaddleDetection::ObjectDetector::Predict(class std::vector<class cv::Mat,class std::allocator<class cv::Mat> >,double,int,int,class std::vector<struct PaddleDetection::ObjectResult,class std::allocator<struct PaddleDetection::ObjectResult> > *,class std::vector<int,class std::allocator<int> > *,class std::vector<double,class std::allocator<double> > *)"" (?Predict@ObjectDetector@PaddleDetection@@QEAAXV?$vector@VMat@cv@@V?$allocator@VMat@cv@@@std@@@std@@NHHPEAV?$vector@UObjectResult@PaddleDetection@@V?$allocator@UObjectResult@PaddleDetection@@@std@@@4@PEAV?$vector@HV?$allocator@H@std@@@4@PEAV?$vector@NV?$allocator@N@std@@@4@@Z) 中引用了该符号

LNK2019:  无法解析的外部符号 ""public: void __cdecl YAML::detail::node_data::mark_defined(void)"" (?mark_defined@node_data@detail@YAML@@QEAAXXZ)，函数 ""public: void __cdecl YAML::detail::node_ref::mark_defined(void)"" (?mark_defined@node_ref@detail@YAML@@QEAAXXZ) 中引用了该符号

LNK2001:  无法解析的外部符号 ""public: static class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > YAML::detail::node_data::empty_scalar"" (?empty_scalar@node_data@detail@YAML@@2V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@A)	

"
[Other General Issues]模型的AP值同时高于P 值和 R值 是否合理,PaddlePaddle/PaddleDetection,2022-03-07 12:01:51,2,question,5319,1161326009,"训练完成会后 模型的验证集 (iou = 0.5,conf=0.3 )ap值为0.964 ，P为0.951，R为0.937  这是否是合理的，是不是AP值的计算代码有点问题
"
picodet的精度和速度是剪枝量化后的模型评估的吗？,PaddlePaddle/PaddleDetection,2022-03-07 09:14:19,2,model compression,5317,1161109999,你好，picodet的精度和速度是剪枝量化后的模型评估的吗？如果不是的话再进行剪枝量化会不会速度更快呢？谢谢
[Other General Issues]训练自己VOC格式数据出错,PaddlePaddle/PaddleDetection,2022-03-05 02:51:55,3,,5308,1160198779,"数据的目录
dataset
-mydata
--JPEGImages
--Annotations
-train.txt
-val.txt
-label_list.txt

运行训练脚本会报错
ValueError: invalid literal for int() with base 10: 'Unspecified'"
使用自己编写的激活函数时报错,PaddlePaddle/PaddleDetection,2022-03-03 08:30:48,3,,5289,1158115138,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.
3. 确认bug是否在新版本里还未修复/The bug has not been fixed in the latest version.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?

```none
请填写命令/A placeholder for the command.
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

3. 您使用的数据集是？/What dataset did you use?

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：

4. 请问您使用的Python版本是？/ Please provide the version of Python you used.

5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.


如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
loss函数中 x轴是 step么？ 怎么算的？,PaddlePaddle/PaddleDetection,2022-03-03 05:31:36,2,,5284,1157988055,"这是我训练的loss函数图 epoch 为500  一次epoch 做218次数据 遍历 

![image](https://user-images.githubusercontent.com/88072901/156502417-a99733fa-bd21-4243-9ef2-6e8ab8cf2da8.png)
"
"PP-TinyPose在C++和python下推理同一张图片得到的效果差距很大,可能是什么原因导致？",PaddlePaddle/PaddleDetection,2022-03-03 05:24:34,2,,5283,1157984214,"## 描述问题/Describe the bug
pp-tinypose的python和c++下推理同一张图片，效果相差大是什么原因导致？

## 复现/Reproduction
1. c++和python都使用的同样的模型，参数均用默认值
```none
./build/main \
    --model_dir=output_inference/picodet_s_320_pedestrian \
    --model_dir_keypoint=output_inference/tinypose_128x96 \
    --image_file=test1.jpg \
    --device=CPU
```
2. 模型使用的是官方直接下载的人体检测和关键点检测模型

3. 第一张是C++推理的结果，第二张是python推理的结果。c++推理的眼睛鼻子位置不对

![keypoint_test1](https://user-images.githubusercontent.com/47072038/156500863-7a06ad92-3515-4866-80ad-a93fba437a94.jpg)
![test1_vis](https://user-images.githubusercontent.com/47072038/156500876-5da25026-dc1d-434c-b1b6-1a6eeb0a7c60.jpg)

## 环境/Environment
1. PaddleDetection=2.3
2. gcc=8.2
3. cuda=10.2,cudnn=8.1
4. tensorrt=8.2.3
5. opencv=4.5.5
6. Ubuntu=16.04
7. python =3.7.4"
SOLOv2: TensorRT inference: no speed gain,PaddlePaddle/PaddleDetection,2022-03-02 12:38:16,4,,5282,1157140874,"Hi,
We are trying to use TensorRT to speed up inference. In particular, we are using DetectorSOLOv2, and installed a version of paddle-paddle-gpu compiled with TensorRT. However, the inference speed remains about the same whether we try `run_mode='paddle'`, `run_mode='trt_fp32'`,  or `run_mode='trt_fp16'`. This is in contrast with the paddle-ocr repo, where we do observe a big increase in speed, especially with fp16. 

Any idea what could be the reason? Any help is appreciated, thanks. Relevant code:

```
    pred_config = PredictConfig(Solov2Config.model_dir)
    detector = DetectorSOLOv2(
        pred_config,
        Solov2Config.model_dir,
        device='GPU',
        run_mode='trt_fp32',
        batch_size=1,
        trt_min_shape=720,
        trt_max_shape=1920,
        trt_opt_shape=1080,
        trt_calib_mode=True,
        cpu_threads=2,
        enable_mkldnn=True)
```"
训练过程中总是突然停掉吗，已经试了很多次。fail to map batch transform [Gt2YoloTarget_6a0f3f],PaddlePaddle/PaddleDetection,2022-03-01 01:32:55,4,,5269,1154742649,"请问这是因为什么呢？报错信息如下：

版本2.3。自建coco类型数据

03/01 00:34:35] ppdet.engine INFO: Epoch: [5] [ 440/2964] learning_rate: 0.002500 loss_xy: 8.643393 loss_wh: 9.077515 loss_iou: 35.186317 loss_iou_aware: 6.146228 loss_obj: 47.617775 loss_cls: 12.512911 loss: 119.323303 eta: 3 days, 18:31:23 batch_cost: 1.0464 data_cost: 0.4236 ips: 15.2905 images/s
[03/01 00:34:54] reader WARNING: fail to map batch transform [Gt2YoloTarget_6a0f3f] with error: index 96 is out of bounds for axis 3 with size 96 and stack:
Traceback (most recent call last):
  File ""/home/xiaokang/projectCode/OCR/PaddleDetection/ppdet/data/reader.py"", line 209, in __next__
    return next(self.loader)
  File ""/home/xiaokang/miniconda3/envs/OCR/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 589, in __next__
    six.reraise(*sys.exc_info())
  File ""/home/xiaokang/miniconda3/envs/OCR/lib/python3.7/site-packages/six.py"", line 719, in reraise
    raise value
  File ""/home/xiaokang/miniconda3/envs/OCR/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 565, in __next__
    data = self._reader.read_next_var_list()
StopIteration

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/xiaokang/projectCode/OCR/PaddleDetection/ppdet/data/reader.py"", line 73, in __call__
    data = f(data)
  File ""/home/xiaokang/projectCode/OCR/PaddleDetection/ppdet/data/transform/batch_operators.py"", line 232, in __call__
    target[best_n, 0, gj, gi] = gx * grid_w - gi
IndexError: index 96 is out of bounds for axis 3 with size 96

ERROR:root:DataLoader reader thread raised an exception!
Exception in thread Thread-6:
Traceback (most recent call last):
  File ""/home/xiaokang/miniconda3/envs/OCR/lib/python3.7/threading.py"", line 926, in _bootstrap_inner
    self.run()
  File ""/home/xiaokang/miniconda3/envs/OCR/lib/python3.7/threading.py"", line 870, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/xiaokang/miniconda3/envs/OCR/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 391, in _thread_loop
    batch = self._get_data()
  File ""/home/xiaokang/miniconda3/envs/OCR/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 505, in _get_data
    batch.reraise()
  File ""/home/xiaokang/miniconda3/envs/OCR/lib/python3.7/site-packages/paddle/fluid/dataloader/worker.py"", line 168, in reraise
    raise self.exc_type(msg)
IndexError: DataLoader worker(3) caught IndexError with message:
Traceback (most recent call last):
  File ""/home/xiaokang/projectCode/OCR/PaddleDetection/ppdet/data/reader.py"", line 209, in __next__
    return next(self.loader)
  File ""/home/xiaokang/miniconda3/envs/OCR/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 589, in __next__
    six.reraise(*sys.exc_info())
  File ""/home/xiaokang/miniconda3/envs/OCR/lib/python3.7/site-packages/six.py"", line 719, in reraise
    raise value
  File ""/home/xiaokang/miniconda3/envs/OCR/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 565, in __next__
    data = self._reader.read_next_var_list()
StopIteration

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/xiaokang/miniconda3/envs/OCR/lib/python3.7/site-packages/paddle/fluid/dataloader/worker.py"", line 320, in _worker_loop
    batch = fetcher.fetch(indices)
  File ""/home/xiaokang/miniconda3/envs/OCR/lib/python3.7/site-packages/paddle/fluid/dataloader/fetcher.py"", line 117, in fetch
    data = self.collate_fn(data)
  File ""/home/xiaokang/projectCode/OCR/PaddleDetection/ppdet/data/reader.py"", line 79, in __call__
    raise e
  File ""/home/xiaokang/projectCode/OCR/PaddleDetection/ppdet/data/reader.py"", line 73, in __call__
    data = f(data)
  File ""/home/xiaokang/projectCode/OCR/PaddleDetection/ppdet/data/transform/batch_operators.py"", line 232, in __call__
    target[best_n, 0, gj, gi] = gx * grid_w - gi
IndexError: index 96 is out of bounds for axis 3 with size 96


Traceback (most recent call last):
  File ""tools/train.py"", line 177, in <module>
    main()
  File ""tools/train.py"", line 173, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 127, in run
    trainer.train(FLAGS.eval)
  File ""/home/xiaokang/projectCode/OCR/PaddleDetection/ppdet/engine/trainer.py"", line 381, in train
    for step_id, data in enumerate(self.loader):
  File ""/home/xiaokang/projectCode/OCR/PaddleDetection/ppdet/data/reader.py"", line 209, in __next__
    return next(self.loader)
  File ""/home/xiaokang/miniconda3/envs/OCR/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 565, in __next__
    data = self._reader.read_next_var_list()
SystemError: (Fatal) Blocking queue is killed because the data reader raises an exception.
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] (at /paddle/paddle/fluid/operators/reader/blocking_queue.h:166)
"
[Other General Issues],PaddlePaddle/PaddleDetection,2022-02-28 12:26:49,2,,5267,1154032697,picodet 系列的目标检测模型最后的输出和其他目标检测模型输出不同，怎么修改配置使其输出和其他模型一致。另外，paddleclas中的那个picodet主体检测模型的输出反而和其他模型的输出是一样的。
C++ Inference of mask-rcnn model not detecting any objects.,PaddlePaddle/PaddleDetection,2022-02-25 10:26:10,5,,5259,1150284405,"Hi, Thank you for your work.

I trained mask-rcnn model with a custom dataset and exported the model using below command

```
 python tools/export_model.py -c configs/mask_rcnn/mask_rcnn_r50_1x_coco.yml --output_dir=./inference_model -o weights=output/mask_rcnn_r50_1x_coco/model_final.pdparams
```

I tested the exported model using python inference and it works well

```
python deploy/python/infer.py --model_dir=inference_model/mask_rcnn_r50_1x_coco --image_dir=dataset/coco/images --device=GPU
```

For C++ inference I build code following the steps in - https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.3/deploy/cpp/docs/windows_vs2019_build.md . Downloaded  cuda11.0_cudnn8.0_avx_mkl_trt7 | paddle_inference.zip and ran the following command : 

```
C:\Users\PaddleDetection\deploy\cpp\build\Release>main.exe --model_dir=mask_rcnn_r50_1x_coco --image_dir=images --device=GPU

Visualized output saved as output\images\0DB22O007-A11@8.jpg
images\0DB22O007-Q11@8.jpg The number of detected box: 0
```
But it doesn't detect any objects. The original image is saved as the output image.

Can you please advise what can be the issue here.   Thanks

Environment Details
-------------
OS : Windows10
GPU : 2080Ti
CUDA : 11.2
CUDANN : 8.2.1

"
cascadercnn使用matrix nms报错[BUG],PaddlePaddle/PaddleDetection,2022-02-25 03:07:15,2,,5257,1149990051,"
<img width=""890"" alt=""0d69010d94b5bf3aacbdbbb7fa5b9a58"" src=""https://user-images.githubusercontent.com/32731935/155646161-7e94c82b-1933-4ca9-95e4-480a61ada544.png"">

"
[BUG]多卡训练PicoDet崩溃,PaddlePaddle/PaddleDetection,2022-02-24 08:21:55,6,,5255,1148993007,"环境：
System: Ubuntu18.04, TiTan RTX*8
PaddlePaddle: 2.2
PaddleDetection: 2.3

现象：
每训练个几轮就报错：
```
INFO 2022-02-24 12:04:29,407 launch_utils.py:341] terminate all the procs
ERROR 2022-02-24 12:04:29,407 launch_utils.py:602] ABORT!!! Out of all 8 trainers, the trainer process with rank=[0] was aborted. Please check its log.
INFO 2022-02-24 12:04:33,411 launch_utils.py:341] terminate all the procs
INFO 2022-02-24 12:04:33,412 launch.py:311] Local processes completed.
```
每次报错的rank=[n]不尽相同，workerlog显示：
```
--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::imperative::BasicEngine::Execute()
1   paddle::imperative::GradientAccumulator::CallReduceHooks()
2   paddle::imperative::Reducer::AddDistHook(unsigned long)
3   paddle::imperative::Reducer::MarkVarReady(unsigned long, bool)
4   paddle::imperative::Reducer::FinalizeBackward()
5   paddle::imperative::Reducer::ProcessUnusedDenseVars()
6   void paddle::memory::Copy<paddle::platform::CPUPlace, paddle::platform::CUDAPlace>(paddle::platform::CPUPlace, void*, paddle::platform::CUDAPlace, void const*, unsigned long, CUstream_st*)
7   paddle::platform::GpuMemcpyAsync(void*, void const*, unsigned long, cudaMemcpyKind, CUstream_st*)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1645607039 (unix time) try ""date -d @1645607039"" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e7000005ba) received by PID 1649 (TID 0x7fdfba9da740) from PID 1466 ***]
```

只能单卡吗？"
Windows 10 : Inference demo fails without an error,PaddlePaddle/PaddleDetection,2022-02-24 07:56:47,9,,5254,1148972432,"Hi all, Thank you for your work.

- I installed PaddlePaddle and PaddleDetection using below steps : 
  -- python -m pip install paddlepaddle-gpu==2.2.2.post112 -f https://www.paddlepaddle.org.cn/whl/windows/mkl/avx/stable.html
  -- git clone https://github.com/PaddlePaddle/PaddleDetection.git
  -- cd PaddleDetection
  -- cython-bbox - pip install -e git+https://github.com/samson-wang/cython_bbox.git#egg=cython-bbox
  -- pycocotools - pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI
  -- pip install -r requirements.txt
  -- python setup.py install

-  python ppdet/modeling/tests/test_architectures.py
```
C:\Users\anaconda3\envs\py_env\lib\site-packages\win32\lib\pywintypes.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp, sys, os
W0224 15:47:09.692596 30896 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.6, Runtime API Version: 11.2
W0224 15:47:09.723845 30896 device_context.cc:465] device: 0, cuDNN Version: 8.2.
.......
----------------------------------------------------------------------
Ran 7 tests in 1.700s

OK
```
-  Tested the import
```
>>> import paddle
>>> paddle.utils.run_check()
Running verify PaddlePaddle program ...
PaddlePaddle works well on 1 GPU.
PaddlePaddle works well on 1 GPUs.
PaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.
>>> print(paddle.__version__)
2.2.2
```
- Ran the Inference demo check, but fails without any error message.  The weights were successfully downloaded. Please see the below output. 
``` 
(py_env) C:\Users\PaddleDetection>python tools/infer.py -c configs/ppyolo/ppyolo_r50vd_dcn_1x_coco.yml -o use_gpu=true weights=https://paddledet.bj.bcebos.com/models/ppyolo_r50vd_dcn_1x_coco.pdparams --infer_img=demo/000000014439.jpg
C:\Users\anaconda3\envs\py_env\lib\site-packages\win32\lib\pywintypes.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp, sys, os
W0224 15:53:00.975919 16344 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.6, Runtime API Version: 11.2
W0224 15:53:00.991541 16344 device_context.cc:465] device: 0, cuDNN Version: 8.2.
[02/24 15:53:04] ppdet.utils.checkpoint INFO: Finish loading model weights: C:\Users/.cache/paddle/weights\ppyolo_r50vd_dcn_1x_coco.pdparams

(py_env) C:\Users\xjera\PaddleDetection>
```

Please advise what can be the issue here. Thanks
"
paddlelite-generic-demo中运行自己模型问题,PaddlePaddle/PaddleDetection,2022-02-24 00:57:34,5,,5251,1148738652,"大家好，我想问下大家是否在部署时有同样的经历，请教下。我是在ssd.yml配置基上的修改batch_size、epoch、自己VOC格式数据集（只有270张图），标签只有一类，windows GPU训练好后导出模型后，在windows上用python deploy/python/infer.py 脚本来测试图和视频都正确，再用paddlelite opt工具在linux下优化得到nb模型和用windows导出的模型，PaddleLite-generic-demo_v2_10_0的image_classification_demo和ssd_detection_demo都在rk1808上都能跑对，但是ssd_detection_demo跑自己模型报中间有个算子维度不匹配，可能是模型输入输出的问题吧。相关资料
[myssd.zip](https://github.com/PaddlePaddle/PaddleDetection/files/8128818/myssd.zip)
"
PPYOLOv2训练自己的数据集，读取速度很慢,PaddlePaddle/PaddleDetection,2022-02-23 12:54:05,4,,5250,1148055533,"![image](https://user-images.githubusercontent.com/49087498/155323198-43344fc9-513c-4d6a-86c4-25c85e2233a0.png)
"
训练maskrcnn报错,PaddlePaddle/PaddleDetection,2022-02-22 09:07:05,6,,5243,1146636683,"使用voc训练集训练maskrcnn，train_val.txt格式如下：
D:/VOC2012/JPEGImages/2007_000032.jpg D:/VOC2012/SegmentationObject/2007_000032.png
报错为：
xml.etree.ElementTree.ParseError: not well-formed (invalid token): line 1, column 0
定位到报错位置发现，在voc.py文件中运行解析xml文件报错，但是我没有xml文件啊，用的是分割标注的图
"
[BUG]旋转框检测export_model后infer的结果不对,PaddlePaddle/PaddleDetection,2022-02-22 00:29:00,4,,5238,1146330573,"export_model之前是正常的，之后就没有结果了。

python tools/export_model.py -c configs/dota/s2anet_1x_spine.yml -o weights=output/s2anet_1x_spine/model_final
python deploy/python/infer.py --model_dir=./output_inference/s2anet_1x_spine --image_file=demo/39006.jpg --device=GPU

"
[BUG] tools/infer.py预测上千张图片时候，程序KILLED，查询原因是内存用完了out of memory。,PaddlePaddle/PaddleDetection,2022-02-21 14:22:19,1,,5237,1145865742,"（1）运行tools/infer.py批量预测到1000+张图片时，程序killed
排查ppdet/enige/trainer.py代码定位到 
 loader = create('TestReader')(self.dataset, 0) 
是out of memory的原因。


（2）继续排查，发现问题出现在core/workspace.py的create（）函数的
    cls = getattr(config.pymodule, name)
    cls_kwargs = {}
    cls_kwargs.update(global_config[name])

（3）进一步排查cls，发现问题在ppdet/data/reader.py
class BaseDataLoader(object):

（4）请问怎么处理上述问题？"
PAFNet模型测速问题,PaddlePaddle/PaddleDetection,2022-02-21 02:38:51,7,,5234,1145250515,"在PAFNet的论文中，开发人员给出TTFNet-Darknet在V100上的infer_time是9.79ms，FPS达到100+
我最近在3090上测试时则发现需要29.4ms，**这个差距让我产生了疑问。**
我的测试方法是根据issue中提及的使用deploy/infer.py进行,开启benchmark得到的结果如下
<img width=""604"" alt=""image"" src=""https://user-images.githubusercontent.com/59326436/154880028-8c907985-541f-4346-8ce1-6ee4232808e8.png"">
期待您的回复～
"
[BUG]UnicodeDecodeError: 'gbk' codec can't decode byte 0xaa in position 182: illegal multibyte sequence,PaddlePaddle/PaddleDetection,2022-02-18 02:59:13,7,,5229,1142246269,"**PaddleDetection team appreciate any suggestion or problem you delivered~**
train.py 训练时报错
UnicodeDecodeError: 'gbk' codec can't decode byte 0xaa in position 182: illegal multibyte sequence
"
Converting ppyolov2 onnx to tensorrt failed,PaddlePaddle/PaddleDetection,2022-02-16 12:19:13,2,,5219,1139942221,"I trained PPYOLOv2 on custom dataset and now i try to convert it to tensorrt for Jetson Nano. I used export_model.py script and Paddle2ONNX lib. But when i try to convert onnx file to tensorrt, i have error

[02/08/2022-13:56:50] [W] [TRT] onnx2trt_utils.cpp:220: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.
ERROR: builtin_op_importers.cpp:2371 In function importRange:
[8] Assertion failed: inputs.at(0).isInt32() && ""For range operator with dynamic inputs, this version of TensorRT only supports INT32!""

I used onnx-tensorrt repo, trtexec, also i tried to simplify onnx with onnx-typecast and onnx-simplifyer. Changing opset and TensorRT versions didn't help me.

--run_mode=trt_fp32 is not good for me because i want to run inference immediately with once converted trt model and tensorrt python API.

How can i fix this error?"
paddle有关于手部关键点的跟踪识别项目吗？,PaddlePaddle/PaddleDetection,2022-02-16 10:30:34,3,,5218,1139828504,paddle有关于手部关键点的跟踪识别项目吗？或者哪个项目可以改为训练手部关键点模型的？
[BUG]pp-tracking bug,PaddlePaddle/PaddleDetection,2022-02-16 04:12:58,3,bug,5216,1139510456,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.
3. 确认bug是否在新版本里还未修复/The bug has not been fixed in the latest version.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.
在 pp-tracking中pipeline.cpp中有2次if (save_result_),直接2次執行或comment 1個都可執行
但產生的flow_statistic.txt為空,
mot_output.txt內容為
43 36429.213 224.424 18.3888 35.3849
44 1683.328 259.566 32.6199 71.6621
...
與records.push_back(""result format: frame_id, track_id, x1, y1, w, h\n"");
定義不符能不能賜教一下,謝謝

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?

```none
请填写命令/A placeholder for the command.
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

3. 您使用的数据集是？/What dataset did you use?

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：

4. 请问您使用的Python版本是？/ Please provide the version of Python you used.

5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.


如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
运行picodet和车流量统计的infer时，越来越卡。,PaddlePaddle/PaddleDetection,2022-02-14 10:08:55,5,,5208,1137066465,"python运行 mot_jde_infer.py
出现以下情况：



-----------  Running Arguments -----------
batch_size: 1
camera_id: 99
cpu_threads: 1
device: GPU
do_entrance_counting: True
draw_center_traj: False
enable_mkldnn: False
image_dir: None
image_file: None
model_dir: output_inference/fairmot_hrnetv2_w18_dlafpn_30e_576x320_bdd100kmot_vehicle
mtmct_cfg: None
mtmct_dir: None
output_dir: output
reid_batch_size: 50
reid_model_dir: None
run_benchmark: False
run_mode: fluid
save_images: False
save_mot_txts: False
scaled: False
secs_interval: 2
threshold: 0.5
trt_calib_mode: False
trt_max_shape: 1920
trt_min_shape: 1
trt_opt_shape: 640
video_file: None
------------------------------------------
-----------  Model Configuration -----------
Model Arch: FairMOT
Transform Order: 
--transform op: LetterBoxResize
--transform op: NormalizeImage
--transform op: Permute
--------------------------------------------
fps: 8, frame_count: -73786976294838208
ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers
  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)
  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared
  libavutil      55. 78.100 / 55. 78.100
  libavcodec     57.107.100 / 57.107.100
  libavformat    57. 83.100 / 57. 83.100
  libavdevice    57. 10.100 / 57. 10.100
  libavfilter     6.107.100 /  6.107.100
  libavresample   3.  7.  0 /  3.  7.  0
  libswscale      4.  8.100 /  4.  8.100
  libswresample   2.  9.100 /  2.  9.100
  libpostproc    54.  7.100 / 54.  7.100
Frame id: 1, Total count: 12, In count: 0, Out count: 0
Input #0, rawvideo, from 'pipe:':
  Duration: N/A, start: 0.000000, bitrate: 398131 kb/s
    Stream #0:0: Video: rawvideo (BGR[24] / 0x18524742), bgr24, 1920x1080, 398131 kb/s, 8 tbr, 8 tbn, 8 tbc
detect frame: 1, fps: 0.958276
Frame id: 2, Total count: 14, In count: 0, Out count: 0
Stream mapping:
  Stream #0:0 -> #0:0 (rawvideo (native) -> h264 (libx264))
[libx264 @ 0x55f5bd4c09c0] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512
[libx264 @ 0x55f5bd4c09c0] profile Constrained Baseline, level 4.0
[libx264 @ 0x55f5bd4c09c0] 264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=0 ref=1 deblock=0:0:0 analyse=0:0 me=dia subme=0 psy=1 psy_rd=1.00:0.00 mixed_ref=0 me_range=16 chroma_me=1 trellis=0 8x8dct=0 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=0 threads=34 lookahead_threads=5 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=0 weightp=0 keyint=250 keyint_min=8 scenecut=0 intra_refresh=0 rc=crf mbtree=0 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=0
Output #0, flv, to 'rtmp://live.speedchina.cn/test/test2?auth_key=1641778928-0-0-f967537b970f94e322128eaa31b2299f':
  Metadata:
    encoder         : Lavf57.83.100
    Stream #0:0: Video: h264 (libx264) ([7][0][0][0] / 0x0007), yuv420p, 1920x1080, q=-1--1, 8 fps, 1k tbn, 8 tbc
    Metadata:
      encoder         : Lavc57.107.100 libx264
    Side data:
      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1
detect frame: 2, fps: 1.817034
Frame id: 3, Total count: 19, In count: 0, Out count: 0
detect frame: 3, fps: 2.578214
Frame id: 4, Total count: 20, In count: 0, Out count: 0
detect frame: 4, fps: 3.285148
Frame id: 5, Total count: 22, In count: 0, Out count: 1
detect frame: 5, fps: 3.909353


以前用的时候都是正常的，现在突然在运行时候出现

![1](https://user-images.githubusercontent.com/71886349/153843507-07d1afdc-7b52-407a-9ec4-2f5e9f6a3f45.png)

以前是没有这一块的提示"
多GPU训练faster-rcnn-swins训练时出错,PaddlePaddle/PaddleDetection,2022-02-12 08:07:15,4,,5202,1133692151,"用同样数据单卡多卡训练过picodet其中的一个小模型是正常的，用同样的数据训练faster-rcnn-swin时，
单gpu训练正常，但是用4卡的多gpu出错。batch_size=4,   img_size=640x640


os.environ[""CUDA_VISIBLE_DEVICES""] = '0,1,2,3'
gpu_device='0,1,2,3'
config_faster_rcnn_swin_640=""configs/faster_rcnn/faster_rcnn_swin_tiny_fpn_3x_coco.yml""
os.system(f""python -m paddle.distributed.launch --gpus {gpu_device} tools/train.py -c {config_faster_rcnn_swin_640} --eval"")

错误信息如下：

`[02/12 15:51:51] ppdet.data.source.voc WARNING: Found an invalid bbox in annotations: xml_file: /mnt//data/Annotations/RGB84.xml, x1: 1084.79, y1: 356.16, x2: 1084.79, y2: 356.16.

[02/12 15:51:53] ppdet.data.source.voc WARNING: Found an invalid bbox in annotations: xml_file: /mnt/data/Annotations/19720210809.xml, x1: 639.0, y1: 146.0, x2: 639.0, y2: 146.0.

INFO 2022-02-12 15:52:12,964 launch_utils.py:320] terminate process group gid:2606667
INFO 2022-02-12 15:52:12,966 launch_utils.py:320] terminate process group gid:2606672
INFO 2022-02-12 15:52:12,967 launch_utils.py:320] terminate process group gid:2606677

INFO 2022-02-12 15:52:16,972 launch_utils.py:341] terminate all the procs

ERROR 2022-02-12 15:52:16,973 launch_utils.py:604] ABORT!!! Out of all 4 trainers, the trainer process with rank=[3] was aborted. Please check its log.

INFO 2022-02-12 15:52:20,977 launch_utils.py:341] terminate all the procs
INFO 2022-02-12 15:52:20,977 launch.py:311] Local processes completed.
`

**log中0卡信息：**
`

C++ Traceback (most recent call last):

0   paddle::imperative::Tracer::TraceOp(std::string const&, paddle::imperative::NameVarBaseMap const&, paddle::imperative::NameVarBaseMap const&, paddle::framework::AttributeMap, std::map<std::string, std::string, std::less<std::string >, std::allocator<std::pair<std::string const, std::string > > > const&)
1   paddle::imperative::Tracer::TraceOp(std::string const&, paddle::imperative::NameVarBaseMap const&, paddle::imperative::NameVarBaseMap const&, paddle::framework::AttributeMap, paddle::platform::Place const&, bool, std::map<std::string, std::string, std::less<std::string >, std::allocator<std::pair<std::string const, std::string > > > const&)
2   paddle::imperative::PreparedOp::Run(paddle::imperative::NameVarBaseMap const&, paddle::imperative::NameVarBaseMap const&, paddle::framework::AttributeMap const&, paddle::framework::AttributeMap const&)
3   std::_Function_handler<void (paddle::framework::ExecutionContext const&), paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CUDAPlace, false, 0ul, paddle::operators::MatMulV2Kernel<paddle::platform::CUDADeviceContext, float>, paddle::operators::MatMulV2Kernel<paddle::platform::CUDADeviceContext, double>, paddle::operators::MatMulV2Kernel<paddle::platform::CUDADeviceContext, paddle::platform::float16>, paddle::operators::MatMulV2Kernel<paddle::platform::CUDADeviceContext, paddle::platform::complex<float> >, paddle::operators::MatMulV2Kernel<paddle::platform::CUDADeviceContext, paddle::platform::complex<double> > >::operator()(char const*, char const*, int) const::{lambda(paddle::framework::ExecutionContext const&)#1}>::_M_invoke(std::_Any_data const&, paddle::framework::ExecutionContext const&)
4   void paddle::operators::MatMulFunction<paddle::platform::CUDADeviceContext, float>(paddle::framework::Tensor const*, paddle::framework::Tensor const*, paddle::framework::Tensor*, bool, bool, paddle::framework::ExecutionContext const&, bool)
5   void paddle::operators::MatMulFunction<paddle::platform::CUDADeviceContext, float>(paddle::framework::Tensor const*, paddle::framework::Tensor const*, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, paddle::framework::Tensor*, bool, bool, paddle::framework::ExecutionContext const&, bool)
6   paddle::framework::Tensor::mutable_data(paddle::platform::Place const&, paddle::framework::proto::VarType_Type, unsigned long)
7   paddle::memory::AllocShared(paddle::platform::Place const&, unsigned long)
8   paddle::memory::allocation::AllocatorFacade::AllocShared(paddle::platform::Place const&, unsigned long)
9   paddle::memory::allocation::AllocatorFacade::Alloc(paddle::platform::Place const&, unsigned long)


Error Message Summary:

FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1644652332 (unix time) try ""date -d @1644652332"" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3dc0027c5f9) received by PID 2606667 (TID 0x7f1bd5f844c0) from PID 2606585 ***]
`



**log中1卡信息：**

`SystemError: (Fatal) Operator elementwise_add raises an paddle::memory::allocation::BadAlloc exception.
The exception content is
:ResourceExhaustedError: 

Out of memory error on GPU 1. Cannot allocate 72.351807MB memory on GPU 1, 10.755859GB memory has been allocated and available memory is only 5.437500MB.

Please check whether there is any other process using GPU 1.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 

 (at /paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:79)
. (at /paddle/paddle/fluid/imperative/tracer.cc:221)


C++ Traceback (most recent call last):

No stack trace in paddle, may be caused by external reasons.


Error Message Summary:

FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1644652332 (unix time) try ""date -d @1644652332"" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3dc0027c5f9) received by PID 2606672 (TID 0x7f350ea804c0) from PID 2606585 ***]
`

**log中卡2信息：**
`SystemError: (Fatal) Operator elementwise_add raises an paddle::memory::allocation::BadAlloc exception.
The exception content is
:ResourceExhaustedError: 

Out of memory error on GPU 2. Cannot allocate 72.351807MB memory on GPU 2, 10.755859GB memory has been allocated and available memory is only 5.437500MB.

Please check whether there is any other process using GPU 2.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 

 (at /paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:79)
. (at /paddle/paddle/fluid/imperative/tracer.cc:221)`



**log中卡3信息：**
`SystemError: (Fatal) Operator elementwise_add raises an paddle::memory::allocation::BadAlloc exception.
The exception content is
:ResourceExhaustedError: 

Out of memory error on GPU 3. Cannot allocate 72.351807MB memory on GPU 3, 10.755859GB memory has been allocated and available memory is only 5.437500MB.

Please check whether there is any other process using GPU 3.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 

 (at /paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:79)
. (at /paddle/paddle/fluid/imperative/tracer.cc:221)`"
实例分割的预测结果怎么保存为 mask.json ？,PaddlePaddle/PaddleDetection,2022-02-11 04:35:47,2,,5197,1131699752,实例分割的 预测结果（infer） 怎么保存为 mask.json ？
ppyolotiny 想把 neck 换成 bifpn 提示 这个错误求 大神怎么修改呢？？,PaddlePaddle/PaddleDetection,2022-02-10 07:22:45,16,,5192,1129564049,"
![image](https://user-images.githubusercontent.com/88072901/153357478-19039c87-db36-4ee7-85b2-05d3e1fedcf2.png)

还有默认输出 一个256 通道的层，我想改成 三个通道 就是 跟inchannels 一样改就可以么？"
mask rcnn 预测无结果,PaddlePaddle/PaddleDetection,2022-02-09 15:15:37,4,,5185,1128670851,"验证集的精度结果
![image](https://user-images.githubusercontent.com/43838913/153230267-4c65ca8f-f5b7-4af9-b846-71d2f2b3c9be.png)

预测时使用的命令
``
python tools/infer.py -c configs/cascade_rcnn/cascade_rcnn_r50_vd_fpn_ssld_2x_coco.yml \
                    --infer_img=dataset/train/images/60989d89338b1a07010142e0.jpg \
                    --output_dir=infer_img/ \
                    --draw_threshold=0.5 \
                    -o weights=output/cascade_mask_rcnn_r50_vd_fpn_ssld_2x_coco/model_final.pdparams
``

打开输出文件夹中的输出图像，没有任何预测结果，上面预测的图像还是训练集中的图像，请问我这可能是哪的问题？"
ppyolotiny head loss 是用了 iou loss 而不是  l1 loss吧,PaddlePaddle/PaddleDetection,2022-02-09 06:12:36,2,,5180,1128097226,"![image](https://user-images.githubusercontent.com/88072901/153132457-a9b4cc63-b1b9-4251-af9c-682ad2e6db75.png)

"
[BUG]src/picodet_postprocess.cc:91:53: error: 'ceil' was not declared in this scope,PaddlePaddle/PaddleDetection,2022-02-09 03:48:58,1,,5179,1128016330,"
PP-TinyPose   following steps below:
///////////////////////////////////////////////////////////////////////////////////////////
cd {PadddleDetection_Root}
cd deploy/lite/

inference_lite_path=/{lite prediction library path}/inference_lite_lib.android.armv8.gcc.c++_static.with_extra.with_cv/
mkdir $inference_lite_path/demo/cxx/lite

cp -r Makefile src/ include/ *runtime_config.json $inference_lite_path/demo/cxx/lite

cd $inference_lite_path/demo/cxx/lite

# 执行编译，等待完成后得到可执行文件main
make ARM_ABI=arm8
#如果是arm7，则执行 make ARM_ABI = arm7 (或者在Makefile中修改该项）
///////////////////////////////////////////////////////////////////////////////////////////
报错src/picodet_postprocess.cc:91:53: error: 'ceil' was not declared in this scope ，这个是否是已经提交的release 2.10版本的bug呢？麻烦帮忙看下吧 

"
tinypose 是否可以run在mali  G52  gpu上，PP目前是否已经支持,PaddlePaddle/PaddleDetection,2022-02-09 01:44:59,2,,5178,1127949850,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

    tinypose 是否可以run在mali c52 gpu上，PP目前是否已经支持
我只看到arm cpu上运行的tinypose文档说明，但没有看到gpu版本的
，如果有的话是否可以提供下链接， tks

"
channel shuffle 不是要分三组么？,PaddlePaddle/PaddleDetection,2022-02-09 00:28:10,2,,5177,1127901445,"![image](https://user-images.githubusercontent.com/88072901/153098626-e8f60785-88d3-4c18-b6bf-f7a35f5f0f9a.png)

图所示 为啥group 不是 3 而是2？

下面是 channel shuffle 函数
![image](https://user-images.githubusercontent.com/88072901/153098716-42bbab9d-43e4-41af-a36e-ffe8926afc3b.png)
"
[BUG]跑tinypose demo 同一张图片裁剪后得到的结果不一样,PaddlePaddle/PaddleDetection,2022-01-31 08:55:36,6,,5173,1119145743,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.
3. 确认bug是否在新版本里还未修复/The bug has not been fixed in the latest version.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.
同一张图片原图和裁剪后的图关键点出现漂移缩放现象

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?

```none
请填写命令/A placeholder for the command.
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

3. 您使用的数据集是？/What dataset did you use?

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：

4. 请问您使用的Python版本是？/ Please provide the version of Python you used.

5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.


如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
[Other General Issues]支持中文吗,PaddlePaddle/PaddleDetection,2022-01-29 16:53:12,3,,5171,1118272732,"简单来说 支持中文吗

识别的目标 返回的内容可以是中文吗（好像只支持英文？）
比如识别目标是苹果 返回的是结果 apple  我想返回“苹果”字样可以吗

我训练了中文的，顺利训练完，但是无法结果返回“苹果”  如果是apple 就一切正常~"
"After replacing hard swish with sigmoid, the training error",PaddlePaddle/PaddleDetection,2022-01-27 01:18:00,5,,5164,1115663606,"command is:
`
os.system(f""python -m paddle.distributed.launch --gpus {gpu_device} tools/train.py -c {config_picodet_l_416} --eval"")
`
batch_size has been reduced

The pretrain_weights in picodet_l_416_coco.yml have been commented out:
`pretrain_weights:https://paddledet.bj.bcebos.com/models/pretrained/ESNet_x1_25_pretrained.pdparams` 
Why is there still the following prompt in the error report:


`[01/26 21:32:59] ppdet.utils.checkpoint INFO: The shape [384, 96, 1, 1] in pretrained weight 4_3._se.conv2.weight is unmatched with the shape [480, 120, 1, 1] in model backbone.4_3._se.conv2.weight. And the weight 4_3._se.conv2.weight will not be loaded
[01/26 21:32:59] ppdet.utils.checkpoint INFO: Finish loading model weights: 

/home/zhangming/.cache/paddle/weights/ESNet_x1_0_pretrained.pdparams
INFO 2022-01-26 21:33:23,518 launch_utils.py:320] terminate process group gid:3533444
INFO 2022-01-26 21:33:23,518 launch_utils.py:320] terminate process group gid:3533454
INFO 2022-01-26 21:33:23,519 launch_utils.py:320] terminate process group gid:3533459
INFO 2022-01-26 21:33:27,531 launch_utils.py:341] terminate all the procs
ERROR 2022-01-26 21:33:27,532 launch_utils.py:604] ABORT!!! Out of all 4 trainers, the trainer process with rank=[1] was aborted. Please check its log.

INFO 2022-01-26 21:33:31,536 launch_utils.py:341] terminate all the procs
INFO 2022-01-26 21:33:31,536 launch.py:311] Local processes completed.
`


log is:
`[01/26 21:32:59] ppdet.utils.checkpoint INFO: The shape [384] in pretrained weight 4_3._se.conv2.bias is unmatched with the shape [480] in model backbone.4_3._se.conv2.bias. And the weight 4_3._se.conv2.bias will not be loaded
[01/26 21:32:59] ppdet.utils.checkpoint INFO: The shape [384, 96, 1, 1] in pretrained weight 4_3._se.conv2.weight is unmatched with the shape [480, 120, 1, 1] in model backbone.4_3._se.conv2.weight. And the weight 4_3._se.conv2.weight will not be loaded
[01/26 21:32:59] ppdet.utils.checkpoint INFO: Finish loading model weights: /home/zhangming/.cache/paddle/weights/ESNet_x1_0_pretrained.pdparams



C++ Traceback (most recent call last):

0   void paddle::memory::Copy<paddle::platform::CPUPlace, paddle::platform::CUDAPlace>(paddle::platform::CPUPlace, void*, paddle::platform::CUDAPlace, void const*, unsigned long, CUstream_st*)
1   paddle::platform::GpuMemcpySync(void*, void const*, unsigned long, cudaMemcpyKind)


Error Message Summary:


FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1643204003 (unix time) try ""date -d @1643204003"" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3dc0035ea21) received by PID 3533444 (TID 0x7f90b59834c0) from PID 3533345 ***]
`"
官方给的 ppyolo_tiny 是 ppyolov1的吧 我看neck 还是 FPN 我想改成PAN 是改这个部分是么？,PaddlePaddle/PaddleDetection,2022-01-18 15:33:25,2,,5132,1107049504,"![捕获yml](https://user-images.githubusercontent.com/88072901/149967740-af0f378e-440d-4aba-9219-0961ed2fa8de.PNG)
如上图所示 修改这个部分就可以吧"
Which model was used for MOT17?,PaddlePaddle/PaddleDetection,2022-01-17 06:39:46,6,mot,5120,1105439431,"Hi,

I see that paddle has the state of the art tracker on MOT17:

![image](https://user-images.githubusercontent.com/48631399/149719901-58808806-91c4-4d74-a645-2d5f6eff252d.png)

Can I check which is the mot model that was used? I cannot find the score of **79.4** on any of the provided trackers in https://github.com/PaddlePaddle/PaddleDetection/tree/develop/configs/mot"
ModuleNotFoundError: No module named 'ppdet.utils.export_utils',PaddlePaddle/PaddleDetection,2022-01-15 14:09:14,4,,5111,1104729782,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://github.com/PaddlePaddle/PaddleDetection/tree/develop/docs/tutorials/FAQ) /I have read the [FAQ documentation](https://github.com/PaddlePaddle/PaddleDetection/tree/develop/docs/tutorials/FAQ) but cannot get the expected help.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.

在static文件夹下想export模型，报错：Traceback (most recent call last):
  File ""tools/export_model.py"", line 53, in <module>
    raise e
  File ""tools/export_model.py"", line 40, in <module>
    from ppdet.utils.export_utils import save_infer_model, dump_infer_config
ModuleNotFoundError: No module named 'ppdet.utils.export_utils'
不知道咋解决，请求大家的帮助
python tools/export_model.py -c ./configs/rcnn_enhance/generic/cascade_rcnn_dcn_r101_vd_fpn_gen_server_side.yml --output_dir=./inference_model -o weights=./weights/cascade_rcnn_dcn_r101_vd_fpn_gen_server_side.pdparams

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?

```none
python tools/export_model.py -c ./configs/rcnn_enhance/generic/cascade_rcnn_dcn_r101_vd_fpn_gen_server_side.yml --output_dir=./inference_model -o weights=./weights/cascade_rcnn_dcn_r101_vd_fpn_gen_server_side.pdparams
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

3. 您使用的数据集是？/What dataset did you use?

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：
Paddle2.2.1 PaddleDetection**
2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：
win10
4. 请问您使用的Python版本是？/ Please provide the version of Python you used.
Python3.7
5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.
CUDA11.2  cuDNN8.1

如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
评估是会报内存oom异常,PaddlePaddle/PaddleDetection,2022-01-13 05:00:24,6,environment,5103,1101208632,在执行eval.py时，内存占用很高，请问这是为什么呢？我在PaddleSeg和PaddleDetection都遇到了这个问题。显存和GPU都正常，内存一直在增长，最后程序崩溃
[Other General Issues]使用自定义数据集coco格式 导出模型预测时候标签不对/mobiledetbenchmark-demo,PaddlePaddle/PaddleDetection,2022-01-12 09:32:32,2,deploy,5099,1100101576,"**PaddleDetection team appreciate any suggestion or problem you delivered~**



## 描述问题/Describe the bug
两个问题：
1.使用自定义的coco格式的数据集(用voc格式转换的) 然后能训练 但是导出模型后发现infer_cfg.yml的label_list下面都是coco的80个类别，然后预测时候框出来的都错了，我又尝试运行https://aistudio.baidu.com/aistudio/projectdetail/2882573?channelType=0&channel=0 这个项目里的 结果就检测不出来图片里的内容 生成的结果图片没有标注框 
还有一个现象就是 同样的数据集voc格式和coco格式训练完效果相差很多 我感觉也是上述label_list出现问题导致的 voc导出的label_list就是我自己定义的
2.mobiledetbenchmark的demo app 的param的input都是data 我转弯onnx又简化后再转成的param的input变成了image 然后我模仿其他的param 改成了对应的data 结果延迟只有个位数。。。 这个结果正常吗? 还有这个benchmark app能否在自己的数据集上进行验证呢


## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：
paddlepaddle-gpu=2.2.0
paddledetection release/2.3
AI studio平台"
[BUG]请问CPU可以运行pp-detection gui吗，我10S的视频导入后一直让我“模型运行中请等待“”，等了很久也没有进行下一步,PaddlePaddle/PaddleDetection,2022-01-12 07:51:34,6,,5097,1099992048,"**PaddleDetection team appreciate any suggestion or problem you delivered~**
![541215c82191b9a94c70f38d7cf40ba](https://user-images.githubusercontent.com/82854498/149085832-2087a25f-bbde-4132-9dc3-09e3b14d1b03.png)
run了main.py后选择行人单检测模式，选择导入视频后一直在这步
## Checklist:
![541215c82191b9a94c70f38d7cf40ba](https://user-images.githubusercontent.com/82854498/149085791-e51b0829-e215-4158-9d2b-5510eee63192.png)

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.
3. 确认bug是否在新版本里还未修复/The bug has not been fixed in the latest version.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?

```none
请填写命令/A placeholder for the command.
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

3. 您使用的数据集是？/What dataset did you use?

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：

4. 请问您使用的Python版本是？/ Please provide the version of Python you used.

5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.


如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
如何在源码中安全地添加代码以实现读取图片数据时若图片数据不存在就跳过这一功能？,PaddlePaddle/PaddleDetection,2022-01-09 14:08:16,2,,5084,1097206007,"使用paddlecloud训练paddledetection-release-2.0时，由于使用挂载的方式将集群上的数据挂载到paddlecloud的目录下，但是由于网络不稳定的原因，数据常常存在读不到的情况，导致transform/operators.py中报错
![image](https://user-images.githubusercontent.com/53338761/148685511-120df8be-7bbe-4546-800a-bb4806eeaab8.png)
如何安全地添加数据不存在就跳过，直接读取下一张这个逻辑呢？
"
paddle2onnx 不支持输入hw不一致的情况吗？,PaddlePaddle/PaddleDetection,2022-01-07 05:42:24,2,,5077,1096004574,"![image](https://user-images.githubusercontent.com/27938658/148497633-1246b809-0d33-47a6-bed5-1d33c3595255.png)
输入是352*576，报错；变成512*512就可以了"
使用的是最新版本的pp-picodet想要复现得到展示的结果，请问除了设置enable_ce和输入export FLAGS_cudnn_deterministic=True，还需要如何设置其他的随机量才行，这些随机量都在哪里？,PaddlePaddle/PaddleDetection,2022-01-06 03:25:17,8,,5067,1094931055,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://github.com/PaddlePaddle/PaddleDetection/tree/develop/docs/tutorials/FAQ) /I have read the [FAQ documentation](https://github.com/PaddlePaddle/PaddleDetection/tree/develop/docs/tutorials/FAQ) but cannot get the expected help.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?

```none
请填写命令/A placeholder for the command.
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

3. 您使用的数据集是？/What dataset did you use?

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：

4. 请问您使用的Python版本是？/ Please provide the version of Python you used.

5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.


如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
deepsort跨境追踪,PaddlePaddle/PaddleDetection,2022-01-05 03:45:47,4,documentation,5063,1093950909,如何使用fairmot做跨境追踪，可以提供详细操作步骤吗，类似deepsort的readme。是否需要重新去做训练
利用PaddleDetection 2.3 增加新模块，历史问题，还没有解决？,PaddlePaddle/PaddleDetection,2022-01-04 02:18:16,2,config,5050,1092952794,"https://github.com/PaddlePaddle/PaddleDetection/issues/4961#issue-1084528879
上面是链接，主要是实在不知道怎么解决，尝试了好几次仍然不行。"
请问yolox，yolop啥时候会有。yolo系列,PaddlePaddle/PaddleDetection,2022-01-03 12:09:42,6,feature request,5047,1092457503,请问yolox，yolop啥时候会有。yolo系列
关于fasterrcnn swin模型这个eval的keep_ratio和test的keep_ratio为什么不一样呀？,PaddlePaddle/PaddleDetection,2022-01-03 05:30:05,3,,5046,1092238072,https://aistudio.baidu.com/paddle/forum/topic/show/993130
paddleserving部署出错,PaddlePaddle/PaddleDetection,2021-12-30 10:26:23,2,,5037,1091013339,"报了如下的错误，案例跑不通

 File ""../../deploy/serving/test_client.py"", line 13, in <module>
    client = Client()
  File ""/home/easyits/anaconda3/envs/paddleserving/lib/python3.7/site-packages/paddle_serving_client/client.py"", line 149, in __init__
    from .serving_client import PredictorRes
ImportError: /lib64/libk5crypto.so.3: undefined symbol: EVP_KDF_ctrl, version OPENSSL_1_1_1b

"
将自己的数据集转换为coco格式的数据集时出现问题,PaddlePaddle/PaddleDetection,2021-12-29 09:06:33,2,,5025,1090372874,"请问我应该如何准备使用solov2进行分割的数据，我用labelme标注四边形的四个点后，把它转换成矩形后，能够转换成coco格式的数据集，但是实例不是矩形的；如果我直接使用四边形，则转换出来的coco格式的标注如下，什么信息都没有，请问该怎么解决？有没有使用solov2分割自己数据集的教程啊？
![image](https://user-images.githubusercontent.com/92715968/147645277-1f92d879-86a9-4c9f-a94e-dbbe60a6ae37.png)
"
怎样将paddledetection的动态图模型放在脚本文件中运行？,PaddlePaddle/PaddleDetection,2021-12-29 03:34:10,2,,5022,1090246311,"类似于静态图的模型文件有""__model__"",""__params__"",我可以在脚本文件中使用下面的命令进行运行：
place = fluid.CUDAPlace(0) if train_parameters['use_gpu'] else fluid.CPUPlace()
exe = fluid.Executor(place)
path = ""../output/ppyolo""
[inference_program, feed_target_names, fetch_targets] = fluid.io.load_inference_model(dirname=path, executor=exe,
                                                                                      model_filename='__model__',
                                                                                      params_filename='__params__')

但是如果将其中的""__model__"",""__params__""替换成“.pdmodel”,“.pdiparams”的动态图模型后就会报错：
Traceback (most recent call last):
  File ""D:/Users/Administrator/Desktop/Pythonfile/Video_test/workfile/video_img_video.py"", line 60, in <module>
    GetObj(images_initial, images_final)
  File ""D:/Users/Administrator/Desktop/Pythonfile/Video_test/workfile/video_img_video.py"", line 32, in GetObj
    result = a.infer(img_paths, '/%d.jpg' % i)
  File ""D:\Users\Administrator\Desktop\Pythonfile\Video_test\workfile\people_det_demo.py"", line 121, in infer
    batch_outputs = exe.run(inference_program,
  File ""D:\Anaconda3\lib\site-packages\paddle\fluid\executor.py"", line 1268, in run
    six.reraise(*sys.exc_info())
  File ""D:\Anaconda3\lib\site-packages\six.py"", line 719, in reraise
    raise value
  File ""D:\Anaconda3\lib\site-packages\paddle\fluid\executor.py"", line 1256, in run
    return self._run_impl(
  File ""D:\Anaconda3\lib\site-packages\paddle\fluid\executor.py"", line 1466, in _run_impl
    return self._run_program(
  File ""D:\Anaconda3\lib\site-packages\paddle\fluid\executor.py"", line 1544, in _run_program
    program = self._add_feed_fetch_ops(
  File ""D:\Anaconda3\lib\site-packages\paddle\fluid\executor.py"", line 759, in _add_feed_fetch_ops
    if not has_feed_operators(global_block, feed, feed_var_name):
  File ""D:\Anaconda3\lib\site-packages\paddle\fluid\executor.py"", line 281, in has_feed_operators
    raise Exception(""'feed_targets' does not have {} variable"".
Exception: 'feed_targets' does not have scale_factor variable
这是为什么？"
[BUG]the nums of out channel is not equal to the declaration of the class in the darknet module,PaddlePaddle/PaddleDetection,2021-12-28 10:31:47,2,,5018,1089779139,"**the nums of out channel is not equal to the declaration of the class in the darknet module**

## LOCATION:
PaddleDetection/ppdet/modeling/backbones/darknet.py >>> class BasicBlock(nn.Layer)

## VERSION 
release 2.3

## 描述问题/Describe the bug
below is the definition codes about the BasicBlock
```python
class BasicBlock(nn.Layer):
    def __init__(self,
                 ch_in,
                 ch_out,
                 norm_type='bn',
                 norm_decay=0.,
                 freeze_norm=False,
                 data_format='NCHW'):
        super(BasicBlock, self).__init__()

        self.conv1 = ConvBNLayer(
            ch_in=ch_in,
            ch_out=ch_out,
            filter_size=1,
            stride=1,
            padding=0,
            norm_type=norm_type,
            norm_decay=norm_decay,
            freeze_norm=freeze_norm,
            data_format=data_format)
        self.conv2 = ConvBNLayer(
            ch_in=ch_out,
            ch_out=ch_out * 2,
            filter_size=3,
            stride=1,
            padding=1,
            norm_type=norm_type,
            norm_decay=norm_decay,
            freeze_norm=freeze_norm,
            data_format=data_format)

    def forward(self, inputs):
        conv1 = self.conv1(inputs)
        conv2 = self.conv2(conv1)
        out = paddle.add(x=inputs, y=conv2)
        return out
```
Suppose that i have a input variable with **shape [5,10,9,9]** and a **BasicBlock** instance named **block with the ch_in=10, ch_out=5**,
that mean i want have a **output with shape [5,5,9,9]**, but the actually the **output is a tensor with shape [5,10,9,9]**
Below is the codes that i have test in jupyternotebook
```python
import os
import paddle
os.chdir(""/home/PaddleDetection"")
from ppdet.modeling.backbones.darknet import BasicBlock

input = paddle.rand([5,10,9,9])
block = BasicBlock(10,5)
output = block(input)
print(f""input shape : {input.shape}\n"")
print(f""layer: \n{block}\n"")
print(f""output shape : {output.shape}"")
```
and the print result is:
```python
input shape : [5, 10, 9, 9]

layer: 
BasicBlock(
  (conv1): ConvBNLayer(
    (conv): Conv2D(10, 5, kernel_size=[1, 1], data_format=NCHW)
    (batch_norm): BatchNorm2D(num_features=5, momentum=0.9, epsilon=1e-05)
  )
  (conv2): ConvBNLayer(
    (conv): Conv2D(5, 10, kernel_size=[3, 3], padding=1, data_format=NCHW)
    (batch_norm): BatchNorm2D(num_features=10, momentum=0.9, epsilon=1e-05)
  )
)

output shape : [5, 10, 9, 9]
```
and if I  input variable with shape [5,10,9,9] to the block with the ch_in=10, ch_out=20 or the block with the ch_in=10, ch_out=20, there will encounter a ValuError:
```python
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/tmp/ipykernel_17986/1117496369.py in <module>
      6 input = paddle.rand([5,10,9,9])
      7 block = BasicBlock(10,10)
----> 8 output = block(input)
      9 output.shape
     10 print(f""input shape : {input.shape}\n"")

/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py in __call__(self, *inputs, **kwargs)
    912                 self._built = True
    913 
--> 914             outputs = self.forward(*inputs, **kwargs)
    915 
    916             for forward_post_hook in self._forward_post_hooks.values():

/home/PaddleDetection/ppdet/modeling/backbones/darknet.py in forward(self, inputs)
    174         conv1 = self.conv1(inputs)
    175         conv2 = self.conv2(conv1)
--> 176         out = paddle.add(x=inputs, y=conv2)
    177         return out
    178 

/usr/local/lib/python3.7/dist-packages/paddle/tensor/math.py in add(x, y, name)
    238 
    239     if in_dygraph_mode():
--> 240         return _C_ops.elementwise_add(x, y)
    241 
    242     return _elementwise_op(LayerHelper('elementwise_add', **locals()))

ValueError: (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [5, 10, 9, 9] and the shape of Y = [5, 20, 9, 9]. Received [10] in X is not equal to [20] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || x_dims_array[i] <= 1 || y_dims_array[i] <= 1 == true, but received x_dims_array[i] == y_dims_array[i] || x_dims_array[i] <= 1 || y_dims_array[i] <= 1:0 != true:1.] (at /paddle/paddle/fluid/operators/elementwise/elementwise_op_function.h:240)
  [operator < elementwise_add > error]
```
and

```python
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/tmp/ipykernel_17986/3869671924.py in <module>
      6 input = paddle.rand([5,10,9,9])
      7 block = BasicBlock(10,20)
----> 8 output = block(input)
      9 print(f""input shape : {input.shape}\n"")
     10 print(f""layer: \n{block}\n"")

/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py in __call__(self, *inputs, **kwargs)
    912                 self._built = True
    913 
--> 914             outputs = self.forward(*inputs, **kwargs)
    915 
    916             for forward_post_hook in self._forward_post_hooks.values():

/home/PaddleDetection/ppdet/modeling/backbones/darknet.py in forward(self, inputs)
    174         conv1 = self.conv1(inputs)
    175         conv2 = self.conv2(conv1)
--> 176         out = paddle.add(x=inputs, y=conv2)
    177         return out
    178 

/usr/local/lib/python3.7/dist-packages/paddle/tensor/math.py in add(x, y, name)
    238 
    239     if in_dygraph_mode():
--> 240         return _C_ops.elementwise_add(x, y)
    241 
    242     return _elementwise_op(LayerHelper('elementwise_add', **locals()))

ValueError: (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [5, 10, 9, 9] and the shape of Y = [5, 40, 9, 9]. Received [10] in X is not equal to [40] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || x_dims_array[i] <= 1 || y_dims_array[i] <= 1 == true, but received x_dims_array[i] == y_dims_array[i] || x_dims_array[i] <= 1 || y_dims_array[i] <= 1:0 != true:1.] (at /paddle/paddle/fluid/operators/elementwise/elementwise_op_function.h:240)
  [operator < elementwise_add > error]
```
I know that is a tiny issue, so would you please fix that in the feature, pleaseure
"
使用solov2训练自己的数据集出现问题,PaddlePaddle/PaddleDetection,2021-12-28 08:19:58,2,,5016,1089698016,"根据数据转换教程，我将自己使用labelme标注的数据集转换成coco格式的数据集后，使用ppyolo进行目标检测时没有出现问题，但是使用solov2进行实例分割训练时，出现了如下问题，自己尝试好多次了，都没解决，望指点，不胜感激，555~
![image](https://user-images.githubusercontent.com/92715968/147544781-322dbcb9-5915-432d-b264-f6fdf07a773e.png)
![image](https://user-images.githubusercontent.com/92715968/147544852-26cfa28d-d9ce-42ea-a233-845d103a5e61.png)
"
训练开启多卡训练开启--eval 验证会卡死,PaddlePaddle/PaddleDetection,2021-12-27 02:28:41,2,training,5002,1088914436,"
## 描述问题/Describe the bug
1. 主要会卡死
2. 训练日志无相关loss
训练中无法开启--eval验证，一开启训练会之间卡死; 关闭训练，相关的loss都没有打印输出

日志位于log/workerlog.0
![image](https://user-images.githubusercontent.com/2733701/147428129-21c96d1a-65d7-4db1-8fde-63b74c51a26a.png)


## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?

```
python3 -m paddle.distributed.launch --gpus 0,1,2,3 tools/train.py -c configs/chefHat_ppyolo/ppyolo_r50vd_dcn_chefHat.yml -r output/ppyolo_r50vd_dcn_chefHat/54
```

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.

## 环境/Environment

使用p4 4卡环境
![image](https://user-images.githubusercontent.com/2733701/147428150-cb3949d7-390d-4df0-baba-44ccb63d95ea.png)
![image](https://user-images.githubusercontent.com/2733701/147428238-aa72a710-239e-4e43-b7f8-4cec68ddfa0a.png)

"
使用自己的数据集训练目标跟踪，推理的时候没有结果?,PaddlePaddle/PaddleDetection,2021-12-22 09:33:31,6,,4987,1086631478,
[Other General Issues]picodet模型使用export_model导出模型后，预测时最多得到64个目标框，此限制怎么去除？,PaddlePaddle/PaddleDetection,2021-12-22 06:45:39,6,,4984,1086506525,
检测速度慢，fps数值低导致结果显示特别卡顿,PaddlePaddle/PaddleDetection,2021-12-21 15:24:46,4,,4978,1085947853,"
## 描述问题/Describe the bug
A clear and concise description of what the bug is.

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?
根据readme中的以下部分：
```bash
# 下载行人跟踪demo视频：
wget https://bj.bcebos.com/v1/paddledet/data/mot/demo/mot17_demo.mp4

# Python预测视频
python deploy/pptracking/python/mot_jde_infer.py --model_dir=output_inference/fairmot_hrnetv2_w18_dlafpn_30e_576x320 --video_file=mot17_demo.mp4 --device=GPU --threshold=0.5 --save_mot_txts --save_images
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.
未做任何更改

3. 您使用的数据集是？/What dataset did you use?

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.
无报错，fps太低，仅仅14左右，检测结果显示卡顿
![7ed319233925daeb01dbc22ccfccbc7](https://user-images.githubusercontent.com/55084546/146954942-84155d63-48fa-4346-8939-785f61a728be.png)


## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：
Paddle版本2.2.0, PaddleDetection版本2.3.0 

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：
无

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：
ubuntu 20.04

4. 请问您使用的Python版本是？/ Please provide the version of Python you used.
python 3.7

5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.
CUDA 10.2， cuDNN 7.6

如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
cascade_rcnn_r50_fpn_1x_coco.yml 如何去掉fpn模块,PaddlePaddle/PaddleDetection,2021-12-21 12:07:20,3,,4976,1085764195,"想去掉fpn部分做个对比实验，但是paddledetection里面没有现成的，看到fasterrcnn的文件夹里面有类似的。所以按照fasterrcnn的样子修改以后，算法直接失效了，请问应该如何修改。
![image](https://user-images.githubusercontent.com/35593546/146927162-54a6aaac-9d8c-4044-99fb-bc4cbd56b4df.png)
![image](https://user-images.githubusercontent.com/35593546/146927181-d424a99c-4a48-4661-aa1b-a1c71e038293.png)
这是我按照faster配置文件修改的两个地方
![a4ce43a0c00b1946c69d9913dd29df3](https://user-images.githubusercontent.com/35593546/146927273-d97290a7-c97a-4eba-8aef-7389b3e57c7e.png)
这个是修炼时评估模型的信息，根本没有效果
![8ff366080a149d72a56da6a5cd5cc5b](https://user-images.githubusercontent.com/35593546/146927436-5efba656-69b3-48ef-a7d7-94add3d3004e.png)
"
使用paddle.summary()显示模型结构参数等信息,PaddlePaddle/PaddleDetection,2021-12-20 14:53:29,2,framework question,4970,1084880937,"利用paddle.summary()函数数想显示ppyolo_r50vd_dcn_1x_visdrone（configs/sniper/ppyolo_r50vd_dcn_1x_visdrone.yml）模型的参数，但是给出了下面的错误提示：
```bash
---------------print params-----------------
Traceback (most recent call last):
  File ""tools/train.py"", line 171, in <module>
    main()
  File ""tools/train.py"", line 167, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 118, in run
    trainer = Trainer(cfg, mode='train')
  File ""/PaddleDetectionNew/ppdet/engine/trainer.py"", line 90, in __init__
    self.model = create(cfg.architecture)
  File ""/PaddleDetectionNew/ppdet/core/workspace.py"", line 275, in create
    return cls(**cls_kwargs)
  File ""/PaddleDetectionNew/ppdet/modeling/architectures/yolo.py"", line 61, in __init__
    params_info = paddle.summary(model, (1, 3, 320, 320))
  File ""/anaconda3/envs/dxl/lib/python3.7/site-packages/paddle/hapi/model_summary.py"", line 223, in summary
    result, params_info = summary_string(net, _input_size, dtypes, input)
  File ""<decorator-gen-278>"", line 2, in summary_string
  File ""/anaconda3/envs/dxl/lib/python3.7/site-packages/paddle/fluid/dygraph/base.py"", line 331, in _decorate_function
    return func(*args, **kwargs)
  File ""/anaconda3/envs/dxl/lib/python3.7/site-packages/paddle/hapi/model_summary.py"", line 353, in summary_string
    model(*x)
  File ""/anaconda3/envs/dxl/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 914, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File ""/PaddleDetectionNew/ppdet/modeling/backbones/resnet.py"", line 581, in forward
    x = inputs['image']
  File ""/anaconda3/envs/dxl/lib/python3.7/site-packages/paddle/fluid/dygraph/varbase_patch_methods.py"", line 598, in __getitem__
    return self._getitem_index_not_tensor(item)
ValueError: (InvalidArgument) Currently, Tensor.__indices__() only allows indexing by Integers, Slices, Ellipsis, None, tuples of these types and list of Bool and Integers, but received str in 1th slice item (at /paddle/paddle/fluid/pybind/imperative.cc:637)
```
添加的代码为：
```python
@register
class YOLOv3(BaseArch):
    __category__ = 'architecture'
    __shared__ = ['data_format']
    __inject__ = ['post_process']

    def __init__(self,
                 backbone='DarkNet',
                 neck='YOLOv3FPN',
                 yolo_head='YOLOv3Head',
                 post_process='BBoxPostProcess',
                 data_format='NCHW',
                 for_mot=False):
        """"""
        YOLOv3 network, see https://arxiv.org/abs/1804.02767

        Args:
            backbone (nn.Layer): backbone instance
            neck (nn.Layer): neck instance
            yolo_head (nn.Layer): anchor_head instance
            bbox_post_process (object): `BBoxPostProcess` instance
            data_format (str): data format, NCHW or NHWC
            for_mot (bool): whether return other features for multi-object tracking
                models, default False in pure object detection models.
        """"""
        super(YOLOv3, self).__init__(data_format=data_format)
        self.backbone = backbone
        self.neck = neck
        self.yolo_head = yolo_head
        self.post_process = post_process
        self.for_mot = for_mot
        self.return_idx = isinstance(post_process, JDEBBoxPostProcess)
        print(""---------------print params-----------------"")
        model = self.backbone
        params_info = paddle.summary(model, (1, 3, 320, 320))
        print(params_info)
```
就是最后几行代码，从print()函数到结束。我也查过贵方的issue中关于打印模型结构的问题，但是仍然没有找到怎么解决。
参考的api文档：https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/summary_cn.html。
问题：想知道应该怎么使用这个函数来显示模型结构，就那上面这个ppyolo_r50vd_dcn_1x_visdrone模型来举例？

paddlepaddle：2.2.0；PaddleDetection：2.3"
[Other General Issues]如何更换模型里的neck部分？,PaddlePaddle/PaddleDetection,2021-12-20 14:10:59,14,config,4968,1084836505,"比如说我想把ppyolo中的yolofpn更换为BIFPN（其在modeling文件中已存在），应该修改哪些yml文件？
谢谢~"
验证集和测试集评估结果差距很大，什么问题？,PaddlePaddle/PaddleDetection,2021-12-20 12:58:31,2,,4967,1084761391,"用ppyolo-tiny 做了 目标检测的二分类，训练后的模型 对验证集的 评估结果Iou=0.5:0.95 时51.4 但是测试集则才16.4左右 ，然后自己一张一张图去infer后看 分类结果都正确。
这个是什么情况呢？？
测试集
![image](https://user-images.githubusercontent.com/88072901/146770590-83ef3a8d-e4a7-450c-bd93-bc8497fbe99f.png)
验证集
![image](https://user-images.githubusercontent.com/88072901/146770733-8e9f4425-9f8e-4ae9-b99b-399ed426df77.png)
"
"导出模型设置了参数TestReader.fuse_normalize=true,模型部署后无输出结果",PaddlePaddle/PaddleDetection,2021-12-20 09:42:20,2,deploy,4964,1084571055,在导出模型时使用了TestReader.fuse_normalize=true， 865上模型无输出结果，没有使用TestReader.fuse_normalize=true的时候，有输出结果，请问是什么原因，应该修改哪些地方
"pp-picodet模型转化为paddle-lite模型用于安卓部署时，出现这个错误Error: This model is not supported, because kernel for 'calib' is not supported by Paddle-Lite.",PaddlePaddle/PaddleDetection,2021-12-20 06:28:07,8,,4957,1084411603,"pp-picodet模型转化为paddle-lite模型用于安卓部署时，出现这个错误Error: This model is not supported, because kernel for 'calib' is not supported by Paddle-Lite.   ，怎么解决啊？


2021-11-01 17:26:46.023 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:46.036 672-898/com.android.systemui E/ResourcesManager: failed to add asset path /data/app/com.baidu.paddledetection.detection-My2Bwk8gI2Gh-YhuN4b3yQ==/base.apk
2021-11-01 17:26:46.036 672-898/com.android.systemui E/ResourcesManager: failed to add asset path /data/app/com.baidu.paddledetection.detection-My2Bwk8gI2Gh-YhuN4b3yQ==/base.apk
2021-11-01 17:26:46.036 672-898/com.android.systemui E/ResourcesManager: failed to add asset path /data/app/com.baidu.paddledetection.detection-My2Bwk8gI2Gh-YhuN4b3yQ==/base.apk
2021-11-01 17:26:46.037 672-898/com.android.systemui E/ResourcesManager: failed to add asset path /data/app/com.baidu.paddledetection.detection-My2Bwk8gI2Gh-YhuN4b3yQ==/base.apk
2021-11-01 17:26:46.037 672-898/com.android.systemui E/ResourcesManager: failed to add asset path /data/app/com.baidu.paddledetection.detection-My2Bwk8gI2Gh-YhuN4b3yQ==/base.apk
2021-11-01 17:26:46.052 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:46.052 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:46.056 263-1025/? E/BufferQueueProducer: [com.android.launcher3/com.android.launcher3.Launcher#0] disconnect: not connected (req=1)
2021-11-01 17:26:46.306 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:46.342 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:46.368 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:46.368 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:46.387 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:46.387 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:46.406 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:46.407 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:46.429 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:46.429 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:46.450 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:46.510 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:46.523 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:46.523 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:46.538 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:46.612 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:46.633 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:46.633 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:46.645 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:46.645 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:46.664 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:47.467 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:47.484 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:26:48.020 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:27:00.004 521-541/system_process E/memtrack: Couldn't load memtrack module
2021-11-01 17:27:00.033 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:27:00.068 263-263/? E/HWComposer: getPresentFence failed for invalid display -1
2021-11-01 17:27:00.081 521-541/system_process E/memtrack: Couldn't load memtrack module
2021-11-01 17:27:00.125 521-541/system_process E/memtrack: Couldn't load memtrack module
2021-11-01 17:27:02.740 18579-18579/? A/Paddle-Lite: [F 11/ 1 17:27: 2.740 /island/Paddle-Lite/lite/core/program.cc RuntimeProgram:347] Check failed: (kernels.size() > 0): 0!>0 
    Error: This model is not supported, because kernel for 'calib' is not supported by Paddle-Lite.
2021-11-01 17:27:02.740 18579-18579/? A/libc: Fatal signal 6 (SIGABRT), code -6 in tid 18579 (ction.detection), pid 18579 (ction.detection)
2021-11-01 17:27:02.793 18727-18727/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
2021-11-01 17:27:02.793 18727-18727/? A/DEBUG: Build fingerprint: 'rockchip/rk3399/rk3399:8.1.0/OPM8.190605.003/101510:userdebug/test-keys'
2021-11-01 17:27:02.793 18727-18727/? A/DEBUG: Revision: '0'
2021-11-01 17:27:02.793 18727-18727/? A/DEBUG: ABI: 'arm64'
2021-11-01 17:27:02.793 18727-18727/? A/DEBUG: pid: 18579, tid: 18579, name: ction.detection  >>> com.baidu.paddledetection.detection <<<
2021-11-01 17:27:02.793 18727-18727/? A/DEBUG: signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------
2021-11-01 17:27:02.795 18727-18727/? A/DEBUG: Abort message: '[F 11/ 1 17:27: 2.740 /island/Paddle-Lite/lite/core/program.cc RuntimeProgram:347] Check failed: (kernels.size() > 0): 0!>0 
    Error: This model is not supported, because kernel for 'calib' is not supported by Paddle-Lite.
    '"
window系统是不是不支持pp-picodet模型多卡训练？,PaddlePaddle/PaddleDetection,2021-12-18 10:31:19,4,,4945,1083802088,window系统是不是不支持pp-picodet模型多卡训练？
configs/picodet/picodet_s_320_coco.yml配置的AutoAugment: {autoaug_type: v1}训练出错,PaddlePaddle/PaddleDetection,2021-12-18 09:25:16,2,,4943,1083790858,"configs/picodet/picodet_s_320_coco.yml配置的
TrainReader: 
     sample_transforms:
          - AutoAugment: {autoaug_type: v1}
 在训练的时候出错了，怎么解决？

训练命令： python tools/train.py -c configs/picodet/picodet_s_320_coco.yml --eval

(base) F:\murong\projects1\PaddleDetection-release-2.3>python tools/train.py -c configs/picodet/picodet_s_320_coco.yml --eval
C:\Users\Dev16\Anaconda3\lib\site-packages\socks.py:58: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  from collections import Callable
C:\Users\Dev16\Anaconda3\lib\site-packages\win32\lib\pywintypes.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp, sys, os
C:\Users\Dev16\Anaconda3\lib\site-packages\paddle\tensor\creation.py:130: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any b
ehavior and is safe.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if data.dtype == np.object:
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
W1218 17:13:32.105926  2104 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 11.1, Runtime API Version: 10.2
W1218 17:13:32.113931  2104 device_context.cc:465] device: 0, cuDNN Version: 7.6.
[12/18 17:13:32] reader WARNING: fail to map sample transform [AutoAugment_f7f412] with error: Cannot handle this data type: (1, 1, 4), <f4 and stack:
Traceback (most recent call last):
  File ""C:\Users\Dev16\Anaconda3\lib\site-packages\PIL\Image.py"", line 2680, in fromarray
    mode, rawmode = _fromarray_typemap[typekey]
KeyError: ((1, 1, 4), '<f4')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""F:\murong\projects1\PaddleDetection-release-2.3\ppdet\data\reader.py"", line 54, in __call__
    data = f(data)
  File ""F:\murong\projects1\PaddleDetection-release-2.3\ppdet\data\transform\operators.py"", line 105, in __call__
    sample = self.apply(sample, context)
  File ""F:\murong\projects1\PaddleDetection-release-2.3\ppdet\data\transform\operators.py"", line 594, in apply
    self.autoaug_type)
  File ""F:\murong\projects1\PaddleDetection-release-2.3\ppdet\data\transform\autoaugment_utils.py"", line 1586, in distort_image_with_autoaugment
    augmentation_hparams)
  File ""F:\murong\projects1\PaddleDetection-release-2.3\ppdet\data\transform\autoaugment_utils.py"", line 1548, in build_and_apply_nas_policy
    tf_policies, image, bboxes)
  File ""F:\murong\projects1\PaddleDetection-release-2.3\ppdet\data\transform\autoaugment_utils.py"", line 1496, in select_and_apply_random_policy
    image, bboxes = policy(image, bboxes)
  File ""F:\murong\projects1\PaddleDetection-release-2.3\ppdet\data\transform\autoaugment_utils.py"", line 1540, in final_policy
    prob, bboxes_)
  File ""F:\murong\projects1\PaddleDetection-release-2.3\ppdet\data\transform\autoaugment_utils.py"", line 1484, in _apply_func_with_prob
    augmented_image, augmented_bboxes = func(image, bboxes, *args)
  File ""F:\murong\projects1\PaddleDetection-release-2.3\ppdet\data\transform\autoaugment_utils.py"", line 747, in translate_y_only_bboxes
    image, bboxes, prob, translate_y, func_changes_bbox, pixels, replace)
  File ""F:\murong\projects1\PaddleDetection-release-2.3\ppdet\data\transform\autoaugment_utils.py"", line 706, in _apply_multi_bbox_augmentation_wrapper
    new_image, new_bboxes, prob, aug_func, func_changes_bbox, *args)
  File ""F:\murong\projects1\PaddleDetection-release-2.3\ppdet\data\transform\autoaugment_utils.py"", line 687, in _apply_multi_bbox_augmentation
    idx, (image, new_bboxes) = body(idx, (image, new_bboxes))
  File ""F:\murong\projects1\PaddleDetection-release-2.3\ppdet\data\transform\autoaugment_utils.py"", line 685, in <lambda>
    _images_and_bboxes[1])]
  File ""F:\murong\projects1\PaddleDetection-release-2.3\ppdet\data\transform\autoaugment_utils.py"", line 655, in <lambda>
    wrapped_aug_func = lambda _image, bbox, _new_bboxes: _apply_bbox_augmentation_wrapper(_image, bbox, _new_bboxes, prob, aug_func, func_changes_bbox, *args)
  File ""F:\murong\projects1\PaddleDetection-release-2.3\ppdet\data\transform\autoaugment_utils.py"", line 611, in _apply_bbox_augmentation_wrapper
    augmentation_func, *args)
  File ""F:\murong\projects1\PaddleDetection-release-2.3\ppdet\data\transform\autoaugment_utils.py"", line 535, in _apply_bbox_augmentation
    augmented_bbox_content = augmentation_func(bbox_content, *args)
  File ""F:\murong\projects1\PaddleDetection-release-2.3\ppdet\data\transform\autoaugment_utils.py"", line 857, in translate_y
    image = Image.fromarray(wrap(image))
  File ""C:\Users\Dev16\Anaconda3\lib\site-packages\PIL\Image.py"", line 2682, in fromarray
    raise TypeError(""Cannot handle this data type: %s, %s"" % typekey)
TypeError: Cannot handle this data type: (1, 1, 4), <f4

[12/18 17:13:34] ppdet.utils.checkpoint INFO: ['_fc.bias', '_fc.weight', '_last_conv._batch_norm._mean', '_last_conv._batch_norm._variance', '_last_conv._batch_norm.bias', '_last_conv._batch_norm.weight', '_last_conv._conv.weight', 'la
st_conv.weight'] in pretrained weight is not used in the model, and its will not be loaded
[12/18 17:13:34] ppdet.utils.checkpoint INFO: Finish loading model weights: C:\Users\Dev16/.cache/paddle/weights\ESNet_x0_75_pretrained.pdparams
"
使用visdrone数据集训练SSD_vgg16_512模型，模型训练到epoch=82，mAP仍然为0？,PaddlePaddle/PaddleDetection,2021-12-17 02:06:28,7,,4925,1082807132,"## 描述问题/Describe the bug
使用visdrone数据集训练SSD_vgg16_512模型，模型训练到epoch=82，mAP仍然为0，对于ssd中配置文件的设置，是我哪里设置错了吗，我使用ssd_vgg16_300训练该数据集，虽然mAP只有9左右（考虑到visdrone这个数据集本身目标尺寸非常小，输入又是300x300像素的图像，经过卷积之后，特征其实已经不能用于分类和定位了），但这个值我个人认为还是正常的，所以我通过提高输入模型的图像的分辨率（512x512），但是结果却为0。（visdrone数据集转换为coco格式，这里转换是没有问题的）

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?

```bash
python tools/train.py -c configs/ssd/ssd_vgg16_512_540e_visdrone.yml -o pretrain_weights='' --evalpython tools/train.py -c configs/s
```
这里没有使用预训练模型，因为贵方给定的是ssd_vgg16_300的预训练参数，所以这里不使用

2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.
> ssd_vgg16_512_540e_visdrone.yml
```yaml
_BASE_: [
  '../datasets/visdrone_detection.yml',
  '../runtime.yml',
  '_base_/optimizer_540e.yml',
  '_base_/ssd_vgg16_512.yml',
  '_base_/ssd_reader_512.yml',
]
weights: output/ssd_vgg16_512_540e_visdrone/model_final

# set collate_batch to false because ground-truth info is needed
# on voc dataset and should not collate data in batch when batch size
# is larger than 1 
EvalReader:
  collate_batch: false
```

> visdrone_detection.yml
```yaml
metric: COCO
num_classes: 9

TrainDataset:
  !COCODataSet
    image_dir: train
    anno_path: annotations/train.json
    dataset_dir: dataset/VisDrone2019_coco
    data_fields: ['image', 'gt_bbox', 'gt_class', 'is_crowd']

EvalDataset:
  !COCODataSet
    image_dir: val
    anno_path: annotations/val.json
    dataset_dir: dataset/VisDrone2019_coco

TestDataset:
  !ImageFolder
    anno_path: annotations/instances_val2017.json
```

> runtime.yml
```yaml
use_gpu: true
log_iter: 20
save_dir: output
snapshot_epoch: 2
print_flops: false
```

> optimizer_540e.yml
```yaml
epoch: 540

LearningRate:
  base_lr: 0.0001
  schedulers:
  - !PiecewiseDecay
    gamma: 0.1
    milestones:
    - 360
    - 450
  - !LinearWarmup
    start_factor: 0.3333333333333333
    steps: 500

OptimizerBuilder:
  optimizer:
    momentum: 0.9
    type: Momentum
  regularizer:
    factor: 0.0005
    type: L2
```

> ssd_vgg16_512.yml
```yaml
architecture: SSD
pretrain_weights: ./pretrainWeights/VGG16_caffe_pretrained.pdparams

# Model Achitecture
SSD:
  # model feat info flow
  backbone: VGG
  ssd_head: SSDHead
  # post process
  post_process: BBoxPostProcess

VGG:
  depth: 16
  normalizations: [20., -1, -1, -1, -1, -1]

SSDHead:
  anchor_generator:
    steps: [26, 51, 102, 170, 256, 512]
    aspect_ratios: [[2.], [2., 3.], [2., 3.], [2., 3.], [2.], [2.]]
    min_ratio: 1
    max_ratio: 15
    # min_sizes: [30.0, 60.0, 111.0, 162.0, 213.0, 264.0]
    # max_sizes: [60.0, 111.0, 162.0, 213.0, 264.0, 315.0]
    min_sizes: []
    max_sizes: []
    offset: 0.5
    flip: true
    min_max_aspect_ratios_order: true

BBoxPostProcess:
  decode:
    name: SSDBox
  nms:
    name: MultiClassNMS
    keep_top_k: 200
    score_threshold: 0.01
    nms_threshold: 0.45
    nms_top_k: 400
    nms_eta: 1.0
```

> ssd_reader_512.yml
```yaml
worker_num: 2
TrainReader:
  inputs_def:
    num_max_boxes: 90

  sample_transforms:
    - Decode: {}
    - RandomDistort: {brightness: [0.5, 1.125, 0.875], random_apply: False}
    - RandomExpand: {fill_value: [104., 117., 123.]}
    - RandomCrop: {allow_no_crop: true}
    - RandomFlip: {}
    - Resize: {target_size: [512, 512], keep_ratio: False, interp: 1}
    - NormalizeBox: {}
    - PadBox: {num_max_boxes: 90}

  batch_transforms:
    - NormalizeImage: {mean: [104., 117., 123.], std: [1., 1., 1.], is_scale: false}
    - Permute: {}

  batch_size: 20
  shuffle: true
  drop_last: true


EvalReader:
  sample_transforms:
    - Decode: {}
    - Resize: {target_size: [512, 512], keep_ratio: False, interp: 1}
    - NormalizeImage: {mean: [104., 117., 123.], std: [1., 1., 1.], is_scale: false}
    - Permute: {}
  batch_size: 16

TestReader:
  inputs_def:
    image_shape: [3, 512, 512]
  sample_transforms:
    - Decode: {}
    - Resize: {target_size: [512, 512], keep_ratio: False, interp: 1}
    - NormalizeImage: {mean: [104., 117., 123.], std: [1., 1., 1.], is_scale: false}
    - Permute: {}
  batch_size: 1
```

说明：首先将epoch修改为了540。然后ssd_vgg16_512.yml文件中的min_ratio和max_ratio修改为了1和15，这里主要还是考虑到visdrone数据集目标尺寸过小，所以根据anchor-free方法生成了一系列anchor的尺寸，得到了这两个值，同时将min_sizes和max_sizes注释掉，使用代码中的生成的min_sizes和max_sizes。最后就是ssd_reader_512.yml文件中target_size修改为了512.

3. 您使用的数据集是？/What dataset did you use?
visdrone2019

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.
结果显示
```bash
[12/17 08:48:50] ppdet.engine INFO: Epoch: [78] [  0/323] learning_rate: 0.000100 loss: 11.744663 eta: 4 days, 1:28:44 batch_cost: 2.1480 data_cost: 1.1991 ips: 9.3111 images/s
[12/17 08:49:32] ppdet.engine INFO: Epoch: [78] [ 20/323] learning_rate: 0.000100 loss: 12.019836 eta: 4 days, 1:27:25 batch_cost: 2.0766 data_cost: 1.1337 ips: 9.6311 images/s
[12/17 08:50:15] ppdet.engine INFO: Epoch: [78] [ 40/323] learning_rate: 0.000100 loss: 12.022238 eta: 4 days, 1:26:15 batch_cost: 2.1580 data_cost: 1.2069 ips: 9.2677 images/s
[12/17 08:50:57] ppdet.engine INFO: Epoch: [78] [ 60/323] learning_rate: 0.000100 loss: 11.515100 eta: 4 days, 1:25:00 batch_cost: 2.1125 data_cost: 1.1613 ips: 9.4672 images/s
[12/17 08:51:45] ppdet.engine INFO: Epoch: [78] [ 80/323] learning_rate: 0.000100 loss: 11.901532 eta: 4 days, 1:24:16 batch_cost: 2.3804 data_cost: 1.4325 ips: 8.4018 images/s
[12/17 08:52:30] ppdet.engine INFO: Epoch: [78] [100/323] learning_rate: 0.000100 loss: 11.967754 eta: 4 days, 1:23:18 batch_cost: 2.2579 data_cost: 1.3061 ips: 8.8579 images/s
[12/17 08:53:16] ppdet.engine INFO: Epoch: [78] [120/323] learning_rate: 0.000100 loss: 12.091450 eta: 4 days, 1:22:22 batch_cost: 2.2727 data_cost: 1.3206 ips: 8.8000 images/s
[12/17 08:54:02] ppdet.engine INFO: Epoch: [78] [140/323] learning_rate: 0.000100 loss: 11.721924 eta: 4 days, 1:21:28 batch_cost: 2.2965 data_cost: 1.3395 ips: 8.7091 images/s
[12/17 08:54:43] ppdet.engine INFO: Epoch: [78] [160/323] learning_rate: 0.000100 loss: 11.813888 eta: 4 days, 1:20:06 batch_cost: 2.0535 data_cost: 1.0939 ips: 9.7393 images/s
[12/17 08:55:28] ppdet.engine INFO: Epoch: [78] [180/323] learning_rate: 0.000100 loss: 11.819774 eta: 4 days, 1:19:09 batch_cost: 2.2604 data_cost: 1.3113 ips: 8.8479 images/s
[12/17 08:56:13] ppdet.engine INFO: Epoch: [78] [200/323] learning_rate: 0.000100 loss: 11.759878 eta: 4 days, 1:18:10 batch_cost: 2.2474 data_cost: 1.2708 ips: 8.8994 images/s
[12/17 08:56:59] ppdet.engine INFO: Epoch: [78] [220/323] learning_rate: 0.000100 loss: 11.874834 eta: 4 days, 1:17:15 batch_cost: 2.2887 data_cost: 1.3329 ips: 8.7387 images/s
[12/17 08:57:42] ppdet.engine INFO: Epoch: [78] [240/323] learning_rate: 0.000100 loss: 11.721977 eta: 4 days, 1:16:05 batch_cost: 2.1504 data_cost: 1.1922 ips: 9.3004 images/s
[12/17 08:58:29] ppdet.engine INFO: Epoch: [78] [260/323] learning_rate: 0.000100 loss: 12.070805 eta: 4 days, 1:15:17 batch_cost: 2.3447 data_cost: 1.4021 ips: 8.5300 images/s
[12/17 08:59:15] ppdet.engine INFO: Epoch: [78] [280/323] learning_rate: 0.000100 loss: 11.869123 eta: 4 days, 1:14:23 batch_cost: 2.2926 data_cost: 1.3417 ips: 8.7239 images/s
[12/17 09:00:00] ppdet.engine INFO: Epoch: [78] [300/323] learning_rate: 0.000100 loss: 11.949878 eta: 4 days, 1:13:23 batch_cost: 2.2322 data_cost: 1.2754 ips: 8.9600 images/s
[12/17 09:00:43] ppdet.engine INFO: Epoch: [78] [320/323] learning_rate: 0.000100 loss: 11.903643 eta: 4 days, 1:12:12 batch_cost: 2.1499 data_cost: 1.2024 ips: 9.3028 images/s
[12/17 09:00:53] ppdet.engine INFO: Epoch: [79] [  0/323] learning_rate: 0.000100 loss: 11.940117 eta: 4 days, 1:12:23 batch_cost: 2.3852 data_cost: 1.4373 ips: 8.3849 images/s
[12/17 09:01:39] ppdet.engine INFO: Epoch: [79] [ 20/323] learning_rate: 0.000100 loss: 11.894506 eta: 4 days, 1:11:30 batch_cost: 2.3017 data_cost: 1.3529 ips: 8.6894 images/s
[12/17 09:02:23] ppdet.engine INFO: Epoch: [79] [ 40/323] learning_rate: 0.000100 loss: 11.938265 eta: 4 days, 1:10:25 batch_cost: 2.1970 data_cost: 1.2541 ips: 9.1034 images/s
[12/17 09:03:06] ppdet.engine INFO: Epoch: [79] [ 60/323] learning_rate: 0.000100 loss: 11.825841 eta: 4 days, 1:09:12 batch_cost: 2.1226 data_cost: 1.1716 ips: 9.4226 images/s
[12/17 09:03:50] ppdet.engine INFO: Epoch: [79] [ 80/323] learning_rate: 0.000100 loss: 11.830574 eta: 4 days, 1:08:11 batch_cost: 2.2294 data_cost: 1.2803 ips: 8.9712 images/s
[12/17 09:04:37] ppdet.engine INFO: Epoch: [79] [100/323] learning_rate: 0.000100 loss: 11.841778 eta: 4 days, 1:07:22 batch_cost: 2.3286 data_cost: 1.3786 ips: 8.5889 images/s
[12/17 09:05:19] ppdet.engine INFO: Epoch: [79] [120/323] learning_rate: 0.000100 loss: 11.721057 eta: 4 days, 1:06:08 batch_cost: 2.1230 data_cost: 1.1675 ips: 9.4207 images/s
[12/17 09:06:04] ppdet.engine INFO: Epoch: [79] [140/323] learning_rate: 0.000100 loss: 11.839280 eta: 4 days, 1:05:06 batch_cost: 2.2176 data_cost: 1.2767 ips: 9.0188 images/s
[12/17 09:06:49] ppdet.engine INFO: Epoch: [79] [160/323] learning_rate: 0.000100 loss: 12.102294 eta: 4 days, 1:04:07 batch_cost: 2.2501 data_cost: 1.3068 ips: 8.8883 images/s
[12/17 09:07:34] ppdet.engine INFO: Epoch: [79] [180/323] learning_rate: 0.000100 loss: 11.982029 eta: 4 days, 1:03:08 batch_cost: 2.2383 data_cost: 1.2957 ips: 8.9354 images/s
[12/17 09:08:18] ppdet.engine INFO: Epoch: [79] [200/323] learning_rate: 0.000100 loss: 12.075586 eta: 4 days, 1:02:08 batch_cost: 2.2438 data_cost: 1.2929 ips: 8.9134 images/s
[12/17 09:09:06] ppdet.engine INFO: Epoch: [79] [220/323] learning_rate: 0.000100 loss: 11.968622 eta: 4 days, 1:01:21 batch_cost: 2.3489 data_cost: 1.3588 ips: 8.5145 images/s
[12/17 09:09:51] ppdet.engine INFO: Epoch: [79] [240/323] learning_rate: 0.000100 loss: 11.653918 eta: 4 days, 1:00:25 batch_cost: 2.2706 data_cost: 1.2983 ips: 8.8082 images/s
[12/17 09:10:37] ppdet.engine INFO: Epoch: [79] [260/323] learning_rate: 0.000100 loss: 12.095556 eta: 4 days, 0:59:31 batch_cost: 2.2843 data_cost: 1.3185 ips: 8.7552 images/s
[12/17 09:11:23] ppdet.engine INFO: Epoch: [79] [280/323] learning_rate: 0.000100 loss: 11.853638 eta: 4 days, 0:58:37 batch_cost: 2.2876 data_cost: 1.3289 ips: 8.7427 images/s
[12/17 09:12:05] ppdet.engine INFO: Epoch: [79] [300/323] learning_rate: 0.000100 loss: 11.789781 eta: 4 days, 0:57:22 batch_cost: 2.1103 data_cost: 1.1599 ips: 9.4774 images/s
[12/17 09:12:52] ppdet.engine INFO: Epoch: [79] [320/323] learning_rate: 0.000100 loss: 11.638730 eta: 4 days, 0:56:36 batch_cost: 2.3536 data_cost: 1.3954 ips: 8.4977 images/s
[12/17 09:12:58] ppdet.utils.checkpoint INFO: Save checkpoint: output/ssd_vgg16_512_540e_visdrone
[12/17 09:12:58] ppdet.engine INFO: Eval iter: 0
[12/17 09:13:16] ppdet.metrics.metrics INFO: The bbox result is saved to bbox.json.
loading annotations into memory...
Done (t=0.25s)
creating index...
index created!
[12/17 09:13:16] ppdet.metrics.coco_utils INFO: Start evaluate...
Loading and preparing results...
DONE (t=0.88s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=40.01s).
Accumulating evaluation results...
DONE (t=0.65s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
[12/17 09:13:58] ppdet.engine INFO: Total sample number: 548, averge FPS: 35.555246669857
[12/17 09:13:58] ppdet.engine INFO: Best test bbox ap is 0.000.
[12/17 09:13:59] ppdet.engine INFO: Epoch: [80] [  0/323] learning_rate: 0.000100 loss: 11.677496 eta: 4 days, 0:56:22 batch_cost: 2.3081 data_cost: 1.3493 ips: 8.6652 images/s
[12/17 09:14:41] ppdet.engine INFO: Epoch: [80] [ 20/323] learning_rate: 0.000100 loss: 11.725384 eta: 4 days, 0:55:08 batch_cost: 2.1134 data_cost: 1.1600 ips: 9.4633 images/s
[12/17 09:15:27] ppdet.engine INFO: Epoch: [80] [ 40/323] learning_rate: 0.000100 loss: 11.678532 eta: 4 days, 0:54:14 batch_cost: 2.2867 data_cost: 1.3434 ips: 8.7464 images/s
[12/17 09:16:10] ppdet.engine INFO: Epoch: [80] [ 60/323] learning_rate: 0.000100 loss: 11.671333 eta: 4 days, 0:53:06 batch_cost: 2.1636 data_cost: 1.2168 ips: 9.2439 images/s
[12/17 09:16:58] ppdet.engine INFO: Epoch: [80] [ 80/323] learning_rate: 0.000100 loss: 11.969685 eta: 4 days, 0:52:23 batch_cost: 2.3896 data_cost: 1.4429 ips: 8.3697 images/s
[12/17 09:17:45] ppdet.engine INFO: Epoch: [80] [100/323] learning_rate: 0.000100 loss: 12.113137 eta: 4 days, 0:51:39 batch_cost: 2.3718 data_cost: 1.4214 ips: 8.4325 images/s
[12/17 09:18:26] ppdet.engine INFO: Epoch: [80] [120/323] learning_rate: 0.000100 loss: 11.810704 eta: 4 days, 0:50:16 batch_cost: 2.0340 data_cost: 1.0813 ips: 9.8330 images/s
[12/17 09:19:11] ppdet.engine INFO: Epoch: [80] [140/323] learning_rate: 0.000100 loss: 11.802250 eta: 4 days, 0:49:18 batch_cost: 2.2541 data_cost: 1.3049 ips: 8.8729 images/s
[12/17 09:20:00] ppdet.engine INFO: Epoch: [80] [160/323] learning_rate: 0.000100 loss: 11.796503 eta: 4 days, 0:48:40 batch_cost: 2.4264 data_cost: 1.4652 ips: 8.2428 images/s
[12/17 09:20:50] ppdet.engine INFO: Epoch: [80] [180/323] learning_rate: 0.000100 loss: 11.835796 eta: 4 days, 0:48:10 batch_cost: 2.4939 data_cost: 1.5226 ips: 8.0195 images/s
[12/17 09:21:35] ppdet.engine INFO: Epoch: [80] [200/323] learning_rate: 0.000100 loss: 12.080269 eta: 4 days, 0:47:15 batch_cost: 2.2745 data_cost: 1.3218 ips: 8.7931 images/s
[12/17 09:22:22] ppdet.engine INFO: Epoch: [80] [220/323] learning_rate: 0.000100 loss: 12.006727 eta: 4 days, 0:46:26 batch_cost: 2.3307 data_cost: 1.3709 ips: 8.5810 images/s
[12/17 09:23:06] ppdet.engine INFO: Epoch: [80] [240/323] learning_rate: 0.000100 loss: 11.818688 eta: 4 days, 0:45:23 batch_cost: 2.2111 data_cost: 1.2649 ips: 9.0453 images/s
[12/17 09:23:47] ppdet.engine INFO: Epoch: [80] [260/323] learning_rate: 0.000100 loss: 11.912148 eta: 4 days, 0:43:57 batch_cost: 2.0080 data_cost: 1.0318 ips: 9.9601 images/s
[12/17 09:24:31] ppdet.engine INFO: Epoch: [80] [280/323] learning_rate: 0.000100 loss: 11.733059 eta: 4 days, 0:42:57 batch_cost: 2.2291 data_cost: 1.2865 ips: 8.9724 images/s
[12/17 09:25:18] ppdet.engine INFO: Epoch: [80] [300/323] learning_rate: 0.000100 loss: 11.759735 eta: 4 days, 0:42:11 batch_cost: 2.3548 data_cost: 1.3917 ips: 8.4934 images/s
[12/17 09:26:03] ppdet.engine INFO: Epoch: [80] [320/323] learning_rate: 0.000100 loss: 11.794756 eta: 4 days, 0:41:14 batch_cost: 2.2568 data_cost: 1.2814 ips: 8.8621 images/s
[12/17 09:26:13] ppdet.engine INFO: Epoch: [81] [  0/323] learning_rate: 0.000100 loss: 11.794756 eta: 4 days, 0:41:21 batch_cost: 2.4686 data_cost: 1.4988 ips: 8.1019 images/s
[12/17 09:26:58] ppdet.engine INFO: Epoch: [81] [ 20/323] learning_rate: 0.000100 loss: 11.816735 eta: 4 days, 0:40:19 batch_cost: 2.2145 data_cost: 1.2569 ips: 9.0313 images/s
[12/17 09:27:40] ppdet.engine INFO: Epoch: [81] [ 40/323] learning_rate: 0.000100 loss: 11.867622 eta: 4 days, 0:39:06 batch_cost: 2.1111 data_cost: 1.1413 ips: 9.4737 images/s
[12/17 09:28:23] ppdet.engine INFO: Epoch: [81] [ 60/323] learning_rate: 0.000100 loss: 11.859762 eta: 4 days, 0:37:53 batch_cost: 2.1199 data_cost: 1.1773 ips: 9.4343 images/s
[12/17 09:29:10] ppdet.engine INFO: Epoch: [81] [ 80/323] learning_rate: 0.000100 loss: 11.872942 eta: 4 days, 0:37:09 batch_cost: 2.3723 data_cost: 1.4262 ips: 8.4307 images/s
[12/17 09:29:54] ppdet.engine INFO: Epoch: [81] [100/323] learning_rate: 0.000100 loss: 11.818016 eta: 4 days, 0:36:04 batch_cost: 2.1899 data_cost: 1.2359 ips: 9.1330 images/s
[12/17 09:30:40] ppdet.engine INFO: Epoch: [81] [120/323] learning_rate: 0.000100 loss: 11.874178 eta: 4 days, 0:35:11 batch_cost: 2.2917 data_cost: 1.3184 ips: 8.7272 images/s
[12/17 09:31:26] ppdet.engine INFO: Epoch: [81] [140/323] learning_rate: 0.000100 loss: 12.007284 eta: 4 days, 0:34:21 batch_cost: 2.3257 data_cost: 1.3762 ips: 8.5995 images/s
[12/17 09:32:10] ppdet.engine INFO: Epoch: [81] [160/323] learning_rate: 0.000100 loss: 11.686701 eta: 4 days, 0:33:14 batch_cost: 2.1646 data_cost: 1.2056 ips: 9.2395 images/s
[12/17 09:32:54] ppdet.engine INFO: Epoch: [81] [180/323] learning_rate: 0.000100 loss: 11.627678 eta: 4 days, 0:32:14 batch_cost: 2.2272 data_cost: 1.2843 ips: 8.9799 images/s
[12/17 09:33:41] ppdet.engine INFO: Epoch: [81] [200/323] learning_rate: 0.000100 loss: 11.935854 eta: 4 days, 0:31:23 batch_cost: 2.3134 data_cost: 1.3706 ips: 8.6451 images/s
[12/17 09:34:23] ppdet.engine INFO: Epoch: [81] [220/323] learning_rate: 0.000100 loss: 11.997539 eta: 4 days, 0:30:13 batch_cost: 2.1457 data_cost: 1.1906 ips: 9.3211 images/s
[12/17 09:35:08] ppdet.engine INFO: Epoch: [81] [240/323] learning_rate: 0.000100 loss: 11.733990 eta: 4 days, 0:29:13 batch_cost: 2.2257 data_cost: 1.2762 ips: 8.9858 images/s
[12/17 09:35:53] ppdet.engine INFO: Epoch: [81] [260/323] learning_rate: 0.000100 loss: 11.869278 eta: 4 days, 0:28:12 batch_cost: 2.2223 data_cost: 1.2729 ips: 8.9999 images/s
[12/17 09:36:39] ppdet.engine INFO: Epoch: [81] [280/323] learning_rate: 0.000100 loss: 11.645747 eta: 4 days, 0:27:24 batch_cost: 2.3404 data_cost: 1.3962 ips: 8.5454 images/s
[12/17 09:37:25] ppdet.engine INFO: Epoch: [81] [300/323] learning_rate: 0.000100 loss: 11.787794 eta: 4 days, 0:26:29 batch_cost: 2.2713 data_cost: 1.3248 ips: 8.8056 images/s
[12/17 09:38:06] ppdet.engine INFO: Epoch: [81] [320/323] learning_rate: 0.000100 loss: 11.791275 eta: 4 days, 0:25:07 batch_cost: 2.0327 data_cost: 1.0813 ips: 9.8392 images/s
[12/17 09:38:11] ppdet.utils.checkpoint INFO: Save checkpoint: output/ssd_vgg16_512_540e_visdrone
[12/17 09:38:12] ppdet.engine INFO: Eval iter: 0
[12/17 09:38:29] ppdet.metrics.metrics INFO: The bbox result is saved to bbox.json.
loading annotations into memory...
Done (t=0.26s)
creating index...
index created!
[12/17 09:38:29] ppdet.metrics.coco_utils INFO: Start evaluate...
Loading and preparing results...
DONE (t=0.89s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=50.54s).
Accumulating evaluation results...
DONE (t=0.81s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
[12/17 09:39:22] ppdet.engine INFO: Total sample number: 548, averge FPS: 35.74674490079638
[12/17 09:39:22] ppdet.engine INFO: Best test bbox ap is 0.000.
```

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：
Paddle: 2.2.0
PaddleDetection: 2.3
"
[Other General Issues] ValueError: (InvalidArgument) The variable to insert is NULL.,PaddlePaddle/PaddleDetection,2021-12-17 01:37:36,2,,4924,1082793264,"可以普通训练，使用量化训练时报错 paddle=2.2，paddledec=2.3，paddleslim=2.2
python tools/train.py -c configs/ppyolo/ppyolov2_r50vd_dcn_voc.yml --slim_config configs/slim/quant/ppyolov2_r50vd_dcn_qat.yml

Traceback (most recent call last):
  File ""tools/train.py"", line 171, in <module>
    main()
  File ""tools/train.py"", line 167, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 127, in run
    trainer.train(FLAGS.eval)
  File ""/media/zn/f/2.3/PaddleDetection/ppdet/engine/trainer.py"", line 366, in train
    self._flops(self.loader)
  File ""/media/zn/f/2.3/PaddleDetection/ppdet/engine/trainer.py"", line 713, in _flops
    flops = flops(self.model, input_spec) / (1000**3)
  File ""/home/zn/anaconda3/envs/paddle/lib/python3.7/site-packages/paddleslim/analysis/flops.py"", line 133, in dygraph_flops
    program = dygraph2program(model, inputs)
  File ""/home/zn/anaconda3/envs/paddle/lib/python3.7/site-packages/decorator.py"", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File ""/home/zn/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/wrapped_decorator.py"", line 25, in __impl__
    return wrapped_func(*args, **kwargs)
  File ""/home/zn/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/framework.py"", line 229, in __impl__
    return func(*args, **kwargs)
  File ""/home/zn/anaconda3/envs/paddle/lib/python3.7/site-packages/paddleslim/core/dygraph.py"", line 119, in dygraph2program
    original_outputs = layer(*inputs)
  File ""/home/zn/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 914, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File ""/media/zn/f/2.3/PaddleDetection/ppdet/modeling/architectures/meta_arch.py"", line 56, in forward
    out = self.get_pred()
  File ""/media/zn/f/2.3/PaddleDetection/ppdet/modeling/architectures/yolo.py"", line 124, in get_pred
    return self._forward()
  File ""/media/zn/f/2.3/PaddleDetection/ppdet/modeling/architectures/yolo.py"", line 79, in _forward
    body_feats = self.backbone(self.inputs)
  File ""/home/zn/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 914, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File ""/media/zn/f/2.3/PaddleDetection/ppdet/modeling/backbones/resnet.py"", line 582, in forward
    conv1 = self.conv1(x)
  File ""/home/zn/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 914, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File ""/home/zn/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/container.py"", line 98, in forward
    input = layer(input)
  File ""/home/zn/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 914, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File ""/media/zn/f/2.3/PaddleDetection/ppdet/modeling/backbones/resnet.py"", line 122, in forward
    out = self.conv(inputs)
  File ""/home/zn/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 914, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File ""/home/zn/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/quant/quant_layers.py"", line 678, in forward
    out = self._layer(*inputs, **kwargs)
  File ""/home/zn/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 914, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File ""/home/zn/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/quant/quant_layers.py"", line 460, in forward
    quant_input = self._fake_quant_input(input)
  File ""/home/zn/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 914, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File ""/home/zn/anaconda3/envs/paddle/lib/python3.7/site-packages/paddle/nn/quant/quant_layers.py"", line 192, in forward
    accum, *attrs)
ValueError: (InvalidArgument) The variable to insert is NULL.
  [Hint: new_var should not be null.] (at /paddle/paddle/fluid/imperative/jit/program_desc_tracer.cc:223)
"
将swin transformer 作为backbone改进ssd，loss一直不收敛,PaddlePaddle/PaddleDetection,2021-12-16 12:01:58,4,,4922,1082114111,"我试图将swin transformer 作为backbone改进ssd
这是我的主要部分配置文件信息，主要改动了swin transformer部分，但是结果的loss如下
#voc
metric: VOC
map_type: 11point
num_classes: 3

#runtime
use_gpu: true
log_iter: 20
save_dir: output
print_flops: false

#optimizer_240e.yml
epoch: 100
LearningRate:
  base_lr: 0.0001
  schedulers:
  - !PiecewiseDecay
    gamma: 0.1
    milestones:
    - 70
    - 90
  - !LinearWarmup
    start_factor: 0.1
    steps: 4000

OptimizerBuilder:
  optimizer:
    momentum: 0.9
    type: Momentum
  regularizer:
    factor: 0.0005
    type: L2
architecture: SSD

SSD:
  backbone: SwinTransformer
  ssd_head: SSDHead
  post_process: BBoxPostProcess

SwinTransformer:
  embed_dim: 128
  depths: [2, 2, 18, 2]
  num_heads: [4, 8, 16, 32]
  window_size: 7
  ape: false
  drop_path_rate: 0.1
  patch_norm: true
  out_indices: [1]

SSDHead:
  anchor_generator:
    steps: [8, 16, 32, 64, 100, 300]
    aspect_ratios: [[2.], [2., 3.], [2., 3.], [2., 3.], [2.], [2.]]
    min_sizes: [21.0, 45.0, 99.0, 153.0, 207.0, 261.0]
    max_sizes: [45.0, 99.0, 153.0, 207.0, 261.0, 315.0]
    offset: 0.5
    clip: True
    min_max_aspect_ratios_order: True
  use_extra_head: True

BBoxPostProcess:
  decode:
    name: SSDBox
  nms:
    name: MultiClassNMS
    keep_top_k: 200
    score_threshold: 0.05
    nms_threshold: 0.5
    nms_top_k: 400
worker_num: 2

TrainReader:
  inputs_def:
    num_max_boxes: 90
  sample_transforms:
    - Decode: {}
    - RandomCrop: {allow_no_crop: true}
    - Resize: {target_size: [512, 512], keep_ratio: False, interp: 2}
    #- RandomFlip: {prob: 0.5}
    #- RandomDistort: {brightness: [0.875, 1.125, 0.5], random_apply: Fals
e}
    - NormalizeBox: {}
    - PadBox: {num_max_boxes: 90}
    - NormalizeImage: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.2
25], is_scale: true}
    - Permute: {}
  batch_transforms:
    - PadBatch: { pad_to_stride: 32 }
  batch_size: 1
  shuffle: true
  drop_last: true
![O3@3FPJFRCV4% ` U``LC6Q](https://user-images.githubusercontent.com/54532885/146367980-3a46541b-b7fa-42b6-9b19-e351368b7871.png)
即使训练到100轮 loss也会是4-5左右波动，而验证的准确率为0，想知道是配置文件哪里出错了呢，如果不是配置文件的问题，请问应该怎么改动呢？谢谢！
"
python tools/train.py -c configs/picodet/picodet_s_320_voc.yml --eval 训练自己的数据很慢是怎么回事？,PaddlePaddle/PaddleDetection,2021-12-16 09:28:31,4,,4918,1081960934,"python tools/train.py -c configs/picodet/picodet_s_320_voc.yml --eval 训练自己的数据很慢是怎么回事？

(base) F:\murong\projects1\PaddleDetection-release-2.3>python tools/train.py -c configs/picodet/picodet_s_320_voc.yml --eval
C:\Users\Dev16\Anaconda3\lib\site-packages\socks.py:58: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  from collections import Callable
C:\Users\Dev16\Anaconda3\lib\site-packages\win32\lib\pywintypes.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp, sys, os
W1216 16:59:38.004186  9828 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 11.1, Runtime API Version: 10.2
W1216 16:59:38.016156  9828 device_context.cc:465] device: 0, cuDNN Version: 7.6.
[12/16 16:59:39] ppdet.utils.checkpoint INFO: ['_fc.bias', '_fc.weight', '_last_conv._batch_norm._mean', '_last_conv._batch_norm._variance', '_last_conv._batch_norm.bias', '_last_conv._batch_norm.weight', '_last_conv._conv.weight', 'la
st_conv.weight'] in pretrained weight is not used in the model, and its will not be loaded
[12/16 16:59:40] ppdet.utils.checkpoint INFO: Finish loading model weights: model/ESNet_x0_75_pretrained.pdparams
[12/16 16:59:41] ppdet.engine INFO: Epoch: [0] [  0/104] learning_rate: 0.040000 loss_vfl: 1.551724 loss_bbox: 0.895358 loss_dfl: 0.521692 loss: 2.968774 eta: 11:32:52 batch_cost: 1.3324 data_cost: 0.6602 ips: 12.0081 images/s
[12/16 17:00:28] ppdet.engine INFO: Epoch: [0] [ 20/104] learning_rate: 0.064000 loss_vfl: 1.410168 loss_bbox: 0.933549 loss_dfl: 0.481081 loss: 2.824308 eta: 18:19:51 batch_cost: 2.1557 data_cost: 1.5162 ips: 7.4222 images/s
[12/16 17:01:11] ppdet.engine INFO: Epoch: [0] [ 40/104] learning_rate: 0.088000 loss_vfl: 1.249832 loss_bbox: 0.865242 loss_dfl: 0.428205 loss: 2.542513 eta: 17:50:12 batch_cost: 2.0022 data_cost: 1.3908 ips: 7.9912 images/s
[12/16 17:01:54] ppdet.engine INFO: Epoch: [0] [ 60/104] learning_rate: 0.112000 loss_vfl: 1.094283 loss_bbox: 0.773494 loss_dfl: 0.380751 loss: 2.250960 eta: 17:33:03 batch_cost: 1.9640 data_cost: 1.4225 ips: 8.1466 images/s
[12/16 17:02:36] ppdet.engine INFO: Epoch: [0] [ 80/104] learning_rate: 0.136000 loss_vfl: 1.112156 loss_bbox: 0.703290 loss_dfl: 0.370269 loss: 2.181904 eta: 17:19:40 batch_cost: 1.9298 data_cost: 1.3763 ips: 8.2908 images/s
[12/16 17:03:18] ppdet.engine INFO: Epoch: [0] [100/104] learning_rate: 0.160000 loss_vfl: 1.186168 loss_bbox: 0.693515 loss_dfl: 0.354381 loss: 2.209870 eta: 17:15:16 batch_cost: 1.9681 data_cost: 1.4238 ips: 8.1295 images/s
[12/16 17:03:26] ppdet.engine INFO: Epoch: [1] [  0/104] learning_rate: 0.164800 loss_vfl: 1.168976 loss_bbox: 0.687513 loss_dfl: 0.346974 loss: 2.186113 eta: 17:10:27 batch_cost: 1.9444 data_cost: 1.4134 ips: 8.2290 images/s
[12/16 17:04:07] ppdet.engine INFO: Epoch: [1] [ 20/104] learning_rate: 0.188800 loss_vfl: 1.119198 loss_bbox: 0.709000 loss_dfl: 0.352092 loss: 2.209464 eta: 17:02:26 batch_cost: 1.8994 data_cost: 1.3629 ips: 8.4236 images/s
[12/16 17:04:50] ppdet.engine INFO: Epoch: [1] [ 40/104] learning_rate: 0.212800 loss_vfl: 1.064116 loss_bbox: 0.663903 loss_dfl: 0.350917 loss: 2.102975 eta: 17:01:04 batch_cost: 1.9642 data_cost: 1.4246 ips: 8.1458 images/s
[12/16 17:05:32] ppdet.engine INFO: Epoch: [1] [ 60/104] learning_rate: 0.236800 loss_vfl: 1.081842 loss_bbox: 0.631488 loss_dfl: 0.334639 loss: 2.041198 eta: 17:00:19 batch_cost: 1.9714 data_cost: 1.4469 ips: 8.1161 images/s
[12/16 17:06:13] ppdet.engine INFO: Epoch: [1] [ 80/104] learning_rate: 0.260800 loss_vfl: 1.123333 loss_bbox: 0.597034 loss_dfl: 0.321654 loss: 2.061877 eta: 16:56:26 batch_cost: 1.9147 data_cost: 1.4007 ips: 8.3563 images/s
[12/16 17:06:52] ppdet.engine INFO: Epoch: [1] [100/104] learning_rate: 0.284800 loss_vfl: 1.187028 loss_bbox: 0.580264 loss_dfl: 0.309837 loss: 2.081032 eta: 16:47:04 batch_cost: 1.7933 data_cost: 1.2734 ips: 8.9221 images/s
[12/16 17:07:01] ppdet.engine INFO: Epoch: [2] [  0/104] learning_rate: 0.289600 loss_vfl: 1.187028 loss_bbox: 0.571176 loss_dfl: 0.308245 loss: 2.081032 eta: 16:45:54 batch_cost: 1.7970 data_cost: 1.2709 ips: 8.9038 images/s
[12/16 17:07:39] ppdet.engine INFO: Epoch: [2] [ 20/104] learning_rate: 0.313600 loss_vfl: 1.152678 loss_bbox: 0.542991 loss_dfl: 0.294221 loss: 1.972140 eta: 16:36:30 batch_cost: 1.7533 data_cost: 1.2296 ips: 9.1256 images/s
[12/16 17:08:24] ppdet.engine INFO: Epoch: [2] [ 40/104] learning_rate: 0.337600 loss_vfl: 1.230537 loss_bbox: 0.464604 loss_dfl: 0.290910 loss: 1.982177 eta: 16:42:07 batch_cost: 2.0815 data_cost: 1.5124 ips: 7.6868 images/s
"
使用自己数据集训练的问题,PaddlePaddle/PaddleDetection,2021-12-16 08:46:39,4,,4917,1081921607,"在使用不同模型的时候发现了一个问题，由于我自己的数据集的json文件中类别的序号是从0开始的，在跑一些实验的时候是正常可行的，例如TTFNet、PAFNet；但是我今天在进行GFLv2的实验时则发现，GFLv2的类别会自动增加一类背景类，因而在进行eval的过程中会导致类别索引为6但是val.json中最大索引只到5(数据集0-5共6个类别)。
因此该如何修改GFLv2的代码使得其适应当前数据集设置呢？"
标签问题[Other General Issues],PaddlePaddle/PaddleDetection,2021-12-16 06:01:46,5,,4912,1081803356,"请问怎么设置训练中文标签并显示中文标签？
"
[Other General Issues]用tensorrt加速后报错,PaddlePaddle/PaddleDetection,2021-12-16 01:54:59,2,,4909,1081685837,"我按照文档要求已经按照了包含trt的paddle，
paddlepaddle_gpu-2.2.1-cp37-cp37m-linux_x86_64.whl

在设置参数--run_mode=trt_int8 --trt_calib_mode=True跑picodet导出模型的的infer.py时候报错
File ""/home/vehicle_detection/PaddleDetection-release-2.3/deploy/python/infer.py"", line 567, in load_predictor
predictor = create_predictor(config)
ValueError: (InvalidArgument) Pass tensorrt_subgraph_pass has not been registered. Please use the paddle inference library compiled with tensorrt or disable the tensorrt engine in inference configuration!
[Hint: Expected Has(pass_type) == true, but received Has(pass_type):0 != true:1.] (at /paddle/paddle/fluid/framework/ir/pass.h:236)

我不用设置trt的时候是可以的，我试了trt_fp32和trt_fp16，也是同样的报错，不知道什么原因啊？

下面是我的环境配置，除了包含trt的paddlepaddle，还需要装nvidia官方的tensorrt吗？

# Name                    Version                   Build  Channel
_libgcc_mutex             0.1                        main    defaults
appdirs                   1.4.4                    pypi_0    pypi
astor                     0.8.1                    pypi_0    pypi
attrs                     21.2.0                   pypi_0    pypi
babel                     2.9.0                    pypi_0    pypi
bce-python-sdk            0.8.53                   pypi_0    pypi
ca-certificates           2020.12.8            h06a4308_0    defaults
cached-property           1.5.2                    pypi_0    pypi
certifi                   2020.12.5        py37h06a4308_0    defaults
cfgv                      3.2.0                    pypi_0    pypi
chardet                   3.0.4                    pypi_0    pypi
click                     7.1.2                    pypi_0    pypi
cma                       3.0.3                    pypi_0    pypi
colorama                  0.4.4                    pypi_0    pypi
colorlog                  4.6.2                    pypi_0    pypi
cycler                    0.10.0                   pypi_0    pypi
cython                    0.29.21                  pypi_0    pypi
decorator                 4.4.2                    pypi_0    pypi
dill                      0.3.4                    pypi_0    pypi
distlib                   0.3.1                    pypi_0    pypi
easydict                  1.9                      pypi_0    pypi
filelock                  3.0.12                   pypi_0    pypi
flake8                    3.8.4                    pypi_0    pypi
flake8-import-order       0.18.1                   pypi_0    pypi
flask                     1.1.2                    pypi_0    pypi
flask-babel               2.0.0                    pypi_0    pypi
flask-cors                3.0.9                    pypi_0    pypi
funcsigs                  1.0.2                    pypi_0    pypi
future                    0.18.2                   pypi_0    pypi
gast                      0.3.3                    pypi_0    pypi
gitdb                     4.0.9                    pypi_0    pypi
gitpython                 3.1.24                   pypi_0    pypi
gunicorn                  20.0.4                   pypi_0    pypi
h5py                      3.5.0                    pypi_0    pypi
identify                  1.5.10                   pypi_0    pypi
idna                      2.10                     pypi_0    pypi
imageio                   2.9.0                    pypi_0    pypi
imgaug                    0.4.0                    pypi_0    pypi
importlib-metadata        3.3.0                    pypi_0    pypi
iniconfig                 1.1.1                    pypi_0    pypi
itsdangerous              1.1.0                    pypi_0    pypi
jieba                     0.42.1                   pypi_0    pypi
jinja2                    2.11.2                   pypi_0    pypi
joblib                    1.0.0                    pypi_0    pypi
kiwisolver                1.3.1                    pypi_0    pypi
lap                       0.4.0                    pypi_0    pypi
ld_impl_linux-64          2.33.1               h53a641e_7    defaults
libedit                   3.1.20191231         h14c3975_1    defaults
libffi                    3.3                  he6710b0_2    defaults
libgcc-ng                 9.1.0                hdf63c60_0    defaults
libstdcxx-ng              9.1.0                hdf63c60_0    defaults
markupsafe                1.1.1                    pypi_0    pypi
matplotlib                3.3.3                    pypi_0    pypi
mccabe                    0.6.1                    pypi_0    pypi
motmetrics                1.2.0                    pypi_0    pypi
multiprocess              0.70.12.2                pypi_0    pypi
ncurses                   6.2                  he6710b0_1    defaults
networkx                  2.5                      pypi_0    pypi
nltk                      3.5                      pypi_0    pypi
nodeenv                   1.5.0                    pypi_0    pypi
numpy                     1.19.4                   pypi_0    pypi
objgraph                  3.5.0                    pypi_0    pypi
onnx                      1.9.0                    pypi_0    pypi
opencv-python             4.2.0.32                 pypi_0    pypi
openssl                   1.1.1i               h27cfd23_0    defaults
packaging                 21.2                     pypi_0    pypi
paddle2onnx               0.8.2                    pypi_0    pypi
paddlefsl                 1.0.0                    pypi_0    pypi
paddlehub                 2.1.1                    pypi_0    pypi
paddlenlp                 2.1.1                    pypi_0    pypi
paddlepaddle-gpu          2.2.1                    pypi_0    pypi
paddleslim                2.1.1                    pypi_0    pypi
paddlex                   2.0.0                    pypi_0    pypi
pandas                    1.2.0                    pypi_0    pypi
pathlib                   1.0.1                    pypi_0    pypi
pillow                    8.2.0                    pypi_0    pypi
pip                       20.3.3           py37h06a4308_0    defaults
pluggy                    1.0.0                    pypi_0    pypi
pre-commit                2.9.3                    pypi_0    pypi
prettytable               2.0.0                    pypi_0    pypi
protobuf                  3.14.0                   pypi_0    pypi
psutil                    5.8.0                    pypi_0    pypi
py                        1.11.0                   pypi_0    pypi
py-cpuinfo                8.0.0                    pypi_0    pypi
pycocotools               2.0.2                    pypi_0    pypi
pycodestyle               2.6.0                    pypi_0    pypi
pycryptodome              3.9.9                    pypi_0    pypi
pyflakes                  2.2.0                    pypi_0    pypi
pyparsing                 2.4.7                    pypi_0    pypi
pytest                    6.2.5                    pypi_0    pypi
pytest-benchmark          3.4.1                    pypi_0    pypi
python                    3.7.9                h7579374_0    defaults
python-dateutil           2.8.1                    pypi_0    pypi
python-graphviz           0.16                     pypi_0    pypi
pytz                      2020.5                   pypi_0    pypi
pywavelets                1.1.1                    pypi_0    pypi
pyyaml                    5.3.1                    pypi_0    pypi
pyzmq                     20.0.0                   pypi_0    pypi
rarfile                   4.0                      pypi_0    pypi
readline                  8.0                  h7b6447c_0    defaults
regex                     2020.11.13               pypi_0    pypi
requests                  2.24.0                   pypi_0    pypi
scikit-image              0.18.1                   pypi_0    pypi
scikit-learn              0.23.2                   pypi_0    pypi
scipy                     1.5.4                    pypi_0    pypi
sentencepiece             0.1.94                   pypi_0    pypi
seqeval                   1.2.2                    pypi_0    pypi
setuptools                51.0.0           py37h06a4308_2    defaults
shapely                   1.7.1                    pypi_0    pypi
shellcheck-py             0.7.2.1                  pypi_0    pypi
six                       1.15.0                   pypi_0    pypi
sklearn                   0.0                      pypi_0    pypi
smmap                     5.0.0                    pypi_0    pypi
sqlite                    3.33.0               h62c20be_0    defaults
threadpoolctl             2.1.0                    pypi_0    pypi
tifffile                  2020.12.8                pypi_0    pypi
tk                        8.6.10               hbc83047_0    defaults
toml                      0.10.2                   pypi_0    pypi
tqdm                      4.27.0                   pypi_0    pypi
typing-extensions         3.7.4.3                  pypi_0    pypi
urllib3                   1.25.11                  pypi_0    pypi
virtualenv                20.2.2                   pypi_0    pypi
visualdl                  2.2.2                    pypi_0    pypi
wcwidth                   0.2.5                    pypi_0    pypi
werkzeug                  1.0.1                    pypi_0    pypi
wheel                     0.36.2             pyhd3eb1b0_0    defaults
xmltodict                 0.12.0                   pypi_0    pypi
xz                        5.2.5                h7b6447c_0    defaults
yapf                      0.26.0                   pypi_0    pypi
zipp                      3.4.0                    pypi_0    pypi
zlib                      1.2.11               h7b6447c_3    defaults"
Detection方向问题,PaddlePaddle/PaddleDetection,2021-12-16 01:30:33,2,,4908,1081675310,"请问一下，PaddleDetection在检测过程中有涉及方向信息吗？
我在json文件中只发现坐标信息"
labelImg标注完的数据怎么数据增强进入模型的训练？,PaddlePaddle/PaddleDetection,2021-12-15 01:13:07,2,,4896,1080452670,labelImg标注完的数据怎么数据增强进入模型的训练？
[Other General Issues] 用tensorrt加速的时候报错,PaddlePaddle/PaddleDetection,2021-12-14 08:02:21,3,,4889,1079431259,"我按照文档要求已经按照了包含trt的paddle，
paddlepaddle_gpu-2.2.1-cp37-cp37m-linux_x86_64.whl

在设置参数--run_mode=trt_int8 --trt_calib_mode=True跑picodet导出模型的的infer.py时候报错
  File ""/home/vehicle_detection/PaddleDetection-release-2.3/deploy/python/infer.py"", line 567, in load_predictor
    predictor = create_predictor(config)
ValueError: (InvalidArgument) Pass tensorrt_subgraph_pass has not been registered. Please use the paddle inference library compiled with tensorrt or disable the tensorrt engine in inference configuration! 
  [Hint: Expected Has(pass_type) == true, but received Has(pass_type):0 != true:1.] (at /paddle/paddle/fluid/framework/ir/pass.h:236)

我不用设置trt的时候是可以的，不知道什么原因啊？
谢谢"
[BUG]训练问题,PaddlePaddle/PaddleDetection,2021-12-13 06:36:48,7,,4880,1078142192,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.
3. 确认bug是否在新版本里还未修复/The bug has not been fixed in the latest version.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.
![image](https://user-images.githubusercontent.com/78945582/145763813-eb72ea80-9ee1-45a3-b126-eac83e08b1ae.png)
                                                                                      图1

![image](https://user-images.githubusercontent.com/78945582/145763874-a011fd8c-f7b5-4c86-8519-7c9640b53b94.png)
                                                                                     图2

上面图1是基于PicoDet-LCNet 1.5x，图2是基于PicoDet-MobileNetv3-large 1x，验证集中是3892张图片，为什么在Total sample number上却不一致？什么原因？如何修改？
## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?

```none
请填写命令/A placeholder for the command.
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

3. 您使用的数据集是？/What dataset did you use?

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：

4. 请问您使用的Python版本是？/ Please provide the version of Python you used.

5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.


如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
BlazeFace部署手机端提示错误,PaddlePaddle/PaddleDetection,2021-12-13 01:50:19,2,,4877,1077995955,"手机端部署错误:

```
/island/Paddle-Lite/lite/core/program.cc Run:619] Check failed: op_->CheckShape()
```"
[BUG] WSL2 - ubuntu20.04 无法使用多进程读取数据,PaddlePaddle/PaddleDetection,2021-12-10 15:47:04,9,,4869,1076982431,"configs/yolov3下面配置位置中配置多进程读取数据（worker_num: 8）总是会崩溃，请问是怎么回事？

[12/10 23:43:30] ppdet.utils.checkpoint INFO: Finish loading model weights: /home/jeffye/.cache/paddle/weights/yolov3_mobilenet_v1_270e_coco.pdparams
Traceback (most recent call last):
  File ""/home/jeffye/anaconda3/lib/python3.8/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 584, in _get_data
    data = self._data_queue.get(timeout=self._timeout)
  File ""/home/jeffye/anaconda3/lib/python3.8/multiprocessing/queues.py"", line 108, in get
    raise Empty
_queue.Empty
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""/home/jeffye/anaconda3/lib/python3.8/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 584, in _get_data
    data = self._data_queue.get(timeout=self._timeout)
  File ""/home/jeffye/anaconda3/lib/python3.8/multiprocessing/queues.py"", line 108, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/jeffye/anaconda3/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
    self.run()
  File ""/home/jeffye/anaconda3/lib/python3.8/threading.py"", line 870, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/jeffye/anaconda3/lib/python3.8/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 506, in _thread_loop
    batch = self._get_data()
  File ""/home/jeffye/anaconda3/lib/python3.8/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 601, in _get_data
    raise RuntimeError(""DataLoader {} workers exit unexpectedly, "" \
RuntimeError: DataLoader 1 workers exit unexpectedly, pids: 18337
Traceback (most recent call last):
  File ""tools/train.py"", line 171, in <module>
    main()
  File ""tools/train.py"", line 167, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 127, in run
    trainer.train(FLAGS.eval)
  File ""/home/jeffye/code/image/PaddleDetection/ppdet/engine/trainer.py"", line 380, in train
    for step_id, data in enumerate(self.loader):
  File ""/home/jeffye/code/image/PaddleDetection/ppdet/data/reader.py"", line 209, in __next__
    return next(self.loader)
  File ""/home/jeffye/anaconda3/lib/python3.8/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 700, in __next__
    data = self._reader.read_next_var_list()
SystemError: (Fatal) Blocking queue is killed because the data reader raises an exception.
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] (at /paddle/paddle/fluid/operators/reader/blocking_queue.h:166)


同样的代码配置，1. 如果worker_num: 1 可以正常训练；2.不在wsl2下面，在存ubuntu 主机运行也是没有问题。

请问是不是WSL2 这个环境有bug啊？

"
[Document Improvement] 关于ssd_vgg16_300.yml文件问题,PaddlePaddle/PaddleDetection,2021-12-10 08:34:52,2,,4867,1076574742,"在该文件中，对于SSDBoxHead模块中的一些设置比较困惑，比如min_ratio和max_ratio、min_sizes和max_sizes，因为这几个部分和原论文中存在较大区别
```yaml
SSDHead:
   anchor_generator:
   steps: [8, 16, 32, 64, 100, 300]
   aspect_ratios: [[2.], [2., 3.], [2., 3.], [2., 3.], [2.], [2.]]
   min_ratio: 20
   max_ratio: 90
   min_sizes: [30.0, 60.0, 111.0, 162.0, 213.0, 264.0]
   max_sizes: [60.0, 111.0, 162.0, 213.0, 264.0, 315.0]
   offset: 0.5
   flip: true
   min_max_aspect_ratios_order: true
```
我的问题就是：min_ratio和max_ratio、min_sizes和max_sizes是怎么计算得到的？"
[Other General Issues],PaddlePaddle/PaddleDetection,2021-12-09 15:11:25,3,,4862,1075683962,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## 描述问题/Describe the bug
使用ssdlite_mobilenetV3_large_fpn训练Visdrone数据集，训练了600多个epoch，map（iou=0.5）值仍然为0.3，损失始终降不下来，也采用了预训练模型。但是使用faster_rcnn_r50_fpn_1x_visdrone训练VisDrone训练，map(iou=0.5)值达到了41.1，这让我有点不解，是我的训练方式不对吗？

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?

```bash
python tools/train.py -c configs/ssd/ssdlite_mobilenet_v3_large_fpn.yml   --eval
python tools/train.py -c configs/sniper/faster_rcnn_r50_fpn_1x_visdrone.yml --eval
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

> PaddleDetection rc-2.0 paddlepaddle 2.2.0
> configs/ssd/ssdlite_mobilenet_v3_large_fpn.yml

```yaml
architecture: SSD
use_gpu: true
max_iters: 80000
snapshot_iter: 3000
log_iter: 20
metric: COCO
pretrain_weights: pretrainWeights/MobileNetV3_large_x1_0_ssld_fpn_pretrained
save_dir: output
weights: output/ssdlite_mobilenet_v3_large_fpn/model_final
# 80(label_class) + 1(background)
num_classes: 10

SSD:
  backbone: MobileNetV3
  fpn: FPN
  multi_box_head: SSDLiteMultiBoxHead
  output_decoder:
    background_label: 0
    keep_top_k: 200
    nms_eta: 1.0
    nms_threshold: 0.45
    nms_top_k: 400
    score_threshold: 0.01

FPN:
  num_chan: 256
  max_level: 7
  norm_type: bn
  norm_decay: 0.00004
  reverse_out: true

MobileNetV3:
  scale: 1.0
  model_name: large
  extra_block_filters: [[256, 512], [128, 256], [128, 256], [64, 128]]
  feature_maps: [5, 7, 8, 9, 10, 11]
  lr_mult_list: [0.25, 0.25, 0.5, 0.5, 0.75]
  conv_decay: 0.00004

SSDLiteMultiBoxHead:
  aspect_ratios: [[2.], [2., 3.], [2., 3.], [2., 3.], [2., 3.], [2., 3.]]
  base_size: 320
  steps: [16, 32, 64, 107, 160, 320]
  flip: true
  clip: true
  max_ratio: 95
  min_ratio: 20
  offset: 0.5
  conv_decay: 0.00004

LearningRate:
  base_lr: 0.001
  schedulers:
  - !PiecewiseDecay
    gamma: 0.1
    milestones: [54000, 74000]

OptimizerBuilder:
  optimizer:
    momentum: 0.9
    type: Momentum
  regularizer:
    factor: 0.0005
    type: L2

TrainReader:
  inputs_def:
    image_shape: [3, 320, 320]
    fields: ['image', 'gt_bbox', 'gt_class']
  dataset:
    !COCODataSet
    dataset_dir: dataset/VisDrone2019_coco
    anno_path: annotations/train.json
    image_dir: train
  sample_transforms:
  - !DecodeImage
    to_rgb: true
  - !RandomDistort
    brightness_lower: 0.875
    brightness_upper: 1.125
    is_order: true
  - !RandomExpand
    fill_value: [123.675, 116.28, 103.53]
  - !RandomCrop
    allow_no_crop: false
  - !NormalizeBox {}
  - !ResizeImage
    interp: 1
    target_size: 320
    use_cv2: false
  - !RandomFlipImage
    is_normalized: false
  - !NormalizeImage
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    is_scale: true
    is_channel_first: false
  - !Permute
    to_bgr: false
    channel_first: true
  batch_size: 64
  shuffle: true
  drop_last: true
  # Number of working threads/processes. To speed up, can be set to 16 or 32 etc.
  worker_num: 8
  # Size of shared memory used in result queue. After increasing `worker_num`, need expand `memsize`.
  memsize: 8G
  # Buffer size for multi threads/processes.one instance in buffer is one batch data.
  # To speed up, can be set to 64 or 128 etc.
  bufsize: 32
  use_process: true


EvalReader:
  inputs_def:
    image_shape: [3, 320, 320]
    fields: ['image', 'gt_bbox', 'gt_class', 'im_shape', 'im_id']
  dataset:
    !COCODataSet
    dataset_dir: dataset/VisDrone2019_coco
    anno_path: annotations/val.json
    image_dir: val
  sample_transforms:
  - !DecodeImage
    to_rgb: true
  - !NormalizeBox {}
  - !ResizeImage
    interp: 1
    target_size: 320
    use_cv2: false
  - !NormalizeImage
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    is_scale: true
    is_channel_first: false
  - !Permute
    to_bgr: false
    channel_first: True
  batch_size: 8
  worker_num: 8
  bufsize: 32
  use_process: false

TestReader:
  inputs_def:
    image_shape: [3,320,320]
    fields: ['image', 'im_id', 'im_shape']
  dataset:
    !ImageFolder
    anno_path: annotations/instances_val2017.json
  sample_transforms:
  - !DecodeImage
    to_rgb: true
  - !ResizeImage
    interp: 1
    max_size: 0
    target_size: 320
    use_cv2: true
  - !NormalizeImage
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    is_scale: true
    is_channel_first: false
  - !Permute
    to_bgr: false
    channel_first: True
  batch_size: 1
```

> PaddleDetection 2.3 paddlepaddle 2.2.0
> configs/sniper/faster_rcnn_r50_fpn_1x_visdrone.yml

**这个配置文件没有任何修改**
```yaml
_BASE_: [
  '../datasets/coco_detection.yml',
  '../runtime.yml',
  '../faster_rcnn/_base_/optimizer_1x.yml',
  '../faster_rcnn/_base_/faster_rcnn_r50_fpn.yml',
  '../faster_rcnn/_base_/faster_fpn_reader.yml',
]
weights: output/faster_rcnn_r50_fpn_1x_visdrone/model_final


metric: COCO
num_classes: 9

TrainDataset:
  !COCODataSet
    image_dir: train
    anno_path: annotations/train.json
    dataset_dir: dataset/VisDrone2019_coco
    data_fields: ['image', 'gt_bbox', 'gt_class', 'is_crowd']

EvalDataset:
  !COCODataSet
    image_dir: val
    anno_path: annotations/val.json
    dataset_dir: dataset/VisDrone2019_coco

TestDataset:
  !ImageFolder
    anno_path: annotations/val.json
```
说明：由于在paddleDetection2.3中没有找到关于SSD_mobileNetv3_fpn的yaml文件，所以还是到老版本中进行训练，但是这训练的结果着实有点低了。。。。

3. 您使用的数据集是？/What dataset did you use?
VisDrone2019

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.
none

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：
Tesla V100 单卡训练"
部署到yolo模型部署到手机端 提示 cast 不支持,PaddlePaddle/PaddleDetection,2021-12-08 07:54:52,2,,4852,1074113875,"5056-5056/com.baidu.paddle.lite.demo.yolo_detection A/Paddle-Lite: [F 12/ 8 15:44:59. 88 /island/Paddle-Lite/lite/core/program.cc RuntimeProgram:347] Check failed: (kernels.size() > 0): 0!>0      Error: This model is not supported, because kernel for 'cast' is not supported by Paddle-Lite."
[Other General Issues]yolov3的训练时间比faster rcnn训练慢是正常的吗?,PaddlePaddle/PaddleDetection,2021-12-08 07:22:11,2,,4851,1074091645,"我在一张2080Ti显卡上跑yolo3,batchsize设置成2,跑50代需要一整天,这正常吗?faster rcnn 12代可能一个小时就跑出来了,batchsize=1"
[Other General Issues]加了ssld的配置文件的改变在哪里？,PaddlePaddle/PaddleDetection,2021-12-07 08:12:34,4,,4840,1073044416,自带的配置文件，比如YOLOv3_mobilenetv3_large_voc.yml和YOLOv3_mobilenetv3_ssld_large_voc.yml.除了预训练权重不一样，其他的参数都是一样的呀，难道知识蒸馏的实现就是通过预训练权重吗？
[Other General Issues]pp-picodet在infer.py的时候如何验证视频？有相关的样例代码吗？,PaddlePaddle/PaddleDetection,2021-12-07 02:10:48,2,,4837,1072832552,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://github.com/PaddlePaddle/PaddleDetection/tree/develop/docs/tutorials/FAQ) /I have read the [FAQ documentation](https://github.com/PaddlePaddle/PaddleDetection/tree/develop/docs/tutorials/FAQ) but cannot get the expected help.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?

```none
请填写命令/A placeholder for the command.
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

3. 您使用的数据集是？/What dataset did you use?

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：

4. 请问您使用的Python版本是？/ Please provide the version of Python you used.

5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.


如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
[Other General Issues]voc格式的数据没法输出Recall、Precision和F1 score这些数据吗？,PaddlePaddle/PaddleDetection,2021-12-06 08:30:30,2,,4826,1071867911,voc格式的数据没法输出Recall、Precision和F1 score这些数据吗？
nano c++部署编译完成后，测试报错。,PaddlePaddle/PaddleDetection,2021-12-06 07:26:08,2,,4823,1071817014,"
cuda 10.2 gcc 7.5 jetpack 4.4 tensorrt 7.0

下载了这个模型picodet_l_640_coco.pdparams

执行命令是：./main --model_dir=/home/ychen/PaddleDetection/deploy/cpp/build/picodet_l_640_coco.pdparams --image_file=/home/ychen/PaddleDetection/demo/000000014439_640x640.jpg

报错是：terminate called after throwing an instance of 'YAML::BadFile'
  what():  bad file
已放弃 (核心已转储)"
二次训练后模型检测效果没有预训练模型检测好是怎么回事？,PaddlePaddle/PaddleDetection,2021-12-06 03:34:53,2,,4812,1071689472,"我准备了1000张数据，在预训练模型基础上进行二次训练，但是检测结果反而没有预训练模型效果好。
Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.616
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.806
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.644
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.720
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.918
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.639
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.824
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.661
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.746
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.929
| AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |
|---|---|---|---|---|---|---|---|---|---|---|
| 0.616 | 0.806 | 0.644 | 0.720 | 0.918 | 0.639 | 0.824 | 0.661 | 0.746 | 0.929 |"
solov2 train loss_ins problem,PaddlePaddle/PaddleDetection,2021-12-06 02:07:45,2,,4810,1071648577,"![loss_ins](https://user-images.githubusercontent.com/38196678/144775901-5480f2c8-66e0-498a-9f6f-73f8046c4286.png)
paddledet2.3
use solov2_r50_enhance_coco.yml
just change base_lr/8 with one gpu
loss_cate is ok
by the way the EIseg is good, we use it to label my datasets."
picodet的deploy中输入的颜色通道顺序,PaddlePaddle/PaddleDetection,2021-12-04 02:45:15,3,deploy,4805,1071101803,
paddlelite 端侧部署编译问题,PaddlePaddle/PaddleDetection,2021-12-03 08:08:31,2,,4800,1070324553,请问各位大佬，如何用aarch64-linux-gnu交叉编译工具编译padllelite预测库以及picodet代码，我的宿主机是x86linux，目标设备是arm linux
[BUG]我的worker_num在picodet_416_reader.yml已经写了4，为什么训练时显示None？,PaddlePaddle/PaddleDetection,2021-12-03 06:41:06,2,,4797,1070270326,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.
3. 确认bug是否在新版本里还未修复/The bug has not been fixed in the latest version.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.
我的worker_num在picodet_416_reader.yml已经写了4，为什么训练时显示None？
![image](https://user-images.githubusercontent.com/78945582/144556994-27275172-8e1b-4e5b-be86-3fb4e80de512.png)

picodet_416_reader.yml文件
![image](https://user-images.githubusercontent.com/78945582/144557148-2834b458-b7ce-4620-b57b-c3ca99050d28.png)

picodet_mobilenetv3_large_1x_416_coco.yml文件：
![image](https://user-images.githubusercontent.com/78945582/144557238-5b9d15e5-cd9d-4355-a8e9-ac4f631eccd1.png)

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?
python -m paddle.distributed.launch --gpus 0,1,2 tools/train.py -c configs/picodet/more_config/picodet_mobilenetv3_large_1x_416_coco.yml --eval --use_vdl=true
```none
请填写命令/A placeholder for the command.
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

3. 您使用的数据集是？/What dataset did you use?

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：

4. 请问您使用的Python版本是？/ Please provide the version of Python you used.

5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.


如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
[BUG]训练时出现如图所示错误,PaddlePaddle/PaddleDetection,2021-12-03 03:14:52,4,,4793,1070176118,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.
3. 确认bug是否在新版本里还未修复/The bug has not been fixed in the latest version.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.
![image](https://user-images.githubusercontent.com/78945582/144538810-a1070c65-c119-4d8a-b472-e98795278428.png)

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?
python -m paddle.distributed.launch --gpus 0,1,2 tools/train.py -c configs/picodet/more_config/picodet_mobilenetv3_large_1x_416_coco.yml --eval --use_vdl=true
```none
请填写命令/A placeholder for the command.
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

3. 您使用的数据集是？/What dataset did you use?

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：
![Q}F6F3S81X~$I@F9P }{E2R](https://user-images.githubusercontent.com/78945582/144564804-ecee8bdf-02d8-4d44-a7d9-9a64a44ff1e5.png)

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：
Ubuntu18.04
4. 请问您使用的Python版本是？/ Please provide the version of Python you used.
3.6
5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.
11.2

如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
关于Cityflow(AIC21/ AI City / MTMCT)数据集如何下载? ,PaddlePaddle/PaddleDetection,2021-12-02 06:49:58,4,,4782,1069190857,您好，在MTMCT部分用导出的模型基于Python去预测。看见您选用的数据集是AIC21 MTMCT (CityFlow)车辆跨境跟踪数据集。但是点进去发现不是很好下载。如果用自己的视频应该怎么进行预测吗
[BUG],PaddlePaddle/PaddleDetection,2021-12-02 03:25:14,8,,4777,1069088107,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.
3. 确认bug是否在新版本里还未修复/The bug has not been fixed in the latest version.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.

[12/02 01:11:48] ppdet.utils.checkpoint INFO: Save checkpoint: output/picodet_s_320_coco
[12/02 01:11:48] ppdet.engine INFO: Eval iter: 0
[12/02 01:11:58] ppdet.engine INFO: Eval iter: 100
Traceback (most recent call last):
  File ""/home/PaddleDetection/tools/train.py"", line 171, in <module>
    main()
  File ""/home/PaddleDetection/tools/train.py"", line 167, in main
    run(FLAGS, cfg)
  File ""/home/PaddleDetection/tools/train.py"", line 127, in run
    trainer.train(FLAGS.eval)
  File ""/home/PaddleDetection/ppdet/engine/trainer.py"", line 442, in train
    self._eval_with_loader(self._eval_loader)
  File ""/home/PaddleDetection/ppdet/engine/trainer.py"", line 466, in _eval_with_loader
    metric.update(data, outs)
  File ""/home/PaddleDetection/ppdet/metrics/metrics.py"", line 106, in update
    outs, self.clsid2catid, bias=self.bias)
  File ""/home/PaddleDetection/ppdet/metrics/coco_utils.py"", line 53, in get_infer_results
    outs['bbox'], outs['bbox_num'], im_id, catid, bias=bias)
  File ""/home/PaddleDetection/ppdet/metrics/json_results.py"", line 30, in get_det_res
    category_id = label_to_cat_id_map[int(num_id)]
KeyError: 1

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?
python tools/train.py -c /home/PaddleDetection/configs/picodet/picodet_s_320_coco.yml --use_vdl=true --vdl_log_dir=/home/PaddleDetection/vdl_log_dir --eval
```
python tools/train.py -c /home/PaddleDetection/configs/picodet/picodet_s_320_coco.yml --use_vdl=true --vdl_log_dir=/home/PaddleDetection/vdl_log_dir --eval
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

3. 您使用的数据集是？/What dataset did you use?
coco
4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：
2021.11.03: 发布[release/2.3版本]
2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：
无
3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：
ubuntu 18
4. 请问您使用的Python版本是？/ Please provide the version of Python you used.
3.7.3
5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Feb_14_21:12:58_PST_2021
Cuda compilation tools, release 11.2, V11.2.152
Build cuda_11.2.r11.2/compiler.29618528_0
如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
训练的时候为什么默认不保持keep ration？，把keep tation设置为True时，训练报错,PaddlePaddle/PaddleDetection,2021-12-02 00:51:55,15,,4774,1069013420,"各位大佬，训练的时候为啥默认把图像resize成正方形呢，保持长宽比不更好吗，而且把keep ration设置成True还报错，错误如下
![7J(T8U F` ~E0Z57SGRM1}M](https://user-images.githubusercontent.com/48303408/144337842-0f64c113-6eb8-4a29-8a21-3cb638bccc5f.png)
"
训练自己的数据集，模型转换后出现问题,PaddlePaddle/PaddleDetection,2021-12-01 06:54:43,7,,4765,1068031035,各位大佬，我训练自己的数据集，训练完用infer脚本预测正常，转为onnx，openvino推理结果就完全不对，请问是什么原因呢？
使用solov2语义分割验证时报错,PaddlePaddle/PaddleDetection,2021-12-01 02:25:22,1,,4763,1067879373,"可以正常训练，但是在每次保存训练权重参数需要验证的时候报错

报错内容：
 main()
  File ""tools/train.py"", line 167, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 127, in run
    trainer.train(FLAGS.eval)
  File ""/home/kxm/PaddleDetection/ppdet/engine/trainer.py"", line 450, in train
    self._eval_with_loader(self._eval_loader)
  File ""/home/kxm/PaddleDetection/ppdet/engine/trainer.py"", line 484, in _eval_with_loader
    metric.accumulate()
  File ""/home/kxm/PaddleDetection/ppdet/metrics/metrics.py"", line 169, in accumulate
    seg_stats = cocoapi_eval(
  File ""/home/kxm/PaddleDetection/ppdet/metrics/coco_utils.py"", line 111, in cocoapi_eval
    coco_eval = COCOeval(coco_gt, coco_dt, style)
  File ""/home/kxm/.local/lib/python3.8/site-packages/pycocotools/cocoeval.py"", line 76, in __init__
    self.params = Params(iouType=iouType) # parameters
  File ""/home/kxm/.local/lib/python3.8/site-packages/pycocotools/cocoeval.py"", line 527, in __init__
    self.setDetParams()
  File ""/home/kxm/.local/lib/python3.8/site-packages/pycocotools/cocoeval.py"", line 507, in setDetParams
    self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)
  File ""<__array_function__ internals>"", line 5, in linspace
  File ""/home/kxm/.conda/envs/paddle/lib/python3.8/site-packages/numpy/core/function_base.py"", line 119, in linspace
    raise TypeError(
TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.

使用的训练命令：
python tools/train.py -c configs/solov2/solov2_r50_fpn_3x_coco-kxm.yml -r output/solov2_r50_fpn_3x_coco-kxm/598  --use_vdl=True  --eval  --vdl_log_dir=vdl_dir

paddle版本号：release 2.3


"
[Other General Issues]ppyolo-tiny如果训练416*416图片，只需要把ppyolo_tiny_reader.yml的4处320改成416就可以了吗？,PaddlePaddle/PaddleDetection,2021-11-30 10:20:09,2,,4758,1067052413,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.
ppyolo-tiny如果训练416*416图片，只需要把ppyolo_tiny_reader.yml的4处320改成416就可以了吗？
![image](https://user-images.githubusercontent.com/78945582/144029227-5ab6f712-2033-4a36-b295-637a232af192.png)

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?

```none
请填写命令/A placeholder for the command.
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

3. 您使用的数据集是？/What dataset did you use?

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：

4. 请问您使用的Python版本是？/ Please provide the version of Python you used.

5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.


如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
求助，S2ANet调参建议  ,PaddlePaddle/PaddleDetection,2021-11-30 09:37:32,1,,4755,1067006032,"
用的是自己制作的数据集，大概是110张，尺寸1280*1280，是检测想表面划痕的，图像本身存在干扰。训练设置学习率=0.00125，batchsize=1，目前epoch=100跑的时候loss一直保持在在3-4之间，不知道是不是我训练的次数太少，下一步打算试下增加epoch，其他的网络模型参数基本没动。刚刚开始学习这个模型，求助大佬对于S2ANet调参的方向还有哪些？
"
Picodet MNN python报错,PaddlePaddle/PaddleDetection,2021-11-29 15:19:14,3,,4750,1066165962,使用官方提供的MNN模型，利用python预测，在demo_mnn.py 567行 `topk_idx = topk_idx[:C]`报错，C没有定义，请问该如何修改代码？
nano c++部署编译问题,PaddlePaddle/PaddleDetection,2021-11-29 06:55:53,3,,4742,1065674175,"cuda 10.2  gcc 7.5  jetpack 4.4 
![1638168864(1)](https://user-images.githubusercontent.com/95206318/143821753-f7bca66d-1940-42a2-b840-a4a5a07e6871.png)
![1638168924(1)](https://user-images.githubusercontent.com/95206318/143821848-43fbed66-3f62-42cd-afa2-bcf0fdc6ee57.png)
"
关于 TensorRT 动态 shape 问题,PaddlePaddle/PaddleDetection,2021-11-28 03:50:53,5,,4735,1065180619,"您好，TensorRT 版本是 7.2.3 已经设置了动态 shape 
infer_mode:norm
infer_quant:False
inference:deploy/python/infer.py
--device:gpu|cpu
--enable_mkldnn:False|True
--cpu_threads:1|4
--batch_size:1
--use_tensorrt:null
--run_mode:fluid|trt_fp32|trt_fp16
--model_dir:./test_tipc/output/norm_train_gpus_0_autocast_null/fast_rcnn_r50_fpn_1x_coco
--image_dir:./test_tipc/demo
--save_log_path:null
--run_benchmark:True
--trt_max_shape:1333
--trt_min_shape:200
--trt_opt_shape:640

并且在输出也体现了
-----------  Model Configuration -----------
Model Arch: RCNN
Transform Order: 
--transform op: Resize
--transform op: NormalizeImage
--transform op: Permute
--transform op: PadStride

{'image': [1, 3, 200, 200]} {'image': [1, 3, 1333, 1333]} {'image': [1, 3, 640, 640]}
trt set dynamic shape done!

但是依然报错
Traceback (most recent call last):
  File ""deploy/python/infer.py"", line 678, in <module>
    main()
  File ""deploy/python/infer.py"", line 618, in main
    enable_mkldnn=FLAGS.enable_mkldnn)
  File ""deploy/python/infer.py"", line 93, in __init__
    enable_mkldnn=enable_mkldnn)
  File ""deploy/python/infer.py"", line 465, in load_predictor
    predictor = create_predictor(config)
ValueError: (InvalidArgument) some trt inputs dynamic shape info not set, check the INFO log above for more details.
  [Hint: Expected all_dynamic_shape_set == true, but received all_dynamic_shape_set:0 != true:1.] (at /paddle/paddle/fluid/inference/tensorrt/convert/op_converter.h:245)
"
[BUG]PPDET训练，可视化日志出现的疑问,PaddlePaddle/PaddleDetection,2021-11-27 13:36:35,2,,4734,1065044568,"用PPDET2.3版本里面的yolo增强版模型训练，
![)G6QL8JF@0%%$E0K7CK1U~A](https://user-images.githubusercontent.com/73452319/143683638-32129ea6-1f9f-45a9-8fa1-35d714dcf70d.png)
![M9SNZVR4V}HL%_U42T@6M6A](https://user-images.githubusercontent.com/73452319/143683651-1942235c-1801-496b-8c15-44517db1026f.png)
按理说，我这个才第二个epoch，迭代次数才300多，为什么loss图显示的步长已经2000多了呢，"
[BUG] 关于 test_tipc 脚本,PaddlePaddle/PaddleDetection,2021-11-26 08:02:03,26,bug,4729,1064195909,"在使用 test_tipc 时，没有报错，请问出现了这个是啥情况呢

aistudio@jupyter-374446-2883407:~/PaddleDetection$ bash test_tipc/test_train_inference_python.sh ./test_tipc/configs/fast_rcnn_r50_fpn_1x_coco.txt 'lite_train_infer'
ppdet python_infer: fast_rcnn_r50_fpn_1x_coco"
"run eval, error",PaddlePaddle/PaddleDetection,2021-11-26 07:52:27,2,bug,4728,1064189778,"paddleDet-2.1/static/ppdet/utils/eval_utils.py"", line 225, in eval_results                            
if 'proposal' in results[0]:                                                                                                                        
IndexError: list index out of range"
Setting input size and augmentation on custom dataset during training,PaddlePaddle/PaddleDetection,2021-11-24 20:40:13,2,question,4705,1062875784,"Hi, 
I am training on PPYOLOv2.

1. How can I disable all augmentations during Training ? I do not want any form of augmentation as my dataset is already augmented.
  (Will deleting the sample transforms and batch transforms in ppyolov2_reader.yml disable all augmentation?)

2. Where can I set the input image size for training. (say 448 x 448) 
"
请教一下在notebook训练完成后下载下来的模型在本地win10/centos7无法预测,PaddlePaddle/PaddleDetection,2021-11-24 13:42:37,3,,4701,1062445084,"同样的图片在notebook环境正常运行infer.py，并且可以预测内容
![6VR3U71BPS%X}UJ}NO4Z63D](https://user-images.githubusercontent.com/23081513/143249128-fbe30102-2ff3-42c3-b52f-31636e130690.png)

但是下载模型到本地运行后会报警告,然后预测不出框，感觉应该是模型加载有问题导致的
![NKCANKV4~$HY~U_71S6 QWM](https://user-images.githubusercontent.com/23081513/143249157-19954c4c-081e-4128-853d-bcc273ac4e8d.png)

[11/23 23:58:39] ppdet.utils.checkpoint INFO: The shape [20] in pretrained weight bbox_head.bbox_delta.bias is unmatched with the shape [320] in model bbox_head.bbox_delta.bias. And the weight bbox_head.bbox_delta.bias will not be loaded
[11/23 23:58:39] ppdet.utils.checkpoint INFO: The shape [1024, 20] in pretrained weight bbox_head.bbox_delta.weight is unmatched with the shape [1024, 320] in model bbox_head.bbox_delta.weight. And the weight bbox_head.bbox_delta.weight will not be loaded
[11/23 23:58:39] ppdet.utils.checkpoint INFO: The shape [6] in pretrained weight bbox_head.bbox_score.bias is unmatched with the shape [81] in model bbox_head.bbox_score.bias. And the weight bbox_head.bbox_score.bias will not be loaded
[11/23 23:58:39] ppdet.utils.checkpoint INFO: The shape [1024, 6] in pretrained weight bbox_head.bbox_score.weight is unmatched with the shape [1024, 81] in model bbox_head.bbox_score.weight. And the weight bbox_head.bbox_score.weight will not be loaded
[11/23 23:58:39] ppdet.utils.checkpoint INFO: The shape [20] in pretrained weight bbox_head.bbox_delta.bias is unmatched with the shape [320] in model bbox_head.bbox_delta.bias. And the weight bbox_head.bbox_delta.bias will not be loaded
[11/23 23:58:39] ppdet.utils.checkpoint INFO: The shape [1024, 20] in pretrained weight bbox_head.bbox_delta.weight is unmatched with the shape [1024, 320] in model bbox_head.bbox_delta.weight. And the weight bbox_head.bbox_delta.weight will not be loaded
[11/23 23:58:39] ppdet.utils.checkpoint INFO: The shape [6] in pretrained weight bbox_head.bbox_score.bias is unmatched with the shape [81] in model bbox_head.bbox_score.bias. And the weight bbox_head.bbox_score.bias will not be loaded
[11/23 23:58:39] ppdet.utils.checkpoint INFO: The shape [1024, 6] in pretrained weight bbox_head.bbox_score.weight is unmatched with the shape [1024, 81] in model bbox_head.bbox_score.weight. And the weight bbox_head.bbox_score.weight will not be loaded
[11/23 23:58:40] ppdet.utils.checkpoint INFO: Finish loading model weights: F:\workspace\Django_restful\Django_api\testapi\model\36.pdparams

"
[BUG]--draw_threshold设置成任何值结果都不变,PaddlePaddle/PaddleDetection,2021-11-24 10:12:45,3,,4697,1062234808,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.
3. 确认bug是否在新版本里还未修复/The bug has not been fixed in the latest version.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.
--draw_threshold设置成任何值结果都不变，设置成0.99，低于0.99的框依然会出现
![image](https://user-images.githubusercontent.com/78945582/143218704-7a916e70-7b6c-4e2f-9291-9ec1fbe3aba2.png)

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?
python tools/infer.py -c configs/picodet/picodet_s_416_coco.yml --infer_img=demo/3892.jpg -o weights=best_model_1.pdparams --save_txt=true --draw_threshold 0.99
```none
请填写命令/A placeholder for the command.
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

3. 您使用的数据集是？/What dataset did you use?

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：

4. 请问您使用的Python版本是？/ Please provide the version of Python you used.

5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.


如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
ttfnet 训练过程中，wh_loss和loss值一直增大，ap为0,PaddlePaddle/PaddleDetection,2021-11-23 08:58:10,4,,4679,1060980941,"
训练过程的信息：loss值一直增加
![训练过程](https://user-images.githubusercontent.com/31815086/142994646-56cf8215-290f-4d21-870a-f8ca0d707f72.png)
数据集配置：
    数据集是通过 tools/x2coco.py  将 labelme  转换到coco
![data_config](https://user-images.githubusercontent.com/31815086/142995013-aae2df21-2d0a-4f80-b67a-242dcfb0783c.JPG)

这种是什么问题导致的？学习率？


"
🤩  PP-TinyPose已发布，欢迎大家试用及讨论,PaddlePaddle/PaddleDetection,2021-11-23 04:53:58,21,,4666,1060838507,"🔥 PP-TinyPose是针对移动端设备研发的轻量级实时关键点检测模型，具有以下特色：

- 精度高：场景适应能力更强，在COCO上AP可达50.3。
- 速度快：在SD865上，单人配置可达122FPS，多人配置可达30FPS以上。
- 部署便捷：支持基于Paddle Inference/Paddle Lite的部署方案，我们同时提供了C++、Python及Paddle Lite的部署示例代码。

详细细节请见：https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.3/configs/keypoint/tiny_pose

我们同时也提供了在APP上实现[健身部署APP的案例](https://github.com/zhiboniu/pose_demo_android)，供大家参考。

为了方便大家交流沟通，欢迎扫码添加微信群，提出您的意见、建议或者使用中的疑问。
<img width=""250"" alt=""tinypose_group"" src=""https://user-images.githubusercontent.com/48054808/161363203-cc7459f9-3f9a-43f4-9dde-6d4badcf3a83.JPG"">

"
[BUG]训练pp-picodet-s，训练3-4天，突然卡住不动,PaddlePaddle/PaddleDetection,2021-11-22 06:30:06,7,,4645,1059747883,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.
3. 确认bug是否在新版本里还未修复/The bug has not been fixed in the latest version.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.
训练pp-picodet-s，训练3-4天，突然卡住不动
![image](https://user-images.githubusercontent.com/78945582/142811038-650480b2-b230-40e0-94b6-f77b665a507d.png)

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?
export CUDA_VISIBLE_DEVICES=1
python tools/train.py -c configs/picodet/picodet_s_416_coco.yml --eval
```none
请填写命令/A placeholder for the command.
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.
无
3. 您使用的数据集是？/What dataset did you use?

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：

4. 请问您使用的Python版本是？/ Please provide the version of Python you used.

5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.


如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
[Other General Issues]hi，尊敬的开发者，有个问题想请教，cascade处理不同图片尺寸输入该如何处理？,PaddlePaddle/PaddleDetection,2021-11-17 07:23:45,2,question,4613,1055796095,"我现在遇到一个问题，在训练模型的时候，因为采集图片的设备多种多样，采集图片的尺寸也就不一样。
使用的是cascade-rcnn
我看配置文件里，
训练时，可以随机增强成5种尺寸的输入大小
评估和预测时只有1种尺寸大小。
我的问题是：
1，cascade-rcnn网络要求的输入图片尺寸是多少呢？是固定大小还是不固定呢？如果不固定是因为全卷积吗？如果算法里使用了全链接层就必须固定对吗？
2，预测不一样尺寸的图片，最后会resize到配置文件中设置的尺寸上输入，那么模型网络的输入是不是还要resize到固定inputsize的呢？比如我设置的输入图片大小是1600800，我拍到的图片是40002000，训练/预测时，先resize到1600800，再resize到网络需要的输入（比如说684684）才送入网络训练/预测。
3，在原始图片上对锚框聚类后，配置文件里的锚框尺寸是以原始图片为基准，还是以resize到网络输入的尺寸大小为准。
4，预测/训练不同大小的图片时，比如尺寸从（500X500）到（2000X2000）按步长200尺寸均匀分布。怎样处理不同尺寸的训练和预测比较合适呢？"
[Other General Issues]训练过程中的问题,PaddlePaddle/PaddleDetection,2021-11-16 09:06:10,13,,4602,1054624792,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.
下图中1992*batchsize与我训练集的图片数量相差较大，是什么原因？
![image](https://user-images.githubusercontent.com/78945582/141955365-48c6987e-c48f-4c13-8935-afe9989cafdf.png)

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?

```none
请填写命令/A placeholder for the command.
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.

3. 您使用的数据集是？/What dataset did you use?

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：

4. 请问您使用的Python版本是？/ Please provide the version of Python you used.

5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.


如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
picodet预测速度缓慢,PaddlePaddle/PaddleDetection,2021-11-12 16:14:57,5,question#deploy,4569,1052130388,"训练picodet_l_640后使用export_model.py导出，模型大小为13M，使用deploy/python/infer.py预测视频，需要61ms每帧，使用的是GTX1650独立显卡，paddle2.1.3，与官方提供的预测速度相差很大，请问该如何提高？
"
加载configs/yolov3_darknet.yml测试出现 keyerror: no key testDataset ,PaddlePaddle/PaddleDetection,2021-11-11 09:05:59,3,,4555,1050753134,"通过运行命令python tools/infer.py -c configs/yolov3_darknet.yml -o weights=https://paddlemodels.bj.bcebos.com/object_detection/yolov3_darknet.tar --infer_img=demo/000000014439.jpg出现以下错误：
    keyerror: no key testDataset ,报的错误是出现在 ppdet / engine / trainer.py文件中的66行[](https://gitee.com/paddlepaddle/PaddleDetection/blob/release/2.2/ppdet/engine/trainer.py)

- [ ] "
could not find suitable distribution for requirement.parse('et-xmlfile') ,PaddlePaddle/PaddleDetection,2021-11-11 04:48:16,2,,4550,1050570320,"os: windows10
gpu: cuda11.2
operation:
 1.  git clone https://github.com/PaddlePaddle/PaddleDetection.git
 2. cd PaddleDetection
 3. python setup.py install

result:
    error:could not find suitable distribution for requirement.parse('et-xmlfile') ,but I have installed the ' et-xmlfile'."
实例分割效果不太好，有没有什么建议？,PaddlePaddle/PaddleDetection,2021-11-10 08:03:38,2,,4535,1049510928,"想要的结果是：
![image](https://user-images.githubusercontent.com/32347832/141073228-2e51365c-92b8-47cc-8270-00bf19126e49.png)
solov2的结果是：
![image](https://user-images.githubusercontent.com/32347832/141073394-9fba9805-389e-4c1c-b516-732e0442126a.png)
mask rcnn的结果是：
![image](https://user-images.githubusercontent.com/32347832/141073521-4d349d13-1233-4bbb-b90a-b4054b5b0635.png)
solov2的box错误率太高，mask rcnn感觉不工作呀，怎么办呢？"
PAFNet训练loss难以下降,PaddlePaddle/PaddleDetection,2021-11-09 06:30:32,5,help wanted#training,4521,1048238885,"在自己的实验过程中更换了PAFNet的Backbone进行相关实验，由于此前没有使用dcn_head导致精度可能还是存在差距，所以在近期使用CSPDarknet+dcn_head的配置进行实验，但是发现在实验过程中模型的loss难以下降，使用COCO数据集跑了12个epoch后可以说是什么也没检测到，这是因为DCN的问题所导致的吗？
![image](https://user-images.githubusercontent.com/59326436/140873661-94ed3280-fa83-45ac-ac1a-2e07516d71c4.png)
"
PAFNet中的Cutmix、AGS？,PaddlePaddle/PaddleDetection,2021-11-08 10:42:38,3,,4508,1047288652,"在PAFNet的论文中提及了CutMix被用于提高模型的检测性能，但是在pafnet_reader.py之中我好像没有看到CutMix的具体调用，请问是如何设置的呢？
![image](https://user-images.githubusercontent.com/59326436/140727977-ff6d3e04-ba03-4ff1-a048-80f4cba48d88.png)
"
ABORT!!! Out of all 4 trainers,PaddlePaddle/PaddleDetection,2021-11-05 12:45:03,3,,4488,1045809609,"四显卡，训练一会儿就会报错
ERROR:root:DataLoader reader thread raised an exception!
ERROR:root:DataLoader reader thread failed((Unavailable) Memory map failed when rebuild shared memory.
  [Hint: Expected ptr != ((void *) -1), but received ptr:0xffffffffffffffff == ((void *) -1):0xffffffffffffffff.] (at /paddle/paddle/fluid/memory/allocation/mmap_allocator.cc:97)
) to read data from workers' result queue.
Exception in thread Thread-1376:
Traceback (most recent call last):
  File ""/root/Downloads/enter/envs/paddle/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/root/Downloads/enter/envs/paddle/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/root/Downloads/enter/envs/paddle/lib/python3.6/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 411, in _thread_loop
    batch = self._get_data()
  File ""/root/Downloads/enter/envs/paddle/lib/python3.6/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 508, in _get_data
    six.reraise(*sys.exc_info())
  File ""/root/Downloads/enter/envs/paddle/lib/python3.6/site-packages/six.py"", line 719, in reraise
    raise value
  File ""/root/Downloads/enter/envs/paddle/lib/python3.6/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 482, in _get_data
    data = self._data_queue.get(timeout=self._timeout)
  File ""/root/Downloads/enter/envs/paddle/lib/python3.6/multiprocessing/queues.py"", line 113, in get
    return _ForkingPickler.loads(res)
RuntimeError: (Unavailable) Memory map failed when rebuild shared memory.
  [Hint: Expected ptr != ((void *) -1), but received ptr:0xffffffffffffffff == ((void *) -1):0xffffffffffffffff.] (at /paddle/paddle/fluid/memory/allocation/mmap_allocator.cc:97)


[11/05 20:37:28] ppdet.utils.checkpoint INFO: Save checkpoint: output/yolov3_mobilenet_v1_270e_coco_ppk
[11/05 20:37:28] ppdet.engine INFO: Epoch: [14210] [0/6] learning_rate: 0.000010 loss_xy: 22.927473 loss_wh: 5.088367 loss_obj: 5.117811 loss_cls: 7.450146 loss: 41.700451 eta: 6:54:29 batch_cost: 0.5585 data_cost: 0.2699 ips: 7.1616 images/s
INFO 2021-11-05 20:38:03,581 launch_utils.py:327] terminate all the procs
ERROR 2021-11-05 20:38:03,581 launch_utils.py:584] ABORT!!! Out of all 4 trainers, the trainer process with rank=[1, 3] was aborted. Please check its log.
INFO 2021-11-05 20:38:06,584 launch_utils.py:327] terminate all the procs
"
加载预训练权重后报错,PaddlePaddle/PaddleDetection,2021-11-04 15:29:06,3,,4471,1044890774,"进行迁移学习的时候，训练自己的数据集加载预训练权重后训练报错
[11/04 23:20:44] ppdet.utils.checkpoint INFO: Finish loading model weights: /project/train/src_repo/PaddleDetection/weights/picodet_l_640_coco.pdparams
Traceback (most recent call last):
  File ""tools/train.py"", line 171, in <module>
    main()
  File ""tools/train.py"", line 167, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 127, in run
    trainer.train(FLAGS.eval)
  File ""/project/train/src_repo/PaddleDetection/ppdet/engine/trainer.py"", line 393, in train
    outputs = model(data)
  File ""/usr/local/lib/python3.6/dist-packages/paddle/fluid/dygraph/layers.py"", line 898, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File ""/project/train/src_repo/PaddleDetection/ppdet/modeling/architectures/meta_arch.py"", line 54, in forward
    out = self.get_loss()
  File ""/project/train/src_repo/PaddleDetection/ppdet/modeling/architectures/picodet.py"", line 79, in get_loss
    loss_gfl = self.head.get_loss(head_outs, self.inputs)
  File ""/project/train/src_repo/PaddleDetection/ppdet/modeling/heads/simota_head.py"", line 393, in get_loss
    flatten_bboxes.detach(), gt_box, gt_labels)
  File ""/project/train/src_repo/PaddleDetection/ppdet/modeling/heads/simota_head.py"", line 55, in multi_apply
    return tuple(map(list, zip(*map_results)))
  File ""/project/train/src_repo/PaddleDetection/ppdet/modeling/heads/simota_head.py"", line 131, in _get_target_single
    F.sigmoid(cls_preds), centors, decoded_bboxes, gt_bboxes, gt_labels)
  File ""/project/train/src_repo/PaddleDetection/ppdet/modeling/assigners/simota_assigner.py"", line 186, in __call__
    priors, gt_bboxes)
  File ""/project/train/src_repo/PaddleDetection/ppdet/modeling/assigners/simota_assigner.py"", line 77, in get_in_gt_and_in_center_info
    is_in_gts_all = is_in_gts.sum(axis=1) > 0
  File ""/usr/local/lib/python3.6/dist-packages/paddle/tensor/math.py"", line 743, in sum
    'reduce_all', reduce_all_flag)
RuntimeError: (NotFound) Operator reduce_sum does not have kernel for data_type[bool]:data_layout[ANY_LAYOUT]:place[CUDAPlace(0)]:library_type[PLAIN].
  [Hint: Expected kernel_iter != kernels.end(), but received kernel_iter == kernels.end().] (at /paddle/paddle/fluid/imperative/prepared_operator.cc:135)
  [operator < reduce_sum > error]

只是将coco detection.yml中类别数目改为2,同时更改了类名，然后修改了picodet_l_640_coco.yml的pretrained weights.如果不加载coco预训练权重的话，没有这个错误"
用自己标记的训练集完成后的模型无法推理图片,PaddlePaddle/PaddleDetection,2021-11-04 05:51:09,26,,4455,1044387055,"我用单核gpu自己打标记的训练集训练的模型，推理不出单张图像，请帮忙判断一下是哪里的问题

1，python tools/train.py -c configs/picodet/picodet_s_320_coco.yml 训练过程无报错
2，picodet_s_320_coco.yml，runtime，picodet_esnet， optimizer_300e 没改动
3，coco_detection 只改了自己的dataset
4，picodet_320_reader改了几个配置
![N(EU%Z `O3E_}INEFH QA{9](https://user-images.githubusercontent.com/93698316/140264577-39b0d660-24a6-474b-ac22-340f1a4ffb47.png)
"
PP-yolo模型训练类别与预测类别数量不一致,PaddlePaddle/PaddleDetection,2021-10-21 12:22:54,2,,4353,1032429660,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

请教一下老师，模型训练的时候，只有两种目标，class id应该只有0和1两个，为什么最后预测出来的序号不是0和1，而且4，5，6，而且还是三个类别呢？
图1和图2是模型训练的类别：red和green两类。
![微信图片_20211021201545](https://user-images.githubusercontent.com/66996161/138275318-0d9f5dd0-d551-49ea-803c-41923fc3969d.png)
![微信图片_20211021201551](https://user-images.githubusercontent.com/66996161/138275326-8067d4c5-64f3-434a-91b4-d73b02d2ad92.png)
图3和图4是模型转换成nb格式后，使用paddlelite进行预测时的代码、结果。
![微信图片_20211021202651](https://user-images.githubusercontent.com/66996161/138276911-5458a78c-65cf-467b-bb93-9cc926e07303.png)
![微信图片_20211021202541](https://user-images.githubusercontent.com/66996161/138276726-8bf83e33-af0c-44e0-a22b-7316201da82a.png)

可以发现，有3个类别，请教一下老师，这是什么原因呢？

```
## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号。
paddlepaddle-gpu==2.1.0
paddledet==2.1.0
2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleLite。PaddleLite==2.9

3. 请提供您使用的操作系统信息。Linux

4. 请问您使用的Python版本是？3.7
"
如何使用ReduceLROnPlateau,PaddlePaddle/PaddleDetection,2021-10-18 13:02:32,4,,4328,1029086218,请问在paddledection中训练模型，目前想使用ReduceLROnPlateau，应该如何实现呢？
paddledetection部署问题,PaddlePaddle/PaddleDetection,2021-10-18 09:42:23,2,,4327,1028882132,"我通过https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.2/deploy/cpp/docs/windows_vs2019_build.md
进行c++部署,并且生成了dll，在c#和python中都可以成功调用。现在我想更换一台电脑（该电脑未安装cudnn tensorRT opencv），请问可以直接调用我之前生成的dll吗？还是得重新部署？"
如何把scale_factor和im_shape设置为常量，再导出模型？,PaddlePaddle/PaddleDetection,2021-10-13 07:07:24,10,,4306,1024871172,"1.服务器环境：RTX3090独显24G，128G主显，centos7.6系统，cuda11.0+cudnn8.0.5

2.深度学习环境：用conda创建虚拟环境搭建好paddle version 2.1.3，PaddleDetection环境

3.本人按照了[30分钟快速上手PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.2/docs/tutorials/GETTING_STARTED_cn.md)从准备数据到训练和验证的整个流程走通，也通过[PaddleDetection模型导出为ONNX格式教程](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.2/deploy/EXPORT_ONNX_MODEL.md)转成yolov3.onnx的文件，模型文件大概为240MB左右

4.用LabelImage标注的3种类别的数据，用yolov3-darknet骨干网络进行训练，进行了150张三种场景的图片进行训练，然后3种场景随机各测试了10张图片，检测准确率可达大约70%

目前一个机器人识别场景的实战落地项目遇到的问题，在线求急帮助！！！
问题：
1.我转成yolov3.onnx模型出来，准备部署到开发板进行推理加速之前，我用Netron打开，发现image输入这里，存在两个超参数scale_factor和im_shape的type: float32[-1,2]；
因为
（1）我发现这两个超参数在训练时使用的，推理时用不着，为了不影响编译程序会认为有3个输入
（2）训练时候是可以接受任意尺寸大小的输入，但我们后期是深度相机获取RGB图像数据流的尺寸大小是固定的，我们后期推理是输入固定尺寸大小的图像

所以想把scale_factor和im_shape设置为常量，但是发现paddle文档只有参数的说明，没有具体如何在代码处设置为固定参数的教程。。。[PaddleDetection模型导出教程](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.2/deploy/EXPORT_MODEL.md)

2.scale_factor和im_shape设置为常量以后，还需要重新训练吗？

3.参考了[test_client.py中scale_factor设置为固定数](https://github.com/PaddlePaddle/PaddleDetection/issues/3819)，但好像帮助不大"
[Feature Request]S2anet  以onnx格式导出,PaddlePaddle/PaddleDetection,2021-10-11 09:40:52,3,feature request,4289,1022485330,"**PaddleDetection team appreciate any suggestion or problem you delivered~**


## 描述问题/Describe the bug
希望将paddlepaddle 框架下的S2anet 以onnx格式导出

## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?
   
   在文件 https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.0/ppdet/modeling/heads/s2anet_head.py
  希望将s2anet 以onnx 格式导出，却总是提示错误
```none
model = S2ANetHead()
input_spec = paddle.static.InputSpec([None,256], 'float32', 'image')
paddle.jit.save(layer=model,path='paddlestatic/linear.model',input_spec=[input_spec])

```
2.会提示如下错误：

Traceback (most recent call last):
  File ""D:\codewz\pds2anet.py"", line 1106, in <module>
    paddle.jit.save(layer=model,path='paddlestatic/linear.model',input_spec=[input_spec])
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\decorator.py"", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\paddle\fluid\wrapped_decorator.py"", line 25, in __impl__
    return wrapped_func(*args, **kwargs)
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\paddle\fluid\dygraph\base.py"", line 40, in __impl__
    return func(*args, **kwargs)
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\paddle\fluid\dygraph\jit.py"", line 736, in save
    concrete_program = static_forward.concrete_program
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 453, in concrete_program
    return self.concrete_program_specify_input_spec(input_spec=None)
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 491, in concrete_program_specify_input_spec
    *desired_input_spec)
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 401, in get_concrete_program
    concrete_program, partial_program_layer = self._program_cache[cache_key]
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 714, in __getitem__
    self._caches[item] = self._build_once(item)
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 705, in _build_once
    class_instance=cache_key.class_instance)
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\decorator.py"", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\paddle\fluid\wrapped_decorator.py"", line 25, in __impl__
    return wrapped_func(*args, **kwargs)
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\paddle\fluid\dygraph\base.py"", line 40, in __impl__
    return func(*args, **kwargs)
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 623, in from_func_spec
    static_func = convert_to_static(dygraph_function)
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 140, in convert_to_static
    static_func = _FUNCTION_CACHE.convert_with_cache(function)
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 77, in convert_with_cache
    static_func = self._convert(func)
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\program_translator.py"", line 115, in _convert
    root_wrapper = self._dygraph_to_static.get_static_ast(root)
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\ast_transformer.py"", line 61, in get_static_ast
    self.transfer_from_node_type(self.static_analysis_root)
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\ast_transformer.py"", line 92, in transfer_from_node_type
    self._apply(transformer, node_wrapper, log_level=index + 1)
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\ast_transformer.py"", line 65, in _apply
    transformer(node_wrapper).transform()
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\tensor_shape_transformer.py"", line 118, in transform
    self.visit(self.root)
  File ""D:\apps\anaconda\envs\paddle\lib\ast.py"", line 253, in visit
    return visitor(node)
  File ""D:\apps\anaconda\envs\paddle\lib\ast.py"", line 308, in generic_visit
    value = self.visit(value)
  File ""D:\apps\anaconda\envs\paddle\lib\ast.py"", line 253, in visit
    return visitor(node)
  File ""D:\apps\anaconda\envs\paddle\lib\ast.py"", line 308, in generic_visit
    value = self.visit(value)
  File ""D:\apps\anaconda\envs\paddle\lib\ast.py"", line 253, in visit
    return visitor(node)
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\tensor_shape_transformer.py"", line 186, in visit_For
    self.generic_visit(node)
  File ""D:\apps\anaconda\envs\paddle\lib\ast.py"", line 308, in generic_visit
    value = self.visit(value)
  File ""D:\apps\anaconda\envs\paddle\lib\ast.py"", line 253, in visit
    return visitor(node)
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\tensor_shape_transformer.py"", line 121, in visit_Assign
    update_static_shape_var_node = self._update_name_to_var_shape(node)
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\paddle\fluid\dygraph\dygraph_to_static\tensor_shape_transformer.py"", line 372, in _update_name_to_var_shape
    static_shape_var_name).body[0].value
  File ""D:\apps\anaconda\envs\paddle\lib\site-packages\gast\gast.py"", line 298, in parse
    return ast_to_gast(_ast.parse(*args, **kwargs))
  File ""D:\apps\anaconda\envs\paddle\lib\ast.py"", line 35, in parse
    return compile(source, filename, mode, PyCF_ONLY_AST)
  File ""<unknown>"", line 1
    self_featmap_sizes[i]__static_convert_var_shape_suffix_0
                                                           ^
SyntaxError: invalid syntax




## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：
   Package                           Version
--------------------------------- ---------
astor                             0.8.1
Babel                             2.9.1
backports.entry-points-selectable 1.1.0
bce-python-sdk                    0.8.62
brotlipy                          0.7.0
cached-property                   1.5.2
certifi                           2021.5.30
cffi                              1.14.6
cfgv                              3.3.1
charset-normalizer                2.0.4
click                             8.0.1
colorama                          0.4.4
colorlog                          6.4.1
cryptography                      3.4.7
cycler                            0.10.0
Cython                            0.29.24
dataclasses                       0.8
decorator                         5.0.9
dill                              0.3.4
distlib                           0.3.3
easydict                          1.9
filelock                          3.0.12
flake8                            3.9.2
Flask                             2.0.1
Flask-Babel                       2.0.0
flatbuffers                       2.0
future                            0.18.2
gast                              0.3.3
gitdb                             4.0.7
GitPython                         3.1.18
h5py                              3.1.0
identify                          2.2.15
idna                              3.2
importlib-metadata                4.8.1
importlib-resources               5.2.2
itsdangerous                      2.0.1
jieba                             0.42.1
Jinja2                            3.0.1
joblib                            1.0.1
kiwisolver                        1.3.1
lap                               0.4.0
MarkupSafe                        2.0.1
matplotlib                        3.3.4
mccabe                            0.6.1
mkl-fft                           1.3.0
mkl-random                        1.1.1
mkl-service                       2.3.0
multiprocess                      0.70.12.2
nodeenv                           1.6.0
numpy                             1.19.2
olefile                           0.46
onnx                              1.9.0
onnxruntime                       1.9.0
opencv-python                     4.5.3.56
packaging                         21.0
paddle2onnx                       0.8.2
paddlehub                         2.1.0
paddlenlp                         2.0.8
paddlepaddle                      2.1.3
pandas                            1.1.5
Pillow                            8.3.1
pip                               21.0.1
platformdirs                      2.4.0
pre-commit                        2.15.0
protobuf                          3.17.2
pycocotools                       2.0.2
pycodestyle                       2.7.0
pycparser                         2.20
pycryptodome                      3.10.4
pyflakes                          2.3.1
pyOpenSSL                         20.0.1
pyparsing                         2.4.7
PySocks                           1.7.1
python-dateutil                   2.8.2
pytz                              2021.1
PyYAML                            5.4.1
pyzmq                             22.3.0
rarfile                           4.0
requests                          2.26.0
scikit-learn                      0.24.2
scipy                             1.5.4
seqeval                           1.2.2
setuptools                        58.0.4
shellcheck-py                     0.7.2.1
six                               1.16.0
smmap                             4.0.0
tensorboardX                      2.4
terminaltables                    3.1.0
threadpoolctl                     2.2.0
toml                              0.10.2
torch                             1.9.1
torchvision                       0.10.1
tqdm                              4.62.3
typing-extensions                 3.10.0.2
urllib3                           1.26.6
virtualenv                        20.8.1
visualdl                          2.2.1
Werkzeug                          2.0.1
wheel                             0.37.0
win-inet-pton                     1.1.0
wincertstore                      0.2
zipp                              3.5.0

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：
   没有

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：
   win10

4. 请问您使用的Python版本是？/ Please provide the version of Python you used.
     Python 3.6.13 

5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.

   没有显卡，ONNX 格式导出，也需是cpu版本。
如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
训练时出现错误，是不是我的配置出错了,PaddlePaddle/PaddleDetection,2021-10-11 02:49:35,3,,4286,1022198557,"训练时出错：
![image](https://user-images.githubusercontent.com/34839719/136726416-5a5ae9e9-a353-4ba6-b3cb-adedc7821f5f.png)
"
无标签训练loss变nan、报错[BUG],PaddlePaddle/PaddleDetection,2021-10-08 02:23:49,9,question,4272,1020614303,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## 描述问题
bs=2时，faster_rcnn_r50_1x_coco.yml和 faster_rcnn_r50_fpn_1x_coco.yml单卡训练loss会变nan。
bs=4，双卡训练faster_rcnn_r50_1x_coco.yml loss仍会变nan，调小lr无用，加上fpn后报错。
## 复现/Reproduction

1. 您使用的命令是？/What command or script did you run?

```
python  tools/train.py -c configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.yml
python -m paddle.distributed.launch  --gpus 1,2  tools/train.py -c configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.yml
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。
3. 您使用的数据集是？/What dataset did you use?
39506正常标注数据+35406无标签数据，设置empty_ratio为 0.1。
4. 请提供您出现的报错信息及相关log。
bs=2，4卡训练faster_rcnn_r50_1x_coco.yml 已经调小lr，LearningRate:  base_lr: 0.0001。
```
[10/08 00:43:11] ppdet.engine INFO: Epoch: [0] [  560/21947] learning_rate: 0.000060 loss_rpn_cls: 0.677536 loss_rpn_reg: 0.017564 loss_bbox_cls: 0.030547 loss_bbox_reg: 0.006676 loss: 0.740015 eta: 1 day, 10:42:49 batch_cost: 0.4942 data_cost: 0.0003 ips: 4.0471 images/s
[10/08 00:43:21] ppdet.engine INFO: Epoch: [0] [  580/21947] learning_rate: 0.000062 loss_rpn_cls: 0.675153 loss_rpn_reg: 0.007527 loss_bbox_cls: 0.026712 loss_bbox_reg: 0.006683 loss: 0.716233 eta: 1 day, 10:44:35 batch_cost: 0.4883 data_cost: 0.0002 ips: 4.0960 images/s
[10/08 00:43:30] ppdet.engine INFO: Epoch: [0] [  600/21947] learning_rate: 0.000064 loss_rpn_cls: 0.673870 loss_rpn_reg: 0.013568 loss_bbox_cls: 0.029132 loss_bbox_reg: 0.004522 loss: 0.727936 eta: 1 day, 10:44:04 batch_cost: 0.4734 data_cost: 0.0002 ips: 4.2245 images/s
[10/08 00:43:40] ppdet.engine INFO: Epoch: [0] [  620/21947] learning_rate: 0.000066 loss_rpn_cls: 0.673325 loss_rpn_reg: 0.010504 loss_bbox_cls: 0.028857 loss_bbox_reg: 0.003622 loss: 0.716996 eta: 1 day, 10:44:18 batch_cost: 0.4787 data_cost: 0.0002 ips: 4.1784 images/s
[10/08 00:43:49] ppdet.engine INFO: Epoch: [0] [  640/21947] learning_rate: 0.000068 loss_rpn_cls: 0.672138 loss_rpn_reg: 0.014652 loss_bbox_cls: 0.027188 loss_bbox_reg: 0.005363 loss: 0.743213 eta: 1 day, 10:44:30 batch_cost: 0.4787 data_cost: 0.0002 ips: 4.1783 images/s
[10/08 00:43:59] ppdet.engine INFO: Epoch: [0] [  660/21947] learning_rate: 0.000069 loss_rpn_cls: 0.670443 loss_rpn_reg: 0.013540 loss_bbox_cls: 0.027636 loss_bbox_reg: 0.005605 loss: 0.720972 eta: 1 day, 10:44:59 batch_cost: 0.4809 data_cost: 0.0002 ips: 4.1593 images/s
[10/08 00:44:09] ppdet.engine INFO: Epoch: [0] [  680/21947] learning_rate: 0.000071 loss_rpn_cls: 0.668971 loss_rpn_reg: 0.025812 loss_bbox_cls: 0.034054 loss_bbox_reg: 0.008743 loss: 0.744311 eta: 1 day, 10:44:34 batch_cost: 0.4742 data_cost: 0.0002 ips: 4.2173 images/s
[10/08 00:44:18] ppdet.engine INFO: Epoch: [0] [  700/21947] learning_rate: 0.000073 loss_rpn_cls: 0.666791 loss_rpn_reg: 0.015758 loss_bbox_cls: 0.024558 loss_bbox_reg: 0.002665 loss: 0.716927 eta: 1 day, 10:44:53 batch_cost: 0.4800 data_cost: 0.0002 ips: 4.1668 images/s
[10/08 00:44:22] ppdet.engine INFO: Epoch: [0] [  720/21947] learning_rate: 0.000075 loss_rpn_cls: nan loss_rpn_reg: nan loss_bbox_cls: nan loss_bbox_reg: nan loss: nan eta: 1 day, 10:10:39 batch_cost: 0.1956 data_cost: 0.0002 ips: 10.2237 images/s
[10/08 00:44:25] ppdet.engine INFO: Epoch: [0] [  740/21947] learning_rate: 0.000077 loss_rpn_cls: nan loss_rpn_reg: nan loss_bbox_cls: nan loss_bbox_reg: nan loss: nan eta: 1 day, 9:33:45 batch_cost: 0.1574 data_cost: 0.0002 ips: 12.7053 images/s
[10/08 00:44:28] ppdet.engine INFO: Epoch: [0] [  760/21947] learning_rate: 0.000078 loss_rpn_cls: nan loss_rpn_reg: nan loss_bbox_cls: nan loss_bbox_reg: nan loss: nan eta: 1 day, 8:58:32 batch_cost: 0.1552 data_cost: 0.0002 ips: 12.8863 images/s
[10/08 00:44:31] ppdet.engine INFO: Epoch: [0] [  780/21947] learning_rate: 0.000080 loss_rpn_cls: nan loss_rpn_reg: nan loss_bbox_cls: nan loss_bbox_reg: nan loss: nan eta: 1 day, 8:24:24 batch_cost: 0.1489 data_cost: 0.0002 ips: 13.4343 images/s
[10/08 00:44:35] ppdet.engine INFO: Epoch: [0] [  800/21947] learning_rate: 0.000082 loss_rpn_cls: nan loss_rpn_reg: nan loss_bbox_cls: nan loss_bbox_reg: nan loss: nan eta: 1 day, 7:53:59 batch_cost: 0.1673 data_cost: 0.0002 ips: 11.9563 images/s
[10/08 00:44:38] ppdet.engine INFO: Epoch: [0] [  820/21947] learning_rate: 0.000084 loss_rpn_cls: nan loss_rpn_reg: nan loss_bbox_cls: nan loss_bbox_reg: nan loss: nan eta: 1 day, 7:24:07 batch_cost: 0.1585 data_cost: 0.0002 ips: 12.6156 images/s
[10/08 00:44:41] ppdet.engine INFO: Epoch: [0] [  840/21947] learning_rate: 0.000086 loss_rpn_cls: nan loss_rpn_reg: nan loss_bbox_cls: nan loss_bbox_reg: nan loss: nan eta: 1 day, 6:55:06 batch_cost: 0.1530 data_cost: 0.0002 ips: 13.0689 images/s
[10/08 00:44:44] ppdet.engine INFO: Epoch: [0] [  860/21947] learning_rate: 0.000087 loss_rpn_cls: nan loss_rpn_reg: nan loss_bbox_cls: nan loss_bbox_reg: nan loss: nan eta: 1 day, 6:27:26 batch_cost: 0.1533 data_cost: 0.0002 ips: 13.0505 images/s
[10/08 00:44:47] ppdet.engine INFO: Epoch: [0] [  880/21947] learning_rate: 0.000089 loss_rpn_cls: nan loss_rpn_reg: nan loss_bbox_cls: nan loss_bbox_reg: nan loss: nan eta: 1 day, 6:02:04 batch_cost: 0.1637 data_cost: 0.0002 ips: 12.2178 images/s
[10/08 00:44:51] ppdet.engine INFO: Epoch: [0] [  900/21947] learning_rate: 0.000091 loss_rpn_cls: nan loss_rpn_reg: nan loss_bbox_cls: nan loss_bbox_reg: nan loss: nan eta: 1 day, 5:39:07 batch_cost: 0.1769 data_cost: 0.0002 ips: 11.3047 images/s
[10/08 00:44:54] ppdet.engine INFO: Epoch: [0] [  920/21947] learning_rate: 0.000093 loss_rpn_cls: nan loss_rpn_reg: nan loss_bbox_cls: nan loss_bbox_reg: nan loss: nan eta: 1 day, 5:14:47 batch_cost: 0.1520 data_cost: 0.0002 ips: 13.1615 images/s
[10/08 00:44:57] ppdet.engine INFO: Epoch: [0] [  940/21947] learning_rate: 0.000095 loss_rpn_cls: nan loss_rpn_reg: nan loss_bbox_cls: nan loss_bbox_reg: nan loss: nan eta: 1 day, 4:51:33 batch_cost: 0.1527 data_cost: 0.0002 ips: 13.1014 images/s
[10/08 00:45:01] ppdet.engine INFO: Epoch: [0] [  960/21947] learning_rate: 0.000096 loss_rpn_cls: nan loss_rpn_reg: nan loss_bbox_cls: nan loss_bbox_reg: nan loss: nan eta: 1 day, 4:30:18 batch_cost: 0.1640 data_cost: 0.0002 ips: 12.1948 images/s
[10/08 00:45:04] ppdet.engine INFO: Epoch: [0] [  980/21947] learning_rate: 0.000098 loss_rpn_cls: nan loss_rpn_reg: nan loss_bbox_cls: nan loss_bbox_reg: nan loss: nan eta: 1 day, 4:08:50 batch_cost: 0.1517 data_cost: 0.0002 ips: 13.1846 images/s
```
报错信息
```
[10/08 01:03:01] ppdet.engine INFO: Epoch: [0] [   0/5487] learning_rate: 0.000000 loss_rpn_cls: 0.690131 loss_rpn_reg: 0.068640 loss_bbox_cls: 1.132736 loss_bbox_reg: 0.000039 loss: 1.891546 eta: 1 day, 17:38:11 batch_cost: 2.2765 data_cost: 0.0006 ips: 1.7571 images/s
[10/08 01:03:14] ppdet.engine INFO: Epoch: [0] [  20/5487] learning_rate: 0.000000 loss_rpn_cls: 0.688798 loss_rpn_reg: 0.021382 loss_bbox_cls: 1.146138 loss_bbox_reg: 0.000087 loss: 1.857482 eta: 13:00:34 batch_cost: 0.6333 data_cost: 0.0027 ips: 6.3165 images/s
[10/08 01:03:27] ppdet.engine INFO: Epoch: [0] [  40/5487] learning_rate: 0.000000 loss_rpn_cls: 0.688631 loss_rpn_reg: 0.020095 loss_bbox_cls: 1.139235 loss_bbox_reg: 0.000073 loss: 1.851724 eta: 12:15:30 batch_cost: 0.6277 data_cost: 0.0003 ips: 6.3723 images/s
[10/08 01:03:39] ppdet.engine INFO: Epoch: [0] [  60/5487] learning_rate: 0.000001 loss_rpn_cls: 0.688777 loss_rpn_reg: 0.014815 loss_bbox_cls: 1.119644 loss_bbox_reg: 0.000068 loss: 1.831939 eta: 11:56:34 batch_cost: 0.6186 data_cost: 0.0003 ips: 6.4661 images/s
[10/08 01:03:52] ppdet.engine INFO: Epoch: [0] [  80/5487] learning_rate: 0.000001 loss_rpn_cls: 0.688067 loss_rpn_reg: 0.013072 loss_bbox_cls: 1.094978 loss_bbox_reg: 0.000858 loss: 1.803280 eta: 11:51:27 batch_cost: 0.6355 data_cost: 0.0003 ips: 6.2945 images/s
Traceback (most recent call last):
  File ""tools/train.py"", line 138, in <module>
    main()
  File ""tools/train.py"", line 134, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 109, in run
    trainer.train(FLAGS.eval)
  File ""/data/gaojun/nolabel/PaddleDetection/ppdet/engine/trainer.py"", line 358, in train
    outputs = model(data)
  File ""/data/gaojun/.miniconda3/envs/paddle/lib/python3.6/site-packages/paddle/fluid/dygraph/layers.py"", line 902, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File ""/data/gaojun/.miniconda3/envs/paddle/lib/python3.6/site-packages/paddle/fluid/dygraph/parallel.py"", line 578, in forward
    outputs = self._layers(*inputs, **kwargs)
  File ""/data/gaojun/.miniconda3/envs/paddle/lib/python3.6/site-packages/paddle/fluid/dygraph/layers.py"", line 902, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File ""/data/gaojun/nolabel/PaddleDetection/ppdet/modeling/architectures/meta_arch.py"", line 26, in forward
    out = self.get_loss()
  File ""/data/gaojun/nolabel/PaddleDetection/ppdet/modeling/architectures/faster_rcnn.py"", line 95, in get_loss
    rpn_loss, bbox_loss = self._forward()
  File ""/data/gaojun/nolabel/PaddleDetection/ppdet/modeling/architectures/faster_rcnn.py"", line 78, in _forward
    self.inputs)
  File ""/data/gaojun/.miniconda3/envs/paddle/lib/python3.6/site-packages/paddle/fluid/dygraph/layers.py"", line 902, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File ""/data/gaojun/nolabel/PaddleDetection/ppdet/modeling/heads/bbox_head.py"", line 237, in forward
    rois, rois_num, targets = self.bbox_assigner(rois, rois_num, inputs)
  File ""/data/gaojun/nolabel/PaddleDetection/ppdet/modeling/proposal_generator/target_layer.py"", line 152, in __call__
    self.cascade_iou[stage])
  File ""/data/gaojun/nolabel/PaddleDetection/ppdet/modeling/proposal_generator/target.py"", line 194, in generate_proposal_target
    gt_class = paddle.squeeze(gt_classes[i], axis=-1)
  File ""/data/gaojun/.miniconda3/envs/paddle/lib/python3.6/site-packages/paddle/tensor/manipulation.py"", line 613, in squeeze
    return layers.squeeze(x, axis, name)
  File ""/data/gaojun/.miniconda3/envs/paddle/lib/python3.6/site-packages/paddle/fluid/layers/nn.py"", line 6266, in squeeze
    out, _ = core.ops.squeeze2(input, 'axes', axes)
OSError: (External)  Cuda error(700), an illegal memory access was encountered.
  [Advise: Please search for the error code(700) on website( https://docs.nvidia.com/cuda/archive/9.0/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038 ) to get Nvidia's official solution about CUDA Error.] (at /paddle/paddle/fluid/platform/gpu_info.cc:394)
  [operator < squeeze2 > error]


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::memory::allocation::CUDADeviceContextAllocatorPool::~CUDADeviceContextAllocatorPool()
1   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
2   std::_Sp_counted_ptr<paddle::memory::allocation::CUDADeviceContextAllocator*, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::platform::build_nvidia_error_msg[abi:cxx11](cudaError)
4   paddle::platform::proto::cudaerrorDesc::ByteSizeLong() const
5   paddle::platform::proto::AllMessageDesc::ByteSizeLong() const
6   paddle::platform::proto::MessageDesc::ByteSizeLong() const
7   google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(google::protobuf::UnknownFieldSet const&)
8   paddle::framework::SignalHandle(char const*, int)
9   paddle::platform::GetCurrentTraceBackString[abi:cxx11]()

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1633655040 (unix time) try ""date -d @1633655040"" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x2c0) received by PID 51756 (TID 0x7f82cb388740) from PID 704 ***]
```
## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：
paddlepaddle-gpu==2.1.3、PaddleDetection release/2.2
2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号
3. 请提供您使用的操作系统信息
 Linux version 4.15.0-144-generic (buildd@lgw01-amd64-031) (gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)) 
4. 请问您使用的Python版本是？/ Please provide the version of Python you used.
python3.6.12
5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.
cuda11.2，cudnn8.0
"
eval 后的 这个 值 是 判断模型正确 检测出类别的 准确率么？,PaddlePaddle/PaddleDetection,2021-09-25 11:26:10,4,,4239,1007060727,"如图所示 X 轴什么意思，哪位大神 给解释下 还有这个训练结果 里 AP 是 MAP 吧？

![10000结果](https://user-images.githubusercontent.com/88072901/134769769-6a3c6866-c7fe-46d2-9a76-abd5ebe6a3db.PNG)
![bbox-mAP](https://user-images.githubusercontent.com/88072901/134769771-d4ae30e7-f62d-43f7-ad3b-cd4e77f10099.png)


"
c++部署s2anet网络时候 报错NotFoundError: Operator (share_data) is not registered.   [Hint: op_info_ptr should not be null.] (at C:\home\workspace\Paddle_release5\paddle/fluid/framework/op_info.h:150),PaddlePaddle/PaddleDetection,2021-09-24 10:09:07,1,,4233,1006302382,"你好！
我使用的是的框架2.1 cuda是11.2，win10系统，
PaddleDetection工程是2.2版本。
用vs2019编译的，opencv使用4.4.0，
运行其他网络的时候没有问题，但是运行s2net网络的时候   就报错。
报错信息：NotFoundError: Operator (share_data) is not registered.   [Hint: op_info_ptr should not be null.] (at C:\home\workspace\Paddle_release5\paddle/fluid/framework/op_info.h:150)"
【PaddlePaddle Hackathon】PaddleDetection 任务合集,PaddlePaddle/PaddleDetection,2021-09-23 09:52:41,0,PaddlePaddle Hackathon,4226,1005230142,"# 【PaddlePaddle Hackathon】PaddleDetection 任务合集

hi，大家好，非常高兴的告诉大家，首届 PaddlePaddle Hackathon 开始啦。PaddlePaddle Hackathon 是面向全球开发者的深度学习领域编程活动，鼓励开发者了解与参与 PaddlePaddle。本次共有四大方向（PaddlePaddle、Paddle Family、Paddle Friends、Paddle Anything）四大方向，共计100个任务共大家完成。详细信息可以参考 [PaddlePaddle Hackathon 说明](https://www.paddlepaddle.org.cn/contributionguide?docPath=hackathon_cn)。大家是否已经迫不及待了呢~

本 ISSUE 是 Paddle Family 专区 PaddleDetection 方向任务合集。具体任务列表如下：

| 序号 | 难度 | 任务 ISSUE                                                    |
| ---- | ---- | :-----------------------------------------------------: |
| 66 | ⭐️⭐️   | [为 PaddleDetection 动态图新增多尺度测试能力](https://github.com/PaddlePaddle/PaddleDetection/issues/4225)           |                    |          |
| 67 | ⭐️⭐️   | [为 PaddleDetection Faster RCNN添加OHEM采样策略](https://github.com/PaddlePaddle/PaddleDetection/issues/4224)           |                    |          |
| 68 | ⭐️⭐️   | [PaddleDetection 支持DeepSort模型C++部署](https://github.com/PaddlePaddle/PaddleDetection/issues/4223)           |                    |          |
| 69 | ⭐️    | [基于 PaddleDetection 中 Inference C++部署代码，支持SOLOv2](https://github.com/PaddlePaddle/PaddleDetection/issues/4222)           |                    |          |
| 70 | ⭐️⭐️   | [在 PaddleDetection 中增加DALI GPU预处理，支持PP YOLOv1/v2 r50训练](https://github.com/PaddlePaddle/PaddleDetection/issues/4221)           |                    |          |
| 71 | ⭐️⭐️   | [在PaddleDection，基于PaddleSlim对MaskRCNN模型进行压缩，实现GPU推理加速](https://github.com/PaddlePaddle/PaddleDetection/issues/4220)           |                    |          |
| 72 | ⭐️⭐️⭐️  | [为 PaddleDetection 添加PAA模型](https://github.com/PaddlePaddle/PaddleDetection/issues/4219)           |                    |          |

若想要认领本次活动任务，请至 [PaddlePaddle Hackathon Pinned ISSUE](https://github.com/PaddlePaddle/Paddle/issues/35940) 完成任务 ISSUE 认领。

活动官网：[PaddlePaddle Hackathon](https://www.paddlepaddle.org.cn/PaddlePaddleHackathon?fr=paddledetectiong)"
【PaddlePaddle Hackathon】66、为 PaddleDetection 动态图新增多尺度测试能力,PaddlePaddle/PaddleDetection,2021-09-23 08:59:48,0,PaddlePaddle Hackathon,4225,1005179836,"（此 ISSUE 为 PaddlePaddle Hackathon 活动的任务 ISSUE，更多详见[PaddlePaddle Hackathon](https://www.paddlepaddle.org.cn/PaddlePaddleHackathon)）

【任务说明】

- 任务标题：为PaddleDetection动态图新增多尺度测试能力

- 技术标签：深度学习框架，目标检测

- 任务难度：中等

- 详细描述：基于PaddleDetection动态图新增多尺度测试能力，增加文档并提交PR给PaddleDetection。

可以参考PaddleDetection静态图实现 https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.2/static/ppdet/modeling/architectures/faster_rcnn.py#L148

https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.2/static/ppdet/utils/post_process.py#L204

还可以参考mmdetection多尺度测试实现

https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/detectors/base.py#L154

【提交内容】

- 任务 PR到 [PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection)

- 相关技术文档

- 任务单测文件

【技术要求】

- 了解飞桨框架使用，目标检测算法，PaddleDetection框架使用"
【PaddlePaddle Hackathon】67、为 PaddleDetection Faster RCNN添加OHEM采样策略,PaddlePaddle/PaddleDetection,2021-09-23 08:59:01,0,PaddlePaddle Hackathon,4224,1005179154,"（此 ISSUE 为 PaddlePaddle Hackathon 活动的任务 ISSUE，更多详见[PaddlePaddle Hackathon](https://www.paddlepaddle.org.cn/PaddlePaddleHackathon)）

【任务说明】

- 任务标题：为PaddleDetection Faster RCNN添加OHEM采样策略

- 技术标签：深度学习框架，目标检测

- 任务难度：中等

- 详细描述：基于PaddleDetection Faster RCNN实现OHEM采样策略，增加文档并提交PR给PaddleDetection。

可以参考mmdetection实现 https://github.com/open-mmlab/mmdetection/blob/master/mmdet/core/bbox/samplers/ohem_sampler.py

【提交内容】

- 任务 PR到 [PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection)

- 相关技术文档

- 任务单测文件

【技术要求】

- 了解飞桨框架使用，目标检测算法"
【PaddlePaddle Hackathon】68、PaddleDetection 支持DeepSort模型C++部署,PaddlePaddle/PaddleDetection,2021-09-23 08:58:27,0,PaddlePaddle Hackathon,4223,1005178652,"（此 ISSUE 为 PaddlePaddle Hackathon 活动的任务 ISSUE，更多详见[PaddlePaddle Hackathon](https://www.paddlepaddle.org.cn/PaddlePaddleHackathon)）

【任务说明】

- 任务标题：PaddleDetection支持DeepSort模型C++部署

- 技术标签：深度学习框架，多目标跟踪

- 任务难度：中等

- 详细描述：基于PaddleDetection 实现Deepsort模型C++部署，增加文档并提交PR给PaddleDetection。

可以参考PaddeDetection python部署方案https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.2/deploy/python/mot_sde_infer.py

和开源deepsort C++部署实现

【提交内容】

- 任务 PR到 [PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection)

- 相关技术文档

【技术要求】

- 了解飞桨框架使用，多目标跟踪算法"
【PaddlePaddle Hackathon】69、基于 PaddleDetection 中 Inference C++部署代码，支持SOLOv2,PaddlePaddle/PaddleDetection,2021-09-23 08:58:03,0,PaddlePaddle Hackathon,4222,1005178275,"（此 ISSUE 为 PaddlePaddle Hackathon 活动的任务 ISSUE，更多详见[PaddlePaddle Hackathon](https://www.paddlepaddle.org.cn/PaddlePaddleHackathon)）

【任务说明】

- 任务标题：基于PaddleDetection中Inference C++部署代码，支持SOLOv2

- 技术标签：深度学习框架，实例分割

- 任务难度：简单

- 详细描述：基于PaddleDetection 实现SOLOv2模型C++部署，增加文档并提交PR给PaddleDetection。

可以参考PaddeDetection python部署方案 https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.2/deploy/python/infer.py#L181

【提交内容】

- 任务 PR到 [PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection)

- 相关技术文档

【技术要求】

- 了解飞桨框架使用，实例分割算法"
【PaddlePaddle Hackathon】70、在 PaddleDetection 中增加DALI GPU预处理，支持PP YOLOv1/v2 r50训练,PaddlePaddle/PaddleDetection,2021-09-23 08:57:28,0,PaddlePaddle Hackathon,4221,1005177772,"（此 ISSUE 为 PaddlePaddle Hackathon 活动的任务 ISSUE，更多详见[PaddlePaddle Hackathon](https://www.paddlepaddle.org.cn/PaddlePaddleHackathon)）

【任务说明】

- 任务标题：在PaddleDetection中增加DALI GPU预处理，支持PP YOLOv1/v2 r50训练

- 技术标签：深度学习框架，GPU预处理，目标检测

- 任务难度：中等

- 详细描述：在PaddleDection中增加DALI GPU预处理，支持PP YOLOv1/v2 r50训练，提升训练速度，增加文档并提交PR给PaddleDetection。数据预处理中，可以不增加mixup

  dali参考文档 https://docs.nvidia.com/deeplearning/dali/user-guide/docs/index.html

  飞桨套件已有集成方式 https://github.com/PaddlePaddle/PaddleClas/blob/release/2.2/ppcls/data/dataloader/dali.py

【提交内容】

- 任务 PR到 [PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection)

- 相关技术文档

【技术要求】

- 了解飞桨框架使用，GPU预处理，目标检测算法"
【PaddlePaddle Hackathon】71、在PaddleDection，基于PaddleSlim对MaskRCNN模型进行压缩，实现GPU推理加速,PaddlePaddle/PaddleDetection,2021-09-23 08:56:57,0,PaddlePaddle Hackathon,4220,1005177296,"（此 ISSUE 为 PaddlePaddle Hackathon 活动的任务 ISSUE，更多详见[PaddlePaddle Hackathon](https://www.paddlepaddle.org.cn/PaddlePaddleHackathon)）

【任务说明】

- 任务标题：在PaddleDection中基于PaddleSlim对MaskRCNN模型进行压缩，实现GPU推理加速

- 技术标签：深度学习框架，实例分割，模型压缩

- 任务难度：中等

- 详细描述：在PaddleDection中基于PaddleSlim对MaskRCNN模型进行压缩，实现GPU推理加速，不限定模型压缩方法（剪裁/量化），增加文档并提交PR给PaddleDetection。

  基于PaddleDetection进行模型压缩教程 https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.2/configs/slim

【提交内容】

- 任务 PR到 [PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection)

- 相关技术文档

【技术要求】

- 了解飞桨框架使用，实例分割算法，模型压缩"
【PaddlePaddle Hackathon】72、为 PaddleDetection 添加PAA模型,PaddlePaddle/PaddleDetection,2021-09-23 08:55:16,0,PaddlePaddle Hackathon,4219,1005175758,"（此 ISSUE 为 PaddlePaddle Hackathon 活动的任务 ISSUE，更多详见[PaddlePaddle Hackathon](https://www.paddlepaddle.org.cn/PaddlePaddleHackathon)）

【任务说明】

- 任务标题：为PaddleDetection添加PAA模型

- 技术标签：深度学习框架，目标检测

- 任务难度：困难

- 详细描述：为PaddleDetection添加PAA模型，增加文档并提交PR给PaddleDetection。

  可以参考mmdetection实现 https://github.com/open-mmlab/mmdetection/blob/master/configs/paa/paa_r50_fpn_1x_coco.py

【提交内容】

- 任务 PR到 [PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection)

- 相关技术文档

【技术要求】

- 了解飞桨框架使用，目标检测算法"
多卡训练地址拒绝访问,PaddlePaddle/PaddleDetection,2021-09-16 01:57:15,2,,4192,997673492,"求教大佬们，我进行多卡训练就报错，显示拒绝访问，但是我自己调用哪个端口是没问题的
python -m paddle.distributed.launch --gpus 0,1,2,3,4,5,6,7 tools/train.py -c configs/ppyolo/flagsm.yml

INFO 2021-09-15 21:42:08,014 launch_utils.py:618] Change selected_gpus into reletive values. --ips:0,1,2,3,4,5,6,7 will change into relative_ips:[0, 1, 2, 3, 4, 5, 6, 7] according to your CUDA_VISIBLE_DEVICES:['0', '1', '2', '3', '4', '5', '6', '7']
INFO 2021-09-15 21:42:08,018 launch_utils.py:507] Local start 8 processes. First process distributed environment info (Only For Debug):
    +=======================================================================================+
    |                        Distributed Envs                      Value                    |
    +---------------------------------------------------------------------------------------+
    |                       PADDLE_TRAINER_ID                        0                      |
    |                 PADDLE_CURRENT_ENDPOINT                 127.0.0.1:40355               |
    |                     PADDLE_TRAINERS_NUM                        8                      |
    |                PADDLE_TRAINER_ENDPOINTS  ... 0.1:27439,127.0.0.1:28763,127.0.0.1:42940|
    |                     PADDLE_RANK_IN_NODE                        0                      |
    |                 PADDLE_LOCAL_DEVICE_IDS                        0                      |
    |                 PADDLE_WORLD_DEVICE_IDS                 0,1,2,3,4,5,6,7               |
    |                     FLAGS_selected_gpus                        0                      |
    |             FLAGS_selected_accelerators                        0                      |
    +=======================================================================================+

INFO 2021-09-15 21:42:08,018 launch_utils.py:512] details abouts PADDLE_TRAINER_ENDPOINTS can be found in log/endpoints.log, and detail running logs maybe found in log/workerlog.0
-----------  Configuration Arguments -----------
gpus: 0,1,2,3,4,5,6,7
heter_worker_num: None
heter_workers:
http_port: None
ips: 127.0.0.1
log_dir: log
nproc_per_node: None
run_mode: None
server_num: None
servers:
training_script: tools/train.py
training_script_args: ['-c', 'configs/ppyolo/flagsm.yml']
worker_num: None
workers:
------------------------------------------------
launch train in GPU mode!
launch proc_id:37673 idx:0
launch proc_id:37691 idx:1
launch proc_id:37703 idx:2
launch proc_id:37719 idx:3
launch proc_id:37734 idx:4
launch proc_id:37755 idx:5
launch proc_id:37778 idx:6
launch proc_id:37812 idx:7
W0915 21:42:10.773465 29655 gen_comm_id_helper.cc:120] connect addr=127.0.0.1:44150 failed 24 times with reason: Connection refused retry after 3 seconds
W0915 21:42:13.774241 29655 gen_comm_id_helper.cc:120] connect addr=127.0.0.1:44150 failed 25 times with reason: Connection refused retry after 3 seconds
W0915 21:42:16.774919 29655 gen_comm_id_helper.cc:120] connect addr=127.0.0.1:44150 failed 26 times with reason: Connection refused retry after 3 seconds
W0915 21:42:19.775590 29655 gen_comm_id_helper.cc:120] connect addr=127.0.0.1:44150 failed 27 times with reason: Connection refused retry after 3 seconds
/root/anaconda3/lib/python3.8/site-packages/paddle/tensor/creation.py:125: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if data.dtype == np.object:
W0915 21:42:22.776187 29655 gen_comm_id_helper.cc:120] connect addr=127.0.0.1:44150 failed 28 times with reason: Connection refused retry after 3 seconds
W0915 21:42:25.776698 29655 gen_comm_id_helper.cc:120] connect addr=127.0.0.1:44150 failed 29 times with reason: Connection refused retry after 3 seconds
W0915 21:42:28.777158 29655 gen_comm_id_helper.cc:120] connect addr=127.0.0.1:44150 failed 30 times with reason: Connection refused retry after 3 seconds
W0915 21:42:31.777606 29655 gen_comm_id_helper.cc:120] connect addr=127.0.0.1:44150 failed 31 times with reason: Connection refused retry after 3 seconds
W0915 21:42:32.935339 37673 gen_comm_id_helper.cc:120] connect addr=127.0.0.1:56443 failed 1 times with reason: Connection refused retry after 0.5 seconds
W0915 21:42:33.436247 37673 gen_comm_id_helper.cc:120] connect addr=127.0.0.1:32653 failed 1 times with reason: Connection refused retry after 0.5 seconds
W0915 21:42:33.936760 37673 gen_comm_id_helper.cc:120] connect addr=127.0.0.1:32653 failed 2 times with reason: Connection refused retry after 1 seconds
W0915 21:42:34.778077 29655 gen_comm_id_helper.cc:120] connect addr=127.0.0.1:44150 failed 32 times with reason: Connection refused retry after 3 seconds
I0915 21:42:34.938714 37673 nccl_context.cc:74] init nccl context nranks: 8 local rank: 0 gpu id: 0 ring id: 0
W0915 21:42:37.778661 29655 gen_comm_id_helper.cc:120] connect addr=127.0.0.1:44150 failed 33 times with reason: Connection refused retry after 3 seconds
W0915 21:42:40.779544 29655 gen_comm_id_helper.cc:120] connect addr=127.0.0.1:44150 failed 34 times with reason: Connection refused retry after 3 seconds
W0915 21:42:42.219158 37673 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 3.7, Driver API Version: 10.2, Runtime API Version: 10.2
W0915 21:42:42.268736 37673 device_context.cc:422] device: 0, cuDNN Version: 7.6.
W0915 21:42:43.780148 29655 gen_comm_id_helper.cc:120] connect addr=127.0.0.1:44150 failed 35 times with reason: Connection refused retry after 3 seconds

"
FairMot模型导出后，能否对单张图片进行预测？,PaddlePaddle/PaddleDetection,2021-09-15 08:53:28,11,feature request#deploy,4189,996825058,"文档中提到：“注意: 跟踪模型是对视频进行预测，不支持单张图的预测”
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.2/configs/mot/README_cn.md

目前模型导出后，能否支持对单张图片的预测？
能否保存预测结果为文本格式？"
solov2_enhance只支持GPU训练以及导出吗？,PaddlePaddle/PaddleDetection,2021-09-15 02:26:22,5,bug#question,4182,996592652,"用CPU导出solov2 enhance模型，提示 **ValueError: Operator ""sync_batch_norm"" has not been registered.**"
动态图模型推理占用显存问题,PaddlePaddle/PaddleDetection,2021-09-10 07:56:09,7,,4163,992984441,相同的CascadeRCNN模型，动态图版本模型预测推理占用显存约为2.7G，静态图版本模型约为1.72G。请问动态图版本占用显存高的原因是什么？有没有什么方法能够降低模型占用的显存？下个版本会优化这个问题嘛？
yolov3训练效果很差,PaddlePaddle/PaddleDetection,2021-09-07 02:25:07,9,training,4131,989534433,"![image](https://user-images.githubusercontent.com/13059352/132274585-f68c5d55-6b45-4438-ad7a-310240fe968f.png)
voc数据训练的yolov3 


![image](https://user-images.githubusercontent.com/13059352/132274628-2c64c8bf-63ad-4191-8828-f96b0c4a829b.png)
自己数据训练的yolov3

好像loss没有一个是正常的，loss都在10以上就无法下降了"
不加载预训练参数，训练模型，eval时报错,PaddlePaddle/PaddleDetection,2021-09-03 02:45:49,2,,4112,987342189,"不加载预训练参数，训练模型，eval时报错：

loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
2021-09-03 10:42:48,428-WARNING: The number of valid mask detected is zero.
             Please use reasonable model and check input data.
Traceback (most recent call last):
  File ""tools/train_multi_machine.py"", line 384, in <module>
    main()
  File ""tools/train_multi_machine.py"", line 312, in main
    if box_ap_stats[0] > best_box_ap_list[0]:
TypeError: 'NoneType' object is not subscriptable
terminate called without an active exception

请问应该怎么解决？"
solov2 paddle serving 部署,PaddlePaddle/PaddleDetection,2021-08-26 02:27:32,13,question#deploy,4066,979766300,"您好：

在用solov2 部署在 paddle serving 时出现了错误，请帮忙看下。

### test_client.py 
```python
import numpy as np
from paddle_serving_client import Client
from paddle_serving_app.reader import *
import cv2

preprocess = Sequential([
    File2Image(), 
    Resize((512, 852), interpolation=cv2.INTER_LINEAR), 
    Div(255),
    Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]),
    Transpose((2, 0, 1))
])

postprocess = SegPostprocess(class_num=5)


img_path = './imgs/000363.jpg'
im = preprocess(img_path)

client = Client()
client.load_client_config(""client_config/det_serving_client_conf.prototxt"")
client.connect(['ip:port'])

fetch_map = client.predict(
    feed={
        ""image"": np.array(im),
        ""im_shape"": np.array(im.shape[1:]),
        ""scale_factor"":np.array([1.0, 1.0]),
    },
    fetch=[""save_infer_model/scale_0.tmp_1"",
            ""save_infer_model/scale_1.tmp_1"",
            ""save_infer_model/scale_2.tmp_1"",
            ""save_infer_model/scale_3.tmp_1""
            ],
    batch=False
)
fetch_map[""filename""] = img_path
postprocess(fetch_map)
```

### 报错信息：
```bash
terminate called after throwing an instance of 'paddle::platform::EnforceNotMet'
  what():  

  Compile Traceback (most recent call last):
    File ""tools/export_model.py"", line 116, in <module>
      main()
    File ""tools/export_model.py"", line 112, in main
      run(FLAGS, cfg)
    File ""tools/export_model.py"", line 78, in run
      trainer.export(FLAGS.output_dir)
    File ""/root/miniconda3/lib/python3.8/site-packages/paddledet-2.2.0-py3.8.egg/ppdet/engine/trainer.py"", line 576, in export
      input_spec, static_model.forward.main_program,
    File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 542, in main_program
      concrete_program = self.concrete_program
    File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 458, in concrete_program
      return self.concrete_program_specify_input_spec(input_spec=None)
    File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 495, in concrete_program_specify_input_spec
      concrete_program, _ = self.get_concrete_program(
    File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 406, in get_concrete_program
      concrete_program, partial_program_layer = self._program_cache[cache_key]
    File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 723, in __getitem__
      self._caches[item] = self._build_once(item)
    File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 709, in _build_once
      concrete_program = ConcreteProgram.from_func_spec(
    File ""<decorator-gen-97>"", line 2, in from_func_spec
      
    File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/wrapped_decorator.py"", line 25, in __impl__
      return wrapped_func(*args, **kwargs)
    File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/base.py"", line 40, in __impl__
      return func(*args, **kwargs)
    File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 662, in from_func_spec
      outputs = static_func(*inputs)
    File ""/root/miniconda3/lib/python3.8/site-packages/paddledet-2.2.0-py3.8.egg/ppdet/modeling/architectures/meta_arch.py"", line 23, in forward
      self.model_arch()
    File ""/root/miniconda3/lib/python3.8/site-packages/paddledet-2.2.0-py3.8.egg/ppdet/modeling/architectures/solov2.py"", line 67, in model_arch
      body_feats = self.backbone(self.inputs)
    File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/layers.py"", line 907, in __call__
      outputs = self.forward(*inputs, **kwargs)
    File ""/root/miniconda3/lib/python3.8/site-packages/paddledet-2.2.0-py3.8.egg/ppdet/modeling/backbones/resnet.py"", line 582, in forward
      conv1 = self.conv1(x)
    File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/layers.py"", line 907, in __call__
      outputs = self.forward(*inputs, **kwargs)
    File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/container.py"", line 98, in forward
      input = layer(input)
    File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/layers.py"", line 907, in __call__
      outputs = self.forward(*inputs, **kwargs)
    File ""/tmp/tmpo4hxct88.py"", line 32, in forward
      out = paddle.jit.dy2static.convert_ifelse(self.norm_type in ['bn',
    File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 210, in convert_ifelse
      return _run_py_ifelse(pred, true_fn, false_fn, true_args, false_args)
    File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 235, in _run_py_ifelse
      return true_fn(*true_args) if pred else false_fn(*false_args)
    File ""/root/miniconda3/lib/python3.8/site-packages/paddledet-2.2.0-py3.8.egg/ppdet/modeling/backbones/resnet.py"", line 133, in forward
      out = self.norm(out)
    File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/layers.py"", line 907, in __call__
      outputs = self.forward(*inputs, **kwargs)
    File ""/root/miniconda3/lib/python3.8/site-packages/paddle/nn/layer/norm.py"", line 1126, in forward
      self._helper.append_op(
    File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/layer_object_helper.py"", line 47, in append_op
      return self.main_program.current_block().append_op(
    File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/framework.py"", line 3108, in append_op
      op = Operator(
    File ""/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/framework.py"", line 2161, in __init__
      for frame in traceback.extract_stack():

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------

----------------------
Error Message Summary:
----------------------
NotFoundError: Operator (sync_batch_norm) does not have kernel for data_type[float]:data_layout[ANY_LAYOUT]:place[CPUPlace]:library_type[PLAIN].
  [Hint: Expected kernel_iter != kernels.end(), but received kernel_iter == kernels.end().] (at /paddle/paddle/fluid/framework/operator.cc:1276)
  [operator < sync_batch_norm > error]
```

环境如下：
paddle-serving-app        0.6.1
paddle-serving-client     0.6.1
paddle-serving-server-gpu 0.6.1.post102
paddlepaddle-gpu          2.1.2
"
使用2.1 2.2训练一个 epoch就自动中断，弹应用程序错误,PaddlePaddle/PaddleDetection,2021-08-25 05:03:50,2,bug,4057,978719253,"在win10 conda环境下 paddle2.1.2 cuda10.02 gpu 
在ppdet2.1和ppdet2.2使用同样的自定义coco数据集分别测试了ppyolov2和maskrcnn，都在一个epoch时就自动中断了，使用ppyolov2时没有弹python应用错误
![图片](https://user-images.githubusercontent.com/82444943/130727742-bfa43b9b-e685-47a4-b711-605a4b8f27d0.png)
![图片](https://user-images.githubusercontent.com/82444943/130728878-7b23d345-becf-42a1-9af0-ee1e595b7703.png)
在ppdet2.0下用同样的数据集使用ppyolov2训练预测都完成了，精度接近0.8，但是使用maskrcnn时报如下错误
![图片](https://user-images.githubusercontent.com/82444943/130729441-463b0238-ace7-4325-a08e-e2b1d384532c.png)
![图片](https://user-images.githubusercontent.com/82444943/130729402-489fa52e-a85c-4232-9541-77af1d1f63af.png)

以上ppdetection都是独立的conda环境下测试的



"
export_model.py导出模型库中的solov2_r50_fpn_3x_coco.pdparams失败,PaddlePaddle/PaddleDetection,2021-08-22 00:26:44,4,,4034,976250013,"****************************************
Paddle version: 2.1.2
Paddle With CUDA: True

OS: Deepin 20.2
Python version: 3.7.3

CUDA version: 9.2.148
cuDNN version: None.None.None
Nvidia driver version: 460.39
Paddle Detection version:2.1
****************************************
从https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.2/configs/solov2/ 页面
下载SOLOv2|R50-FPN|True|3x(https://paddledet.bj.bcebos.com/models/solov2_r50_fpn_3x_coco.pdparams)
使用脚本:

```
tools/export_model.py -c configs/solov2/solov2_r50_fpn_3x_coco.yml --output_dir=./inference_model -o     weights=output/solov2/solov2_r50_fpn_3x_coco.pdparams

```
错误:

```
ppdet/modeling/architectures/solov2.py"", line 105, in get_pred (* user code *)
        test = self.inputs['image'].reshape(shape=[batch,-1, 100,100])

AssertionError: Only one dimension value of 'shape' in reshape can be -1. But received shape[1] is also -1.

```"
运行ppyolov2_mbv3_small报错,PaddlePaddle/PaddleDetection,2021-08-19 06:36:21,2,,4018,974323270,"我运行github.com/wangxinxin08/PaddleDetection/里面的ppyolov2_mbv3_large可以运行，我将ppyolov2_mbv3_large改为small版本报错，改的代码如下
![截屏2021-08-19 下午2 35 27](https://user-images.githubusercontent.com/87430649/130019734-f91c6d2f-5297-4517-ac0b-8a1b14623dfb.png)
报错如下
<img width=""1076"" alt=""截屏2021-08-19 下午2 36 00"" src=""https://user-images.githubusercontent.com/87430649/130019792-be908a04-e56b-4d37-adaa-b807bc791af7.png"">
"
关于Sa2net的问题,PaddlePaddle/PaddleDetection,2021-08-16 03:02:31,7,bug#feature request#algorithm,3982,971318689,"在我使用sa2net训练一个obb目标检测数据集时发现，infer时同一个目标会有多于一个的预测框，并且置信度不同。不知道这种情况时是否在论文里就是如此？我理解或许应该取置信度最高的一个预测框
![Uploading 4.png…]()
[4.txt](https://github.com/PaddlePaddle/PaddleDetection/files/6989514/4.txt)
"
自定义数据增强算子，训练时报错,PaddlePaddle/PaddleDetection,2021-08-12 09:33:58,2,,3966,968450783,"我在paddledetection/ppdet/data/transform/operators.py新增加了一个自定义的数据增强算子，并在相应的reader.yml文件中加入了该算子名称，但是训练的时候直接报如下的错误，所有修改的地方如下图所示，请问是什么问题呢？
![image](https://user-images.githubusercontent.com/46999456/129172864-a7e13f42-cfc0-4b2c-b9d6-30d92566ff2e.png)
![image](https://user-images.githubusercontent.com/46999456/129174344-9ddbbee1-ede1-4232-b5ec-9a666185c5f7.png)
![image](https://user-images.githubusercontent.com/46999456/129174427-a453785b-edf2-4104-88c5-240eb5506569.png)
"
coco格式的数据集改类别名字,PaddlePaddle/PaddleDetection,2021-08-12 03:23:00,4,,3957,967936939,"自己coco格式的数据集改类别名字在哪个文件改呢？ 我看voc会有个label_list.txt，我coco格式的训练完发现，类别没改，请问在哪修改？
"
ppyolov2评估时间长,PaddlePaddle/PaddleDetection,2021-08-12 02:02:29,2,,3955,967834795,"
![Snipaste_2021-08-12_10-00-34](https://user-images.githubusercontent.com/79153269/129127074-3c8ab176-2c1f-41b4-9985-dd6d5aed4949.jpg)
已将数据集改成voc数据集，用其他模型都很快，唯独ppyolov2非常慢，这正常吗？"
模型对象作为参数传入到回调函数里无法调用GPU？,PaddlePaddle/PaddleDetection,2021-08-10 00:30:51,4,help wanted,3925,964492336,业务需求中存在回调函数，数据源来源回调函数，因此需要把模型初始化放在主函数，推理部分通过传参进入回调函数进行推理，这样做代码可以跑，但是无法调用GPU，把推理函数放到主函数内推理就可以调用GPU，因此不知道如何解决在回调函数内进行推理无法调用GPU的问题如何解决？
mask rcnn 训练自己数据集出现的错误,PaddlePaddle/PaddleDetection,2021-08-08 08:19:27,9,,3915,963380831,"用mask rcnn 训练自己的数据集出现加载模型出错。
我的环境 win10 cuda11.2 paddlepaddle-gpu=2.1.1.post112 RTX3060
是正常安装和使用paddleDetection
 
![image](https://user-images.githubusercontent.com/50097546/128625719-f1819b91-7a38-4e6b-802a-f17f83eacdeb.png)
请问这是什么问题

我主要是去修改了mask rcnn 的数据集配置文件。才用的小熊数据集
![image](https://user-images.githubusercontent.com/50097546/128625768-89570e28-50aa-44b1-8aa1-29173a02f580.png)
![image](https://user-images.githubusercontent.com/50097546/128625798-c85bc44c-71a3-4315-9482-25b521b32627.png)

求求解惑"
s2anet问题：,PaddlePaddle/PaddleDetection,2021-08-03 03:13:11,6,help wanted#question,3870,958677238,"用release2.2中s2anet_1x_spine.yml训练spine_coco数据集，配置文件如下：
_BASE_: [
  '../datasets/spine_coco.yml',
  '../runtime.yml',
  '_base_/s2anet_optimizer_1x.yml',
  '_base_/s2anet.yml',
  '_base_/s2anet_reader.yml',
]

weights: output/s2anet_1x_spine/model_final

# for 4 card
LearningRate:
  base_lr: 0.00125

S2ANetHead:
  anchor_strides: [8, 16, 32, 64, 128]
  anchor_scales: [4]
  anchor_ratios: [4.0]
  anchor_assign: RBoxAssigner
  stacked_convs: 2
  feat_in: 256
  feat_out: 256
  num_classes: 9
  align_conv_type: 'AlignConv'  # AlignConv Conv
  align_conv_size: 3
  use_sigmoid_cls: True
  reg_loss_weight: [1.0, 1.0, 1.0, 1.0, 1.05]
  cls_loss_weight: [1.05, 1.0]
  reg_loss_type: 'l1'

训练12epoch后：mAP(0.50, 11point) = 9.45%

[08/03 10:48:35] ppdet.engine INFO: Epoch: [11] [200/230] learning_rate: 0.000013 fam_cls_loss: 0.093969 fam_reg_loss: 1.341817 odm_cls_loss: 0.100388 odm_reg_loss: 0.080870 loss: 1.627664 eta: 0:00:15 batch_cost: 0.5041 data_cost: 0.0002 ips: 1.9838 images/s
[08/03 10:48:53] ppdet.utils.checkpoint INFO: Save checkpoint: output/s2anet_1x_spine
[08/03 10:48:54] ppdet.engine INFO: Eval iter: 0
[08/03 10:49:36] ppdet.metrics.metrics INFO: The bbox result is saved to bbox.json.
[08/03 10:49:36] ppdet.metrics.metrics INFO: Accumulating evaluatation results...
[08/03 10:49:36] ppdet.metrics.metrics INFO: mAP(0.50, 11point) = 9.45%
[08/03 10:49:36] ppdet.engine INFO: Total sample number: 57, averge FPS: 1.3343229141074622
[08/03 10:49:36] ppdet.engine INFO: Best test bbox ap is 0.094.

训练自己的数据集（通过roLabelImg 来标注旋转矩形框，并转成coco），训练24epoch后：mAP(0.50, 11point) = 3.1%。
问题：1、首先两个数据集没有问题，为何mAP很低？
2、 其中anchor_scales: [4]和  anchor_ratios: [1.0] 与anchor生成相关，如果自己数据集anchor分布很分散可能多个尺寸更好回归，当anchor_ratios设置多种比例时就会报错（如下），只能固定一种吗？该如何调整以提高mAP？
  Traceback (most recent call last):
  File ""tools/train.py"", line 139, in <module>
    main()
  File ""tools/train.py"", line 135, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 110, in run
    trainer.train(FLAGS.eval)
  File ""/home/aistudio/PaddleDetection-release-2.2/ppdet/engine/trainer.py"", line 357, in train
    outputs = model(data)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 898, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File ""/home/aistudio/PaddleDetection-release-2.2/ppdet/modeling/architectures/meta_arch.py"", line 26, in forward
    out = self.get_loss()
  File ""/home/aistudio/PaddleDetection-release-2.2/ppdet/modeling/architectures/s2anet.py"", line 97, in get_loss
    loss = self._forward()
  File ""/home/aistudio/PaddleDetection-release-2.2/ppdet/modeling/architectures/s2anet.py"", line 73, in _forward
    self.s2anet_head(body_feats)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 898, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File ""/home/aistudio/PaddleDetection-release-2.2/ppdet/modeling/heads/s2anet_head.py"", line 441, in forward
    featmap_size, self.anchor_strides[feat_idx])
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py"", line 898, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File ""/home/aistudio/PaddleDetection-release-2.2/ppdet/modeling/heads/s2anet_head.py"", line 90, in forward
    all_anchors = self.base_anchors[:, :] + shifts[:, :]
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py"", line 250, in __impl__
    return math_op(self, other_var, 'axis', axis)
ValueError: (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 4] and the shape of Y = [13824, 4]. Received [3] in X is not equal to [13824] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || x_dims_array[i] <= 1 || y_dims_array[i] <= 1 == true, but received x_dims_array[i] == y_dims_array[i] || x_dims_array[i] <= 1 || y_dims_array[i] <= 1:0 != true:1.] (at /paddle/paddle/fluid/operators/elementwise/elementwise_op_function.h:169)
  [operator < elementwise_add > error]"
动态图ppyolo模型工业部署时报错,PaddlePaddle/PaddleDetection,2021-08-03 02:52:07,4,,3869,958667361,"![image](https://user-images.githubusercontent.com/87430649/127949996-b79c2966-fe52-4dfb-b25c-e00005b617e9.png)
![e1310c4b8ce67a05d6a143244d533a69](https://user-images.githubusercontent.com/87430649/127950044-ae9ba973-6a64-48b2-81f7-2e71daa3e915.jpg)
"
tools/x2coco.py voc转coco数据格式，如果xml <obj>标签中检测目标名带空格，会报错，有待解决,PaddlePaddle/PaddleDetection,2021-07-29 12:31:25,2,feature request,3827,955794410,"**PaddleDetection team appreciate any suggestion or problem you delivered~**

## Checklist:

1. 查找历史相关issue寻求解答/I have searched related issues but cannot get the expected help.
2. 翻阅[FAQ](https://paddledetection.readthedocs.io/FAQ.html) /I have read the [FAQ documentation](https://paddledetection.readthedocs.io/FAQ.html) but cannot get the expected help.
3. 确认bug是否在新版本里还未修复/The bug has not been fixed in the latest version.

## 描述问题/Describe the bug
A clear and concise description of what the bug is.

## 复现/Reproduction
当我把xml，要检测的目标对象的名字中间的空格去掉，此错误消失，加上空格，错误出现

1. 您使用的命令是？/What command or script did you run?
2. python tools/x2coco.py --dataset_type voc --voc_anno_dir ./dataset/voc/VOCdevkit/VOC2007/AnnotationsTest/ --voc_anno_list ./dataset/voc/VOCdevkit/VOC2007/ImageSets/Main/trainvalTest.txt --voc_label_list ./dataset/voc/label_list.txt --voc_out_name voc_train.json

```none
请填写命令/A placeholder for the command.
```
2. 您是否更改过代码或配置文件？您是否理解您所更改的内容？还请您提供所更改的部分代码。/Did you make any modifications on the code or config? Did you understand what you have modified? Please provide the codes that you modified.
3. 修改过tools/x2coco.py 中间加入打印语句

3. 您使用的数据集是？/What dataset did you use?
4. 自定义数据集

4. 请提供您出现的报错信息及相关log。/Please provide the error messages or relevant log information.
5. Traceback (most recent call last):
  File ""tools/x2coco.py"", line 450, in <module>
    main()
  File ""tools/x2coco.py"", line 346, in main
    output_file=args.voc_out_name)
  File ""tools/x2coco.py"", line 270, in voc_xmls_to_cocojson
    ann = voc_get_coco_annotation(obj=obj, label2id=label2id)
  File ""tools/x2coco.py"", line 231, in voc_get_coco_annotation
    assert label in label2id, ""label is not in label2id.""
AssertionError: label is not in label2id.

## 环境/Environment
1. 请提供您使用的Paddle和PaddleDetection的版本号/Please provide the version of Paddle and PaddleDetection you use：
2. paddle 2.0

2. 如您在使用PaddleDetection的同时还在使用其他产品，如PaddleServing、PaddleInference等，请您提供其版本号/ Please provide the version of any other related tools/products used, such as the version of PaddleServing and etc：

3. 请提供您使用的操作系统信息，如Linux/Windows/MacOS /Please provide the OS information, e.g., Linux：
4. aistudio平台，顺带吐槽一下，paddlex 可视化界面只能用18.0.4版本，可是aistudio是16.0.4如果两个匹配起来，不是更好吗

4. 请问您使用的Python版本是？/ Please provide the version of Python you used.
5. 3.7

5. 请问您使用的CUDA/cuDNN的版本号是？/ Please provide the version of CUDA/cuDNN you used.
6. 10.1 7.6


如果您的issue是关于安装或环境，您可以先查询[安装文档](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL_cn.md)尝试解决~

If your issue looks like an installation issue / environment issue,
please first try to solve it yourself with the instructions in
https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/docs/tutorials/INSTALL.md
"
paddle2.0 static 量化模型支持 多batch预测吗？该如何测试。,PaddlePaddle/PaddleDetection,2021-07-29 11:57:49,3,deploy,3826,955764219,"1、目前只支持batch=1 预测，paddle2.0 static 量化模型支持 多batch预测吗？该如何测试
2、量化模型测试，预处理时间过长
<preprocess.Resize object at 0x7f91c31b2190>
preInference: 0.4837512969970703 ms per batch image
<preprocess.Normalize object at 0x7f91c31b20d0>
preInference: 6.020545959472656 ms per batch image（如何降低时间）
<preprocess.Permute object at 0x7f91c31b2290>
preInference: 0.2448558807373047 ms per batch image
3、预处理时间  0.4837512969970703 +  6.020545959472656 + 0.2448558807373047  时间< 10.703325271606445 
preInference: 10.703325271606445 ms per batch image"
 v2.1版本运行python ppdet/modeling/tests/test_architectures.py后显示w0279后自动退出,PaddlePaddle/PaddleDetection,2021-07-29 11:33:43,2,install,3825,955746710,"E:\study\target detection\PaddleDetection-release-2.1>python ppdet/modeling/tests/test_architectures.py
C:\Users\ibdong\AppData\Local\Programs\Python\Python36\lib\site-packages\numpy\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:
C:\Users\ibdong\AppData\Local\Programs\Python\Python36\lib\site-packages\numpy\.libs\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll
C:\Users\ibdong\AppData\Local\Programs\Python\Python36\lib\site-packages\numpy\.libs\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll
  stacklevel=1)
W0729 17:00:25.984594 12576 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 5.0, Driver API Version: 11.3, Runtime API Version: 10.1
W0729 17:00:25.993571 12576 dynamic_loader.cc:238] Note: [Recommend] copy cudnn into CUDA installation directory.
 For instance, download cudnn-10.0-windows10-x64-v7.6.5.32.zip from NVIDIA's official website,
then, unzip it and copy it into C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0
You should do this according to your CUDA installation directory and CUDNN version.

E:\study\target detection\PaddleDetection-release-2.1>

检查cuda10.1和cudnn-10.1-windows10-x64-v8.0.5.39安装无误，路径为系统默认安装路径，且经过了其他目标检测程序的验证，此处显示w0279后，程序直接退出，无法运行成功。

另外，安装文档中有一问题：
# CUDA10.1
python -m pip install paddlepaddle-gpu==2.1.0.post101 -f https://paddlepaddle.org.cn/whl/mkl/stable.html
根据官网显示，win10，cuda10.1下，应安装2.1.1.post101"
无标签学习batch_size问题,PaddlePaddle/PaddleDetection,2021-07-26 08:36:01,14,question,3790,952651178,"在configs/datasets下设置了 allow_empty: True 进行faster_rcnn模型的无标签学习，batch_size只能设为1，尝试大的batch_size提示显存不够，但是设为1时，
![image](https://user-images.githubusercontent.com/44492927/126959157-69604a0a-e17a-489d-810c-381b5aa6a7cf.png)
显存非常充足，请问这是哪里的问题？"
关于精度,PaddlePaddle/PaddleDetection,2021-07-26 07:32:36,4,question,3789,952597664,我看介绍文件上说精度有50%，是不是有点低啊？
PaddleDetection与paddlepaddle-gpu使用问题,PaddlePaddle/PaddleDetection,2021-07-25 14:06:05,13,question,3776,952281755,"您好
我在华为云服务器上使用PaddleDetection2.1时，安装paddlepaddle-gpu2.0.0时报错paddle.fluid.dataloader.collate不存在，安装paddlepaddle-gpu2.1.0时报错OSError: (External)  Cudnn error, CUDNN_STATUS_BAD_PARAM，请问应该如何解决

"
eval评估出错,PaddlePaddle/PaddleDetection,2021-07-24 04:09:41,8,help wanted,3768,951995022,"W0723 16:30:03.859206  1127 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0
W0723 16:30:03.864948  1127 device_context.cc:260] device: 0, cuDNN Version: 7.6.
2021-07-23 16:30:12,625-INFO: Test iter 0
2021-07-23 16:30:14,206-INFO: Test finish iter 91
2021-07-23 16:30:14,206-INFO: Total number of images: 91, inference time: 50.773135477533835 fps.
2021-07-23 16:30:14,206-INFO: Start evaluate...
Traceback (most recent call last):
  File ""/home/aistudio/work/PaddleDetection/static/tools/eval.py"", line 206, in <module>
    main()
  File ""/home/aistudio/work/PaddleDetection/static/tools/eval.py"", line 188, in main
    save_only=save_only)
  File ""/home/aistudio/work/PaddleDetection/static/ppdet/utils/eval_utils.py"", line 267, in eval_results
    map_type=map_type)
  File ""/home/aistudio/work/PaddleDetection/static/ppdet/utils/voc_eval.py"", line 71, in bbox_eval
    gt_boxes = t['gt_bbox'][0]
KeyError: 'gt_bbox'

这个是什么原因呢？有什么解决办法吗？"
如何打印precision和recall值？,PaddlePaddle/PaddleDetection,2021-07-23 08:36:47,6,,3765,951370561,加了classwise后可以输出每类的ap值，想输出precision和recall值，在ppdet/metrics/map_utils.py里多处添加print语句，却没有任何输出，难道map_utils.py没有被调用吗
run the python -u tools/train.py -c configs/cascade_rcnn/cascade_rcnn_r50_fpn_1x_coco.yml --eval  error,PaddlePaddle/PaddleDetection,2021-07-21 08:31:01,7,training,3740,949463556,"
![75c6af499983fc5d022d375a31eb191](https://user-images.githubusercontent.com/57214227/126457308-2755defd-d40d-4678-a8f9-c710687b2877.png)
![image](https://user-images.githubusercontent.com/57214227/126457464-d776a803-8f1d-4dbc-96f8-0a4c4cdfb1e8.png)

run the python -u tools/train.py -c configs/cascade_rcnn/cascade_rcnn_r50_fpn_1x_coco.yml --eval  

give a  error, I need  help
"
将自己的数据集转换成VOC格式以后，训练一轮出错,PaddlePaddle/PaddleDetection,2021-07-21 00:42:57,2,training,3734,949219481,"将自己的数据集转换成VOC格式以后，使用yolov3_darknet53训练了一轮，然后直接python.exe停止运行，没有任何报错。
错误位置定位到

https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.1/ppdet/engine/trainer.py里288行的for step_id, data in enumerate(self.loader):循环体，到一个epoch结束的时候就会自动停止运行"
ppyolov2_r50vd_dcn_365e_coco 模型无法运行,PaddlePaddle/PaddleDetection,2021-07-18 16:00:28,6,,3712,947066013,"win10  python3.6.4 paddleDetection2.1
无法运行ppyolov2_r50vd_dcn_365e_coco模型，报错如下，但是配置文件本来就没有那个参数，不知道怎么解决
![image](https://user-images.githubusercontent.com/86935392/126073947-a74a625d-5aaa-4c8c-993b-9544147c9c3a.png)
"
训练完成后效果很差,PaddlePaddle/PaddleDetection,2021-07-17 01:36:52,2,question,3709,946679394,我的数据集有400+张图片，五个类别，我训练了2000个epoch，训练最终的损失为19，进行推测的时候显示的最高的置信度只有0.06，这个需要怎么解决？
python api,PaddlePaddle/PaddleDetection,2021-07-14 22:49:25,4,feature request,3697,944852299,"do you have python api like mmdetection that easily allows you to configure new dataset and modify configuration using code?

in mmdetection you can do something like:

@DATASETS.register_module()                                                                                                                                                                      
class XXXDataset(CustomDataset):                                                                                                                                                            
                                                                                                                                                                                                 
    CLASSES = (""ignored regions"", ...., ""others"")                                             
                                                                                                                                                                                                 
    def load_annotations(self, ann_file):     
.
.

and modify the config like this:
cfg = Config.fromfile('./configs/fp16/faster_rcnn_r50_fpn_fp16_1x_coco.py')                                                                                                                      
cfg.workflow=[('train',1),('val',1)]                         

you code structure seems very similar but i couldn't find this functionality."
为什么程序执行过程中有时候Process finished with exit code -1073741819，没有报错程序自动退出了,PaddlePaddle/PaddleDetection,2021-07-14 05:46:47,3,,3684,944065471,"  isinstance(merge_dct[k], collections.Mapping)):
2021-07-14 13:44:02,054-INFO: Found 1 inference images in total.
2021-07-14 13:44:07,022-INFO: Not found annotation file annotations/instances_val2017.json, load coco17 categories.
2021-07-14 13:44:12,082-INFO: Infer iter 0
2021-07-14 13:44:12,095-INFO: Detection bbox results save in output/images\32.jpg

Process finished with exit code -1073741819 (0xC0000005)"
运行图像分割时，AutoAugment并默认没有自动开启，请问如何开启？,PaddlePaddle/PaddleDetection,2021-07-11 11:07:17,2,question,3663,941427866,"![image](https://user-images.githubusercontent.com/84836583/125192460-98a88300-e27a-11eb-8f7a-c4b256297683.png)
我在Lib\site-packages\paddledet-2.0.1-py3.6.egg\ppdet\data\transform\autoaugment_utils.py中插入了多句打印，但是都没有效果，因此判断我运行的ppyolo并没有启动这些数据增强脚本，configs下的yaml是默认设置，【请问如何方便地开启该脚本的数据增强一功能】，还是说这一部分代码目前实际并不能用？"
PPYOLO v2训练时数据集不同尺寸会导致训练一轮后自动停止,PaddlePaddle/PaddleDetection,2021-07-09 09:14:18,3,training,3654,940578876,PPYOLO v2训练时数据集不同尺寸会导致训练一轮后会自动停止，我测试了尺寸一致时没有问题，数据集中如果有多个尺寸的图片，应该怎样设置才能正常训练？或者说这是这个版本的局限性？望解答，谢谢！
模型训练完成并导出后 如何在windows下使用python进行预测，我在windows下安装了paddle后直接调用./deploy/python/infer.py进行预测报了以下错误,PaddlePaddle/PaddleDetection,2021-07-09 01:36:31,2,question,3646,940339101,"```
C:\Users\zfsw\AppData\Local\Programs\Python\Python37\python.exe F:/code/PaddleDetection-release-2.1/deploy/python/infer.py --model_dir=F:\code\PaddleDetection-release-2.1\faster_rcnn_r101_vd_fpn_1x_coco --image_file=H:\2021\1.JPG --use_gpu=True
-----------  Running Arguments -----------
batch_size: 1
camera_id: -1
cpu_threads: 1
enable_mkldnn: False
image_dir: None
image_file: H:\2021\1.JPG
model_dir: F:\code\PaddleDetection-release-2.1\faster_rcnn_r101_vd_fpn_1x_coco
output_dir: output
run_benchmark: False
run_mode: fluid
threshold: 0.5
trt_calib_mode: False
trt_max_shape: 1280
trt_min_shape: 1
trt_opt_shape: 640
use_dynamic_shape: False
use_gpu: True
video_file: None
------------------------------------------
-----------  Model Configuration -----------
Model Arch: RCNN
Transform Order: 
--transform op: Resize
--transform op: NormalizeImage
--transform op: Permute
--transform op: PadStride
--------------------------------------------
Traceback (most recent call last):
  File ""F:/code/PaddleDetection-release-2.1/deploy/python/infer.py"", line 639, in <module>
    main()
  File ""F:/code/PaddleDetection-release-2.1/deploy/python/infer.py"", line 606, in main
    predict_image(detector, img_list, FLAGS.batch_size)
  File ""F:/code/PaddleDetection-release-2.1/deploy/python/infer.py"", line 520, in predict_image
    results = detector.predict(batch_image_list, FLAGS.threshold)
  File ""F:/code/PaddleDetection-release-2.1/deploy/python/infer.py"", line 160, in predict
    self.predictor.run()
ValueError: In user code:

    File ""tools/export_model.py"", line 105, in <module>
      main()
    File ""tools/export_model.py"", line 101, in main
      run(FLAGS, cfg)
    File ""tools/export_model.py"", line 69, in run
      trainer.export(FLAGS.output_dir)
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddledet-2.1.0-py3.8.egg/ppdet/engine/trainer.py"", line 506, in export
      input_spec, static_model.forward.main_program,
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 534, in main_program
      concrete_program = self.concrete_program
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 454, in concrete_program
      return self.concrete_program_specify_input_spec(input_spec=None)
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 487, in concrete_program_specify_input_spec
      concrete_program, _ = self.get_concrete_program(
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 402, in get_concrete_program
      concrete_program, partial_program_layer = self._program_cache[cache_key]
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 711, in __getitem__
      self._caches[item] = self._build_once(item)
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 698, in _build_once
      concrete_program = ConcreteProgram.from_func_spec(
    File ""<decorator-gen-65>"", line 2, in from_func_spec
      
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddle/fluid/wrapped_decorator.py"", line 25, in __impl__
      return wrapped_func(*args, **kwargs)
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddle/fluid/dygraph/base.py"", line 40, in __impl__
      return func(*args, **kwargs)
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/program_translator.py"", line 652, in from_func_spec
      outputs = static_func(*inputs)
    File ""/tmp/tmppf19xzqh.py"", line 26, in forward
      out = paddle.jit.dy2static.convert_ifelse(self.training, true_fn_1,
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 210, in convert_ifelse
      return _run_py_ifelse(pred, true_fn, false_fn, true_args, false_args)
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 235, in _run_py_ifelse
      return true_fn(*true_args) if pred else false_fn(*false_args)
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddledet-2.1.0-py3.8.egg/ppdet/modeling/architectures/meta_arch.py"", line 29, in forward
      out = self.get_pred()
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddledet-2.1.0-py3.8.egg/ppdet/modeling/architectures/faster_rcnn.py"", line 104, in get_pred
      bbox_pred, bbox_num = self._forward()
    File ""/tmp/tmpvun9zvo4.py"", line 51, in _forward
      _, __return_value_0, rois, rois_num = paddle.jit.dy2static.convert_ifelse(
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 210, in convert_ifelse
      return _run_py_ifelse(pred, true_fn, false_fn, true_args, false_args)
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 235, in _run_py_ifelse
      return true_fn(*true_args) if pred else false_fn(*false_args)
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddledet-2.1.0-py3.8.egg/ppdet/modeling/architectures/faster_rcnn.py"", line 86, in _forward
      bbox, bbox_num = self.bbox_post_process(preds, (rois, rois_num),
    File ""/tmp/tmpufu5mqqf.py"", line 35, in __call__
      bbox_num, bbox_pred = paddle.jit.dy2static.convert_ifelse(self.nms is not
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 210, in convert_ifelse
      return _run_py_ifelse(pred, true_fn, false_fn, true_args, false_args)
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 235, in _run_py_ifelse
      return true_fn(*true_args) if pred else false_fn(*false_args)
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddledet-2.1.0-py3.8.egg/ppdet/modeling/post_process.py"", line 65, in __call__
      bboxes, score = self.decode(head_out, rois, im_shape, scale_factor)
    File ""/tmp/tmpnaxnj8ca.py"", line 31, in __call__
      ] = paddle.jit.dy2static.convert_while_loop(for_loop_condition_10,
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 44, in convert_while_loop
      loop_vars = _run_py_while(cond, body, loop_vars)
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddle/fluid/dygraph/dygraph_to_static/convert_operators.py"", line 58, in _run_py_while
      loop_vars = body(*loop_vars)
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddledet-2.1.0-py3.8.egg/ppdet/modeling/layers.py"", line 347, in __call__
      expand_im_shape = paddle.expand(im_shape[idx, :],
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddle/tensor/manipulation.py"", line 1489, in expand
      helper.append_op(
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddle/fluid/layer_helper.py"", line 43, in append_op
      return self.main_program.current_block().append_op(*args, **kwargs)
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddle/fluid/framework.py"", line 3226, in append_op
      op = Operator(
    File ""/home/ubuntu01/anaconda3/envs/paddle/lib/python3.8/site-packages/paddle/fluid/framework.py"", line 2312, in __init__
      for frame in traceback.extract_stack():

    InvalidArgumentError: The 0th element of 'shape' for expand_v2 op must be greater than 0, but the value given is -2.
      [Hint: Expected expand_shape[i] > 0, but received expand_shape[i]:-2 <= 0:0.] (at D:\v2.0.0\paddle\paddle\fluid\operators\expand_v2_op.cc:75)
      [operator < expand_v2 > error]
```"
训练时输入尺寸RandomResize应该怎么设置anchor的大小,PaddlePaddle/PaddleDetection,2021-07-08 09:21:13,5,config,3642,939654906,"[https://github.com/PaddlePaddle/PaddleDetection/issues/3516](url)
里面说：anchor_cluster.py中设置的size是eval时输入模型的尺寸
请问，eval时输入模型的尺寸指的是这个吗？
![image](https://user-images.githubusercontent.com/42796059/124897383-d6a26e80-e010-11eb-931e-4c7d4bdc6f21.png)
这个 值可以修改吗？我训练用的图片比较大（且大小不一），感觉320有点小，怕影响模型的效果"
请问有pafnet-lite的FLOPs数据吗？使用Paddle提供的API计算FLOPs报错,PaddlePaddle/PaddleDetection,2021-07-07 09:50:59,5,,3625,938706062,
训练过程无法保存,PaddlePaddle/PaddleDetection,2021-07-06 07:30:40,2,windows#training,3606,937601516,"环境： win10 + cuda10.2 + paddle2.1.1 + paddledet2.1（今天刚clone）

运行 blazeface 使用voc格式数据训练，训练过程如下
```
In Paddle <= 2.0, data is in format '[Tensor(shape=(1, 2, 3), dtype=float32)]', and in Paddle >= 2.1, data is in format 'Tensor(shape=(1, 2, 3), dtype=float32)'

[07/06 15:18:11] ppdet.utils.checkpoint INFO: Finish loading model weights: C:\Users\ANZE/.cache/paddle/weights\blazenet_pretrain.pdparams
[07/06 15:18:12] ppdet.engine INFO: Epoch: [0] [ 0/87] learning_rate: 0.000333 loss: 19.039179 eta: 4:35:48 batch_cost: 0.1902 data_cost: 0.0000 ips: 42.0581 images/s
[07/06 15:18:55] ppdet.engine INFO: Epoch: [0] [20/87] learning_rate: 0.000360 loss: 11.350872 eta: 2 days, 1:56:48 batch_cost: 2.1611 data_cost: 2.0163 ips: 3.7018 images/s
[07/06 15:19:43] ppdet.engine INFO: Epoch: [0] [40/87] learning_rate: 0.000387 loss: 9.275948 eta: 2 days, 5:44:07 batch_cost: 2.3897 data_cost: 2.2493 ips: 3.3476 images/s
[07/06 15:20:28] ppdet.engine INFO: Epoch: [0] [60/87] learning_rate: 0.000413 loss: 8.554915 eta: 2 days, 5:57:25 batch_cost: 2.2541 data_cost: 2.1143 ips: 3.5491 images/s
[07/06 15:21:19] ppdet.engine INFO: Epoch: [0] [80/87] learning_rate: 0.000440 loss: 7.966538 eta: 2 days, 7:57:21 batch_cost: 2.5716 data_cost: 2.4313 ips: 3.1109 images/s
Segmentation fault

```"
脚本任务运行先导杯使用2.1版本detection  Finish loading model weights后不运行，直接显示失败  ，前面也没报错,PaddlePaddle/PaddleDetection,2021-07-03 04:35:39,2,training,3577,936169364,"[07/03 04:22:51] ppdet.data.source.coco WARNING: Found an invalid bbox in annotations: im_id: 5213, area: 0.0 x1: 387.0, y1: 113.0, x2: 391.0, y2: 113.0.
[07/03 04:22:51] ppdet.data.source.coco WARNING: Found an invalid bbox in annotations: im_id: 5810, area: 0.0 x1: 552.0, y1: 54.0, x2: 553.0, y2: 54.0.
[07/03 04:22:52] ppdet.data.source.coco WARNING: Found an invalid bbox in annotations: im_id: 6623, area: 0.0 x1: 301.0, y1: 398.0, x2: 303.0, y2: 398.0.
[07/03 04:22:52] ppdet.data.source.coco WARNING: Found an invalid bbox in annotations: im_id: 6623, area: 0.0 x1: 298.0, y1: 398.0, x2: 305.0, y2: 398.0.
[07/03 04:22:52] ppdet.data.source.coco WARNING: Found an invalid bbox in annotations: im_id: 10247, area: 0.0 x1: 1091.0, y1: 26.0, x2: 1092.0, y2: 26.0.
[07/03 04:22:52] ppdet.data.source.coco WARNING: Found an invalid bbox in annotations: im_id: 11131, area: 0.0 x1: 615.0, y1: 135.0, x2: 616.0, y2: 135.0.
[07/03 04:22:53] ppdet.utils.checkpoint INFO: unique_endpoints {'127.0.0.1:35534'}
[07/03 04:23:04] ppdet.utils.checkpoint INFO: Finish loading model weights: /root/.cache/paddle/weights/ResNet50_vd_ssld_pretrained.pdparams
/mnt
[INFO]: train job failed! train_ret: 1"
迁移学习finetune，冻结网络层（按理停止计算梯度）为什么训练速度不增加？,PaddlePaddle/PaddleDetection,2021-07-02 12:05:00,4,training,3574,935707196,"我试着冻结网络前面层的参数更新，采用的方法是在tools/train.py里将参数的stop_gradient的值设置为True：
![image](https://user-images.githubusercontent.com/67451272/124270665-18459c00-db6f-11eb-991c-5b3f6dfaf314.png)
对于参数的冻结是实现了：
![image](https://user-images.githubusercontent.com/67451272/124271746-6b6c1e80-db70-11eb-851a-5c8fff8c8d9a.png)
loss是很快就趋于平稳，但是速度上，并没有提升，是因为梯度还是求导了吗，还是因为抑制求导对训练速度的提升没有那么明显（只是节省内存）？"
预测mask和原图片高宽反了报错,PaddlePaddle/PaddleDetection,2021-06-29 09:32:30,2,bug,3539,932419889,"我们用maskrcnn预测一个(2304, 3072, 3)大小的图片，但在visualize mask时候报错如下

我们打印了mask的shape，发现是(3072, 2304)；进一步打印test loader里这个图片的shape Tensor(shape=[1, 3, 1088, 800]）就发现长宽颠倒了。请问有什么解决办法吗？是在input resize的时候有些问题吗？

ppdet=2.1.0


报错如下
```
Traceback (most recent call last):
  File ""PaddleDetection/tools/infer.py"", line 158, in <module>
    main()
  File ""PaddleDetection/tools/infer.py"", line 154, in main
    run(FLAGS, cfg)
  File ""PaddleDetection/tools/infer.py"", line 132, in run
    save_txt=FLAGS.save_txt)
  File ""/root/ikcest/PaddleDetection/ppdet/engine/trainer.py"", line 448, in predict
    int(im_id), catid2name, draw_threshold)
  File ""/root/ikcest/PaddleDetection/ppdet/utils/visualizer.py"", line 47, in visualize_results
    image = draw_mask(image, im_id, mask_res, threshold)
  File ""/root/ikcest/PaddleDetection/ppdet/utils/visualizer.py"", line 83, in draw_mask
    img_array[idx[0], idx[1], :] *= 1.0 - alpha
IndexError: index 2304 is out of bounds for axis 0 with size 2304
```
"
YOLOV3-训练自定义coco格式数据集报错,PaddlePaddle/PaddleDetection,2021-06-29 02:33:09,9,custom dataset,3531,932147688,"该自定义数据集在fasterrcnn模型上已经跑过没有问题，但不知道为什么在yolov3上报错。
```
[06/29 10:12:55] ppdet.engine INFO: Epoch: [14] [  0/261] learning_rate: 0.001142 loss_xy: 3.539429 loss_wh: 1.569762 loss_obj: 7.965590 loss_cls: 0.847763 loss: 14.178358 eta: 7:35:29 batch_cost: 0.4247 data_cost: 0.2433 ips: 9.4186 images/s
[06/29 10:13:03] ppdet.engine INFO: Epoch: [14] [ 20/261] learning_rate: 0.001148 loss_xy: 4.150413 loss_wh: 1.634536 loss_obj: 8.588768 loss_cls: 0.780585 loss: 14.390470 eta: 7:35:14 batch_cost: 0.3903 data_cost: 0.1949 ips: 10.2492 images/s
[06/29 10:13:10] ppdet.engine INFO: Epoch: [14] [ 40/261] learning_rate: 0.001154 loss_xy: 5.121928 loss_wh: 1.915970 loss_obj: 9.221029 loss_cls: 1.040466 loss: 17.048763 eta: 7:34:44 batch_cost: 0.3476 data_cost: 0.1670 ips: 11.5071 images/s
[06/29 10:13:17] ppdet.engine INFO: Epoch: [14] [ 60/261] learning_rate: 0.001161 loss_xy: 3.654006 loss_wh: 1.515937 loss_obj: 8.141011 loss_cls: 0.916687 loss: 15.310648 eta: 7:34:19 batch_cost: 0.3627 data_cost: 0.1700 ips: 11.0279 images/s
[06/29 10:13:24] ppdet.engine INFO: Epoch: [14] [ 80/261] learning_rate: 0.001167 loss_xy: 3.943925 loss_wh: 1.593351 loss_obj: 9.177536 loss_cls: 0.809466 loss: 16.202129 eta: 7:33:58 batch_cost: 0.3715 data_cost: 0.1608 ips: 10.7685 images/s
[06/29 10:13:32] ppdet.engine INFO: Epoch: [14] [100/261] learning_rate: 0.001173 loss_xy: 4.507587 loss_wh: 1.768098 loss_obj: 9.911394 loss_cls: 0.836492 loss: 17.249420 eta: 7:33:38 batch_cost: 0.3763 data_cost: 0.1780 ips: 10.6307 images/s
Traceback (most recent call last):
  File ""/home/suliang/suliang_git/PaddleDetection-2.0.0/sl_package/train.py"", line 155, in <module>
    main()
  File ""/home/suliang/suliang_git/PaddleDetection-2.0.0/sl_package/train.py"", line 151, in main
    run(FLAGS, cfg)
  File ""/home/suliang/suliang_git/PaddleDetection-2.0.0/sl_package/train.py"", line 111, in run
    trainer.train(FLAGS.eval)
  File ""/home/suliang/suliang_git/PaddleDetection-2.0.0/ppdet/engine/trainer.py"", line 250, in train
    for step_id, data in enumerate(self.loader):
  File ""/home/suliang/suliang_git/PaddleDetection-2.0.0/ppdet/data/reader.py"", line 216, in __next__
    data = next(self.loader)
  File ""/home/suliang/anaconda3/envs/paddle_env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 585, in __next__
    data = self._reader.read_next_var_list()
  File ""/home/suliang/anaconda3/envs/paddle_env/lib/python3.7/site-packages/paddle/fluid/multiprocess_utils.py"", line 138, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 21781) exited is killed by signal: Killed. (at /paddle/paddle/fluid/imperative/data_loader.cc:181)

Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File ""/home/suliang/anaconda3/envs/paddle_env/lib/python3.7/site-packages/paddle/fluid/multiprocess_utils.py"", line 74, in _func_exectuor
    function()
  File ""/home/suliang/anaconda3/envs/paddle_env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 569, in _shutdown_on_exit
    self._try_shutdown_all(1)
  File ""/home/suliang/anaconda3/envs/paddle_env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 385, in _try_shutdown_all
    w.join(timeout)
  File ""/home/suliang/anaconda3/envs/paddle_env/lib/python3.7/multiprocessing/process.py"", line 140, in join
    res = self._popen.wait(timeout)
  File ""/home/suliang/anaconda3/envs/paddle_env/lib/python3.7/multiprocessing/popen_fork.py"", line 45, in wait
    if not wait([self.sentinel], timeout):
  File ""/home/suliang/anaconda3/envs/paddle_env/lib/python3.7/multiprocessing/connection.py"", line 921, in wait
    ready = selector.select(timeout)
  File ""/home/suliang/anaconda3/envs/paddle_env/lib/python3.7/selectors.py"", line 415, in select
    fd_event_list = self._selector.poll(timeout)
  File ""/home/suliang/anaconda3/envs/paddle_env/lib/python3.7/site-packages/paddle/fluid/multiprocess_utils.py"", line 138, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 21781) exited is killed by signal: Killed. (at /paddle/paddle/fluid/imperative/data_loader.cc:181)
```

我的yolov3设置如下：
```
_BASE_: [
  '../configs/datasets/coco_detection.yml',
  '../configs/runtime.yml',
  '../configs/yolov3/_base_/optimizer_270e.yml',
  '../configs/yolov3/_base_/yolov3_darknet53.yml',
  '../configs/yolov3/_base_/yolov3_reader.yml',
]

snapshot_epoch: 5
weights: /home/suliang/suliang_git/PaddleDetection-2.0.0/sl_package/output/sl_package_yolov3_darknet53_270e_coco/model_final.pdparams

# 数据集调整
# change n_classes： 增加了ok-noexposed1、ok-noexposed2类后变为6类
num_classes: 6
TrainDataset:
  !COCODataSet
    image_dir: train
    anno_path: annotations/instance_train.json
    dataset_dir: ../dataset/Package/coco_format
    data_fields: ['image', 'gt_bbox', 'gt_class', 'is_crowd']

EvalDataset:
  !COCODataSet
    image_dir: val
    anno_path: annotations/instance_val.json
    dataset_dir: ../dataset/Package/coco_format
TestDataset:
  !ImageFolder
    anno_path: ../dataset/Package/coco_format/annotations/instance_val.json

worker_num: 2
TrainReader:
  # 初次训练在epoch15意外退出，然后尝试把batch size从8变到4，依然报错，GPU占用显存8G/11G
  batch_size: 4

LearningRate:
  # from 0.01 to 0.01/8
  base_lr: 0.00125

# 保持yolov3现有anchor设置不变
YOLOv3Head:
  anchors: [[10, 13], [16, 30], [33, 23],
            [30, 61], [62, 45], [59, 119],
            [116, 90], [156, 198], [373, 326]]
  anchor_masks: [[6, 7, 8], [3, 4, 5], [0, 1, 2]]
  loss: YOLOv3Loss
```
请问下可能是什么问题？"
大规模实用目标检测模型,PaddlePaddle/PaddleDetection,2021-06-27 13:42:59,4,feature request,3512,930934653,大规模实用目标检测模型有动态图版本吗，或者可以转换为动态图版本吗？
windows上采用c++调用dll库的方式预测时间问题,PaddlePaddle/PaddleDetection,2021-06-25 01:18:53,2,,3495,929741537,"**一次预测一张图像的耗时：**
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0625 09:11:11.220824 18804 analysis_config.cc:424] use_dlnne_:0
I0625 09:11:11.221714 18804 analysis_config.cc:424] use_dlnne_:0
I0625 09:11:11.222712 18804 analysis_config.cc:424] use_dlnne_:0
I0625 09:11:11.222712 18804 analysis_config.cc:424] use_dlnne_:0
**Predict一次检测处理时间:1.55**
一次检测处理时间:1.55
第0张图像缺陷个数：2
result: class=4 confidence=0.65 rect=[233 119 243 129]
result: class=4 confidence=0.59 rect=[246 134 255 142]

**一次预测六张图像的耗时：**
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0625 09:09:13.766381  4792 analysis_config.cc:424] use_dlnne_:0
I0625 09:09:23.955991  4792 analysis_config.cc:424] use_dlnne_:0
I0625 09:09:23.955991  4792 analysis_config.cc:424] use_dlnne_:0
I0625 09:09:23.956990  4792 analysis_config.cc:424] use_dlnne_:0
**Predict一次检测处理时间:1.735**
一次检测处理时间:1.736
第0张图像缺陷个数：2
result: class=4 confidence=0.65 rect=[233 119 243 129]
result: class=4 confidence=0.59 rect=[246 134 255 142]
第1张图像缺陷个数：3
result: class=3 confidence=0.80 rect=[225 85 498 227]
result: class=3 confidence=0.73 rect=[165 77 199 106]
result: class=0 confidence=0.60 rect=[216 377 223 384]
第2张图像缺陷个数：1
result: class=1 confidence=0.60 rect=[4 54 138 151]
第3张图像缺陷个数：1
result: class=4 confidence=0.72 rect=[254 140 262 148]
第4张图像缺陷个数：1
result: class=4 confidence=0.74 rect=[243 218 250 226]
第5张图像缺陷个数：1
result: class=4 confidence=0.74 rect=[236 312 246 322]
第6张图像缺陷个数：2
result: class=1 confidence=0.78 rect=[0 7 189 404]
result: class=3 confidence=0.39 rect=[488 10 498 491]


dll库中加载模型时的参数：
**bool use_gpu = true;**
bool use_mkldnn = false;
double threshold = 0.2;
std::string run_mode = ""fluid"";
int batch_size = 1;
int gpu_id = 0;
bool use_dynamic_shape = false;
int trt_min_shape = 1;
int trt_max_shape = 1280;
int trt_opt_shape = 640;
bool trt_calib_mode = false;

window10+cuda10.1+cudnn7.6.5+cmake3.17+opencv3.4.6+vs2019
显卡：2080s


"
使用RandomDistort时候出现错误,PaddlePaddle/PaddleDetection,2021-06-22 10:39:28,2,,3472,927089764,"在动态图的cascade rcnn中使用RandomDistort出现如下错误：
![image](https://user-images.githubusercontent.com/71364549/122910765-29c9bf80-d389-11eb-9ebe-f05dbe6621c1.png)
"
分布式训练报错！！！,PaddlePaddle/PaddleDetection,2021-06-22 09:16:12,4,,3469,927016983,"![image](https://user-images.githubusercontent.com/50700993/122898384-67c0e680-d37d-11eb-9a2b-72e7e2cac599.png)
正常跑训练代码 CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -m paddle.distributed.launch --gpus 0,1,2,3,4,5,6,7 tools/train.py -c configs/solov2/solov2_r50_fpn_3x_coco.yml -o pretrain_weights=pre_weights/solov2_r50_fpn_3x_coco/solov2_r50_fpn_3x_coco.pdparams --vdl_log_dir=vdl_dir/scalar
直接报错，有大佬帮忙看下吗
"
目标特别小，但数据集足够情况下，目前哪些模型比较合适？,PaddlePaddle/PaddleDetection,2021-06-22 08:21:01,3,help wanted,3468,926967354,"目前使用的是fater-rcnn+FPN，也调了合适的anchor,但是结果仍然不太理想，选取一张：图像大小5472 * 3078  而目标大小仅有70*60,其余图大小和目标大小都差不多，针对该单类类别数据集有7W左右。"
打印模型的FLOPS,PaddlePaddle/PaddleDetection,2021-06-22 04:54:53,5,help wanted,3464,926828542,"我想打印出faster rcnn的网络结构和参数信息，使用了paddle.flops（）函数，在infer.py中修改如下：
`
def run(FLAGS, cfg):
    # build trainer
    trainer = Trainer(cfg, mode='test')

    # load weights
    trainer.load_weights(cfg.weights)

    # get inference images
    images = get_test_images(FLAGS.infer_dir, FLAGS.infer_img)

    # inference
    trainer.predict(
        images,
        draw_threshold=FLAGS.draw_threshold,
        output_dir=FLAGS.output_dir,
        save_txt=FLAGS.save_txt)
    #打印模型的基础结构和参数信息
    FLOPs = paddle.flops(trainer.model, [1, 3, 608, 608], custom_ops= {nn.LeakyReLU: trainer.predict}, print_detail=True)
    print(FLOPs)


def main():
    FLAGS = parse_args()
    cfg = load_config(FLAGS.config)
    cfg['use_vdl'] = FLAGS.use_vdl
    cfg['vdl_log_dir'] = FLAGS.vdl_log_dir
    merge_config(FLAGS.opt)

    place = paddle.set_device('gpu' if cfg.use_gpu else 'cpu')

    if 'norm_type' in cfg and cfg['norm_type'] == 'sync_bn' and not cfg.use_gpu:
        cfg['norm_type'] = 'bn'

    if FLAGS.slim_config:
        cfg = build_slim_model(cfg, FLAGS.slim_config, mode='test')

    check_config(cfg)
    check_gpu(cfg.use_gpu)
    check_version()

    run(FLAGS, cfg)

if __name__ == '__main__':
    main()
`
结果报错如下
![1](https://user-images.githubusercontent.com/62825744/122866002-d8eea280-d359-11eb-9a9e-8f26b979c576.png)
![2](https://user-images.githubusercontent.com/62825744/122866016-dee48380-d359-11eb-9f15-d5d2dd5bad7d.png)

"
报错,PaddlePaddle/PaddleDetection,2021-06-22 03:59:09,6,environment,3463,926800167,"WARNING 2021-06-22 03:57:39,633 launch.py:357] Not found distinct arguments and compiled with cuda or xpu. Default use collective mode
launch train in GPU mode!
INFO 2021-06-22 03:57:39,634 launch_utils.py:621] Change selected_gpus into reletive values. --ips:0,1,2,3,4,5,6,7 will change into relative_ips:[0, 1, 2, 3, 4, 5, 6, 7] according to your CUDA_VISIBLE_DEVICES:['0', '1', '2', '3', '4', '5', '6', '7']
can't find avilable port and use the specified static port now!
Traceback (most recent call last):
  File ""/ai_zhang401/conda_files/envs/paddle_py36/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/ai_zhang401/conda_files/envs/paddle_py36/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/ai_zhang401/conda_files/envs/paddle_py36/lib/python3.6/site-packages/paddle/distributed/launch.py"", line 16, in <module>
    launch.launch()
  File ""/ai_zhang401/conda_files/envs/paddle_py36/lib/python3.6/site-packages/paddle/distributed/fleet/launch.py"", line 369, in launch
    launch_collective(args)
  File ""/ai_zhang401/conda_files/envs/paddle_py36/lib/python3.6/site-packages/paddle/distributed/fleet/launch.py"", line 241, in launch_collective
    devices_per_proc)
  File ""/ai_zhang401/conda_files/envs/paddle_py36/lib/python3.6/site-packages/paddle/distributed/fleet/launch.py"", line 210, in get_cluster_from_args
    trainer_endpoints.append([""%s:%d"" % (ip, port) for port in free_ports])
TypeError: 'NoneType' object is not iterable"
预训练模型,PaddlePaddle/PaddleDetection,2021-06-21 14:42:27,2,,3462,926290825,是只提供了部分的backbone的预训练模型吗，paddleclas提供的预训练权重没法直接使用，比如HRNet系列
SOLOV2支持C++部署吗,PaddlePaddle/PaddleDetection,2021-06-21 06:51:51,14,feature request#deploy,3449,925887450,打算将Python部署的转到C++，现在可以吗
建议增加一下RCNN、Fast-RCNN的模型,PaddlePaddle/PaddleDetection,2021-06-18 07:21:55,0,feature request,3427,924621194,在进行论文复现的时候发现这两个模型缺失，但在目标检测领域很重要，希望官方能复现下这两个模型加进去
windows编译PaddleTensort版错误，麻烦指点一下,PaddlePaddle/PaddleDetection,2021-06-15 08:09:55,4,,3388,921126174,"编译器VS2015 UPDATE3，CUDA10.2
![image](https://user-images.githubusercontent.com/51236891/122016827-f967ab80-cdf3-11eb-9a70-b622eb9652ee.png)

30>------ 已启动生成: 项目: framework_py_proto, 配置: Release x64 ------
28>  broadcast.cc
28>d:\work\build\paddle\build\third_party\eigen3\src\extern_eigen3\eigen\src/Core/ArithmeticSequence.h(345): fatal error C1001: 编译器中发生内部错误。
28>  (编译器文件“f:\dd\vctools\compiler\cxxfe\sl\p1\c\p0gettok.c”，第 6502 行)
28>   要解决此问题，请尝试简化或更改上面所列位置附近的程序。
28>  请选择 Visual C++
28>  “帮助”菜单上的“技术支持”命令，或打开技术支持帮助文件来获得详细信息。
30>  data_feed_pb2.py
30>  distributed_strategy_pb2.py
30>  framework_pb2.py
30>  trainer_desc_pb2.py
30>  __init__.py
30>  已复制         5 个文件。
30>  distributed_strategy_pb2.py
30>  已复制         1 个文件。
31>------ 已启动生成: 项目: third_party, 配置: Release x64 ------
32>------ 已启动生成: 项目: profiler, 配置: Release x64 ------
32>  Compiling CUDA source file ..\..\..\..\paddle\fluid\platform\profiler.cu...
32>
32>  D:\work\build\Paddle\build\paddle\fluid\platform>""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\bin\nvcc.exe"" -gencode=arch=compute_35,code=\""sm_35,compute_35\"" -gencode=arch=compute_50,code=\""sm_50,compute_50\"" -gencode=arch=compute_52,code=\""sm_52,compute_52\"" -gencode=arch=compute_60,code=\""sm_60,compute_60\"" -gencode=arch=compute_61,code=\""sm_61,compute_61\"" -gencode=arch=compute_70,code=\""sm_70,compute_70\"" -gencode=arch=compute_75,code=\""sm_75,compute_75\"" --use-local-env 2015 -ccbin ""D:\VS2015\VC\bin\x86_amd64"" -x cu  -ID:\work\build\Paddle\build -ID:\work\build\Paddle\paddle\fluid\framework\io -I""D:\3rdparty\TensorRT-7.0.0.11\include"" -ID:\work\build\Paddle\build\third_party\install\zlib\include -ID:\work\build\Paddle\build\third_party\install -ID:\work\build\Paddle\build\third_party\install\gflags\include -ID:\work\build\Paddle\build\third_party\install\glog\include -ID:\work\build\Paddle\build\third_party\boost\src\extern_boost -ID:\work\build\Paddle\build\third_party\eigen3\src\extern_eigen3 -ID:\work\build\Paddle\build\third_party\threadpool\src\extern_threadpool -ID:\work\build\Paddle\build\third_party\dlpack\src\extern_dlpack\include -ID:\work\build\Paddle\build\third_party\install\xxhash\include -ID:\work\build\Paddle\build\third_party\install\warpctc\include -ID:\work\build\Paddle\build\third_party\install\mklml\include -ID:\work\build\Paddle\build\third_party\install\mkldnn\include -ID:\work\build\Paddle\build\third_party\install\protobuf\include -IC:\Users\Gaotiantian\anaconda3\include -I""C:\Users\Gaotiantian\anaconda3\Lib\site-packages\numpy\core\include"" -ID:\work\build\Paddle\build\third_party\pybind\src\extern_pybind\include -ID:\work\build\Paddle\build\third_party\cub\src\extern_cub -ID:\work\build\Paddle\build\third_party\install\cryptopp\include -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include"" -ID:\work\build\Paddle -ID:\work\build\Paddle\build\..\paddle\fluid\framework\io -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include""     --keep-dir x64\Release -maxrregcount=0  --machine 64 --compile -cudart static -Wno-deprecated-gpu-targets -w --expt-relaxed-constexpr --expt-extended-lambda -O3 -Xcompiler=""/EHsc /wd4244 /wd4267 /wd4819 /bigobj /arch:AVX""    -D_WINDOWS -D_MWAITXINTRIN_H_INCLUDED -D__STRICT_ANSI__ -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -DWIN32 -D_WINDOWS -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -D_MBCS -Xcompiler ""/EHsc /W0 /nologo /O2 /FS  /MT /GR"" -o profiler.dir\Release\/profiler.cu.obj ""D:\work\build\Paddle\paddle\fluid\platform\profiler.cu""
32>  nvcc fatal   : A single input file is required for a non-link phase when an outputfile is specified
32>C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\BuildCustomizations\CUDA 10.2.targets(764,9): error MSB3721: 命令“""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\bin\nvcc.exe"" -gencode=arch=compute_35,code=\""sm_35,compute_35\"" -gencode=arch=compute_50,code=\""sm_50,compute_50\"" -gencode=arch=compute_52,code=\""sm_52,compute_52\"" -gencode=arch=compute_60,code=\""sm_60,compute_60\"" -gencode=arch=compute_61,code=\""sm_61,compute_61\"" -gencode=arch=compute_70,code=\""sm_70,compute_70\"" -gencode=arch=compute_75,code=\""sm_75,compute_75\"" --use-local-env 2015 -ccbin ""D:\VS2015\VC\bin\x86_amd64"" -x cu  -ID:\work\build\Paddle\build -ID:\work\build\Paddle\paddle\fluid\framework\io -I""D:\3rdparty\TensorRT-7.0.0.11\include"" -ID:\work\build\Paddle\build\third_party\install\zlib\include -ID:\work\build\Paddle\build\third_party\install -ID:\work\build\Paddle\build\third_party\install\gflags\include -ID:\work\build\Paddle\build\third_party\install\glog\include -ID:\work\build\Paddle\build\third_party\boost\src\extern_boost -ID:\work\build\Paddle\build\third_party\eigen3\src\extern_eigen3 -ID:\work\build\Paddle\build\third_party\threadpool\src\extern_threadpool -ID:\work\build\Paddle\build\third_party\dlpack\src\extern_dlpack\include -ID:\work\build\Paddle\build\third_party\install\xxhash\include -ID:\work\build\Paddle\build\third_party\install\warpctc\include -ID:\work\build\Paddle\build\third_party\install\mklml\include -ID:\work\build\Paddle\build\third_party\install\mkldnn\include -ID:\work\build\Paddle\build\third_party\install\protobuf\include -IC:\Users\Gaotiantian\anaconda3\include -I""C:\Users\Gaotiantian\anaconda3\Lib\site-packages\numpy\core\include"" -ID:\work\build\Paddle\build\third_party\pybind\src\extern_pybind\include -ID:\work\build\Paddle\build\third_party\cub\src\extern_cub -ID:\work\build\Paddle\build\third_party\install\cryptopp\include -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include"" -ID:\work\build\Paddle -ID:\work\build\Paddle\build\..\paddle\fluid\framework\io -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include""     --keep-dir x64\Release -maxrregcount=0  --machine 64 --compile -cudart static -Wno-deprecated-gpu-targets -w --expt-relaxed-constexpr --expt-extended-lambda -O3 -Xcompiler=""/EHsc /wd4244 /wd4267 /wd4819 /bigobj /arch:AVX""    -D_WINDOWS -D_MWAITXINTRIN_H_INCLUDED -D__STRICT_ANSI__ -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -DWIN32 -D_WINDOWS -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -D_MBCS -Xcompiler ""/EHsc /W0 /nologo /O2 /FS  /MT /GR"" -o profiler.dir\Release\/profiler.cu.obj ""D:\work\build\Paddle\paddle\fluid\platform\profiler.cu""”已退出，返回代码为 1。
33>------ 已启动生成: 项目: device_context, 配置: Release x64 ------
33>  device_context.cc
33>  Unknown compiler version - please run the configure tests and report the results
33>d:\work\build\paddle\build\third_party\eigen3\src\extern_eigen3\eigen\src/Core/ArithmeticSequence.h(345): fatal error C1001: 编译器中发生内部错误。
33>  (编译器文件“f:\dd\vctools\compiler\cxxfe\sl\p1\c\p0gettok.c”，第 6502 行)
33>   要解决此问题，请尝试简化或更改上面所列位置附近的程序。
33>  请选择 Visual C++
33>  “帮助”菜单上的“技术支持”命令，或打开技术支持帮助文件来获得详细信息。
33>  init.cc
33>  Unknown compiler version - please run the configure tests and report the results
33>d:\work\build\paddle\build\third_party\eigen3\src\extern_eigen3\eigen\src/Core/ArithmeticSequence.h(345): fatal error C1001: 编译器中发生内部错误。
33>  (编译器文件“f:\dd\vctools\compiler\cxxfe\sl\p1\c\p0gettok.c”，第 6502 行)
33>   要解决此问题，请尝试简化或更改上面所列位置附近的程序。
49>  D:\work\build\Paddle\build\paddle\fluid\operators\math>""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\bin\nvcc.exe"" -gencode=arch=compute_35,code=\""sm_35,compute_35\"" -gencode=arch=compute_50,code=\""sm_50,compute_50\"" -gencode=arch=compute_52,code=\""sm_52,compute_52\"" -gencode=arch=compute_60,code=\""sm_60,compute_60\"" -gencode=arch=compute_61,code=\""sm_61,compute_61\"" -gencode=arch=compute_70,code=\""sm_70,compute_70\"" -gencode=arch=compute_75,code=\""sm_75,compute_75\"" --use-local-env 2015 -ccbin ""D:\VS2015\VC\bin\x86_amd64"" -x cu  -ID:\work\build\Paddle\build -ID:\work\build\Paddle\paddle\fluid\framework\io -I""D:\3rdparty\TensorRT-7.0.0.11\include"" -ID:\work\build\Paddle\build\third_party\install\zlib\include -ID:\work\build\Paddle\build\third_party\install -ID:\work\build\Paddle\build\third_party\install\gflags\include -ID:\work\build\Paddle\build\third_party\install\glog\include -ID:\work\build\Paddle\build\third_party\boost\src\extern_boost -ID:\work\build\Paddle\build\third_party\eigen3\src\extern_eigen3 -ID:\work\build\Paddle\build\third_party\threadpool\src\extern_threadpool -ID:\work\build\Paddle\build\third_party\dlpack\src\extern_dlpack\include -ID:\work\build\Paddle\build\third_party\install\xxhash\include -ID:\work\build\Paddle\build\third_party\install\warpctc\include -ID:\work\build\Paddle\build\third_party\install\mklml\include -ID:\work\build\Paddle\build\third_party\install\mkldnn\include -ID:\work\build\Paddle\build\third_party\install\protobuf\include -IC:\Users\Gaotiantian\anaconda3\include -I""C:\Users\Gaotiantian\anaconda3\Lib\site-packages\numpy\core\include"" -ID:\work\build\Paddle\build\third_party\pybind\src\extern_pybind\include -ID:\work\build\Paddle\build\third_party\cub\src\extern_cub -ID:\work\build\Paddle\build\third_party\install\cryptopp\include -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include"" -ID:\work\build\Paddle -ID:\work\build\Paddle\build\..\paddle\fluid\framework\io -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include""     --keep-dir x64\Release -maxrregcount=0  --machine 64 --compile -cudart static -Wno-deprecated-gpu-targets -w --expt-relaxed-constexpr --expt-extended-lambda -O3 -Xcompiler=""/EHsc /wd4244 /wd4267 /wd4819 /bigobj /arch:AVX""    -D_WINDOWS -D_MWAITXINTRIN_H_INCLUDED -D__STRICT_ANSI__ -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -DWIN32 -D_WINDOWS -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -D_MBCS -Xcompiler ""/EHsc /W0 /nologo /O2 /FS  /MT /GR"" -o sample_prob.dir\Release\/sample_prob.cu.obj ""D:\work\build\Paddle\paddle\fluid\operators\math\sample_prob.cu""
49>  nvcc fatal   : A single input file is required for a non-link phase when an outputfile is specified
49>C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\BuildCustomizations\CUDA 10.2.targets(764,9): error MSB3721: 命令“""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\bin\nvcc.exe"" -gencode=arch=compute_35,code=\""sm_35,compute_35\"" -gencode=arch=compute_50,code=\""sm_50,compute_50\"" -gencode=arch=compute_52,code=\""sm_52,compute_52\"" -gencode=arch=compute_60,code=\""sm_60,compute_60\"" -gencode=arch=compute_61,code=\""sm_61,compute_61\"" -gencode=arch=compute_70,code=\""sm_70,compute_70\"" -gencode=arch=compute_75,code=\""sm_75,compute_75\"" --use-local-env 2015 -ccbin ""D:\VS2015\VC\bin\x86_amd64"" -x cu  -ID:\work\build\Paddle\build -ID:\work\build\Paddle\paddle\fluid\framework\io -I""D:\3rdparty\TensorRT-7.0.0.11\include"" -ID:\work\build\Paddle\build\third_party\install\zlib\include -ID:\work\build\Paddle\build\third_party\install -ID:\work\build\Paddle\build\third_party\install\gflags\include -ID:\work\build\Paddle\build\third_party\install\glog\include -ID:\work\build\Paddle\build\third_party\boost\src\extern_boost -ID:\work\build\Paddle\build\third_party\eigen3\src\extern_eigen3 -ID:\work\build\Paddle\build\third_party\threadpool\src\extern_threadpool -ID:\work\build\Paddle\build\third_party\dlpack\src\extern_dlpack\include -ID:\work\build\Paddle\build\third_party\install\xxhash\include -ID:\work\build\Paddle\build\third_party\install\warpctc\include -ID:\work\build\Paddle\build\third_party\install\mklml\include -ID:\work\build\Paddle\build\third_party\install\mkldnn\include -ID:\work\build\Paddle\build\third_party\install\protobuf\include -IC:\Users\Gaotiantian\anaconda3\include -I""C:\Users\Gaotiantian\anaconda3\Lib\site-packages\numpy\core\include"" -ID:\work\build\Paddle\build\third_party\pybind\src\extern_pybind\include -ID:\work\build\Paddle\build\third_party\cub\src\extern_cub -ID:\work\build\Paddle\build\third_party\install\cryptopp\include -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include"" -ID:\work\build\Paddle -ID:\work\build\Paddle\build\..\paddle\fluid\framework\io -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include""     --keep-dir x64\Release -maxrregcount=0  --machine 64 --compile -cudart static -Wno-deprecated-gpu-targets -w --expt-relaxed-constexpr --expt-extended-lambda -O3 -Xcompiler=""/EHsc /wd4244 /wd4267 /wd4819 /bigobj /arch:AVX""    -D_WINDOWS -D_MWAITXINTRIN_H_INCLUDED -D__STRICT_ANSI__ -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -DWIN32 -D_WINDOWS -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -D_MBCS -Xcompiler ""/EHsc /W0 /nologo /O2 /FS  /MT /GR"" -o sample_prob.dir\Release\/sample_prob.cu.obj ""D:\work\build\Paddle\paddle\fluid\operators\math\sample_prob.cu""”已退出，返回代码为 1。
50>------ 已启动生成: 项目: cross_entropy, 配置: Release x64 ------
48>
48>  D:\work\build\Paddle\build\paddle\fluid\operators\math>""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\bin\nvcc.exe"" -gencode=arch=compute_35,code=\""sm_35,compute_35\"" -gencode=arch=compute_50,code=\""sm_50,compute_50\"" -gencode=arch=compute_52,code=\""sm_52,compute_52\"" -gencode=arch=compute_60,code=\""sm_60,compute_60\"" -gencode=arch=compute_61,code=\""sm_61,compute_61\"" -gencode=arch=compute_70,code=\""sm_70,compute_70\"" -gencode=arch=compute_75,code=\""sm_75,compute_75\"" --use-local-env 2015 -ccbin ""D:\VS2015\VC\bin\x86_amd64"" -x cu  -ID:\work\build\Paddle\build -ID:\work\build\Paddle\paddle\fluid\framework\io -I""D:\3rdparty\TensorRT-7.0.0.11\include"" -ID:\work\build\Paddle\build\third_party\install\zlib\include -ID:\work\build\Paddle\build\third_party\install -ID:\work\build\Paddle\build\third_party\install\gflags\include -ID:\work\build\Paddle\build\third_party\install\glog\include -ID:\work\build\Paddle\build\third_party\boost\src\extern_boost -ID:\work\build\Paddle\build\third_party\eigen3\src\extern_eigen3 -ID:\work\build\Paddle\build\third_party\threadpool\src\extern_threadpool -ID:\work\build\Paddle\build\third_party\dlpack\src\extern_dlpack\include -ID:\work\build\Paddle\build\third_party\install\xxhash\include -ID:\work\build\Paddle\build\third_party\install\warpctc\include -ID:\work\build\Paddle\build\third_party\install\mklml\include -ID:\work\build\Paddle\build\third_party\install\mkldnn\include -ID:\work\build\Paddle\build\third_party\install\protobuf\include -IC:\Users\Gaotiantian\anaconda3\include -I""C:\Users\Gaotiantian\anaconda3\Lib\site-packages\numpy\core\include"" -ID:\work\build\Paddle\build\third_party\pybind\src\extern_pybind\include -ID:\work\build\Paddle\build\third_party\cub\src\extern_cub -ID:\work\build\Paddle\build\third_party\install\cryptopp\include -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include"" -ID:\work\build\Paddle -ID:\work\build\Paddle\build\..\paddle\fluid\framework\io -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include""     --keep-dir x64\Release -maxrregcount=0  --machine 64 --compile -cudart static -Wno-deprecated-gpu-targets -w --expt-relaxed-constexpr --expt-extended-lambda -O3 -Xcompiler=""/EHsc /wd4244 /wd4267 /wd4819 /bigobj /arch:AVX""    -D_WINDOWS -D_MWAITXINTRIN_H_INCLUDED -D__STRICT_ANSI__ -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -DWIN32 -D_WINDOWS -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -D_MBCS -Xcompiler ""/EHsc /W0 /nologo /O2 /FS  /MT /GR"" -o concat_and_split.dir\Release\/concat_and_split.cu.obj ""D:\work\build\Paddle\paddle\fluid\operators\math\concat_and_split.cu""
48>  nvcc fatal   : A single input file is required for a non-link phase when an outputfile is specified
48>C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\BuildCustomizations\CUDA 10.2.targets(764,9): error MSB3721: 命令“""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\bin\nvcc.exe"" -gencode=arch=compute_35,code=\""sm_35,compute_35\"" -gencode=arch=compute_50,code=\""sm_50,compute_50\"" -gencode=arch=compute_52,code=\""sm_52,compute_52\"" -gencode=arch=compute_60,code=\""sm_60,compute_60\"" -gencode=arch=compute_61,code=\""sm_61,compute_61\"" -gencode=arch=compute_70,code=\""sm_70,compute_70\"" -gencode=arch=compute_75,code=\""sm_75,compute_75\"" --use-local-env 2015 -ccbin ""D:\VS2015\VC\bin\x86_amd64"" -x cu  -ID:\work\build\Paddle\build -ID:\work\build\Paddle\paddle\fluid\framework\io -I""D:\3rdparty\TensorRT-7.0.0.11\include"" -ID:\work\build\Paddle\build\third_party\install\zlib\include -ID:\work\build\Paddle\build\third_party\install -ID:\work\build\Paddle\build\third_party\install\gflags\include -ID:\work\build\Paddle\build\third_party\install\glog\include -ID:\work\build\Paddle\build\third_party\boost\src\extern_boost -ID:\work\build\Paddle\build\third_party\eigen3\src\extern_eigen3 -ID:\work\build\Paddle\build\third_party\threadpool\src\extern_threadpool -ID:\work\build\Paddle\build\third_party\dlpack\src\extern_dlpack\include -ID:\work\build\Paddle\build\third_party\install\xxhash\include -ID:\work\build\Paddle\build\third_party\install\warpctc\include -ID:\work\build\Paddle\build\third_party\install\mklml\include -ID:\work\build\Paddle\build\third_party\install\mkldnn\include -ID:\work\build\Paddle\build\third_party\install\protobuf\include -IC:\Users\Gaotiantian\anaconda3\include -I""C:\Users\Gaotiantian\anaconda3\Lib\site-packages\numpy\core\include"" -ID:\work\build\Paddle\build\third_party\pybind\src\extern_pybind\include -ID:\work\build\Paddle\build\third_party\cub\src\extern_cub -ID:\work\build\Paddle\build\third_party\install\cryptopp\include -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include"" -ID:\work\build\Paddle -ID:\work\build\Paddle\build\..\paddle\fluid\framework\io -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include""     --keep-dir x64\Release -maxrregcount=0  --machine 64 --compile -cudart static -Wno-deprecated-gpu-targets -w --expt-relaxed-constexpr --expt-extended-lambda -O3 -Xcompiler=""/EHsc /wd4244 /wd4267 /wd4819 /bigobj /arch:AVX""    -D_WINDOWS -D_MWAITXINTRIN_H_INCLUDED -D__STRICT_ANSI__ -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -DWIN32 -D_WINDOWS -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -D_MBCS -Xcompiler ""/EHsc /W0 /nologo /O2 /FS  /MT /GR"" -o concat_and_split.dir\Release\/concat_and_split.cu.obj ""D:\work\build\Paddle\paddle\fluid\operators\math\concat_and_split.cu""”已退出，返回代码为 1。
41>
41>  D:\work\build\Paddle\build\paddle\fluid\operators\math>""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\bin\nvcc.exe"" -gencode=arch=compute_35,code=\""sm_35,compute_35\"" -gencode=arch=compute_50,code=\""sm_50,compute_50\"" -gencode=arch=compute_52,code=\""sm_52,compute_52\"" -gencode=arch=compute_60,code=\""sm_60,compute_60\"" -gencode=arch=compute_61,code=\""sm_61,compute_61\"" -gencode=arch=compute_70,code=\""sm_70,compute_70\"" -gencode=arch=compute_75,code=\""sm_75,compute_75\"" --use-local-env 2015 -ccbin ""D:\VS2015\VC\bin\x86_amd64"" -x cu  -ID:\work\build\Paddle\build -ID:\work\build\Paddle\paddle\fluid\framework\io -I""D:\3rdparty\TensorRT-7.0.0.11\include"" -ID:\work\build\Paddle\build\third_party\install\zlib\include -ID:\work\build\Paddle\build\third_party\install -ID:\work\build\Paddle\build\third_party\install\gflags\include -ID:\work\build\Paddle\build\third_party\install\glog\include -ID:\work\build\Paddle\build\third_party\boost\src\extern_boost -ID:\work\build\Paddle\build\third_party\eigen3\src\extern_eigen3 -ID:\work\build\Paddle\build\third_party\threadpool\src\extern_threadpool -ID:\work\build\Paddle\build\third_party\dlpack\src\extern_dlpack\include -ID:\work\build\Paddle\build\third_party\install\xxhash\include -ID:\work\build\Paddle\build\third_party\install\warpctc\include -ID:\work\build\Paddle\build\third_party\install\mklml\include -ID:\work\build\Paddle\build\third_party\install\mkldnn\include -ID:\work\build\Paddle\build\third_party\install\protobuf\include -IC:\Users\Gaotiantian\anaconda3\include -I""C:\Users\Gaotiantian\anaconda3\Lib\site-packages\numpy\core\include"" -ID:\work\build\Paddle\build\third_party\pybind\src\extern_pybind\include -ID:\work\build\Paddle\build\third_party\cub\src\extern_cub -ID:\work\build\Paddle\build\third_party\install\cryptopp\include -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include"" -ID:\work\build\Paddle -ID:\work\build\Paddle\build\..\paddle\fluid\framework\io -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include""     --keep-dir x64\Release -maxrregcount=0  --machine 64 --compile -cudart static -Wno-deprecated-gpu-targets -w --expt-relaxed-constexpr --expt-extended-lambda -O3 -Xcompiler=""/EHsc /wd4244 /wd4267 /wd4819 /bigobj /arch:AVX""    -D_WINDOWS -D_MWAITXINTRIN_H_INCLUDED -D__STRICT_ANSI__ -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -DWIN32 -D_WINDOWS -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -D_MBCS -Xcompiler ""/EHsc /W0 /nologo /O2 /FS  /MT /GR"" -o depthwise_conv.dir\Release\depthwise_conv.obj ""D:\work\build\Paddle\paddle\fluid\operators\math\depthwise_conv.cu""
41>  nvcc fatal   : A single input file is required for a non-link phase when an outputfile is specified
41>C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\BuildCustomizations\CUDA 10.2.targets(764,9): error MSB3721: 命令“""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\bin\nvcc.exe"" -gencode=arch=compute_35,code=\""sm_35,compute_35\"" -gencode=arch=compute_50,code=\""sm_50,compute_50\"" -gencode=arch=compute_52,code=\""sm_52,compute_52\"" -gencode=arch=compute_60,code=\""sm_60,compute_60\"" -gencode=arch=compute_61,code=\""sm_61,compute_61\"" -gencode=arch=compute_70,code=\""sm_70,compute_70\"" -gencode=arch=compute_75,code=\""sm_75,compute_75\"" --use-local-env 2015 -ccbin ""D:\VS2015\VC\bin\x86_amd64"" -x cu  -ID:\work\build\Paddle\build -ID:\work\build\Paddle\paddle\fluid\framework\io -I""D:\3rdparty\TensorRT-7.0.0.11\include"" -ID:\work\build\Paddle\build\third_party\install\zlib\include -ID:\work\build\Paddle\build\third_party\install -ID:\work\build\Paddle\build\third_party\install\gflags\include -ID:\work\build\Paddle\build\third_party\install\glog\include -ID:\work\build\Paddle\build\third_party\boost\src\extern_boost -ID:\work\build\Paddle\build\third_party\eigen3\src\extern_eigen3 -ID:\work\build\Paddle\build\third_party\threadpool\src\extern_threadpool -ID:\work\build\Paddle\build\third_party\dlpack\src\extern_dlpack\include -ID:\work\build\Paddle\build\third_party\install\xxhash\include -ID:\work\build\Paddle\build\third_party\install\warpctc\include -ID:\work\build\Paddle\build\third_party\install\mklml\include -ID:\work\build\Paddle\build\third_party\install\mkldnn\include -ID:\work\build\Paddle\build\third_party\install\protobuf\include -IC:\Users\Gaotiantian\anaconda3\include -I""C:\Users\Gaotiantian\anaconda3\Lib\site-packages\numpy\core\include"" -ID:\work\build\Paddle\build\third_party\pybind\src\extern_pybind\include -ID:\work\build\Paddle\build\third_party\cub\src\extern_cub -ID:\work\build\Paddle\build\third_party\install\cryptopp\include -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include"" -ID:\work\build\Paddle -ID:\work\build\Paddle\build\..\paddle\fluid\framework\io -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include""     --keep-dir x64\Release -maxrregcount=0  --machine 64 --compile -cudart static -Wno-deprecated-gpu-targets -w --expt-relaxed-constexpr --expt-extended-lambda -O3 -Xcompiler=""/EHsc /wd4244 /wd4267 /wd4819 /bigobj /arch:AVX""    -D_WINDOWS -D_MWAITXINTRIN_H_INCLUDED -D__STRICT_ANSI__ -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -DWIN32 -D_WINDOWS -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -D_MBCS -Xcompiler ""/EHsc /W0 /nologo /O2 /FS  /MT /GR"" -o depthwise_conv.dir\Release\depthwise_conv.obj ""D:\work\build\Paddle\paddle\fluid\operators\math\depthwise_conv.cu""”已退出，返回代码为 1。
51>------ 已启动生成: 项目: sequence2batch, 配置: Release x64 ------
44>
40>  Compiling CUDA source file ..\..\..\..\..\paddle\fluid\operators\math\im2col.cu...
44>  D:\work\build\Paddle\build\paddle\fluid\operators\math>""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\bin\nvcc.exe"" -gencode=arch=compute_35,code=\""sm_35,compute_35\"" -gencode=arch=compute_50,code=\""sm_50,compute_50\"" -gencode=arch=compute_52,code=\""sm_52,compute_52\"" -gencode=arch=compute_60,code=\""sm_60,compute_60\"" -gencode=arch=compute_61,code=\""sm_61,compute_61\"" -gencode=arch=compute_70,code=\""sm_70,compute_70\"" -gencode=arch=compute_75,code=\""sm_75,compute_75\"" --use-local-env 2015 -ccbin ""D:\VS2015\VC\bin\x86_amd64"" -x cu  -ID:\work\build\Paddle\build -ID:\work\build\Paddle\paddle\fluid\framework\io -I""D:\3rdparty\TensorRT-7.0.0.11\include"" -ID:\work\build\Paddle\build\third_party\install\zlib\include -ID:\work\build\Paddle\build\third_party\install -ID:\work\build\Paddle\build\third_party\install\gflags\include -ID:\work\build\Paddle\build\third_party\install\glog\include -ID:\work\build\Paddle\build\third_party\boost\src\extern_boost -ID:\work\build\Paddle\build\third_party\eigen3\src\extern_eigen3 -ID:\work\build\Paddle\build\third_party\threadpool\src\extern_threadpool -ID:\work\build\Paddle\build\third_party\dlpack\src\extern_dlpack\include -ID:\work\build\Paddle\build\third_party\install\xxhash\include -ID:\work\build\Paddle\build\third_party\install\warpctc\include -ID:\work\build\Paddle\build\third_party\install\mklml\include -ID:\work\build\Paddle\build\third_party\install\mkldnn\include -ID:\work\build\Paddle\build\third_party\install\protobuf\include -IC:\Users\Gaotiantian\anaconda3\include -I""C:\Users\Gaotiantian\anaconda3\Lib\site-packages\numpy\core\include"" -ID:\work\build\Paddle\build\third_party\pybind\src\extern_pybind\include -ID:\work\build\Paddle\build\third_party\cub\src\extern_cub -ID:\work\build\Paddle\build\third_party\install\cryptopp\include -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include"" -ID:\work\build\Paddle -ID:\work\build\Paddle\build\..\paddle\fluid\framework\io -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include""     --keep-dir x64\Release -maxrregcount=0  --machine 64 --compile -cudart static -Wno-deprecated-gpu-targets -w --expt-relaxed-constexpr --expt-extended-lambda -O3 -Xcompiler=""/EHsc /wd4244 /wd4267 /wd4819 /bigobj /arch:AVX""    -D_WINDOWS -D_MWAITXINTRIN_H_INCLUDED -D__STRICT_ANSI__ -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -DWIN32 -D_WINDOWS -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -D_MBCS -Xcompiler ""/EHsc /W0 /nologo /O2 /FS  /MT /GR"" -o prelu.dir\Release\prelu.obj ""D:\work\build\Paddle\paddle\fluid\operators\math\prelu.cu""
44>  nvcc fatal   : A single input file is required for a non-link phase when an outputfile is specified
44>C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\BuildCustomizations\CUDA 10.2.targets(764,9): error MSB3721: 命令“""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\bin\nvcc.exe"" -gencode=arch=compute_35,code=\""sm_35,compute_35\"" -gencode=arch=compute_50,code=\""sm_50,compute_50\"" -gencode=arch=compute_52,code=\""sm_52,compute_52\"" -gencode=arch=compute_60,code=\""sm_60,compute_60\"" -gencode=arch=compute_61,code=\""sm_61,compute_61\"" -gencode=arch=compute_70,code=\""sm_70,compute_70\"" -gencode=arch=compute_75,code=\""sm_75,compute_75\"" --use-local-env 2015 -ccbin ""D:\VS2015\VC\bin\x86_amd64"" -x cu  -ID:\work\build\Paddle\build -ID:\work\build\Paddle\paddle\fluid\framework\io -I""D:\3rdparty\TensorRT-7.0.0.11\include"" -ID:\work\build\Paddle\build\third_party\install\zlib\include -ID:\work\build\Paddle\build\third_party\install -ID:\work\build\Paddle\build\third_party\install\gflags\include -ID:\work\build\Paddle\build\third_party\install\glog\include -ID:\work\build\Paddle\build\third_party\boost\src\extern_boost -ID:\work\build\Paddle\build\third_party\eigen3\src\extern_eigen3 -ID:\work\build\Paddle\build\third_party\threadpool\src\extern_threadpool -ID:\work\build\Paddle\build\third_party\dlpack\src\extern_dlpack\include -ID:\work\build\Paddle\build\third_party\install\xxhash\include -ID:\work\build\Paddle\build\third_party\install\warpctc\include -ID:\work\build\Paddle\build\third_party\install\mklml\include -ID:\work\build\Paddle\build\third_party\install\mkldnn\include -ID:\work\build\Paddle\build\third_party\install\protobuf\include -IC:\Users\Gaotiantian\anaconda3\include -I""C:\Users\Gaotiantian\anaconda3\Lib\site-packages\numpy\core\include"" -ID:\work\build\Paddle\build\third_party\pybind\src\extern_pybind\include -ID:\work\build\Paddle\build\third_party\cub\src\extern_cub -ID:\work\build\Paddle\build\third_party\install\cryptopp\include -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include"" -ID:\work\build\Paddle -ID:\work\build\Paddle\build\..\paddle\fluid\framework\io -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include""     --keep-dir x64\Release -maxrregcount=0  --machine 64 --compile -cudart static -Wno-deprecated-gpu-targets -w --expt-relaxed-constexpr --expt-extended-lambda -O3 -Xcompiler=""/EHsc /wd4244 /wd4267 /wd4819 /bigobj /arch:AVX""    -D_WINDOWS -D_MWAITXINTRIN_H_INCLUDED -D__STRICT_ANSI__ -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -DWIN32 -D_WINDOWS -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -D_MBCS -Xcompiler ""/EHsc /W0 /nologo /O2 /FS  /MT /GR"" -o prelu.dir\Release\prelu.obj ""D:\work\build\Paddle\paddle\fluid\operators\math\prelu.cu""”已退出，返回代码为 1。
52>------ 已启动生成: 项目: sequence_padding, 配置: Release x64 ------
43>
42>
46>
43>  D:\work\build\Paddle\build\paddle\fluid\operators\math>""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\bin\nvcc.exe"" -gencode=arch=compute_35,code=\""sm_35,compute_35\"" -gencode=arch=compute_50,code=\""sm_50,compute_50\"" -gencode=arch=compute_52,code=\""sm_52,compute_52\"" -gencode=arch=compute_60,code=\""sm_60,compute_60\"" -gencode=arch=compute_61,code=\""sm_61,compute_61\"" -gencode=arch=compute_70,code=\""sm_70,compute_70\"" -gencode=arch=compute_75,code=\""sm_75,compute_75\"" --use-local-env 2015 -ccbin ""D:\VS2015\VC\bin\x86_amd64"" -x cu  -ID:\work\build\Paddle\build -ID:\work\build\Paddle\paddle\fluid\framework\io -I""D:\3rdparty\TensorRT-7.0.0.11\include"" -ID:\work\build\Paddle\build\third_party\install\zlib\include -ID:\work\build\Paddle\build\third_party\install -ID:\work\build\Paddle\build\third_party\install\gflags\include -ID:\work\build\Paddle\build\third_party\install\glog\include -ID:\work\build\Paddle\build\third_party\boost\src\extern_boost -ID:\work\build\Paddle\build\third_party\eigen3\src\extern_eigen3 -ID:\work\build\Paddle\build\third_party\threadpool\src\extern_threadpool -ID:\work\build\Paddle\build\third_party\dlpack\src\extern_dlpack\include -ID:\work\build\Paddle\build\third_party\install\xxhash\include -ID:\work\build\Paddle\build\third_party\install\warpctc\include -ID:\work\build\Paddle\build\third_party\install\mklml\include -ID:\work\build\Paddle\build\third_party\install\mkldnn\include -ID:\work\build\Paddle\build\third_party\install\protobuf\include -IC:\Users\Gaotiantian\anaconda3\include -I""C:\Users\Gaotiantian\anaconda3\Lib\site-packages\numpy\core\include"" -ID:\work\build\Paddle\build\third_party\pybind\src\extern_pybind\include -ID:\work\build\Paddle\build\third_party\cub\src\extern_cub -ID:\work\build\Paddle\build\third_party\install\cryptopp\include -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include"" -ID:\work\build\Paddle -ID:\work\build\Paddle\build\..\paddle\fluid\framework\io -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include""     --keep-dir x64\Release -maxrregcount=0  --machine 64 --compile -cudart static -Wno-deprecated-gpu-targets -w --expt-relaxed-constexpr --expt-extended-lambda -O3 -Xcompiler=""/EHsc /wd4244 /wd4267 /wd4819 /bigobj /arch:AVX""    -D_WINDOWS -D_MWAITXINTRIN_H_INCLUDED -D__STRICT_ANSI__ -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -DWIN32 -D_WINDOWS -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -D_MBCS -Xcompiler ""/EHsc /W0 /nologo /O2 /FS  /MT /GR"" -o pooling.dir\Release\/pooling.cu.obj ""D:\work\build\Paddle\paddle\fluid\operators\math\pooling.cu""
46>  D:\work\build\Paddle\build\paddle\fluid\operators\math>""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\bin\nvcc.exe"" -gencode=arch=compute_35,code=\""sm_35,compute_35\"" -gencode=arch=compute_50,code=\""sm_50,compute_50\"" -gencode=arch=compute_52,code=\""sm_52,compute_52\"" -gencode=arch=compute_60,code=\""sm_60,compute_60\"" -gencode=arch=compute_61,code=\""sm_61,compute_61\"" -gencode=arch=compute_70,code=\""sm_70,compute_70\"" -gencode=arch=compute_75,code=\""sm_75,compute_75\"" --use-local-env 2015 -ccbin ""D:\VS2015\VC\bin\x86_amd64"" -x cu  -ID:\work\build\Paddle\build -ID:\work\build\Paddle\paddle\fluid\framework\io -I""D:\3rdparty\TensorRT-7.0.0.11\include"" -ID:\work\build\Paddle\build\third_party\install\zlib\include -ID:\work\build\Paddle\build\third_party\install -ID:\work\build\Paddle\build\third_party\install\gflags\include -ID:\work\build\Paddle\build\third_party\install\glog\include -ID:\work\build\Paddle\build\third_party\boost\src\extern_boost -ID:\work\build\Paddle\build\third_party\eigen3\src\extern_eigen3 -ID:\work\build\Paddle\build\third_party\threadpool\src\extern_threadpool -ID:\work\build\Paddle\build\third_party\dlpack\src\extern_dlpack\include -ID:\work\build\Paddle\build\third_party\install\xxhash\include -ID:\work\build\Paddle\build\third_party\install\warpctc\include -ID:\work\build\Paddle\build\third_party\install\mklml\include -ID:\work\build\Paddle\build\third_party\install\mkldnn\include -ID:\work\build\Paddle\build\third_party\install\protobuf\include -IC:\Users\Gaotiantian\anaconda3\include -I""C:\Users\Gaotiantian\anaconda3\Lib\site-packages\numpy\core\include"" -ID:\work\build\Paddle\build\third_party\pybind\src\extern_pybind\include -ID:\work\build\Paddle\build\third_party\cub\src\extern_cub -ID:\work\build\Paddle\build\third_party\install\cryptopp\include -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include"" -ID:\work\build\Paddle -ID:\work\build\Paddle\build\..\paddle\fluid\framework\io -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include""     --keep-dir x64\Release -maxrregcount=0  --machine 64 --compile -cudart static -Wno-deprecated-gpu-targets -w --expt-relaxed-constexpr --expt-extended-lambda -O3 -Xcompiler=""/EHsc /wd4244 /wd4267 /wd4819 /bigobj /arch:AVX""    -D_WINDOWS -D_MWAITXINTRIN_H_INCLUDED -D__STRICT_ANSI__ -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -DWIN32 -D_WINDOWS -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -D_MBCS -Xcompiler ""/EHsc /W0 /nologo /O2 /FS  /MT /GR"" -o bert_encoder_functor.dir\Release\bert_encoder_functor.obj ""D:\work\build\Paddle\paddle\fluid\operators\math\bert_encoder_functor.cu""
42>  D:\work\build\Paddle\build\paddle\fluid\operators\math>""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\bin\nvcc.exe"" -gencode=arch=compute_35,code=\""sm_35,compute_35\"" -gencode=arch=compute_50,code=\""sm_50,compute_50\"" -gencode=arch=compute_52,code=\""sm_52,compute_52\"" -gencode=arch=compute_60,code=\""sm_60,compute_60\"" -gencode=arch=compute_61,code=\""sm_61,compute_61\"" -gencode=arch=compute_70,code=\""sm_70,compute_70\"" -gencode=arch=compute_75,code=\""sm_75,compute_75\"" --use-local-env 2015 -ccbin ""D:\VS2015\VC\bin\x86_amd64"" -x cu  -ID:\work\build\Paddle\build -ID:\work\build\Paddle\paddle\fluid\framework\io -I""D:\3rdparty\TensorRT-7.0.0.11\include"" -ID:\work\build\Paddle\build\third_party\install\zlib\include -ID:\work\build\Paddle\build\third_party\install -ID:\work\build\Paddle\build\third_party\install\gflags\include -ID:\work\build\Paddle\build\third_party\install\glog\include -ID:\work\build\Paddle\build\third_party\boost\src\extern_boost -ID:\work\build\Paddle\build\third_party\eigen3\src\extern_eigen3 -ID:\work\build\Paddle\build\third_party\threadpool\src\extern_threadpool -ID:\work\build\Paddle\build\third_party\dlpack\src\extern_dlpack\include -ID:\work\build\Paddle\build\third_party\install\xxhash\include -ID:\work\build\Paddle\build\third_party\install\warpctc\include -ID:\work\build\Paddle\build\third_party\install\mklml\include -ID:\work\build\Paddle\build\third_party\install\mkldnn\include -ID:\work\build\Paddle\build\third_party\install\protobuf\include -IC:\Users\Gaotiantian\anaconda3\include -I""C:\Users\Gaotiantian\anaconda3\Lib\site-packages\numpy\core\include"" -ID:\work\build\Paddle\build\third_party\pybind\src\extern_pybind\include -ID:\work\build\Paddle\build\third_party\cub\src\extern_cub -ID:\work\build\Paddle\build\third_party\install\cryptopp\include -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include"" -ID:\work\build\Paddle -ID:\work\build\Paddle\build\..\paddle\fluid\framework\io -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include""     --keep-dir x64\Release -maxrregcount=0  --machine 64 --compile -cudart static -Wno-deprecated-gpu-targets -w --expt-relaxed-constexpr --expt-extended-lambda -O3 -Xcompiler=""/EHsc /wd4244 /wd4267 /wd4819 /bigobj /arch:AVX""    -D_WINDOWS -D_MWAITXINTRIN_H_INCLUDED -D__STRICT_ANSI__ -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -DWIN32 -D_WINDOWS -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -D_MBCS -Xcompiler ""/EHsc /W0 /nologo /O2 /FS  /MT /GR"" -o vol2col.dir\Release\/vol2col.cu.obj ""D:\work\build\Paddle\paddle\fluid\operators\math\vol2col.cu""
46>  nvcc fatal   : A single input file is required for a non-link phase when an outputfile is specified
43>  nvcc fatal   : A single input file is required for a non-link phase when an outputfile is specified
47>d:\work\build\paddle\build\third_party\eigen3\src\extern_eigen3\eigen\src/Core/ArithmeticSequence.h(345): fatal error C1001: 编译器中发生内部错误。
42>  nvcc fatal   : A single input file is required for a non-link phase when an outputfile is specified
43>C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\BuildCustomizations\CUDA 10.2.targets(764,9): error MSB3721: 命令“""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\bin\nvcc.exe"" -gencode=arch=compute_35,code=\""sm_35,compute_35\"" -gencode=arch=compute_50,code=\""sm_50,compute_50\"" -gencode=arch=compute_52,code=\""sm_52,compute_52\"" -gencode=arch=compute_60,code=\""sm_60,compute_60\"" -gencode=arch=compute_61,code=\""sm_61,compute_61\"" -gencode=arch=compute_70,code=\""sm_70,compute_70\"" -gencode=arch=compute_75,code=\""sm_75,compute_75\"" --use-local-env 2015 -ccbin ""D:\VS2015\VC\bin\x86_amd64"" -x cu  -ID:\work\build\Paddle\build -ID:\work\build\Paddle\paddle\fluid\framework\io -I""D:\3rdparty\TensorRT-7.0.0.11\include"" -ID:\work\build\Paddle\build\third_party\install\zlib\include -ID:\work\build\Paddle\build\third_party\install -ID:\work\build\Paddle\build\third_party\install\gflags\include -ID:\work\build\Paddle\build\third_party\install\glog\include -ID:\work\build\Paddle\build\third_party\boost\src\extern_boost -ID:\work\build\Paddle\build\third_party\eigen3\src\extern_eigen3 -ID:\work\build\Paddle\build\third_party\threadpool\src\extern_threadpool -ID:\work\build\Paddle\build\third_party\dlpack\src\extern_dlpack\include -ID:\work\build\Paddle\build\third_party\install\xxhash\include -ID:\work\build\Paddle\build\third_party\install\warpctc\include -ID:\work\build\Paddle\build\third_party\install\mklml\include -ID:\work\build\Paddle\build\third_party\install\mkldnn\include -ID:\work\build\Paddle\build\third_party\install\protobuf\include -IC:\Users\Gaotiantian\anaconda3\include -I""C:\Users\Gaotiantian\anaconda3\Lib\site-packages\numpy\core\include"" -ID:\work\build\Paddle\build\third_party\pybind\src\extern_pybind\include -ID:\work\build\Paddle\build\third_party\cub\src\extern_cub -ID:\work\build\Paddle\build\third_party\install\cryptopp\include -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include"" -ID:\work\build\Paddle -ID:\work\build\Paddle\build\..\paddle\fluid\framework\io -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include""     --keep-dir x64\Release -maxrregcount=0  --machine 64 --compile -cudart static -Wno-deprecated-gpu-targets -w --expt-relaxed-constexpr --expt-extended-lambda -O3 -Xcompiler=""/EHsc /wd4244 /wd4267 /wd4819 /bigobj /arch:AVX""    -D_WINDOWS -D_MWAITXINTRIN_H_INCLUDED -D__STRICT_ANSI__ -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -DWIN32 -D_WINDOWS -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -D_MBCS -Xcompiler ""/EHsc /W0 /nologo /O2 /FS  /MT /GR"" -o pooling.dir\Release\/pooling.cu.obj ""D:\work\build\Paddle\paddle\fluid\operators\math\pooling.cu""”已退出，返回代码为 1。
47>  (编译器文件“f:\dd\vctools\compiler\cxxfe\sl\p1\c\p0gettok.c”，第 6502 行)
42>C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\BuildCustomizations\CUDA 10.2.targets(764,9): error MSB3721: 命令“""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\bin\nvcc.exe"" -gencode=arch=compute_35,code=\""sm_35,compute_35\"" -gencode=arch=compute_50,code=\""sm_50,compute_50\"" -gencode=arch=compute_52,code=\""sm_52,compute_52\"" -gencode=arch=compute_60,code=\""sm_60,compute_60\"" -gencode=arch=compute_61,code=\""sm_61,compute_61\"" -gencode=arch=compute_70,code=\""sm_70,compute_70\"" -gencode=arch=compute_75,code=\""sm_75,compute_75\"" --use-local-env 2015 -ccbin ""D:\VS2015\VC\bin\x86_amd64"" -x cu  -ID:\work\build\Paddle\build -ID:\work\build\Paddle\paddle\fluid\framework\io -I""D:\3rdparty\TensorRT-7.0.0.11\include"" -ID:\work\build\Paddle\build\third_party\install\zlib\include -ID:\work\build\Paddle\build\third_party\install -ID:\work\build\Paddle\build\third_party\install\gflags\include -ID:\work\build\Paddle\build\third_party\install\glog\include -ID:\work\build\Paddle\build\third_party\boost\src\extern_boost -ID:\work\build\Paddle\build\third_party\eigen3\src\extern_eigen3 -ID:\work\build\Paddle\build\third_party\threadpool\src\extern_threadpool -ID:\work\build\Paddle\build\third_party\dlpack\src\extern_dlpack\include -ID:\work\build\Paddle\build\third_party\install\xxhash\include -ID:\work\build\Paddle\build\third_party\install\warpctc\include -ID:\work\build\Paddle\build\third_party\install\mklml\include -ID:\work\build\Paddle\build\third_party\install\mkldnn\include -ID:\work\build\Paddle\build\third_party\install\protobuf\include -IC:\Users\Gaotiantian\anaconda3\include -I""C:\Users\Gaotiantian\anaconda3\Lib\site-packages\numpy\core\include"" -ID:\work\build\Paddle\build\third_party\pybind\src\extern_pybind\include -ID:\work\build\Paddle\build\third_party\cub\src\extern_cub -ID:\work\build\Paddle\build\third_party\install\cryptopp\include -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include"" -ID:\work\build\Paddle -ID:\work\build\Paddle\build\..\paddle\fluid\framework\io -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include""     --keep-dir x64\Release -maxrregcount=0  --machine 64 --compile -cudart static -Wno-deprecated-gpu-targets -w --expt-relaxed-constexpr --expt-extended-lambda -O3 -Xcompiler=""/EHsc /wd4244 /wd4267 /wd4819 /bigobj /arch:AVX""    -D_WINDOWS -D_MWAITXINTRIN_H_INCLUDED -D__STRICT_ANSI__ -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -DWIN32 -D_WINDOWS -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -D_MBCS -Xcompiler ""/EHsc /W0 /nologo /O2 /FS  /MT /GR"" -o vol2col.dir\Release\/vol2col.cu.obj ""D:\work\build\Paddle\paddle\fluid\operators\math\vol2col.cu""”已退出，返回代码为 1。
46>C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\BuildCustomizations\CUDA 10.2.targets(764,9): error MSB3721: 命令“""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\bin\nvcc.exe"" -gencode=arch=compute_35,code=\""sm_35,compute_35\"" -gencode=arch=compute_50,code=\""sm_50,compute_50\"" -gencode=arch=compute_52,code=\""sm_52,compute_52\"" -gencode=arch=compute_60,code=\""sm_60,compute_60\"" -gencode=arch=compute_61,code=\""sm_61,compute_61\"" -gencode=arch=compute_70,code=\""sm_70,compute_70\"" -gencode=arch=compute_75,code=\""sm_75,compute_75\"" --use-local-env 2015 -ccbin ""D:\VS2015\VC\bin\x86_amd64"" -x cu  -ID:\work\build\Paddle\build -ID:\work\build\Paddle\paddle\fluid\framework\io -I""D:\3rdparty\TensorRT-7.0.0.11\include"" -ID:\work\build\Paddle\build\third_party\install\zlib\include -ID:\work\build\Paddle\build\third_party\install -ID:\work\build\Paddle\build\third_party\install\gflags\include -ID:\work\build\Paddle\build\third_party\install\glog\include -ID:\work\build\Paddle\build\third_party\boost\src\extern_boost -ID:\work\build\Paddle\build\third_party\eigen3\src\extern_eigen3 -ID:\work\build\Paddle\build\third_party\threadpool\src\extern_threadpool -ID:\work\build\Paddle\build\third_party\dlpack\src\extern_dlpack\include -ID:\work\build\Paddle\build\third_party\install\xxhash\include -ID:\work\build\Paddle\build\third_party\install\warpctc\include -ID:\work\build\Paddle\build\third_party\install\mklml\include -ID:\work\build\Paddle\build\third_party\install\mkldnn\include -ID:\work\build\Paddle\build\third_party\install\protobuf\include -IC:\Users\Gaotiantian\anaconda3\include -I""C:\Users\Gaotiantian\anaconda3\Lib\site-packages\numpy\core\include"" -ID:\work\build\Paddle\build\third_party\pybind\src\extern_pybind\include -ID:\work\build\Paddle\build\third_party\cub\src\extern_cub -ID:\work\build\Paddle\build\third_party\install\cryptopp\include -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include"" -ID:\work\build\Paddle -ID:\work\build\Paddle\build\..\paddle\fluid\framework\io -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include""     --keep-dir x64\Release -maxrregcount=0  --machine 64 --compile -cudart static -Wno-deprecated-gpu-targets -w --expt-relaxed-constexpr --expt-extended-lambda -O3 -Xcompiler=""/EHsc /wd4244 /wd4267 /wd4819 /bigobj /arch:AVX""    -D_WINDOWS -D_MWAITXINTRIN_H_INCLUDED -D__STRICT_ANSI__ -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -DWIN32 -D_WINDOWS -DNDEBUG -DTRT_PLUGIN_FP16_AVALIABLE -D""CUDA_VERSION_MAJOR=\""10\"""" -D""CUDA_VERSION_MINOR=\""2\"""" -D""CUDA_TOOLKIT_ROOT_DIR=\""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"""" -D""CUDNN_MAJOR_VERSION=\""7\"""" -DPADDLE_WITH_TENSORRT -DGOOGLE_GLOG_DLL_DECL= -DBOOST_HAS_STATIC_ASSERT -DEIGEN_STRONG_INLINE=inline -DPADDLE_WITH_MKLML -DLAPACK_FOUND -DPADDLE_WITH_MKLDNN -DPADDLE_WITH_CRYPTO -DPADDLE_VERSION=0.0.0 -DPADDLE_VERSION_INTEGER=0 -DPADDLE_DISABLE_PROFILER -DPADDLE_WITH_AVX -DPADDLE_WITH_SSE3 -D_XKEYCHECK_H -DPADDLE_DLL_INFERENCE -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -DPADDLE_ON_INFERENCE -DPADDLE_DLL_EXPORT -D""CMAKE_INTDIR=\""Release\"""" -D_MBCS -Xcompiler ""/EHsc /W0 /nologo /O2 /FS  /MT /GR"" -o bert_encoder_functor.dir\Release\bert_encoder_functor.obj ""D:\work\build\Paddle\paddle\fluid\operators\math\bert_encoder_functor.cu""”已退出，返回代码为 1。
"
包含背景类别的数据准备及训练,PaddlePaddle/PaddleDetection,2021-06-15 01:37:53,2,,3383,920904095,"你好，我想请教一下，包含背景类别的VOC格式数据，label_list.txt中是否需要添加背景类别名字？
是否需要建立背景类的.xml文件？如果建立.xml文件的话，里面的内容是什么样的？包含背景类别
时生成的train.txt中包含背景类别.jpg和.xml文件的路径名吗？"
optimizer中优化器支持哪几种？只有默认的Momentum优化器是正常的。,PaddlePaddle/PaddleDetection,2021-06-14 09:44:04,4,,3380,920235144,optimizer中优化器支持哪几种？只有默认的Momentum优化器是正常的。将.yml配置文件中的优化器改成Adam会出现训练正常但评估过程“卡住”，不报错就一直卡在评估过程中。
安装PaddleDetection时报错,PaddlePaddle/PaddleDetection,2021-06-11 07:52:04,7,install,3367,918368064,"我用的远程服务器，系统Ubuntu，cudn10.2-cudnn7.6.5，anacinda下的python3.8，按照教程成功安装PaddlePaddle，但在安装PaddleDetection时，通过pip安装，在终端执行pip install paddledet==2.1.0 -i https://mirror.baidu.com/pypi/simple时报错，如下：
```
Building wheels for collected packages: lap, pycocotools
  Building wheel for lap (setup.py) ... error
  ERROR: Command errored out with exit status 1:
   command: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-1774b_rx/lap_4f1fc1cf14b4477d8a97c3d283d0ef8e/setup.py'""'""'; __file__='""'""'/tmp/pip-install-1774b_rx/lap_4f1fc1cf14b4477d8a97c3d283d0ef8e/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-zfajpco9
       cwd: /tmp/pip-install-1774b_rx/lap_4f1fc1cf14b4477d8a97c3d283d0ef8e/
  Complete output (36 lines):
  Partial import of lap during the build process.
  Generating cython files
  running bdist_wheel
  running build
  running config_cc
  unifing config_cc, config, build_clib, build_ext, build commands --compiler options
  running config_fc
  unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options
  running build_src
  build_src
  building extension ""lap._lapjv"" sources
  building data_files sources
  build_src: building npy-pkg config files
  running build_py
  creating build
  creating build/lib.linux-x86_64-3.5
  creating build/lib.linux-x86_64-3.5/lap
  copying lap/lapmod.py -> build/lib.linux-x86_64-3.5/lap
  copying lap/__init__.py -> build/lib.linux-x86_64-3.5/lap
  running build_ext
  customize UnixCCompiler
  customize UnixCCompiler using build_ext
  customize UnixCCompiler
  customize UnixCCompiler using build_ext
  building 'lap._lapjv' extension
  compiling C++ sources
  C compiler: x86_64-linux-gnu-g++ -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC
  
  creating build/temp.linux-x86_64-3.5/lap
  compile options: '-I/home/ubuntu/.local/lib/python3.5/site-packages/numpy/core/include -Ilap -I/home/ubuntu/.local/lib/python3.5/site-packages/numpy/core/include -I/usr/include/python3.5m -c'
  x86_64-linux-gnu-g++: lap/_lapjv.cpp
  x86_64-linux-gnu-g++: lap/lapjv.cpp
  x86_64-linux-gnu-g++: lap/lapmod.cpp
  lap/_lapjv.cpp:4:20: fatal error: Python.h: No such file or directory
  compilation terminated.
  error: Command ""x86_64-linux-gnu-g++ -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/home/ubuntu/.local/lib/python3.5/site-packages/numpy/core/include -Ilap -I/home/ubuntu/.local/lib/python3.5/site-packages/numpy/core/include -I/usr/include/python3.5m -c lap/_lapjv.cpp -o build/temp.linux-x86_64-3.5/lap/_lapjv.o -MMD -MF build/temp.linux-x86_64-3.5/lap/_lapjv.o.d"" failed with exit status 1
  ----------------------------------------
  ERROR: Failed building wheel for lap
  Running setup.py clean for lap
  Building wheel for pycocotools (setup.py) ... error
  ERROR: Command errored out with exit status 1:
   command: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-1774b_rx/pycocotools_e97d7b8fb07049f6b658ac57b06ccef0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-1774b_rx/pycocotools_e97d7b8fb07049f6b658ac57b06ccef0/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-vrshlese
       cwd: /tmp/pip-install-1774b_rx/pycocotools_e97d7b8fb07049f6b658ac57b06ccef0/
  Complete output (27 lines):
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.linux-x86_64-3.5
  creating build/lib.linux-x86_64-3.5/pycocotools
  copying pycocotools/coco.py -> build/lib.linux-x86_64-3.5/pycocotools
  copying pycocotools/cocoeval.py -> build/lib.linux-x86_64-3.5/pycocotools
  copying pycocotools/mask.py -> build/lib.linux-x86_64-3.5/pycocotools
  copying pycocotools/__init__.py -> build/lib.linux-x86_64-3.5/pycocotools
  running build_ext
  cythoning pycocotools/_mask.pyx to pycocotools/_mask.c
  /home/ubuntu/.local/lib/python3.5/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /tmp/pip-install-1774b_rx/pycocotools_e97d7b8fb07049f6b658ac57b06ccef0/pycocotools/_mask.pyx
    tree = Parsing.p_module(s, pxd, full_module_name)
  building 'pycocotools._mask' extension
  creating build/temp.linux-x86_64-3.5
  creating build/temp.linux-x86_64-3.5/common
  creating build/temp.linux-x86_64-3.5/pycocotools
  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/home/ubuntu/.local/lib/python3.5/site-packages/numpy/core/include -I./common -I/usr/include/python3.5m -c ./common/maskApi.c -o build/temp.linux-x86_64-3.5/./common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99
  ./common/maskApi.c: In function ‘rleToBbox’:
  ./common/maskApi.c:141:31: warning: ‘xp’ may be used uninitialized in this function [-Wmaybe-uninitialized]
         if(j%2==0) xp=x; else if(xp<x) { ys=0; ye=h-1; }
                                 ^
  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/home/ubuntu/.local/lib/python3.5/site-packages/numpy/core/include -I./common -I/usr/include/python3.5m -c pycocotools/_mask.c -o build/temp.linux-x86_64-3.5/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99
  pycocotools/_mask.c:6:20: fatal error: Python.h: No such file or directory
  compilation terminated.
  error: command 'x86_64-linux-gnu-gcc' failed with exit status 1
  ----------------------------------------
  ERROR: Failed building wheel for pycocotools
  Running setup.py clean for pycocotools
Failed to build lap pycocotools
Installing collected packages: pycocotools, openpyxl, opencv-python, motmetrics, lap, paddledet
    Running setup.py install for pycocotools ... error
    ERROR: Command errored out with exit status 1:
     command: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-1774b_rx/pycocotools_e97d7b8fb07049f6b658ac57b06ccef0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-1774b_rx/pycocotools_e97d7b8fb07049f6b658ac57b06ccef0/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /tmp/pip-record-9_kf6yrf/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /home/ubuntu/.local/include/python3.5m/pycocotools
         cwd: /tmp/pip-install-1774b_rx/pycocotools_e97d7b8fb07049f6b658ac57b06ccef0/
    Complete output (25 lines):
    running install
    running build
    running build_py
    creating build
    creating build/lib.linux-x86_64-3.5
    creating build/lib.linux-x86_64-3.5/pycocotools
    copying pycocotools/coco.py -> build/lib.linux-x86_64-3.5/pycocotools
    copying pycocotools/cocoeval.py -> build/lib.linux-x86_64-3.5/pycocotools
    copying pycocotools/mask.py -> build/lib.linux-x86_64-3.5/pycocotools
    copying pycocotools/__init__.py -> build/lib.linux-x86_64-3.5/pycocotools
    running build_ext
    skipping 'pycocotools/_mask.c' Cython extension (up-to-date)
    building 'pycocotools._mask' extension
    creating build/temp.linux-x86_64-3.5
    creating build/temp.linux-x86_64-3.5/common
    creating build/temp.linux-x86_64-3.5/pycocotools
    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/home/ubuntu/.local/lib/python3.5/site-packages/numpy/core/include -I./common -I/usr/include/python3.5m -c ./common/maskApi.c -o build/temp.linux-x86_64-3.5/./common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99
    ./common/maskApi.c: In function ‘rleToBbox’:
    ./common/maskApi.c:141:31: warning: ‘xp’ may be used uninitialized in this function [-Wmaybe-uninitialized]
           if(j%2==0) xp=x; else if(xp<x) { ys=0; ye=h-1; }
                                   ^
    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/home/ubuntu/.local/lib/python3.5/site-packages/numpy/core/include -I./common -I/usr/include/python3.5m -c pycocotools/_mask.c -o build/temp.linux-x86_64-3.5/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99
    pycocotools/_mask.c:6:20: fatal error: Python.h: No such file or directory
    compilation terminated.
    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1
    ----------------------------------------
ERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-1774b_rx/pycocotools_e97d7b8fb07049f6b658ac57b06ccef0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-1774b_rx/pycocotools_e97d7b8fb07049f6b658ac57b06ccef0/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /tmp/pip-record-9_kf6yrf/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /home/ubuntu/.local/include/python3.5m/pycocotools Check the logs for full command output.
```
，求大佬指导，感谢"
Support for wandb experiment management,PaddlePaddle/PaddleDetection,2021-06-10 19:22:50,4,,3358,917766716,"Hi, I'm an engineer at [W&B](https://wandb.ai/site) and I'm interested in building an integration with this tool. [Here's an example](https://wandb.ai/cayush/yolov5-dsviz-demo/reports/Object-Detection-with-YOLO-and-Weights-Biases--Vmlldzo0NTgzMjk) how integration can enhance the features of the tool. I don't know the official process of feature requests, so I thought I should ask the maintainers via a GitHub issue. Would you guys be interested in the integration?"
"ppdet.engine WARNING: Evaluation results empty, this may be due to training iterations ...",PaddlePaddle/PaddleDetection,2021-06-10 05:17:05,17,question#training,3345,916909683,"ppyolo tiny训练
单显卡：1080Ti
paddlepaddle版本：2.1.0
paddledet版本：2.1.0
epoch：650
训练batch_size：16
学习率设置：0.005/8=0.000625
开始训练还有评估值
![image](https://user-images.githubusercontent.com/12136347/121468590-a48bf580-c9ed-11eb-9fff-e7ec309fb739.png)
后面就报
WARNING: Evaluation results empty, this may be due to training iterations being  too few or not loading the correct weights.
后面又报这个错误程序终止
![image](https://user-images.githubusercontent.com/12136347/121468793-f59be980-c9ed-11eb-9248-64fb7654699a.png)

请问下，是学习率设置不对吗？
"
Training not saving any check points or model result in output folder and stopped after 1 or 2 epochs,PaddlePaddle/PaddleDetection,2021-06-09 07:11:06,4,,3336,915870717,"![image](https://user-images.githubusercontent.com/76849182/121308833-57593800-c8b6-11eb-8cc3-09d4c86f7147.png)



and getting some outputs on another computer.  Only one difference is that this computer that having some results use CPU and the first one having error using GPU "
学习率的疑惑，loss随着学习不断变大，最终为nan ，2.1版本，2GPU，lr0.00025，其余除了类个数均未修改,PaddlePaddle/PaddleDetection,2021-06-08 09:01:38,26,,3326,914632513,"[06/08 16:58:10] ppdet.utils.checkpoint INFO: Finish loading model weights: /home/ubuntu01/.cache/paddle/weights/ResNet50_cos_pretrained.pdparams
[06/08 16:58:12] ppdet.engine INFO: Epoch: [0] [   0/3308] learning_rate: 0.000025 loss_rpn_cls: 0.698832 loss_rpn_reg: 0.319019 loss_bbox_cls: 1.223035 loss_bbox_reg: 0.000154 loss: 2.241040 eta: 13:33:58 batch_cost: 1.2303 data_cost: 0.0002 ips: 0.8128 images/s
[06/08 16:58:17] ppdet.engine INFO: Epoch: [0] [  20/3308] learning_rate: 0.000030 loss_rpn_cls: 0.695908 loss_rpn_reg: 0.092636 loss_bbox_cls: 1.146842 loss_bbox_reg: 8035386370816811357582655488.000000 loss: 8035386370816811357582655488.000000 eta: 3:18:52 batch_cost: 0.2543 data_cost: 0.0002 ips: 3.9328 images/s
[06/08 16:58:22] ppdet.engine INFO: Epoch: [0] [  40/3308] learning_rate: 0.000034 loss_rpn_cls: 0.692222 loss_rpn_reg: 0.074917 loss_bbox_cls: 2149442433699935594479616.000000 loss_bbox_reg: 5083389209306981538767680045056.000000 loss: 5083391627158620768026029457408.000000 eta: 3:06:06 batch_cost: 0.2614 data_cost: 0.0001 ips: 3.8251 images/s

"
Win10环境下C++端TensorRT推理部署问题,PaddlePaddle/PaddleDetection,2021-06-08 01:07:20,4,deploy#windows,3314,914126080,Win10环境下使用Paddle-TensorRT库预测，将--use_static设置为true，按照文档说明应该是会自动加载已经保存的序列化文件进行推理，但是实际上会重新生成新的序列化文件而不是加载已经保存的文件，导致每次重新预测都会重新生成序列化文件，速度非常慢。在ubuntu环境下测试没有这个bug。
关于静态图中 MultiScaleTest时遇到的问题,PaddlePaddle/PaddleDetection,2021-06-06 11:33:14,2,help wanted,3297,912797206,"我在使用Cascade RCNN的结构，在测试时发现 multi scale的效果会变得很差很差，而single scale每个scale的效果都还算可以，在debug过程中我有如下几个问题:
1. 为什么mutli scale过程中，每个尺度调用 self.bbox_head.get_prediction 的结果个数都为1000，而single scale 中调用 self.bbox_head.get_prediction的过程中结果为NMS之后的个数。请问这是什么原因造成mutli scale中bbox_head中的nms没有生效的呢？
2. 为何 performance会下降的很厉害，我试过将 mutli scale中每个scale 单独测试再用 soft_nms对所有结果 进行nms，发现performance 并没有变化太多，所以应该是 mutli scale 这里的细节问题。

如下是config文件:
```
architecture: CascadeRCNN
max_iters: 40968
snapshot_iter: 1707
use_gpu: true
log_iter: 20
save_dir: output
# pretrain_weights: https://paddlemodels.bj.bcebos.com/object_detection/cascade_rcnn_dcn_r50_fpn_1x.tar
pretrain_weights: https://paddle-imagenet-models-name.bj.bcebos.com/ResNet50_vd_ssld_v2_pretrained.tar
weights: output/cascade_rcnn_dcn_r50_fpn_2x_gn/model_final
metric: COCO
num_classes: 4

CascadeRCNN:
  backbone: ResNet
  fpn: FPN
  rpn_head: FPNRPNHead
  roi_extractor: FPNRoIAlign
  bbox_head: CascadeBBoxHead
  bbox_assigner: CascadeBBoxAssigner

ResNet:
  norm_type: bn
  depth: 50
  feature_maps: [2, 3, 4, 5]
  freeze_at: 2
  variant: d
  dcn_v2_stages: [3, 4, 5]

FPN:
  min_level: 2
  max_level: 6
  num_chan: 256
  spatial_scale: [0.03125, 0.0625, 0.125, 0.25]
  norm_type: gn

FPNRPNHead:
  anchor_generator:
    anchor_sizes: [32, 64, 128, 256, 512]
    aspect_ratios: [0.5, 1.0, 2.0]
    stride: [16.0, 16.0]
    variance: [1.0, 1.0, 1.0, 1.0]
  anchor_start_size: 32
  min_level: 2
  max_level: 6
  num_chan: 256
  rpn_target_assign:
    rpn_batch_size_per_im: 256
    rpn_fg_fraction: 0.5
    rpn_positive_overlap: 0.7
    rpn_negative_overlap: 0.3
    rpn_straddle_thresh: 0.0
  train_proposal:
    min_size: 0.0
    nms_thresh: 0.7
    pre_nms_top_n: 2000
    post_nms_top_n: 2000
  test_proposal:
    min_size: 0.0
    nms_thresh: 0.7
    pre_nms_top_n: 1000
    post_nms_top_n: 1000

FPNRoIAlign:
  canconical_level: 4
  canonical_size: 224
  min_level: 2
  max_level: 5
  box_resolution: 7
  sampling_ratio: 2

CascadeBBoxAssigner:
  batch_size_per_im: 512
  bbox_reg_weights: [10, 20, 30]
  bg_thresh_lo: [0.0, 0.0, 0.0]
  bg_thresh_hi: [0.5, 0.6, 0.7]
  fg_thresh: [0.5, 0.6, 0.7]
  fg_fraction: 0.25

CascadeBBoxHead:
  head: CascadeXConvNormHead
  nms: MultiClassSoftNMS

CascadeXConvNormHead:
  norm_type: gn

MultiClassSoftNMS:
  score_threshold: 0.01
  keep_top_k: 300
  softnms_sigma: 0.5

MultiScaleTEST:
  score_thresh: 0.05
  nms_thresh: 0.5
  detections_per_im: 100
  enable_voting: true
  vote_thresh: 0.9

LearningRate:
  base_lr: 0.02
  schedulers:
  - !PiecewiseDecay
    gamma: 0.1
    milestones: [27312, 37554]
  - !LinearWarmup
    start_factor: 0.001
    steps: 500

OptimizerBuilder:
  optimizer:
    momentum: 0.9
    type: Momentum
  regularizer:
    factor: 0.0001
    type: L2

_READER_: 'faster_fpn_reader.yml'
TrainReader:
  batch_size: 2

EvalReader:
  batch_size: 1
  inputs_def:
    fields: ['image', 'im_info', 'im_id', 'im_shape']
    multi_scale: true
    num_scales: 2
    use_flip: true
  sample_transforms:
  - !DecodeImage
    to_rgb: true
  - !NormalizeImage
    is_channel_first: false
    is_scale: true
    mean:
    - 0.485
    - 0.456
    - 0.406
    std:
    - 0.229
    - 0.224
    - 0.225
  - !MultiscaleTestResize
    origin_target_size: 1280
    origin_max_size: 2000
    use_flip: true
  - !Permute
    channel_first: true
    to_bgr: false
  - !PadMultiScaleTest
    pad_to_stride: 32
  worker_num: 2
```"
ppdet2.0rc 的 solov2 在CPU推理的时候如何修改使用多线程推理？,PaddlePaddle/PaddleDetection,2021-06-06 05:35:08,5,deploy,3293,912639648,在pytorch里面torch.set_num_threads(n)可以设置cpu推理时使用的线程数，但是看2.0rc的inference脚本好像没有这个参数？
多尺度测试,PaddlePaddle/PaddleDetection,2021-06-05 13:03:03,2,bug,3292,912254352,"当我尝试使用多尺度测试时，出现以下错误

[06/05 21:01:13] reader WARNING: fail to map op [PadBatch_f39505] with error: list indices must be integers or slices, not str and stack:
Traceback (most recent call last):
  File ""E:\PaddleDetection\ppdet\data\reader.py"", line 75, in __call__
    data = f(data)
  File ""E:\PaddleDetection\ppdet\data\transform\batch_operators.py"", line 62, in __call__
    max_shape = np.array([data['image'].shape for data in samples]).max(
  File ""E:\PaddleDetection\ppdet\data\transform\batch_operators.py"", line 62, in <listcomp>
    max_shape = np.array([data['image'].shape for data in samples]).max(
TypeError: list indices must be integers or slices, not str

Exception in thread Thread-10:
Traceback (most recent call last):
  File ""C:\Users\sxl\anaconda3\envs\pd2\lib\threading.py"", line 926, in _bootstrap_inner
    self.run()
  File ""C:\Users\sxl\anaconda3\envs\pd2\lib\threading.py"", line 870, in run
    self._target(*self._args, **self._kwargs)
  File ""C:\Users\sxl\anaconda3\envs\pd2\lib\site-packages\paddle\fluid\dataloader\dataloader_iter.py"", line 199, in _thread_loop
    six.reraise(*sys.exc_info())
  File ""C:\Users\sxl\anaconda3\envs\pd2\lib\site-packages\six.py"", line 719, in reraise
    raise value
  File ""C:\Users\sxl\anaconda3\envs\pd2\lib\site-packages\paddle\fluid\dataloader\dataloader_iter.py"", line 167, in _thread_loop
    batch = self._dataset_fetcher.fetch(indices)
  File ""C:\Users\sxl\anaconda3\envs\pd2\lib\site-packages\paddle\fluid\dataloader\fetcher.py"", line 66, in fetch
    data = self.collate_fn(data)
  File ""E:\PaddleDetection\ppdet\data\reader.py"", line 80, in __call__
    raise e
  File ""E:\PaddleDetection\ppdet\data\reader.py"", line 75, in __call__
    data = f(data)
  File ""E:\PaddleDetection\ppdet\data\transform\batch_operators.py"", line 62, in __call__
    max_shape = np.array([data['image'].shape for data in samples]).max(
  File ""E:\PaddleDetection\ppdet\data\transform\batch_operators.py"", line 62, in <listcomp>
    max_shape = np.array([data['image'].shape for data in samples]).max(
TypeError: list indices must be integers or slices, not str"
请问动态图中如何使用MultiscaleTestResize,PaddlePaddle/PaddleDetection,2021-06-03 11:33:10,1,,3271,910403620,请问动态图中如何使用MultiscaleTestResize，可以给个配置的示例吗
SOLOV2出现Infer报错的情况,PaddlePaddle/PaddleDetection,2021-06-03 11:02:33,5,,3270,910379291,"系统：Ubuntu 18.04
平台：Jetson AGX Xavier
系统版本：Jetpack 4.4
——————————————————————————
1.切换infer中run_mode的值为trt_int8时报错，打印信息如下：
2.run_mode值为trt_fp16时，如果不将dyname_shape设为True的话，也会报错，打印信息如下：
3.运行速度慢，有办法提升吗，trt_fp16模式下每帧比pc的fluid慢了700ms,达到了1.5s

"
静态图使用问题,PaddlePaddle/PaddleDetection,2021-06-03 01:03:58,3,framework question,3258,909999009,请问静态图中，跟动态图RandomResize一样，可以设置多个图片的**长宽**尺寸随机resize的算子是哪一个呢，静态图中的Resize、RandomShape似乎只能给定int值、将图片resize成正方形吧
solov2支持TensorRT吗，有没有什么办法可以加速,PaddlePaddle/PaddleDetection,2021-06-02 03:43:17,6,,3241,909056493,"环境：WIN10
          GTX2060
版本：Paddle 2.0
模型：solov2_r50_fpn_3x_coco
运行视频尺寸1024*540，速度很慢，请教一下提升方法"
SOLOV2在JETSON AGX上如何部署？,PaddlePaddle/PaddleDetection,2021-06-02 02:13:22,2,jeston,3238,908988815,PaddlePaddle和Paddle Infer啥关系，我应该怎么开始部署。网上的教程有点乱，以及预编译库的作用。
2.0 python infer报错,PaddlePaddle/PaddleDetection,2021-06-02 01:59:58,4,,3237,908975926,"使用的是paddledet2.0 + cuda10.1 + paddle2.0.1
导出模型后，使用python/infer.py 进行推理预测。对预测代码做了下需要，可以批量预测文件夹下的图片。
在之前的0.3，0.4 也是这么使用的，没有问题。
这次出现报错，应该怎么修改。
报错信息为
```
  File "".\infer_xml.py"", line 162, in predict
    self.predictor.run()
RuntimeError: transform: failed to synchronize: cudaErrorIllegalAddress: an illegal memory access was encountered
```
附修改后的代码

```python
predict_image(detector):
    if FLAGS.image_file != '':
        if FLAGS.run_benchmark:
            detector.predict(
                FLAGS.image_file,
                FLAGS.threshold,
                warmup=100,
                repeats=100,
                run_benchmark=True)
        else:
            results = detector.predict(FLAGS.image_file, FLAGS.threshold)
            visualize(
                FLAGS.image_file,
                results,
                detector.pred_config.labels,
                output_dir=FLAGS.output_dir,
                threshold=FLAGS.threshold)
                
    elif FLAGS.image_path != '':
        img_list = os.listdir(FLAGS.image_path)
        for image in img_list:
            t1 = time.time()
            image_file = os.path.join(FLAGS.image_path, image)
            print(image_file)
            if FLAGS.run_benchmark:
                detector.predict(
                    image_file,
                    FLAGS.threshold,
                    warmup=100,
                    repeats=100,
                    run_benchmark=True)
            else:
                results = detector.predict(image_file, FLAGS.threshold)
                visualize(
                    image_file,
                    results,
                    detector.pred_config.labels,
                    output_dir=FLAGS.output_dir + '/img',
                    threshold=FLAGS.threshold)
                ## bbox
                save_xml(image_file,
                     results,
                     detector.pred_config.labels,
                     output_dir=FLAGS.output_dir + '/xml')
                t2 = time.time()
                ms = (t2 - t1) * 1000.0
                print(ms,'ms--')
```"
关键点检测和多目标跟踪，是否支持C++推理预测？,PaddlePaddle/PaddleDetection,2021-06-01 03:20:14,6,,3227,907831975,
模型选择及优化问题,PaddlePaddle/PaddleDetection,2021-05-30 15:10:30,2,help wanted,3212,906786851,"我的需求是识别同一种鱼的不同个体，训练集为连续两日的1分钟时长的视频，测试集为第三日30秒时长的视频，仅凭肉眼观察，难以分辨出不同个体，且采集到的视频鱼的姿态变化不大，配合度也较低，同时也存在鱼缸这样一些背景，我之前用卷积神经网络对整张图片做训练，效果很差，考虑到可能是背景有一些影响，开始采用目标检测算法只对鱼做训练。

1. 首先请问理论上用目标检测比单纯的图像分类正确率会有提升吗。

其次，我选用的 PPYOLOV2 模型，对视频的每一帧都做了标注进行训练，总共有5个类别，16593张训练图片，每张图片只有一个个体，原图是1080P，评估及测试图像的目标尺寸设置为 [640, 640] 和接近原比例的 [640, 352] 我都有尝试，训练迭代几次后，loss 值都在3左右波动，不会有大的下降，那么，

2. 对于这样的训练集，缩放成正方形更利于收敛还是缩放成接近原比例效果更好？
3. 为了丰富数据集，我取了视频的每一帧做训练，但连续帧的差异不大，这种方式丰富的数据集有没有意义？这种选取方式更利于提高模型的泛化能力还是取视频的关键帧做训练效果更好？
4. 最后，还想咨询一下模型的选取问题，目标检测的数据集大多是类别识别，那么对于同一类的个体识别问题，具有同样的适用性吗？或者是有其它方法更适用于这种个体识别的需求？

希望能得到各位开发者及工程师的回复，十分感谢！！！
* 下面是一张训练图片及其标注。
![001_4_20_15](https://user-images.githubusercontent.com/34644177/120109513-516ba480-c19c-11eb-8781-35143d6c7dcf.png)

```xml
<?xml version=""1.0"" encoding=""utf-8""?>
<annotation>
	<folder>JPEGImages</folder>
	<filename>001_4_20_15.png</filename>
	<path>/fish_recognition_yolov4/fish/second_select/JPEGImages/001_4_20_15.png</path>
	<source>
		<database>Unknown</database>
	</source>
	<size>
		<width>1920</width>
		<height>1080</height>
		<depth>3</depth>
	</size>
	<segmented>0</segmented>
	<object>
		<name>fish_001</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>335.39923</xmin>
			<ymin>199.92563</ymin>
			<xmax>1770.6204</xmax>
			<ymax>839.81396</ymax>
		</bndbox>
	</object>
</annotation>
```
"
边训练边测试时存在bug,PaddlePaddle/PaddleDetection,2021-05-30 02:50:21,4,training,3209,906610920,"使用训练边测试，在评估mAP时出现如下报错。开始以为是没有生成正确coco类型的json，排查后发现格式无误，使用./tools/eval.py直接验证，正确输出。
<img width=""854"" alt=""wrong"" src=""https://user-images.githubusercontent.com/26462054/120090391-b25f9200-c134-11eb-8d90-14a509d801dd.png"">
<img width=""705"" alt=""right"" src=""https://user-images.githubusercontent.com/26462054/120090392-b55a8280-c134-11eb-9622-a46f91152ea3.png"">

coco_detection.yml如下：
```
metric: COCO
num_classes: 3 # 将其修改为数据集的类别，不包含背景类

TrainDataset:
  !COCODataSet
    image_dir: train # 训练集的图片所在文件相对于dataset_dir的路径
    anno_path: annotations/t.json # 训练集的标注文件相对于dataset_dir的路径
    dataset_dir: dataset/camera #数据集所在路径
    data_fields: ['image', 'gt_bbox', 'gt_class', 'is_crowd']

EvalDataset:
  !COCODataSet
    image_dir: train # 训练集的图片所在文件相对于dataset_dir的路径
    anno_path: annotations/val.json # 训练集的标注文件相对于dataset_dir的路径
    dataset_dir: dataset/camera #数据集所在路径
    data_fields: ['image', 'gt_bbox', 'gt_class', 'is_crowd']

TestDataset:
  !COCODataSet
    image_dir: train # 训练集的图片所在文件相对于dataset_dir的路径
    anno_path: annotations/val.json # 训练集的标注文件相对于dataset_dir的路径
    dataset_dir: dataset/camera #数据集所在路径
    data_fields: ['image', 'gt_bbox', 'gt_class', 'is_crowd']
```"
s2anet在release2.1上出现检测框混乱问题,PaddlePaddle/PaddleDetection,2021-05-26 11:02:25,5,,3178,902209862,"![samples_0](https://user-images.githubusercontent.com/26131654/119649302-0f1d2d00-be55-11eb-889d-17ab820276b4.png)
![samples_2](https://user-images.githubusercontent.com/26131654/119649345-17756800-be55-11eb-8c44-9f46a7c73450.png)
"
使用预训练模型进行infer报错,PaddlePaddle/PaddleDetection,2021-05-26 02:18:55,4,,3169,901641533,"![image](https://user-images.githubusercontent.com/50972451/119592733-a067b180-be0b-11eb-8c19-7c8974510f8b.png)
"
多卡训练ppyolo时出现 std::runtime_error,PaddlePaddle/PaddleDetection,2021-05-26 01:05:36,2,training,3167,901584202,"我在训练多卡的ppyolo时出现了下列问题，尝试了3次 都会在训练了数百个epoch后出现下列问题，复现100%
Traceback (most recent call last):
  File ""tools/train.py"", line 140, in <module>
    main()
  File ""tools/train.py"", line 136, in main
    run(FLAGS, cfg)
  File ""tools/train.py"", line 111, in run
    trainer.train(FLAGS.eval)
  File ""/usr/src/app/pd_detection/ppdet/engine/trainer.py"", line 307, in train
    outputs = model(data)
  File ""/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py"", line 898, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/parallel.py"", line 578, in forward
    outputs = self._layers(*inputs, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py"", line 898, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File ""/usr/src/app/pd_detection/ppdet/modeling/architectures/meta_arch.py"", line 27, in forward
    out = self.get_loss()
  File ""/usr/src/app/pd_detection/ppdet/modeling/architectures/yolo.py"", line 101, in get_loss
    return self._forward()
  File ""/usr/src/app/pd_detection/ppdet/modeling/architectures/yolo.py"", line 64, in _forward
    neck_feats = self.neck(body_feats, self.for_mot)
  File ""/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py"", line 898, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File ""/usr/src/app/pd_detection/ppdet/modeling/necks/yolo_fpn.py"", line 997, in forward
    route, tip = self.fpn_blocks[i](block)
  File ""/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py"", line 898, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File ""/usr/src/app/pd_detection/ppdet/modeling/necks/yolo_fpn.py"", line 417, in forward
    conv_left = self.conv_module(conv_left)
  File ""/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py"", line 898, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/container.py"", line 97, in forward
    input = layer(input)
  File ""/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py"", line 898, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File ""/usr/src/app/pd_detection/ppdet/modeling/necks/yolo_fpn.py"", line 205, in forward
    matrix = paddle.cast(paddle.rand(x.shape, x.dtype) < gamma, x.dtype)
  File ""/usr/local/lib/python3.7/dist-packages/paddle/tensor/random.py"", line 722, in rand
    return uniform(shape, dtype, min=0.0, max=1.0, name=name)
  File ""/usr/local/lib/python3.7/dist-packages/paddle/tensor/random.py"", line 502, in uniform
    float(max), 'seed', seed, 'dtype', dtype)
SystemError: (Fatal) Operator uniform_random raises an std::runtime_error exception.
The exception content is
:random_device::random_device(const std::string&). (at /paddle/paddle/fluid/imperative/tracer.cc:192)"
paddledetection预测为0,PaddlePaddle/PaddleDetection,2021-05-25 01:47:39,2,,3157,900170813,paddledetection2.0中使用faster_rcnn_r55_vd_fpn_2x，train很正常，loss在下降。但eval时map一直全是0，可视化预测结果是原图，没有标注框，请问这个是什么原因造成的？同样的数据在paddledetection0.4上的faster_rcnn_r55_vd_fpn_2x可以用，在2.0版本的yolov3上也可以用
"请问目前2.0支持ppyolo,ppyoloV2 Anchor聚类方法吗",PaddlePaddle/PaddleDetection,2021-05-22 14:54:08,2,help wanted,3129,898819075,"请问目前2.0支持ppyolo,ppyoloV2 Anchor聚类方法，发现在ppyolov2_r50vd_dcn模型使用Anchor聚类方法V2,精度反而更差，默认的anchors反而精度更高"
deploy/python/infer.py 的seg结果和原图大小不一致,PaddlePaddle/PaddleDetection,2021-05-22 14:09:34,2,deploy,3128,898810901,"您好，我在使用solov2推测时，结果的seg和原图大小不一致，似乎是padding的那部分没有考虑？
![image](https://user-images.githubusercontent.com/65381750/119229348-28b72f80-bb4a-11eb-8379-c174b91398f1.png)
如图，左边是seg的大小，后边是原图的大小。"
动态图使用相关问题,PaddlePaddle/PaddleDetection,2021-05-22 04:41:55,2,feature request,3124,898718406,请问动态图中如何使用res2net作为backbone，CBResNet应如何在动态图中使用
fasterrcnn inference报错-(Fatal) Blocking queue is killed because the data reader raises an exception.,PaddlePaddle/PaddleDetection,2021-05-20 10:16:32,4,training,3090,896597461,"你好: 
我训练的fasterrcnn模型在infer时偶然报错，一共200张图像进行预测，到中间才报错。
错误提示不太理解，请问可能是什么原因？
```
Traceback (most recent call last):
  File ""/home/suliang/suliang_git/PaddleDetection-2.0.0/sl_package/infer.py"", line 192, in <module>
    main()
  File ""/home/suliang/suliang_git/PaddleDetection-2.0.0/sl_package/infer.py"", line 188, in main
    run(FLAGS, cfg)
  File ""/home/suliang/suliang_git/PaddleDetection-2.0.0/sl_package/infer.py"", line 137, in run
    save_txt=FLAGS.save_txt)
  File ""/home/suliang/suliang_git/PaddleDetection-2.0.0/ppdet/engine/trainer.py"", line 355, in predict
    for step_id, data in enumerate(loader):
  File ""/home/suliang/suliang_git/PaddleDetection-2.0.0/ppdet/data/reader.py"", line 216, in __next__
    data = next(self.loader)
  File ""/home/suliang/anaconda3/envs/paddle_env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py"", line 351, in __next__
    return self._reader.read_next_var_list()
SystemError: (Fatal) Blocking queue is killed because the data reader raises an exception.
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] (at /paddle/paddle/fluid/operators/reader/blocking_queue.h:158)


Process finished with exit code 1
```"
目标检测模型咨询,PaddlePaddle/PaddleDetection,2021-05-20 08:58:17,4,question,3085,896505760,老师你好，python tools/train.py -c configs/yolov3_mobilenet_v1_roadsign.yml --eval -o use_gpu=true。我在用这个命令加入自己的数据集进行模型训练时，模型要自己下载数据集，如果要用我自己的数据集，怎么设置不让他下载而直接读取现有的呢，还有就是labelme软件生成的标注格式是json格式的，而样例数据集的格式都是xml。请问是需要转换吗，如果需要用什么工具转换最简单啊。谢谢
动态图里有Retinanet的实现吗,PaddlePaddle/PaddleDetection,2021-05-19 16:35:09,2,feature request,3075,895655106,静态图里都有，但动态图里没有
PP yolo 部署批量预测和精度测量,PaddlePaddle/PaddleDetection,2021-05-18 07:52:33,9,deploy,3056,894088066,你好，请问现在有支持ppyolo tensorrt部署批量预测的方法吗？有没有可以直接run tensorrt 精度测量的方法呢？
ppyolov2训练官方指定roadsign数据集loss不下降,PaddlePaddle/PaddleDetection,2021-05-17 02:51:19,2,framework question#training,3032,892839640,"根据官方教程跑了ppyolov2模型，利用roadsign数据集训练，Loss训练到最后一直卡在10几不下降，AP不到0.1
单卡训练，学习率也缩小了10倍

_BASE_: [
  '../datasets/coco_detection.yml',
  '../runtime.yml',
  './_base_/ppyolov2_r50vd_dcn.yml',
  './_base_/optimizer_365e.yml',
  './_base_/ppyolov2_reader.yml',
]

snapshot_epoch: 8
weights: output/ppyolov2_r50vd_dcn_365e_coco/model_final

architecture: YOLOv3
pretrain_weights: https://paddledet.bj.bcebos.com/models/pretrained/ResNet50_vd_ssld_pretrained.pdparams
norm_type: sync_bn
use_ema: true
ema_decay: 0.9998

YOLOv3:
  backbone: ResNet
  neck: PPYOLOPAN
  yolo_head: YOLOv3Head
  post_process: BBoxPostProcess

ResNet:
  depth: 50
  variant: d
  return_idx: [1, 2, 3]
  dcn_v2_stages: [3]
  freeze_at: -1
  freeze_norm: false
  norm_decay: 0.

PPYOLOPAN:
  drop_block: true
  block_size: 3
  keep_prob: 0.9
  spp: true

YOLOv3Head:
  anchors: [[10, 13], [16, 30], [33, 23],
            [30, 61], [62, 45], [59, 119],
            [116, 90], [156, 198], [373, 326]]
  anchor_masks: [[6, 7, 8], [3, 4, 5], [0, 1, 2]]
  loss: YOLOv3Loss
  iou_aware: true
  iou_aware_factor: 0.5

YOLOv3Loss:
  ignore_thresh: 0.7
  downsample: [32, 16, 8]
  label_smooth: false
  scale_x_y: 1.05
  iou_loss: IouLoss
  iou_aware_loss: IouAwareLoss

IouLoss:
  loss_weight: 2.5
  loss_square: true

IouAwareLoss:
  loss_weight: 1.0

BBoxPostProcess:
  decode:
    name: YOLOBox
    conf_thresh: 0.01
    downsample_ratio: 32
    clip_bbox: true
    scale_x_y: 1.05
  nms:
    name: MatrixNMS
    keep_top_k: 100
    score_threshold: 0.01
    post_threshold: 0.01
    nms_top_k: -1
    background_label: -1

worker_num: 8
TrainReader:
  inputs_def:
    num_max_boxes: 100
  sample_transforms:
    - Decode: {}
    - Mixup: {alpha: 1.5, beta: 1.5}
    - RandomDistort: {}
    - RandomExpand: {fill_value: [123.675, 116.28, 103.53]}
    - RandomCrop: {}
    - RandomFlip: {}
  batch_transforms:
    - BatchRandomResize: {target_size: [320, 352, 384, 416, 448, 480, 512, 544, 576, 608, 640, 672, 704, 736, 768], random_size: True, random_interp: True, keep_ratio: False}
    - NormalizeBox: {}
    - PadBox: {num_max_boxes: 100}
    - BboxXYXY2XYWH: {}
    - NormalizeImage: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225], is_scale: True}
    - Permute: {}
    - Gt2YoloTarget: {anchor_masks: [[6, 7, 8], [3, 4, 5], [0, 1, 2]], anchors: [[10, 13], [16, 30], [33, 23], [30, 61], [62, 45], [59, 119], [116, 90], [156, 198], [373, 326]], downsample_ratios: [32, 16, 8]}
  batch_size: 1
  shuffle: true
  drop_last: true
  mixup_epoch: 25000
  use_shared_memory: true

EvalReader:
  sample_transforms:
    - Decode: {}
    - Resize: {target_size: [640, 640], keep_ratio: False, interp: 2}
    - NormalizeImage: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225], is_scale: True}
    - Permute: {}
  batch_size: 8
  drop_empty: false

epoch: 365

LearningRate:
  base_lr: 0.0005
  schedulers:
  - !PiecewiseDecay
    gamma: 0.1
    milestones:
    - 243
  - !LinearWarmup
    start_factor: 0.
    steps: 4000

OptimizerBuilder:
  clip_grad_by_norm: 35.
  optimizer:
    momentum: 0.9
    type: Momentum
  regularizer:
    factor: 0.0005
    type: L2

TestReader:
  inputs_def:
    image_shape: [3, 640, 640]
  sample_transforms:
    - Decode: {}
    - Resize: {target_size: [640, 640], keep_ratio: False, interp: 2}
    - NormalizeImage: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225], is_scale: True}
    - Permute: {}
  batch_size: 1

metric: COCO
num_classes: 4

TrainDataset:
  !COCODataSet
    image_dir: images
    anno_path: annotations/train.json
    dataset_dir: E:/Download/Dataset/roadsign_coco
    data_fields: ['image', 'gt_bbox', 'gt_class', 'is_crowd']

EvalDataset:
  !COCODataSet
    image_dir: images
    anno_path: annotations/valid.json
    dataset_dir: E:/Download/Dataset/roadsign_coco

TestDataset:
  !ImageFolder
    anno_path: E:/Download/Dataset/roadsign_coco/annotations/valid.json


"
在jetson xavier nx上使用trt_int8推理ssdlite_mobilenet_v3模型，推理时间反而比不使用trt_int8更大？？,PaddlePaddle/PaddleDetection,2021-05-16 15:09:16,2,deploy,3030,892690820,"使用了static/deploy/python/infer.py 不过使用的是**static**下的。推理模型是ssdlite_mobilenet_v3。
说明：推理的模型是经过感知量化训练之后的。但是，没有经过感知量化训练的模型我也测试了，数值有差别，但是trt_int8的推理时间仍然比不使用trt_int8更长。
运行输出：
```
(test) coded@coded-desktop:~/PaddleDetection/static$ python deploy/python/infer.py --model_dir=../../PaddleDetection_old/bestModel/ssdlite_mobilenet_v3_large_fpn/ --image_file=1478896904942573873.jpg  --use_gpu=True --threshold=0.2 --run_mode=trt_fp16
WARNING: AVX is not support on your machine. Hence, no_avx core will be imported, It has much worse preformance than avx core.
/home/coded/.local/virtualenvs/test/lib/python3.6/site-packages/paddle/utils/cpp_extension/extension_utils.py:461: UserWarning: Not found CUDA runtime, please use `export CUDA_HOME= XXX` to specific it.
  ""Not found CUDA runtime, please use `export CUDA_HOME= XXX` to specific it.""
-----------  Running Arguments -----------
camera_id: -1
image_file: 1478896904942573873.jpg
model_dir: ../../PaddleDetection_old/bestModel/ssdlite_mobilenet_v3_large_fpn/
output_dir: output
run_benchmark: False
run_mode: trt_fp16
threshold: 0.2
use_gpu: True
video_file: 
------------------------------------------
-----------  Model Configuration -----------
Model Arch: SSD
Use Paddle Executor: False
Transform Order: 
--transform op: Resize
--transform op: Normalize
--transform op: Permute
--------------------------------------------
W0513 20:54:04.807541 18561 analysis_predictor.cc:1145] Deprecated. Please use CreatePredictor instead.
Inference: 57.614803314208984 ms per batch image
class_id:1, confidence:0.4840,left_top:[1101.23,423.78], right_bottom:[1142.89,499.94]
class_id:4, confidence:0.6002,left_top:[1218.22,574.18], right_bottom:[1377.13,638.06]
class_id:4, confidence:0.5065,left_top:[359.96,611.97], right_bottom:[460.14,656.98]
class_id:4, confidence:0.3243,left_top:[650.40,604.32], right_bottom:[697.91,646.41]
class_id:4, confidence:0.2525,left_top:[746.66,601.77], right_bottom:[787.74,639.54]
save result to: output/1478896904942573873.jpg
```
上面是trt_fp16，但模型是经过感知量化之后的。

下面是trt_int8的输出：
```
(test) coded@coded-desktop:~/PaddleDetection/static$ python deploy/python/infer.py --model_dir=../../PaddleDetection_old/bestModel/ssdlite_mobilenet_v3_large_fpn/ --image_file=1478896904942573873.jpg  --use_gpu=True --threshold=0.2 --run_mode=trt_int8
WARNING: AVX is not support on your machine. Hence, no_avx core will be imported, It has much worse preformance than avx core.
/home/coded/.local/virtualenvs/test/lib/python3.6/site-packages/paddle/utils/cpp_extension/extension_utils.py:461: UserWarning: Not found CUDA runtime, please use `export CUDA_HOME= XXX` to specific it.
  ""Not found CUDA runtime, please use `export CUDA_HOME= XXX` to specific it.""
-----------  Running Arguments -----------
camera_id: -1
image_file: 1478896904942573873.jpg
model_dir: ../../PaddleDetection_old/bestModel/ssdlite_mobilenet_v3_large_fpn/
output_dir: output
run_benchmark: False
run_mode: trt_int8
threshold: 0.2
use_gpu: True
video_file: 
------------------------------------------
-----------  Model Configuration -----------
Model Arch: SSD
Use Paddle Executor: False
Transform Order: 
--transform op: Resize
--transform op: Normalize
--transform op: Permute
--------------------------------------------
W0513 19:59:15.472364 18171 analysis_predictor.cc:1145] Deprecated. Please use CreatePredictor instead.
Inference: 33687.761545181274 ms per batch image
class_id:1, confidence:0.4905,left_top:[1101.20,423.82], right_bottom:[1142.83,499.95]
class_id:4, confidence:0.6004,left_top:[1218.18,574.16], right_bottom:[1377.05,638.02]
class_id:4, confidence:0.5076,left_top:[359.73,611.98], right_bottom:[459.73,656.93]
class_id:4, confidence:0.3248,left_top:[650.41,604.29], right_bottom:[697.92,646.41]
class_id:4, confidence:0.2520,left_top:[746.22,601.75], right_bottom:[787.28,639.51]
save result to: output/1478896904942573873.jpg
```
trt_fp16的推理时间为57.6ms，而trt_int8的推理时间为30000多ms，这实在不合理啊。
问题：

两者差别太大，并且我的模型是经过感知量化之后的，为什么？
在对模型训练完之后，使用tools/eval.py进行评估的时候，fps在30多，但这里推理的时间是不是太长了，不管是哪种方式，是不是计算方式不一致？
环境：jetson xavier nx，测试模型：ssdlite-mobilenetv3_large_fpn，数据集：自定义数据集"
关于新增模型遇到的问题,PaddlePaddle/PaddleDetection,2021-05-15 15:00:56,7,,3022,892469541,"需要基于paddle开发新的检测模型，新增了shufflenetv2魔改的模型`shufflenet_v2.py`
```
import paddle
import paddle.nn as nn
from ppdet.core.workspace import register, serializable
from ..shape_spec import ShapeSpec

__all__ = ['ShuffleNetV2']

class ShuffleV2Block(nn.Layer):
    def __init__(self, inp, oup, mid_channels, *, ksize, stride, r=16):
        super(ShuffleV2Block, self).__init__()
        self.stride = stride
        assert stride in [1, 2]

        self.mid_channels = mid_channels
        self.ksize = ksize
        pad = ksize // 2
        self.pad = pad
        self.inp = inp
        
        outputs = oup - inp  

        branch_main = [
            # pw
            nn.Conv2D(inp, mid_channels, 1, 1, 0, bias_attr=True),
            nn.BatchNorm2D(mid_channels),
            nn.ReLU(),
            # dw
            nn.Conv2D(mid_channels, mid_channels, ksize, stride, pad, groups=mid_channels, bias_attr=True),
            nn.BatchNorm2D(mid_channels),
            # pw-linear
            nn.Conv2D(mid_channels, outputs, 1, 1, 0, bias_attr=True),
            nn.BatchNorm2D(outputs),
            nn.ReLU(),
        ]
        self.branch_main = nn.Sequential(*branch_main)

        if stride == 2:
            branch_proj = [
                # dw
                nn.Conv2D(inp, inp, ksize, stride, pad, groups=inp, bias_attr=True),
                nn.BatchNorm2D(inp),
                # pw-linear
                nn.Conv2D(inp, inp, 1, 1, 0, bias_attr=True),
                nn.BatchNorm2D(inp),
                nn.ReLU(),
            ]
            self.branch_proj = nn.Sequential(*branch_proj)
        else:
            self.branch_proj = None

    def forward(self, old_x):
        if self.stride==1:
            x_proj, x = self.channel_shuffle(old_x)
            return paddle.concat((x_proj, self.branch_main(x)), 1)
        elif self.stride==2:
            x_proj = old_x
            x = old_x
            return paddle.concat((self.branch_proj(x_proj), self.branch_main(x)), 1)

    def channel_shuffle(self, x):
        batchsize, num_channels, height, width = x.shape
        assert (num_channels % 4 == 0)
        x = x.reshape([batchsize * num_channels // 2, 2, height * width])
        x = x.transpose([1, 0, 2])
        x = x.reshape([2, -1, num_channels // 2, height, width])
        return x[0], x[1]


@register
@serializable
class ShuffleNetV2(nn.Layer):
    def __init__(self):
        """"""
        ShuffleNet, see https://

        Args:
            freeze_at (int): freeze the backbone at which stage
            return_idx (list): index of stages whose feature maps are returned
        """"""
        super(ShuffleNetV2, self).__init__()

        self.stage_repeats = [4, 8, 4]
        self.r = 16
        self.stage_out_channels = [-1, 24, 256, 512, 1024]


        # building first layer
        input_channel = self.stage_out_channels[1]
        self.stage0 = nn.Sequential(
            nn.Conv2D(3, input_channel, 3, 2, 1, bias_attr=True),
            nn.BatchNorm2D(input_channel),
            nn.ReLU(),
        )

        self.maxpool = nn.MaxPool2D(kernel_size=3, stride=2, padding=1)

        
        for idxstage in range(len(self.stage_repeats)):
            numrepeat = self.stage_repeats[idxstage]
            output_channel = self.stage_out_channels[idxstage+2]

            layers = []
            for i in range(numrepeat):
                if i == 0:
                    layers.append(ShuffleV2Block(input_channel, output_channel,
                                                mid_channels=output_channel // 2, ksize=3, stride=2, r=self.r))
                else:
                    layers.append(ShuffleV2Block(input_channel // 2, output_channel,
                                                mid_channels=output_channel // 2, ksize=3, stride=1, r=self.r))

                input_channel = output_channel

            if idxstage == 0:
                self.stage1= nn.Sequential(*layers)
            if idxstage == 1:
                self.stage2= nn.Sequential(*layers)
            if idxstage == 2:
                self.stage3= nn.Sequential(*layers)            


        self._initialize_weights()


    def _initialize_weights(self):
        normal = nn.initializer.KaimingNormal()
        constant = nn.initializer.Constant(value=0)
        for name, m in self.named_sublayers():
            if isinstance(m, nn.Conv2D):
                normal(m.weight)
                if m.bias is not None:
                    constant(m.bias)
            elif isinstance(m, nn.Linear):
                normal(m.weight)
                if m.bias is not None:
                    constant(m.bias)


    def forward(self, x):

        blocks = []


        x = self.stage0(inputs['image'])
        x = self.maxpool(x)
        x = self.stage1(x)
        blocks.append(x)
        x = self.stage2(x)
        blocks.append(x)
        x = self.stage3(x)
        blocks.append(x)


        return blocks


    @property
    def out_shape(self):
        return [ShapeSpec(channels=c) for c in [256,512,1024]]
```
在backbone文件下`__init__`增加
```
from . import shufflenet_v2
from .shufflenet_v2 import *
```
然后在YOLOv3配置下`__base__`新增文件`yolov3_shufflenet_v2`
```
architecture: YOLOv3
pretrain_weights: https://paddledet.bj.bcebos.com/models/pretrained/MobileNetV1_pretrained.pdparams
norm_type: sync_bn

YOLOv3:
  backbone: ShuffleNetV2
  neck: YOLOv3FPN
  yolo_head: YOLOv3Head
  post_process: BBoxPostProcess


# use default config
# YOLOv3FPN:

YOLOv3Head:
  anchors: [[10, 13], [16, 30], [33, 23],
            [30, 61], [62, 45], [59, 119],
            [116, 90], [156, 198], [373, 326]]
  anchor_masks: [[6, 7, 8], [3, 4, 5], [0, 1, 2]]
  loss: YOLOv3Loss

YOLOv3Loss:
  ignore_thresh: 0.7
  downsample: [32, 16, 8]
  label_smooth: false

BBoxPostProcess:
  decode:
    name: YOLOBox
    conf_thresh: 0.005
    downsample_ratio: 32
    clip_bbox: true
  nms:
    name: MultiClassNMS
    keep_top_k: 100
    score_threshold: 0.01
    nms_threshold: 0.45
    nms_top_k: 1000
```
运行`yolov3_shufflenet_v2_roadsign.yaml`
```
_BASE_: [
  '../datasets/roadsign_voc.yml',
  '../runtime.yml',
  '_base_/yolov3_shufflenet_v2.yml',
  '_base_/yolov3_reader.yml',
]
pretrain_weights: None
weights: output/yolov3_mobilenet_v1_roadsign/model_final

YOLOv3Loss:
  ignore_thresh: 0.7
  label_smooth: true

snapshot_epoch: 2
epoch: 40

LearningRate:
  base_lr: 0.0001
  schedulers:
  - !PiecewiseDecay
    gamma: 0.1
    milestones: [32, 36]
  - !LinearWarmup
    start_factor: 0.3333333333333333
    steps: 100

OptimizerBuilder:
  optimizer:
    momentum: 0.9
    type: Momentum
  regularizer:
    factor: 0.0005
    type: L2

```
报错
![45414bb4d2b9ef7e76242d6e0b5b5ee](https://user-images.githubusercontent.com/60082702/118365828-dc5f7300-b5d0-11eb-8409-a4a4868257c5.png)
"
"训练coco数据集卡死，if isinstance(item, collections.Sequence) and len(item) == 0: 一直卡住",PaddlePaddle/PaddleDetection,2021-05-14 09:02:59,6,training,3006,891750931,"```
  if isinstance(item, collections.Sequence) and len(item) == 0:
/data/xupengao/PaddleDetection/static/ppdet/data/reader.py:89: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
  if isinstance(item, collections.Sequence) and len(item) == 0:
/data/xupengao/PaddleDetection/static/ppdet/data/reader.py:89: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
  if isinstance(item, collections.Sequence) and len(item) == 0:
/data/xupengao/PaddleDetection/static/ppdet/data/reader.py:89: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
  if isinstance(item, collections.Sequence) and len(item) == 0:
```

一直卡死在这里，并且GPU显存都占满了
"
"ExternalError:  Cudnn error, CUDNN_STATUS_BAD_PARAM  (at /paddle/paddle/fluid/operators/batch_norm_op.cu:199)       [operator < batch_norm > error]",PaddlePaddle/PaddleDetection,2021-05-14 07:37:54,7,environment,3002,891693281,"`ExternalError:  Cudnn error, CUDNN_STATUS_BAD_PARAM  (at /paddle/paddle/fluid/operators/batch_norm_op.cu:199)
      [operator < batch_norm > error]`


--use_gpu=True, 报错; False, 可以运行

安装环境；
ubantu: 16.04.6
cudnn: 7.6.5
cuda: 10.0.130
paddlepaddle:  cuda10.0-cudnn7.6-trt6.0.1.5(paddlepaddle_gpu-2.0.2.post100-cp36-cp36m-linux_x86_64.whl)"
关于动态图的MultiscaleTestResize使用问题,PaddlePaddle/PaddleDetection,2021-05-14 05:42:25,6,config,3000,891626501,关于动态图的MultiscaleTestResize使用，静态图里面有使用示例，动态图里貌似现在还没有示例，直接添加到EvalReader里也不行
请问ppdet里的logger，为什么在那么多个文件里被创建了那么多次？并且明明预留了output接口，来存放log，却不给用。,PaddlePaddle/PaddleDetection,2021-05-13 06:52:27,2,enhancement,2985,890778082,感觉十分的蛋疼不解。logger的定义又是在import阶段完成的，global_config信息未导入，也没办法根据config信息来指定log save的位置。是有意为之还是啥情况。搞得十分蛋疼
如果想使用paddle-inference是不是的安装paddle-inference，只是安装了paddlepaddle无法使用paddle-inference啊？？,PaddlePaddle/PaddleDetection,2021-05-13 06:36:17,12,documentation#question,2983,890765331,
在jetson xavier nx上使用trt_int8出现段错误,PaddlePaddle/PaddleDetection,2021-05-11 13:05:13,2,,2953,887221776,"说明：这里是使用的paddleDetection版本是 release 2.0。paddlepaddle：2.0.0。
这里下载好官方的已经训练好的模型参数yolov3_mobilenet_v3_large_270e_coco.pdparams。
使用export_model.py 导出模型：python tools/export_model.py -c configs/yolov3/yolov3_mobilenet_v3_large_270e_coco.yml --output_dir=./inferModel/ -o weights=./model/yolov3_mobilenet_v3_large_270e_coco.pdparams。
使用deploy/python/infer.py 预测模型：python deploy/python/infer.py --model_dir=./inferModel/yolov3_mobilenet_v3_large_270e_coco/ --image_fil=dataset/000000502910.jpg --run_mode=trt_int8。
错误信息：
```
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::framework::SignalHandle(char const*, int)
1   paddle::platform::GetCurrentTraceBackString[abi:cxx11]()

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1620737053 (unix time) try ""date -d @1620737053"" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 31569 (TID 0x7f812c0010) from PID 0 ***]

Segmentation fault (core dumped)
```
上面我使用了tensorRT_int8，但是出现了段错误，不知道什么原因？？
"
ppyolo训练,PaddlePaddle/PaddleDetection,2021-05-11 08:20:44,2,,2944,886459186,"在aistudio上训练ppyolov2，开始时可以正常训练，后面就变成nan了。
![MCY Q05{UG@{6((@8HBV~_1](https://user-images.githubusercontent.com/71364549/117782238-04b73c80-b274-11eb-8990-1f1c2de9507a.png) 
我按照文档将学习率除以8，然后将batch size调成4，请问是我的修改有错吗？
"
使用PaddleDetection中的ppyolo_mbv3_large_coco模型，进行冻结部分网络层的微调训练,PaddlePaddle/PaddleDetection,2021-05-11 08:13:54,2,,2943,886442112,选用PaddleDetection套件中的ppyolo_mbv3_large_coco模型，使用自己的数据集，进行微调训练，但是想冻结网络的部分层，训练时冻结层的参数不可变。套件中的ppyolo_r50vd_dcn_1x_coco中使用的backbone是resn50，配置文件中有freeze_at来选择冻结至resnet中的第几个stage，但是ppyolo_mbv3_large_coco中没有这个配置参数，自己手动加上会报错，说不存在这个参数。在ppyolo_mbv3_large_coco的yml配置文件中该如何修改呢？
tool/infer.py评测没有nms过程吗？,PaddlePaddle/PaddleDetection,2021-05-10 07:33:22,2,,2926,883601661,调低阈值后明显很多类有重复框
请问下ppyolov2是否支持转onnx,PaddlePaddle/PaddleDetection,2021-05-08 08:54:16,2,deploy,2904,880529083,"我想将ppyolov2转换为onnx模型，过程中遇到如下问题：

我首先执行如下命令将ppyolov2成功导出模型
python3 tools/export_model.py -c configs/ppyolo/ppyolov2_r50vd_dcn_365e_coco.yml -o weights=./models/ppyolov2_r50vd_dcn_365e_coco.pdparams

然后参照https://github.com/PaddlePaddle/Paddle2ONNX/blob/develop/README_zh.md，执行如下命令：
paddle2onnx --model_dir output_inference/ppyolov2_r50vd_dcn_365e_coco/  --model_filename model.pdmodel --params_filename model.pdiparams --save_file onnx_file --opset_version 10 --enable_onnx_checker True

产生报错，似乎是模型里有无法支持转换的op，想请问下如何解决：

λ afe5ee05416f /home/ubt/detection/PaddleDetection paddle2onnx --model_dir output_inference/ppyolov2_r50vd_dcn_365e_coco/  --model_filename model.pdmodel --params_filename model.pdiparams --save_file onnx_file --opset_version 10 --enable_onnx_checker True
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
Traceback (most recent call last):
  File ""/usr/local/python2.7.15/bin/paddle2onnx"", line 8, in <module>
    sys.exit(main())
  File ""/usr/local/python2.7.15/lib/python2.7/site-packages/paddle2onnx/command.py"", line 142, in main
    enable_onnx_checker=args.enable_onnx_checker)
  File ""/usr/local/python2.7.15/lib/python2.7/site-packages/paddle2onnx/command.py"", line 114, in program2onnx
    enable_onnx_checker=enable_onnx_checker)
  File ""/usr/local/python2.7.15/lib/python2.7/site-packages/paddle2onnx/convert.py"", line 77, in program2onnx
    export_onnx(paddle_graph, save_file, opset_version, enable_onnx_checker)
  File ""/usr/local/python2.7.15/lib/python2.7/site-packages/paddle2onnx/convert.py"", line 32, in export_onnx
    onnx_graph = ONNXGraph.build(paddle_graph, opset_version, verbose)
  File ""/usr/local/python2.7.15/lib/python2.7/site-packages/paddle2onnx/graph/onnx_graph.py"", line 229, in build
    onnx_graph.build_op_nodes(paddle_graph.node_map)
  File ""/usr/local/python2.7.15/lib/python2.7/site-packages/paddle2onnx/graph/onnx_graph.py"", line 183, in build_op_nodes
    OpMapper.check_support_status(node_map, self.opset_version)
  File ""/usr/local/python2.7.15/lib/python2.7/site-packages/paddle2onnx/op_mapper/op_mapper.py"", line 144, in check_support_status
    raise NotImplementedError(error_info)
NotImplementedError:
There's 4 ops are not supported yet
=========== softplus ===========
=========== logical_not ===========
=========== select_input ===========
=========== conditional_block ===========


"
ppyolov2在大分辨率上速度远低于yolov4 和yolov5,PaddlePaddle/PaddleDetection,2021-05-08 02:48:38,2,deploy,2896,880143856, 在i9-10000k 内存128g，显卡3080 。windows10平台，cuda11。发现同样1920*1080图片，ppyolov2，训练模型608 单张检测耗时需要接近300ms，而采用yolov5训练模型1280，最慢是50ms左右。
yolov3 backbone输出shape问题,PaddlePaddle/PaddleDetection,2021-05-07 10:55:16,3,,2892,878776138,"yolov3三个输出shape分别是52 * 52，26 * 26，13 * 13，我能不能修改这三个输出shape大小？想修改为一个26 * 26，两个13*13,请问neck，head,loss三部分怎么修改参数？"
PPYOLO模型单卡 训练&评估 自制数据时，评估时被Killed,PaddlePaddle/PaddleDetection,2021-05-07 03:02:55,4,training,2884,878372488,"- Environment，Images 同 #2804
- [ppyolo_config.zip](https://github.com/PaddlePaddle/PaddleDetection/files/6438726/ppyolo_config.zip)
- Error
```
2021-05-06 11:14:33,931-INFO: Test iter 39500
2021-05-06 11:14:38,836-INFO: Test finish iter 39512
2021-05-06 11:14:38,837-INFO: Total number of images: 39512, inference time: 2.31748052666 fps.
loading annotations into memory...
Done (t=0.62s)
creating index...
index created!
W0506 11:16:34.989540    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:18:33.545070    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:20:33.278232    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:22:35.328816    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:24:32.599476    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:26:04.314370    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:28:12.610067    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:29:51.302060    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:30:37.864058    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:31:25.510361    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:31:55.373211    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:32:04.727373    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:33:28.832084    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:34:59.285615    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:37:16.281620    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:39:10.234481    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:40:13.878899    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:41:38.766180    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:43:20.494897    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:44:42.697191    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:46:20.210536    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:47:30.928135    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:48:38.832187    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:49:44.536505    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:50:54.880196    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:51:58.389420    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:53:18.843338    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:55:11.519363    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:57:00.464871    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 11:58:44.448000    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:00:33.856367    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:02:36.551040    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:04:27.783215    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:06:25.699518    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:08:13.449936    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:09:58.038261    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:11:53.508752    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:13:45.543397    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:15:43.280170    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:17:36.647133    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:19:28.919907    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:20:47.844053    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:22:30.101073    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:24:33.718323    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:26:35.807687    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:28:44.468787    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:29:40.177767    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:30:21.720806    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:31:08.470664    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:32:46.039049    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:33:31.002614    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:34:05.852691    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:35:49.317538    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
W0506 12:36:57.692786    61 sampler.cpp:139] bvar is busy at sampling for 2 seconds!
Killed
```
- All Error
[ppyolo_评估_error.txt](https://github.com/PaddlePaddle/PaddleDetection/files/6438743/ppyolo_._error.txt)
"
AGS module hasn't been implemented?,PaddlePaddle/PaddleDetection,2021-05-06 10:13:42,2,,2880,877335777,I haven't seen the ags module in PAFNet?
感知量化训练,PaddlePaddle/PaddleDetection,2021-05-06 02:34:22,6,model compression,2874,877015766,贵方的paddleslim中有感知量化训练的手段，想问一下贵方具体的实现原理，有没有相关的文档啊。
s2anet batch>1情况与--eval时报错,PaddlePaddle/PaddleDetection,2021-05-04 01:19:27,5,bug,2862,875025937,"batch>1时
Hint: Expected killed_ != true, but received killed_:1 == true:1
似乎是训练数据得自行事先预处理统一成相同大小才行

训练 --eval时 
Exception: list input can be bounding box(Nx4) or RLEs([RLE])
似乎eval的[xc, yc, bow_w, bow_h, angle]还未适配
"
请问如何在dygraph中运行Cascade-Faster-RCNN-CBResNet200vd-FPN,PaddlePaddle/PaddleDetection,2021-05-02 18:36:15,2,feature request,2856,874012399,我使用2.0rc版本的PaddleDetection，当我在dygraph尝试运行Cascade-Faster-RCNN-CBResNet200vd-FPN的算例时，出现了AssertionError: the module CBResNet is not registered，检查了一下dygraph下的backbones路径，确实没有cb_resnet.py，而在PaddleDetection/ppdet/modeling/backbones/下存在cb_resnet.py。因此我想问下如何在dygraph中运行Cascade-Faster-RCNN-CBResNet200vd-FPN呢？简单替换ppdet的方法已测试过，会出现其他的问题。
关于训练学习率调整问题,PaddlePaddle/PaddleDetection,2021-04-30 06:41:04,2,,2834,872079541,"看到

默认学习率是适配多GPU训练(8x GPU)，若使用单GPU训练，须对应调整学习率（例如，除以8）。 计算规则表如下所示，它们是等价的，表中变化节点即为piecewise decay里的boundaries:
GPU数 | 学习率 | 最大轮数 | 变化节点
-- | -- | -- | --
2 | 0.0025 | 720000 | [480000, 640000]
4 | 0.005 | 360000 | [240000, 320000]
8 | 0.01 | 180000 | [120000, 160000]
piecewise decay里的boundaries这个具体怎么设置呢，能给个例子吗，我在代码没看到
boundaries，关键词piecewisedecay倒是有

"
EfficientNet b7 测试时image ResizeAndPad target_dim不是512就报异常,PaddlePaddle/PaddleDetection,2021-04-30 06:00:46,2,,2833,872055836,"试过1024，800，608，480，都报错

architecture: EfficientDet
max_iters: 14000
use_gpu: true
pretrain_weights: https://paddle-imagenet-models-name.bj.bcebos.com/EfficientNetB7_pretrained.tar
weights: output/efficientdet_d7/model_final
log_iter: 20
snapshot_iter: 280
metric: COCO
save_dir: output
num_classes: 5
use_ema: true
ema_decay: 0.9998

EfficientDet:
  backbone: EfficientNet
  fpn: BiFPN
  efficient_head: EfficientHead
  anchor_grid: AnchorGrid
  box_loss_weight: 50.

EfficientNet:
  # norm_type: sync_bn
  # TODO
  norm_type: bn
  scale: b7
  use_se: true

BiFPN:
  num_chan: 64
  repeat: 3
  levels: 5

EfficientHead:
  repeat: 3
  num_chan: 64
  prior_prob: 0.01
  num_anchors: 9
  gamma: 1.5
  alpha: 0.25
  delta: 0.1
  output_decoder:
    score_thresh: 0.05   # originally 0.
    nms_thresh: 0.5
    pre_nms_top_n: 1000  # originally 5000
    detections_per_im: 100
    nms_eta: 1.0

AnchorGrid:
  anchor_base_scale: 4
  num_scales: 3
  aspect_ratios: [[1, 1], [1.4, 0.7], [0.7, 1.4]]

LearningRate:
  base_lr: 0.001
  schedulers:
  - !CosineDecayWithSkip
    total_steps: 14000
    skip_steps: 560
  - !LinearWarmup
    start_factor: 0.05
    steps: 560

OptimizerBuilder:
  clip_grad_by_norm: 10.
  optimizer:
    momentum: 0.9
    type: Momentum
  regularizer:
    factor: 0.00004
    type: L2

TrainReader:
  inputs_def:
    fields: ['image', 'im_id', 'fg_num', 'gt_label', 'gt_target']
  dataset:
    !COCODataSet
    dataset_dir: /home/aistudio/data/wd
    anno_path: annotations/train_c_f5_1.json
    image_dir: images
  sample_transforms:
  - !DecodeImage
    to_rgb: true
  - !RandomFlipImage
    prob: 0.5
  - !NormalizeImage
    is_channel_first: false
    is_scale: true
    mean: [0.485,0.456,0.406]
    std: [0.229, 0.224,0.225]
  - !RandomScaledCrop
    target_dim: 512
    scale_range: [1., 2.8]
    interp: 1
  - !Permute
    to_bgr: false
    channel_first: true
  - !TargetAssign
    image_size: 512
  batch_size: 4
  shuffle: true
  worker_num: 32
  bufsize: 16
  use_process: true
  drop_empty: false

EvalReader:
  inputs_def:
    fields: ['image', 'im_info', 'im_id']
  dataset:
    !COCODataSet
    dataset_dir: /home/aistudio/data/wd
    anno_path: annotations/eval_c_f5_1.json
    image_dir: images
  sample_transforms:
  - !DecodeImage
    to_rgb: true
    with_mixup: false
  - !NormalizeImage
    is_channel_first: false
    is_scale: true
    mean: [0.485,0.456,0.406]
    std: [0.229, 0.224,0.225]
  - !ResizeAndPad
    target_dim: 1024
    interp: 1
  - !Permute
    channel_first: true
    to_bgr: false
  drop_empty: false
  batch_size: 4
  shuffle: false
  worker_num: 2

TestReader:
  inputs_def:
    fields: ['image', 'im_info', 'im_id']
    image_shape: [3, 512, 512]
  dataset:
    !ImageFolder
    anno_path: /home/aistudio/data/wd/annotations/eval_c_f5_1.json
  sample_transforms:
  - !DecodeImage
    to_rgb: true
    with_mixup: false
  - !NormalizeImage
    is_channel_first: false
    is_scale: true
    mean: [0.485,0.456,0.406]
    std: [0.229, 0.224,0.225]
  - !ResizeAndPad
    target_dim: 512
    interp: 1
  - !Permute
    channel_first: true
    to_bgr: false
  batch_size: 4
  shuffle: false




Traceback (most recent call last):
  File ""/home/aistudio/PaddleDetection-master/tools/train.py"", line 405, in <module>
    main()
  File ""/home/aistudio/PaddleDetection-master/tools/train.py"", line 322, in main
    resolution=resolution)
  File ""/home/aistudio/PaddleDetection-master/ppdet/utils/eval_utils.py"", line 148, in eval_run
    return_numpy=False)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py"", line 1110, in run
    six.reraise(*sys.exc_info())
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/six.py"", line 703, in reraise
    raise value
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py"", line 1108, in run
    return_merged=return_merged)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py"", line 1252, in _run_impl
    return_merged=return_merged)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py"", line 913, in _run_parallel
    tensors = exe.run(fetch_var_names, return_merged)._move_to_list()
ValueError: In user code:

    File ""/home/aistudio/PaddleDetection-master/tools/train.py"", line 405, in <module>
      main()
    File ""/home/aistudio/PaddleDetection-master/tools/train.py"", line 170, in main
      fetches = model.eval(feed_vars)
    File ""/home/aistudio/PaddleDetection-master/ppdet/modeling/architectures/efficientdet.py"", line 147, in eval
      return self.build(feed_vars, 'test')
    File ""/home/aistudio/PaddleDetection-master/ppdet/modeling/architectures/efficientdet.py"", line 86, in build
      im_info)
    File ""/home/aistudio/PaddleDetection-master/ppdet/modeling/anchor_heads/efficient_head.py"", line 162, in get_prediction
      im_info=im_info)
    File ""/home/aistudio/PaddleDetection-master/ppdet/core/workspace.py"", line 179, in partial_apply
      return op(*args, **kwargs_)
    File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/detection.py"", line 3257, in retinanet_detection_output
      outputs={'Out': output})
    File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layer_helper.py"", line 43, in append_op
      return self.main_program.current_block().append_op(*args, **kwargs)
    File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py"", line 3225, in append_op
      attrs=kwargs.get(""attrs"", None))
    File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py"", line 2305, in __init__
      for frame in traceback.extract_stack():

    InvalidArgumentError: The 1st dimension of each Variables in Input(Anchors) must be equal to the 2nd dimension of corresponding Variables in Input(BBoxes), which represents the number of the predicted boxes, but received Anchors 1st dimension is:36864, BBoxes 2nd dimension is:147456.
      [Hint: Expected anchor_dims[0] == bbox_dims[1], but received anchor_dims[0]:36864 != bbox_dims[1]:147456.] (at /paddle/paddle/fluid/operators/detection/retinanet_detection_output_op.cc:134)
      [operator < retinanet_detection_output > error]
terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::framework::SignalHandle(char const*, int)
1   paddle::platform::GetCurrentTraceBackString[abi:cxx11]()

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1619760295 (unix time) try ""date -d @1619760295"" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3e800006dbe) received by PID 28094 (TID 0x7fc33dcb3700) from PID 28094 ***]

Segmentation fault (core dumped)
"
人脸关键点的模型有没有？,PaddlePaddle/PaddleDetection,2021-04-30 03:08:49,2,,2830,871889700,
cascade_rcnn无输出,PaddlePaddle/PaddleDetection,2021-04-29 11:34:31,9,help wanted,2823,870913985,使用cascade_rcnn网络训练自己的数据，训练过程有loss，但是使用eval时，输出均为0
Eval评估没有json文件,PaddlePaddle/PaddleDetection,2021-04-29 06:03:30,5,,2811,870643267,您好，请问!python tools/eval.py -c configs/cascade_rcnn/cascade_rcnn_r50_fpn_1x_coco.yml -o weights=output/cascade_rcnn_r50_fpn_1x_coco/model_final.pdparams EvalReader.batch_size=1 --save_prediction_only 测试后没有json文件出现是什么原因？
PP-YOLO tiny 模型如何 做量化,PaddlePaddle/PaddleDetection,2021-04-29 05:52:51,8,model compression,2810,870637293,PP-YOLO tiny 模型如何 做量化 减少模型的大小
PPYOLO模型单卡训练instances_train2017.json，不久就Killed,PaddlePaddle/PaddleDetection,2021-04-28 12:00:22,4,,2805,869867368,"PPYOLO模型单卡训练instances_train2017.json，不久就Killed。
那怎么设置呢？

- PC Perfomance

+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce GTX 166...  Off  | 00000000:01:00.0 Off |                  N/A |
| 37%   36C    P0    21W / 125W |      0MiB /  5941MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

CPU 16 CORE 
Memeory：16GB

- Environment

Docker Image
hub.baidubce.com/paddlepaddle/paddle:1.8.5-gpu-cuda10.0-cudnn7

- PaddleDetection

 * master
  origin/release/0.5"
PaddleDetection模型导到paddlex部署C++环境使用吗？,PaddlePaddle/PaddleDetection,2021-04-28 04:03:55,2,,2796,869470792,PaddleDetection模型导到paddlex部署C++环境使用吗？
有关ppyolo loss问题,PaddlePaddle/PaddleDetection,2021-04-28 02:42:48,4,,2793,869435520,训练后发现loss一直处于500多，降不下来。
训练速度问题,PaddlePaddle/PaddleDetection,2021-04-27 07:47:40,4,,2778,868565328,"旧版本ssd训练12w轮，相当于120epoch，需要8-9小时
新版本ssd训练120epoch，却需要17小时多，这速度差的太远了，不知道是什么原因，不应该差这么多啊
[04/27 15:36:53] ppdet.engine INFO: Epoch: [0] [   0/1034] learning_rate: 0.000333 loss: 26.950333 eta: 16:56:21 batch_cost: 0.4915 data_cost: 0.0022 ips: 32.5557 images/s
[04/27 15:37:03] ppdet.engine INFO: Epoch: [0] [  20/1034] learning_rate: 0.000360 loss: 17.336845 eta: 17:22:26 batch_cost: 0.5048 data_cost: 0.0017 ips: 31.6959 images/s
[04/27 15:37:13] ppdet.engine INFO: Epoch: [0] [  40/1034] learning_rate: 0.000387 loss: 12.285319 eta: 17:25:59 batch_cost: 0.5079 data_cost: 0.0027 ips: 31.5053 images/s
[04/27 15:37:23] ppdet.engine INFO: Epoch: [0] [  60/1034] learning_rate: 0.000413 loss: 9.443251 eta: 17:21:53 batch_cost: 0"
如何固定检测框颜色为红色 加粗检测框线条？,PaddlePaddle/PaddleDetection,2021-04-27 07:42:30,2,,2777,868560674,"代码在哪里呢  线条粗细是修改`# draw bbox
        draw.line(
            [(xmin, ymin), (xmin, ymax), (xmax, ymax), (xmax, ymin),
             (xmin, ymin)],
            width=8,
            fill=color)` 
修改这个width是修改线条粗细吗？
颜色如何固定为红色呢  colormap里那么多数值那个是红色呢"
s2anet 用cpu训练,PaddlePaddle/PaddleDetection,2021-04-24 15:30:24,8,bug#enhancement#install,2751,866772411,"您好：
准备尝试用CPU训练S2ANET，在编译 ppdet/ext_op时报错：
```bash
building 'rbox_iou_ops' extension
gcc -pthread -B /root/miniconda3/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/root/miniconda3/lib/python3.7/site-packages/paddle/include -I/root/miniconda3/lib/python3.7/site-packages/paddle/include/third_party -I/usr/local/cuda/include -I/root/miniconda3/include/python3.7m -c /home/ext_op/rbox_iou_op.cc -o /home/ext_op/build/rbox_iou_ops/lib.linux-x86_64-3.7/rbox_iou_op.o -g -w -std=c++11
cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
/usr/local/cuda/bin/nvcc -I/root/miniconda3/lib/python3.7/site-packages/paddle/include -I/root/miniconda3/lib/python3.7/site-packages/paddle/include/third_party -I/usr/local/cuda/include -I/root/miniconda3/include/python3.7m -c /home/ext_op/rbox_iou_op.cu -o /home/ext_op/build/rbox_iou_ops/lib.linux-x86_64-3.7/rbox_iou_op.cu.o -DPADDLE_WITH_CUDA -DEIGEN_USE_GPU -O3 -ccbin cc -Xcompiler -fPIC -w --expt-relaxed-constexpr -DNVCC -g -w -std=c++11
unable to execute '/usr/local/cuda/bin/nvcc': No such file or directory
error: command '/usr/local/cuda/bin/nvcc' failed with exit status 1
```

paddle版本如下：
paddlepaddle==2.0.2   [CPU 版]

"
有设置随机数，以便复现结果的功能吗？,PaddlePaddle/PaddleDetection,2021-04-22 04:54:32,5,question,2736,864538530,看了一下Objects365 2019 Challenge夺冠模型的配置，没看到seed的设置，另外在paddlex那儿也说目前没设定随机数以便复现结果的方法
检测结果不稳定：,PaddlePaddle/PaddleDetection,2021-04-20 13:40:36,3,,2716,862830702,"用cascadercnn_clc_aware的配置文件训练后导出模型，用predictor接口预测出现以下现象：
1、分值不稳定，同种场景目标检测出来的分值不稳定，
2、类别不稳定，同种目标会出现类别错误。
求指教！！谢谢！
"
trt_int8预测时间和CPU差不多，并且没有结果,PaddlePaddle/PaddleDetection,2021-04-17 10:31:13,3,,2681,860379102,"环境：
1、paddle-gpu: 2.0.0rc1，jetson nano源码编译的python包，已经编译TensorRT。
2、PaddleDetection： develop分支（新增了trt_int8那个）
使用命令：
cd PaddleDetection
python3 static/deploy/python/infer.py --model_dir=./export_model/lianghua/float/int/ssd_mobilenet_v1_voc/ --camera_id=0 --use_gpu=1  --run_mode=trt_int8
说明：
模型由量化训练产生：用的是PaddleDetection=2.0rc下的slim/quantization/train.py训练，然后导出的。
量化训练命令：（PaddleDetection=2.0rc，aistudio notebook）
#量化训练
!python slim/quantization/train.py --not_quant_pattern yolo_output \
    --eval \
    -c ./configs/ssd/ssd_mobilenet_v1_voc.yml \
    -o max_iters=15000 \
    save_dir=./output/quantization \
    LearningRate.base_lr=0.0001 \
    LearningRate.schedulers=""[!PiecewiseDecay {gamma: 0.1, milestones: [5000]}]"" \
    pretrain_weights=https://paddlemodels.bj.bcebos.com/object_detection/ssd_mobilenet_v1_voc.tar

模型文件：
链接：https://pan.baidu.com/s/1QocR2BJRgbMEVtOyFx5XKA 
提取码：x5ii 

报错：
![11](https://user-images.githubusercontent.com/78419361/115109636-8e435980-9fa9-11eb-9a37-f61dc640c9e1.png)
![22](https://user-images.githubusercontent.com/78419361/115109643-94d1d100-9fa9-11eb-8e26-1474927463fa.png)
![33](https://user-images.githubusercontent.com/78419361/115109651-9a2f1b80-9fa9-11eb-9f98-3774cd1557e8.png)

问题：
1、量化训练后的模型float和int文件夹下的模型只有 模型和参数 两个文件，没有infer_cfg.yml这个文件，但是预测的时候提示没有infer_cfg.yml文件，我这里直接用的这个 ssd_mobilenet_v1_voc.yml导出后生成的infer_cfg.yml，请问这样可以吗？还是如何使用？
2、模型预测时间达1s多，但是使用没有量化训练的ssd_mobilenet_v1_voc.yml模型是可以使用trt_fp16和trt_fp32的。
麻烦有空的时候跟进下，谢谢！！！
"
动态图版本怎么加载训练好的模型继续训练,PaddlePaddle/PaddleDetection,2021-04-16 11:53:28,12,,2661,859744517,动态图版本，调整了网络结构，已经训练了140轮，怎么加载140轮的模型继续训练，修改了pretrain_weights之后，感觉没有成功，loss2000多，load_static_model是否要改为false？
fcos的eval出现问题,PaddlePaddle/PaddleDetection,2021-04-14 08:30:45,4,,2622,857659178,"使用eval进行评估时出现如下错误：
![XCD0$40V0P6$ YHNYW}52SS](https://user-images.githubusercontent.com/71364549/114679461-c09d4e80-9d3e-11eb-940d-d3e0686ad7c3.png)
"
后面是否会提供部署成http服务吗,PaddlePaddle/PaddleDetection,2021-04-12 16:01:02,2,deploy,2595,856124418,目前服务端的部署是RPC方式，后面会提供http服务部署的方式吗
动态图训练PPYOLO,PaddlePaddle/PaddleDetection,2021-04-12 10:32:17,2,bug#windows,2587,855828037,"动态图训练PPYOLO-tiny时，snapshot_epoch设置成1，训练一轮后，无报错终止，也没有保存模型，请问是什么原因 ？

![QQ图片20210412183156](https://user-images.githubusercontent.com/53975909/114381145-61640080-9bbd-11eb-9925-d2bfd8b72e93.png)
"
将SSD的 smooth L1 loss 替换为 iou_loss,PaddlePaddle/PaddleDetection,2021-04-12 08:50:52,2,,2585,855726712,"打算将SSD中的smooth L1 Loss替换为iou_loss，也就是ciou loss，但是看到贵方的实现中有anchor参数，我看论文中关于ciou loss没有提及anchor，这里贵方是为了和Yolo 相适配吗。如果我的框架是SSD，想使用这个模块，是直接使用(直接使用的参数怎么设置)；还是说我得根据SSD的输出以及模仿贵方的实现，重写ciou loss？？
![image](https://user-images.githubusercontent.com/32217129/114367652-396da080-9baf-11eb-94da-6b7c0cf22be7.png)
"
"paddleX训练好的模型,不能直接用到PaddleDetection吗",PaddlePaddle/PaddleDetection,2021-04-11 07:45:00,2,,2578,855242941,"模型是 FasterRCNN

paddleX训练好的模型
model.yml

改名为 infer_cfg.yml

让PaddleDetection进行预测
deploy\python\infer.py

SUPPORT_MODELS 中没发现有FasterRCNN  要选择RCNN？？

SUPPORT_MODELS = {
    'YOLO',
    'SSD',
    'RetinaNet',
    'EfficientDet',
    'RCNN',
    'Face',
    'TTF',
    'FCOS',
    'SOLOv2',
}


FasterRCNN中，需要将 with_background=true 且 num_classes+1
已经更改

对 infer_cfg.yml 进行增加更改  还是各种错误

是我有些东西没改对  还是 需要在PaddleDetection中重新训练"
训练速度差异,PaddlePaddle/PaddleDetection,2021-04-11 04:31:32,2,,2577,855215751,动态图版本ssd和之前静态图版本ssd的训练速度差异较大，比如对voc数据集，batch_size为16时静态图12万iter差不多100epoch只要8-9小时，动态图100epoch却要多出一倍，不知道是何原因？
关于检测的Bbox,PaddlePaddle/PaddleDetection,2021-04-08 15:03:55,2,,2562,853569272,我使用paddledetection进行预测，可是输出的Bbox有负数，这是怎么回事。
关于pp-yolo和mask r-cnn单卡训练自定义数据集学习率的问题,PaddlePaddle/PaddleDetection,2021-04-08 07:17:10,4,,2556,853149770,"用的是batch size=12 学习率用的0.05/8  但是训练发现迭代9W轮左右 loss还是很大 map大概能到38多  如何能再进一步提高map呢  学习率稍微大一点就nan了  。。。。
mask r-cnn用的batch size=4和2两种，  学习率0.01/8=0.00125 但是也用了0.00875等学习率 发现一般在迭代2W轮基本就是最优  但是map才29左右  如何进一步提升map呢
数据集的样本量大概2000张  "
请问一下，cornernet_squeeze_hg104.yml模型中是将输入图像resize到511x511像素大小吗？,PaddlePaddle/PaddleDetection,2021-04-07 05:33:20,2,question,2534,852023984,"尊敬的开发者，您好，
请问一下，在cornernet_squeeze_hg104.yml模型中，图像预处理的第一部操作，是将输入图像resize到511x511像素大小吗？
期待您的回复！"
cfg对象的序列化serializable与保存到yaml文件,PaddlePaddle/PaddleDetection,2021-04-06 16:16:47,0,question,2532,851585194,"想请教一下，PaddleDetection有没有提供将cfg对象dump到yaml文件的方法呢？我看到有读取cfg文件并且序列化的`ppdet/core/config/yaml_helplers.py`脚本，但是如何将一个cfg文件dump到yaml文件中，使得这个文件符合PaddleDetection的cfg文件格式呢？

我的目的是想要修改config文件，并将修改后的cfg对象重新写到yaml文件中。目前config文件的读取和序列化参考ppdet的实现是可以的，但重新写回yaml文件还不太清楚如何操作？简单地使用`yaml.dump(cfg, f)`的方式貌似不能识别到`COCODataSet`等对象。"
裁剪出错：使用PaddleDetection下slim里的裁剪出现问题,PaddlePaddle/PaddleDetection,2021-04-06 05:42:19,2,model compression,2525,850997643,"执行命令：（jupyter notebook）
!python slim/prune/prune.py \
-c ./configs/ssd/ssd_mobilenet_v1_voc.yml \
--pruned_params ""conv1_weights"" \
--pruned_ratios=""0.2""

报错：
C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddle\fluid\layers\math_op_patch.py:298: UserWarning: C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddle\fluid\layers\detection.py:1751
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
2021-04-06 13:29:54,175-INFO: If regularizer of a Parameter has been set by 'fluid.ParamAttr' or 'fluid.WeightNormParamAttr' already. The Regularization[L2Decay, regularization_coeff=0.000050] in Optimizer will not take effect, and it will only be applied to other Parameters!
2021-04-06 13:30:19,093-INFO: pruned params: ['conv7_2_extra1_weights']
2021-04-06 13:30:19,094-INFO: pruned ratios: [0.2]
2021-04-06 13:30:19,354-INFO: pruning: conv7_2_extra1_weights
C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py:96: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  _logger.warn(""Skip operator [{}]"".format(op.type()))
2021-04-06 13:30:21,463-WARNING: Skip operator [conditional_block]
2021-04-06 13:30:21,463-WARNING: Skip operator [conditional_block]
2021-04-06 13:30:21,463-WARNING: Skip operator [conditional_block]
2021-04-06 13:30:21,463-WARNING: Skip operator [conditional_block]
2021-04-06 13:30:21,463-WARNING: Skip operator [conditional_block]
Traceback (most recent call last):
  File ""slim/prune/prune.py"", line 415, in <module>
    main()
  File ""slim/prune/prune.py"", line 199, in main
    only_graph=False)[0]
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\pruner.py"", line 102, in prune
    visited)[0]  # [(name, axis, pruned_idx)]
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\group_param.py"", line 87, in collect_convs
    walker.prune(param, pruned_axis=0, pruned_idx=[])
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 60, in prune
    self._prune(var, pruned_axis, pruned_idx)
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 169, in _prune
    self._prune_op(op, output_var, channel_axis, pruned_idx)
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 109, in _prune_op
    walker.prune(var, pruned_axis, pruned_idx)
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 60, in prune
    self._prune(var, pruned_axis, pruned_idx)
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 258, in _prune
    self._prune_op(op, param_var, 0, pruned_idx)
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 109, in _prune_op
    walker.prune(var, pruned_axis, pruned_idx)
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 60, in prune
    self._prune(var, pruned_axis, pruned_idx)
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 610, in _prune
    self._prune_op(op, out_var, pruned_axis, pruned_idx)
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 109, in _prune_op
    walker.prune(var, pruned_axis, pruned_idx)
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 60, in prune
    self._prune(var, pruned_axis, pruned_idx)
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 443, in _prune
    self._prune_op(op, in_var, pruned_axis, pruned_idx)
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 109, in _prune_op
    walker.prune(var, pruned_axis, pruned_idx)
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 60, in prune
    self._prune(var, pruned_axis, pruned_idx)
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 437, in _prune
    self._prune_op(op, in_var, pruned_axis, pruned_idx)
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 109, in _prune_op
    walker.prune(var, pruned_axis, pruned_idx)
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 60, in prune
    self._prune(var, pruned_axis, pruned_idx)
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 614, in _prune
    self._prune_op(op, in_var, pruned_axis, pruned_idx)
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 109, in _prune_op
    walker.prune(var, pruned_axis, pruned_idx)
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 60, in prune
    self._prune(var, pruned_axis, pruned_idx)
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 370, in _prune
    self._visit_and_search(in_var, pruned_axis, pruned_idx)
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 80, in _visit_and_search
    self._prune_op(op, var, axis, transforms)
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 109, in _prune_op
    walker.prune(var, pruned_axis, pruned_idx)
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 58, in prune
    if self._visit(var, pruned_axis):
  File ""C:\Users\Administrator\.conda\envs\paddle_gpu_2\lib\site-packages\paddleslim\prune\prune_walker.py"", line 65, in _visit
    key = ""_"".join([key, self.op.all_inputs()[0].name()])
IndexError: list index out of range
W0406 13:29:55.975282 10404 device_context.cc:362] Please NOTE: device: 0, GPU Compute Capability: 5.0, Driver API Version: 10.2, Runtime API Version: 10.0
W0406 13:29:55.987251 10404 device_context.cc:372] device: 0, cuDNN Version: 7.6.


请问如何排除问题？用的是PaddleDetection自带的模型配置文件。"
关于获取检测特征的问题,PaddlePaddle/PaddleDetection,2021-04-04 09:56:43,6,question,2509,849840287,使用infer.py如何获得检测出来的特征呢
"InvalidArgumentError: The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 8, input(X)'s shape = [12, 30, 4], Attr(dim) = 0.",PaddlePaddle/PaddleDetection,2021-04-03 07:12:19,3,,2507,849598434,"整体报错日志：
2021-04-03 15:05:29,054 - WARNING - variable yolo_output.0.conv.1.weights not used
2021-04-03 15:05:29,054 - WARNING - variable yolo_output.2.conv.1.bias not used
2021-04-03 15:05:29,054 - WARNING - variable yolo_output.1.conv.1.bias not used
2021-04-03 15:05:29,054 - WARNING - variable yolo_output.0.conv.1.bias not used
2021-04-03 15:05:29,054 - WARNING - variable yolo_output.1.conv.1.weights not used
2021-04-03 15:05:29,054 - WARNING - variable yolo_output.2.conv.1.weights not used
2021-04-03 15:05:35,331 - INFO - places would be ommited when DataLoader is not iterable
Traceback (most recent call last):
  File ""PaddleDetection/tools/train.py"", line 374, in <module>
    main()
  File ""PaddleDetection/tools/train.py"", line 247, in main
    outs = exe.run(compiled_train_prog, fetch_list=train_values)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py"", line 1110, in run
    six.reraise(*sys.exc_info())
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/six.py"", line 703, in reraise
    raise value
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py"", line 1108, in run
    return_merged=return_merged)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py"", line 1251, in _run_impl
    return_merged=return_merged)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py"", line 913, in _run_parallel
    tensors = exe.run(fetch_var_names, return_merged)._move_to_list()
ValueError: In user code:

    File ""PaddleDetection/tools/train.py"", line 374, in <module>
      main()
    File ""PaddleDetection/tools/train.py"", line 120, in main
      train_fetches = model.train(feed_vars)
    File ""/home/aistudio/work/PaddleDetection/ppdet/modeling/architectures/yolo.py"", line 159, in train
      return self.build(feed_vars, mode='train')
    File ""/home/aistudio/work/PaddleDetection/ppdet/modeling/architectures/yolo.py"", line 85, in build
      gt_score, targets)
    File ""/home/aistudio/work/PaddleDetection/ppdet/modeling/anchor_heads/yolo_head.py"", line 401, in get_loss
      self.prefix_name)
    File ""/home/aistudio/work/PaddleDetection/ppdet/modeling/losses/yolo_loss.py"", line 69, in __call__
      mask_anchors, self._ignore_thresh)
    File ""/home/aistudio/work/PaddleDetection/ppdet/modeling/losses/yolo_loss.py"", line 191, in _get_fine_grained_loss
      num_classes, downsample, self._ignore_thresh, scale_x_y)
    File ""/home/aistudio/work/PaddleDetection/ppdet/modeling/losses/yolo_loss.py"", line 322, in _calc_obj_loss
      gts = fluid.layers.split(gt_box, batch_size, dim=0)
    File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/nn.py"", line 4944, in split
      type='split', inputs=inputs, outputs={'Out': outs}, attrs=attrs)
    File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layer_helper.py"", line 43, in append_op
      return self.main_program.current_block().append_op(*args, **kwargs)
    File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py"", line 3023, in append_op
      attrs=kwargs.get(""attrs"", None))
    File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py"", line 2107, in __init__
      for frame in traceback.extract_stack():

    InvalidArgumentError: The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 8, input(X)'s shape = [12, 30, 4], Attr(dim) = 0.
      [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:4 != 0:0.] (at /paddle/paddle/fluid/operators/split_op.h:43)
      [operator < split > error]

配置：
paddle2.0 gpu，paddledetection release/0.4    

感觉是维度问题，但又不知道咋改，向百度大佬请教"
请问一下，cornernet_squeeze_hg104.yml里面是只用了Soft-NMS和detections_per_im这两种后处理操作吗？,PaddlePaddle/PaddleDetection,2021-04-01 10:47:27,7,question,2498,848353703,"尊敬的开发者，你好，
请问一下，[cornernet_squeeze_hg104.yml](https://github.com/PaddlePaddle/PaddleDetection/blob/master/configs/anchor_free/cornernet_squeeze_hg104.yml)里面是只用了Soft-NMS和detections_per_im这两种后处理操作吗？
https://github.com/PaddlePaddle/PaddleDetection/blob/84e79e8760ba2ef7fbc3972d865316af9aade014/configs/anchor_free/cornernet_squeeze_hg104.yml#L27-L31

期待您的回复！
"
focs的预测输出出现-1,PaddlePaddle/PaddleDetection,2021-03-31 14:06:06,4,,2489,846734610,
训练一轮后自动退出了？,PaddlePaddle/PaddleDetection,2021-03-31 00:32:20,13,question,2475,845536145,安装的paddle2.0.1，训练maskrcnn，静态图训练10000次后自动退出了，没有任何错误，配置文件里面max_iter是180000，动态图训练的时候一轮结束后也自动退出了，配置文件里面epoch是12，也就是每次到了保存模型参数的时候，程序就自己退出了，配置文件里面只改了coco数据集路径，退出没有报任何错误，知道这是什么问题吗？（我都没有加载预训练的权重）
aistudio评估retinanet_r50_fpn_1x的checkpoint报错,PaddlePaddle/PaddleDetection,2021-03-27 06:30:57,2,bug,2448,842431161,"我在aistudio用retinanet_r50_fpn_1x训练模型后使用eval.py评估模型生成的checkpoint，结果报错，版本是PaddleDetection-release-2.0-rc，yml配置文件我只修改了lr和数据集路径
Traceback (most recent call last):
  File ""tools/eval.py"", line 193, in <module>
    main()
  File ""tools/eval.py"", line 160, in main
    sub_eval_prog, sub_keys, sub_values, resolution)
  File ""/home/aistudio/work/PaddleDetection1/ppdet/utils/eval_utils.py"", line 148, in eval_run
    return_numpy=False)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py"", line 1110, in run
    six.reraise(*sys.exc_info())
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/six.py"", line 703, in reraise
    raise value
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py"", line 1108, in run
    return_merged=return_merged)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py"", line 1251, in _run_impl
    return_merged=return_merged)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py"", line 913, in _run_parallel
    tensors = exe.run(fetch_var_names, return_merged)._move_to_list()
RuntimeError: (PreconditionNotMet) Tensor not initialized yet when Tensor::place() is called.
  [Hint: holder_ should not be null.] (at /paddle/paddle/fluid/framework/tensor.h:194)

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::framework::SignalHandle(char const*, int)
1   paddle::platform::GetCurrentTraceBackString[abi:cxx11]()

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1616826130 (unix time) try ""date -d @1616826130"" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3e800000846) received by PID 2118 (TID 0x7fb6037fe700) from PID 2118 ***]

Aborted (core dumped)"
请问一下，cornernet中的ae_threshold参数表示什么含义呢？,PaddlePaddle/PaddleDetection,2021-03-25 05:32:31,4,question,2429,840574088,"尊敬的开发者，你好，
请问一下，cornernet中的ae_threshold参数表示什么含义呢，
https://github.com/PaddlePaddle/PaddleDetection/blob/84e79e8760ba2ef7fbc3972d865316af9aade014/configs/anchor_free/cornernet_squeeze_hg104.yml#L23

期待你们的回复！"
maskRcnn推理内存溢出,PaddlePaddle/PaddleDetection,2021-03-24 10:15:57,19,,2420,839564497,"输入到maskrcnn推理程序的图片如果包含全白色的图片，会导致程序崩溃，怀疑网络在检测不出任何目标的情况下会出问题。复现步骤如下：
1.下载https://paddlemodels.bj.bcebos.com/object_detection/mask_rcnn_r50_1x.tar模型，解压
2. 用命令python tools/export_model.py  -c configs/mask_rcnn_r50_1x.yml --output_dir=inference_model -o weights=PaddleDetection-release-2.0-beta/output/mask_rcnn_r50_1x 转换成deploy的模型格式。
3. 修改deploy/python/infer.py文件，使得它可以推理文件夹里所有的图片。增加的函数类似

```
def predict_image(detector):
    for img_file in os.listdir(FLAGS.image_dir):
        img_file = os.path.join(FLAGS.image_dir, img_file)
        results = detector.predict(img_file, FLAGS.threshold)
        visualize(
            img_file,
            results,
            detector.config.labels,
            mask_resolution=detector.config.mask_resolution,
            output_dir=FLAGS.output_dir,
            threshold=FLAGS.threshold)
```
4. 在图片文件夹中放入普通街景图片，同时混入几张全白色图片，运行命令：
python deploy/python/infer.py --model_dir=inference_model/mask_rcnn_r50_1x --image_dir=./image --use_gpu=True
在推理几张图片后程序崩溃：

```
-----------  Running Arguments -----------
image_dir: /home/sy/work/ocr_car/PaddleDetection/bill_image
model_dir: inference_model/mask_rcnn_r50_1x
output_dir: output
run_mode: fluid
threshold: 0.5
use_gpu: True
------------------------------------------
-----------  Model Configuration -----------
Model Arch: RCNN
Use Paddle Executor: False
Transform Order: 
--transform op: Normalize
--transform op: Resize
--transform op: Permute
--------------------------------------------
W0324 17:55:07.368857  6811 analysis_predictor.cc:1058] Deprecated. Please use CreatePredictor instead.
Inference: 102.0808219909668 ms per batch image
[WARNNING] No object detected.
save result to: output/4.jpg
Inference: 101.22370719909668 ms per batch image
[WARNNING] No object detected.
save result to: output/5.jpg
Inference: 102.63204574584961 ms per batch image
save result to: output/sp6940546707212751477_NoBarcode1.jpg
Traceback (most recent call last):
  File ""deploy/python/flask_infer.py"", line 586, in <module>
    main()
  File ""deploy/python/flask_infer.py"", line 544, in main
    predict_image(detector)
  File ""deploy/python/flask_infer.py"", line 484, in predict_image
    results = detector.predict(img_file, FLAGS.threshold)
  File ""deploy/python/flask_infer.py"", line 190, in predict
    np_boxes = boxes_tensor.copy_to_cpu()
OSError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   void paddle::ZeroCopyTensor::copy_to_cpu<float>(float*)
1   void paddle::memory::Copy<paddle::platform::CPUPlace, paddle::platform::CUDAPlace>(paddle::platform::CPUPlace, void*, paddle::platform::CUDAPlace, void const*, unsigned long, CUstream_st*)
2   paddle::platform::GpuMemcpyAsync(void*, void const*, unsigned long, cudaMemcpyKind, CUstream_st*)
3   paddle::platform::EnforceNotMet::EnforceNotMet(paddle::platform::ErrorSummary const&, char const*, int)
4   paddle::platform::GetCurrentTraceBackString[abi:cxx11]()

----------------------
Error Message Summary:
----------------------
ExternalError:  Cuda error(700), an illegal memory access was encountered.
  [Advise: Please search for the error code(700) on website( https://docs.nvidia.com/cuda/archive/10.0/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038 ) to get Nvidia's official solution about CUDA Error.] (at /paddle/paddle/fluid/platform/gpu_info.cc:296)

terminate called after throwing an instance of 'paddle::platform::EnforceNotMet'
  what():  

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::AnalysisPredictor::~AnalysisPredictor()
1   paddle::AnalysisPredictor::~AnalysisPredictor()
2   paddle::memory::allocation::AllocatorFacade::Release(paddle::platform::Place const&)
3   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
4   paddle::memory::allocation::AlignedAllocation::~AlignedAllocation()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(paddle::memory::allocation::Allocation*)
6   paddle::platform::RecordedCudaFree(void*, unsigned long, int)
7   paddle::platform::EnforceNotMet::EnforceNotMet(paddle::platform::ErrorSummary const&, char const*, int)
8   paddle::platform::GetCurrentTraceBackString[abi:cxx11]()

----------------------
Error Message Summary:
----------------------
ExternalError:  Cuda error(700), an illegal memory access was encountered.
  [Advise: Please search for the error code(700) on website( https://docs.nvidia.com/cuda/archive/10.0/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038 ) to get Nvidia's official solution about CUDA Error.] (at /paddle/paddle/fluid/platform/gpu_info.cc:411)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::AnalysisPredictor::~AnalysisPredictor()
1   paddle::AnalysisPredictor::~AnalysisPredictor()
2   paddle::memory::allocation::AllocatorFacade::Release(paddle::platform::Place const&)
3   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
4   paddle::memory::allocation::AlignedAllocation::~AlignedAllocation()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(paddle::memory::allocation::Allocation*)
6   paddle::framework::SignalHandle(char const*, int)
7   paddle::platform::GetCurrentTraceBackString[abi:cxx11]()

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1616579713 (unix time) try ""date -d @1616579713"" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3e800001a9b) received by PID 6811 (TID 0x7fc5ce220700) from PID 6811 ***]

Aborted (core dumped)
```
请问这个问题如何修复？"
预测报错 KeyError: 1,PaddlePaddle/PaddleDetection,2021-03-24 08:16:35,2,,2418,839469836,"E:\Development\StudioApp\PaddleDetection>python tools/infer.py -c configs/yolov3
_mobilenet_v1_roadsign.yml --infer_img=demo/123.jpg -o use_gpu=false
config.metric=VOC
2021-03-24 16:15:27,514 - INFO - Load categories from dataset/water_meter_voc/la
bel_list.txt
2021-03-24 16:15:28,684 - INFO - Infer iter 0
Traceback (most recent call last):
  File ""tools/infer.py"", line 272, in <module>
    main()
  File ""tools/infer.py"", line 195, in main
    bbox_results = bbox2out([res], clsid2catid, is_bbox_normalized)
  File ""E:\Development\StudioApp\PaddleDetection\ppdet\utils\coco_eval.py"", line
 315, in bbox2out
    catid = (clsid2catid[int(clsid)])
KeyError: 1
"
关于C#使用GPU进行持续模型预测时内存不断上升的问题,PaddlePaddle/PaddleDetection,2021-03-24 08:03:52,9,,2417,839460907,"ppdet版本-master，预测库版本2.0.0rc0
出现问题描述如下
1、使用官方的cpp，生成了dll，具体参考的是[c++部署](https://zhuanlan.zhihu.com/p/268657833?utm_source=wechat_session&utm_medium=social&utm_oi=692640605041340416&s_r=0)和[c++生成dll,c#调用](https://zhuanlan.zhihu.com/p/280206376?utm_source=wechat_session&utm_medium=social&utm_oi=692640605041340416&s_r=0)
2、所使用网络为SSD+VGG16，在实际应用过程中发现，当使用定时器，每隔100ms预测一次，每次预测大概耗时15ms
3、载入模型之后，程序内存为560M，第一次预测之后，程序内存为1668M
4、随着预测的不断进行，程序内存也随之不断上升，预测完3千次之后，程序内存达到2500M，且继续上升
5、GC.COLLECT无效果"
自定义数据集评估出错,PaddlePaddle/PaddleDetection,2021-03-23 12:43:19,2,custom dataset,2413,838687201,"  File ""tools/eval.py"", line 193, in <module>
    main()
  File ""tools/eval.py"", line 175, in main
    save_only=save_only)
  File ""/home/zzf/Desktop/PaddleDetection/ppdet/utils/eval_utils.py"", line 241, in eval_results
    save_only=save_only)
  File ""/home/zzf/Desktop/PaddleDetection/ppdet/utils/coco_eval.py"", line 102, in bbox_eval
    map_stats = cocoapi_eval(outfile, 'bbox', coco_gt=coco_gt)
  File ""/home/zzf/Desktop/PaddleDetection/ppdet/utils/coco_eval.py"", line 238, in cocoapi_eval
    coco_dt = coco_gt.loadRes(jsonfile)
  File ""/home/zzf/miniconda3/envs/torch1.7.1/lib/python3.7/site-packages/pycocotools/coco.py"", line 388, in loadRes
    'Results do not correspond to current coco set'
AssertionError: Results do not correspond to current coco set


infer.py可以得到正确的图片和标注结果
但是eval.py对自己训练的model 不能得出结果。"
mask rcnn serving 部署出错,PaddlePaddle/PaddleDetection,2021-03-19 04:25:33,6,deploy,2378,835546661,"PaddleDetection版本：2.0 beta
serving 版本：0.4
paddlepaddle-gpu :         2.0.0rc1
用serving方式部署了一个基于私有数据集的MaskRcnn模型，测试如下（采用多线程同时发送http请求方式）：
1）全部用只有背景的图片测试，正常工作。
2）全部用带有目标的图片测试，正常工作，检测结果正常，目标都能检出。
3）混合有目标的图片和没有目标的背景图片测试，出现如下错误
ERROR 2021-03-19 12:09:25,110 [operator.py:733] (data_id=19 log_id=19) [bill_det|0] Failed to process(batch: [19]): In user code:

    File ""tools/export_serving_model.py"", line 107, in <module>
      main()
    File ""tools/export_serving_model.py"", line 93, in main
      save_serving_model(FLAGS, exe, feed_vars, test_fetches, infer_prog)
    File ""tools/export_serving_model.py"", line 63, in save_serving_model
      main_program=infer_prog)
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle_serving_client/io/__init__.py"", line 47, in save_model
      main_program=main_program)
    File ""<decorator-gen-79>"", line 2, in save_inference_model
      
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle/fluid/wrapped_decorator.py"", line 25, in __impl__
      return wrapped_func(*args, **kwargs)
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle/fluid/framework.py"", line 232, in __impl__
      return func(*args, **kwargs)
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle/fluid/io.py"", line 1335, in save_inference_model
      var, 1., name=""save_infer_model/scale_{}"".format(i))
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle/fluid/layers/nn.py"", line 11410, in scale
      type='scale', inputs=inputs, outputs={'Out': out}, attrs=attrs)
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle/fluid/layer_helper.py"", line 43, in append_op
      return self.main_program.current_block().append_op(*args, **kwargs)
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle/fluid/framework.py"", line 2846, in append_op
      attrs=kwargs.get(""attrs"", None))
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle/fluid/framework.py"", line 1931, in __init__
      for frame in traceback.extract_stack():

    ExternalError:  Cuda error(700), an illegal memory access was encountered.
      [Advise: Please search for the error code(700) on website( https://docs.nvidia.com/cuda/archive/10.0/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038 ) to get Nvidia's official solution about CUDA Error.] (at /paddle/paddle/fluid/platform/stream/cuda_stream.cc:65)
      [Hint: If you need C++ stacktraces for debugging, please set `FLAGS_call_stack_level=2`.]
      [operator < scale > error]
Traceback (most recent call last):
  File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle_serving_server_gpu/pipeline/operator.py"", line 728, in _run_process
    midped_batch = self.process(feed_batch, typical_logid)
  File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle_serving_server_gpu/pipeline/operator.py"", line 437, in process
    log_id=typical_logid)
  File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle_serving_app/local_predict.py"", line 201, in predict
    self.predictor.zero_copy_run()
OSError: In user code:

    File ""tools/export_serving_model.py"", line 107, in <module>
      main()
    File ""tools/export_serving_model.py"", line 93, in main
      save_serving_model(FLAGS, exe, feed_vars, test_fetches, infer_prog)
    File ""tools/export_serving_model.py"", line 63, in save_serving_model
      main_program=infer_prog)
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle_serving_client/io/__init__.py"", line 47, in save_model
      main_program=main_program)
    File ""<decorator-gen-79>"", line 2, in save_inference_model
      
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle/fluid/wrapped_decorator.py"", line 25, in __impl__
      return wrapped_func(*args, **kwargs)
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle/fluid/framework.py"", line 232, in __impl__
      return func(*args, **kwargs)
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle/fluid/io.py"", line 1335, in save_inference_model
      var, 1., name=""save_infer_model/scale_{}"".format(i))
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle/fluid/layers/nn.py"", line 11410, in scale
      type='scale', inputs=inputs, outputs={'Out': out}, attrs=attrs)
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle/fluid/layer_helper.py"", line 43, in append_op
      return self.main_program.current_block().append_op(*args, **kwargs)
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle/fluid/framework.py"", line 2846, in append_op
      attrs=kwargs.get(""attrs"", None))
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle/fluid/framework.py"", line 1931, in __init__
      for frame in traceback.extract_stack():

    ExternalError:  Cuda error(700), an illegal memory access was encountered.
      [Advise: Please search for the error code(700) on website( https://docs.nvidia.com/cuda/archive/10.0/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038 ) to get Nvidia's official solution about CUDA Error.] (at /paddle/paddle/fluid/platform/stream/cuda_stream.cc:65)
      [Hint: If you need C++ stacktraces for debugging, please set `FLAGS_call_stack_level=2`.]
      [operator < scale > error]
ERROR 2021-03-19 12:09:25,141 [dag.py:409] (data_id=19 log_id=0) Failed to predict: (data_id=19 log_id=19) [bill_det|0] Failed to process(batch: [19]): In user code:

    File ""tools/export_serving_model.py"", line 107, in <module>
      main()
    File ""tools/export_serving_model.py"", line 93, in main
      save_serving_model(FLAGS, exe, feed_vars, test_fetches, infer_prog)
    File ""tools/export_serving_model.py"", line 63, in save_serving_model
      main_program=infer_prog)
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle_serving_client/io/__init__.py"", line 47, in save_model
      main_program=main_program)
    File ""<decorator-gen-79>"", line 2, in save_inference_model
      
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle/fluid/wrapped_decorator.py"", line 25, in __impl__
      return wrapped_func(*args, **kwargs)
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle/fluid/framework.py"", line 232, in __impl__
      return func(*args, **kwargs)
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle/fluid/io.py"", line 1335, in save_inference_model
      var, 1., name=""save_infer_model/scale_{}"".format(i))
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle/fluid/layers/nn.py"", line 11410, in scale
      type='scale', inputs=inputs, outputs={'Out': out}, attrs=attrs)
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle/fluid/layer_helper.py"", line 43, in append_op
      return self.main_program.current_block().append_op(*args, **kwargs)
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle/fluid/framework.py"", line 2846, in append_op
      attrs=kwargs.get(""attrs"", None))
    File ""/home/sy/.conda/envs/solov2-paddle-2.0rc1/lib/python3.7/site-packages/paddle/fluid/framework.py"", line 1931, in __init__
      for frame in traceback.extract_stack():

    ExternalError:  Cuda error(700), an illegal memory access was encountered.
      [Advise: Please search for the error code(700) on website( https://docs.nvidia.com/cuda/archive/10.0/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038 ) to get Nvidia's official solution about CUDA Error.] (at /paddle/paddle/fluid/platform/stream/cuda_stream.cc:65)
      [Hint: If you need C++ stacktraces for debugging, please set `FLAGS_call_stack_level=2`.]
      [operator < scale > error]

请问这个问题如何解决？

"
"重新安装英伟达官方的系统，jetson xavier nx 出现 ""no kernel image""问题，迫切希望哪位大佬能帮帮忙！",PaddlePaddle/PaddleDetection,2021-03-16 01:45:34,13,install,2359,832333665,"![image](https://user-images.githubusercontent.com/32217129/111243401-fd8cfd00-863b-11eb-8714-89b0110ea982.png)
问题 描述如上，包含命令，就是官方的测试用例。
硬件：jetson xavier nx
系统：jetpack 4.5.1（最新）
paddlepaddle：gpu_2.0.0

这个问题困扰很久了，实在不知道怎么办？迫切希望哪位大佬能帮帮忙！非常感谢！！！
"
ppyolo训练正常，评估过程出现错误,PaddlePaddle/PaddleDetection,2021-03-14 03:07:50,8,custom dataset,2343,831052298,"`2021-03-14 10:41:27,787-INFO: Test iter 0
2021-03-14 10:41:31,010-INFO: Test finish iter 44
2021-03-14 10:41:31,011-INFO: Total number of images: 87, inference time: 24.439726468611074 fps.
loading annotations into memory...
Done (t=0.05s)
creating index...
index created!
Traceback (most recent call last):
  File ""train.py"", line 399, in <module>
    main()
  File ""train.py"", line 320, in main
    cfg['EvalReader']['dataset'])
  File ""/home/shujisheng/download/PaddleDetection_r_2.0_rc/ppdet/utils/eval_utils.py"", line 241, in eval_results
    save_only=save_only)
  File ""/home/shujisheng/download/PaddleDetection_r_2.0_rc/ppdet/utils/coco_eval.py"", line 87, in bbox_eval
    results, clsid2catid, is_bbox_normalized=is_bbox_normalized)
  File ""/home/shujisheng/download/PaddleDetection_r_2.0_rc/ppdet/utils/coco_eval.py"", line 315, in bbox2out
    catid = (clsid2catid[int(clsid)])
**KeyError: 2
terminate called without an active exception
W0314 10:41:32.189633 21160 init.cc:226] Warning: PaddlePaddle catches a failure signal, it may not work properly
W0314 10:41:32.189677 21160 init.cc:228] You could check whether you killed PaddlePaddle thread/process accidentally or report the case to PaddlePaddle**
W0314 10:41:32.189685 21160 init.cc:231] The detail failure signal is:

W0314 10:41:32.189697 21160 init.cc:234] *** Aborted at 1615689692 (unix time) try ""date -d @1615689692"" if you are using GNU date ***
W0314 10:41:32.193388 21160 init.cc:234] PC: @                0x0 (unknown)
W0314 10:41:32.193491 21160 init.cc:234] *** SIGABRT (@0x3eb00004a9c) received by PID 19100 (TID 0x7fe6e97fe700) from PID 19100; stack trace: ***
W0314 10:41:32.196112 21160 init.cc:234]     @     0x7fea36d24390 (unknown)
W0314 10:41:32.199249 21160 init.cc:234]     @     0x7fea36066438 gsignal
W0314 10:41:32.201252 21160 init.cc:234]     @     0x7fea3606803a abort
W0314 10:41:32.202811 21160 init.cc:234]     @     0x7fe9e20eb892 __gnu_cxx::__verbose_terminate_handler()
W0314 10:41:32.204593 21160 init.cc:234]     @     0x7fe9e20e9f69 __cxxabiv1::__terminate()
W0314 10:41:32.206133 21160 init.cc:234]     @     0x7fe9e20e9fab std::terminate()
W0314 10:41:32.207245 21160 init.cc:234]     @     0x7fe9e20e9c7c __gxx_personality_v0
W0314 10:41:32.208137 21160 init.cc:234]     @     0x7fe9e23dcbc8 _Unwind_ForcedUnwind_Phase2
W0314 10:41:32.209748 21160 init.cc:234]     @     0x7fe9e23dceae _Unwind_ForcedUnwind
W0314 10:41:32.212064 21160 init.cc:234]     @     0x7fea36d23070 __GI___pthread_unwind
W0314 10:41:32.213795 21160 init.cc:234]     @     0x7fea36d1b845 __pthread_exit
W0314 10:41:32.216584 21160 init.cc:234]     @     0x55b17954be96 PyThread_exit_thread
W0314 10:41:32.218968 21160 init.cc:234]     @     0x55b17946aed8 PyEval_RestoreThread
W0314 10:41:32.219485 21160 init.cc:234]     @     0x7fe9d494f7c2 (unknown)
W0314 10:41:32.222203 21160 init.cc:234]     @     0x55b179506427 _PyMethodDef_RawFastCallKeywords
W0314 10:41:32.222749 21160 init.cc:234]     @     0x55b179507ad8 call_function
W0314 10:41:32.224927 21160 init.cc:234]     @     0x55b17953274a _PyEval_EvalFrameDefault
W0314 10:41:32.227216 21160 init.cc:234]     @     0x55b179475af2 _PyEval_EvalCodeWithName
W0314 10:41:32.229890 21160 init.cc:234]     @     0x55b17947706d _PyFunction_FastCallDict
W0314 10:41:32.231832 21160 init.cc:234]     @     0x55b179495c4e _PyObject_Call_Prepend
W0314 10:41:32.232100 21160 init.cc:234]     @     0x55b179575e99 slot_tp_call
W0314 10:41:32.234983 21160 init.cc:234]     @     0x55b179506a5b _PyObject_FastCallKeywords
W0314 10:41:32.235390 21160 init.cc:234]     @     0x55b179507b59 call_function
W0314 10:41:32.237493 21160 init.cc:234]     @     0x55b17952e4ac _PyEval_EvalFrameDefault
W0314 10:41:32.240200 21160 init.cc:234]     @     0x55b179476ead _PyFunction_FastCallDict
W0314 10:41:32.242513 21160 init.cc:234]     @     0x55b179495c4e _PyObject_Call_Prepend
W0314 10:41:32.242767 21160 init.cc:234]     @     0x55b179575e99 slot_tp_call
W0314 10:41:32.245306 21160 init.cc:234]     @     0x55b179506a5b _PyObject_FastCallKeywords
W0314 10:41:32.245849 21160 init.cc:234]     @     0x55b179507b59 call_function
W0314 10:41:32.248188 21160 init.cc:234]     @     0x55b17953274a _PyEval_EvalFrameDefault
W0314 10:41:32.250527 21160 init.cc:234]     @     0x55b179475af2 _PyEval_EvalCodeWithName
W0314 10:41:32.253216 21160 init.cc:234]     @     0x55b17947706d _PyFunction_FastCallDict
Aborted
`"
关于使用VOC数据来训练ssdlite_mobilenetV3_small mAP太低的问题,PaddlePaddle/PaddleDetection,2021-03-12 07:00:57,14,config,2333,829820079,"我利用VOC2012来训练ssdlite_mobilenetV3_small, 根据在训练时显示的信息，loss: 6-7之间；然后利用voc2007进行验证的时候，mAP只有9.39%，如图：
![image](https://user-images.githubusercontent.com/32217129/110904313-bc4ad380-8343-11eb-9912-ff4d7dee09d1.png)

配置文件如下：
```yml
architecture: SSD
use_gpu: true
max_iters: 5000
snapshot_iter: 20000
log_iter: 20
metric: VOC
pretrain_weights: pretrainWeights/MobileNetV3_small_x1_0_ssld_pretrained
save_dir: output
weights: output/ssdlite_mobilenet_v3_small/best_model
# 80(label_class) + 1(background)
num_classes: 21

SSD:
  backbone: MobileNetV3
  multi_box_head: SSDLiteMultiBoxHead
  output_decoder:
    background_label: 0
    keep_top_k: 200
    nms_eta: 1.0
    nms_threshold: 0.45
    nms_top_k: 400
    score_threshold: 0.01

MobileNetV3:
  scale: 1.0
  model_name: small
  extra_block_filters: [[256, 512], [128, 256], [128, 256], [64, 128]]
  feature_maps: [5, 7, 8, 9, 10, 11]
  lr_mult_list: [0.25, 0.25, 0.5, 0.5, 0.75]
  conv_decay: 0.00004
  multiplier: 0.5

SSDLiteMultiBoxHead:
  aspect_ratios: [[2.], [2., 3.], [2., 3.], [2., 3.], [2., 3.], [2., 3.]]
  base_size: 320
  steps: [16, 32, 64, 107, 160, 320]
  flip: true
  clip: true
  max_ratio: 95
  min_ratio: 20
  offset: 0.5
  conv_decay: 0.00004

LearningRate:
  base_lr: 0.4
  schedulers:
  - !CosineDecay
    max_iters: 400000
  - !LinearWarmup
    start_factor: 0.33333
    steps: 2000

OptimizerBuilder:
  optimizer:
    momentum: 0.9
    type: Momentum
  regularizer:
    factor: 0.0005
    type: L2

TrainReader:
  inputs_def:
    image_shape: [3, 320, 320]
    fields: ['image', 'gt_bbox', 'gt_class']
  dataset:
    !VOCDataSet
    dataset_dir: dataset/voc
    anno_path: trainval.txt
  sample_transforms:
  - !DecodeImage
    to_rgb: true
  - !RandomDistort
    brightness_lower: 0.875
    brightness_upper: 1.125
    is_order: true
  - !RandomExpand
    fill_value: [123.675, 116.28, 103.53]
  - !RandomCrop
    allow_no_crop: false
  - !NormalizeBox {}
  - !ResizeImage
    interp: 1
    target_size: 320
    use_cv2: false
  - !RandomFlipImage
    is_normalized: false
  - !NormalizeImage
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    is_scale: true
    is_channel_first: false
  - !Permute
    to_bgr: false
    channel_first: true
  batch_size: 32
  shuffle: true
  drop_last: true
  # Number of working threads/processes. To speed up, can be set to 16 or 32 etc.
  worker_num: 8
  # Size of shared memory used in result queue. After increasing `worker_num`, need expand `memsize`.
  memsize: 8G
  # Buffer size for multi threads/processes.one instance in buffer is one batch data.
  # To speed up, can be set to 64 or 128 etc.
  bufsize: 32
  use_process: true


EvalReader:
  inputs_def:
    image_shape: [3, 320, 320]
    fields: ['image', 'im_shape', 'im_id', 'gt_bbox', 'gt_class', 'is_difficult']
  dataset:
    !VOCDataSet
    dataset_dir: dataset/voc
    anno_path: test.txt
  sample_transforms:
  - !DecodeImage
    to_rgb: true
  - !NormalizeBox {}
  - !ResizeImage
    interp: 1
    target_size: 320
    use_cv2: false
  - !NormalizeImage
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    is_scale: true
    is_channel_first: false
  - !Permute
    to_bgr: false
    channel_first: True
  batch_size: 8
  worker_num: 8
  bufsize: 32
  use_process: false

TestReader:
  inputs_def:
    image_shape: [3,320,320]
    fields: ['image', 'im_id', 'im_shape']
  dataset:
    !ImageFolder
    anno_path: annotations/instances_val2017.json
  sample_transforms:
  - !DecodeImage
    to_rgb: true
  - !ResizeImage
    interp: 1
    max_size: 0
    target_size: 320
    use_cv2: true
  - !NormalizeImage
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    is_scale: true
    is_channel_first: false
  - !Permute
    to_bgr: false
    channel_first: True
  batch_size: 1
```"
我在win10训练solov2，使用的是coco数据集，效果很差，需要修改什么配置文件吗？,PaddlePaddle/PaddleDetection,2021-03-08 08:05:41,10,documentation#question,2304,824296156,如题，我没动配置文件，只是把数据集下载下来并放到了dataset下，然后就开始训练了，训练到30000个epoch时，测试结果很差，这是什么问题啊？需要修改什么东西吗？
训练-裁剪-训练-导出报错,PaddlePaddle/PaddleDetection,2021-03-04 02:27:15,5,,2282,821688005,"PaddleDetection是2.0版本
项目是在平台上运行的
使用PaddleDetection训练好yolov3_mobilenet_v3模型后进行模型裁剪
裁剪完评估完成后导出报错
这是我执行的脚本
`
!python slim/prune/export_model.py \
-c configs/yolov3_mobilenet_v3.yml \
--pruned_params ""yolo_block.0.0.0.conv.weights,yolo_block.0.0.1.conv.weights,yolo_block.0.1.0.conv.weights,yolo_block.0.1.1.conv.weights,yolo_block.0.2.conv.weights,yolo_block.0.tip.conv.weights,yolo_block.1.0.0.conv.weights,yolo_block.1.0.1.conv.weights,yolo_block.1.1.0.conv.weights,yolo_block.1.1.1.conv.weights,yolo_block.1.2.conv.weights,yolo_block.1.tip.conv.weights,yolo_block.2.0.0.conv.weights,yolo_block.2.0.1.conv.weights,yolo_block.2.1.0.conv.weights,yolo_block.2.1.1.conv.weights,yolo_block.2.2.conv.weights,yolo_block.2.tip.conv.weights"" \
--pruned_ratios=""0.7150126596733395,0.8177442961035291,0.8274278897456334,0.8373393786362668,0.7956892620674756,0.8445719578292334,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9"" \
-o weights=output/yolov3_mobilenet_v3/model_final
`

以下是报错信息，

```
[03-04 10:10:58 MainThread @logger.py:242] Argv: slim/prune/export_model.py -c configs/yolov3_mobilenet_v3.yml --pruned_params yolo_block.0.0.0.conv.weights,yolo_block.0.0.1.conv.weights,yolo_block.0.1.0.conv.weights,yolo_block.0.1.1.conv.weights,yolo_block.0.2.conv.weights,yolo_block.0.tip.conv.weights,yolo_block.1.0.0.conv.weights,yolo_block.1.0.1.conv.weights,yolo_block.1.1.0.conv.weights,yolo_block.1.1.1.conv.weights,yolo_block.1.2.conv.weights,yolo_block.1.tip.conv.weights,yolo_block.2.0.0.conv.weights,yolo_block.2.0.1.conv.weights,yolo_block.2.1.0.conv.weights,yolo_block.2.1.1.conv.weights,yolo_block.2.2.conv.weights,yolo_block.2.tip.conv.weights --pruned_ratios=0.7150126596733395,0.8177442961035291,0.8274278897456334,0.8373393786362668,0.7956892620674756,0.8445719578292334,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9 -o weights=output/yolov3_mobilenet_v3/model_final
[03-04 10:10:58 MainThread @utils.py:79] WRN paddlepaddle version: 2.0.0. The dynamic graph version of PARL is under development, not fully tested and supported
/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/parl/remote/communication.py:38: DeprecationWarning: 'pyarrow.default_serialization_context' is deprecated as of 2.0.0 and will be removed in a future version. Use pickle or the pyarrow IPC functionality instead.
  context = pyarrow.default_serialization_context()
/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pandas/core/tools/datetimes.py:3: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  from collections import MutableMapping
/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  from collections import Iterable, Mapping
/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  from collections import Sized
2021-03-04 10:11:00,126-INFO: pruned params: ['yolo_block.0.0.0.conv.weights', 'yolo_block.0.0.1.conv.weights', 'yolo_block.0.1.0.conv.weights', 'yolo_block.0.1.1.conv.weights', 'yolo_block.0.2.conv.weights', 'yolo_block.0.tip.conv.weights', 'yolo_block.1.0.0.conv.weights', 'yolo_block.1.0.1.conv.weights', 'yolo_block.1.1.0.conv.weights', 'yolo_block.1.1.1.conv.weights', 'yolo_block.1.2.conv.weights', 'yolo_block.1.tip.conv.weights', 'yolo_block.2.0.0.conv.weights', 'yolo_block.2.0.1.conv.weights', 'yolo_block.2.1.0.conv.weights', 'yolo_block.2.1.1.conv.weights', 'yolo_block.2.2.conv.weights', 'yolo_block.2.tip.conv.weights']
2021-03-04 10:11:00,126-INFO: pruned ratios: [0.7150126596733395, 0.8177442961035291, 0.8274278897456334, 0.8373393786362668, 0.7956892620674756, 0.8445719578292334, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9]
2021-03-04 10:11:00,169-INFO: pruning: yolo_block.0.0.0.conv.weights
Traceback (most recent call last):
  File ""slim/prune/export_model.py"", line 123, in <module>
    main()
  File ""slim/prune/export_model.py"", line 88, in main
    only_graph=True)
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddleslim/prune/pruner.py"", line 112, in prune
    g = self._transform(self.idx_selector(scores, ratio))
  File ""/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddleslim/prune/idx_selector.py"", line 57, in default_idx_selector
    0]  # sort channels by the first convolution's score
IndexError: list index out of range
```"
支持增量训练吗？,PaddlePaddle/PaddleDetection,2021-03-01 13:29:33,2,,2274,818868308,通过已有数据集训练出模型，上线后新增数据（标签与原数据集一致或者新增标签），不改变模型结构，paddleDetection支持增量训练吗？用原训练模型作为预训练模型加载，直接训练新增数据。请大神指教！！！！
faster rcnn 使用vgg16做骨干网络，生成yml配置文件问题,PaddlePaddle/PaddleDetection,2021-02-28 06:45:21,2,,2272,818137562,"使用VGG16作为faster rcnn的骨干网络，文件内容如下：

```
architecture: FasterRCNN
use_gpu: true
max_iters: 16000
log_iter: 50
save_dir: output
snapshot_iter: 200
pretrain_weights: https://paddle-imagenet-models-name.bj.bcebos.com/ResNet50_cos_pretrained.tar
metric: COCO
weights: output/faster_rcnn_vgg/model_final
num_classes: 5

FasterRCNN:
  backbone: VGG
  fpn: null
  rpn_head: RPNHead
  roi_extractor: RoIAlign
  bbox_head: BBoxHead
  bbox_assigner: BBoxAssigner
  rpn_only: false


VGG:
  depth: 16
  extra_block_filters:
  - - 256
    - 512
    - 1
    - 2
    - 3
  - - 128
    - 256
    - 1
    - 2
    - 3
  - - 128
    - 256
    - 0
    - 1
    - 3
  - - 128
    - 256
    - 0
    - 1
    - 3
  normalizations:
  - 20.0
  - -1
  - -1
  - -1
  - -1
  - -1
  with_extra_blocks: false

ResNetC5:
  depth: 50
  norm_type: affine_channel

RPNHead:
  anchor_generator:
    anchor_sizes:
    - 32
    - 64
    - 128
    - 256
    - 512
    aspect_ratios:
    - 0.5
    - 1.0
    - 2.0
    stride:
    - 16.0
    - 16.0
    variance:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
  num_classes: 1
  rpn_target_assign:
    rpn_batch_size_per_im: 256
    rpn_fg_fraction: 0.5
    rpn_negative_overlap: 0.3
    rpn_positive_overlap: 0.7
    rpn_straddle_thresh: 0.0
    use_random: true
  test_proposal:
    eta: 1.0
    min_size: 0.1
    nms_thresh: 0.5
    post_nms_top_n: 1000
    pre_nms_top_n: 6000
  train_proposal:
    eta: 1.0
    min_size: 0.1
    nms_thresh: 0.5
    post_nms_top_n: 2000
    pre_nms_top_n: 12000

RoIAlign:
  resolution: 7
  sampling_ratio: 0
  spatial_scale: 0.0625

BBoxAssigner:
  batch_size_per_im: 512
  bbox_reg_weights:
  - 0.1
  - 0.1
  - 0.2
  - 0.2
  bg_thresh_hi: 0.5
  bg_thresh_lo: 0.0
  fg_fraction: 0.25
  fg_thresh: 0.5
  num_classes: 81
  shuffle_before_sample: true

BBoxHead:
  bbox_loss:
    sigma: 1.0
  box_coder:
    axis: 1
    box_normalized: false
    code_type: decode_center_size
    prior_box_var:
    - 0.1
    - 0.1
    - 0.2
    - 0.2
  head: ResNetC5
  nms:
    background_label: 0
    keep_top_k: 100
    nms_eta: 1.0
    nms_threshold: 0.5
    nms_top_k: -1
    normalized: false
    score_threshold: 0.05
  num_classes: 5

LearningRate:
  base_lr: 0.001
  schedulers:
  - !PiecewiseDecay
    gamma:
    - 0.1
    - 0.1
    milestones:
    - 60000
    - 80000
    values: null
  - !LinearWarmup
    start_factor: 0.3333333333333333
    steps: 500

OptimizerBuilder:
  clip_grad_by_norm: null
  optimizer:
    momentum: 0.9
    type: Momentum
  regularizer:
    factor: 0.0001
    type: L2
_READER_: 'faster_reader.yml'
```

当我使用python tools/train.py -c configs/faster_vgg.yml  -o use_gpu=true --use_vdl=True --vdl_log_dir=vdl_dir/scalar  --eval训练的时候，报如下错误：

```
Traceback (most recent call last):
  File ""tools/train.py"", line 399, in <module>
    main()
  File ""tools/train.py"", line 136, in main
    train_fetches = model.train(feed_vars)
  File ""/home/aistudio/PaddleDetection/ppdet/modeling/architectures/faster_rcnn.py"", line 240, in train
    return self.build(feed_vars, 'train')
  File ""/home/aistudio/PaddleDetection/ppdet/modeling/architectures/faster_rcnn.py"", line 90, in build
    body_feat_names = list(body_feats.keys())
AttributeError: 'Variable' object has no attribute 'keys'
```
请问我哪里配置出错了吗"
yaml文件读取错误：UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 41: illegal multibyte sequence,PaddlePaddle/PaddleDetection,2021-02-26 19:29:24,4,,2268,817607475,"开发环境
OS: Window10 
Paddle = 2.0.0, git clone 最新版PaddleDetection
---
**问题复现：**
(pp) D:\pp\PaddleDetection>python -u tools/train.py -c configs/hw_configs/faster_rcnn_r50_vd_fpn_roadsign_coco_template.yml -o use_gpu=true --eval
Traceback (most recent call last):
  File ""tools/train.py"", line 399, in <module>
    main()
  File ""tools/train.py"", line 71, in main
    cfg = load_config(FLAGS.config)
  File ""D:\pp\PaddleDetection\ppdet\core\workspace.py"", line 91, in load_config
    cfg = merge_config(yaml.load(f, Loader=yaml.Loader), cfg)
  File ""C:\Users\Administrator\.conda\envs\pp\lib\site-packages\yaml\__init__.py"", line 112, in load
    loader = Loader(stream)
  File ""C:\Users\Administrator\.conda\envs\pp\lib\site-packages\yaml\loader.py"", line 44, in __init__
    Reader.__init__(self, stream)
  File ""C:\Users\Administrator\.conda\envs\pp\lib\site-packages\yaml\reader.py"", line 85, in __init__
    self.determine_encoding()
  File ""C:\Users\Administrator\.conda\envs\pp\lib\site-packages\yaml\reader.py"", line 124, in determine_encoding
    self.update_raw()
  File ""C:\Users\Administrator\.conda\envs\pp\lib\site-packages\yaml\reader.py"", line 178, in update_raw
    data = self.stream.read(size)
UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 41: illegal multibyte sequence

**推荐解决方案：**
PaddleDetection\ppdet\core\workspace.py"", line 90改为：
with open(file_path, 'r', encoding='utf-8') as f:
"
The number of valid bbox detected is zero ,PaddlePaddle/PaddleDetection,2021-02-26 02:26:30,2,,2266,816969006,"在《新手入门第五课-PaddleDetection十分钟上手》项目中，使用https://paddlemodels.bj.bcebos.com/object_detection/cascade_rcnn_dcn_r50_fpn_1x.tar 进行迁移学习，
训练过程出现The number of valid bbox detected is zero，请教原因。
个人经过如下排查：
1. 貌似只有dcn的预训练模型才会出现上面的问题，使用其他非dcn预训练模型进行迁移学习不会出现这个问题；
2. 应该不是环境问题，测试两个环境没有区别：paddle1.8.4+detection0.4，和paddle2.0+detection0.5；
3. yml配置应该不会有啥问题，也用更小的学习率测试，问题同样在；
4. 也不是数据集的问题，测试过roadsign、PBC等数据集。

个人结论：问题可能出在dcn的预训练模型！

附件：
使用roadsign_coco数据集，yml如下：
```
architecture: CascadeRCNN
use_gpu: true
max_iters: 9600
log_smooth_window: 20
save_dir: output
snapshot_iter: 800
metric: COCO
pretrain_weights: https://paddlemodels.bj.bcebos.com/object_detection/cascade_rcnn_dcn_r50_fpn_1x.tar
weights: output/f2/best_model
num_classes: 5
finetune_exclude_pretrained_params: ['cls_score, bbox_pred']

CascadeRCNN:
  backbone: ResNet
  fpn: FPN
  rpn_head: FPNRPNHead
  roi_extractor: FPNRoIAlign
  bbox_head: CascadeBBoxHead
  bbox_assigner: CascadeBBoxAssigner

ResNet:
  norm_type: affine_channel
  depth: 50
  feature_maps: [2, 3, 4, 5]
  freeze_at: 2
  variant: b
  dcn_v2_stages: [3, 4, 5]

FPN:
  max_level: 6
  min_level: 2
  num_chan: 256
  spatial_scale: [0.03125, 0.0625, 0.125, 0.25]
FPNRPNHead:
  anchor_generator:
    anchor_sizes: [32, 64, 128, 256, 512]
    aspect_ratios: [0.5, 1.0, 2.0]
    stride: [16.0, 16.0]
    variance: [1.0, 1.0, 1.0, 1.0]
  anchor_start_size: 32
  min_level: 2
  max_level: 6
  num_chan: 256
  rpn_target_assign:
    rpn_batch_size_per_im: 256
    rpn_fg_fraction: 0.5
    rpn_negative_overlap: 0.3
    rpn_positive_overlap: 0.7
    rpn_straddle_thresh: 0.0
#    use_random: true
  train_proposal:
    min_size: 0.0
    nms_thresh: 0.7
    pre_nms_top_n: 2000
    post_nms_top_n: 2000
  test_proposal:
    min_size: 0.0
    nms_thresh: 0.7
    pre_nms_top_n: 1000
    post_nms_top_n: 1000
FPNRoIAlign:
  canconical_level: 4
  canonical_size: 224
  max_level: 5
  min_level: 2
  sampling_ratio: 2
  box_resolution: 7

CascadeBBoxAssigner:
  batch_size_per_im: 512
  bbox_reg_weights: [10, 20, 30]
  bg_thresh_lo: [0.0, 0.0, 0.0]
  bg_thresh_hi: [0.5, 0.6, 0.7]
  fg_thresh: [0.5, 0.6, 0.7]
  fg_fraction: 0.25
CascadeBBoxHead:
  head: CascadeTwoFCHead
  nms:
    keep_top_k: 100
    nms_threshold: 0.5
    score_threshold: 0.05
CascadeTwoFCHead:
  mlp_dim: 1024

LearningRate:
  base_lr: 0.0001
  schedulers:
  - !PiecewiseDecay
    gamma: 0.1
    milestones:
    - 6400
    - 8800
  - !LinearWarmup
    start_factor: 0.3333333333333333
    steps: 100
OptimizerBuilder:
  optimizer:
    momentum: 0.9
    type: Momentum
  regularizer:
    factor: 0.0001
    type: L2

#####################################数据配置#####################################
TrainReader:
  inputs_def:
    fields: ['image', 'im_info', 'im_id', 'gt_bbox', 'gt_class', 'is_crowd']
  dataset:
    !COCODataSet
    dataset_dir: dataset/roadsign_coco
    image_dir: images
    anno_path: annotations/train.json
    with_background: true
  batch_size: 1
  bufsize: 2
  shuffle: true
  drop_empty: true
  drop_last: true
  mixup_epoch: -1
  use_process: true
  worker_num: 4
  sample_transforms:
  - !DecodeImage
    to_rgb: true
  - !RandomFlipImage
    is_normalized: true
    prob: 0.5
  - !NormalizeImage
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    is_scale: true
    is_channel_first: false
  - !ResizeImage
    target_size: 800
    max_size: 1333
    interp: 1
    use_cv2: true
  - !Permute
    channel_first: true
    to_bgr: false
  batch_transforms:
  - !PadBatch
    pad_to_stride: 32
    use_padded_im_info: false

EvalReader:
  batch_size: 1
  bufsize: 2
  shuffle: false
  drop_empty: false
  drop_last: false
  use_process: true
  worker_num: 4
  inputs_def:
    fields: ['image', 'im_info', 'im_id', 'im_shape']
  dataset:
    !COCODataSet
    dataset_dir: dataset/roadsign_coco
    image_dir: images
    anno_path: annotations/valid.json
    with_background: true
  sample_transforms:
  - !DecodeImage
    to_rgb: true
  - !NormalizeImage
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    is_scale: true
    is_channel_first: false
  - !ResizeImage
    target_size: 800
    max_size: 1333
    interp: 1
    use_cv2: true
  - !Permute
    to_bgr: false
    channel_first: true
  batch_transforms:
  - !PadBatch
    pad_to_stride: 32
    use_padded_im_info: true

TestReader:
  batch_size: 1
  drop_empty: false
  drop_last: false
  inputs_def:
    fields: ['image', 'im_info', 'im_id', 'im_shape']
  dataset:
    !ImageFolder
    anno_path: annotations/valid.json
    with_background: true
  sample_transforms:
  - !DecodeImage
    to_rgb: true
    with_mixup: false
  - !NormalizeImage
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    is_scale: true
    is_channel_first: false
  - !ResizeImage
    target_size: 800
    max_size: 1333
    interp: 1
    use_cv2: true
  - !Permute
    to_bgr: false
    channel_first: true
  batch_transforms:
  - !PadBatch
    pad_to_stride: 32
    use_padded_im_info: true
```
"
蒸馏模型报错WARNING: recv endsignal from outq with errmsg[consumer[consumer-12b-37] failed to map with error:[target0 not in samples]],PaddlePaddle/PaddleDetection,2021-02-10 08:42:40,2,model compression,2210,805301243,"命令：python slim/distillation/distill.py  -c configs/yolov3_mobilenet_v3_blade.yml -t configs/yolov3_r34_blade.yml --teacher_pretrained=https://paddlemodels.bj.bcebos.com/object_detection/yolov3_r34_voc.tar  -o use_fine_grained_loss=1


yolov3_r34_blade.yml文件:

```
architecture: YOLOv3
use_gpu: true
max_iters: 20000
log_iter: 20
save_dir: output
snapshot_iter: 200
metric: VOC
map_type: integral
pretrain_weights: https://paddle-imagenet-models-name.bj.bcebos.com/ResNet34_pretrained.tar
weights: output/yolov3_r34_blade/best_model
num_classes: 4
finetune_exclude_pretrained_params: ['yolo_output']
use_fine_grained_loss: false

YOLOv3:
  backbone: ResNet
  yolo_head: YOLOv3Head

ResNet:
  norm_type: sync_bn
  freeze_at: 0
  freeze_norm: false
  norm_decay: 0.
  depth: 34
  feature_maps: [3, 4, 5]

YOLOv3Head:
  anchor_masks: [[6, 7, 8], [3, 4, 5], [0, 1, 2]]
  anchors: [[10, 13], [16, 30], [33, 23],
            [30, 61], [62, 45], [59, 119],
            [116, 90], [156, 198], [373, 326]]
  yolo_loss: YOLOv3Loss
  nms:
    background_label: -1
    keep_top_k: 100
    nms_threshold: 0.3
    nms_top_k: 1000
    normalized: false
    score_threshold: 0.01

YOLOv3Loss:
  ignore_thresh: 0.5
  label_smooth: true

LearningRate:
  base_lr: 0.001
  schedulers:
  - !PiecewiseDecay
    gamma: 0.1
    milestones:
    - 55000
    - 62000
  - !LinearWarmup
    start_factor: 0.
    steps: 1000

OptimizerBuilder:
  optimizer:
    momentum: 0.9
    type: Momentum
  regularizer:
    factor: 0.0005
    type: L2

_READER_: 'yolov3_reader.yml'
# will merge TrainReader into yolov3_reader.yml
TrainReader:
  inputs_def:
    fields: ['image', 'gt_bbox', 'gt_class', 'gt_score']
    num_max_boxes: 50
  dataset:
    !VOCDataSet
    dataset_dir: dataset/voc
    anno_path: train.txt
    with_background: false
    use_default_label: false
  sample_transforms:
    - !DecodeImage
      to_rgb: True
      with_mixup: True
    - !MixupImage
      alpha: 1.5
      beta: 1.5
    - !ColorDistort {}
    - !RandomExpand
      fill_value: [123.675, 116.28, 103.53]
      ratio: 1.5
    - !RandomCrop {}
    - !RandomFlipImage
      is_normalized: false
    - !NormalizeBox {}
    - !PadBox
      num_max_boxes: 50
    - !BboxXYXY2XYWH {}
  batch_transforms:
  - !RandomShape
    sizes: [320, 352, 384, 416, 448, 480, 512, 544, 576, 608]
    random_inter: True
  - !NormalizeImage
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    is_scale: True
    is_channel_first: false
  - !Permute
    to_bgr: false
    channel_first: True
  batch_size: 8
  shuffle: true
  mixup_epoch: 250
  drop_last: true
  worker_num: 40
  bufsize: 200
  use_process: true

EvalReader:
  inputs_def:
    fields: ['image', 'im_size', 'im_id', 'gt_bbox', 'gt_class', 'is_difficult']
    num_max_boxes: 50
  dataset:
    !VOCDataSet
    dataset_dir: dataset/voc
    anno_path: val.txt
    use_default_label: false
    with_background: false
  sample_transforms:
    - !DecodeImage
      to_rgb: True
    - !ResizeImage
      target_size: 320
      interp: 2
    - !NormalizeImage
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
      is_scale: True
      is_channel_first: false
    - !PadBox
      num_max_boxes: 50
    - !Permute
      to_bgr: false
      channel_first: True
  batch_size: 8
  drop_empty: false
  worker_num: 4
  bufsize: 16

TestReader:
  inputs_def:
    image_shape: [3, 320, 320]
    fields: ['image', 'im_size', 'im_id']
  dataset:
    !ImageFolder
    anno_path: dataset/voc/label_list.txt
    use_default_label: false
    with_background: false
  sample_transforms:
    - !DecodeImage
      to_rgb: True
    - !ResizeImage
      target_size: 320
      interp: 2
    - !NormalizeImage
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
      is_scale: True
      is_channel_first: false
    - !Permute
      to_bgr: false
      channel_first: True
  batch_size: 1


yolov3_mobilenet_v3_blade.yml文件：
architecture: YOLOv3
use_gpu: true
max_iters: 20000
log_iter: 20
save_dir: output
snapshot_iter: 200
metric: VOC
map_type: integral
pretrain_weights: https://paddle-imagenet-models-name.bj.bcebos.com/MobileNetV3_large_x1_0_pretrained.tar
weights: output/yolov3_mobilenet_v3_blade/best_model
num_classes: 4
finetune_exclude_pretrained_params: ['yolo_output']
use_fine_grained_loss: false

YOLOv3:
  backbone: MobileNetV3
  yolo_head: YOLOv3Head

MobileNetV3:
  norm_type: sync_bn
  norm_decay: 0.
  model_name: large
  scale: 1.
  extra_block_filters: []
  feature_maps: [1, 2, 3, 4, 6]


YOLOv3Head:
  anchor_masks: [[6, 7, 8], [3, 4, 5], [0, 1, 2]]
  anchors: [[10, 13], [16, 30], [33, 23],
            [30, 61], [62, 45], [59, 119],
            [116, 90], [156, 198], [373, 326]]
  yolo_loss: YOLOv3Loss
  nms:
    background_label: -1
    keep_top_k: 100
    nms_threshold: 0.3
    nms_top_k: 1000
    normalized: false
    score_threshold: 0.01

YOLOv3Loss:
  ignore_thresh: 0.5
  label_smooth: true

LearningRate:
  base_lr: 0.001
  schedulers:
  - !PiecewiseDecay
    gamma: 0.1
    milestones:
    - 55000
    - 62000
  - !LinearWarmup
    start_factor: 0.
    steps: 1000

OptimizerBuilder:
  optimizer:
    momentum: 0.9
    type: Momentum
  regularizer:
    factor: 0.0005
    type: L2

_READER_: 'yolov3_reader.yml'
# will merge TrainReader into yolov3_reader.yml
TrainReader:
  inputs_def:
    fields: ['image', 'gt_bbox', 'gt_class', 'gt_score']
    num_max_boxes: 50
  dataset:
    !VOCDataSet
    dataset_dir: dataset/voc
    anno_path: train.txt
    with_background: false
    use_default_label: false
  sample_transforms:
    - !DecodeImage
      to_rgb: True
      with_mixup: True
    - !MixupImage
      alpha: 1.5
      beta: 1.5
    - !ColorDistort {}
    - !RandomExpand
      fill_value: [123.675, 116.28, 103.53]
      ratio: 1.5
    - !RandomCrop {}
    - !RandomFlipImage
      is_normalized: false
    - !NormalizeBox {}
    - !PadBox
      num_max_boxes: 50
    - !BboxXYXY2XYWH {}
  batch_transforms:
  - !RandomShape
    sizes: [320, 352, 384, 416, 448, 480, 512, 544, 576, 608]
    random_inter: True
  - !NormalizeImage
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    is_scale: True
    is_channel_first: false
  - !Permute
    to_bgr: false
    channel_first: True
  batch_size: 8
  shuffle: true
  mixup_epoch: 250
  drop_last: true
  worker_num: 40
  bufsize: 200
  use_process: true

EvalReader:
  inputs_def:
    fields: ['image', 'im_size', 'im_id', 'gt_bbox', 'gt_class', 'is_difficult']
    num_max_boxes: 50
  dataset:
    !VOCDataSet
    dataset_dir: dataset/voc
    anno_path: val.txt
    use_default_label: false
    with_background: false
  sample_transforms:
    - !DecodeImage
      to_rgb: True
    - !ResizeImage
      target_size: 320
      interp: 2
    - !NormalizeImage
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
      is_scale: True
      is_channel_first: false
    - !PadBox
      num_max_boxes: 50
    - !Permute
      to_bgr: false
      channel_first: True
  batch_size: 8
  drop_empty: false
  worker_num: 4
  bufsize: 16

TestReader:
  inputs_def:
    image_shape: [3, 320, 320]
    fields: ['image', 'im_size', 'im_id']
  dataset:
    !ImageFolder
    anno_path: dataset/voc/label_list.txt
    use_default_label: false
    with_background: false
  sample_transforms:
    - !DecodeImage
      to_rgb: True
    - !ResizeImage
      target_size: 320
      interp: 2
    - !NormalizeImage
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
      is_scale: True
      is_channel_first: false
    - !Permute
      to_bgr: false
      channel_first: True
  batch_size: 1


yolov3_reader.yml文件：
TrainReader:
  inputs_def:
    fields: ['image', 'gt_bbox', 'gt_class', 'gt_score']
    num_max_boxes: 50
  dataset:
    !COCODataSet
      image_dir: train2017
      anno_path: annotations/instances_train2017.json
      dataset_dir: dataset/coco
      with_background: false
  sample_transforms:
    - !DecodeImage
      to_rgb: True
      with_mixup: True
    - !MixupImage
      alpha: 1.5
      beta: 1.5
    - !ColorDistort {}
    - !RandomExpand
      fill_value: [123.675, 116.28, 103.53]
    - !RandomCrop {}
    - !RandomFlipImage
      is_normalized: false
    - !NormalizeBox {}
    - !PadBox
      num_max_boxes: 50
    - !BboxXYXY2XYWH {}
  batch_transforms:
  - !RandomShape
    sizes: [320, 352, 384, 416, 448, 480, 512, 544, 576, 608]
    random_inter: True
  - !NormalizeImage
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    is_scale: True
    is_channel_first: false
  - !Permute
    to_bgr: false
    channel_first: True
  # Gt2YoloTarget is only used when use_fine_grained_loss set as true,
  # this operator will be deleted automatically if use_fine_grained_loss
  # is set as false
  - !Gt2YoloTarget
    anchor_masks: [[6, 7, 8], [3, 4, 5], [0, 1, 2]]
    anchors: [[10, 13], [16, 30], [33, 23],
              [30, 61], [62, 45], [59, 119],
              [116, 90], [156, 198], [373, 326]]
    downsample_ratios: [32, 16, 8]
  batch_size: 8
  shuffle: true
  mixup_epoch: 250
  drop_last: true
  worker_num: 48
  bufsize: 48
  use_process: true


EvalReader:
  inputs_def:
    fields: ['image', 'im_size', 'im_id']
    num_max_boxes: 50
  dataset:
    !COCODataSet
      image_dir: val2017
      anno_path: annotations/instances_val2017.json
      dataset_dir: dataset/coco
      with_background: false
  sample_transforms:
    - !DecodeImage
      to_rgb: True
    - !ResizeImage
      target_size: 608
      interp: 2
    - !NormalizeImage
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
      is_scale: True
      is_channel_first: false
    - !PadBox
      num_max_boxes: 50
    - !Permute
      to_bgr: false
      channel_first: True
  batch_size: 8
  drop_empty: false
  worker_num: 48
  bufsize: 48

TestReader:
  inputs_def:
    image_shape: [3, 608, 608]
    fields: ['image', 'im_size', 'im_id']
  dataset:
    !ImageFolder
      anno_path: annotations/instances_val2017.json
      with_background: false
  sample_transforms:
    - !DecodeImage
      to_rgb: True
    - !ResizeImage
      target_size: 608
      interp: 2
    - !NormalizeImage
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
      is_scale: True
      is_channel_first: false
    - !Permute
      to_bgr: false
      channel_first: True
  batch_size: 1
```

报错信息：
WARNING: recv endsignal from outq with errmsg[consumer[consumer-12b-37] failed to map with error:[target0 not in samples]]

"
deploy/cpp编译问题,PaddlePaddle/PaddleDetection,2021-02-02 10:27:14,6,,2168,799147989,"CMakeFiles/main.dir/src/main.cc.o：在函数‘__static_initialization_and_destruction_0(int, int)’中：
/app/paddle/cpp/src/main.cc:34：对‘google::FlagRegisterer::FlagRegisterer<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >(char const*, char const*, char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*)’未定义的引用

需要把哪个插件改到哪个版本呢？"
您好，使用例程里面的人脸关键点模型BlazeFace_keypoint，如何提前关键点？,PaddlePaddle/PaddleDetection,2021-02-02 08:54:13,2,,2166,799072244,"我使用BlazeFace_keypoint模型，先转换了模型，然后使用deploy\cpp\下的代码，进行推理预测，可以提取到人脸矩形框信息，但是不知道如何提前关键点？
![图片](https://user-images.githubusercontent.com/40679769/106575610-3d5dc080-6577-11eb-9b74-2a917726cda7.png)
"
请问一下，PiecewiseDecay对训练COCO数据集最终的mAP影响大吗？,PaddlePaddle/PaddleDetection,2021-01-30 14:11:10,2,,2147,797429464,"尊敬的开发者，你好，
我自己复现了一下CenterNet，不过在COCO128上mAP只有22左右；
我在训练设置中没有使用lr的PiecewiseDecay，请问一下，这个设置对训练COCO128数据集的最终结果影响大吗？

期待您的回复！"
更新2.0稳定版后训练报错,PaddlePaddle/PaddleDetection,2021-01-30 03:42:49,2,,2143,797326213,"今天更新到paddlepaddle的2.0稳定版后，执行快速开始的roadsign训练报错，没有修改过配置文件。
训练命令如下：
`python tools/train.py -c configs/yolov3_mobilenet_v1_roadsign.yml --eval -o use_gpu=true`
报错信息如下：
```
2021-01-30 11:37:47,682-INFO: If regularizer of a Parameter has been set by 'fluid.ParamAttr' or 'fluid.WeightNormParamAttr' already. The Regularization[L2Decay, regularization_coeff=0.000500] in Optimizer will not take effect, and it will only be applied to other Parameters!
2021-01-30 11:37:48,489-ERROR: Config dataset_dir dataset/roadsign_voc is not exits!
2021-01-30 11:37:48,489-WARNING: Config annotation dataset/roadsign_voc\valid.txt is not a file, dataset config is not valid
2021-01-30 11:37:48,490-INFO: Dataset E:\workspace\PaddleDetection\dataset\roadsign_voc is not valid for reason above, try searching C:\Users\thugbobby/.cache/paddle/dataset or downloading dataset...
2021-01-30 11:37:48,492-INFO: Found C:\Users\thugbobby/.cache/paddle/dataset\roadsign_voc\annotations
2021-01-30 11:37:48,508-INFO: Found C:\Users\thugbobby/.cache/paddle/dataset\roadsign_voc\images
W0130 11:37:48.674729 10292 device_context.cc:362] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.2, Runtime API Version: 10.2
W0130 11:37:48.707641 10292 device_context.cc:372] device: 0, cuDNN Version: 7.6.
2021-01-30 11:37:50,916-WARNING: variable yolo_output.1.conv.bias not used
2021-01-30 11:37:50,916-WARNING: variable yolo_output.0.conv.bias not used
2021-01-30 11:37:50,917-WARNING: variable yolo_output.0.conv.weights not used
2021-01-30 11:37:50,917-WARNING: variable yolo_output.2.conv.weights not used
2021-01-30 11:37:50,917-WARNING: variable yolo_output.1.conv.weights not used
2021-01-30 11:37:50,917-WARNING: variable yolo_output.2.conv.bias not used
2021-01-30 11:37:51,031-ERROR: Config dataset_dir dataset/roadsign_voc is not exits!
2021-01-30 11:37:51,032-WARNING: Config annotation dataset/roadsign_voc\train.txt is not a file, dataset config is not valid
2021-01-30 11:37:51,032-INFO: Dataset E:\workspace\PaddleDetection\dataset\roadsign_voc is not valid for reason above, try searching C:\Users\thugbobby/.cache/paddle/dataset or downloading dataset...
2021-01-30 11:37:51,033-INFO: Found C:\Users\thugbobby/.cache/paddle/dataset\roadsign_voc\annotations
2021-01-30 11:37:51,033-INFO: Found C:\Users\thugbobby/.cache/paddle/dataset\roadsign_voc\images
W0130 11:37:51.497886 10292 build_strategy.cc:171] fusion_group is not enabled for Windows/MacOS now, and only effective when running with CUDA GPU.
E:\workspace\PaddleDetection\ppdet\data\reader.py:89: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working
  if isinstance(item, collections.Sequence) and len(item) == 0:
tools/train.py:283: RuntimeWarning: divide by zero encountered in double_scalars
  ips = float(cfg['TrainReader']['batch_size']) / time_cost
2021-01-30 11:37:52,254-INFO: iter: 0, lr: 0.000033, 'loss': '15265.234375', eta: 0:00:00, batch_cost: 0.00000 sec, ips: inf images/sec
2021-01-30 11:38:00,457-INFO: iter: 20, lr: 0.000047, 'loss': '27.008541', eta: 0:25:11, batch_cost: 0.42224 sec, ips: 18.94664 images/sec
2021-01-30 11:38:09,789-INFO: iter: 40, lr: 0.000060, 'loss': '21.406960', eta: 0:28:14, batch_cost: 0.47588 sec, ips: 16.81111 images/sec
2021-01-30 11:38:17,577-INFO: iter: 60, lr: 0.000073, 'loss': '15.545045', eta: 0:23:27, batch_cost: 0.39759 sec, ips: 20.12127 images/sec
2021-01-30 11:38:26,032-INFO: iter: 80, lr: 0.000087, 'loss': '13.640253', eta: 0:24:33, batch_cost: 0.41864 sec, ips: 19.10942 images/sec
2021-01-30 11:38:36,156-INFO: iter: 100, lr: 0.000100, 'loss': '12.718939', eta: 0:28:25, batch_cost: 0.48719 sec, ips: 16.42061 images/sec
2021-01-30 11:38:45,802-INFO: iter: 120, lr: 0.000100, 'loss': '10.709991', eta: 0:27:41, batch_cost: 0.47736 sec, ips: 16.75901 images/sec
2021-01-30 11:38:53,403-INFO: iter: 140, lr: 0.000100, 'loss': '12.132683', eta: 0:23:03, batch_cost: 0.39985 sec, ips: 20.00771 images/sec
2021-01-30 11:39:02,317-INFO: iter: 160, lr: 0.000100, 'loss': '11.912944', eta: 0:24:43, batch_cost: 0.43114 sec, ips: 18.55555 images/sec
2021-01-30 11:39:09,470-INFO: iter: 180, lr: 0.000100, 'loss': '11.560947', eta: 0:20:04, batch_cost: 0.35221 sec, ips: 22.71392 images/sec
2021-01-30 11:39:17,629-INFO: iter: 200, lr: 0.000100, 'loss': '10.136917', eta: 0:24:29, batch_cost: 0.43212 sec, ips: 18.51357 images/sec
2021-01-30 11:39:17,632-INFO: Save model to output\yolov3_mobilenet_v1_roadsign\200.
W0130 11:39:18.891667 10292 build_strategy.cc:171] fusion_group is not enabled for Windows/MacOS now, and only effective when running with CUDA GPU.
Traceback (most recent call last):
  File ""tools/train.py"", line 399, in <module>
    main()
  File ""tools/train.py"", line 308, in main
    results = eval_run(
  File ""E:\workspace\PaddleDetection\ppdet\utils\eval_utils.py"", line 146, in eval_run
    outs = exe.run(compile_program,
  File ""C:\Users\thugbobby\anaconda3\lib\site-packages\paddle\fluid\executor.py"", line 1110, in run
    six.reraise(*sys.exc_info())
  File ""C:\Users\thugbobby\anaconda3\lib\site-packages\six.py"", line 703, in reraise
    raise value
  File ""C:\Users\thugbobby\anaconda3\lib\site-packages\paddle\fluid\executor.py"", line 1098, in run
    return self._run_impl(
  File ""C:\Users\thugbobby\anaconda3\lib\site-packages\paddle\fluid\executor.py"", line 1244, in _run_impl
    return self._run_parallel(
  File ""C:\Users\thugbobby\anaconda3\lib\site-packages\paddle\fluid\executor.py"", line 913, in _run_parallel
    tensors = exe.run(fetch_var_names, return_merged)._move_to_list()
OSError: (External)  Cublas error, `CUBLAS_STATUS_ALLOC_FAILED`. Resource allocation failed inside the cuBLAS library.  (at D:\v2.0.0\paddle\paddle/fluid/platform/cuda_helper.h:81)
```
请问是什么问题？谢谢。"
Frcnn剪枝后再训练夯住,PaddlePaddle/PaddleDetection,2021-01-26 09:37:59,2,,2124,794080775,"训练环境：
-paddleslim 1.1.1
-paddle1.8.4-gpu-post97
-GPU: Tesla V100, 16G, cuda 9, cudnn 7.6
-代码分支：release/0.4
复现步骤：
使用faster_rcnn_r50_fpn_2x，修改input_size=640，max_size=800,进行预训练；
再使用prune 均匀剪枝0.3的conv2d op后，训练夯住。此时查看该进程CPU/GPU利用率都为零，栈信息似乎卡在Executor；
剪枝op:
```
 for param in program.global_block().all_parameters():
            if ""_weights"" in param.name:
                params.append(param.name)
```
剪枝
夯住日志：
![image](https://user-images.githubusercontent.com/20673237/105827460-fca4fb00-5ffc-11eb-9a77-485c176616ca.png)

夯住部分的栈信息：
![image](https://user-images.githubusercontent.com/20673237/105827055-78527800-5ffc-11eb-8eef-e745d936c967.png)
![image](https://user-images.githubusercontent.com/20673237/105827082-80121c80-5ffc-11eb-99cb-4d25a9e167e1.png)
线程日志：
![image](https://user-images.githubusercontent.com/20673237/105827762-59081a80-5ffd-11eb-9fa1-a9c4ddbe6228.png)
"
faster rcnn tta问题,PaddlePaddle/PaddleDetection,2021-01-25 10:45:17,3,,2119,793267557,我想用faster rcnn去配置tta ，在reader配置文件里，我仿照了[#1110](https://github.com/PaddlePaddle/PaddleDetection/issues/1110)这个方法，但是失败了，我想问问有没有关于这个的教程，感想
蒸馏通道剪裁ppyolo模型时程序运行卡住,PaddlePaddle/PaddleDetection,2021-01-21 10:09:44,2,,2106,790899410,"使用连接https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.0-beta/slim/extensions/distill_pruned_model示例进行蒸馏通道剪裁时，总是卡在迭代运行的位置。

运行命令如下：
python distill_pruned_model.py \
-c ../../../configs/yolov3_mobilenet_v1_voc.yml \
-t ../../../configs/yolov3_r34_voc.yml \
--teacher_pretrained=https://paddlemodels.bj.bcebos.com/object_detection/yolov3_r34_voc.tar \
--pruned_params ""yolo_block.0.0.0.conv.weights,yolo_block.0.0.1.conv.weights,yolo_block.0.1.0.conv.weights"" \
--pruned_ratios=""0.2,0.3,0.4"" \
-o use_fine_grained_loss=true pretrain_weights=https://paddlemodels.bj.bcebos.com/object_detection/yolov3_mobilenet_v1_voc.tar

卡住位置如下（到了这一步等了半个小时都没向下运行，调试状态和运行状态都这样）：
![image](https://user-images.githubusercontent.com/37362070/105335092-955d0480-5c12-11eb-95a1-abc888754eae.png)

显卡状态如下：
![image](https://user-images.githubusercontent.com/37362070/105335243-c9382a00-5c12-11eb-87b1-2e7b799fc27f.png)

线程状态如下（python相关的线程只有这一个在跑，而且一直是100%）：
![image](https://user-images.githubusercontent.com/37362070/105335354-ebca4300-5c12-11eb-8e70-dcdf7338302d.png)

CPU占用3%，内存占用20%，均够用

# 机器配置如下：
python：3.6.12
paddle：1.8.4
PaddleDetection版本：release/0.5（更新时间：2021.1.17）
GPU：4块 Titan X
CPU：E5 26883v4，64核
内存：252G
"
【求助】ppyolo 训练，lose 下降，mAP 不增长反而下降,PaddlePaddle/PaddleDetection,2021-01-19 07:02:32,2,,2096,788764539,"环境： aistudio
PaddlePaddle: v1.8.4
PaddleDetection: v0.5

![下载 (3)](https://user-images.githubusercontent.com/12800439/104999084-373fee00-5a67-11eb-9911-0dae34964597.png)
![下载 (4)](https://user-images.githubusercontent.com/12800439/104999097-3c9d3880-5a67-11eb-8845-67efa75f3050.png)
"
在v2.0-beta版本如何使用VisualDL可视化数据预处理图片,PaddlePaddle/PaddleDetection,2021-01-19 06:28:22,2,,2094,788745782,在v2.0-beta版本的版本信息中说支持VisualDL可视化数据预处理图片？这个功能是如何使用的？看了detection代码，没找到这个功能。
PPyolo训练速度慢，请大佬们帮忙看看是什么问题。,PaddlePaddle/PaddleDetection,2021-01-19 02:08:30,2,,2090,788648868,"训练PPYOLO时，GPU使用长时间为0，经常GPU沉寂十几秒，活跃一两秒，修改work_num也没明显改善，请大佬们帮忙看看是什么问题。


# 配置文件如下：
```
architecture: YOLOv3
use_gpu: true
max_iters: 50000
log_smooth_window: 100
log_iter: 100
save_dir: output
snapshot_iter: 1000
metric: VOC
pretrain_weights: https://paddle-imagenet-models-name.bj.bcebos.com/MobileNetV3_large_x1_0_ssld_pretrained.tar
weights: output/ppyolo_voc/best_model
num_classes: 19
use_fine_grained_loss: true
use_ema: true
ema_decay: 0.9998

YOLOv3:
  backbone: MobileNetV3
  yolo_head: YOLOv3Head
  use_fine_grained_loss: true

MobileNetV3:
  norm_type: sync_bn
  norm_decay: 0.
  model_name: large
  scale: 1.
  extra_block_filters: []
  feature_maps: [1, 2, 3, 4, 6]


YOLOv3Head:
  anchor_masks: [[3, 4, 5], [0, 1, 2]]
  anchors: [[18, 20], [28, 19], [82, 8],
            [30, 35], [44, 45], [94, 63]]
  norm_decay: 0.
  conv_block_num: 0
  coord_conv: true
  scale_x_y: 1.05
  yolo_loss: YOLOv3Loss
  spp: true
  nms:
    background_label: -1
    keep_top_k: 100
    nms_threshold: 0.45
    nms_top_k: 1000
    normalized: false
    score_threshold: 0.005
  drop_block: true

YOLOv3Loss:
  ignore_thresh: 0.5
  scale_x_y: 1.05
  label_smooth: false
  use_fine_grained_loss: true
  iou_loss: IouLoss

IouLoss:
  loss_weight: 2.5
  max_height: 512
  max_width: 512

LearningRate:
  base_lr: 0.005
  schedulers:
  - !PiecewiseDecay
    gamma: 0.1
    milestones:
    - 35000
    - 40000
    - 45000
  - !LinearWarmup
    start_factor: 0.
    steps: 4000

OptimizerBuilder:
  optimizer:
    momentum: 0.9
    type: Momentum
  regularizer:
    factor: 0.0005
    type: L2

#_READER_: 'ppyolo_reader.yml'

TrainReader:
  inputs_def:
    fields: ['image', 'gt_bbox', 'gt_class', 'gt_score']
    num_max_boxes: 50
  dataset:
    !VOCDataSet
    dataset_dir: dataset/ppyolo_voc
    anno_path: trainval.txt
    use_default_label: false
    with_background: false
  sample_transforms:
    - !DecodeImage
      to_rgb: True
      with_mixup: True
    - !MixupImage
      alpha: 1.5
      beta: 1.5
    - !ColorDistort {}
    - !RandomExpand
      fill_value: [123.675, 116.28, 103.53]
    - !RandomCrop {}
    - !RandomFlipImage
      is_normalized: false
    - !NormalizeBox {}
    - !PadBox
      num_max_boxes: 50
    - !BboxXYXY2XYWH {}
  batch_transforms:
  - !RandomShape
    sizes: [224, 256, 288, 320, 352, 384, 416, 448, 480, 512]
    random_inter: True
  - !NormalizeImage
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    is_scale: True
    is_channel_first: false
  - !Permute
    to_bgr: false
    channel_first: True

  - !Gt2YoloTarget
    anchor_masks: [[3, 4, 5], [0, 1, 2]]
    anchors: [[18, 20], [28, 19], [82, 8],
              [30, 35], [44, 45], [94, 63]]
    downsample_ratios: [32, 16]
    iou_thresh: 0.25
    num_classes: 19
  batch_size: 32
  shuffle: true
  mixup_epoch: 100000
  drop_last: true
  worker_num: 16
  bufsize: 16
  use_process: true

EvalReader:
  inputs_def:
    fields: ['image', 'im_size', 'im_id', 'gt_bbox', 'gt_class', 'is_difficult']
    num_max_boxes: 50
  dataset:
    !VOCDataSet
    dataset_dir: dataset/ppyolo_voc
    anno_path: test.txt
    use_default_label: false
    with_background: false
  sample_transforms:
    - !DecodeImage
      to_rgb: True
    - !ResizeImage
      target_size: 320
      interp: 2
    - !NormalizeImage
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
      is_scale: True
      is_channel_first: false
    - !PadBox
      num_max_boxes: 50
    - !Permute
      to_bgr: false
      channel_first: True
  batch_size: 8
  drop_empty: false
  worker_num: 16
  bufsize: 8

TestReader:
  inputs_def:
    image_shape: [3, 320, 320]
    fields: ['image', 'im_size', 'im_id']
  dataset:
    !ImageFolder
      anno_path: dataset/ppyolo_voc/label_list.txt
      use_default_label: false
      with_background: false
  sample_transforms:
    - !DecodeImage
      to_rgb: True
    - !ResizeImage
      target_size: 320
      interp: 2
    - !NormalizeImage
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
      is_scale: True
      is_channel_first: false
    - !Permute
      to_bgr: false
      channel_first: True
  batch_size: 1
```

# 显卡使用情况：
![image](https://user-images.githubusercontent.com/37362070/104978795-a86aab80-5a3d-11eb-9db6-23bb5c282b3b.png)


# 日志打印情况：
![image](https://user-images.githubusercontent.com/37362070/104978918-f8e20900-5a3d-11eb-9042-0732efbf12fd.png)


# 不同worker_num实验对比情况：
![image](https://user-images.githubusercontent.com/37362070/104992353-d232cb00-5a5b-11eb-9a54-eee9bedee770.png)


# 配置
python版本：3.6.12
paddle版本：1.8.4

"
faster_rcnn_r34_fpn_1x 训练用的配置文件如何配置,PaddlePaddle/PaddleDetection,2021-01-16 09:43:04,2,question,2076,787431006,"我的配置文件如下：

```
architecture: FasterRCNN
max_iters: 90000
use_gpu: true
snapshot_iter: 100
log_iter: 20
save_dir: output
pretrain_weights: https://paddle-imagenet-models-name.bj.bcebos.com/ResNet34_vd_pretrained.tar
metric: COCO
weights: output/faster_rcnn_r34_fpn_1x/model_final
num_classes: 42

FasterRCNN:
  backbone: ResNet
  fpn: FPN
  rpn_head: FPNRPNHead
  roi_extractor: FPNRoIAlign
  bbox_head: BBoxHead
  bbox_assigner: BBoxAssigner

ResNet:
  norm_type: bn
  norm_decay: 0.
  depth: 34
  feature_maps: [2, 3, 4, 5]
  freeze_at: 2
  variant: d

FPN:
  min_level: 2
  max_level: 6
  num_chan: 256
  spatial_scale: [0.03125, 0.0625, 0.125, 0.25]

FPNRPNHead:
  anchor_generator:
    anchor_sizes: [32, 64, 128, 256, 512]
    aspect_ratios: [0.5, 1.0, 2.0]
    stride: [16.0, 16.0]
    variance: [1.0, 1.0, 1.0, 1.0]
  anchor_start_size: 32
  min_level: 2
  max_level: 6
  num_chan: 256
  rpn_target_assign:
    rpn_batch_size_per_im: 256
    rpn_fg_fraction: 0.5
    rpn_positive_overlap: 0.7
    rpn_negative_overlap: 0.3
    rpn_straddle_thresh: 0.0
  train_proposal:
    min_size: 0.0
    nms_thresh: 0.7
    pre_nms_top_n: 2000
    post_nms_top_n: 2000
  test_proposal:
    min_size: 0.0
    nms_thresh: 0.7
    pre_nms_top_n: 1000
    post_nms_top_n: 1000

FPNRoIAlign:
  canconical_level: 4
  canonical_size: 224
  min_level: 2
  max_level: 5
  box_resolution: 7
  sampling_ratio: 2

BBoxAssigner:
  batch_size_per_im: 512
  bbox_reg_weights: [0.1, 0.1, 0.2, 0.2]
  bg_thresh_lo: 0.0
  bg_thresh_hi: 0.5
  fg_fraction: 0.25
  fg_thresh: 0.5

BBoxHead:
  head: TwoFCHead
  nms:
    keep_top_k: 100
    nms_threshold: 0.5
    score_threshold: 0.05

TwoFCHead:
  mlp_dim: 1024

LearningRate:
  base_lr: 0.02
  schedulers:
  - !PiecewiseDecay
    gamma: 0.1
    milestones: [60000, 80000]
  - !LinearWarmup
    start_factor: 0.1
    steps: 1000

OptimizerBuilder:
  optimizer:
    momentum: 0.9
    type: Momentum
  regularizer:
    factor: 0.0001
    type: L2

_READER_: 'faster_fpn_reader.yml'
TrainReader:
  batch_size: 1

#####################################\u6570\u636E\u914D\u7F6E#####################################

# \u6A21\u578B\u8BAD\u7EC3\u96C6\u8BBE\u7F6E\u53C2\u8003
# \u8BAD\u7EC3\u3001\u9A8C\u8BC1\u3001\u6D4B\u8BD5\u4F7F\u7528\u7684\u6570\u636E\u914D\u7F6E\u4E3B\u8981\u533A\u522B\u5728\u6570\u636E\u8DEF\u5F84\u3001\u6A21\u578B\u8F93\u5165\u3001\u6570\u636E\u589E\u5F3A\u53C2\u6570\u8BBE\u7F6E
# \u5982\u679C\u4F7F\u7528 yolov3_reader.yml\uFF0C\u4E0B\u9762\u7684\u53C2\u6570\u8BBE\u7F6E\u4F18\u5148\u7EA7\u9AD8\uFF0C\u4F1A\u8986\u76D6yolov3_reader.yml\u4E2D\u7684\u53C2\u6570\u8BBE\u7F6E\u3002
# _READER_: 'yolov3_reader.yml'

TrainReader:
  # \u8BAD\u7EC3\u8FC7\u7A0B\u4E2D\u6A21\u578B\u7684\u8F93\u5165\u8BBE\u7F6E
  # \u5305\u62EC\u56FE\u7247\uFF0C\u56FE\u7247\u957F\u5BBD\u9AD8\u7B49\u57FA\u672C\u4FE1\u606F\uFF0C\u56FE\u7247id\uFF0C\u6807\u8BB0\u7684\u76EE\u6807\u6846\uFF0C\u7C7B\u522B\u7B49\u4FE1\u606F,, 'gt_score'
  inputs_def:
    fields: ['image', 'im_info', 'im_id', 'gt_bbox', 'gt_class', 'is_crowd']
  # \u8BAD\u7EC3\u6570\u636E\u96C6\u8DEF\u5F84
  dataset:
    # \u6307\u5B9A\u6570\u636E\u96C6\u683C\u5F0F
    !COCODataSet
      # \u56FE\u7247\u6587\u4EF6\u5939\u76F8\u5BF9\u8DEF\u5F84\uFF0C\u8DEF\u5F84\u662F\u76F8\u5BF9\u4E8Edataset_dir\uFF0C\u56FE\u50CF\u8DEF\u5F84= dataset_dir + image_dir + image_name
      image_dir: images
      # anno_path\uFF0C\u8DEF\u5F84\u662F\u76F8\u5BF9\u4E8Edataset_dir
      anno_path: annotations/coco_label.json
      # \u6570\u636E\u96C6\u76F8\u5BF9\u8DEF\u5F84\uFF0C\u8DEF\u5F84\u662F\u76F8\u5BF9\u4E8EPaddleDetection
      dataset_dir: dataset/light_coco_gd
      # \u662F\u5426\u5305\u542B\u80CC\u666F\u7C7B\uFF0C\u82E5with_background=true\uFF0Cnum_classes\u9700\u8981+1
      # YOLO \u7CFB\u5217with_background\u5FC5\u987B\u662Ffalse\uFF0CFasterRCNN\u7CFB\u5217\u662Ftrue ###
      with_background: true
  sample_transforms:
    # \u8BFB\u53D6Image\u56FE\u50CF\u4E3Anumpy\u6570\u7EC4
    # \u53EF\u4EE5\u9009\u62E9\u5C06\u56FE\u7247\u4ECEBGR\u8F6C\u5230RGB\uFF0C\u53EF\u4EE5\u9009\u62E9\u5BF9\u4E00\u4E2Abatch\u4E2D\u7684\u56FE\u7247\u505Amixup\u589E\u5F3A
    - !DecodeImage
      to_rgb: True
      with_mixup: True
    # MixupImage
    - !MixupImage
      alpha: 1.5
      beta: 1.5
    # ColorDistort
    - !ColorDistort {}
    # RandomExpand
    - !RandomExpand
      fill_value: [123.675, 116.28, 103.53]
      # \u968F\u673A\u6269\u5145\u6BD4\u4F8B\uFF0C\u9ED8\u8BA4\u503C\u662F4.0
      ratio: 1.5
    - !RandomCrop {}
    - !RandomFlipImage
      is_normalized: false
     # \u5F52\u4E00\u5316\u5750\u6807
    - !NormalizeBox {}
    # \u5982\u679C bboxes \u6570\u91CF\u5C0F\u4E8E num_max_boxes\uFF0C\u586B\u5145\u503C\u4E3A0\u7684 box
    - !PadBox
      num_max_boxes: 50
    # \u5750\u6807\u683C\u5F0F\u8F6C\u5316\uFF0C\u4ECEXYXY\u8F6C\u6210XYWH\u683C\u5F0F
    - !BboxXYXY2XYWH {}
  # \u4EE5\u4E0B\u662F\u5BF9\u4E00\u4E2Abatch\u4E2D\u7684\u6240\u6709\u56FE\u7247\u540C\u65F6\u505A\u7684\u6570\u636E\u5904\u7406
  batch_transforms:
  # \u591A\u5C3A\u5EA6\u8BAD\u7EC3\u65F6\uFF0C\u4ECElist\u4E2D\u968F\u673A\u9009\u62E9\u4E00\u4E2A\u5C3A\u5BF8\uFF0C\u5BF9\u4E00\u4E2Abatch\u6570\u636E\u540C\u65F6\u540C\u65F6resize
  - !RandomShape
    sizes: [320, 352, 384, 416, 448, 480, 512, 544, 576, 608]
    random_inter: True
  # NormalizeImage
  - !NormalizeImage
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    is_scale: True
    is_channel_first: false
  - !Permute
    to_bgr: false
    channel_first: True
  # Gt2YoloTarget is only used when use_fine_grained_loss set as true,
  # this operator will be deleted automatically if use_fine_grained_loss
  # is set as false
  - !Gt2YoloTarget
    anchor_masks: [[6, 7, 8], [3, 4, 5], [0, 1, 2]]
    anchors: [[10, 13], [16, 30], [33, 23],
              [30, 61], [62, 45], [59, 119],
              [116, 90], [156, 198], [373, 326]]
    downsample_ratios: [32, 16, 8]
  # 1\u4E2AGPU\u7684batch size\uFF0C\u9ED8\u8BA4\u4E3A1\u3002\u9700\u8981\u6CE8\u610F\uFF1A\u6BCF\u4E2Aiter\u8FED\u4EE3\u4F1A\u8FD0\u884Cbatch_size * device_num\u5F20\u56FE\u7247
  batch_size: 8
  # \u662F\u5426shuffle
  shuffle: true
  # mixup\uFF0C-1\u8868\u793A\u4E0D\u505AMixup\u6570\u636E\u589E\u5F3A\u3002\u6CE8\u610F\uFF0C\u8FD9\u91CC\u662Fepoch\u4E3A\u5355\u4F4D
  mixup_epoch: 250
  # \u6CE8\u610F\uFF0C\u5728\u67D0\u4E9B\u60C5\u51B5\u4E0B\uFF0Cdrop_last=false\u65F6\u8BAD\u7EC3\u8FC7\u7A0B\u4E2D\u53EF\u80FD\u4F1A\u51FA\u9519\uFF0C\u5EFA\u8BAE\u8BAD\u7EC3\u65F6\u90FD\u8BBE\u7F6E\u4E3Atrue
  drop_last: true
  # \u82E5\u9009\u7528\u591A\u8FDB\u7A0B\uFF0C\u8BBE\u7F6E\u4F7F\u7528\u591A\u8FDB\u7A0B/\u7EBF\u7A0B\u7684\u6570\u76EE
  # \u5F00\u542F\u591A\u8FDB\u7A0B\u540E\uFF0C\u5360\u7528\u5185\u5B58\u4F1A\u6210\u500D\u589E\u52A0\uFF0C\u6839\u636E\u5185\u5B58\u8BBE\u7F6E###
  worker_num: 8
  # \u5171\u4EAB\u5185\u5B58bufsize\u3002\u6CE8\u610F\uFF0C\u7F13\u5B58\u662F\u4EE5batch\u4E3A\u5355\u4F4D\uFF0C\u7F13\u5B58\u7684\u6837\u672C\u6570\u636E\u603B\u91CF\u4E3Abatch_size * bufsize\uFF0C\u6240\u4EE5\u8BF7\u6CE8\u610F\u4E0D\u8981\u8BBE\u7F6E\u592A\u5927\uFF0C\u8BF7\u6839\u636E\u60A8\u7684\u786C\u4EF6\u8BBE\u7F6E\u3002
  bufsize: 16
  # \u662F\u5426\u4F7F\u7528\u591A\u8FDB\u7A0B
  use_process: true


EvalReader:
  # \u8BC4\u4F30\u8FC7\u7A0B\u4E2D\u6A21\u578B\u7684\u8F93\u5165\u8BBE\u7F6E
  # \u5305\u62EC\u56FE\u7247\uFF0C\u56FE\u7247\u957F\u5BBD\u9AD8\u7B49\u57FA\u672C\u4FE1\u606F\uFF0C\u56FE\u7247id\uFF0C\u6807\u8BB0\u7684\u76EE\u6807\u6846\uFF0C\u7C7B\u522B\u7B49\u4FE1\u606F
  inputs_def:
    fields: ['image', 'im_size', 'im_id']
    # num_max_boxes\uFF0C\u6BCF\u4E2A\u6837\u672C\u7684groud truth\u7684\u6700\u591A\u4FDD\u7559\u4E2A\u6570\uFF0C\u82E5\u4E0D\u591F\u75280\u586B\u5145\u3002
    num_max_boxes: 50
  # \u6570\u636E\u96C6\u8DEF\u5F84
  dataset:
    !COCODataSet
      # \u56FE\u7247\u6587\u4EF6\u5939\u76F8\u5BF9\u8DEF\u5F84\uFF0C\u8DEF\u5F84\u662F\u76F8\u5BF9\u4E8Edataset_dir\uFF0C\u56FE\u50CF\u8DEF\u5F84= dataset_dir + image_dir + image_name
      image_dir: images
      # anno_path\uFF0C\u8DEF\u5F84\u662F\u76F8\u5BF9\u4E8Edataset_dir
      anno_path: annotations/eval.json
      # \u6570\u636E\u96C6\u76F8\u5BF9\u8DEF\u5F84\uFF0C\u8DEF\u5F84\u662F\u76F8\u5BF9\u4E8EPaddleDetection
      dataset_dir: dataset/light_coco_gd
      # \u662F\u5426\u5305\u542B\u80CC\u666F\u7C7B\uFF0C\u82E5with_background=true\uFF0Cnum_classes\u9700\u8981+1
      # YOLO \u7CFB\u5217with_background\u5FC5\u987B\u662Ffalse\uFF0CFasterRCNN\u7CFB\u5217\u662Ftrue ###
      with_background: true
  sample_transforms:
    # \u8BFB\u53D6Image\u56FE\u50CF\u4E3Anumpy\u6570\u7EC4
    # \u53EF\u4EE5\u9009\u62E9\u5C06\u56FE\u7247\u4ECEBGR\u8F6C\u5230RGB\uFF0C\u53EF\u4EE5\u9009\u62E9\u5BF9\u4E00\u4E2Abatch\u4E2D\u7684\u56FE\u7247\u505Amixup\u589E\u5F3A
    - !DecodeImage
      to_rgb: True
    # ResizeImage
    - !ResizeImage
      target_size: 608
      interp: 2
    # NormalizeImage
    - !NormalizeImage
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
      is_scale: True
      is_channel_first: false
    # \u5982\u679C bboxes \u6570\u91CF\u5C0F\u4E8E num_max_boxes\uFF0C\u586B\u5145\u503C\u4E3A0\u7684 box
    - !PadBox
      num_max_boxes: 50
    - !Permute
      to_bgr: false
      channel_first: True
  # 1\u4E2AGPU\u7684batch size\uFF0C\u9ED8\u8BA4\u4E3A1\u3002\u9700\u8981\u6CE8\u610F\uFF1A\u6BCF\u4E2Aiter\u8FED\u4EE3\u4F1A\u8FD0\u884Cbatch_size * device_num\u5F20\u56FE\u7247
  batch_size: 8
  # drop_empty
  drop_empty: false
  # \u82E5\u9009\u7528\u591A\u8FDB\u7A0B\uFF0C\u8BBE\u7F6E\u4F7F\u7528\u591A\u8FDB\u7A0B/\u7EBF\u7A0B\u7684\u6570\u76EE
  # \u5F00\u542F\u591A\u8FDB\u7A0B\u540E\uFF0C\u5360\u7528\u5185\u5B58\u4F1A\u6210\u500D\u589E\u52A0\uFF0C\u6839\u636E\u5185\u5B58\u8BBE\u7F6E###
  worker_num: 8
  # \u5171\u4EAB\u5185\u5B58bufsize\u3002\u6CE8\u610F\uFF0C\u7F13\u5B58\u662F\u4EE5batch\u4E3A\u5355\u4F4D\uFF0C\u7F13\u5B58\u7684\u6837\u672C\u6570\u636E\u603B\u91CF\u4E3Abatch_size * bufsize\uFF0C\u6240\u4EE5\u8BF7\u6CE8\u610F\u4E0D\u8981\u8BBE\u7F6E\u592A\u5927\uFF0C\u8BF7\u6839\u636E\u60A8\u7684\u786C\u4EF6\u8BBE\u7F6E\u3002
  bufsize: 16

TestReader:
  # \u9884\u6D4B\u8FC7\u7A0B\u4E2D\u6A21\u578B\u7684\u8F93\u5165\u8BBE\u7F6E
  # \u5305\u62EC\u56FE\u7247\uFF0C\u56FE\u7247\u957F\u5BBD\u9AD8\u7B49\u57FA\u672C\u4FE1\u606F\uFF0C\u56FE\u7247id\uFF0C\u6807\u8BB0\u7684\u76EE\u6807\u6846\uFF0C\u7C7B\u522B\u7B49\u4FE1\u606F
  inputs_def:
    # \u9884\u6D4B\u56FE\u50CF\u8F93\u5165\u5C3A\u5BF8
    image_shape: [3, 608, 608]
    fields: ['image', 'im_size', 'im_id']
  # \u6570\u636E\u96C6\u8DEF\u5F84
  dataset:
    !ImageFolder
      # anno_path\uFF0C\u8DEF\u5F84\u662F\u76F8\u5BF9\u4E8Edataset_dir
      anno_path: annotations/eval.json
      # \u662F\u5426\u5305\u542B\u80CC\u666F\u7C7B\uFF0C\u82E5with_background=true\uFF0Cnum_classes\u9700\u8981+1
      # YOLO \u7CFB\u5217with_background\u5FC5\u987B\u662Ffalse\uFF0CFasterRCNN\u7CFB\u5217\u662Ftrue ###
      with_background: true
  sample_transforms:
    - !DecodeImage
      to_rgb: True
    # ResizeImage
    - !ResizeImage
      target_size: 608
      interp: 2
    # NormalizeImage
    - !NormalizeImage
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
      is_scale: True
      is_channel_first: false
    # Permute
    - !Permute
      to_bgr: false
      channel_first: True
  # 1\u4E2AGPU\u7684batch size\uFF0C\u9ED8\u8BA4\u4E3A1
  batch_size: 1
```

上述文件在执行时报错，想了解下是哪里配置错了吗，我的系统是Ubuntu,GPU环境。
```
loading annotations into memory...
Traceback (most recent call last):
  File ""tools/train.py"", line 399, in <module>
    main()
  File ""tools/train.py"", line 228, in main
    train_reader = create_reader(
  File ""/root/chenhx/PaddleDetection/ppdet/data/reader.py"", line 442, in create_reader
    reader = Reader(**cfg)()
  File ""/root/chenhx/PaddleDetection/ppdet/data/reader.py"", line 210, in __init__
    self._roidbs = self._dataset.get_roidb()
  File ""/root/chenhx/PaddleDetection/ppdet/data/source/dataset.py"", line 68, in get_roidb
    self.load_roidb_and_cname2cid()
  File ""/root/chenhx/PaddleDetection/ppdet/data/source/coco.py"", line 81, in load_roidb_and_cname2cid
    coco = COCO(anno_path)
  File ""/root/anaconda3/envs/paddle_env/lib/python3.8/site-packages/pycocotools/coco.py"", line 85, in __init__
    dataset = json.load(f)
  File ""/root/anaconda3/envs/paddle_env/lib/python3.8/json/__init__.py"", line 293, in load
    return loads(fp.read(),
  File ""/root/anaconda3/envs/paddle_env/lib/python3.8/json/__init__.py"", line 357, in loads
    return _default_decoder.decode(s)
  File ""/root/anaconda3/envs/paddle_env/lib/python3.8/json/decoder.py"", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/root/anaconda3/envs/paddle_env/lib/python3.8/json/decoder.py"", line 355, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
```
"
请问一下，“affine_channel”是什么norm方法呢？,PaddlePaddle/PaddleDetection,2021-01-08 08:38:58,2,,2028,781949393,"尊敬的开发者，你好，
今天我在学习PaddleDetection中resnet的代码，我看到这样一个参数，
![image](https://user-images.githubusercontent.com/27288110/103992961-d810ec80-51cf-11eb-835c-78bab09002c1.png)
我想请问一下，这里的“affine_channel”是什么norm方法呢？
我好像没有听过这个方法。

期待你的回复！"
在新的2.0版本代码里，faster rcnn resnet50作为前置网络的算法比0.4版本的代码map显示低了30%左右，同样的数据集，学习率啥的也都一样,PaddlePaddle/PaddleDetection,2021-01-08 06:10:22,3,,2025,781873955,
多进程读取数据失败,PaddlePaddle/PaddleDetection,2021-01-04 02:05:54,3,,1996,777768460,"1） paddle1.8.4
2）GPU：V100，CUDA7.0和CUDNN9.0
3）cpu内存信息
![image](https://user-images.githubusercontent.com/20673237/103495699-a538c280-4e76-11eb-80e8-363f831be63b.png)
4）系统环境：Linux
5）训练信息，V100, 单机，单卡，进行faster_rcnn_r50_vd_fpn的前向推理

问题：偶发问题，开启多进程进行图片infer，在推理结束前报错提示某张图不存在，但本地检查过该图片是存在的；
具体报错如下，请问可能是什么原因？
![image](https://user-images.githubusercontent.com/20673237/103494876-96044580-4e73-11eb-9170-eb91902b47df.png)
"
用训练完的yolov3_darknet进行评估然后报错,PaddlePaddle/PaddleDetection,2020-12-30 08:54:56,4,,1987,776344935,"错误如下：
/home/ubuntu/anaconda3/envs/paddle_env/lib/python3.8/site-packages/paddle/fluid/executor.py:1069: UserWarning: The following exception is not an EOF exception.
  warnings.warn(
Traceback (most recent call last):
  File ""tools/eval.py"", line 179, in <module>
    main()
  File ""tools/eval.py"", line 145, in main
    results = eval_run(exe, compile_program, loader, keys, values, cls, cfg,
  File ""/home/ubuntu/shenwenxin/PaddleDetection/ppdet/utils/eval_utils.py"", line 146, in eval_run
    outs = exe.run(compile_program,
  File ""/home/ubuntu/anaconda3/envs/paddle_env/lib/python3.8/site-packages/paddle/fluid/executor.py"", line 1071, in run
    six.reraise(*sys.exc_info())
  File ""/home/ubuntu/anaconda3/envs/paddle_env/lib/python3.8/site-packages/six.py"", line 703, in reraise
    raise value
  File ""/home/ubuntu/anaconda3/envs/paddle_env/lib/python3.8/site-packages/paddle/fluid/executor.py"", line 1056, in run
    return self._run_impl(
  File ""/home/ubuntu/anaconda3/envs/paddle_env/lib/python3.8/site-packages/paddle/fluid/executor.py"", line 1160, in _run_impl
    return self._run_parallel(
  File ""/home/ubuntu/anaconda3/envs/paddle_env/lib/python3.8/site-packages/paddle/fluid/executor.py"", line 879, in _run_parallel
    tensors = exe.run(fetch_var_names, return_merged)._move_to_list()
paddle.fluid.core_avx.EnforceNotMet: 

--------------------------------------------
C++ Call Stacks (More useful to developers):
--------------------------------------------
0   std::string paddle::platform::GetTraceBackString<std::string const&>(std::string const&, char const*, int)
1   paddle::platform::EnforceNotMet::EnforceNotMet(std::string const&, char const*, int)
2   paddle::framework::LoDTensor::MergeLoDTensor(std::vector<paddle::framework::LoDTensor const*, std::allocator<paddle::framework::LoDTensor const*> > const&, paddle::platform::Place)
3   paddle::framework::details::ParallelSSAGraphExecutor::Run(std::vector<std::string, std::allocator<std::string> > const&, bool)
4   paddle::framework::details::ScopeBufferedMonitor::Apply(std::function<void ()> const&, bool)
5   paddle::framework::details::ScopeBufferedSSAGraphExecutor::Run(std::vector<std::string, std::allocator<std::string> > const&, bool)
6   paddle::framework::ParallelExecutor::Run(std::vector<std::string, std::allocator<std::string> > const&, bool)

----------------------
Error Message Summary:
----------------------
Error: An error occurred here. There is no accurate error hint for this error yet. We are continuously in the process of increasing hint for this kind of error check. It would be helpful if you could inform us of how this conversion went by opening a github issue. And we will resolve it with high priority.
  - New issue link: https://github.com/PaddlePaddle/Paddle/issues/new
  - Recommended issue content: all error stack information
  [Hint: Expected framework::product(new_dim) / new_dim[0] == framework::product(t->dims()) / t->dims()[0], but received framework::product(new_dim) / new_dim[0]:6 != framework::product(t->dims()) / t->dims()[0]:1.] at (/paddle/paddle/fluid/framework/lod_tensor.cc:400)

terminate called without an active exception
W1230 08:50:10.087417  8939 init.cc:226] Warning: PaddlePaddle catches a failure signal, it may not work properly
W1230 08:50:10.087430  8939 init.cc:228] You could check whether you killed PaddlePaddle thread/process accidentally or report the case to PaddlePaddle
W1230 08:50:10.087432  8939 init.cc:231] The detail failure signal is:

W1230 08:50:10.087435  8939 init.cc:234] *** Aborted at 1609318210 (unix time) try ""date -d @1609318210"" if you are using GNU date ***
W1230 08:50:10.088344  8939 init.cc:234] PC: @                0x0 (unknown)
W1230 08:50:10.097903  8939 init.cc:234] *** SIGABRT (@0x3e800002287) received by PID 8839 (TID 0x7f63317ee700) from PID 8839; stack trace: ***
W1230 08:50:10.098971  8939 init.cc:234]     @     0x7f64cc84e890 (unknown)
W1230 08:50:10.099788  8939 init.cc:234]     @     0x7f64cc489e97 gsignal
W1230 08:50:10.100589  8939 init.cc:234]     @     0x7f64cc48b801 abort
W1230 08:50:10.101922  8939 init.cc:234]     @     0x7f649fe3284a __gnu_cxx::__verbose_terminate_handler()
W1230 08:50:10.102527  8939 init.cc:234]     @     0x7f649fe30f47 __cxxabiv1::__terminate()
W1230 08:50:10.103390  8939 init.cc:234]     @     0x7f649fe30f7d std::terminate()
W1230 08:50:10.104044  8939 init.cc:234]     @     0x7f649fe30c5a __gxx_personality_v0
W1230 08:50:10.104671  8939 init.cc:234]     @     0x7f64c7e59b97 _Unwind_ForcedUnwind_Phase2
W1230 08:50:10.105250  8939 init.cc:234]     @     0x7f64c7e59e7d _Unwind_ForcedUnwind
W1230 08:50:10.105947  8939 init.cc:234]     @     0x7f64cc84cf10 __GI___pthread_unwind
W1230 08:50:10.106617  8939 init.cc:234]     @     0x7f64cc844ae5 __pthread_exit
W1230 08:50:10.107802  8939 init.cc:234]     @     0x5608f9d80909 PyThread_exit_thread
W1230 08:50:10.107967  8939 init.cc:234]     @     0x5608f9c13d3a PyEval_RestoreThread.cold.962
W1230 08:50:10.108531  8939 init.cc:234]     @     0x7f64c63e31a9 (unknown)
W1230 08:50:10.109484  8939 init.cc:234]     @     0x5608f9d17f76 PyCFunction_Call
W1230 08:50:10.110359  8939 init.cc:234]     @     0x5608f9cd585f _PyObject_MakeTpCall
W1230 08:50:10.111093  8939 init.cc:234]     @     0x5608f9d5cf56 _PyEval_EvalFrameDefault
W1230 08:50:10.111763  8939 init.cc:234]     @     0x5608f9d22a92 _PyEval_EvalCodeWithName
W1230 08:50:10.112432  8939 init.cc:234]     @     0x5608f9d2443f _PyObject_FastCallDict
W1230 08:50:10.113095  8939 init.cc:234]     @     0x5608f9d24733 _PyObject_Call_Prepend
W1230 08:50:10.113451  8939 init.cc:234]     @     0x5608f9d2483a slot_tp_call
W1230 08:50:10.114197  8939 init.cc:234]     @     0x5608f9cd585f _PyObject_MakeTpCall
W1230 08:50:10.114925  8939 init.cc:234]     @     0x5608f9d58e35 _PyEval_EvalFrameDefault
W1230 08:50:10.115583  8939 init.cc:234]     @     0x5608f9d2460d _PyObject_FastCallDict
W1230 08:50:10.116379  8939 init.cc:234]     @     0x5608f9d24733 _PyObject_Call_Prepend
W1230 08:50:10.116921  8939 init.cc:234]     @     0x5608f9d2483a slot_tp_call
W1230 08:50:10.117920  8939 init.cc:234]     @     0x5608f9cd585f _PyObject_MakeTpCall
W1230 08:50:10.118659  8939 init.cc:234]     @     0x5608f9d5cf56 _PyEval_EvalFrameDefault
W1230 08:50:10.119326  8939 init.cc:234]     @     0x5608f9d22a92 _PyEval_EvalCodeWithName
W1230 08:50:10.119693  8939 init.cc:234]     @     0x5608f9d23943 _PyFunction_Vectorcall.localalias.355
W1230 08:50:10.119877  8939 init.cc:234]     @     0x5608f9d23e79 method_vectorcall
W1230 08:50:10.120534  8939 init.cc:234]     @     0x5608f9d245fa _PyObject_FastCallDict
Aborted (core dumped)

能帮看看嘛
"
利用mask_rcnn_r50_vd_2x评估时遇到问题,PaddlePaddle/PaddleDetection,2020-12-26 04:37:29,2,,1972,774820554,"您好，我利用mask_rcnn_r50_vd_2x迁移学习训练之后，想对它进行评估。
我重写了其evalReader，重写内容如下：

EvalReader:
  dataset:
    !COCODataSet
      dataset_dir: /home/ubuntu/ShareFiles/shenwenxin/coco
      anno_path: annotations/instance_val.json
      image_dir: val

但是报了如下错误：

```
2020-12-26 04:27:38,848-WARNING: Illegal image file: /home/ubuntu/ShareFiles/shenwenxin/coco/val/440.jpg, and it will be ignored
2020-12-26 04:27:38,865-WARNING: Illegal image file: /home/ubuntu/ShareFiles/shenwenxin/coco/val/380.jpg, and it will be ignored
W1226 04:27:38.882901 20005 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 75, Driver API Version: 10.1, Runtime API Version: 10.0
W1226 04:27:38.885038 20005 device_context.cc:260] device: 0, cuDNN Version: 7.6.
Traceback (most recent call last):
  File ""tools/eval.py"", line 179, in <module>
    main()
  File ""tools/eval.py"", line 145, in main
    results = eval_run(exe, compile_program, loader, keys, values, cls, cfg,
  File ""/home/ubuntu/shenwenxin/PaddleDetection/ppdet/utils/eval_utils.py"", line 178, in eval_run
    res['mask'] = mask_encode(res, resolution)
  File ""/home/ubuntu/shenwenxin/PaddleDetection/ppdet/utils/post_process.py"", line 284, in mask_encode
    expand_bbox = expand_boxes(bbox, scale)
  File ""/home/ubuntu/shenwenxin/PaddleDetection/ppdet/utils/coco_eval.py"", line 470, in expand_boxes
    w_half = (boxes[:, 2] - boxes[:, 0]) * .5
IndexError: index 2 is out of bounds for axis 1 with size 0
terminate called without an active exception
W1226 04:27:42.609120 20099 init.cc:226] Warning: PaddlePaddle catches a failure signal, it may not work properly
W1226 04:27:42.609133 20099 init.cc:228] You could check whether you killed PaddlePaddle thread/process accidentally or report the case to PaddlePaddle
W1226 04:27:42.609136 20099 init.cc:231] The detail failure signal is:

W1226 04:27:42.609139 20099 init.cc:234] *** Aborted at 1608956862 (unix time) try ""date -d @1608956862"" if you are using GNU date ***
W1226 04:27:42.609975 20099 init.cc:234] PC: @                0x0 (unknown)
W1226 04:27:42.610057 20099 init.cc:234] *** SIGABRT (@0x3e800004e25) received by PID 20005 (TID 0x7f8877ff7700) from PID 20005; stack trace: ***
W1226 04:27:42.610932 20099 init.cc:234]     @     0x7f8a0afc2890 (unknown)
W1226 04:27:42.611642 20099 init.cc:234]     @     0x7f8a0abfde97 gsignal
W1226 04:27:42.612280 20099 init.cc:234]     @     0x7f8a0abff801 abort
W1226 04:27:42.613337 20099 init.cc:234]     @     0x7f89de5a684a __gnu_cxx::__verbose_terminate_handler()
W1226 04:27:42.613912 20099 init.cc:234]     @     0x7f89de5a4f47 __cxxabiv1::__terminate()
W1226 04:27:42.614760 20099 init.cc:234]     @     0x7f89de5a4f7d std::terminate()
W1226 04:27:42.615396 20099 init.cc:234]     @     0x7f89de5a4c5a __gxx_personality_v0
W1226 04:27:42.615931 20099 init.cc:234]     @     0x7f8a065cdb97 _Unwind_ForcedUnwind_Phase2
W1226 04:27:42.616457 20099 init.cc:234]     @     0x7f8a065cde7d _Unwind_ForcedUnwind
W1226 04:27:42.617110 20099 init.cc:234]     @     0x7f8a0afc0f10 __GI___pthread_unwind
W1226 04:27:42.617750 20099 init.cc:234]     @     0x7f8a0afb8ae5 __pthread_exit
W1226 04:27:42.618746 20099 init.cc:234]     @     0x55a114466909 PyThread_exit_thread
W1226 04:27:42.618902 20099 init.cc:234]     @     0x55a1142f9d3a PyEval_RestoreThread.cold.962
W1226 04:27:42.619438 20099 init.cc:234]     @     0x7f8a04b2ed4e (unknown)
W1226 04:27:42.620124 20099 init.cc:234]     @     0x55a1143fdf76 PyCFunction_Call
W1226 04:27:42.620889 20099 init.cc:234]     @     0x55a1143bb85f _PyObject_MakeTpCall
W1226 04:27:42.621624 20099 init.cc:234]     @     0x55a114442f56 _PyEval_EvalFrameDefault
W1226 04:27:42.622290 20099 init.cc:234]     @     0x55a114408a92 _PyEval_EvalCodeWithName
W1226 04:27:42.622961 20099 init.cc:234]     @     0x55a11440a43f _PyObject_FastCallDict
W1226 04:27:42.623618 20099 init.cc:234]     @     0x55a11440a733 _PyObject_Call_Prepend
W1226 04:27:42.623973 20099 init.cc:234]     @     0x55a11440a83a slot_tp_call
W1226 04:27:42.624720 20099 init.cc:234]     @     0x55a1143bb85f _PyObject_MakeTpCall
W1226 04:27:42.625458 20099 init.cc:234]     @     0x55a11443ee35 _PyEval_EvalFrameDefault
W1226 04:27:42.626123 20099 init.cc:234]     @     0x55a11440a60d _PyObject_FastCallDict
terminate called recursively
W1226 04:27:42.626791 20099 init.cc:234]     @     0x55a11440a733 _PyObject_Call_Prepend
W1226 04:27:42.627147 20099 init.cc:234]     @     0x55a11440a83a slot_tp_call
W1226 04:27:42.627897 20099 init.cc:234]     @     0x55a1143bb85f _PyObject_MakeTpCall
W1226 04:27:42.628624 20099 init.cc:234]     @     0x55a114442f56 _PyEval_EvalFrameDefault
W1226 04:27:42.629297 20099 init.cc:234]     @     0x55a114408a92 _PyEval_EvalCodeWithName
W1226 04:27:42.629667 20099 init.cc:234]     @     0x55a114409943 _PyFunction_Vectorcall.localalias.355
W1226 04:27:42.629853 20099 init.cc:234]     @     0x55a114409e79 method_vectorcall
W1226 04:27:42.630506 20099 init.cc:234]     @     0x55a11440a5fa _PyObject_FastCallDict
Aborted (core dumped)
```

能否提示下该如何解决呢？"
多GPU预测问题,PaddlePaddle/PaddleDetection,2020-12-22 09:50:57,2,,1947,772806805,请问infer.py能否使用多gpu进行推理？目前看文档，只允许进行单卡推理
使用YOLOv3和PP-YOLO模型训练单类数据集导出模型后，计算出的flops和Parameters比PP-YOLO论文的实验结果大,PaddlePaddle/PaddleDetection,2020-12-17 09:53:14,0,,1920,769790715,使用PaddleDetection中的YOLOv3-Darknet53和PP-YOLO模型训练自己的单类VOC格式数据集后，在tools/export_model.py文件中利用work/PaddleDetection/slim/prune/export_model.py文件里相同的方式调用paddleslim.analysis 中的flops计算flops，并通过fluid.io.get_program_parameter( infer_prog )获取模型参数列表，计算所有层数的shape乘积之和得到Parameters，输入图像大小都为608x608，最终在YOLOv3-Darknet53模型中获得的flops比PP-YOLO论文中的Darknet53 YOLOv3模型的65.52G高出了4.2G左右，Parameters比论文中的59.13 M多了2.45M，而PP-YOLO模型的flops比PP-YOLO论文中的45.12G高出了2.7G左右，Parameters比论文中的44.93 M多了1.7M。理论上单类别模型的flops应该比多类别模型的flops小才对，模型的Parameters应该不变吧？直接将.yml文件里的num_classes由1改为80，计算出两个模型的flops和Parameters也比论文中的大几个G和几个M，请问是我计算的方式不对吗？
关于solov2原论文说可以用来做目标检测,PaddlePaddle/PaddleDetection,2020-12-07 14:29:14,6,question,1837,758561648,"请问paddle有做这一方面的拓展吗？
如果想做的话 该如何下手呢
如果我想将solov2用作于目标检测，将得到的mask的4个坐标获取后得到anchors 如何进行bbox的回归和loss设计呢？
以及后续的loss问题"
ppyolo使用paddle-trt出现问题,PaddlePaddle/PaddleDetection,2020-11-30 02:17:07,3,deploy,1783,753086884,ppyolo使用paddle-trt在jetson平台上cpp推理出现问题，没有框，全是标签。fluid模式检测正常。因为后面打算做一下量化，必须使用trt才能加速。
Is the models in modelZOO exported model?,PaddlePaddle/PaddleDetection,2020-11-28 13:43:19,4,,1778,752679173,DO I have to export the models in modelzoo for deployment purpose or use it as it is
请问我怎么利用PaddleDetection实现给定原图以及ground-truth的boxes位置，输出每个box对应的向量feature,PaddlePaddle/PaddleDetection,2020-11-27 08:51:49,3,,1773,752085841,
ppyolo训练问题：,PaddlePaddle/PaddleDetection,2020-11-24 14:01:46,5,,1759,749731967,"运行PPYOLO.yml，batch size :4,显存占用20%，显卡CUDA利用率0%，一直卡在这个位置，光标闪烁，求指导！！
![image](https://user-images.githubusercontent.com/63913709/100103948-82db7e80-2ea0-11eb-9729-4bb0880332b0.png)
"
When will the updated code be pushed,BaoWangMath/DNN-DataDependentActivation,2019-07-13 05:11:38,0,,1,467672757,"I am interested in your work. Since you said in your readme that the code will be updated, I wonder which part of the current code needs to be updated and when will the updated code to be pushed? Thanks!"
Broken Mujoco links,hughsalimbeni/bayesian_benchmarks,2021-04-08 18:16:15,0,,18,853725725,"The [Mujoco dataset download links](https://github.com/hughsalimbeni/bayesian_benchmarks/blob/master/bayesian_benchmarks/data.py#L759) are broken. I have tested a few of them, including Reacher-V2 and Humanoid-V2.

![Screen Shot of broken link: ""403. That’s an error. We're sorry, but you do not have access to this page. That’s all we know.""](https://user-images.githubusercontent.com/824157/114076662-e628e480-9874-11eb-94b5-3af3b006a57a.png)
"
Cannot do pip install git+... (setup.py/missing __init__ issues),hughsalimbeni/bayesian_benchmarks,2019-05-17 13:04:09,0,,10,445432345,"Cloning the repository and running `python setup.py develop` works and lets you import e.g. `bayesian_benchmarks.tasks`. However, simply running `pip install git+https://github.com/hughsalimbeni/bayesian_benchmarks@master` does not: this doesn't actually put any of the subdirectories into the python site-packages directory. I suspect this is because of the missing `__init__.py` files in the subdirectories.

Also, there's an inconsistency with the naming, you set `name='bayesian_benchmarking'` for the setup() call, whereas the package is actually called `bayesian_benchmarks`, would be good if this was consistent."
adversarial examples,hughsalimbeni/bayesian_benchmarks,2018-06-14 16:30:02,4,enhancement#help wanted,2,332476705,"A key work in this area is https://github.com/YingzhenLi/Dropout_BBalpha

A problem with implementing this method here is that it needs model gradients. Either we could build a  task that supports multiple backends (not ideal) and get the gradients directly, or the model could provide its own gradients which could be manipulated in numpy. @YingzhenLi any thoughts? "
density estimation ,hughsalimbeni/bayesian_benchmarks,2018-06-14 16:23:39,2,enhancement#help wanted,1,332474501,"There are various ways this could be done:
* purely from samples. pros: model agnostic, cons: high variance, need to use a kernel method
* using mixture of gaussians: pros: lower variance, cons: very specific class of models
* synthetic with normalizing flows: pros: explicit density, cons: specific, perhaps unrealistic class of densities 
* synthetic with other functions (e.g. NNs) pros: maybe people care about these functions more, cons: no density, function class is not obviously useful (i.e. just because NNs solve many tasks with lots of data it doesn't follow that draws from random networks are useful functions to learn about)

"
some problems about space_to_depth ,mrjel/group_equivariant_capsules_pytorch,2021-03-12 06:27:33,0,,6,829801069," @mrjel Hi,thanks for sharing this wonderful repo.I'm a beginner to the Capsule Network.

When I run the affnist.py,I got something wrong.
It was about the mismatching of the data format.(The function space_to_depth in pooling_capsule_layer.py)

The wrong statement:
""t_t.contiguous().view(batch_size, d_height, 1, d_depth, s_posev)
RuntimeError: shape '[48, 2, 1, 256, 1]' is invalid for input of size 18432""

I have found the value of t_t when the error occured,which was ""[48,3,2,64,1]"".It's obviously that there got something wrong.
But I didn't change any code while I got the wrong result,and I don't know how to fix it.
Could you help me?Thank you very much!~"
__init__() got an unexpected keyword argument 'norm',mrjel/group_equivariant_capsules_pytorch,2019-08-06 06:10:04,0,,5,477187607,"Hi. After some workarounds, when I tried using your code, I found that the splineconv declaration does not accept the norm argument.
I received the following error: __init__() got an unexpected keyword argument 'norm'
for the conv1 member.
Do clarify this."
a little question about Pseudo-Coordinates,mrjel/group_equivariant_capsules_pytorch,2019-06-18 02:24:21,1,,4,457227884,"Hello @mrjel :
I cloned your code to my cpu-only computer. I follow the instructions in your readme.md. when I run the mnist example, I got a RuntimeError. Here is the error:
```
  warnings.warn('We do not recommend using the non-optimized CPU '
Traceback (most recent call last):
  File ""examples/mnist.py"", line 202, in <module>
    train(epoch)
  File ""examples/mnist.py"", line 140, in train
    f_out, a_out, _, img_out = model(img_batch.to(device))
  File ""/home/wmf997/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File ""examples/mnist.py"", line 71, in forward
    x = F.relu(self.conv1(x, be1, bp1, pose))
  File ""/home/wmf997/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/wmf997/anaconda3/lib/python3.7/site-packages/group_capsules/nn/modules/group_conv_layer.py"", line 58, in forward
    f_out = self.convs[0](x, edge_index, p).squeeze(-1)
  File ""/home/wmf997/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/wmf997/anaconda3/lib/python3.7/site-packages/torch_geometric/nn/conv/spline_conv.py"", line 114, in forward
    min_pseudo, max_pseudo))


RuntimeError: Pseudo-coordinates must lay in the fixed interval [0, 1] but found them in the interval [-5.960464477539063e-08, 1.0]

```
The negative part's absolute value is *quite small*, but it is NOT 0. ...
I do not know where to fix it. 

Yours, 
@wmf1997
"
space_to_depth function meaning?,mrjel/group_equivariant_capsules_pytorch,2019-01-10 06:17:34,3,,3,397687563,"Hi, thanks for sharing this wonderful repo. I found that there is a function space_to_depth defined as follows:

 def space_to_depth(input, block_size):
     block_size = int(block_size)
     block_size_sq = block_size * block_size
     output = input
     (batch_size, s_height, s_width, s_depth, s_posev) = output.size()
     d_depth = s_depth * block_size_sq
     d_height = int((s_height + (block_size - 1)) / block_size)
     t_1 = output.split(block_size, 2)
     stack = [
         t_t.contiguous().view(batch_size, d_height, 1, d_depth, s_posev)
         for t_t in t_1
     ]
     output = torch.cat(stack, 2)
     return output

**Wha's the exact meaning of this function to process pose and aggrement in the forward pass?**

    def forward(self, x, a, pose, size):
        pooled_size = (size[0], int((size[1] + 1) / (self.pool_length)),
                       int((size[2] + 1) / (self.pool_length)))

        a = a.view(*size, self.in_channels)
        pose = pose.view(*size, self.in_channels, 2)

        a = space_to_depth(a.unsqueeze(-1), self.pool_length).squeeze(-1)
        pose = space_to_depth(pose, self.pool_length)

        pose = pose.view(*pooled_size, self.pool_size, self.in_channels, 2)
        a = a.view(*pooled_size, self.pool_size, self.in_channels)
"
setup not working,mrjel/group_equivariant_capsules_pytorch,2018-11-26 16:22:03,4,,1,384406686,"When I run setup.py, it gives the message that installation is complete. However, import does not work. I get the message 
 File ""examples/mnist.py"", line 10, in <module>
  from group_capsules.utils import grid, grid_cluster, make_batch, spread_loss
 ModuleNotFoundError: No module named 'group_capsules'"
Project is not supporting Tf 2.2.0 or higher version,SimonKohl/probabilistic_unet,2020-12-28 17:39:30,1,,17,775511121,"Hi @SimonKohl, did you change your project to support the higher version of tf. Actually, I am working on a project and cannot downgrade my tf 2.2.0 (due to conflict with other work). Any help from your side will be really appreciated. Hope to get a reply from you soon. "
dependency problem at the time of running,SimonKohl/probabilistic_unet,2020-12-03 05:45:03,0,,16,755887597,"I am working on probabilistic Unet. However, there is dependency problem among installed versions. I am using the Linux platform with a GPU cluster. May you please tell me your installed version of TensorFlow GPU, Keras, python, and others.  In the requirement file, some are given, but not all. Following is my error stack, anybody please reply (if encountered the same issue):

a00636.science.domain
0,1
WARNING:tensorflow:From /home/bdp954/PU_Project1/probabilistic_unet/utils/training_utils.py:181: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.
Instructions for updating:
`normal` is a deprecated alias for `truncated_normal`
Logging to /home/bdp954/PU_Project1/probabilistic_unet/model/train.log
Assembling file dict from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train.
Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/jena
Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/stuttgart
Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/aachen
Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/hanover
Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/dusseldorf
Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/hamburg
Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/zurich
Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/erfurt
Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/cologne
Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/krefeld
Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/strasbourg
Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/weimar
Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/bremen
Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/bochum
Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/tubingen
train set comprises 2701 files.
Assembling file dict from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train.
Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/monchengladbach
Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/ulm
Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/darmstadt
train set comprises 274 files.
INFO:tensorflow:Building U-Net.
Building U-Net.
INFO:tensorflow:Building ConvGaussian.
Building ConvGaussian.
INFO:tensorflow:Building ConvGaussian.
Building ConvGaussian.
INFO:tensorflow:encoder scale 0: (?, 27, 256, 512)
encoder scale 0: (?, 27, 256, 512)
INFO:tensorflow:encoder scale 1: (?, 32, 256, 512)
encoder scale 1: (?, 32, 256, 512)
INFO:tensorflow:encoder scale 2: (?, 64, 128, 256)
encoder scale 2: (?, 64, 128, 256)
INFO:tensorflow:encoder scale 3: (?, 128, 64, 128)
encoder scale 3: (?, 128, 64, 128)
INFO:tensorflow:encoder scale 4: (?, 192, 32, 64)
encoder scale 4: (?, 192, 32, 64)
INFO:tensorflow:encoder scale 5: (?, 192, 16, 32)
encoder scale 5: (?, 192, 16, 32)
INFO:tensorflow:encoder scale 6: (?, 192, 8, 16)
encoder scale 6: (?, 192, 8, 16)


Failed to import TensorFlow. Please note that TensorFlow is not installed by default when you install TensorFlow Probability. This is so that users can decide whether to install the GPU-enabled TensorFlow package. To use TensorFlow Probability, please install the most recent version of TensorFlow, by following instructions at https://tensorflow.org/install.


Traceback (most recent call last):
  File ""train_prob_unet.py"", line 188, in <module>
    train(cf)
  File ""train_prob_unet.py"", line 62, in train
    prob_unet(x, y, is_training=True, one_hot_labels=cf.one_hot_labels)
  File ""/home/bdp954/miniconda3/envs/PU_env/lib/python3.6/site-packages/sonnet/python/modules/base.py"", line 389, in __call__
    outputs, subgraph_name_scope = self._template(*args, **kwargs)
  File ""/home/bdp954/miniconda3/envs/PU_env/lib/python3.6/site-packages/tensorflow/python/ops/template.py"", line 359, in __call__
    return self._call_func(args, kwargs)
  File ""/home/bdp954/miniconda3/envs/PU_env/lib/python3.6/site-packages/tensorflow/python/ops/template.py"", line 310, in _call_func
    result = self._func(*args, **kwargs)
  File ""/home/bdp954/miniconda3/envs/PU_env/lib/python3.6/site-packages/sonnet/python/modules/base.py"", line 246, in _build_wrapper
    output = self._build(*args, **kwargs)
  File ""/home/bdp954/PU_Project1/probabilistic_unet/model/probabilistic_unet.py"", line 410, in _build
    self._q = self._posterior(img, seg)
  File ""/home/bdp954/miniconda3/envs/PU_env/lib/python3.6/site-packages/sonnet/python/modules/base.py"", line 389, in __call__
    outputs, subgraph_name_scope = self._template(*args, **kwargs)
  File ""/home/bdp954/miniconda3/envs/PU_env/lib/python3.6/site-packages/tensorflow/python/ops/template.py"", line 359, in __call__
    return self._call_func(args, kwargs)
  File ""/home/bdp954/miniconda3/envs/PU_env/lib/python3.6/site-packages/tensorflow/python/ops/template.py"", line 310, in _call_func
    result = self._func(*args, **kwargs)
  File ""/home/bdp954/miniconda3/envs/PU_env/lib/python3.6/site-packages/sonnet/python/modules/base.py"", line 246, in _build_wrapper
    output = self._build(*args, **kwargs)
  File ""/home/bdp954/PU_Project1/probabilistic_unet/model/probabilistic_unet.py"", line 340, in _build
    return tfd.MultivariateNormalDiag(loc=mu, scale_diag=tf.exp(log_sigma))
  File ""/home/bdp954/miniconda3/envs/PU_env/lib/python3.6/site-packages/tensorflow_probability/python/internal/lazy_loader.py"", line 57, in __getattr__
    module = self._load()
  File ""/home/bdp954/miniconda3/envs/PU_env/lib/python3.6/site-packages/tensorflow_probability/python/internal/lazy_loader.py"", line 41, in _load
    self._on_first_access()
  File ""/home/bdp954/miniconda3/envs/PU_env/lib/python3.6/site-packages/tensorflow_probability/python/__init__.py"", line 41, in _validate_tf_environment
    import tensorflow.compat.v1 as tf
ModuleNotFoundError: No module named 'tensorflow.compat.v1'
"
LIDC data crops download no longer public,SimonKohl/probabilistic_unet,2020-10-21 14:14:00,1,,14,726533794,"Hello,

the download link for the cropped LIDC data is no longer publically available. It instead leads to a SSO application for google employees. 


From the Readme file:
> The **LIDC data** can be downloaded as pngs, cropped to size 180 x 180 from Google Cloud Storage, see here: [data link](https://pantheon.corp.google.com/storage/browser/hpunet-data/lidc_crops).

Is this intentional? Where can we find the preprocessed data?"
Posterior net input,SimonKohl/probabilistic_unet,2020-06-03 02:09:12,0,,12,629635865,Why do you subtract segmentation ground truth by 0.5 before concatenating with the image to feed into the posterior net?
how can i solve the problem!,SimonKohl/probabilistic_unet,2019-10-30 03:29:52,0,,9,514373412,"ModuleNotFoundError: No module named 'batchgenerators'

the  'batchgenerators' at the location of '\data\cityscapes\data_loader.py'"
Why do you call the loss function ELBO？,SimonKohl/probabilistic_unet,2019-10-29 12:11:35,1,,8,513873718,"Thanks for awsome codes and paper. However, I cannot formulate the first term in ELBO as the cross-entropy function which used in your loss function. 

What's more, it is a little strange to add the cross-entropy related to the segmentation result from Z_q. Because the Q is generated from the ground-truth, and the S_q is from Z_q. Is it meaningful to only calculate the CE(Y, S_q) rather than CE(Y, S_p)? I mean that the model has gotten the ground-truth in training phase, it is unfair to calculate CE for this model."
Inference on a single image,SimonKohl/probabilistic_unet,2019-07-22 06:04:11,1,,7,470908525,How do I do the inference on a single image using the provided pretrained weights?
Single channel images application,SimonKohl/probabilistic_unet,2019-07-03 15:40:00,0,,6,463824201,"Hi!
First of all, thank you very much for sharing your code.

I would like to know if I could use it with single channel MR images.

Thank you!"
Model does not train at all,SimonKohl/probabilistic_unet,2019-06-24 13:17:58,0,,5,459886918,"I have successfully ran the training on the Cityscapes dataset while only changing the paths in the config files and fixing issue #2 and executed the evaluation scripts.

However, the results were completely different than those presented in the readme. In fact, after investigating, it seems that the trained model only outputs a single number across all images - 3, which seems to be the id for the class 'wall'. This shows that the model failed to learn any task, even though all the learning parameters were preserved from your code.

Do you have any pointers you could give as to what might be causing such a failure and how could I go about making the model actually learn something?"
Editing exp_dir and using weights from checkpoint files,SimonKohl/probabilistic_unet,2019-02-06 09:06:29,0,,3,407137583,"Hello Simon! Tank you for sharing this code, it helps me with my research on semantic segmentation! 

I have a fairly stupid issue: I would like to use the pertained model and I downloaded the weights from the zenodo.org. However, after running the --write samples  function, I get this error in conda prompt: 
""AttributeError: module 'model.pretrained_weights' has  no attribute __file__ It comes to the problem of extracting weights, i.e. I guess I don't know how to edit exp_dir path and use weights from ckpt files. Would anyone please help me understand how to edit this path and use the data? 

I tried extracting the ckpt data with tf.Session but I encountered other issues and I assume there's simpler solution to this?

Thank you!"
ValueError: MirrorTransform now takes the axes as the spatial dimensions.,SimonKohl/probabilistic_unet,2019-01-31 13:52:51,2,,2,405255561,"I installed all the required dependancies and tried hard to run the code but stuck at following error. Please help

Reading from /home/gpu3/prob_unet/output/quarter/train/stuttgart
Reading from /home/gpu3/prob_unet/output/quarter/train/weimar
Reading from /home/gpu3/prob_unet/output/quarter/train/jena
Reading from /home/gpu3/prob_unet/output/quarter/train/aachen
Reading from /home/gpu3/prob_unet/output/quarter/train/bochum
Reading from /home/gpu3/prob_unet/output/quarter/train/hanover
Reading from /home/gpu3/prob_unet/output/quarter/train/tubingen
Reading from /home/gpu3/prob_unet/output/quarter/train/erfurt
Reading from /home/gpu3/prob_unet/output/quarter/train/bremen
Reading from /home/gpu3/prob_unet/output/quarter/train/zurich
Reading from /home/gpu3/prob_unet/output/quarter/train/hamburg
Reading from /home/gpu3/prob_unet/output/quarter/train/dusseldorf
Reading from /home/gpu3/prob_unet/output/quarter/train/cologne
Reading from /home/gpu3/prob_unet/output/quarter/train/strasbourg
Reading from /home/gpu3/prob_unet/output/quarter/train/krefeld
train set comprises 2701 files.
Traceback (most recent call last):
  File ""train_prob_unet.py"", line 188, in <module>
    train(cf)
  File ""train_prob_unet.py"", line 39, in train
    data_provider = get_train_generators(cf)
  File ""/home/gpu3/probabilistic_unet/training/data/cityscapes/data_loader.py"", line 335, in get_train_generators
    n_batches=cf.n_train_batches)
  File ""/home/gpu3/probabilistic_unet/training/data/cityscapes/data_loader.py"", line 300, in create_data_gen_pipeline
    mirror_transform = MirrorTransform(axes=(3,))
  File ""/home/gpu3/probabilistic_unet/training/batchgenerators/transforms/spatial_transforms.py"", line 192, in __init__
    raise ValueError(""MirrorTransform now takes the axes as the spatial dimensions. What previously was ""
ValueError: MirrorTransform now takes the axes as the spatial dimensions. What previously was axes=(2, 3, 4) to mirror along all spatial dimensions of a 5d tensor (b, c, x, y, z) is now axes=(0, 1, 2). Please adapt your scripts accordingly.
"
possibly a bug? ,jthsieh/DDPAE-video-prediction,2019-10-08 19:34:09,5,,3,504235896,"in pose_rnn.py, L80:
```
#h = torch.cat([hidden[0][0:1], hidden[0][1:]], dim=2)
#c = torch.cat([hidden[1][0:1], hidden[1][1:]], dim=2)
# these 2 lines throw a dim error, is it supposed to be:  
h = hidden[0] 
c = hidden[1]
```"
how to decompose the components of the video?,jthsieh/DDPAE-video-prediction,2019-03-07 08:51:51,0,,2,418193238,"Hi,
It is really an interested and wonderful work!
I just want to know how you decompose the components of the video? I have read the paper carefully, but I still can not quite  figure it out.
And another question is that how long it takes to complete a Moving MNIST experiment on your device?"
QSGD implementation question,hwang595/ATOMO,2019-08-12 21:11:41,0,,2,479852402,"Hi! Thanks a lot for the ATOMO paper and sharing your code over here. I just had a look at your implementation of 1-bit QSGD compression, and was wondering where the Elias coding is implemented. I would be curious if you have any thoughts on doing this efficiently. I found `utils.compress`, which I thought could be related, but I cannot find a use of this function in this repo.
"
MPI TRUNCATED when run imagenet dataset on resnet18,hwang595/ATOMO,2019-05-24 15:44:11,2,,1,448236850,"hello， I‘m trying to test this code on imagenet, but I find that when the program runs to `self.comm.Bcast([self.model_recv_buf.recv_buf[layer_idx], MPI.DOUBLE], root=0)` in function `async_fetch_weights_bcast` in distributed_worker.py at the first step, it thrown an error that is `MPI_ERR_TRUNCATE: message truncated
` , but I check the memory size in Bcast and it works when the program ran on Cifar10/100, have u encountered this problem? 

And another issue: then I replaced the Pytorch0.3.0 with Pytorch0.4/1.1, the proceeding time on decode of QSGD is significantly higher than 0.3.0, almost 10 times than it, have u tried this?
"
bmaml for discrete actions,jaesik817/bmaml_rl,2020-06-08 21:37:04,0,,5,634951335,Any chances to release code for applying bmaml on discrete actions? Thanks!
Growing memory consumption,jaesik817/bmaml_rl,2019-08-27 23:24:05,1,,1,486081863,"When I ran multiple training runs in parallel, some of them started crashing due to running out of cpu memory. The memory consumption of the bmaml code keeps growing during training. The memory profile of running the `bmaml_point.py` at d563f99c9a372a3a28a2a56dee5bde8aeb6bf78a is presented in the following image. The python environment is from the supplied `environment.yml`.

![bmaml_memory_profile](https://user-images.githubusercontent.com/4975769/63814514-2a18e100-c8ff-11e9-93fb-ec8df9be5449.png)

The command I ran is the following
`python bmaml_examples/point/bmaml_point.py --num_particles=5 --num_parallel=10 --fast_lr=0.1 --meta_step_size=0.01 --fast_batch_size=10 --meta_batch_size=20 --meta_iter=100 --meta_method=trpo --method=svpg --svpg_alpha=1.0`

Any ideas on how to fix the issue or work around it?"
"Values X1, X2 empty in the first",kirthevasank/nasbot,2020-06-09 03:19:06,0,,9,635078807,"Dear @kirthevasank,

Thanks for your great implementation. I try to run the code with **demos/demo_cnn.py**. But I see X1, X2 is empty to compute the distance, leading to a bug in computation.
In file **opt/nasbot.py**
init_gp_fitter = NNGPFitter([], [], self.domain.get_type(), tp_comp=self.tp_comp,
                                list_of_dists=None, options=self.options,
                                reporter=self.reporter)

 Could you help to solve this issue? Thanks so much."
The different error rate curves,kirthevasank/nasbot,2019-10-17 11:39:38,1,,8,508417001,"Hi!
Just as your paper, the Cifar-10's error rate curves could be plot as this.
![561379e649e4025ae4bc3c77ee6c7d7](https://user-images.githubusercontent.com/45760614/67005618-97820a80-f115-11e9-82b6-03a5cfd263fd.png)
However, When I tried to run your code, I got this curves.
![39bbaaa6812ac5b06449fd68b1de4f9](https://user-images.githubusercontent.com/45760614/67005644-ad8fcb00-f115-11e9-9297-164343320b96.png)
I want to know if there is something wrong "
SliceLocalization.p is unreadable  ,kirthevasank/nasbot,2019-01-10 07:17:14,4,,6,397701041,"SliceLocalization. P is unreadable  and always reports errors：
ValueError: insecure string pickle.
It was already dumped. Forget to close when saving：https://stackoverflow.com/questions/1746825/valueerror-insecure-string-pickle
Can you give a new one ?"
The accuracy problem,kirthevasank/nasbot,2018-12-29 02:20:55,0,,5,394751928,"Hi!,@kirthevasank, I have forked the project and run it in the cifar10 dataset, all the architectures searched by the Nasbot have low accuracy compare to the SOTA architectures. I have changed some HyperParameters, but it's still not work, can you guess the reason for the ""lower"" accuracy ? Thanks!! "
Support with big dataset ,kirthevasank/nasbot,2018-11-22 09:50:09,2,,3,383455974,"Hi, 

If we want to find architectures on big datasets such as ImageNet, how to change our code to support it?

Thanks"
Download: data augmentation files: 404 Not Found,ronghanghu/speaker_follower,2021-09-25 13:06:34,2,,26,1007081752,"Hi, while executing ./tasks/R2R/data/download.sh I get the following result:

```
silversurfer42@ideas:~/fun/grounded/speaker_follower/tasks/R2R/data$ sudo bash download.sh 
[sudo] password for silversurfer42: 
--2021-09-25 15:00:49--  https://www.dropbox.com/s/lztjsji51pr5ig2/R2R_train.json
Resolving www.dropbox.com (www.dropbox.com)... 2620:100:6022:18::a27d:4212, 162.125.66.18
Connecting to www.dropbox.com (www.dropbox.com)|2620:100:6022:18::a27d:4212|:443... connected.
HTTP request sent, awaiting response... 301 Moved Permanently
Location: /s/raw/lztjsji51pr5ig2/R2R_train.json [following]
--2021-09-25 15:00:49--  https://www.dropbox.com/s/raw/lztjsji51pr5ig2/R2R_train.json
Reusing existing connection to [www.dropbox.com]:443.
HTTP request sent, awaiting response... 404 Not Found
2021-09-25 15:00:50 ERROR 404: Not Found.

--2021-09-25 15:00:50--  https://www.dropbox.com/s/66nowglznzx1le9/R2R_val_seen.json
Resolving www.dropbox.com (www.dropbox.com)... 2620:100:6022:18::a27d:4212, 162.125.66.18
Connecting to www.dropbox.com (www.dropbox.com)|2620:100:6022:18::a27d:4212|:443... connected.
HTTP request sent, awaiting response... 301 Moved Permanently
Location: /s/raw/66nowglznzx1le9/R2R_val_seen.json [following]
--2021-09-25 15:00:50--  https://www.dropbox.com/s/raw/66nowglznzx1le9/R2R_val_seen.json
Reusing existing connection to [www.dropbox.com]:443.
HTTP request sent, awaiting response... 404 Not Found
2021-09-25 15:00:50 ERROR 404: Not Found.

--2021-09-25 15:00:50--  https://www.dropbox.com/s/it9zpexb97d6bes/R2R_val_unseen.json
Resolving www.dropbox.com (www.dropbox.com)... 2620:100:6022:18::a27d:4212, 162.125.66.18
Connecting to www.dropbox.com (www.dropbox.com)|2620:100:6022:18::a27d:4212|:443... connected.
HTTP request sent, awaiting response... 301 Moved Permanently
Location: /s/raw/it9zpexb97d6bes/R2R_val_unseen.json [following]
--2021-09-25 15:00:51--  https://www.dropbox.com/s/raw/it9zpexb97d6bes/R2R_val_unseen.json
Reusing existing connection to [www.dropbox.com]:443.
HTTP request sent, awaiting response... 404 Not Found
2021-09-25 15:00:51 ERROR 404: Not Found.

--2021-09-25 15:00:51--  https://www.dropbox.com/s/0huat2lc5iy5o8j/R2R_test.json
Resolving www.dropbox.com (www.dropbox.com)... 2620:100:6022:18::a27d:4212, 162.125.66.18
Connecting to www.dropbox.com (www.dropbox.com)|2620:100:6022:18::a27d:4212|:443... connected.
HTTP request sent, awaiting response... 301 Moved Permanently
Location: /s/raw/0huat2lc5iy5o8j/R2R_test.json [following]
--2021-09-25 15:00:52--  https://www.dropbox.com/s/raw/0huat2lc5iy5o8j/R2R_test.json
Reusing existing connection to [www.dropbox.com]:443.
HTTP request sent, awaiting response... 404 Not Found
2021-09-25 15:00:52 ERROR 404: Not Found.

--2021-09-25 15:00:52--  http://people.eecs.berkeley.edu/~ronghang/projects/speaker_follower/data_augmentation/R2R_data_augmentation_paths.json
Resolving people.eecs.berkeley.edu (people.eecs.berkeley.edu)... 128.32.244.190
Connecting to people.eecs.berkeley.edu (people.eecs.berkeley.edu)|128.32.244.190|:80... connected.
HTTP request sent, awaiting response... 404 Not Found
2021-09-25 15:00:54 ERROR 404: Not Found.
```

Similarly, when I execute ./tasks/R2R/data/download_precomputed_augmentation.sh, I get 

`--2021-09-25 15:05:13--  http://people.eecs.berkeley.edu/~ronghang/projects/speaker_follower/data_augmentation/R2R_literal_speaker_data_augmentation_paths.json
Resolving people.eecs.berkeley.edu (people.eecs.berkeley.edu)... 128.32.244.190
Connecting to people.eecs.berkeley.edu (people.eecs.berkeley.edu)|128.32.244.190|:80... connected.
HTTP request sent, awaiting response... 404 Not Found
2021-09-25 15:05:14 ERROR 404: Not Found.`

Now, it seems I already have the files from the dropbox downloaded from some time ago (can mirror if somone needs them). However, I do not have the berkeley files. Does somebody have them and can share them?

"
Killed while evaluating,ronghanghu/speaker_follower,2020-11-11 14:09:03,0,,20,740794341,I changed the dataset from R2R to R4R which contains over 45k instructions in the val_unseen dataset. The training is killed when about 15k of them are evaluated. The machine I am using has 64 GB memory and a Tesla V100 graphic card. The batch_size is set to 8. I am not sure what is the bottleneck here. I guess it is the result dictionary that is taking up too much memory? Is it a good practice to release the memory every about 10k results?
How to use non-panoramic space ,ronghanghu/speaker_follower,2018-12-03 13:43:43,1,,14,386823816,"Hi,
could you please describe what changes ones need to make in order to train your model with data augmentation and pragmatic inference, but without panoramic space, so normal camera view? (5th row in Table 1 from the paper)"
Second step of training is unclear,ronghanghu/speaker_follower,2018-12-03 11:37:57,1,,13,386778341,"Hi,
when I try to do the second step of the training 
`export SPEAKER_PATH_PREFIX=tasks/R2R/speaker/snapshots/speaker_teacher_imagenet_mean_pooled_train_iter_20000`
it fails because the files available after training are speaker_teacher_imagenet_mean_pooled_train_iter_20000_enc and speaker_teacher_imagenet_mean_pooled_train_iter_20000_dec, which one should we export to the path? Or did something go wrong during the speaker training?"
How to get images from simulator?,ronghanghu/speaker_follower,2018-11-29 05:52:13,0,,12,385580649,"Hi, if we want to use our own image feature extractor, how do we get the images for a given viewpoint?"
Why don't you use beam search to synthesize instructions? ,ronghanghu/speaker_follower,2018-11-22 07:36:29,0,,11,383412469,"Hi, have you tried using beam search to synthesize instructions? I saw your beam search code in speaker but you simply use greedy search to synthesize instructions.

https://github.com/ronghanghu/speaker_follower/blob/389ee0229b6fabe6ab93b00bdcaf1131912a8474/tasks/R2R/speaker.py#L342"
Visualization Code,ronghanghu/speaker_follower,2018-10-25 16:56:38,11,,8,374041381,"Hi
In https://arxiv.org/pdf/1806.02724.pdf, there are some visualizations of the predicted trajectories in Fig 4. Is the code to generate these available?

"
Multistep simultaneous move game and visualizatoin,deepmind/open_spiel,2022-10-24 03:46:13,1,,957,1420164088,I am looking for a zero-sum or fully cooperative multistep simultaneous move games (SMG) that are used as standard benchmarks for evaluation (especially where I can control the size of game)? I am also looking for a way to use and visualize SMG as imperfect information extensive form game. I am assuming I will have to use `convert_to_turn_based` method to convert the SMG to imperfect information extensive form game but I am not sure how to visualize it?
EFG parser assumes optional labels are unique and required,deepmind/open_spiel,2022-10-22 03:05:44,10,,955,1419049147,"The EFG parser should not depend on the presence or uniqueness of action labels.

Gambit's [documentation for the EFG format](https://gambitproject.readthedocs.io/en/v16.0.2/formats.html#conventions-common-to-all-file-formats) states
> it is encouraged for users to assign nonempty text labels to objects if the game is going to be viewed in the graphical interface.

The current EFG parser uses labels to identify actions, and fails in cases where the labels are empty or even non-unique.

Consider this game with a value of 4:

```
EFG 2 R ""Unlabeled"" { """" """" }
""no labels""

p """" 1 1 """" { """" """" } 0
c """" 1 """" { """" .99 """" .01 } 0
p """" 2 1 """" { """" """" } 0
t """" 1 """" { 2 -2 }
t """" 2 """" { 3 -3 }
p """" 2 2 """" { """" """" } 0
t """" 1 """" { 5 -5 }
t """" 2 """" { 3 -3 }
c """" 2 """" { """" .01 """" .99 } 0
p """" 2 1 """" { """" """" } 0
t """" 3 """" { 6 -6 }
t """" 4 """" { 4, -4 }
p """" 2 2 """" { """" """" } 0
t """" 3 """" { 6, -6 }
t """" 4 """" { 4, -4 }
```

With no labels, OpenSpiel thinks the value is 8; with labels, it correctly returns 4. Gambit returns 4 in both instances.

``` py
import sys
import pyspiel
from open_spiel.python.algorithms import sequence_form_lp


if __name__ == '__main__':
    inp = sys.stdin.read()
    game = pyspiel.load_efg_game(inp)
    actual, _, _, _ = sequence_form_lp.solve_zero_sum_game(game)
    print(actual)

```
Test code in `pygambit` for reference.
```py
import sys
import pygambit


if __name__ == '__main__':
    inp = sys.stdin.read()
    efg = pygambit.Game.parse_game(inp)
    equilibrium = pygambit.nash.enummixed_solve(efg, rational=False)
    print(equilibrium[0].payoff(efg.players[0]))

```"
A question about laser_tag,deepmind/open_spiel,2022-10-19 12:09:54,4,,953,1414858365,"Is laser_tag not available for this game? When I run example --game=laser_tag, the following message appears:
Creating game..

Starting new game...
Initial state:
State:
.......
.......
..*.*..
.**.**.
..*.*..
.......
.......
Orientations: 1 1
Chance Node
player -1
sampled outcome: (spawned at location #2)
State: 
.......
.......
..*.*..
.**.**.
..*.*..
.......
B......
Orientations: 1 1
Chance Node
player -1
sampled outcome: (spawned at location #1)
State: 
......A
.......
..*.*..
.**.**.
..*.*..
.......
B......
Orientations: 1 1

player -2
Spiel Fatal Error: InformationStateTensorShape unimplemented.
When I run pyspiel.load_game(""laser_tag"",{""players"": 2}) I get the following error:
pyspiel.SpielError: Unknown parameter 'players'. Available parameters are: grid, horizon, zero_sum"
Loading sample EFG files throws an exception,deepmind/open_spiel,2022-10-19 01:53:30,2,,952,1414121349,"Trying to load sample games in the Gambit EFG format from Gambit's documentation results in a thrown exception.

Here are their sample games
https://gambitproject.readthedocs.io/en/v16.0.2/samples.html

Here is the affected code
```
import pyspiel


efg = """"""
EFG 2 R ""Bagwell Commitment and Unobservability Example"" { ""Player 1"" ""Player 2"" }
""Stackelberg game with imperfectly observed commitment, from Bagwell (1993)""

p """" 1 1 """" { ""S"" ""C"" } 0
c """" 1 """" { ""s"" 99/100 ""c"" 1/100 } 0
p """" 2 1 """" { ""S"" ""C"" } 0
t """" 1 ""SS"" { 5, 2 }
t """" 2 ""SC"" { 3, 1 }
p """" 2 2 """" { ""S"" ""C"" } 0
t """" 1 ""SS"" { 5, 2 }
t """" 2 ""SC"" { 3, 1 }
c """" 2 """" { ""s"" 1/100 ""c"" 99/100 } 0
p """" 2 1 """" { ""S"" ""C"" } 0
t """" 3 ""CS"" { 6, 3 }
t """" 4 ""CC"" { 4, 4 }
p """" 2 2 """" { ""S"" ""C"" } 0
t """" 3 ""CS"" { 6, 3 }
t """" 4 ""CC"" { 4, 4 }
""""""

pyspiel.load_efg_game(efg)
```

Which throws
```
$ python main.py 
OpenSpiel exception: /project/open_spiel/games/efg_game.cc:663 CHECK_TRUE(ParseDoubleValue(NextToken(), &utility))
 while parsing line #7:
p """" 2 1 """" { ""S"" ""C"" } 0

Traceback (most recent call last):
  File ""/home/votroto/spiel_bug_report/main.py"", line 25, in <module>
    pyspiel.load_efg_game(efg)
pyspiel.SpielError: /project/open_spiel/games/efg_game.cc:663 CHECK_TRUE(ParseDoubleValue(NextToken(), &utility))
 while parsing line #7:
p """" 2 1 """" { ""S"" ""C"" } 0
```"
Simplest 2 player perfect information game?,deepmind/open_spiel,2022-10-14 01:00:53,5,question,948,1408602579,"As the title says, I'm wondering which game is the simplest (as in smallest, fastest to train on) for 2 players and perfect information. It looks like it should be Pathfinding but I am unsure how large the grid is and whether I can reduce the rows/columns. I'm not sure if multiagent Pathfinding is zero-sum or cooperative but that matters less."
Optimal pricing policy in the game between the entrepreneur and the environment in a B2B setting,deepmind/open_spiel,2022-10-07 15:37:25,4,,944,1401367326,"Hi,

I find it interesting to model prices from the point of view of marketing in an entrepreneurial setting. This is relevant because the product is sold on the basis of the entrepreneur's characteristics too along with the product and consumer economics. 

So, the environment comprises of consumers that adopt the new product (Bass, 1969 (https://pdodds.w3.uvm.edu/files/papers/others/everything/bass1969a.pdf)). Adoption is driven by their innovativeness which is relative in that an individual has either more or less of it than others in a social system (Rogers & Shoemaker, 1971).

For B2B products, early adopters are interested in adopting the product early and using it in their supply chain. Innovators are interested in trying the product and take risks and can also buy products that eventually do not succeed in the marketplace. And then there is a segment of customers that are price conscious.

Do you want me to take a deeper look into this game between the entrepreneur and the environment. This may be important to understand from the technology point of view as IP creation may be involved and the game may have serious cascading implications for the entire supply chain."
Possible contribution: LOLA ,deepmind/open_spiel,2022-09-21 08:43:23,2,contribution welcome,934,1380558468,"Hi, 
I am planning to contribute a JAX implementation of [Learning with Opponent-Learning Awareness"" (Foerster et al.)](https://arxiv.org/abs/1709.04326) if no one currently works on it and if it is still appreciated (I noticed it was mentioned in your call for contributions list).

I just wanted to check before starting to implement it :slightly_smiling_face: "
"BestResponsePolicy requires concrete policy probability values, fails with JAX abstract tracer values",deepmind/open_spiel,2022-09-11 18:52:02,3,,932,1369032241,"[`best_response.BestResponsePolicy`](https://github.com/deepmind/open_spiel/blob/master/open_spiel/python/algorithms/best_response.py#L87) (which is being called by my code via [`exploitability.nash_conv`](https://github.com/deepmind/open_spiel/blob/master/open_spiel/python/algorithms/exploitability.py#L154)) does not work when policy probabilities are JAX abstract tracer values:
```
Traceback (most recent call last):
[...]
  File ""/usr/local/lib/python3.10/site-packages/open_spiel/python/algorithms/best_response.py"", line 181, in <genexpr>
    return sum(p * self.q_value(state, a)
jax._src.errors.ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[])>with<BatchTrace(level=1/1)> with
  val = Traced<ShapedArray(bool[11])>with<DynamicJaxprTrace(level=0/1)>
  batch_dim = 0
The problem arose with the `bool` function. 
This Tracer was created on line /usr/local/lib/python3.10/site-packages/open_spiel/python/algorithms/best_response.py:182 (<genexpr>)

See https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError
```

This is due to the `if p > self._cut_threshold` clauses, which require `p` to be concrete. Is there a recommended workaround? If not, perhaps a `threshold` flag could be added to disable the threshold check completely."
Any methods to use a pytorch model to generate trajectory in C++ side?,deepmind/open_spiel,2022-09-06 08:56:53,1,contribution welcome,928,1362938766,"Hi there, suppose I have a pytorch model, and I want to use it to generate trajectories for reinforcement learning, for example REINFORCE. The code in python should be like
```
state = game.new_initial_state()
while not state.is_terminal():
    action = use_model_to_get_action(state)
    state.apply_action(action)
    save_data() # like state, action, reward, next_state
```
but it seems like a bit slow in python, so i'm wondering if I can use pybind and libtorch to generate trajectories in parallel and get buffers return to python. But I can't find an example, that is, is it possible to pass a network to libtorch and get tensors which need gradients back to python?

Also, I don't know whether it will be faster or should I write this totally in C++?

Any idea for this?"
On the acceleration of convergence process in neural replicator dynamics,deepmind/open_spiel,2022-08-31 02:19:24,1,,923,1356652678,"Hi,guys, i was wondering whether it is possible to accelerate the convergence process in Neural Replicator Dynamics. In this paper,https://arxiv.org/abs/1906.00190, the accumulated values **y**<sub>t</sub> at time t is approximated by Euler discretization. From my knowledge, the second order approximation is available and is it possible to use to accelerate the convergence process?

The analysis is as follows:

The Adams-Bashforth formula is as follows (second order approximation):

y<sub>k+1</sub> = y<sub>k</sub> + h* ( 3/2 * f(t<sub>k</sub>, y<sub>k</sub>) - 1/2 * f(t<sub>k-1</sub>, y<sub>k-1</sub> ))
dy/dt = f(t,y)

From the neural replicator dynamics paper, the NeuRD's policy update formula comes from minimizing the Logit error, and the Logit estimates are given by the Euler discretization.  We know that the Euler discretization is a first-order method, and the Adams-Bashforth method is a second-order method. The Adams-Bashforth method satisfies the root condition and its local truncation error is second order, converges according to the Dahlquist equivalence theorem, and the global error is also second order with higher accuracy than the Euler discretization method. So We can choose  a larger learning step size. And the update applied to NeuRD means that the critic networks, not only at time t, but also at the previous time t-1, give different advantage function estimates. Does this mean that policy will converge faster? 

Any suggestion is appreciated.

"
non-marginal strategy selectors in psro_v2,deepmind/open_spiel,2022-08-29 04:43:55,1,,922,1353692879,"Hello. In the current implementation of psro_v2, right before it is going to do best response, it will first select a subset of strategies from the current strategy pool and then decided how to do BR using these selected strategies:
https://github.com/deepmind/open_spiel/blob/0e0ae2f1827f24dafe82372d1bf4ea1729cb9566/open_spiel/python/algorithms/psro_v2/psro_v2.py#L293-L313

However here is one thing that is unclear to me. When we are considering non-marginal/joint strategy, according to line 310-313 it seems that currently_used_policies should be a list of joint strategies, while current_indexes should be a ""num_player""-sized list each recording a list of policy indexes for each player. This design looks strange to me. 

And I doubled checked in psro_v2/strategy_selector.py, there were non-marginal selector implemented. For example,
https://github.com/deepmind/open_spiel/blob/0e0ae2f1827f24dafe82372d1bf4ea1729cb9566/open_spiel/python/algorithms/psro_v2/strategy_selectors.py#L356-L376

However the interface is not consistent with line 293 in psro_v2.py. Does that mean we need to write a customized strategy selector based on these? If so, is there a easy way we get used_indexes? Thanks."
on neural replicator dynamics,deepmind/open_spiel,2022-08-29 02:26:48,2,,921,1353598039,"Hi guys,
I am trying to implement the neural replicator dynamics algorithm in the paper ""Neural Replicator Dynamics:
Multiagent Learning via Hedging Policy Gradients"", and the link of the article is https://arxiv.org/pdf/1906.00190.pdf.

The pseudo code is:
![2022-08-29 10-04-34屏幕截图](https://user-images.githubusercontent.com/6730777/187108954-03238248-e51d-4b81-bed9-e73620853ed4.png)

I have a few questions, and any suggestions are appreciated
(1) The critic network outputs q value, not v value. Is it correct?  Is there any benefits for using q value? Because the critic network outputs v value in A2C,so i think v value is more reasonable.

(2) The algorithm first sample some trajectories and then do the policy evaluation, which seems like monte carlo sampling. Is it right?
It uses TD error in A2C to do the update. Is it a good idea to use TD error to do the policy evaluation?

(3) A2C and neural RD is very similar, and they both have a critic network and a policy network. There is only one difference, and the last softmax layer is cut off in the neural RD. If i am right, can i just cut off the last softmax layer in A2C and then train the rest part of the network?

 
"
Incorrect docstring in policy_gradient.py,deepmind/open_spiel,2022-08-22 09:22:51,4,documentation,912,1346091747,"https://github.com/deepmind/open_spiel/blob/f4ae28033cc1f92ca4cc9f8bc87477ba9c8c4237/open_spiel/python/algorithms/policy_gradient.py

Hi,
The docstring states the critic learning rate is 0.001, but it really defaults to 0.01.
It has the same issue in PPO, etc., I guess it's related to some copy-pasting or something similar :)

Thanks"
Well-trained model,deepmind/open_spiel,2022-08-10 14:12:53,1,question#contribution welcome,904,1334695170,"Is it possible to release some well-trained models for the games supported by the open spiel framework such as connect four, breakthrough, and othello?"
Comparison between two game states,deepmind/open_spiel,2022-08-10 13:33:58,2,question,903,1334638287,"As the question says, my code tries to compare two different game states. When is `state1 == state2`?

From my limited knowledge of pybind11, I can see that equality for games is determined from this code snippet: https://github.com/deepmind/open_spiel/blob/b5e0bf6495bd8baf7c6011c18fa4fd403e21385d/open_spiel/python/pybind11/pyspiel.cc#L378-L381

However, I cannot find anything similar for states. Any help will be appreciated!"
matrix_game_utils missing sequence-form sparse matrix conversion? ,deepmind/open_spiel,2022-07-09 05:51:19,4,contribution welcome,880,1299580731,"It looks like ExtensiveToMatrixGame defaults to creating a payoff matrix of all pure strategies but that is far from desirable now. Especially with the new sparsification techniques, is there any option to convert the extensive form game (kuhn poker for example) into its sparse matrix form based on sequences? "
How to solve the exploitability of Large Game,deepmind/open_spiel,2022-07-06 13:49:06,5,question,878,1295872454,"In open_spiel/python/example/deep_cfr, the tabular method is usually used to solve the exploitability or NashConv of kuhn poker. 
    """"""
    average_policy = policy.tabular_policy_from_callable(game, deep_cfr_solver.action_probabilities)
    conv = exploitability.nash_conv(game, average_policy)
    """"""
Can we find a good way to calculate the exploitability when the scale of game is large?
Thank you for your reply!"
OpenSpiel Implementation of Diplomacy,deepmind/open_spiel,2022-07-04 16:55:43,3,question#legal,877,1293388610,"Hi,

I was wondering - is there a plan to include Diplomacy in OpenSpiel?
Anthony et al. wrote a (beautiful) paper about SBR which cites OpenSpiel - and I was not sure if it means that it was implemented using the framework.

The thing is, I can probably ""easily"" (nothing is easy but kind of) extend MILA's game engine (https://github.com/diplomacy/diplomacy) or maybe dipcc (SearchBot's) for OpenSpiel (as I partially implemented state encoding, etc. from MILA's engine based on MILA's encoding, to infer using available models) but:

1. It's Python (at least MILA's), not CPP - therefore slower.
2. I am not sure about the copyright (it's possible that they don't allow it anyway).
3. It has ""heavy"" dependencies currently.

So I guess my main question would be - does it worth thinking about/contracting the creators of the game engine, or are you planning to integrate your version? Are there additional copyright issues that you are aware of regarding that game?

Thanks!"
Call for New Games,deepmind/open_spiel,2022-05-23 10:29:54,12,,843,1244948060,"Hi all, 

I'm starting a thread to collect requests for new games and plans for upcoming games. We will keep this updated as new games get released, and as requests come in (please feel free to comment on this thread to add a request). 

Games we would like to add to OpenSpiel:

- [Chinese Checkers](https://en.wikipedia.org/wiki/Chinese_checkers)
- [Chinese Chess](https://en.wikipedia.org/wiki/Xiangqi) (see https://github.com/deepmind/open_spiel/issues/802)
- [Chinese Dark Chess](https://en.wikipedia.org/wiki/Banqi)
- [Cribbage](https://en.wikipedia.org/wiki/Cribbage)
- Dominoes
- [Dou dizhu](https://en.wikipedia.org/wiki/Dou_dizhu)
- [EinStein würfelt nicht!](https://en.wikipedia.org/wiki/EinStein_w%C3%BCrfelt_nicht!)
- Latent Tic-Tac-Toe (see description in [this paper](http://mlanctot.info/files/papers/nips09mccfr.pdf))
- [Mahjong](https://en.wikipedia.org/wiki/Mahjong)
- [Mensch-Ärgere-Dich-Nicht](https://en.wikipedia.org/wiki/Mensch_%C3%A4rgere_Dich_nicht) (PR submitted: https://github.com/deepmind/open_spiel/pull/915)
- [Nine Men's Morris](https://en.wikipedia.org/wiki/Nine_men%27s_morris)
- Pursuit-evasion game played on a graph or grid
- [Shogi](https://en.wikipedia.org/wiki/Shogi)
- [Yacht](https://en.wikipedia.org/wiki/Yacht_(dice_game))

Implementing one of these games is a great way to get involved with contributing to OpenSpiel. If you would like to volunteer to implement one, please contact us and let us know as we might already be working on them. 

Games that have been added since posting this thread:
- [2048](https://en.wikipedia.org/wiki/2048_(video_game)) (Thanks @Jazeem. PR now merged: https://github.com/deepmind/open_spiel/pull/894)
- Checkers (thanks @Jazeem! PR now merged: https://github.com/deepmind/open_spiel/pull/861)
- Euchre (thanks @jhtschultz! Added in https://github.com/deepmind/open_spiel/commit/a3dbb1b7483b3b02077a92bb3a84389907afb79b)
- Mancala (Kalah) (thanks @Jazeem! see https://github.com/deepmind/open_spiel/pull/841)
- [Nim](https://en.wikipedia.org/wiki/Nim) (thanks @acforvs! PR merged: https://github.com/deepmind/open_spiel/pull/868)
- Phantom Go (thanks @Syor! PR now merged: https://github.com/deepmind/open_spiel/pull/867)

With these additions, OpenSpiel now has 95 games. Can we make up to 100 in 2022?
"
Minor discussions about pathfinding games,deepmind/open_spiel,2022-04-22 21:04:14,1,,822,1212780848,"Hello there. I am currently working on pathfinding games and found the implementation here really helpful!

Some minor questions about the implementation. According to the reference [here](https://www.jmlr.org/papers/volume4/hu03a/hu03a.pdf) there maybe several mechanics that are missing: (1) when an agent hits an obstacle, actually there could be a probability to pass through the block. (2) when two agents contest they may also incur a penalty (3) two agents may share the same destination. According to my understanding, they are not currently supported?"
Gym Wrapper to work with multiagent RL frameworks?,deepmind/open_spiel,2022-04-22 15:45:13,3,contribution welcome,820,1212517945,"I found Openspiel super useful in my research. I am wondering if the API is friendly to any of the popular multiagent RL frameworks(e.g. rllib, stablebaselines3, tianshou) so that we can use different algorithms/develop custom algorithms directly from the RL frameworks? "
[RBC] Various issues encountered during the use of RBC,deepmind/open_spiel,2022-04-04 23:54:43,4,,811,1192440037,"Hello, first off thanks for creating this amazing group of environments for board games! It is only place where I have found an implementation of ReconBuildChess besides APL's own implementation.

I've been trying to utilize the RBC environment and here are some issues/pain points that I have encountered and thought I'd share:

Note: I am using the RBC environment through a gym like wrapper created by the [ray-project](https://github.com/ray-project) team [here](https://github.com/ray-project/ray/blob/master/rllib/env/wrappers/open_spiel.py)

So far my main goal has been to recreate the random, attacker, and trout bots that APL provides located [here](https://github.com/reconnaissanceblindchess/reconchess/tree/master/reconchess/bots).
[Here](https://github.com/acxz/blinding-ray/tree/master/src/blinding_ray/agents) is my progress. Which I have been able to recreate with some slight issues and workarounds.

Ultimately, I'd like to have the same API for creating bots as APL does so that a class that defines an APL bot can be used one to one for a ray/rllib policy with their open_spiel wrapper. Right now I have a rough skeleton on where the APL bot functions go in the ray/rllib pipeline, but don't separate them into their own functions.

Basically, be able to train an RBC agent using a rllib with the open_spiel environment and then deploy it on the APL rbc servers.

Anyway here are some issues I have had concerning open_spiel's rbc implementation.

## 1) Not all sense actions are encoded.
https://github.com/deepmind/open_spiel/blob/b6f88b6f2fddadd7e0f8e01c78d2becb61c51a33/open_spiel/games/rbc.cc#L421-L423

From what I understand of the above code, since a sense grid is 3x3, it make less sense for an agent to sense at the edges of the board, i.e. ranks 1 and 8 and files a and h. First off, this is different from APL's implementation as they do allow you to sense any location. Secondly, this is currently not implemented properly as the `cached_legal_actions_` is simply resized. this gives the first (inner_size x inner_size 6x6) 36 actions, instead of the 36 actions corresponding to the inner board. Effectively, allowing the agent to sense file a, but not files g or h.

I believe the proper way to fix this is to allow the agent to sense anywhere on the game board, even on the edges, for APL consistency's sake, but honestly just fixing the logic for the `cached_legal_actions_` should be good enough.

Related commit: https://github.com/deepmind/open_spiel/commit/d1f6735d3c0ddc1ac70ebe18c33bfff3a06e7d88

## 2) Return the previous taken_action in ObservationString/ObservationTensor

RBC is an uncertain game and the requested action may not be the action that is taken due to the board state. This information is required for agents to properly keep track of the game state and is information that APL's RBC provides. See 6i) for more info on how I incorrectly workaround this issue currently using state.history. But essentially, the previous taken_action should be provided to the agents via both the ObservationString and the ObservationTensor. See APL's [`hande_move_result`](https://github.com/reconnaissanceblindchess/reconchess/blob/master/reconchess/player.py#L135) for more info.

Maybe this information exists, but I have not been able to extract it from ObservationString.

Current rationale:

https://github.com/deepmind/open_spiel/blob/0c265fbaa56c418788333fdf3bd373ff12620d14/open_spiel/games/rbc.h#L93-L94

I believe this should be changed to provide the information explicitly in the ObservationString/ObservationTensor.
This could also help speed up learning, as this important doesn't have to be extracted.

## 3) Return your captured piece and location in ObservationString/ObservationTensor

Previously related issue: https://github.com/deepmind/open_spiel/issues/696

APL's RBC will return the your captured piece and location for you to handle any internal logic based on the opponents move. This allows you to update say your internal board representation based on pieces you have lost.
From what I can tell, in open_spiel, you are told that your piece is captured, but not which one or where.

https://github.com/deepmind/open_spiel/blob/b6f88b6f2fddadd7e0f8e01c78d2becb61c51a33/open_spiel/games/rbc.cc#L220-L221

This looks to be missing functionality.
See [`handle_opponent_move_result`](https://github.com/reconnaissanceblindchess/reconchess/blob/master/reconchess/player.py#L47) for more info.

Current rationale:
https://github.com/deepmind/open_spiel/blob/0c265fbaa56c418788333fdf3bd373ff12620d14/open_spiel/games/rbc.h#L75-L76
I believe this should be changed to provide the information explicitly in the ObservationString/ObservationTensor.

This could also help speed up learning, as this important doesn't have to be extracted.

## 4) Return the previous sense action in ObservationString/ObservationTensor

This is similar to (2): return the previous taken_action. The ObservationString contains the current observed board and does contain the newly sensed information when calling state.observation_string() during the move phase. However, the open_spiel env should return which grids were requested to sense so that the agent can parse the ObservationString to determine the latest sense results. See: [`handle_sense_result`](https://github.com/reconnaissanceblindchess/reconchess/blob/master/reconchess/player.py#L92) for what APL provides the agents.

## 5) Pawn promotion to queen by default

Certain moves that are illegal will still have an effect on the board and this case is not captured in open_spiel's implementation. Particularly, say my pawn is at d7 and the move my algorithm choose is d7e8 to capture an opponent piece. Based on the legal moves (for both open_spiel and APL), d7e8 does not exist, but d7e8q does. (with the queen promotion). APL's RBC will convert my d7e8 to d7e8q but open_spiel's rbc will throw an error saying d7e8 does not exist.


-------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Note: The qualms below are only valid since (2)-(4) are not currently properly implemented. This has made try to use state.action_to_string() to attempt to obtain the missing information in uci form, which made me realize the limitations that the current `action_to_string()` method has. (6) may not/is not worth the time if (2)-(4) are properly handled and the information is provided in ObservationString.

## 6) Cannot call `action_to_string()` and `string_to_action()` with the different action spaces (sense action space, move action space).

https://github.com/deepmind/open_spiel/blob/b6f88b6f2fddadd7e0f8e01c78d2becb61c51a33/open_spiel/games/rbc.cc#L444-L454 

This one might be a bit harder to explain, but essentially when creating an agent that uses the uci action encoding (for example an agent based on stockfish) then moving between the open_spiel action representation and the uci representation is important. Moreover, it is important to be able to use the `action_to_string` method on the input space of move actions while I am currently in sense mode and vice versa. Here are the following cases where it is particularly needed.

i) During an agent's sense phase, the agent would like to [`hande_move_result`](https://github.com/reconnaissanceblindchess/reconchess/blob/master/reconchess/player.py#L135) of its last move that it took. This means that (See (2)) the open_spiel env should provide the agent the taken_move (note which is different from the requested move as rbc is an uncertain game). This taken_move should be provided in the `ObservationString`, but since it is not I actually use the `state.history()` to obtain the last requested_move by the player (which is in open_spiel move action space). For me to convert this into something that my agent can utilize (say to keep track of its internal board representation) then I need to use `action_to_string(taken_action)`. This will fail since `action_to_string` is being called during the sense phase and not the move phase. To handle this, I actually keep a deep copy of a state which is in sense phase, and a copy of a state which is in move phase, so that I can call `move_state.action_to_string(taken_action)` during the sense phase to get the proper string of the corresponding previous move action.

See the following for how I handle this in detail:
```python
                # handle_move_result
                # if a move was executed, apply it to our board
                taken_action_history = state.history()
                color_shift = not self.color
                player_taken_action_history = \
                    [player_taken_action
                     for move_count in range(len(taken_action_history)//4)
                     for player_taken_action in
                     (taken_action_history[2*(2*move_count+color_shift)],
                      taken_action_history[2*(2*move_count+color_shift)+1])]
                taken_action = player_taken_action_history[-1]
                # transform action into string
                taken_action_string = self.prev_state.action_to_string(
                    taken_action)
                # if taken action was pass then don't push to board
                if taken_action_string != 'pass':
                    taken_move = chess.Move.from_uci(taken_action_string)
                    self.board.push(taken_move)
```

Note: I am assuming that the previous requested action was the taken action which is not the case (See: (2))

There are two other places (See: (3)/(4)) where I utilized this action_to_string, but they are sort of needed because of the lack of functionality elsewhere. Similarly, in case i) I would not be resorted to using `state.action_to_string` if taken_move was provided to me in the ObservationString.

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------

cc'ing as original implementer and avid user
@michalsustr and @VitamintK

if you got here then have a cookie: :cookie:"
"Can't import pyspiel, module not found error",deepmind/open_spiel,2022-03-23 11:03:21,13,contribution welcome#Windows,808,1177959614,"I'm trying to build open spiel on windows 10. I've followed the installation instructions for windows, but I'm getting an error importing pyspiel when running example.py.

$ python example.py
Traceback (most recent call last):
  File ""C:\Users\henry.lei\Documents\Projects\open_spiel\open_spiel\python\examples\example.py"", line 26, in <module>
    from open_spiel.python import games  # pylint: disable=unused-import
  File ""C:\Users\henry.lei\Documents\Projects\open_spiel\open_spiel\python\games\__init__.py"", line 29, in <module>
    from open_spiel.python.games import dynamic_routing
  File ""C:\Users\henry.lei\Documents\Projects\open_spiel\open_spiel\python\games\dynamic_routing.py"", line 50, in <module>
    from open_spiel.python.observation import IIGObserverForPublicInfoGame
  File ""C:\Users\henry.lei\Documents\Projects\open_spiel\open_spiel\python\observation.py"", line 56, in <module>
    import pyspiel
ModuleNotFoundError: No module named 'pyspiel'


My PYTHONPATH contains the root openspiel directory, as well as out/build/x64/python:

C:\Users\henry.lei\Documents\Projects\open_spiel\open_spiel\out\build\x64\python;C:\Users\henry.lei\Documents\Projects\open_spiel\open_spiel\out\build\x64;C:\Users\henry.lei\Documents\Projects\open_spiel;
"
"Installation via pip failing on Windows 10/11 with ""legacy-install-failure""",deepmind/open_spiel,2022-03-15 19:17:06,1,contribution welcome#Windows,804,1170087987,"I'm on Windows 10, Python 3.8.2 (but the same issue is happening on another machine with Windows 11, Python 3.7.3).

Running `py -m pip install open-spiel==1.0.2` in a Windows Powershell terminal (both with and without administrator privileges) results in a `legacy-install-failure` error preceded by a `subprocess-exited-with-error` error. The full output is dumped [here](https://pastebin.com/e3w8XwmX) (note that Pastebin doesn't format this properly in the first window, so scroll down to the raw input at the bottom of the page).

One small locale footnote: `[WinError 2] Het systeem kan het opgegeven bestand niet vinden` translates to `[WinError 2] The system cannot find the file specified`.

I just upgraded from Windows 7 recently, exactly because the OpenSpiel installation required CMake and clang++. Now that I have those (as evidenced by the output), different errors arise. Is OpenSpiel's pip install not meant to work on Windows, perhaps?
"
Implement Xiangqi game,deepmind/open_spiel,2022-03-13 13:21:00,1,contribution welcome,802,1167578484,"Hello, I want to implement the Xiangqi game. I'm referring the Chess game implementation as an example for Xiangqi. For chess, there is chess folder which has files like chess_board, chess-common. I want to create the same xiangqi folder with same files (my programming skill is not so good, so i'm just mimicking Chess). Will the files be recognized by the compiler when I run `build_and_run_tests.sh`? How can I make sure that my implementation will be merged into open_spiel? Thanks!"
Infostate Tree Memory Usage,deepmind/open_spiel,2022-02-03 18:48:50,10,contribution welcome,783,1123437844,"I've been working on using the new LP tools to solve some games, including poker subgames and was wondering if anyone had any ideas for optimizing infostate trees and their memory usage. 

It seems like there are lots of calls to `.Clone` or `.Child`, specifically with subtree rebalancing and updating the leaf nodes. It doesn't seem like `std::vector<std::unique_ptr<State>> corresponding_states_` is used for anything at the moment either. 

There are a number of other things that seem like they could be cut directly for my own work, but I was wondering if there was an approach more inline with open spiel as a whole that might help.

Thanks in advance for the help!"
Bug in pickle serialization for Universal poker,deepmind/open_spiel,2022-01-28 14:49:23,7,bug,778,1117491467,"```
printing to: cfrplus_solver_19.32756_exploitability.pickle
Loading the model...
OpenSpiel exception: Unknown parameter 'gamedef=GAMEDEFlimitnumPlayers = 2numRounds = 3blind = 5 10raiseSize = 10 10 20firstPlayer = 1maxRaises = 2 2 3numSuits = 4numRanks = 5numHoleCards = 1numBoardCards '. Av
ailable parameters are: betting, bettingAbstraction, blind, boardCards, firstPlayer, gamedef, handReaches, maxRaises, numBoardCards, numHoleCards, numPlayers, numRanks, numRounds, numSuits, potSize, raiseSize,
stack
```

Saving a pickle file of a universal poker solver and then loading it gives an error. I've played around with removing or editing some of the parameters here from the string used to load the game (e.g. removing `GAMEDEF` and `limit`, adding a space before `GAMEDEF`) but it still gives an exception.

a hopefully reproducible example
```
""""""Example use of the CFR algorithm on Kuhn Poker.""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import pickle
from absl import app
from absl import flags

import pyspiel

FLAGS = flags.FLAGS

CUSTOM_HOLDEM = """"""\
GAMEDEF
limit
numPlayers = 2
numRounds = 1
blind = 5 10
raiseSize = 10 10 20
firstPlayer = 1
maxRaises = 2 2 3
numSuits = 4
numRanks = 5
numHoleCards = 1
numBoardCards = 0 2 1
END GAMEDEF
""""""

flags.DEFINE_enum(""solver"", ""cfrplus"", [""cfr"", ""cfrplus"", ""cfrbr""],
                  ""CFR solver"")
flags.DEFINE_integer(""iterations"", 2000, ""Number of iterations"")
flags.DEFINE_integer(""print_freq"", 100, ""Print frequency of exploitability"")
flags.DEFINE_string(""game"", ""universal_poker"", ""Name of the game"")


def main(_):
    game = pyspiel.load_game(
        FLAGS.game,
        {""gamedef"": CUSTOM_HOLDEM},
    )

    if FLAGS.solver == ""cfr"":
        solver = pyspiel.CFRSolver(game)
    elif FLAGS.solver == ""cfrplus"":
        solver = pyspiel.CFRPlusSolver(game)
    elif FLAGS.solver == ""cfrbr"":
        solver = pyspiel.CFRBRSolver(game)

    solver.evaluate_and_update_policy()

    final_exploitability = 0.333

    print(""Persisting the model..."")
    print(""printing to: {}_solver_{:.5f}_exploitability.pickle"".format(
        FLAGS.solver, final_exploitability))
    with open(
            ""{}_solver_{:.5f}_exploitability.pickle"".format(
                FLAGS.solver, final_exploitability), ""wb"") as file:
        pickle.dump(solver, file, pickle.HIGHEST_PROTOCOL)

    print(""Loading the model..."")
    with open(
            ""{}_solver_{:.5f}_exploitability.pickle"".format(
                FLAGS.solver, final_exploitability), ""rb"") as file:
        loaded_solver = pickle.load(file)


if __name__ == ""__main__"":
    app.run(main)
```"
Loading LibTorch AlphaZero checkpoints in Python,deepmind/open_spiel,2022-01-28 12:56:58,3,contribution welcome,777,1117373992,The AlphaZero documentation states that the checkpoint are compatible between the C++ and Python versions. Does this still hold true for LibTorch AlphaZero? I could not find a way to use those checkpoints in python.
AlphaZero for Backgammon,deepmind/open_spiel,2022-01-26 12:35:45,39,,774,1114971533,"Hi, I am trying to create a good Ai for the backgammon game, as there are no examples for it. I tried pure PPO, but it did not give very good results. I was thinking to apply the AlphaZero algorithm, but realized it is only for deterministic games. Is there are inherent reason for this? I see that it currently does not handle ChanceNode's correctly but that could be changed rather easily right? 


I would appreciate any feedback in this area!"
Are proprietary games welcomed?,deepmind/open_spiel,2022-01-22 17:43:14,1,question,770,1111637176,"I was interested in implementing and submitting a PR for these two games:

* No Thanks! — VERY simple mechanics.
* Mamono Sweeper — a basic extension of Minesweeper, where instead of each ""bomb"" being a simple binary yes/no, each ""bomb"" has a value from 1 to 9, and you ""level up"" over time and are able to clear increasingly more powerful ""bombs"". Even though this seems like a radical departure from Minesweeper, it still plays mostly the same.

However, both are proprietary (e.g. No Thanks! is published in English by Z-Man Games). Are these types of games welcomed to be submitted via a PR?"
MuZero implementation using OpenSpiel,deepmind/open_spiel,2021-05-14 17:55:24,20,,595,892124495,"First of all, I want to thank the developers for this awesome project! It's simple, clean yet powerful. I really enjoyed playing with it.

I'm currently studying at the University of Alberta under the supervision of Prof. Martin Mueller. My primary research focus is learning/planning with a model, and general game playing. MuZero was a big step in this direction and I would like to implement an open-source version of it as a foundation of my project. I'm aware of other open-source implementations but I would like to have a more efficient and robust implementation. (This is partially why I opened #592, since utilizing cloud computation power well is a must-have.) There's also #135 but unfortunately no follow-up.

My plan is to implement MuZero in a separate repo first using both the C++ and Python interfaces of OpenSpiel. I'll use C++ for search (MuZero flavored MCTS) and Python for everything else. I'll use JAX for neural-net-related things (I heard that's what you are using in DeepMind for MuZero). If the project works, we can integrate the project into OpenSpiel at some point in the future. 

Here are some questions of mine:
1. What are the caveats of writing a MuZero flavored MCTS in C++ similar to OpenSpiel's own MCTS?
2. I presume since the interfaces of the games are quite unified, the algorithm should work on all the games without too much tweaking (work in terms of running without errors, but not necessarily performing well on the task). Is this correct?
3. What do you do to display/visualize information/metrics? Right now I'm just reading console logs.

It would be great if the developers can help me with these questions and any other tips regarding the project would be greatly appreciated! 😃 "
running model on gdb doesn't work,bowenliu16/rl_graph_generation,2021-11-22 12:25:22,0,,13,1060095652,"although the code natively supports the gdb13 dataset, choosing it as the dataset option in the code raises an exception after a few iterations.
it raises an error on molecule env in the get_observation method line 548
``        F[0,n:n+n_shift,:] = auxiliary_atom_features``
which it tried to broadcast shape (5,5) to shape (4,5), this occurs because the last list of the array is from 16 to 21 but the array is only size 20.
can you please share a fix so it could also run on the dataset you already support?"
load_conditional(),bowenliu16/rl_graph_generation,2021-02-02 04:11:09,0,,12,798909235,"Hi, 
Just wondering what the function load_conditional() does in gym_molecule/envs/molecule.py.

Sorry I'm very new in this field.

Thanks.

Cheers,
Lesley"
About oldpi in function traj_segment_generator,bowenliu16/rl_graph_generation,2020-12-23 06:06:56,0,,10,773512502,"Hello, thank you very much for sharing the code. But, I have a question in line 525 of ppo1.pposgd_simple_gcn.py, passing the parameter pi to the function traj_segment_generator. However, I think oldpi should be passed to the function instead of pi. Is it true? Please let me know. Thank you, look forward to your reply.

Best wishes!
Anny"
Running this model on new dataset,bowenliu16/rl_graph_generation,2020-06-30 18:37:44,0,,8,648407720,"Hi,

I am trying to run this model on new dataset. I know one of the data file I have to give as input is SMILES as by default is ZINC 250K dataset.
When I checked the code it also requires, other files too, such as opt.test.logP-SA/zinc_plogp_sorted.csv. These files contains 800 molecules with I guess logP values but I am not sure how to generate such files from any other dataset. Did you providing any helper function or could you please suggest me how to do that and also how to get the penalized logP values?

Also what other data files are required to run the model?

In the code there are some hard coded values such as for the normalization, do I need to change according to new dataset and also is there any components needs to be updated in the code?

Thanks."
env problem,bowenliu16/rl_graph_generation,2019-08-23 09:11:00,0,,7,484419567," read your paper, i am interested in the thoughts,but when i deploy the project ,i met too many problems , the first command i got the problem ,so can you supply the detail of env,such as the version of python, mujoco and some else .

i got too many problems, and debug several days got fail"
Takes too long to complete,bowenliu16/rl_graph_generation,2019-05-08 04:11:34,0,,5,441542610,How long will it take for this command `mpirun -np 8 python run_molecule.py 2>/dev/null` to terminate? It is running on my machine for more than 2 days. I am running the program on GeForce GTX 1080 Ti along with 4 CPU cores.
many bugs,bowenliu16/rl_graph_generation,2019-01-09 06:32:18,6,,2,397230128,"Hello,
thanks you for sharing your code. I am very interested in your model but there is no way to make it working.
I have fixed a lot of bugs (I cannot summarize all since there are a lot) but there are many other that would require to re-write some parts.
Do you have an up-to-date version that works?
Thanks again."
Cannot find the module of Multi-way registration in the Source codes ?,isl-org/FastGlobalRegistration,2022-07-11 08:58:31,0,,31,1300412050,
Unit of RMSE for the synthetic range data sets,isl-org/FastGlobalRegistration,2022-04-12 00:37:55,0,,30,1200722199,"Hello, I have a question about the unit of the RMS error calculated with the 25 synthetic range data sets. The paper says the RMS unit is the diameter of the surface. Could you please explain this? Thanks!"
speed of the code,isl-org/FastGlobalRegistration,2020-10-19 12:14:19,0,,29,724554839,"The code runs very slowly when searching the nearest point,how can I speed it up?"
How can i compile using g++??,isl-org/FastGlobalRegistration,2020-10-08 23:14:33,0,,28,717731794,"/tmp/ccvT6nCV.o: In function `main':
main.cpp:(.text+0xda): undefined reference to `fgr::CApp::ReadFeature(char const*)'
main.cpp:(.text+0xfa): undefined reference to `fgr::CApp::ReadFeature(char const*)'
main.cpp:(.text+0x109): undefined reference to `fgr::CApp::NormalizePoints()'
main.cpp:(.text+0x118): undefined reference to `fgr::CApp::AdvancedMatching()'
main.cpp:(.text+0x12c): undefined reference to `fgr::CApp::OptimizePairwise(bool)'
main.cpp:(.text+0x14c): undefined reference to `fgr::CApp::WriteTrans(char const*)'
collect2: error: ld returned 1 exit status

if i input in terminal using ""g++ main.cpp"", i got this error

Is there any way I can use it with g++ without using cmake?"
Modify optimization for 4 DoF transformations?,isl-org/FastGlobalRegistration,2020-08-21 23:00:41,0,,27,683867860,"In my problem, the two point clouds are transformed by only x,y,z position and yaw angle. Would it make sense to modify FGR to enforce this constraint? Should better accuracy/speed be expected? 

It was certainly a benefit for RANSAC, and I'm curious if I can get even better results with FGR."
Parameters tuning in Open3D,isl-org/FastGlobalRegistration,2020-06-05 04:39:09,0,,26,631305652,"For convenience, I used the FastGlobalRegistrtion implementation in the Open3D library and used the sample code from this page:
http://www.open3d.org/docs/release/tutorial/Advanced/global_registration.html#id2

But I found that the parameters in Open3D is different with this repo. As I know, there is only a `voxel_size` para in Open3D's implemention. When I tune `voxel_size` , I can only get some not very good results like this:
![image](https://user-images.githubusercontent.com/34240632/83841999-951c0180-a734-11ea-81ce-a8ed47b5af4f.png)


So I wonder if there is a instruction of how to tune paras of Open3D's implemention."
Does FGR work for point clouds with large rotation?,isl-org/FastGlobalRegistration,2019-10-02 21:17:28,0,,25,501731587,"Since FGR is global registration algorithm, initial pose is not necessary. Does the algorithm work when the target point cloud is rotated by 90 degrees or more? I am working on RGBD data (partial point cloud vs clean and complete 3D model), but didn't get good results using FGR."
Speeding up FPFH generation,isl-org/FastGlobalRegistration,2019-08-14 17:08:48,1,,24,480791167,"Hi,
I have successfully used FGR to align a pointcloud (~27000 points) with a transformed (rotated and translated) version of itself and it worked.

I noticed that the FPFH calculations take extremely long relative to the actual FGR alignment itself.

 I am trying to use FGR for a real-time application (aligning consecutive pointclouds from a bag file), and clearly the speed of the feature detection will not do.

Any ideas on how I could speed this up? Is there any way I could use CUDA/GPU to accelerate this?

Would it be possible to ""save"" features found with FPFH for faster FPFH computation of a subsequent pointcloud (in a real-time application)?"
The question concentrate on equation 8.,isl-org/FastGlobalRegistration,2019-08-06 18:21:44,0,,23,477526807,"Hello author,
    Thanks your friendly idea about this work. Here I have a question about eq.8. In my idea, JTJ△\lamada=-JTr. However, through your code and paper, JTJ\lamada=-JTr, where confused me for a long time. "
Question about Fig2 in the paper page5,isl-org/FastGlobalRegistration,2019-07-15 07:47:26,2,,22,467984687,"Hi, this is an amazing work, and thank you very much.
But, I have a question about the fig2 in the page 5 of the paper. Fig2 is the picture of $$ ρ(x)=\frac{μ x^2}{μ+x^2} $$
![FGR_loss_paper](https://user-images.githubusercontent.com/35629220/61201517-5bef3300-a717-11e9-91a6-0adc833139b5.png)
But when I draw the picture, the result is different from it. Why?
![FGR_loss_my](https://user-images.githubusercontent.com/35629220/61201516-5b569c80-a717-11e9-8ca0-d6c0c416cfe3.png)

This is the code I am drawing the image:

```python
# -*- coding: utf-8 -*-
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt  

mpl.rcParams['font.family'] = 'SimHei'
plt.rcParams['axes.unicode_minus'] = False

x = np.linspace(-10, 10, 100)
M_list = [0.25, 1, 4, 16]
color_list = ['k', 'b', 'g', 'r']

for idx, _ in enumerate(M_list):
    M = M_list[idx]
    color = color_list[idx]
    y = (M * np.square(x)) / (M + np.square(x)) # formulation
    plt.plot(x, y, color, label='μ='+str(M))
    plt.annotate(s='μ='+str(M), xy=(x[0], y[0]), xytext=(5,5),
                 xycoords='data',textcoords='offset points', fontsize=10, color=color)

plt.title('Geman-McClure estimator')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()

plt.axis(""equal"")
plt.show()
```"
Error building Matlab_binding/fast_global_registration.,isl-org/FastGlobalRegistration,2019-04-03 08:54:48,3,,21,428643570,"I want to get the .mexw64 files and use them in MATLAB but I failed to build one of them. I used VS 2017 and the errors it showed were something about undeclared identifiers in ""fast_global_registration.cpp"". I checked the file and I can find the definition of these identifiers in app.h. 
![image](https://user-images.githubusercontent.com/39957989/55466138-2c695e80-5631-11e9-8e80-85079f18d4e4.png)
"
seg fault when linking PCL?,isl-org/FastGlobalRegistration,2019-02-09 01:04:39,3,,20,408379002,"Hi, this is an amazing work, and I'm working on a related paper and try to use your work.

However, I met some problem. Is there anything in the program conflict with PCL? When I do:
`TARGET_LINK_LIBRARIES(FastGlobalRegistration FastGlobalRegistrationLib ${PCL_LIBRARIES})`

Although it compiles successfully, when I run the main program, it run into seg fault at the end during desctruction of class ""fgr::CApp"".

However, after remove ${PCL_LIBRARIES}, everything works well. Do you know why? This is very important to me, because I'm using PCL heavily in other parts. 
"
Can not understand Jacobian Computation Process in function OptimizePairwise,isl-org/FastGlobalRegistration,2019-01-21 03:17:23,2,,19,401178198,"hello,
i read the paper Fast Global Registration, which uses Gauss-Newton method to optimize T by equation 8 at the Section3.2
![image](https://user-images.githubusercontent.com/21361256/51450300-29b5d780-1d6b-11e9-8e55-423b5cfe6ffa.png)
 I find your calculation of the Jacobian as follows：
`			





int ii = corres_[c].first;
			int jj = corres_[c].second;
			Eigen::Vector3f p, q;
			p = pointcloud_[i][ii];
			q = pcj_copy[jj];
			Eigen::Vector3f rpq = p - q;

			int c2 = c;

			float temp = par / (rpq.dot(rpq) + par);
			s[c2] = temp * temp;
                        J.setZero();
			J(1) = -q(2);
			J(2) = q(1);
			J(3) = -1;
			r = rpq(0);
			JTJ += J * J.transpose() * s[c2];
			JTr += J * r * s[c2];
			r2 += r * r * s[c2];

			J.setZero();
			J(2) = -q(0);
			J(0) = q(2);
			J(4) = -1;
			r = rpq(1);
			JTJ += J * J.transpose() * s[c2];
			JTr += J * r * s[c2];
			r2 += r * r * s[c2];

			J.setZero();
			J(0) = -q(1);
			J(1) = q(0);
			J(5) = -1;
			r = rpq(2);
			JTJ += J * J.transpose() * s[c2];
			JTr += J * r * s[c2];
			r2 += r * r * s[c2];

			r2 += (par * (1.0 - sqrt(s[c2])) * (1.0 - sqrt(s[c2])));`
To have a better understanding, I tried to substitute lp,q (equation 6) into objective 3 E(T;L).
![image](https://user-images.githubusercontent.com/21361256/51450322-4ce08700-1d6b-11e9-9920-fdaf28c98137.png)
![image](https://user-images.githubusercontent.com/21361256/51450329-51a53b00-1d6b-11e9-8fae-a7391e905893.png)
Then I  got:
![image](https://user-images.githubusercontent.com/21361256/51450569-20793a80-1d6c-11e9-8fae-25b470c8a032.png)
Using chain rule:
![image](https://user-images.githubusercontent.com/21361256/51450588-371f9180-1d6c-11e9-9d7b-c06abaf48b1f.png)
So：
![image](https://user-images.githubusercontent.com/21361256/51450615-5a4a4100-1d6c-11e9-9954-8f744fc98090.png)
And I realize that this equation represents the following line in the code:
`s[c2] = temp * temp;`
As far as I am concerned, J(2) should be calculated as follows:
![image](https://user-images.githubusercontent.com/21361256/51450663-78b03c80-1d6c-11e9-9f22-51c2e5792a2f.png)
![image](https://user-images.githubusercontent.com/21361256/51450673-8a91df80-1d6c-11e9-841c-090a0947413a.png)
![image](https://user-images.githubusercontent.com/21361256/51450690-9f6e7300-1d6c-11e9-9a99-9ffe400c810b.png)
![image](https://user-images.githubusercontent.com/21361256/51450786-0ab84500-1d6d-11e9-9942-ba1114b4ab69.png)
•	I see that your calculation of Jacobian is divided into three parts which are quite different from I expected before. Could you explain more about your codes of Jacobian calculation?
Thanks,
yechuankun
"
if without ground truth，如何进行误差评估呢？,isl-org/FastGlobalRegistration,2018-09-30 02:52:54,1,,15,365185970,"周老师你好！测试了您的这个库，结果很惊艳，可是当我用来进行人脸建模的匹配上时，多帧融合之后，还是会引入不小的误差，所以想请教一下，有没有类似重投影误差这样一种评估的方法，来筛选FGR得到的位姿结果呢？Evaluation，需要输入gt.lg, 如果没有怎么办？"
Wrong aligment on full room scans,isl-org/FastGlobalRegistration,2018-04-01 16:19:57,2,,12,310335434,"Hello, 
I would like to publish a comparison between FGR and my 3D alignment method.
I conducted an experiment of 12 tests, each test contains scene + template, each 3D Model can be represented as polygon mesh or point cloud.
10 of the scenes are from [sceneNN](http://people.sutd.edu.sg/~saikit/projects/sceneNN/) and 2 from [scanNet](http://www.scan-net.org/), the templates are taken from [shapeNet](https://www.shapenet.org/).
The alignment presented is between the scene (source) and template (target) point clouds.
I used the voxel radius of 0.04 and the 1:2:5 radios are suggested in the troubleshoot.
The parameters of FGR remind the defaults. 

In the link below I have visualized the transformation T generate by FGR for all 12 tests. It seems that in all tests the result is not successful. Can you suggest general parametrization that will yield a much better alignment for the tests? 

In the folder you can find the python script I used to create the FPFH and run FGR.
![scene_8](https://user-images.githubusercontent.com/37779442/38175201-7a4a4914-35e1-11e8-9c39-6feeb52d9267.png)
![scene_9](https://user-images.githubusercontent.com/37779442/38175202-7a683c26-35e1-11e8-8e9d-a6f0cb08c585.png)
![scene_10](https://user-images.githubusercontent.com/37779442/38175203-7a8756e2-35e1-11e8-89da-397c5dea397c.png)
![scene_11](https://user-images.githubusercontent.com/37779442/38175204-7aa6eb88-35e1-11e8-8471-2dd47b5977a5.png)
![scene_12](https://user-images.githubusercontent.com/37779442/38175205-7ac55190-35e1-11e8-9f5e-ca0090232ca5.png)
![scene_1](https://user-images.githubusercontent.com/37779442/38175206-7ae2321a-35e1-11e8-9d51-dc6de5efe409.png)
![scene_2](https://user-images.githubusercontent.com/37779442/38175208-7b097d02-35e1-11e8-9c0c-aca491ca3596.png)
![scene_3](https://user-images.githubusercontent.com/37779442/38175209-7b302c18-35e1-11e8-94d7-34206466da71.png)
![scene_4](https://user-images.githubusercontent.com/37779442/38175210-7b5ef868-35e1-11e8-93d6-3e54c815a8c5.png)
![scene_5](https://user-images.githubusercontent.com/37779442/38175211-7b93770a-35e1-11e8-9144-7dc2d5f5303d.png)
![scene_6](https://user-images.githubusercontent.com/37779442/38175212-7bdcdf26-35e1-11e8-99f2-18ed717da769.png)
![scene_7](https://user-images.githubusercontent.com/37779442/38175213-7c11ab02-35e1-11e8-972d-10f0c69d7ac7.png)

All data is here: https://www.dropbox.com/sh/dxglr4ga1f2ly0k/AACIE4eNcCYBMhYgy3qjbO_7a?dl=0


Thanks,
Tamir
"
Does the Input must be one-to-one?,isl-org/FastGlobalRegistration,2018-02-23 10:06:23,3,,10,299658801,"If the cloud's number are difference, Should I do the matching work?
"
Using Live PCL clouds?,isl-org/FastGlobalRegistration,2017-10-13 11:37:51,9,,7,265259333,"Hi, and thank you for making this code available. I am trying to implement it in my GICP application to test speed/accuracy, but I am having trouble.

With live cloud input, what is the workflow?

Read clouds
extract features.

This gives me my original cloud `<pcl::PointXYZI>` and my features `PointCloud<FPFHSignature33>`.
i do this for both clouds. What do i need to do to pass this data into your lib?

thank you!"
Descriptor for noisy data,isl-org/FastGlobalRegistration,2017-05-17 17:29:24,3,,6,229431994,"Hello all,

First of all thank you very much for open sourcing your work!
I am having trouble getting an alignment between 2 pointclouds. I am trying to globally localize in a larger map (base map) based on a smaller excerpt (query map).
The data however are having rather high noise between the base map and the query map. I have been using FPFH features as suggested in the readme, but either receive no or a wrong final transformation. I believe the low performance I am experiencing is due to the descriptor choice / parametrization.

My question is: Do you have an intuition on how to parametrize the descriptors / which descriptors to use on very noisy data, e.g., I believe the normals estimation is not very reliable on my data? Furthermore, does it make sense to select keypoints or should the matching always be done densely?

Thank you very much for your help!"
Issues getting decent transformations,isl-org/FastGlobalRegistration,2017-01-12 16:17:32,2,,5,200410963,"Hi,

I am trying to use this algorithm to replace my PCL ICP based system but have not been able to get any decent transformations. I am trying to align a laser scanned model of an object with the scene point cloud of the object in a basic environment. I followed the instructions to create input for the algorithm and got outputs as follows:

ReadFeature ... done.
ReadFeature ... done.
normalize points :: mean[0] = [-0.202804 -0.075849 0.622425]
normalize points :: mean[1] = [0.015940 -0.047936 0.001295]
normalize points :: global scale : 1.000000
Advanced matching : [0 - 1]
points are remained : 2451
	[cross check] points are remained : 11
	[tuple constraint] 0 tuples (1100 trial, 1100 actual).
	[final] matches 0.
Pairwise rigid pose optimization

with transformations of 

0 1 2
1.0000000000 0.0000000000 0.0000000000 -0.2187434137
0.0000000000 1.0000000000 0.0000000000 -0.0279131606
0.0000000000 0.0000000000 1.0000000000 0.6211291552
0.0000000000 0.0000000000 0.0000000000 1.0000000000

This is clearly not correct as there is a rotation to align the clouds.

I tried playing with some of the parameters but have not gotten better results.

Do you have any tips for getting better results? I attached two sample files, one of the model and one of the scene, to see if anyone is able to get a good transformation that aligns the model ([sample_files.zip](https://github.com/IntelVCL/FastGlobalRegistration/files/702218/sample_files.zip))

Thanks!

"
Cannot find the  module of Multi-way registration in the Source codes ?,isl-org/FastGlobalRegistration,2016-12-07 05:45:33,1,,3,193963289,"Currently, I have tested the Pairwise registration of your codes. It's so amaze that the registration results are very accurate and the programs are very speedy. But when I want to input a set of point clouds, I cannot find the module of Multi-way registration just as the proposed algorithms in your paper. So can you give me some suggestions?"
License of COCO Attributes,genp/cocottributes,2021-12-23 09:25:05,0,,18,1087531067,Hi @genp I am wondering what license is COCO Attributes labels under. How about commercial use? Thanks!
Pre-trained PyTorch Models,genp/cocottributes,2021-01-26 18:00:00,0,,9,794442790,"Hi, In the ReadMe for the repo, it says you have improved models based on PyTorch: ""New pytorch models! Significant improvement over 2016 baseline network. See updates in pytorch dir."" 

I have been looking into the PyTorch dir, but could not find the directions to get PyTorch models or a checkpoint from where to start. Do you mean, one can start from an ImageNet trained model with coco-attributes dataset and that you are essentially not providing pre-trained PyTorch models but instead code to train PyTorch models?

Looking forward to your answer.
Thanks, 
Touqeer
 "
Label noise very high,genp/cocottributes,2020-12-18 03:21:29,2,,8,770534192,"Looking through the annotated samples, I noticed that for the gender and age attributes of the person class, the label noise is extremely high. In the range 20 to 30 percent label errors, see photos attached. The largest sources of errors are:

- There's a lot of photos of Person class where the gender is simply not assigned even when it's clearly visible (~30%).
- There's a lot of photos of females that are wrongly labeled as males (~20%).
- For the age categories (Young, Adult, Old), almost all photos are simply labeled Adult, even when the object in question is clearly either Young or Old. There's substantially more old people without the Old label than old people with the Old label...

I haven't checked the other object categories but i suspect the label noise to be of similar magnitude. It would be nice if it was declared in the Repo that the labels in the data-set suffer from high error rate. 

![182952](https://user-images.githubusercontent.com/73218975/102570718-e6b13a00-4122-11eb-95eb-990449180ee9.png)
![183056](https://user-images.githubusercontent.com/73218975/102570723-e9ac2a80-4122-11eb-9186-f2a93cea2a09.png)
![184930](https://user-images.githubusercontent.com/73218975/102570750-fd579100-4122-11eb-95f9-7fda10c6aefc.png)

"
"Python3  ""cocottributes_eccv_version_py3.jbl"" file not available.",genp/cocottributes,2019-03-07 10:06:00,0,,5,418225041,"Hi there, 

Would you perhaps make the python3 joblib Coco attributes file available, please? The file found [here](http://cs.brown.edu/~gmpatter/cocottributes.html) seems to be the python2 joblib file. 

Regards
Shane"
How do I run the classifier on my own images?,genp/cocottributes,2018-09-01 02:16:19,0,,3,356163238,"Hi, I was wondering if you had any documentation on how to run the classifier provided here: http://cs.brown.edu/~gmpatter/cocottributes/coco_attributes_2016.tar.gz. This link can be found on this page: http://cs.brown.edu/~gmpatter/cocottributes.html. Thank you!"
Some problems about function.,gdlg/panoramic-depth-estimation,2020-06-12 04:27:29,1,,7,637474588,"I think 
~~~python
    def generate_image_left(self, img, disp):
        if self.params.equirectangular_mode:
            return bilinear_sampler_equirectangular(img, -disp, self.params.fov)
        else:
            return bilinear_sampler_1d_h(img, -disp, self.params.fov)
    def generate_image_right(self, img, disp):
        if self.params.equirectangular_mode:
            return bilinear_sampler_equirectangular(img, disp)
        else:
            return bilinear_sampler_1d_h(img, disp)
~~~

should be 

~~~python
    def generate_image_left(self, img, disp):
        if self.params.equirectangular_mode:
            return bilinear_sampler_equirectangular(img, -disp, self.params.fov)
        else:
            return bilinear_sampler_1d_h(img, -disp)
    def generate_image_right(self, img, disp):
        if self.params.equirectangular_mode:
            return bilinear_sampler_equirectangular(img, disp, self.params.fov)
        else:
            return bilinear_sampler_1d_h(img, disp)
~~~

Thank you."
About training dataset & dataset transformation,gdlg/panoramic-depth-estimation,2020-06-10 07:05:23,3,,6,635988237,"Hi. 
I have questions about datasets used for training.
In your paper, the best performance was achieved by using  CARLA &  Mapillary datasets.
However, CARLA datasets which you uploaded, do not have stereo pair images.
How the training proceeds using those datasets??

Also, I wonder how the rectilinear images were transformed to equi rectangular images.
Any codes or reference links will be helpful.
Thank you."
Bad depth prediction results,gdlg/panoramic-depth-estimation,2019-09-23 10:25:21,2,,5,497013785,"Hi,

Thank you for this amazing work. I wanted to try your inference code on the proposed validation Carla dataset using one of your trained models. However when doing that, I get bad results that have nothing to do with the scores reported in your paper. For instance, when testing using the model in mixed_warp folder, I get the following results:

Abs. rel. & Sq. rel. & RMSE & RMSE log. & Depth acc < 1.25
0.810 & 67.533 & 8.520 & 1.427 & 0.051

The same thing goes for carla_warp and carla models
Is there something I am missing ? Note that I used the same evaluation code available with your dataset.
I will be grateful if you give some insights about what might cause the problem.

Best regards "
Rules for Definition of Depth,gdlg/panoramic-depth-estimation,2019-03-29 03:18:15,9,,4,426797740,"I got the depth data from the. npy file, but I found that the nearer the point in the panoramic image, the greater the depth value. I want to know how you define the depth. thank you."
Some questions about the dataset celebA,zhixinshu/DeformingAutoencoders-pytorch,2019-07-22 12:04:44,0,,9,471057238,"Hello,

I have some questions about the dataset you used.

I'm sorry that I am not familiar with celebA dataset.

Is the train/eval/test split you used same as the official one? http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html

Did you split your training set into another subsets? How and Why?

How are the cropped and resized version you provided (https://drive.google.com/open?id=1ueB8BJxid2rZbvh3RaoZ9lDdlKH4B-pL) different from the official cropped images?

Thank you,

Best,
Ahyun"
DenseBlockEncoder and DenseBlockDecoder seem incorrect,zhixinshu/DeformingAutoencoders-pytorch,2019-06-13 00:08:51,1,,8,455477763,"In https://github.com/zhixinshu/DeformingAutoencoders-pytorch/blob/master/DAENet.py#L260, the forward function of `DenseBlockEncoder` class is given as:

 ```
   def forward(self, inputs):
        outputs = []

        for i, layer in enumerate(self.layers):
            if i > 0:
                next_output = 0
                for no in outputs:
                    next_output = next_output + no 
                outputs.append(next_output)
            else:
                outputs.append(layer(inputs))
        return outputs[-1]
```
It seems the `layer` is only applied once. Thus, `outputs[-1]` will output `some scalar * self.layers[0](inputs)` and it does not look like a valid dense block. Same problem also occurs in `DenseBlockDecoder` class."
Previous checkpoint not compatible with latest code,zhixinshu/DeformingAutoencoders-pytorch,2019-04-26 21:52:56,0,,7,437859118,"Hi @zhixinshu 

Thanks for sharing the code. I did a clean clone and downloaded the pretrained model. However, the code is rejecting the model. It seems in the last a few commits there were some updates in the network. I am wondering if you have a new pretrained model that is compatible with the latest code. Or did I miss something here?

Thank you.

Missing key(s) in state_dict:
""encoder.main.1.layers.0.2.weight"",
""encoder.main.1.layers.1.2.weight"",
""encoder.main.1.layers.2.2.weight"",
""encoder.main.1.layers.3.2.weight"",
""encoder.main.1.layers.4.2.weight"",
""encoder.main.1.layers.5.2.weight"",
""encoder.main.2.main.2.weight"",
""encoder.main.3.layers.6.2.weight"",
""encoder.main.3.layers.7.2.weight"",
""encoder.main.3.layers.8.2.weight"",
""encoder.main.3.layers.9.2.weight"",
""encoder.main.3.layers.10.2.weight"",
""encoder.main.3.layers.11.2.weight"",
""encoder.main.5.layers.12.2.weight"",
""encoder.main.5.layers.13.2.weight"",
""encoder.main.5.layers.14.2.weight"",
""encoder.main.5.layers.15.2.weight"",
""encoder.main.5.layers.16.2.weight"",
""encoder.main.5.layers.17.2.weight"",
""encoder.main.5.layers.18.2.weight"",
""encoder.main.5.layers.19.2.weight"",
""encoder.main.5.layers.20.2.weight"",
""encoder.main.5.layers.21.2.weight"",
""encoder.main.5.layers.22.2.weight"",
""encoder.main.5.layers.23.2.weight"". 
Unexpected key(s) in state_dict: 
""encoder.main.10.main.0.weight"",
""encoder.main.10.main.0.bias"",
""encoder.main.10.main.0.running_mean"",
""encoder.main.10.main.0.running_var"",
""encoder.main.10.main.2.weight"",
""encoder.main.0.running_mean"",
""encoder.main.0.running_var"",
""encoder.main.2.weight"",
""encoder.main.2.bias"",
""encoder.main.3.layers.0.0.weight"",
""encoder.main.3.layers.0.0.bias"",
""encoder.main.3.layers.1.0.weight"",
""encoder.main.3.layers.1.0.bias"",
""encoder.main.3.layers.2.0.weight"",
""encoder.main.3.layers.2.0.bias"",
""encoder.main.3.layers.3.0.weight"",
""encoder.main.3.layers.3.0.bias"",
""encoder.main.3.layers.4.0.weight"",
""encoder.main.3.layers.4.0.bias"",
""encoder.main.3.layers.5.0.weight"",
""encoder.main.3.layers.5.0.bias"",
""encoder.main.4.main.0.weight"",
""encoder.main.4.main.0.bias"",
""encoder.main.5.layers.0.0.weight"",
""encoder.main.5.layers.0.0.bias"",
""encoder.main.5.layers.1.0.weight"",
""encoder.main.5.layers.1.0.bias"",
""encoder.main.5.layers.2.0.weight"",
""encoder.main.5.layers.2.0.bias"",
""encoder.main.5.layers.3.0.weight"",
""encoder.main.5.layers.3.0.bias"",
""encoder.main.5.layers.4.0.weight"",
""encoder.main.5.layers.4.0.bias"",
""encoder.main.5.layers.5.0.weight"",
""encoder.main.5.layers.5.0.bias"",
""encoder.main.5.layers.6.0.weight"",
""encoder.main.5.layers.6.0.bias"",
""encoder.main.5.layers.7.0.weight"",
""encoder.main.5.layers.7.0.bias"",
""encoder.main.5.layers.8.0.weight"",
""encoder.main.5.layers.8.0.bias"",
""encoder.main.5.layers.9.0.weight"",
""encoder.main.5.layers.9.0.bias"",
""encoder.main.5.layers.10.0.weight"",
""encoder.main.5.layers.10.0.bias"",
""encoder.main.5.layers.11.0.weight"",
""encoder.main.5.layers.11.0.bias"",
""encoder.main.6.main.0.weight"",
""encoder.main.6.main.0.bias"",
""encoder.main.7.layers.16.0.weight"",
.....

	size mismatch for encoder.main.0.weight: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([32, 3, 4, 4]).
	size mismatch for encoder.main.0.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([32]).
	size mismatch for encoder.main.3.layers.0.0.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for encoder.main.3.layers.0.0.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for encoder.main.3.layers.0.2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).
	size mismatch for encoder.main.3.layers.1.0.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for encoder.main.3.layers.1.0.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).
	size mismatch for encoder.main.3.layers.1.2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).
	size mismatch for encoder.main.3.layers.2.0.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).
....."
The models trained on 128x128 dataset,zhixinshu/DeformingAutoencoders-pytorch,2018-12-03 05:59:24,9,,6,386670424,"Hi, @zhixinshu 
I want to train the model with 128x128 input on CelebA dataset.
Thus, i add additional layers in both encoder and decoder.
The codes are as follows:
```
class waspDenseEncoder(nn.Module):
    def __init__(self, opt, ngpu=1, nc=1, ndf = 32, ndim = 128, activation=nn.LeakyReLU, args=[0.2, False], f_activation=nn.Sigmoid, f_args=[]):
        super(waspDenseEncoder, self).__init__()
        self.ngpu = ngpu
        self.ndim = ndim

        self.main = nn.Sequential(
                # input is (nc) x 128 x 128
                nn.BatchNorm2d(nc),
                nn.ReLU(True),
                nn.Conv2d(nc, ndf, 4, stride=2, padding=1),

                # state size. (ndf) x 64 x 64
                DenseBlockEncoder(ndf, 6),
                DenseTransitionBlockEncoder(ndf, ndf*2, 2, activation=activation, args=args),

                # state size. (ndf*2) x 32 x 32
                DenseBlockEncoder(ndf*2, 12),
                DenseTransitionBlockEncoder(ndf*2, ndf*4, 2, activation=activation, args=args),

                # state size. (ndf*4) x 16 x 16
                DenseBlockEncoder(ndf*4, 24),
                DenseTransitionBlockEncoder(ndf*4, ndf*8, 2, activation=activation, args=args),

                # state size. (ndf*8) x 8 x 8
                DenseBlockEncoder(ndf*8, 24),
                DenseTransitionBlockEncoder(ndf*8, ndf*16, 2, activation=activation, args=args),
                
                # state size. (ndf*16) x 4 x 4
                DenseBlockEncoder(ndf*16, 16),
                DenseTransitionBlockEncoder(ndf*16, ndim, 4, activation=activation, args=args),
                f_activation(*f_args),
        )

    def forward(self, input):
        output = self.main(input).view(-1,self.ndim)
        return output   

class waspDenseDecoder(nn.Module):
    def __init__(self, opt, ngpu=1, nz=128, nc=1, ngf=32, lb=0, ub=1, activation=nn.ReLU, args=[False], f_activation=nn.Hardtanh, f_args=[0,1]):
        super(waspDenseDecoder, self).__init__()
        self.ngpu   = ngpu
        self.main   = nn.Sequential(
            # input is Z, going into convolution
            nn.BatchNorm2d(nz),
            activation(*args),
            nn.ConvTranspose2d(nz, ngf * 16, 4, 1, 0, bias=False),

            # state size. (ngf*16) x 4 x 4
            DenseBlockDecoder(ngf*16, 16),
            DenseTransitionBlockDecoder(ngf*16, ngf*8),
            
            # state size. (ngf*8) x 8 x 8
            DenseBlockDecoder(ngf*8, 24),
            DenseTransitionBlockDecoder(ngf*8, ngf*4),

            # state size. (ngf*4) x 16 x 16
            DenseBlockDecoder(ngf*4, 24),
            DenseTransitionBlockDecoder(ngf*4, ngf*2),

            # state size. (ngf*2) x 32 x 32
            DenseBlockDecoder(ngf*2, 12),
            DenseTransitionBlockDecoder(ngf*2, ngf),

            # state size. (ngf) x 64 x 64
            DenseBlockDecoder(ngf, 6),
            DenseTransitionBlockDecoder(ngf, ngf),

            # state size (ngf) x 128 x 128
            nn.BatchNorm2d(ngf),
            activation(*args),
            nn.ConvTranspose2d(ngf, nc, 3, stride=1, padding=1, bias=False),
            f_activation(*f_args),
        )
    def forward(self, inputs):
        return self.main(inputs)
```
Although the training processing is normal, the texture is wired.
Texture：
![iter_225200_tex0_](https://user-images.githubusercontent.com/16815652/49355607-5b170d00-f703-11e8-827f-31805ef397e0.png)
Output：
![iter_225200_output0_](https://user-images.githubusercontent.com/16815652/49355632-6c601980-f703-11e8-8a06-e980aa4a2f4a.png)

Could you give me some advice for training 128x128 size image?
Many thanks!"
Does not implement the affine parameter in the BiasReduce loss,zhixinshu/DeformingAutoencoders-pytorch,2018-10-30 06:53:48,1,,5,375345331,"Hi, I found that the code does not implement the loss term ||S_A - S_0||^2 in the BiasReduce loss (i.e., eq.(6)).  Is this a mistake?"
About the adversarial loss,zhixinshu/DeformingAutoencoders-pytorch,2018-10-24 03:12:18,0,,3,373286595,"Hi, @zhixinshu 
It's really a marvelous work.

But i am still confused about the page 7 in your page. There is a adversarial loss in equation 7. 
Howerver, it is not imiplemented in your code.
Could you please give a complete version of your code?

Many thanks."
Results look bad on pretrained models,zhixinshu/DeformingAutoencoders-pytorch,2018-10-15 09:22:31,3,,2,370072468,"Hi, Thanks for sharing the code. I used the pretrained model as ""DAE for CelebA with Dense encoder decoder, where opt.idim = 8 (./DAE_CelebA_idim8)"" and result doesn't look that promising. Is it because of some preprocessing on face image crop? or this is the expected results? Please see the link for the resulting images: https://drive.google.com/file/d/13TO13cYOcJ6cQZdnZAAaTyZ_eOrIIDf3/view?usp=sharing.

Thanks"
Using mask rcnn data,fabienbaradel/object_level_visual_reasoning,2019-11-22 23:27:21,1,,7,527462047,"Hi Fabien, 

I want to use the mask-rcnn predictions for EPIC-Kitchen dataset. Can you elaborate how to interpret the pickle files provided as Complementary data-masks. For beginning, I would like to know a way to overlay mask and bounding boxes on the corresponding frames. Any tutorial or readme for doing so would be highly appreciated

Thanks,
Nirat"
"I want to use this project on ""Something-something""",fabienbaradel/object_level_visual_reasoning,2019-08-30 11:44:12,2,,6,487439762,"here have something questions.
1、I want to use this project on ""Something-something"", but i haven't this mask data.
2、The dataset of ""something-something"" with 174 classes in 20bn. but I find only 157 classes in your paper. so I don't know that whether ""SS"" in 20bn ."
how to train mask-r-cnn,fabienbaradel/object_level_visual_reasoning,2019-08-20 11:36:28,1,,5,482806895,"Hi Fabien,
thank you for your works!
the paper doesn't mention how to train a mask-r-cnn since VLOG dataset does has any mask annotation, i wonder if your team has annotate the mask by yourself or use other open datasets to train mask-r-cnn? 
thanks again!"
i have a question,fabienbaradel/object_level_visual_reasoning,2019-05-14 14:02:24,4,,4,443933939,"Hello,I run your project,but i can only get maskRCNN__png. I can not get other that you show your result in your article.I want to know how i need to do."
Accuracy,xcnkx/fine_grained_classification,2018-12-16 14:28:09,0,,2,391472559,What about your result without MAMC loss. Could u pls tell me.
A mistake in labml_nn/diffusion/stable_diffusion/sampler/ddpm.py,labmlai/annotated_deep_learning_paper_implementations,2022-10-31 10:44:07,0,,152,1429634406,"Hi, I find a mistake here:

https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/e58a8f973d05af7d7934bea55ad3a07326739909/labml_nn/diffusion/stable_diffusion/sampler/ddpm.py#L71

This line should be
```python
self.sqrt_1m_alpha_bar = (1. - alpha_bar) ** .5
```
instead."
"Guideline on setting ""n_z"" parameter of hyperLSTM",labmlai/annotated_deep_learning_paper_implementations,2022-09-15 17:50:07,0,,148,1374879279,"How to set ""n_z"" parameter of hyperLSTM? Are there any guidelines?"
StyleGAN2 network error,labmlai/annotated_deep_learning_paper_implementations,2022-05-03 06:02:02,0,question,123,1223722587,"When training with custom .png image, the following error occurs.
 
```
class Smooth(nn.Module):
    def __init__(self):
        super().__init__()
        kernel = [[1, 2, 1], [2, 4, 2], [1, 2, 1]]  # define the blue kernel
        kernel = torch.tensor([[kernel]], dtype=torch.float32)  # making it a pytorch tensor

        kernel /= kernel.sum()  # Normalize the kernel
        self.kernel = nn.Parameter(kernel, requires_grad=False)  # fix the kernel so it doesn't get update
        self.pad = nn.ReplicationPad2d(1)  # pad the kernel

    def forward(self, x):
        print(x.shape)
        b, c, h, w = x.shape
        x = x.view(-1, 1, h, w)  # reshape
        x = self.pad(x)  # pad
        x = F.conv2d(x, self.kernel)  # smooth
        return x.view(b, c, h, w)  # reshape back and return
```

>   File ""/home/mjkim1/workspace/fl_learning/flower/model.py"", line 314, in forward
>     b, c, h, w = x.shape
> ValueError: not enough values to unpack (expected 4, got 3)


So I try to debuging, and I could see the x.shape changed. How can I solve this problem?
# #"
Request - Object Detection Papers,labmlai/annotated_deep_learning_paper_implementations,2022-03-02 03:31:42,5,paper implementation,109,1156298171,"Currently, labmlai has no implementation for Object Detection Papers such as Yolo Family, FPN, Retinanet. 

Do you have any timeline to share them as well? "
[BUG] StyleGAN2: latent vector is ignored,labmlai/annotated_deep_learning_paper_implementations,2022-02-07 10:31:48,1,bug,107,1125799811,"The [implementation](https://github.com/labmlai/annotated_deep_learning_paper_implementations/tree/master/labml_nn/gan/stylegan) of StyleGAN2 does not learn a mapping for the latent vector `z`. The vector `z` is completely ignored, and a variety of generated images is provided by `noise`. To demonstrate the issue, I created a google [colab](https://colab.research.google.com/drive/1SvVPRVMvHCpUZKMotz15DoRKYfw4I0PY) with a pre-trained model that I trained for 55400 iterations.

Images genertd with a random `z` and a **fixed `noise`**:
![image](https://user-images.githubusercontent.com/1911342/152770077-f4d96a1c-9e7c-47ec-bb6a-721d4f4ae282.png)


Images generated with a fixed `z` and **random `noise`**
![image](https://user-images.githubusercontent.com/1911342/152770292-ae25a2e3-863e-4a18-a24f-87e19df2aedf.png)
"
Request Spatial Transformer Networks,labmlai/annotated_deep_learning_paper_implementations,2021-08-17 09:16:06,0,paper implementation,81,972474836,https://arxiv.org/abs/1506.02025
Siamese Recurrent Architectures for Learning Sentence Similarity ,labmlai/annotated_deep_learning_paper_implementations,2021-08-08 00:59:53,0,paper implementation,74,963330248,"I would like to understand the following paper

Mueller, J. and Thyagarajan, A., 2016, March. ""Siamese recurrent architectures for learning sentence similarity"". In Proceedings of the AAAI conference on artificial intelligence (Vol. 30, No. 1).

available from
https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12195/12023

It would be great if you could annotate it like the ones you have already."
YOLOv4-CSP,labmlai/annotated_deep_learning_paper_implementations,2021-04-24 12:18:20,1,paper implementation,46,866733352,"yolov4-CSP paper implementation for large scale object detection 
including small and large object "
Request - performers,labmlai/annotated_deep_learning_paper_implementations,2021-02-08 18:54:26,4,paper implementation,12,803833560,"https://www.youtube.com/watch?v=xJrKIPwVwGM&t=767s


https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html"
About video_ info_ New.csv,wzmsltw/BSN-boundary-sensitive-network,2022-08-09 08:49:31,0,,53,1332928252,"Hello! I have benefited a lot from reading your code base, but I have another question to ask you.
What is the meaning of the attribute ""rfps"" in the fifth column of video_info_new.csv?
I hope you can answer in time, thank you！"
"where is  *_image_labels.bin,*_temporal_labels.bin,*_feature_data.bin",wzmsltw/BSN-boundary-sensitive-network,2022-01-09 14:39:15,0,,52,1097213272,"where is  *_image_labels.bin,*_temporal_labels.bin,*_feature_data.bin?"
how to get video_info.csv,wzmsltw/BSN-boundary-sensitive-network,2021-07-02 06:30:14,0,,51,935462737,"Excusme ,could you tell me how you get video_info.csv and anet_anno_action.csv files ?
Is the two files you created from activity-net.v1-3.min.json ? Then could you please send the script to me?
Thank you very much!!!
My email is m13332019336@163.com"
how to get THUMOS14 features?,wzmsltw/BSN-boundary-sensitive-network,2021-06-12 11:33:35,1,,50,919533564,
urllib2.URLError: <urlopen error [Errno 110] Connection timed out>,wzmsltw/BSN-boundary-sensitive-network,2021-04-06 06:23:50,0,,49,851036639,"Thanks for your great work.
I want to get the evaluation results. So I followed the readme in Evaluation and use the command python get_classification_performance.py data/activity_net.v1-3.min.json sample_classification_prediction.json. But I got the error urllib2.URLError: <urlopen error [Errno 110] Connection timed out>. Then I checked the url: http://ec2-52-11-11-89.us-west-2.compute.amazonaws.com/challenge16/api.py?action=get_blocked. Ant it is not accessible. Could you please help me with the probelm ?
Thank you !
"
Thumos annotation files,wzmsltw/BSN-boundary-sensitive-network,2021-03-03 06:34:22,0,,48,820797760,"Hi! Thanks a lot for the code! I am referring to this model as well as BMN model for my own work and I need to test my model on the Thumos dataset too. Can you please share the annotation files for the THUMOS dataset? I would really appreciate it if you do!
Thanks!"
Apply for video,wzmsltw/BSN-boundary-sensitive-network,2021-01-02 17:25:49,0,,47,777495491,"Hi,

This is nice work, I would like to apply this for video, How this applies to video?"
Feature extraction for THUMOS14 is strange,wzmsltw/BSN-boundary-sensitive-network,2020-10-20 16:25:16,3,,46,725755732,"According to the ""Implementation Details"" part in Section 4 of the original paper, you use the model pre-trained on the training set of ActivityNet-1.3 as the features extractor. And I don't make anything wrong, your BMN uses the output of the last layer as the feature. That's why you get 400-dimensions length features (200 classes, 2 streams). 

But the output of the last layer should represent the predicted class scores of the input frame of 200 ActivityNet actions. This kind of feature should be meaningless for THUMOS14 because half of the action classes in THUMOS14 don't belong to ActivityNet. How can you detect an unseen type of action using the classification scores of 200 irrelative actions?"
csv_mean_100/v_nt4Ag91306U.csv does not exist,wzmsltw/BSN-boundary-sensitive-network,2020-09-03 03:27:55,0,,45,691603390,"Hi,

I got this problem of data/activitynet_feature_cuhk/csv_mean_100/v_nt4Ag91306U.csv does not exist. It seems that some of the files are missing.Can you please tell me how to solve this problem?"
why multi 0.01 before sigmoid function?,wzmsltw/BSN-boundary-sensitive-network,2020-05-29 12:47:25,0,,44,627244032,"""x = torch.sigmoid(0.01*self.conv3(x))"" why multi 0.01 before sigmoid function?"
csv_mean_100/v_t_D9MYkEPEo.csv' does not exist,wzmsltw/BSN-boundary-sensitive-network,2020-02-21 00:15:30,1,,42,568659286,"the file download from Baidu as your guide, would you please help? thanks."
Visualize Actionness probability Sequence,wzmsltw/BSN-boundary-sensitive-network,2019-12-18 21:05:36,0,,41,539923670,"Hi, thanks for the great work.

For my project, I would like to visualize the the probability actionness score for input video. How can I visualize that sequence? Any help will be appreciated.

Thank you!"
"Baidu Yun files are broken, how to re-generate the files?",wzmsltw/BSN-boundary-sensitive-network,2019-11-19 13:24:51,4,,40,525007543,"I download the tsn_feature files for **many times** from the link you provided: https://pan.baidu.com/s/19GI3_-uZbd_XynUO6g-8YQ (i also tried the google drive download link, but   will be abruptly ended every time) but every time when i unzip the BaiduYun files, 

```
$ unzip -d ../BMN-Boundary-Matching-Network/data/activitynet_feature_cuhk/ csv_mean_100.zip
```

i got hundreds of ```bad zipfile offset``` errors in my terminal like:

```
Archive:  csv_mean_100.zip
error: End-of-centdir-64 signature not where expected (prepended bytes?)
  (attempting to process anyway)
warning [csv_mean_100.zip]:  zipfile claims to be last disk of a multi-part archive;
  attempting to process anyway, assuming all parts have been concatenated
  together in order.  Expect ""errors"" and warnings...true multi-part support
  doesn't exist yet (coming soon).
warning [csv_mean_100.zip]:  5242880000 extra bytes at beginning or within zipfile
  (attempting to process anyway)
file #1:  bad zipfile offset (local header sig):  5242880004
  (attempting to re-compensate)
...
file #3998:  bad zipfile offset (local header sig):  5293978810
file #3999:  bad zipfile offset (local header sig):  5294255897
file #4000:  bad zipfile offset (local header sig):  5294532787
file #4001:  bad zipfile offset (lseek):  5294809088
file #4002:  bad zipfile offset (lseek):  5295079424
file #4003:  bad zipfile offset (lseek):  5295357952
file #4004:  bad zipfile offset (lseek):  5295636480
file #4005:  bad zipfile offset (lseek):  5295915008
file #4006:  bad zipfile offset (lseek):  5296185344
file #4007:  bad zipfile offset (lseek):  5296463872
file #4008:  bad zipfile offset (lseek):  5296742400
file #4009:  bad zipfile offset (lseek):  5297004544
file #4010:  bad zipfile offset (lseek):  5297283072
file #4011:  bad zipfile offset (lseek):  5297553408
file #4012:  bad zipfile offset (lseek):  5297831936
file #4013:  bad zipfile offset (lseek):  5298110464

...
```

Also, when i run BMN program in pytorch(https://github.com/JJBOY/BMN-Boundary-Matching-Network), i will find some files are missing:

like:
```
FileNotFoundError: [Errno 2] File b'./data/activitynet_feature_cuhk/csv_mean_100/v_sx_npA4wRrw.csv' does not exist: b'./data/activitynet_feature_cuhk/csv_mean_100/v_sx_npA4wRrw.csv'
```
 
**Could you please tell me how to re-generate the tsn feature files? So i can make them by myself...**

same issue at: https://github.com/JJBOY/BMN-Boundary-Matching-Network/issues/11"
Question about the thumos14 features ,wzmsltw/BSN-boundary-sensitive-network,2019-11-14 08:46:44,0,,39,522707563,"Hi, Tianwei Lin
Thanks for your great sharing.
When I use the thumos14 features you shared, I find the row number of rgb and flow features of the same videos are different,why so and how can I do?"
question about poolData,wzmsltw/BSN-boundary-sensitive-network,2019-09-17 12:30:13,0,,38,494593568,"Hi, Tianwei Lin
Thanks for your BSN codes sharing.
From paper and the function [poolData](https://github.com/wzmsltw/BSN-boundary-sensitive-network/blob/master/data/activitynet_feature_cuhk/data_process.py#L62) code in this project, I noticed that there is another operation before you pass TSN features to BSN network.
That operation does interpolation from var-length TSN feature to fixed-length processed feature.

I have read your BSN paper and ActivityNet 2017 paper, but can't find much instruction about that.
Can you give me some reference for it? Any paper or forum discussion will be appreciated.

Another question, as BSN network accepted fix-length feature, how about extract fixed number of frames (and optical flow) for every single video (comparison to extract 1 snippet every 16 frames)?
I think the poolData function, after the TSN feature extraction stage, is the reason why we need to use a pre-trained TSN network. otherwise, we can train the feature extraction network (RGB and Flow) together with the BSN (and BMN ) network online, which may achieve better results."
How to train this on a custom dataset ?,wzmsltw/BSN-boundary-sensitive-network,2019-05-22 09:06:12,18,,35,447017749,I've been looking at this for the past few days and I can't seem to figure out how I can use this to train on any other dataset than Thumos or ActivityNet. How would you train this in the end-to-end fashion proposed in the article please ?
how to unpack the original features?,wzmsltw/BSN-boundary-sensitive-network,2019-03-31 02:32:31,0,,29,427350164,"@wzmsltw Lin, thanks for your repo.

 I have downloaded the original features you provided, but how to unpack and use the features, any ideas?

i use command `cat original_spatial_csv.tar.gz.0* > original_spatial_csv.tar.gz`, and then `tar -zxvf original_spatial_csv.tar.gz`, but error occurred 
```
tar: Unrecognized archive format
tar: Error exit delayed from previous errors.
```"
About test of two patch consistence ,minyoungg/selfconsistency,2021-07-09 04:08:04,0,,9,940396170,"Have you tested the results of the last two categories. How accurate is it.
In other words, how accurate is your model to predict whether two image blocks are correctly sampling the same image or different images."
Can't get EXIF-consistency result,minyoungg/selfconsistency,2021-02-04 14:15:12,0,,6,801324089,"Hello, I read your code and find that it seems like there's no EXIF-consistency function provided.  When I tried to use all the EXIF tags in the run_vote_no_threads() , then it shows that the CustomRunner object has no attribute ""tags"". 

Besides,  in the demo code if you set dense=True you actually will not run the dense method."
unable to run the demo.py file,minyoungg/selfconsistency,2020-05-28 02:35:23,0,,5,626166963,"when i try to run the demo.py file i am getting the following error in windows.

  ForkingPickler(file, protocol).dump(obj)
TypeError: can't pickle _thread.RLock objects"
Update cause ERRORs,minyoungg/selfconsistency,2019-12-05 15:26:25,1,,4,533421421,"For version later than tensorflow 2.0,the service for tensorflow.config has been stopped .That means tensorflow.config cannot be use.
Then how can I deal with the program with Python 3 and tensorflow 2.0?It's a really hard thing for undergraduate.
PLEASE HELP!!!
对于tensorflow 2.0之后的版本，tensorflow.config服务已停止，这意味着tensorflow.config无法使用。
然后我该如何使用Python 3和tensorflow 2.0处理该程序?
请帮忙！！！
"
i can't get normalized cut results ,minyoungg/selfconsistency,2018-11-23 12:49:23,0,,3,383813389,"Thanks your works for every researchers.

the detection result (Cluster w/ MeanShift) of demo.py is same as your showing,
but i get a wrong result (Segment with NCusts) when i run ncuts_demo.py.
the error message is 'Intel MKL ERROR: Parameter 6 was incorrect on entry to DLASWP'.
i tried to find answer for this question, but i failed.
could you give me some solving suggestions? 
thanks again.
@minyoungg @andrewowens @Eschew @AndrewHLiu "
I think  it is a great work !Can you share your train code and dataset?,minyoungg/selfconsistency,2018-05-25 04:43:27,18,,2,326379415,"I think  it is a great work !Can you share your train code and dataset?
Thank you very much!"
Code we can use to recreate your results?,lelechen63/3d_gan,2019-04-22 03:32:31,1,,3,435586478,"""I am still orgnizing this repo. I will write a clean one ASAP. Thanks.""

Any status on this? Code we can actually use to recreate your results?"
Could you release your code?,lelechen63/3d_gan,2018-11-09 09:30:46,0,,2,379086006,"Could you release your code? such as the 'model_vgan' 
Even though your paper said that you have released your code, it is not integrated.
Moreover, your code is  without the detail description.

Thanks a lot!"
Could you release a description of your code?,lelechen63/3d_gan,2018-10-29 15:17:27,2,,1,375067770,"Your work and result are amazing and I am so interested in this work. Could you release a description of your code in details? How to run the codes? 

Thanks a lot!"
Can't find concentration with a truncated loss mentioned in paper (Section:Moving as a unit.Eq.(6)),Liusifei/UVC,2019-12-12 09:04:51,4,,2,536843650,"Hi, thank you for providing your project. In this code, I can't find concentration regularization with a truncated loss in region tracking process mentioned in paper (Section:Moving as a unit.Eq.(6)). Could you please tell me where it is? Or if you did'nt upload the part of concentration regularization with a truncated loss, could you please upload this part?
![image](https://user-images.githubusercontent.com/50615777/70697657-e0fb6a00-1d00-11ea-9835-a5e3ef21fa78.png)
"
"Why the accuracy of my running results is close to zero for the previous categories, that is, each prediction is predicted as the last category",WuChenshen/MeRGAN,2020-05-12 17:26:52,0,,7,616830015,"Task: 10   Class: 0-8   Accuracy: 0.0
Task: 10   Class: 9   Accuracy: 0.8823437500000002
Average accuracy: 0.11187500000000002
"
how to make a pretrainning model?,WuChenshen/MeRGAN,2019-12-25 13:04:16,0,,6,542338548,"Thanks for your work firstly.

After training my own data set(not images) on your model, it need a pretraining model to test the model.
BUT I don't know how to prepare a pretraining model like""MeRGAN/pretrain/mnist/mnist.model-10197"".

I appreciate that you could direct me how to prepare such a model."
label nums,WuChenshen/MeRGAN,2019-04-23 02:46:56,0,,5,435973113,"Thank you for your sharing.In your code,you manually set category num to ten.However,when a new class sequential which is different from former ten categories comes,how do you adjust it to new task?"
question for `disjoint`,WuChenshen/MeRGAN,2019-02-05 18:52:15,0,,3,406924704,"Dear @WuChenshen 

I try to reproduce your works. 
`joint.py` call load function from `tflib/mnist_disjoint.py`  and `tflib/svhn_disjoint.py`but this repository don't include it. so `python joint.py --dataset mnist --result_path mnist_joint/` got error as below 
 
```
  File ""joint.py"", line 170, in <module>
    train_gen, dev_gen, _ = lib.mnist_disjoint.load(BATCH_SIZE, all_classes, data_dir = DATASET_DIR)
AttributeError: 'module' object has no attribute 'mnist_disjoint'
```

I found `mnst.py` file have `disjoint_mnst` subroutine and `mnist.load` also return result of l `disjoint_mnist`.   So I replace `lib.mnist.load` instead of `lib.mnist_disjoint.load`   It works but I'm not sure it's your original intention or not. Plz confirm it.  

moreover, joint.py don't have option for other dataset such as LSUN.   and `tflib/lsun.py`  don't have def of `disjoint_lsun` not same as `mnist.py `or `svhn.py`.  if you try disjoin for lsun dataset, plz comment on it. 
 "
some issue in `joint.py`,WuChenshen/MeRGAN,2019-01-30 01:40:23,0,,2,404563657,"Trying `joint.py` insteady of `mergan.py`, also facing problem.

First, the `deconv2d` module not found bug, this was handled by adding
`import tflib.ops.deconv2d` in `gan32_model.py`. 

Even then, I am facing following error
```
Traceback (most recent call last):
  File ""joint.py"", line 255, in <module>
    lib.plot.flush(path = RESULT_DIR)
  File ""/home/junho/MeRGAN/tflib/plot.py"", line 40, in flush
    plt.savefig(os.path.join(path,name.replace(' ', '_')+'.jpg'))
  File ""/home/junho/anaconda3/envs/mergan-py27/lib/python2.7/site-packages/matplotlib/pyplot.py"", line 695, in savefig
    res = fig.savefig(*args, **kwargs)
  File ""/home/junho/anaconda3/envs/mergan-py27/lib/python2.7/site-packages/matplotlib/figure.py"", line 2062, in savefig
    self.canvas.print_figure(fname, **kwargs)
  File ""/home/junho/anaconda3/envs/mergan-py27/lib/python2.7/site-packages/matplotlib/backend_bases.py"", line 2263, in print_figure
    **kwargs)
  File ""/home/junho/anaconda3/envs/mergan-py27/lib/python2.7/site-packages/matplotlib/backends/backend_agg.py"", line 588, in print_jpg
    return background.save(filename_or_obj, format='jpeg', **options)
  File ""/home/junho/anaconda3/envs/mergan-py27/lib/python2.7/site-packages/PIL/Image.py"", line 1439, in save
    save_handler(self, fp, filename)
  File ""/home/junho/anaconda3/envs/mergan-py27/lib/python2.7/site-packages/PIL/JpegImagePlugin.py"", line 471, in _save
    ImageFile._save(im, fp, [(""jpeg"", (0,0)+im.size, 0, rawmode)])
  File ""/home/junho/anaconda3/envs/mergan-py27/lib/python2.7/site-packages/PIL/ImageFile.py"", line 495, in _save
    e = Image._getencoder(im.mode, e, a, im.encoderconfig)
  File ""/home/junho/anaconda3/envs/mergan-py27/lib/python2.7/site-packages/PIL/Image.py"", line 399, in _getencoder
    return apply(encoder, (mode,) + args + extra)
TypeError: integer argument expected, got float
```

Any help?
It would be great help to reproduce your work and make the code runnable from clean slate.
Thank you!"
fixed_noise in mergan.py,WuChenshen/MeRGAN,2019-01-29 12:09:53,3,,1,404261061,"Hi, interested in your work and trying running the code.
But immediately facing a trouble in line 181 `mergan.py`

`_fixed_noise = pickle.load(open('/datatmp/result/fixed_noise/fixed_noise','r'))`

The `fixed_noise` file is not available. 
Is this file supposed to be provided by you? Or cached during training other model?

Thank you!"
"MemoryError: Unable to allocate 3.80 TiB for an array with shape (722570, 722570) and data type float64",bsafacicek/saas,2020-02-26 04:36:35,0,,1,571063542,when i run “python saas_nozca.py --dataset 'svhn' --net_name resnet” I meet this，what should i do？
Model performance,facebookresearch/WSL-Images,2022-02-16 12:06:38,0,,23,1139926829,"Hi, I have to ask a question. it seems that the model `resnext101_32x48d ` is slower and it is taking the real time of the video while extracting the features. Is there any way to run it faster?"
cuda out of memory,facebookresearch/WSL-Images,2021-11-05 14:35:24,0,,22,1045916883,"Hi,
I am working on this code at Kaggle .com and facing this issue.
please suggest your valuable solution.
![Capture](https://user-images.githubusercontent.com/65006502/140527585-eaac54b4-9c98-4ec0-8b65-e17259c7b370.PNG)
"
Error,facebookresearch/WSL-Images,2021-11-02 11:12:23,0,,21,1042199860,"ValueError: Cannot find master in https://github.com/facebookresearch/WSL-Images. If it's a commit from a forked repo, please call hub.load() with forked repo directly.ResNeXtModel
We are facing above error. Can Anyone help us we are working on kaggle
"
404 error when pulling model from hub,facebookresearch/WSL-Images,2021-10-22 07:17:10,7,,20,1033258967,"I'm trying to pull resnext101_32x48d with the following code: 

```
model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl')
```

but am receiving the following error 


```
Downloading: ""https://github.com/facebookresearch/WSL-Images/archive/master.zip"" to /tmp/cache/torch/hub/master.zip
...
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
```
"
Significant numerical differences with torch.amp.autocast() compared with stock pretrained resnext101,facebookresearch/WSL-Images,2020-11-05 17:06:35,0,,17,737111042,"When making inferences `with torch.amp.autocast()`, the forward results show significant numerical differences compared with pretrained `resnext101_32x8d` from `torchvision` as the sample outputs in the following given the same input batch:

Output from `WSL` pretrained `resnext101_32x8d_wsl` shows significant differences:
```py
actual(w/ amp.autocast) = tensor([ 1.0162e-01,  3.0859e-01, -1.9760e-03,  3.7750e-02, -4.5996e-01,
        -7.2510e-01, -8.7402e-02, -9.4727e-01...698e-02, -1.4392e-01,
        -2.1533e-01, -5.7666e-01, -8.1787e-02,  1.8103e-01,  2.3596e-01],
       device='cuda:0')
expected(w/o amp.autocast) = tensor([ 1.2948e-01,  4.0339e-01,  6.4677e-02,  6.7963e-02, -3.6953e-01,
        -6.0408e-01, -1.5742e-01, -8.2637e-01...613e-02, -1.7330e-01,
        -2.1253e-01, -5.2314e-01, -1.2327e-01,  1.0499e-01,  1.7262e-01],
       device='cuda:0')
```

Output from `torchvision` pretrained `resnext101_32x8d` shows approximate numerical values:
```py
actual(w/ amp.autocast) = tensor([-2.9844e+00, -6.8945e-01,  5.9668e-01, -1.2510e+00, -7.2168e-01,
        -2.1992e+00, -1.2686e+00, -5.0879e-01...953e+00, -4.4453e+00,
        -4.8984e+00, -3.2617e+00, -2.6641e+00, -2.2344e+00,  5.4922e+00],
       device='cuda:0')
expected(w/o amp.autocast) = tensor([-2.9887e+00, -6.8953e-01,  5.9514e-01, -1.2496e+00, -7.2139e-01,
        -2.2008e+00, -1.2737e+00, -5.1238e-01...971e+00, -4.4485e+00,
        -4.9002e+00, -3.2653e+00, -2.6683e+00, -2.2359e+00,  5.5001e+00],
       device='cuda:0')
```

Is it because the pretrained resnext101 from torchvision is already trained in mixed precision or something else?
Any clarifications would be appreciated.

PS: sample `pytest` code to load the models and run the tests:

```py
import torch as th

@pytest.fixture
def batch_size():
    return 2

@pytest.fixture
def shape():
    return 3, 720, 1280

@pytest.fixture
def dev():
    return th.device('cuda') if th.cuda.is_available() else torch.device('cpu')

@pytest.fixture
def batch(batch_size, shape):
    return th.rand(batch_size, *shape)

@pytest.fixture
def x101_32x8d(dev):
    from torchvision.models.resnet import _resnet
    from torchvision.models.resnet import Bottleneck
    from torchvision.ops.misc import FrozenBatchNorm2d
    kwargs = {}
    frozen = True
    kwargs['groups'] = gs = kwargs.get('groups', 32)
    kwargs['width_per_group'] = gw = kwargs.get('width_per_group', 8)
    kwargs['norm_layer'] = kwargs.get('norm_layer', FrozenBatchNorm2d if frozen else None)
    arch = f""resnext101_{gs}x{gw}d""
    model = _resnet(arch, Bottleneck, [3, 4, 23, 3], True, True, **kwargs)
    model.to(dev).eval()
    return model

@pytest.fixture
def x101_32x8d_wsl(dev):
    from torchvision.ops.misc import FrozenBatchNorm2d
    kwargs = {}
    frozen = True
    kwargs['groups'] = gs = kwargs.get('groups', 32)
    kwargs['width_per_group'] = gw = kwargs.get('width_per_group', 8)
    kwargs['norm_layer'] = kwargs.get('norm_layer', FrozenBatchNorm2d if frozen else None)
    model = th.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl', **kwargs)
    model.to(dev).eval()
    return model

@pytest.mark.parametrize(""B"", [2])
def test_x101_amp(benchmark, x101_32x8d, dev, batch, B):
    model = x101_32x8d
    with th.no_grad():
        with th.cuda.amp.autocast(enabled=False):
            outputs_fp32 = model(batch[:B].to(dev)).float()
        with th.cuda.amp.autocast():
            outputs_amp = model(batch[:B].to(dev)).float()

    for i, (output_fp32, output_amp) in enumerate(zip(outputs_fp32, outputs_amp)):
        logging.info(f""output[{i}] shape={tuple(output_fp32.shape)}, norm_fp32={output_fp32.norm()}, norm_amp={output_amp.norm()}"")
        th.testing.assert_allclose(output_amp, output_fp32, rtol=1e-03, atol=3e-04)

@pytest.mark.parametrize(""B"", [2])
def test_x101_wsl_amp(benchmark, x101_32x8d_wsl, dev, batch, B):
    model = x101_32x8d_wsl
    with th.no_grad():
        with th.cuda.amp.autocast(enabled=False):
            outputs_fp32 = model(batch[:B].to(dev)).float()
        with th.cuda.amp.autocast():
            outputs_amp = model(batch[:B].to(dev)).float()
    
    for i, (output_fp32, output_amp) in enumerate(zip(outputs_fp32, outputs_amp)):
        logging.info(f""output[{i}] shape={tuple(output_fp32.shape)}, norm_fp32={output_fp32.norm()}, norm_amp={output_amp.norm()}"")
        th.testing.assert_allclose(output_amp, output_fp32, rtol=1e-03, atol=3e-04)
```"
How to find the precision of the various layers when laoding the models resnext101 models,facebookresearch/WSL-Images,2020-10-07 19:23:18,0,,16,716790626,"When I load the model below, is there a way to find out what parts are fp16 precision and what parts are fp32 or some other precision.

     `https://download.pytorch.org/models/ig_resnext101_32x8-c38310e5.pth`

Thank you"
how can I do the final hashtag prediction using the pretrained model?,facebookresearch/WSL-Images,2020-07-30 11:09:25,0,,14,668616441,"```
import torch
model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl')
# sample execution (requires torchvision)
# Download an example image from the pytorch website
import urllib
url, filename = (""https://github.com/pytorch/hub/raw/master/dog.jpg"", ""dog.jpg"")
try: urllib.URLopener().retrieve(url, filename)
except: urllib.request.urlretrieve(url, filename)
from PIL import Image
from torchvision import transforms
# filename='/home/lalit/notebooks/Lalit/image_caption/pytorch-tutorial/tutorials/03-advanced/image_captioning/png/5e96fea74a01a44750ff4d36.png'
input_image = Image.open(filename)
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
input_tensor = preprocess(input_image)
input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model

# move the input and model to GPU for speed if available
if torch.cuda.is_available():
    input_batch = input_batch.to('cuda')
    model.to('cuda')

with torch.no_grad():
    output = model(input_batch)
# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes
# print(output[0])
# The output has unnormalized scores. To get probabilities, you can run a softmax on it.
print(torch.nn.functional.softmax(output[0], dim=0))
```"
Pretrained hashtag prediction model,facebookresearch/WSL-Images,2020-03-30 13:14:23,4,,13,590280252,"Hello,

These models are the models ultimately used for Imagenet classification. I am looking for the pre-trained models used to predict hashtags. From the paper:

""In our experiments, we pre-train convolutional networks for hashtag prediction [...]""
""Full network finetuning is performed by removing the hashtag-specific fully
connected classification layer from the network""

I am looking for this hashtag prediction model for an open-source project which needs to clean and improve (by adding more) hashtags from Flickr.

At least it would help a lot knowing which layers of the available model on pytorch hub correspond to this underlying hashtag prediction model.

Thanks!"
Is there anyone who  implemented the ResNeXt WSL reproduction results in tensorflow2.x or tensorflow1.x，Thx!,facebookresearch/WSL-Images,2019-11-15 02:35:09,0,,12,523217379,Is there anyone who  implemented the ResNeXt WSL reproduction results in tensorflow2.x or tensorflow1.x，Thx!
About training set,facebookresearch/WSL-Images,2019-10-27 13:03:20,0,,11,512963328,"Hi,
Is there any access to the training data for training WSL, for instance, train-IG-940M-1.5k?
Jeff"
Passing 'pretrained' parameter to torch.hub.load raises error,facebookresearch/WSL-Images,2019-10-21 07:14:46,0,,10,509774819,"## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->
Tried to load FB released pre-trained model on Instagram on PyTorch Hub. It works without passing the 'pretrained' parameter. But if the parameter is passed it raises error. I think this will cause inconsistency with loading of other models.

## To Reproduce

Steps to reproduce the behavior:

model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl', pretrained=True)

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->
TypeError: _resnext() got multiple values for argument 'pretrained'
## Expected behavior
Load the architecture with random weights if the 'pretrained' parameter is False, and with the pretrained weights if it is True.
<!-- A clear and concise description of what you expected to happen. -->"
How to solve ZeroDivisionError: float division by zero?,facebookresearch/WSL-Images,2019-10-10 20:35:30,1,,9,505496735,"Why i get ZeroDivisionError while trying resnext101_32x16d_wsl?

here is my model : 

# Model

device = torch.device(""cuda:0"")
model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')
model.fc = torch.nn.Linear(2048, n_classes)

model.to(device)

criterion = torch.nn.BCEWithLogitsLoss()
plist = [{'params': model.parameters(), 'lr': 2e-5}]
optimizer = optim.Adam(plist, lr=2e-5)

model, optimizer = amp.initialize(model, optimizer, opt_level=""O1"")



HERE IS MY TRAINING LOG : 

Epoch 0/0
----------
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.5
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.03125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.015625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0078125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.00390625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.001953125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0009765625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.00048828125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.000244140625
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0001220703125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.103515625e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0517578125e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.52587890625e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.62939453125e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.814697265625e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9073486328125e-06
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.5367431640625e-07
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.76837158203125e-07
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.384185791015625e-07
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1920928955078125e-07
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.960464477539063e-08
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9802322387695312e-08
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4901161193847656e-08
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.450580596923828e-09
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.725290298461914e-09
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.862645149230957e-09
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.313225746154785e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.656612873077393e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3283064365386963e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1641532182693481e-10
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.820766091346741e-11
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9103830456733704e-11
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4551915228366852e-11
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.275957614183426e-12
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.637978807091713e-12
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8189894035458565e-12
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.094947017729282e-13
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.547473508864641e-13
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2737367544323206e-13
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1368683772161603e-13
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.684341886080802e-14
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.842170943040401e-14
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4210854715202004e-14
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.105427357601002e-15
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.552713678800501e-15
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7763568394002505e-15
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.881784197001252e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.440892098500626e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.220446049250313e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1102230246251565e-16
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.551115123125783e-17
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7755575615628914e-17
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3877787807814457e-17
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.938893903907228e-18
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.469446951953614e-18
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.734723475976807e-18
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.673617379884035e-19
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.336808689942018e-19
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.168404344971009e-19
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0842021724855044e-19
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.421010862427522e-20
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.710505431213761e-20
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3552527156068805e-20
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.776263578034403e-21
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3881317890172014e-21
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6940658945086007e-21
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.470329472543003e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.235164736271502e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.117582368135751e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0587911840678754e-22
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.293955920339377e-23
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6469779601696886e-23
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3234889800848443e-23
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.617444900424222e-24
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.308722450212111e-24
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6543612251060553e-24
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.271806125530277e-25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.1359030627651384e-25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0679515313825692e-25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0339757656912846e-25
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.169878828456423e-26
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5849394142282115e-26
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2924697071141057e-26
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.462348535570529e-27
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2311742677852644e-27
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6155871338926322e-27
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.077935669463161e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0389678347315804e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0194839173657902e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0097419586828951e-28
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.048709793414476e-29
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.524354896707238e-29
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.262177448353619e-29
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.310887241768095e-30
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1554436208840472e-30
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5777218104420236e-30
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.888609052210118e-31
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.944304526105059e-31
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9721522630525295e-31
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.860761315262648e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.930380657631324e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.465190328815662e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.232595164407831e-32
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.162975822039155e-33
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0814879110195774e-33
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5407439555097887e-33
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.703719777548943e-34
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.851859888774472e-34
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.925929944387236e-34
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.62964972193618e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.81482486096809e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.407412430484045e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2037062152420224e-35
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.018531076210112e-36
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.009265538105056e-36
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.504632769052528e-36
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.52316384526264e-37
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.76158192263132e-37
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.88079096131566e-37
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.4039548065783e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.70197740328915e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.350988701644575e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1754943508222875e-38
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.877471754111438e-39
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.938735877055719e-39
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4693679385278594e-39
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.346839692639297e-40
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6734198463196485e-40
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8367099231598242e-40
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.183549615799121e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.591774807899561e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2958874039497803e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1479437019748901e-41
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.739718509874451e-42
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8698592549372254e-42
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4349296274686127e-42
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.174648137343064e-43
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.587324068671532e-43
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.793662034335766e-43
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.96831017167883e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.484155085839415e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2420775429197073e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1210387714598537e-44
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.605193857299268e-45
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.802596928649634e-45
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.401298464324817e-45
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.006492321624085e-46
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.503246160812043e-46
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7516230804060213e-46
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.758115402030107e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.3790577010150533e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1895288505075267e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0947644252537633e-47
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.473822126268817e-48
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7369110631344083e-48
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3684555315672042e-48
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.842277657836021e-49
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4211388289180104e-49
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7105694144590052e-49
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.552847072295026e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.276423536147513e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1382117680737565e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0691058840368783e-50
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.345529420184391e-51
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6727647100921956e-51
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3363823550460978e-51
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.681911775230489e-52
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3409558876152446e-52
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6704779438076223e-52
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.352389719038111e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.176194859519056e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.088097429759528e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.044048714879764e-53
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.22024357439882e-54
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.61012178719941e-54
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.305060893599705e-54
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.525304467998525e-55
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2626522339992623e-55
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6313261169996311e-55
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.156630584998156e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.078315292499078e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.039157646249539e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0195788231247695e-56
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.0978941156238473e-57
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5489470578119236e-57
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2744735289059618e-57
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.372367644529809e-58
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1861838222649046e-58
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5930919111324523e-58
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.965459555662261e-59
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.982729777831131e-59
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9913648889155653e-59
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.956824444577827e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.9784122222889134e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4892061111444567e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2446030555722283e-60
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.223015277861142e-61
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.111507638930571e-61
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5557538194652854e-61
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.778769097326427e-62
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8893845486632136e-62
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9446922743316068e-62
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.723461371658034e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.861730685829017e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4308653429145085e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2154326714572542e-63
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.077163357286271e-64
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0385816786431356e-64
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5192908393215678e-64
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.596454196607839e-65
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7982270983039195e-65
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8991135491519597e-65
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.495567745759799e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.7477838728798994e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3738919364399497e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1869459682199748e-66
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.934729841099874e-67
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.967364920549937e-67
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4836824602749686e-67
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.418412301374843e-68
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7092061506874214e-68
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8546030753437107e-68
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.273015376718553e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.636507688359277e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3182538441796384e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1591269220898192e-69
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.795634610449096e-70
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.897817305224548e-70
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.448908652612274e-70
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.24454326306137e-71
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.622271631530685e-71
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8111358157653425e-71
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.055679078826712e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.527839539413356e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.263919769706678e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.131959884853339e-72
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.659799424266695e-73
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8298997121333476e-73
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4149498560666738e-73
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.074749280333369e-74
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5373746401666845e-74
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7686873200833423e-74
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.843436600416711e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.421718300208356e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.210859150104178e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.105429575052089e-75
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.527147875260445e-76
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7635739376302223e-76
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3817869688151111e-76
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.908934844075556e-77
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.454467422037778e-77
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.727233711018889e-77
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.636168555094445e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.3180842775472223e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1590421387736112e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0795210693868056e-78
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.397605346934028e-79
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.698802673467014e-79
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.349401336733507e-79
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.747006683667535e-80
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3735033418337674e-80
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6867516709168837e-80
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.433758354584419e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.2168791772922093e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1084395886461046e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0542197943230523e-81
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.271098971615262e-82
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.635549485807631e-82
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3177747429038154e-82
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.588873714519077e-83
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2944368572595385e-83
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6472184286297693e-83
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.236092143148846e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.118046071574423e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0590230357872116e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0295115178936058e-84
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.147557589468029e-85
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5737787947340145e-85
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2868893973670072e-85
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.434446986835036e-86
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.217223493417518e-86
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.608611746708759e-86
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.043058733543795e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.021529366771898e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.010764683385949e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0053823416929744e-87
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.026911708464872e-88
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.513455854232436e-88
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.256727927116218e-88
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.28363963558109e-89
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.141819817790545e-89
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5709099088952725e-89
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.854549544476363e-90
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.9272747722381812e-90
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9636373861190906e-90
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.818186930595453e-91
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.909093465297727e-91
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4545467326488633e-91
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2272733663244316e-91
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.136366831622158e-92
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.068183415811079e-92
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5340917079055395e-92
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.670458539527698e-93
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.835229269763849e-93
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9176146348819244e-93
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.588073174409622e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.794036587204811e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3970182936024055e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1985091468012028e-94
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.992545734006014e-95
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.996272867003007e-95
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4981364335015035e-95
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.490682167507517e-96
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.745341083753759e-96
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8726705418768793e-96
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.363352709384397e-97
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.6816763546921983e-97
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3408381773460992e-97
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1704190886730496e-97
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.852095443365248e-98
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.926047721682624e-98
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.463023860841312e-98
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.31511930420656e-99
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.65755965210328e-99
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.82877982605164e-99
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.1438991302582e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5719495651291e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.28597478256455e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.142987391282275e-100
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.714936956411375e-101
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8574684782056875e-101
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4287342391028437e-101
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.143671195514219e-102
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5718355977571093e-102
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7859177988785547e-102
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.929588994392773e-103
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.464794497196387e-103
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2323972485981933e-103
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1161986242990967e-103
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.5809931214954833e-104
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7904965607477417e-104
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3952482803738708e-104
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.976241401869354e-105
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.488120700934677e-105
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7440603504673385e-105
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.720301752336693e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.3601508761683463e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1800754380841732e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0900377190420866e-106
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.450188595210433e-107
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7250942976052165e-107
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3625471488026082e-107
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.812735744013041e-108
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4063678720065206e-108
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7031839360032603e-108
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.515919680016301e-109
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.257959840008151e-109
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1289799200040754e-109
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0644899600020377e-109
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.3224498000101884e-110
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6612249000050942e-110
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3306124500025471e-110
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.653062250012736e-111
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.326531125006368e-111
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.663265562503184e-111
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.31632781251592e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.15816390625796e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.07908195312898e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.03954097656449e-112
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.19770488282245e-113
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.598852441411225e-113
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2994262207056124e-113
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.497131103528062e-114
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.248565551764031e-114
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6242827758820155e-114
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.121413879410078e-115
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.060706939705039e-115
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0303534698525194e-115
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0151767349262597e-115
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.075883674631299e-116
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5379418373156492e-116
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2689709186578246e-116
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.344854593289123e-117
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1724272966445615e-117
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5862136483222808e-117
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.931068241611404e-118
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.965534120805702e-118
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.982767060402851e-118
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.913835302014255e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.9569176510071274e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4784588255035637e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2392294127517818e-119
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.196147063758909e-120
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0980735318794546e-120
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5490367659397273e-120
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.745183829698637e-121
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8725919148493183e-121
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9362959574246591e-121
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.681479787123296e-122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.840739893561648e-122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.420369946780824e-122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.210184973390412e-122
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.05092486695206e-123
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.02546243347603e-123
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.512731216738015e-123
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.563656083690075e-124
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7818280418450374e-124
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8909140209225187e-124
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.454570104612593e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.727285052306297e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3636425261531484e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1818212630765742e-125
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.909106315382871e-126
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9545531576914354e-126
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4772765788457177e-126
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.386382894228589e-127
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6931914471142943e-127
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8465957235571472e-127
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.232978617785736e-128
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.616489308892868e-128
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.308244654446434e-128
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.154122327223217e-128
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.770611636116085e-129
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8853058180580424e-129
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4426529090290212e-129
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.213264545145106e-130
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.606632272572553e-130
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8033161362862765e-130
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.016580681431383e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5082903407156913e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2541451703578456e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1270725851789228e-131
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.635362925894614e-132
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.817681462947307e-132
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4088407314736535e-132
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.044203657368268e-133
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.522101828684134e-133
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.761050914342067e-133
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.805254571710335e-134
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.4026272858551673e-134
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2013136429275836e-134
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1006568214637918e-134
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.503284107318959e-135
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7516420536594796e-135
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3758210268297398e-135
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.879105134148699e-136
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4395525670743494e-136
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7197762835371747e-136
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.598881417685874e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.299440708842937e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1497203544214684e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0748601772107342e-137
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.374300886053671e-138
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6871504430268355e-138
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3435752215134178e-138
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.717876107567089e-139
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3589380537835444e-139
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6794690268917722e-139
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.397345134458861e-140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.1986725672294305e-140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0993362836147152e-140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0496681418073576e-140
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.248340709036788e-141
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.624170354518394e-141
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.312085177259197e-141
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.560425886295985e-142
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2802129431479926e-142
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6401064715739963e-142
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.200532357869981e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.100266178934991e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0501330894674953e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0250665447337477e-143
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.1253327236687384e-144
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5626663618343692e-144
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2813331809171846e-144
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.406665904585923e-145
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2033329522929615e-145
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6016664761464807e-145
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.008332380732404e-146
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.004166190366202e-146
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.002083095183101e-146
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0010415475915505e-146
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.0052077379577523e-147
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5026038689788762e-147
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2513019344894381e-147
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.256509672447191e-148
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1282548362235952e-148
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5641274181117976e-148
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.820637090558988e-149
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.910318545279494e-149
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.955159272639747e-149
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.775796363198735e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.887898181599368e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.443949090799684e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.221974545399842e-150
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.10987272699921e-151
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.054936363499605e-151
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5274681817498023e-151
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.637340908749012e-152
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.818670454374506e-152
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.909335227187253e-152
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.546676135936265e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.7733380679681323e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3866690339840662e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1933345169920331e-153
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.966672584960166e-154
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.983336292480083e-154
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4916681462400413e-154
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.458340731200207e-155
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7291703656001034e-155
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8645851828000517e-155
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.322925914000258e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.661462957000129e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3307314785000646e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1653657392500323e-156
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.826828696250162e-157
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.913414348125081e-157
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4567071740625404e-157
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.283535870312702e-158
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.641767935156351e-158
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8208839675781755e-158
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.104419837890877e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.552209918945439e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2761049594727193e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1380524797363597e-159
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.6902623986817984e-160
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8451311993408992e-160
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4225655996704496e-160
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.112827998352248e-161
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.556413999176124e-161
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.778206999588062e-161
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.89103499794031e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.445517498970155e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2227587494850775e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1113793747425387e-162
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.556896873712694e-163
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.778448436856347e-163
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3892242184281734e-163
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.946121092140867e-164
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4730605460704336e-164
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7365302730352168e-164
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.682651365176084e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.341325682588042e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.170662841294021e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0853314206470105e-165
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.426657103235053e-166
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7133285516175262e-166
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3566642758087631e-166
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.783321379043816e-167
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.391660689521908e-167
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.695830344760954e-167
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.47915172380477e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.239575861902385e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1197879309511924e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0598939654755962e-168
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.299469827377981e-169
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6497349136889905e-169
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3248674568444952e-169
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.624337284222476e-170
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.312168642111238e-170
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.656084321055619e-170
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.280421605278095e-171
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.140210802639048e-171
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.070105401319524e-171
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.035052700659762e-171
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.17526350329881e-172
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.587631751649405e-172
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2938158758247024e-172
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.469079379123512e-173
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.234539689561756e-173
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.617269844780878e-173
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.08634922390439e-174
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.043174611952195e-174
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0215873059760975e-174
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0107936529880487e-174
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.053968264940244e-175
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.526984132470122e-175
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.263492066235061e-175
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.317460331175305e-176
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1587301655876523e-176
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5793650827938261e-176
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.896825413969131e-177
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.9484127069845653e-177
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9742063534922827e-177
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.871031767461413e-178
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.935515883730707e-178
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4677579418653533e-178
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2338789709326767e-178
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.169394854663383e-179
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.084697427331692e-179
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.542348713665846e-179
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.71174356832923e-180
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.855871784164615e-180
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9279358920823073e-180
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.639679460411536e-181
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.819839730205768e-181
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.409919865102884e-181
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.204959932551442e-181
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.02479966275721e-182
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.012399831378605e-182
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5061999156893026e-182
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.530999578446513e-183
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7654997892232564e-183
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8827498946116282e-183
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.413749473058141e-184
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.706874736529071e-184
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3534373682645353e-184
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1767186841322676e-184
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.883593420661338e-185
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.941796710330669e-185
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4708983551653345e-185
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.354491775826673e-186
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6772458879133364e-186
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8386229439566682e-186
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.193114719783341e-187
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5965573598916705e-187
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2982786799458352e-187
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1491393399729176e-187
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.745696699864588e-188
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.872848349932294e-188
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.436424174966147e-188
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.182120874830735e-189
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5910604374153675e-189
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7955302187076838e-189
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.977651093538419e-190
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.4888255467692094e-190
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2444127733846047e-190
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1222063866923024e-190
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.611031933461512e-191
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.805515966730756e-191
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.402757983365378e-191
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.01378991682689e-192
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.506894958413445e-192
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7534474792067224e-192
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.767237396033612e-193
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.383618698016806e-193
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.191809349008403e-193
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0959046745042015e-193
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.479523372521008e-194
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.739761686260504e-194
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.369880843130252e-194
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.84940421565126e-195
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.42470210782563e-195
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.712351053912815e-195
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.561755269564074e-196
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.280877634782037e-196
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1404388173910186e-196
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0702194086955093e-196
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.351097043477547e-197
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6755485217387732e-197
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3377742608693866e-197
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.688871304346933e-198
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3444356521734666e-198
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6722178260867333e-198
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.361089130433666e-199
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.180544565216833e-199
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0902722826084166e-199
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0451361413042083e-199
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.225680706521042e-200
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.612840353260521e-200
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3064201766302604e-200
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.532100883151302e-201
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.266050441575651e-201
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6330252207878255e-201
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.165126103939127e-202
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.082563051969564e-202
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.041281525984782e-202
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.020640762992391e-202
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.103203814961955e-203
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5516019074809773e-203
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2758009537404886e-203
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.379004768702443e-204
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1895023843512216e-204
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5947511921756108e-204
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.973755960878054e-205
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.986877980439027e-205
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9934389902195135e-205
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.967194951097568e-206
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.983597475548784e-206
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.491798737774392e-206
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.245899368887196e-206
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.22949684443598e-207
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.11474842221799e-207
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.557374211108995e-207
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.786871055544975e-208
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8934355277724873e-208
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9467177638862437e-208
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.733588819431218e-209
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.866794409715609e-209
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4333972048578046e-209
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2166986024289023e-209
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.083493012144512e-210
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.041746506072256e-210
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.520873253036128e-210
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.60436626518064e-211
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.80218313259032e-211
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.90109156629516e-211
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.5054578314758e-212
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.7527289157379e-212
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.37636445786895e-212
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.188182228934475e-212
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.940911144672375e-213
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9704555723361872e-213
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4852277861680936e-213
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.426138930840468e-214
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.713069465420234e-214
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.856534732710117e-214
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.282673663550585e-215
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.641336831775293e-215
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3206684158876463e-215
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1603342079438231e-215
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.801671039719116e-216
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.900835519859558e-216
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.450417759929779e-216
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.252088799648895e-217
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6260443998244473e-217
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8130221999122236e-217
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.065110999561118e-218
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.532555499780559e-218
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2662777498902796e-218
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1331388749451398e-218
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.665694374725699e-219
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8328471873628494e-219
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4164235936814247e-219
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.082117968407124e-220
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.541058984203562e-220
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.770529492101781e-220
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.852647460508905e-221
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.4263237302544523e-221
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2131618651272261e-221
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1065809325636131e-221
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.5329046628180653e-222
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7664523314090327e-222
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3832261657045163e-222
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.916130828522582e-223
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.458065414261291e-223
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7290327071306454e-223
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.645163535653227e-224
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.322581767826614e-224
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.161290883913307e-224
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0806454419566534e-224
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.403227209783267e-225
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7016136048916335e-225
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3508068024458167e-225
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.754034012229084e-226
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.377017006114542e-226
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.688508503057271e-226
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.442542515286355e-227
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.2212712576431773e-227
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1106356288215886e-227
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0553178144107943e-227
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.276589072053972e-228
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.638294536026986e-228
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.319147268013493e-228
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.595736340067465e-229
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2978681700337323e-229
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6489340850168661e-229
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.244670425084331e-230
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.1223352125421653e-230
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0611676062710827e-230
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0305838031355413e-230
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.152919015677707e-231
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5764595078388533e-231
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2882297539194267e-231
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.441148769597133e-232
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.220574384798567e-232
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6102871923992833e-232
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.051435961996417e-233
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0257179809982083e-233
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0128589904991042e-233
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0064294952495521e-233
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.0321474762477604e-234
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5160737381238802e-234
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2580368690619401e-234
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.290184345309701e-235
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1450921726548502e-235
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5725460863274251e-235
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.862730431637126e-236
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.931365215818563e-236
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9656826079092814e-236
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.828413039546407e-237
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.914206519773204e-237
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.457103259886602e-237
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.228551629943301e-237
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.142758149716505e-238
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0713790748582522e-238
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5356895374291261e-238
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.678447687145631e-239
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8392238435728152e-239
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9196119217864076e-239
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.598059608932038e-240
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.799029804466019e-240
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3995149022330095e-240
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1997574511165048e-240
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.998787255582524e-241
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.999393627791262e-241
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.499696813895631e-241
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.498484069478155e-242
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7492420347390774e-242
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8746210173695387e-242
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.373105086847693e-243
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.686552543423847e-243
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3432762717119234e-243
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1716381358559617e-243
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.858190679279809e-244
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9290953396399042e-244
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4645476698199521e-244
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.322738349099761e-245
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.6613691745498803e-245
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8306845872749401e-245
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.153422936374701e-246
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5767114681873503e-246
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2883557340936752e-246
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1441778670468376e-246
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.720889335234188e-247
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.860444667617094e-247
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.430222333808547e-247
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.151111669042735e-248
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5755558345213674e-248
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7877779172606837e-248
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.938889586303419e-249
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.4694447931517093e-249
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2347223965758547e-249
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1173611982879273e-249
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.586805991439637e-250
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7934029957198183e-250
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3967014978599092e-250
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.983507489299546e-251
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.491753744649773e-251
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7458768723248864e-251
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.729384361624432e-252
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.364692180812216e-252
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.182346090406108e-252
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.091173045203054e-252
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.45586522601527e-253
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.727932613007635e-253
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3639663065038175e-253
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.819831532519088e-254
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.409915766259544e-254
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.704957883129772e-254
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.52478941564886e-255
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.26239470782443e-255
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.131197353912215e-255
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0655986769561075e-255
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.327993384780537e-256
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6639966923902686e-256
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3319983461951343e-256
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.659991730975672e-257
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.329995865487836e-257
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.664997932743918e-257
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.32498966371959e-258
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.162494831859795e-258
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0812474159298974e-258
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0406237079649487e-258
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.2031185398247434e-259
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.6015592699123717e-259
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3007796349561859e-259
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.503898174780929e-260
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2519490873904646e-260
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6259745436952323e-260
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.129872718476162e-261
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.064936359238081e-261
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0324681796190404e-261
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0162340898095202e-261
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.081170449047601e-262
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5405852245238005e-262
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2702926122619002e-262
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.351463061309501e-263
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.1757315306547506e-263
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5878657653273753e-263
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.939328826636877e-264
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.9696644133184383e-264
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9848322066592191e-264
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.924161033296096e-265
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.962080516648048e-265
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.481040258324024e-265
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.240520129162012e-265
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.20260064581006e-266
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.10130032290503e-266
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.550650161452515e-266
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.753250807262575e-267
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8766254036312874e-267
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9383127018156437e-267
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.691563509078218e-268
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.845781754539109e-268
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.4228908772695546e-268
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2114454386347773e-268
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.057227193173887e-269
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0286135965869433e-269
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5143067982934716e-269
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.571533991467358e-270
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.785766995733679e-270
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8928834978668395e-270
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.464417489334198e-271
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.732208744667099e-271
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3661043723335494e-271
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1830521861667747e-271
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.915260930833874e-272
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.957630465416937e-272
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4788152327084684e-272
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.394076163542342e-273
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.697038081771171e-273
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8485190408855855e-273
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.242595204427927e-274
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.621297602213964e-274
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.310648801106982e-274
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.155324400553491e-274
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.776622002767455e-275
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8883110013837273e-275
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4441555006918637e-275
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.220777503459318e-276
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.610388751729659e-276
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8051943758648296e-276
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.025971879324148e-277
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.512985939662074e-277
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.256492969831037e-277
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1282464849155185e-277
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.641232424577593e-278
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.8206162122887962e-278
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4103081061443981e-278
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.051540530721991e-279
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5257702653609953e-279
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7628851326804976e-279
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.814425663402488e-280
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.407212831701244e-280
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.203606415850622e-280
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.101803207925311e-280
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.509016039626555e-281
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.7545080198132776e-281
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3772540099066388e-281
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.886270049533194e-282
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.443135024766597e-282
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7215675123832985e-282
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.607837561916492e-283
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.303918780958246e-283
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.151959390479123e-283
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0759796952395615e-283
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.379898476197808e-284
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.689949238098904e-284
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.344974619049452e-284
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.72487309524726e-285
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.36243654762363e-285
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.681218273811815e-285
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.406091369059075e-286
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.2030456845295373e-286
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1015228422647686e-286
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0507614211323843e-286
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.253807105661922e-287
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.626903552830961e-287
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.3134517764154804e-287
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.567258882077402e-288
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.283629441038701e-288
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6418147205193505e-288
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.209073602596753e-289
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.1045368012983762e-289
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0522684006491881e-289
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0261342003245941e-289
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.1306710016229703e-290
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.5653355008114852e-290
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.2826677504057426e-290
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.413338752028713e-291
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2066693760143564e-291
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6033346880071782e-291
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.016673440035891e-292
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.008336720017946e-292
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.004168360008973e-292
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0020841800044864e-292
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.010420900022432e-293
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.505210450011216e-293
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.252605225005608e-293
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.26302612502804e-294
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.13151306251402e-294
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.56575653125701e-294
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.82878265628505e-295
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.914391328142525e-295
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9571956640712625e-295
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.785978320356312e-296
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.892989160178156e-296
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.446494580089078e-296
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.223247290044539e-296
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.116236450222695e-297
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0581182251113476e-297
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.5290591125556738e-297
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.645295562778369e-298
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.8226477813891845e-298
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9113238906945923e-298
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.556619453472961e-299
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.778309726736481e-299
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3891548633682403e-299
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1945774316841202e-299
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.972887158420601e-300
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9864435792103004e-300
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4932217896051502e-300
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.466108948025751e-301
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.7330544740128755e-301
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8665272370064378e-301
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.332636185032189e-302
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.6663180925160944e-302
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3331590462580472e-302
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1665795231290236e-302
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.832897615645118e-303
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.916448807822559e-303
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4582244039112795e-303
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.291122019556398e-304
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.645561009778199e-304
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8227805048890994e-304
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.113902524445497e-305
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.5569512622227484e-305
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2784756311113742e-305
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1392378155556871e-305
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.696189077778436e-306
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.848094538889218e-306
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.424047269444609e-306
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.120236347223045e-307
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.5601181736115222e-307
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.7800590868057611e-307
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.900295434028806e-308
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.450147717014403e-308
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2250738585072014e-308
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1125369292536007e-308
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.562684646268003e-309
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.781342323134e-309
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.390671161567e-309
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.953355807835e-310
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.4766779039175e-310
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.73833895195875e-310
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.691694759794e-311
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.345847379897e-311
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.1729236899484e-311
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.086461844974e-311
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.43230922487e-312
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.716154612436e-312
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.35807730622e-312
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.7903865311e-313
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.39519326554e-313
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.69759663277e-313
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.487983164e-314
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.243991582e-314
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.121995791e-314
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0609978955e-314
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.304989477e-315
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.65249474e-315
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.32624737e-315
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.63123685e-316
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.3156184e-316
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6578092e-316
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.289046e-317
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.144523e-317
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0722615e-317
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.036131e-317
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.180654e-318
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.590327e-318
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.295163e-318
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.4758e-319
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.2379e-319
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.61895e-319
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.095e-320
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0474e-320
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0237e-320
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.012e-320
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.06e-321
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.53e-321
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.265e-321
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.3e-322
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.16e-322
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.6e-322
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8e-323
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4e-323
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2e-323
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1e-323
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5e-324
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0
---------------------------------------------------------------------------
ZeroDivisionError                         Traceback (most recent call last)
<ipython-input-38-ad44e3432457> in <module>()
     23 
     24         with amp.scale_loss(loss, optimizer) as scaled_loss:
---> 25             scaled_loss.backward()
     26 
     27         tr_loss += loss.item()

4 frames
/usr/local/lib/python3.6/dist-packages/apex/amp/scaler.py in unscale_with_stashed(self, model_grads, stashed_master_grads, master_grads, scale_override)
    174                                  self._overflow_buf,
    175                                  [model_grads, stashed_master_grads, master_grads],
--> 176                                  out_scale/grads_have_scale,   # 1./scale,
    177                                  out_scale/stashed_have_scale, # 1.0,
    178                                  0) # check only arg 0, aka the incoming model grads, for infs

ZeroDivisionError: float division by zero"
WSL models not fine-tuned for ImageNet,facebookresearch/WSL-Images,2019-09-05 15:43:50,7,,8,489846827,"Hi!

Are models trained on WSL-images before fine-tuning on ImageNet available somewhere?

Best,
Simen"
OSError: [Errno 22] Invalid argument,facebookresearch/WSL-Images,2019-08-19 03:09:49,4,,7,482083686,"OS: Windows 7
Python: v3.7.4
PyTorch: v1.2.0
Model: resnext101_32x48d_wsl

```
      1 import torch
----> 2 model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl')
      3 model.eval()

c:\dev\python37\lib\site-packages\torch\hub.py in load(github, model, *args, **kwargs)
    361     entry = _load_entry_from_hubconf(hub_module, model)
    362 
--> 363     model = entry(*args, **kwargs)
    364 
    365     sys.path.remove(repo_dir)

~/.cache\torch\hub\facebookresearch_WSL-Images_master/hubconf.py in resnext101_32x48d_wsl(progress, **kwargs)
     76     kwargs['groups'] = 32
     77     kwargs['width_per_group'] = 48
---> 78     return _resnext('resnext101_32x48d', Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)

~/.cache\torch\hub\facebookresearch_WSL-Images_master/hubconf.py in _resnext(arch, block, layers, pretrained, progress, **kwargs)
     22 def _resnext(arch, block, layers, pretrained, progress, **kwargs):
     23     model = ResNet(block, layers, **kwargs)
---> 24     state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)
     25     model.load_state_dict(state_dict)
     26     return model

c:\dev\python37\lib\site-packages\torch\hub.py in load_state_dict_from_url(url, model_dir, map_location, progress)
    461         hash_prefix = HASH_REGEX.search(filename).group(1)
    462         _download_url_to_file(url, cached_file, hash_prefix, progress=progress)
--> 463     return torch.load(cached_file, map_location=map_location)

c:\dev\python37\lib\site-packages\torch\serialization.py in load(f, map_location, pickle_module, **pickle_load_args)
    384         f = f.open('rb')
    385     try:
--> 386         return _load(f, map_location, pickle_module, **pickle_load_args)
    387     finally:
    388         if new_fd:

c:\dev\python37\lib\site-packages\torch\serialization.py in _load(f, map_location, pickle_module, **pickle_load_args)
    578     for key in deserialized_storage_keys:
    579         assert key in deserialized_objects
--> 580         deserialized_objects[key]._set_from_file(f, offset, f_should_read_directly)
    581         if offset is not None:
    582             offset = f.tell()

OSError: [Errno 22] Invalid argument
```

maybe because it is biggest pre-trained model (more than 2GB) and maybe because Python has bugs in pickle on Windows?"
x101-32*8d doesn't work well when I put it in cascade rcnn in mmdetection,facebookresearch/WSL-Images,2019-07-02 02:52:49,5,,5,462994541,It seems the x101-32*8d backbone is even worse than resnet101 when I experiment on cascade rcnn.
About performance in Cub-200 finetune.,facebookresearch/WSL-Images,2019-06-27 13:42:49,2,,4,461537417,"Thank you for opensource this pretrained models.
I finetuned 32x32, 32x16 and 32x8 on Cub-200 dataset with 2 1080Ti, which only achive 87.5% accuracy on testset with image size 448x448. Image size with 224x224 only get 84.43% test acc. I find biger batch size got better performance but I only have 2 GPUs.
In the paper with 32x16 pretrained model reached 89.2 accuracy. 
Could you please show more details about finetune.
Thanks."
Will ResNeXt-101 32x4d be available?,facebookresearch/WSL-Images,2019-06-26 09:11:58,1,,2,460852512,I'm currently using ResNeXt-101_32x4d (pretrained on ImageNet) doing research on object detection. I want to try replace the ImageNet pretrained model with the WSL-Images pretrained one. So I am concerning where can I obtain the ResNeXt-101 32x4d model? Thanks.
作者您好，我在使用d_adapt.py 程序时出现了一些bug。,thuml/Transfer-Learning-Library,2022-10-28 12:00:52,3,,175,1427157970,"
Traceback (most recent call last):
  File ""d_adapt.py"", line 348, in <module>
    args=(args, args_cls, args_box),
  File ""/home/shishijie/anaconda3/envs/detectron-na/lib/python3.6/site-packages/detectron2/engine/launch.py"", line 82, in launch
    main_func(*args)
  File ""d_adapt.py"", line 277, in main
    train(model, logger, cfg, args, args_cls, args_box)
  File ""d_adapt.py"", line 163, in train
    bbox_adaptor.fit(data_loader_source, data_loader_target, data_loader_validation)
  File ""/home/shishijie/shishijie_projects/domain_adapatation/Transfer-Learning-Library-master/examples/domain_adaptation/object_detection/d_adapt/bbox_adaptation.py"", line 354, in fit
    x_s, labels_s = next(iter_source)
  File ""/home/shishijie/shishijie_projects/domain_adapatation/Transfer-Learning-Library-master/examples/domain_adaptation/object_detection/d_adapt/tllib/utils/data.py"", line 55, in __next__
    data = next(self.iter)
  File ""/home/shishijie/anaconda3/envs/detectron-na/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 521, in __next__
    data = self._next_data()
  File ""/home/shishijie/anaconda3/envs/detectron-na/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 1176, in _next_data
    raise StopIteration
StopIteration


我调了一天，我目前的看法是：a_adapt程序有四个训练阶段：源域预训练，类别适应，边界框适应，⽬标域伪标签训练。但是现在源域预训练没问题，我用源域的训练权重作为后续训练的预训练权重，10个epcho的类别适应训练也没问题，边界框适应训练出现了问题。

examples/domain_adaptation/object_detection/d_adapt/d_adapt.py 文件调用的bbox_adaptor.fit(data_loader_source, data_loader_target, data_loader_validation) 函数出现了问题，examples/domain_adaptation/object_detection/d_adapt/bbox_adaptation.py 文件中的 iter_source 拿不到标签数据，

（ps：自己的数据集和官方的voc2007和clipart数据集都是这样，我的训练命令：CUDA_VISIBLE_DEVICES=0  python d_adapt.py --config-file  config/retinanet_R_101_FPN_voc.yaml  -s VOC2007  ../datasets/VOC2007  -t Clipart ../datasets/clipart  --test Clipart ../datasets/clipart  --finetune  --bbox-refine   OUTPUT_DIR  logs/retinanet_R_101_FPN_voc/voc2clipart/phase2
）

劳烦作者费心给我这个废物一些指导

"
Code for Mean Embedding Test,thuml/Transfer-Learning-Library,2022-10-23 13:52:56,1,enhancement,174,1419802310,"Hi there,

In the paper ""Transferable Representation Learning with Deep Adaptation Networks"",

There are 2 Two-Sample Test Statistics, MK-MMD and Mean Embedding Test (ME), I can find the code for MK-MMD but can't find any clue for ME, is there any code for ME or will the code for ME be avaliable?"
ADDA Architecture,thuml/Transfer-Learning-Library,2022-10-17 15:58:46,1,question,172,1411830013,"Hi,

I am trying to modify ADDA from your code. In the original paper, there seems to be 2 CNNs in the second training phase, one to process the examples of the target set and another with the frozen weights to process the examples of the source set. In this way, the CNN of the target set is 'adjusted' to that of the source set depending on the output of the domain discriminator.

However, when reviewing the code it seems that only a single CNN is used, adjusting in a first phase the model (extractor+classifier) and in a second phase only the domain discriminator. My questions is how then the training of both phases is reproduced in the library. Thank you very much!


Best regards,



Eva"
目标域数据json文件的真值,thuml/Transfer-Learning-Library,2022-10-17 10:24:47,7,question,171,1411319393,想请问一下作者在用目标域数据训练时，生成的目标域数据proposal文件中含有真值信息，然而域自适应是在没有目标域真值的情况下使用的方法，那么请问当不提供目标域数据的真值时，还有可以用目标域数据训练吗
Learning rate,thuml/Transfer-Learning-Library,2022-09-28 07:58:28,1,question,169,1388905388,"HI，

Can the learning rate of discriminator be updated? Because in the function class DomainDiscriminator(nn.Sequential): **lr=1.**Does the learning rate(1.0) can update follow the epoch？

def get_parameters(self) -> List[Dict]:
    return [{""params"": self.parameters(), **""lr"": 1.**}]

Such as we adopt the following learning rate updating method：
lr_scheduler_ad = LambdaLR(
    ad_optimizer, lambda x: args.lr * (1. + args.lr_gamma * float(x)) ** (-args.lr_decay))



Because we found in the experiment that the discrimination loss remained stable when the epoch was very small, although the task loss was still decreasing.

Thank you"
在MCD code内，训练代码，数据载入好像有点Bug,thuml/Transfer-Learning-Library,2022-09-26 02:29:22,3,question,168,1385317202,"**Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
 - OS: [e.g. iOS]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Smartphone (please complete the following information):**
 - Device: [e.g. iPhone6]
 - OS: [e.g. iOS8.1]
 - Browser [e.g. stock browser, safari]
 - Version [e.g. 22]

**Additional context**
Add any other context about the problem here.
"
是否需要加入训练多核对应的权重的过程？,thuml/Transfer-Learning-Library,2022-09-22 17:34:25,1,question,167,1382791693,"我在阅读DAN时，注意到原文是一个minimax过程，在学习网络参数（\Theta）外，还需要学习多核权重（\beta）；而后者似乎未在此代码文件中体现，5个核被分配了相同的权重；我写了一个二次规划求核权重的函数，供参考讨论  :-)

`
def train_weights(train_source_iter: DataLoader, train_traget_iter: DataLoader,
                  model: ImageClassifier,  kernels: list):
    print(""=>Train Beta(weights) of the kernels"")
    num_kernels = len(kernels)
    #协方差矩阵（原文中的矩阵Q）
    covar_matrix = torch.zeros((num_kernels, num_kernels))

    for i in range(num_kernels):
        for j in range(i, num_kernels):
            kernel1 = kernels[i]
            kernel2 = kernels[j]
            count = 0
            #线性时间估计Cov（u，u'）
            for x_s, x_t in zip(train_source_iter, train_traget_iter):
                x_s = model(x_s[0])[1]
                x_t = model(x_t[0])[1]

                idx = 0
                res = []
 
                while idx < len(x_s) // 4:
                    t1 = (kernel1(x_s[4 * idx : 4 * idx + 2]) + kernel1(x_t[4 * idx : 4 * idx + 2]) - kernel1(
                        torch.cat((x_s[4 * idx].unsqueeze(dim=0), x_t[4 * idx + 1].unsqueeze(dim=0)))) \
                         - kernel1(torch.cat((x_s[4 * idx + 1].unsqueeze(dim=0), x_t[4 * idx].unsqueeze(dim=0)))))[0][1]
                    t2 = (kernel1(x_s[4 * idx+2 : 4 * idx + 4]) + kernel1(x_t[4 * idx+2 : 4 * idx + 4]) - kernel1(
                        torch.cat((x_s[4 * idx+2].unsqueeze(dim=0), x_t[4 * idx + 3].unsqueeze(dim=0)))) \
                          - kernel1(torch.cat((x_s[4 * idx + 3].unsqueeze(dim=0), x_t[4 * idx+2].unsqueeze(dim=0)))))[0][1]
                    t3 = (kernel2(x_s[4 * idx: 4 * idx + 2]) + kernel2(x_t[4 * idx: 4 * idx + 2]) - kernel2(
                        torch.cat((x_s[4 * idx].unsqueeze(dim=0), x_t[4 * idx + 1].unsqueeze(dim=0)))) \
                          - kernel2(torch.cat((x_s[4 * idx + 1].unsqueeze(dim=0), x_t[4 * idx].unsqueeze(dim=0)))))[0][1]
                    t4 = (kernel2(x_s[4 * idx + 2: 4 * idx + 4]) + kernel2(x_t[4 * idx + 2: 4 * idx + 4]) - kernel2(
                        torch.cat((x_s[4 * idx + 2].unsqueeze(dim=0), x_t[4 * idx + 3].unsqueeze(dim=0)))) \
                          - kernel2(torch.cat((x_s[4 * idx + 3].unsqueeze(dim=0), x_t[4 * idx + 2].unsqueeze(dim=0)))))[
                        0][1]

                    res.append((t1 - t2) * (t3 - t4))
                    idx += 1
                res = sum(res) / len(res)
                covar_matrix[i][j] = covar_matrix[i][j] * count / (count + 1) + res / (count + 1)
                count += 1
            covar_matrix[j][i] = covar_matrix[i][j]

    epsilon = .001
    reg = torch.eye(num_kernels) * epsilon
    covar_matrix = covar_matrix + reg
    #使用cvxopt包求解二次规划问题（from cvxopt import matrix, solvers）
    P = covar_matrix.t().numpy().tolist()
    q = [0. for i in range(num_kernels)]
    G = (torch.eye(num_kernels) * -1).t().numpy().tolist()
    h = [0. for i in range(num_kernels)]
    A = [[1.] for i in range(num_kernels)]
    P = matrix(P)
    q = matrix(q)
    G = matrix(G)
    h = matrix(h)
    A = matrix(A)
    b = matrix([1.0])
    result = solvers.qp(P, q, G, h, A, b)
    print('The qp result')
    print(result['x'])
    return list(result['x'])
`"
Visualization of results,liyi14/mx-DeepIM,2020-10-06 11:58:46,0,,61,715603927,"Did someone have accomplished the visualization of the train_results,like a picture of the object？Could you please tell me how to do it?thanks.
"
Support files for YCB-Video dataset,liyi14/mx-DeepIM,2019-06-12 16:24:07,0,,37,455309172,"Hi,
Thank you for sharing the source code of your work. Do you have support files for the YCB-Video dataset? If so, would you able to share them? Best regards."
training log,liyi14/mx-DeepIM,2019-05-26 07:51:53,2,,31,448539557,"Hi,

Would you be able to provide the training log especially interested in the point matching loss after few epochs. I'm training it on google compute VM with 4 Nvidia Tesla K80 GPUs but the training speed is about 2.2 frames/sec. The point matching loss is between 10-11 for last few hour (although the flowLoss and the maskLoss is showing a good downward trend).


poch[3] Batch [260]    Speed: 2.26 samples/sec Train-Flow_L2Loss=0.321852,     Flow_CurLoss=0.000000,  PointMatchingLoss=10.396450,    MaskLoss=0.108871,
Epoch[3] Batch [280]    Speed: 2.33 samples/sec Train-Flow_L2Loss=0.309070,     Flow_CurLoss=0.000000,  PointMatchingLoss=10.542913,    MaskLoss=0.109692,
Epoch[3] Batch [300]    Speed: 2.31 samples/sec Train-Flow_L2Loss=0.309031,     Flow_CurLoss=0.000000,  PointMatchingLoss=10.530530,    MaskLoss=0.109173,
Epoch[3] Batch [320]    Speed: 2.36 samples/sec Train-Flow_L2Loss=0.299514,     Flow_CurLoss=0.000000,  PointMatchingLoss=10.616660,    MaskLoss=0.108448,
Epoch[3] Batch [340]    Speed: 2.37 samples/sec Train-Flow_L2Loss=0.291039,     Flow_CurLoss=0.000000,  PointMatchingLoss=10.707121,    MaskLoss=0.107355,
Epoch[3] Batch [360]    Speed: 2.34 samples/sec Train-Flow_L2Loss=0.280393,     Flow_CurLoss=0.000000,  PointMatchingLoss=10.794026,    MaskLoss=0.107118,
Epoch[3] Batch [380]    Speed: 2.35 samples/sec Train-Flow_L2Loss=0.276359,     Flow_CurLoss=0.202416,  PointMatchingLoss=10.857506,    MaskLoss=0.110217,
Epoch[3] Batch [400]    Speed: 2.33 samples/sec Train-Flow_L2Loss=0.265881,     Flow_CurLoss=0.000000,  PointMatchingLoss=11.151155,    MaskLoss=0.109521,
Epoch[3] Batch [420]    Speed: 2.36 samples/sec Train-Flow_L2Loss=0.256921,     Flow_CurLoss=0.000000,  PointMatchingLoss=11.267041,    MaskLoss=0.110485,
Epoch[3] Batch [440]    Speed: 2.32 samples/sec Train-Flow_L2Loss=0.250065,     Flow_CurLoss=0.000000,  PointMatchingLoss=11.396601,    MaskLoss=0.111172,
Epoch[3] Batch [460]    Speed: 2.32 samples/sec Train-Flow_L2Loss=0.244040,     Flow_CurLoss=0.000000,  PointMatchingLoss=11.543343,    MaskLoss=0.110957,
Epoch[3] Batch [480]    Speed: 2.31 samples/sec Train-Flow_L2Loss=0.239315,     Flow_CurLoss=0.000000,  PointMatchingLoss=11.643859,    MaskLoss=0.114532,"
NaN values in Predictions,liyi14/mx-DeepIM,2018-10-11 14:48:38,3,awaiting_response,7,369158351,"After following the instructions in the latest commit and then running the train_and_test_deepim_all.sh I got the following error:
> Traceback (most recent call last):
  File ""experiments/deepim/deepim_train_test.py"", line 20, in <module>
    train.main()
  File ""experiments/deepim/../../deepim/train.py"", line 287, in main
    config.TRAIN.begin_epoch, config.TRAIN.end_epoch, config.TRAIN.lr, config.TRAIN.lr_step)
  File ""experiments/deepim/../../deepim/train.py"", line 280, in train_net
    prefix=prefix)
  File ""experiments/deepim/../../deepim/core/module.py"", line 1026, in fit
    data_batch = interBatchUpdater.forward(data_batch, preds, config)
  File ""experiments/deepim/../../lib/pair_matching/batch_updater_py_multi.py"", line 231, in forward
    rot_type='QUAT')
  File ""experiments/deepim/../../lib/pair_matching/RT_transform.py"", line 34, in calc_RT_delta
    r = mat2quat(Rm_delta)
  File ""experiments/deepim/../../lib/pair_matching/RT_transform.py"", line 459, in mat2quat
    vals, vecs = np.linalg.eigh(K)
  File ""/home/saadhana/.local/lib/python2.7/site-packages/numpy/linalg/linalg.py"", line 1410, in eigh
    w, vt = gufunc(a, signature=signature, extobj=extobj)
  File ""/home/saadhana/.local/lib/python2.7/site-packages/numpy/linalg/linalg.py"", line 95, in _raise_linalgerror_eigenvalues_nonconvergence
    raise LinAlgError(""Eigenvalues did not converge"")
numpy.linalg.linalg.LinAlgError: Eigenvalues did not converge

Looks like the predicted poses are all NaN values.
I printed the rotation and translation predicted:

> [array([[nan, nan, nan, nan],
       [nan, nan, nan, nan],
       [nan, nan, nan, nan],
       [nan, nan, nan, nan]], dtype=float32)] [array([[nan, nan, nan],
       [nan, nan, nan],
       [nan, nan, nan],
       [nan, nan, nan]], dtype=float32)]

Has anybody successfully trained the network for LINEMOD or OCCLUSION datasets?


"
[Question] is it possible to generalise? ,ogroth/shapestacks,2020-10-21 14:38:06,4,,15,726555612,"Hi! 
I am trying to integrate physics intuition during robot training. I am currently using Mujoco and that's why I think this project would be relatively easy to integrate.

How difficult would it be to extend this project to different tasks/physic events?   "
ValueError: select wrong model name:PDGAN,KumapowerLIU/PD-GAN,2022-10-20 03:28:41,0,,7,1415878233,您好，一直提示这个错误，我应该怎么修改呢
how to use ddp not dp ?,KumapowerLIU/PD-GAN,2022-09-22 06:41:11,0,,5,1381918007,
how can i train the model with 8 gpu?,KumapowerLIU/PD-GAN,2022-09-19 09:07:36,0,,4,1377639974,
Kinetics Dataset,chaoyuaw/pytorch-vcii,2020-08-28 01:33:01,0,,13,687645831,"How did you shortlist videos from Kinetics? Are these motion stabilized/ less camera motion and so on?
What criterion did you use?"
H.264 encoder in pytorch framework,chaoyuaw/pytorch-vcii,2020-04-10 16:47:45,0,,12,597984352,"First of all great project. I just want to know did this project implements H.264 encoder in pytorch framework?

Or can u please provide me with any other project which implements H.264 in pytorch will be really helpful."
Computing and storing the motion vectors,chaoyuaw/pytorch-vcii,2019-06-20 05:22:46,0,,10,458352459,"Hi Chao-Yuan,

First of all thanks for sharing this great work! I am trying to train on my own videos so I need to extract motion vectors (MVs) first, which I'm trying to do by calling your CoViAR's ```load```method (a big shout out for that work as well!). I have two questions:

1) The ```load``` method, by default, computes the MV w.r.t. the previous frame, correct? So to get the ""after"" flows as specified in the sample training/eval data, I can call ```load``` for a second time on the video with its frames reversed, correct?

2) By looking at ```read_bmv```, it seems that to convert the ```int32``` MV into ```uint8```, you added 128 to all of its elements and possibly did another clamping to [0, 255]. I'm not very familiar with MV so could you please explain a little bit on why this will not cause loss of information?

Many thanks in advance.
"
Tools to facilitate motion extraction,chaoyuaw/pytorch-vcii,2019-01-21 21:14:16,3,,4,401512367,"Hi, chaoyuaw.

Thanks for your research and also the released code.
In the DATA.md, you said that ""Tools to facilitate motion extraction are coming soon.""
I wonder when these tools will be offered.

Thank you so much,
Hyukryul Yang."
throw errors in windows,chaoyuaw/pytorch-vcii,2018-11-20 14:18:03,3,,3,382683831,"when i run this command:-
train.sh 2
on my git bash it produces following results:-
Namespace(batch_size=16, bits=8, checkpoint_iters=10000, clip=0.5, decoder_fuse_                                                                                                                level=1, distance1=1, distance2=2, encoder_fuse_level=1, eval='data/eval', eval_                                                                                                                batch_size=1, eval_iters=4500, eval_mv='data/eval_mv', fuse_encoder=True, gamma=                                                                                                                0.5, gpus='0', iterations=10, load_iter=None, load_model_name=None, lr=0.00025,                                                                                                                 max_train_iters=100, model_dir='model', num_crops=2, out_dir='output', patch=64,                                                                                                                 save_codes=False, save_model_name='demo', save_out_img=True, schedule='50000,60                                                                                                                000,70000,80000,90000', shrink=2, stack=True, train='data/train', train_mv='data                                                                                                                /train_mv', v_compress=True, warp=True)

Creating loader for data/train...
448 images loaded.
        distance=1/2
Loader for 448 images (28 batches) created.
        Encoder fuse level: 1
        Decoder fuse level: 1
Namespace(batch_size=16, bits=8, checkpoint_iters=10000, clip=0.5, decoder_fuse_level=1, distance1=1, distance2=2, encoder_fuse_level=1, eval='data/eval', eval_batch_size=1, eval_iters=4500, eval_mv='data/eval_mv', fuse_encoder=True, gamma=0.5, gpus='0', iterations=10, load_iter=None, load_model_name=None, lr=0.00025, max_train_iters=100, model_dir='model', num_crops=2, out_dir='output', patch=64, save_codes=False, save_model_name='demo', save_out_img=True, schedule='50000,60000,70000,80000,90000', shrink=2, stack=True, train='data/train', train_mv='data/train_mv', v_compress=True, warp=True)

Creating loader for data/train...
448 images loaded.
        distance=1/2
Loader for 448 images (28 batches) created.
        Encoder fuse level: 1
        Decoder fuse level: 1
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
Traceback (most recent call last):
  File ""train.py"", line 111, in <module>
  File ""C:\Users\rohit\AppData\Local\Programs\Python\Python36\lib\multiprocessing\spawn.py"", line 105, in spawn_main
    for batch, (crops, ctx_frames, _) in enumerate(train_loader):
  File ""C:\Users\rohit\AppData\Local\Programs\Python\Python36\lib\site-packages\torch\utils\data\dataloader.py"", line 501, in __iter__
    exitcode = _main(fd)
  File ""C:\Users\rohit\AppData\Local\Programs\Python\Python36\lib\multiprocessing\spawn.py"", line 114, in _main
    return _DataLoaderIter(self)
  File ""C:\Users\rohit\AppData\Local\Programs\Python\Python36\lib\site-packages\torch\utils\data\dataloader.py"", line 289, in __init__
    prepare(preparation_data)
  File ""C:\Users\rohit\AppData\Local\Programs\Python\Python36\lib\multiprocessing\spawn.py"", line 225, in prepare
    w.start()
  File ""C:\Users\rohit\AppData\Local\Programs\Python\Python36\lib\multiprocessing\process.py"", line 105, in start
    _fixup_main_from_path(data['init_main_from_path'])    self._popen = self._Popen(self)

  File ""C:\Users\rohit\AppData\Local\Programs\Python\Python36\lib\multiprocessing\context.py"", line 223, in _Popen
  File ""C:\Users\rohit\AppData\Local\Programs\Python\Python36\lib\multiprocessing\spawn.py"", line 277, in _fixup_main_from_path
    return _default_context.get_context().Process._Popen(process_obj)
  File ""C:\Users\rohit\AppData\Local\Programs\Python\Python36\lib\multiprocessing\context.py"", line 322, in _Popen
    run_name=""__mp_main__"")
  File ""C:\Users\rohit\AppData\Local\Programs\Python\Python36\lib\runpy.py"", line 263, in run_path
    return Popen(process_obj)
  File ""C:\Users\rohit\AppData\Local\Programs\Python\Python36\lib\multiprocessing\popen_spawn_win32.py"", line 65, in __init__
    reduction.dump(process_obj, to_child)
  File ""C:\Users\rohit\AppData\Local\Programs\Python\Python36\lib\multiprocessing\reduction.py"", line 60, in dump
    pkg_name=pkg_name, script_name=fname)
  File ""C:\Users\rohit\AppData\Local\Programs\Python\Python36\lib\runpy.py"", line 96, in _run_module_code
    ForkingPickler(file, protocol).dump(obj)
BrokenPipeError: [Errno 32] Broken pipe
    mod_name, mod_spec, pkg_name, script_name)
  File ""C:\Users\rohit\AppData\Local\Programs\Python\Python36\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""G:\video compression\pytorch-vcii-master\train.py"", line 111, in <module>
    for batch, (crops, ctx_frames, _) in enumerate(train_loader):
  File ""C:\Users\rohit\AppData\Local\Programs\Python\Python36\lib\site-packages\torch\utils\data\dataloader.py"", line 501, in __iter__
    return _DataLoaderIter(self)
  File ""C:\Users\rohit\AppData\Local\Programs\Python\Python36\lib\site-packages\torch\utils\data\dataloader.py"", line 289, in __init__
    w.start()
  File ""C:\Users\rohit\AppData\Local\Programs\Python\Python36\lib\multiprocessing\process.py"", line 105, in start
    self._popen = self._Popen(self)
  File ""C:\Users\rohit\AppData\Local\Programs\Python\Python36\lib\multiprocessing\context.py"", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File ""C:\Users\rohit\AppData\Local\Programs\Python\Python36\lib\multiprocessing\context.py"", line 322, in _Popen
    return Popen(process_obj)
  File ""C:\Users\rohit\AppData\Local\Programs\Python\Python36\lib\multiprocessing\popen_spawn_win32.py"", line 33, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File ""C:\Users\rohit\AppData\Local\Programs\Python\Python36\lib\multiprocessing\spawn.py"", line 143, in get_preparation_data
    _check_not_importing_main()
  File ""C:\Users\rohit\AppData\Local\Programs\Python\Python36\lib\multiprocessing\spawn.py"", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError:
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The ""freeze_support()"" line can be omitted if the program
        is not going to be frozen to produce an executable.


I tried running train.py with all the necessary parameters like train,eval,distance1, distance2. but still no lead."
Granularity,irwinherrmann/stochastic-gates,2022-05-02 16:32:50,0,,1,1223092774,What is the purpose of having granularity as the input to model design?
When will release quant? Thanks,open-mmlab/mmrazor,2022-11-03 07:35:01,1,enhancement,343,1434228812,
How to make this code base support mmrotate model? I want to prune rotated object detection model.,open-mmlab/mmrazor,2022-10-21 13:28:04,0,,335,1418343231,"### Checklist

- I have searched related issues but cannot get the expected help.
- I have read related documents and don't know what to do.

### Describe the question you meet

      I want to prune rotated object detection model. The current newest version of mmrazor doesn't support this kind of task. I want to know how to make the mmrazor support mmrotate task. Is there any instruction for this kind modify? For example, if I want to use AutoSlim method to prune oriented R-CNN model, then what should I do? 

### Post related information

1. The output of `pip list | grep ""mmcv\|mmrazor\|^torch""`
   \[here\]
2. Your config file if you modified it or created a new one.

```python
mmcv-full            1.6.0
mmrazor              0.3.1    
mmrotate             0.3.2   
```

3. Your train log file if you meet the problem during training.
   \[here\]
4. Other code you modified in the `mmrazor` folder.
   \[here\]
"
Posec3d KD,open-mmlab/mmrazor,2022-10-13 08:36:04,0,,318,1407381291,"Thanks for great repository....!!
1-How can i apply KD on PYSKL(posec3d model)?
2-If I follow the step mentioned in [[customize kd algorithms](https://github.com/open-mmlab/mmrazor/blob/master/docs/en/tutorials/Tutorial_6_customize_kd_algorithms.md)] it would help me?
3-If not kindly guide me about how can i use KD on POsec3d model.


"
[wontfix] test lark bot,open-mmlab/mmrazor,2022-10-11 14:14:20,2,wontfix,317,1404720659,"### Checklist

- I have searched related issues but cannot get the expected help.
- I have read related documents and don't know what to do.

### Describe the question you meet

\[here\]

### Post related information

1. The output of `pip list | grep ""mmcv\|mmrazor\|^torch""`
   \[here\]
2. Your config file if you modified it or created a new one.

```python
[here]
```

3. Your train log file if you meet the problem during training.
   \[here\]
4. Other code you modified in the `mmrazor` folder.
   \[here\]
"
Question on AutoSlim on yoloxnano,open-mmlab/mmrazor,2022-10-09 08:15:07,2,community discussion,309,1402194488,"### Checklist

- I have searched related issues but cannot get the expected help.
- I have read related documents and don't know what to do.

### Describe the question you meet

I tried to use autoSlim to prune yoloxnano. The training process can be ran successfully. However, I found the mAP does not converge. 

### My config below:

```
_base_ = [
    '../../../_base_/schedules/mmdet/schedule_1x.py',
    '../../../_base_/mmdet_runtime.py'
]
img_scale = (640, 640)
test_img_scale = (416, 416)
NC = 1 
CLASSES = ('person', )
BATCHSIZE_PER_GPU = 32
WORKERS = 8
dataset_type = 'CocoDataset'
train_path = ""my_data_train.json""
val_path = ""my_data_val.json""


# Run Time Setting 
max_epochs = 300
num_last_epochs = 15
resume_from = None

# For evaluation
eval_interval = 5
eval_metric = ""bbox"" # not 'bbox' | mAP
save_best_metric = ""auto"" # not 'auto' | mAP

# model settings
model = dict(
    type='mmdet.YOLOX',
    input_size=img_scale,
    random_size_range=(10, 20),
    random_size_interval=10,
    backbone=dict(
        type='MobileNetV2',
        widen_factor=0.5, 
        out_indices=(2, 4, 6),
        act_cfg=dict(type='ReLU6')),
    neck=dict(
        type='YOLOXPAFPN',
        in_channels=[16, 48, 160],
        out_channels=64,
        num_csp_blocks=1, 
        use_depthwise=True),
    bbox_head=dict(
        type='YOLOXHead', 
        num_classes=NC, 
        in_channels=64, 
        feat_channels=64, 
        use_depthwise=True),
    train_cfg=dict(assigner=dict(type='SimOTAAssigner', center_radius=2.5)),
    # In order to align the source code, the threshold of the val phase is
    # 0.01, and the threshold of the test phase is 0.001.
    test_cfg=dict(score_thr=0.01, nms=dict(type='nms', iou_threshold=0.65)))

# Training pipeline 
train_pipeline = [
    dict(type='Mosaic', img_scale=img_scale, pad_val=114.0),
    dict(
        type='RandomAffine',
        scaling_ratio_range=(0.5, 1.5),
        border=(-img_scale[0] // 2, -img_scale[1] // 2)),
    dict(type='YOLOXHSVRandomAug'),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='Resize', img_scale=img_scale, keep_ratio=True),
    dict(
        type='Pad',
        pad_to_square=True,
        pad_val=dict(img=(114.0, 114.0, 114.0))),
    dict(type='FilterAnnotations', min_gt_bbox_wh=(1, 1), keep_empty=False),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]

# Training dataset 
train_dataset = dict(
    type='MultiImageMixDataset',
    dataset=dict(
        type=dataset_type, 
        ann_file=train_path,
        classes=CLASSES, 
        img_prefix=None,
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True)
        ],
        filter_empty_gt=False,
    ),
    pipeline=train_pipeline)

# Test pipeline 
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=test_img_scale,
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Pad',
                pad_to_square=True,
                pad_val=dict(img=(114.0, 114.0, 114.0))),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img'])
        ])
]

data = dict(
    samples_per_gpu=BATCHSIZE_PER_GPU,
    workers_per_gpu=WORKERS,
    persistent_workers=True,
    train=train_dataset,
    val=dict(
        type=dataset_type, 
        ann_file=val_path,
        classes=CLASSES, 
        img_prefix=None,
        pipeline=test_pipeline),
    test=dict(
        type=dataset_type, 
        ann_file=val_path,
        classes=CLASSES, 
        img_prefix=None,
        pipeline=test_pipeline))


# Algorithm Setting 
algorithm = dict(
    type='AutoSlim',
    num_sample_training=4, 
    architecture=dict(type='MMDetArchitecture', model=model),
    pruner=dict(
        type='RatioPruner',
        except_start_keys=[
            ""bbox_head.multi_level_conv_cls.0"",
            ""bbox_head.multi_level_conv_cls.1"",
            ""bbox_head.multi_level_conv_cls.2"",
            ""bbox_head.multi_level_conv_reg.0"",
            ""bbox_head.multi_level_conv_reg.1"",
            ""bbox_head.multi_level_conv_reg.2"",
            ""bbox_head.multi_level_conv_obj.0"",
            ""bbox_head.multi_level_conv_obj.1"",
            ""bbox_head.multi_level_conv_obj.2""
        ], 
        ratios=(2 / 12, 3 / 12, 4 / 12, 5 / 12, 6 / 12, 7 / 12, 8 / 12, 9 / 12, 10 / 12, 11 / 12, 1.0)),
    retraining=False,
    bn_training_mode=True,
    input_shape=None)


# default 8 gpu
optimizer = dict(
    type='SGD',
    lr=0.02,
    momentum=0.9,
    weight_decay=5e-4,
    nesterov=True,
    paramwise_cfg=dict(norm_decay_mult=0., bias_decay_mult=0.))
optimizer_config = None

# learning policy
lr_config = dict(
    _delete_=True,
    policy='YOLOX',
    warmup='exp',
    by_epoch=False,
    warmup_by_epoch=True,
    warmup_ratio=1,
    warmup_iters=5,  # 5 epoch
    num_last_epochs=num_last_epochs,
    min_lr_ratio=0.05)


runner = dict(type='EpochBasedRunner', max_epochs=max_epochs)

custom_hooks = [
    dict(
        type='YOLOXModeSwitchHook',
        num_last_epochs=num_last_epochs,
        priority=48),
    dict(
        type='SyncNormHook',
        num_last_epochs=num_last_epochs,
        interval=eval_interval,
        priority=48),
    dict(
        type='ExpMomentumEMAHook',
        resume_from=resume_from,
        momentum=0.0001,
        priority=49)
]
# How many epochs saving one model, -1 we never save the intermediate checkpoints 
checkpoint_config = dict(interval=-1)
evaluation = dict(
    save_best=save_best_metric,
    # The evaluation interval is 'interval' when running epoch is
    # less than ‘max_epochs - num_last_epochs’.
    # The evaluation interval is 1 when running epoch is greater than
    # or equal to ‘max_epochs - num_last_epochs’.
    interval=eval_interval,
    dynamic_intervals=[(max_epochs - num_last_epochs, 1)],
    metric=eval_metric)
log_config = dict(interval=50)

# NOTE: `auto_scale_lr` is for automatically scaling LR,
# USER SHOULD NOT CHANGE ITS VALUES.
# base_batch_size = (8 GPUs) x (8 samples per GPU)
auto_scale_lr = dict(base_batch_size=64)


use_ddp_wrapper = True

```

### My effort to solve the problem
* First, I try to use mmrazor training pipeline to train the max_model only, which means that I set self.pruner.set_max_channel() only in def train_step() method. And the model converges as normal. 
* Then I try to train only the max and min model in one iteration, and evaluate the max model only, I found that the model can not converge. 
* I found that in mmrazor, you did not apply out_mask for BN layers, which does not align with the original implementation of AutoSlim, and you only apply mask for Conv and Linear layers. My question is that: is it missing or not necessary to apply out_mask for BN layers? I think this will make the update of BN layers incorrect. Can it be the reason why the training of yoloxnano does not converge? 


### My question:
* Any suggestion on using AutoSlim on yoloxnano?
* Regarding the third opinion above, any insight from your perspective? Is it simple a BUG or you got any trick which i did not find? 

Best regards
"
"When will the quantization algorithm be released, and is it expected to be applicable to the model of transformer structure?",open-mmlab/mmrazor,2022-09-24 05:52:03,4,,300,1384538815,"### Checklist

- I have searched related issues but cannot get the expected help.
- I have read related documents and don't know what to do.

### Describe the question you meet

\[here\]

### Post related information

1. The output of `pip list | grep ""mmcv\|mmrazor\|^torch""`
   \[here\]
2. Your config file if you modified it or created a new one.

```python
[here]
```

3. Your train log file if you meet the problem during training.
   \[here\]
4. Other code you modified in the `mmrazor` folder.
   \[here\]
"
How to get the network structure after slimming?,open-mmlab/mmrazor,2022-09-21 10:26:56,0,usage,294,1380697002,"### Checklist

- I have searched related issues but cannot get the expected help.
- I have read related documents and don't know what to do.

### Describe the question you meet

I used the official configuration file to do the autoslim test on the cifar100 dataset. But after using split_checkpoint.py to split the retrained weight file, the size of the obtained weight file is the same. And when doing the test, it will report the error 'The model and loaded state dict do not match exactly' and 'AttributeError: 'MMDataParallel' object has no attribute 'CLASSES''.

### Post related information

1. The output of `pip list | grep ""mmcv\|mmrazor\|^torch""`
mmcv-full          1.5.0
torch              1.10.0
torchsummary       1.5.1
torchvision        0.11.1
mmrazor   0.3.1
2. Your config file if you modified it or created a new one.

```python
# autoslim_mbv2_supernet_8xb32_in100_cifar100_test.py
_base_ = [
    '/home/wenjie/PycharmProjects/mmrazor_demo/configs/_base_/datasets/mmcls/cifar100_bs32_autoslim.py',
    '/home/wenjie/PycharmProjects/mmrazor_demo/configs/_base_/schedules/mmcls/cifar100_bs2048_autoslim.py',
    '/home/wenjie/PycharmProjects/mmrazor_demo/configs/_base_/mmcls_runtime.py'
]

model = dict(
    type='mmcls.ImageClassifier',
    backbone=dict(type='MobileNetV2', widen_factor=1.5),
    neck=dict(type='GlobalAveragePooling'),
    head=dict(
        type='LinearClsHead',
        num_classes=100,
        in_channels=1920,
        loss=dict(
            type='LabelSmoothLoss',
            mode='original',
            label_smooth_val=0.1,
            loss_weight=1.0),
        topk=(1, 5),
    ))

algorithm = dict(
    type='AutoSlim',
    architecture=dict(type='MMClsArchitecture', model=model),
    distiller=dict(
        type='SelfDistiller',
        components=[
            dict(
                student_module='head.fc',
                teacher_module='head.fc',
                losses=[
                    dict(
                        type='KLDivergence',
                        name='loss_kd',
                        tau=1,
                        loss_weight=1,
                    )
                ]),
        ]),
    pruner=dict(
        type='RatioPruner',
        ratios=(2 / 12, 3 / 12, 4 / 12, 5 / 12, 6 / 12, 7 / 12, 8 / 12, 9 / 12,
                10 / 12, 11 / 12, 1.0)),
    retraining=False,
    bn_training_mode=True,
    input_shape=None)

runner = dict(type='EpochBasedRunner', max_epochs=50)

use_ddp_wrapper = True

# autoslim_mbv2_search_8xb32_in100_cifar100_test.py
_base_ = [
    './autoslim_mbv2_supernet_8xb32_in100_cifar100_test.py',
]

algorithm = dict(distiller=None, input_shape=(1, 32, 32))

searcher = dict(
    type='GreedySearcher',
    target_flops=[14000000, 13000000, 12000000],
    max_channel_bins=12,
    metrics='accuracy')

data = dict(samples_per_gpu=1024, workers_per_gpu=4)

# autoslim_mbv2_subnet_8xb32_in100_cifar100_test.py
_base_ = [
    './autoslim_mbv2_supernet_8xb32_in100_cifar100_test.py',
]

model = dict(
    head=dict(
        loss=dict(
            type='LabelSmoothLoss',
            mode='original',
            label_smooth_val=0.1,
            loss_weight=1.0)))

# FIXME: you may replace this with the channel_cfg searched by yourself
channel_cfg = [
    '/home/wenjie/PycharmProjects/mmrazor_demo/autoslim_test/search/subnet_13978536.yaml',  # noqa: E501
    '/home/wenjie/PycharmProjects/mmrazor_demo/autoslim_test/search/subnet_12989328.yaml',  # noqa: E501
    '/home/wenjie/PycharmProjects/mmrazor_demo/autoslim_test/search/subnet_11942370.yaml',  # noqa: E501
]

algorithm = dict(
    architecture=dict(type='MMClsArchitecture', model=model),
    distiller=None,
    retraining=True,
    bn_training_mode=False,
    channel_cfg=channel_cfg)

runner = dict(type='EpochBasedRunner', max_epochs=300)

find_unused_parameters = True

# cifar100_bs32_autoslim.py
# dataset settings
dataset_type = 'CIFAR100'
img_norm_cfg = dict(
    mean=[129.304, 124.070, 112.434],
    std=[68.170, 65.392, 70.418],
    to_rgb=False)
train_pipeline = [
    dict(type='RandomCrop', size=32, padding=4),
    dict(type='RandomFlip', flip_prob=0.5, direction='horizontal'),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='ImageToTensor', keys=['img']),
    dict(type='ToTensor', keys=['gt_label']),
    dict(type='Collect', keys=['img', 'gt_label'])
]
test_pipeline = [
    dict(type='Normalize', **img_norm_cfg),
    dict(type='ImageToTensor', keys=['img']),
    dict(type='Collect', keys=['img'])
]
data = dict(
    samples_per_gpu=64,
    workers_per_gpu=2,
    train=dict(
        type=dataset_type,
        data_prefix='data/cifar100',
        pipeline=train_pipeline),
    val=dict(
        type=dataset_type,
        data_prefix='data/cifar100',
        pipeline=test_pipeline,
        test_mode=True),
    test=dict(
        type=dataset_type,
        data_prefix='data/cifar100',
        pipeline=test_pipeline,
        test_mode=True))
evaluation = dict(interval=1, metric='accuracy')

# cifar100_bs2048_autoslim.py
# optimizer
paramwise_cfg = dict(
    bias_decay_mult=0.0, norm_decay_mult=0.0, dwconv_decay_mult=0.0)
optimizer = dict(
    type='SGD',
    lr=0.1,
    momentum=0.9,
    nesterov=True,
    weight_decay=0.0001,
    paramwise_cfg=paramwise_cfg)

optimizer_config = None

# learning policy
lr_config = dict(policy='poly', power=1.0, min_lr=0.0, by_epoch=False)
runner = dict(type='EpochBasedRunner', max_epochs=300)

```

3. Your train log file if you meet the problem during training.
   \[here\]
4. Other code you modified in the `mmrazor` folder.
   \[here\]
"
"[Bug] Got ""'NoneType' object has no attribute 'next_functions'"" error in running autoslim",open-mmlab/mmrazor,2022-09-16 08:40:39,2,bug,290,1375626531,"### Describe the bug

I wanna train the supernet of autoslim from scratch. And I got bug informations: AttributeError: 'NoneType' object has no attribute 'next_functions'.

It seems like the recursion of ` conv_grad_fn.next_functions[0][0]` do not perform as we expected. 

### To Reproduce

I run command in [configs/pruning/autoslim/readme.md](https://github.com/open-mmlab/mmrazor/tree/master/configs/pruning/autoslim)
``` shell
python ./tools/mmcls/train_mmcls.py \
  configs/pruning/autoslim/autoslim_mbv2_supernet_8xb256_in1k.py \
  --work-dir <your_work_dir>

```

### Post related information

I use the latest mmrazor-master repo.

1. The output of `pip list | grep ""mmcv\|mm*\|^torch""`
``` shell 
mmcls                  0.23.2
mmcv-full              1.6.0
mmdet                  2.25.1
mmsegmentation         0.28.0
torch                  1.12.1
torchvision            0.13.1
```

2. Your train log file if you meet the problem during training.
``` shell
Traceback (most recent call last):
  File ""/root/miniconda3/envs/wyh_graduate/lib/python3.10/site-packages/mmcv/utils/registry.py"", line 69, in build_from_cfg
    return obj_cls(**args)
  File ""/root/compression/mmrazor/mmrazor/models/algorithms/autoslim.py"", line 42, in __init__
    super(AutoSlim, self).__init__(**kwargs)
  File ""/root/compression/mmrazor/mmrazor/models/algorithms/base.py"", line 56, in __init__
    self._init_pruner(pruner)
  File ""/root/compression/mmrazor/mmrazor/models/algorithms/autoslim.py"", line 69, in _init_pruner
    pseudo_pruner.prepare_from_supernet(pseudo_architecture)
  File ""/root/compression/mmrazor/mmrazor/models/pruners/ratio_pruning.py"", line 49, in prepare_from_supernet
    super(RatioPruner, self).prepare_from_supernet(supernet)
  File ""/root/compression/mmrazor/mmrazor/models/pruners/structure_pruning.py"", line 185, in prepare_from_supernet
    self.trace_norm_conv_links(pseudo_loss.grad_fn, module2name,
  File ""/root/compression/mmrazor/mmrazor/models/pruners/structure_pruning.py"", line 741, in trace_norm_conv_links
    self.trace_norm_conv_links(parent, module2name,
  File ""/root/compression/mmrazor/mmrazor/models/pruners/structure_pruning.py"", line 741, in trace_norm_conv_links
    self.trace_norm_conv_links(parent, module2name,
  File ""/root/compression/mmrazor/mmrazor/models/pruners/structure_pruning.py"", line 741, in trace_norm_conv_links
    self.trace_norm_conv_links(parent, module2name,
  [Previous line repeated 4 more times]
  File ""/root/compression/mmrazor/mmrazor/models/pruners/structure_pruning.py"", line 710, in trace_norm_conv_links
    conv_grad_fn = conv_grad_fn.next_functions[0][0]
AttributeError: 'NoneType' object has no attribute 'next_functions'

```

### Additional context

Add any other context about the problem here.

\[here\]
"
 The effect was not improved after distillation ,open-mmlab/mmrazor,2022-09-03 03:58:26,3,community discussion,276,1360747616,"### Describe the question you meet

I use the CWD method,When resnet50 is used to distill resnet18, the training accuracy of the teacher's network is 80%, but the network accuracy after distillation is only 46%. What is the reason

### Post related information

1. The output of `pip list | grep ""mmcv\|mmrazor\|^torch""`
   \[here\]
2. Your config file if you modified it or created a new one.

```python
_base_ = [
    '../../_base_/datasets/mmdet/coco_instance.py',
    '../../_base_/schedules/mmdet/schedule_1x.py',
    '../../_base_/mmdet_runtime.py'
]

# model settings
student = dict(
    type='mmdet.PointRend',
    backbone=dict(
        type='ResNet',
        depth=18,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=False),
        norm_eval=True,
        style='caffe',
        init_cfg=dict(
            type='Pretrained',
            checkpoint='https://download.openmmlab.com/mmclassification/v0/resnet/resnet18_b16x8_cifar10_20210528-bd6371c8.pth')
    ),
    neck=dict(
        type='FPN',
        in_channels=[64, 128, 256, 512],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[.0, .0, .0, .0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    roi_head=dict(
        type='PointRendRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=4,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0., 0., 0., 0.],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
        mask_roi_extractor=dict(
            type='GenericRoIExtractor',
            aggregation='concat',
            roi_layer=dict(
                type='SimpleRoIAlign', output_size=14),
            out_channels=256,
            featmap_strides=[4]),
        mask_head=dict(
            type='CoarseMaskHead',
            num_fcs=2,
            in_channels=256,
            conv_out_channels=256,
            fc_out_channels=1024,
            num_classes=4,
            loss_mask=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),
        point_head=dict(
            type='MaskPointHead',
            num_fcs=3,
            in_channels=256,
            fc_channels=256,
            num_classes=4,
            coarse_pred_each_layer=True,
            loss_point=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),
    # model training and testing settings
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            mask_size=7,
            num_points=14 * 14,
            oversample_ratio=3,
            importance_sample_ratio=0.75,
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,  # 在nms之前保留的的得分最高的proposal数量
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.05,
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100,
            mask_thr_binary=0.5,
            subdivision_steps=5,
            subdivision_num_points=28 * 28,
            scale_factor=2)))

checkpoint = '/media/jidong/code/xuhao/mmdetection/load/point_rend_r50_caffe_fpn_mstrain_3x_coco-e0ebb6b7.pth'

teacher = dict(
    type='mmdet.PointRend',
    init_cfg=dict(type='Pretrained', checkpoint=checkpoint),
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=False),
        norm_eval=True,
        style='caffe',
        init_cfg=dict(
            type='Pretrained',
            checkpoint='open-mmlab://detectron2/resnet50_caffe')),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[.0, .0, .0, .0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    roi_head=dict(
        type='PointRendRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=4,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0., 0., 0., 0.],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
        mask_roi_extractor=dict(
            type='GenericRoIExtractor',
            aggregation='concat',
            roi_layer=dict(
                type='SimpleRoIAlign', output_size=14),
            out_channels=256,
            featmap_strides=[4]),
        mask_head=dict(
            type='CoarseMaskHead',
            num_fcs=2,
            in_channels=256,
            conv_out_channels=256,
            fc_out_channels=1024,
            num_classes=4,
            loss_mask=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),
        point_head=dict(
            type='MaskPointHead',
            num_fcs=3,
            in_channels=256,
            fc_channels=256,
            num_classes=4,
            coarse_pred_each_layer=True,
            loss_point=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),
    # model training and testing settings
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            mask_size=7,
            num_points=14 * 14,
            oversample_ratio=3,
            importance_sample_ratio=0.75,
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,  # 在nms之前保留的的得分最高的proposal数量
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.05,
            # nms=dict(type='soft_nms', iou_threshold=0.5, min_score=0.05),
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100,
            mask_thr_binary=0.5,
            subdivision_steps=5,
            subdivision_num_points=28 * 28,
            scale_factor=2)))

algorithm = dict(
    type='GeneralDistill',
    architecture=dict(
        type='MMDetArchitecture',
        model=student,
    ),
    distiller=dict(
        type='SingleTeacherDistiller',
        teacher=teacher,
        teacher_trainable=False,
        components=[
            dict(
                student_module='rpn_head.rpn_reg',
                teacher_module='rpn_head.rpn_reg',
                losses=[
                    dict(
                        type='ChannelWiseDivergence',
                        name='loss_cwd_point_rend',
                        tau=1,
                        loss_weight=5,
                    )
                ])
        ]),
)

find_unused_parameters = True

# python tools/mmdet/train_mmdet.py configs/distill/cwd/pointrend.py --work-dir pointrend-18 --cfg-options algorithm.distiller.teacher.init_cfg.type=Pretrained algorithm.distiller.teacher.init_cfg.checkpoint=https://download.openmmlab.com/mmdetection/v2.0/point_rend/point_rend_r50_caffe_fpn_mstrain_3x_coco/point_rend_r50_caffe_fpn_mstrain_3x_coco-e0ebb6b7.pth
```

3. Your train log file if you meet the problem during training.
    Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.311
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.460
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.350
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.613
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.615
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.615
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.615
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.615
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = -1.000

2022-09-02 09:53:28,087 - mmdet - INFO - Evaluating segm...
2022-09-02 09:53:30,272 - mmdet - INFO - 
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.372
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.467
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.417
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.743
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.746
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.746
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.746
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.746
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = -1.000
4. Other code you modified in the `mmrazor` folder.
   \[here\]
"
When will the QAT be released?,open-mmlab/mmrazor,2022-08-29 08:21:23,1,,249,1353894777,"### Checklist

- I have searched related issues but cannot get the expected help.
- I have read related documents and don't know what to do.

### Describe the question you meet

I wonder when the QAT will be released?

Thank you.
"
[Feature] MMOCR API,open-mmlab/mmrazor,2022-08-24 08:42:03,1,enhancement,245,1349082186,"### Describe the feature

I see mmrazor has already added mmdet, mmcls and mmseg apis, so could you add the mmocr api? 

"
Why can't I get the accuracy of CWD？,open-mmlab/mmrazor,2022-08-19 01:27:14,1,,238,1343816022,"![image](https://user-images.githubusercontent.com/83870362/185522665-f0d12f28-d506-4858-82c8-edf363be63cb.png)
"
How to train supernet on my own dataset?,open-mmlab/mmrazor,2022-08-17 08:29:05,6,,236,1341378965,"### Checklist

- I have searched related issues but cannot get the expected help.
- I have read related documents and don't know what to do.

### Describe the question you meet

Most of the existing datasets that can train supernet are ImageNet and CIFAR.What configs files should be modified in order to replace them with other datasets?

### Post related information

![f2118c77321dd07a0b744a79f677508](https://user-images.githubusercontent.com/77597292/185071282-904187c8-5d24-4d3e-af55-333eb0bb627b.png)

"
mmRazor workflow,open-mmlab/mmrazor,2022-08-16 08:11:35,1,usage,234,1339977670,"### Checklist

- I have searched related issues but cannot get the expected help.
- I have read related documents and don't know what to do.

### Describe the question you meet

I currently have a number of pre-trained mmdetection architectures (maskrcnn, yolact, solo) re-trained on my own custom segmentation task. They work well, but I am interested in reducing these models as much as possible and am interested in using a knowledge distillation technique. 

Can I achieve this using mmrazor? If yes, how do I do that? What is the workflow? 

Some specific questions:
1. Do I need to retrain a teacher model using mmrazor or can I use my already trained models trained using the mmdetection?
2. Do I need to extend the KD algorithms such that I can use them for my specific task and chosen architectures or does it just support any mmdetection architecture and any segmentation task in general?

### Post related information

n/a
"
[Feature] kd,open-mmlab/mmrazor,2022-08-12 10:28:13,0,enhancement,229,1337030540,Have any useful kd algo for yolox?
"[Bug] Checkpoint fails with ""with_cp=True"" in mmdet swin-transformer distillation. ",open-mmlab/mmrazor,2022-08-08 21:02:26,3,bug#enhancement,224,1332393553,"### Describe the bug

A clear and concise description of what the bug is.

\[Hi Developers, Thanks for your efforts.  I was trying to distill swint-ransformers and use the (with_cp=True) to train with larger batch size. But it turns out some errors as attached below.\]

### To Reproduce

```shell
[python ./tools/mmdet/train_mmdet.py ./configs/distill/cwd/my_config.py]
```

### Post related information

1. The output of `pip list | grep ""mmcv\|mmrazor\|^torch""`
   \[here\]
2. Your config file if you modified it or created a new one.

```python
[_base_ = [
    '../../_base_/datasets/mmdet/coco_detection.py',
    '../../_base_/schedules/mmdet/schedule_1x.py',
    '../../_base_/mmdet_runtime.py'
]

pretrained = 'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_small_patch4_window7_224.pth'  # noqa

student = dict(
    type='mmdet.MaskRCNN',
        backbone=dict(
        type='SwinTransformer',
        embed_dims=96,
        depths=[2, 2, 18, 2],
        num_heads=[3, 6, 12, 24],
        window_size=7,
        mlp_ratio=4,
        qkv_bias=True,
        qk_scale=None,
        drop_rate=0.,
        attn_drop_rate=0.,
        drop_path_rate=0.2,
        patch_norm=True,
        out_indices=(0, 1, 2, 3),
        with_cp=True,
        convert_weights=True,
        init_cfg=dict(type='Pretrained', checkpoint=pretrained)),
    neck=dict(
        type='FPN',
        in_channels=[96, 192, 384, 768],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[.0, .0, .0, .0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    roi_head=dict(
        type='StandardRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=80,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0., 0., 0., 0.],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
        ),
    # model training and testing settings
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            mask_size=28,
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.05,
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100,
            mask_thr_binary=0.5)))

teacher_ckpt = 'https://github.com/Mrxiaoyuer/coco_checkpoints/releases/download/v0.0.2/swinl_epoch_24.pth'  # noqa

teacher = dict(
    type='mmdet.MaskRCNN',
    init_cfg=dict(type='Pretrained', checkpoint=teacher_ckpt),
    backbone=dict(
        type='SwinTransformer',
        embed_dims=192,
        depths=[2, 2, 18, 2],
        num_heads=[6, 12, 24, 48],
        window_size=7,
        mlp_ratio=4,
        qkv_bias=True,
        qk_scale=None,
        drop_rate=0.,
        attn_drop_rate=0.,
        drop_path_rate=0.2,
        patch_norm=True,
        out_indices=(0, 1, 2, 3),
        with_cp=False,
        convert_weights=True,
        init_cfg=None),
    neck=dict(
        type='FPN',
        in_channels=[192, 384, 768, 1536],
        out_channels=256,
        start_level=0,
        add_extra_convs='on_output',
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[.0, .0, .0, .0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    roi_head=dict(
        type='StandardRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=80,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0., 0., 0., 0.],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
        ),
    # model training and testing settings
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            mask_size=28,
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.05,
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100,
            mask_thr_binary=0.5)))


# algorithm setting
algorithm = dict(
    type='GeneralDistill',
    architecture=dict(
        type='MMDetArchitecture',
        model=student,
    ),
    distiller=dict(
        type='SingleTeacherDistiller',
        teacher=teacher,
        teacher_trainable=False,
        components=[
            dict(
                student_module='neck.fpn_convs.3.conv',
                teacher_module='neck.fpn_convs.3.conv',
                losses=[
                    dict(
                        type='ChannelWiseDivergence',
                        name='loss_cwd_cls_head',
                        tau=1,
                        loss_weight=5,
                    )
                ])
        ]),
)

find_unused_parameters = True

# optimizer
optimizer = dict(
    _delete_=True,
    type='AdamW',
    lr=0.0001,
    betas=(0.9, 0.999),
    weight_decay=0.05,
    paramwise_cfg=dict(
        custom_keys={
            'absolute_pos_embed': dict(decay_mult=0.),
            'relative_position_bias_table': dict(decay_mult=0.),
            'norm': dict(decay_mult=0.)
        }))
lr_config = dict(warmup_iters=1000, step=[8, 11])
runner = dict(max_epochs=12)

```

3. Your train log file if you meet the problem during training.
   \[loading annotations into memory...
Done (t=51.46s)
creating index...
Done (t=51.98s)
creating index...
Done (t=51.93s)
creating index...
Done (t=51.97s)
creating index...
index created!
index created!
index created!
index created!
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=1.01s)
creating index...
Done (t=1.24s)
creating index...
Done (t=1.25s)
creating index...
Done (t=1.02s)
creating index...
index created!
index created!
index created!
index created!
2022-08-08 20:44:14,703 - mmdet - INFO - Start running, host: root@da8f74b40bf146bf925ac3a4349ccef80000Z0, work_dir: /mnt/azureml/cr/j/89a4752b1bff40f9951561de40f847a6/cap/data-capability/wd/INPUT_3324fea54867408dbec17da2dfe673f2_data/fuxun/output/fuxun-mmrazor-cwd-swin-l-s_1x_adamw_2xbs/cwd_cls_head_swin-l-s_1x_coco_adamw_2xbs
2022-08-08 20:44:14,704 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) StepLrUpdaterHook                  
(ABOVE_NORMAL) Fp16OptimizerHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) StepLrUpdaterHook                  
(NORMAL      ) NumClassCheckHook                  
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) StepLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) Fp16OptimizerHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(NORMAL      ) NumClassCheckHook                  
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2022-08-08 20:44:14,704 - mmdet - INFO - workflow: [('train', 1)], max: 12 epochs
2022-08-08 20:44:14,710 - mmdet - INFO - Checkpoints will be saved to /mnt/azureml/cr/j/89a4752b1bff40f9951561de40f847a6/cap/data-capability/wd/INPUT_3324fea54867408dbec17da2dfe673f2_data/fuxun/output/fuxun-mmrazor-cwd-swin-l-s_1x_adamw_2xbs/cwd_cls_head_swin-l-s_1x_coco_adamw_2xbs by HardDiskBackend.
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
Traceback (most recent call last):
  File ""tools/mmdet/train_mmdet.py"", line 230, in <module>
    main()
  File ""tools/mmdet/train_mmdet.py"", line 226, in main
    meta=meta)
  File ""/tmp/code/mmrazor/apis/mmdet/train.py"", line 206, in train_mmdet_model
    runner.run(data_loader, cfg.workflow)
  File ""/opt/conda/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 127, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""/opt/conda/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 51, in train
    self.call_hook('after_train_iter')
  File ""/opt/conda/lib/python3.7/site-packages/mmcv/runner/base_runner.py"", line 309, in call_hook
    getattr(hook, fn_name)(self)
  File ""/opt/conda/lib/python3.7/site-packages/mmcv/runner/hooks/optimizer.py"", line 272, in after_train_iter
    self.loss_scaler.scale(runner.outputs['loss']).backward()
  File ""/opt/conda/lib/python3.7/site-packages/torch/_tensor.py"", line 255, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File ""/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py"", line 149, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
  File ""/opt/conda/lib/python3.7/site-packages/torch/autograd/function.py"", line 87, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore[attr-defined]
  File ""/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py"", line 138, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File ""/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py"", line 149, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
Parameter at index 323 has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration. You can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print parameter names for further debugging.\]
4. Other code you modified in the `mmrazor` folder.
   \[here\]

### Additional context

Add any other context about the problem here.

\[(1) I tried add (with_cp=True) in the r101_r50 cwd distillation and it works fine. So this is probably related to the swin transformer structure
(2) I also tried (with_cp=True) by training swin-transformer alone in the mmdetection repo and it works fine. \]
"
When is the quantitative algorithm coming out? Does it have anything to do with MQBench?,open-mmlab/mmrazor,2022-07-27 12:48:37,1,,208,1319516646,"When is the quantitative algorithm coming out?
I noticed SenseTime's MQBench, is there any difference and connection between the coming quantization algorithm and MQBench?"
Do MMrazor's existing pruning and NAS algorithms support compression of models with Transformer structures?,open-mmlab/mmrazor,2022-07-19 00:40:55,2,,201,1308835881,"### Checklist

- I have searched related issues but cannot get the expected help.
- I have read related documents and don't know what to do.

### Describe the question you meet

\[here\]

### Post related information

1. The output of `pip list | grep ""mmcv\|mmrazor\|^torch""`
   \[here\]
2. Your config file if you modified it or created a new one.

```python
[here]
```

3. Your train log file if you meet the problem during training.
   \[here\]
4. Other code you modified in the `mmrazor` folder.
   \[here\]
"
"When performing knowledge distillation (e.g., using CWD algorithm), how to determine the distillation modules corresponding to the two models? ",open-mmlab/mmrazor,2022-07-11 08:41:05,5,,197,1300393171,"For example, the teacher model is faster rcnn and the student model is yolo v3.Where can I find out what modules the models have? When I write a random module, I get a key error."
test,open-mmlab/mmrazor,2022-07-05 06:38:35,3,Installation,196,1293855886,"在测试过程中出现了
![Uploading 27A90B074AD305E371519B3DDBA8DB4F.jpg…]()
的问题，不知道如何进行解决"
[Bug] AutoSlim throw error when the distiller is a DDP,open-mmlab/mmrazor,2022-06-28 07:39:56,1,bug,194,1286926515,"### Describe the bug

When the distiller contain parameter, it woudl be converted to DDP. After running autoslim, it would throw the error like

```bash
DDP does have `exec_teacher_forward`
```

https://github.com/open-mmlab/mmrazor/blob/master/mmrazor/models/algorithms/autoslim.py#L171

### To Reproduce

When the loss contains learned paramters. I can't provide the code. You can try to add some parameters in wsld loss.

"
Distillation in fp16 mode,open-mmlab/mmrazor,2022-06-24 11:18:33,1,enhancement,190,1283625167,"when student and teacher network traing in fp16, if  align_module is none, training is work well. but if i set align_module, the align_module's weight is fp32, but th student's outputs are fp16, it will crash, how to solve the problem?
"
autoslim后的模型测试,open-mmlab/mmrazor,2022-06-22 10:34:10,7,,189,1279929846,"
![image](https://user-images.githubusercontent.com/44716550/175203266-3d43fcbb-608c-402b-9b3e-65d4a1251bf1.png)

- 其中origin 是使用mmcls训练的mobilenetv2_1.5
- autoslim是下载的权重（计算量和参数量是拿的README），参见：
![4107e4804cfb9673aa278075b397bed](https://user-images.githubusercontent.com/44716550/175008510-f977c17e-47ba-42eb-af86-4a73ce64d7ab.png)


在推理速度上没有体现？还是操作有误，分别推理了10000次，预热100次取平均"
[Bug]Distll for point_rend,open-mmlab/mmrazor,2022-06-22 06:02:34,3,usage,188,1279594754,"### Describe the bug

A clear and concise description of what the bug is.

\[here\]I try to use CWD for point_ Rend for distillation.It has the following bugs：Traceback (most recent call last):
  File ""tools/mmdet/train_mmdet.py"", line 210, in <module>
    main()
  File ""tools/mmdet/train_mmdet.py"", line 206, in main
    meta=meta)
  File ""/media/jidong/code/xuhao/mmrazor-master/mmrazor/apis/mmdet/train.py"", line 206, in train_mmdet_model
    runner.run(data_loader, cfg.workflow)
  File ""/home/jidong/anaconda3/envs/mmrazor/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 130, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""/home/jidong/anaconda3/envs/mmrazor/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 51, in train
    self.run_iter(data_batch, train_mode=True, **kwargs)
  File ""/home/jidong/anaconda3/envs/mmrazor/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 30, in run_iter
    **kwargs)
  File ""/home/jidong/anaconda3/envs/mmrazor/lib/python3.7/site-packages/mmcv/parallel/data_parallel.py"", line 75, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File ""/media/jidong/code/xuhao/mmrazor-master/mmrazor/models/algorithms/general_distill.py"", line 49, in train_step
    distill_losses = self.distiller.compute_distill_loss(data)
  File ""/media/jidong/code/xuhao/mmrazor-master/mmrazor/models/distillers/single_teacher.py"", line 240, in compute_distill_loss
    losses[loss_name] = loss_module(s_out, t_out)
  File ""/home/jidong/anaconda3/envs/mmrazor/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/media/jidong/code/xuhao/mmrazor-master/mmrazor/models/losses/cwd.py"", line 50, in forward
    logsoftmax(preds_S.view(-1, W * H) / self.tau)) * (
RuntimeError: The size of tensor a (220) must match the size of tensor b (12) at non-singleton dimension 0



### To Reproduce

The command you executed.

```shell
[here]python tools/mmdet/train_mmdet.py configs/distill/cwd/pointrend.py --work-dir test/ --cfg-options algorithm.distiller.teacher.init_cfg.checkpoint=https://download.openmmlab.com/mmdetection/v2.0/point_rend/point_rend_r50_caffe_fpn_mstrain_3x_coco/point_rend_r50_caffe_fpn_mstrain_3x_coco-e0ebb6b7.pth

```

### Post related information

1. The output of `pip list | grep ""mmcv\|mmrazor\|^torch""`
   \[here\]
2. Your config file if you modified it or created a new one.

```python
[here]_base_ = [
    '../../_base_/datasets/mmdet/coco_instance.py',
    '../../_base_/schedules/mmdet/schedule_1x.py',
    '../../_base_/mmdet_runtime.py'
]

# model settings
student = dict(
    type='mmdet.PointRend',
    backbone=dict(
        type='ResNet',
        depth=18,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=False),
        norm_eval=True,
        style='caffe'
        # init_cfg=dict(
        #     type='Pretrained',
        #     checkpoint='open-mmlab://detectron2/resnet18_caffe')
    ),
    neck=dict(
        type='FPN',
        in_channels=[64, 128, 256, 512],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[.0, .0, .0, .0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    roi_head=dict(
        type='PointRendRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=4,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0., 0., 0., 0.],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
        mask_roi_extractor=dict(
            type='GenericRoIExtractor',
            aggregation='concat',
            roi_layer=dict(
                type='SimpleRoIAlign', output_size=14),
            out_channels=256,
            featmap_strides=[4]),
        mask_head=dict(
            type='CoarseMaskHead',
            num_fcs=2,
            in_channels=256,
            conv_out_channels=256,
            fc_out_channels=1024,
            num_classes=4,
            loss_mask=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),
        point_head=dict(
            type='MaskPointHead',
            num_fcs=3,
            in_channels=256,
            fc_channels=256,
            num_classes=4,
            coarse_pred_each_layer=True,
            loss_point=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),
    # model training and testing settings
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            mask_size=7,
            num_points=14 * 14,
            oversample_ratio=3,
            importance_sample_ratio=0.75,
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,  # 在nms之前保留的的得分最高的proposal数量
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.05,
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100,
            mask_thr_binary=0.5,
            subdivision_steps=5,
            subdivision_num_points=28 * 28,
            scale_factor=2)))

checkpoint = '/media/jidong/code/xuhao/mmdetection/load/point_rend_r50_caffe_fpn_mstrain_3x_coco-e0ebb6b7.pth'

teacher = dict(
    type='mmdet.PointRend',
    init_cfg=dict(type='Pretrained', checkpoint=checkpoint),
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=False),
        norm_eval=True,
        style='caffe',
        init_cfg=dict(
            type='Pretrained',
            checkpoint='open-mmlab://detectron2/resnet50_caffe')),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[.0, .0, .0, .0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    roi_head=dict(
        type='PointRendRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=4,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0., 0., 0., 0.],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
        mask_roi_extractor=dict(
            type='GenericRoIExtractor',
            aggregation='concat',
            roi_layer=dict(
                type='SimpleRoIAlign', output_size=14),
            out_channels=256,
            featmap_strides=[4]),
        mask_head=dict(
            type='CoarseMaskHead',
            num_fcs=2,
            in_channels=256,
            conv_out_channels=256,
            fc_out_channels=1024,
            num_classes=4,
            loss_mask=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),
        point_head=dict(
            type='MaskPointHead',
            num_fcs=3,
            in_channels=256,
            fc_channels=256,
            num_classes=4,
            coarse_pred_each_layer=True,
            loss_point=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),
    # model training and testing settings
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            mask_size=7,
            num_points=14 * 14,
            oversample_ratio=3,
            importance_sample_ratio=0.75,
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,  # 在nms之前保留的的得分最高的proposal数量
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.05,
            # nms=dict(type='soft_nms', iou_threshold=0.5, min_score=0.05),
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100,
            mask_thr_binary=0.5,
            subdivision_steps=5,
            subdivision_num_points=28 * 28,
            scale_factor=2)))

algorithm = dict(
    type='GeneralDistill',
    architecture=dict(
        type='MMDetArchitecture',
        model=student,
    ),
    distiller=dict(
        type='SingleTeacherDistiller',
        teacher=teacher,
        teacher_trainable=False,
        components=[
            dict(
                student_module='roi_head.mask_head',
                teacher_module='roi_head.mask_head',
                losses=[
                    dict(
                        type='ChannelWiseDivergence',
                        name='loss_cwd_logits',
                        tau=1,
                        loss_weight=5,
                    )
                ])
        ]),
)

find_unused_parameters = True
```

3. Your train log file if you meet the problem during training.
   \[here\]
4. Other code you modified in the `mmrazor` folder.
   \[here\]

### Additional context

Add any other context about the problem here.

\[here\]
"
[Bug]distll for training point_rend,open-mmlab/mmrazor,2022-06-21 01:39:05,4,enhancement#usage,185,1277615376,"### Describe the bug

A clear and concise description of what the bug is.

\[here\]Traceback (most recent call last):
  File ""tools/mmdet/train_mmdet.py"", line 210, in <module>
    main()
  File ""tools/mmdet/train_mmdet.py"", line 206, in main
    meta=meta)
  File ""/media/jidong/code/xuhao/mmrazor-master/mmrazor/apis/mmdet/train.py"", line 206, in train_mmdet_model
    runner.run(data_loader, cfg.workflow)
  File ""/home/jidong/anaconda3/envs/mmrazor/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 130, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""/home/jidong/anaconda3/envs/mmrazor/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 51, in train
    self.run_iter(data_batch, train_mode=True, **kwargs)
  File ""/home/jidong/anaconda3/envs/mmrazor/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 30, in run_iter
    **kwargs)
  File ""/home/jidong/anaconda3/envs/mmrazor/lib/python3.7/site-packages/mmcv/parallel/data_parallel.py"", line 75, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File ""/media/jidong/code/xuhao/mmrazor-master/mmrazor/models/algorithms/general_distill.py"", line 49, in train_step
    distill_losses = self.distiller.compute_distill_loss(data)
  File ""/media/jidong/code/xuhao/mmrazor-master/mmrazor/models/distillers/single_teacher.py"", line 240, in compute_distill_loss
    losses[loss_name] = loss_module(s_out, t_out)
  File ""/home/jidong/anaconda3/envs/mmrazor/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/media/jidong/code/xuhao/mmrazor-master/mmrazor/models/losses/weighted_soft_label_distillation.py"", line 32, in forward
    gt_labels = self.current_data['gt_label']
KeyError: 'gt_label'
### To Reproduce

The command you executed.

```shell
[here]python tools/mmdet/train_mmdet.py configs/distill/wsld/pointrend.py --work-dir pointrend/ --cfg-options algorithm.distiller.teacher.init_cfg.checkpoint=https://download.openmmlab.com/mmdetection/v2.0/point_rend/point_rend_r50_caffe_fpn_mstrain_3x_coco/point_rend_r50_caffe_fpn_mstrain_3x_coco-e0ebb6b7.pth
```


### Post related information

1. The output of `pip list | grep ""mmcv\|mmrazor\|^torch""`
   \[here\]
2. Your config file if you modified it or created a new one.

```python
[here]_base_ = [
    '../../_base_/datasets/mmdet/coco_instance.py',
    '../../_base_/schedules/mmdet/schedule_1x.py',
    '../../_base_/mmdet_runtime.py'
]

# model settings
student = dict(
    type='mmdet.PointRend',
    backbone=dict(
        type='ResNet',
        depth=18,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=False),
        norm_eval=True,
        style='caffe'),
    neck=dict(
        type='FPN',
        in_channels=[64, 128, 256, 512],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[.0, .0, .0, .0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    roi_head=dict(
        type='PointRendRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=4,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0., 0., 0., 0.],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
        mask_roi_extractor=dict(
            type='GenericRoIExtractor',
            aggregation='concat',
            roi_layer=dict(
                type='SimpleRoIAlign', output_size=14),
            out_channels=256,
            featmap_strides=[4]),
        mask_head=dict(
            type='CoarseMaskHead',
            num_fcs=2,
            in_channels=256,
            conv_out_channels=256,
            fc_out_channels=1024,
            num_classes=4,
            loss_mask=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),
        point_head=dict(
            type='MaskPointHead',
            num_fcs=3,
            in_channels=256,
            fc_channels=256,
            num_classes=4,
            coarse_pred_each_layer=True,
            loss_point=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),
    # model training and testing settings
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            mask_size=7,
            num_points=14 * 14,
            oversample_ratio=3,
            importance_sample_ratio=0.75,
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,  # 在nms之前保留的的得分最高的proposal数量
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.05,
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100,
            mask_thr_binary=0.5,
            subdivision_steps=5,
            subdivision_num_points=28 * 28,
            scale_factor=2)))

checkpoint = '/media/jidong/code/xuhao/mmdetection/load/point_rend_r50_caffe_fpn_mstrain_3x_coco-e0ebb6b7.pth'

teacher = dict(
    type='mmdet.PointRend',
    init_cfg=dict(type='Pretrained', checkpoint=checkpoint),
    backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=False),
        norm_eval=True,
        style='caffe',
        init_cfg=dict(
            type='Pretrained',
            checkpoint='open-mmlab://detectron2/resnet101_caffe')),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[.0, .0, .0, .0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    roi_head=dict(
        type='PointRendRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=4,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0., 0., 0., 0.],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
        mask_roi_extractor=dict(
            type='GenericRoIExtractor',
            aggregation='concat',
            roi_layer=dict(
                type='SimpleRoIAlign', output_size=14),
            out_channels=256,
            featmap_strides=[4]),
        mask_head=dict(
            type='CoarseMaskHead',
            num_fcs=2,
            in_channels=256,
            conv_out_channels=256,
            fc_out_channels=1024,
            num_classes=4,
            loss_mask=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),
        point_head=dict(
            type='MaskPointHead',
            num_fcs=3,
            in_channels=256,
            fc_channels=256,
            num_classes=4,
            coarse_pred_each_layer=True,
            loss_point=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),
    # model training and testing settings
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            mask_size=7,
            num_points=14 * 14,
            oversample_ratio=3,
            importance_sample_ratio=0.75,
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,  # 在nms之前保留的的得分最高的proposal数量
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.05,
            # nms=dict(type='soft_nms', iou_threshold=0.5, min_score=0.05),
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100,
            mask_thr_binary=0.5,
            subdivision_steps=5,
            subdivision_num_points=28 * 28,
            scale_factor=2)))

algorithm = dict(
    type='GeneralDistill',
    architecture=dict(
        type='MMDetArchitecture',
        model=student,
    ),
    with_student_loss=True,
    with_teacher_loss=False,
    distiller=dict(
        type='SingleTeacherDistiller',
        teacher=teacher,
        teacher_trainable=False,
        teacher_norm_eval=True,
        components=[
            dict(
                student_module='roi_head.bbox_head',
                teacher_module='roi_head.bbox_head',
                losses=[
                    dict(
                        type='WSLD',
                        name='loss_wsld',
                        tau=2,
                        loss_weight=2.5,
                        num_classes=4)
                ])
        ]),
)

find_unused_parameters = True
```
"
How can I use DetNAS in my datasets about Object Detection？,open-mmlab/mmrazor,2022-06-13 03:45:12,6,,178,1268859070,"Do I need to pre-training on imageNet? What's meaning of pre-training on imagenet? Can I use DetNAS in distillation model and How do I do?Appreciate it!
![微信图片_20220613113759](https://user-images.githubusercontent.com/35597652/173275429-5077e7fa-e359-4fb0-8f0f-d1312f7ca5f4.png)

"
find_make_group_parser for depthwise,open-mmlab/mmrazor,2022-06-08 12:02:23,3,bug,177,1264634965,"### Describe the bug

    def find_make_group_parser(self, node_name, name2module):
        """"""Find the corresponding make_group_parser according to the
        ``node_name``""""""
        if 'concat' in node_name and node_name not in name2module:
            return MAKE_GROUP_PARSER_DICT['concat']
        elif 'chunk' in node_name and node_name not in name2module:
            return MAKE_GROUP_PARSER_DICT['chunk']
        elif (node_name in name2module
              and isinstance(name2module[node_name], nn.Conv2d)
              and name2module[node_name].in_channels
              == name2module[node_name].groups):
            return MAKE_GROUP_PARSER_DICT['depthwise']
        else:
            return MAKE_GROUP_PARSER_DICT['common']

if in_channels is 1，there may be a mistake
"
"KeyError: ""GeneralDistill: 'bbox_head.gfl_cls'""",open-mmlab/mmrazor,2022-06-07 03:07:48,17,usage,174,1262660481,"
2022-06-07 10:58:14,781 - mmdet - INFO - Set random seed to 979972066, deterministic: False
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\mmcv\utils\registry.py"", line 52, in build_from_cfg
    return obj_cls(**args)
  File ""D:\mmrazor-master\mmrazor-master\mmrazor\models\algorithms\general_distill.py"", line 23, in __init__
    super(GeneralDistill, self).__init__(**kwargs)
  File ""D:\mmrazor-master\mmrazor-master\mmrazor\models\algorithms\base.py"", line 57, in __init__
    self._init_distiller(distiller)
  File ""D:\mmrazor-master\mmrazor-master\mmrazor\models\algorithms\base.py"", line 135, in _init_distiller
    self.distiller.prepare_from_student(self.architecture)
  File ""D:\mmrazor-master\mmrazor-master\mmrazor\models\distillers\single_teacher.py"", line 123, in prepare_from_student
    student_module = self.student_name2module[student_module_name]
KeyError: 'bbox_head.gfl_cls'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:/mmrazor-master/mmrazor-master/tools/mmdet/train_mmdet.py"", line 214, in <module>
    main()
  File ""D:/mmrazor-master/mmrazor-master/tools/mmdet/train_mmdet.py"", line 187, in main
    algorithm = build_algorithm(cfg.algorithm)
  File ""D:\mmrazor-master\mmrazor-master\mmrazor\models\builder.py"", line 20, in build_algorithm
    return ALGORITHMS.build(cfg)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\mmcv\utils\registry.py"", line 212, in build
    return self.build_func(*args, **kwargs, registry=self)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\mmcv\cnn\builder.py"", line 27, in build_model_from_cfg
    return build_from_cfg(cfg, registry, default_args)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\mmcv\utils\registry.py"", line 55, in build_from_cfg
    raise type(e)(f'{obj_cls.__name__}: {e}')
KeyError: ""GeneralDistill: 'bbox_head.gfl_cls'""

I don't know why, hope to get help
"
[Bug] test_pruner.py run errors,open-mmlab/mmrazor,2022-06-02 09:28:05,4,bug,171,1257931226,"1. When run test_pruner.py with 0.3.1, below error occured.

```sh
python tests/test_models/test_pruner.py
Traceback (most recent call last):
  File ""tests/test_models/test_pruner.py"", line 339, in <module>
    test_ratio_pruner()
  File ""tests/test_models/test_pruner.py"", line 38, in test_ratio_pruner
    _test_reset_bn_running_stats(architecture_cfg, pruner_cfg, False)
  File ""tests/test_models/test_pruner.py"", line 315, in _test_reset_bn_running_stats
    pruner1.prepare_from_supernet(architecture1)
  File ""/root/mmlab/mmrazor/mmrazor/models/pruners/ratio_pruning.py"", line 49, in prepare_from_supernet
    super(RatioPruner, self).prepare_from_supernet(supernet)
  File ""/root/mmlab/mmrazor/mmrazor/models/pruners/structure_pruning.py"", line 185, in prepare_from_supernet
    var2module, norm_conv_links, visited)
  File ""/root/mmlab/mmrazor/mmrazor/models/pruners/structure_pruning.py"", line 732, in trace_norm_conv_links
    visited)
  File ""/root/mmlab/mmrazor/mmrazor/models/pruners/structure_pruning.py"", line 732, in trace_norm_conv_links
    visited)
  File ""/root/mmlab/mmrazor/mmrazor/models/pruners/structure_pruning.py"", line 732, in trace_norm_conv_links
    visited)
  [Previous line repeated 5 more times]
  File ""/root/mmlab/mmrazor/mmrazor/models/pruners/structure_pruning.py"", line 699, in trace_norm_conv_links
    conv_grad_fn = conv_grad_fn.next_functions[0][0]
AttributeError: 'NoneType' object has no attribute 'next_functions'
```


2. After downgrade the version to v0.3.0,  issue not reproduced.
but it seems set_min_channel() not works(after set_min_channel, and export_subnet will get the original net info)

```python
-----------------------------codes in test_pruner.py-------------------------------------------------

    model_cfg = dict(
        type='mmcls.ImageClassifier',
        backbone=dict(
            type='mmcls.ResNet',
            depth=18,
            num_stages=4,
            out_indices=(3, ),
            style='pytorch'),
        neck=dict(type='mmcls.GlobalAveragePooling'),
        head=dict(
            type='mmcls.LinearClsHead',
            num_classes=1000,
            in_channels=512,
            loss=dict(type='mmcls.CrossEntropyLoss', loss_weight=1.0),
            topk=(1, 5),
        ))

    architecture_cfg = dict(
        type='MMClsArchitecture',
        model=model_cfg,
    )

    pruner_cfg = dict(
        type='RatioPruner',
        ratios=[1 / 8, 2 / 8, 3 / 8, 4 / 8, 5 / 8, 6 / 8, 7 / 8, 1.0])

    _test_reset_bn_running_stats(architecture_cfg, pruner_cfg, False)
    with pytest.raises(AssertionError):
        _test_reset_bn_running_stats(architecture_cfg, pruner_cfg, True)

    imgs = torch.randn(16, 3, 224, 224)
    label = torch.randint(0, 1000, (16, ))

    architecture = ARCHITECTURES.build(architecture_cfg)
    pruner = PRUNERS.build(pruner_cfg)

    pruner.prepare_from_supernet(architecture)
    assert hasattr(pruner, 'channel_spaces')

    # test set_min_channel
    pruner_cfg_ = deepcopy(pruner_cfg)
    #pruner_cfg_['ratios'].insert(0, 0)
    pruner_ = PRUNERS.build(pruner_cfg_)
    architecture_ = ARCHITECTURES.build(architecture_cfg)
    pruner_.prepare_from_supernet(architecture_)
    #with pytest.raises(AssertionError):
        # Output channels should be a positive integer not zero
    pruner_.set_min_channel()

    subnet_dict = pruner_.export_subnet()
    print(subnet_dict)
    assert isinstance(subnet_dict, dict)
    pruner_.deploy_subnet(architecture, subnet_dict)
```

```sh
-----------------------------output for subnet_dict----------------------------------

{'backbone.conv1': {'in_channels': 3, 'raw_in_channels': 3, 'out_channels': 64, 'raw_out_channels': 64}, 'backbone.bn1': {'out_channels': 64, 'raw_out_channels': 64}, 'backbone.layer1.0.conv1': {'in_channels': 64, 'raw_in_channels': 64, 'out_channels': 64, 'raw_out_channels': 64}, 'backbone.layer1.0.bn1': {'out_channels': 64, 'raw_out_channels': 64}, 'backbone.layer1.0.conv2': {'in_channels': 64, 'raw_in_channels': 64, 'out_channels': 64, 'raw_out_channels': 64}, 'backbone.layer1.0.bn2': {'out_channels': 64, 'raw_out_channels': 64}, 'backbone.layer1.1.conv1': {'in_channels': 64, 'raw_in_channels': 64, 'out_channels': 64, 'raw_out_channels': 64}, 'backbone.layer1.1.bn1': {'out_channels': 64, 'raw_out_channels': 64}, 'backbone.layer1.1.conv2': {'in_channels': 64, 'raw_in_channels': 64, 'out_channels': 64, 'raw_out_channels': 64}, 'backbone.layer1.1.bn2': {'out_channels': 64, 'raw_out_channels': 64}, 'backbone.layer2.0.conv1': {'in_channels': 64, 'raw_in_channels': 64, 'out_channels': 128, 'raw_out_channels': 128}, 'backbone.layer2.0.bn1': {'out_channels': 128, 'raw_out_channels': 128}, 'backbone.layer2.0.conv2': {'in_channels': 128, 'raw_in_channels': 128, 'out_channels': 128, 'raw_out_channels': 128}, 'backbone.layer2.0.bn2': {'out_channels': 128, 'raw_out_channels': 128}, 'backbone.layer2.0.downsample.0': {'in_channels': 64, 'raw_in_channels': 64, 'out_channels': 128, 'raw_out_channels': 128}, 'backbone.layer2.0.downsample.1': {'out_channels': 128, 'raw_out_channels': 128}, 'backbone.layer2.1.conv1': {'in_channels': 128, 'raw_in_channels': 128, 'out_channels': 128, 'raw_out_channels': 128}, 'backbone.layer2.1.bn1': {'out_channels': 128, 'raw_out_channels': 128}, 'backbone.layer2.1.conv2': {'in_channels': 128, 'raw_in_channels': 128, 'out_channels': 128, 'raw_out_channels': 128}, 'backbone.layer2.1.bn2': {'out_channels': 128, 'raw_out_channels': 128}, 'backbone.layer3.0.conv1': {'in_channels': 128, 'raw_in_channels': 128, 'out_channels': 256, 'raw_out_channels': 256}, 'backbone.layer3.0.bn1': {'out_channels': 256, 'raw_out_channels': 256}, 'backbone.layer3.0.conv2': {'in_channels': 256, 'raw_in_channels': 256, 'out_channels': 256, 'raw_out_channels': 256}, 'backbone.layer3.0.bn2': {'out_channels': 256, 'raw_out_channels': 256}, 'backbone.layer3.0.downsample.0': {'in_channels': 128, 'raw_in_channels': 128, 'out_channels': 256, 'raw_out_channels': 256}, 'backbone.layer3.0.downsample.1': {'out_channels': 256, 'raw_out_channels': 256}, 'backbone.layer3.1.conv1': {'in_channels': 256, 'raw_in_channels': 256, 'out_channels': 256, 'raw_out_channels': 256}, 'backbone.layer3.1.bn1': {'out_channels': 256, 'raw_out_channels': 256}, 'backbone.layer3.1.conv2': {'in_channels': 256, 'raw_in_channels': 256, 'out_channels': 256, 'raw_out_channels': 256}, 'backbone.layer3.1.bn2': {'out_channels': 256, 'raw_out_channels': 256}, 'backbone.layer4.0.conv1': {'in_channels': 256, 'raw_in_channels': 256, 'out_channels': 512, 'raw_out_channels': 512}, 'backbone.layer4.0.bn1': {'out_channels': 512, 'raw_out_channels': 512}, 'backbone.layer4.0.conv2': {'in_channels': 512, 'raw_in_channels': 512, 'out_channels': 512, 'raw_out_channels': 512}, 'backbone.layer4.0.bn2': {'out_channels': 512, 'raw_out_channels': 512}, 'backbone.layer4.0.downsample.0': {'in_channels': 256, 'raw_in_channels': 256, 'out_channels': 512, 'raw_out_channels': 512}, 'backbone.layer4.0.downsample.1': {'out_channels': 512, 'raw_out_channels': 512}, 'backbone.layer4.1.conv1': {'in_channels': 512, 'raw_in_channels': 512, 'out_channels': 512, 'raw_out_channels': 512}, 'backbone.layer4.1.bn1': {'out_channels': 512, 'raw_out_channels': 512}, 'backbone.layer4.1.conv2': {'in_channels': 512, 'raw_in_channels': 512, 'out_channels': 512, 'raw_out_channels': 512}, 'backbone.layer4.1.bn2': {'out_channels': 512, 'raw_out_channels': 512}, 'head.fc': {'in_channels': 512, 'raw_in_channels': 512, 'out_channels': 1000, 'raw_out_channels': 1000}}
----------------------------------------------------------------------------------------
```
"
trace_non_pass_path Error for YOLOX architecture[Bug],open-mmlab/mmrazor,2022-06-01 03:24:24,4,bug,170,1254809484,"Hi~ @all ，I found that the trace_non_pass_path result for YOLOX architecture exists some problems. For Stage_4 of YOLOX, as shown below,  Parents of the ""Concat"" Node are ""short_conv"" and ""main_conv"", while ""block_conv1"" and ""block_conv2"" are mistakenly ignored.
<img width=""377"" alt=""image"" src=""https://user-images.githubusercontent.com/40205112/171320799-3a759ce2-5012-4e97-a345-fd9bdceab533.png"">
"
About supporting repo,open-mmlab/mmrazor,2022-05-30 11:57:56,4,,167,1252634569,"### Checklist
- I have searched related issues but cannot get the expected help.
- I have read related documents and don't know what to do.

### Describe the question you meet

Hi，Have mmrazor supported mmocr？

### Post related information
1. The output of `pip list | grep ""mmcv\|mmrazor\|^torch""`
[here]
2. Your config file if you modified it or created a new one.
```python
[here]
```
3. Your train log file if you meet the problem during training.
[here]
4. Other code you modified in the `mmrazor` folder.
[here]
"
[GLCC] GitLink Code Camp (首届 CCF GitLink 开源编程夏令营),open-mmlab/mmrazor,2022-05-23 11:43:00,1,GLCC,166,1245036021,"The issue is about the first GitLink Code Camp activity.
Related link: https://mp.weixin.qq.com/s/upVI-2TQ0ezQGpzhDiF3Pg
## 题目名称
基于MMRazor，实现AdaRound量化算法
## 题目描述
### 题目简介
MMRazor是深度学习模型压缩算法库，支持网络结构搜索、剪枝、蒸馏等主流技术方向，为 OpenMMLab 其他算法库提供即插即用、可自由组合的模型压缩算法，让用户更简单更快速的实现模型轻量化。
基于MMRazor，复现AdaRound量化算法，并使其算法可应用于[MMClassification](https://github.com/open-mmlab/mmclassification)和[MMDetection](https://github.com/open-mmlab/mmdetection)两类任务
### 编码任务
在MMRazor中复现量化算法，对齐论文精度，并将算法迁移应用到检测任务
### 技能要求和编程语言
- Python
- 一定的论文阅读能力
## 预期成果
- 完成算法复现工作，达到论文精度
- 实现及使用方式符合MMRazor整体设计风格
- 将复现的量化算法应用于[MMDetection](https://github.com/open-mmlab/mmdetection)任务上
## 导师及联系方式
GitHub id: humu789
Email: [openmmlab@gmail.com](mailto:openmmlab@gmail.com)
微信交流群
![20220523-220129](https://user-images.githubusercontent.com/88702197/169837210-1d3c5261-74e2-41e1-8de9-3ab8d2c9681c.png)

"
Can this library be used in mm detection? How to use？,open-mmlab/mmrazor,2022-05-18 09:13:08,9,documentation,162,1239663693,"### Checklist
- I have searched related issues but cannot get the expected help.
- I have read related documents and don't know what to do.

### Describe the question you meet

[here]

### Post related information
1. The output of `pip list | grep ""mmcv\|mmrazor\|^torch""`
[here]
2. Your config file if you modified it or created a new one.
```python
[here]
```
3. Your train log file if you meet the problem during training.
[here]
4. Other code you modified in the `mmrazor` folder.
[here]
"
[Bug] AutoSlim For faster_rcnn_r50_fpn Error,open-mmlab/mmrazor,2022-05-13 07:07:41,1,bug,161,1234827856,"### Describe the bug
error log:
```
Traceback (most recent call last):
  File ""/root/project/mmrazor/mmrazor/models/algorithms/autoslim.py"", line 69, in _init_pruner
    pseudo_pruner.prepare_from_supernet(pseudo_architecture)
  File ""/root/project/mmrazor/mmrazor/models/pruners/structure_pruning.py"", line 152, in prepare_from_supernet
    pseudo_img = supernet.forward_dummy(pseudo_img)
  File ""/root/project/mmrazor/mmrazor/models/architectures/base.py"", line 21, in forward_dummy
    return self.model.forward_dummy(img)
  File ""/root/project/mmdetection/mmdet/models/detectors/two_stage.py"", line 86, in forward_dummy
    roi_outs = self.roi_head.forward_dummy(x, proposals)
  File ""/root/project/mmdetection/mmdet/models/roi_heads/standard_roi_head.py"", line 44, in forward_dummy
    bbox_results = self._bbox_forward(x, rois)
  File ""/root/project/mmdetection/mmdet/models/roi_heads/standard_roi_head.py"", line 121, in _bbox_forward
    bbox_feats = self.bbox_roi_extractor(
  File ""/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py"", line 198, in new_func
    return old_func(*args, **kwargs)
  File ""/root/project/mmdetection/mmdet/models/roi_heads/roi_extractors/single_level_roi_extractor.py"", line 103, in forward
    roi_feats_t = self.roi_layers[i](feats[i], rois_)
  File ""/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/mmcv/ops/roi_align.py"", line 213, in forward
    return roi_align(input, rois, self.output_size, self.spatial_scale,
  File ""/opt/conda/lib/python3.8/site-packages/mmcv/ops/roi_align.py"", line 93, in forward
    ext_module.roi_align_forward(
RuntimeError: roi_width >= 0 && roi_height >= 0 INTERNAL ASSERT FAILED at ""/tmp/mmcv/mmcv/ops/csrc/pytorch/cpu/roi_align.cpp"":139, please report a bug to PyTorch. ROIs in ROIAlign cannot have non-negative size!

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/mmcv/utils/registry.py"", line 52, in build_from_cfg
    return obj_cls(**args)
  File ""/root/project/mmrazor/mmrazor/models/algorithms/autoslim.py"", line 42, in __init__
    super(AutoSlim, self).__init__(**kwargs)
  File ""/root/project/mmrazor/mmrazor/models/algorithms/base.py"", line 56, in __init__
    self._init_pruner(pruner)
  File ""/root/project/mmrazor/mmrazor/models/algorithms/autoslim.py"", line 78, in _init_pruner
    raise NotImplementedError('Our current StructurePruner does not '
NotImplementedError: Our current StructurePruner does not support pruning this architecture. StructurePruner is not perfect enough to handle all the corner cases. We will appreciate it if you create a issue.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""MMCV/train.py"", line 54, in <module>
    main()
  File ""MMCV/train.py"", line 48, in main
    train_pruning(args)
  File ""MMCV/../../XbrainAiModels/MMCV/tools/train_pruning.py"", line 43, in train_pruning
    train_mmdet(args, cfg)
  File ""MMCV/../../XbrainAiModels/MMCV/tools/train_mmdet.py"", line 26, in train_mmdet
    algorithm = build_algorithm(cfg.algorithm)
  File ""/root/project/mmrazor/mmrazor/models/builder.py"", line 20, in build_algorithm
    return ALGORITHMS.build(cfg)
  File ""/opt/conda/lib/python3.8/site-packages/mmcv/utils/registry.py"", line 215, in build
    return self.build_func(*args, **kwargs, registry=self)
  File ""/opt/conda/lib/python3.8/site-packages/mmcv/cnn/builder.py"", line 27, in build_model_from_cfg
    return build_from_cfg(cfg, registry, default_args)
  File ""/opt/conda/lib/python3.8/site-packages/mmcv/utils/registry.py"", line 55, in build_from_cfg
    raise type(e)(f'{obj_cls.__name__}: {e}')
NotImplementedError: AutoSlim: Our current StructurePruner does not support pruning this architecture. StructurePruner is not perfect enough to handle all the corner cases. We will appreciate it if you create a issue.
```

### config

```
_base_ = [
    '../../_base_/datasets/coco_detection.py',
    '../../_base_/schedules/schedule_det.py',
    '../../_base_/default_runtime_det.py'
]

model = dict(
    type='mmdet.FasterRCNN',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[.0, .0, .0, .0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    roi_head=dict(
        type='StandardRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=80,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0., 0., 0., 0.],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0))),
    # model training and testing settings
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=False,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.05,
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100)
        # soft-nms is also supported for rcnn testing
        # e.g., nms=dict(type='soft_nms', iou_threshold=0.5, min_score=0.05)
    ))

algorithm = dict(
    type='AutoSlim',
    architecture=dict(type='MMDetArchitecture', model=model),
    pruner=dict(
        type='RatioPruner',
        ratios=(2 / 12, 3 / 12, 4 / 12, 5 / 12, 6 / 12, 7 / 12, 8 / 12, 9 / 12,
                10 / 12, 11 / 12, 1.0)),
    retraining=False,
    bn_training_mode=True,
    input_shape=None)

runner = dict(type='EpochBasedRunner', max_epochs=50)

use_ddp_wrapper = True
```"
distill result,open-mmlab/mmrazor,2022-05-06 07:40:00,7,usage,157,1227525531,"在修改了配置文件训练自己的数据后，我得到了teacher的权重文件，运行的代码为：
```python
python tools/mmcls/train_mmcls.py   configs/distill/rkd/rkd_neck_resnet34_resnet18_8xb32_in1k.py   \
--work-dir /data/dataset_81/jd_result/distill_mmcls   \
--cfg-options algorithm.distiller.teacher.init_cfg.type=Pretrained
```
生成的结果为：
![image](https://user-images.githubusercontent.com/104477720/167088410-c592c9a3-6d05-461f-8cc0-1179d7f1782a.png)

但是出现了一个问题：
生成的经过distill 的权重文件的大小是167MB，但是teacher的权重文件只有163MB，没有起到模型瘦身的效果，这是为什么呢
"
NAS on semantic segmentation,open-mmlab/mmrazor,2022-05-03 12:59:44,1,,154,1224063331,"Hi,

I am wondering if its possible to currently run a NAS algorithm (e.g., SPOS) on a semantic segmentation model from mmsg (e.g., Deeplab). I see that for SPOS, the documentation only refers to image classification. Is there a limitation as to apply this algorithm on a segmentation model ?

"
How to train my own datasets?,open-mmlab/mmrazor,2022-04-18 12:19:53,8,usage,143,1206995096,"I use data in coco format, the category is a category, how to modify the category？
![微信截图_20220418201903](https://user-images.githubusercontent.com/89955551/163807364-7a9b8432-f9d6-4064-9d91-600379de71da.png)

"
mmrazor output file writing,open-mmlab/mmrazor,2022-04-05 04:10:28,2,usage,136,1192583452,"mmdetection can write output to json file.

but, mmrazor/tools/mmdet/test_mmdet.py cannot write output

ex ) python tools/mmdet/test_mmdet.py ./configs/distill/cwd/cwd_cls_head_deformable_detr_rx101_r50.py ./deformable_detr_rx101_r50/epoch_211.pth --eval bbox --eval--options=""jsonfile_prefix=./output_path""

how can I write output file?




the config file 

# model settings
student = dict(
    type='mmdet.DeformableDETR',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=-1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    neck=dict(
        type='ChannelMapper',
        in_channels=[512, 1024, 2048],
        kernel_size=1,
        out_channels=256,
        act_cfg=None,
        norm_cfg=dict(type='GN', num_groups=32),
        num_outs=4),
    bbox_head=dict(
        type='DeformableDETRHead',
        num_query=300,
        num_classes=2,
        in_channels=2048,
        sync_cls_avg_factor=True,
        as_two_stage=True,
        transformer=dict(
            type='DeformableDetrTransformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention', embed_dims=256),
                    feedforward_channels=1024,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DeformableDetrTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='MultiScaleDeformableAttention',
                            embed_dims=256)
                    ],
                    feedforward_channels=1024,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0),
        with_box_refine=True),
    train_cfg=dict(
        assigner=dict(
            type='HungarianAssigner',
            cls_cost=dict(type='FocalLossCost', weight=2.0),
            reg_cost=dict(type='BBoxL1Cost', weight=5.0, box_format='xywh'),
            iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0))),
    test_cfg=dict(max_per_img=100))

teacher = dict(
    type='mmdet.DeformableDETR',
    backbone=dict(
        type='ResNeXt',
        depth=101,
        groups=64,
        base_width=4,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        style='pytorch',
        init_cfg=None),
    neck=dict(
        type='ChannelMapper',
        in_channels=[512, 1024, 2048],
        kernel_size=1,
        out_channels=256,
        act_cfg=None,
        norm_cfg=dict(type='GN', num_groups=32),
        num_outs=4),
    bbox_head=dict(
        type='DeformableDETRHead',
        num_query=300,
        num_classes=2,
        in_channels=2048,
        sync_cls_avg_factor=True,
        as_two_stage=True,
        transformer=dict(
            type='DeformableDetrTransformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention', embed_dims=256),
                    feedforward_channels=1024,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DeformableDetrTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='MultiScaleDeformableAttention',
                            embed_dims=256)
                    ],
                    feedforward_channels=1024,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0),
        with_box_refine=True),
    train_cfg=dict(
        assigner=dict(
            type='HungarianAssigner',
            cls_cost=dict(type='FocalLossCost', weight=2.0),
            reg_cost=dict(type='BBoxL1Cost', weight=5.0, box_format='xywh'),
            iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0))),
    test_cfg=dict(max_per_img=100))




img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='AutoAugment',
        policies=[[{
            'type':
            'Resize',
            'img_scale': [(480, 800), (512, 800), (544, 800), (576, 800),
                          (608, 800), (640, 800), (672, 800), (704, 800),
                          (736, 800), (768, 800), (800, 800)],
            'multiscale_mode':
            'value',
            'keep_ratio':
            True
        }],
                  [{
                      'type': 'Resize',
                      'img_scale': [(400, 4200), (500, 4200), (600, 4200)],
                      'multiscale_mode': 'value',
                      'keep_ratio': True
                  }, {
                      'type': 'RandomCrop',
                      'crop_type': 'absolute_range',
                      'crop_size': (384, 600),
                      'allow_negative_crop': True
                  }, {
                      'type':
                      'Resize',
                      'img_scale': [(480, 800), (512, 800), (544, 800),
                                    (576, 800), (608, 800), (640, 800),
                                    (672, 800), (704, 800), (736, 800),
                                    (768, 800), (800, 800)],
                      'multiscale_mode':
                      'value',
                      'override':
                      True,
                      'keep_ratio':
                      True
                  }]]),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=1),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(800, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=1),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
#  dataset settings
dataset_type = 'CocoDataset'
classes = ('[T]', '[F]')
data_source = './220308/imgs_all/'
ann_source = './annotations/'


light_valid_1124_train_ann_file = './train.json'
light_valid_1124_train_img_prefix = './train_images/'

light_valid_1124_val_ann_file = './annotations/valid.json'
light_valid_1124_val_img_prefix = './valid_images/'


######################### train set

data_date = 220308
train_set_1st= dict(
    type=dataset_type,
    img_prefix=data_source,
    classes=classes,
    ann_file=ann_source + str(data_date) + '_train.json',
    filter_empty_gt=False,
    pipeline=train_pipeline)

train_set_2nd= dict(
    type=dataset_type,
    img_prefix=train_set_2nd_img_prefix,
    classes=classes,
    ann_file=train_set_2nd_ann_file,
    filter_empty_gt=False,
    pipeline=train_pipeline)
######################### val set

valid_set_1st= dict(
    type=dataset_type,
    img_prefix=data_source,
    classes=classes,
    ann_file=ann_source + str(data_date) + '_valid.json',
    pipeline=test_pipeline)

valid_set_2nd= dict(
    type=dataset_type,
    img_prefix=valid_set_2nd_img_prefix,
    classes=classes,
    ann_file=valid_set_2nd_ann_file,
    pipeline=test_pipeline)

######################### test set

test_set_1st= dict(
    type=dataset_type,
    img_prefix=data_source,
    classes=classes,
    ann_file=ann_source + str(data_date) + '_test.json',
    pipeline=test_pipeline)

test_set_2nd= dict(
    type=dataset_type,
    img_prefix=valid_set_2nd_img_prefix,
    classes=classes,
    ann_file=valid_set_2nd_ann_file,
    pipeline=test_pipeline)

'''
data = dict(
    samples_per_gpu=8,
    workers_per_gpu=4,
    train=dict(type='ConcatDataset',
        datasets=[train_set_1st, train_set_2nd]),
    val=dict(type='ConcatDataset',
        datasets=[valid_set_1st, valid_set_2nd],
        separate_eval=True),
    test=test_set_2nd)
'''

data = dict(
    samples_per_gpu=8,
    workers_per_gpu=4,
    train=train_set_2nd,
    val=valid_set_2nd,
    test=valid_set_2nd)


'''
optimizer = dict(
    type='Adamax',
    lr=0.002,
    weight_decay=0.0001,
    paramwise_cfg=dict(
        custom_keys=dict(
            backbone=dict(lr_mult=0.1),
            sampling_offsets=dict(lr_mult=0.1),
            reference_points=dict(lr_mult=0.1))))
optimizer_config = dict(_delete_=True, grad_clip=dict(max_norm=0.1, norm_type=2))
'''

optimizer = dict(type='SGD', lr=0.001, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=dict(max_norm=0.1, norm_type=2))

# learning policy by mmdetection documents
lr_config = dict(policy='step', 
                 min_lr=2e-3,
                 # by_epoch=True,
                 warmup='linear',
                 warmup_iters=5000,
                 warmup_ratio=1.0 / 5,
                 step=1,
                 warmup_by_epoch=False)
runner = dict(type='EpochBasedRunner', max_epochs=300)
evaluation= dict(classwise=True, interval=1, metric='bbox') 
checkpoint_config = dict(interval=1)

log_config = dict(
    interval=50,
    hooks=[dict(type='TextLoggerHook')])

custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None

work_dir = './deformable_detr_rx101_r50'
workflow = [('train', 1), ('val', 1)]

'''# optimizer
optimizer = dict(type='SGD', lr=0.001, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
# learning policy
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[8, 11])
runner = dict(type='EpochBasedRunner', max_epochs=300)
evaluation= dict(classwise=True, interval=1, metric='bbox')
'''


# algorithm setting
algorithm = dict(
    type='GeneralDistill',
    architecture=dict(
        type='MMDetArchitecture',
        model=student,
    ),
    distiller=dict(
        type='SingleTeacherDistiller',
        teacher=teacher,
        teacher_trainable=False,
        components=[
            dict(
                student_module='bbox_head.cls_branches',
                teacher_module='bbox_head.cls_branches',
                losses=[
                    dict(
                        type='ChannelWiseDivergence',
                        name='loss_cwd_cls_head',
                        tau=1, 
                        loss_weight=5, 
                    )
                ])
        ]),
)

find_unused_parameters = True"
How could I use MMRazor to train YoloX with KD ?,open-mmlab/mmrazor,2022-03-30 06:26:15,5,,130,1185950694,"I need the MMRazor API to the yolox to make KD, and the config file of YOLOX too. "
Autoslim on mmsegmentation models example?,open-mmlab/mmrazor,2022-03-13 14:56:15,3,,111,1167603863,"### Describe the question you meet
Can autoslim be used on mmsegmentation models like [pspnet](https://github.com/open-mmlab/mmsegmentation/tree/master/configs/pspnet) or [segformer](https://github.com/open-mmlab/mmsegmentation/tree/master/configs/segformer)? I can't find search_mmseg.py in dir `tools/mmseg` like [search_mmcls.py](https://github.com/open-mmlab/mmrazor/blob/master/tools/mmcls/search_mmcls.py) in dir `tools/mmcls`
Do you please offer some more prune method or detailed examples ?
Thanks!"
[Bug] Can't use RepVGG with AutoSlim,open-mmlab/mmrazor,2022-02-23 12:52:42,1,bug,97,1148054150,"It's very similar to #79, it's a bug of the parser, except the exception is that it reached maximum recursion when calling `trace_bn_conv_links` (https://github.com/open-mmlab/mmrazor/blob/master/mmrazor/models/pruners/structure_pruning.py#L124). 

To reproduce, simply replace the model (https://github.com/open-mmlab/mmrazor/blob/master/configs/pruning/autoslim/autoslim_mbv2_supernet_8xb256_in1k.py) with RepVGG.

I am refactoring the parser to make it more general and easily to debug. Stay tuned."
How to get the student model checkpoint after finishing knowledge distillation? ,open-mmlab/mmrazor,2022-02-10 11:24:43,10,enhancement,76,1129881583,"Hi,
I find that after finishing knowledge distillation, the checkpoint file is very huge I get, so I think the checkpoint file include the student model and the teacher model. But when we deploy the model, we only need the student model, and I don't know how to split the checkpoint file with mmrazor.
"
[Feature] The design of the loss name field is controversial,open-mmlab/mmrazor,2021-12-27 09:49:57,1,enhancement,32,1089116179,"### Describe the feature

```python
losses=[
     dict(
             type='ChannelWiseDivergence',
             name='loss_cwd_logits', # ---------------- this---------------
             tau=1,
             loss_weight=5,
          )
  ]
```
From the OpenMMLab development model, there should be no additional loss name field, which will bring the following problems:

1. Registrar mode generally does not have this way of writing, which does not conform to the way of class registration. 
2. The user needs to set this parameter every time, is it necessary?  This will increase the burden on users.

I am not sure about your specific needs, I hope to discuss this. Looking forward to your feedback."
Please provide use cases or unit tests for align_methods in BaseDistiller,open-mmlab/mmrazor,2021-12-26 03:19:15,1,,28,1088690986,"When I was reading the code, I found that the `align_method`s description was too few, which is not convenient to understand the code. Can you provide use cases or configurations or unit tests for `align_methods`?

```python
class BaseDistiller(BaseModule, metaclass=ABCMeta):
    """"""Base Distiller.

    In the distillation algorithm, some intermediate results of the teacher
    need to be obtained and passed to the student.

    For nn.Module's outputs, obtained by pytorch forward hook.
    For python function's outputs, obtained by a specific context manager.

    Args:
        align_functions (dict): The details of the functions which outputs need
        to be obtained.
    """"""

    def __init__(self, align_methods=None, **kwargs):
        super(BaseDistiller, self).__init__(**kwargs)
```

And the `align_methods` parameter name and docstr do not match. 


"
About ResNet-12 and ResNet-25.,yaoyao-liu/E3BM,2021-07-27 12:42:00,4,,6,953842408,"你好！
      首先感谢分享这项有趣工作的代码。
      1. 因为我是从MTL跟过来的，所以我不大清楚这个repo代码里的ResNet-12指的就是ECCV 20论文里的ResNet-25吗？因为我看这里的结果跟论文里报告的是一样的，但是网络结构写了个ResNet-12；
      2. 我看这个repo里说网络结构是跟MetaOptNet一样，意思是MetaOptNet的ResNet-12跟MTL论文里的ResNet-12不一样吗？前者更深参数量更多？
      
**最后再一次感谢你们的源代码，100%可复现没毛病！（除了hard task至今没有更新，哈哈）**"
HVU modeling details,holistic-video-understanding/HVU-Dataset,2021-06-13 23:24:31,0,,9,919902054,"Hi, I am trying to replicate the performance of 3D-Resnet-18 on HVU dataset by training for scene labels. 

In the paper (https://arxiv.org/pdf/1904.11451.pdf), Table 3, the MAP is listed as 50.6%.

Is it possible to share the training details for 3D-Resnet-18? 

I am getting a MAP of 37% on 251,794 videos by training using the following configuration:

1. **Learning rate**: 1e-4

2. **Scheduler**: LR scheduler on the plateau (decrease the learning rate by 0.1 when the validation MAP does not improve for 3 epochs)

3. **Optimizer**: Adam

4. **Batch size**: 64

5. Temporal Random crop as augmentations for training followed by scaling and normalization using mean and std of kinetics dataset.

6. Center crop as augmentations for validation followed by scaling and normalization using mean and std of kinetics dataset.

Thanks in advance "
SimCLR version is possible?,SsnL/moco_align_uniform,2022-07-15 01:22:54,0,,4,1305463037,"Hi, thanks for sharing your work,
Have you tried the align and uniform loss combined with SimCLR? or any other contrastive or non-contrastive method? Since the memory requirement for MoCo is very high and only works better for DDP environment.  Thanks!"
Module 'pymesh' has no attribute 'triangle',bertjiazheng/Structured3D,2022-07-06 13:27:00,0,,27,1295842881,This is coming and nothing on internet is helping. pls solve this issue or let us know the solution
Visualize Mesh3D of perspective scenes instead of  Panorama,bertjiazheng/Structured3D,2021-12-15 22:55:49,0,,21,1081589704,"I was trying to use the code to visualize the mesh 3D. I noticed that the code only allow me to visualize the scene using the panorama images. Is it possible instead to visualize the scene only using the prospective images instead of the panorama. 
For example given this 2D image visualize the 3D mesh of it, using only this view 

![rgb_rawlight](https://user-images.githubusercontent.com/19528605/146277467-7ac9ad4a-e299-4362-a17a-48eb719fd8ab.png)
"
Only about 77% of bounding boxes labels can be inferred,bertjiazheng/Structured3D,2020-09-03 09:56:56,4,enhancement,17,691825579,"Hello,

I followed your advice in Issue #13 to infer the semantic labels associated with the bounding boxes, but have only succeeded in inferring 77% of them from the instance.png and semantic.png images of each scene (for 438311 bounding boxes, only 337345 could be inferred, that is more exactly 76.9647579002%).

This missing quarter of information is quite big, and seems as I feared in my message in Issue #13, to be due to object occlusion in the images. Therefore, it seems that it is impossible for end users to infer the missing data. Would you please consider filling in the gap =) ?

Wish you a nice day !"
Ceiling lamps are sofas in scene 00000,bertjiazheng/Structured3D,2020-08-28 13:56:12,6,bug,16,688093908,"Hello again !

I succeeded in associating the 3D bounding boxes with semantic labels, and started digging around the dataset, and while exploring the first scene, I realized that the ceiling lamps were misidentified as sofas (you can check by simply using a color picker to compare the color of the sofas in the background with the ceiling lamps in image Structured3D/scene_00000/2D_rendering/485142/panorama/full/semantic.png). I'll try to put in place some automatic detectors and report back if I find any other incongruity.

Wish you a pleasant day."
Dataset download fails,bertjiazheng/Structured3D,2019-10-09 15:47:49,8,good first issue,3,504738347,"Hi,

I noticed that when downloading the dataset from OneDrive, the operation fails(always), at least for my Ubuntu 16 system. I guess the problem is with unstable OneDrive server.

I was able to solve this issue by using a download manager."
Attention normalization,iVMCL/AOGNet-v2,2022-02-24 12:11:06,1,,5,1149212673,Where is the code of attention normalization? I cannot find it in the whole project.
IS there a way to turn off discriminator training?,barisgecer/TBGAN,2022-10-11 12:59:49,1,,11,1404602724,
H_ab 和H_ba的相关问题,JirongZhang/DeepHomography,2022-09-14 14:50:06,2,,43,1373128885,作者你好，我看论文中是twoline的过程，而代码是oneline，也没有H_ab*H_ba-I的loss. 是我看错了，还是什么情况？
fc输出的8个值是四个顶点坐标？而不是变换矩阵的8个参数？,JirongZhang/DeepHomography,2022-09-06 09:57:09,1,,42,1363025602,
运行test.py出现下面这个问题,JirongZhang/DeepHomography,2022-07-01 03:44:42,0,,41,1290867956,![image](https://user-images.githubusercontent.com/43646121/176833832-24eb10a8-a766-482d-a300-0d49689f6761.png)
Some questions about mask m() function,JirongZhang/DeepHomography,2022-06-01 02:25:24,0,,40,1254718659,"Hello, as for the realization of mask M () network mentioned in your paper, I still don't understand how you realize the function mentioned in your paper through the mask M () module after reading the code. I wonder if you could explain it to me again. Thank you."
SIFT with RANSAC Results Reproducibility,JirongZhang/DeepHomography,2022-02-01 11:21:47,0,,39,1120542244,"Hi,
Thank you very much for sharing your code.

I tried to reproduce the results for **SIFT** with **RANSAC** but they are:
{'RE': 1.7144430284550183, 'LT': 1.8459903052573097, 'LL': 2.4626396163105597, 'SF': 1.8192603742769062, 'LF': 1.799048634072144}
In the paper (Table 1-a row 5) the results are matching for **RE** and **SF** and close to **LF**. However, very far from **LT** and **LL**. 

Could you please help us with that concern?

Thank you,
Kerim"
Training tensor not match error ,JirongZhang/DeepHomography,2022-01-11 10:05:51,3,,37,1098941687,"Hi I face some errors,



Oneline-DLTv1/resnet.py"", line 288, in forward
    feature_loss = torch.sum(torch.mul(feature_loss_mat, mask_ap)) / sum_value
RuntimeError: The size of tensor a (315) must match the size of tensor b (560) at non-singleton dimension 3

when training:
python train.py --gpus 2 --cpus 8 --lr 0.0001 --batch_size 32




Could you please share me how to solve it thx!
"
测试集中为什么要使用args.finetune？,JirongZhang/DeepHomography,2021-12-03 07:21:55,0,,36,1070294277,模型中的参数使用的是 'models/freeze-mask-first-fintune.pth'里面的数据，为什么不是使用我们之前训练好的模型啦？
Mask in training processing,JirongZhang/DeepHomography,2021-09-23 08:35:30,1,,34,1005157605,"Hi,
Firstly,  thanks for sharing your code!
And I have a question with the training process.
As training the model from scratch, I found that for most cases the mask(displayed in tensorboard) was nearly black with only a few bright point and it was normal for scenes  with low texture(like below).

![cases](https://user-images.githubusercontent.com/40864036/134476118-2245d093-92cc-4f47-8445-331d067dcec1.PNG)

![low texture](https://user-images.githubusercontent.com/40864036/134476787-ff30fc27-35fd-482f-a665-8d98494637e0.PNG)

I want know why should this be happening and whether the same case occured in your training process?

"
About the (.npy) files,JirongZhang/DeepHomography,2021-07-04 09:03:38,4,,33,936412280,"A: hi!

Thank you for your contribution. If I need to annotate my data set, what tools should I use to annotate it? In other words, how do I generate the (.npy) file and what is in the (.npy) file? Can you show me the full contents of the npy file?

Thank you and I am looking forward to your reply."
Testing your model on my own data,JirongZhang/DeepHomography,2021-05-02 11:04:54,1,,32,873913376,"Hi,
First of all thanks for sharing your code.

I want to test the model on my own dataset.

What changes will be needed in the test file to work?

1. Am I limited to a specific image size?
2. What values should I use for --w_patch and --h_patch ?
3. Should the chosen coroners need to be with respect to the original image coordinate system or with respect to the patched image?

Thanks,
Yuval"
Question about feature_loss_mat dimension,JirongZhang/DeepHomography,2021-01-22 10:49:41,0,,31,791903081,"I have a question about the dimensions of the feature_loss_mat calculated within resnet.forward.

~~~
feature_loss_mat = triplet_loss(patch_2, pred_I2_CnnFeature, patch_1)

feature_loss = torch.sum(torch.mul(feature_loss_mat, mask_ap)) / sum_value
~~~

feature_loss_mat is calculated using TripletMarginLoss, and the dimensions of the input are (64, 1, H, W). (if batch size is 64)
At this time, the calculated dimension of feature_loss_mat is (64, H, W).

And finally, to calculate the loss, it is multiplied by mask_ap, and the dimension of mask_ap is (64,1,H,W).

In my opinion, the dimensions of feature_loss_mat and mask_ap should be the same. Is the difference between the two dimensions intended?"
Question about folder Coordinate,JirongZhang/DeepHomography,2021-01-08 09:27:37,2,,30,781977980,How is the .npy file generated in the Coordinate folder?
mean and std of datasets,JirongZhang/DeepHomography,2020-12-21 11:25:08,0,,29,772088494,"why use these mean and std values? And not normalise it to [-1, 1] by using transforms"
"hi~ in train.py, if finetune is true, the model will load 'models/freeze-mask-first-fintune.pth'. Can you share the .pth file？Thanks~~",JirongZhang/DeepHomography,2020-12-14 09:00:33,2,,28,766159881,
malloc_consolidate(): invalid chunk size,JirongZhang/DeepHomography,2020-12-13 07:22:50,0,,27,765122742,"Does anyone encounter this error:""malloc_consolidate(): invalid chunk size"" without more error information?
Every time I train the model for 1 to 4 epochs it happens. "
Homography Matrix for optical flow,JirongZhang/DeepHomography,2020-11-03 08:12:20,1,,25,735110762,"Im trying to use the output of the net (H -> homography matrix) to create an optical flow map 
the pred_full is correct and the interpolate returns the correct frame, however when is use the formula the create the map from the matrix i get worng flow values 

code use to calc the map: 
   * new cord is the cord for frame 2
   * H is homo matrix
 
   ``` [X,Y] = np.meshgrid(np.arange(0,cols),np.arange(0,rows))
    newCord = np.matmul(**H**,oldCord)                      
    newCord[0,:] = newCord[0,:]/newCord[2,:]                                 
    newCord[1,:] = newCord[1,:]/newCord[2,:]
    newX = np.reshape(newCord[0,:],(rows,cols))
    newY = np.reshape(newCord[1,:],(rows,cols))
    dX = newCord[0,:] - X.flatten()
    dY = newCord[1,:] - Y.flatten()```


What am i doing wrong???"
Data processing: Image normalization,JirongZhang/DeepHomography,2020-10-08 13:15:14,1,,24,717335177,"Hello,
I'm working with this code and have a question concerning data processing involved. In the following line for TrainData loader, what are these mean and std values? Where are they calculated from? I was wondering if I can use some normalization functions from OpenCV or Numpy for this purpose. 
https://github.com/JirongZhang/DeepHomography/blob/fab5701bc4680b83821c83d4a06992fe1bddf609/Oneline-DLTv1/dataset.py#L26

Thank you
Warm regards,
Dharm"
'BatchNorm2d' object has no attribute '_non_persistent_buffers_set',JirongZhang/DeepHomography,2020-09-22 06:28:17,1,,23,706114161,"Hai @JirongZhang,

When I try to run test.py I encountered this error can you please help me what is that I'm missing here ?.
Pytorch version: 1.6.0
 "
Code for baseline method,JirongZhang/DeepHomography,2020-09-14 11:30:50,0,,22,701014253,"Hi, thanks for open source the code.

Can I ask where is the baseline method proposed by the paper _Deep Image Homography Estimation_ ?  The original model is pretty small, I hope to try a bigger baseline model first. Looking forward to your reply, thanks"
May I ask whether this project is for image stitching? I look forward to your answer,JirongZhang/DeepHomography,2020-09-02 08:31:03,2,,21,690827902,
The effect of using Triplet loss,JirongZhang/DeepHomography,2020-08-11 14:38:57,11,,20,676932961,"Hi @JirongZhang ,

those two statements seem to be mutually exclusive. Could you please elaborate on that? Is using the triplet loss beneficial or not? 

Readme:

> The ""Oneline"" model can produce almost comparable performance as ""Doubleline"" model, but much easier to optimize.


Paper:

> Triplet loss. We further exam the effectiveness of our triplet loss by removing the term of Eq. 5 from Eq. 6. As shown in Table 2(c) “w/o. triplet loss”, the triplet loss decreases errors over 50%, especially beneficial in LT (118.42%lower error) and LL (70.10% lower error) scenes, demonstrating that it not only avoids the problem of obtaining trivial solutions,  but also facilitates a  better optimization.
"
about mask predictor ,JirongZhang/DeepHomography,2020-03-04 09:42:15,0,,11,575262152," Hi Zhang, 
as you said in the paper, 'a sub-network m(·) learns to produce an inlier probability map or mask, highlighting the content in the feature maps that contribute much for the homography estimation. '  I know clearly what is the function of this predictor but could you pls show me in detail what is the sub-network like and how does it work to reject the outlier? Thank you."
Question about the image transformation range,JirongZhang/DeepHomography,2020-01-07 10:40:22,2,,9,546210348,"In your project's homepage, the differences between the source images and the target images are small, most are the translation. I wonder whether your method could deal with the large transform such as random affine transform."
Question about the paper,JirongZhang/DeepHomography,2019-11-23 08:34:08,1,,6,527529078," i indicates a pixel location in the masks and feature maps. Here we utilize spatial transform network [18] to achieve the warping operation.

Eg5     i   indicates a pixel location ?   what the pixel loacation mean ?   image location in  batch? 

how to understand the  ""Here we utilize spatial transform network [18] to achieve the warping operation.""  

thx~ "
Some questions about paper,JirongZhang/DeepHomography,2019-11-23 04:18:16,1,,4,527507571,"Hi,Mr.Zhang ，i have some questions about the paper，can you explain it ？ thanks~
1、image input size is  315 × 560 ？
2、How to get Ga with Ma  and  Fa ?
3、And then ， np.dstack  Ga and Gb   to  Resnet34 ？
4、Training data is  imageA imageB and Homography matrix  from imageA to imageB ?
5、Can the training set be generated like 《Deep image homography estimation》?

I want to try to implement your paper, but I feel that there will be some difficulties in the loss function part, can I get your help?"
Is ZAP beneficial also for Leaky Relu,gilshm/zap,2020-09-07 13:22:12,2,,2,695096428,"Very interesting paper.
I see the point in predicting zero activations which reduces the MAC. And central to this argument is the widespread usage of ReLU.

Will ZAP be beneficial if my CNN is using Leaky Relu and thus i expect not many activations will be zero?

Thankyou"
How to train and test the model? Could you please give me some detail descriptions of the codes?,gilshm/zap,2019-10-09 07:15:43,1,,1,504462315,"Thanks for the cool tool! 

Could you please give me some detail descriptions of the code?
And something user should pay special attention to?

"
"require the train, valid and test list of UTKFace",fregu856/ebms_regression,2021-10-17 14:47:45,0,,5,1028346465,Could you share the train/valid and test list of UTKFace dataset?
/root/ebms_regression/1dregression/2/gt_x_values_2_scores.pkl,fregu856/ebms_regression,2020-12-03 07:57:04,0,,4,755960646,"Hi, grateful for your excellent work! Reading your paper make me to have a more clear understanding for neural networks. 

I try to run ""nce+_eval.py"" but failed. The file with path ""/root/ebms_regression/1dregression/2/gt_x_values_2_scores.pkl"" is needed but I did not found it. Could you offer the file as a download link?"
Codes about head-pose estimation and age estimation,fregu856/ebms_regression,2020-11-29 06:22:28,1,,3,752850390,"Hi, grateful for your excellent work!  I want to ask a question that when the code of **head-pose estimation and age estimation** could be updated? Thanks a lot!"
question about dimp-nce+,fregu856/ebms_regression,2020-10-10 07:28:54,0,,2,718556782,"I applied your NCE+ method to super_dimp.
As written in the paper, only the loss function was modified.
And both training and inference used super_dimp's method as it is.
However, it is performing worse than super_dimp.
(GOT-10K AUC - 0.801 vs 0.759)
Are there any other modifications not mentioned in the paper?
(Are the same values ​​for the loss weight and learning rate used as the super_dimp method?)
"
question about code for dimp-nce+,fregu856/ebms_regression,2020-08-05 10:07:26,2,,1,673419417,"hi，i'm grateful for your excellent work. I want to ask a question about code for dimp-nce+, when the code could be uploaded in pytracking? Thanks."
tabular data/ noisy instances ,google-research/noisy-fewshot-learning,2022-05-09 10:37:11,0,,2,1229510709,"Hi,
thanks for sharing your implementation. I have two questions about it:

1.	Does it also work on tabular data?
2.	Is it possible to identify the noisy instances (return the noisy IDs or the clean set)?

Thanks!"
Code for CORe50 dataset,tyler-hayes/REMIND,2021-09-12 10:41:25,0,,5,994123773,"Hey, 

Thank you for this awesome repo!

May I know if there are any plans to share code for the CORe50 dataset? 
"
the purpose of using a pooling layer,XudongLinthu/context-gated-convolution,2022-11-07 02:41:45,0,,5,1437680012,"Hello, I would like to ask you what is the purpose of using a pooling layer to make the feature map of h*w into h'*w'? AdaptiveAvgPool2d((ws,ws)) in the code you posted, why output_size=kernel_size?"
About  context-gated-convolution for action recgnition,XudongLinthu/context-gated-convolution,2020-11-29 12:53:50,1,,2,752910534,"Nice work!
I wonna use context-gated-convolution in action recgnition. I noticed that you use torch.nn.Unfold in layer.py, and it needs a 4-D input(batch x channel x height x width). But I don't know how to do to extend context-gated-convolution to a 5-D input (batch x channel x frams x height x width) . Will you release your code for action recgnition in the paper?
Thanks!"
Some questions,jiahaowork/idam,2022-05-26 11:30:15,0,,3,1249451845,"Thank you for your wonderful work! I encounter some problems when I try to test the performance of ICP, FPFH+RANSAC and FGR. I use the Open3D library but do not know how to set hyper-parameters. Could you share the codes of ICP, FPFH+RANSAC and FGR with Open3d? "
Some question about weight loss,jiahaowork/idam,2022-04-20 10:12:36,0,,2,1209472617,"Hello, can't your one-to-one LOSS design method lead to the default correspondence according to index when testing? In this way, the loss during training will be very low, but the test results will be very poor."
Is all the relevant code here?,jandress94/adversarial_tshirt,2021-02-03 10:00:46,1,,2,800169089,"Is all the relevant code here? I can't find the details in the paper such as the formula (3). I want to know more codes about the details in your paper. Thanks. What's more, If you could supplement your code documentation I would appreciate it."
Plan about open source training code,Dong1P/MTRNN,2021-09-15 12:48:58,0,,1,997045233,"Hi, thanks for you great work and I wonder if there is plan to release the training code?"
No test accuracy improvement between baseline augmentation and searched augmentation,moskomule/dda,2021-09-27 10:54:01,2,,9,1007997770,"Hi, I have noticed that there is no noticeable improvement from the baseline test accuracy when using the policy searched by the code from the repo.

Baseline: 
![image](https://user-images.githubusercontent.com/32617884/134895001-d7b4fdcd-58e1-461e-b1fb-8375fed51ead.png)

CIFAR10 searched policy:
![image](https://user-images.githubusercontent.com/32617884/134895235-a3286b88-98f0-446c-a390-841ae85be45b.png)

Is this a common occurence?
"
"search on cifar10 cost about 6 hours, using 1080Ti ",moskomule/dda,2021-05-13 12:18:57,3,,8,890992487,
TypeError: cannot pickle 'cv2.TonemapReinhard' object,hywang99/LCDPNet,2022-10-18 02:43:50,1,,3,1412485499,"你好！我想用自己的图片测试一下pretrain模型的效果，
在src/config/ds/test.yaml配置了图片路径
然后运行 python src/test.py checkpoint_path=trained_on_ours.ckptpython src/test.py checkpoint_path=trained_on_ours.ckpt

报了这个错误解决不了，想请教一下：
![屏幕截图 2022-10-18 104314](https://user-images.githubusercontent.com/14309504/196323366-f6a9a69d-983f-42e5-a354-4efd12f92737.png)
"
NotFoundError: ../crfasrnn_keras/./src/cpp/high_dim_filter.so: undefined symbol,sadeepj/crfasrnn_keras,2022-08-10 19:09:31,0,,81,1335067967,"I'm having this problem. My TensorFlow version is 2.9.1 


File /local/vol00/home/troy3/Graph_seg_leixin/crf/crfasrnn_keras/./src/high_dim_filter_loader.py:28 in <module>
    custom_module = tf.load_op_library(os.path.join(os.path.dirname(__file__), 'cpp', 'high_dim_filter.so'))

  File ~/anaconda3/envs/tf-gpu-roys/lib/python3.9/site-packages/tensorflow/python/framework/load_library.py:54 in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)

NotFoundError:../crf/crfasrnn_keras/./src/cpp/high_dim_filter.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb"
How to use crfasrnn_keras in Google Colab,sadeepj/crfasrnn_keras,2022-01-31 12:46:59,0,,80,1119384632,I have tried everything however I have been unable to get this to work in Google Colab. Any suggestions on how to solve this issue?
using crfrnn in windows 10,sadeepj/crfasrnn_keras,2021-03-04 05:35:02,0,,79,821771938,"how can i build crfrnn to work in windows 10
please i need specific instruction to follow to achieve my work
i am still a beginner in python programing  
thank you in advanced "
Keras from Tensorflow Version,sadeepj/crfasrnn_keras,2021-01-08 18:18:57,1,,78,782310342,Will there be an update where it uses keras from tensorflow and not keras the api?
2 root error(s) found. ,sadeepj/crfasrnn_keras,2020-12-25 17:25:14,0,,77,774763140,"Hey there, I added the crfrnn layer to the segmentation model U-Net from https://github.com/qubvel/segmentation_models/blob/master/segmentation_models/models/unet.py library. But at the fit time this error occurs: InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Incompatible shapes: [524288] vs. [16384]
	 [[node loss_1/crfrnn_loss/custom_loss/mul_1 (defined at /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3009) ]]
	 [[gradients_1/crfrnn_1/truediv_4_grad/RealDiv/_774]]
  (1) Invalid argument:  Incompatible shapes: [524288] vs. [16384]
	 [[node loss_1/crfrnn_loss/custom_loss/mul_1 (defined at /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3009) ]]
0 successful operations.
0 derived errors ignored. [Op:__inference_keras_scratch_graph_143907]

Function call stack:
keras_scratch_graph -> keras_scratch_graph

Here are the last few lines of model summary:
decoder_stage4a_conv (Conv2D)   (None, 128, 128, 16) 4608        decoder_stage4_upsampling[0][0]  
__________________________________________________________________________________________________
decoder_stage4a_bn (BatchNormal (None, 128, 128, 16) 64          decoder_stage4a_conv[0][0]       
__________________________________________________________________________________________________
decoder_stage4a_relu (Activatio (None, 128, 128, 16) 0           decoder_stage4a_bn[0][0]         
__________________________________________________________________________________________________
decoder_stage4b_conv (Conv2D)   (None, 128, 128, 16) 2304        decoder_stage4a_relu[0][0]       
__________________________________________________________________________________________________
decoder_stage4b_bn (BatchNormal (None, 128, 128, 16) 64          decoder_stage4b_conv[0][0]       
__________________________________________________________________________________________________
decoder_stage4b_relu (Activatio (None, 128, 128, 16) 0           decoder_stage4b_bn[0][0]         
__________________________________________________________________________________________________
final_conv (Conv2D)             (None, 128, 128, 1)  145         decoder_stage4b_relu[0][0]       
__________________________________________________________________________________________________
sigmoid (Activation)            (None, 128, 128, 1)  0           final_conv[0][0]                 
__________________________________________________________________________________________________
crfrnn (CrfRnnLayer)            [(None, 128, 128, 1) 3           sigmoid[0][0]                    
                                                                 input_1[0][0]                    
==================================================================================================
Total params: 29,933,108
Trainable params: 29,896,692
Non-trainable params: 36,416

Can anyone help me?"
FCN pretrained weights on COCO data,sadeepj/crfasrnn_keras,2020-12-08 03:21:28,0,,76,759026113,I am looking for a pre-trained FCN model on COCO data. I noticed in your article that FCN is trained on COCO data. Could you provide weights? Thank you very much!
Using the CRF_Layer results is Seg Fault 11,sadeepj/crfasrnn_keras,2020-07-07 17:05:38,1,,75,652481987,"I'm trying to create a UNet with the CRF Layer as the output layer but when I run it through terminal (Mac OS Mojave 10.14.6), the images are found and the two classes are found but the next line is Segmentation Fault: 11

The image size are below the 500 limit, the batch size is 1. 
My TF version is Version: 2.1.0
Keras is Version: 2.3.1

When I run the code it returns this:

Epoch 1/2
Found 512 images belonging to 1 classes.
Found 512 images belonging to 1 classes.
Segmentation fault: 11


"
sorry，i have the same problem，can you tell how should i solve it？thanks！,sadeepj/crfasrnn_keras,2020-06-18 08:04:57,0,,74,640998456,"> sorry，i have the same problem，can you tell how should i solve it？thanks！
Hello! When I run the sh.compile.sh command, it compiles successfully, but when I run train.py, I get an error: undefined symbol:_ZTIN10tensorflow8opkernelE. I checked a lot of information on the Internet and did not solve it. Hope you help me see what is the problem. Thanks again! ! !
ubuntu :16.04 ; nvidia-driver-440;cuda:10.0; cudnn:7.5.0; tensorflow-gpu:1.13.1; gcc/g++:5.4.0;
The compile instructions are as follows:
TF_INC=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())')
TF_LIB=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_lib())')
g++ -std=c++11 -D_GLIBCXX_USE_CXX11_ABI=0 -shared high_dim_filter.cc modified_permutohedral.cc -o high_dim_filter.so -fPIC -I $TF_INC -I$TF_INC/external/nsync/public/ -L$TF_LIB -ltensorflow_framework -O2

"
"> @panfengliHello, may I ask how this problem was finally solved?Thank you very much!",sadeepj/crfasrnn_keras,2020-06-18 05:52:28,0,,73,640929257,"> @panfengli
> Could you tell me that where should I fixed using the follow code？Thank you!
> 
> TF_CFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_compile_flags()))') )
> TF_LFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_link_flags()))') )
> 
> g++ -std=c++11 -shared high_dim_filter.cc modified_permutohedral.cc -o high_dim_filter.so -fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} -O2

These can be run as terminal commands. I'm using tensorflow 1.10 and these commands removed the error for me

_Originally posted by @qchenclaire in https://github.com/sadeepj/crfasrnn_keras/issues/53#issuecomment-457768795_"
"GPU version does not refine unary potential, but CPU version does",sadeepj/crfasrnn_keras,2020-05-26 14:49:24,0,,72,624943185,"I managed to implement the crf-rnn layer into my custom model. When using the CPU version (master branch), the crf-rnn layer nicely refines my segmentation mask, which is great! 

However, when I tried to use the GPU version, the output of the crf-rnn layer was still the same as its input, meaning that the segmentation map did not get refined. I used the same parameters as for the CPU version. 

Did anyone encounter the same issue? Is there a fix for this problem?"
does anybody know how to use this code to refine the existing segmentation fcn8 code?,sadeepj/crfasrnn_keras,2019-08-01 21:04:06,1,,68,475888437,Does anybody know how to use this code to refine the existing segmentation fcn8s code?
Segmentation fault (core dumped),sadeepj/crfasrnn_keras,2019-07-09 15:23:08,3,,66,465844542,"Hi,
I am a college student.I have used several days to run this demo.In the last steep,I use `python3 run_demo.py` ,the result are as follow:
`Using TensorFlow backend.
WARNING: Logging before flag parsing goes to stderr.
W0709 23:16:12.372608 140072129275712 deprecation_wrapper.py:119] From /home/aimer/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

W0709 23:16:12.372946 140072129275712 deprecation_wrapper.py:119] From /home/aimer/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W0709 23:16:12.376471 140072129275712 deprecation_wrapper.py:119] From /home/aimer/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

W0709 23:16:12.395432 140072129275712 deprecation_wrapper.py:119] From /home/aimer/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

W0709 23:16:12.508162 140072129275712 deprecation_wrapper.py:119] From /home/aimer/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

W0709 23:16:12.512938 140072129275712 deprecation.py:506] From /home/aimer/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Segmentation fault (core dumped)
`
Could anyone help me?
Thanks."
"ValueError: Operands could not be broadcast together with shapes (21, 34, 4) (21, 34, 3)",sadeepj/crfasrnn_keras,2019-04-08 20:05:25,2,,60,430633795,"Running the demo as instructed, giving this output

`Traceback (most recent call last):
  File ""run_demo.py"", line 48, in <module>
    main()
  File ""run_demo.py"", line 38, in main
    model = get_crfrnn_model_def()
  File ""./src/crfrnn_model.py"", line 91, in get_crfrnn_model_def
    score_fused = Add()([score2, score_pool4c])
  File ""/home/hallab/.local/lib/python3.5/site-packages/keras/engine/base_layer.py"", line 431, in __call__
    self.build(unpack_singleton(input_shapes))
  File ""/home/hallab/.local/lib/python3.5/site-packages/keras/layers/merge.py"", line 91, in build
    shape)
  File ""/home/hallab/.local/lib/python3.5/site-packages/keras/layers/merge.py"", line 61, in _compute_elemwise_op_output_shape
    str(shape1) + ' ' + str(shape2))
ValueError: Operands could not be broadcast together with shapes (21, 34, 4) (21, 34, 3)
`"
Memory allocation runtime errors,sadeepj/crfasrnn_keras,2019-02-19 11:31:21,0,,57,411878036,"I have memory allocation errors upon running 'run_demo.py'.

There were no compilation issues. Could someone please print a new requirements.txt with the exact versions of all the packages? through `pip freeze -r requirements.txt`

`python(4054,0x10dc135c0) malloc: *** error for object 0x7fbb9ca44d38: pointer being freed was not allocated
Python(4054,0x10dc135c0) malloc: *** set a breakpoint in malloc_error_break to debug
Abort trap: 6`
"
Cannot flatten the output of the crfnn_layer,sadeepj/crfasrnn_keras,2019-02-16 23:24:35,0,,56,411126934,"Hey,

I am trying to flatten the output of the crffnn_layer, because I have a binary classification task, instead of a segmentation (i know... ). I am getting the error of the a dimension mistatch:


`
ValueError: Dimensions must be equal, but are 250000 and 94864 for 'dense_1/MatMul' (op: 'MatMul') with input shapes: [1,250000], [94864,512].

`

The input are 64x64x3 images (channels last), which are convoluted and then fed to the model. Currently I am using the network architecture that is provided with the examples, adapted:
`

        channels, height, weight = 3, 500, 500

        # Input
        input_shape = (height, weight, 3)
        img_input = Input(shape=self.input_shape)
        #img_input = Cropping2D((3,3))(img_input)

        # Add plenty of zero padding
        x = ZeroPadding2D(padding=(218, 218))(img_input)

        # VGG-16 convolution block 1
        x = Conv2D(64, (3, 3), activation='relu', padding='valid', name='conv1_1')(x)
        x = Conv2D(64, (3, 3), activation='relu', padding='same', name='conv1_2')(x)
        x = MaxPooling2D((2, 2), strides=(2, 2), name='pool1')(x)

        # VGG-16 convolution block 2
        x = Conv2D(128, (3, 3), activation='relu', padding='same', name='conv2_1')(x)
        x = Conv2D(128, (3, 3), activation='relu', padding='same', name='conv2_2')(x)
        x = MaxPooling2D((2, 2), strides=(2, 2), name='pool2', padding='same')(x)

        # VGG-16 convolution block output3
        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='conv3_1')(x)
        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='conv3_2')(x)
        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='conv3_3')(x)
        x = MaxPooling2D((2, 2), strides=(2, 2), name='pool3', padding='same')(x)
        pool3 = x

        # VGG-16 convolution block 4
        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv4_1')(x)
        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv4_2')(x)
        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv4_3')(x)
        x = MaxPooling2D((2, 2), strides=(2, 2), name='pool4', padding='same')(x)
        pool4 = x

        # VGG-16 convolution block 5
        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv5_1')(x)
        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv5_2')(x)
        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv5_3')(x)
        x = MaxPooling2D((2, 2), strides=(2, 2), name='pool5', padding='same')(x)

        # Fully-connected layers converted to convolution layers
        x = Conv2D(4096, (7, 7), activation='relu', padding='valid', name='fc6')(x)
        x = Dropout(0.5)(x)
        x = Conv2D(4096, (1, 1), activation='relu', padding='valid', name='fc7')(x)
        x = Dropout(0.5)(x)
        x = Conv2D(21, (1, 1), padding='valid', name='score-fr')(x)

        # Deconvolution
        score2 = Conv2DTranspose(1, (4, 4), strides=2, name='score2')(x)

        # Skip connections from pool4
        score_pool4 = Conv2D(1, (1, 1), name='score-pool4')(pool4)
        score_pool4c = Cropping2D((5, 5))(score_pool4)
        score_fused = Add()([score2, score_pool4c])
        score4 = Conv2DTranspose(1, (4, 4), strides=2, name='score4', use_bias=False)(score_fused)

        # Skip connections from pool3
        score_pool3 = Conv2D(1, (1, 1), name='score-pool3')(pool3)
        score_pool3c = Cropping2D((9, 9))(score_pool3)
        score_pool3c = ZeroPadding2D(padding=((1,0), (1,0)))(score_pool3c)


        # Fuse things together
        score_final = Add()([score4, score_pool3c])

        # Final up-sampling and cropping
        upsample = Conv2DTranspose(1, (16, 16), strides=8, name='upsample', use_bias=False)(score_final)
        upscore = Cropping2D(((31, 37), (31, 37)))(upsample)

        output = CrfRnnLayer(image_dims=(height, weight),
                            num_classes=1,
                            theta_alpha=160.,
                            theta_beta=3.,
                            theta_gamma=3.,
                            num_iterations=10,
                            name='crfrnn')([upscore, img_input])

        k = Flatten()(output)

        k = Dense(512, activation='relu')(k)
        k = Dropout(.5)(k)
        k = Dense(512, activation='relu')(k)
        predictions = Dense(1, activation='sigmoid')(k)

        # Build the model
        model = Model(img_input, predictions, name='crfrnn_net')

        return model

`

I am stuck, as my intuition would say that it is fine to flatten the probability map and then feed it to a NN. 

Any tips? 
"
modified_permutohedral.cc line 362 feature is not defined.,sadeepj/crfasrnn_keras,2018-11-26 09:00:16,2,,51,384231207,"modified_permutohedral.cc line 362:

const float * f = (feature + k * num_dimensions);

it is not defined"
An illegal memory access was encounteredan illegal memory access was encountered,sadeepj/crfasrnn_keras,2018-11-05 23:41:59,4,,49,377630724,"Tensorflow 1.11.0
Cuda 9.0
Python 2.7.15
GTX 1060, 396 driver
Happens after run python demo. CPU version works well

`an illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredCuda kernel failed. Error: an illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredCuda kernel failed. Error: an illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredCuda kernel failed. Error: an illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredCuda kernel failed. Error: an illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredCuda kernel failed. Error: an illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredCuda kernel failed. Error: an illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredCuda kernel failed. Error: an illegal memory access was encounteredCuda kernel failed. Error: an illegal memory access was encounteredCuda kernel failed. Error: an illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredCuda kernel failed. Error: an illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredCuda kernel failed. Error: an illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredCuda kernel failed. Error: an illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredCuda kernel failed. Error: an illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredCuda kernel failed. Error: an illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredCuda kernel failed. Error: an illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredan illegal memory access was encounteredCuda kernel failed. Error: an illegal memory access was encounteredan illegal memory access was encountered2018-11-06 02:40:13.266582: E tensorflow/stream_executor/cuda/cuda_dnn.cc:353] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR`
"
About the Batchsize,sadeepj/crfasrnn_keras,2018-11-02 05:10:01,0,,48,376671712,"I notice the README showing the batchsize must equal one, 
But I write a simple training code about classification problem ,  just applying Keras API with batchsize==5. 
`model.fit(data, labels, epochs=10, batch_size=5)`
And there is no error when running it. May there be some latent incorrectness?

"
Training code available now?,sadeepj/crfasrnn_keras,2018-10-24 12:38:34,15,,47,373460910,I want to use the CRF layer in my own work which class number is 2. I would appreciate if you share your training code. Thank you!
Using the CRF layer with some other segmentation network,sadeepj/crfasrnn_keras,2018-10-07 08:15:33,8,,46,367532084,"Hi,
I wish to use the CRF layer with my own segmentation network which is an encoder-decoder network with skip connections. If I use the crf layer as the last layer what kind of things I should keep in mind?  
The output of my model has number of channels equal to number of classes (2 in my case) and activation is sigmoid. I connect this layer directly with the CRF layer without any modification to CRF parameters like theta_alpha, theta_beta and theta_gamma. The final loss function that I am using is binary cross entropy. Output of the CRF is being directly fed to the loss function. At the time of inference I am taking argmax for getting the labels. But the output label for every pixel is mostly the same.  
Can anyone please help me to understand that what are the things that are particular to the FCN output such that the CRF layer works with it?

Also, If you could please share the training script? "
Compile Issue with high_dim_filter.so On Nvidia TX1: modified_permutohedral.cc error: 'feature' was not declared in this scope,sadeepj/crfasrnn_keras,2018-09-25 16:35:11,1,,45,363667052,"Hi. I ran into a modified_permutohedral.cc compile issue with Nvidia TX1 (Ubuntu 16). I am running Tensorflow 1.5 and Keras 2.2.2. I am compiling the gpu_version (USE_GPU := 1) with CUDA 8.

---------------------------------------------
modified_permutohedral.cc:359:22: error: 'feature' was not declared in this scope
   const float * f = (feature + k * num_dimensions);
                      ^
modified_permutohedral.cc: In member function 'void ModifiedPermutohedral::sseCompute(tensorflow::Tensor&, const tensorflow::Tensor&, int, bool, bool) const':
modified_permutohedral.cc:622:51: error: 'seqCompute_cpu' was not declared in this scope
  seqCompute_cpu( out, in, value_size, reverse, add);

-------------------------------
(cv40py35) nvidia@tegra-ubuntu:~/cviz/crfasrnn_keras/src/cpp$ make
nvcc -std=c++11 -c -o cudacode.o high_dim_filter.cu -I/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include -I/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/external/nsync/public -D_GLIBCXX_USE_CXX11_ABI=1  -x cu -Xcompiler -fPIC --expt-relaxed-constexpr -D FILTER_GPU=1
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(56): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(80): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(100): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(192): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(216): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(236): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(52): warning: integer conversion resulted in a change of sign

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(147): warning: integer conversion resulted in a change of sign

nvcc -std=c++11 -c -o modified_permutohedral.o modified_permutohedral.cu -I/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include -I/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/external/nsync/public -D_GLIBCXX_USE_CXX11_ABI=1  -x cu -Xcompiler -fPIC --expt-relaxed-constexpr -D FILTER_GPU=1
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(56): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(80): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(100): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(192): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(216): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(236): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(52): warning: integer conversion resulted in a change of sign

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(147): warning: integer conversion resulted in a change of sign

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(56): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(80): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(100): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(192): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(216): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(236): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(52): warning: integer conversion resulted in a change of sign

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(147): warning: integer conversion resulted in a change of sign

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/generated_message_reflection.h(689): warning: variable ""unused"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(56): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(80): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(100): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(192): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(216): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/stubs/atomicops_internals_arm64_gcc.h(236): warning: variable ""temp"" was set but never used

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(52): warning: integer conversion resulted in a change of sign

/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/google/protobuf/arena_impl.h(147): warning: integer conversion resulted in a change of sign

g++ -std=c++11 -D_GLIBCXX_USE_CXX11_ABI=0 -shared -fPIC -I/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include -O2  -I/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow/include/external/nsync/public -o high_dim_filter.so high_dim_filter.cc modified_permutohedral.cc cudacode.o modified_permutohedral.o -L/home/nvidia/.virtualenvs/cv40py35/lib/python3.5/site-packages/tensorflow -ltensorflow_framework -L/usr/local/cuda/lib64 -lcuda -lcudart -D FILTER_GPU=1
modified_permutohedral.cc: In member function 'void ModifiedPermutohedral::init_cpu(const float*, int, int)':
modified_permutohedral.cc:359:22: error: 'feature' was not declared in this scope
   const float * f = (feature + k * num_dimensions);
                      ^
modified_permutohedral.cc: In member function 'void ModifiedPermutohedral::sseCompute(tensorflow::Tensor&, const tensorflow::Tensor&, int, bool, bool) const':
modified_permutohedral.cc:622:51: error: 'seqCompute_cpu' was not declared in this scope
  seqCompute_cpu( out, in, value_size, reverse, add);
                                                   ^
Makefile:50: recipe for target 'high_dim_filter.so' failed
make: *** [high_dim_filter.so] Error 1
"
Question about the loss and pretrained FCN-8s,sadeepj/crfasrnn_keras,2018-09-04 07:56:59,1,,43,356693692,"Hi Sadeep,

Thanks for making this code available!

I would like to ask a few questions regarding the paper.

When you plugged the CRF layer to the pretrained FCN-8s and trained end-to-end, did the (softmax) **loss** actually go down? How many epochs did you train the model to see that it eventually went down? I tried training end-to-end ('lr=1e-13, momentum=0.99' as described in the paper) with the pretrained FCN-8s weights extracted from your model but the loss does not seem to go down :(

Could you please share the (Keras) pretrained FCN-8s weights that you used? Also, did you use any data augmentation?

Thank you very much in advance!
Best regards."
Running error,sadeepj/crfasrnn_keras,2018-08-26 09:00:48,2,,40,354078808,"I meet the error ""tensorflow.python.framwork.errors_impl.NotFoundError: libtensorflow_framework.so: cannot open shared object file : No such file or directory"". I hope you have time to help solve it."
NaN value Issue,sadeepj/crfasrnn_keras,2018-07-19 04:20:09,1,,38,342573826,"Hi, I use the crf layer as an output layer to fine tune the prediction of a contour detection CNN. However, after 1000-2000 iterations, all value is changed to nan. How can I avoid the nan value issue? Thx so much for ur help."
A repo that incorporate superpixel-based higher order CRF,sadeepj/crfasrnn_keras,2018-06-02 17:38:16,0,,36,328762526,"Hi, just want to let people know, I have a [repo ](https://github.com/liyin2015/superpixel_crfasrnn)build on this one here to incorporate superpixel-based higher-order cues, which by simply feeding in one or multiple segmented images and tune one or two hyperparamters can work.

https://github.com/liyin2015/superpixel_crfasrnn"
Adding an additional pair-wise potential,sadeepj/crfasrnn_keras,2018-05-11 11:24:41,0,,33,322262628,"Hello Sadeep, 

First all thank for your work, very helpful. I am trying to improve the semantic segmentation results by adding additional potentials in the crf energy function but I don't know if I am doing it right. I am adding these lines in the code to add a potential for the depth  : 


```
        # Weights of the bilateral kernel
        self.bilateral_ker_weights = self.add_weight(name='bilateral_ker_weights',
                                                     shape=(self.num_classes, self.num_classes),
                                                     initializer='uniform',
                                                     trainable=True)

        # Weights of the bilateral kernel_dep
        self.bilateral_ker_weights_dep = self.add_weight(name='bilateral_ker_weights_dep',
                                                     shape=(self.num_classes, self.num_classes),
                                                     initializer='uniform',
                                                     trainable=True)

        # Compatibility matrix
        self.compatibility_matrix = self.add_weight(name='compatibility_matrix',
                                                    shape=(self.num_classes, self.num_classes),
                                                    initializer='uniform',
                                                    trainable=True)

        super(CrfRnnLayer, self).build(input_shape)

    def call(self, inputs):
        unaries = tf.transpose(inputs[0][0, :, :, :], perm=(2, 0, 1))
        rgb = tf.transpose(inputs[1][0, :, :, :], perm=(2, 0, 1))

        dep = tf.transpose(inputs[2][0, :, :, :], perm=(2, 0, 1))


        c, h, w = self.num_classes, self.image_dims[0], self.image_dims[1]
        all_ones = np.ones((c, h, w), dtype=np.float32)

        # Prepare filter normalization coefficients
        spatial_norm_vals = custom_module.high_dim_filter(all_ones, rgb, bilateral=False,
                                                          theta_gamma=self.theta_gamma)

        bilateral_norm_vals = custom_module.high_dim_filter(all_ones, rgb, bilateral=True,
                                                            theta_alpha=self.theta_alpha,
                                                            theta_beta=self.theta_beta)

        bilateral_norm_vals_dep = custom_module.high_dim_filter(all_ones, dep, bilateral=True,
                                                            theta_alpha=self.theta_alpha,
                                                            theta_beta=self.theta_beta)

        q_values = unaries

        for i in range(self.num_iterations):
            softmax_out = tf.nn.softmax(q_values, dim=0)

            # Spatial filtering
            spatial_out = custom_module.high_dim_filter(softmax_out, rgb, bilateral=False,
                                                        theta_gamma=self.theta_gamma)

            spatial_out = spatial_out / spatial_norm_vals

            # Bilateral filtering
            bilateral_out = custom_module.high_dim_filter(softmax_out, rgb, bilateral=True,
                                                          theta_alpha=self.theta_alpha,
                                                          theta_beta=self.theta_beta)

            bilateral_out = bilateral_out / bilateral_norm_vals

            # Bilateral filtering_dep
            bilateral_out_dep = custom_module.high_dim_filter(softmax_out, dep, bilateral=True,
                                                          theta_alpha=self.theta_alpha,
                                                          theta_beta=self.theta_beta)

            bilateral_out_dep = bilateral_out_dep / bilateral_norm_vals_dep

            # Weighting filter outputs
            message_passing = (tf.matmul(self.spatial_ker_weights,tf.reshape(spatial_out, (c, -1))) + tf.matmul(self.bilateral_ker_weights,tf.reshape(bilateral_out, (c, -1))) + tf.matmul(self.bilateral_ker_weights_dep,tf.reshape(bilateral_out_dep, (c, -1))) )
```


Could you please tell me if I am doing it right ? 

Regards. "
Not found modules in trainCRF.py,sadeepj/crfasrnn_keras,2018-04-30 18:37:03,2,,31,318990998,"Hello
Thank you for sharing your code.
There are some classes in `trainCRF.py` that cannot be imported. The followings are unresolved references:
```python
import labels
from dataGenerator import DataGenerator
from weighted_categorical_crossentropy import weighted_categorical_crossentropy, weighted_loss
from BillinearUpsampling import BilinearUpSampling2D
from modelPredictor import computeMeanIou, plot_confusion_matrix
from labels import listLabels
```"
all-GPU version will be released soon,sadeepj/crfasrnn_keras,2018-04-20 15:12:48,0,,30,316306998,  so when ?
Permutohedral Lattice implementation,sadeepj/crfasrnn_keras,2018-01-16 10:42:12,18,,24,288860206,"Hello,

I have trying to use the permutohedral lattice code to perform bilateral filtering just to see if it works as expected.
What happens is: I get an image with a repeating artefact overlayed over the original image instead of a blur effect.
I don't know if this is how your implementation is supposed to work or if there is some kind of bug.
Would you mind attempting to perform bilateral filtering on your side to check this issue?

Also I have across a paper that might interest you since it can speed up the message passing step:

Recursive bilateral filtering - Yang, Qingxiong - European Conference on Computer Vision - 2012

Thanks."
How can I add your code behind my model,sadeepj/crfasrnn_keras,2017-12-04 03:11:31,2,,21,278857400,"    Hello, I have run your model successful. 
    Because I want your model to modify my classify result, could you please tell me how can I add your code behind my model? 
    Such what is the shape of input and label, how can I save my own weight of model?
    My input shape is (512,512, 3) , it has five classes(0~5)."
Can you share your gpu verson?,sadeepj/crfasrnn_keras,2017-10-24 14:44:27,2,,15,268060952,"Thanks very much for your code,its really a good job! When I use your code to train my datasets on tensorflow/keras, it's really too slow.And I realize its reason that can only run on cpu! So can you give us your GPU verson? I will appreciate it!  @sadeepj @sirotenko "
CRFrnn_layer batch size,sadeepj/crfasrnn_keras,2017-09-12 14:31:34,5,,6,257064930,"Hi,

Your implementation of the crf layer supports only batch_size = 1.
Correct?"
[Feature] Support CAE full checkpoint,open-mmlab/mmselfsup,2022-11-08 07:20:31,1,Feature,571,1439632011,"### What is the problem this feature will solve?

Adding full checkpoints will help users to finetune the new datasets without expensive computational costs. It will make the model easier to adopt and use. 

### What is the feature?

Full checkpoint of CAE model including encoder and decoder

### What alternatives have you considered?

_No response_"
Support Resnet18 backbone?,open-mmlab/mmselfsup,2022-11-06 15:34:42,1,Feature,569,1437444473,"### What is the problem this feature will solve?

Hi, I wonder does this repo have SSL pretrained weights for Resnet-18? It would be very useful to have weights with resnet-18 as backbone that can be used in other areas that don't have a lot of data for fine-tuning.

### What is the feature?

Pretrained model for Resnet-18 backbone.

### What alternatives have you considered?

_No response_"
[Bug] ,open-mmlab/mmselfsup,2022-11-04 05:00:31,1,Bug,568,1435554703,"### Branch

master branch (0.x version, such as `v0.10.0`, or `dev` branch)

### Prerequisite

- [X] I have searched [Issues](https://github.com/open-mmlab/mmselfsup/issues) and [Discussions](https://github.com/open-mmlab/mmselfsup/discussions) but cannot get the expected help.
- [X] I have read the [documentation](https://mmselfsup.readthedocs.io/en/latest/) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmselfsup).

### Environment

ubuntu 22.04.1
torch 1.14.0.dev20221027+cu117 pypi_0 pypi
torchaudio 0.14.0.dev20221027+cu117 pypi_0 pypi
torchvision 0.15.0.dev20221027+cu117
mmselfsup 0.10.0
cuda 11.7
nvidia driver 520.06

### Describe the bug

whenever i pass“bash tools/dist_train.sh configs/selfsup/byol/byol_resnet50_8xb32-accum16-coslr-100e_in1k.py 4”,1~2 minutes, the machine will restart。
Run 3 graphics cards, the machine no restart.

I use stress-ng. I can't connect the machine to ssh, the machine does not restart, and after the pressure test is over, it also recovers, but as long as I run this, the machine will really restart

### Reproduces the problem - code sample

_No response_

### Reproduces the problem - command or script

bash tools/dist_train.sh configs/selfsup/byol/byol_resnet50_8xb32-accum16-coslr-100e_in1k.py 4

### Reproduces the problem - error message

_No response_

### Additional information

_No response_"
[Feature] Support Masked Image Modeling reconstruction visualization,open-mmlab/mmselfsup,2022-10-28 06:49:04,0,,553,1426774585,"### What is the problem this feature will solve?

Add scripts and functions to support visualize MIM reconstruction.

### What is the feature?

Like MAE visuzalization, support other MIM algorithms.
![image](https://user-images.githubusercontent.com/36138628/198520601-f27444ed-7d72-4ab8-93ab-31622ba11043.png)


### What alternatives have you considered?

- [x] MAE
- [x] SimMIM
- [x] MaskFeat
- [ ] ConvMAE"
How to use mmslfesup to train resnet50 caffe pre-training weights in mmdetection,open-mmlab/mmselfsup,2022-10-24 10:43:45,9,,546,1420624355,"### What is the problem this feature will solve?

I saw the weight of resnet50 caffe.pth in mmdetection, can we use self-supervised model to train him
![image](https://user-images.githubusercontent.com/86449151/197507913-1326f435-cdde-4c79-a2fd-d59cab493c24.png)
Do I only need to modify the style in resnet.py in mmselfsup？
![image](https://user-images.githubusercontent.com/86449151/197508329-8c463739-a8ad-4088-8768-a7cf692149f2.png)



### What is the feature?

I saw the weight of resnet50 caffe.pth in mmdetection, can we use self-supervised model to train him

### What alternatives have you considered?

Do I only need to modify the style in resnet.py in mmselfsup？"
[Feature] Support Self-Supervised Learning via Maximum Entropy Coding,open-mmlab/mmselfsup,2022-10-21 01:56:00,0,,539,1417579201,"### What is the problem this feature will solve?

A latest SSL work accepted by NeurIPS 2022

- [ ] https://arxiv.org/pdf/2210.11464.pdf

### What is the feature?

Enrich the model zoo of MMSelfSup. 
Community contribution is welcomed.

### What alternatives have you considered?

_No response_"
[Bug] URL invalid in benchmark config,open-mmlab/mmselfsup,2022-10-18 07:26:49,1,,521,1412709058,"

**Describe the Issue**
I find the url in the comment of these two files are invalid currently. Maybe we need to re-run the configs in the benchmark to check the availability. Or we can add some unit test or CI to check the availability of the config files in benchmark automatically.

- https://github.com/open-mmlab/mmselfsup/blob/1.x/configs/benchmarks/mmsegmentation/cityscapes/fcn_r50-d8_769x769_40k_cityscapes.py
- https://github.com/open-mmlab/mmselfsup/blob/1.x/configs/benchmarks/mmsegmentation/voc12aug/fcn_r50-d8_512x512_20k_voc12aug.py

"
this dockerfile provided bellow can work well！,open-mmlab/mmselfsup,2022-10-17 08:16:06,1,,517,1411116851,"```
# docker build -f ./docker/Dockerfile --rm -t mmselfsup:torch1.10.0-cuda11.3-cudnn8 .
# docker run --gpus all --shm-size=8g -it -v {DATA_DIR}:/workspace/mmselfsup/data mmselfsup:torch1.10.0-cuda11.3-cudnn8 /bin/bash
ARG PYTORCH=""1.10.0""
ARG CUDA=""11.3""
ARG CUDNN=""8""

FROM pytorch/pytorch:${PYTORCH}-cuda${CUDA}-cudnn${CUDNN}-devel

ENV TORCH_CUDA_ARCH_LIST=""6.0 6.1 8.0 7.0+PTX""
ENV TORCH_NVCC_FLAGS=""-Xfatbin -compress-all""
ENV CMAKE_PREFIX_PATH=""$(dirname $(which conda))/../"" 

# To fix GPG key error when running apt-get update
RUN apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub
RUN apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub

RUN apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 python3-pip \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Install MMCV MMDetection MMSegmentation
RUN pip install --no-cache-dir --upgrade pip wheel setuptools
RUN pip install --no-cache-dir mmcv-full==1.6.2 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.10.0/index.html
RUN pip install mmsegmentation mmdet
# Install MMSelfSup
RUN conda clean --all
RUN git clone https://github.com/open-mmlab/mmselfsup.git  /mmselfsup
WORKDIR /mmselfsup
ENV FORCE_CUDA=""1""
RUN pip install --no-cache-dir -e .

#避免选时区
ENV DEBIAN_FRONTEND=noninteractive 
RUN apt-get update && apt-get install -y openssh-server vim git-all 
RUN  echo ""PubkeyAuthentication yes"" >> /etc/ssh/sshd_config  && \
        echo ""PermitRootLogin yes"" >> /etc/ssh/sshd_config  && \
        echo ""PasswordAuthentication yes"" >> /etc/ssh/sshd_config  && \
        systemctl enable ssh && /etc/init.d/ssh start && \
        echo ""export PATH=$PATH:/usr/local/cuda/bin"" >> ~/.bashrc && /bin/bash -c ""source ~/.bashrc""

CMD [""/bash/bash""]
```"
"error after running for 3 epoch , ERROR:torch.distributed.elastic.multiprocessing.api:failed",open-mmlab/mmselfsup,2022-10-06 20:29:53,8,,505,1400262282,"Thanks for reporting the unexpected results and we appreciate it a lot.

**Checklist**

1. I have searched related issues but cannot get the expected help. yes
2. The unexpected results still exist in the latest version.
no results 
**Describe the Issue**
A clear and concise description of what the bug is, including what results are expected and what the real results you got.

**Reproduction**

1. What command, code, or script did you run?
I am try to train a model for object detection , I used a pretrained model of simclr as a self-supervised approach. 

```bash
bash tools/benchmarks/mmdetection/mim_dist_train_fpn.sh configs/benchmarks/mmdetection/coco/faster_rcnn_r50_c4_mstrain_2x_coco_new.py checkpoints/simclr_resnet50_imagenet.pth 1
```
I used one gpu for the training , and I did not find the python file (mim_dist_train.sh) which  mentioned in your document about running a training for downstream task 
2. Did you make any modifications on the code? Did you understand what you have modified?
yes, I used my custom dataset which has similar format of coco dataset. 
i used to train model with it in mmdetection environment. 
**Environment**

1. Please run `python -c ""from mmselfsup.utils import collect_env; print(collect_env())""` to collect necessary environment information and paste it here.

![environment in selfsup](https://user-images.githubusercontent.com/10245810/194414300-eaa855fc-26aa-479b-9617-27c4de11aa70.jpg)


2. You may add addition that may be helpful for locating the problem, such as
   - How you installed PyTorch \[e.g., pip, conda, source\]
          I install it by create a python environment.
 i used this command :

`
source open_env/bin/activate

git clone https://github.com/open-mmlab/mmselfsup.git
cd mmselfsup
pip install -v -e .
`
I already install the mmdetection, mmcv and all other dependance at same environment. 

   - Other environment variables that may be related (such as `$PATH`, `$LD_LIBRARY_PATH`, `$PYTHONPATH`, etc.)
   
![config error selfsup](https://user-images.githubusercontent.com/10245810/194413074-009dc3e9-1785-48a5-84e1-55c13ce12352.jpg)


**Error traceback**
If applicable, paste the error traceback here.

```none
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 24194) of binary: /home/shubbak/open_env/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
```
for more detailed find the attached picture 
![selfsupervised learning mm error](https://user-images.githubusercontent.com/10245810/194412206-829fa576-bc52-4452-a4d5-07dc99dff49c.jpg)

**Bug fix**
If you have already identified the reason, you can provide the information here. If you are willing to create a PR to fix it, please also leave a comment here and that would be much appreciated!
"
About your latest selfsup work MFM,open-mmlab/mmselfsup,2022-10-01 11:16:33,1,,504,1393363654,"May I ask when will you release the codes of MFM? I would like to do some research based on this work.
"
Introducing mmselfsup.core in mmdet leads to unexpected results.,open-mmlab/mmselfsup,2022-09-10 00:13:08,4,,479,1368426774,"Hi @fangyixiao18 @Jiahao000 @YuanLiuuuuuu @scnuhealthy, happy mid-Autumn Festival~

Thanks for reporting the unexpected results and we appreciate it a lot.

**Checklist**

1. I have searched related issues but cannot get the expected help.
2. The unexpected results still exist in the latest version.

**Describe the Issue**
I tried to combine mmselfsup and mmdet to do something, however, when I added:
```
custom_imports = dict(imports=['mmselfsup.core'])
```

I found that DETR's training fails to converge, I also added this line into other methods like fcos/retinanet/mask-rcnn. In the case of fixed random seeds, their loss values and training results have huge changes. I have no clues about this and what's causing the huge discrepancy, hoping to get some help from the community.

**Reproduction**

1. What command, code, or script did you run?

Add this line into any original mmdet config, you can find the loss and result changed a lot, even with fixed random seed.
```bash
custom_imports = dict(imports=['mmselfsup.core'])
```

2. Did you make any modifications on the code? Did you understand what you have modified?
Yes. Yes.

**Bug fix**
If you have already identified the reason, you can provide the information here. If you are willing to create a PR to fix it, please also leave a comment here and that would be much appreciated!"
Support Video MAE,open-mmlab/mmselfsup,2022-08-05 06:53:44,1,,401,1329527929,"Could you please support Video MAE: https://github.com/MCG-NJU/VideoMAE

Thank you ~"
"""new_dataset is not in the dataset registry""",open-mmlab/mmselfsup,2022-06-24 14:24:02,18,,335,1283816453,"I have modified the file as the tutorial ""1_new_dataset.md"", including create new file ""ls_dataset.py""  and ""ls_data_source.py"", and modified the ""__init__.py"" according to the tutorial.
However when I run the command
 ""bash tools/benchmarks/mmdetection/mim_dist_train_c4.sh configs/benchmarks/mmdetection/voc0712/faster_rcnn_r50_c4_mstrain_2x_voc0712.py /home/ls/mmselfsup/checkpoints/simclr_resnet50_8xb32-coslr-200e_in1k_20220428-46ef6bb9.pth 1 ""

It raised the error :
![2022-06-24 22-23-04 的屏幕截图](https://user-images.githubusercontent.com/49121651/175555734-95ec4b1f-9e32-4479-944e-188d8ab426a3.png)

May I ask how to solve it? Thanks for your help.

"
Errors in running SelectiveSearchCode,aosokin/cnn_head_detection,2021-09-22 10:09:45,1,,11,1004110540,"Thanks for your excellent work.
I want to test the model on a new image and I encounter an error when running the demo in SelectiveSearchCode.
<img width=""999"" alt=""Screen Shot 2021-09-22 at 11 09 32 AM"" src=""https://user-images.githubusercontent.com/44634290/134325184-7a264ae6-29d5-4e6d-b0c7-48a7b40dcbb0.png"">
"
some error in compile_mex,aosokin/cnn_head_detection,2018-04-02 09:27:31,0,,9,310439592,">> compile_mex('/Developer/NVIDIA/CUDA-9.1')

root =

    '/Users/macos/Desktop/CNNN/utils/cropRectanglesMex'

Hi

while trying to run compile_mex, I've been omit some semicolon to show the output just before the error
get this error

compileCmd =

    '""/Developer/NVIDIA/CUDA-9.1/bin/nvcc"" -c cropRectanglesMex.cu -DNDEBUG -DENABLE_GPU -I""/Applications/MATLAB_R2017b.app/extern/include"" -I""/Applications/MATLAB_R2017b.app/toolbox/distcomp/gpu/extern/include"" -I""/Developer/NVIDIA/CUDA-9.1/include"" -I""/Developer/NVIDIA/CUDA-9.1/samples/7_CUDALibraries/common/UtilNPP"" -I""/Developer/NVIDIA/CUDA-9.1/samples/common/inc"" -Xcompiler -fPIC -o ""/Users/macos/Desktop/CNNN/utils/cropRectanglesMex/cropRectanglesMex.o""'

nvcc fatal   : The version ('90100') of the host compiler ('Apple clang') is not supported

ans =

     1

Building with 'Xcode with Clang'.
Error using mex
clang: error: no such file or directory:
'/Users/macos/Desktop/CNNN/utils/cropRectanglesMex/cropRectanglesMex.o'


Error in build_cropRectanglesMex (line 38)
mex(mopts{:}) ;

Error in compile_mex (line 9)
	build_cropRectanglesMex( cudaRoot );"
A question about the global model?,aosokin/cnn_head_detection,2017-12-18 07:03:31,2,,6,282782089,"hello, when I check the global model code, I find that the 22 layer of global model has 4096 nodes, but in the oquab model, there is only 2048 nodes, why double numbers of nodes? and how it come out is not find in the code also."
error when import probreg after pip installtion on ubuntu 20.04 + python3.6,neka-nat/probreg,2022-11-02 15:40:24,1,,98,1433371323,"run in conda environment

`from probreg import cpd, filterreg, gmmtree, l2dist_regs, features`

Error message show below:

```
Traceback (most recent call last):
  File ""/home/szx/PycharmProjects/test_project/run.py"", line 23, in <module>
    from probreg import cpd, filterreg, gmmtree, l2dist_regs, features
  File ""/home/szx/anaconda3/envs/pipe_4cams/lib/python3.6/site-packages/probreg/__init__.py"", line 1, in <module>
    from . import bcpd, callbacks, cpd, filterreg, gmmtree, l2dist_regs, log, math_utils, transformation
  File ""/home/szx/anaconda3/envs/pipe_4cams/lib/python3.6/site-packages/probreg/filterreg.py"", line 16, in <module>
    from . import se3_op as so
  File ""/home/szx/anaconda3/envs/pipe_4cams/lib/python3.6/site-packages/probreg/se3_op.py"", line 1
    from __future__ import annotations, division, print_function
    ^
SyntaxError: future feature annotations is not defined
```"
Registration between a sparse and a dense point cloud,neka-nat/probreg,2022-10-25 16:07:46,3,,97,1422725892,"I have a set of two point clouds where the source is much more dense in comparison to the target one. See examples below:
![image](https://user-images.githubusercontent.com/10189018/197821264-f2d5c598-ec1f-4db0-ae7b-30576f59baac.png)
and in case I load the source pcd with some noise:
![image](https://user-images.githubusercontent.com/10189018/197821622-94c904b4-e3f7-42ad-aec4-8c239bf985ca.png)

Now I want to register the two point clouds so that I get the best overlap as shown here:
![image](https://user-images.githubusercontent.com/10189018/197822958-07fc7fdb-6f8c-44a4-93f9-0ecee9377248.png)

Initially I used the cpd registration `tf_param, _, _ = probreg.cpd.registration_cpd(A_pcd, B_pcd, update_scale=False, maxiter=20000, use_cuda=False, tol=0.000001, tf_type_name='rigid')` with the following results (without the noise):
![image](https://user-images.githubusercontent.com/10189018/197823659-ecb86476-0477-4a35-91cd-c9da2d9f1de5.png)
and with noise:
![image](https://user-images.githubusercontent.com/10189018/197824211-fc4e0ab0-5e7a-449b-9a6d-4e8fc82a5cde.png)

While the registration in the without noise case is not that bad, it is still a bit far from the desired result while the output with the noise is totally bad.

Thus, @neka-nat I wanted to ask if you think that with any of the provided algorithms it would be possible to improve the result or if there is any other suggestion that would help towards that. 

`filterreg`, `gmm` and `svr` didn't really seem to do any better and mostly the results were worse.

Thanks.

[pcds.zip](https://github.com/neka-nat/probreg/files/9862045/pcds.zip)
"
Question about outliers,neka-nat/probreg,2022-07-19 13:49:30,1,question,94,1309554835,"First of all, thanks for sharing your great code.
I'm trying to implement a paper on ""Point Set Registration: Coherent Point Drift"" and I found the code and am using it. Among them I am using the affine method and I have a question about outliers here. Looking at the paper, I think that an outlier(""w"") is an unnecessary point or noise when matching two point clouds. We determined that performance could be improved by increasing the outliers(""w"") of the noisy target point cloud in e_step. However, it was confirmed that the performance deteriorated as the number of outliers increased. If anyone knows why, please comment."
unexpected results in random rotation validation,neka-nat/probreg,2021-09-13 19:04:53,0,bug,76,995225596,"Hi, thank you for providing this amazing library. 

I was trying to apply random rotation validation on bcpd_nonrigid.py with the given bunny example, and obtained some weird results. 

For example, when applying rotation in X direction by 90 degree counterclockwise. It only took 4 iterations and had an RMSE of 0.3329578008. You could tell from the visualization that it produced a weird shape. (validation result in red dots)
<img width=""965"" alt=""Screen Shot 2021-09-13 at 11 53 00 AM"" src=""https://user-images.githubusercontent.com/62967042/133140372-85b65f06-fd0d-4e78-b6ec-909ca589849e.png"">

When apply rotation in X direction in X direction by 15 degree counterclockwise. It took 39 iterations and had an RMSE of 0.037521531764196. However, the visualization shows that it produced a 3d stick-like shape. (validation result in red dots)
<img width=""962"" alt=""Screen Shot 2021-09-13 at 11 56 40 AM"" src=""https://user-images.githubusercontent.com/62967042/133140811-89cc5886-38f3-4a5c-8863-79b34e17258f.png"">

Here is the link to the results of all of my experiment. 
https://docs.google.com/spreadsheets/d/1h7GLeuVPQ0mmztkSOsyosEjR5jvQw7SVpCX44bua_V0/edit?usp=sharing
And here is the link to the validation output PLY files.
https://drive.google.com/drive/folders/1j3NVyijAx_NZHTKLMuxTn9HF1xGPS7ko?usp=sharing

These results do not show a pattern, so I'm wondering what I'm doing wrong here. Are there specific parameters that I need to tune to get the algorithm working? Any help would be appreciated. 
"
Anaconda support,neka-nat/probreg,2020-02-26 12:05:00,1,enhancement,19,571294112,#16 
images_train_lmdb,pathak22/ccnn,2018-07-25 09:14:11,0,,3,344358810,"$ python ./src/train.py
cnn/ccnn/src/ccnn.py:19: RuntimeWarning: to-Python converter for std::vector<int, std::allocator<int> > already registered; second conversion method ignored.
  from python.ccnn import *
F0725 15:37:23.709703  4820 db.cpp:18] Check failed: status.ok() Failed to open leveldb /mnt/a/pathak/fcn_mil_cache/VOC2012/images_train_lmdb
IO error: /mnt/a/pathak/fcn_mil_cache/VOC2012/images_train_lmdb/LOCK: No such file or directory
*** Check failure stack trace: ***
    @     0x7f0e98f9e04d  google::LogMessage::Fail()
    @     0x7f0e98fa0603  google::LogMessage::SendToLog()
    @     0x7f0e98f9dbdb  google::LogMessage::Flush()
    @     0x7f0e98f9f54e  google::LogMessageFatal::~LogMessageFatal()
    @     0x7f0e99320e23  caffe::db::LevelDB::Open()
    @     0x7f0e9937dfb6  caffe::DataLayer<>::DataLayerSetUp()
    @     0x7f0e9938e882  caffe::BaseDataLayer<>::LayerSetUp()
    @     0x7f0e9938e919  caffe::BasePrefetchingDataLayer<>::LayerSetUp()
    @     0x7f0e99367e99  caffe::Net<>::Init()
    @     0x7f0e9936a05e  caffe::Net<>::Net()
    @     0x7f0e993000a5  caffe::Solver<>::InitTrainNet()
    @     0x7f0e9930138e  caffe::Solver<>::Init()
    @     0x7f0e99301556  caffe::Solver<>::Solver()
    @     0x7f0e9992edf0  caffe::GetSolver<>()
    @     0x7f0e998fc7d3  caffe::GetSolverFromString()
    @     0x7f0e999086b8  boost::python::objects::caller_py_function_impl<>::operator()()
    @     0x7f0e987175cd  boost::python::objects::function::call()
    @     0x7f0e987177c8  (unknown)
    @     0x7f0e9871f823  boost::python::detail::exception_handler::operator()()
    @     0x7f0e3a8a8d20  boost::python::detail::translate_exception<>::operator()()
    @     0x7f0e3a884f3d  boost::_bi::list3<>::operator()<>()
    @     0x7f0e3a871ec7  boost::_bi::bind_t<>::operator()<>()
    @     0x7f0e3a86b268  boost::detail::function::function_obj_invoker2<>::invoke()
    @     0x7f0e9871f5dd  boost::python::handle_exception_impl()
    @     0x7f0e98714999  (unknown)
    @     0x7f0e9b4567a3  PyObject_Call
    @     0x7f0e9b4ecb69  PyEval_EvalFrameEx
    @     0x7f0e9b4f24e9  PyEval_EvalCodeEx
    @     0x7f0e9b4f270a  PyEval_EvalCode
    @     0x7f0e9b50b9cd  run_mod
    @     0x7f0e9b50cb48  PyRun_FileExFlags
    @     0x7f0e9b50dd68  PyRun_SimpleFileExFlags
已放弃 (核心已转储)
请问这个问题什么意思？该怎么解决啊？着急，在线等！！！"
Import caffe.so,pathak22/ccnn,2017-10-16 18:04:01,2,,2,265861271,"Hi, 

I am trying to reproduce your work. During this period, sadly, I failed to load python.caffe in ccnn.py. It seems that this lib cannot be found. Besides, I am confused about how you implement the latent distribution optimisation.

Looking forward to your reply. Thank you~"
VOC training dataset,fqnchina/CEILNet,2020-04-23 14:47:01,0,,9,605600680,"Hi, I have some questions about the selection of dataset in your work. You put the index of training data into the txt file.  However, could you tell me according to what criteria do you choose which VOC dataset are used for training?"
Regarding synthetic training dataset for reflection removal,fqnchina/CEILNet,2020-03-26 07:05:14,0,,8,588201347,"Hi, 

How do I save the synthetic training dataset for reflection removal generated using PASCAL VOC2012 images locally? Is it being generated on the fly? Could you please provide code pointers / provide the synthetic training dataset? (mixed, background and reflection layer images).

Thanks
Rohan"
Do you have other language version like python?,fqnchina/CEILNet,2019-11-25 05:41:53,0,,7,527860811,
how to use this code train on my generated data?,fqnchina/CEILNet,2018-07-09 08:01:32,0,,6,339343858,"excuse me, i wonder how to use this code train on myself data? i'm a beginner to lua torch. is there any other version like keras, tensorflow? thanks. "
problem when training ECNN individually,fqnchina/CEILNet,2018-01-19 09:51:19,1,,5,289918193,"Hi,
I trained the ecnn individually using the provided codes (training_reflection_ecnn.lua) but found that the loss went down from about 10^8 to about 10^6 and just stuck.
Tracing the code, I found that the images were rescaled by 255 (line 208,209) before computing the edges and all the elements of the image were divided with 0.02 in computeEdge (line 75). I guessed this was the reason why the initial MSE error looked so large.
Is there any reason why rescaling the image to 0~255 and rescaling again before computing the edges?
I'm not sure if this cause the network stuck to such high loss. 
I will be grateful if anyone can give me some tips!

p.s. I trained on VOC2012(cropped to 224x224) just as the paper done  


"
optim.adam_state,fqnchina/CEILNet,2017-12-26 02:46:59,4,,4,284483823,"In line 230 of training_reflection_ecnn.lua, optim.adam_state was a nil value as I called it. You mean optim.adam?"
E-CNN ground truth,fqnchina/CEILNet,2017-10-19 07:20:07,2,,3,266743069,"Hi,

As you have mentioned in your paper 4-1, the sub-networks are trained separately.
I just wonder about how you obtain the label of your E-CNN, which should be an edge map(and denoted E^t* in your paper).

"
synthetic ground truth request,fqnchina/CEILNet,2017-10-03 20:23:13,1,,1,262572842,"This is very interesting work, thanks for sharing! Is it possible to also post the ground truth (e.g. transmission and reflection layer) of the synthetic dataset?

Thanks for the consideration!"
Problem while calling create_vgg16_FCN  in FCN_da.py,YangZhang4065/AdaptationSeg,2021-09-13 18:38:19,0,,21,995204746,"Hi @YangZhang4065 ,
Im using create_vgg16_FCN to create FCN model , but on calling the function i'm facing following error :

 Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d_21/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=""NHWC"", explicit_paddings=[], ksize=[1, 2, 2, 1], padding=""VALID"", strides=[1, 2, 2, 1]](Placeholder)' with input shapes: [?,1,160,128].

I have used padding ='same' on maxpooling in create_vgg16_FCN function , then above error does not comes , but while loading the pretrained weights from Synthia_FCA , i facing incompatible dimensions issue .

Kindly let me know If m missing something ."
SYNTHIA_FCN.h5 file not found ,YangZhang4065/AdaptationSeg,2021-09-13 15:39:07,1,,20,995042174,"Hi , Im not able to find SYNTHIA_FCN.h5 file you provided in the url . Kindly check."
How to get the gamut-based color constancy method in the Paper？,YangZhang4065/AdaptationSeg,2021-08-31 06:51:50,0,,19,983508703,Can you share the code，please？
Pretrained model?,YangZhang4065/AdaptationSeg,2021-02-06 18:37:19,3,,17,802761535,"Do you have pretrained model for your final version of model ((CC+I+SP) of TPAMI paper) or for any of them?
Thank you. "
Logistic regression model,YangZhang4065/AdaptationSeg,2020-01-21 15:59:09,8,,15,552969715,"Hello @YangZhang4065 ,
Thanks for the nice work. I'm trying to train model to estimate the global label distribution. Can you give more details of the architecture and training scheme?
Thanks! "
About color-constancy,YangZhang4065/AdaptationSeg,2019-11-06 03:44:02,1,,14,518217691,"Thanks for your great help first, I wonder how to obtain the gamut-based color
constancy method in the Paper. I have searched for a long time, but I can not find it. Can you share me the code about color constancy."
Source only (baseline) training,YangZhang4065/AdaptationSeg,2019-03-12 16:21:14,1,,13,420080024,Is the code for source-only training available?
Get stuck before training,YangZhang4065/AdaptationSeg,2018-03-08 07:17:16,3,,10,303379750,"Hi,

I tried to run the code, but the training does not proceed even for one iteration. Actually, it does nothing after printing: 
```
Start loading files
Start training
Epoch 1/100
```
And starts consuming memory.
Any idea on that?

Thanks."
Need some help.,YangZhang4065/AdaptationSeg,2017-11-28 03:34:46,0,,7,277247289,"I'm sorry, but I just can't find where to download the datasheet.Please give me some instruction."
nan-loss after iteration 9.,YangZhang4065/AdaptationSeg,2017-11-24 01:50:21,0,,6,276496240,"train_val_FCN_DA.py:155: RuntimeWarning: divide by zero encountered in divide
  SP_weight=avg_pixel_number/SP_pixelperSP_num
   1/4543 [..............................] - ETA: 7:04:32 - loss: 1.4686 - output_loss: 1.249   2/4543 [..............................] - ETA: 5:56:23 - loss: 1.2655 - output_loss: 1.076   3/4543 [..............................] - ETA: 6:04:32 - loss: 1.2219 - output_loss: 1.064   4/4543 [..............................] - ETA: 7:24:51 - loss: 1.0931 - output_loss: 0.937   5/4543 [..............................] - ETA: 8:40:20 - loss: 1.1262 - output_loss: 0.974   6/4543 [..............................] - ETA: 10:07:33 - loss: 1.0875 - output_loss: 0.94   7/4543 [..............................] - ETA: 11:10:59 - loss: 1.0446 - output_loss: 0.90   8/4543 [..............................] - ETA: 11:22:12 - loss: 0.9914 - output_loss: 0.85   9/4543 [..............................] - ETA: 11:30:05 - loss: 0.9571 - output_loss: 0.82  10/4543 [..............................] - ETA: 11:22:53 - loss: 0.9300 - output_loss: 0.80  11/4543 [..............................] - ETA: 11:24:29 - loss: nan - output_loss: nan - o  12/4543 [..............................] - ETA: 11:17:30 - loss: nan - output_loss: nan - o

After I manually modify the output_shape, otherwise it will lead to the dimension dismatch problem. 
 out=Lambda(lambda x:x+0., name='output', output_shape=(class_num + 1,nb_rows,nb_cols))(output)
    out_2=Lambda(lambda x:x+0., name='output_2', output_shape=(class_num ,1,1))(output)
"
"expected input_1 to have shape (None, 3, 320, 640) but got array with shape (1, 3, 1, 0)",YangZhang4065/AdaptationSeg,2017-11-23 22:28:39,0,,5,276479995,"  I manually check the size of using ""print loaded_im.shape, loaded_label.shape, loaded_target_obj_pre.shape"", it looks good. Have no idea why it crashed. Any idea?
File ""train_val_FCN_DA.py"", line 207, in <module>
    seg_model.fit_generator(myGenerator(),callbacks=[Validate_on_CityScape()], steps_per_epoch=steps_per_epoch, epochs=60)
  File ""build/bdist.linux-x86_64/egg/keras/legacy/interfaces.py"", line 87, in wrapper
  File ""build/bdist.linux-x86_64/egg/keras/engine/training.py"", line 2110, in fit_generator
  File ""build/bdist.linux-x86_64/egg/keras/callbacks.py"", line 85, in on_batch_begin
  File ""train_val_FCN_DA.py"", line 193, in on_batch_begin
    current_predicted_val=self.model.predict(loaded_val_im,batch_size=batch_size)
  File ""build/bdist.linux-x86_64/egg/keras/engine/training.py"", line 1765, in predict
  File ""build/bdist.linux-x86_64/egg/keras/engine/training.py"", line 153, in _standardize_input_data
ValueError: Error when checking : expected input_1 to have shape (None, 3, 320, 640) but got array with shape (1, 3, 1, 0)
"
What's the superpixel loss?,YangZhang4065/AdaptationSeg,2017-11-20 03:27:16,0,,4,275227020,"Thanks for your remarkable work! But I have a question: 
What's the superpixel loss? Does it mean the segmentation loss on the target image, or label distributions over local landmark superpixels? But how to produce label distributions over local landmark superpixels?
Looking forward to your answer!Thanks!"
I got some error with running train_val_FCN_Da.py .,YangZhang4065/AdaptationSeg,2017-09-18 07:25:08,1,,3,258389330,"I did set data path folder and .h5 file.
When I ran train_val_FCN_DA.py code , I got this error.


Using Theano backend.
/home/tf/anaconda2/lib/python2.7/site-packages/keras/layers/core.py:633: UserWarning: `output_shape` argument not specified for layer output and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 22, 320, 640)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.
  .format(self.name, input_shape))
/home/tf/anaconda2/lib/python2.7/site-packages/keras/layers/core.py:633: UserWarning: `output_shape` argument not specified for layer output_2 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 22, 320, 640)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.
  .format(self.name, input_shape))
Start loading files
Start training
Epoch 1/60
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""/home/tf/anaconda2/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/home/tf/anaconda2/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/home/tf/anaconda2/lib/python2.7/site-packages/keras/engine/training.py"", line 612, in data_generator_task
    generator_output = next(self._generator)
  File ""train_val_FCN_DA.py"", line 144, in myGenerator
    tar_idx=sample(range(len(cityscape_im_generator)),target_batch_size)
  File ""/home/tf/anaconda2/lib/python2.7/random.py"", line 323, in sample
    raise ValueError(""sample larger than population"")
ValueError: sample larger than population

Traceback (most recent call last):
  File ""train_val_FCN_DA.py"", line 203, in <module>
    seg_model.fit_generator(myGenerator(),callbacks=[Validate_on_CityScape()], steps_per_epoch=steps_per_epoch, epochs=60)
  File ""/home/tf/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.py"", line 88, in wrapper
    return func(*args, **kwargs)
  File ""/home/tf/anaconda2/lib/python2.7/site-packages/keras/engine/training.py"", line 1877, in fit_generator
    str(generator_output))
ValueError: output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: None
"
Inconsistent Label Resizing during evaluation,YangZhang4065/AdaptationSeg,2017-09-15 05:15:09,1,,2,257929553,"Hi, Nice work! I have a query regarding your evaluation. 

In your paper you have mentioned that: "" Since we have to resize the images before feeding them to the segmentation network, we resize the output segmentation mask back to the original image size before running the evaluation against the groundtruth annotations. ""

However, in warp_data.py which is called by your eval. code, it seems like the label is also resized to (320,640). 

Can you please clarify this inconsistency for me ? Thanks!


"
SYNTHIA_FCN.h5 hdf5 file not found...,YangZhang4065/AdaptationSeg,2017-09-12 10:26:50,4,,1,256993807,"Dear Sir/Madam,

I was executing your test_FCN_DA.py . In line no:30, i found an exception @ seg_model.load_weights('SYNTHIA_FCN.h5') because i don't from where i had to get SYNTHIA_FCN.h5.
 **Kindly suggest**"
About the dataset,szq0214/GRP-DSOD,2020-10-18 09:09:18,0,,8,723963320,"Good code as you do. When I use this code, I want to use it to detect the hybrid dataset. Could you tell me how to modify the code?"
Waiting for code release,szq0214/GRP-DSOD,2018-11-15 19:57:16,0,,7,381316020,Hi waiting for the code.Id like to help if needed to get it ready and useable
Pytorch version,szq0214/GRP-DSOD,2018-06-04 04:36:57,0,,6,328905607,We are looking forward your pytorch version. Thanks for your great work.
Release of Pytorch version source?,szq0214/GRP-DSOD,2018-03-12 02:39:30,0,,5,304224153,"Hi, thank you for the Caffe version source.

Could you tell me when Pytorch version source will be released?

Thanks."
Is the weight of gate structure shared?,szq0214/GRP-DSOD,2018-02-07 02:58:00,2,,2,294989869,"Each of the feature maps has connected a gate structure, I am wondering if their weights shared? or just each of them connects a different module?"
CBP for video and sub?,seilna/RWMN,2018-04-03 20:14:17,0,,5,310981447,"Hi Seilna

I've being trying to implement your RWMN. However, when I tried to use video_sub features, the required compact_bilinear_pooling module is missing. So may I ask if it is possible to provide the compact_biliear_pooling as well in the repository?

Thanks very much!"
"how to get start, setup the dataset path etc",seilna/RWMN,2018-01-16 06:36:10,0,,4,288800393,
ValueError: a must be greater than 0,seilna/RWMN,2018-01-16 06:23:39,0,,3,288798372,"mldl@mldlUB1604:~/ub16_prj/RWMN$ python train.py 
2018-01-16 22:21:50.274948: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-16 22:21:50.274975: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-16 22:21:50.274983: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-01-16 22:21:50.274989: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-16 22:21:50.274995: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-01-16 22:21:50.351769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-01-16 22:21:50.352033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: GeForce GTX 950M
major: 5 minor: 0 memoryClockRate (GHz) 1.124
pciBusID 0000:01:00.0
Total memory: 3.95GiB
Free memory: 2.76GiB
2018-01-16 22:21:50.352051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2018-01-16 22:21:50.352057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2018-01-16 22:21:50.352064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 950M, pci bus id: 0000:01:00.0)
---------
Variables: name (type shape) [size]
---------
Total size of variables: 0
Total bytes of variables: 0
memory_write/query_w:0
memory_write/query_b:0
Exception in thread Thread-7:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/usr/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/home/mldl/ub16_prj/RWMN/custom_input_ops.py"", line 80, in thread_main
    for mini_batch in self.iterator():
  File ""/home/mldl/ub16_prj/RWMN/custom_input_ops.py"", line 50, in iterator
    movie_index = np.random.choice(self.num_movie, 1)
  File ""mtrand.pyx"", line 1104, in mtrand.RandomState.choice (numpy/random/mtrand/mtrand.c:17062)
ValueError: a must be greater than 0

"
Could you release your features & training config to reproduce results?,seilna/RWMN,2017-10-24 16:54:45,1,,1,268108890,"Could you release your features & training config to reproduce results?
Thanks!"
calculation of bleu and meteor,audio-captioning/caption-evaluation-tools,2021-07-28 23:21:22,0,,1,955305016,"Hi, I have one problem regarding to bleu and meteor.
The function evaluate_metrics_from_lists() return metrics and per_file_metrics, where metrics are for the whole dataset while per_file_metrics are for each file. I found the mean of the blue and meteor in per_file_metrics are not equal to those in metrics. 
For example, bleu1 in metrics doesn't equal mean(bleu1) of all files in per_file_metrics. Other metrics (cider, spice and spider) are equal. 
Do you known the reason? I noticed there were someone mentioned this in cococaption's issues but was not solved.
Thank you very much!
"
About the code,jfc4050/detect-to-track,2019-07-08 08:20:48,2,,1,465118849,"Hi! Thanks for your code.
Now I am looking for the Pytorch version code of the Detect to Track paper. I have found https://github.com/Feynman27/pytorch-detect-to-track but it seems that their performance is not good. How about your re-implement? Can this code achieve the points mentioned in the paper?"
Training code is missing,sciencefans/RSA-for-object-detection,2017-12-20 03:04:29,0,,6,283435320,"@sciencefans : 
Dear sir, 
in your readme you have written that 
train/: Training code for modules scale-forecast network and RSA

but the train folder itself is missing on your repository, either you forgot to upload OR its not public."
train code,sciencefans/RSA-for-object-detection,2017-12-13 04:51:52,0,,5,281624740,"@sciencefans @liyuanyaun  Could you please send me the train code? I want use RSA in my own DataSet.
My email is gaojing994919@163.com"
code available date?,sciencefans/RSA-for-object-detection,2017-08-30 07:37:27,0,,1,253905580,"Hi @sciencefans 
thank you for your work and paper
Do you know any approximate date to release your code because I want to look into and try it.

Regards,
Anand"
Implement this in windows ?,xiamenwcy/extended-caffe,2022-07-16 01:26:00,0,,1,1306662555,"I have spent a whole week to implement this ML framework to windows, but i have failed using CMake, then VS to compile. Is there any prebuild vs project(sln file) to work with? If i have sln file, it is more easy to compile and use.
I did successfully compiled using happynear `s caffe version[https://github.com/happynear/caffe-windows], but it is lack of your parameters of TransformImgAndSeg2 and so on.  I really need your caffe to finish my master project."
Regarding to paper: why learning will gives better result than the supervision itself?,zhengshou/AutoLoc,2020-05-17 01:23:05,0,,10,619587866,"Hi,

Thanks for the nice work. I thought about the question a lot, but can't figure out why. If I may, can I take several minutes from you to answer that?

The thing is: I have the impression that, there are no shared parameters between localization branch and classification branch, so classification branch should be trained firstly, and then the localization branch, right? That is to say, OIC /layer or OIC selection is serving as the supervision for the localization branch. If all are correct, I don't understand why the autoloc result is better than the supervision(OIC selection)? How it outperforms the supervision if no shared parameters allowed?

That confused me a lot, and also hard for me to find the answer in the paper. I really appreciate it if you can help. Hope to get you back soon!

Thanks!
June"
video width,zhengshou/AutoLoc,2020-04-10 22:46:20,0,,9,598126903,"Hi, many thanks for your work! May I ask what is the format of video width when used to clip [x1,x2].  I thought the video width is a number that shows the number of snippets, but it seems to be an array in the code. "
label tool,zhengshou/AutoLoc,2020-02-15 11:53:00,0,,8,565737836,Which temporal annotation tool did you use?Thank you.
number of validation videos,zhengshou/AutoLoc,2019-03-30 07:18:01,0,,6,427253788,"Dear author:
Why there are only features of 2304 validation videos provided, while in your paper it is claimed that there are 2383 videos in validation set.

Looking forward to your answer! Thanks a lot!"
File missing？,zhengshou/AutoLoc,2019-03-13 09:20:31,2,,5,420392632,"Dear author:
   
   When I tried to compile your BVLC_Caffe, I encountered a bug which told me that  ""caffe/layers/log_layer. hpp ""  was missing. I downloaded it from GITHUB of BVLC and covered it with your version. After that, I successfully compiled Caffe. 

   I'm not sure if this is a bug or if there's something wrong with my operation.

   Thanks a lot！

当我尝试去编译你的BVLC_CAFFE时，我遇到了一个bug，它提示我缺少了 ""caffe/layers/log_layer.hpp""这个文件，我从BVLC的GITHUB上下载了CAFFE并用你们的版本覆盖他的版本后，我成功的编译了CAFFE。不知道这是不是一个问题？  顺便感谢~"
How to transform to result of RGB color images for multi-exposure experimentation.,hli1221/Imagefusion_deepfuse,2021-02-21 06:32:28,1,,3,812786790,"Hollow, Li Hui.

Currently I'm studying about the multi-exposure image fusion.
So, I want to experiment with your code...
But, your result of experiment is only gray image ones.
Please help me, where i change your code for fused RGB color images that are multi-exposured imaged.

"
Do you have the implementation code?,JoyLuo/face-attribute-recognition-paper-list,2020-06-09 08:51:08,0,,1,635237451,you only showed the link of paper...
Did anyone has the deepdraw code,ZhaofanQiu/pseudo-3d-residual-networks,2020-12-29 08:55:03,0,,32,775779510,Did anyone has the deepdraw code
action similarity labeling challenge,ZhaofanQiu/pseudo-3d-residual-networks,2019-06-25 10:49:31,0,,31,460350861,"Hi, ZhaoFan,

Thanks for your code.

Now I am running P3D model for the action similarity labeling (2nd task in your paper). 

For some similarity index, we need to compute division (e.g. in  x1*(log(x1/x2)), and the output from layers could be zero. Therefore, the divisor is zero, leading to the problem that the similarity is NAN. 

Did you meet such problem? If so, how did you address this case? Many thanks. "
how to change resnet18?,ZhaofanQiu/pseudo-3d-residual-networks,2019-06-02 14:38:44,0,,30,451198907," If each block is repeated three times in the network structure, it will be changed into three forms A, B, and C in turn, but each block of r is only repeated twice, how to change resnet18? 
 Resnet网络结构中如果每块重复三次，依次改成A,B,C三种形式，但是resnet18每一块只重复2次，该怎么改？"
Question about classifier,ZhaofanQiu/pseudo-3d-residual-networks,2019-01-22 02:57:34,1,,29,401578886,Thank you for your grate work! I have some questions about classifier：Whether the experiments in your paper were implemented by P3D+SVM？If so，did you use softmax to optimize the P3D until convergence and then get the final feature to train SVM for final predict result？
"what's the image channel ordering before mean-value substract, rgb or bgr?",ZhaofanQiu/pseudo-3d-residual-networks,2018-09-03 09:12:24,2,,26,356420084,"That is , [104, 117, 123]  is the mean-value list, which one is the blue-channel ? 104 or 123?"
what's the version of caffe needed?,ZhaofanQiu/pseudo-3d-residual-networks,2018-07-04 09:51:29,1,,25,338207636,"Could you tell me what's the version of `caffe` you use or the date when you download official caffe to your local machine?
I follow your instructions in README.md to modify the latest caffe. However, it occurs an error when loading pretrained model provided from you. Without the pretrained model, the network prototxt is good to use for training.

Here is the error info:

F0704 17:35:29.248016 42380 upgrade_proto.cpp:95] Check failed: ReadProtoFromBinaryFile(param_file, param) Failed to parse NetParameter
 file: ~/p3d/p3d_resnet_sports1m_iter_150000.caffemodel                                                                     
*** Check failure stack trace: ***                                                                                                     
    @     0x7faa0842bdaa  (unknown)                                                                                                    
    @     0x7faa0842bce4  (unknown)                                                                                                    
    @     0x7faa0842b6e6  (unknown)                                                                                                    
    @     0x7faa0842e687  (unknown)                                                                                                    
    @     0x7faa08a9145e  caffe::ReadNetParamsFromBinaryFileOrDie()                                                                    
    @     0x7faa08be4197  caffe::Net<>::CopyTrainedLayersFromBinaryProto()                                                             
    @     0x7faa08be4230  caffe::Net<>::CopyTrainedLayersFrom()                                                                        
    @           0x407ca4  CopyLayers()                                                                      
    @           0x408654  train()                                                                                                      
    @           0x4059ec  main                                                                                                         
    @     0x7faa07657ec5  (unknown)                                                                                                    
    @           0x4063ab  (unknown)                                                                                                    
    @              (nil)  (unknown)                                                                                                 
Aborted                                                    
"
the meaning of res5,ZhaofanQiu/pseudo-3d-residual-networks,2018-04-25 15:18:03,0,,23,317675322,"Hi，
Thanks for your code about pesudo-3d.I have a question about the network.That's why the network has 5 dim([batch_size, t_size, n_size, n_size, out_dim]) at res4? but the output of res4 will be reshape 4 dim ,and the two last one is 10.what's the meaning of it ?it will be changed to conv2d .what's the meaning about it? @ZhaofanQiu "
train.prototxt and solver.prototxt,ZhaofanQiu/pseudo-3d-residual-networks,2018-04-11 12:40:56,0,,22,313306187,"Hi, thank you for sharing this great work.
I want to training the P3D,but I facing some problem about training detail. If you release the train.prototxt and solver.prototxt, I will be very grateful to you. 
Thank you !"
DeepDraw pseudo-3d,ZhaofanQiu/pseudo-3d-residual-networks,2018-03-19 21:01:57,2,,21,306633993,"Would you share you deepdraw code.
Thank you"
IDT,ZhaofanQiu/pseudo-3d-residual-networks,2018-03-10 01:38:38,0,,20,304029550,"Hi, thank you for sharing this great work.

Would you give more details about how the IDT is used to improve the results? Which library you used to calculate? How the merge with the pseudo features is done?"
Label order missing for testing on Kinetics dataset,ZhaofanQiu/pseudo-3d-residual-networks,2018-02-26 18:35:39,1,,18,300348541,"I'm having trouble casually testing the model. What label order was used for the Kinetics dataset when training?
I assumed alphabetic order for one attempt, but the results were not representative.
Thank you."
"google.protobuf.text_format.ParseError: 65:3 : Message type ""caffe.LayerParameter"" has no field named ""bn_param"".",ZhaofanQiu/pseudo-3d-residual-networks,2018-01-22 08:21:26,2,,17,290382842,"Hello, when i try to draw the picture of network structure as the follow :
`python /caffe-master/python/draw_net.py deploy_p3d_resnet_sports1m.prototxt deploy_p3d_resnet_sports1m.jpg --rankdir=TB`

there has a error that `google.protobuf.text_format.ParseError: 65:3 : Message type ""caffe.LayerParameter"" has no field named ""bn_param"". `

Thanks~"
about input format,ZhaofanQiu/pseudo-3d-residual-networks,2018-01-15 07:01:49,3,,16,288496789,"Hi,Zhanfan,i found you did not mention the input format.Should the train data is showed as a training folder,then it include some clip floders?Could you show some training or testing folders as a demo file?
Thank you very much!!!"
about the input,ZhaofanQiu/pseudo-3d-residual-networks,2017-12-27 06:06:49,4,,15,284655407,"hi,
according to the paper,the input of the model is a 16-frames clip，but most video is much longer than 16 frames.Do you use the 16-frame clip  presenting the whole video clip to train the model?"
Consult whether this codes can be complied and run on Linux,ZhaofanQiu/pseudo-3d-residual-networks,2017-11-28 12:16:50,12,,13,277363731,"Hello ZhaoFan,

Thanks for your great work.

I would like to consult whether your codes can be compiled and run on Caffe for Linux without any modifications except updating configurations. I suspect that it will be ""NO"" since I got many compiling error under Linux....Could you please help to point it?  I only need answer ""Yes"" Or ""NO"", a simple reply.  Thanks in advance. 

"
questions on training with p3d,ZhaofanQiu/pseudo-3d-residual-networks,2017-11-27 10:15:41,7,,12,276964576,"i am currently trying to reimplement  on UCF101.

first, when i do the training with p3d-b, i find that the total loss suddenly increases from around 4 to 11,  after four or five epoch. did you meet the same problems?

second, how do you come up with doing a two-stage training, finetune p3d on a trained tsn  model? 

third, why do you decide to decrease the input from 224x224 to 160x160? "
Detail of using it to extract feature from video?,ZhaofanQiu/pseudo-3d-residual-networks,2017-11-20 10:57:53,2,,10,275314323,"I'm really sorry to bother you,I want use P3D to extract the feature of video like C3D,  Can you show me some detail of implement?
In the caffe version of P3D, I have compiled the original caffe， shoud I  complie the caffe with some file you have supported？"
question about merge file ,ZhaofanQiu/pseudo-3d-residual-networks,2017-11-15 06:02:32,2,,9,274044369,"Thank you for your master work! I thank it may help me a lot. I download the addition layers and merge into original caffe. then I compile it, but I get the follow errors. _# I am not very sure that the directory I put is right  ._ 

 CXX src/caffe/layers/pooling3d_layer.cpp
src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer<Dtype>::LayerSetUp(const std::vector<caffe::Blob<Dtype>*>&, const std::vector<caffe::Blob<Dtype>*>&) [with Dtype = float]’:
src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here
src/caffe/layers/pooling3d_layer.cpp:24:69: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
  Pooling3DParameter pool_param = this->layer_param_.pooling3d_param();
                                                                     ^
In file included from ./include/caffe/common.hpp:6:0,
                 from ./include/caffe/blob.hpp:8,
                 from ./include/caffe/layers/pooling3d_layer.hpp:20,
                 from src/caffe/layers/pooling3d_layer.cpp:14:
src/caffe/layers/pooling3d_layer.cpp:87:5: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
     == Pooling3DParameter_PoolMethod_AVE
     ^
src/caffe/layers/pooling3d_layer.cpp:89:5: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
     == Pooling3DParameter_PoolMethod_MAX)
     ^
src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer<Dtype>::Reshape(const std::vector<caffe::Blob<Dtype>*>&, const std::vector<caffe::Blob<Dtype>*>&) [with Dtype = float]’:
src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here
src/caffe/layers/pooling3d_layer.cpp:140:51: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
   if (this->layer_param_.pooling3d_param().pool() ==
                                                   ^
src/caffe/layers/pooling3d_layer.cpp:146:51: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
   if (this->layer_param_.pooling3d_param().pool() ==
                                                   ^
src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer<Dtype>::Forward_cpu(const std::vector<caffe::Blob<Dtype>*>&, const std::vector<caffe::Blob<Dtype>*>&) [with Dtype = float]’:
src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here
src/caffe/layers/pooling3d_layer.cpp:167:3: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
   switch (this->layer_param_.pooling3d_param().pool()) {
   ^
src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer<Dtype>::Backward_cpu(const std::vector<caffe::Blob<Dtype>*>&, const std::vector<bool>&, const std::vector<caffe::Blob<Dtype>*>&) [with Dtype = float]’:
src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here
src/caffe/layers/pooling3d_layer.cpp:288:3: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
   switch (this->layer_param_.pooling3d_param().pool()) {
   ^
src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer<Dtype>::LayerSetUp(const std::vector<caffe::Blob<Dtype>*>&, const std::vector<caffe::Blob<Dtype>*>&) [with Dtype = double]’:
src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here
src/caffe/layers/pooling3d_layer.cpp:24:69: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
  Pooling3DParameter pool_param = this->layer_param_.pooling3d_param();
                                                                     ^
In file included from ./include/caffe/common.hpp:6:0,
                 from ./include/caffe/blob.hpp:8,
                 from ./include/caffe/layers/pooling3d_layer.hpp:20,
                 from src/caffe/layers/pooling3d_layer.cpp:14:
src/caffe/layers/pooling3d_layer.cpp:87:5: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
     == Pooling3DParameter_PoolMethod_AVE
     ^
src/caffe/layers/pooling3d_layer.cpp:89:5: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
     == Pooling3DParameter_PoolMethod_MAX)
     ^
src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer<Dtype>::Reshape(const std::vector<caffe::Blob<Dtype>*>&, const std::vector<caffe::Blob<Dtype>*>&) [with Dtype = double]’:
src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here
src/caffe/layers/pooling3d_layer.cpp:140:51: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
   if (this->layer_param_.pooling3d_param().pool() ==
                                                   ^
src/caffe/layers/pooling3d_layer.cpp:146:51: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
   if (this->layer_param_.pooling3d_param().pool() ==
                                                   ^
src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer<Dtype>::Forward_cpu(const std::vector<caffe::Blob<Dtype>*>&, const std::vector<caffe::Blob<Dtype>*>&) [with Dtype = double]’:
src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here
src/caffe/layers/pooling3d_layer.cpp:167:3: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
   switch (this->layer_param_.pooling3d_param().pool()) {
   ^
src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer<Dtype>::Backward_cpu(const std::vector<caffe::Blob<Dtype>*>&, const std::vector<bool>&, const std::vector<caffe::Blob<Dtype>*>&) [with Dtype = double]’:
src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here
src/caffe/layers/pooling3d_layer.cpp:288:3: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
   switch (this->layer_param_.pooling3d_param().pool()) {
   ^
Makefile:581: recipe for target '.build_release/src/caffe/layers/pooling3d_layer.o' failed
make: *** [.build_release/src/caffe/layers/pooling3d_layer.o] Error 1
"
src/caffe/layers/pooling3d_layer.cpp:24:69: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’,ZhaofanQiu/pseudo-3d-residual-networks,2017-11-14 07:10:31,4,,8,273689043,"After adding your layers to my caffe , I met the following errors when I compile the caffe :

src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer<Dtype>::LayerSetUp(const std::vector<caffe::Blob<Dtype>*>&, const std::vector<caffe::Blob<Dtype>*>&) [with Dtype = float]’:
src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here
src/caffe/layers/pooling3d_layer.cpp:24:69: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
  Pooling3DParameter pool_param = this->layer_param_.pooling3d_param();
                                                                     ^
In file included from ./include/caffe/common.hpp:6:0,
                 from ./include/caffe/blob.hpp:8,
                 from ./include/caffe/layers/pooling3d_layer.hpp:20,
                 from src/caffe/layers/pooling3d_layer.cpp:14:
src/caffe/layers/pooling3d_layer.cpp:87:5: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
     == Pooling3DParameter_PoolMethod_AVE
     ^
src/caffe/layers/pooling3d_layer.cpp:89:5: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
     == Pooling3DParameter_PoolMethod_MAX)
     ^
src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer<Dtype>::Reshape(const std::vector<caffe::Blob<Dtype>*>&, const std::vector<caffe::Blob<Dtype>*>&) [with Dtype = float]’:
src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here
src/caffe/layers/pooling3d_layer.cpp:140:51: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
   if (this->layer_param_.pooling3d_param().pool() ==
                                                   ^
src/caffe/layers/pooling3d_layer.cpp:146:51: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
   if (this->layer_param_.pooling3d_param().pool() ==
                                                   ^
src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer<Dtype>::Forward_cpu(const std::vector<caffe::Blob<Dtype>*>&, const std::vector<caffe::Blob<Dtype>*>&) [with Dtype = float]’:
src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here
src/caffe/layers/pooling3d_layer.cpp:167:3: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
   switch (this->layer_param_.pooling3d_param().pool()) {
   ^
src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer<Dtype>::Backward_cpu(const std::vector<caffe::Blob<Dtype>*>&, const std::vector<bool>&, const std::vector<caffe::Blob<Dtype>*>&) [with Dtype = float]’:
src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here
src/caffe/layers/pooling3d_layer.cpp:288:3: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
   switch (this->layer_param_.pooling3d_param().pool()) {
   ^
src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer<Dtype>::LayerSetUp(const std::vector<caffe::Blob<Dtype>*>&, const std::vector<caffe::Blob<Dtype>*>&) [with Dtype = double]’:
src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here
src/caffe/layers/pooling3d_layer.cpp:24:69: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
  Pooling3DParameter pool_param = this->layer_param_.pooling3d_param();
                                                                     ^
In file included from ./include/caffe/common.hpp:6:0,
                 from ./include/caffe/blob.hpp:8,
                 from ./include/caffe/layers/pooling3d_layer.hpp:20,
                 from src/caffe/layers/pooling3d_layer.cpp:14:
src/caffe/layers/pooling3d_layer.cpp:87:5: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
     == Pooling3DParameter_PoolMethod_AVE
     ^
src/caffe/layers/pooling3d_layer.cpp:89:5: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
     == Pooling3DParameter_PoolMethod_MAX)
     ^
src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer<Dtype>::Reshape(const std::vector<caffe::Blob<Dtype>*>&, const std::vector<caffe::Blob<Dtype>*>&) [with Dtype = double]’:
src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here
src/caffe/layers/pooling3d_layer.cpp:140:51: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
   if (this->layer_param_.pooling3d_param().pool() ==
                                                   ^
src/caffe/layers/pooling3d_layer.cpp:146:51: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
   if (this->layer_param_.pooling3d_param().pool() ==
                                                   ^
src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer<Dtype>::Forward_cpu(const std::vector<caffe::Blob<Dtype>*>&, const std::vector<caffe::Blob<Dtype>*>&) [with Dtype = double]’:
src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here
src/caffe/layers/pooling3d_layer.cpp:167:3: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
   switch (this->layer_param_.pooling3d_param().pool()) {
   ^
src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer<Dtype>::Backward_cpu(const std::vector<caffe::Blob<Dtype>*>&, const std::vector<bool>&, const std::vector<caffe::Blob<Dtype>*>&) [with Dtype = double]’:
src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here
src/caffe/layers/pooling3d_layer.cpp:288:3: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’
   switch (this->layer_param_.pooling3d_param().pool()) {
   ^
Makefile:581: recipe for target '.build_release/src/caffe/layers/pooling3d_layer.o' failed
make: *** [.build_release/src/caffe/layers/pooling3d_layer.o] Error 1

How can I solve these errors? Thank you ."
Question about how to extract the feature.,ZhaofanQiu/pseudo-3d-residual-networks,2017-11-09 12:28:10,9,,7,272546899,"Hi, I am learning how to extract the video feature from your proposed P3d.
What's the format of input? Can you show more details such as the usage.
Thank you very much. "
leave_one_out,ZhaofanQiu/pseudo-3d-residual-networks,2017-11-07 06:24:39,4,,6,271722721,"Hi,@ZhaofanQiu,thanks for your nice job!

How do you implement the leave-one-video-out algorithm for the Dynamic Scene dataset?
This dataset has 130 videos(13 class,each class 10 videos), Just train 130 models with the libsvm ?
"
Is there a pytorch version of Pseudo-3d？,ZhaofanQiu/pseudo-3d-residual-networks,2017-10-30 09:25:13,6,,4,269530318,"sorry to impose,   have you ever reproduced result of P-3d in PyTorch?
"
Pretrained model download,wadimkehl/ssd-6d,2022-07-08 03:39:45,0,,25,1298440007,"Hello, I can not download the pretrained model from the link you provided. I would appreciate it if you could send me the trained networks.   My email address is chengweirao@gmail.com, thanks a lot ! "
More than one renderer instance,wadimkehl/ssd-6d,2020-05-16 04:38:22,2,,24,619376607,"I am using your Renderer class for images with different height and width. So, I need to have more than one Renderer instance. Although I clear the previous renderer, I still can not build a new one with different height and width. What is your suggestion?
"
ModuleNotFoundError: No module named 'utils.sixd' ,wadimkehl/ssd-6d,2019-10-03 14:25:10,0,,22,502099760,"I got this error.
(tf_gpu) D:\ssd-6d-master\ssd-6d-master>python run.py -/sixd/tejani/, -1, -10, -/weights_pretrained/tejani/tejani_obj_01.pb, -0.5, -3, -3
Traceback (most recent call last):
File ""run.py"", line 28, in
from utils.sixd import load_sixd
ModuleNotFoundError: No module named 'utils.sixd'"
raise ValueError('Program.bind() requires a VertexBuffer.'),wadimkehl/ssd-6d,2019-08-05 14:49:49,0,,20,476903022,"How can I solve this problem?

Traceback (most recent call last):
  File ""run.py"", line 103, in <module>
    final = verify_6D_poses(dets_6d, model_map, bench.cam, f.color)
  File ""/home/~/ssd-6d-master/rendering/utils.py"", line 223, in verify_6D_poses
    ren.draw_model(model, pose)
  File ""/home/~/ssd-6d-master/rendering/renderer.py"", line 160, in draw_model
    used_program.bind(model.vertex_buffer)
  File ""/usr/local/lib/python3.5/dist-packages/vispy/gloo/program.py"", line 293, in bind
    raise ValueError('Program.bind() requires a VertexBuffer.')
ValueError: Program.bind() requires a VertexBuffer.
"
 pretrained network ,wadimkehl/ssd-6d,2019-06-25 03:29:50,0,,19,460190425,I can't open the pretrained network link.  I would appreciate it if you could send me the trained networks. zlm901204@gmail.com.
Pretrained NetWork,wadimkehl/ssd-6d,2018-09-27 07:44:02,0,,15,364344467,"Hi, thank you for your great work.
But I can't open the pretrained network link. And how can i get it? 
Thx
"
something about code,wadimkehl/ssd-6d,2018-05-28 05:49:44,2,,13,326904108,"hello,I plan to reproduce the model, but you only provide some code. Can you provide further information ? Thank you"
Some bugs with Pyglet,wadimkehl/ssd-6d,2018-04-10 09:08:11,18,,11,312835029,"Traceback (most recent call last):
  File ""run.py"", line 26, in <module>
    from rendering.utils import precompute_projections, build_6D_poses, verify_6D_poses
  File ""/home/shenqi/SSD-6D/ssd-6d/rendering/utils.py"", line 10, in <module>
    from rendering.renderer import Renderer
  File ""/home/shenqi/SSD-6D/ssd-6d/rendering/renderer.py"", line 8, in <module>
    app.use_app('Pyglet')   # Set backend
  File ""/usr/local/lib/python3.5/dist-packages/vispy/app/_default_app.py"", line 47, in use_app
    default_app = Application(backend_name)
  File ""/usr/local/lib/python3.5/dist-packages/vispy/app/application.py"", line 49, in __init__
    self._use(backend_name)
  File ""/usr/local/lib/python3.5/dist-packages/vispy/app/application.py"", line 223, in _use
    raise RuntimeError(msg)
RuntimeError: Could not import backend ""Pyglet"":
Cannot connect to ""None""

I run the code ,but there is some bugs. So could you please tell me how to solve it?
"
Something about Network architeture,wadimkehl/ssd-6d,2018-03-02 14:02:19,5,,8,301775489,"""Thereafter, we successively branch off after the ’Inception-A’ blocks for a 35 × 35 × 384 feature map, after the ’Inception-B’ blocks for a 17 × 17 × 1024 feature map and after the ’Inception-C’ blocks for a 9 × 9 × 1536 map. (We changed the padding of Inception-B s.t. the next block contains a map with odd dimensionality to always contain a central position.)""

Hi,in your paper,we need to get feature maps with size 9 × 9 × 1536.but the original Network come with 8 × 8 × 1536. you explain that you changed the padding of Inception-B.However this feature map is branch off after Inception-C.Is there anything I understand wrongly?I guess.
"
A question about the loss,ducksoup/autodial,2021-06-19 12:14:45,0,,2,925382311,"Why log loss[-sum(log f  (yis; xsi ))] is used in multi classification problem instead of cross entropy？
For example, I have three categories. When the prediction y of the network is correct, such as equal to (1,0,0) or (0,1,0), the loss is infinite."
what is the file of  'resFileList'  in the file computeDivStats.py?,rakshithShetty/captionGAN,2021-05-25 22:35:11,1,,5,901454577,"can anyone tell what is the file of  'resfile' in the file computeDivStats?
![image](https://user-images.githubusercontent.com/50869722/119577011-705ce600-bdec-11eb-84bc-a74d3242df72.png)
Thank you so much!"
How  to evaluate the pretrained models and get the scores of metrics?,rakshithShetty/captionGAN,2021-05-25 19:51:47,0,,4,901284272,"could anyone tell me how to use the code to evaluate the pretrained model and get the scores of metrics like bleu,meteor,cider,spice?Thank you very much!"
Downloading data and pretrained models,rakshithShetty/captionGAN,2018-10-28 07:28:57,0,,3,374725951,"Data: [https://drive.google.com/open?id=0B76QzqVJdOJ5VjlaR294SVV6Z00](url) 
Pre-Trained: [https://drive.google.com/open?id=0B76QzqVJdOJ5TV9FMjhpVmlsTFE](url)
The above two links are not available, it seems that you put all of them to the rubbish bin, could you make them available? "
Question about your data file,rakshithShetty/captionGAN,2018-03-25 22:46:31,2,,2,308393547,"Dear Rakshith,

May I ask the exact format of your file ""resnet150_2048-mean.npy""?

1) Does it have to be in same order with the items in your ""labels.txt"" file and ""fasterRcnn_clasDetFEat80.npy""? 

2) By running the scripts under [https://github.com/akirafukui/vqa-mcb/tree/master/preprocess ](url), it extract feature in 2048x14x14 dims. Did you then calculate the mean value of each 14x14 block? Is this the content in file ""resnet150_2048-mean.npy"" ?

Thank you so much. 
Best Regards,
Li"
How to use .npz files from ResNet feature extractor,rakshithShetty/captionGAN,2018-02-28 06:25:53,10,,1,300906787,"Hi Rakshith,

Thank you for putting up your code! It is extremely helpful.

I used the link (https://github.com/akirafukui/vqa-mcb/tree/master/preprocess) in your README to extract ResNet features for use in the adversarial training.

The extractor gave me a large number of .npz files. Your example makes it seem as though everything should be in one file. I am thus wondering how I should specify to the program which files to use as image features."
What if I train these models from scratch with not-so-good computer?,facebookresearch/low-shot-shrink-hallucinate,2021-03-07 12:05:34,0,,18,823901992,"Hello, I am a fresh learner. I have noticed that the model is trained on 4 GPUs for two days(In this paper).

So does that mean, if I don't have a good computer , I can't run the experiments on my computer?

Thanks!"
Does the ConvNet  trained on all 1k lasses,facebookresearch/low-shot-shrink-hallucinate,2021-02-25 14:52:22,2,,16,816509445,
question about three training stages,facebookresearch/low-shot-shrink-hallucinate,2020-12-25 07:59:24,0,,15,774670366,"I have some questions about three training stages.  

1. I find you train novel class together with base class. However, the feature of base class far greater than novel class.  So it suffers a severe label imbalance during low shot training. why it still can work?
2. During base training, I find you use 389 class out of 1000 class, however you train a classifier with 1000 output, it is so weired.
3. When low shot training finished, I got a linear module, but its output is combine novel class with base class, so if I use imagenet1k for base training and use flowers102 for few-shot(one image per class) novel class training,  so I can get a model with a 1102 classifier? It's very weird.

"
How do I add novel classes from my own data?,facebookresearch/low-shot-shrink-hallucinate,2019-07-22 19:33:40,0,,14,471275377,It isn't clear to me ow I can train on my own dataset from novel classes I introduce. How would one go about this?
Running out of memory when training ResNet50,facebookresearch/low-shot-shrink-hallucinate,2019-06-05 02:23:25,0,,13,452286391,"Hi, I am trying to train a ResNet50 for the initial representation learning stage on Python 3.7 and PyTorch version 0.4.1.post2, and for some reason every time I run this I get an out of memory error, even when running on multiple GPUs with 24 GB memory. This most likely has to do with backwards passing, as the script finishes running if I just comment out loss.backward(). I've also looked into running with no_grad() but with errors. The command I used was:

python ./main.py --model ResNet50 \
  --traincfg base_classes_train_template.yaml \
  --valcfg base_classes_val_template.yaml \
  --print_freq 10 --save_freq 10 \
  --aux_loss_wt 0.02 --aux_loss_type sgm \
  --checkpoint_dir checkpoints/ResNet50_sgm

The code worked perfectly with ResNet10, so I was wondering if there could be a solution to this issue? Do I have to run it on a different version of PyTorch or is it possible to fix with my current setup? Thank you for your help!"
The implement of matching network seems to not to use cosine distance.,facebookresearch/low-shot-shrink-hallucinate,2019-04-12 15:16:51,0,,12,432617707,"In the paper of matching network, it use cosine distance as metric. But in matching_networks.py, I didn't find cosine distance.  I wonder why this file didn't use cosine distance. Thanks"
Question about training the generator,facebookresearch/low-shot-shrink-hallucinate,2018-08-24 10:23:18,0,,7,353728215,"Hi, I'm trying to use the method on a completely different dataset. My implementation is in Keras and I built the generator model such that I can view the two losses (rms loss and cls loss) during the training phase. I trained for 10 epochs and I get suspicious results:

1. The classification loss/accuracy does not change at all (is it because I don't change the weights of the classifier?) and the accuracy is *very* small: `~0.0818` (the classifier achieved `~0.7` accuracy on a test set for trained categories).
2. The rms loss/accuracy improves - from `~0.6390` accuracy in the first epoch to `~0.7245` in the last epoch while I used `λ=1`.
3. The rms loss/accuracy converges to `~0.6286` in the second epoch (and does not change at all aftwards) while I used `λ=10` (as you did).

Is there something I should be worried about those results? 
Do you have any enlightenments?

Thanks!"
Generated results do not match those in the paper / website,facebookresearch/low-shot-shrink-hallucinate,2018-08-01 21:09:49,12,,6,346770140,"Hello, when running this code I am not able to reproduce your result.

For example, using ResNet10 with no auxiliary loss and no generation, these are my results.

```
> n 	Novel 	Novel 	Base 	Base 	All 	All
>   	Top-1 	Top-5 	Top-1 	Top-5 	Top-1 	Top-5
> =====================================================
> 1	10.05	35.00	72.58	88.29	34.22	55.60
> 2	17.12	48.77	70.82	86.70	37.88	63.43
> 5	23.97	59.94	70.30	86.18	41.88	70.09
> 10	26.69	64.39	70.58	86.12	43.65	72.79
> 20	28.37	66.75	70.84	86.19	44.79	74.27
> =====================================================
> mean	21.24	54.97	71.02	86.70	40.48	67.24
```

A difference of 8 points for n=1 on Novel Top-1 seems very large. Also, this baseline model performs a lot worse than reported large n.

For reproducibility, I used the following command (and pytorch 0.4.0):

```
python ./main.py --model ResNet10 \
  --traincfg base_classes_train_template.yaml \
  --valcfg base_classes_val_template.yaml \
  --print_freq 10 --save_freq 10 \
  --checkpoint_dir checkpoints/ResNet10_sgm

python3.6 ./save_features.py \
  --cfg train_save_data.yaml \
  --outfile features/ResNet10/train.hdf5 \
  --modelfile checkpoints/ResNet10/89.tar \
  --model ResNet10
python3.6 ./save_features.py \
  --cfg val_save_data.yaml \
  --outfile features/ResNet10/val.hdf5 \
  --modelfile checkpoints/ResNet10/89.tar \
  --model ResNet10


for c in {1..5}
do
for n in 1 2 5 10 20
do
  python3.6 ./low_shot.py \
   --lowshotmeta ./label_idx.json \
   --experimentpath ./experiment_cfgs/splitfile_$c.json \
   --experimentid  $c \
   --lowshotn $n  \
   --trainfile features/ResNet10/train.hdf5 \
   --testfile features/ResNet10/val.hdf5 \
   --outdir results/ResNet10 \
   --lr 1 --wd 0.001  --testsetup 1 &
done
wait
done

python3.6 ./parse_results.py --resultsdir results/ResNet10/ \
  --repr ResNet10 \
  --lr 1 --wd 0.001 \
  --max_per_label 0
```

This is the code found on your website with the SGM option removed -- perhaps these are not the hyperparams you are using. If this is the case then please let me know.

Best,
Mitchell
"
Questions about the dataset .,facebookresearch/low-shot-shrink-hallucinate,2018-04-23 14:02:14,2,,4,316826560,"i dont know where i can find the dataset you used.And  please tell me what i should do if i want to set my own dataset? please help me,thanks!"
Missing images when running save_features,facebookresearch/low-shot-shrink-hallucinate,2018-04-17 17:50:36,1,,3,315165861,What version of ImageNet is necessary for saving features? I'm currently using the ILSVRC2012 Challenge dataset that has remain unchanged in challenges from 2012-now but I'm getting an error saying certain images are missing when I run the save_features command specified in the README.
Error about the size of tensor,fxia22/kdnet.pytorch,2019-08-02 22:20:24,7,,11,476375559,"Hi, Thanks for your great work. 
When I ran the train.py, it gave me an error:
{'Airplane': 0, 'Bag': 1, 'Cap': 2, 'Car': 3, 'Chair': 4, 'Earphone': 5, 'Guitar': 6, 'Knife': 7, 'Lamp': 8, 'Laptop': 9, 'Motorbike': 10, 'Mug': 11, 'Pistol': 12, 'Rocket': 13, 'Skateboard': 14, 'Table': 15}
16 15990
Traceback (most recent call last):
  File ""train.py"", line 89, in <module>
    pred = net(points_v, cutdim_v)
  File ""/home/ywyue/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File ""train.py"", line 45, in forward
    x1 = kdconv(x, 2048, 8, c[-1], self.conv1)
  File ""train.py"", line 37, in kdconv
    sel = Variable(sel + (torch.arange(0, dim) * 3).long())
RuntimeError: The size of tensor a (2) must match the size of tensor b (2048) at non-singleton dimension 0

I am using pytorch 1.1.0, but I don't think this error was caused by the pytorch version. Could you help me with that? Thanks."
can I get kd_tree sorted pointcloud by kd_tree.py,fxia22/kdnet.pytorch,2019-05-09 18:14:03,0,,10,442365926,"Hi,
First,thank you for the contribution. I want to know that is it possible to get the sorted pointcloud by your code, since I found that the 'tree[-1]' output of make_cKDTree() is a pointcloud which have same size of input pointcloud and seemed to be spatial sorted. However, there are some overlapping points in it which makes it a little different from the original pointcloud. Thank you again.
Best wishes

"
"the code in ""kdnet.py"" may be wrong",fxia22/kdnet.pytorch,2018-12-17 02:32:14,3,,9,391536454,"In my opinion, the index selection part in the ""KDNet_Batch"" in ""kdnet.py"" may be wrong. The offset addition part of variable ""sel"" makes that the order in ""sel"" can not match the real points' order in ""x"".
The mistake can be fixed by change 
""sel = Variable(sel + (torch.arange(0,dim) * 3).repeat(batchsize,1).long()).view(-1,1)"" 
into 
""sel = Variable(sel * dim + torch.arange(0,dim).repeat(batchsize,1).long()).view(-1,1)"""
Could you please share the captions for KTH datasets?,Singularity42/cap2vid,2019-09-05 14:28:00,0,,6,489800788,"I noticed that you manually added captions for KTH dataset.
It still needs a lot of work. Could you please share the captions?
Thank you very much!"
How to generate videos from a given caption ?,Singularity42/cap2vid,2018-10-16 11:01:33,0,,4,370556267,"Could you please elaborate on how to perform the actual cap2vid generation, since the testing phase generates only arbitrary videos in the given code."
How are words in the captions represented in the BiLSTM model? ,Singularity42/cap2vid,2018-04-08 20:31:46,0,,3,312338346,"They are mapped to unique integers while saving the captions in h5py file. However, before feeding them into the bidirectional are they represented as an embedding/fixed size vector? or the y_encode function in this [comment](https://github.com/Singularity42/cap2vid/blob/f21ac40a2581a58e54bec7288c94ba99c9664876/cap2vid_with_cnn.py#L284) does it?"
where can I get the caption label,Singularity42/cap2vid,2017-12-06 09:00:38,2,,2,279674490,"where can I get the caption label?
Thank you."
multi-shot setting,KovenYu/CAMEL,2018-10-25 11:43:06,0,,2,373905866,"Hi, how do you perform multi-shot setting for Market1501? I am confused because in query set, there is only one image in one camera for one person, no multiple images."
CAMEL_supervised.p,KovenYu/CAMEL,2018-10-12 06:14:13,5,,1,369413107,"@KovenYu 同学，你好！看了你的论文，自己非常感兴趣，想看看你的效果和代码，但是发现CAMEL_supervised.p打不开，同学，请问可不可以看看你这里面的源码？
                                              祝好！"
Could you give me CASIA NIR-VIS2.0 dataset? Thanks,stephenkung/NIR_VIS_Face_Recognition,2020-08-04 09:13:06,0,,4,672637707,"I really need this database but the official website seems cannot open.
If you have it, appreciate that send to my email: jack.liangweijie@gmail.com
Or I can pay for it too, many thanks!"
Could you give me the CASIA NIR-VIS2.0 dataset? It can be paid. ,stephenkung/NIR_VIS_Face_Recognition,2019-05-20 14:42:51,3,,3,446156225,"My email is :
jhzhang_8@163.com
Please,Thank you!"
I test arc face model however the acc only reach,stephenkung/NIR_VIS_Face_Recognition,2019-02-26 03:15:32,3,,2,414410902,
Can you share your nir database with me?,stephenkung/NIR_VIS_Face_Recognition,2019-02-23 07:21:18,0,,1,413667721,"Hey,
the official link has expired [http://www.cbsr.ia.ac.cn/english/NIR-VIS-2.0-Database.html](url).
Can you share your nir database with me?
Thanks."
no image,SeokjuLee/VPGNet,2022-10-10 07:22:47,0,,60,1402711690,"I downloaded your data set, but there are no image in it."
the loss of vp,SeokjuLee/VPGNet,2021-08-09 06:30:13,0,,59,963688395,Could you tell me how to calculate the loss of vp ？What method is better?
Could you provide the camera parameters?,SeokjuLee/VPGNet,2021-08-08 11:16:23,0,,58,963409485,"Thanks for providing the dataset.
But, I have a few questions, how can I extract the mat file to get the jpg or jpeg image?
Besides, could you please provide the camera intrinsic and extrinsic parameters?
If so, it's really helpful for my research.
Thank you very much!"
Could you provide me how you split training set and testing set in your dataset like Table 1,SeokjuLee/VPGNet,2021-04-19 03:24:39,1,,57,860860068,"<img width=""667"" alt=""Screen Shot 2021-04-19 at 04 54 49"" src=""https://user-images.githubusercontent.com/76802127/115177514-98746d80-a0f9-11eb-92d6-9019458eb51e.png"">
"
Inference scripts using pretrained model,SeokjuLee/VPGNet,2021-02-24 15:12:12,0,,56,815569910,"As per readme, there are steps for training on caltech and VPGNet Dataset.
Are there any scripts/steps written to run inference on a dataset using trained model."
Training on VPGNet dataset.,SeokjuLee/VPGNet,2020-09-29 11:07:53,3,,54,711006415,"Dear authors,
#1 Can you show me how to perform training on your VPGNet dataset? 
I don't know how to generate the dataset_list file of your VPGNet dataset in order to run the make_lmbd.sh

#2 The Caltech Lanes dataset seems to be removed from the official website. Can you share a snapshot of the dataset.

Thank you very much 
"
"Bounding Box Regression task not learning, it has constant loss",SeokjuLee/VPGNet,2020-08-18 17:55:47,0,,53,681226872,"I am implementing VPGNET in keras. Task 2,3,4 are classification tasks, so they are learning and inferring okay. But grid box regression task isn't learning. It has a constant loss. I am using linear activation function on the last layer and using Mean Absolute error L1 loss function for bounding box regression task as per paper. I am not normalising any bounding box coordinates, so their range is 0-640 (width) and 0-480 height. The training MAE loss starts off really high, 17.5 to be exact and then stays there."
Dataset,SeokjuLee/VPGNet,2020-03-20 08:36:16,14,,50,584917321,"VPGNet dataset is available.
Please fill out a [form](http://forms.gle/LNCPUgEu4B7XGjLZA) for the download link, and check the README.
Thanks."
Make runtest failed on Ubuntu 16.04 and CUDA 10.0,SeokjuLee/VPGNet,2019-11-13 02:55:20,0,,47,521925650,"### Issue summary
Make runtest failed on Ubuntu 16.04 and CUDA 10.0

> *** Aborted at 1573612683 (unix time) try ""date -d @1573612683"" if you are using GNU date ***
> PC: @     0x7fad4225218e caffe::CuDNNConvolutionLayer<>::LayerSetUp()
> *** SIGFPE (@0x7fad4225218e) received by PID 27175 (TID 0x7fad4d5b9740) from PID 1109729678; stack trace: ***
>     @     0x7fad41776390 (unknown)
>     @     0x7fad4225218e caffe::CuDNNConvolutionLayer<>::LayerSetUp()
>     @           0x4a10b4 caffe::Layer<>::SetUp()
>     @           0x7b36d8 caffe::CuDNNConvolutionLayerTest_TestSimpleConvolutionCuDNN_Test<>::TestBody()
>     @           0x88cc8c testing::internal::HandleSehExceptionsInMethodIfSupported<>()
>     @           0x887d99 testing::internal::HandleExceptionsInMethodIfSupported<>()
>     @           0x8736d4 testing::Test::Run()
>     @           0x873ec6 testing::TestInfo::Run()
>     @           0x874511 testing::TestCase::Run()
>     @           0x8799bf testing::internal::UnitTestImpl::RunAllTests()
>     @           0x88de69 testing::internal::HandleSehExceptionsInMethodIfSupported<>()
>     @           0x888a4a testing::internal::HandleExceptionsInMethodIfSupported<>()
>     @           0x8785ce testing::UnitTest::Run()
>     @           0x496d9b main
>     @     0x7fad413bb830 __libc_start_main
>     @           0x496429 _start
>     @                0x0 (unknown)
> make: *** [Makefile:478: runtest] Floating point exception (core dumped)
> 

### Steps to reproduce
make clean
make all -j16
make test -j16
make runtest

### Tried solutions
modify the makefile.config
```
## Refer to http://caffe.berkeleyvision.org/installation.html
# Contributions simplifying and improving our build system are welcome!

# cuDNN acceleration switch (uncomment to build with cuDNN).
USE_CUDNN := 1

# CPU-only switch (uncomment to build without GPU support).
# CPU_ONLY := 1

# To customize your choice of compiler, uncomment and set the following.
# N.B. the default for Linux is g++ and the default for OSX is clang++
# CUSTOM_CXX := g++

# CUDA directory contains bin/ and lib/ directories that we need.
CUDA_DIR := /usr/local/cuda-10.0
# On Ubuntu 14.04, if cuda tools are installed via
# ""sudo apt-get install nvidia-cuda-toolkit"" then use this instead:
# CUDA_DIR := /usr

# CUDA architecture setting: going with all of them.
# For CUDA < 6.0, comment the *_50 lines for compatibility.
CUDA_ARCH := -gencode arch=compute_30,code=sm_30 \
		-gencode arch=compute_35,code=sm_35 \
		-gencode arch=compute_50,code=sm_50 \
		-gencode arch=compute_52,code=sm_52 \
        	-gencode arch=compute_60,code=sm_60 \
        	-gencode arch=compute_61,code=sm_61 \
        	-gencode arch=compute_61,code=compute_61 

# BLAS choice:
# atlas for ATLAS (default)
# mkl for MKL
# open for OpenBlas
BLAS := atlas
# Custom (MKL/ATLAS/OpenBLAS) include and lib directories.
# Leave commented to accept the defaults for your choice of BLAS
# (which should work)!
# BLAS_INCLUDE := /path/to/your/blas
# BLAS_LIB := /path/to/your/blas

# Homebrew puts openblas in a directory that is not on the standard search path
# BLAS_INCLUDE := $(shell brew --prefix openblas)/include
# BLAS_LIB := $(shell brew --prefix openblas)/lib

# This is required only if you will compile the matlab interface.
# MATLAB directory should contain the mex binary in /bin.
MATLAB_DIR := /usr/local/MATLAB/R2018b
# MATLAB_DIR := /Applications/MATLAB_R2012b.app

# NOTE: this is required only if you will compile the python interface.
# We need to be able to find Python.h and numpy/arrayobject.h.
PYTHON_INCLUDE := /usr/include/python3.5 \
		/home/liying/.local/lib/python3.5/site-packages/numpy/core/include/numpy 	
# Anaconda Python distribution is quite popular. Include path:
# Verify anaconda location, sometimes it's in root.
# ANACONDA_HOME := $(HOME)/anaconda
# PYTHON_INCLUDE := $(ANACONDA_HOME)/include \
		# $(ANACONDA_HOME)/include/python2.7 \
		# $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \

# We need to be able to find libpythonX.X.so or .dylib.
PYTHON_LIB := /usr/lib
# PYTHON_LIB := $(ANACONDA_HOME)/lib

# Homebrew installs numpy in a non standard path (keg only)
# PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include
# PYTHON_LIB += $(shell brew --prefix numpy)/lib

# Uncomment to support layers written in Python (will link against Python libs)
# WITH_PYTHON_LAYER := 1

# Whatever else you find you need goes here.
INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial /usr/local/include/opencv /usr/local/include/opencv2
LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu /usr/lib/x86_64-linux-gnu/hdf5/serial

# If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies
# INCLUDE_DIRS += $(shell brew --prefix)/include
# LIBRARY_DIRS += $(shell brew --prefix)/lib

# Uncomment to use `pkg-config` to specify OpenCV library paths.
# (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)
USE_PKG_CONFIG := 1

BUILD_DIR := build
DISTRIBUTE_DIR := distribute

# Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171
# DEBUG := 1

# The ID of the GPU that 'make runtest' will use to run unit tests.
TEST_GPUID := 0

# enable pretty build (comment to see full commands)
Q ?= @
```

### System configuration

* Operating system: Ubuntu 16.04
* Compiler: GNU make 4.2
* CUDA version (if applicable): CUDA 10.0
* CUDNN version (if applicable): CUDNN 7.6.2
* BLAS: ATLAS
* Python version (if using pycaffe): python 3.5
* MATLAB version (if using matcaffe): R2018b
"
about multi label grid box and object mask some problem,SeokjuLee/VPGNet,2019-11-06 02:38:09,1,,46,518199633,"The output size of multi label is 60 * 80, but the output size of grid box and object mask is 120 * 180. Can you explain the reason for this setting and how to generate grid box and object box through 8 * 8 grid? Thank you again for your work!"
A good value for test result,SeokjuLee/VPGNet,2019-10-22 13:31:49,0,,45,510658500,"Hi @SeokjuLee,

I have been using VPGNet to do some lane detection on our own dataset. For the test loss, we are getting around 3.5. I'm just curious what would be a good number for the test loss? 

Thanks!"
Test code on other data sets,SeokjuLee/VPGNet,2019-07-23 20:16:08,1,,43,471916041,"Hi,

I'm a little confused since most of the files are solely about caffe. Is there a way to test the vpgnet on an image or video from other data sets (kitti, culane, etc)? If so, which script would I run, with the image/video as a file input?"
ENet-Label-Torch is available now (a light-weight and effective lane detection model),SeokjuLee/VPGNet,2019-07-17 11:06:40,0,,42,469129660,"Our **ENet-Label-Torch** has been released. More details can be found in [my repo](https://github.com/cardwing/Codes-for-Lane-Detection).

Key features:

(1) ENet-label is a **light-weight** lane detection model based on [ENet](https://arxiv.org/abs/1606.02147) and adopts **self attention distillation** (more details can be found in our paper which will be published soon).

(2) It has **20** × fewer parameters and runs **10** × faster compared to the state-of-the-art SCNN, and achieves **72.0** (F1-measure) on CULane testing set (better than SCNN which achieves 71.6).

(**Do not hesitate to try our model!!!**)

Performance on CULane testing set (F1-measure):

|Category|[SCNN-Torch](https://github.com/XingangPan/SCNN)|SCNN-Tensorflow|[ENet-Label-Torch](https://github.com/cardwing/Codes-for-Lane-Detection)|
|:---:|:---:|:---:|:---:|
|Normal|90.6|90.2|**90.7**|
|Crowded|69.7|71.9|70.8|
|Night|66.1|64.6|65.9|
|No line|43.4|45.8|44.7|
|Shadow|66.9|73.8|70.6|
|Arrow|84.1|83.8|**85.8**|
|Dazzle light|58.5|59.5|**64.4**|
|Curve|64.4|63.4|**65.4**|
|Crossroad|1990|4137|2729|
|Total|71.6|71.3|**72.0**|
|Runtime(ms)|133.5|--|**13.4**|
|Parameter(M)|20.72|--|**0.98**|"
Nobody cares this project?,SeokjuLee/VPGNet,2019-06-10 08:18:48,0,,41,454051614,
IPM corner points,SeokjuLee/VPGNet,2019-04-05 06:56:02,0,,40,429609386,"Hi seokju,

I have similar problem like the picture below
![visual](https://user-images.githubusercontent.com/44756093/55608964-277fe880-57b2-11e9-8d2e-57d476e775ff.png)
Even though I tried SCNN but I believe the postprocessing step is similar.
below is my result
![hw_curve55_result](https://user-images.githubusercontent.com/44756093/55609048-5ac27780-57b2-11e9-8c98-c0af16421d71.jpg)
the model confuse itself to see points from different lanes. Thus I want to cluster all points again using IPM to seperate the points near VP.

May i know which 4 src points did you use in cv2.getperspective() function. Since the prediction of mine has a large variety, I have some trouble finding these 4 points which encloses all points found.
Could you share some insight how you find these 4 points ?

Best regards,

ZX"
facing a problem when  make all,SeokjuLee/VPGNet,2019-01-09 12:49:08,2,,36,397356518,"CXX/LD -o .build_release/tools/compute_image_mean.bin
.build_release/lib/libcaffe.so: undefined reference to `void caffe::hdf5_load_nd_dataset<double>(int, char const*, int, int, caffe::Blob<double>*)'
.build_release/lib/libcaffe.so: undefined reference to `void caffe::hdf5_load_nd_dataset<float>(int, char const*, int, int, caffe::Blob<float>*)'
.build_release/lib/libcaffe.so: undefined reference to `caffe::hdf5_get_name_by_idx[abi:cxx11](int, int)'
.build_release/lib/libcaffe.so: undefined reference to `void caffe::hdf5_save_nd_dataset<float>(int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, caffe::Blob<float> const&, bool)'
.build_release/lib/libcaffe.so: undefined reference to `caffe::hdf5_get_num_links(int)'
.build_release/lib/libcaffe.so: undefined reference to `void caffe::hdf5_save_nd_dataset<double>(int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, caffe::Blob<double> const&, bool)'
collect2: error: ld returned 1 exit status
Makefile:568: recipe for target '.build_release/tools/compute_image_mean.bin' failed
make: *** [.build_release/tools/compute_image_mean.bin] Error 1
"
How to visualize output of multi-label,SeokjuLee/VPGNet,2018-12-21 11:57:18,1,,35,393433674,"Hello everyone.

I would like to know how I can visualize the output of the multi-label with an example image like it is done here:

https://github.com/SeokjuLee/VPGNet/issues/8#issuecomment-360976797

![35471300-769bf2b6-0393-11e8-9a23-433c9d55e217](https://user-images.githubusercontent.com/26686384/50341620-b9275b00-051f-11e9-9c61-80a1adc81859.png)
"
@SeokjuLee,SeokjuLee/VPGNet,2018-12-07 03:19:21,2,,33,388489764,"Hi, SeokjuLee,  Recently, I am also doing some job about lane and road mask detection and reconnition. I am  so amazing about your job about it.  Now I have a question to ask you for some help. How do you annotate you trainning data set ?   what you steps are? "
Tensorflow version of SCNN is available now (lane detection),SeokjuLee/VPGNet,2018-11-30 05:46:34,3,,32,386046883,"I have implemented SCNN using Tensorflow and put the full codes [here](https://github.com/cardwing/Codes-for-Lane-Detection). You can test the code in popular lane detection benchmarks like TuSimple, CULane and BDD100K or your custom dataset with minor modification. Welcome to raising issues if you have problems in reproducing the results. My code is based on [LaneNet](https://github.com/MaybeShewill-CV/lanenet-lane-detection) and [SCNN-Torch](https://github.com/XingangPan/SCNN)."
Change output shape,SeokjuLee/VPGNet,2018-09-20 11:03:27,0,,31,362135273,"In the data layer, layer output is list as below:
data -> data     32 3 480 640 (29491200)
data -> label    32 8 120 160 (4915200)
data -> type     32 1 60 80 (153600)

I know the shape of ""type"" is depended on ""catalog_resolution"" 
But I can't find out where shape of ""label"" is depended.
If I want to change the output shape of binary-mask, I should change shape of ""label"" firstly, how can I change ""data -> label    32 8 120 160 (4915200)""  to  ""data -> label    32 8 240 320 (4915200)""?"
How to know which channel of 64 multi-labels  is the lane channel?,SeokjuLee/VPGNet,2018-08-15 09:26:55,1,,28,350743499,How can I find each label name of 64 multi-labels?
the VP part problem,SeokjuLee/VPGNet,2018-06-12 08:29:43,4,,24,331487917,as the paper said. can the first task VPP be provide?
"What is the ""build"" in the make_lmdb.sh file  ",SeokjuLee/VPGNet,2018-05-25 08:37:35,1,,21,326430659,"Hi, I want to run the make_lmdb.sh file, but I got the following errors:
make_lmdb.sh: line 3: ../../build/tools/convert_driving_data: No such file or directory
make_lmdb.sh: line 4: ../../build/tools/compute_driving_mean: No such file or directory
make_lmdb.sh: line 6: ../../build/tools/convert_driving_data: No such file or directory

I checked the directories and find out that there is no folder named build, and the tools folder is under caffe. So I want to ask what does ../../build mean?

BTW, I use windows so I run this file on Git.   
 "
The diffenrence between Lane and RoadＭａｒｋｉｎｇ　ｄｅｔｅｃｔｉｏｎ,SeokjuLee/VPGNet,2018-05-11 06:00:50,4,,19,322177199,"I have trained the net two times with similar data which has two different type labels .One of the two type is Lane,conclude broken yellow, broken white and so on.Another type is about roadmarking,conclude straight arrow,right arrow and so on.But the test results were so strange.The lane detection result was good,such as pic 1.But the detection of roadmarking was  beyond my expectation，ａlmost nothing cｏｕｌｄ be detected，such as pic 2. I want to know why are there such a big difference. Can you explain the reason for me？ Thanks a lot @SeokjuLee 
![33333333](https://user-images.githubusercontent.com/35653403/39909060-8d140b82-5523-11e8-9ae9-ad660bab175d.png)
ｐｉｃ　１
![２](https://user-images.githubusercontent.com/35653403/39908177-809f0996-551f-11e8-9132-5717edbfa7cf.png)
ｐｉｃ　２"
"some questions  about  ""convert_driving_data.cpp """,SeokjuLee/VPGNet,2018-05-07 02:38:48,1,,18,320656613,"@SeokjuLee 
  VPGNet/caffe/tools/convert_driving_data.cpp      
[line 115]
  int resize_height = std::max<int>(0, FLAGS_resize_height);   ///480+32=512
 int resize_width = std::max<int>(0, FLAGS_resize_width);       ///640+32=672

  original  images are  resized to  672*512, should coordinates of lines  be  reseized also??

[line 96]
           lines.push_back(std::make_pair(filename, boxes));
"
How to visualize test results?,SeokjuLee/VPGNet,2018-04-14 12:23:01,1,,16,314324834,"Hello,I have finished train and test. Here is a output.log of test.How can I visualize test results without post-process."
difference from the standard caffe ,SeokjuLee/VPGNet,2018-04-04 05:15:41,8,,14,311088213,"Is the caffe used in this project different from the standard caffe?(do your add some special layers?)
can i replace the caffe in this project with the newest caffe? "
How to evaluate the results,SeokjuLee/VPGNet,2018-03-28 03:27:50,6,,12,309208297,"I am now doing the evaluation of the results ,in your paper you said: we compute the minimum distance from the center of each cell to the sampled lane points for every cell. If the minimum distance is within the boundary R, we mark these sampled points as true positive and the corresponding grid cell as detected.
how about the R and the class FP?"
How to validate the model using C++ code or python code?,SeokjuLee/VPGNet,2018-03-19 08:12:52,1,,10,306366650,"I have trained VPGNet, but i don't know how to validate the codel by visualizing outputs. Do you smaple code to work on each image and visualize lane detection output? Kindly shared the code if you have."
caffemodel for VPGNet,SeokjuLee/VPGNet,2018-01-25 03:30:37,4,,9,291439112,"I have trained a caffemodel from your tutorial, but the test result is bad.  I think it is because of little dataset. Could you please tell me where I can download the lane and road marking benchmark which consists of about 20,000 images or give me the caffemodel described in your paper? Thank you very much!



"
Some questions about lane post-processing,SeokjuLee/VPGNet,2018-01-23 09:54:43,25,,8,290773593,"@SeokjuLee hi，In section 4.4 of this paper，how to understand the process of point sampling：“First, we subsample local peaks from the region where the probability of lane channels from the multi-label task is high.”，such as：
1、how to find ""the region""?  in one of the categories's feature map (60×80) or in original image (640×480)？
2、how to understand the peaks？what does its horizontal and vertical coordinates stand for？

thank you very much."
Could you explain more details about how to run make_lmdb.sh?,SeokjuLee/VPGNet,2017-12-27 07:57:15,47,,6,284669763,"I tried to edit the make_lmdb.sh file as following:
../../build/tools/convert_driving_data /media/aquarius/Backup/VPGNet/caltech-lanes-dataset/cordova1/ /media/aquarius/Backup/VPGNet/caltech-lanes-dataset/cordova1/list.txt LMDB_train
../../build/tools/compute_driving_mean LMDB_train ./driving_mean_train.binaryproto lmd
../../build/tools/convert_driving_data /media/aquarius/Backup/VPGNet/caltech-lanes-dataset/cordova2/ /media/aquarius/Backup/VPGNet/caltech-lanes-dataset/cordova2/list.txt LMDB_test
but it seem doesn't work.
Here is my output:
./make_lmdb.sh 
E1227 16:49:05.001368  4610 convert_driving_data.cpp:73] argv[1]: /media/aquarius/Backup/VPGNet/caltech-lanes-dataset/cordova1/
E1227 16:49:05.001581  4610 convert_driving_data.cpp:74] argv[2]: /media/aquarius/Backup/VPGNet/caltech-lanes-dataset/cordova1/list.txt
E1227 16:49:05.264470  4610 convert_driving_data.cpp:145] Total to be processed: 0.
F1227 16:49:05.354425  4617 compute_driving_mean.cpp:68] Unknown db backend lmd
*** Check failure stack trace: ***
    @     0x7f38d4fba5cd  google::LogMessage::Fail()
    @     0x7f38d4fbc433  google::LogMessage::SendToLog()
    @     0x7f38d4fba15b  google::LogMessage::Flush()
    @     0x7f38d4fbce1e  google::LogMessageFatal::~LogMessageFatal()
    @           0x402c73  main
    @     0x7f38d3ed9830  __libc_start_main
    @           0x402f39  _start
    @              (nil)  (unknown)
./make_lmdb.sh: line 7:  4617 Aborted                 (core dumped) ../../build/tools/compute_driving_mean LMDB_train ./driving_mean_train.binaryproto lmd
E1227 16:49:05.457511  4619 convert_driving_data.cpp:73] argv[1]: /media/aquarius/Backup/VPGNet/caltech-lanes-dataset/cordova2/
E1227 16:49:05.457718  4619 convert_driving_data.cpp:74] argv[2]: /media/aquarius/Backup/VPGNet/caltech-lanes-dataset/cordova2/list.txt
E1227 16:49:05.694105  4619 convert_driving_data.cpp:145] Total to be processed: 0.
---------------------------
Could you explain to me more details about this process? I am a newbie in caffe so it took me more than 3 days to fix this and nothing happened.
Thank you in advance."
About post-processing for lane visualization,SeokjuLee/VPGNet,2017-12-27 06:11:37,45,,5,284656033,"Here is an additional explanation about the lane post-processing. There are four steps basically. First, sampling seed points with the lane heat map from the multi-label task. Then, cluster the seeds in the IPM-ed coordinate with our clustering method (“We sequentially decide the cluster by the pixel distance. After sorting the points by the vertical index, we stack the point in a bin if there is a close point among the top of the existing bins. Otherwise, we create a new bin for a new cluster. By doing this, we can reduce the time complexity of the clustering.”). Lastly, we did polynomial line fitting for each cluster. If the cluster is near the VP, we include that VP while clustering for the stability. Related contents are described in our paper, Section 4.4.
![lane-pp](https://user-images.githubusercontent.com/17178662/34372441-0b853b9c-eb18-11e7-8722-76f2b4a68d39.png)"
"Hi, I have some problem with your post process for the lane detection.",SeokjuLee/VPGNet,2017-12-21 03:18:52,14,,4,283761873,"Hi, I found in your paper you declared your algorithm speed could achieve to 20Fps, so I am confused whether the speed you declared is just the speed of the network inference time or with the post process included clustering and curve fitting? If it is with the post-process, could you  share your post process code? Thanks for your good job!"
Code issues,SeokjuLee/VPGNet,2017-12-20 05:55:43,55,,3,283456754,"Please ask installation, training and test issues in this panel."
Questions about quantitative comparison,lmb-freiburg/hand3d,2021-06-09 15:00:45,0,,44,916309980,"Hello, thank you very much for your outstanding work!

My question is, the GT root node of the RHD data set in your paper is at the  position of the wrist. But the root node of the predicted hand pose is in the center of the palm.
So, how do you ensure the fairness of quantitative comparison?"
Please can someone share the link to the binary file that is needed to run the model,lmb-freiburg/hand3d,2021-02-16 13:49:40,0,,43,809328718,
How to get bounding box?,lmb-freiburg/hand3d,2021-01-23 09:18:12,0,,42,792514926,"I would like to visualize a bounding box and visualize it on the full size input image. Is there a option to achieve that?
Thank you!"
Translate normalized 3D coordinates to raw image,lmb-freiburg/hand3d,2020-12-23 10:50:49,0,,41,773665626,"Hi! I have ran your demo code which provides both 2D and normalized 3D coordinates. The first ones can be easily translated to pixel coordinates and be overlaed in the original image. Is there any way to do the same for the 3D coordinates? i.e. translate the coordinates to the pixel scale to overlap x,y values on the original image."
Discrepancy in index bone indices,lmb-freiburg/hand3d,2020-08-20 12:48:48,0,,39,682687438,"According to [STB](https://github.com/zhjwustc/icip17_stereo_hand_pose_dataset) dataset home page, indices of index finger joints are 13 to 16. 

However in `BinaryReaderDbSTB.py`, the `index_root_bone_length` is computed from 11 and 12th joint indices at this [line](https://github.com/lmb-freiburg/hand3d/blob/9f0063391e7075cf4ab1e4edd0461d38213a3fd6/data/BinaryDbReaderSTB.py#L196).

Am I missing something?"
How to get the weight folder?,lmb-freiburg/hand3d,2020-06-03 07:10:50,1,,38,629750540,"Hi!
Thanks for you great work!
I was confused that how you get the weight folder which I directly download from the data you showed on Readme. when I run the run.py, it shows ""Loaded 102 variables from weights/posenet3d-rhd-stb-slr-finetuned.pickle"", ""Loaded 37 variables from weights/handsegnet-rhd.pickle"". I was confused how you get those pickle files, cause after I finish training, I cannot find any model saved like those pickle files.
Thank you 

"
Detecting both hands,lmb-freiburg/hand3d,2020-01-11 08:12:00,2,,37,548399344,"Hi.
Thanks for the great work!
Can this network detect both hands at a time?
I couldn't find a code that can pose estimate both hands simultaneously from run.py.
So I manually masked one side of image if both hands are present.
Would this be only option for detecting 2 hands? or would there be a more convenient way?
Thankyou"
A question about training on STB,lmb-freiburg/hand3d,2019-10-02 22:15:28,0,,35,501754096,"Hello!
I'm trying to achieve the same results that you describe in your paper on the posenet stage when adding the STB dataset. However, the results are far from what you have achieved, and I can not find the reason why. I was hoping if you could enlighten me on this step.

After training with the RHD dataset using the pipeline you've published on `posenet_training.py`, I load `BinaryDbReaderST`B with the following parameters:

`dataset = BinaryDbReaderSTB(mode='training', batch_size=train_para['BATCH_SIZE'], shuffle=True, coord_uv_noise=True, hand_crop=True, crop_center_noise=True, use_wrist_coord=True)`

And proceed to run the session passing the tensors:

`_, loss_v = sess.run([train_op, loss])`

The `BinaryDbReaderSTB` class was not modified and I've processed the data using the scripts you provided. 

I then proceed to evaluate the training, using:

`dataset = BinaryDbReaderSTB(mode='evaluation', shuffle=False, use_wrist_coord=True)`

When executing with `USE_RETRAINED=False`, the metrics are as expected:
`Average mean EPE: 18.581 pixels 
`
However, when using my model trained with RHD+STB, the lowest mean EPE I got was ~40 pixels. Could you please point me to what I am forgetting?

I tried some ideas, as using different epochs combinations, tweaking the lr decay and different configurations on the data loader, but no effect.

Thank you for your attention"
STB xyz convert uv coordinat is false,lmb-freiburg/hand3d,2019-09-05 10:26:09,3,,33,489665610,"I try your create_db.m, I find uv coordinate is false in 'BB', how should I do?"
there is lack of  RollingMeasure in package utils,Ram81/AC-VAEGAN-PyTorch,2020-07-23 07:53:16,2,,8,664272324,Hope to fix it !
Bug #1: VAEGAN module loss function,Ram81/AC-VAEGAN-PyTorch,2019-10-04 10:41:15,0,hacktoberfest,5,502563330,"Current implementation of the AC VAEGAN model is not able to converge properly. After investigating the issue we found some bugs in implementation of loss function.

```
        # reconstruction errors, not used as part of loss just to monitor
        nle = 0.5 * (ten_original.view(len(ten_original), -1)) - ten_predict.view((len(ten_predict), -1)) ** 2

        # kl-divergence
        kl = -0.5 * torch.sum(-variances.exp() - torch.pow(mu, 2) + variances + 1, 1)

        # mse between intermediate layers
        mse = torch.sum((layer_original - layer_predicted) ** 2, 1)

        # BCE for decoder & discriminator for original, sampled & reconstructed
        # the only excluded is the bce_gen original

        bce_dis_original = -torch.log(labels_original)
        bce_dis_sampled = -torch.log(1 - labels_sampled)

        bce_gen_original = -torch.log(1 - labels_original)
        bce_gen_sampled = -torch.log(labels_sampled)

        aux_criteron = nn.NLLLoss()
        nllloss_aux_original = aux_criteron(aux_labels_predicted, aux_labels_original)
        nllloss_aux_sampled = aux_criteron(aux_labels_sampled, aux_labels_original)
````

Please refer the above code. Full source code is available in `model.py`."
Missing .pth file ?,xch-liu/geom-tex-dg,2022-07-04 04:35:06,1,,2,1292607608,"Thanks for your great work. I'm wondering are you miss the `stylepredictor.pt` under the directory `Dassl\\dassl\\modeling\\backbone\\styleaugment\\styleaugeckpoints`. If so, can you upload this file? Thank you very much!"
 Problem of Augmenting Geometric Style,xch-liu/geom-tex-dg,2022-06-23 04:24:27,1,,1,1281716835,Thanks for your great work. I want to verify whether you release the code of **Augmenting Geometric Style**?
Question about pretrained models,nipponjo/deepfillv2-pytorch,2022-10-11 06:57:23,0,,10,1404103762,Are these pretrained models same as the original ones? or you trained it on own? Can you please tell about the stats as well thankyou
"Custom grayscale data, d_loss not decreasing.",nipponjo/deepfillv2-pytorch,2022-09-27 03:18:29,0,,9,1387013607,"Hi, I am training the deepfillv2 model with custom data, and have a few issues.

I am training with grayscale images(Modeled Ozone Image). The changes I have made are

- generator cin=3 
- applied custom mask creation which randomly removes parts of input pixels
- added a few more lines to plot input, mask and output during training instead of using tesorboard log images.

![image](https://user-images.githubusercontent.com/23354456/192423317-85fe037d-631c-403c-84b7-115453ef56b1.png)

I am using default cmap for the plots. 

While training the d_loss doesn't change at all.

INFO : @iter: 118000: 0.5280 it/s
 d_loss: 1.0000
g_loss: 0.0170
 ae_loss: 0.0080
ae_loss1: 0.0052
 ae_loss2: 0.0028

I have normalized the input 0-0.45 before sending it to the model. I used the specific normalization to compare with a PCNN model, as we have used same normalization factors for that.

 Even though its filling the mask, the output is not great.
Can you please give me some suggestions for as to what might be the issue and how I could improve the training? Thanks in advance"
"Why do I train my dataset, the output picture is just masked and not fixed, and 300,000 epochs are trained like this?",nipponjo/deepfillv2-pytorch,2022-09-01 07:54:32,1,,8,1358413374,"![iter_500](https://user-images.githubusercontent.com/91020859/187862127-de6907ee-7737-436e-a827-c28bec32ba48.png)
just like this？ "
training time and convergence,nipponjo/deepfillv2-pytorch,2022-06-12 17:58:12,6,,4,1268656711,"Thanks for the nice code! Could you given an estimate about the training time of the model on CeleBA-HQ and Places2 ? And how is the model's performances with a smaller dataset with, for instance, thousands of data?"
How to identity a face with occlution,yubangji123/Interpret_FR,2022-02-20 12:34:08,1,,6,1145007502,If i want to verify whether the two faces with occlusion belong to one identity，do I need any extra operation on their 320-D features？do I need to discard the features affected by the occlusion？If yes，how to do？
The zip of code cannot be downloaded.,yubangji123/Interpret_FR,2020-09-30 06:46:23,0,,5,711681235,
How to produce 68 coordinate points？,yubangji123/Interpret_FR,2020-03-27 10:29:23,1,,4,589034673,"I want to get IJB-A synthetic occluded faces, but I don't have landmarks68_IJBA.mat. Could you provide  landmarks68_IJBA.mat ? Or could you tell me the method to get the landmarks?
Thanks!"
How to reproduce the figure.1 in your paper,yubangji123/Interpret_FR,2020-03-11 04:01:13,1,,3,578994361,"Hi, 
I have read your paper and want to know how to reproduce the visualization of the interpretable feature maps in the fig.1 of your paper. Can you share any code examples that produce the feature maps given one face image?
Thanks!"
How to identity one person?,yubangji123/Interpret_FR,2019-11-20 02:14:00,1,,2,525377239,"I have 2 face of same person, if face one have feature shape is 320, face two have shape is 320. How to identity 2 face of same person? Use L1, L2 ? "
Broken links,yubangji123/Interpret_FR,2019-08-28 06:29:36,1,,1,486184332,"Almost all data download links are broken. This makes us difficult to reproduce the results and compare with your model.

![image](https://user-images.githubusercontent.com/17662095/63830931-f3df6e00-c99f-11e9-9d03-3e8413a9420c.png)
"
Can I train this model on RTX 3090 GPU,Justin-Tan/generative-compression,2022-08-09 08:28:35,1,,47,1332903165,"Hi Justin,

May I ask if I can train this model on RTX 3090 GPU? It seems that tensorflow1.8 doesn't work properly on RTX3090.

Sincerely,
Zihang"
"The quantizer sets quantization centers as [-2, -1, 0, 1,2], which makes no sense after ReLU. Update: this should work for low bitrate compression. Sorry. ",Justin-Tan/generative-compression,2021-09-29 19:06:34,0,,46,1011347597,Do you have Arithmetic codes for this one?
Performance issues in you project (by P3),Justin-Tan/generative-compression,2021-08-30 08:06:03,1,,45,982510594,"Hello! I've found a performance issue in your project: `batch()` should be called before `map()`, which could make your program more efficient. Here is [the tensorflow document](https://tensorflow.google.cn/guide/data_performance?hl=zh_cn#vectorized_mapping) to support it.

Detailed description is listed below:

- /data.py: `dataset.batch(batch_size)`[(here)](https://github.com/Justin-Tan/generative-compression/blob/38d34c8538360dc350f935da0fb1caa0b346688d/data.py#L79) should be called before `dataset.map(_parser)`[(here)](https://github.com/Justin-Tan/generative-compression/blob/38d34c8538360dc350f935da0fb1caa0b346688d/data.py#L77).
- /data.py: `dataset.batch(batch_size)`[(here)](https://github.com/Justin-Tan/generative-compression/blob/38d34c8538360dc350f935da0fb1caa0b346688d/data.py#L121) should be called before `dataset.map(_parser)`[(here)](https://github.com/Justin-Tan/generative-compression/blob/38d34c8538360dc350f935da0fb1caa0b346688d/data.py#L119).
- /data.py: `dataset.batch(batch_size)`[(here)](https://github.com/Justin-Tan/generative-compression/blob/38d34c8538360dc350f935da0fb1caa0b346688d/data.py#L145) should be called before `dataset.map(_preprocess_inference)`[(here)](https://github.com/Justin-Tan/generative-compression/blob/38d34c8538360dc350f935da0fb1caa0b346688d/data.py#L144).
- /cGAN/data.py: `dataset.batch(batch_size)`[(here)](https://github.com/Justin-Tan/generative-compression/blob/38d34c8538360dc350f935da0fb1caa0b346688d/cGAN/data.py#L78) should be called before `dataset.map(_parser)`[(here)](https://github.com/Justin-Tan/generative-compression/blob/38d34c8538360dc350f935da0fb1caa0b346688d/cGAN/data.py#L76).
- /cGAN/data.py: `dataset.batch(batch_size)`[(here)](https://github.com/Justin-Tan/generative-compression/blob/38d34c8538360dc350f935da0fb1caa0b346688d/cGAN/data.py#L120) should be called before `dataset.map(_parser)`[(here)](https://github.com/Justin-Tan/generative-compression/blob/38d34c8538360dc350f935da0fb1caa0b346688d/cGAN/data.py#L118).
- /cGAN/data.py: `dataset.batch(batch_size)`[(here)](https://github.com/Justin-Tan/generative-compression/blob/38d34c8538360dc350f935da0fb1caa0b346688d/cGAN/data.py#L144) should be called before `dataset.map(_preprocess_inference)`[(here)](https://github.com/Justin-Tan/generative-compression/blob/38d34c8538360dc350f935da0fb1caa0b346688d/cGAN/data.py#L143).

Besides, you need to check the function called in `map()`(e.g., `_preprocess_inference` called in `dataset.map(_preprocess_inference)`) whether to be affected or not to make the changed code work properly. For example, if `_preprocess_inference` needs data with shape (x, y, z) as its input before fix, it would require data with shape (batch_size, x, y, z).

Looking forward to your reply. Btw, I am very glad to create a PR to fix it if you are too busy."
Performance issues in data.py(P2),Justin-Tan/generative-compression,2021-08-22 09:12:51,0,,43,976327907,"Hello,I found a performance issue in the definition of `load_dataset` ,
data.py,
[dataset = dataset.map(_parser)](https://github.com/Justin-Tan/generative-compression/blob/38d34c8538360dc350f935da0fb1caa0b346688d/data.py#L77) was called without **num_parallel_calls**.
I think it will increase the efficiency of your program if you add this.

The same issues also exist in [dataset = dataset.map(_parser)](https://github.com/Justin-Tan/generative-compression/blob/38d34c8538360dc350f935da0fb1caa0b346688d/data.py#L119) ,
[dataset = dataset.map(_preprocess_inference)](https://github.com/Justin-Tan/generative-compression/blob/38d34c8538360dc350f935da0fb1caa0b346688d/data.py#L144),
[dataset = dataset.map(_parser)](https://github.com/Justin-Tan/generative-compression/blob/38d34c8538360dc350f935da0fb1caa0b346688d/cGAN/data.py#L76),
[dataset = dataset.map(_parser)](https://github.com/Justin-Tan/generative-compression/blob/38d34c8538360dc350f935da0fb1caa0b346688d/cGAN/data.py#L118) 
and  [dataset = dataset.map(_preprocess_inference)](https://github.com/Justin-Tan/generative-compression/blob/38d34c8538360dc350f935da0fb1caa0b346688d/cGAN/data.py#L143) 

Here is [the documemtation of tensorflow](https://tensorflow.google.cn/api_docs/python/tf/data/Dataset?hl=en#map) to support this thing.

Looking forward to your reply. Btw, I am very glad to create a PR to fix it if you are too busy."
Can't compress own file with pretrained model or self trained model,Justin-Tan/generative-compression,2021-05-20 10:01:16,0,,42,896578433,"Hi there,

first of all, thanks a lot for your reimplementation on the paper!

I wanted to see how it works on compressing an image of mine. I downloaded the pretrained model and wanted to use the compress.py script as mentioned in the read.me. This didn't work, so I saw a closed issue here, that for using the compression it needed the checkpoint file. So I trained the model to get such a checkpoint file. Now I tried to run the command with compress.py again:

`python compress.py -r ./checkpoints/best/my_network_epoch34.ckpt-34 -i ./pic.jpeg -o ./pic_out.jpeg`

I get the following error message: 

`Traceback (most recent call last):
  File ""compress.py"", line 85, in <module>
    main()
  File ""compress.py"", line 82, in main
    single_compress(config_test, args)
  File ""compress.py"", line 53, in single_compress
    new_saver = tf.train.import_meta_graph('{}.meta'.format(args.restore_path))
  File ""/Users/python_envs/gan/lib/python2.7/site-packages/tensorflow_core/python/training/saver.py"", line 1453, in import_meta_graph
    **kwargs)[0]
  File ""/Users/python_envs/gan/lib/python2.7/site-packages/tensorflow_core/python/training/saver.py"", line 1477, in _import_meta_graph_with_return_elements
    **kwargs))
  File ""/Users/python_envs/gan/lib/python2.7/site-packages/tensorflow_core/python/framework/meta_graph.py"", line 810, in import_scoped_meta_graph_with_return_elements
    return_elements=return_elements)
  File ""/Users/python_envs/gan/lib/python2.7/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/Users/python_envs/gan/lib/python2.7/site-packages/tensorflow_core/python/framework/importer.py"", line 405, in import_graph_def
    producer_op_list=producer_op_list)
  File ""/Users/python_envs/gan/lib/python2.7/site-packages/tensorflow_core/python/framework/importer.py"", line 505, in _import_graph_def_internal
    raise ValueError(str(e))
ValueError: Cannot add function '__inference_Dataset_map__parser_56' because a different function with the same name already exists.`

My venv:
Package                            Version
---------------------------------- -----------
absl-py                            0.12.0
appnope                            0.1.2
astor                              0.8.1
backports-abc                      0.5
backports.functools-lru-cache      1.6.4
backports.shutil-get-terminal-size 1.0.0
backports.weakref                  1.0.post1
bleach                             1.5.0
cachetools                         3.1.1
certifi                            2020.12.5
chardet                            4.0.0
cycler                             0.10.0
decorator                          4.4.2
enum34                             1.1.10
funcsigs                           1.0.2
functools32                        3.2.3.post2
futures                            3.3.0
gast                               0.2.2
google-auth                        1.30.0
google-auth-oauthlib               0.4.1
google-pasta                       0.2.0
grpcio                             1.37.1
h5py                               2.10.0
html5lib                           0.9999999
idna                               2.10
ipykernel                          4.10.1
ipython                            5.10.0
ipython-genutils                   0.2.0
jupyter-client                     5.3.5
jupyter-core                       4.6.3
Keras-Applications                 1.0.8
Keras-Preprocessing                1.1.2
kiwisolver                         1.1.0
Markdown                           3.1.1
matplotlib                         2.2.5
mock                               3.0.5
numexpr                            2.7.3
numpy                              1.16.6
oauthlib                           3.1.0
opt-einsum                         2.3.2
pandas                             0.24.2
pathlib2                           2.3.5
pexpect                            4.8.0
pickleshare                        0.7.5
pip                                20.3.4
prompt-toolkit                     1.0.18
protobuf                           3.17.0
ptyprocess                         0.7.0
pyasn1                             0.4.8
pyasn1-modules                     0.2.8
Pygments                           2.5.2
pyparsing                          2.4.7
python-dateutil                    2.8.1
pytz                               2021.1
pyzmq                              19.0.2
requests                           2.25.1
requests-oauthlib                  1.3.0
rsa                                4.5
scandir                            1.10.0
scipy                              1.2.2
seaborn                            0.9.1
setuptools                         44.1.1
simplegeneric                      0.8.1
singledispatch                     3.6.1
six                                1.16.0
subprocess32                       3.5.4
tables                             3.5.2
tensorboard                        1.15.0
tensorflow                         1.8.0
tensorflow-cpu                     1.15.0
tensorflow-cpu-estimator           1.15.1
tensorflow-estimator               2.1.0
termcolor                          1.1.0
tornado                            5.1.1
traitlets                          4.3.3
urllib3                            1.26.4
wcwidth                            0.2.5
Werkzeug                           1.0.1
wheel                              0.36.2
wrapt                              1.12.1


*********************************************************************************************************************
**I also tried to just add the checkpoint file to the checkpoints folder where the pretrained model is and got the following error:**


`Sampling noise...
('Real image shape:', [None, None, None, 3])
('Reconstruction shape:', [None, 512, 1024, 3])
2021-05-20 11:55:08.755659: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-05-20 11:55:08.790379: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f911c318720 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-05-20 11:55:08.790421: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-05-20 11:55:13.008342: W tensorflow/core/graph/graph_constructor.cc:1491] Importing a graph with a lower producer version 24 into an existing graph with producer version 134. Shape inference will have run different parts of the graph with different producer versions.
2021-05-20 11:55:25.044370: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key generator/encoder_image/batch_normalization/beta not found in checkpoint
Traceback (most recent call last):
  File ""compress.py"", line 85, in <module>
    main()
  File ""compress.py"", line 82, in main
    single_compress(config_test, args)
  File ""compress.py"", line 54, in single_compress
    new_saver.restore(sess, args.restore_path)
  File ""/Users/python_envs/gan/lib/python2.7/site-packages/tensorflow_core/python/training/saver.py"", line 1306, in restore
    err, ""a Variable name or other graph key that is missing"")
tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Key generator/encoder_image/batch_normalization/beta not found in checkpoint
	 [[node save/RestoreV2 (defined at /Users/python_envs/gan/lib/python2.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]

Original stack trace for u'save/RestoreV2':
  File ""compress.py"", line 85, in <module>
    main()
  File ""compress.py"", line 82, in main
    single_compress(config_test, args)
  File ""compress.py"", line 34, in single_compress
    saver = tf.train.Saver()
  File ""/Users/python_envs/gan/lib/python2.7/site-packages/tensorflow_core/python/training/saver.py"", line 828, in __init__
    self.build()
  File ""/Users/python_envs/gan/lib/python2.7/site-packages/tensorflow_core/python/training/saver.py"", line 840, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/Users/python_envs/gan/lib/python2.7/site-packages/tensorflow_core/python/training/saver.py"", line 878, in _build
    build_restore=build_restore)
  File ""/Users/python_envs/gan/lib/python2.7/site-packages/tensorflow_core/python/training/saver.py"", line 508, in _build_internal
    restore_sequentially, reshape)
  File ""/Users/python_envs/gan/lib/python2.7/site-packages/tensorflow_core/python/training/saver.py"", line 328, in _AddRestoreOps
    restore_sequentially)
  File ""/Users/python_envs/gan/lib/python2.7/site-packages/tensorflow_core/python/training/saver.py"", line 575, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/Users/python_envs/gan/lib/python2.7/site-packages/tensorflow_core/python/ops/gen_io_ops.py"", line 1696, in restore_v2
    name=name)
  File ""/Users/python_envs/gan/lib/python2.7/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 794, in _apply_op_helper
    op_def=op_def)
  File ""/Users/python_envs/gan/lib/python2.7/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/Users/python_envs/gan/lib/python2.7/site-packages/tensorflow_core/python/framework/ops.py"", line 3357, in create_op
    attrs, op_def, compute_device)
  File ""/Users/python_envs/gan/lib/python2.7/site-packages/tensorflow_core/python/framework/ops.py"", line 3426, in _create_op_internal
    op_def=op_def)
  File ""/Users/python_envs/gan/lib/python2.7/site-packages/tensorflow_core/python/framework/ops.py"", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()`

*********************************************************************************************************************
Any help would be highly appreciated!! Thanks a lot in advance!"
How to generate the required HDF5 file?,Justin-Tan/generative-compression,2021-05-20 01:20:17,2,,41,896156856,"This is my first use HDF5 dataset in neural network and I am clueless about how to generate the file with required format.
Could anyone offer a example code about it?"
tensorflow.python.framework.errors_impl.NotFoundError,Justin-Tan/generative-compression,2021-04-14 10:58:36,3,,40,857780913,tensorflow.python.framework.errors_impl.NotFoundError:  data/leftImg8bit/train/bochum/bochum_000000_023435_leftImg8bit.png; No such file or directory
training problem(resize 512X256),Justin-Tan/generative-compression,2020-10-28 13:19:38,0,,39,731434925,"When I changed the image size of the dataset to 512x256(out of Memory), and I encountered the following problem:

```Traceback (most recent call last):
  File ""/home/jathy/anaconda3/envs/generative-compression/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1322, in _do_call
    return fn(*args)
  File ""/home/jathy/anaconda3/envs/generative-compression/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/jathy/anaconda3/envs/generative-compression/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: _MklConcatOp : Dimensions of inputs should match: shape[0][1]= 16 vs. shape[1][1] = 32
         [[Node: generator/concat = _MklConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](generator/quantizer_image/Round, generator/noise_generator/conv_out/conv2d/BiasAdd, generator/quantizer_image/ArgMin/dimension, DMT/_164, generator/noise_generator/conv_out/conv2d/BiasAdd:2, DMT/_165)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train.py"", line 119, in <module>
    main()
  File ""train.py"", line 116, in main
    train(config_train, args)
  File ""train.py"", line 70, in train
    start_time, epoch, args.name, G_loss_best, D_loss_best)
  File ""/home/jathy/PycharmProjects/pythonProject/generative-compression/utils.py"", line 78, in run_diagnostics
    G_loss, D_loss, summary = sess.run([model.G_loss, model.D_loss, model.merge_op], feed_dict=feed_dict_test)
  File ""/home/jathy/anaconda3/envs/generative-compression/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/home/jathy/anaconda3/envs/generative-compression/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/jathy/anaconda3/envs/generative-compression/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/home/jathy/anaconda3/envs/generative-compression/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: _MklConcatOp : Dimensions of inputs should match: shape[0][1]= 16 vs. shape[1][1] = 32
         [[Node: generator/concat = _MklConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](generator/quantizer_image/Round, generator/noise_generator/conv_out/conv2d/BiasAdd, generator/quantizer_image/ArgMin/dimension, DMT/_164, generator/noise_generator/conv_out/conv2d/BiasAdd:2, DMT/_165)]]

Caused by op 'generator/concat', defined at:
  File ""train.py"", line 119, in <module>
    main()
  File ""train.py"", line 116, in main
    train(config_train, args)
  File ""train.py"", line 34, in train
    gan = Model(config, paths, name=args.name, dataset=args.dataset)
  File ""/home/jathy/PycharmProjects/pythonProject/generative-compression/model.py"", line 77, in __init__
    self.z = tf.concat([self.w_hat, Gv], axis=-1)
  File ""/home/jathy/anaconda3/envs/generative-compression/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 1189, in concat
    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)
  File ""/home/jathy/anaconda3/envs/generative-compression/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 953, in concat_v2
    ""ConcatV2"", values=values, axis=axis, name=name)
  File ""/home/jathy/anaconda3/envs/generative-compression/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/jathy/anaconda3/envs/generative-compression/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""/home/jathy/anaconda3/envs/generative-compression/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): _MklConcatOp : Dimensions of inputs should match: shape[0][1]= 16 vs. shape[1][1] = 32
         [[Node: generator/concat = _MklConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](generator/quantizer_image/Round, generator/noise_generator/conv_out/conv2d/BiasAdd, generator/quantizer_image/ArgMin/dimension, DMT/_164, generator/noise_generator/conv_out/conv2d/BiasAdd:2, DMT/_165)]]
```

Is there any part of the code that needs to be modified?"
16G RAM is not enough?,Justin-Tan/generative-compression,2020-05-07 09:35:39,5,,35,613913788,"as title, I use 16G RAM , monitor show my RAM using 100% memory,
Does anyone have the same problem? How many G RAM is enough?
Thanks~"
 Failed to create a directory,Justin-Tan/generative-compression,2019-11-22 03:27:29,1,,34,526965423,"
  File ""D:\Code\python\Machine Learning\GAN\generative-compression\model.py"", line 180, in __init__
    os.path.join(directories.tensorboard, '{}_train_{}'.format(name, time.strftime('%d-%m_%I:%M'))), graph=tf.get_default_graph())

**tensorflow.python.framework.errors_impl.InvalidArgumentError: Failed to create a directory: /tensorboard/gan-train_train_22-11_11:25; Invalid argument**
"
Loss term of entropy H(w_hat),Justin-Tan/generative-compression,2019-11-07 06:56:15,0,,33,519078741,"Hi, Justin-Tan, 

Thanks for your code!
After checking the code and the original paper, I didn't find the loss term of entropy H(w_hat) in your project.
So is there no entropy loss part in your project? I also wonder how to compute this term.
I hope to get your response. Thanks~
"
Questions about encoding,Justin-Tan/generative-compression,2019-07-30 10:13:08,6,,32,474497040,"Hello, while reading the paper, it proposed ""When encoding the channels of w^ to a bit-stream, we use an arithmetic encoder where frequencies are stored for each channel separately and then encode them in a static (non-adaptive) manner (i.e. context model and adaptive arithmetic encoding)."" 
But in your code, the quantized value w^ is concatenated with noise v to form the latent vector z, and then the generator G  tries to generate an image x^ = G(z). I did not find the step: encoding the channels of w^ to a bit-stream in context model and adaptive arithmetic encoding. Did you not conside this step？
Thank you very much."
Hardware environment,Justin-Tan/generative-compression,2019-07-09 10:39:22,2,,31,465705894,"Hi Justin,

May I ask for your hardware environment for running this programme?

Sincerely,
Yanfei"
The model file .mata I've trained 52 epoch on Citycapes can't be used in compress single image,Justin-Tan/generative-compression,2019-05-22 07:23:24,1,,30,446973117,"The model .mata I've trained 52 epoch on Citycapes can't be used in compress single image, and the storage of mata is 7823KB, which is not similar to noiseMScsC8_epoch15.ckpt-15.meta 24880KB. Did I trained in the wrong way? Lookingfoward you  response, thank you."
Error when compress a single image,Justin-Tan/generative-compression,2019-03-29 02:30:17,1,,29,426787640,"I was compress a single image according the ""Usage"" part , but got this error:
`Traceback (most recent call last):
  File ""/home/xx/PycharmProjects/generative-compression-master/compress.py"", line 79, in <module>
    main()
  File ""/home/xx/PycharmProjects/generative-compression-master/compress.py"", line 76, in main
    single_compress(config_test, args)
  File ""/home/xx/PycharmProjects/generative-compression-master/compress.py"", line 18, in single_compress
    ckpt = tf.train.get_checkpoint_state(directories.checkpoints, latest_filename='noiseMScsC8_epoch15.ckpt-15.data-00000-of-00001')
  File ""/home/xx/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/checkpoint_management.py"", line 273, in get_checkpoint_state
    coord_checkpoint_filename)
  File ""/home/xx/anaconda3/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 331, in read_file_to_string
    return f.read()
  File ""/home/xx/anaconda3/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 132, in read
    pywrap_tensorflow.ReadFromStream(self._read_buf, length, status))
  File ""/home/xx/anaconda3/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 100, in _prepare_value
    return compat.as_str_any(val)
  File ""/home/xx/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/compat.py"", line 107, in as_str_any
    return as_str(value)
  File ""/home/xx/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/compat.py"", line 80, in as_text
    return bytes_or_text.decode(encoding)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd5 in position 25: invalid continuation byte`

It sames the ckpt file has some problem. Did i miss something?

PS:
    My os encoding is utf-8. 
    This is my command: python compress.py -r ./checkpoints -i ./test_img -o ./output

Best regards."
Question about gradient calculation in the quantizer,Justin-Tan/generative-compression,2019-01-24 05:02:43,5,,27,402543922,"1. I found that you have used ""tf.stop_gradient()"" to deal with the nondifferentiable properties of ""tf.argmin()"", ""tf.round()"". However, ""tf.stop_gradient()"" is used to ignore the gradient contirbution of present node, which means that your encoder network will not update its parameters since all the nodes before the quantizer (specifically the encoder network) will be ignored in the gradient calculation.  

2. Are you tring to make the quantizer have a fixed gradient (such as ""1"") value at any time? If you are, I think you have to re-define the gradient of the quantizer rather than use ""tf.stop_gradient()"" ."
training problem,Justin-Tan/generative-compression,2018-11-13 13:15:25,1,,26,380220338,"Training on dataset ADE20k
Building computational graph ...
Training on ADE20k
Training on ADE20k
<------------ Building global image generator architecture ------------>
Sampling noise...
Traceback (most recent call last):
  File ""/home/tq/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1567, in _create_c_op
    c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimension 2 in both shapes must be equal, but are 32 and 64. Shapes are [?,?,32] and [?,32,64]. for 'generator/concat' (op: 'ConcatV2') with input shapes: [?,?,32,8], [?,32,64,8], [] and with computed input tensors: input[2] = <-1>.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train.py"", line 119, in <module>
    main()
  File ""train.py"", line 116, in main
    train(config_train, args)
  File ""train.py"", line 34, in train
    gan = Model(config, paths, name=args.name, dataset=args.dataset)
  File ""/home/tq/generative-compression-master/model.py"", line 77, in __init__
    self.z = tf.concat([self.w_hat, Gv], axis=-1)
  File ""/home/tq/.local/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py"", line 1189, in concat
    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)
  File ""/home/tq/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 953, in concat_v2
    ""ConcatV2"", values=values, axis=axis, name=name)
  File ""/home/tq/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/tq/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""/home/tq/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1734, in __init__
    control_input_ops)
  File ""/home/tq/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1570, in _create_c_op
    raise ValueError(str(e))
ValueError: Dimension 2 in both shapes must be equal, but are 32 and 64. Shapes are [?,?,32] and [?,32,64]. for 'generator/concat' (op: 'ConcatV2') with input shapes: [?,?,32,8], [?,32,64,8], [] and with computed input tensors: input[2] = <-1>.


what can i do ?"
how to generate checkpoint file for noise sampling model,Justin-Tan/generative-compression,2018-11-13 07:27:37,2,,24,380092493,"i have downloaded the noise sampling model but no checkpoint file found, could you tell how to generate one with detail? thanks"
How is the effect on the test set?,Justin-Tan/generative-compression,2018-10-18 08:40:03,3,,22,371420460,"Hi, author:
    When I re-train this model, it works pretty well on training dataset after 250 epoch. but it doesn't appear to work very well on the testing dataset. Is your result the same?

Thank you!
"
compress single image with error,Justin-Tan/generative-compression,2018-07-04 13:39:00,3,,16,338280388,"Traceback (most recent call last):
  File ""compress.py"", line 80, in <module>
    main()
  File ""compress.py"", line 77, in main
    single_compress(config_test, args)
  File ""compress.py"", line 60, in single_compress
    Utils.single_plot(0, 0, sess, gan, handle, save_path, config, single_compress=True)
  File ""/home/zqzhu/EX/GAN_CC/generative-compression-master/utils.py"", line 107, in single_plot
    r, g = sess.run([real, gen], feed_dict={model.training_phase:True, model.handle: handle})
  File ""/home/zqzhu/tensorflow3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/home/zqzhu/tensorflow3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/zqzhu/tensorflow3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/home/zqzhu/tensorflow3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected image (JPEG, PNG, or GIF), got unknown format starting with 'BM\212\000\014\000\000\000\000\000\212\000\000\000|\000'
	 [[Node: DecodePng = DecodePng[channels=3, dtype=DT_UINT8](ReadFile)]]
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,?,3]], output_types=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](IteratorFromStringHandle)]]
	 [[Node: generator/decoder/conv2d_transpose_3/conv2d_transpose-0-VecPermuteNHWCToNCHW-LayoutOptimizer/_395 = _HostRecv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_1470_...tOptimizer"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

The compressed image format is bmp, and then  occurre an error. "
upsample noise to concatenate with quantied representation,Justin-Tan/generative-compression,2018-06-15 07:10:38,6,,13,332668139,"Hi, while reading the paper, it proposed an optionally choice to concatenate the representaion with noise v. In your code, a dcgan_generator is used to generate noise v, while the output size is [*,32,64,32]. If the dataset is cityscapes, the input image is resized to 512*1024, the feature maps' size is 32*64*C( C=8 )，so I think the concatenated feature maps-z's size is [*,32,64,32+8]=[*,32,64,40], questions are:
1> why generate the noise by dcgan network?
2> if the input size changes, for example I train the ADE20k with the input size 512*512, then the noise'size cannot be concatenated to the quantized representation, so we need to change the dcgan network?
3> adding noise will increase the bpp by a large margin, as the its output size is too big.
These are my questions, looking forward to your reply.
Thanks for sharing your code.
generate the noise"
Relationship between channels  C  and bbp,Justin-Tan/generative-compression,2018-05-26 10:02:54,20,,5,326726424,"you said    **C=8 channels - (compression to 0.072 bbp)**  in Results section
I don't know the relationship between C and bbp,  can you explain to me?

We compare this image compression effect with BPG
png image -> decode -> our encode -> our quantize     Result:   **quantized representation**

the bbp comparation objects are quantized representation  and encoded BPG,
same bbp which decoded image quality is better or same decoded image quality which bbp is lower, right?"
Integrating selective generative compression,Justin-Tan/generative-compression,2018-05-13 16:45:08,0,help wanted,1,322609208,"I would like to start implementing selective compression so that the network can be instructed to compress/synthesize certain regions while preserving selected regions. (Section 4.2 of https://arxiv.org/pdf/1804.02958.pdf). This will require working with a semantic map of the original image, which is available for the Cityscapes/ ADE20k dataset. "
Request for videos,hangzhaomit/HACS-dataset,2022-04-24 03:36:38,1,,19,1213517632,"I'm in a region without direct access to Youtube,and unfortunately the remote server we're using doesn't have vpn. May I  send a request for the direct link for the whole video dataset ,which is split into multiple missing text ,following the submit instruction you have given?"
Request for missing videos,hangzhaomit/HACS-dataset,2022-04-21 02:20:19,1,,18,1210411951,"hi， thanks for your datasets.
I have submitted the form to request the downloading of the missing video, but there is currently no response.
Is it still available for downloading?"
"Hi, may I ask what annotation tool you are using?",hangzhaomit/HACS-dataset,2022-03-17 01:52:36,1,,17,1171828594,
Why is the actual length of some downloaded videos different from that in the annotation files？,hangzhaomit/HACS-dataset,2021-10-10 15:19:34,0,,16,1022012379,Why is the actual length of some downloaded videos different from that in the annotation files
Question on calculating C x C matrix,xu-ji/IIC,2022-07-11 02:47:36,0,,117,1300132943,"When calculating IIC, let's say there are n samples in the batch and the corresponding pairs. 
Is it correct that the sample of the ""same class"" is in one batch ?
Otherwise, what does it mean to divide by n when calculating CxC matrix P ? 


"
Giving memory error while running unsupervised clustering on Custom Data,xu-ji/IIC,2022-03-22 04:46:55,0,,116,1176261552,"RuntimeError: unsupported operation: more than one element of the written-to tensor refers to a single memory location. Please clone() the tensor before performing the operation.

Can you tell me, what's causing this issue"
How to use own dataset for fully unsupervised learning,xu-ji/IIC,2022-02-10 13:10:11,0,,115,1130053545,"First of all, thank you very much for your contribution. I have some doubts when I read the paper. The training data set used in the Auxiliary overclustering section of Section 3.2 includes two parts, one known to contain only relevant classes and the other known to contain irrelevant or distractor classes. Isn't there no label in unsupervised learning? Why do I know this information? Maybe I didn't understand it thoroughly. I hope you can give me some suggestions, thank you very much!!!"
Computing IID_loss and get a negative result.,xu-ji/IIC,2021-12-21 09:27:18,0,,114,1085617797,"Hello, I computed IID_loss, but the result is negative （approximate to zero). Is it right?"
Why 128x128 for coco-stuff in segmentation task?,xu-ji/IIC,2021-11-14 08:25:15,0,,113,1052880474,Why need to shrink and resize to 128x128 for coco-stuff input image in segmentation task? Can I get a segmentation map having the same size with the original input image without resizing?
Suggested Fork for Python 3 is still not ready ,xu-ji/IIC,2021-11-11 21:30:51,0,,112,1051377152,"Hi, 

Thank you for this amazing work. Just to let you know that the suggested fork still has some python 2 code in the clustering part. 

"
Render CoCo Segmentation Result,xu-ji/IIC,2021-11-04 08:16:11,0,,111,1044461441,"Hi, can you please tell me where I can find the original code for rendering CoCo Segmentation results? I only find the Potsdam-verion in code/scripts/segmentation/analysis/render_potsdam.py . Thank you!"
Loss becomes 0 and does not change again,xu-ji/IIC,2021-09-22 07:25:08,0,,110,1003931550,"Thanks for your great work firstly. I use your model in nlp tasks, but after training some batches, the loss becomes 0 and doesn't change anymore. Can you provide some ideas for this problem？Thank you in advance."
evaluated accuracy seems weird,xu-ji/IIC,2021-04-26 04:11:09,0,,108,867235292,"Hi, thanks for your great work. I had an issue that when I used my own data set to conduct fully unsupervised clustering. The outputted accuracy showed a random choice. For example, I have 3 classes and the numbers of these data are balanced. But the accuracies in 5 sub_heads were all around 0.333. Did I set the parameters wrong? Could you help me with this? Thanks a lot!"
Unable to setup environment for IIC:,xu-ji/IIC,2021-03-12 22:55:49,5,,106,830617404,"It'll be a great help if the conda environment.yml file (or any equivalent format) could be provided.

package_versions.txt file is not of any help. 

Thank you, "
Several Issues about unsupervised segmentation part.,xu-ji/IIC,2021-02-01 06:58:14,0,,103,798034010,"I am trying to reproduce your segmentation research recently. However, I have some problems.
Firstly, I noticed that in the paper, you mentioned lamb was set as 1.5 for COCO3 and PostDam3 and 1.0 for other experiments. However, when I checked the setting for COCO3, I notice the lamd_A = 1.0 and lamd_B = 1.5, and I am a little confused.
In your shared model, I also noticed that take 555 for example, the num_epoches = 4800, while the loss data seems to have 109 rows in total. Does this mean you record it every 4800/109 epochs?
Another question is that shouldn't the minimum of loss _no_lamd be -1 since it is -1*mutual_information? Why the loss drops to a little higher than -1.5? Is the ideal minimum loss affected by the number of clusters?"
Show both heads all data or use auxillary overclustering?,xu-ji/IIC,2021-01-25 23:30:55,0,,102,793802006,"In the case of STL10, it says that using overclustering with an extra 100000 images helps improve clustering of those initial 5000 images significantly.  
What would have been the trade off if you had ran the algorithm with all 105000 images and showed each head the same images.  Is there a better clustering achieved when you use auxillary head performing overclustering.
If I have 200,000 images for fully unsupervised clustering, would it be better to show both head A and head B all 200,000 images?  Or would it better to shown one head 20,000 and the other head 200,000 (with more clusters)?  
Furthermore, if I show both heads the same images, I might as well use one head right?

Thank you. "
Question about loss function during segmentation,xu-ji/IIC,2021-01-06 01:51:07,1,,98,779861366,"Hi Xu-Ji,

Currently, I have modified the Segmentation Script to train another dataset. Both the dataset loader B and A are the same but I noticed the avg loss returned (between head B and A) have large margin of difference:
![image](https://user-images.githubusercontent.com/26461142/103719706-8ff19e80-5004-11eb-96d5-40aacc4526fb.png)

Any idea why this happened? Am I training it correctly

Thank you and regards"
KeyError: 'ClusterNet6cTwoHead',xu-ji/IIC,2021-01-01 07:34:47,1,,97,777233382,"Hi, when I tried to run ""cluster part“,  the command I used is :
`python -m code.scripts.cluster.cluster_greyscale_twohead --model_ind 685 --arch ClusterNet6cTwoHead --mode IID --dataset MNIST --dataset_root /scratch/local/ssd/xuji/MNIST --gt_k 10 --output_k_A 50 --output_k_B 10  --lamb_A 1.0 --lamb_B 1.0 --lr 0.0001 --num_epochs 3200 --batch_sz 700 --num_dataloaders 5 --num_sub_heads 5 --crop_orig --crop_other --tf1_crop centre_half --tf2_crop random --tf1_crop_sz 20  --tf2_crop_szs 16 20 24 --input_sz 24 --rot_val 25 --no_flip --head_B_epochs 2 > out/sh10_gpu3_m685.out`,
However, the result suggests that there is a KeyError, can you help me with this problem, thanks a lot!
"
The code content is inconsistent with the paper.,csm9493/FC-AIDE,2021-11-04 01:17:14,0,,4,1044268116,"The network structure in core/models.py is inconsistent with the content in the paper. For example, the activation function PReLU, the number of times the sliding window is used."
Error when running test code,csm9493/FC-AIDE,2020-02-23 15:04:30,2,,2,569508548,"I ran the test_fc_aide_ft.py without any modification, but the following error occurs:
``
 tracking <tf.Variable 'naide__conv2d__q1_1/Variable:0' shape=(3, 3, 1, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__e1_1/Variable:0' shape=(3, 3, 1, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d_dow_n1_1/Variable:0' shape=(3, 3, 1, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__q2_1/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__e2_1/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d_dow_n2_1/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__q2_2/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__e2_2/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d_dow_n2_2/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__q2_3/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__e2_3/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d_dow_n2_3/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__q2_4/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__e2_4/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d_dow_n2_4/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__q2_5/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__e2_5/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d_dow_n2_5/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__q2_6/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__e2_6/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d_dow_n2_6/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__q2_7/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__e2_7/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d_dow_n2_7/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__q2_8/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__e2_8/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d_dow_n2_8/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__q2_9/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__e2_9/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d_dow_n2_9/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__q2_10/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__e2_10/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d_dow_n2_10/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__q2_11/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__e2_11/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d_dow_n2_11/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__q2_12/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__e2_12/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d_dow_n2_12/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__q2_13/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__e2_13/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d_dow_n2_13/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__q2_14/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__e2_14/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d_dow_n2_14/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__q2_15/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__e2_15/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d_dow_n2_15/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__q2_16/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__e2_16/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d_dow_n2_16/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__q2_17/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__e2_17/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d_dow_n2_17/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__q2_18/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__e2_18/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d_dow_n2_18/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__q2_19/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__e2_19/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d_dow_n2_19/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__q2_20/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__e2_20/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d_dow_n2_20/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__q2_21/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d__e2_21/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
tracking <tf.Variable 'naide__conv2d_dow_n2_21/Variable:0' shape=(3, 3, 64, 64) dtype=float32> mask
Traceback (most recent call last):
  File ""test_fc_aide_ft.py"", line 21, in <module>
    denoised_img, psnr, ssim = t_ft.fine_tuning()
  File ""/home/bob/FC-AIDE-Keras/core/test_ft.py"", line 128, in fine_tuning
    self.model.load_weights('./weights/' + 'sigma' + str(self.noise_sigma) + '.hdf5')
  File ""/home/bob/anaconda3/envs/bobsense/lib/python3.7/site-packages/keras/engine/saving.py"", line 492, in load_wrapper
    return load_function(*args, **kwargs)
  File ""/home/bob/anaconda3/envs/bobsense/lib/python3.7/site-packages/keras/engine/network.py"", line 1230, in load_weights
    f, self.layers, reshape=reshape)
  File ""/home/bob/anaconda3/envs/bobsense/lib/python3.7/site-packages/keras/engine/saving.py"", line 1235, in load_weights_from_hdf5_group
    ' elements.')
ValueError: Layer #0 (named ""naide__conv2d__q1_1"" in the current model) was found to correspond to layer naide__conv2d__q1_1 in the save file. However the new layer naide__conv2d__q1_1 expects 3 weights, but the saved weights have 2 elements.
``

Is there some problem with the pretrained model?

thanks!


"
which one is the training code? thanks~,csm9493/FC-AIDE,2019-08-27 08:48:14,2,,1,485673079,
Reproduce results on Tokyo247 dataset,yxgeee/openibl,2022-05-22 19:17:23,1,,34,1244342900,"Hi Dr. Ge,
Thanks for sharing this nice work!
I have successfully reproduced the reported results on Pittsbugh250k. However, on Tokyo247, I got slightly worse results than reported. Here is what we reproduced:
Recall@1 84.1
Recall@5 91.1
Recall@10 92.4
Since we got exactly the same results on Pittsburgh, there should be no problem with the checkpoint loading.  I guess the tokyo247 dataset is wrongly installed in my case. I notice that there are three versions of tokyo247 queries, so I want to check with you whether your results are based on  queries_v3 or v2.
Besides, I notice that tokyo247 query images are preprocessed with different resizing transformation, may I know the reason of it?
"
about modify d=25 meters,yxgeee/openibl,2022-03-21 05:03:44,2,,33,1174893386,"Thank you for your work. If I want to change the threshold of distance, what should I modify in the code"
Test on RParis and ROxford dataset,yxgeee/openibl,2022-02-16 07:07:12,0,,32,1139620502,"Thank you for open-sourcing the code and the detailed documentation. I want to know whether you evaluate the model on Rparis and ROxford datasets. I have tried to evaluate it using the image processing configuration in your document [https://github.com/yxgeee/OpenIBL#:~:text=Start%20without%20Installation-,Extract%20descriptor%20for%20a%20single%20image,-import%20torch%0Afrom] but got  poor performance on mAP score (ROxf:Medium 42.57, ROxford:Hard 19.25, RPar:Medium 44.8 RPar:Hard 20.27). Do you have any idea about it? I think it should not be like this. Something may be wrong in image processing  but not related to the model itself. (I use your model_best.pth.tar and the corresponding pca parameters). 
I am looking forward to your reply.
"
Reproducing SARE results,yxgeee/openibl,2022-01-31 15:57:11,0,,31,1119613602,"Thank you for releasing the code. When reproducing SARE results, I am able to reproduce the results in your paper when I use the dot product based code, but not the original Euclidean distance. Are the results in your work based on the code for the dot product? 

For sare_joint the difference between the Euclidean distance and the dot product is about 15% for R@1, for both Pitts250k and Tokyo 24/7, which seems very high to me. Did you experience this as well?"
How to visualize the feature map like Fig.5 in paper?,yxgeee/openibl,2021-11-24 11:46:23,0,,30,1062327526,"Hello, thanks to your amazing work! I want to visualize the feature map like Fig.5 in paper, but I don't know how to do it, could you help me please?
Looking forward to your reply!"
Extract descriptor of single image using models trained on custom datasets,yxgeee/openibl,2021-11-01 10:24:03,0,,29,1041014819,"Hello, thanks a lot for this valuable work. I tried your extract.py for image trieval during visual localization, it's great. But I want to train your SFRS on my own  datasets and use the best model to extract descriptors of images. I tried to load pretrained models in model zoo, but it seems some parameters of base model and netvlad are missing. Only vgg16_netvlad.pth can be used for extraction, can you help me with this. How can I produce a new vgg16_netvlad.pth after training. I would be very grateful if you could answer my question."
reproduction problem,yxgeee/openibl,2021-08-21 01:44:41,0,,28,976009981,"nice work, could you please upload the pitts and tokyo dataset to a google drive? I find nowhere to get them"
reproduction problem,yxgeee/openibl,2021-08-19 01:11:20,9,,27,974173459,"I just run the code without any change and find the initial recall scores is as follow:
Recall Scores:
  top-1           1.2%
  top-5           4.6%
  top-10          8.7%
then I continue the training process util the generation1 epoch2 with the recall score:
Recall Scores:
  top-1           1.4%
  top-5           6.3%
  top-10         11.9%
* Finished generation   1 epoch   2 recall@1:  1.4%  recall@5:  6.3%  recall@10: 11.9%  best@5:  7.7%
it seems no obvious promotion, is that right?"
About Normalize in get_transformer_train and get_transformer_test,yxgeee/openibl,2021-07-14 13:23:21,1,,26,944425483,"Thanks for your work!
I saw you are using 
T.Normalize(mean=[0.48501960784313836, 0.4579568627450961, 0.4076039215686255],
                        std=[0.00392156862745098, 0.00392156862745098, 0.00392156862745098]) in get_transformer_train and get_transformer_test, different from the usually used:
T.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225]), which is also used in https://github.com/Nanne/pytorch-NetVlad.
(1) Is it due to that you are using the pretrained model vgg16_pitts_64_desc_cen.hdf5 which is matched to the former standard deviation while the pretrained VGG model in torchvision is using the later standard deviation?
And respectively, the learing rate you are using is 0.001, while Nanne/pytorch-NetVlad is using 0.0001, is it due to this?
(2) I'm trying to reproduce the SARE-joint result under the framework of Nanne/pytorch-NetVlad. I added the loss function written on my own and I'm using pretrained model vgg16_pitts_64_desc_cen.hdf5. The learning rate is still 0.0001 as Nanne/pytorch-NetVlad did. The standard deviation fallows Nanne/pytorch-NetVlad.
But I cannot achieve 89% Recall1 result on Pitts250k-test. I only have 3 GPUs to use so I have to set batch size to (3) I didn't add T.ColorJitter(0.7, 0.7, 0.7, 0.5) in get_transformer_train as Nanne/pytorch-NetVlad did. 
Is it due to the batch size or T.ColorJitter or I should just use the pretrained VGG model in torchvision with T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])? Or would you give me some other suggestions? 
Really thanks for your help!"
pitts 250k    top-1          88.2%,yxgeee/openibl,2021-07-06 06:07:50,1,,25,937550165,"Thansk for your work. I use your model to test  data  pitts 250k 。


  top-1          88.2%
  top-5          95.4%
  top-10         96.7%
Inconsistent with the paper。"
missing keys in state_dict,yxgeee/openibl,2021-07-05 10:14:13,1,,24,936931960,"Thansk for your work. I use your model.
missing keys in state_dict: {'base_model.base.28.bias', 'net_vlad.conv.weight', 'base_model.base.19.bias', 'base_model.base.28.weight', 'base_model.base.24.bias', 'base_model.base.26.weight', 'base_model.base.5.weight', 'base_model.base.7.weight', 'base_model.base.17.bias', 'base_model.base.12.weight', 'net_vlad.centroids', 'base_model.base.7.bias', 'base_model.base.0.bias', 'base_model.base.14.weight', 'base_model.base.5.bias', 'base_model.base.21.bias', 'base_model.base.19.weight', 

"
models for torch.hub,yxgeee/openibl,2021-04-11 09:15:38,0,enhancement,22,855258838,"Hi, sorry to disturb you. When I study about Quick Start without Installation, I find that you only uploaded the SFRS model for torch.hub, can you upload SARE's and NetVLAD's? Thanks a lot."
Performances on Oxford and Paris,yxgeee/openibl,2021-04-04 04:31:06,6,,21,849795522,"Hello,
I tried your pretrained model on cnnimageretrieval-pytorch test script, and I got 
mAP : 67.90 for oxford (73.9 on your paper)
mAP : 76.64 for paris (82.5 on your paper)

Am i missing something ? (both with and without PCA gave similar performances)"
how the training set is set up ,yxgeee/openibl,2021-03-19 09:59:55,1,,18,835780793,"hello, can I know how the training set is set up? The Tokyo24/7  I downloaded is in the form of a picture plus a csv file, but in your code it's  a mat file. Is this mat file generated by yourself, and what format is it in?"
About negative samples,yxgeee/openibl,2021-03-13 13:55:14,3,,17,830904247,"Hi, I have some doubts aoubt the line 84 of ibl/utils/data/dataset.py
self.train_neg = [self.train_neg[idx] for idx in select]
I don't know the effect of this line, maybe it can't help to produce the negative samples outside 25m.
Maybe it should be deleted?"
Unable to train,yxgeee/openibl,2021-01-28 15:50:03,1,,13,796127498,"I tried a lot to train but for some reason I just cant train it with custom dataset. It throws the following error, :(

`RuntimeError: There were no tensor arguments to this function (e.g., you passed an empty list of Tensors), but no fallback function is registered for schema aten::_cat.  This usually means that this function requires a non-empty list of Tensors.  Available functions are [CPU, CUDA, QuantizedCPU, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].
`

Any ways to fix it?"
How to visualize the feature map,yxgeee/openibl,2021-01-03 11:57:23,1,,12,777623762,"Hi, thanks for your excellent work!!
I have a question about how do you visualize the feature map(before VLAD aggregation)? Is it done by simply adding the channels? Then get a 30x40 heatmap"
about the dataset,yxgeee/openibl,2020-12-17 08:10:13,1,,10,769670816,"Thansk for your work, i have some problems about the dataset. I have emailed to the netvlad author but can not get reply. So, could you share the dataset link of Tokyo 24/7, Pitts250k and Pitts30k. I just use it in my research."
"the meaning of labels,and why  is the labels not involved in the loss calculation？",StanfordASL/Trajectron,2022-07-03 12:31:00,0,,13,1292294759,"Hello, I am a little confused now, that is, what participates in the loss calculation is the prediction_horizon of the input trajectory and the trajectory predicted by the model, so what does labels mean?  I really appreciate your answer."
Why the autocorrelation matrix is an estimate of the inverse of the covariance matrix of the corner position?,pengsongyou/CalibrationWizard,2022-02-15 14:07:01,0,,6,1138742168,"Hi @pengsongyou , amazing work :) Thanks for open-sourcing it!

If I may, I have a question regarding the following paragraph of the paper:

> Consider a corner point extracted in an image; the uncertainty of its position can be estimated by computing the autocorrelation matrix C for a window of a given size around the point (see for instance [[5](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.434.4816&rep=rep1&type=pdf)]). 
**Concretely, C is an estimate of the inverse of the covariance matrix of the corner position**.

I am struggling to understand this. Given that the autocorrelation matrix can be computed as explained e.g. in this [tutorial](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.482.1724&rep=rep1&type=pdf)).  I don't see where this result of being the inverse covariance matrix comes from.

I would highly appreciate if you could shed some light on this,
Thanks in advance!"
How to do Synthetic evaluation?,pengsongyou/CalibrationWizard,2021-01-28 08:40:53,0,,4,795793327,"In the 'Result and Evaluation' section of the paper, it said:  
```
 To assess the proposed system, we simulate the process of camera calibration   
with pre-defined intrinsic parameters, with Matlab.  
```

But how to simulate the process of camera calibration in Matlab? Which Toolbox do you use?  

Thanks.  "
How to use pretrained HICO object detection model,BigRedT/no_frills_hoi_det,2021-09-06 13:17:53,0,,16,989166672,I want to detect actions on my customized dataset using pretrained model on HICO dataset which scripts do I use and what specific setting I need to do as I don't want to download features/dataset/json files etc that is provided in this repository
Can I run the project on V-COCO?,BigRedT/no_frills_hoi_det,2020-04-08 08:30:28,2,,14,596403116,"I want to compare the performance on V-COCO dataset, I read the code and find it basically wrote for HICO-DET, so can I adjust it to V-COCO by modifying several parts of it?"
Big chunks of data,BigRedT/no_frills_hoi_det,2020-03-03 10:58:29,1,,13,574584851,"Hi! 

Thanks a lot for the very clear and understandable code and repository. One small comment I can add is maybe this repo can benefit from a warning that extracted data size is huge: Currently it occupies around 700G in my disk. I am not sure if this is the expected behavior? "
How to make the confusion matrix?,BigRedT/no_frills_hoi_det,2019-11-20 13:00:15,0,,11,525788767,"Hi,

I came across some problems on Figure3 in your paper. I don't know how you collected the ground truth and corresponding interaction. Could you give me some more details on how to construct the confusion matrix? If possible, could you mind sending me the related code?  

Thanks a lot. Look forward to hearing from you.

Best regards."
feature_refine_module,NovasMax/R3Det-Refined-Single-Stage-Detector-with-Feature-Refinement-for-RO,2021-02-06 15:50:57,0,,1,802729202,"  I noticed a function:  ''**feature_refine_cuda.forward**(features, best_rbboxes)'' in FR stage to refine feature,but I can't see the details.  Can you point out where the specific code is, or explain how some features are refined ？
  looking for your reply,thank you!"
作者您好，我在使用d_adapt.py 程序时出现了一些bug。,thuml/Transfer-Learning-Library,2022-10-28 12:00:52,3,,175,1427157970,"
Traceback (most recent call last):
  File ""d_adapt.py"", line 348, in <module>
    args=(args, args_cls, args_box),
  File ""/home/shishijie/anaconda3/envs/detectron-na/lib/python3.6/site-packages/detectron2/engine/launch.py"", line 82, in launch
    main_func(*args)
  File ""d_adapt.py"", line 277, in main
    train(model, logger, cfg, args, args_cls, args_box)
  File ""d_adapt.py"", line 163, in train
    bbox_adaptor.fit(data_loader_source, data_loader_target, data_loader_validation)
  File ""/home/shishijie/shishijie_projects/domain_adapatation/Transfer-Learning-Library-master/examples/domain_adaptation/object_detection/d_adapt/bbox_adaptation.py"", line 354, in fit
    x_s, labels_s = next(iter_source)
  File ""/home/shishijie/shishijie_projects/domain_adapatation/Transfer-Learning-Library-master/examples/domain_adaptation/object_detection/d_adapt/tllib/utils/data.py"", line 55, in __next__
    data = next(self.iter)
  File ""/home/shishijie/anaconda3/envs/detectron-na/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 521, in __next__
    data = self._next_data()
  File ""/home/shishijie/anaconda3/envs/detectron-na/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 1176, in _next_data
    raise StopIteration
StopIteration


我调了一天，我目前的看法是：a_adapt程序有四个训练阶段：源域预训练，类别适应，边界框适应，⽬标域伪标签训练。但是现在源域预训练没问题，我用源域的训练权重作为后续训练的预训练权重，10个epcho的类别适应训练也没问题，边界框适应训练出现了问题。

examples/domain_adaptation/object_detection/d_adapt/d_adapt.py 文件调用的bbox_adaptor.fit(data_loader_source, data_loader_target, data_loader_validation) 函数出现了问题，examples/domain_adaptation/object_detection/d_adapt/bbox_adaptation.py 文件中的 iter_source 拿不到标签数据，

（ps：自己的数据集和官方的voc2007和clipart数据集都是这样，我的训练命令：CUDA_VISIBLE_DEVICES=0  python d_adapt.py --config-file  config/retinanet_R_101_FPN_voc.yaml  -s VOC2007  ../datasets/VOC2007  -t Clipart ../datasets/clipart  --test Clipart ../datasets/clipart  --finetune  --bbox-refine   OUTPUT_DIR  logs/retinanet_R_101_FPN_voc/voc2clipart/phase2
）

劳烦作者费心给我这个废物一些指导

"
Code for Mean Embedding Test,thuml/Transfer-Learning-Library,2022-10-23 13:52:56,1,enhancement,174,1419802310,"Hi there,

In the paper ""Transferable Representation Learning with Deep Adaptation Networks"",

There are 2 Two-Sample Test Statistics, MK-MMD and Mean Embedding Test (ME), I can find the code for MK-MMD but can't find any clue for ME, is there any code for ME or will the code for ME be avaliable?"
ADDA Architecture,thuml/Transfer-Learning-Library,2022-10-17 15:58:46,1,question,172,1411830013,"Hi,

I am trying to modify ADDA from your code. In the original paper, there seems to be 2 CNNs in the second training phase, one to process the examples of the target set and another with the frozen weights to process the examples of the source set. In this way, the CNN of the target set is 'adjusted' to that of the source set depending on the output of the domain discriminator.

However, when reviewing the code it seems that only a single CNN is used, adjusting in a first phase the model (extractor+classifier) and in a second phase only the domain discriminator. My questions is how then the training of both phases is reproduced in the library. Thank you very much!


Best regards,



Eva"
目标域数据json文件的真值,thuml/Transfer-Learning-Library,2022-10-17 10:24:47,7,question,171,1411319393,想请问一下作者在用目标域数据训练时，生成的目标域数据proposal文件中含有真值信息，然而域自适应是在没有目标域真值的情况下使用的方法，那么请问当不提供目标域数据的真值时，还有可以用目标域数据训练吗
Learning rate,thuml/Transfer-Learning-Library,2022-09-28 07:58:28,1,question,169,1388905388,"HI，

Can the learning rate of discriminator be updated? Because in the function class DomainDiscriminator(nn.Sequential): **lr=1.**Does the learning rate(1.0) can update follow the epoch？

def get_parameters(self) -> List[Dict]:
    return [{""params"": self.parameters(), **""lr"": 1.**}]

Such as we adopt the following learning rate updating method：
lr_scheduler_ad = LambdaLR(
    ad_optimizer, lambda x: args.lr * (1. + args.lr_gamma * float(x)) ** (-args.lr_decay))



Because we found in the experiment that the discrimination loss remained stable when the epoch was very small, although the task loss was still decreasing.

Thank you"
在MCD code内，训练代码，数据载入好像有点Bug,thuml/Transfer-Learning-Library,2022-09-26 02:29:22,3,question,168,1385317202,"**Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
 - OS: [e.g. iOS]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Smartphone (please complete the following information):**
 - Device: [e.g. iPhone6]
 - OS: [e.g. iOS8.1]
 - Browser [e.g. stock browser, safari]
 - Version [e.g. 22]

**Additional context**
Add any other context about the problem here.
"
是否需要加入训练多核对应的权重的过程？,thuml/Transfer-Learning-Library,2022-09-22 17:34:25,1,question,167,1382791693,"我在阅读DAN时，注意到原文是一个minimax过程，在学习网络参数（\Theta）外，还需要学习多核权重（\beta）；而后者似乎未在此代码文件中体现，5个核被分配了相同的权重；我写了一个二次规划求核权重的函数，供参考讨论  :-)

`
def train_weights(train_source_iter: DataLoader, train_traget_iter: DataLoader,
                  model: ImageClassifier,  kernels: list):
    print(""=>Train Beta(weights) of the kernels"")
    num_kernels = len(kernels)
    #协方差矩阵（原文中的矩阵Q）
    covar_matrix = torch.zeros((num_kernels, num_kernels))

    for i in range(num_kernels):
        for j in range(i, num_kernels):
            kernel1 = kernels[i]
            kernel2 = kernels[j]
            count = 0
            #线性时间估计Cov（u，u'）
            for x_s, x_t in zip(train_source_iter, train_traget_iter):
                x_s = model(x_s[0])[1]
                x_t = model(x_t[0])[1]

                idx = 0
                res = []
 
                while idx < len(x_s) // 4:
                    t1 = (kernel1(x_s[4 * idx : 4 * idx + 2]) + kernel1(x_t[4 * idx : 4 * idx + 2]) - kernel1(
                        torch.cat((x_s[4 * idx].unsqueeze(dim=0), x_t[4 * idx + 1].unsqueeze(dim=0)))) \
                         - kernel1(torch.cat((x_s[4 * idx + 1].unsqueeze(dim=0), x_t[4 * idx].unsqueeze(dim=0)))))[0][1]
                    t2 = (kernel1(x_s[4 * idx+2 : 4 * idx + 4]) + kernel1(x_t[4 * idx+2 : 4 * idx + 4]) - kernel1(
                        torch.cat((x_s[4 * idx+2].unsqueeze(dim=0), x_t[4 * idx + 3].unsqueeze(dim=0)))) \
                          - kernel1(torch.cat((x_s[4 * idx + 3].unsqueeze(dim=0), x_t[4 * idx+2].unsqueeze(dim=0)))))[0][1]
                    t3 = (kernel2(x_s[4 * idx: 4 * idx + 2]) + kernel2(x_t[4 * idx: 4 * idx + 2]) - kernel2(
                        torch.cat((x_s[4 * idx].unsqueeze(dim=0), x_t[4 * idx + 1].unsqueeze(dim=0)))) \
                          - kernel2(torch.cat((x_s[4 * idx + 1].unsqueeze(dim=0), x_t[4 * idx].unsqueeze(dim=0)))))[0][1]
                    t4 = (kernel2(x_s[4 * idx + 2: 4 * idx + 4]) + kernel2(x_t[4 * idx + 2: 4 * idx + 4]) - kernel2(
                        torch.cat((x_s[4 * idx + 2].unsqueeze(dim=0), x_t[4 * idx + 3].unsqueeze(dim=0)))) \
                          - kernel2(torch.cat((x_s[4 * idx + 3].unsqueeze(dim=0), x_t[4 * idx + 2].unsqueeze(dim=0)))))[
                        0][1]

                    res.append((t1 - t2) * (t3 - t4))
                    idx += 1
                res = sum(res) / len(res)
                covar_matrix[i][j] = covar_matrix[i][j] * count / (count + 1) + res / (count + 1)
                count += 1
            covar_matrix[j][i] = covar_matrix[i][j]

    epsilon = .001
    reg = torch.eye(num_kernels) * epsilon
    covar_matrix = covar_matrix + reg
    #使用cvxopt包求解二次规划问题（from cvxopt import matrix, solvers）
    P = covar_matrix.t().numpy().tolist()
    q = [0. for i in range(num_kernels)]
    G = (torch.eye(num_kernels) * -1).t().numpy().tolist()
    h = [0. for i in range(num_kernels)]
    A = [[1.] for i in range(num_kernels)]
    P = matrix(P)
    q = matrix(q)
    G = matrix(G)
    h = matrix(h)
    A = matrix(A)
    b = matrix([1.0])
    result = solvers.qp(P, q, G, h, A, b)
    print('The qp result')
    print(result['x'])
    return list(result['x'])
`"
Some questions about the data load.,nschor/CompoNet,2022-06-07 08:53:55,0,,3,1262957893,"it seems that the 'test' and 'trainval' of trian.py is not fit the data loader.

###
train.py
```
PCN_TRAIN_DATASET, PCN_TEST_DATASET, AE_TRAIN_DATASET, AE_TEST_DATASET, NUM_PARTS = data_utils.load_data(DATA_PATH,
                                                                                                         NUM_POINTS,
                                                                                                         CATEGORY,
                                                                                                         'test',
                                                                                                         'trainval')
```

###
data_utils.py
```
def load_data(data_path, num_point, category, seen_split, unseen_split):
    pcn_train_dataset, pcn_test_dataset, num_parts = load_pcn_data(data_path, num_point, category, seen_split,
                                                                   unseen_split)
    ae_train_dataset, ae_test_dataset = load_aes_data(data_path, num_point, category, seen_split, unseen_split,
                                                      num_parts)

    return pcn_train_dataset, pcn_test_dataset, ae_train_dataset, ae_test_dataset, num_parts


def load_pcn_data(data_path, num_point, category, seen_split, unseen_split):
    pcn_train_dataset = part_dataset_pcn.PartDatasetPCN(root=data_path, npoints=num_point, class_choice=category,
                                                        split=seen_split)

    pcn_test_dataset = part_dataset_pcn.PartDatasetPCN(root=data_path, npoints=num_point, class_choice=category,
                                                       split=unseen_split)
    num_parts = pcn_train_dataset.get_number_of_parts()

    return pcn_train_dataset, pcn_test_dataset, num_parts
```"
Requesting scripts for data generation,xumingze0308/TRN.pytorch,2021-08-11 20:41:25,0,,18,967424971,"Hi,

Thank you for making the code available for TRN. I am having a very hard time reproducing the results using features pretrained on Kinetics. Also I am noticing huge variation in performance based on minor changes in the data generation code? Could you please share the script you used to generate the training and test split for training TRN? Without the actual code to generate data, there are inconsistencies in the experiments and this hurts in making any progress. Please help!

Thanks
Gaurav"
How to get .npy file of target?,xumingze0308/TRN.pytorch,2021-01-02 03:23:39,0,,12,777395525,"For THUMOS'14 dataset,  when I get chunk-level feature vectors(e.g. Lx2048) for each video,  how could I get chunk-level labels for each video?

Groundtruth of BasketballDunk_test for THUMOS'14 dataset:
> video_test_0000179  7.8 9.0
> video_test_0000179  10.5 12.3
> video_test_0000179  17.3 18.7
> video_test_0000179  20.2 22.9

Cause groundtruth of raw video is the category of action and the interval of action, should I get the frames'labels from each interval (seconds) firstly, then get the union of 6 frames' labels as a chunk's label and combine all chunk-level labels for each video?"
Asking for test features,xumingze0308/TRN.pytorch,2020-09-27 21:32:06,0,,11,709820223,"Hi Ming, thank you for releasing the code.

May I request you to share features of test set or at least frame-level annotations of test set?
Since OAD task does not have a uniform annotation for test, it is difficult to make a fair comparison.
It would be very helpful if you share the annotations for fair comparison.

Thank you.
"
Offer Features,xumingze0308/TRN.pytorch,2020-07-17 13:00:03,0,,10,659237414,Could you offer the dataset features because I lack the GPU source？Thanks a lot!
Unable to replicated training results,xumingze0308/TRN.pytorch,2020-06-01 05:42:40,0,,9,628169007,"We have been attempting to train the TRN model on the THUMOS'15 dataset, and have used resnet and inception to extract the features from the RGB and optical flow images respectively. The optical flow was computed using Farneback algorithm on OpenCV. 

We were not training using the full dataset, only a subset of 120 videos, with a batch size of 8. However, we are unable to get any reasonable results, with the mAP score for any given class not exceeding 8%, after over 60 epochs. Would you have any insights as to why this is?

Additional note:- We also tried extracting features using different models, but attained the same results. "
on the point cloud data range,YoungXIAO13/PoseFromShape,2022-05-08 22:41:04,0,,30,1229006346,"Hi congrats on the nice work! I have a small question on the point cloud data of objectnet3d and pascal3d datasets. It seems that the data value ranges in [0,1], so it is not standard processed like modelnet or shapenet dataset where point cloud samples are scaled to a unit ball?"
Classifier Issues,ningyu1991/GANFingerprints,2021-11-03 02:29:24,1,,5,1043012985,"Hello,

I am testing your code on generated images from the pre-trained models you have provided. There are quite a few issues encountered while attempted to run the classifier on this dataset we have creating (testing portion of your code). 
- The environment had to be build as CUDA 10.0, Python3.7, and tensorflow-gpu 1.13.1, as previous versions were incompatible
- once run with the pk pre-trained model provided, in file GANFingerprints/classifier/util_scripts.py, line 23 `C_im = misc.load_network_pkl(model_path)` cannot be used as is anymore when executing line 57 `logits = C_im.run(im, minibatch_size=1, num_gpus=1, out_dtype=np.float32)` as this ends up being a tuple. I ended up changing line 23 to `G, D, Gs = misc.load_network_pkl(model_path)` and using `logits = D.run(im, minibatch_size=1, num_gpus=1, out_dtype=np.float32)` to run the classifier.
- upon running the classifier, assignment of labels gets skipped over. This would mean it does not match the labels at the top. What could we use as labels if logits.shape[1] = 1?
- If I use labels as list(labels_1) or list(labels_2) , it classifies everything as from CelebA, which is incorrect as the dataset I am passing in is all fake images generated from sngan model.

Please let me know how I can fix this.
Thanks"
Data,AlonShoshan10/dynamic_net,2021-06-02 13:17:27,0,,5,909492875,"Hi,
I find your article super cool and really enjoyed reading it!

I am willing to add a second set of tuning blocks in order to check if the interpolation is still working, in order to do that I need to train it, can you please send the data that you've trained your nets on?

Regards,
Noam"
Error while trying to run inference.py,AlonShoshan10/dynamic_net,2020-02-19 02:52:53,1,,4,567281349,"Hi, I've been having lots of trouble to get this program to work and I keep getting many errors, is there any chance you could provide a more detailed explanation of how to get the program running and what versions of python and dependencies are being used as I think that's what's causing lots of the issues. 

`(image) D:\dynamic_net-master\dynamic_style_transfer>python inference.py --network_name=mosaic --use_saved_config=False --set_net_version=normal
Traceback (most recent call last):
  File ""inference.py"", line 2, in <module>
    from models.inference_model import InferenceModel
  File ""D:\dynamic_net-master\dynamic_style_transfer\models\inference_model.py"", line 1, in <module>
    from models.base_model import BaseModel
  File ""D:\dynamic_net-master\dynamic_style_transfer\models\base_model.py"", line 1, in <module>
    import torch
  File ""D:\anaconda\envs\image\lib\site-packages\torch\__init__.py"", line 81, in <module>
    from torch._C import *
ImportError: DLL load failed: The operating system cannot run %1.`

This is one of the error codes I have got"
Demo - OpenCV Error (Windows),AlonShoshan10/dynamic_net,2019-10-24 01:43:23,1,,2,511655026,"Hi, when I run the demo the main GUI comes up and when I try to select 'take photo' button the photo window can't be closed and the input image is never updated. When I force close the progrm I see the following error:

> File ""[MyFileTree]\dynamic_net\dynamic_style_transfer\gui\webcam_style_transfer_widget.py"", line 23, in on_take_photo_click    
>     self.input_image = self.webcam.take_photo()
>   File ""[MyFileTree]\dynamic_net\dynamic_style_transfer\gui\webcam\webcam.py"", line 17, in take_photo
>     cv2.imshow(winname, image)
> cv2.error: OpenCV(3.4.7) C:\projects\opencv-python\opencv\modules\highgui\src\window.cpp:358: 
> error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'

Which version of OpenCV are you using and what should be the correct result of pressing 'take photo'?"
Natural Leakage,uvavision/Balanced-Datasets-Are-Not-Enough,2022-04-02 06:47:24,0,,6,1190527666,"I'm trying to replicate your results on natural leakage. Can you tell me how natural leakage (a function of F1) is computed on COCO? After sampling the probabilities to flip labels and checking F1 scores, did you do a linear regression to extrapolate for new F1 scores?

What if this relationship is highly nonlinear?

Thanks in advance!"
GPU issue,yuliangguo/Pytorch_Generalized_3D_Lane_Detection,2022-10-14 23:17:04,0,,26,1409958217,"I have two GPU on my desktop, however, even I set os.environ[""CUDA_VISIBLE_DEVICES""] = ""1"", the main_train_GenLaneNet_ext program sill uses the gpu 0. How can I make it use gpu 1?"
cuDNN error,yuliangguo/Pytorch_Generalized_3D_Lane_Detection,2022-10-13 18:03:37,0,,25,1408217060,"When I run main_demo_GenLanezNet_ext.py, I got the following error:

cuDNN error: CUDNN_STATUS_EXECUTION_FAILED 
Traceback (most recent call last):   
    File ""main_demo_GenLaneNet_ext.py"", line 147, in <module>     
        output_geo = output_geo[0].data.cpu().numpy() 
NameError: name 'output_geo' is not defined


How can I fix it?

"
"how to get the number of x_off_std and z_std in ""geo_anchor_std.json""",yuliangguo/Pytorch_Generalized_3D_Lane_Detection,2022-08-25 06:33:08,0,,24,1350393350,"RT.
Thank you!"
can't extract model from data_splits//illus_chg/Gen_LaneNet_ext,yuliangguo/Pytorch_Generalized_3D_Lane_Detection,2022-07-26 08:39:37,0,,23,1317897478,"Hello, author:
when I extract model from below file, I got the errors like these:

$ tar -xvf model_best_epoch_29.pth.tar
tar: This does not look like a tar archive
tar: Skipping to next header
tar: Exiting with failure status due to previous errors

How can I solve it? Thank you!
"
关于估计路面高度的疑惑,yuliangguo/Pytorch_Generalized_3D_Lane_Detection,2022-06-20 12:31:40,0,,20,1276859329,你好，路面高度估计是怎么计算的，对应的估计路面高度的loss是什么
ERFNet,yuliangguo/Pytorch_Generalized_3D_Lane_Detection,2022-04-18 12:00:14,2,,17,1206976323,
Need of gt_height and gt_pitch for inference: GeoNet3D_ext,yuliangguo/Pytorch_Generalized_3D_Lane_Detection,2022-04-07 11:56:00,4,,16,1195939954,"Hey! I have explored your implementation quite nicely. The GeoNet3D_ext model takes as an input the features from the lane segmentation model. Thus those features are then fed to the GeoNet3D_ext model for 3d lane detection. From your implementation, I have realized that you are not predicting camera height and camera pitch instead you are using the gt camera height and gt camera pitch for the same and in your training script you are updating your M_inv using the same gt_camera_height  and gt_camera_pitch. 

https://github.com/yuliangguo/Pytorch_Generalized_3D_Lane_Detection/blob/5bb190ed9f0dc211cd6265ab30c151d3699d3ed4/main_train_GenLaneNet_ext.py#L218
```
            if not args.fix_cam and not args.pred_cam:
                model2.update_projection(args, gt_hcam, gt_pitch)
```
I tried to run the train script for [GeoNet3D_ext](https://github.com/yuliangguo/Pytorch_Generalized_3D_Lane_Detection/blob/master/main_train_GenLaneNet_ext.py) and try to found out that **gt_cam_height == pred_cam_height and the same for pitch**. 

Whereas in the unofficial implementation of 3D lanenet as per the train [script](https://github.com/yuliangguo/Pytorch_Generalized_3D_Lane_Detection/blob/master/experiments/main_train_3DLaneNet.py), the height and pitch are different from gt's. 

**Consider a real-world scenario where I want to use the pre-trained models provided by you for 3D lane Detection: ** in that particular case camera_height and camera_pitch are fixed as not really predicted by the model. Does in that particular case the results for 3d lane detection will differ? Have you tested your approach with any real-world data?

"
"val loss=0,pixel acc=1.000，miou=0.5000，始终不变,应该怎么解决",yuliangguo/Pytorch_Generalized_3D_Lane_Detection,2021-06-11 07:47:35,0,,14,918361980,
# Bug,yuliangguo/Pytorch_Generalized_3D_Lane_Detection,2021-04-12 11:07:07,0,,11,855858612,"https://github.com/yuliangguo/Pytorch_Generalized_3D_Lane_Detection/blob/5bb190ed9f0dc211cd6265ab30c151d3699d3ed4/dataloader/Load_Data_3DLane_ext.py#L104

should be
`self.anchor_y_steps - self.y_ref`"
How to use webcam?,yuliangguo/Pytorch_Generalized_3D_Lane_Detection,2021-03-04 10:40:07,1,,10,822001991,"Hello, i'm student and study your code.

I use webcam and video file to test the test.py. But i don't know how to use webcam and video file

please to teach how to use webcam and video test."
关于R_g2c计算方式的疑惑,yuliangguo/Pytorch_Generalized_3D_Lane_Detection,2021-02-24 09:20:29,3,,9,815284647,"tools/utils.py文件:
```python
def homograpthy_g2im(cam_pitch, cam_height, K):
    # transform top-view region to original image region
    R_g2c = np.array([[1, 0, 0],
                      [0, np.cos(np.pi / 2 + cam_pitch), -np.sin(np.pi / 2 + cam_pitch)],
                      [0, np.sin(np.pi / 2 + cam_pitch), np.cos(np.pi / 2 + cam_pitch)]])
    H_g2im = np.matmul(K, np.concatenate([R_g2c[:, 0:2], [[0], [cam_height], [0]]], 1))
    return H_g2im
```

R_g2c是车体坐标系绕x轴旋转90+pitch度的矩阵可以理解, 为什么H_g2im却只取前两列呢? 

GeoNet3D_ext.py文件 230行:
```python
        # homograph ground to camera
        # H_g2cam = np.array([[1,                             0,               0],
        #                     [0, np.cos(np.pi / 2 + cam_pitch), args.cam_height],
        #                     [0, np.sin(np.pi / 2 + cam_pitch),               0]])
        H_g2cam = np.array([[1,                  0,               0],
                            [0, np.sin(-cam_pitch), args.cam_height],
                            [0, np.cos(-cam_pitch),               0]])
```
这里H_g2cam其实是 np.concatenate([R_g2c[:, 0:2], [[0], [cam_height], [0]]], 1)的结果, 但是为什么又与上面的定义方式不同了呢?"
License question,OasisYang/SSG,2021-01-27 15:14:05,0,,38,795188303,"Hi,

Thank you for the interesting paper and sharing the code. 
I am currently working on a project based on your implementation, and before publishing it I need to figure out what kind of licence your work is under. Since I understood it's based on open-reid and DomainAdaptiveReID, whose licenses are MIT, I figured yours was also, although I can't find any license information in your project.

Thank you again for your great work,"
SSG+&&SSG++,OasisYang/SSG,2020-09-16 07:18:41,0,,36,702521497,你好，刚看完论文，还没来及看代码。对SSG+以及SSG++的训练过程，这点有些疑问。SGG+是不是先利用无监督方法将SSG模型完全收敛以后，利用稳定的聚类来选取label参考集，对target上面每个数据取标签然后再去fine-tuneSSG？
Can you share the trained model on the dataset of Market and Duke？,OasisYang/SSG,2020-07-31 08:17:21,0,,35,669550238,
where is source_train.py,OasisYang/SSG,2020-07-14 17:41:13,0,,34,656785010,"Hi, I am reproducing your work, could you please update the code and fix the error? thanks"
"TypeError: Can't instantiate abstract class Euclidean with abstract methods get_metric, score_pairs",OasisYang/SSG,2020-07-01 07:53:51,1,,33,648758466," File ""C:\Project\Self-Similarity-Grouping-master\reid\metric_learning\__init__.py"", line 25, in get_metric
    return __factory[algorithm](*args, **kwargs)
TypeError: Can't instantiate abstract class Euclidean with abstract methods get_metric, score_pairs

I meet this question in    Self-Similarity-Grouping-master\reid\metric_learning\__init__.py    line25"
module 'reid.models' has no attribute 'create',OasisYang/SSG,2020-05-29 10:11:17,0,,32,627156838,
dataset写的比较难懂,OasisYang/SSG,2020-05-28 14:34:14,1,,31,626562932,写的冗长又完全没有必要，谁会去从解压文件开始
"where is the script ""source_train.py""?",OasisYang/SSG,2020-03-31 03:45:55,0,,29,590766915,"I cannot find the ""source_train.py"" in this project, where can I find it?"
There might be a bug in line 195 of SSG-master/reid/trainers.py,OasisYang/SSG,2020-03-21 01:54:52,0,,28,585407834,"To my understanding, should it be ""loss += loss_tri"" instead of ""loss + loss_tri""?"
Cannot obtain the reported performance by directly running the run.sh,OasisYang/SSG,2020-01-05 04:52:58,4,,26,545370020,"Hi, I would like to thank you for releasing the codes in the first place.

Following the readme.md, we directly run the run.sh without any modification, but fail to obtain the reported performance. Can you help us figure out what the problem is?

To be more specific, we obtain Mean AP: 54.3% and top1: 77.3% (best) after training for 30 epoches by running the run.sh for SSG in Duke->Market1501, which should have Mean AP: 58.3% and top1: 80.0%."
How to do PK sampler to ensure the calculating of  three triplet losses  when the labels pf one image are different?,OasisYang/SSG,2019-12-19 11:42:57,4,,23,540261985,"你好，有两个问题想要请教一下
1. 由于每张图片根据三个聚类结果有三个标签，代码里是根据第一个标签进行PK采样得到一个batch的图片数据，来进行triplet loss的计算。
但是，如何保证这个batch里的每一张图片在根据后两个聚类结果打上伪标签时，在这个batch中能找到正样本来保证triplet loss的正确计算呢？
2. 在selftraining.py文件中的compute_dist(）函数中采用了源域数据特征来计算距离，怎么解释呢？我看论文里并没有提到"
something confused me in JointTrainer2,OasisYang/SSG,2019-12-16 11:57:09,0,,22,538366978,"why used yt as the label for feature ft, ft_up, ft_low,it's different from your paper.
here is the code,calculate the loss Lsemi
""loss_global_eug, prec_global_eug = self.criterions[1](outputs_eug[1], pids_eug, epoch) 
for i, output_p in enumerate(outputs_eug[0]): 
       loss_tri, prec_tri = self.criterions[0](output_p, pids_eug, epoch) loss_os += loss_tri""
and it is different from loss Lssg,
code here
""loss_global, prec_global = self.criterions[1](outputs[1], pids[0], epoch) 
for i, output_p in enumerate(outputs[0]):
       loss_tri, prec_tri = self.criterions[0](output_p, pids[i], epoch) loss_uns += loss_tri""
I'm tring to recurrent your paper,pls help me,Thank you"
I thinkYour code is taking up unnecessary 6GB memory in selftraining.py,OasisYang/SSG,2019-12-02 15:33:05,5,,20,531205650,"you don't need cluster_list on line 366,it takes up 6GB,beacuse cluster.components_ is a is a matrix shape of (15xxx,16552),you can see this in site-packages/sklearn/cluster/dbscan_.py.
I think eps_list can have the same effect.
I don't know if I am right, please tell me"
why use the same compute_dist way in both selftraining.py and semitraining.py?,OasisYang/SSG,2019-10-16 14:03:35,1,,17,507874043,"I am confused that why computing the distance between the source features and target ones in SSG in your codes. And it is similar to SSG+.  Isn`t it that Self-similarity Grouping is just about target features grouping?
if you could, please point out my mistakes in the comprehension of your paper or code. 
  
thanks for your great works!
"
features about rerank,OasisYang/SSG,2019-09-23 02:11:46,4,,14,496866276,"Usually rerank is used in one dataset,but in your rerank.py you rerank source feature with target feature? Why? Or my understanding is fault? At this moment, what's the mean of your e_dist and r_dist? Is it still distence between each sample in target datasets?"
parameter about dce_loss,OasisYang/SSG,2019-09-21 08:24:40,0,,13,496630050,"  in you train file 
    parser.add_argument('--no-rerank', action='store_true', help=""train without rerank"")
    parser.add_argument('--dce-loss', action='store_true', help=""train without rerank"")
both args.dce_loss and args.no-rerank default is False, it seems like you default to use rerank, but if I don't want to use rerank how can I set the parameter about --no-rerank and --dce-loss, I can't find the instuction about --dce-loss in your paper"
the parase in selftraining.py and in semitrain.py,OasisYang/SSG,2019-09-21 02:08:15,0,,12,496601688,"SGG++:use rerank both in selftraining.py and in semitrain.py
SGG:only  use rerank in selftraining.py and don't train semitrain.py
Is my understanding right?"
market1501_trained.pth.tar load error from your google drive download link,OasisYang/SSG,2019-09-21 01:28:51,2,,11,496598323,"Following is my problem, I download your pretrained model from google drive. 
I load dukemtmc.pth.tar successful but fall at market1501.
Any suggestion about this? Anyway, I could retrain a market1501 model using source_train.py
Thanks!

Files already downloaded and verified
Market1501 dataset loaded
  subset   | # ids | # images
  ---------------------------
  train    |   676 |    11744
  val      |    75 |     1192
  trainval |   751 |    12936
  query    |   750 |     3368
  gallery  |   751 |    15913
Files already downloaded and verified
DukeMTMC dataset loaded
  subset   | # ids | # images
  ---------------------------
  train    |   632 |    14923
  val      |    70 |     1599
  trainval |   702 |    16522
  query    |   702 |     2228
  gallery  |  1110 |    17661
Resuming checkpoints from finetuned model on another dataset...

Traceback (most recent call last):
  File ""/home/node3/xxx/SSG/selftraining.py"", line 405, in <module>
    main(parser.parse_args())
  File ""/home/node3/xxx/SSG/selftraining.py"", line 136, in main
    checkpoint = load_checkpoint(args.resume)
  File ""/home/node3/xxx/SSG/reid/utils/serialization.py"", line 34, in load_checkpoint
    checkpoint = torch.load(fpath)['state_dict']
  File ""/home/node3/anaconda3/envs/py36_cu10/lib/python3.6/site-packages/torch/serialization.py"", line 386, in load
    return _load(f, map_location, pickle_module, **pickle_load_args)
  File ""/home/node3/anaconda3/envs/py36_cu10/lib/python3.6/site-packages/torch/serialization.py"", line 573, in _load
    result = unpickler.load()
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe8 in position 0: ordinal not in range(128)
"
"DukeMTMC-reID dataset download is broken, can you share this dataset download link?",OasisYang/SSG,2019-09-20 01:47:58,1,,9,496101018,"Hi, thanks for your excellent work and your release code.
However, I found the DukeMTMC dataset is broken, can you share your data download link?
"
some error of eug.py,OasisYang/SSG,2019-09-18 07:11:08,1,,8,495032462,"when I run semitrain.py no matter I use cluster or random it will report :No such file/directory: 'random_split/random_marker1501.pkl', the error code is in eug.py line406:
    with open(load_path, ""wb"") as fp:
        pickle.dump({""label set"": label_dataset, ""unlabel set"":unlabel_dataset}, fp)

and another question: what's the mean of parser --sample cluster and --sample random"
why reranking so time-cosuming?,OasisYang/SSG,2019-09-03 13:24:41,2,,4,488604608,"When I run the selftraining.py, it is very time-consuming to calculate the source distance and original distance, and why?"
About the source_train.py,OasisYang/SSG,2019-09-02 14:53:21,2,,3,488228073,"I run the source_train.py, and there is a error, as follows:
Traceback (most recent call last):
  File ""source_train.py"", line 311, in <module>
    main(parser.parse_args())
  File ""source_train.py"", line 238, in main
    top1 = rank_score.allshots[0]
AttributeError: 'numpy.float64' object has no attribute 'allshots'.
Why?"
Market2Duke results,OasisYang/SSG,2019-08-26 03:48:06,13,,2,485028293,"I runned the code in Market2Duke and Duke2Market, the result of Duke2Market is a match to the reported numbers while the result of Market2Duke has a drop in performance. The result is showed as below.(I runned on Linux LTS 16.04 with pytorch 0.4.0 and python3.6)
|SSG method| rank-1 |  mAP |
|    reported   |73.0%  |53.4% |
|   observed   |70.2% | 49.8% |

|SSG++ method| rank-1 |  mAP |
|       reported    | 76.0% | 60.3% |
|      observed    | 72.7% | 53.7% |
No change was been made to the training codes, can you please give me some advice about what the reasons probably be?  Thank you."
some problems in source_train.py ,OasisYang/SSG,2019-08-16 06:40:51,5,,1,481465507,"hello, when I read your code in source_train.py, I can not find the definition of the function ""get_one_shot_in_cam1"", so I can't not understanding the meaning of the variable ""l_data"" & ""u_data"". Thank you"
Can't get GTA val set,ucbdrive/3d-vehicle-tracking,2022-10-27 09:50:06,0,,54,1425354551,"When I'm in Data Preparation step, after I run the following code:
`python loader/download.py mini`
I got a result:
```
Connecting to dl.yf.io (dl.yf.io)|128.32.162.150|:80... --2022-10-27 17:46:01--  (try: 3)  http://dl.yf.io/bdd-data/3d-vehicle-tracking/label/gta_3d_tracking_val_label_0001.zip
Connecting to dl.yf.io (dl.yf.io)|128.32.162.150|:80... failed: Unknown error.
Retrying.

failed: Unknown error.
Retrying.

failed: Unknown error.
Retrying.
```

Is there something wrong with my Internet or VPN? How can I get this GTA dataset?"
AssertionError: ERROR: length of GT frames 4741 not equals detected ones 46250,ucbdrive/3d-vehicle-tracking,2022-07-10 02:15:46,1,,52,1299808375,
How can we change this detection of Cars and Trucks to Traffic lights and Signs without training? Any help  please,ucbdrive/3d-vehicle-tracking,2022-01-28 04:10:41,0,,50,1116981227,
AssertaionError ,ucbdrive/3d-vehicle-tracking,2021-12-12 21:04:58,0,,49,1077908012,"Hi, 
I am getting the following error:
`Traceback (most recent call last):
  File ""mono_3d_tracking.py"", line 274, in <module>
    main()
  File ""mono_3d_tracking.py"", line 271, in main
    te.eval_app()
  File ""/home/husam/Carla_3D_Tracking/3d-tracking/tools/eval_mot_bdd.py"", line 96, in eval_app
    result[i_s] = self.eval_parallel(seq_gt, seq_pd)
  File ""/home/husam/Carla_3D_Tracking/3d-tracking/tools/eval_mot_bdd.py"", line 171, in eval_parallel
    evaluator.evaluate()
  File ""/home/husam/Carla_3D_Tracking/3d-tracking/tools/pymot/pymot.py"", line 99, in evaluate
    self.evaluateFrame(frame)
  File ""/home/husam/Carla_3D_Tracking/3d-tracking/tools/pymot/pymot.py"", line 269, in evaluateFrame
    assert self.mappings_[gt_id] != hypo_id
AssertionError
`

when running the following command:
`python run_tracking.py gta val --session 616 --epoch 030`

I wonder what could be the problem here?
I read the comments that you had for the AsserationError which was the following(in pymot.py): 

>           # Assert no known mappings have been added to hungarian, 
>          # since keep correspondence should have 
>          # this case.

but I couldn't understand it"
what are prerequisites and steps for running fastercnn.pytorch?,ucbdrive/3d-vehicle-tracking,2020-02-04 09:08:07,0,,23,559579403,"initially, I planned to run faster-rcnn.pytorch firstly before testing your whole project. Then I download all required datasets(KITTI_Tracking), put them into corresponding dirs and generate soft links.  

1. run `./init.sh`,some new dir and links were built.
2. modify some options in `./run_train.sh`(for example, DATASET=""kitti"") 
3.` sh ./run_train.sh` got an error as follows:
![Screenshot from 2020-02-02 16-41-05](https://user-images.githubusercontent.com/38068286/73729538-93d3cb80-4778-11ea-813f-9699dbf224a4.png)
as I guess, it means no inputs have been loaded.  However, I don't make any modifications to faster-rcnn. if possible, any cues would be highly appreciated.
 "
Training too slow,zth667/Diverse-Image-Synthesis-from-Semantic-Layout,2020-12-29 14:44:14,0,,2,775928201,"Hi,

Thanks for sharing the code of your work. I have one question, during training instead of working on batches , you are passing a single video input to the model at a time. As we know this can significantly increase the training time. I want to know have tried passing complete batch input to the model during training. If yes, then were the results the same?

Thank you."
Data download from google drive,mikittt/easy-to-understand-REG,2021-10-16 02:30:45,0,,2,1027928468,"Hi, I'm interested in the GTA data, but I'm not able to download the images due to this error, which I've gotten two days in a row now:
```
Access denied with the following error:

        Too many users have viewed or downloaded this file recently. Please
        try accessing the file again later. If the file you are trying to
        access is particularly large or is shared with many people, it may
        take up to 24 hours to be able to view or download the file. If you
        still can't access a file after 24 hours, contact your domain
        administrator. 

You may still be able to access the file from the browser:

         https://drive.google.com/uc?id=1gsMpdboUQss-fExik4MMSSikH3iOp40Y ```
Can you please make the data available in some other way? Thanks!"
about the pretrained model,mikittt/easy-to-understand-REG,2019-08-20 13:02:43,10,,1,482847741,"@mikittt Dear Mikihiro, thank you for the share of your work. I have a question about the uploaded models, sp.h5, splm.h5, and spve.h5 are for RefGTA, it is right? could you also please provide the generation models from Refcoco, Refcoco+ and Refcocog? Thank you a lot."
License,ozantezcan/BSUV-Net-inference,2022-06-13 19:18:02,0,,8,1269851088,"Hi, 

You haven't specified any license for your code. Could you please add one?
Thanks."
Error when running BSUV-Net 2.0,ozantezcan/BSUV-Net-inference,2021-09-23 19:16:15,6,,7,1005764414,"When I run the model on a video of 576x1008, I get the following error: 

`RuntimeError: torch.cat(): Sizes of tensors must match except in dimension 0. Got 288 and 576 in dimension 1 (The offending index is 2)`

Have you ever encountered this issue? If so, do you have a fix for that? 

Thanks "
train code,ozantezcan/BSUV-Net-inference,2020-10-10 08:34:36,0,,5,718568370,Thanks for your work. May I get your code about training?
Train code,ozantezcan/BSUV-Net-inference,2020-05-19 13:51:29,2,,2,620994757,"Hi! Thanks for your awesome work! Do you have any plans of sharing train code?

Thanks!"
How do you deal with the tail and head in FairNAS,xiaomi-automl/FairNAS,2022-05-31 07:50:38,0,,8,1253469952,"Hi,

Thanks for your work. I was wondering how do you deal with gradient updates on the non-searchable stages of the model.
The searchable layers will only be updated once, but multiple forward and backward passess would then go through the tail/stem and the detection head. Would you perhaps average the gradients ? or perhaps freeze the parameters of the non-searchable stages ?"
Kendall Tau in FAIRNAS,xiaomi-automl/FairNAS,2021-03-08 02:26:32,1,,7,824106529,"Another paper EVALUATING THE SEARCH PHASE OF NEURAL ARCHITECTURE SEARCH tested FairNAS on NASBench101 but get the Kendall Tau of -0.23. 

FairNAS using 13 models to evaulate the rank and get the Kendall Tau of 0.9487.

I think the number of models used in FairNAS is the way too little and can not really reflect the rank ability of FairNAS"
where can I find the training network code?,xiaomi-automl/FairNAS,2020-12-23 07:11:46,0,,6,773543004,
参数为什么要一起更新？,xiaomi-automl/FairNAS,2020-08-18 06:16:21,2,,5,680727001,"你好，
我对FairNAS的理解是，在训练超网的时候，每个batch是等待所有路径 反向传播 梯度相加之后，统一进行参数更新。 我的问题是，对于超网中的每个节点，它只存在于一条路劲中，所以只会接收到一次梯度，没有相加的过程，也没有必要等所有梯度反传之后一起更新参数，请问算法中提到的梯度相加是指什么？
另外，FariNAS虽然解决了很多公平性的问题，但是是否依然存在路径先后问题？就是说对于有相同节点noda P的路径L1和L2，先训练L1的时候，节点P已经被改变，再训练L2的时候，该节点是否会影响到L2的效果？
谢谢！"
Can you release training network code?,xiaomi-automl/FairNAS,2019-08-26 05:42:25,3,,4,485053557,I want to know your strategy for retraining the network. Can you release training network code?
How about the inference time on different platforms?,xiaomi-automl/FairNAS,2019-07-11 03:02:12,2,,2,466633505,the paper didn't offer the speed of the network compared with other lightweight network？
Will you release model-searching codes in future?,xiaomi-automl/FairNAS,2019-07-05 08:06:08,3,,1,464516343,"Hello!

Thanks for your amazing work! I was wondering if you will release the searching code in future, for I wish to build new model on my own dataset.

Best,"
torch package is not found while trying to recreate the environment from environment.yml,soniabaee/MEDIRL-EyeCar,2022-04-01 20:54:14,0,,4,1190260720,"$ conda-env create -n medirl -f environment.yml
Collecting package metadata (repodata.json): done
Solving environment: failed

ResolvePackageNotFound:
  - torch"
anaconda-project.yml is not exist,soniabaee/MEDIRL-EyeCar,2022-04-01 20:50:47,0,,3,1190258337,anaconda-project.yml which mentioned in the README file is not exist. So environment regeneration is not easy
Missing File?,soniabaee/MEDIRL-EyeCar,2021-08-11 13:38:18,1,,2,966736568,"In /attention/attention.py you import a file which is not present?

`import maximumEtropyDeepIRL.py as deep_maxent`"
Where can i get the eyecar video data,soniabaee/MEDIRL-EyeCar,2021-03-02 15:01:04,15,,1,820099983,Where can i get the eyecar video data?
VERY SLOW training on audio-video dataset like kinetics400 and UCF101,facebookresearch/GDT,2021-11-29 14:11:52,3,,7,1066087170,"Hi authors!
Thank you for making the paper and code open source. It is very helpful.
I am trying to pretrain the GDT model on kinetics400 dataset, while I spent more than 1 day on each epoch. I run on the 8 3090 GPU server and set the batch size on each GPU to 16, and the total batch size is 128, which is a quarter of the original setting in the paper. 
According to the paper, the authors spent 3 days on pretraining with 512 batch size, under normal circumstances it should not cost more than 3 hours on each epoch.
I change the video decode method from `pyav` to `decord`, which brings a bit of improvement in training speed. I wonder if the speed of the provided code is tested before release? What should I do to find the cues for speeding up training?

Some logs below:

```bash
Epoch: [0]  [  360/14961]  eta: 13:42:52  lr: 0.01  clips/s: 16.263  loss: 2.7961 (2.8411)  batch_t/s: 1.0088 (1.4428)  time: 2.8681  data: 1.3705  max mem: 20040
Epoch: [0]  [  370/14961]  eta: 13:46:51  lr: 0.01  clips/s: 13.694  loss: 2.7992 (2.8464)  batch_t/s: 1.0067 (1.0740)  time: 4.3781  data: 3.3474  max mem: 20040
Epoch: [0]  [  370/14961]  eta: 13:46:48  lr: 0.01  clips/s: 13.769  loss: 2.7919 (2.8454)  batch_t/s: 1.0110 (1.7200)  time: 4.3779  data: 1.3611  max mem: 20040
Epoch: [0]  [  370/14961]  eta: 13:46:48  lr: 0.01  clips/s: 13.532  loss: 2.7913 (2.8402)  batch_t/s: 1.0089 (1.4563)  time: 4.3786  data: 2.4327  max mem: 20040
Epoch: [0]  [  380/14961]  eta: 13:31:23  lr: 0.01  clips/s: 14.072  loss: 2.7891 (2.8451)  batch_t/s: 1.0196 (1.0736)  time: 2.5644  data: 1.5199  max mem: 20040
Epoch: [0]  [  380/14961]  eta: 13:31:20  lr: 0.01  clips/s: 14.029  loss: 2.7738 (2.8434)  batch_t/s: 1.0512 (1.7027)  time: 2.5646  data: 0.5402  max mem: 20040
Epoch: [0]  [  380/14961]  eta: 13:31:19  lr: 0.01  clips/s: 14.026  loss: 2.7874 (2.8387)  batch_t/s: 1.0548 (1.4459)  time: 2.5643  data: 1.0631  max mem: 20040
Epoch: [0]  [  390/14961]  eta: 13:36:54  lr: 0.01  clips/s: 15.097  loss: 2.7765 (2.8417)  batch_t/s: 1.0534 (1.7432)  time: 2.6929  data: 0.5196  max mem: 20040
Epoch: [0]  [  390/14961]  eta: 13:36:56  lr: 0.01  clips/s: 14.988  loss: 2.7927 (2.8441)  batch_t/s: 1.0630 (1.0732)  time: 2.6932  data: 1.6344  max mem: 20040
Epoch: [0]  [  390/14961]  eta: 13:36:53  lr: 0.01  clips/s: 16.121  loss: 2.7775 (2.8376)  batch_t/s: 1.0481 (1.4640)  time: 2.6923  data: 1.0834  max mem: 20040
Epoch: [0]  [  400/14961]  eta: 13:43:48  lr: 0.01  clips/s: 16.551  loss: 2.7957 (2.8433)  batch_t/s: 1.0546 (1.0725)  time: 4.4575  data: 3.4058  max mem: 20040
Epoch: [0]  [  400/14961]  eta: 13:43:45  lr: 0.01  clips/s: 1.458  loss: 2.7986 (2.8373)  batch_t/s: 1.0390 (1.4786)  time: 4.4577  data: 2.3538  max mem: 20040
Epoch: [0]  [  400/14961]  eta: 13:43:46  lr: 0.01  clips/s: 0.679  loss: 2.7963 (2.8410)  batch_t/s: 1.0598 (1.7822)  time: 4.4580  data: 1.1610  max mem: 20040
Epoch: [0]  [  410/14961]  eta: 13:29:18  lr: 0.01  clips/s: 15.575  loss: 2.7954 (2.8418)  batch_t/s: 1.0273 (1.0715)  time: 2.8114  data: 1.7718  max mem: 20040
Epoch: [0]  [  410/14961]  eta: 13:29:15  lr: 0.01  clips/s: 15.525  loss: 2.7892 (2.8399)  batch_t/s: 1.0306 (1.7639)  time: 2.8114  data: 0.6421  max mem: 20040
```

Sincerely yours."
 Pretrained STiCA model,facebookresearch/GDT,2021-09-17 07:58:24,0,,5,999062056,"Hi,
Can you share the Fully supervised Kinetics trained R(2+1D)-18  and the Kinetics pretrained STiCA models?  I am doing a self-supervised learning survey where I am comparing different self-supervised methods. I would like to include your STiCa method and a comparison with fully supervised learning too. Hoping for a positive response."
无法复现论文的结果,csbhr/Deep-Blind-VSR,2022-09-25 04:01:42,0,,9,1384887681,您好，您ICCV论文中提到小模型可以在退化后的REDS4上可以PSNR达到29.2，大模型PSNR可以达到30.2，我按照您大模型的配置最后只得到了29.33，训练了将近500epoch，大概300epoch的时就已经是29.3了，可能是已经收敛了。请问您可以分享一下训练策略吗？
loss function ,csbhr/Deep-Blind-VSR,2022-08-26 15:28:30,0,,8,1352420517,Could you explain the loss function in the ICCV paper?
rfft and performances,csbhr/Deep-Blind-VSR,2022-05-02 10:21:09,0,,7,1222718318,"I tested your code with --quick_test Gaussian_Vid4 and got different results:

The rfft (& irfft) function is deprecated and the torch version is very old, not compatible with my python and CUDA versions. I replaced that with:
def rfft(t):
# Real-to-complex Discrete Fourier Transform, onesided=False
t = torch.fft.fft2(t)
return torch.stack([t.real, t.imag], dim=-1)

def irfft(t):
# Complex-to-real Inverse Discrete Fourier Transform, onesided=False
t = t[..., 0] + 1j*t[..., 1]
return torch.fft.ifft2(t).real

and validate that with Colab (which supports torch 0.4.1 as you used).
It seems to be accurate part to numeric errors.

The score I get:
Total AVG-PSNR=23.777, AVG-SSIM=0.7013
![image](https://user-images.githubusercontent.com/57372254/166219796-dcf4d4ff-8f2c-48e3-b6ad-7e037ab0dea1.png)


What can cause the different performance?
Thx!"
correlation error,csbhr/Deep-Blind-VSR,2022-02-17 08:06:32,0,,6,1140999147,"I tried to test your code by this command : 
python inference.py --input_path ../LR_videos --gt_path ../GT_videos --model_path ../pretrain_models/gaussian_e1r3.pt
which was mentioned in the repo with Pytorch=0.4.1 and 1.2.0 and always get this error : 
input=correlation.FunctionCorrelation(tensorFirst=tensorFirst, tensorSecond=tensorSecond),
AttributeError: module 'correlation' has no attribute 'FunctionCorrelation'

How can i solve it?
Thanks for your work!"
Can you publish more pre-trained face recognition models?I noticed that there are many models in the config.py. I hope you can share them. Thank you very much,shawnxyang/tip-im,2022-07-16 14:10:54,0,,9,1306816382,
Evaluation Method?,shawnxyang/tip-im,2022-04-17 22:45:23,0,,7,1206503793,"Hi, I'm very interested in this and have already generated adversarial masks to the images. However, after I got the generated image, I don't know how to evaluate if this method works? Is there any way for me to do the evaluation?

Thank you!"
Could you release the dataset that you have selected in the paper?,shawnxyang/tip-im,2022-04-10 11:20:42,1,,6,1198991598,"I would like to reproduce the results in the paper. However, the dataset in the paper is selected from the public dataset by you. It's hard for me to reproduce the results in the paper without the dataset that you have selected.

Thanks a lot."
gamma如何设置？,shawnxyang/tip-im,2022-02-18 02:19:48,1,,3,1142197141,"我尝试使用LFW(500张图(one image per identity)作为attack_images, 10张作为targets)复现，但是当我把gamma设置成0.1时，生成的adv_images就不能成功保护id了，而论文中可以设置到2.5。这是为什么？"
Cannot be reproduced,dvlab-research/Robust-Semantic-Segmentation,2022-05-26 14:37:11,1,,4,1249659587,"Hello I am a computer student, currently reproduced your code on the voc2012 dataset, but the result of the BIM attack is 2 is 33, which is very different from the result in the paper, I encountered this problem during the test, please ask you also encountered it. If you can, you can ask for some suggestions for this problem.Thank you very much.
![423ad2b5b3d7bb9f9267fb322a54d8f](https://user-images.githubusercontent.com/48797719/170510360-c4b7bbd4-0708-474f-80a2-fe194ff0ebd1.png)


"
Corrupt Pre-trained Model,dvlab-research/Robust-Semantic-Segmentation,2022-01-19 09:23:29,2,,3,1107851551,"Hi,

I saw this message after downloading your provided pre-trained model from Google Drive and type the `unzip pretrained_model.zip` command.

```shell
Archive:  pretrained_model.zip
warning [pretrained_model.zip]:  4294967296 extra bytes at beginning or within zipfile
  (attempting to process anyway)
file #1:  bad zipfile offset (local header sig):  4294967296
  (attempting to re-compensate)
   creating: pretrain/
error: invalid zip file with overlapped components (possible zip bomb)
```

I wonder whether it is a corrupt file so that I cannot correctly decode it.
Thanks.

Best,
Tsung-Han Wu"
Default Training Config,dvlab-research/Robust-Semantic-Segmentation,2022-01-12 12:31:21,0,,2,1100286392,"Hi,

First, thanks for your excellent work.
I am a master student major in CS who is now working on adversarial training. I would like to ask whether your provided config is able to reach the comparable result as your paper report.

Specifically, 
1. In [./config/cityscapes/cityscapes_aspp.yaml](https://github.com/dvlab-research/Robust-Semantic-Segmentation/blob/main/config/cityscapes/cityscapes_aspp.yaml), is it reasonable to set `test_h = 449, test_w = 449` while testing?
2. In your README, you stated that ""For multiprocessing training, we use apex, tested with pytorch 1.0.1."". However, it seems that your [config](https://github.com/dvlab-research/Robust-Semantic-Segmentation/blob/main/config/cityscapes/cityscapes_aspp.yaml) set `opt_level: 'O0'` and does not use float16 during training.
 
I would be very grateful if you could provide me any suggestions or further explanation.
Thank you very much.

Best,
Tsung-Han Wu."
Where is the code of C&W and DeepFool?,dvlab-research/Robust-Semantic-Segmentation,2021-11-10 05:21:10,0,,1,1049414791,"Hi, @xiaogang00 Thanks for your work.But I do not find the code of C&W and DeepFool in test code,are the code at another position?
Could you supplement the code of C&W and DeepFool code if possible?
Thanks a lot!"
The link of trained relationship detection models is Error 404,sherif-abdelkarim/LTVRR,2022-10-15 02:15:15,0,,9,1410012180,
Is there any json related to the training and testing relationships in the annotations file of the VG dataset?,sherif-abdelkarim/LTVRR,2022-08-02 10:28:40,1,,8,1325643404,
Training workflow does not work on Visual Genome,sherif-abdelkarim/LTVRR,2022-04-02 05:32:42,1,,7,1190507674,"`    python tools/train_net_step_rel.py --dataset vg8k --cfg configs/vg8k/e2e_relcnn_VGG16_8_epochs_vg8k_y_loss_only_hubness100k.yaml --nw 8 --use_tfboard --seed 3`

    INFO train_net_step_rel.py: 663: Save ckpt on exception ...
    INFO train_net_step_rel.py: 150: save model: Outputs/e2e_relcnn_VGG16_8_epochs_vg8k_y_loss_only_hubness100k/vg8k/Apr02-00-17-05_bock_step_with_prd_cls
    _v3/ckpt/model_step0.pth
    INFO train_net_step_rel.py: 665: Save ckpt done.
    Traceback (most recent call last):
      File ""tools/train_net_step_rel.py"", line 604, in main
        net_outputs = maskRCNN(**input_data)
      File ""/home/wentaoy2/anaconda3/envs/vr/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 477, in __call__
        result = self.forward(*input, **kwargs)
      File ""/home/wentaoy2/LTVRR/lib/nn/parallel/data_parallel.py"", line 108, in forward
        outputs = [self.module(*inputs[0], **kwargs[0])]
      File ""/home/wentaoy2/anaconda3/envs/vr/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 477, in __call__
        result = self.forward(*input, **kwargs)
      File ""/home/wentaoy2/LTVRR/lib/modeling/model_builder_rel.py"", line 513, in forward
        return self._forward(data, im_info, dataset_name, roidb, use_gt_labels, include_feat, **rpn_kwargs)
      File ""/home/wentaoy2/LTVRR/lib/modeling/model_builder_rel.py"", line 553, in _forward
        assert len(roidb) == 1
  AssertionError"
Fiile missing Mar11-07-01-07_gpu210-18_step_with_prd_cls_v3/ckpt/best.pth,sherif-abdelkarim/LTVRR,2022-04-02 05:05:22,1,,6,1190501087,"On the command for evaluating the **Visual Genome dataset** there is a file best.pth is missing, could you help us locate it?
`python tools/test_net_rel.py --dataset vg8k --cfg configs/vg8k/e2e_relcnn_VGG16_8_epochs_vg8k_y_loss_only_hubness.yaml --do_val --load_ckpt Outputs/e2e_relcnn_VGG16_8_epochs_gvqa_y_loss_only_hubness/vg8k/Mar11-07-01-07_gpu210-18_step_with_prd_cls_v3/ckpt/best.pth  --use_gt_boxes --use_gt_labels --seed 0`

     FileNotFoundError: [Errno 2] No such file or directory: 'Outputs/e2e_relcnn_VGG16_8_epochs_gvqa_y_loss_only_hubness/vg8k/Mar11-07-01-07_gpu210-18_step_with_prd_cls_v3/ckpt/best.pth'
"
CUDA version with CUDA 11.4 and CUDA 9.0,sherif-abdelkarim/LTVRR,2022-03-31 07:16:52,0,,5,1187625224,"Yes, I also got the CUDA version issue,  the error message I am getting is 

    ImportError: /home/wentaoy2/LTVRR/lib/model/roi_pooling/_ext/roi_pooling/_roi_pooling.so: undefined symbol: __cudaRegisterFatBinaryEnd
    
Which CUDA version are you using?
And the version of CUDA and pytorch I am on is 

   $ pip list| grep torch
   torch           0.4.1.post2        
   torchvision     0.1.8 
   $ nvcc --version
   nvcc: NVIDIA (R) Cuda compiler driver
   Copyright (c) 2005-2021 NVIDIA Corporation
Built on Mon_Oct_11_21:27:02_PDT_2021
Cuda compilation tools, release 11.4, V11.4.152
Build cuda_11.4.r11.4/compiler.30521435_0
(vr) wentaoy2@bock:~/LTVRR$ 


I also tried CUDA-9.0 and got simiar issures:
     
    File ""/home/wentaoy2/LTVRR/lib/model/roi_pooling/functions/roi_pool.py"", line 3, in <module>
       from .._ext import roi_pooling
    File ""/home/wentaoy2/LTVRR/lib/model/roi_pooling/_ext/roi_pooling/__init__.py"", line 3, in <module>
       from ._roi_pooling import lib as _lib, ffi as _ffi
    ImportError: /home/wentaoy2/LTVRR/lib/model/roi_pooling/_ext/roi_pooling/_roi_pooling.so: undefined symbol: __cudaRegisterFatBinaryEnd

        $ nvcc --version
        nvcc: NVIDIA (R) Cuda compiler driver
        Copyright (c) 2005-2017 NVIDIA Corporation
        Built on Fri_Sep__1_21:08:03_CDT_2017
        Cuda compilation tools, release 9.0, V9.0.176"
Will this challenge be held on CVPR2022？,sherif-abdelkarim/LTVRR,2022-03-25 02:14:09,1,,4,1180261062,
请问要让代码在cuda11.0以及pytorch1.7.1的环境中运行应该修改什么内容呢,sherif-abdelkarim/LTVRR,2021-10-09 12:50:27,6,,3,1021701457,
我想问一下预训练数据模型是怎么训得,LongguangWang/Scale-Arbitrary-SR,2022-09-07 13:24:26,0,,20,1364661602,
Cannot get the objective quality（PSNR/SSIM）,LongguangWang/Scale-Arbitrary-SR,2022-01-10 05:58:40,1,,18,1097487164,"Hi, Longguang,

Thank your for providing the resource code and the pre-trained model. It is a wonderful and elegant work. However, when I use the pre-trained model or the retrained model according to your training code to test the Set5 dataset, there are some problems as follows:

1) Compared with the bucubic method, the pre-trained or retrained model cannot get good the objective quality in terms of PSNR and SSIM. It is X2 super-resolution. The related data is attached. I don't know why this result is so strange. 

![psnr](https://user-images.githubusercontent.com/80899378/148722878-2a9303b6-810e-44af-a1c2-d3e973ae7b86.png)
![ssim](https://user-images.githubusercontent.com/80899378/148723035-47befc0a-117e-4878-b76b-2fcc21a636b8.png)

2) It should be mentioned that the subjective quality of the super-resolution image from pretrained or retrained model is obviously better than that from bicubic. 

3) In addition, when I have this problem, I tested these model in some other images( not public). And the related results are similar as above mentioned. The traditional quality measures cannot have good performance, while the subjective quality is better than bicubic.

As a result, I am confused why this happened. Sincerely hope that I can get your help at your convenience.

Regards,
Bolin
"
test ,LongguangWang/Scale-Arbitrary-SR,2021-12-29 11:47:42,1,,17,1090466004,"FileNotFoundError: No such file: '/data2/qinfl/benchmark/Set5/LR_bicubic/X1.60/baby.png'  
您好，我根据要求执行gen_test_data.m没有得到相关尺度的下采样图"
test error,LongguangWang/Scale-Arbitrary-SR,2021-12-23 09:25:01,0,,16,1087531010,"
![image](https://user-images.githubusercontent.com/44860428/147219023-5202dad7-3db9-44e2-81e9-6456b30352ec.png)




The test data file was set up myself according to readme.txt， consistent file structure，what is the cause of the error?"
doubt regarding computation of LR space and HR space,LongguangWang/Scale-Arbitrary-SR,2021-11-01 05:30:52,2,,14,1040786776,"Hi thanks for your such interesting work, Can you please tell me why we need to subtract 0.5 and while calculating the coordinates in LR space.
coor_h = ((coor_hr[0] + 0.5) / scale) - (torch.floor((coor_hr[0] + 0.5) / scale + 1e-3)) - 0.5"
"Hello,  the published model model_150.ckpt is the best weighting?",LongguangWang/Scale-Arbitrary-SR,2021-09-29 08:29:53,2,,9,1010655505,The measured accuracy does not meet the paper's specifications
 supplementary material,LongguangWang/Scale-Arbitrary-SR,2021-08-19 12:55:23,1,,8,974633795,Thanks for sharing the work. I want to know where is the  supplementary material?
Looking forward to open source,LongguangWang/Scale-Arbitrary-SR,2021-07-28 06:09:17,1,,7,954510804,
about figure 1 of your paper,LongguangWang/Scale-Arbitrary-SR,2021-07-26 04:41:11,1,,6,952494395,"Hi, I am confused about figure 1 in your paper. It seems that (c) are intermediate feature maps? But what does the values (i.e. range [0, 5000]) mean. And how do you get figures of par (b)? Thx!"
Where is the supplemental material?,LongguangWang/Scale-Arbitrary-SR,2021-03-21 12:27:42,1,,5,837064210,"I cannot find the supplemental material mentioned in the paper from anywhere. If there is, please let us know.
Thanks."
请问什么时候开源,LongguangWang/Scale-Arbitrary-SR,2021-02-17 07:29:32,1,,4,809934390,
when does the code release?,LongguangWang/Scale-Arbitrary-SR,2020-11-09 11:25:02,1,,3,738943105,"Hi, 
Thanks for the awesome repo.
Could you please tell me when do you plan to release the code?
Thanks.
B.R."
Please release the test code,LongguangWang/Scale-Arbitrary-SR,2020-09-22 07:02:16,1,,2,706130541,
The application of the algorithm in some 2D-CNN image classification networks,liu-zhy/TANet,2022-10-15 06:02:05,0,,15,1410059829,"Thank you for your work.
I want to know whether some codes on the 2D-CNN network have been adjusted, such as MobileNetV2-TAM mentioned by erwangccc. I want to know how TAM is embedded in this type of network."
Could please provide a small demo dataset used to debug?,liu-zhy/TANet,2022-06-01 07:35:55,2,,11,1255235103,"Thanks for the great work TAM!
If you can provide a small demo dataset.I think it would more easy for others to learn your wonderful work.
@liu-zhy "
About MobileNetV2 arch,liu-zhy/TANet,2022-05-24 15:46:31,2,,10,1246760334,"Thank you for your work. 
And do you implement MobileNetV2-TAM arch? Could you release those code?

Thanks!"
About n_segment,liu-zhy/TANet,2022-02-10 15:02:46,2,,9,1130245456,n_segment是视频序列中帧的个数，但如果不确定视频序列帧数应该怎么办呢，这里不能自适应调整嘛？
n_segment？,liu-zhy/TANet,2021-05-24 08:42:34,1,,4,899447896,TAM中的n_segment表示的输入视频的帧数吗？
Torchvision Encoder,njuhuojing/mast,2022-02-09 19:37:34,0,,7,1128947304,"Hi @NJUHuoJing, thanks for sharing your great code and congratulations on your amazing work. 
I have a question about the encoder that you applied in your framework, I can get similar results as yours when I use your code with your provided encoder (vgg_r51.pth). However, when I use the pre-trained VGG-19 [model](https://pytorch.org/vision/stable/models.html#torchvision.models.vgg19) provided by pytorch-torchvision, I cannot get any reasonable result. I have tried the torchvision VGG-19 with the exact same layers as you. For instance, in the `test_artistic.py` code, I used the `r41`,`r31`,`r21`, and I defined the following code for the encoder. 

```
from torchvision import models 
class VGG19_encoder(torch.nn.Module):
    def __init__(self,requires_grad=False,n_layers=[1,6,11,20,29]):       
        super(VGG19_encoder,self).__init__()
        vgg_pretrained_features=models.vgg19(pretrained=True).features   
        self.slice0=torch.nn.Sequential()
        self.slice1=torch.nn.Sequential()
        self.slice2=torch.nn.Sequential()
        self.slice3=torch.nn.Sequential()
        self.slice4=torch.nn.Sequential()
        for x in range(n_layers[0]):#relu1_1
            self.slice0.add_module(str(x),vgg_pretrained_features[x])        
        for x in range(n_layers[0],n_layers[1]): #relu2_1
            self.slice1.add_module(str(x),vgg_pretrained_features[x])        
        for x in range(n_layers[1],n_layers[2]): #relu3_1
            self.slice2.add_module(str(x),vgg_pretrained_features[x])
        for x in range(n_layers[2],n_layers[3]):#relu4_1
            self.slice3.add_module(str(x),vgg_pretrained_features[x])
        for x in range(n_layers[3],n_layers[4]):#relu5_1
            self.slice4.add_module(str(x),vgg_pretrained_features[x])        
        if not requires_grad:
            for param in self.parameters():
                param.requires_grad=False            
    def forward(self,x):
        h0=self.slice0(x)    #[1,64,512,512]
        h1=self.slice1(h0)   #[1,128,256,256]
        h2=self.slice2(h1)   #[1,256,128,128]
        h3=self.slice3(h2)   #[1,512,64,64]
        h4=self.slice4(h3)   #[1,512,32,32]
        return {'r11':h0,'r21':h1,'r31':h2,'r41':h3,'r51':h4}
```
Then, I defined the encoder as the following line and added it in `multi_level_test` by using just the `r41`,`r31`,`r21` layers: 
```
encoder = VGG19_encoder(n_layers=[1,6,11,20,29]).to(device)
```
I am wondering how did you train your encoder and what is the difference between your VGG-19 model and the VGG-19 model from torchvision since both of them are trained on ImageNet.

I would appreciate a lot any help that you could provide regarding this issue.
Thank you very much."
How to modify the data representation to support different sizes of layouts?,kampta/DeepLayout,2021-12-25 19:56:51,0,,8,1088649094,"Hi, Currently the PubLayNet example generates layouts of size 256x256, a fixed square aspect ratio.
How do I modify the representation to make it support different aspect ratios during inference?
"
Can you provide the pretrained models?,kampta/DeepLayout,2021-12-20 07:56:38,0,,7,1084470857,"Thanks for the excellent work!
Do you have plan to provide the training code on RICO datasets and related pre-trained models? "
Has anyone successfully trained on COCO?,kampta/DeepLayout,2021-12-01 06:43:29,3,,6,1068023883,"Hello, thank you for sharing this code.
I used the code with no changes and when I saw the visualization of the generated boxes are not really good.
![image](https://user-images.githubusercontent.com/59387731/144185130-ac33b9c2-7b11-4a87-9b04-e660f84b6402.png)
![image](https://user-images.githubusercontent.com/59387731/144185142-6c9d3ca5-5487-4c2b-b1c7-12735ed4b9c4.png)


Has anyone successfully trained on COCO?"
Questions related to your paper,kampta/DeepLayout,2021-11-22 05:42:59,1,,5,1059720118,"Hi~ I've read your paper and the code, and I am little confused about some details.

Firstly, the coordinates of bounding boxes are discretized and the vocabulary for layouts contains the 8-bit uniform quantization token (which is token indexes are 0-127 in your code), categorial token, padding token, bos and eos token. But what if during inference stage, the predicted coordinate of the trained model doesn't lie in 0-127 tokens? Will you do any post processing to correct these mis-predicted layouts?

Secondly, in your paper, it is said that 'we minimize KL-Divergence between soft-max predictions and output one-hot distribution with Label Smoothing', but why the code is still a cross entropy loss?"
Wrong Testing result from demo code,kampta/DeepLayout,2021-11-17 14:04:51,0,,4,1056170941,"Thanks for your repo.

I used the default training code of publayout, and trained for 10 epoches.
The final training loss reaches around 1.27.
epoch 10 iter 5245: train loss 1.27828. lr 2.880000e-04: 100%|██████████| 5246/5246 [46:58<00:00,  1.86it/s]

I opened the annotated code groups (total 4), and found that the input image is fine while the output seems not satisfactory.
`                for i, layout in enumerate(layouts):
                    layout = self.test_dataset.render(layout)
                    layout.save(os.path.join(self.config.samples_dir, f'recon_{epoch:02d}_{i:02d}.png'))`

![image](https://user-images.githubusercontent.com/11433902/142362797-6242b926-6ac0-406b-8669-da34f8b9b3a8.png)


Any suggestions or comments will be appreciated. thanks!"
How to generate the 3D Shape?,kampta/DeepLayout,2021-10-17 09:28:33,14,enhancement,1,1028268637,Thanks for releasing your code. And could you please tell me how to generate the 3D shape? I would be grateful!!! :-)
tf and keras versions,xdhe1216/ResRep,2021-11-27 15:21:37,1,,1,1065065687,"如题，请问一下大佬运行程序的具体tf和keras版本号啊？

试了好几个版本都不行，感谢"
ImportError: cannot import name 'walk_files' from 'torchaudio.datasets.utils' ,fangwei123456/Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron,2022-10-11 09:10:01,2,,10,1404288594,"Hi, thanks for your interesting work!

can you tell me the requirments that you used?  I train your work on cifar10-dvs with `torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1`, it generates an error as 


```
Namespace(T=20, T_max=64, alpha_learnable=False, batch_size=16, channels=128, dataset_name='CIFAR10DVS', detach_reset=True, device='cuda:6', init_tau=2.0, learning_rate=0.001, log_dir_prefix='./plif_test/logsd', max_epoch=1024, normalization='None', number_layer=4, split_by='number', use_max_pool=True, use_plif=True, val_ratio=0.15)
./plif_test/logsd/val_0.15_CIFAR10DVS_init_tau_2.0_use_plif_True_use_max_pool_True_T_20_c_128_n_4_split_by_number_normalization_None_detach_reset_True ./plif_test/logsd/pt_val_0.15_CIFAR10DVS_init_tau_2.0_use_plif_True_use_max_pool_True_T_20_c_128_n_4_split_by_number_normalization_None_detach_reset_True
Traceback (most recent call last):
  File ""./codes/train_val.py"", line 185, in <module>
    from spikingjelly.datasets.cifar10_dvs import CIFAR10DVS
  File ""<frozen zipimport>"", line 259, in load_module
  File ""/opt/conda/envs/dual_learning/lib/python3.8/site-packages/spikingjelly-0.0.0.0.1-py3.8.egg/spikingjelly/datasets/__init__.py"", line 6, in <module>
  File ""<frozen zipimport>"", line 259, in load_module
  File ""/opt/conda/envs/dual_learning/lib/python3.8/site-packages/spikingjelly-0.0.0.0.1-py3.8.egg/spikingjelly/datasets/speechcommands.py"", line 7, in <module>
ImportError: cannot import name 'walk_files' from 'torchaudio.datasets.utils' (/opt/conda/envs/dual_learning/lib/python3.8/site-packages/torchaudio/datasets/utils.py)
```

"
Data transform when training,fangwei123456/Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron,2022-09-28 08:43:14,3,,9,1388967592,"Can you please help me understand the transform added to some dataset when training? E.g. 

if dataset_name == 'MNIST':
        transform_train = transforms.Compose([
            transforms.RandomAffine(degrees=30, translate=(0.15, 0.15), scale=(0.85, 1.11)),
            transforms.ToTensor(),
            transforms.Normalize(0.1307, 0.3081),
        ])

I tried to train without the RandomAffine and it seemed that the performance was not that good as with this transform."
Problems in pre-trained faster-rcnn detector,bknyaz/sgg,2022-07-22 08:18:50,1,,9,1314622578,"Hi, thank you for sharing these wonderful works!

I found a problem in loading the pre-trained file 'vg-faster-rcnn.tar'.
The anchor ratios and anchor scales in neural-motifs are inconsistent with the `torchvision.models.detection` 
motifs
anchor ratios: (0.23232838, 0.63365731, 1.28478321, 3.15089189); scales: (2.22152954, 4.12315647, 7.21692515, 12.60263013, 22.7102731)
torchvision
anchor ratios: (0.5, 1.0, 2.0); scales: (32, 64, 128, 256, 512).
Thus the pre-trained weights 'vg-faster-rcnn.tar' mismatch the torchvision in `rpn.head.bbox_pred` (120, 512, 1, 1) vs (60, 512, 1, 1).

I don't know if my analysis above is correct and if this will affect the performance of rpn."
Training with a smaller training set,bknyaz/sgg,2021-06-29 07:32:46,0,,7,932313394,"Is there any way to decrease the size of the training set? (specifically when using GQA)
"
How to create a scenegraph from a custom image?,bknyaz/sgg,2021-02-20 14:50:15,3,,6,812634370,"## Your research is very good, and it is very helpful to me. Thank you for always.


### I would like to proceed with creating a scenegraph for my custom image.
Can you help me how I can do it?

I would like to modify the Jupiter notebook source code provided by you, if possible, to give a custom image as input.

* I mean, I don't add learning data, I just want to see the scene graph as the image I specified!"
Is there an intention to release the forward code for motifs model?,bknyaz/sgg,2020-06-09 09:47:05,4,,2,635281144,
Code realted to Robustness in Experiments section,ningyu1991/ArtificialGANFingerprints,2022-09-19 04:12:42,0,,5,1377370312,"Hi, thanks for sharing your excellent work. Can you release the experiment related (such as verifying Robustness) code for reproducing? @ningyu1991 "
Source code for running the Generator,ningyu1991/ArtificialGANFingerprints,2022-03-27 14:42:21,0,,4,1182552488,"Hi, I want to reproduce your code but there are too many issue running the nvidia stylegan.

Could you provide the source code of running the fingerprintted generator?"
how to use --check in embed_fingerprints.py,ningyu1991/ArtificialGANFingerprints,2022-02-08 08:21:38,0,,3,1126931613,"Hi，
I'm trying to produce some fingerprints using your code but i don't know how to validate my fingerprint detection accuracy.
Could you tell me how to use arguments '--check' to validate fingerprint detection accuracy？
thanks
"
Not-compatible out-of-the-box with datasets (low-fidelity or 0.5 bitwise accuracy).,ningyu1991/ArtificialGANFingerprints,2021-10-28 12:52:26,0,,2,1038480108,"Hi, I'm trying to produce some fingerprints using your code.
I've been experimenting with AFHQ (cats only) but to no avail.

First of all, the bitwise accuracy stays at 50% for most runs and occasionally it converges to sth like 90%+. But when it does converge, the fidelity of the output images is virtually nonexistent.

I'm running the autoencoder training as `python3 train.py --data_dir data/afhq/train/cat/ --image_resolution 128 --output_dir ./output_cat --fingerprint_length 100 --batch_size 32 --num_epochs 30`.

Followed by `python3 embed_fingerprints.py --encoder_path output_cat/checkpoints/stegastamp_100_28102021_10:00:00_encoder.pth --data_dir data/afhq/train/cat --image_resolution 128 --output_dir output_cat_encoded --batch_size 16`.

Am I missing something?"
how to export fbx from a single image,Arthur151/ROMP,2022-11-08 05:32:15,0,,365,1439518573,"Hi, thanks for the great work!
I want to ask how to export fbx from a npz file of one image, not a video. 
I checked the issues below.
#228  #270
and trying to export using Blender like #228.

But I'm facing some errors, and I cannot fix it.
Could you tell me how to export fbx from single image.

The error is below:

```
Missing tracking IDs in results. Using the first pose results for animation.
To get the tracking IDs, please use temporal optimization during inference.
Traceback (most recent call last):
  File ""/Users/.../romp/romp_fbx.blend/Text"", line 370, in <module>
  File ""/Users/.../romp/romp_fbx.blend/Text"", line 233, in process_poses
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices
Error: Python script failed, check the message in the system console

```

Following script is my script with error. I changed convert2fbx.py following to #228.
```
 if len(sequence_results)>0:
        subject_ids = list(sequence_results.keys())
        if subject_id == -1 or subject_id not in subject_ids:
            print('Get motion sequence with subject IDs:', subject_ids)
            subject_id = int(input('Please select one subject ID (int):'))
        poses = np.array(sequence_results[subject_id]['smpl_thetas'])
        trans = np.array(sequence_results[subject_id]['cam_trans'])
    else:
        print('Missing tracking IDs in results. Using the first pose results for animation.')
        print('To get the tracking IDs, please use temporal optimization during inference.')
        frame_names = sorted(list(frame_results.keys()))
        poses, trans = np.zeros((len(frame_names), 72)), np.zeros((len(frame_names), 3))
        
        frame_nums = 1
        poses, trans = np.zeros((frame_nums, 72)), np.zeros((frame_nums, 3))
        for inds, frame_id in enumerate(frame_names):
            poses[inds] = frame_results[frame_id]['smpl_thetas']
            trans[inds] = frame_results[frame_id]['cam_trans']

```"
What are the difference between the output['predicts_j3ds'] and the joints got from the SMPL model using  output['betas'] and output['thetas'] ?,Arthur151/ROMP,2022-11-03 17:29:28,0,,364,1435029400,"What are the difference between the output['predicts_j3ds'] and the joints got from the SMPL model using  output['betas'] and output['thetas'] ? I found the results are different. And I use the joints got from the SMPL model using  output['betas'] and output['thetas'],  to reproject to the 2D image(the green points) using the estimated K, which is wrong comparing the outputs['pj2d_org'](the red points).
![test1](https://user-images.githubusercontent.com/56253997/199792549-c564de63-c480-4427-88a3-6aae8212b1f1.png)

Could you please answer my question? Thanks!

"
problem in convert2fbx.py,Arthur151/ROMP,2022-11-03 11:05:55,1,,363,1434476911,"I used the demo video:https://github.com/Arthur151/ROMP/blob/assets/demo/videos/sample_video2.mp4,and used ""bev -m video -i ... -o ... --save_video"". Then I used blender and applpy convert2fbx.py,but  I was encountered with a problem that :
> 
  File ""/Text"", line 275, in process_poses
KeyError: 'bpy_prop_collection[key]: key ""f_avg_Pelvis"" not found'

"
Why is the betas  of BEV output in 11 dimensions instead of 10 dimensions? ,Arthur151/ROMP,2022-11-02 08:58:40,2,,362,1432742703,"Why is the betas  of BEV output in 11 dimensions instead of 10 dimensions? 
Which ten numbers represent the betas of SMPL? Thank you!"
Question about details for the BEV model training,Arthur151/ROMP,2022-11-02 06:56:24,0,,361,1432602117,"Hi thanks for the wonderful work!
I am trying to replicate this nice work, in my environment. Before diving in to reproduction I have a few questions. 

1. From the supplement material I know that BEV is trained with 4 V100 GPUs which is know to have 24 GB memory.
Since I can only utilize 3090 GPUs, that matches the required memory, I will have to use CUDA version 11.x. 
Your installation.md make me to install cuda 10.2.. so will it work in 3090 GPUs (with just small modification maybe)?

2. How long will it takes to train from scratch using 3090? or with V100 how long did it take?


Thanks! :)"
Missing parameters of layers,Arthur151/ROMP,2022-11-01 13:13:21,0,,360,1431410415,"Thanks so much for sharing the code and solving problems.
I've encounterd a similiar problem as #116 but not the same. I have already downloaded the fine tuned model ROMP_HRNet32_V1.pkl
but it still cannot process my images.


python -m romp.predict.image --inputs /root/autodl-tmp/data/my_video/output/images --output_dir /root/autodl-tmp/data/my_video/output/smpl_pred


No configs_yml is set, set it to the default --configs_yml=configs/image.yml
yaml_timestamp  /root/autodl-tmp/neuman/preprocess/ROMP/active_configs/active_context_2022-11-01_21_11_05.yaml
Loading the configurations from configs/image.yml
INFO:root:{'tab': 'hrnet_cm64_process_images', 'configs_yml': 'configs/image.yml', 'inputs': '/root/autodl-tmp/data/my_video/output/images', 'output_dir': '/root/autodl-tmp/data/my_video/output/smpl_pred', 'interactive_vis': False, 'show_largest_person_only': False, 'show_mesh_stand_on_image': False, 'soi_camera': 'far', 'make_tracking': False, 'temporal_optimization': False, 'save_dict_results': True, 'save_visualization_on_img': True, 'fps_save': 24, 'character': 'smpl', 'renderer': 'pytorch3d', 'f': None, 'model_return_loss': False, 'model_version': 1, 'multi_person': True, 'new_training': False, 'perspective_proj': False, 'FOV': 60, 'focal_length': 443.4, 'lr': 0.0003, 'adjust_lr_factor': 0.1, 'weight_decay': 1e-06, 'epoch': 120, 'fine_tune': True, 'GPUS': 0, 'batch_size': 64, 'input_size': 512, 'master_batch_size': -1, 'nw': 4, 'optimizer_type': 'Adam', 'pretrain': 'simplebaseline', 'fix_backbone_training_scratch': False, 'backbone': 'hrnet', 'model_precision': 'fp32', 'deconv_num': 0, 'head_block_num': 2, 'merge_smpl_camera_head': False, 'use_coordmaps': True, 'hrnet_pretrain': '/root/autodl-tmp/neuman/preprocess/ROMP/trained_models/pretrain_hrnet.pkl', 'resnet_pretrain': '/root/autodl-tmp/neuman/preprocess/ROMP/trained_models/pretrain_resnet.pkl', 'loss_thresh': 1000, 'max_supervise_num': -1, 'supervise_cam_params': False, 'match_preds_to_gts_for_supervision': False, 'matching_mode': 'all', 'supervise_global_rot': False, 'HMloss_type': 'MSE', 'eval': False, 'eval_datasets': 'pw3d', 'val_batch_size': 4, 'test_interval': 2000, 'fast_eval_iter': -1, 'top_n_error_vis': 6, 'eval_2dpose': False, 'calc_pck': False, 'PCK_thresh': 150, 'calc_PVE_error': False, 'centermap_size': 64, 'centermap_conf_thresh': 0.25, 'collision_aware_centermap': False, 'collision_factor': 0.2, 'center_def_kp': True, 'local_rank': 0, 'distributed_training': False, 'distillation_learning': False, 'teacher_model_path': '/export/home/suny/CenterMesh/trained_models/3dpw_88_57.8.pkl', 'print_freq': 50, 'model_path': 'trained_models/ROMP_HRNet32_V1.pkl', 'log_path': '/root/autodl-tmp/neuman/preprocess/log/', 'learn_2dpose': False, 'learn_AE': False, 'learn_kp2doffset': False, 'shuffle_crop_mode': False, 'shuffle_crop_ratio_3d': 0.9, 'shuffle_crop_ratio_2d': 0.1, 'Synthetic_occlusion_ratio': 0, 'color_jittering_ratio': 0.2, 'rotate_prob': 0.2, 'dataset_rootdir': '/root/autodl-tmp/neuman/preprocess/dataset/', 'dataset': 'h36m,mpii,coco,aich,up,ochuman,lsp,movi', 'voc_dir': '/root/autodl-tmp/neuman/preprocess/dataset/VOCdevkit/VOC2012/', 'max_person': 64, 'homogenize_pose_space': False, 'use_eft': True, 'smpl_mesh_root_align': False, 'Rot_type': '6D', 'rot_dim': 6, 'cam_dim': 3, 'beta_dim': 10, 'smpl_joint_num': 22, 'smpl_model_path': '/root/autodl-tmp/neuman/preprocess/ROMP/model_data/parameters', 'smpl_J_reg_h37m_path': '/root/autodl-tmp/neuman/preprocess/ROMP/model_data/parameters/J_regressor_h36m.npy', 'smpl_J_reg_extra_path': '/root/autodl-tmp/neuman/preprocess/ROMP/model_data/parameters/J_regressor_extra.npy', 'smpl_uvmap': '/root/autodl-tmp/neuman/preprocess/ROMP/model_data/parameters/smpl_vt_ft.npz', 'wardrobe': '/root/autodl-tmp/neuman/preprocess/ROMP/model_data/wardrobe', 'mesh_cloth': 'ghostwhite', 'nvxia_model_path': '/root/autodl-tmp/neuman/preprocess/ROMP/model_data/characters/nvxia', 'track_memory_usage': False, 'adjust_lr_epoch': [], 'kernel_sizes': [5], 'collect_subdirs': False, 'save_mesh': True, 'save_centermap': False}
INFO:root:------------------------------------------------------------------
visualize in gpu mode
INFO:root:start building model.
Using ROMP v1
Confidence: 0.25
INFO:root:using fine_tune model: trained_models/ROMP_HRNet32_V1.pkl
INFO:root:missing parameters of layers:['_result_parser.params_map_parser.smpl_model.betas', '_result_parser.params_map_parser.smpl_model.faces_tensor', '_result_parser.params_map_parser.smpl_model.v_template', '_result_parser.params_map_parser.smpl_model.shapedirs', '_result_parser.params_map_parser.smpl_model.J_regressor', '_result_parser.params_map_parser.smpl_model.J_regressor_extra9', '_result_parser.params_map_parser.smpl_model.J_regressor_h36m17', '_result_parser.params_map_parser.smpl_model.posedirs', '_result_parser.params_map_parser.smpl_model.parents', '_result_parser.params_map_parser.smpl_model.lbs_weights', '_result_parser.params_map_parser.smpl_model.vertex_joint_selector.extra_joints_idxs']
INFO:root:Train all layers, except: ['_result_parser.params_map_parser.smpl_model.betas']
visualize in gpu mode
Initialization finished!
Processing /root/autodl-tmp/data/my_video/output/images, saving to /root/autodl-tmp/data/my_video/output/smpl_pred
INFO:root:gathering datasets
Loading 23 images to process
Processed 0 / 23 images


Then it terminates. I really have no idea what to do. Could you please help me out of the situation?"
Bug of the usage of CUDA,Arthur151/ROMP,2022-11-01 05:22:55,0,,359,1430891415,"作者你好，我训练到一半，会报错如下：
python: /opt/conda/conda-bld/magma-cuda113_1619629459349/work/interface_cuda/interface.cpp:899: void magma_queue_create_from_cuda_internal(magma_device_t, cudaStream_t, cublasHandle_t, cusparseHandle_t, magma_queue**, const char*, const char*, int): Assertion `queue->dBarray__ != __null' failed.
报错阶段很随机，有时候训练开始不不久就报错，有时候训练了一段时间才报错，
请问这个可能是什么原因造成的呢？
@Arthur151 "
Something wrong with Exported FBX in UE,Arthur151/ROMP,2022-11-01 03:49:33,0,,358,1430821201,"Hello, thank you for your excellent job. I find that the animation sequence imported into UE is lying down, which has 90 degrees from the correct result. How can I fix this problem? Thank you."
Training questions,Arthur151/ROMP,2022-10-10 16:43:24,1,,353,1403451329,"I'm trying to reproduce BEV training results, and I have a few questions.
1. The source tree references a ""posetrack"" dataset. It is not mentioned in the paper, its official web site is dead (as in, the domain name does not even resolve), and last activity in its maintainers' Twitter account was two years ago. For now, I've deleted it from the configs, but is there any other place where I can download it?
2. The paper says that training was done on 4x V100; was that 16 GB or 32 GB? I am currently training on 2x 1080Ti (12 GB each), and I can only go up to batch size 18 without running out of GPU memory. I assume that you can do 64 because you have twice as many GPUs (so, your 64 is 16 per GPU)? Should I adjust the learning rate to compensate?
3. The paper describes a two-step training strategy: ""We first learn monocular 3D pose and shape estimation for 120 epochs on basic training datasets. Then we add the weak annotations of RH to training samples and train for 120 epochs."" Does the v6_train.sh script correspond to the first step or the second step?
4. How can I tell if it is training correctly? What kind of ""Losses"" should I see after one or two epochs? How long did it take to train?
5. Do I understand correctly that the model assumes that all humans are seen at fairly low field-of-view angles (the paper mentions 60 degrees)? I tried the pretrained checkpoint on some wide-angle photos with 90 degree FOV and the results weren't very satisfactory.
6. Is the ""SMIL infant template"" the same one that's available for download on the AGORA web site? (I've managed to import that one into Blender and to have a look at it, but I can't figure out how to do the same with your SMPLA_NEUTRAL.pth.) If it is, are you aware of its defects? Most notably, its hands are clenched into fists rather than flat, lips and eyeballs are messed up, and feet are way too small (I'd say they need to be about 50% larger.)"
Generate Annotations for a New Dataset,Arthur151/ROMP,2022-09-30 22:45:20,1,,351,1393096041,"Hello,

Looking at the datasets [here](https://drive.google.com/drive/folders/1oZkSrBjYUMthQBJBiSPE_J8IWea1LTH4), seems like you have a customized annotation for each dataset so that it can be used for your code. If I want to annotate another dataset to be run with your code, is there any guidance or code that I can look into?

I would appreciate it if you could guide me with this.
Thanks,
"
"some questions about transform function, especial batch_orth_proj",Arthur151/ROMP,2022-09-28 09:30:12,1,,350,1389035941,"batch_orth_proj(j3d_preds, cam_preds) as i understand, j3d_preds is world coords,the reason is cv2. solvePnPRansac to get world translate vec, cam_preds is the camara info  of Camera space from other closed issue， from convert_cam_to_3d_trans2() we can know the pj3d is [-1, -1](when mode=""2d""), need View transform . but cam_preds =(s, tx, ty), and in function convert_cam_to_3d_trans, depth, dx, dy = 1.0 / s, tx / s, ty / s which is trans3d, and is outputs['cam_trans'], although it can replace by trans caculate from cv2. solvePnPRansac.why depth, dx, dy is trans(transform by convert_cam_to_3d_trans as input cam from  output of romp), and batch_orth_proj(j3d_preds, cam_preds, mode=""2d"") can do projection transform? 
1. what the means of cam_preds from romp 
2. function batch_orth_proj in poset_parser.py is Camera transform and Orth Projection transform? or its meanning
3. convert_cam_to_3d_trans can transform romp's cam to tranlate vec as world trans vec? or its meanning
4. thx."
could you public the codes of evaluating BEV on 3dpw/human3.6 datasets? ,Arthur151/ROMP,2022-09-28 07:21:48,0,,349,1388860159,"In your supply materials, you give the results on MuPoTS and 3dpw dataset.
could you public the codes of evaluating BEV on 3dpw/human3.6 datasets? "
"Details about the format of ""results"" and ""sequence_results"" in the output npz file?",Arthur151/ROMP,2022-09-25 20:09:55,2,,348,1385149381,Where can I get the details about it? It seems to be very complex ndarray
Animation data and trans3d,Arthur151/ROMP,2022-09-16 11:19:00,3,,347,1375806128,"Hey Arthur151, here is a question to ask about the extraction of animation data and trans3d.

By following the Simple_ROMP guide, the code works amazingly well: romp --mode=webcam --show
I would like to extract the location of bones/poses and the translation info like the webcam.py part:
                trans = np.array([convert_cam_to_3d_trans(cam) for cam in cams])
                poses = np.array([result['poses'] for result in results[frame_id]])
How to print the trans and poses, and which file to edit? 
(After installation: python setup.py install, the romp command is in the virtual environment /romp/bin/romp, should I edit something and reinstall romp or directly edit the file to print the trans and poses)
"
Mixamo charactor retarget problem,Arthur151/ROMP,2022-08-17 06:06:21,1,,338,1341213591,"Hi, I using `convert_fbx.py` to convert model outupiut to fbx animation, the SMPL model works OK, but using other mixamo characters the result looks not right.

I have tried many differenct mixamo models, they have same problems:

![image](https://user-images.githubusercontent.com/21303438/185046442-d9866a68-6ad7-41e2-8c20-2afabe51bf79.png)
![image](https://user-images.githubusercontent.com/21303438/185046463-96833ea8-7952-4b46-8e03-3e9ffa896020.png)
"
"Low performance, but low hardware utilization",Arthur151/ROMP,2022-08-15 14:10:52,4,,337,1339054116,"## Problem
**Simple ROMP** has very poor performance on my machine:
 - around 10 FPS (standalone: `romp --mode=webcam --show -t`)
 - around 7 FPS (as an module: `from romp import ROMP`)

But my hardware utilization on my GPU with CUDA is still low:
![Utilization](https://user-images.githubusercontent.com/1502082/184651064-7cc17eda-e11b-44eb-af41-71a11814cf15.png)

## Steps to reproduce
```
conda create -n romp python=3.10
conda activate romp
pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116
pip install simple-romp cython
romp --mode=webcam --show -t
```

"
[Bug] ONNX not working with CUDA,Arthur151/ROMP,2022-08-15 13:08:00,3,,336,1338981904,"__Steps to reproduce__
```
conda create -n romp python=3.10
conda activate romp
pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113
pip install simple-romp cython
romp --mode=webcam --show -t --onnx
```

__Error message__
```
RuntimeError: ... onnxruntime::ProviderLibrary::Get [ONNXRuntimeError] : 1 : FAIL : LoadLibrary failed with error 126 """" when trying to load ""...\onnxruntime_providers_tensorrt.dll""
```
But the file `D:\miniconda3\envs\romp\lib\site-packages\onnxruntime\capi\onnxruntime_providers_tensorrt.dll` exists. 🤔 

__Full error message__
```
To use onnx model, we need to install the onnxruntime python package. Please install it by youself if failed!
Collecting onnxruntime-gpu
  Downloading onnxruntime_gpu-1.12.1-cp310-cp310-win_amd64.whl (110.8 MB)
     ---------------------------------------- 110.8/110.8 MB 4.2 MB/s eta 0:00:00
Collecting coloredlogs
  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)
     ---------------------------------------- 46.0/46.0 kB 1.2 MB/s eta 0:00:00
Collecting sympy
  Downloading sympy-1.10.1-py3-none-any.whl (6.4 MB)
     ---------------------------------------- 6.4/6.4 MB 8.9 MB/s eta 0:00:00
Collecting protobuf
  Downloading protobuf-4.21.5-cp310-abi3-win_amd64.whl (525 kB)
     ---------------------------------------- 525.5/525.5 kB 5.5 MB/s eta 0:00:00
Collecting flatbuffers
  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)
Requirement already satisfied: numpy>=1.21.0 in d:\miniconda3\envs\romp\lib\site-packages (from onnxruntime-gpu) (1.23.1)
Requirement already satisfied: packaging in d:\miniconda3\envs\romp\lib\site-packages (from onnxruntime-gpu) (21.3)
Collecting humanfriendly>=9.1
  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)
     ---------------------------------------- 86.8/86.8 kB 981.5 kB/s eta 0:00:00
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\miniconda3\envs\romp\lib\site-packages (from packaging->onnxruntime-gpu) (3.0.9)
Collecting mpmath>=0.19
  Downloading mpmath-1.2.1-py3-none-any.whl (532 kB)
     ---------------------------------------- 532.6/532.6 kB 4.2 MB/s eta 0:00:00
Collecting pyreadline3
  Downloading pyreadline3-3.4.1-py3-none-any.whl (95 kB)
     ---------------------------------------- 95.2/95.2 kB 2.7 MB/s eta 0:00:00
Installing collected packages: pyreadline3, mpmath, flatbuffers, sympy, protobuf, humanfriendly, coloredlogs, onnxruntime-gpu
Successfully installed coloredlogs-15.0.1 flatbuffers-2.0 humanfriendly-10.0 mpmath-1.2.1 onnxruntime-gpu-1.12.1 protobuf-4.21.5 pyreadline3-3.4.1 sympy-1.10.1
creating onnx model
Traceback (most recent call last):
  File ""D:\miniconda3\envs\romp\lib\runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""D:\miniconda3\envs\romp\lib\runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""D:\miniconda3\envs\romp\Scripts\romp.exe\__main__.py"", line 7, in <module>
  File ""D:\miniconda3\envs\romp\lib\site-packages\romp\main.py"", line 177, in main
    romp = ROMP(args)
  File ""D:\miniconda3\envs\romp\lib\site-packages\romp\main.py"", line 67, in __init__
    self._build_model_()
  File ""D:\miniconda3\envs\romp\lib\site-packages\romp\main.py"", line 87, in _build_model_
    self.ort_session = onnxruntime.InferenceSession(self.settings.model_onnx_path,\
  File ""D:\miniconda3\envs\romp\lib\site-packages\onnxruntime\capi\onnxruntime_inference_collection.py"", line 347, in __init__
    self._create_inference_session(providers, provider_options, disabled_optimizers)
  File ""D:\miniconda3\envs\romp\lib\site-packages\onnxruntime\capi\onnxruntime_inference_collection.py"", line 395, in _create_inference_session
    sess.initialize_session(providers, provider_options, disabled_optimizers)
RuntimeError: D:\a\_work\1\s\onnxruntime\core\session\provider_bridge_ort.cc:1029 onnxruntime::ProviderLibrary::Get [ONNXRuntimeError] : 1 : FAIL : LoadLibrary failed with error 126 """" when trying to load ""D:\miniconda3\envs\romp\lib\site-packages\onnxruntime\capi\onnxruntime_providers_tensorrt.dll""
```

__System__
 - OS: Windows 10
 - Cuda:
   ```
   ...
   Built on Fri_Dec_17_18:28:54_Pacific_Standard_Time_2021
   Cuda compilation tools, release 11.6, V11.6.55
   Build cuda_11.6.r11.6/compiler.30794723_0
   ```"
Some questions regarding pretraining,Arthur151/ROMP,2022-08-11 22:06:29,2,,334,1336545178,"Hello,

I have several questions regarding the pretraining phase. I would like to use pre-trained `hrnet` or `resnet50` models. 
1. the `pretrain_hrnet.pkl` mentioned in many previous issues is not in the repository. Could you please tell me where I can find it? I believe this is made by you. 
2. Does `pretrain_hrnet.pkl` contain backbone parameters only? 
3. The `pretrain_hrnet.pkl` path should be provided to hrnet_pretrain parameter in `v1.yml`. Is that correct? I am confused about this because in [this](https://github.com/Arthur151/ROMP/issues/161#issuecomment-1046808494) issue you mention `resnet_pretrain` should be set while in the [other one](https://github.com/Arthur151/ROMP/issues/122#issuecomment-1019880961) you mention `model_path` should be set. `model_path` is for trained ROMP model. Is not it? So for pertaining either `hrnet_pretrain` or `resnet_pretrain` should be set.
4. Also in the `val_result` function you have the following line:
```
    if self.backbone == 'resnet':
        eval_model.train()
    else:
        eval_model.eval()
```
Why do you train the model during evaluation for resnet and not for hrnet?


I would really appreciate it if you could help me with this questions."
 question about the difference between the intrinsics and extrinsics calibrated and estimated by ROMP,Arthur151/ROMP,2022-07-29 14:50:42,3,,324,1322348370,"Hello, I use ROMP to estimate the SMPL and use it to render novel view pictures. But I found that if using the intrinsics and extrinsics estimated by ROMP can correctly generate the bounding box in 2D picture, but using intrinsics and extrinsics calibrated by the camera ourselves will be wrong, which is really inexplicable. Why can't I use the  intrinsics and extrinsics calibrated by the camera? Do you know the possible reason? Thank you very much!"
一些关于深度预测的方法的询问,Arthur151/ROMP,2022-07-28 12:59:21,1,,323,1320895406,"您好，我在翻一些数据集(比如coco)的数据处理的时候发现您好像没有传入相机内参。这让我有点疑惑，想确认一下这是否是我看错了。
如果您在预测时确实没用内参，那么我想询问一下就是照片上相同大小的人，在不同内参下，深度是不一样的。但BEV似乎是可以直接预测深度的，请问您是怎么解决这件事情的？"
如何更改romp姿态参数提取模型？,Arthur151/ROMP,2022-07-25 02:18:08,1,,319,1316114544,您好，运行`romp --mode=video --calc_smpl -i=path/to/input -o=path/to/output`，使用的是ROMPv1模型，但是我想要更换为ROMP_HRNet32_V1.pkl模型，请问从哪里下载？又在哪里更改代码呢？
[simple-romp] Dependency issues & questions,Arthur151/ROMP,2022-07-19 20:48:05,4,,315,1310031269,"I have..
## Some dependency questions
 1. [For CPU rendering you recommend to use the `--onnx` parameter](https://github.com/Arthur151/ROMP/blob/master/simple_romp/README.md#usage), wich [installs `onnxruntime`](https://github.com/Arthur151/ROMP/blob/bafc86897c387caae125e7119b31dc30ee317bf0/simple_romp/romp/main.py#L82). Why you not also recommend this [for GPU rendering with `onnxruntime-gpu`](https://github.com/Arthur151/ROMP/blob/bafc86897c387caae125e7119b31dc30ee317bf0/simple_romp/romp/main.py#L84)?


and..
## Some `pip` issues
 1. `pip install simple-romp` requires also `numpy` and `cython` but `pip` does not resolve it properly, so i need to use `pip install simple-romp cython numpy` instead.
 2. If i am using the `-t` parameter for temporal optimization, [then it installs `norfair`](https://github.com/Arthur151/ROMP/blob/bafc86897c387caae125e7119b31dc30ee317bf0/simple_romp/romp/main.py#L122). `simple-romp` works as expected but i get the message:
```
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
simple-romp 1.0.5 requires typing-extensions>=4.1scipy, but you have typing-extensions 3.10.0.2 which is incompatible.
```

Full error message:
```
(romp) PS C:\Users\Vivien> romp --mode=webcam --show -t
Using ROMP v1
To perform temporal optimization, installing norfair for tracking.
Collecting norfair
  Using cached norfair-1.0.0-py3-none-any.whl (24 kB)
Collecting filterpy<2.0.0,>=1.4.5
  Using cached filterpy-1.4.5-py3-none-any.whl
Collecting rich<10.0.0,>=9.10.0
  Using cached rich-9.13.0-py3-none-any.whl (197 kB)
Collecting scipy
  Using cached scipy-1.8.1-cp310-cp310-win_amd64.whl (36.9 MB)
Requirement already satisfied: numpy in c:\tools\miniconda3\envs\romp\lib\site-packages (from filterpy<2.0.0,>=1.4.5->norfair) (1.23.1)
Collecting matplotlib
  Using cached matplotlib-3.5.2-cp310-cp310-win_amd64.whl (7.2 MB)
Collecting typing-extensions<4.0.0,>=3.7.4
  Using cached typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)
Collecting pygments<3.0.0,>=2.6.0
  Using cached Pygments-2.12.0-py3-none-any.whl (1.1 MB)
Collecting commonmark<0.10.0,>=0.9.0
  Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB)
Collecting colorama<0.5.0,>=0.4.0
  Using cached colorama-0.4.5-py2.py3-none-any.whl (16 kB)
Collecting kiwisolver>=1.0.1
  Using cached kiwisolver-1.4.4-cp310-cp310-win_amd64.whl (55 kB)
Collecting pyparsing>=2.2.1
  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)
Collecting fonttools>=4.22.0
  Using cached fonttools-4.34.4-py3-none-any.whl (944 kB)
Requirement already satisfied: pillow>=6.2.0 in c:\tools\miniconda3\envs\romp\lib\site-packages (from matplotlib->filterpy<2.0.0,>=1.4.5->norfair) (9.2.0)
Collecting cycler>=0.10
  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)
Collecting packaging>=20.0
  Using cached packaging-21.3-py3-none-any.whl (40 kB)
Collecting python-dateutil>=2.7
  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)
Collecting six>=1.5
  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)
Installing collected packages: typing-extensions, commonmark, six, scipy, pyparsing, pygments, kiwisolver, fonttools, cycler, colorama, rich, python-dateutil, packaging, matplotlib, filterpy, norfair
  Attempting uninstall: typing-extensions
    Found existing installation: typing_extensions 4.3.0
    Uninstalling typing_extensions-4.3.0:
      Successfully uninstalled typing_extensions-4.3.0
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
simple-romp 1.0.5 requires typing-extensions>=4.1scipy, but you have typing-extensions 3.10.0.2 which is incompatible.
Successfully installed colorama-0.4.5 commonmark-0.9.1 cycler-0.11.0 filterpy-1.4.5 fonttools-4.34.4 kiwisolver-1.4.4 matplotlib-3.5.2 norfair-1.0.0 packaging-21.3 pygments-2.12.0 pyparsing-3.0.9 python-dateutil-2.8.2 rich-9.13.0 scipy-1.8.1 six-1.16.0 typing-extensions-3.10.0.2
C:\tools\miniconda3\envs\romp\lib\site-packages\romp\post_parser.py:34: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  topk_ys = (topk_inds.long() // w).float()
C:\tools\miniconda3\envs\romp\lib\site-packages\romp\post_parser.py:39: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  topk_clses = index.long() // K
C:\tools\miniconda3\envs\romp\lib\site-packages\romp\post_parser.py:144: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  parsed_results['center_preds'] = torch.stack([flat_inds%64, flat_inds//64],1) * 512 // 64
ROMP 'forward' executed in 2.5916s, FPS 0.4
ROMP 'forward' executed in 0.1020s, FPS 9.8
...
```

__How to reproduce__
```
conda create -n romp python=3.10
conda activate romp
pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113
pip install simple-romp cython
romp --mode=webcam --show -t
```

__Installed packages__
```
(romp) PS C:\Users\Vivien> conda list
# packages in environment at C:\tools\miniconda3\envs\romp:
#
# Name                    Version                   Build  Channel
bzip2                     1.0.8                he774522_0
ca-certificates           2022.4.26            haa95532_0
certifi                   2022.6.15       py310haa95532_0
charset-normalizer        2.1.0                    pypi_0    pypi
colorama                  0.4.5                    pypi_0    pypi
commonmark                0.9.1                    pypi_0    pypi
cycler                    0.11.0                   pypi_0    pypi
cython                    0.29.30                  pypi_0    pypi
filterpy                  1.4.5                    pypi_0    pypi
fonttools                 4.34.4                   pypi_0    pypi
idna                      3.3                      pypi_0    pypi
kiwisolver                1.4.4                    pypi_0    pypi
lap                       0.4.0                    pypi_0    pypi
libffi                    3.4.2                hd77b12b_4
matplotlib                3.5.2                    pypi_0    pypi
norfair                   1.0.0                    pypi_0    pypi
numpy                     1.23.1                   pypi_0    pypi
opencv-python             4.6.0.66                 pypi_0    pypi
openssl                   1.1.1q               h2bbff1b_0
packaging                 21.3                     pypi_0    pypi
pillow                    9.2.0                    pypi_0    pypi
pip                       22.1.2          py310haa95532_0
pygments                  2.12.0                   pypi_0    pypi
pyparsing                 3.0.9                    pypi_0    pypi
python                    3.10.4               hbb2ffb3_0
python-dateutil           2.8.2                    pypi_0    pypi
requests                  2.28.1                   pypi_0    pypi
rich                      9.13.0                   pypi_0    pypi
scipy                     1.8.1                    pypi_0    pypi
setuptools                61.2.0          py310haa95532_0
simple-romp               1.0.5                    pypi_0    pypi
six                       1.16.0                   pypi_0    pypi
sqlite                    3.38.5               h2bbff1b_0
tk                        8.6.12               h2bbff1b_0
torch                     1.12.0+cu113             pypi_0    pypi
torchaudio                0.12.0+cu113             pypi_0    pypi
torchvision               0.13.0+cu113             pypi_0    pypi
typing-extensions         3.10.0.2                 pypi_0    pypi
tzdata                    2022a                hda174b7_0
urllib3                   1.26.10                  pypi_0    pypi
vc                        14.2                 h21ff451_1
vs2015_runtime            14.27.29016          h5e58377_2
wheel                     0.37.1             pyhd3eb1b0_0
wincertstore              0.2             py310haa95532_2
xz                        5.2.5                h8cc25b3_1
zlib                      1.2.12               h8cc25b3_2
```


and..
## Some `conda`, or to be more clear, `cython` issues
I get: `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf`.

Full error message:
```
(romp) PS C:\Users\Vivien> romp --mode=webcam --show
Using ROMP v1
Traceback (most recent call last):
  File ""__init__.pxd"", line 942, in numpy.import_array
RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\tools\miniconda3\envs\romp\lib\runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""C:\tools\miniconda3\envs\romp\lib\runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""C:\tools\miniconda3\envs\romp\Scripts\romp.exe\__main__.py"", line 7, in <module>
  File ""C:\tools\miniconda3\envs\romp\lib\site-packages\romp\main.py"", line 177, in main
    romp = ROMP(args)
  File ""C:\tools\miniconda3\envs\romp\lib\site-packages\romp\main.py"", line 68, in __init__
    self._initilization_()
  File ""C:\tools\miniconda3\envs\romp\lib\site-packages\romp\main.py"", line 102, in _initilization_
    self.renderer = setup_renderer(name=self.settings.renderer)
  File ""C:\tools\miniconda3\envs\romp\lib\site-packages\vis_human\main.py"", line 13, in setup_renderer
    from vis_human.sim3drender import Sim3DR
  File ""C:\tools\miniconda3\envs\romp\lib\site-packages\vis_human\sim3drender\__init__.py"", line 3, in <module>
    from .renderer import Sim3DR
  File ""C:\tools\miniconda3\envs\romp\lib\site-packages\vis_human\sim3drender\renderer.py"", line 4, in <module>
    import Sim3DR_Cython
  File ""vis_human\sim3drender\lib\rasterize.pyx"", line 10, in init Sim3DR_Cython
  File ""__init__.pxd"", line 944, in numpy.import_array
ImportError: numpy.core.multiarray failed to import
```

__How to reproduce__
```
conda create -n romp python=3.10
conda activate romp
conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch
pip install simple-romp cython
romp --mode=webcam --show
```

__Installed packages__
```
(romp) PS C:\Users\Vivien> conda list
# packages in environment at C:\tools\miniconda3\envs\romp:
#
# Name                    Version                   Build  Channel
blas                      1.0                         mkl
brotlipy                  0.7.0           py310h2bbff1b_1002
bzip2                     1.0.8                he774522_0
ca-certificates           2022.4.26            haa95532_0
certifi                   2022.6.15       py310haa95532_0
cffi                      1.15.0          py310h2bbff1b_1
charset-normalizer        2.0.4              pyhd3eb1b0_0
cryptography              37.0.1          py310h21b164f_0
cudatoolkit               11.3.1               h59b6b97_2
cython                    0.29.30                  pypi_0    pypi
freetype                  2.10.4               hd328e21_0
idna                      3.3                pyhd3eb1b0_0
intel-openmp              2021.4.0          haa95532_3556
jpeg                      9e                   h2bbff1b_0
lap                       0.4.0                    pypi_0    pypi
libffi                    3.4.2                hd77b12b_4
libpng                    1.6.37               h2a8f88b_0
libtiff                   4.2.0                he0120a3_1
libuv                     1.40.0               he774522_0
libwebp                   1.2.2                h2bbff1b_0
lz4-c                     1.9.3                h2bbff1b_1
mkl                       2021.4.0           haa95532_640
mkl-service               2.4.0           py310h2bbff1b_0
mkl_fft                   1.3.1           py310ha0764ea_0
mkl_random                1.2.2           py310h4ed8f06_0
numpy                     1.22.3          py310h6d2d95c_0
numpy-base                1.22.3          py310h206c741_0
opencv-python             4.6.0.66                 pypi_0    pypi
openssl                   1.1.1q               h2bbff1b_0
pillow                    9.2.0           py310hdc2b20a_1
pip                       22.1.2          py310haa95532_0
pycparser                 2.21               pyhd3eb1b0_0
pyopenssl                 22.0.0             pyhd3eb1b0_0
pysocks                   1.7.1           py310haa95532_0
python                    3.10.4               hbb2ffb3_0
pytorch                   1.12.0          py3.10_cuda11.3_cudnn8_0    pytorch
pytorch-mutex             1.0                        cuda    pytorch
requests                  2.28.1          py310haa95532_0
setuptools                61.2.0          py310haa95532_0
simple-romp               1.0.5                    pypi_0    pypi
six                       1.16.0             pyhd3eb1b0_1
sqlite                    3.38.5               h2bbff1b_0
tk                        8.6.12               h2bbff1b_0
torchaudio                0.12.0              py310_cu113    pytorch
torchvision               0.13.0              py310_cu113    pytorch
typing_extensions         4.1.1              pyh06a4308_0
tzdata                    2022a                hda174b7_0
urllib3                   1.26.9          py310haa95532_0
vc                        14.2                 h21ff451_1
vs2015_runtime            14.27.29016          h5e58377_2
wheel                     0.37.1             pyhd3eb1b0_0
win_inet_pton             1.1.0           py310haa95532_0
wincertstore              0.2             py310haa95532_2
xz                        5.2.5                h8cc25b3_1
zlib                      1.2.12               h8cc25b3_2
zstd                      1.5.2                h19a0ad4_0
```


## My system
 * Conda: `4.12.0`
 * OS: Windows 10 Enterprise `Version 21H2 (Build 19044.1826)`"
使用最新代码训练出错,Arthur151/ROMP,2022-07-19 16:17:51,1,,314,1309759118,"您好，我使用旧版代码是可以正常训练的，但是使用最新代码就会有问题

config如下：
[v1.yml.txt](https://github.com/Arthur151/ROMP/files/9142892/v1.yml.txt)


错误日志如下：
```
INFO:root:Training all layers.
Traceback (most recent call last):
  File ""D:\Software\AnacondaP\Install\envs\ROMP\lib\runpy.py"", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""D:\Software\AnacondaP\Install\envs\ROMP\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""E:\Git-Project\ROMP\ROMP-master\romp\train.py"", line 164, in <module>
    main()
  File ""E:\Git-Project\ROMP\ROMP-master\romp\train.py"", line 161, in main
    trainer.train()
  File ""E:\Git-Project\ROMP\ROMP-master\romp\train.py"", line 34, in train
    self.train_epoch(epoch)
  File ""E:\Git-Project\ROMP\ROMP-master\romp\train.py"", line 84, in train_epoch
    for iter_index, meta_data in enumerate(self.loader):
  File ""D:\Software\AnacondaP\Install\envs\ROMP\lib\site-packages\torch\utils\data\dataloader.py"", line 359, in __iter__
    return self._get_iterator()
  File ""D:\Software\AnacondaP\Install\envs\ROMP\lib\site-packages\torch\utils\data\dataloader.py"", line 305, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File ""D:\Software\AnacondaP\Install\envs\ROMP\lib\site-packages\torch\utils\data\dataloader.py"", line 918, in __init__
    w.start()
  File ""D:\Software\AnacondaP\Install\envs\ROMP\lib\multiprocessing\process.py"", line 121, in start
    self._popen = self._Popen(self)
  File ""D:\Software\AnacondaP\Install\envs\ROMP\lib\multiprocessing\context.py"", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File ""D:\Software\AnacondaP\Install\envs\ROMP\lib\multiprocessing\context.py"", line 327, in _Popen
    return Popen(process_obj)
  File ""D:\Software\AnacondaP\Install\envs\ROMP\lib\multiprocessing\popen_spawn_win32.py"", line 93, in __init__
    reduction.dump(process_obj, to_child)
  File ""D:\Software\AnacondaP\Install\envs\ROMP\lib\multiprocessing\reduction.py"", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
AttributeError: Can't pickle local object 'H36M.<locals>.H36M'

(ROMP) E:\Git-Project\ROMP\ROMP-master>Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""D:\Software\AnacondaP\Install\envs\ROMP\lib\multiprocessing\spawn.py"", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File ""D:\Software\AnacondaP\Install\envs\ROMP\lib\multiprocessing\spawn.py"", line 126, in _main
    self = reduction.pickle.load(from_parent)
EOFError: Ran out of input

```

很奇怪，麻烦大佬看一下为什么会这样~"
Evaluation on Crowdpose test/val set,Arthur151/ROMP,2022-07-19 15:33:51,1,,313,1309701207,"您好，我在做遮挡评估的时候遇到了一些问题

前提：我的模型加入了3dpw-train set训练而成

我评估了我训练的模型结果如下：
```
DONE (t=0.22s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.267
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.548
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.231
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.480
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.781
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.500
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=  easy | maxDets= 20 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.075
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=  hard | maxDets= 20 ] = -1.000
```
同时评估了ROMP_HRNet32_V1.pkl，结果如下：
```
DONE (t=0.18s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.002
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.004
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.020
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.001
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=  easy | maxDets= 20 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=  hard | maxDets= 20 ] = -1.000
```

这个结果和论文中的结果一点也不一样
![image](https://user-images.githubusercontent.com/31726551/179789844-89483e3b-38c5-4189-bfe7-7b2c56e7e537.png)

请问这个结果还需要再进行一步计算才能得到论文中的结果吗？
"
关于数据集处理中的减去root_trans。,Arthur151/ROMP,2022-07-18 16:57:29,1,,311,1308240947,"在之前一个版本的数据集代码中多数数据集有：
kp3ds -= root_trans[:,None]
就是所有的3d的坐标标签都是减去根节点之后的坐标。
这样处理的话，模型预测的坐标是否就不是世界坐标系下的坐标了，而是以根节点（左、右臀中点）为零点的坐标？

最新版本的数据集是不是因为这个原因，把root_trans也传入了模型？"
question about Gaussion kernel size and CAR useage during training ROMP,Arthur151/ROMP,2022-07-18 04:30:29,3,,309,1307391266,"作者大大你好
I have a question about the training codes. I have read through the codes and I couldn't find the methods of Gaussion kernel size and CAR calculation used for the training. Did I missed some part lol "
SMPL pkl not found in model data.,Arthur151/ROMP,2022-07-15 13:39:46,2,,308,1306051695,"### Question for dataset checking
Hi, Doc. Sun. After evaluation on BEV, now I want to train a model by myself from scratch. But when I run ` python -m romp.lib.dataset.h36m --configs_yml='configs/v1.yml'` to test my dataset installation, a bug pops out like: 

`AssertionError: Path /home/gpu/content/ROMP0/model_data/parameters/SMPL_FEMALE.pkl does not exist!`

I wonder where can I get the SMPL_FEMALE.**pkl**. I can only find SMPL_FEMALE.**pth** in the model_data.zip. Hope for your answer : -) 
"
Some models are shifting after temporal_optimize enabled,Arthur151/ROMP,2022-07-14 04:19:21,1,,306,1304232053,"I'm using bev to  infer a video. After adding -t -sc 5.0 when inferring a video, I got some models at the far end shifting randomly. Is it likely causing by the OneEuro algorithm or the calculation of cam_trans? 


https://user-images.githubusercontent.com/80388134/178896919-e5c6e395-df6c-4289-a4f7-70ebea611aee.mp4

"
关于3DPW评估中的三种协议,Arthur151/ROMP,2022-07-13 15:59:52,2,,304,1303642615,"大佬好，我在阅读论文中有些困惑，我对于3DPW评估中的三种协议是这样理解的

协议一：不用3dpw-train数据进行训练，对整个3dpw数据集进行评估
协议二：不用3dpw-train数据进行训练，对3dpw-test进行评估
协议三：使用3dpw-train数据进行训练，对3dpw-test进行评估

请问是这样吗？"
No such file or directory ,Arthur151/ROMP,2022-07-13 04:48:28,2,,303,1302899012,"No such file or directory

mpi_inf_3dhp/S1/Seq1/annot.mat

mpi_inf_3dhp in google drive has only annots.npz

How can i find annot.mat file ?? @Arthur151 "
issue with multi-person pose tracking,Arthur151/ROMP,2022-07-12 10:29:41,1,,302,1301863905,"Hi Yu,

we used your work to extract 3D motion from 2D videos. In general, it works quite well for single-person scenarios. Nice work! However, when I tried with multiperson cases, I have two issues:
1. regarding exporting motion to fbx/bvh. It seems to me that convert2fbx.py only exports the motion for one character. I looked into .npz file, and the 3D joint position data for all person are there. So this is not really critical since I still can extract 3D motion data.
2. another issue is a little bit more annoying to me. I noticed that the tracking result seems to be inconsistent for multi-person. You can see an example below. There are two consecutive frames. The estimated poses look fine for me. However, the colors are swapped. I assume the color is the character's identity. And the extract 3D motion is corresponding to the colored skeleton, which is flipped.
This might not be a big problem for 3D pose estimation in the video since every person in the scene still has an estimated pose attached. But for the 3D motion, the motion is a mix of two-person and not consistent in time, so you will see a sudden jump at some point in time. 

![00000012](https://user-images.githubusercontent.com/6459454/178456649-7bf20fa3-be73-4c27-81ba-301f9a9fce83.png)
![00000013](https://user-images.githubusercontent.com/6459454/178456894-5e282d17-a2e8-4dfb-b583-4ef20c3a7dee.png)
It would be great if you can help us with the mentioned issues. Thank you very much in advance.
"
To get cam_intrinsics and cam_extrinsics from .npz files,Arthur151/ROMP,2022-07-07 13:07:50,11,,300,1297411264,"Hi Yu, thanks for your work and such an organized repo! 

I'm now using ROMP to get SMPL poses and would like to visualize the meshes via a perspective camera. I usually use a similar way as https://github.com/chungyiweng/humannerf/issues/1 to convert the `s`, `t_x`, and `t_y` along with a human bbox to the pinhole camera parameters and it does work on VIBE output. However, it seems like I cannot easily get the parameters with the output from ROMP .npz outputs (I can get a rough bbox from `pj2d_org` ). I found that the scaling factor `s` is quite different between the VIBE and ROMP estimation for the same input image (~1.14 in VIBE and ~0.58 in ROMP). Could you please point out how I can quickly obtain (estimate) the camera intrinsic and extrinsic? Thanks! "
A bug in 3DPW evaluation,Arthur151/ROMP,2022-07-07 06:48:26,1,,299,1296916347,"Hi, I got another bug when I tried to evaluate the model.
Here is my command
```
CUDA_VISIBLE_DEVICES=0 python romp/lib/evaluation/collect_3DPW_results.py --configs_yml=configs/eval_3dpw_challenge.yml
```
Here is the bug report
```
Traceback (most recent call last):
  File ""romp/lib/evaluation/collect_3DPW_results.py"", line 208, in <module>
    submitor = Submit()
  File ""romp/lib/evaluation/collect_3DPW_results.py"", line 26, in __init__
    self.evaluation()
  File ""/mnt3/lijiahao/miniconda3/envs/romp/lib/python3.7/site-packages/torch/autograd/grad_mode.py"",
 line 28, in decorate_context
    return func(*args, **kwargs)
  File ""romp/lib/evaluation/collect_3DPW_results.py"", line 78, in evaluation
    kp3d_smpl = outputs['joints_smpl24']
KeyError: 'joints_smpl24'
```"
写入fbx的帧率问题,Arthur151/ROMP,2022-07-06 08:34:37,8,,297,1295457421,"作者你好，我使用.npz文件转.fbx的时候，设置了fps_source，fps_target这两个参数为30，但转换后的fbx结果帧率还是24，如图所示
![image](https://user-images.githubusercontent.com/55923060/177507155-d9cbc0a8-8548-4bd0-8c0d-a4d83857c7a8.png)
请问这是什么问题"
关于复现论文中的结果,Arthur151/ROMP,2022-07-06 06:08:33,7,,296,1295221230,"大佬您好，非常感谢您的工作~ 我在复现论文结果的过程中遇到了一些问题

我采用了v1.yml来进行训练，由于显卡原因（我的电脑一张3070显卡，8G显存）我的batch调小到了8，其他基本都是默认，以下是我的yml文件
[v1.yml.txt](https://github.com/Arthur151/ROMP/files/9052026/v1.yml.txt)

我训练了5个epoch之后，发现Loss虽然在下降，但是下降的很慢，下面是我的log

[v1_hrnet-0704-002.log](https://github.com/Arthur151/ROMP/files/9052034/v1_hrnet-0704-002.log)


我之前有尝试过将3DPW的数据集加入到训练之中，发现很快就达到了不错的结果，下面是加入了3DPW之后的log
[v1_hrnet-0704-001.log](https://github.com/Arthur151/ROMP/files/9052044/v1_hrnet-0704-001.log)


我想确认下我的训练是否有问题，因为看起来虽然下降的比较慢，但是还是在下降的
1、想请大佬帮忙看下日志，能否确认我的训练是有问题的状态？
2、大佬能否发下您的训练日志（使用了pretrian/pretrain_hrnet.pkl），然后我自己对比一下

PS：我没有使用最新的代码，因为看起来和代码并没有关系"
unstable 3D translation (especially along depth) for monocular pose estimation,Arthur151/ROMP,2022-06-29 12:27:39,3,,285,1288650216,"Hi, long time no see. Glad to see your BEV's contribution. I have been playing with ROMP+blender now and developing something interesting. But the translation along the depth direction is always shaking, we can only set the camera right in front of the character, otherwise, it's a bit ugly.

I have two questions now:

1. In ROMP, I remember that you said you determine the depth order by the scale. But I remember that you just transform add cam_trans to the verts then directly go into render process. I didn't see the code to ""determine the depth order"". I guess maybe you actually regard the translation recovery process: `verts += cam_trans` as the ""determine depth order"" process?

2. In BEV, do we have a more stable 3D translation now? Will it still shake along the depth direction? I haven't tried BEV with Blender yet. I'll try this later.
"
Why sample smpl / camera directly from parameter map?,Arthur151/ROMP,2022-06-29 11:29:01,3,,284,1288573746,"Hi,

Really impressive work with ROMP and BEV!

I was wondering generally around both implementations - why do you decide to create a dense parameter map for things like smpl rotations / body center translations and then sample from them based on the estimated body center indices instead of sampling from the final feature map based on your inferred centers and then predicting smpl rotations / centers based on the sampled feature vector?

Have you done any experimentation around this with respect to model training? Can you think of any advantages / disadvantages based on either of those methodologies?

Thank you and amazing work!
"
"[simple-romp, README] Using as an module in 'video' mode",Arthur151/ROMP,2022-06-24 01:26:32,2,,277,1283139945,"[According to the README](https://github.com/Arthur151/ROMP/tree/master/simple_romp#calling-as-python-lib) i need to use the following code for using it as an module in 'image' mode:
```python
#!/usr/bin/env python3

import romp

settings = romp.main.default_settings
romp_model = romp.ROMP(settings)
outputs = romp_model(cv2.imread('path/to/image.jpg'))
```
But if iam using it in the 'video' mode, i need to do the following:
```python
#!/usr/bin/env python3

import romp

settings = romp.main.default_settings
settings.mode = ""video""
romp_model = romp.ROMP(settings)
...
outputs = romp_model(frame_image)
```
According to here: https://github.com/Arthur151/ROMP/blob/4a5fecd140a3c54866c3f7e040bf0ac19d810190/simple_romp/romp/main.py#L189

My question:
If ROMP get's in both cases ('video' and 'image' mode) only an single image as input, then why i need to change the `mode` from `image` to `video`?
"
Loss function converges,Arthur151/ROMP,2022-06-22 07:05:42,1,,274,1279661346,"hello,The loss function converges relatively high, and I want to know how much the loss function converges to a better model."
运行romp/predict/webcam.py出错,Arthur151/ROMP,2022-06-10 16:01:02,2,,269,1267737887,"我想运行webcam和webcam_blender，用来在blender里驱动3D的模型(https://github.com/yanch2116/CharacterDriven-BlenderAddon)。

但是在运行webcam.py时报错了，报错信息如下：
TypeError: forward() missing 1 required positional argument: 'meta_data'，并且每次在print ""Reseting Mesh 5""之后出现这个错误。

我看到之前有一个issue说指定CUDA_VISIBLE_DEVICES，但这个方法在这个bug前是不起效的

同时，我尝试了predict/video和predict/image，他俩都是可以正常工作的。

不知道孙博你在平时使用的过程中是否遇到过类似的问题，或者你建议从哪些方面可以定位bug和进行改进？"
[simple-romp] argument '--show_largest' not working properly?,Arthur151/ROMP,2022-06-09 00:10:22,1,,267,1265436889,"Tested with the following command:
```ps
romp -t -sc=3 --onnx --show_largest --mode=video -i ""E:\Projekte\romp-test\test.mp4"" --calc_smpl --render_mesh --save_video -o ""E:\Projekte\romp-test\test_result.mp4""
```
And get the following result at frame `215` of [this video](https://www.pexels.com/video/man-texting-on-the-street-855574): 
![Frame 215](https://user-images.githubusercontent.com/1502082/172736818-cfda7c2c-6b6c-4954-bf30-cec21032bb8a.png)

Why does it evaluate not only the largest person (`--show_largest`), according to the [README](https://github.com/Arthur151/ROMP/blob/master/simple_romp/README.md#usage)?:
`[..] to show the largest person only (remove the small subjects in background) [..]`

The data:
```python
{
  'cam': array([
    [0.6815796, -0.7091072, 0.5703686],
    [1.1357857, -0.18335153, 0.77637017]
  ], dtype = float32),
  'global_orient': array([
    [2.991697, -0.11487218, 0.02521912],
    [3.1261547, 0.06689704, -0.28957558]
  ], dtype = float32),
  'body_pose': array([
    [-5.85280538e-01, -2.17623878e-02, 1.04836568e-01,
      -5.94840944e-01, 2.05031112e-02, -4.04515043e-02,
      4.90758419e-01, -1.56615092e-03, -1.22337053e-02,
      1.18982077e+00, 6.25830796e-03, -1.33860081e-01,
      1.03421855e+00, -4.33490239e-02, 9.75086764e-02,
      8.08686297e-03, -1.64354779e-02, -1.56924091e-02,
      -5.29559441e-02, 1.50208578e-01, -8.31045508e-02,
      -6.23615980e-02, -1.55124173e-01, 8.85685012e-02,
      3.80534641e-02, -1.49868811e-02, -1.34220216e-02,
      -2.74092108e-01, 9.23865065e-02, 1.74044952e-01,
      -2.23217666e-01, -2.57145371e-02, -1.95539817e-01,
      -1.50648043e-01, 2.01505367e-02, -2.51643322e-02,
      3.70577164e-02, -2.34452099e-01, -3.42204958e-01,
      2.55778432e-02, 2.65333027e-01, 2.82382011e-01,
      8.90259165e-03, 5.71487285e-03, 1.26959458e-02,
      9.14356261e-02, -3.04939717e-01, -9.25578892e-01,
      1.75735921e-01, 3.51879328e-01, 9.20472264e-01,
      6.75733909e-02, -5.05667686e-01, 6.31040707e-02,
      2.48549446e-01, 8.01332653e-01, -9.67753083e-02,
      7.34215826e-02, -6.02314919e-02, 1.28485382e-01,
      5.53916991e-02, 8.01604688e-02, -1.09875299e-01,
      0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
      0.00000000e+00, 0.00000000e+00, 0.00000000e+00
    ],
    [-4.36773539e-01, -4.78146598e-02, 1.86988581e-02,
      -4.45884168e-01, 6.41934425e-02, -3.56641151e-02,
      3.63900691e-01, -1.11775724e-02, 1.87774356e-02,
      9.40925658e-01, 1.05378553e-02, -1.03348374e-01,
      8.98670733e-01, 8.93227570e-03, 3.17471549e-02,
      -1.37289772e-02, -2.22832896e-02, 1.29298996e-02,
      -5.16718961e-02, 1.28186598e-01, -6.89635724e-02,
      -5.80723621e-02, -1.42094582e-01, 1.20365016e-01,
      2.41966341e-02, -1.15594221e-02, 1.08879823e-02,
      -2.70833313e-01, 1.29253507e-01, 1.61255777e-01,
      -2.09726378e-01, -1.71624217e-02, -2.16740996e-01,
      -1.97537243e-01, -1.50388693e-02, 1.52160926e-02,
      -9.75993183e-03, -2.94516653e-01, -2.75821269e-01,
      -4.38967049e-02, 3.35539162e-01, 1.97340801e-01,
      2.59927511e-02, 4.17219533e-04, 1.58897098e-02,
      8.48777741e-02, -4.73912209e-01, -9.98008311e-01,
      9.62874740e-02, 5.07905126e-01, 9.15271461e-01,
      1.01814926e-01, -7.08249331e-01, 1.66313887e-01,
      9.26086754e-02, 9.67943072e-01, -2.75810599e-01,
      -6.72952756e-02, -8.33046138e-02, 1.30613551e-01,
      -2.68768333e-02, 9.00805369e-02, -1.20457843e-01,
      0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
      0.00000000e+00, 0.00000000e+00, 0.00000000e+00
    ]
  ], dtype = float32),
  'smpl_betas': array([
    [-5.5625755e-03, -6.6001648e-03, 2.7066749e-04, 7.3520839e-04,
      1.0587471e-03, -1.6262289e-05, -1.6828703e-03, -1.9729475e-04,
      1.1870718e-03, -1.1305653e-03
    ],
    [9.1921221e-03, -2.7188463e-03, 1.5820982e-04, -3.2527829e-03,
      1.9372397e-05, -9.4282208e-05, -1.2378157e-03, 3.5660830e-04,
      7.2848942e-04, -5.3632428e-04
    ]
  ], dtype = float32),
  'smpl_thetas': array([
    [2.99169707e+00, -1.14872180e-01, 2.52191164e-02,
      -5.85280538e-01, -2.17623878e-02, 1.04836568e-01,
      -5.94840944e-01, 2.05031112e-02, -4.04515043e-02,
      4.90758419e-01, -1.56615092e-03, -1.22337053e-02,
      1.18982077e+00, 6.25830796e-03, -1.33860081e-01,
      1.03421855e+00, -4.33490239e-02, 9.75086764e-02,
      8.08686297e-03, -1.64354779e-02, -1.56924091e-02,
      -5.29559441e-02, 1.50208578e-01, -8.31045508e-02,
      -6.23615980e-02, -1.55124173e-01, 8.85685012e-02,
      3.80534641e-02, -1.49868811e-02, -1.34220216e-02,
      -2.74092108e-01, 9.23865065e-02, 1.74044952e-01,
      -2.23217666e-01, -2.57145371e-02, -1.95539817e-01,
      -1.50648043e-01, 2.01505367e-02, -2.51643322e-02,
      3.70577164e-02, -2.34452099e-01, -3.42204958e-01,
      2.55778432e-02, 2.65333027e-01, 2.82382011e-01,
      8.90259165e-03, 5.71487285e-03, 1.26959458e-02,
      9.14356261e-02, -3.04939717e-01, -9.25578892e-01,
      1.75735921e-01, 3.51879328e-01, 9.20472264e-01,
      6.75733909e-02, -5.05667686e-01, 6.31040707e-02,
      2.48549446e-01, 8.01332653e-01, -9.67753083e-02,
      7.34215826e-02, -6.02314919e-02, 1.28485382e-01,
      5.53916991e-02, 8.01604688e-02, -1.09875299e-01,
      0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
      0.00000000e+00, 0.00000000e+00, 0.00000000e+00
    ],
    [3.12615466e+00, 6.68970421e-02, -2.89575577e-01,
      -4.36773539e-01, -4.78146598e-02, 1.86988581e-02,
      -4.45884168e-01, 6.41934425e-02, -3.56641151e-02,
      3.63900691e-01, -1.11775724e-02, 1.87774356e-02,
      9.40925658e-01, 1.05378553e-02, -1.03348374e-01,
      8.98670733e-01, 8.93227570e-03, 3.17471549e-02,
      -1.37289772e-02, -2.22832896e-02, 1.29298996e-02,
      -5.16718961e-02, 1.28186598e-01, -6.89635724e-02,
      -5.80723621e-02, -1.42094582e-01, 1.20365016e-01,
      2.41966341e-02, -1.15594221e-02, 1.08879823e-02,
      -2.70833313e-01, 1.29253507e-01, 1.61255777e-01,
      -2.09726378e-01, -1.71624217e-02, -2.16740996e-01,
      -1.97537243e-01, -1.50388693e-02, 1.52160926e-02,
      -9.75993183e-03, -2.94516653e-01, -2.75821269e-01,
      -4.38967049e-02, 3.35539162e-01, 1.97340801e-01,
      2.59927511e-02, 4.17219533e-04, 1.58897098e-02,
      8.48777741e-02, -4.73912209e-01, -9.98008311e-01,
      9.62874740e-02, 5.07905126e-01, 9.15271461e-01,
      1.01814926e-01, -7.08249331e-01, 1.66313887e-01,
      9.26086754e-02, 9.67943072e-01, -2.75810599e-01,
      -6.72952756e-02, -8.33046138e-02, 1.30613551e-01,
      -2.68768333e-02, 9.00805369e-02, -1.20457843e-01,
      0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
      0.00000000e+00, 0.00000000e+00, 0.00000000e+00
    ]
  ], dtype = float32),
  'center_preds': array([
    [64, 320],
    [208, 256]
  ], dtype = int64),
  'center_confs': array([
    [0.8024817],
    [0.7580417]
  ], dtype = float32),
  'cam_trans': array([
    [-1.0660633, 0.82609326, 2.563384],
    [-0.17035195, 0.69707036, 1.6288631]
  ], dtype = float32),
  'verts': array([
      [
        [2.27561016e-02, -8.49465251e-01, -2.02246144e-01],
        [1.77022722e-02, -8.34393740e-01, -2.07562864e-01],
        [2.83376612e-02, -8.32082689e-01, -1.98540911e-01],
        ...,
        [-9.59926844e-02, -8.05583000e-01, -1.08467147e-01],
        [-9.61440280e-02, -8.07182789e-01, -1.10141560e-01],
        [-9.87054482e-02, -8.04640174e-01, -1.12478524e-01]
      ],

      [
        [-5.74013777e-03, -8.62683892e-01, -2.11462319e-01],
        [-1.25920828e-02, -8.48159194e-01, -2.15562344e-01],
        [3.53422482e-04, -8.45132947e-01, -2.10286155e-01],
        ...,
        [-9.34009403e-02, -8.15017402e-01, -8.92492980e-02],
        [-9.39570889e-02, -8.16764951e-01, -9.07215476e-02],
        [-9.71432328e-02, -8.14422727e-01, -9.23290998e-02]
      ]
    ],
    dtype = float32),
  'joints': array([
      [
        [-8.75359285e-04, -2.11405411e-01, 2.78547406e-02],
        [7.71344453e-02, -1.27239078e-01, 2.42792498e-02],
        [-6.28520697e-02, -1.17797144e-01, 1.80168450e-02],
        [-1.16209695e-02, -3.09760869e-01, 6.48679286e-02],
        [1.67023391e-01, 1.37000591e-01, -2.11569801e-01],
        [-8.47365186e-02, 1.62121654e-01, -2.26580769e-01],
        [-1.47305625e-02, -4.30535108e-01, 2.35851929e-02],
        [1.64994359e-01, 4.62181956e-01, -3.93877923e-03],
        [-2.52084620e-02, 5.12286067e-01, -7.54158944e-02],
        [-1.37441671e-02, -4.67082232e-01, -1.94460638e-02],
        [2.08720669e-01, 5.55598855e-01, -8.11662003e-02],
        [-5.12957349e-02, 5.89834034e-01, -1.74600944e-01],
        [-2.08159927e-02, -6.78509831e-01, -5.24114817e-02],
        [5.85397482e-02, -5.91862917e-01, -2.98147835e-02],
        [-9.37665179e-02, -5.87976933e-01, -2.48514041e-02],
        [-1.57599710e-02, -7.27229357e-01, -1.15754671e-01],
        [1.48860753e-01, -5.89739203e-01, -4.31195199e-02],
        [-1.87773302e-01, -5.85855842e-01, -4.17570584e-02],
        [2.08096951e-01, -3.50017488e-01, 1.17042586e-02],
        [-2.41771400e-01, -3.48148525e-01, -1.39592215e-03],
        [2.16475591e-01, -1.29206508e-01, -7.70033151e-02],
        [-1.79364145e-01, -1.50368407e-01, -1.32336810e-01],
        [2.23127067e-01, -5.13947457e-02, -9.96852368e-02],
        [-1.60347641e-01, -8.30583051e-02, -1.73942208e-01],
        [-1.70510653e-02, -7.58414805e-01, -2.23913938e-01],
        [-5.17625324e-02, -7.99668312e-01, -1.99502841e-01],
        [1.39427818e-02, -8.01895857e-01, -1.98612869e-01],
        [-9.27167684e-02, -7.96374202e-01, -1.06735304e-01],
        [4.98565473e-02, -8.01094651e-01, -1.04890868e-01],
        [2.02723354e-01, 5.74146807e-01, -1.53339222e-01],
        [2.50166506e-01, 5.60621142e-01, -9.33759212e-02],
        [1.61667734e-01, 4.69619781e-01, 6.36569262e-02],
        [-2.62610093e-02, 5.97724438e-01, -2.45288610e-01],
        [-8.77840370e-02, 6.06458247e-01, -1.91111773e-01],
        [-1.72716081e-02, 5.39703488e-01, -1.13741159e-02],
        [1.51403591e-01, -5.47422171e-02, -1.65926725e-01],
        [2.09864736e-01, 1.91850066e-02, -1.69591159e-01],
        [2.36379266e-01, 3.70456576e-02, -1.51955724e-01],
        [2.49117568e-01, 3.04618478e-02, -1.22832656e-01],
        [2.55384326e-01, 1.02766156e-02, -8.32607448e-02],
        [-7.05657899e-02, -1.23868883e-01, -2.01645881e-01],
        [-1.09749392e-01, -3.75478268e-02, -2.41809100e-01],
        [-1.37292266e-01, -1.17408037e-02, -2.37244278e-01],
        [-1.61259174e-01, -6.99681044e-03, -2.15106040e-01],
        [-1.87746868e-01, -1.43653750e-02, -1.77723676e-01],
        [-1.41412631e-01, -2.21176982e-01, 3.33458707e-02],
        [1.36064485e-01, -2.43871048e-01, 3.80166210e-02],
        [-1.81482788e-02, -6.73065543e-01, -6.46214262e-02],
        [-2.47125942e-02, -9.27237451e-01, -1.38407394e-01],
        [-2.91173905e-03, -2.29785115e-01, 5.74037433e-02],
        [-1.75861567e-02, -6.15533948e-01, -4.31892462e-02],
        [-1.86750144e-02, -4.61245537e-01, 3.85091230e-02],
        [-1.10631902e-02, -7.39810348e-01, -1.47367150e-01],
        [-2.42438503e-02, -8.68159294e-01, -1.16868481e-01],
        [-5.09126931e-02, 4.62709785e-01, -6.21346831e-02],
        [-8.70343596e-02, 1.12852998e-01, -2.04238355e-01],
        [-1.54232696e-01, -2.26036042e-01, 4.65241112e-02],
        [1.47722170e-01, -2.32770562e-01, 6.69552684e-02],
        [1.73710331e-01, 1.05205201e-01, -1.99201345e-01],
        [1.68997586e-01, 4.21945602e-01, 5.61075006e-03],
        [-1.62865728e-01, -1.81941822e-01, -1.37460947e-01],
        [-2.57157743e-01, -3.64012361e-01, -3.35582905e-02],
        [-1.58776015e-01, -6.15496516e-01, -4.24131751e-02],
        [1.24996789e-01, -6.21989846e-01, -3.98044065e-02],
        [2.30993152e-01, -3.69168103e-01, -2.47584917e-02],
        [2.16143608e-01, -1.45533621e-01, -8.91659111e-02],
        [-1.66408848e-02, -6.68344438e-01, -6.19635694e-02],
        [-1.91201922e-02, -8.48284483e-01, -1.23113543e-01],
        [-2.91173905e-03, -2.29785115e-01, 5.74037433e-02],
        [-1.86750144e-02, -4.61245537e-01, 3.85091230e-02],
        [-1.10631902e-02, -7.39810348e-01, -1.47367150e-01]
      ],

      [
        [-8.72004253e-04, -2.11494520e-01, 2.78658886e-02],
        [6.69146627e-02, -1.18896343e-01, 2.28809305e-02],
        [-7.15944245e-02, -1.25474513e-01, 4.54743356e-02],
        [5.13643166e-03, -3.14690709e-01, 4.99035120e-02],
        [5.85741661e-02, 2.13484466e-01, -1.29358888e-01],
        [-1.57629505e-01, 2.08044887e-01, -9.67456549e-02],
        [3.16202501e-03, -4.34743136e-01, 6.29164651e-03],
        [5.96898869e-02, 5.26029468e-01, 9.74046588e-02],
        [-1.38327852e-01, 5.35311222e-01, 1.07657224e-01],
        [-3.62423109e-03, -4.71504658e-01, -3.60684060e-02],
        [6.61572665e-02, 6.28144503e-01, 1.89573616e-02],
        [-1.87257305e-01, 6.22663975e-01, 2.68568695e-02],
        [-9.12475213e-03, -6.83830440e-01, -6.39995337e-02],
        [6.95043057e-02, -5.93567193e-01, -6.20615482e-02],
        [-7.72789195e-02, -5.95631480e-01, -2.07347907e-02],
        [-2.07917206e-02, -7.36753404e-01, -1.23018846e-01],
        [1.51381314e-01, -5.92458904e-01, -1.02582440e-01],
        [-1.72504127e-01, -6.03741705e-01, -2.22234093e-02],
        [1.79247081e-01, -3.40870231e-01, -1.00833274e-01],
        [-2.30828285e-01, -3.63753557e-01, -3.43577750e-02],
        [9.12324637e-02, -1.72965273e-01, -2.45193064e-01],
        [-2.11503327e-01, -2.54886925e-01, -2.53490299e-01],
        [6.40000105e-02, -1.10988609e-01, -2.90337294e-01],
        [-2.05894306e-01, -2.17146173e-01, -3.25428933e-01],
        [-5.27248047e-02, -7.73335636e-01, -2.24851474e-01],
        [-7.79257268e-02, -8.13711047e-01, -1.89176619e-01],
        [-1.47682410e-02, -8.15564036e-01, -2.07549661e-01],
        [-9.00610983e-02, -8.05614114e-01, -8.89883637e-02],
        [4.71771993e-02, -8.09178174e-01, -1.28230155e-01],
        [3.98829728e-02, 6.48788810e-01, -4.76828590e-02],
        [1.02863163e-01, 6.40633702e-01, -2.60376930e-03],
        [7.30167255e-02, 5.30748844e-01, 1.64137542e-01],
        [-1.72266304e-01, 6.46206439e-01, -4.30187732e-02],
        [-2.27713630e-01, 6.35894299e-01, 1.72304213e-02],
        [-1.26233533e-01, 5.54075599e-01, 1.74781561e-01],
        [-1.58951432e-02, -1.57982886e-01, -3.21915060e-01],
        [1.05489790e-02, -7.86837339e-02, -3.68011534e-01],
        [3.22467089e-02, -4.97187972e-02, -3.66411805e-01],
        [5.30117005e-02, -4.13036644e-02, -3.43233258e-01],
        [7.53686577e-02, -4.25913036e-02, -3.04793507e-01],
        [-1.29493281e-01, -2.79582411e-01, -3.54001462e-01],
        [-1.79006279e-01, -2.14428082e-01, -4.17458266e-01],
        [-2.03316197e-01, -1.85863614e-01, -4.18327212e-01],
        [-2.19058529e-01, -1.68442532e-01, -3.95724744e-01],
        [-2.32219815e-01, -1.54547781e-01, -3.54206979e-01],
        [-1.35218203e-01, -2.37014890e-01, 5.46305366e-02],
        [1.36564359e-01, -2.26552993e-01, 9.52230417e-04],
        [-1.07802376e-02, -6.80716157e-01, -7.64743090e-02],
        [-3.14050429e-02, -9.37717974e-01, -1.32277668e-01],
        [4.66703111e-03, -2.33509585e-01, 4.95351143e-02],
        [-7.48797646e-03, -6.23024404e-01, -6.16109520e-02],
        [4.62672906e-03, -4.64726567e-01, 1.98676772e-02],
        [-2.51746681e-02, -7.50786602e-01, -1.54403657e-01],
        [-2.62231641e-02, -8.77399325e-01, -1.14986442e-01],
        [-1.53047413e-01, 4.81023699e-01, 1.14571474e-01],
        [-1.50440589e-01, 1.53840095e-01, -8.83602947e-02],
        [-1.44449502e-01, -2.46057451e-01, 6.92632571e-02],
        [1.52617037e-01, -2.19550952e-01, 2.88612153e-02],
        [7.21402764e-02, 1.78554684e-01, -1.27464011e-01],
        [7.17887357e-02, 4.86620486e-01, 1.02428593e-01],
        [-1.96798086e-01, -2.87897229e-01, -2.47597411e-01],
        [-2.51997828e-01, -3.90448928e-01, -5.74505851e-02],
        [-1.42850280e-01, -6.29573107e-01, -2.87982225e-02],
        [1.29633889e-01, -6.23592734e-01, -9.25542116e-02],
        [1.90605044e-01, -3.66491824e-01, -1.40887275e-01],
        [9.23811197e-02, -1.93608642e-01, -2.50356674e-01],
        [-7.82386772e-03, -6.77114964e-01, -7.55808204e-02],
        [-2.35089418e-02, -8.58024240e-01, -1.23406678e-01],
        [4.66703111e-03, -2.33509585e-01, 4.95351143e-02],
        [4.62672906e-03, -4.64726567e-01, 1.98676772e-02],
        [-2.51746681e-02, -7.50786602e-01, -1.54403657e-01]
      ]
    ],
    dtype = float32),
  'pj2d_org': array([
    [
      [278.6843, 949.2278],
      [329.72742, 1004.2992],
      [238.13193, 1010.4773],
      [271.65326, 884.8723],
      [388.5432, 1177.1956],
      [223.81256, 1193.6327],
      [269.61862, 805.84766],
      [387.2156, 1389.9672],
      [262.76276, 1422.751],
      [270.264, 781.9343],
      [415.82645, 1451.0913],
      [245.69344, 1473.492],
      [265.63684, 643.59375],
      [317.5606, 700.2882],
      [217.90408, 702.83093],
      [268.94507, 611.7158],
      [376.65912, 701.67773],
      [156.39392, 704.2189],
      [415.4183, 858.5316],
      [121.06207, 859.7545],
      [420.9006, 1003.01196],
      [161.8962, 989.1653],
      [425.25278, 1053.9254],
      [174.33896, 1033.2074],
      [268.10028, 591.3106],
      [245.388, 564.31775],
      [288.38007, 562.8602],
      [218.59097, 566.47314],
      [311.87903, 563.3844],
      [411.90228, 1463.2275],
      [442.94513, 1454.3774],
      [385.03894, 1394.8339],
      [262.07404, 1478.6547],
      [221.81854, 1484.3694],
      [267.95596, 1440.6907],
      [378.3229, 1051.7351],
      [416.575, 1100.1069],
      [433.92392, 1111.7935],
      [442.25876, 1107.4855],
      [446.35922, 1094.278],
      [233.08473, 1006.5044],
      [207.44626, 1062.9856],
      [189.4245, 1079.8716],
      [173.74254, 1082.9757],
      [156.41121, 1078.1543],
      [186.72844, 942.8341],
      [368.28632, 927.985],
      [267.38232, 647.156],
      [263.0872, 480.84717],
      [277.35187, 937.20166],
      [267.75015, 684.7999],
      [267.0377, 785.7533],
      [272.01822, 603.4838],
      [263.3939, 519.503],
      [245.94406, 1390.3125],
      [222.30904, 1161.3954],
      [178.34009, 939.65466],
      [375.9141, 935.2483],
      [392.91858, 1156.3914],
      [389.83496, 1363.6398],
      [172.69135, 968.50635],
      [110.99453, 849.3745],
      [175.36731, 684.82446],
      [361.0445, 680.5757],
      [430.39972, 846.0011],
      [420.68338, 992.32886],
      [268.36865, 650.2451],
      [266.7464, 532.5074],
      [277.35187, 937.20166],
      [267.0377, 785.7533],
      [272.01822, 603.4838]
    ],

    [
      [783.03174, 1054.7114],
      [856.94324, 1155.6763],
      [705.91925, 1148.5037],
      [789.58307, 942.1909],
      [847.8491, 1518.089],
      [612.11053, 1512.158],
      [787.43024, 811.2914],
      [849.0657, 1858.874],
      [633.1562, 1868.9941],
      [780.0308, 771.20825],
      [856.11743, 1970.2153],
      [579.8058, 1964.2397],
      [774.0333, 539.69794],
      [859.76685, 638.1168],
      [699.7211, 635.86597],
      [761.3122, 481.99316],
      [949.0418, 639.3253],
      [595.8919, 627.02295],
      [979.4253, 913.646],
      [532.2979, 888.69507],
      [883.45825, 1096.7219],
      [553.369, 1007.3983],
      [853.7652, 1164.2985],
      [559.4848, 1048.5491],
      [726.49384, 442.10553],
      [699.01587, 398.08203],
      [767.8799, 396.06165],
      [685.784, 406.91058],
      [835.42236, 403.02448],
      [827.46906, 1992.7249],
      [896.1398, 1983.833],
      [863.5966, 1864.0195],
      [596.15125, 1989.9092],
      [535.694, 1978.6653],
      [646.34326, 1889.4541],
      [766.6512, 1113.0581],
      [795.4847, 1199.5222],
      [819.1429, 1231.1042],
      [841.78406, 1240.2798],
      [866.1611, 1238.8759],
      [642.789, 980.47144],
      [588.8023, 1051.5128],
      [562.29584, 1082.6582],
      [545.1311, 1101.6533],
      [530.78064, 1116.8035],
      [636.54675, 1026.8853],
      [932.8861, 1038.2924],
      [772.2283, 543.09357],
      [749.73987, 262.87054],
      [789.0713, 1030.7072],
      [775.818, 605.99805],
      [789.02734, 778.59875],
      [756.5332, 466.69202],
      [755.39, 328.63922],
      [617.1066, 1809.8018],
      [619.949, 1453.0555],
      [626.4814, 1017.0255],
      [950.38916, 1045.927],
      [862.641, 1480.0033],
      [862.2577, 1815.904],
      [569.4029, 971.4054],
      [509.21564, 859.58765],
      [628.22516, 598.85767],
      [925.3294, 605.3784],
      [991.8096, 885.7095],
      [884.7107, 1074.2134],
      [775.4518, 547.02014],
      [758.3494, 349.76495],
      [789.0713, 1030.7072],
      [789.02734, 778.59875],
      [756.5332, 466.69202]
    ]
  ], dtype = float32)
}
```
Exported according to the [README](https://github.com/Arthur151/ROMP/blob/master/simple_romp/README.md#how-to-load-the-results-saved-in-npz-file) by:
```python
import numpy as np
results = np.load('E:/Projekte/romp-test/00000215.npz', allow_pickle=True)['results'][()]
print(results)
```

My `pip freeze` output:
```ps
black==22.3.0
certifi==2021.10.8
click==8.1.2
colorama==0.4.4
Cython==0.29.30
lap==0.4.0
mypy-extensions==0.4.3
numpy==1.22.3
opencv-python==4.5.5.64
pathspec==0.9.0
platformdirs==2.5.1
simple-romp==1.0.5
tomli==2.0.1
torch==1.11.0
typing_extensions==4.1.1
wincertstore==0.2
```
System:
 - Python: v`3.9.12`
 - OS: Windows 10 Enterprise (64-bit, Version `21H2`, Build `19044.1706`)
"
[simple-romp] Error instead of usage hints on invocation without arguments,Arthur151/ROMP,2022-06-08 23:14:13,1,,266,1265406724,"Invocation without any arguments:
```ps
(sromp) PS C:\Users\Vivien\Projekte> romp
Using ROMP v1
[ WARN:0@3.340] global D:\a\opencv-python\opencv-python\opencv\modules\imgcodecs\src\loadsave.cpp (239) cv::findDecoder imread_(''): can't open/read file: check file path/integrity
Traceback (most recent call last):
  File ""C:\tools\miniconda3\envs\sromp\lib\runpy.py"", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""C:\tools\miniconda3\envs\sromp\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""C:\tools\miniconda3\envs\sromp\Scripts\romp.exe\__main__.py"", line 7, in <module>
  File ""C:\tools\miniconda3\envs\sromp\lib\site-packages\romp\main.py"", line 181, in main
    outputs = romp(image)
  File ""C:\tools\miniconda3\envs\sromp\lib\site-packages\torch\nn\modules\module.py"", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File ""C:\tools\miniconda3\envs\sromp\lib\site-packages\romp\utils.py"", line 728, in wrap_func        
    result = func(*args, **kwargs)
  File ""C:\tools\miniconda3\envs\sromp\lib\site-packages\romp\main.py"", line 158, in forward
    outputs, image_pad_info = self.single_image_forward(image)
  File ""C:\tools\miniconda3\envs\sromp\lib\site-packages\romp\main.py"", line 105, in single_image_forward
    input_image, image_pad_info = img_preprocess(image)
  File ""C:\tools\miniconda3\envs\sromp\lib\site-packages\romp\utils.py"", line 27, in img_preprocess    
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
cv2.error: OpenCV(4.5.5) D:\a\opencv-python\opencv-python\opencv\modules\imgproc\src\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'
```
Invocation with explicit `--help` argument:
```ps
(sromp) PS C:\Users\Vivien\Projekte> romp --help 
usage: romp [-h] [-m MODE] [-i INPUT] [-o SAVE_PATH] [--GPU GPU] [--onnx] [-t]
            [--center_thresh CENTER_THRESH] [--show_largest] [-sc SMOOTH_COEFF] [--calc_smpl]
            [--render_mesh] [--renderer RENDERER] [--show] [--show_items SHOW_ITEMS] [--save_video]    
            [--frame_rate FRAME_RATE] [--smpl_path SMPL_PATH] [--model_path MODEL_PATH]
            [--model_onnx_path MODEL_ONNX_PATH] [--root_align ROOT_ALIGN]

ROMP: Monocular, One-stage, Regression of Multiple 3D People

optional arguments:
  -h, --help            show this help message and exit
  -m MODE, --mode MODE  Inferece mode, including image, video, webcam
  -i INPUT, --input INPUT
                        Path to the input image / video
  -o SAVE_PATH, --save_path SAVE_PATH
                        Path to save the results
  --GPU GPU             The gpu device number to run the inference on. If GPU=-1, then running in cpu  
                        mode
  --onnx                Whether to use ONNX for acceleration.
  -t, --temporal_optimize
                        Whether to use OneEuro filter to smooth the results
  --center_thresh CENTER_THRESH
                        The confidence threshold of positive detection in 2D human body center
                        heatmap.
  --show_largest        Whether to show the largest person only
  -sc SMOOTH_COEFF, --smooth_coeff SMOOTH_COEFF
                        The smoothness coeff of OneEuro filter, the smaller, the smoother.
  --calc_smpl           Whether to calculate the smpl mesh from estimated SMPL parameters
  --render_mesh         Whether to render the estimated 3D mesh mesh to image
  --renderer RENDERER   Choose the renderer for visualizaiton: pyrender (great but slow), sim3dr       
                        (fine but fast)
  --show                Whether to show the rendered results
  --show_items SHOW_ITEMS
                        The items to visualized, including
                        mesh,pj2d,j3d,mesh_bird_view,mesh_side_view,center_conf. splited with ,        
  --save_video          Whether to save the video results
  --frame_rate FRAME_RATE
                        The frame_rate of saved video results
  --smpl_path SMPL_PATH
                        The path of smpl model file
  --model_path MODEL_PATH
                        The path of ROMP checkpoint
  --model_onnx_path MODEL_ONNX_PATH
                        The path of ROMP onnx checkpoint
  --root_align ROOT_ALIGN
                        Please set this config as True to use the ROMP checkpoints trained by
                        yourself.
(sromp) PS C:\Users\Vivien\Projekte> 
```
I recommend the following solution (not tested):
https://stackoverflow.com/a/47440202/3699361

My `pip freeze` output:
```ps
black==22.3.0
certifi==2021.10.8
click==8.1.2
colorama==0.4.4
Cython==0.29.30
lap==0.4.0
mypy-extensions==0.4.3
numpy==1.22.3
opencv-python==4.5.5.64
pathspec==0.9.0
platformdirs==2.5.1
simple-romp==1.0.5
tomli==2.0.1
torch==1.11.0
typing_extensions==4.1.1
wincertstore==0.2
```
System:
 - Python: v`3.9.12`
 - OS: Windows 10 Enterprise (64-bit, Version `21H2`, Build `19044.1706`)
"
蒸馏模型,Arthur151/ROMP,2022-06-08 07:48:41,2,,265,1264339447,"我们想通过蒸馏的方式训练一个模型，看到您做过相关的工作：

还有一种简洁高效的思路，就是模型蒸馏，直接蒸大模型的输出就好了，我试过蒸小模型，很有效，很快就能训好。

请问您用的哪个模型作为teacher模型以及蒸馏相关的花费方程如何设置会比较好？"
Why does the test and evaluation shows different results?,Arthur151/ROMP,2022-05-10 07:14:03,3,,235,1230720211,"I'm training ROMP model with your code
It saves every validation process 
but, I test the results with ""python -m romp.test"" code I got the different result as evaluation 
I think they are same process, do you know why ?
Or did I do something wrong?
 "
Running ROMP in modular mode,Arthur151/ROMP,2022-05-09 17:44:13,5,,234,1230048167,"Hi,

I am making changes to the ROMP and I would like to run the code to see the results. I do `python -m torch.utils.bottleneck romp.train --configs_yml='configs/v1_hrnet_3dpw_ft.yml'`. 
```
Traceback (most recent call last):
  File ""/z/home/mahzad-khosh/env/romp2/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/z/home/mahzad-khosh/env/romp2/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/z/home/mahzad-khosh/env/romp2/lib/python3.8/site-packages/torch/utils/bottleneck/__main__.py"", line 229, in <module>
    main()
  File ""/z/home/mahzad-khosh/env/romp2/lib/python3.8/site-packages/torch/utils/bottleneck/__main__.py"", line 192, in main
    with open(scriptfile, 'rb') as stream:
FileNotFoundError: [Errno 2] No such file or directory: 'romp.train'
```

When I do `python -m romp.train --configs_yml='configs/v1_hrnet_3dpw_ft.yml'` it works fine. But adding `torch.utils.bottleneck `causes error. How can I avoid this error?

Thanks,"
Changing the background of 3d visualization,Arthur151/ROMP,2022-05-08 02:52:20,0,,231,1228777147,Hello. I am running your demo webcam.sh and I generated a cartoon character. I wonder are there any apis for changing the background?
Distributed training raise an error,Arthur151/ROMP,2022-05-05 22:50:33,8,,230,1227247657,"Hi, 

I am trying to run Romp in distributed mode. I follow this [Script](https://github.com/Arthur151/ROMP/blob/master/scripts/train_distributed.sh). Since there is no folder called `core` in the repository I replaced it with `romp`. However, when I run the code it raises the error that there is no file called `train.py`. How can I avoid this error?

Thanks"
[Config] Introduce calibration option,Arthur151/ROMP,2022-05-01 22:14:07,1,,225,1222307436,"I have run some tests with different dancing video materials.
Sometimes it recognizes the wrong body size at some frames, because of difficult movements.
That doesn't look pretty.

So how about an calibration option to:
detect the body size at the beginning of an video or webcam session and set it fixed until the end."
Is it possible to get the Body Part Segmentations,Arthur151/ROMP,2022-05-01 05:00:39,3,,224,1222021767,"Hi team

first of all ´great job !  

I am currently trying to do some body part swaps project and came across your project. 

I would like to know if its possible to get the body parts as shown in the following pictures. 


![image](https://user-images.githubusercontent.com/77654049/166132723-13dcf868-b064-4f06-8a8b-615069050589.png)

I want to not only have a ""3D avatars"" but also have the body regions itself to cut away the regions I am not interested in for post processing 

Is there any way to get them as shown in the pictures ? It doesn't have to be so detailed but it would be great if it was. 
"
How can I change the cartoon character?,Arthur151/ROMP,2022-04-30 04:06:57,8,,223,1221718388,"Hello. Thanks for your work.
I successfully ran your demo webcam.sh, which generates a cartoon character like this.
<img width=""527"" alt=""Screenshot 2022-04-30 at 12 06 12"" src=""https://user-images.githubusercontent.com/39186017/166089929-6e6fa6d5-4299-49c6-a63f-71582a7905b3.png"">
I wonder how to generate such a character, and can I change it? Any there any references about it?

"
如何确定每帧视频中人物的对应关系,Arthur151/ROMP,2022-04-29 15:25:28,8,,222,1221082111,"感谢您之前的指导，我已经可以根据单人视频的参数在unity中驱动单人模型了~
对于人数不变的视频，我发现single_batch_results每帧输出的人物的参数的数组是对应的，第一帧的single_batch_results[0]和第二帧的single_batch_results[0]是同一个人。
但是对于人数变化的视频，请问我应该怎么知道每帧视频中人物是如何对应的呢~"
How to value？,Arthur151/ROMP,2022-04-29 07:57:02,2,,221,1220155979,"你好，
import numpy as np
import torch
test = np.load('./00000000.npz',allow_pickle=True)
print(test.files)
print(test['results']['smpl_thetas'])我想用这个取值，为何报了这个错
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices
"
Is it possible to continue learning from any epoch?,Arthur151/ROMP,2022-04-27 05:26:11,1,,219,1216814370,"Hey!
Thanks for your work!
I have a question: can I continue training the model from the epoch before which the model was trained?

I mean, I started training from scratch, after the training was interrupted, and after a while, can I continue training (maybe using existed .pkl file)?

I'm trying to connect mobilenet_v2 as a backbone, but it needs distillation and experimentation to improve. But the training process is long, so I want to be able to train up to epoch 10 (for example), and after other experiments, continue training from epoch 10 (the same model and with the same source code as before)"
key item,Arthur151/ROMP,2022-04-26 07:45:34,1,,218,1215546451,"I try to run the code, but when i want to run smpl_parser there're no key values ""outputs['smpl_betas'], outputs['smpl_thetas']"",do I miss somthing? thanks for the answer"
批处理视频用哪个文件,Arthur151/ROMP,2022-04-26 03:40:12,7,,217,1215345253,你好，有批处理视频的文件吗
ModuleNotFoundError: No module named 'bpy',Arthur151/ROMP,2022-04-26 03:38:03,2,,216,1215344209,你好，怎么解决这个问题？
'with_renderer' argument raises TypeError,Arthur151/ROMP,2022-04-24 17:31:33,1,,215,1213716538,"I tried to test the dataset loading process as described in https://github.com/Arthur151/ROMP/blob/master/docs/dataset.md
`python -m romp.lib.dataset.lsp`

It raised an error as below:
`visualizer = Visualizer(resolution = (512,512,3), result_img_dir=save_dir,with_renderer=True)
TypeError: __init__() got an unexpected keyword argument 'with_renderer'`

This error occured in the line https://github.com/Arthur151/ROMP/blob/7b4734270672e602f4fc5659d37479478e72b78b/romp/lib/dataset/image_base.py#L485

Initialization function of visualizer does not have such argument: https://github.com/Arthur151/ROMP/blob/7b4734270672e602f4fc5659d37479478e72b78b/romp/lib/visualization/visualization.py#L28

Is this an obsolete argument or did I miss something?"
运行模型问题,Arthur151/ROMP,2022-04-24 08:13:42,3,,214,1213582688,"作者您好，我在运行您的模型时，输出mesh等功能正常，但是输出center_conf时会报错，请问这是否和图片格式有关？我可以怎样调整去解决？
具体报错如下：
(base) wangyang@wangyangdeMacBook-Pro ~ % romp --mode=image --calc_smpl --render_mesh --input=/Users/wangyang/Downloads/3323.JPG --save_path=/Users/wangyang/Downloads/893.JPG --show_items=center_conf
Using ROMP v1
/Users/wangyang/opt/anaconda3/lib/python3.8/site-packages/romp/post_parser.py:34: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  topk_ys = (topk_inds.long() // w).float()
/Users/wangyang/opt/anaconda3/lib/python3.8/site-packages/romp/post_parser.py:39: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  topk_clses = index.long() // K
/Users/wangyang/opt/anaconda3/lib/python3.8/site-packages/romp/post_parser.py:144: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  parsed_results['centers_pred'] = torch.stack([flat_inds%64, flat_inds//64],1) * 512 // 64
Traceback (most recent call last):
  File ""/Users/wangyang/opt/anaconda3/bin/romp"", line 8, in <module>
    sys.exit(main())
  File ""/Users/wangyang/opt/anaconda3/lib/python3.8/site-packages/romp/main.py"", line 179, in main
    outputs = romp(image)
  File ""/Users/wangyang/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/Users/wangyang/opt/anaconda3/lib/python3.8/site-packages/romp/utils.py"", line 655, in wrap_func
    result = func(*args, **kwargs)
  File ""/Users/wangyang/opt/anaconda3/lib/python3.8/site-packages/romp/main.py"", line 167, in forward
    outputs = rendering_romp_bev_results(self.renderer, outputs, image, rendering_cfgs)
  File ""/Users/wangyang/opt/anaconda3/lib/python3.8/site-packages/vis_human/main.py"", line 101, in rendering_romp_bev_results
    cv2.putText(result_image[1], '{:.3f}'.format(outputs['top_score'][ind]), tuple(kp.astype(int)), cv2.FONT_HERSHEY_COMPLEX,1,(255,0,255),1)  
IndexError: list index out of range"
我打算使用服务器实时驱动模型，但是我服务器上没有硬件，就使用了run_on_remote_server参数,Arthur151/ROMP,2022-04-22 02:44:19,1,,213,1211747677,"我认为该参数是方便服务器启用sever调用本地摄像头，该参数需要使用https://github.com/Arthur151/ROMP/blob/v1.1/src/lib/utils/remote_server_utils.py
文件，该文件内
    run_server(server_host, server_port)  # first, run this function only in server
    # run_client(server_host, server_port)  # then, run this function only in client，请问在客户端是直接单独运行该.py文件吗？显示有错误，该函数找不到"
train in coco dataset,Arthur151/ROMP,2022-04-18 00:14:49,1,,206,1206521347,"
Hello, I am using coco dataset to train, after 19 epochs of training I get this error![image](https://user-images.githubusercontent.com/96688784/163737354-9470ca1f-e5cb-4e99-b64e-28c4e37843dc.png)
"
question about training with crop image as input,Arthur151/ROMP,2022-04-13 07:17:40,2,,204,1202862331,"作者您好，我尝试用center crop的图像作为训练的输入。我改动了如下内容：
1）Image_base中get_item_single_frame，对目标中心做center crop。
2）all_person_detected_mask，所有sample都是true
3）max_person为1，multi_person为false
4）训练集h36m,mpiinf,mpii,lsp,crowdpose,pw3d 
用pretrained model finetune
loss中MPJPE和PAMPJPE基本不降(维持在200左右)，训练图片可视化中，能看到mpiinf预测结果越来越差，训练几个epoch后这个数据集的结果就完全不对了，但其他数据集看上去没什么问题。
然后，我将mpiinf数据集去掉训练，loss和可视化结果看上去都正常。

整图训练，用mpiinf，MPJPE和PAMPJPE是在下降的，但是crop图时，只有这个数据集降不下来，您知道原因不?
"
data,Arthur151/ROMP,2022-04-13 01:48:08,2,,201,1202648754,"How to get this .npy file, only find the .JSON file in the dataset. Is this automatically generated based on the code?"
model_data.zip和ned_models_try.zip下载失败，显示需要获取授权,Arthur151/ROMP,2022-04-12 02:20:48,2,,200,1200837048,
训练自己的数据集,Arthur151/ROMP,2022-04-08 07:18:26,5,,197,1196934998,请问如果我自己的数据集中并不是按照SMPL的24个关键点，比如我有26个关节，与SMPL中的24个不同，可以用您的方法生成最后的旋转参数poses吗
如果用256x256训练的话性能会有很大的影响嘛？,Arthur151/ROMP,2022-04-07 04:37:41,1,,192,1195508820,现在512x512的大概cpu需要100ms，我想尝试一下降低分辨率，想请教下作者有没有类似的尝试？
Still learning 3D keypoints if even the person is truncated in the image?,Arthur151/ROMP,2022-04-06 16:32:12,1,,191,1194848646,"In image_dataset.py, if we activate shuffle_mode and then the image will be cropped. Now if a person is cropped out of the region, the 2d kps will mostly fall out of the image region, and these outlier 2d keypoints will be updated to (-2 , -2)

However, if there is one 2d keypoint still remaining inside the image, for example a head point like this:
![image](https://user-images.githubusercontent.com/63605407/162023328-176f616f-dde5-448b-a715-0e32c05ff117.png)

Then the 2d keypoints of this person on the right will be updated to [[-2, -2], [-2, -2], ... [x_head, y_head], ...[-2, -2]]. So the 2d loss will only be calculated for head joint.

However, in the current code, the 3d keypoints is not synchronized with 2d joints, all of the 3d joints (whole body) will still be learned, is there any reason behind it? Are you meaning to give the network to infer the 'invisible' part of the body?
"
某些场景下出现了人体无法识别,Arthur151/ROMP,2022-04-06 07:09:08,4,,188,1194147970,"感谢您提供的ROMP方法！我尝试使用提供的方法，效果很棒！但是在某些场景下出现了人体无法识别的情况。如图中，红色箭头所指被试者在进行大幅前屈动作时。

由于我不是计算机相关专业的，所以想请问一下，这是什么原因呢？是否有解决方法？
![360截图174110245393106](https://user-images.githubusercontent.com/52640723/161916022-58baabfc-738d-4b75-8d93-c94fc2fba564.jpg)
"
[FAQ] Why should i train 'romp' by myself?,Arthur151/ROMP,2022-04-04 15:07:22,7,,186,1191934456,"I am missing an notice about the purpose of training `romp` by myself.
For example: Are there any benefits of usage, if i train it again by myself instead of using the attached dataset?

Would be great to get some guidelines about this question."
[FAQ] What are the differences between 'romp' and 'simple-romp'?,Arthur151/ROMP,2022-04-04 15:04:17,5,,185,1191930226,"a) I was not able to found an notice about the detailed differences and benefits of 'simple-romp' against 'romp'.
Only the name implies, that `simple-romp` might be easier to use.
In which situations should i use `romp` or `simple-romp`?

b) But how about dependecies like `pytorch3d` and all requirements?
How to use it with `conda env` instead of plain `pip`?

Please add further information, if possible."
To reproduce the final model,Arthur151/ROMP,2022-03-31 01:22:51,6,,182,1187329313,"
In the paper, it says that fine tuned model shows the best performance at 120Epoch, but after running the v1_hrnet_3dpw_ft.yml file, it showed the best performance at 1 epoch. Did I do something wrong?"
fbx出来的效果抖动比较厉害,Arthur151/ROMP,2022-03-30 09:55:54,1,,179,1186194002,有什么原因和优化思路呢？人体模型驱动系统驱动出来的动作就比较自然，我发现很多smplx版本的数字人体，出来的fbx，动画动作都会有抖动和不连贯，不连贯是采样问题？抖动应该是没做动力学限制？
小模型训练到30epoch loss值不下降，infer视频之后发现SMPL body始终在人物上方,Arthur151/ROMP,2022-03-24 03:04:09,17,,177,1178881357,"[V1_mbv3_h36m,mpiinf,coco,mpii,lsp,muco,crowdpose_g0,1,2,3.log](https://github.com/Arthur151/ROMP/files/8338065/V1_mbv3_h36m.mpiinf.coco.mpii.lsp.muco.crowdpose_g0.1.2.3.log)
可以帮忙看一下这个mobilenetv3的训练日志吗？
他的validation 指标下降到140&90附近就感觉卡住了，而且我把30epoch的模型拿出来测试了一下，发现SMPL body会飘在人物上面，我的想法是训练的时候body center heatmap应该是最先收敛的，所以即便pose上误差比较大，det loss应该收敛，也就是不会产生这种飘在人物上方的现象，不知道您训练的时候有没有这种问题，希望您能给出建议～

我在resnet的基础上将batch_size 调整为了128，lr 相应x2 为 0.0001
图示：
<img width=""735"" alt=""image"" src=""https://user-images.githubusercontent.com/58206232/159833765-1131142f-e42d-4be8-bbda-e57d7b3f4bc9.png"">
"
image cropping method might be wrong,Arthur151/ROMP,2022-03-20 19:43:00,1,,175,1174662412,"https://github.com/Arthur151/ROMP/blob/623687a37cb7d1ba4538baf1e3c6f65808a36e2c/romp/lib/utils/augments.py#L167-L169

Here, I guess you meant to calculate a bounding box for a single person, based on the visible kpts and then expend the box.

However, when you do `leftTop, rightBottom = np.clip(box[0], 0, width), np.clip(box[1], 0, height)`, as I understand box[0] is (Xmin, Ymin). But why is Ymin also constrained by width? The same goes for the next term, why is Xmax constrained by height? This might cause truncation of a person.

For example, if there is a very long image size like 300 x 900, the person is in the right of the image, the green box is the box calculated by cal_aabb(), the red dots are kpts (Fig 1.):

But after Line 168 and Line 169, it will become fig3. ****This is because, in the Line168, the X value will be constrained by 'height'.** This will make the X value of RightBottom even smaller than LeftTop's X value, and gave a box like Fig 2.**

Finally, the cropped image will be **a completely blank image  (!)**
![image](https://user-images.githubusercontent.com/63605407/159183645-9380dfaa-4185-4df0-9130-c8e06a02b502.png)


**You can reproduce the results very quick, by copying the code below and use this image, it's exactly the same as your code, you can put it directly into your main() in augments.py**, I pre-defined 5 kpts based for the person (red dots):
![QQ截图20220321030726](https://user-images.githubusercontent.com/63605407/159180866-b911ef7b-42ae-438b-8f3e-fe16fbfcc7c0.jpg)
kps2d = np.array([[[800,70, 1],[840,133, 1],[750,137, 1],[750,245, 1], [840, 235, 1]]])
```
def processssss(originImage, full_kp2ds=None, augments=None, is_pose2d=True, multiperson=False):
    crop_trbl, bbox = (0,0,0,0), None

    if augments is not None:
        height, width = originImage.shape[0], originImage.shape[1]
        scale, rot, flip = augments

        if rot != 0:
            originImage, full_kp2ds = img_kp_rotate(originImage, full_kp2ds, rot)

        if flip:
            originImage = np.fliplr(originImage)
            full_kp2ds = [flip_kps(kps_i, width=originImage.shape[1], is_pose=is_2d_pose) for kps_i, is_2d_pose in zip(full_kp2ds, is_pose2d)]

        if not multiperson and is_pose2d.sum()>0:
            kps_vis = full_kp2ds[0]#[valid_range][np.where(np.array(is_pose2d[valid_range]))[0][random_idx]]
            if (kps_vis[:,2]>0).sum()>2:
                box = calc_aabb(kps_vis[kps_vis[:,2]>0,:2].copy())

                x = originImage.copy()
                cv2.rectangle(x, (int(box[0][0]), int(box[0][1])), (int(box[1][0]), int(box[1][1])), (0, 255, 0), thickness=2, lineType=4)
                cv2.circle(x, (int(box[0][0]), int(box[0][1])), color=(255, 0, 0), radius=5, thickness = -1)
                cv2.circle(x, (int(box[1][0]), int(box[1][1])), color=(255, 0, 0), radius=5, thickness = -1)
                for i in kps_vis[kps_vis[:,2]>0,:2]:
                    cv2.circle(x, (i[0], i[1]), color=(0, 0, 255), radius=3, thickness = -1)
                cv2.imwrite(f'./original_box_with_kp2d.jpg', x)

                y = originImage.copy()
                leftTop, rightBottom = np.clip(box[0], 0, width), np.clip(box[1], 0, height)
                cv2.rectangle(y, (int(leftTop[0]) ,int(leftTop[1])), (int(rightBottom[0]), int(rightBottom [1])), (0, 255, 0), thickness=2, lineType=4)
                for i in kps_vis[kps_vis[:,2]>0,:2]:
                    cv2.circle(y, (i[0], i[1]), color=(0, 0, 255), radius=3, thickness = -1)
                cv2.imwrite(f'./clipped_original_box_with_kp2d.jpg', y)

                z = originImage.copy()
                [l, t], [r, b] = get_image_cut_box(leftTop, rightBottom, scale)
                bbox = (l,t,r,b)
                cv2.rectangle(z, (int(l) ,int(t)), (int(r), int(b)), (0, 255, 0), thickness=2, lineType=4)  
                for i in kps_vis[kps_vis[:,2]>0,:2]:
                    cv2.circle(z, (i[0], i[1]), color=(0, 0, 255), radius=3, thickness = -1)
                cv2.imwrite(f'./rectified_box_with_kp2d.jpg', z)

    orgImage_white_bg, pad_trbl = image_pad_white_bg(originImage)
    if full_kp2ds is None and augments is None:
        return orgImage_white_bg, pad_trbl
    
    image_aug, kp2ds_aug, offsets = image_crop_pad(originImage, kp2ds=full_kp2ds, crop_trbl=crop_trbl, bbox=bbox, pad_ratio=1.)
    cv2.imwrite(f'./final.jpg', image_aug)
    return image_aug, orgImage_white_bg, kp2ds_aug, offsets

if __name__ == '__main__':
    image = cv2.imread('/apdcephfs/share_1290939/zhengdiyu/projects/multi-hand-recovery/2.jpg', cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)
    kps2d = np.array([[[800,70, 1],[840,133, 1],[750,137, 1],[750,245, 1], [840, 235, 1]]])
    scale = np.random.rand() * (1.7 - 1.2) + 1.2
    print(image.shape)
    print('kps shape', kps2d.shape)
    return_img = processssss(image, kps2d, augments=(scale, 0, False), is_pose2d=np.array([True]))
```
===========
I don't know if I understand your meaning correctly. If not, could you explain to me why we need truncated person, will it benefit training? 

**Supp:**
If I want to get a reasonable result, I can just get it by comment Line 167 `leftTop, rightBottom = np.clip(box[0], 0, width), np.clip(box[1], 0, height) `:
My RESULTS
![image](https://user-images.githubusercontent.com/63605407/159183924-5b35aef2-6235-421f-ae68-0d92ea64b5d7.png)


By the way, I think that we can choose full_kps within the range: `valid_range = (full_kp2ds[:, :, 2]>0).sum(-1) > 2` instead of randomly choosing one from full_kp2ds and then judging whether it has `if (kps_vis[:,2]>0).sum()>2` or not:

```
            valid_range = (full_kp2ds[:, :, 2]>0).sum(-1) > 2
            kps_vis = full_kp2ds[valid_range][np.where(np.array(is_pose2d[valid_range]))[0][random_idx]]
            #if (kps_vis[:,2]>0).sum()>2:
            box = calc_aabb(kps_vis[kps_vis[:,2]>0,:2].copy())
```"
About 2D to 3D mapping,Arthur151/ROMP,2022-03-18 06:25:55,1,,174,1173229980,"Hello，author！
I'm learning romp.I reconstructed a human 3D model from the image. Now I find a two-dimensional coordinate point on the person in the image. I want to know how to find the corresponding three-dimensional coordinate point on the human 3D model. I think it should be to obtain the camera parameters. I don't know how to obtain it on romp? Can you give me some advice，thanks！"
Any ideas about to driving characters in Unreal Engine?,Arthur151/ROMP,2022-03-15 02:00:14,3,,172,1169093078,"Thank Arthur Sun for sharing this wonderful and great work!

ROMP shows great performance with Blender, and I just wonder that is there a way to drive characters in Unreal Engine other than Blender in real-time?

Thank you very much.
"
Evaluation on CMU Panoptic,Arthur151/ROMP,2022-03-01 11:37:28,1,,167,1155230409,"Hi, I read your code for Panoptic evaluation and want to make sure that the mesh parameter map information is fetched directly from the GT and not from the center map information , right?
code in romp/lib/maps_utils/result_parser.py line 97-100"
关于数据集的问题,Arthur151/ROMP,2022-02-28 13:25:10,1,,166,1154090721,"您提供的h36m数据集是311124张图片用来训练，106648张用来测试，在images文件夹中一共是417772张图片。
Q1：human3.6m的[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6682899&tag=1)中Fig2（a）中可见，train、val、test一共是3640788张图片。请问您的417772张图片是如何选取和划分的？
Q2：human3.6m的[官网的overview](http://vision.imar.ro/human3.6m/description.php)中看到，只有带衣服的mesh没有不带衣服的SMPL mesh，请问您的mesh数据是如何生成的？"
continue train,Arthur151/ROMP,2022-02-27 02:34:21,1,,164,1152670089,"[myv2_train_pretrained_hrnet_h36m,mpiinf,coco,mpii,lsp,muco,crowdpose_g0,1.log](https://github.com/Arthur151/ROMP/files/8147875/myv2_train_pretrained_hrnet_h36m.mpiinf.coco.mpii.lsp.muco.crowdpose_g0.1.log)
Q1:训练到一半的时候，由于OOM或其他原因，导致训练终止，如何设置参数，使得可以继续训练？
Q2：即使成功训练完成120个epoch，如果发现loss还没有收敛，那么如何继续设置参数，使得其继续训练更多的epoch？
"
how to get and use the pretrained models and reproduce the results of paper,Arthur151/ROMP,2022-02-21 11:00:43,12,,161,1145642243,作者您好，您提供的ROMP_HRNet32_V1.pkl是否是通过v1.yml在pretrain_hrnet.pkl的基础上训练得到的吗？
Inaccurate fitting on images,Arthur151/ROMP,2022-01-08 17:13:14,2,,142,1096985478,"Hi,I tried your code on some images obtained from web-cam as well as some images from yoga-82 dataset(on which mediapipe) was trained.
Since,the pose changes are extreme,it seems the model is not able to get a correct fit.
Any reaons on why that is happening
results of fitting romp 
![download (1)](https://user-images.githubusercontent.com/34626942/148653240-cc6bb6f3-9116-4f31-b3fd-89ebe8331ae6.jpg)
![download (2)](https://user-images.githubusercontent.com/34626942/148653243-45973f70-0960-49a3-b9ff-f5857231cded.jpg)
![download](https://user-images.githubusercontent.com/34626942/148653245-4bf969bf-5cd7-4700-b6b5-e6cc43578852.jpg)

original images
![yoga1](https://user-images.githubusercontent.com/34626942/148653279-4b417b29-941c-48fa-b012-43be5585934a.jpg)


![douche](https://user-images.githubusercontent.co
![real_cap00](https://user-images.githubusercontent.com/34626942/148653289-d2f3bbe8-1e33-4dc0-babb-4aab2e59815c.jpg)
m/34626942/148653266-6fa10108-7b54-42b9-b6cc-8de280b8992e.jpg)
?
I hope you can give me some insight into what can be done to improve the fitting ,should i tune the parameters in the config file?"
Bug in h36m_extract_img.py ?,Arthur151/ROMP,2022-01-07 13:16:32,8,,141,1096308722,"https://github.com/Arthur151/ROMP/blob/5fb833d15d11f94b2f62d9c65619eedc85c40d3f/romp/lib/dataset/preprocess/h36m_extract_frames.py#L19

https://github.com/Arthur151/ROMP/blob/5fb833d15d11f94b2f62d9c65619eedc85c40d3f/romp/lib/dataset/preprocess/h36m_extract_frames.py#L33"
PA_MPJPE calculation failed! svd_cuda: (Batch element 0): The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated singular values (error code: 55).,Arthur151/ROMP,2022-01-02 12:43:05,14,,136,1092012956,"Hi, I have difficulty training for the 6 dataset(mpiinf, coco, mpii, lsp, muco, crowdpose). The training code can run successfully for a period of time (not more than some epoch) and then will encounter this error in the training logs file.  Can you give me a solution for this?

>6 epoch

In 6 epoch , the Losses can be output, but the ""INFO:root:Evaluation on pw3d"" is nan value.

![image](https://user-images.githubusercontent.com/29176256/147876046-2679b5f3-2eda-46bb-8315-ce3eaa99cf4e.png)
![image](https://user-images.githubusercontent.com/29176256/147876064-45c2e673-c7b6-4755-9163-3237e711d78b.png)



>7 epoch

In 7 epoch, the log is "" PA_MPJPE calculation failed! svd_cuda: (Batch element 0): The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated singular values (error code: 55)"".

![image](https://user-images.githubusercontent.com/29176256/147876119-2c94e625-1984-4843-91cc-1b5e551a3562.png)


"
"KeyError: 'bpy_prop_collection[key]: key ""m_avg_Pelvis"" not found'",Arthur151/ROMP,2022-01-01 04:56:05,4,,135,1091741906,"Hi, brother, I run convert blender fbx script got this error:

```
KeyError: 'bpy_prop_collection[key]: key ""m_avg_Pelvis"" not found'

```
why this error happens?"
3 camera parameters of weak-perspective camera,Arthur151/ROMP,2021-12-30 05:54:25,6,,134,1090892920,"大佬您好，感谢您所做的工作，目前遇到些问题想向您请教，希望您提供思路。

当只使用save_dict_results里面保存的smpl 85维参数实现可视化时，cam相机结果不准，使用以下值：
  -  cam (3,) # 3 camera parameters of weak-perspective camera, (scale, tranlation_x, tranlation_y)
  -  pose (72,) # 72 SMPL pose parameters.
  -  betas (10,) # 10 SMPL shape parameters.

更改smpl_mesh_root_align: False没用，cam、pose、betas值并没有改变，所以结果还是不对。

效果如下：
[results.zip](https://github.com/Arthur151/ROMP/files/7791664/results.zip)





"
How to control vedo windows shape?,Arthur151/ROMP,2021-12-29 14:17:49,29,,133,1090550452,"![image](https://user-images.githubusercontent.com/21303438/147671374-ea029e53-c522-42cb-b6f9-9cfd843706f8.png)

Hi, default is 1960x1960, I make scaled to 1 rather than 2.
I want it to be rectangle window, how to controll it?"
关于转换fbx的脚本中trans参数的顺序,Arthur151/ROMP,2021-12-28 06:00:06,1,,129,1089633292,"您好，请教一下关于convert_fbx.py中写入位移的代码
bones[bone_name_from_index[0]].location = Vector((100*trans[1], 100*trans[2], 100*trans[0])) - pelvis_position
这里的顺序为什么是trans[1][2][0]呢？不是按XYZ的顺序吗"
Could you provide a complete test code for convert_to_bvh.py?,Arthur151/ROMP,2021-12-27 14:49:41,2,,128,1089289703,"

Could you provide a complete test code for convert_to_bvh.py？Very Thanks."
About webcam_blender.sh,Arthur151/ROMP,2021-12-18 07:07:39,2,,119,1083766829,"Hi. I am trying to use your prediction to control the animator in blender. However, I met a problem when I am running ""webcam_blender.sh""

The ""webcam.sh"" demo can be run successfully. However, when I ran ""webcam_blender.sh"", the program keeps blocked in the accepting signal operation, 
https://github.com/Arthur151/ROMP/blob/99564ad7aaa2fea05d9873832e6914481026fe26/romp/lib/visualization/socket_utils.py#L205
, and the program will not move forward.

It's multi-threading needed for accepting the signal?"
训练时间,Arthur151/ROMP,2021-12-17 12:51:03,12,,118,1083229853,请问作者使用4张p40gpu训练完整的数据集需要多久？
video test promblem,Arthur151/ROMP,2021-11-16 08:06:41,7,,99,1054573463,"Dear Arthur151, when test video on your method, last frame always gets mistake, hope you reply.

this is issue:



pygame 2.0.0 (SDL 2.0.12, python 3.7.9)
Hello from the pygame community. https://www.pygame.org/contribute.html
/home/neu307/miniconda3/envs/env_centerhmr/lib/python3.7/site-packages/quaternion/numba_wrapper.py:21: UserWarning: 

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Could not import from numba, which means that some
parts of this code may run MUCH more slowly.  You
may wish to install numba.
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

  warnings.warn(warning_text)
INFO - 2021-11-16 15:56:39,929 - acceleratesupport - No OpenGL_accelerate module loaded: No module named 'OpenGL_accelerate'
INFO - 2021-11-16 15:56:40,399 - base - {'tab': 'hrnet_cm64_test', 'configs_yml': 'configs/video.yml', 'demo_image_folder': 'None', 'local_rank': 0, 'model_version': 1, 'multi_person': True, 'collision_aware_centermap': False, 'collision_factor': 0.2, 'kp3d_format': 'smpl24', 'eval': False, 'max_person': 64, 'input_size': 512, 'Rot_type': '6D', 'rot_dim': 6, 'centermap_conf_thresh': 0.25, 'centermap_size': 64, 'deconv_num': 0, 'model_precision': 'fp32', 'backbone': 'hrnet', 'gmodel_path': '../trained_models/ROMP_hrnet32.pkl', 'print_freq': 50, 'fine_tune': True, 'gpu': '0', 'batch_size': 64, 'val_batch_size': 1, 'nw': 4, 'calc_PVE_error': False, 'dataset_rootdir': '/home/neu307/wh/dataset/', 'high_resolution': True, 'save_best_folder': '/home/neu307/wh/checkpoints/', 'log_path': '/home/neu307/wh/log/', 'total_param_count': 85, 'smpl_mean_param_path': '/home/neu307/wh/ROMP-master/models/satistic_data/neutral_smpl_mean_params.h5', 'smpl_model': '/home/neu307/wh/ROMP-master/models/statistic_data/neutral_smpl_with_cocoplus_reg.txt', 'smplx_model': True, 'cam_dim': 3, 'beta_dim': 10, 'smpl_joint_num': 22, 'smpl_model_path': '/home/neu307/wh/ROMP-master/models', 'smpl_uvmap': '/home/neu307/wh/ROMP-master/models/smpl/uv_table.npy', 'smpl_female_texture': '/home/neu307/wh/ROMP-master/models/smpl/SMPL_sampleTex_f.jpg', 'smpl_male_texture': '/home/neu307/wh/ROMP-master/models/smpl/SMPL_sampleTex_m.jpg', 'smpl_J_reg_h37m_path': '/home/neu307/wh/ROMP-master/models/smpl/J_regressor_h36m.npy', 'smpl_J_reg_extra_path': '/home/neu307/wh/ROMP-master/models/smpl/J_regressor_extra.npy', 'kernel_sizes': [5], 'GPUS': 0, 'use_coordmaps': True, 'video_or_frame': True, 'input_video_path': '../demo/test_videos/C1/C1_single_2.mp4', 'webcam_mesh_color': 'LightCyan', 'save_mesh': False, 'save_centermap': False, 'save_dict_results': True, 'webcam': False, 'fps_save': 30, 'multiprocess': False}
INFO - 2021-11-16 15:56:40,399 - base - ------------------------------------------------------------------
INFO - 2021-11-16 15:56:40,399 - base - start building model.
Using ROMP v1
INFO - 2021-11-16 15:56:43,931 - train_utils - using fine_tune model: ../trained_models/ROMP_hrnet32.pkl
INFO - 2021-11-16 15:56:44,085 - base - finished build model.
Initialization finished!
Running the code on video  ../demo/test_videos/C1/C1_single_2.mp4
Processing video 0/164
Processing video 1/164
Processing video 2/164
Processing video 3/164
Processing video 4/164
Processing video 5/164
Processing video 6/164
Processing video 7/164
Processing video 8/164
Processing video 9/164
Processing video 10/164
Processing video 11/164
Processing video 12/164
Processing video 13/164
Processing video 14/164
Processing video 15/164
Processing video 16/164
Processing video 17/164
Processing video 18/164
Processing video 19/164
Processing video 20/164
Processing video 21/164
Processing video 22/164
Processing video 23/164
Processing video 24/164
Processing video 25/164
Processing video 26/164
Processing video 27/164
Processing video 28/164
Processing video 29/164
Processing video 30/164
Processing video 31/164
Processing video 32/164
Processing video 33/164
Processing video 34/164
Processing video 35/164
Processing video 36/164
Processing video 37/164
Processing video 38/164
Processing video 39/164
Processing video 40/164
Processing video 41/164
Processing video 42/164
Processing video 43/164
Processing video 44/164
Processing video 45/164
Processing video 46/164
Processing video 47/164
Processing video 48/164
Processing video 49/164
Processing video 50/164
Processing video 51/164
Processing video 52/164
Processing video 53/164
Processing video 54/164
Processing video 55/164
Processing video 56/164
Processing video 57/164
Processing video 58/164
Processing video 59/164
Processing video 60/164
Processing video 61/164
Processing video 62/164
Processing video 63/164
Processing video 64/164
Processing video 65/164
Processing video 66/164
Processing video 67/164
Processing video 68/164
Processing video 69/164
Processing video 70/164
Processing video 71/164
Processing video 72/164
Processing video 73/164
Processing video 74/164
Processing video 75/164
Processing video 76/164
Processing video 77/164
Processing video 78/164
Processing video 79/164
Processing video 80/164
Processing video 81/164
Processing video 82/164
Processing video 83/164
Processing video 84/164
Processing video 85/164
Processing video 86/164
Processing video 87/164
Processing video 88/164
Processing video 89/164
Processing video 90/164
Processing video 91/164
Processing video 92/164
Processing video 93/164
Processing video 94/164
Processing video 95/164
Processing video 96/164
Processing video 97/164
Processing video 98/164
Processing video 99/164
Processing video 100/164
Processing video 101/164
Processing video 102/164
Processing video 103/164
Processing video 104/164
Processing video 105/164
Processing video 106/164
Processing video 107/164
Processing video 108/164
Processing video 109/164
Processing video 110/164
Processing video 111/164
Processing video 112/164
Processing video 113/164
Processing video 114/164
Processing video 115/164
Processing video 116/164
Processing video 117/164
Processing video 118/164
Processing video 119/164
Processing video 120/164
Processing video 121/164
Processing video 122/164
Processing video 123/164
Processing video 124/164
Processing video 125/164
Processing video 126/164
Processing video 127/164
Processing video 128/164
Processing video 129/164
Processing video 130/164
Processing video 131/164
Processing video 132/164
Processing video 133/164
Processing video 134/164
Processing video 135/164
Processing video 136/164
Processing video 137/164
Processing video 138/164
Processing video 139/164
Processing video 140/164
Processing video 141/164
Processing video 142/164
Processing video 143/164
Processing video 144/164
Processing video 145/164
Processing video 146/164
Processing video 147/164
Processing video 148/164
Processing video 149/164
Processing video 150/164
Processing video 151/164
Processing video 152/164
Processing video 153/164
Processing video 154/164
Processing video 155/164
Processing video 156/164
Processing video 157/164
Processing video 158/164
Processing video 159/164
Processing video 160/164
Traceback (most recent call last):
  File ""/home/neu307/wh/ROMP-master/src/core/test.py"", line 235, in <module>
    main()
  File ""/home/neu307/wh/ROMP-master/src/core/test.py"", line 224, in main
    demo.process_video(args.input_video_path)
  File ""/home/neu307/wh/ROMP-master/src/core/test.py"", line 114, in process_video
    outputs = self.single_image_forward(frame)
  File ""/home/neu307/wh/ROMP-master/src/core/test.py"", line 98, in single_image_forward
    meta_data = img_preprocess(image, '0', input_size=args.input_size, single_img_input=True)
  File ""/home/neu307/wh/ROMP-master/src/core/../lib/models/../utils/demo_utils.py"", line 31, in img_preprocess
    image = image[:,:,::-1]
TypeError: 'NoneType' object is not subscriptable

Process finished with exit code 1
"
Train on custom 2D pose dataset,Arthur151/ROMP,2021-10-31 15:16:19,3,,92,1040500153,"Hi and thanks for great work !

I would like to finetune the model on some custom 2D pose dataset. From what I read you trained on 3D and 2D pose datasets so that should be possible, right ?

Best
Alexander"
How to create a new dataloader for your own dataset.,Arthur151/ROMP,2021-10-18 04:58:18,3,documentation,88,1028628139,"Hi, 

I am wondering if there is a way to use the results as training data for this model itself. 

Thank you!"
pytorch3d texture render?,Arthur151/ROMP,2021-09-29 07:20:55,1,,83,1010588076,"wonderful Project! thank you very much.
I found that you have vedo and open3d visualizer which have texture render , but for pytorch3d you only render a simple color. I found the pytorch3d official web and can not find a good way to how to use. I think maybe this is easier for you :
```
        verts, faces = verts.to(self.device), faces.to(self.device)
        verts_rgb = torch.ones_like(verts)
        if len(colors.shape) == 1:
            verts_rgb[:, :] = colors
        elif len(colors.shape) == 2:
            verts_rgb[:, :] = colors.unsqueeze(1)
        textures = TexturesVertex(verts_features=verts_rgb)
        verts[:,:,:2] *= -1
        meshes = Meshes(verts, faces, textures)
```
I think only need  to change the textures"
some questions about the train code,Arthur151/ROMP,2021-09-28 11:20:26,10,,82,1009620413,"1 romp/lib/dataset/image_base.py
  你好 请问下，valid_masks   = np.zeros((self.max_person, 6), dtype=np.bool)  第二维度一共是6种类型，不知道523行这里为什么有判断，并且下标是6，超过了valid_masks的范围。
![image](https://user-images.githubusercontent.com/17424385/135076899-94ad36c4-c176-444e-93af-04bfab995b23.png)

![image](https://user-images.githubusercontent.com/17424385/135076842-698b0e62-cb97-4f35-8dec-cc0cb1d86bce.png)
https://github.com/Arthur151/ROMP/blob/ee5e2f21f35a1072327a11ecd4a36c0c64d805e1/romp/lib/dataset/image_base.py#:~:text=if%20r%5B%27valid_masks%27%5D%5B0%2C0%2C6%5D%3A"
eft,Arthur151/ROMP,2021-09-26 08:48:09,17,,78,1007325002,"Thanks for sharing, there are some questions about the dataset.Some datasets, such as coco, LSP, use pseudo 3D annotations. Will you share these pseudo-annotations, such as data/eft_fit/ coco2014-all-ver01.json mentioned in the code"
"AICH, UP and OH datasets",Arthur151/ROMP,2021-09-16 04:32:16,5,,75,997760051,"Thanks for your outstanding work and repo!

In your paper, you have noticed that AICH, UP and OH datasets are also included during training (and the dataloaders are provided). However, it seems that they are missing in the data preparation guide and cannot be downloaded. 

Is it possible that you can provide these data as well? 

Thanks again!"
About how to get multi-person 2D pose estimation    accuracy rating ,Arthur151/ROMP,2021-08-04 06:12:09,14,,67,959940067,"How can 2-D pose estimation be realized in real time，Whether to use Openpose（https://github.com/CMU-Perceptual-Computing-Lab/openpose），Or any other way，I found this when I was testing
![image](https://user-images.githubusercontent.com/48466610/128130801-aae9e85d-26e7-4dc3-9bac-6f735654ec36.png)


What measures do I need to take to prevent this from happening，Looking forward to your reply"
fbx results not so good,Arthur151/ROMP,2021-07-07 07:06:17,8,,57,938562199,"Hi, thanks for your generous sharing of this wonderful work, and I have tried to test on my own video.
when only look the mesh that projected onto the image,  the results look good, but when converting to fbx and see in blender, although the pose looks normal, the root motion has big problems like drifting and jitter.

It seems this root motion problem are common in methods based on smpl, and I have noticed that the[ MTC](https://github.com/CMU-Perceptual-Computing-Lab/MonocularTotalCapture) and [CHD](https://github.com/davrempe/contact-human-dynamics) have given some methods to improve the performance. but they are time-consuming.

I wonder if there may be a way to do some work combine these two works or you have some other ideas, thanks~"
"How to generate targets for Body Center heatmap C_m, Camera map A_m, and SMPL map S_m",Arthur151/ROMP,2021-06-30 02:51:13,11,,55,933255976,"Hi authors,

First of all, I would like to thank you so much for your great work ROMP!

I am facing issues related to training. Could you share the code for generating the ground truths for training the models?

Thank you in advance!"
Wrong Axis Unity Import,Arthur151/ROMP,2021-06-22 17:52:07,9,,53,927491923,"Hello! 

Firstly, greatly appreciate all the work you have put into this project.

I would have two questions for you:

1. I want to import an .fbx into Unity but the axis are wrong. The fbx was converted using the Blender python script you have provided. Even in Blender the fbx has wrong axis. If I try to rotate the Armature in Blender, when I play the animation, it gets back to the original wrong pose. In Unity it seems that it should be rotated with 90 degrees around the Z axis and in Blender it should be rotated 90 degrees around the Y Axis. 

2. Also, is it possible for us to remove the camera distance from the FBX? As you can see in the Blender picture, the armature is translated with some distance from the center of the world, would it be possible to center it? 

Thank you! 
![fbx1](https://user-images.githubusercontent.com/48161631/122974843-6ef6ed00-d39b-11eb-8330-ea2dbba7139e.png)
![fbx2](https://user-images.githubusercontent.com/48161631/122974858-71f1dd80-d39b-11eb-98ab-01f41d28626d.png)
"
Question about getting single person data from custom video,Arthur151/ROMP,2021-06-08 08:24:16,8,,49,914568177,"Hello! I have recently run your work. I want to get one main person from my video. However there are some other persons showing up during the video sometimes. How should i set the configs? I have tried set ""multi_person"" to False, but seems it won't work.
It would be so nice if you could help me here. Thank you.
"
Strange Result on custom video,Arthur151/ROMP,2021-05-12 12:12:46,4,,44,890020607,"Hello @Arthur151,
Congratulations on the great work and Thanks for making the code available for use.

I tried running the inference script on a custom video via: 
CUDA_VISIBLE_DEVICES=0 python core/test.py --gpu=0 --configs_yml=configs/video.yml

But the result is strange.. 
Is this because of the camera view ??
If yes then, Is there any way to tune the model for such camera views??
![Screenshot from 2021-05-12 17-35-47](https://user-images.githubusercontent.com/27979479/117972888-5087e580-b349-11eb-84af-3eef9081391f.png)"
3D model jittering ,Arthur151/ROMP,2021-05-03 10:06:42,8,,40,874431327,The model runs perfectly but I noticed there is a lot of jittering in the output. Is there any way to remove the jittering?
A tutorial for how to run the code?,Arthur151/ROMP,2021-04-13 04:45:10,1,,35,856563953,"Hi, is it possible if you can post a step-by-step tutorial on how to run and install the program? Thank you in advance."
A problem about head pose estimation,Arthur151/ROMP,2021-02-26 10:04:52,6,,28,817217468,"I haven't read the source code yet.
I ran the demo, and found that the head pose estimation is not supported, which confirmed in the results of the experimental section of paper.
Would you like to know whether the support for head pose estimation will be increased in the future?
Thanks."
axis in SMPL result,Arthur151/ROMP,2021-01-12 04:00:16,17,,24,783888751,"I'm trying to animate 3D character in Unity using SMPL 24 joints data. Everything is going well but the only problem is that there is different in Unity and SMPL final data axis. 

Below axis are from SMPL data (I draw 3D skeleton using 24 joints from CenterHMR)
![Screenshot from 2021-01-12 11-50-04](https://user-images.githubusercontent.com/58796370/104267687-14449580-54cd-11eb-99e6-df4e3e0e171b.png)

Below image is about axis in Unity
![WeChat Image_20210112115115](https://user-images.githubusercontent.com/58796370/104267710-245c7500-54cd-11eb-8045-3ffb8a201009.png)

Seems that we need to change the Y and Z axis in SMPL results. 

So suggest me that we should swap/change axis (Y and Z) in `j3d_smpl24` list that I got from CenterHMR from [here](https://github.com/Arthur151/CenterHMR/blob/00c43337855c41fffae7ea552dda9be05cacd282/src/core/test.py#L79) or in Unity side?"
function to export to bvh and use in blender,Arthur151/ROMP,2021-01-09 17:10:22,2,,23,782647411,"
you can add a function to export to bvh and use in blender"
"How to use pose (72,) joint rotations?",Arthur151/ROMP,2021-01-04 13:24:30,6,,22,778092626,"Hey @Arthur151,
I was able to understand the joint positions and order in SMPL24 and I know that the pose (72,) is the rotation values for 24 joints in SMPL.
I am trying to convert the output into a BVH file, but I don't understand, how to use these rotation values while converting into BVH. Do i need to change the format of the joint rotations. If yes, then to which format and how?
Can you please guide me on this?
"
How to run CenterHMR on CPU?,Arthur151/ROMP,2020-12-21 12:19:40,10,,20,772126120,First of all thanks @Arthur151. I'm able to generate 3D meshes for my videos however I'm curious to know if we can also run this on CPU. it would be very kind of you if you can guide me on this. 
Poorly Segmented body,Arthur151/ROMP,2020-12-17 09:03:24,1,,19,769728924,"Poor results on some images, one of the examples is attached below:


![images-fat1](https://user-images.githubusercontent.com/15937296/102466168-7813a400-4070-11eb-81a9-aceeb39d2e1c.jpg)

How can we improve it"
Does Coordconv play a key role when training?,Arthur151/ROMP,2020-12-11 03:21:44,2,,18,761843017,"I removed Coordconv when training centermap task independently, the training loss similar to the network with Coordconv, but the centermap seems to get bad on the test set."
Use different input size on train and inference,Arthur151/ROMP,2020-12-11 00:40:15,7,,17,761748209,"We use 256 image input to your trained 512 input model, and adjust the coordconv size, but the result seems not right. Does your network need the same input size on train and test stage?"
Some clarifications and .npz files visualization,Arthur151/ROMP,2020-12-08 18:08:14,11,question,15,759663609,"I would like to visualize the body landmarks projected onto the 2d image however I have some problems understanding how the values are stored. When I run the demo script I get a file called .npz and I can load it with numpy. Inside it I found f->results where two dictionaries are stored one for each person in the image. Inside each dictionary I can see 'cam' (3,), 'pose' (72,), 'betas' (10,), 'j3d_smpl24' (24, 3), 'j3d_op25' (25, 3), and 'verts' (6890, 3) variables (I put their corresponding shapes in parentheses). From your paper I thought 'pose' variable would be of length 132 (22 landmarks x 6D). Anyway, I assume you have saved 24 landmarks in 3D which makes an array of length 72. From the 'cam' variable (tx, ty = cam[1:]) I calculated the center for each people trivially since those numbers are normalized between -1 and 1 just as you mentioned in your paper. However when it comes to visualizing 'pose', 'j3d_smpl24' or 'verts' variables I ran into some problems. Could you explain how each of these variables store their data? Apparently they are not normalized between -1 and 1. Also I'm having some problems understanding the first number in 'cam' variable which should correspond to scale according to the paper. In the paper it is said that this variable ""reflects the size and depth of the human body to some extent."". How can this scale be used to visualize pose points? In my example I get 5.51 for one person and -5.156 for the other. Would you also explain what does a negative scale represent?"
how to use the generated results for MAYA,Arthur151/ROMP,2020-10-21 08:07:55,4,,8,726244312,"After runing the demo code, I can get the .obj and .npz files. 
How to put these files in MAYA for further usage.

Thanks"
Looking for appendices mentioned in the paper for more details,Mattdl/ContinualPrototypeEvolution,2021-11-23 11:24:26,1,,2,1061139653,"Hi Authors,

Your work looks very interesting and I would like to dive more into the details, however, I was not able to refer the appendices mentioned in your paper. Can you please direct me to the version where I can find the Appendices.

Regards,
tomar-s"
What is the function of train.txt file,HugoBA92/DeePSD,2022-03-05 12:02:30,0,,6,1160328385,
Could you please upload the testing script and pretrained model?,HugoBA92/DeePSD,2021-11-09 12:33:14,1,,4,1048554626,"Hi, @HugoBA92 
Thanks for sharing your great work. But could you please provide the testing script and pretrained model?  "
can you provide the evaluation code?,wbw520/scouter,2021-12-03 08:14:36,0,,8,1070329001,"I am very happy to see such an interesting work open source. Does the evaluation in your paper refer to one image or all ImageNet images? How to evaluate the various indicators of ImageNet, can you provide the evaluation code?"
Training Time,wbw520/scouter,2021-10-09 12:42:16,0,,7,1021699836,"Hi, 
Initially, it's a great work congratulations. Secondly, How much does your training take and what was the data amount for the imageNet's first 100 classes ?

Thanks in advance"
hyper parameters for Table8 results,wonchulSon/DGKD,2022-03-01 02:26:38,0,,1,1154776686,"Hi,
      Thx for your resource code. But I could not find the hyper parameters for Table 8 results of different teacher-student model pairs, such as the KD KL loss weight and GT cross entropy loss weight for different models and algorithms. Looking forward to your reply, thx a lot."
Can TResNet run under dataparallel?,mrT23/TResNet,2022-01-02 14:10:17,0,,43,1092028052,
"A question about ""Dataset Render"".",hansen7/OcCo,2022-10-15 05:05:35,0,,33,1410047593,"Thank you for organizing a good code. I have a question about 'dataset render'. The code of processing the dataset in the 'Render' folder looks about the code of ModelNet40. 'PC_Normalisation.py' file should be to transforme to '.ply' to '.obj'. However, the data in the original ModelNet40 was not '.ply' format. Can you provide an unpreprocessing ModelNet40 dataset? Do I need to transform the files in the Model40 to '.ply' format? "
the log of train_completion.py,hansen7/OcCo,2022-03-30 15:54:46,0,,30,1186665919,"Hi @hansen7 ,
Thanks for sharing this great work with us. 
I have a question regarding the output log during pretraining. The detail is [here](https://gist.github.com/zshyang/08fb47c7f4aa2835da25e6e98349cf51).
The question is, it seems that the loss did not drop too much or its range is varied from 0.03 to 0.04. Did this happen to your pretraining as well?
But visually, I could see some improvements in the `plots` folder.
Best regards"
problem about partseg,hansen7/OcCo,2021-12-08 03:09:31,0,,26,1073961823,"![image](https://user-images.githubusercontent.com/95679927/145141468-76a1b6c9-b17e-44b5-b895-10b631d899a6.png)
![image](https://user-images.githubusercontent.com/95679927/145141476-e266fdc3-935d-4877-9141-79a8216bf606.png)
There is no mode containing partseg
Please have a look. Thank you!"
How the ModelNet40_test_1024_middle.lmdb was generated?,hansen7/OcCo,2021-11-14 06:14:14,1,Resolved,25,1052857781,
Super parameter to ShapeNet  car  dataset,hansen7/OcCo,2021-10-19 08:50:55,6,Resolved,23,1030033101," hello, when I reproduce the pre-training experiment on your shapenet car data set, I found that the training does not converge very well. I think it is related to the parameter of piece-wise_constant, wish  you can provide the  hyperparameter  setting?"
Question about the dataset used for Point Cloud Completion,hansen7/OcCo,2021-08-24 06:22:18,1,Resolved,17,977747486,Thanks for your awesome work. Do you use the code in the render directory to generate complete point cloud and partial point cloud from 3D models for the training of point cloud completion?
Contents for next PR,hansen7/OcCo,2021-01-04 07:01:18,0,InProgress,13,777870108,"Basically in consistent with the updated iccv submission version:

1. Few-shot learning
2. Network dissection and adjusted mutual information
3. Object-Level Contrastive Learning
4. Readme and website"
What Makes a Good Pre-Training on Points Cloud?,hansen7/OcCo,2020-11-16 00:38:45,1,Literatures,9,743387662,"**General Interpretability:**
- [interpretable ml book](https://christophm.github.io/interpretable-ml-book/shapley.html), specifically sections on `learned features`, `Shapley values`, `Influential Instance`
- ""Network dissection: Quantifying interpretability of deep visual representations"", CVPR 2017
- ""Feature Visualisation"", Olah, et al., Distill 2017.
- [Bolei](http://bzhou.ie.cuhk.edu.hk)'s Portfolio
- [Chiyuan](https://pluskid.org)'s Portfolio (also, transfer learning)

**General Pre-Training:** 
- ""Rethinking ImageNet Pre-training"", ICCV 2019
- ""Rethinking Pre-training and Self-training"", NeurIPS 2020
- ""What is being transferred in transfer learning?"", NeurIPS 2020
- ""What Makes Instance Discrimination Good for Transfer Learning?"", ICLR 2021 Sub

**Ideas from Contrastive Learning:**
- [Yonglong's Portfolio](http://people.csail.mit.edu/yonglong/)

**Point Cloud Specific:**
- ""Rotation Invariant Convolutions for 3D Point Clouds Deep Learning"", 3DV 2019
- ""Quaternion Equivariant Capsule Networks for 3D Point Clouds"", ECCV 2020
- ""Label-Efficient Learning on Point Clouds using Approximate Convex Decompositions"", ECCV 2020
- ""On the Universality of Rotation Equivariant Point Cloud Networks"", ICLR 2021 Sub

**Extensions**:
- ""Neural Similarity Learning"", NeurIPS 2019"
Update on Literatures,hansen7/OcCo,2020-08-03 14:05:27,4,Literatures,2,672101145,"**Point Cloud Completion**
- ""Topnet: Structural point cloud decoder"", CVPR 2019
- ""3D Shape Completion with Multi-view Consistent Inference"", AAAI 2020
- ""Morphing and Sampling Network for Dense Point Cloud Completion"", AAAI 2020
- ""Cascaded Refinement Network for Point Cloud Completion"", CVPR 2020
- ""PF-Net: Point Fractal Network for 3D Point Cloud Completion"", CVPR 2020
- ""Point Cloud Completion by Skip-Attention Network With Hierarchical Folding"", CVPR 2020
- ""GRNet: Gridding Residual Network for Dense Point Cloud Completion"", ECCV 2020
- ""SoftPoolNet: Shape Descriptor for Point Cloud Completion and Classification"", ECCV 2020
- ""Weakly-supervised 3D Shape Completion in the Wild"", ECCV 2020
- ""Variational Relational Point Completion Network"", CVPR 2021"
GL2Vec - Edge Features,benedekrozemberczki/karateclub,2022-11-05 23:40:41,0,,116,1437222676,Is there a way to make use of edge features while using GL2Vec? 
torch.linalg_cholesky error,patel-zeel/AAAI22,2022-03-24 17:44:44,6,,2,1179834148,"Thanks for your kind work

I'm running you code at [nonstat_gp_cat](https://github.com/patel-zeel/AAAI22/tree/main/nonstat_gp_cat). However, the code constantly throws an error: 

> torch.linalg_cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 314 is not positive-definite).

It seems that you have code to reinitialize all the parameters at [gp_train.py](https://github.com/patel-zeel/AAAI22/blob/main/nonstat_gp_cat/gp_train.py):

>             # except Exception as e:
>             #     loss_val = np.inf
>             #     torch.cuda.manual_seed(rand)
>             #     rand+=1
>             #     init_vars(model)
>                 pprint('Iter', i, 'Batch', bi, 'Failed with', e, 're inited the params')
>                 # break

I also tried this for many time, but the model still cannot get the right initializations. This reinitialization of all the parameters is the only way to solve this problem? Do you have other suggestions?

Thanks in advance :)"
Failed to load checkpoint files problem,HsuWanTing/unified-summarization,2020-05-18 07:53:54,0,,30,619989463,"; Input/output error
INFO:tensorflow:Failed to load checkpoint from log\end2end\exp_sample\train. Sleeping for 10 secs...

Did anyone meet the above problem while merely run sh end2end.sh? at this time, sinece mode='train' in thee script, how come loading?"
Decoded text and reference text are not pointed at the same article in evalall,HsuWanTing/unified-summarization,2020-04-29 21:58:14,0,,29,609387199,"When I run evalall, the text in decoded and the text in reference do not point at the same article.
I checked that the data binaries are written correctly (article + @highlight + reference). 

This problem occurs only with end2end training.

Do you have any suggestion?"
About the evaluation method for the selector. ,HsuWanTing/unified-summarization,2020-03-08 01:29:58,2,,27,577428070,Why is the model restored in each evaluation on the selector the model saved in the last iteration? So how to choose the best model in training?
Selector model train,HsuWanTing/unified-summarization,2019-10-01 07:22:39,6,,25,500713018,"I have a problem when I train the selector model.
The console always output recall:0, ratio:1, thres:0
![image](https://user-images.githubusercontent.com/28684483/65942101-28db5680-e45f-11e9-9a21-e21c54c1f982.png)
Is this a normal phenomenon?
"
Is it possible to run this on Tensorflow Version >= 1.5,HsuWanTing/unified-summarization,2019-01-28 09:09:41,2,,8,403712110,"Is it possible to run this on Tensorflow Version >= 1.5 where CUDA 9 is supported ?

[
![image](https://user-images.githubusercontent.com/46809697/51825179-3ce81a80-230a-11e9-8224-45df8aa73b1c.png)
](url)"
Cannot restore pretrained model,HsuWanTing/unified-summarization,2018-07-31 10:48:07,4,,3,346133873,"Thank you for the great work!

I'm trying to use the pretrained model to decode my own preprocessed data (I do not need to evaluate it). However, I'm getting this exception when running main.py:


`INFO:tensorflow:Loading checkpoint ./end2end/train/bestmodel-51000
INFO:tensorflow:Restoring parameters from ./end2end/train/bestmodel-51000
INFO:tensorflow:Failed to load checkpoint from ./end2end/train. Sleeping for 10 secs...`


I figured this happens in `util.py` in `load_ckpt()`, when it gets to ` saver.restore(sess, ""./end2end/train/bestmodel-51000"")
`


I'm running main.py in the following way:
`python2 main.py --mode eval --model end2end --vocab_path ./data/finished_files/vocab --data_path ./data/pgn/output/finished_files/chunked/test_* --decode_method greedy  --eval_method loss --log_root . --pretrained_selector_path ./end2end/train/extractor_model --pretrained_rewriter_path ./end2end/train/abstracter_model --single_pass 1`


So, my parameters are:
`{'adagrad_init_acc': 0.1,
 'batch_size': 16,
 'beam_size': 4,
 'convert_to_coverage_model': False,
 'cov_loss_wt': 1.0,
 'coverage': False,
 'data_path': './data/pgn/output/finished_files/chunked/test_000.bin',
 'decode_method': 'greedy',
 'emb_dim': 128,
 'eval_ckpt_path': '',
 'eval_gt_rouge': False,
 'eval_method': 'loss',
 'exp_name': '',
 'hidden_dim_rewriter': 256,
 'hidden_dim_selector': 200,
 'inconsistent_loss': True,
 'inconsistent_topk': 3,
 'load_best_eval_model': False,
 'log_root': '.',
 'lr': 0.15,
 'max_art_len': 50,
 'max_dec_steps': 100,
 'max_enc_steps': 600,
 'max_grad_norm': 2.0,
 'max_select_sent': 20,
 'max_sent_len': 50,
 'max_train_iter': 10000,
 'min_dec_steps': 35,
 'min_select_sent': 5,
 'mode': 'eval',
 'model': 'end2end',
 'model_max_to_keep': 5,
 'pretrained_rewriter_path': './end2end/train/abstracter_model',
 'pretrained_selector_path': './end2end/train/extractor_model',
 'rand_unif_init_mag': 0.02,
 'save_model_every': 1000,
 'save_pkl': False,
 'save_vis': False,
 'select_method': 'prob',
 'selector_loss_wt': 5.0,
 'single_pass': True,
 'start_eval_rouge': 30000,
 'thres': 0.4,
 'trunc_norm_init_std': 0.0001,
 'vocab_path': './data/finished_files/vocab',
 'vocab_size': 50000}`


I'm using Tensowflow 1.1.0 and python 2.7.

- Is this the right way to configure the settings in order to decode my own documents?
- How to solve the exception issue?
Any help is appreciated, thank you!"
"How many steps have you trained at stage1 and stage2, respectively?",alex04072000/CyclicGen,2020-05-15 08:54:59,2,,14,618809251,"@alex04072000 
Does the steps in stage1 need to be the same as stage2?"
Error while testing the pretrained model,alex04072000/CyclicGen,2020-04-17 12:20:59,0,,13,601936528,"```
InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: Incompatible shapes: [65536] vs. [131072]
	 [[{{node Cycle_DVF/interpolate/add_4}}]]
	 [[Cycle_DVF/add_3/_135]]
  (1) Invalid argument: Incompatible shapes: [65536] vs. [131072]
	 [[{{node Cycle_DVF/interpolate/add_4}}]]

```
Facing this issue.
Any help appreciated!"
Pre-trained model restored from chkpt model,alex04072000/CyclicGen,2020-01-09 02:54:52,0,,12,547217650,"Error: 
I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Pre-trained model restored from ./ckpt/ckpt/CyclicGen/model

facing above issue while executing run file-
python run.py --pretrained_model_checkpoint_path=./ckpt/ckpt/CyclicGen/model --first=./myData/ucf101_interp_ours/1/frame_00.png --second=./myData/ucf101_interp_ours/1/frame_01_gt.png --out=./myData/ucf101_interp_ours/1/Output/out.png

Any help appreciated!!
"
Video demo code,alex04072000/CyclicGen,2019-10-08 16:56:32,1,,11,504162669,"Hi,
I saw your video which was very cool. But can you share your demo code that given a video, it generates the output video?

Currently I see `run.py` only gets two consequitive frames as input. So I'm not sure how to produce a video. Thanks."
Should I replace CyclicGen_train_stage1's `ucf101_train_files_frameX.txt` to `frameX.txt`  ?,alex04072000/CyclicGen,2019-08-22 12:23:05,0,,10,483961876,"Should I replace CyclicGen_train_stage1's `ucf101_train_files_frameX.txt` to `frameX.txt` generated by following steps ?
```
# 1. download UCF101 and rename:
wget https://www.crcv.ucf.edu/data/UCF101/UCF101.rar && unrar x UCF101.rar && mv UCF-101 UCF101
# 2. download and prepare train/test list:
mkdir ucfTrainTestlist && mv ucf101_train_test_split/*.txt  ucfTrainTestlist
# 3. split UCF101 to tran/test:
python3 1_move_file.py
# 4. split .avi to .png:
brew install parallel
parallel -j 12 ./extract_only.sh ::: $( find ./ -name *.avi )
# 5. generate frame1.txt frame2.txt frame3.txt
python3 2_filter_psnr.py
```"
Different evalutaion result on UCF dataset using frames generated by run.py,alex04072000/CyclicGen,2019-08-16 20:55:00,2,,9,481782293,"Hi!

I used run.py to generate interpolated frames on UCF101 dataset first and then calculated average PSNR. However, there is a little difference between my result and yours from the paper. The model I use is CyclicGen_model.py and I also test your result images. Without motion mask, the difference is around 0.4dB. So is it because of the motion mask or other fact that has influence on the result images?"
The motion of video is not smooth as your demo video.,alex04072000/CyclicGen,2019-06-29 05:05:11,0,,7,462266184,"I just tested on video surf in DAVIS dataset.
I don't know where went wrong, but, like the title, the motion is not as smooth as your demo.
The interpolated frame seems closer to Frame 1 than Frame 3, the interpolated time probably is not exact 0.5
*Regarding the modification, I refrain to change significantly, just make a loop to run on video.
Here are the modified run test on video and result.

Video: https://drive.google.com/open?id=1Hg8e1YvIBYM4lzGe71w4ke4t6yfQSJvL

`

    """"""Train a voxel flow model on ucf101 dataset.""""""
    from __future__ import absolute_import
    from __future__ import division
    from __future__ import print_function

    import numpy as np
    import os
    import tensorflow as tf
    from datetime import datetime
    from CyclicGen_model_large import Voxel_flow_model
    import scipy as sp
    import cv2
    from vgg16 import Vgg16

    FLAGS = tf.app.flags.FLAGS

    # Define necessary FLAGS
    tf.app.flags.DEFINE_string('pretrained_model_checkpoint_path', None,
                            """"""If specified, restore this pretrained model """"""
                            """"""before beginning any training."""""")
    tf.app.flags.DEFINE_integer('batch_size', 1, 'The number of samples in each batch.')
    tf.app.flags.DEFINE_string('video', '',
                            """"""video"""""")
    tf.app.flags.DEFINE_string('out', '',
                            """"""output image """""")


    def normalize(img):
        """"""Read image from file.
        Args:
        filename: .
        Returns:
        im_array: .
        """"""
        # im = sp.misc.imread(filename, mode='RGB')
        return img / 127.5 - 1.0


    def test(video_dir, out_dir):

        _name = os.path.basename(video_dir).split('.')[0]
        cap = cv2.VideoCapture(video_dir)
        fcounter = 0
        _, first = cap.read()
        first = cv2.cvtColor(first, cv2.COLOR_BGR2RGB)
        fps = cap.get(cv2.CAP_PROP_FPS)
        h,w,_ = first.shape
        print('HxW: {}, FPS: {}'.format((h,w), fps))

        fourcc = cv2.VideoWriter_fourcc(*'XVID')
        out = cv2.VideoWriter(os.path.join(out_dir, _name + '_x2.avi'), fourcc, fps*2, (w,h))

        while True:
            _, second = cap.read()
            if second is None:
                break

            second = cv2.cvtColor(second, cv2.COLOR_BGR2RGB)
            data_frame1 = np.expand_dims(normalize(first), 0)
            data_frame3 = np.expand_dims(normalize(second), 0)

            H = data_frame1.shape[1]
            W = data_frame1.shape[2]

            adatptive_H = int(np.ceil(H / 32.0) * 32.0)
            adatptive_W = int(np.ceil(W / 32.0) * 32.0)

            pad_up = int(np.ceil((adatptive_H - H) / 2.0))
            pad_bot = int(np.floor((adatptive_H - H) / 2.0))
            pad_left = int(np.ceil((adatptive_W - W) / 2.0))
            pad_right = int(np.floor((adatptive_W - W) / 2.0))

            print(str(H) + ', ' + str(W))
            print(str(adatptive_H) + ', ' + str(adatptive_W))

            """"""Perform test on a trained model.""""""
            with tf.Graph().as_default():
                # Create input and target placeholder.
                input_placeholder = tf.placeholder(tf.float32, shape=(None, H, W, 6))

                input_pad = tf.pad(input_placeholder, [[0, 0], [pad_up, pad_bot], [pad_left, pad_right], [0, 0]], 'SYMMETRIC')

                edge_vgg_1 = Vgg16(input_pad[:, :, :, :3], reuse=None)
                edge_vgg_3 = Vgg16(input_pad[:, :, :, 3:6], reuse=True)

                edge_1 = tf.nn.sigmoid(edge_vgg_1.fuse)
                edge_3 = tf.nn.sigmoid(edge_vgg_3.fuse)

                edge_1 = tf.reshape(edge_1, [-1, input_pad.get_shape().as_list()[1], input_pad.get_shape().as_list()[2], 1])
                edge_3 = tf.reshape(edge_3, [-1, input_pad.get_shape().as_list()[1], input_pad.get_shape().as_list()[2], 1])

                with tf.variable_scope(""Cycle_DVF""):
                    # Prepare model.
                    model = Voxel_flow_model(is_train=False)
                    prediction = model.inference(tf.concat([input_pad, edge_1, edge_3], 3))[0]

                # Create a saver and load.
                gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)
                sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))

                # Restore checkpoint from file.
                if FLAGS.pretrained_model_checkpoint_path:
                    restorer = tf.train.Saver()
                    restorer.restore(sess, FLAGS.pretrained_model_checkpoint_path)
                    print('%s: Pre-trained model restored from %s' %
                        (datetime.now(), FLAGS.pretrained_model_checkpoint_path))

                feed_dict = {input_placeholder: np.concatenate((data_frame1, data_frame3), 3)}
                # Run single step update.
                prediction_np = sess.run(prediction, feed_dict=feed_dict)

                output = prediction_np[-1, pad_up:adatptive_H - pad_bot, pad_left:adatptive_W - pad_right, :]
                output = np.round(((output + 1.0) * 255.0 / 2.0)).astype(np.uint8)
                output = np.dstack((output[:, :, 2], output[:, :, 1], output[:, :, 0]))
                # cv2.imwrite(out, output)
                out.write(cv2.cvtColor(first, cv2.COLOR_RGB2BGR))
                out.write(output)
            first = second
        out.write(cv2.cvtColor(first, cv2.COLOR_RGB2BGR))
        cap.release()
        out.release()

    if __name__ == '__main__':
        #os.environ[""CUDA_VISIBLE_DEVICES""] = """"

        video = FLAGS.video
        out = FLAGS.out

        test(video, out)

`"
"group[""initial_lr""]: unsupported operand type(s) for *: 'NoneType' and 'int'",greatlog/DAN,2022-08-19 09:02:18,1,,44,1344160084,"`Traceback (most recent call last):
  File ""C:\dev\PycharmProjects\DAN\codes\config\DANv2\train.py"", line 349, in <module>
    main()
  File ""C:\dev\PycharmProjects\DAN\codes\config\DANv2\train.py"", line 207, in main
    model = create_model(opt)  # load pretrained model of SFTMD
  File ""C:\dev\PycharmProjects\DAN\codes\config\DANv2\models\__init__.py"", line 17, in create_model
    m = M(opt)
  File ""C:\dev\PycharmProjects\DAN\codes\config\DANv2\models\blind_model.py"", line 90, in __init__
    lr_scheduler.MultiStepLR_Restart(
  File ""C:\dev\PycharmProjects\DAN\codes\config\DANv2\models\lr_scheduler.py"", line 27, in __init__
    super(MultiStepLR_Restart, self).__init__(optimizer, last_epoch)
  File ""C:\dev\PycharmInterpreters\PyTorchStar\lib\site-packages\torch\optim\lr_scheduler.py"", line 77, in __init__
    self.step()
  File ""C:\dev\PycharmInterpreters\PyTorchStar\lib\site-packages\torch\optim\lr_scheduler.py"", line 154, in step
    values = self.get_lr()
  File ""C:\dev\PycharmProjects\DAN\codes\config\DANv2\models\lr_scheduler.py"", line 34, in get_lr
    return [
  File ""C:\dev\PycharmProjects\DAN\codes\config\DANv2\models\lr_scheduler.py"", line 35, in <listcomp>
    group[""initial_lr""] * weight for group in self.optimizer.param_groups
TypeError: unsupported operand type(s) for *: 'NoneType' and 'int'`


PyTorch 1.11 and 1.5. tried both.
"
training dataset,greatlog/DAN,2022-04-08 16:33:47,0,,43,1197535754,请问在setting1下训练不同分辨率模型使用的x2HR.lmdb、x3HR.lmdb、x4HR.lmdb 是相同的么，如果不相同，各自是怎么得到的呢，感谢解答！
关于训练出现loss变为NAN的问题,greatlog/DAN,2021-12-15 11:57:05,0,,42,1080950851,"作者你好，谢谢你非常不错的工作，
   在我训练时大约在60k个iter会出现loss都变成NAN的情况并且按照 [issue#8](https://github.com/greatlog/DAN/issues/8)  的解决办法选取checkpoint继续train 但是一段时间后还是会出现loss变为NAN的情况（如下图），似乎无法解决，想请教一下您怎么train让模型收敛到最终的结果的？
![image](https://user-images.githubusercontent.com/48617091/146182365-6065cbf9-d0dc-467e-bc9c-54321e76eb96.png)


  "
请问迭代求解的传统方法指的是？,greatlog/DAN,2021-11-26 15:42:01,1,,40,1064633361,"非常感谢您的工作，我受益匪浅！关于论文，我想请教您是如何设计这个结构的。
在https://www.bilibili.com/video/BV1Zg411c7zn 这个talk第13:35秒，您提到这种迭代求解k和SR的方法在传统方法中很常见，请问您能给一些参考文献吗？"
Image in greyscale : 1-Channel training and testing,greatlog/DAN,2021-11-10 13:50:13,1,,39,1049857930,"Hello,

Thank you for this implementation. I would like to train DAN on my own dataset which is in greyscale. So I created a new yml file in options and I set color option to grey. However weight are still of the size [nbkernel, 3, kernelsize, kernelsize] instead of [nbkernel, 1, kernelsize, kernelsize]. 
Can't we change the number of channel of the weights ? and How ?

Thank you a lot."
Can you provide the config of DIN ?,xue-pai/FuxiCTR,2022-06-30 15:34:23,0,,32,1290313318,"Hi, can you provide the config of the dataset and model about DIN on 'taobao_x1_001' ?  
"
FuxiCTR v2.0 updates,xue-pai/FuxiCTR,2022-06-25 02:51:22,0,,31,1284417864,"To update:
DESTINE
InterHAt
EDCN
MaskNet
DLRM
DSSM
"
你好，我需要怎么才能跑起来你的代码,entalent/MemCap,2021-11-04 17:56:05,0,,4,1045042775,在前几周我发现了您的论文并发现其中的idea对我的研究有大的帮助，但是我发现想复现这个文章的效果是困难的。在config.py里，存在大量不存在于本仓库的路径，这些路径往往指向一些资源文件，并在代码里面都起到了相当的作用，我想相关的资源也非常的重要，如果您能看见这个issue的话，方便您更新一下资源或者提供资源的获取方式么，谢谢。
How to get sent_sg_parse?,entalent/MemCap,2020-07-19 16:59:36,6,,3,660945775,"Hi, wentian, I am very interested in MemCap. I would like to get the sent_sg_parse since use_sg is True for the train. But I failed to find where and how to generate based on the code. I am thinking it is the part called ""Stylized Sentence Decomposing"" in the paper. Any implementation on how to generate that? "
Performance,hyona-yu/SA-convlstm,2022-04-08 02:09:19,0,,1,1196743932,"Hi, did you get the performance on KTH action dataset reported in the paper?"
请教3个参数的设置,LorrinWWW/weakly-supervised-slot-filling,2022-02-22 03:11:42,3,,1,1146404231,"您好，很有幸拜读了这个工作，非常有趣&solid，以及感谢您们开源代码 ！在re-run代码的时候，发现有3个参数需要设置:

1. PATH_TO_WV_FILE, 请问这个是下载glove吗？方便说一下具体是哪个版本吗 ?
2. PATH_TO_CKPT_TO_WRITE 这个是自己设置一个保存checkpoint的路径即可吗 ?
3. PATH_TO_LM_EMB_PICKLE_OBJECT 请问这个是指下载bert embedding吗 ? 但是我下载到的bert embedding 是有3个文件.ckpt, vocab这样子，请问这个是指哪1个呢 ?

想麻烦您通过邮箱share以上3个参数的配置及对应的文件 (582326366@qq.com) ，期待您的回复，祝您paper多多，感谢。"
Inconsistent labels in DrugCombDB,AstraZeneca/chemicalx,2022-10-31 22:34:00,0,,104,1430583911,"Hi , it looks like there are a few drug pairs within the same context are labelled inconsistently . For example , drug 59691338 and drug 11960529  in EFM192B. 

"
Incorporate various dataset splits,AstraZeneca/chemicalx,2022-06-24 19:00:06,1,,102,1284091003,"I also have one suggestion for future updates of this library perhaps.  The current dataloaders, if I'm not mistaken, are not considering the different dataset split strategies.  Recent works have highlighted the importance of evaluations on different dataset splits, e.g. split pairs, split drugs, split cell lines (for synergy), etc.  It would be great to see this library also having such features."
How are the methods implemented outside of the domain they are designed for?,AstraZeneca/chemicalx,2022-06-24 18:56:23,1,,101,1284087568,"For example, `DeepSynergy` and `MatchMaker` are requiring cell line information, and they are both implemented in the DrugBankDDI & TWOSIDES benchmarks where no cell line information is available at all (and TWOSIDES is even at the patient level), with DS reaching the highest performance among all methods.  What then was the ""cell line gene expression"" component in both methods replaced within those tasks?  Also, does this ensure a fair comparison?"
Tensor's device mismatch,AstraZeneca/chemicalx,2022-03-15 05:45:29,3,,96,1169212889,"Hi! I have found a bug during the training of the caster model. It was caused by the `torch.eye` manipulation, simply it did not specify the device. When the Cuda is available, `torch.eye` will create the tensor on the CPU while the whole model is on the GPU."
Make dataset loaders on-the-fly,AstraZeneca/chemicalx,2022-01-21 12:54:30,0,dataset,58,1110438824,"I think it would be better to have the dataset download and processing happen client-side, then use `pystow` to store the results in a reliable place. This would also allow the TWOSIDES and DrugBank datasets, which require random negative sampling, to be used with multiple random seeds, e.g. to investigate the robustness of results. Further, it would allow for a more idiomatic dataset loader that's extensible to new datasets

Depends on:

- [ ] #50
- [x] #57
- [x] #59"
Add the DrugComb and DrugCombDB data cleaning,AstraZeneca/chemicalx,2022-01-17 21:49:40,2,dataset,50,1106285922,
Missing generate_datasets.py,wanganran/HybridBeam,2022-09-07 19:24:42,0,,1,1365091832,The repo is missing ```generate_datasets.py``` that is discussed in the README.
