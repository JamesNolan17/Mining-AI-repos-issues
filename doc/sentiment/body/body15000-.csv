"How does cross-validation happen in OS-CNN? Does it apply shuffling to the testset? And if not, how can I prevent overfitting?"
"I got a CSV with this format. I see the examples use the  . Is there an easy way to convert data to that format, or to import a normal CSV file?    "
"Hello Wensi,    Can your code be used for 2D CNN? If so, how? Thank you much~"
"Hello Wensi,    This is nice paper and code! Paper mentions in section 3.3 an 'ensemble' mode. Is this available? For uni and multivariate?    How would one utilize to train for 1-step ahead binary (up/down) market prediction?    thank you much~"
"Hi,  I have learnt your paper these day, which is very enlightening to me. I have a question about model selection.  I do not know which model (The model will be saved many times during the training) to choose to reproduce your results. Could you please give me some help?"
I was wondering what is the ideal method of importing these modules. Python is not recognizing the path it should take to reach the modules.
"Hi,  Thanks for your work, code and paper.  In    you define the default `paramenter_number_of_layer_list` that is used in   to train the model.    * Could you provide some detail on the meaning of values in `[8*128, 5*128*256 + 2*256*128]`?  * Did you use this specific setting for all the UCR datasets or did you vary this hyperparameter?  "
"According to the paper, from table 4, I think the Bregman stepsize is the `klcoeff` in the code, and from table5 we can set tsallis_coeff for each task. As discussed in the paper, the entropy-coeff set 0.2 in all tasks. However, since I run `python run_mujoco.py --env HalfCheetah-v3 --num_timesteps 3e6 --sgd_steps 5 --klcoeff 0.3 --lam 0.2 --tsallis_coeff 0.5 --run 0`, I got negative mean reward at last timesteps. And some other tasks are the same. Do I miss something?"
"requirements.txt has temp paths and leads to the following error.  `Invalid requirement: 'argon2-cffi @ file:///tmp/build/80754af9/argon2-cffi_1613037097816/work'  It looks like a path. Does it exist ?`    Commenting out all the temp paths also doesn't help. It leads to  `Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-build-f_NX_W/astroid/`    "
"Hi~  I have a question about the search space, why is the number of edges 14 in the file configspace.json ï¼Ÿ  "
- I've already installed nasbench301 on windows10 suceesfullly. It takes me a few moments to run the code without a bug.  I find it is easy to begin nasbench 301 with example.py. 
It might be worth mentioning that the `requirements.txt` specifically installs torch libraries for `cuda 10.2` as this took a while to debug.    I am using this with `torch 1.8.0+cu111` and it all seems to work as intended.   
"I'm a beginner of deep learning, and i need an example to process data in this codeï¼Œ which means to process json and let json become the Graph the latter process needed in NASBenchDataset. Could someone please give me an example."
"Hi,    The version of ConfigSpace required by the nasbench301 is set to exactly `0.4.12`. This clashes with NASLib, which requires at least `0.4.17`.    I've raised #16 to fix this issue.    Best  Arjun        "
"Hi all,    Many thanks for the great work!     I am aware that the training logs of the architectures used as train data for the surrogate in NAS-Bench-301 are released, but I'm wondering whether I could access the original scripts used to train the architectures from scratch? I appreciate that the training details are mentioned in App A of the paper, but with some additional augmentations used (e.g. MixUp) compared to standard DARTS procedure, I think it'd be great if I could use the exact original training script used to construct the benchmark.    If they are already in the repo, I'd be grateful if you could point me to the right place.    Many thanks in advance for your help.    Xingchen"
If I do directly `pip install nasbench301` it throws an error when I go to execute the example.    Distributor ID: Ubuntu  Description:    Ubuntu 18.04.5 LTS  Release:        18.04  Codename:       bionic     
"Hi,    When I go to install the dependencies via `cat requirements.txt | xargs -n 1 -L 1 pip install` on a new conda python 3.8 environment in Ubuntu 18.04 64-bit LTS I get a number of errors especially with respect to tensorflow version and then the various geometric libraries. See screenshots below. I am working through them but if you have any leads please let me know :-)    !     !     Distributor ID: Ubuntu  Description:    Ubuntu 18.04.5 LTS  Release:        18.04  Codename:       bionic    "
"The PyPi version does not seem to include most of the source code.    This can be confirmed by going to PyPi, looking up `nasbench301`, download files and then downloading and viewing the tar ball.  *      I can also confirm it happens locally:       There are only the 4 files located in the installed package.    Unfortunately I have not actually released a package to PyPi so I am not sure of the fix but I would imagine it is a relatively straight forward issue to do with the `setup.py` file.  "
Can you give a simple tutorial about the meaning of items in the config file? What's the meaning of the options of the item `NetworkSelectorDatasetInfo:darts:inputs_node_normal_3`? The option `0_1` refers a connection from node 0 to node 1 or just represents the encoding of the input node?
Could you release the code for generating the models based on a ConfigSpace Instance of the search space so that it is possible to retrain them yourself?     While it is probably possible to get something close to what you have used based on the code in the original DARTS repo I'd much rather get identical models if possible.     Thanks
`import nasbench301` yields         due to `/surrogate_models` not being added via `sys.path`
None
Does the file    `mean2_mhc1_pred_affinity_pivot.pkl`    list the mean predicted binding affinity from MHCflurry and NetMHCpan? Are the values listed in the dataframe represent the logistic transformed values? Is there a way to select peptides originated from a particular SARS-Cov-2 protein / ORF using the dataframe represented in the above reference .pkl file? 
"In line   and   of `population_optimization_hap2.py`, the script requires the file *preprocess_haplotype_new{mhc class}.pkl*, which is not provided in the current version of your repository. However, in an older version of the README.md I found this dropbox link   with the missing files. Can you include those files to your repository for people who come across your repo in the future? Thanks in advance ðŸ˜Š    ----------  Edit: I just checked the Mendeley Data again and found the mentioned files, I must have overlooked them completely when I first downloaded the data. Feel free to delete this issue, sorry for the circumstances. "
None
"Hello,     I've just tried to apply seqImg_CKCNN to a regression task, but the accuracy its achieving indicates the model is not acting causally. Is there something I can change to make the model return causal predictions over the inputs?    In forward method of `CKConv` it runs through `ckconv_f.causal_fftconv(x, conv_kernel, self.bias)`, suggesting it should be acting causally already, but it appears not to be.    Best,  James"
"Hello, I've found your work is impressive and still learning it.     I have 2 questions about the code:  (1) In `seqImg_CKCNN`, the last layer is `out = self.finallyr(out[:, :, -1])`. It is kind of confusing, because if only the `out[:, :, -1]` is considered, the convolution operation has no effect and the computation is wasted. In this situation, `out[:, :, -1]` is the result of point-wise multiply of input (x) and kernel. What is the role of `out[:, :, -1]` ?    (2) Why the length of kernel should be equal to input data? We all know that the size of standard CNNs should be carefully designed, but the length of kernel in your work is fixed and equal to input data. Why does it work?    Best,  Kzq    "
"Hello,    Thank you for your really interesting work. I have a question about the initiation of the CKConv Kernel:       In the code as I understand it, a 1 dimensional convolution with a kernel size of 1 is used for the Siren. Could you explain the difference of using this method instead of a regular MLP layer (using for example nn.Linear)?"
What is the 'intuition' about omega values ? Can they be learned as well ?
"Hi,  I am getting a test AUC of 0.9310 with the pre-trained model you provided on Physionet which is higher than what you reported (0.8965).  How did you get that number? Is there anything that I'm missing?    Thanks,  Iman."
"Hi,  Thank you for this awesome paper and great repository.  I wanted to try this model on MIMIC III and Physionet datasets and I see that there is a physionet.py file in the datasets folder.  If you already tried this on Physionet I would really appreciate it if you can share your code for running it with the configurations you used?    Thanks."
"Hi there!    Very cool work! I'm interested in the potential application of this work to protein structure prediction. I've a repo for doing that with RNNs currently (   ), and since it's a problem where long-range interactions in sequence space play an important role, I'd like to explore CKCNNs ( I guess they (   ) could be used as a drop-in replacement for LSTMs here:   )    What do you think? I'd like to read your thoughts on it.       Sincerely,   Eric Alcaide"
     Changing the input to         causes this error:     ```
" i tried this code, but not working        `import os,sys  ckconv_source =  os.path.join(os.getcwd(), '..')  if ckconv_source not in sys.path:      sys.path.append(ckconv_source)    import numpy as np  import torch  from torch.nn.utils import weight_norm  import ckconv  from matplotlib import pyplot as plt  in_channels = 10  out_channels = 10  hidden_channels = 32  activation_function = 'Sine'  norm_type = ''  dim_linear = 1  bias = True  omega_0 = 30.5  weight_dropout = 0.0    ckconv_example = ckconv.CKConv(in_channels,                                out_channels,                                hidden_channels,                                activation_function,                                norm_type,                                dim_linear,                                bias,                                omega_0,                                weight_dropout,                                )`      and then,    ModuleNotFoundError                       Traceback (most recent call last)    in          7 import torch        8 from torch.nn.utils import weight_norm  ----> 9 import ckconv       10 from matplotlib import pyplot as plt       11 in_channels = 10    /home/jihye/ckconv/ckconv/__init__.py in    ----> 1 from .nn import *        2 from .utils import *    /home/jihye/ckconv/ckconv/nn/__init__.py in    ----> 1 from .activation_functions import Swish, Sine        2 from .linear import Linear1d, Linear2d        3 from .norm import LayerNorm        4 from .ckconv import CKConv, KernelNet        5 from .ck_block import CKBlock    /home/jihye/ckconv/ckconv/nn/activation_functions.py in          1 # torch        2 import torch  ----> 3 from ckconv.nn.misc import Expression        4         5     ModuleNotFoundError: No module named 'ckconv.nn'      what should i do?"
None
Excellent work! Could you share the traning log in ImageNet?   Thank you very muchï¼
"Hi, thank you for sharing the work and your code. I was wondering while calculating the unseen accuracy, if there should also be a filter on the predictions not being one of the seen classes? The paper mentions that the additional classification heads get activated for unseen classes.      "
"Hello, I have a question that why here it uses the prob2 instead of prob during train function. Look forward to your reply,thanks!  `pos_prob = prob2[pos_pairs, :]    pos_sim = torch.bmm(prob.view(args.batch_size, 1, -1), pos_prob.view(args.batch_size, -1, 1)).squeeze()`  "
"Hi, thank you for the great work and for publishing the code! I have some questions about the parameter settings in SimCLR pretraining. Can you provide the pretraining codes or the config file about hyperparameter settings?"
"Hello,    Thank you for the great work and for publishing the code!  On CIFAR-100, I followed the instruction and ran the following line three times, without making any changes to the code. The maximum unseen accuracy I got was 0.4114, 0.4148, and 0.3890 for these three runs. May I ask how can I get the 43.0 reported in the paper? Any hyperparameters I need to change?     "
"Hi greatly appreciate your code and your research work! I tried your code and found it significantly improved open-set accuracy on multiple datasets, but I am confused by the difference between the MarginLoss in the codebase and the MarginLoss in the paper.   The code ""x_m = x - self.m * self.s"" where self.m = 0.2 and self.s = 10, accelerates the learning process of seen classes instead slowing down seen class learning and waiting for unseen class clustering during the begining epochs.   From my understanding it should be  ""x_m = x + self.m * self.s"". Am I right about this point?"
"Thank you for the great job. I have some question about the ce_loss, When I use MarginLoss to calculate the ce_loss, I debug and find that the output of this ""output"" is always the value of X. In this case, as a result, uncertainty  margin adaptation are not really realized. I would appreciate it if you could prove this and look forward to your reply!  !   "
"Hi, thanks for sharing the nice code!  I have some questions about ""margin loss "" as follows: the code of margin loss is the same as cross_entropy_loss. I find the value of ""output"" is the same as the value of ""x"". what does the code of margin loss do?      _class MarginLoss(nn.Module):            def __init__(self, m=0.2, weight=None, s=10):          super(MarginLoss, self).__init__()          self.m = m          self.s = s          self.weight = weight        def forward(self, **x**, target):          index = torch.zeros_like(x, dtype=torch.uint8)          index.scatter_(1, target.data.view(-1, 1), 1)          x_m = x - self.m * self.s                output = torch.where(index, x_m, x)          return F.cross_entropy(**output**, target, weight=self.weight)_"
"Hello,    Thank you for the great work and for publishing the code! I had a question about the regularization term (Equation 6) in the paper. In the paper, this regularization term is a KL divergence, but from the following line of code, seems like it is an entropy term? May I ask why the difference? Maybe I am missing something?         "
"Hi, Thank you for releasing the code.  When I was running the code,  the result on the cifar100 dataset(over 200 epochs) was as follows:  orca: seen_acc: 66.02% unseen_acc: 40.30%  The unseen_acc result was 3 percentage points lower than your paper.  Do you know why? I didn't change the parameters.  Looking forward to your response."
"Hi,   Thanks for releasing this code!  I notice that the  train_unlabel_set and the test_set of CIFAR100 are from the same file: â€˜./datasets/cifar-100-python/trainâ€™. This confuses me, and I think the test_set should come from  â€˜./datasets/cifar-100-python/testâ€™  Looking forward to your response."
"Hi.     I noticed in       `train_unlabel_set = datasets.ImageNetDataset(root=args.dataset_root, anno_file='./data/ImageNet100_unlabel_{}_{:.1f}.txt'.format(args.labeled_num, args.labeled_ratio), transform=TransformTwice(transform_train))`    and        `test_unlabel_set = datasets.ImageNetDataset(root=args.dataset_root, anno_file='./data/ImageNet100_unlabel_50_0.5.txt', transform=transform_test)`    are possibly using the same file according to the instruction.     Just want to double check that the files should be differently generated from the ImageNet val and train folder separately.     If that is the case, would you mind sharing your index file since it is randomly generated? That would make it fair for the comparison between different methods.     Thanks!       "
"Hi again. Thank you for the great work. How did you evaluate the trained model (including the ID, OOD Ads, OOD Cat, OOD Both subsets) on the validation set using the OCP framework on validation set? Is it possible to share commands/scripts used for this process?    Best regards,"
"Test subgraphx example:  explainer = SubgraphX(grace, num_classes=4, device=device,                        explain_graph=False, reward_method='nc_mc_l_shapley')    then get this error    TypeError                                 Traceback (most recent call last)  Input In [19], in  ()  ----> 1 explainer = SubgraphX(grace, num_classes=4, device=device,        2                       explain_graph=False, reward_method='nc_mc_l_shapley')    File ~\DIG\dig\xgraph\method\subgraphx.py:636, in SubgraphX.__init__(self, model, num_classes, device, num_hops, verbose, explain_graph, rollout, min_atoms, c_puct, expand_atoms, high2low, local_radius, sample_num, reward_method, subgraph_building_method, save_dir, filename, vis)      629 def __init__(self, model, num_classes: int, device, num_hops: Optional[int] = None, verbose: bool = False,      630              explain_graph: bool = True, rollout: int = 20, min_atoms: int = 5, c_puct: float = 10.0,      631              expand_atoms=14, high2low=False, local_radius=4, sample_num=100, reward_method='mc_l_shapley',      632              subgraph_building_method='zero_filling', save_dir: Optional[str] = None,      633              filename: str = 'example', vis: bool = True):      635     self.model = model  --> 636     self.model.eval()      637     self.device = device      638     self.model.to(self.device)    File ~\anaconda3\envs\tf\lib\site-packages\torch\nn\modules\module.py:1926, in Module.eval(self)     1910 def eval(self: T) -> T:     1911     r""""""Sets the module in evaluation mode.     1912      1913     This has any effect only on certain modules. See documentations of     (...)     1924         Module: self     1925     """"""  -> 1926     return self.train(False)    TypeError: train() missing 3 required positional arguments: 'data_loader', 'optimizer', and 'epochs'  "
None
"Hello, when I try to run  , there is an error:    > splitted_dataset.slices['mask'] = splitted_dataset.slices['train_mask']  TypeError: 'NoneType' object is not subscriptable    I am using torch-geometric 2.1.0. Could any one please take a look? Thanks"
"When I execute rand_gen.py, I get this error: `RuntimeError: a leaf Variable that requires grad is being used in an in-place operation`.  and my dig version is 1.0.0, What should I do? "
"I ran the git installation procedure to install DIG on my local Windows machine. The dependency pyscf could not be found. On investigating further, I found the package is not supported on Windows.       The console output of the installation is given below with the errors and my further efforts of installation using pip/conda.    (base) C:\WINDOWS\system32>conda activate DIG    (DIG) C:\WINDOWS\system32>cd DIG    (DIG) C:\Windows\System32\DIG>pip insall .  ERROR: unknown command ""insall"" - maybe you meant ""install""    (DIG) C:\Windows\System32\DIG>pip install .  Processing c:\windows\system32\dig    Preparing metadata (setup.py) ... done  Collecting scipy    Downloading scipy-1.9.1-cp39-cp39-win_amd64.whl (38.6 MB)       ---------------------------------------- 38.6/38.6 MB 12.8 MB/s eta 0:00:00  Collecting cilog    Downloading cilog-1.2.3-py3-none-any.whl (14 kB)  Collecting typed-argument-parser==1.7.2    Downloading typed-argument-parser-1.7.2.tar.gz (27 kB)    Preparing metadata (setup.py) ... done  Collecting captum==0.2.0    Downloading captum-0.2.0-py3-none-any.whl (1.4 MB)       ---------------------------------------- 1.4/1.4 MB 21.9 MB/s eta 0:00:00  Collecting munch    Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)  Collecting gdown    Downloading gdown-4.5.1.tar.gz (14 kB)    Installing build dependencies ... done    Getting requirements to build wheel ... done    Preparing metadata (pyproject.toml) ... done  Collecting shap    Downloading shap-0.41.0-cp39-cp39-win_amd64.whl (435 kB)       ---------------------------------------- 435.6/435.6 kB 28.4 MB/s eta 0:00:00  Collecting IPython    Downloading ipython-8.5.0-py3-none-any.whl (752 kB)       ---------------------------------------- 752.0/752.0 kB 49.5 MB/s eta 0:00:00  Collecting tqdm    Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)       ---------------------------------------- 78.5/78.5 kB ? eta 0:00:00  Collecting rdkit-pypi    Downloading rdkit_pypi-2022.3.5-cp39-cp39-win_amd64.whl (27.8 MB)       ---------------------------------------- 27.8/27.8 MB 14.5 MB/s eta 0:00:00  Collecting pandas    Downloading pandas-1.5.0-cp39-cp39-win_amd64.whl (10.9 MB)       ---------------------------------------- 10.9/10.9 MB 20.4 MB/s eta 0:00:00  Collecting sympy    Downloading sympy-1.11.1-py3-none-any.whl (6.5 MB)       ---------------------------------------- 6.5/6.5 MB 21.8 MB/s eta 0:00:00  ERROR: Could not find a version that satisfies the requirement pyscf==1.7.6.post1 (from dive-into-graphs) (from versions: 1.4.0, 1.4.0.9, 1.4.2, 1.4.3, 1.4.4, 1.4.5, 1.5.0, 1.5.1, 1.5.3, 1.5.4, 1.5.5, 1.6, 1.6.1, 1.6.2, 1.6.3, 1.6.4, 1.6.5, 1.6.6, 1.7.0, 1.7.1, 1.7.2, 1.7.3, 1.7.4, 1.7.5, 1.7.6, 2.0.0, 2.0.1, 2.1.0)  ERROR: No matching distribution found for pyscf==1.7.6.post1    (DIG) C:\Windows\System32\DIG>pip install pyscf  Collecting pyscf    Downloading pyscf-2.1.0.tar.gz (8.5 MB)       ---------------------------------------- 8.5/8.5 MB 12.1 MB/s eta 0:00:00    Preparing metadata (setup.py) ... done  Collecting numpy!=1.16,!=1.17,>=1.13    Downloading numpy-1.23.3-cp39-cp39-win_amd64.whl (14.7 MB)       ---------------------------------------- 14.7/14.7 MB 19.3 MB/s eta 0:00:00  Collecting scipy!=1.5.0,!=1.5.1    Using cached scipy-1.9.1-cp39-cp39-win_amd64.whl (38.6 MB)  Collecting h5py>=2.7    Downloading h5py-3.7.0-cp39-cp39-win_amd64.whl (2.6 MB)       ---------------------------------------- 2.6/2.6 MB 33.6 MB/s eta 0:00:00  Building wheels for collected packages: pyscf    Building wheel for pyscf (setup.py) ... error    error: subprocess-exited-with-error      Ã— python setup.py bdist_wheel did not run successfully.    â”‚ exit code: 1    â•°â”€> [6 lines of output]        running bdist_wheel        running build        running build_ext        Configuring extensions        cmake -SC:\Users\Akhilesh\AppData\Local\Temp\pip-install-cdh3vh42\pyscf_c44f599cd7784334b55a4937d49d1d4e\pyscf\lib -Bbuild\temp.win-amd64-cpython-39\Release        error: command 'cmake' failed: None        [end of output]      note: This error originates from a subprocess, and is likely not a problem with pip.    ERROR: Failed building wheel for pyscf    Running setup.py clean for pyscf  Failed to build pyscf  Installing collected packages: numpy, scipy, h5py, pyscf    Running setup.py install for pyscf ... error    error: subprocess-exited-with-error      Ã— Running setup.py install for pyscf did not run successfully.    â”‚ exit code: 1    â•°â”€> [8 lines of output]        running install        C:\Users\Akhilesh\anaconda3\envs\DIG\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.          warnings.warn(        running build        running build_ext        Configuring extensions        cmake -SC:\Users\Akhilesh\AppData\Local\Temp\pip-install-cdh3vh42\pyscf_c44f599cd7784334b55a4937d49d1d4e\pyscf\lib -Bbuild\temp.win-amd64-cpython-39\Release        error: command 'cmake' failed: None        [end of output]      note: This error originates from a subprocess, and is likely not a problem with pip.  error: legacy-install-failure    Ã— Encountered error while trying to install package.  â•°â”€> pyscf    note: This is an issue with the package mentioned above, not pip.  hint: See above for output from the failure.    (DIG) C:\Windows\System32\DIG>conda install -c pyscf pyscf  Collecting package metadata (current_repodata.json): done  Solving environment: failed with initial frozen solve. Retrying with flexible solve.  Collecting package metadata (repodata.json): done  Solving environment: failed with initial frozen solve. Retrying with flexible solve.    PackagesNotFoundError: The following packages are not available from current channels:      - pyscf    Current channels:      -      -      -      -      -      -      -      -      To search for alternate channels that may provide the conda package you're  looking for, navigate to             and use the search bar at the top of the page.        (DIG) C:\Windows\System32\DIG>"
"Congratulations on your NeurIPS 22 paper 'ComENet: Towards Complete and Efficient Message Passing for 3D Molecular Graphs', pretty great work! Could you provide the optimal hyper-parameters for your proposed ComENet like SphereNet?"
"Hi does those explanation methods work for Heterogeneous GNN?  Suppose my edges have multiple types, will they be able to identify which type is more critical?  Thanks."
"Thank you for your issue. I have updated the  , and add the hyper-parameter of `x`.    _Originally posted by @Oceanusity in  "
Hi there!  I use DIG. I'm trying to run a code gnn_lrp.py     Versions of installed packages:  python 3.9.12  pytorch 1.11.0  torch-geometric 2.0.4  dive-into-graphs 0.2.0    But I get an error  !   Perhaps the problem is that the implementation of gnn_lrp is outdated and requires non-existent GATConv attributes from pytorch geometric 2.0.4?
"Hi,   I would like to use the methods in xgraph modules for 3D graphs such as SchNet and DimenetPP. Particularly I am interested in SubgraphX. I had to modify quite a few parts to use in my code but I would like to ask if you would consider to add the Feature: Graph Explainability for 3D graphs covering other models as well.    Thank you very much for the repo and your time.  "
"Hello, great work on ComENet, thanks for sharing! I meant to ask in the poster session in NeurIPS, can you provide the splits for Molecule3D? In your poster it stated as 'random', i believe it would be great to have some canonical split (similar to QM9) for researchers to use. I believe ComENet is great start for this. Also similar to #155 it would be great to have the pipeline ready for researchers to test.     Best regards,  "
The MAE I trained with the spherenet model is 0.018. but MAE of paper is 22ï¼Œwhy are they so different
"When I used the **torch-geometric2.1.0,** I meet such an error:  **ModuleNotFoundError: No module named 'torch_geometric.nn.acts'**  Then, I did find thah there are really no acts in this version.  Next, I download **torch-geometric2.0.4**, the previous error is gone. But, I meet a new error:  **AttributeError: Can't get attribute 'DataEdgeAttr' on    It seems to be caused by ""version dependency"".  When I used torch-geometric2.1.0, I don't have this error. I have no idea how to sovle this problem. It looks like neither version will work.    "
"Hi, Can I ask for a simple example code for Graph classification example for subgraphx?  Such as use MUTAG or BA_2motif.  Thanks a lot!"
"in `   In line 285:      def forward(self, batch_data):          z, pos, batch = batch_data.z, batch_data.pos, batch_data.batch    I know that 'z' represents' atom_type 'and 'pos' represents 3D coordinates. But I don't know how this `batch` was obtained. Could you please help me with it? I didn't find out how in the qm9/qm17 datasets.  Thank you very much!"
"Hi, I've recently read your paper about SphereNet and it was impressive.  But I had a error when I tried import SphereNet with code below     error message was like this.      I think it's because DIG library is not compatible with PyG = 2.1.0    Could you manage your project to support PyG = 2.1.0?  or let me know how to handle those error.  "
"Hello,    I try to use the dig library, but I am unable to use the call function of SubgraphX. The error is that it saves a `list of results`, which is turned into a `list of list of MCTSNode`, which in turn is used in `find_closest_node_result` as if it was a `list of MCTSNode`, with the `x.coalition` on line 27 of /DIG/dig/xgraph/method/subgraphx.py returning an error as `x` is a `list`.  I also have a cache problem with GNNExplainer, with the `edge_masks` kwarg being used as the name of the cache as well as when calling the `self.eval_related_pred` function. I think you should simply remove `edge_masks` from the kwargs once it has been used."
"Hi,    There is an error when I tried to train a new model for `PGExplainer`:  ` $ /bin/bash ./models/train_gnns.sh`:    > AttributeError: 'GCNConv' object has no attribute '\_\_explain__'    The error is from here      Could you please take a look at this issue? Thanks."
"   Hi! when I follow the code example, I find that in step 2:  `def check_checkpoints(root='./'):      if osp.exists(osp.join(root, 'checkpoints')):          return      url = ('       path = download_url(url, root)      extract_zip(path, root)      os.unlink(path)  `  here download_url is not defined, so what should I do to solve the problems?  "
Hello! I'm trying to reproduce the standardized MAE values reported in SphereNet paer. The values I'm getting are slightly off on QM9 and MD17 datasets. I was wondering if it is possible to clarify the calculation (with the std value used to calculate it).     Thanks!
"Hello,  There are no explanation results saved in  . At least folders for gcn explanations are empty when I open them using my google account."
"Hi, my GNN is trained on DGL, it seems your lib does not support this.   Is it possible to add this feature? Or you mind providing a quick way to do it?   Thanks. (Specifically for explainability.)  BTW, a mode general question, do these methods support Relational Graph Convolutional Network?"
"Hi,  I have tried subgraphx on ba2motif dataset to capture 50% of nodes, it mostly works well but sometimes it didn't like the following image.  !   here is my subgraphx config:  `  SubgraphX(self.model, num_classes=2, device=...,                           explain_graph=True, reward_method='mc_l_shapley',                          min_atoms=12, high2low=True,                           subgraph_building_method='split', verbose=True)  `"
"Hi thanks for sharing the dataset  ,  There are some graphs like 81st that has NO2 but are labeled as 1 (non-mutagen). Is there something that I'm missing?  thanks  !   "
"Hi,  I think indirect graph datasets (like ba-2motifs) had better have symmetric edge masks because it doesn't mean to have two different probabilities for an indirect edge. If you agree about this, what do you think about handling it at the end of these two methods?  thanks"
"The version requirement for captum is **captum==0.2.0**. As version 0.5.0 is already published, is there any specific reason for the strong version requirement in DIG?"
I'd like to create visualisations for the explanations of graph predictions. I have followed along with the documentation and I am currently meeting the following problem.         !     
`mask_features` option is an argument of `forward` for GNNExplainer but it is not passed to `gnn_explainer_alg`. I actually have fixed and merged it to dig branch but not to dig-stable.   
"Hi,  Thanks for sharing the library, I was trying to train pg explainer within the benchmark on ba2motifs and I was printing the accuracy of graph classification using the provided edge mask and I got 50% accuracy as it always underfits during training (always provides poor explanation for samples with house motif). I have tried various configurations but nothing has changed so far. Any help would be appreciated."
"Hi,      I am running experiments on dig-stable branch trying to reproduce the results in figure 6 and figure 7. I am running subgraphx.py code in benchmark/xgraph . I see in the config file, for subgraphx, there is no sparsity parameter like other exlpainers have. How can I control the sparsity for subgraphx. Thanks in advance."
"Hello,           I tried to run the following command on the dig-stable branch `python -m benchmarks.xgraph.train_gnns datasets=ba_2motifs` and I got the following error:     Could you please help me. Thanks in advance."
"Hi,  I was trying to use `mask_feature` option in gnn explainer, but I got an error telling me `self.coeffs` is not defined. I also tried to find its defintion within the class and its parent but didn't find anything.   "
"Hello, my name is kyeongrok. I am student of south korea.    Now, I am trying to work the examples code of subgraph x.(subgraphx.ipynb)  But it was not working.  In your code, the dimension of bashpaes graph's feature is just one.  So to use my model(it is same with gcnl3 in models.py) and data(it is same with bashapes in dig dataset, but has 10 dimension feature), i just deleted the 'dataset.data.x = dataset.data.x[:, :1]' line in second block  And i modified the gcn3l model a little to work in 10 dimension input.    However, subgraphx spend a lot of time(It seems not to work)  I would like to ask if there is a possibility that the subgraphx may not work as the input feature of the data increases.    Thank you for reading.:)  "
"Hello,    Thank you for your library. I am doing research on graph explainability and it is really useful to have that kind of resource for comparison.  However, I have a small problem with PGExplainer when running it on ba_shapes: some parameters are not defined, and when I add these parameters, there is a problem with a matrix multiplication about size. It looks like the code was written for the ba_community dataset only (and it works just fine for that specific dataset).  Here is the command line I used:     And hereunder is the error message.  What can I do to run PGExplainer on other datasets?    Have a nice day!    _________________________       "
"Hello, I am trying to install the packages required by PGExplainer, but I have encountered a problem that I cannot solve. I hope to get your help.thx  The problem is as follows:      pip install cilog typed-argument-parser==1.5.4 tqdm  Traceback (most recent call last):    File ""D:\Anaconda\Anaconda3-2021.05\envs\xgraph\Scripts\pip-script.py"", line 5, in        from pip._internal.cli.main import main    File ""D:\Anaconda\Anaconda3-2021.05\envs\xgraph\lib\site-packages\pip\_internal\cli\main.py"", line 9, in        from pip._internal.cli.autocompletion import autocomplete    File ""D:\Anaconda\Anaconda3-2021.05\envs\xgraph\lib\site-packages\pip\_internal\cli\autocompletion.py"", line 10, in        from pip._internal.cli.main_parser import create_main_parser    File ""D:\Anaconda\Anaconda3-2021.05\envs\xgraph\lib\site-packages\pip\_internal\cli\main_parser.py"", line 8, in        from pip._internal.cli import cmdoptions    File ""D:\Anaconda\Anaconda3-2021.05\envs\xgraph\lib\site-packages\pip\_internal\cli\cmdoptions.py"", line 23, in        from pip._internal.cli.parser import ConfigOptionParser    File ""D:\Anaconda\Anaconda3-2021.05\envs\xgraph\lib\site-packages\pip\_internal\cli\parser.py"", line 12, in        from pip._internal.configuration import Configuration, ConfigurationError    File ""D:\Anaconda\Anaconda3-2021.05\envs\xgraph\lib\site-packages\pip\_internal\configuration.py"", line 24, in        from pip._internal.utils import appdirs    File ""D:\Anaconda\Anaconda3-2021.05\envs\xgraph\lib\site-packages\pip\_internal\utils\appdirs.py"", line 13, in        from pip._vendor import platformdirs as _appdirs    File ""D:\Anaconda\Anaconda3-2021.05\envs\xgraph\lib\site-packages\pip\_vendor\platformdirs\__init__.py"", line 33, in        PlatformDirs = _set_platform_dir_class()  #: Currently active platform    File ""D:\Anaconda\Anaconda3-2021.05\envs\xgraph\lib\site-packages\pip\_vendor\platformdirs\__init__.py"", line 29, in _set_platform_dir_class      result: type[PlatformDirsABC] = getattr(importlib.import_module(module), name)    File ""D:\Anaconda\Anaconda3-2021.05\envs\xgraph\lib\importlib\__init__.py"", line 127, in import_module      return _bootstrap._gcd_import(name[level:], package, level)    File ""D:\Anaconda\Anaconda3-2021.05\envs\xgraph\lib\site-packages\pip\_vendor\platformdirs\windows.py"", line 3, in        import ctypes    File ""D:\Anaconda\Anaconda3-2021.05\envs\xgraph\lib\ctypes\__init__.py"", line 8, in        import numpy as np    File ""D:\Anaconda\Anaconda3-2021.05\envs\xgraph\lib\site-packages\numpy\__init__.py"", line 162, in        from . import ctypeslib    File ""D:\Anaconda\Anaconda3-2021.05\envs\xgraph\lib\site-packages\numpy\ctypeslib.py"", line 85, in        c_intp = nic._getintp_ctype()    File ""D:\Anaconda\Anaconda3-2021.05\envs\xgraph\lib\site-packages\numpy\core\_internal.py"", line 231, in _getintp_ctype      val = ctypes.c_long  AttributeError: module 'ctypes' has no attribute 'c_long'  ?[0m    "
"I am attempting to use various explainers to explain a node classification output.    `from dig.xgraph.method import DeepLIFT, GNN_LRP, GNNExplainer, GradCAM`  `explainer = GNNExplainer(model, explain_graph=False)`  `explainer(data.x, data.edge_index, node_idx=node_indices[20], sparcity=0.5)`    Following the documentation, the explainer, requires x, edge_index and node_idx (int)          However, when I assign a variable to node_idx it raises the error below;    <img width=""1122"" alt=""image"" src=""   "
"Dear developers,    I am trying to produce crystal structures by graph generative models (like GraphDF) provided in DIG. Can these models generate global features of graph (e.g.  the cell of crystal)? If so, how can I construct a new dataset of crystals to train generative models?    Thanks!"
"Hi,  Thanks for sharing datasets, I have a question about BA-2Motif dataset. I printed dataset and it shows this `Data(edge_index=[2, 50960], x=[25000, 10], y=[1000])` and as `y` shows it has 1000 samples but why the first dimension of `edge_index` and `x` is not 1000 too?   also is there any pretrained graph/node level GCN model on this dataset?"
"When using the GNN_GI explainer for the graph classification explanation, when the following cells are run, a RuntimeError is raised. Is there a workaround for this?    `dataset = MoleculeDataset(root='MoleculeDataset', name='mutag', transform=None)`  `x = dataset.data.x.to(torch.float32).to(device)`  `edge_index = dataset.data.edge_index.to(device)`  `num_classes = dataset.num_classes`    `from dig.xgraph.models import GCN_2l`  `model = GCN_2l(model_level='graph', dim_node=num_node_features, dim_hidden=64, num_classes=num_classes)`    `from dig.xgraph.method import GNN_GI`  `explainer = GNN_GI(model, explain_graph=True)`  `explainer(x, edge_index, num_classes=num_classes)`    !   "
"I am attempting to use the predefined models in the framework to perform a classification task and explanation on graphs. Upon initializing the models, using;    `model = GCN_2l(model_level='graph', dim_node=num_node_features, dim_hidden=64, num_classes=num_classes)`    The following error comes up.  `AttributeError: 'GCNConv' object has no attribute 'lin'`    Previously I was met with the error `AttributeError: 'GCNConv' object has no attribute 'weight'` when I ran the following;  `explainer = SubgraphX(model=model_gcn_2l, num_classes=num_classes, device=device, explain_graph=True)`  `explainer(x, edge_index)`    It was recommended that I change my `torch_geometric` version to `1.7.2`, **which I did now**, I am meeting this error `AttributeError: 'GCNConv' object has no attribute 'lin'` when initializing the model.    <img width=""1025"" alt=""image"" src=""   "
"Hello, it seems the output of SchNet is of dimension 1, which is the prediction dimension. Is it possible to change to output the graph embedding instead of the predicton? This way it's more flexible for the users to put whatever downstream classifier after the embedding and have a control of how many prediction dimension there can be (multi-task potentially)"
Hi there!  I use DIG. I'm trying to run a code from the 'Tutorial for GNN Explainability'.        Versions of installed packages:  python 3.10.4  pwtorch 1.12.0  torch-geometric 2.0.4  dive-into-graphs 0.1.2    But I get an error:  !   What can it mean? The model does not find the necessary weights in GCN_2l_best.ckpt?    
"there is a problem in pgexplainer graph visualization as in `plot_bashapes`, 'node_idx' and 'y' must be filled within `kwargs`, though during visualization for graph explanation there is no passing of them"
"Hi,  I read pseudo code of paper and it seems the loss calculation and backpropagation should be done after getting prediction probabilities for all graph inputs (graph explanation) or nodes  (node explanation), though current code calculates loss and backpropagates it per sample"
"Hello, I've write a simple script to test DimeNetPP. However I got an error like this:   !          I used QM9 from pyg on deliberately because my real data input is not .csv and does not have attributes such as 'z'. So I would like to see if this model works for a simpler data input.    Also, the pyg-nightly has updated     `act` to `from torch_geometric.nn.resolver import activation_resolver`"
"Hi,  based on the paper pseudo code, I believe there is a monte carlo part, but I couldn't found it within the code."
"Hello,  I am trying to use GNNExplainer on BA_2Motifs with GIN_3l as the base model. Consider the following command:         A snippet of the output is:       At first, I thought that `edge_mask.shape=data.edge_index.shape[1]` since this should be the edge-level explanation. Now, it looks like `edge_mask.shape=data.edge_index.shape[1]+data.x.shape[0]`. Is that how it is supposed to work?     Also, since it is undirected, does the mask remove a certain edge in both directions, e.g. `(0,1)` and `(1,0)`? I also tried something like `edge_mask[data.x.shape[0]:]` or `edge_mask[:-data.x.shape[0]]`, but when I applied these masks to `edge_index` the results are not convincing. Am I missing something? Am I doing something wrong? Many thanks for any clarification."
"as it seems, an explanation output is generated by the GNNExplainer per each label exists in the dataset. isn't better to only define the benchmark based on the original model prediction? "
"Hi  as I understood, the only option for node-based explanation is to choose a subgraph with the same k-hop with the original model. is it possible to choose the k-hop of GNNExplainer arbitrarily? I believe the `subset` generated by the `subgraph` method could help this matter."
"Howdy,    I found when I call the `geom2alpha` in DIG/dig/ggraph3D/utils/eval_prop_utils.py, it threw the error ""AttributeError: 'RKS' object has no attribute 'Polarizability'"".     Could you please help me fix this issue?    Best regards.  Layne"
None
"Does the implemented DeepLift provide edge-level explanation result for each edge? I wonder whether DeepLift could be reimplemented in a node-level version, that calculation a score for each node."
"Hi,  Thank you for sharing this very useful package. Are there any available explanations on how the models used for the Expandability (x-graphs) portion are trained? I mean the output shape or objective. I am trying to check If I can use all the explainable models on a single dataset like Tox-21. Please let me know if there are any available resources. Thanks.   "
"When I run the 3D Diagram tutorial, an error occurs. I don't know how to use it.    Traceback (most recent call last):    File ""D:\rubbihs\pyg\mol\3d\qm9.py"", line 14, in        model = SphereNet(energy_and_force=False, cutoff=5.0, num_layers=4,    File ""D:\down\anaconda\envs\pyg\lib\site-packages\dig\threedgraph\method\spherenet\spherenet.py"", line 265, in __init__      self.emb = emb(num_spherical, num_radial, self.cutoff, envelope_exponent)    File ""D:\down\anaconda\envs\pyg\lib\site-packages\dig\threedgraph\method\spherenet\spherenet.py"", line 23, in __init__      self.dist_emb = dist_emb(num_radial, cutoff, envelope_exponent)    File ""D:\down\anaconda\envs\pyg\lib\site-packages\dig\threedgraph\method\spherenet\features.py"", line 178, in __init__      self.reset_parameters()    File ""D:\down\anaconda\envs\pyg\lib\site-packages\dig\threedgraph\method\spherenet\features.py"", line 181, in reset_parameters      torch.arange(1, self.freq.numel() + 1, out=self.freq).mul_(PI)  RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.  "
"Hello!!  This is a graph classification task and all the samples are divided into 2 categories. I'm trying to use SubgraphX to explain my GCN model on one sample graph. Concretely,     Then I get `related_preds` like this     I am wondering what means 'masked', 'maskout', 'origin'? And how can I get the fidelity of the explaination result?  Thanks a lot!   ;)"
for the constrained optimization task there is only ZINC dataset: zinc_800_graphaf.csv     how to obtain this same csv on moses and qm9?  is this given from the output from the property optimization task?  thanks!
"Hi all,    I have the following question:    I saw that for PGExplainer you defined a custom propagate function in order to account for the edge matrix computed by the explainer, and this is fine. But, is it possible to achieve the same result by leaving the default propagate implementation and feeding the weights to the network with the additional parameter `edge_weight`?  Specifically, I am referring for simplicity just to the   architecture.    Thanks!"
"When I run `model.load_state_dict(torch.load(ckpt_path) , I got the error  `KeyError: 'conv1.lin.weight'`.  Even if I set `strict=False`, it doesn't work.    My PyTorch version is `1.11.0` and PyTorch Geometric version is `2.0.4`.    The specific error logs:     "
"Hi:    Thank you for sharing this very useful package.  I noticed that many of the examples have not been updated to the new API's.  It is not that important since we could still follow the examples after minor modifications, with the help of documentation provided.     Regards,"
"> Hi @yuanqidu,  >   > I also met this issue and believe it's due to the PyTorch version. You can also try to replace line 181 to be  >   >      This did not work for me. Neither did the first solution. There is a dependency in   line 29 which still causes an error   "
"Hi,    When calling closest node result for metric score as suggest in example examples/xgraph/subgraphx.ipynb.    like this:     I am getting following error     I fixed it by offering an option for dictionaries, but I am still wondering why it happens.  Can someone give me a hint, why that is happening?    Thank you  Hanne"
"Hi,     I am trying to run bash train_gnns.sh.  I have installed a tap library through pip install tap.py.  But I continuously met an error below.  ImportError: cannot import name 'Tap' from 'tap'  I am wondering whether I have installed the right one.  Could you help me with that?    Thank you,  Zhaoning"
"Hi,    I am planning to train a new SubgraphX on the Mutagenicity dataset.  I find a line in MUTAGDataset class:  adj_all = np.zeros((len(nodes_all), len(nodes_all)))  This line will create a really large array when the size of the dataset is large. (For Mutagenicity, the shape of the array is (131488, 131488))  Could you tell me how can I do to avoid the out-of-memory issue?    Thank you,  Zhaoning"
"- line 89 self.mols has not been defined  - line 103 will report error for the molecules without carbon  - it is inconvenient to split dataset and create loader like the dig.threedgraph can have ""get_idx_split"" methods"
"Hi,    May I ask for the reference paper that proposes the below ResGCN method? Thank you!    I didn't find this reference in your repository.         Best regards,  Simon"
What is the version of torch-geometric are you using? It seems that there is an error message **AttributeError: module 'torch.nn.parameter' has no attribute 'UninitializedParameter'** when I use version 2.0.3 and an error message **KeyError: 'model.gnn_layers.0.lin.weight'** when I use version 2.0.4.
"When I run the codes in the ""Display example output"": out = model(data.x, data.edge_index)  I got the error message:         Could someone give me a hint on how to solve this issue?"
"Hi,    I am trying to visualize all saved searching results of mutag dataset.  However, when I am loading those pt files using torch.load().  It shows ModuleNotFoundError: No module named 'pipeline'.  Could you help me with that?    Thank you"
"Hey, I want to use a pretrained model to play around with knowledge distillation from GNN to MLP. So I wrote the code for this. I use the `BA_shapes` dataset with 700 nodes and I import the pretrained `GCN_2l` model for distillation just like in the generic explanation example code:         the part of the code doing the GNN predictions is here:       The thing is, the prediction accuracy for this GNN with this dataset seems to be really poor, output from above:       Am I doing something wrong, or is this just not a very good model at predicting this dataset? Or am I overestimating the accuracy such a model should reach with this dataset?"
"Hi, I want to train on some new dataset and I tried to run train_gnns.sh to see if the original code works and then I can modify it as needed. It output errors:     It seems the original code is no long working. Can you please take a look at this? Thanks."
"hi!  I am trying to use the implementation of the self-supervised GNNs (e.g., InfoGraph). However, the   is using a graph dataset that is labeled and actually evaluate the model by using sklearn supervised evaluation process.  I am looking for an end-to-end example of applying the algorithm in an unsupervised way and get an embeddings representation per graph.    thanks a lot!"
Can someone give me a basic overview of how to proceed for explanation on a link prediction task.  I built the node embeddings and the link prediction class in Pytorch geometric.   can I use DIG with PYG models for link prediction
"Dear developers,    I meet a problem in the generation process in `examples/ggraph/GraphDF`:       And my input is:       Could you please give me some suggestions?    Best,  Hao"
"I am confused as to how I would run SubgraphX on my GCN model with my own created datasets. My datasets are collections of `torch_geometric.data.Data` objects with the expected attributes. However `PlotUtils` seems to require me to choose from a set of dataset names, which are not relevant to my dataset. How do I produce the visualization after executing the code below? The API does not seem to allow me to specify the node labels.             explainer = SubgraphX(model, num_classes=2,  device=torch.device('cuda:0'))          _, explanation_results, related_preds = explainer(data.x.cuda(), data.edge_index.cuda(), max_nodes=10)"
"hello dear,    thank you so much for the wonderful library.     I am having a run time problem while executing SubgraphX. For explaining a single node my code is taking a few hours.  I am using a GCN model to train the synthetic dataset; This is how my model looks like:    **GCN    (conv1): GCNConv(10, 20)    (conv2): GCNConv(20, 20)    (lin_pred): Linear(in_features=40, out_features=8, bias=True)  )**    I am using the synthetic dataset where features are from a normal distribution:       **Dataset(num_node_features=10, num_classes=8)       Data(edge_index=[2, 8920], test_mask=[1400], train_mask=[1400], x=[1400, 10], y=[1400])**    I am using the following dependencies:    **Cuda=10.2  dive-into-graphs          0.1.2                   torch                          1.10.1                     torch-geometric        1.7.2                    torch-scatter             2.0.9                     torch-sparse             0.6.12                     torchaudio                0.10.1                    torchvision                0.11.2**              Is this time taken by SubgraphX is normal or I am doing some mistake? Can you please help me with this?     thanks in advance!!                                  "
"Hi  Hope that you will be fine. In the paper of XGNN, reported accuracy on MUTAG dataset is 0.963 whereas when I used the given code of XGNN I am getting 0.936. So, roughly given code is 2.7% lower in accuracy. Can you guide why this is happening and how can I achieve the accuracy which is reported in the XGNN paper? Thanks  !   !   "
"Great work!    When I copied the code from the README file and run it with the QM9 dataset provided by DIG, it showed me the following error when I attempt to create a SphereNet model.    Traceback (most recent call last):    File ""main_qm9.py"", line 19, in        model = SphereNet(energy_and_force=False, cutoff=5.0, num_layers=4,    File ""/opt/conda/lib/python3.8/site-packages/dig/threedgraph/method/spherenet/spherenet.py"", line 265, in __init__      self.emb = emb(num_spherical, num_radial, self.cutoff, envelope_exponent)    File ""/opt/conda/lib/python3.8/site-packages/dig/threedgraph/method/spherenet/spherenet.py"", line 23, in __init__      self.dist_emb = dist_emb(num_radial, cutoff, envelope_exponent)    File ""/opt/conda/lib/python3.8/site-packages/dig/threedgraph/method/spherenet/features.py"", line 178, in __init__      self.reset_parameters()    File ""/opt/conda/lib/python3.8/site-packages/dig/threedgraph/method/spherenet/features.py"", line 181, in reset_parameters      torch.arange(1, self.freq.numel() + 1, out=self.freq).mul_(PI)  RuntimeError: a leaf Variable that requires grad is being used in an in-place operation."
"Hi,    Is DIG Model agnostic? I would appreciate it if you can point me towards a guide/examples which show how can we use DIG explainability with existing GNN models developed in PyG, DGL or any other library.     Essentially the question is can DIG be used as an 'explainability wrapper' around existing graph models (which aren't developed using DIG library)?    Happy new year!    Thanks,  Mladen  "
"Update: Thank you so much for the developers' active response. Following @Oceanusity's advice, I have added the ""arguments_read "" function into our model and it works.  ---------------------------------------------------------------------------  I have sent an email to you since I am not sure if this is caused by our self-build dataset or not.    I have successfully installed the DIG library using virtualenv: PyTorch 1.6.0, torch_geometric 1.7.0. But when I run the code, it causes bugs:   ---------------------------------------------------------------------------  TypeError                                 Traceback (most recent call last)  /tmp/ipykernel_9260/3395182850.py in         25 prediction = model(data.x, data.edge_index).argmax(1)       26 _, explanation_results, related_preds = \  ---> 27 explainer(data.x, data.edge_index, max_nodes=max_nodes)       28        29 explanation_results = explanation_results[prediction]    ~/dig-mengliu/lib/python3.8/site-packages/dig/xgraph/method/subgraphx.py in __call__(self, x, edge_index, **kwargs)      849       850         for label_idx, label in enumerate(ex_labels):  --> 851             results, related_pred = self.explain(x, edge_index,      852                                                  label=label,      853                                                  max_nodes=max_nodes,    .....    ~/dig-mengliu/lib/python3.8/site-packages/dig/xgraph/method/shapley.py in value_func(batch)       12     def value_func(batch):       13         with torch.no_grad():  ---> 14             logits = gnnNets(data=batch)       15             probs = F.softmax(logits, dim=-1)       16             score = probs[:, target_class]    ~/dig-mengliu/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)      720             result = self._slow_forward(*input, **kwargs)      721         else:  --> 722             result = self.forward(*input, **kwargs)      723         for hook in itertools.chain(      724                 _global_forward_hooks.values(),    TypeError: forward() got an unexpected keyword argument 'data'"
"Hi,  I am trying to install and test this package, but it seems that the newest pyg wouldn't work. May I ask what version of the following packages did you use when creating DIG?  pytorch and torch-scatter, torch-sparse, torch-geometric, torch-cluster, torch-spline-conv  Thank you so much"
"When I follow benchmarks/xgraph/pgexplainer_edges.py to train PGExplainer for explanation tasks in graph classification in my personal data, the error happens in assert   `      assert out.size(self.node_dim) == edge_mask.size(0)  `    The trace bask give like follows:  `  Traceback (most recent call last):    File "".pycharm_helpers/pydev/pydevd.py"", line 1483, in _exec      pydev_imports.execfile(file, globals, locals)  # execute the script    File "".pycharm_helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile      exec(compile(contents+""\n"", file, 'exec'), glob, loc)    File ""Documents/Code/GraphResearch/DIG_test/PGExplainer_android/PGExplain_and.py"", line 85, in        pgexplainer.train_explanation_network(train_graphs[:10])    File ""anaconda3/envs/graph/lib/python3.8/site-packages/dig/xgraph/method/pgexplainer.py"", line 638, in train_explanation_network      prob, edge_mask = self.explain(data.x, data.edge_index, embed=emb_dict[gid], tmp=tmp, training=True)    File ""anaconda3/envs/graph/lib/python3.8/site-packages/dig/xgraph/method/pgexplainer.py"", line 604, in explain      logits = self.model(x, edge_index)    File ""anaconda3/envs/graph/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1102, in _call_impl      return forward_call(*input, **kwargs)    File ""anaconda3/envs/graph/lib/python3.8/site-packages/dig/xgraph/models/models.py"", line 209, in forward      post_conv = self.conv1(x, edge_index)    File ""anaconda3/envs/graph/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1102, in _call_impl      return forward_call(*input, **kwargs)    File ""anaconda3/envs/graph/lib/python3.8/site-packages/dig/xgraph/models/models.py"", line 410, in forward      out = self.propagate(edge_index, x=x[0], size=None)    File ""anaconda3/envs/graph/lib/python3.8/site-packages/dig/xgraph/models/models.py"", line 498, in propagate      assert out.size(self.node_dim) == edge_mask.size(0)  AssertionError  `    I found that it happens after 'PGExplainer.__set_masks__' updates 'edge_mask' in my model (I trained the graph classifier follows the gnn_train with GIN_3L).   The errors happen when I train the PGExplainer.      May I have your suggestion to fix that      "
"Hi DIG maintainers,    I have tried to train GIN_3l on freesolv dataset with regression task. But it seems the model will not learn. So my question is, can I use freesolv dataset for GradCAM? if yes could you share the checkpoint for GIN_3l model?    Thanks in advance,  Boma"
"Hello,    I run the SubgraphX model on the ba_2motifs datasets for the first 10 graphs in the dataset, and I got negative fidelity scores as the following. May I ask why this is the case and how can we reproduce the paper result on ba_2motifs? Is it because of different hyperparameters? Thanks.          I trained a 3-layer GCN according to the code in `benchmarks/xgraph/train_gnns.py`, and it has accuracy 1 on the first 10 graphs I tried to explain and accuracy 0.9780 on all 1000 graphs in ba_2motifs.    I was mostly following the code in `examples/xgraph/subgraphx.ipynb`     I constructed the explainer with the following code       Then running the following piece of code gives the output above       I also checked what's inside the X_collector, and it is the following.           "
"I would like to use subgraphX to interpret subgraphs in single graph instances.  When I use pertained GIN model which is officially implemented, the data format seems inconsistent with DIG.  I am wondering whether there is demo to train a graph classifier with our own data.    Thank you very much in advance."
"Hi, I enjoyed the GraphEBM preprint, but have had a couple issues applying the code.    - When trying to train the models from scratch I invariably get mode collapse, with all generated molecules identical (and unrealistic, below), any suggestions to get model to train successfully?  !     - when trying to use the downloaded GraphEBM_Zinc* weights from DIG_storage, I get an incompatibility with the example model structure:   `RuntimeError: Error(s) in loading state_dict for EnergyFunc:   Missing key(s) in state_dict: ""graphconv1.linear_node.bias"", ""graphconv1.linear_node.weight_orig"", ""graphconv1.linear_node.weight"", ""graphconv1.linear_node.weight_u"", ""graphconv.0.linear_node.bias"", ""graphconv.0.linear_node.weight_orig"", ""graphconv.0.linear_node.weight"", ""graphconv.0.linear_node.weight_u"", ""graphconv.1.linear_node.bias"", ""graphconv.1.linear_node.weight_orig"", ""graphconv.1.linear_node.weight"", ""graphconv.1.linear_node.weight_u"".`"
"Hello,     when I try to run the code of the Example-Notebook for a graph-classification problem I get this error:     `---------------------------------------------------------------------------  UnboundLocalError                         Traceback (most recent call last)    in  ()       22   # Speichern der Werte in XCollector       23   with torch.no_grad():  ---> 24       walks, masks, related_preds = explainer(x, edge_index, top_k=top_k)       25       masks = [mask.detach() for mask in masks]       26     1 frames  /usr/local/lib/python3.7/dist-packages/dig/xgraph/method/pgexplainer.py in forward(self, x, edge_index, **kwargs)      757         pred_mask = [edge_mask]      758         related_preds = [{  --> 759             'masked': masked_pred,      760             'maskout': maskout_pred,      761             'origin': probs[label],    UnboundLocalError: local variable 'masked_pred' referenced before assignment`    When looking at the source code it seems, that masked_pred is only for node classification defined.   Thanks for help  "
"After downloading the data through:  `  dig.xgraph.dataset.SynGraphDataset('./datasets', 'BA_shapes')  `  When I test PGExplainer,   `  splitted_dataset.slices['mask'] = splitted_dataset.slices['train_mask']  `    the error occurs:    `  splitted_dataset.slices['mask'] = splitted_dataset.slices['train_mask']  TypeError: 'NoneType' object is not subscriptable  `    May I have any suggestions? THX"
"when I test the demo for   the error rises:    `  Traceback (most recent call last):    File ""main.py"", line 35, in        model.load_state_dict(torch.load(ckpt_path)['state_dict'])    File ""module.py"", line 1482, in load_state_dict      raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(  RuntimeError: Error(s) in loading state_dict for GCN_2l:   Missing key(s) in state_dict: ""conv1.lin.weight"", ""convs.0.lin.weight"".    Unexpected key(s) in state_dict: ""conv1.weight"", ""convs.0.weight"".   `    which corresponds to the last line of [3] in      "
"Hey there,     i tried to train the GCN_2l Model and always get this error:    '  /usr/local/lib/python3.7/dist-packages/dig/xgraph/models/models.py in forward(self, x, edge_index, edge_weight)      323         edge_weight.requires_grad_(True)      324   --> 325         x = torch.matmul(x, self.weight)'    AttributeError: 'GCNConv' object has no attribute 'weight'    I can't see were weight is instantiated    "
"When I try to run the   or the  ,    on the line        I get the error:     This doesn't happen with the SubgraphX tutorial but I'm not sure if it happens with all the other xgraph tutorials.    torch.version = 1.6.0  torch_geometric.version = 1.6.0  torch_scatter.version = 2.0.6  torch_sparse.version = 0.6.9  torch.version.cuda = 10.2.        "
"Hi,    I'm trying to use SubgraphX for Graph Classification task.  I loaded the Tox21 dataset and model/checkpoints using the first 3 cells in the  .    Then, I instantiated SubgraphX:  `from dig.xgraph.method import SubgraphX`  `explainer = SubgraphX(model, num_classes=2, device=device,explain_graph=True)`    Next, I ran:  `data = dataset  runs fine.    **Do you know why this error is occurring or do you have a working example of using SubgraphX for Graph Classification?**    torch.__version__ = 1.6.0  torch_geometric.__version__ = 1.6.0  torch_scatter.__version__ = 2.0.6  torch_sparse.__version__ = 0.6.9  torch.version.cuda = 10.2.    This is how I installed everything:     "
"Hi,    I am trying to reproduce energy MAE results of SphereNet on MD17 dataset. I used the `examples/threedgraph/threedgraph.ipynb` in this project but couldn't reproduce. What should be the hyperparameters for results reproduction?    Thanks,  Xiyuan Wang"
     The zero order of first kind bessel function is  sin(freq * x) / x  Here is sin(freq * x)  Is this a bug?
"Hey, I was wondering if you had or know where to find the optimal parameter set for SchNet.   Thanks for your work. "
"Hi DIG,  I cannot find the config for dataset MUTAG.  Does bace represent MUTAG?  Thank you,  Zhaoning"
"in GCNConv, inside the forward function you perform the following operation  `x = torch.matmul(x, self.weight)`    torch geometric has replaced the weight with a linear layer"
"Hello there,    May I ask a question about the Shapley value calculation, especially the following line of code `set_exclude_mask = np.ones(num_nodes)`?         From the implementation above, seems like the mask is initialized as including all the nodes in the graph. Then all the selected nodes in the `node_exclude_subset` are included, and other nodes in the `local_region` are not. However, doesn't this includes all the nodes outside the local region as well?    Based on this description from the paper,    <img width=""658"" alt=""Screen Shot 2021-11-03 at 9 40 20 PM"" src=""      I feel like the line 122 above should be changed to  `set_exclude_mask = np.zeros(num_nodes)` ? Or maybe I am missing something here? I appreciate your help.        "
"Hi DIG,    I have tried to understand how to implement the visualization from the ipynb example on pgexplainer and subgraphx for the visualization on gradcam and gnnexplainer. But, I still don't know how to start with it.    As I try to visualize on gradcam example:     from dig.xgraph.method.base_explainer import ExplainerBase  explainer_base = ExplainerBase(model)    visualize_graph = explainer_base.visualize_graph(node_idx=5, edge_index=data.edge_index, edge_mask=masks     Could you perhaps give me some tips how to visualize on ipynb example given on gradcam and gnnexplainer?"
"Hi,  I got an AttributeError when I am running subgraphx.ipynb.  Could you help me with that?  "
"from dig.xgraph.models import GCN_2l  model = GCN_2l(model_level='node', dim_node=dim_node, dim_hidden=300, num_classes=num_classes)  model.to(device)  ckpt_path = osp.join('checkpoints', 'ba_shapes', 'GCN_2l', '0', 'GCN_2l_best.ckpt')  model.load_state_dict(torch.load(ckpt_path, map_location=torch.device(device))['state_dict'])    for the above code(given in tutorial for using explainability models)  the following error is produced    FileNotFoundError                         Traceback (most recent call last)    in          3 model.to(device)        4 ckpt_path = osp.join('checkpoints', 'ba_shapes', 'GCN_2l', '0', 'GCN_2l_best.ckpt')  ----> 5 model.load_state_dict(torch.load(ckpt_path, map_location=torch.device(device))['state_dict'])    ~\anaconda3\lib\site-packages\torch\serialization.py in load(f, map_location, pickle_module, **pickle_load_args)      592         pickle_load_args['encoding'] = 'utf-8'      593   --> 594     with _open_file_like(f, 'rb') as opened_file:      595         if _is_zipfile(opened_file):      596             # The zipfile reader is going to advance the current file position.    ~\anaconda3\lib\site-packages\torch\serialization.py in _open_file_like(name_or_buffer, mode)      228 def _open_file_like(name_or_buffer, mode):      229     if _is_path(name_or_buffer):  --> 230         return _open_file(name_or_buffer, mode)      231     else:      232         if 'w' in mode:    ~\anaconda3\lib\site-packages\torch\serialization.py in __init__(self, name, mode)      209 class _open_file(_opener):      210     def __init__(self, name, mode):  --> 211         super(_open_file, self).__init__(open(name, mode))      212       213     def __exit__(self, *args):    FileNotFoundError: [Errno 2] No such file or directory: 'checkpoints\\ba_shapes\\GCN_2l\\0\\GCN_2l_best.ckpt'        And because of this error not able to follow further tuttorial.        "
"Dear DIG maintainers,    First of all many thanks for this repo!     I was checking out your implementation of the SphereNet architecture, and I noticed that in some cases I cannot successfully perform a forward pass of the network. Notably, the problem appears to lie in the `xyz_to_dat` function that is called within that submodule. Here I provide a minimal example of the issue.    I start with 3D coordinates in cartesian space and compute edge indices with `torch_geometric.nn.pool.radius_graph`:         I then try and call the `xyz_to_dat` routine present in the forward pass, and I encounter this error:         Interestingly, this problem seems to be more frequent the larger the graph I consider, the calculation being successful for some:    !     I'm wondering whether the problem is on my side and I'm misunderstanding how these functions shoud be called. Any help is appreciated!"
"I am facing the following error on running the installation commands given      '''  pip install dive-into-graphs  Collecting dive-into-graphs    Using cached    Collecting captum==0.2.0 (from dive-into-graphs)    Downloading   (1.4MB)       |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.4MB 9.6MB/s   Collecting shap (from dive-into-graphs)    Using cached    Collecting rdkit-pypi (from dive-into-graphs)    ERROR: Could not find a version that satisfies the requirement rdkit-pypi (from dive-into-graphs) (from versions: none)  ERROR: No matching distribution found for rdkit-pypi (from dive-into-graphs)    '''    It would be great if you could fix the installation code here.  I think the issue has to do with the cross platform nature of rdkit. Based on what I understand, the conda alternative works fine.   "
"I tried to run the   for GNNexplainer. However, I am consistently getting the following value error. It would be a great help if you could help me out here.:     `---------------------------------------------------------------------------  ValueError                                Traceback (most recent call last)    in          8         if torch.isnan(data.y[0].squeeze()):        9             continue  ---> 10         edge_masks, hard_edge_masks, related_preds = explainer(data.x, data.edge_index, sparsity=sparsity, num_classes=num_classes, node_idx=node_idx)       11        12         x_collector.collect_data(hard_edge_masks, related_preds, data.y[0].squeeze().long().item())    ~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)     1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks     1050                 or _global_forward_hooks or _global_forward_pre_hooks):  -> 1051             return forward_call(*input, **kwargs)     1052         # Do not call functions when jit is used     1053         full_backward_hooks, non_full_backward_hooks = [], []    ~/.local/lib/python3.8/site-packages/dig/xgraph/method/gnnexplainer.py in forward(self, x, edge_index, mask_features, **kwargs)      143             self.__clear_masks__()      144             self.__set_masks__(x, self_loop_edge_index)  --> 145             edge_masks.append(self.control_sparsity(self.gnn_explainer_alg(x, edge_index, ex_label), sparsity=kwargs.get('sparsity')))      146             # edge_masks.append(self.gnn_explainer_alg(x, edge_index, ex_label))      147     ~/.local/lib/python3.8/site-packages/dig/xgraph/method/gnnexplainer.py in gnn_explainer_alg(self, x, edge_index, ex_label, mask_features, **kwargs)       84                 h = x       85             raw_preds = self.model(x=h, edge_index=edge_index, **kwargs)  ---> 86             loss = self.__loss__(raw_preds, ex_label)       87             if epoch % 20 == 0 and debug:       88                 print(f'Loss:{loss.item()}')    ~/.local/lib/python3.8/site-packages/dig/xgraph/method/gnnexplainer.py in __loss__(self, raw_preds, x_label)       44     def __loss__(self, raw_preds: Tensor, x_label: Union[Tensor, int]):       45         if self.explain_graph:  ---> 46             loss = cross_entropy_with_logit(raw_preds, x_label)       47         else:       48             loss = cross_entropy_with_logit(raw_preds[self.node_idx].unsqueeze(0), x_label)    ~/.local/lib/python3.8/site-packages/dig/xgraph/method/gnnexplainer.py in cross_entropy_with_logit(y_pred, y_true, **kwargs)       10        11 def cross_entropy_with_logit(y_pred: torch.Tensor, y_true: torch.Tensor, **kwargs):  ---> 12     return cross_entropy(y_pred, y_true.long(), **kwargs)       13        14 class GNNExplainer(ExplainerBase):    ~/.local/lib/python3.8/site-packages/torch/nn/functional.py in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction)     2822     if size_average is not None or reduce is not None:     2823         reduction = _Reduction.legacy_get_string(size_average, reduce)  -> 2824     return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)     2825      2826     ValueError: Expected input batch_size (700) to match target batch_size (1).`"
"I installed DIG and then cloned the repo locally, I tried to run the three3graph.ipynb tutorial as is, but it threw an error on line 4, which reads as follows  `model = SphereNet(energy_and_force=False, cutoff=5.0, num_layers=4,           hidden_channels=128, out_channels=1, int_emb_size=64,           basis_emb_size_dist=8, basis_emb_size_angle=8, basis_emb_size_torsion=8, out_emb_channels=256,           num_spherical=3, num_radial=6, envelope_exponent=5,            num_before_skip=1, num_after_skip=2, num_output_layers=3,          use_node_features=True          )  loss_func = torch.nn.L1Loss()  evaluation = ThreeDEvaluator()`    the error I get is:    ---------------------------------------------------------------------------  AttributeError                            Traceback (most recent call last)    in          4         num_spherical=3, num_radial=6, envelope_exponent=5,        5         num_before_skip=1, num_after_skip=2, num_output_layers=3,  ----> 6         use_node_features=True        7         )        8 loss_func = torch.nn.L1Loss()    ~/.local/lib/python3.6/site-packages/dig/threedgraph/method/spherenet/spherenet.py in __init__(self, energy_and_force, cutoff, num_layers, hidden_channels, out_channels, int_emb_size, basis_emb_size_dist, basis_emb_size_angle, basis_emb_size_torsion, out_emb_channels, num_spherical, num_radial, envelope_exponent, num_before_skip, num_after_skip, num_output_layers, act, output_init, use_node_features)      263         self.init_v = update_v(hidden_channels, out_emb_channels, out_channels, num_output_layers, act, output_init)      264         self.init_u = update_u()  --> 265         self.emb = emb(num_spherical, num_radial, self.cutoff, envelope_exponent)      266       267         self.update_vs = torch.nn.ModuleList([    ~/.local/lib/python3.6/site-packages/dig/threedgraph/method/spherenet/spherenet.py in __init__(self, num_spherical, num_radial, cutoff, envelope_exponent)       22         super(emb, self).__init__()       23         self.dist_emb = dist_emb(num_radial, cutoff, envelope_exponent)  ---> 24         self.angle_emb = angle_emb(num_spherical, num_radial, cutoff, envelope_exponent)       25         self.torsion_emb = torsion_emb(num_spherical, num_radial, cutoff, envelope_exponent)       26         self.reset_parameters()    ~/.local/lib/python3.6/site-packages/dig/threedgraph/method/spherenet/features.py in __init__(self, num_spherical, num_radial, cutoff, envelope_exponent)      196         # self.envelope = Envelope(envelope_exponent)      197   --> 198         bessel_forms = bessel_basis(num_spherical, num_radial)      199         sph_harm_forms = real_sph_harm(num_spherical)      200         self.sph_funcs = []    ~/.local/lib/python3.6/site-packages/dig/threedgraph/method/spherenet/features.py in bessel_basis(n, k)       56         normalizer += [normalizer_tmp]       57   ---> 58     f = spherical_bessel_formulas(n)       59     x = sym.Symbol('x')       60     bess_basis = []    ~/.local/lib/python3.6/site-packages/dig/threedgraph/method/spherenet/features.py in spherical_bessel_formulas(n)       35        36 def spherical_bessel_formulas(n):  ---> 37     x = sym.Symbol('x')       38        39     f = [sym.sin(x) / x]    AttributeError: 'NoneType' object has no attribute 'symbols'    Now I did try a solution which was changing the sym.symbols('x') to sym.Symbol('x'), which is the correct usage according to the SymPy page. However this doesn't solve the problem and gives the exact same error on the same line.    I'm running on a linux pod using docker and kubernetes, other notebooks from the DIG tutorials have worked.    Pytorch 1.8.0  Python 3.6  PyG 2.0.1  #91-Ubuntu SMP  5.4.0-81-generic"
"Hi, this is a nice library. I have two questions regarding the  .    1. For the Property Optimization part, the code uses a pretrained model as show below:         Is the pretrained model obtained from the first part, ""**Random Generation Example**""?    2. How much RAM is needed to load the `ZINC250k` dataset `dataset = ZINC250k(one_shot=False, use_aug=True)` in Random Generation Example? My program keeps crashing due to my 16GB RAM filling up."
"I run your program `xgraph/PGExplainer` on BA-shapes dataset using command `python pipeline.py --dataset_name BA_shapes --random_split True --latent_dim 20 20 20 --concate True --adj_normlize False --emb_normlize True` provided in scripts.sh. However, I get negative fidelity score -0.0706 when I set top_k as 6 because it is the number edge in motifs as paper illustrated:    `fetch network parameters from the saved files  training time is 0.007172066951170564s   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [00:06<00:00,  6.76it/s]  fidelity score: -0.0706, sparsity score: 0.6932`    Secondly, when I add metric 'auc' under the guidance of official code of the paper, I get 0.5348. But the result reported in paper is 0.963, and I can get higher results when I run official code of the paper.    Why is that?  "
"# Describe the bug  I followed the instructions in Readme in SubgraphX, created the env and installed the requirements. However I still cannot reproduce the results. When I ran the script files, I still got error and didn't know why.     The same error also occurred when I ran another bash file.     # Configuration  OS: Release Linux Mint 20.2 Uma 64-bit  python version: 3.7.0  torch and cuda version: 1.9.0+cu102"
"Thanks for this great project.  I notice that XGNN and this project are both your team's work, I wonder if you will add XGNN to this project recently?  Thanks!"
"First, thanks for opening your wonderful code  But I have few questions.    I use the xgraph in  'main' branch, and i got  AttributeError : 'Batch' object has no attribute 'mask'  What should i do to solve this problem?    Moreover, i want to  utilize other GNN models  such as GAT,  APPNP.  How can i use some other GNN models, not the checkpoints?    Thank you, have a nice day!"
"    from dig.xgraph.method import PGExplainer      explainer = PGExplainer(model, in_channels=900, device=device, explain_graph=False)        explainer.train_explanation_network(splitted_dataset)        torch.save(explainer.state_dict(), 'tmp.pt')        state_dict = torch.load('tmp.pt')        explainer.load_state_dict(state_dict)    When running the block of code above, it returns the following error:        ---------------------------------------------------------------------------      UnboundLocalError                         Traceback (most recent call last)      /tmp/ipykernel_1299148/3789046373.py in              2 explainer = PGExplainer(model, in_channels=900, device=device, explain_graph=False)            3       ----> 4 explainer.train_explanation_network(splitted_dataset)            5 torch.save(explainer.state_dict(), 'tmp.pt')            6 state_dict = torch.load('tmp.pt')            ~/anaconda3/envs/xai_conda_env/lib/python3.8/site-packages/dig/xgraph/method/pgexplainer.py in train_explanation_network(self, dataset)          594                     x, edge_index, y, subset, _ = \          595                         self.get_subgraph(node_idx=node_idx, x=data.x, edge_index=data.edge_index, y=data.y)      --> 596                     logits = self.model(data.x, data.edge_index)          597                     emb = self.model.get_emb(data.x, data.edge_index)          598             ~/anaconda3/envs/xai_conda_env/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)         1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks         1050                 or _global_forward_hooks or _global_forward_pre_hooks):      -> 1051             return forward_call(*input, **kwargs)         1052         # Do not call functions when jit is used         1053         full_backward_hooks, non_full_backward_hooks = [], []            ~/anaconda3/envs/xai_conda_env/lib/python3.8/site-packages/dig/xgraph/models/models.py in forward(self, *args, **kwargs)          270         :return:          271         """"""      --> 272         x, edge_index, batch = self.arguments_read(*args, **kwargs)          273           274             ~/anaconda3/envs/xai_conda_env/lib/python3.8/site-packages/dig/xgraph/models/models.py in arguments_read(self, *args, **kwargs)           42             elif len(args) == 2:           43                 x, edge_index, batch = args[0], args[1], \      ---> 44                                        torch.zeros(args[0].shape[0], dtype=torch.int64, device=x.device)           45             elif len(args) == 3:           46                 x, edge_index, batch = args[0], args[1], args[2]            UnboundLocalError: local variable 'x' referenced before assignment    A similar error occurs with SubgraphX, but not with GNNExplainer. What is the problem?"
Thanks for your Repo.  I'm wondering why config the target model takes so much time?
"Hi, thanks for the great repo!     I am wondering if there is an optimal parameter set to reproduce the results for your SphereNet paper? "
"Thanks for providing this amazing work.     When I run the GraphAF benchmark, I encounter this error:    RuntimeError: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cpu    After checking the code, I find that in line 32 of graphflow.py, the model MaskedGraphAF still uses CPU. I can remove the above error by adding:    self.flow_core = self.flow_core.cuda()    Just want to make sure whether this is a bug. Thanks.  "
"Hello,     I would like to ask you for detailed info, especially on how to identify the subgraph with the following questions:    1. How can we select N0 as initialization which you define the root of search tree?      N_i means node, so Figure1 in the paper of subgraphX, which node is the N0 as initialization in root?       2. It is mentioned that h(N_i) denotes the associated subgraph of tree node N_i. I would like to know how you define ""associated subgraph"" and in Figure1, do you describe h(N_i)?    3. To obtain concrete understanding notation S, in Figure1 S as the possible coalition set of players is (node1), (node6) respectively, and (node 1 and 6). But in pseudo-code of Algorithms1, S_l is the leaf set so that the Leaves graph (which node list is equal to [ 2, 4] and edge list is equal to [(2, 4)]) belongs to S_l.    Thank you !"
"Hey folks,     Quick question regarding XGNN. It seems missing from dig branch (but available in main branch). Any specific reasons behind its (temporary) unavailability?    Cheer,    V"
"Hi:     In SubgraphX/models/train_gnn.py, you first trained a graph neural network for later calculation of each subgraph shapley value. While I use MUTAG dataset, run train_gnn.py, I found there is a overfitting problem. In this training process, MUTAG train accuracy is near to 100 percent and the gap between train accuracy and valid accuracy is large. I want to learn whether there occurs overfitting."
dig/ggraph/method/GraphDF/model/df_utils.py line 117         Would you fix this problem in the future?
dig/ggraph/method/GraphDF/model/df_utils.py line 117         Would you fix this problem in the future?
"Hi,    I read the paper ""On Explainability of Graph Neural Networks via Subgraph Explorations"" and find your work quite exciting. I take a look at your codes and find subgraphx can only explain graphs one by one. Is it possible to send a batch of graphs to subgraphx so that the explanation can speed up?"
"in line 683 of subgraphx.py:             "" related_preds.append({'masked': maskout_score,""  Maybe ""related_preds.append({'maskout': maskout_score,"""
"While running the codes in   at line `evaluator.evaluate(learning_model=graphcl, encoder=encoder)  `  I'm getting error:     I run this code:       Torch version: 1.9.0 (cpu)    Any help will be highly appreciated."
The error comes out while executing your  .  The code block:       The error message:       My installed packages:       I have installed the latest version of DIG from source.
"First of all, I really appreciate your work in contributing this library, it's really useful. However, while following your  , I got the following error:       The corresponding code block is:       My installed packages are as follow:       Thanks in advanced!"
"Hi,    When I read the GNNExplainer source code, I have a little confused about how to edge_mask was applied to the forward step.    For instance, considering the demo:   I do found that we call the model **GCN_2l** and the **gnn_explainer_alg** also optimized the loss function including both feature mask and edge mask. It seems that the code did not use the class GCN_2l_mask which directly using of the edge_mask.    Thank you so much for your time.    Best,  Chao    "
"Hi,  When I try to run the gnnexplainer.ipynb, error comes as:   from inspect import isclass  ----> 9 import dig.xgraph.models.models as models       10 import torch    AttributeError: module 'dig.xgraph' has no attribute 'models'    Any suggestions? Thanks"
"I am trying to run the GNN Explainer code. Although I have used the install.batch to generate the conda environment, I am having issues with the packages. Namely with tap (in the line ""from tap import Tap"") . I tried installing it myself and I can import tap but doesn't seem to find anything called Tap on it. Could you guide me here please?    Also, I was going through the example and I was wondering which settings should I change to run it on the graph level predictions. Is there an example for that? And, is it possible to get the feature importances and the important subgraphs? I didn't see any outputs in the explain functions.    Thanks in advance!  "
Thanks for this very useful library. Is it please possible to add some code similar to benchmarks/threedgraph/threedgraph.ipynb for running Spherenet on MD17 together with the necessary parameters to reproduce the results from the paper ? Thanks!
"Thanks for the work behind this repo! I was wondering, do you have implemented the explainability methods for graph-level predictions as well? Or only node features?    Also, do you plan to develop the library to work with  ?    Thanks!"
"Hello,     I would like to ask about the loss function of XGNN in gnn_explain.py with a following question:    Specifically at line 100~ 119 in gnn_explain.py  when total_reward < 0 then loss is also negative, but loss should not be below zero.. is there any preprocessing the negative reward? (I hardly find this part of the code)           Thank you for your help in advance!"
"Is it please possible to publish the full set of parameters that are required in order to reproduce the published results of Spherical 3D MPNN on QM9 and MD17 ? I tried using the default parameters in   and played with some variations (lr, bs, etc) but the results I got were quite far from the published results, for both Spherical Net and DimeNet. Worse, the models reach a plateau quite early in the optimization (e.g. after 20 epochs or so). Would be really great if you could publish the full set of hyperparameters used to obtain the results in your paper. Thank you!"
"Dear author, I have two questions about **XGNN**.    First, why the explainable results of each generation differ greatly? Moreover, the generated molecular graph belonging to mutagenic class often contains no carbon-ring or nitro group. Are there effective hyperparameters available for generating better explanations?    Second, I found the loss value is, by turns, positive and negative. So why does this phenomenon appear? "
"Hi,    Thanks for the nice work. When trying to run threedgraph with dimenet with target='mu', i get the folllowing assertion error:      File ""/usr/local/lib/python3.9/site-packages/dig/threedgraph/method/run.py"", line 71, in run      valid_mae = self.val(model, valid_loader, energy_and_force, p, evaluation, device)    File ""/usr/local/lib/python3.9/site-packages/dig/threedgraph/method/run.py"", line 174, in val      return evaluation.eval(input_dict)['mae']    File ""/usr/local/lib/python3.9/site-packages/dig/threedgraph/evaluation/eval.py"", line 30, in eval      assert(len(y_true.shape) == 1)  AssertionError    Thanks for looking into this!"
"hello dear,    Can you please share the gnns.py  and gnn_explain.py for synthetic , Text2Graph and Molecule datasets (except MUTAG)?    thanks in advance"
"I find that the edge mask values in the pgexplainer implementation can be larger than 1 when i run the code on BA_shapes dataset. In the original paper and the official implementation the values are in (0, 1). Would this be a problem?  an example edge mask output:  ori_node_idx = 699  tensor([6.2408, 6.2408, 6.2408, 6.2408, 6.2408, 6.2408, 6.2408, 6.2408, 6.2408,          6.2408, 7.1816, 7.1816, 5.8144, 5.4829, 5.3828, 5.8144, 5.2910, 5.1266,          5.2910, 4.9597, 5.4829, 4.9597, 5.3828, 5.1266], device='cuda:0')  "
"In SubgraphX, it is very inspiring that zero-padding is introduced.    I have the same observation that GNN is very very sensitive to topology perturbation. Thus I have some questions:    1. Regarding setting features to be 0 - in MUTAG dataset, I suppose the default node feature 0 represents the atom is Carbon, do you set feature value to 0 in zero-padding part in a way that they actually all turn into Carbon or any other atom?   (but after I think it through, I think you mean setting features all to zero given the node label is encoded in a one-hot way, such that setting zeros will deactivate the weights for node features, correct me if I'm wrong.)    2. Compared with simply removing the nodes from the graph, how does zero-padding affect the modelâ€™s prediction in empirical study?    Thanks!"
"Hello DIG team,    In   the training script is     But seems that `vae_train.py` is missing.    Also `sample.py` in   is missing."
"Hello DIG team,    In JT-VAE sub-repo (  when I run `python mol_tree.py --data_file ""moses.csv""`, it outcomes:   "
"I finished the Installation and tried to run the demo of GradCAM, but the error happened as below.    "
"Hello,  I wish to use sslgraph from DIG.   I downloaded the codes from github. However, I cannot install the packages  by doing   pip install git+git://github.com/divelab/DIG.git ,  inside a conda environment.  This immediately gives errors.   ERROR: Command errored out with exit status1: python setup.py egg_info Check the logs for full command output.    Do you have suggestions? Is there a setup.py file available?  Thank you,  Saheli"
How do i get the required packages for xgraph/PGExplainer? The install.sh does not work for me.
"I was initially confused by getting a negative loss using the JSELoss and by the implementation here where the Jensen Shannon Divergence is shifted:       To avoid others stumbling over this:  Apparently, this choice was made for consistency with other divergences, and it does not affect anything in training and model performance.  This issue in the Deep Infomax library explains why the shift is included  "
"HI, I wonder if the XGNN toolkit can be applied to graphs with temporally varying features. Currently, my model was trained using GCN-LSTM algorithm, for a regression problem, not classification. Thanks ahead."
"Is there a reason some of the methods are evaluated for fidelity by zeroing out the node features, and for subgraphX it seems that the fidelity is being reported as the output of the subgraph itself which is not consistent with the definition of fidelity. Could you please elaborate more on this? Also zeroing out the features should change the data distribution itself as the model might not have seen nodes with 0 features before, so why use zeroing out?"
"Thanks for this great work!     I suggest that adding the   to the ssl-graph classification could be more perfect. As far as I know, the OGB benchmark is more challenging and convincing than the TUDataset. Are the developers interested in merging it into DIG? Thanks a lot!    "
"Thanks for your great work.    In   line 2, `from sslgraph.eval_graph import EvalUnsupevised` should be `from sslgraph.utils.eval_graph import EvalUnsupevised`. Feel free to close it upon correction."
"Here is an error message :       In particular it is an issue with this bit of lean code:     The issue is in two ways to parse the argument of `unfreezingI`:  1. Parse the tactic block manually by first matching parentheses and then taking any whitespace and comments.  2. Use the beginning and end positions of the parameters as provided by Lean.    We do (1) but then check with an `assert` that it agrees with (2).  The reason we do this is that the parsing is quite finicky, so this gives us assurance that we are parsing correctly.  Also, there are other times, e.g. `conv` tactics, where Lean can't give us information about the position of the argument and we just have to parse it using (1).  This check makes sure we are using the right method in those cases too.    The problem here is that doc strings shouldn't count toward the parameter (while line comments and normal block comments should).      There are two possible fixes:  1. Modify the parsing code (around line 851 of parser.py) to not consume all white space.  Instead, (similarly to the parsing of other arguments) we should parse itactics as follows:      1. Parse the tactic block (matching parentheses).      2. Test that we are before the end of the Lean-recorded parameter.  If not raise an error (not using an assert).      3. Take white space and comment tokens one at a time until we get to the desired end (like the code for parsing other parameters).  Raise an error if a non-whitespace token is present.  2. Remove doc strings from `consume_space` method.  To do that, we would have to change the tokenizer to treat doc strings as different from other block comments.  We would also have to probably add doc strings to places in the code where we are looking for a command (e.g. `def`) since a doc string would mean we are at the end of a proof.    (2) is technically better and ideally it should still be done since it would prevent doc strings from being recorded as part of the tactics, but it is also pedantic and a lot more work.  (1) should be sufficient for this problem, especially since all comments including doc strings are stripped out of the training data.  I recommend doing (1) which should be fast and easy to implement."
"Currently, we get the following error when trying to process the data for bernstein.lean.     The issue, is two fold:  - There is a local notation in the file using `|`, which is normally used to delimit cases in a `match` or in an `inductively` defined term proof.  - It is used inside `calc` which is one of the few cases where we have no information from Lean on how to parse, so I have to just scan along until we reach a reasonable end point.  Currently that is set as one of ""end"", "";"", "","", ""|"", "" "", a right bracket, or a user command (but taking nesting parentheses and match statements into account).    Hence it is done scanning too early on the calc tactic.  Luckily this is caught as an error, since the calc tactic is inside a tactic list so we know that the calc tactic can actually only end with "","" or ""end"" (or "";"", or "" "", if it was inside a semicolon list or alternative inside that outer tactic list.)    I see two solutions:  1. Scan the calc tactic in stages.  Since the calc tactic is of the form `calc xxx : xxx ... xxx : xxx ... xxx : xxx` (where the `xxx` are terms), I could first scan until `:`, and then scan until `...` or one of the end delimiters.  In this particular case, that should catch the user notation since it is always on the left of the `:`.  2. Make the end points for scanning the calc tactic depend on if I am inside a tactic list or a `by`.    I think I like option (1) better, but let me think about it for a bit first."
"The core `.lean` files used to be ""-rw-r--r--"", but I they are now ""-r--r--r--"" (as per a recently change to `elan`).  So when I copy them over they keep these permissions.  I need to change the permissions back, either when copying them over or write before I modify the file.  (Note, the mathlib files still have write permissions, but that isn't guaranteed to last, so I should force them to be writable.)"
"Here is an error message :       In particular it is an issue with this bit of lean code:     The issue is in two ways to parse the argument of `unfreezingI`:  1. Parse the tactic block manually by first matching parentheses and then taking any whitespace and comments.  2. Use the beginning and end positions of the parameters as provided by Lean.    We do (1) but then check with an `assert` that it agrees with (2).  The reason we do this is that the parsing is quite finicky, so this gives us assurance that we are parsing correctly.  Also, there are other times, e.g. `conv` tactics, where Lean can't give us information about the position of the argument and we just have to parse it using (1).  This check makes sure we are using the right method in those cases too.    The problem here is that doc strings shouldn't count toward the parameter (while line comments and normal block comments should).      There are two possible fixes:  1. Modify the parsing code (around line 851 of parser.py) to not consume all white space.  Instead, (similarly to the parsing of other arguments) we should parse itactics as follows:      1. Parse the tactic block (matching parentheses).      2. Test that we are before the end of the Lean-recorded parameter.  If not raise an error (not using an assert).      3. Take white space and comment tokens one at a time until we get to the desired end (like the code for parsing other parameters).  Raise an error if a non-whitespace token is present.  2. Remove doc strings from `consume_space` method.  To do that, we would have to change the tokenizer to treat doc strings as different from other block comments.  We would also have to probably add doc strings to places in the code where we are looking for a command (e.g. `def`) since a doc string would mean we are at the end of a proof.    (2) is technically better and ideally it should still be done since it would prevent doc strings from being recorded as part of the tactics, but it is also pedantic and a lot more work.  (1) should be sufficient for this problem, especially since all comments including doc strings are stripped out of the training data.  I recommend doing (1) which should be fast and easy to implement."
"Currently, we get the following error when trying to process the data for bernstein.lean.     The issue, is two fold:  - There is a local notation in the file using `|`, which is normally used to delimit cases in a `match` or in an `inductively` defined term proof.  - It is used inside `calc` which is one of the few cases where we have no information from Lean on how to parse, so I have to just scan along until we reach a reasonable end point.  Currently that is set as one of ""end"", "";"", "","", ""|"", "" "", a right bracket, or a user command (but taking nesting parentheses and match statements into account).    Hence it is done scanning too early on the calc tactic.  Luckily this is caught as an error, since the calc tactic is inside a tactic list so we know that the calc tactic can actually only end with "","" or ""end"" (or "";"", or "" "", if it was inside a semicolon list or alternative inside that outer tactic list.)    I see two solutions:  1. Scan the calc tactic in stages.  Since the calc tactic is of the form `calc xxx : xxx ... xxx : xxx ... xxx : xxx` (where the `xxx` are terms), I could first scan until `:`, and then scan until `...` or one of the end delimiters.  In this particular case, that should catch the user notation since it is always on the left of the `:`.  2. Make the end points for scanning the calc tactic depend on if I am inside a tactic list or a `by`.    I think I like option (1) better, but let me think about it for a bit first."
"The core `.lean` files used to be ""-rw-r--r--"", but I they are now ""-r--r--r--"" (as per a recently change to `elan`).  So when I copy them over they keep these permissions.  I need to change the permissions back, either when copying them over or write before I modify the file.  (Note, the mathlib files still have write permissions, but that isn't guaranteed to last, so I should force them to be writable.)"
"Hello, In section3 COMPUTATIONAL COMPLEXITY ANALYSIS, you claimed the gpu usage of bgrl on Coauthor CS is 2.86 GB, while i follows the discription of read.md. By running python3 train_transductive.py --flagfile=config/coauthor-cs.cfg. The total GPU usage is 5.6 G. what am i missing?  Maybe this experiments conducted in anther setting, which is not consistent with here u provided? like a smaller hiddden units? half of them? right  "
"Dear authors,     Thanks for open-sourcing the code. I'm wondering what're the settings to reproduce results reported in the paper?    "
"Hello, first of all, thank you for a job well done. I have a problem with BGRL. In the data reading stage, I found that the code only standardized WikiCS data sets with mean 0 and standard deviation 1, while other data sets were not processed in this way. May I ask why this was done?  !   "
"Dear authors, thanks for sharing the code of BGRL, which is really helpful for my research! Would you mind sharing your codes and running instructions for ogbn-arXiv datasets? It would be better if you can also share the codes of the baseline model: GRACE-SUBSAMPLING.    Best"
"Hello, could you share something about the processing of the dataset? Thank you!"
"Hi Burhaneddin,    It is really a nice job. I use your code and train on my data. The result is perfect but it seems to take a long time to train (in my case, ~10 hours for 100 epochs). Does this make sense to you or something is wrong when I am training?  Thank you so much for your help!    Best,  Heng "
"Hello,    I'm trying to install all the repository of TOGL, including pyper and torch_persistent_homology. All seems to be run correctly except for some warnings after the installation of the 4 dependenses of torch-geometric. But when I run the command **poetry run python topognn/train_model.py --model TopoGNN --dataset DD --max_epochs 10** in my Mac an error occurs:    OSError: dlopen(/Users/user/Library/Caches/pypoetry/virtualenvs/topognn-PYWNzDgl-py3.8/lib/python3.8/site-packages/torch_sparse/_convert.so, 6): Symbol not found: __ZN2at5emptyEN3c108ArrayRefIxEERKNS0_13TensorOptionsENS0_8optionalINS0_12MemoryFormatEEE      Referenced from: /Users/user/Library/Caches/pypoetry/virtualenvs/topognn-PYWNzDgl-py3.8/lib/python3.8/site-packages/torch_sparse/_convert.so      Expected in: /Users/user/Library/Caches/pypoetry/virtualenvs/topognn-PYWNzDgl-py3.8/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib     in /Users/user/Library/Caches/pypoetry/virtualenvs/topognn-PYWNzDgl-py3.8/lib/python3.8/site-packages/torch_sparse/_convert.so    For instance I have installed python 3.8.10 and Iâ€™m using a MacBook Air with the following features: MacBook Air (M1, 2020), with chip Apple M1, 8 Giga RAM.    I have tried to install the current repository also in linux with suitable docker, following the suggestions in the conversation with the title ""Undefined Symbol _ZN3c106detail19maybe_wrap_dim_slowEllb"", but the code still doesn't work and stops with the following error:    !      No matter what OS involved, the issue seems to be related to some .so file that cannot be open or be found. Now I have no idea about the nature of this problem and how to solve it. Can you please give me any suggestions in order to manage this issue, please??  I have found really interesting your paper and it's indeed a pity having these problems in running codes.    Thanks in advance."
"Hi there,  Thank you for providing the codebase.  I am currently testing without using Poetry to understand how the topology layer works.  However, the following error occurs when importing the ""torch_persistent_homology"" used to import the ""TopologyLayer"".  Is there a way to solve the error below or a calculation file of Python version instead of C++ in ""torch_person_homology""?  !     "
"Hi, thank you for your codebase. While I set the environment for torch-persistent-homology in Win10 system, I met many problems. So I changed to a Ubuntu server. I read the issues in  , so I use Anaconda3 to bulid a Python3.8 virtual envrionment and installed torch==1.12.1, torch-geometric==2.1.0, scipy==1.7.3. Then I installed torch-persistent-homology with the command `pip install .` , it goes well. And I checked my environment with `conda list`, it's listed up there. But when I trid to test it with   `>>> from torch_persistent_homology.persistent_homology_cpu import compute_persistence_homology_batched_mt`  The result comes to an error  `Traceback (most recent call last):`  `  File "" "", line 1, in  `  `ModuleNotFoundError: No module named 'torch_persistent_homology.persistent_homology_cpu'  `  I don't know why I still can't use it, so I would like to ask for your help, thank you.  "
"Hi,  When running TopoGNN with `poetry run python topognn/train_model.py --model TopoGNN --dataset MUTAG --max_epochs 10`, the run fails with: `/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.`  I am almost sure that this is not an environment issue since other datasets such as DD, IMDB-MULTI runs successfully.  My guess is that the labels for MUTAG is incorrect."
"Hi Bastain  I resolved the earlier issue related to Pytorch Lightning, and tried running your code again  I ran the command   **(togl) mohit@xavier:~/TOGL$ WANDB_MODE=disabled python topognn/train_model.py --model TopoGNN --dataset DD --batch_size 20 --lr 0.0007**    The traceback of the error is as follows:         "
"Hi Bastian,    I went through your video at Oxford Summer School and landed here after watching the talk on topo GNN. This is impressive.    I was thinking of whether this work can be applied to LiDAR point clouds where node relations between adjacent nodes have to be discovered.  Given this work is applied on graphs where edge information is available unlike LiDAR point clouds where we the edges and their strength has to be discovered. Would this work there or what modification would I need to make in order to get this code running for point clouds.    Eagerly awaiting your reply.    Thanks,  Prashant"
"Hi Bastain!  I tried installing without poetry and running your code.  Everything worked...  I am not able to figure out how to set the DATA_DIR , as the code is looking for the data in the wrong directory.   Here is the output that I get         _Originally posted by @mohit-kumar-27 in  "
"(togl1) mohit@user-Default-string:~/mohit/TOGL$ poetry run topognn/train_model.py --model GNN --dataset MNIST  Traceback (most recent call last):    File ""topognn/train_model.py"", line 12, in        import topognn.models as models    File ""/home/mohit/mohit/TOGL/topognn/models.py"", line 15, in        from topognn.layers import GCNLayer, GINLayer, GATLayer, SimpleSetTopoLayer, fake_persistence_computation#, EdgeDropout    File ""/home/mohit/mohit/TOGL/topognn/layers.py"", line 8, in        from torch_persistent_homology.persistent_homology_cpu import compute_persistence_homology_batched_mt  ImportError: /home/mohit/mohit/TOGL/repos/torch_persistent_homology/torch_persistent_homology/persistent_homology_cpu.cpython-38-x86_64-linux-gnu.so: invalid ELF header      I am trying to run your code, I followed all your instructions in the README but I am getting this error.  I am using pytorch 1.11.0 with cuda 11.3, I have installed the dependencies accordingly.    the specifications of the linux machine I am using is as follows:    x86_64  DISTRIB_ID=Ubuntu  DISTRIB_RELEASE=18.04  DISTRIB_CODENAME=bionic  DISTRIB_DESCRIPTION=""Ubuntu 18.04.6 LTS""  NAME=""Ubuntu""  VERSION=""18.04.6 LTS (Bionic Beaver)""  ID=ubuntu  ID_LIKE=debian  PRETTY_NAME=""Ubuntu 18.04.6 LTS""  VERSION_ID=""18.04""  HOME_URL=""   SUPPORT_URL=""   BUG_REPORT_URL=""   PRIVACY_POLICY_URL=""   VERSION_CODENAME=bionic  UBUNTU_CODENAME=bionic    Could you please suggest how I could solve this issue"
"After poetry is installed, there exits ""  ValueError"" when I run ""poetry install"".  It prompts that ""  Directory .\TOGL-main\repos\pyper does not seem to be a Python package""?  How can I solve this problem?"
"I tried replicating the results in your paper following your instructions, however it seems like the parameters are not correct. For example, when I run:  `poetry run topognn/train_model.py --model TopoGNN --dataset ENZYMES --depth 3 --batch_size 20 --lr 0.0007 --lr_patience 25 --min_lr 0.000001`    I get an error, as these are the arguments that are indicated as valid for the train_model.py script when running the above instruction:   "
"Hi, I am sorry in advance for the long post, but I am having a lot of issues installing this repository, and I would highly appreciate any help you can provide.    I am trying to install this code on a machine with Linux 20.04.3 and CUDA 11.1.    The first thing I tried was to follow exactly the instructions you provide, so I did:         but I get the following error:       I thought the error could be that the Pytorch Geometric libraries are installed for CUDA 11.0 and not 11.1 as in my machine, so I modified the dep.py file accordingly and reinstalled everything, but I then got the following error:         At this point I thought I should try installing without using poetry, as it seems to be messing up the installation of PyTorch and PyG (specially the CUDA versions). I then did the following:     When I run   `python topognn/train_model.py --model TopoGNN --dataset ENZYMES --max_epochs 10`  The code finally starts, but as soon as it enters the training loop, I get the following error:       I thought the issue could be that the version of PyTorch Geometric that is installed this way is not the one you have in your poetry.lock file, so I then redid this same procedure but installed `torch-geometric==1.6.3`. However I then get the error:         I then tried to install PyG from pip instead of from conda:     But I then get:       At this point I was desperate and then tried to just install the CPU version, first like this:     which leads to        And finally like this:     which leads to       Is there something wrong that I am doing, or have you encountered some of these issues before? "
Hi!  I was trying to use this repo but it has a git submodule pointing to a no longer existing repo:      Could be possible to restore that third repo or to include it here?  Maybe @ExpectationMax?    Thanks!
"Some of the sweeps fail, no pattern apperent. Example:   "
Hi    This work is interesting!   I see in the README that the plan for releasing code is 2021.5.01 but currently I do not see anything.   Do you have any plans to release code for this project?    Thanks!
Would you be releasing the pretrained model weights of stylegan2 used for your experiments for dsprites?
"hi, thanks for your excellent work and when will you release the code? "
"Hi, thanks for releasing the code!    When I tried to use multiple GPUs by `model = RotoGrad(backbone, heads, 64, normalize_losses=True)`, `model = torch.nn.DataParallel(model, device_ids=[0,1]).to(torch.device(""cuda:0""))`, there was a RuntimeError:         And when I tried to use single GPU by `model = model.to(torch.device(""cuda:0""))`, it looks like no problem.     I'm not sure if this project can support multiple GPUs, or if there is something wrong with my own code. Any help woud be greatly appreciated. Thanks!"
"**Problem:** RotoGrad accepts as `latent_size` argument a single integer, as the assumption is that gradients are of the form `batch_size x latent_size`. However, some architectures have different shapes for their gradients (see #2).    **Solution:** Change the argument so that `latent_size` can be an integer or a tuple/shape object. If it is a tuple, we set the rotations to the product of its elements, flatten the gradients upon entry, and reshape them before returning them.  "
"hi adrianjav,     thanks for your library.  I install rotograd as pip install rotograd. when running my project, i get errors like this:    Traceback (most recent call last):    File ""train_frontend2.py"", line 282, in        train(0, args, configs, batch_size, num_gpus)    File ""train_frontend2.py"", line 124, in train      seg_socres, pinyin_scores = model((text_id, mask))    File ""/home/xxxl/anaconda3/envs/py37_torch1.71/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl      result = self.forward(*input, **kwargs)    File ""/home/xxx/anaconda3/envs/py37_torch1.71/lib/python3.7/site-packages/rotograd/rotograd.py"", line 291, in forward      out_i = head(rep_i)    File ""/home/xxx/anaconda3/envs/py37_torch1.71/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl      result = self.forward(*input, **kwargs)    File ""/home/xxx/anaconda3/envs/py37_torch1.71/lib/python3.7/site-packages/torch/nn/modules/container.py"", line 117, in forward      input = module(input)    File ""/home/xxx/anaconda3/envs/py37_torch1.71/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl      result = self.forward(*input, **kwargs)    File ""/home/xxx/anaconda3/envs/py37_torch1.71/lib/python3.7/site-packages/rotograd/rotograd.py"", line 146, in forward      new_z = rotate(z, R, self.p.input_size)    File ""/home/xxx/anaconda3/envs/py37_torch1.71/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 779, in __getattr__      type(self).__name__, name))  torch.nn.modules.module.ModuleAttributeError: 'ParametrizedRotoGrad' object has no attribute 'input_size'      the input_size can be found in version 0.1.2.  when changing input_size as latent_size,   I get errors like this:      Traceback (most recent call last):    File ""train_frontend2.py"", line 282, in        train(0, args, configs, batch_size, num_gpus)    File ""train_frontend2.py"", line 124, in train      seg_socres, pinyin_scores = model((text_id, mask))    File ""/home/xxx/anaconda3/envs/py37_torch1.71/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl      result = self.forward(*input, **kwargs)    File ""/home/xxx/anaconda3/envs/py37_torch1.71/lib/python3.7/site-packages/rotograd/rotograd.py"", line 291, in forward      out_i = head(rep_i)    File ""/home/xxx/anaconda3/envs/py37_torch1.71/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl      result = self.forward(*input, **kwargs)    File ""/home/xxx/anaconda3/envs/py37_torch1.71/lib/python3.7/site-packages/torch/nn/modules/container.py"", line 117, in forward      input = module(input)    File ""/home/xxx/anaconda3/envs/py37_torch1.71/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl      result = self.forward(*input, **kwargs)    File ""/home/xxx/anaconda3/envs/py37_torch1.71/lib/python3.7/site-packages/rotograd/rotograd.py"", line 146, in forward      new_z = rotate(z, R, self.p.latent_size)    File ""/home/xxx/anaconda3/envs/py37_torch1.71/lib/python3.7/site-packages/rotograd/rotograd.py"", line 109, in rotate      return torch.einsum('ij,bj->bi', rotation, points)    File ""/home/xxx/anaconda3/envs/py37_torch1.71/lib/python3.7/site-packages/torch/functional.py"", line 344, in einsum      return _VF.einsum(equation, operands)  # type: ignore  RuntimeError: dimension mismatch for operand 1: equation 2 tensor 3    Can you give me some suggestions ï¼Ÿ"
"Thanks for your good job!  I donâ€™t find any assignment for â€œself.gradsâ€ in RotateOnly so the â€œself.gradsâ€ remains None when running _rep_grad, is there any solutionï¼Ÿ  !   "
"Hi author,  May I ask when the latest version of code can be updated? The latest version of code is related to the pseudocode below:  !   "
None
"The visualisers were recently improved in #10, but several confusing aspects still remain. This issue is meant for tracking all these quirks while they are being cleaned up.  - [ ] In the pictures of the partial molecules produced in HTML mode, only some of the atoms are displayed with IDs. However, atoms without IDs can still later appear in edge prediction steps, at which point it's not possible to match the IDs shown in the table to atoms in the picture.  - [ ] The atom/motif choice step omits the ""no more nodes"" class, which can be especially confusing at the last step when this is the ground-truth class.  - [ ] Steps are not numbered consistently (depending on the mode, either 0-based or 1-based), and the first node prediction step is not numbered at all. Moreover, the division of the different choices into steps is centered around edge/attachment decisions, with node decisions being glued to one of these, and this can sometimes lead to confusing outputs. In particular, in the ""decode from samples"" mode, in some cases the first _two_ node selection steps are shown _before_ the ""Step 1"" heading is rendered. It would be clearer to just show one step for every node/edge/attachment decision. One complication is that some of these decisions are skipped in the decoder if there is only one choice to be made (e.g. choosing an attachment point in a benzene ring); ideally, the visualisers would explicitly call it out and label it as ""skipped""."
"We intended to turn off tensorflow warnings (such as those about TensorRT mentioned in #18), but currently that doesn't seem to be working. The problem is likely that we're setting the appropriate environment variable _after_ we import tensorflow (see `cli/` and `utils/cli_utils.py`).    Also, the `supress_tensorflow_warnings` function has a typo in ""suppress""."
"Currently   does not pin versions for `black` and `flake8`. We should add a pre-commit config (with appropriate instructions in the `README.md`), which could then be used in the CI to make sure it uses the same package versions as the repo maintainers."
"Currently CGVAE is not properly tested and not supported by the inference server used by MoLeR. Even though CGVAE works worse than MoLeR across the board, it may still be worthwhile to add full support for it for any future comparisons, as generalizing the inference server to support both CGVAE and MoLeR shouldn't be too hard."
"There are no tests covering the CLI entry points or the model wrappers, making it easy to break the wrappers during refactoring. While there is one integration test, it's slightly lower-level and uses the `MoLeRInferenceServer` directly. We should add some higher-level end-to-end tests."
"Hi,    It is mentioned in the paper that motif embeddings of size 64 were learned and concatenated to the atom input. Could I check, how do we generate these learned embeddings? Or are they already included in the partial node features when we run the preprocessing command using the CLI?     Additionally what does partial_node_categorical_features refer to in the input data? Thanks!"
!   
"Hi,  I am trying to run the command     `molecule_generation sample MODEL_DIR 10`    On my M1 mac, trying this with the current install instructions leads to the following error:    `zsh: illegal hardware instruction  molecule_generation sample MODEL_DIR 10`    Do you know any fix for this?    Thanks,  Mike  "
"Thanks for this excellent work !    In the README inference part, ""molecule_generation encode MODEL_DIR SMILES_PATH OUTPUT_PATH"" is used to generate molecules. How can I sample molecules based on a specific scaffold?    Looking forward to your reply"
"Hi,    I'm trying an experiment, to fine-tune the generator with a small set of molecules with specific properties, (so it will generate new molecules with similar properties) but I'm running into some errors that I have been unable to solve. I'd really appreciate if anyone could shed some light into what I'm doing wrong.    What I'm doing:     1. From a set of 10 molecules, split into 80:10:10 for train:valid:test. Put into folder `finetune_moler/input`.  2. Run MoLeR in pre-process mode:   `$ molecule_generation preprocess finetune_moler/input finetune_moler/output finetune_moler/trace`  4. Then try to finetune the pre-trained model provided with the small set of molecules above:    molecule_generation train MoLeR finetune_moler/trace \      --load-saved-model ./PRETRAINED_MODEL/GNN_Edge_MLP_MoLeR__2022-02-24_07-16-23_best.pkl \      --load-weights-only \      --save-dir finetune_moler/tuned_model     (dumps a lot of informational messages)  Traceback (most recent call last):    File ""/opt/miniconda3/envs/moler-env/bin/molecule_generation"", line 33, in        sys.exit(load_entry_point('molecule-generation', 'console_scripts', 'molecule_generation')())    File ""/home/seabra/work/source/repos/microsoft/molecule-generation/molecule_generation/cli/cli.py"", line 35, in main      run_and_debug(lambda: commands[args.command].run_from_args(args), getattr(args, ""debug"", False))    File ""/opt/miniconda3/envs/moler-env/lib/python3.10/site-packages/dpu_utils/utils/debughelper.py"", line 21, in run_and_debug      func()    File ""/home/seabra/work/source/repos/microsoft/molecule-generation/molecule_generation/cli/cli.py"", line 35, in        run_and_debug(lambda: commands[args.command].run_from_args(args), getattr(args, ""debug"", False))    File ""/home/seabra/work/source/repos/microsoft/molecule-generation/molecule_generation/cli/train.py"", line 140, in run_from_args      loaded_model_dataset = training_utils.get_model_and_dataset(    File ""/opt/miniconda3/envs/moler-env/lib/python3.10/site-packages/tf2_gnn/cli_utils/model_utils.py"", line 319, in get_model_and_dataset      load_weights_verbosely(trained_model_file, model)    File ""/opt/miniconda3/envs/moler-env/lib/python3.10/site-packages/tf2_gnn/cli_utils/model_utils.py"", line 148, in load_weights_verbosely      K.batch_set_value(tfvar_weight_tuples)    File ""/opt/miniconda3/envs/moler-env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler      raise e.with_traceback(filtered_tb) from None    File ""/opt/miniconda3/envs/moler-env/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 911, in assign      raise ValueError(  ValueError: Cannot assign value to variable ' decoder/node_categorical_features_embedding/categorical_features_embedding:0': Shape mismatch.The variable shape (98, 64), and the assigned value shape (166, 64) are incompatible.  ```    Could someone point me to what I'm doing wrong? Would it be possible to get an example of successfully fine-tuning the model?    Thanks a lot!  Gustavo."
"Hi,    First of all, I want to thank you guys for making this available for the community. This is a great step forward in molecular generation.     However, I noticed a slight problem when using GPU. After strictly following the steps in the installation, and saving the pretrained model in the `PRETRAINED_MODEL` dir, I got the results below. It does still work, but there's something weird going on with the Tensorflow.    I tried installing `tensorflow-gpu==2.1.0`, but the message persists. What am I missing?    I'm working on a RHEL8 station with CUDA-10.2.    All the best,  Gustavo.         "
"Due to various dependency issues we pin tensorflow to 2.1.0, which is not compatible with Python 3.8+, meaning that only Python 3.7 is supported. It would be great to unpin or at least update some of the dependencies, so that we can support all modern versions of Python."
"If `MoLeRInferenceServer` receives an invalid SMILES string (e.g. through `molecule_generation encode`), it just hangs, likely because the corresponding process failed, and the server ends up waiting for it indefinitely. This is far from ideal.    I see two kinds of behaviour we could aim for instead when encoding receives an invalid molecule:  - The top-level process itself also fails with an exception.  - The encoding carries on, returning `None` for the invalid molecules.    If possible, it could be nice to support both by having `__init__` accept an `fail_on_invalid_smiles: bool` flag."
"Currently all CLI entry points only work for `MoLeRVae` as they use `VaeWrapper`, while e.g. `molecule_generation sample` could also support `MoLeRGenerator`. While generalizing things, we may also want to rethink the choice to do model type discovery based on filenames (@sarahnlewis may have thoughts on this)."
"Hi, I am able to encode and decode a latent vector. How can we do the scaffold-constrained search?"
"Hi, What Cuda version are you using?"
"I am running the code of `MinDiscrepancyGreedy` adapted to run with pytorch and I am observing some strange behavior. In particular, the `potential` vector is always made of a single element and the algorithm always selects the point with index zero.  It could be that my adaptation introduced some bugs, but I am having hard time debugging because there are very few comments and information available in the code.    I believe this is an issue with the shapes of the matrices used in the lines before but I am currently not sure about it.  I looking into the computation of `deltas_d_plus_minus` (and lines below) because I would have expected this to be num_points x 1 but it's a 1x1 matrix.    It would be very useful if you could provide a few comments with the expected shapes of the different variables.    Below you can see the output of the debugger when running the algorithm on a toy dataset.  !       "
I see that the `MinDiscrepancyGreedy` class has been removed and the algorithm has been removed also from the new version of the paper. Is there any specific reason for this choice?  Should people avoid leveraging that code? was there a problem with the method?
Could you please clarify which license applies?  Thanks
"Hi,  I would like to verify that this algorithm    is exactly what in your paper is called ""Pseudo-Labels Discrepancy Minimization"" (Appendix F3).  Thanks."
"I'm attempting to reproduce the graphs shown in Figure1, namely the increasing tendency towards white noise as number of layers increase.    I've implemented this in PyTorch.         To plot the gradients I'm performing this activity.         With this code I'm able to produce the following two graphs.  !     However upon increasing the number of layers, I do not observe the white noise like plot. Am I doing things right? This is the plot I get when I run `plot_grads(24)`.  !   "
"Hi.    I was playing with this code when I saw this line in the benchmark comparing different mean estimation techniques:         Shouldn't this be:         to correspond to the $\Pi_2$ definition in Appendix B.1 in the paper?    I didn't see any substantial change in the plots after making this change, for what it's worth, but I thought maybe I should let you know anyway."
"Hi,  Really like this piece of research.  Planning on building on it with a master's project.  I understand it as you calculate the mean and covariance.  So essentially, could I then use these terms to build a more  Robust kalman filter in theory, or in it's current state is it already a state space   Estimator.  Many thanks,  Best,  Andrew"
"Numpy version is 1.19.5, run fail,  I upgrade Numpy version, 1.21.0,  I success!      Reference:  [1]   "
"!   **The main issue is that as following:**  E:\conda_Env\python.exe E:/Algorithm/pytorch_cuda/soft-dtw-master/examples/plot_interpolation.py build  soft_dtw_fast.c  C:\Users\hp\.pyxbld\temp.win-amd64-3.8\Release\pyrex\sdtw\soft_dtw_fast.c(612): fatal error C1083: Cannot open include file: 'numpy/arrayobject.h': No such file or directory    - so I dont know whats goin on with my running process, whether it's caused by my previous compiler processï¼ˆbut it seems fineï¼‰ or my Microsoft Visual Studio's MSVC has some issues , pls help me."
"Why do I get negative values from distance with some values for gamma?  For example, on a trajectory dataset, I had to choose gamma=1e-15 in order to get positive values. But this is nothing than dtw!    The dataset I used is available here:      In other datasets that I examined, gamma needs to be chosen carefully and small enough to works. Am I right?"
Could you please tell me how can I install and use soft-dtw on Google Colab?    Thanks
"I want to confirm that the issues I'm experiencing are a fundamental issue with the loss and not my implementation (which is a slight modification of this).    It seems to me that because the final loss is a sum of different paths, changing the (0, 0,) entry in the cost matrix will cause a much larger change in the loss than changing a later entry, as changing (0, 0,) influences every other entry in the cost matrix. Some simple test cases seem to confirm this. Can someone else confirm?"
"Hello. Just wanted to clarify.    Is the SoftDTWLoss function designed to accept arrays of time-series? Since during training and testing in Chainer, the loss is computed per batch (true and predicted), the input to the SoftDTWLoss is a batch of time-series labels and predictions. Does the SoftDTWLoss function consider the whole batch as 1 pair of time series or does it treat each pair of time-series (true and predicted) in a batch pair-wise?    Thank you."
"   will it work for  multivariate  time series classification for example mixture of categorical and continues data?    for example at time t1 we have observation: red, 2.4 , 5, 12.456 and time t2: green, 3.5, 2, 45.78; time t3: black, 5.6, 7, 23.56; t4: red, 2.1, 5, 12.6 ?"
"I have data with variable number of rows, which cannot be used with the current algorithm for numpy categorizes missing values as NAs. Can we modify the code, so that it can take variable length series."
"Hey,     for my masterthesis on I want to implement a tolerance proposed in this paper:     The aim is: ""To better represent a set of signals, we added to its modelling the tolerance that depicts  admissible values around the average signal. This tolerance is computed for each time  step of the average signal. It corresponds to the standard deviation of the sets of signals  aligned on each point of the average time-series. The tolerance of the CDBA at time  step k is thus computed as the standard deviation of the points aligned on this time step  k of the average time-series.""   The pseudo-code looks like this:    !         In order to produce a figure like this we need to add the standard deviation for the list of points which are regarded for computing each respective point of the centroid:        Can someone point me in the right direction where to start? Like in which function are the centroids  computed and which variables I need to mess with?    Thank you very much"
"When I run ""python setup.py build "" , it return the following statesment and I cannot import sdtw normally :  ""Using deprecated NumPy API, disable it by "" ""#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION"" [-Wcpp]   #warning ""Using deprecated NumPy API, disable it by "" \    ^  gcc -pthread -shared -B /home/gkb/anaconda3/envs/py3_7/compiler_compat -L/home/gkb/anaconda3/envs/py3_7/lib -Wl,-rpath=/home/gkb/anaconda3/envs/py3_7/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/sdtw/soft_dtw_fast.o -o build/lib.linux-x86_64-3.7/sdtw/soft_dtw_fast.cpython-37m-x86_64-linux-gnu.so  creating build/bdist.linux-x86_64  creating build/bdist.linux-x86_64/egg  creating build/bdist.linux-x86_64/egg/sdtw  copying build/lib.linux-x86_64-3.7/sdtw/__init__.py -> build/bdist.linux-x86_64/egg/sdtw  copying build/lib.linux-x86_64-3.7/sdtw/barycenter.py -> build/bdist.linux-x86_64/egg/sdtw  copying build/lib.linux-x86_64-3.7/sdtw/chainer_func.py -> build/bdist.linux-x86_64/egg/sdtw  copying build/lib.linux-x86_64-3.7/sdtw/dataset.py -> build/bdist.linux-x86_64/egg/sdtw  copying build/lib.linux-x86_64-3.7/sdtw/distance.py -> build/bdist.linux-x86_64/egg/sdtw  copying build/lib.linux-x86_64-3.7/sdtw/path.py -> build/bdist.linux-x86_64/egg/sdtw  copying build/lib.linux-x86_64-3.7/sdtw/setup.py -> build/bdist.linux-x86_64/egg/sdtw  copying build/lib.linux-x86_64-3.7/sdtw/soft_dtw.py -> build/bdist.linux-x86_64/egg/sdtw  copying build/lib.linux-x86_64-3.7/sdtw/soft_dtw_fast.cpython-37m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg/sdtw  byte-compiling build/bdist.linux-x86_64/egg/sdtw/__init__.py to __init__.cpython-37.pyc  byte-compiling build/bdist.linux-x86_64/egg/sdtw/barycenter.py to barycenter.cpython-37.pyc  byte-compiling build/bdist.linux-x86_64/egg/sdtw/chainer_func.py to chainer_func.cpython-37.pyc  byte-compiling build/bdist.linux-x86_64/egg/sdtw/dataset.py to dataset.cpython-37.pyc  byte-compiling build/bdist.linux-x86_64/egg/sdtw/distance.py to distance.cpython-37.pyc  byte-compiling build/bdist.linux-x86_64/egg/sdtw/path.py to path.cpython-37.pyc  byte-compiling build/bdist.linux-x86_64/egg/sdtw/setup.py to setup.cpython-37.pyc  byte-compiling build/bdist.linux-x86_64/egg/sdtw/soft_dtw.py to soft_dtw.cpython-37.pyc  creating stub loader for sdtw/soft_dtw_fast.cpython-37m-x86_64-linux-gnu.so  byte-compiling build/bdist.linux-x86_64/egg/sdtw/soft_dtw_fast.py to soft_dtw_fast.cpython-37.pyc  creating build/bdist.linux-x86_64/egg/EGG-INFO  copying soft_dtw.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO  copying soft_dtw.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO  copying soft_dtw.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO  copying soft_dtw.egg-info/not-zip-safe -> build/bdist.linux-x86_64/egg/EGG-INFO  copying soft_dtw.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO  writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt  creating dist  creating 'dist/soft_dtw-0.1.dev0-py3.7-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it  removing 'build/bdist.linux-x86_64/egg' (and everything under it)  Processing soft_dtw-0.1.dev0-py3.7-linux-x86_64.egg  creating /home/gkb/anaconda3/envs/py3_7/lib/python3.7/site-packages/soft_dtw-0.1.dev0-py3.7-linux-x86_64.egg  Extracting soft_dtw-0.1.dev0-py3.7-linux-x86_64.egg to /home/gkb/anaconda3/envs/py3_7/lib/python3.7/site-packages  Adding soft-dtw 0.1.dev0 to easy-install.pth file    Installed /home/gkb/anaconda3/envs/py3_7/lib/python3.7/site-packages/soft_dtw-0.1.dev0-py3.7-linux-x86_64.egg  Processing dependencies for soft-dtw==0.1.dev0  Finished processing dependencies for soft-dtw==0.1.dev0  Warning: Assuming default configuration (sdtw/tests/{setup_tests,setup}.py was not found)(py3_7) [gkb@localhost soft-dtw]$ python setup.build  python: can't open file 'setup.build': [Errno 2] No such file or directory  (py3_7) [gkb@localhost soft-dtw]$ python setup.py build  Appending sdtw.tests configuration to sdtw  Ignoring attempt to set 'name' (from 'sdtw' to 'sdtw.tests')  Appending sdtw configuration to   Ignoring attempt to set 'name' (from '' to 'sdtw')  running build  running config_cc  unifing config_cc, config, build_clib, build_ext, build commands --compiler options  running config_fc  unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options  running build_src  build_src  building extension ""sdtw.soft_dtw_fast"" sources  build_src: building npy-pkg config files  running build_py  running build_ext  customize UnixCCompiler  customize UnixCCompiler using build_ext  Warning: Assuming default configuration (sdtw/tests/{setup_tests,setup}.py was not found)"
"Hello,    I wanted to build up some intuition about how the soft-dtw cost function works by playing around with some fake data. I started comparing scaled and shifted cosines and wanted to see what numbers soft-dtw would spit out. However, when I compared a cosine to itself I couldn't understand why I was getting a negative result. Wouldn't the cumulative alignment cost for signal to itself be zero? Could someone please help me understand why the result is negative?    Also, when I compared similarly shifted cosines, but one was scaled by 0.5 and the other scaled by 2.0, the discrepancies were very different from each other. Why does scaling have such a strong effect on the scoring? Should I be self-normalizing my data before using soft-dtw?    I tried the following:         I got the following output:    "
"Soft-dtw looks like the perfect solution for my deep-learning model. However the speed is a major bottleneck in training (minibatches of 64 samples, w. 2000 positions x 25 classes).    Would it be possible to add a parameter for greedy scoring which would scale better in time?     For example, I never need alignments with more than a few insertions/deletions. Perhaps this can be achieved by controlling the maximum recursion depth?  "
Hi !     Did you start working on the pytorch feature? Or should I start looking into it?  Thanks for sharing the code!  
To avoid conversion to float64.
"I was re-reading your paper recently a bit more carefully, and I have a question about the dimension of some matrices.    Specifically, in Eq 2.5, it's not really clear what the dimension of the jacobian matrix `d(delta(x, y)) / dx` is expected to be. Below, I've depicted the dimensions of all the matrices involved in the computation per the paper's notations. The right side of the equation yields `p x n` (same shape as `X`). But I don't understand how the dimensions are working out for the left side of the equation (the green doodles).    <img width=""682"" alt=""Equation"" src=""     Another question: my understanding is that in Alg. 2, the dimensionality of `E` is `n x m`, and per the above, the output of Alg. 2 should be `p x n` (same as `X`). Am I correct?    Thanks in advance!"
"I've installed the project with following steps:     I can run `example/plot_barycenter.py` successfully, but in `plot_chainer_MLP.py`, the module `chainer` is not found."
"Dear Matthew,    I enjoyed reading your article as well as reviewing your very clear code. Thanks for posting this on Github. Have you considered adding initial and terminal constraints? At the moment, the algorithm does not enforce boundary conditions such that domain of the warping path is ([0,0], [n,m]); instead the domain is ([i,j], [n,m]) for 0 <= i < n and 0<= j < m. Banding was proposed, which does enforce this constraint, but at the expense of a larger search space.    Cheers,  Dave"
"Hi,  I've been looking at this for quite some time now, but is there a way to extend this loss function with mini batches in neural networks? I came across the Pytorch project as well -   - but can't understand how to use it there as well. Here's a small issue that I came across, and would really appreciate any inputs to accomplish this:  For usage in PyTorch, I found the method `dtw_value or dtw_grad` here :   - but seems like it needs a `theta` array - is it supposed to be the pairwise distance matrix between time series in the minibatch, or what is it exactly?    If it's the former I think that `dtaidistance` package could achieve it : `distance_matrix_fast` method  . But it gives a (n,n) square ndarray unlike what's mentioned in the docs of the method as mentioned above -      If it's anything else could you suggest how to get the distance matrix? Any suggestions would be really helpful. Thank you! ðŸ˜„ @mblondel "
"Hi,    Is it possible for the author to convert python code into a MATLAB version?"
"Hi,    The mentioned step throws the following error,    Warning: Assuming default configuration (sdtw/tests/{setup_tests,setup}.py was not found)Appending sdtw.tests configuration to sdtw  Ignoring attempt to set 'name' (from 'sdtw' to 'sdtw.tests')  Appending sdtw configuration to   Ignoring attempt to set 'name' (from '' to 'sdtw')  running build  running config_cc  unifing config_cc, config, build_clib, build_ext, build commands --compiler options  running config_fc  unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options  running build_src  build_src  building extension ""sdtw.soft_dtw_fast"" sources  build_src: building npy-pkg config files  running build_py  creating build  creating build/lib.linux-x86_64-2.7  creating build/lib.linux-x86_64-2.7/sdtw  copying sdtw/path.py -> build/lib.linux-x86_64-2.7/sdtw  copying sdtw/chainer_func.py -> build/lib.linux-x86_64-2.7/sdtw  copying sdtw/soft_dtw.py -> build/lib.linux-x86_64-2.7/sdtw  copying sdtw/distance.py -> build/lib.linux-x86_64-2.7/sdtw  copying sdtw/__init__.py -> build/lib.linux-x86_64-2.7/sdtw  copying sdtw/setup.py -> build/lib.linux-x86_64-2.7/sdtw  copying sdtw/barycenter.py -> build/lib.linux-x86_64-2.7/sdtw  copying sdtw/dataset.py -> build/lib.linux-x86_64-2.7/sdtw  running build_ext  customize UnixCCompiler  customize UnixCCompiler using build_ext  building 'sdtw.soft_dtw_fast' extension  compiling C sources  C compiler: x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fdebug-prefix-map=/build/python2.7-nbjU53/python2.7-2.7.15~rc1=. -fstack-protector-strong -Wformat -Werror=format-security -fPIC    creating build/temp.linux-x86_64-2.7/sdtw  compile options: '-I/home/xxx/.local/lib/python2.7/site-packages/numpy/core/include -I/home/xxx/.local/lib/python2.7/site-packages/numpy/core/include -I/usr/include/python2.7 -c'  x86_64-linux-gnu-gcc: sdtw/soft_dtw_fast.c  sdtw/soft_dtw_fast.c:4:10: fatal error: Python.h: No such file or directory   #include ""Python.h""            ^~~~~~~~~~  compilation terminated.  sdtw/soft_dtw_fast.c:4:10: fatal error: Python.h: No such file or directory   #include ""Python.h""            ^~~~~~~~~~  compilation terminated.      Can you please help me resolve it?    Thanks,  Hari."
how can I implement the DP recursion in Tensorflow? Tensor does not support assign op.
"Traceback (most recent call last):    File ""setup.py"", line 5, in        from numpy.distutils.core import setup  ImportError: No module named numpy.distutils.core    When I follow your step, but why I have the Error?"
"I define te function as following:  def test_loss(y_pred, y_true):      return tsm_sdtw(y_pred, y_true)    and compile it with keras model ==>  model.compile(optimizer='Adam', loss=test_loss)  However, the following error are shown:    File ""D:\python35\lib\site-packages\keras\engine\training.py"", line 910, in compile      sample_weight, mask)    File ""D:\python35\lib\site-packages\keras\engine\training.py"", line 436, in weighted      score_array = fn(y_true, y_pred)    File ""D:/Q/cdads/py_code/turn_upsample.py"", line 17, in test_loss      return tsm_sdtw(y_pred, y_true)    File ""D:\python35\lib\site-packages\tslearn\metrics.py"", line 597, in soft_dtw      return SoftDTW(SquaredEuclidean(ts1[:ts_size(ts1)], ts2[:ts_size(ts2)]), gamma=gamma).compute()    File ""D:\python35\lib\site-packages\tslearn\utils.py"", line 320, in ts_size      ts_ = to_time_series(ts)    File ""D:\python35\lib\site-packages\tslearn\utils.py"", line 84, in to_time_series      ts_out = ts_out.astype(numpy.float)  ValueError: setting an array element with a sequence.  @mblondel   Would you please give some toy examples for this."
"Hello, I'm testing this python version of soft-DTW and I am seeing the following:         It seems counter intuitive to obtain negative values for a discrepancy, but maybe it is normal in this case? I would like to corroborate."
I've installed the program as mentioned:         I have also added the soft-dtw folder to PYTHONPATH    When I open a new python session and try to do `from sdtw import SoftDTW` I get the following error:         Could it be that I'm missing a certain step during the installation?
"the method ""file"" in soft-dtw/sdtw/dataset.py cannot be found while executing or trying to test the code.    Error occurs in   method line 13 in     "
"When I used the optnet to  run all the three experiments, I got the following error:    `  RuntimeError: invalid argument 3: dimensions of A and b must be equal at /pytorch/torch/lib/TH/generic/THTensorLapack.c:1036  > /usr/local/lib/python3.5/dist-packages/qpth/solvers/pdipm/batch.py(357)solve_kkt()      355         h = (invQ_rx.unsqueeze(1).bmm(G.transpose(1, 2)) + rs / d - rz).squeeze(1)      356   --> 357     w = -(h.btrisolve(*S_LU))      358       359     g1 = -rx - w[:, neq:].unsqueeze(1).bmm(G)  `    Did I miss something or do something wrong with the qpth or other environment?  I can run the fc and lenet network in the classification experiment.  Since I  don't have a CUDA, I uncomment all the â€œ.cuda()â€ in the models.py "
"Hi,    I was running the optnet code for MNIST classification with the default configurations for only 10 epochs. In the first couple of epochs I get the warning ""qpth warning: Returning an inaccurate and potentially incorrect solutino"" and in the subsequent iterations the loss becomes nan. Is there something obviously wrong with my configurations?"
"Hi @bamos ,    Thank you for this promising work and clean code if you don't mind.    I have some questions related to the solver.    Is it possible to :        1) Add extra terms in the objective function and variable constraints ?       A) extra terms in the objective function    `\hat z =   argmin_z 1/2 z^T Q z + p^T z+......+....+......`      B) Add more variables  constraints :  for instance add  `y_, F_, K_, n_`     to      `Q_, p_, G_, h_, A_, b_`        2) Is this constraint solver can deal with maximum solution rather than minimum ?    `\hat z =   argmax_z 1/2 z^T Q z + p^T z+......+....+......`      Thank you a lot for your answer "
"Hi there, thx for sharing this wonderful works.  I am trying to run the code, and encounter some situations:  1. when I trying to run test and profile code, I fail to import adact.  But I have no idea where the lib is?  2. when running denoising example, I use the command as  `python main.py --nEpoch 50 optnet --learnD --Dpenalty 0.01 `  and I got the following error message:  `('===>', 'Building model')  Testing model: 0/1000  Traceback (most recent call last):    File ""main.py"", line 229, in        main()    File ""main.py"", line 136, in main      test(args, 0, model, testF, testW, testX, testY)    File ""main.py"", line 202, in test      output = model(batch_data)    File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 206, in __call__      result = self.forward(*input, **kwargs)    File ""/home/jdily/Desktop/project/optnet-master/denoising/models.py"", line 155, in forward      x = QPFunction()(p.double(), Q.double(), G.double(), h.double(), e, e).float()    File ""/usr/local/lib/python2.7/dist-packages/qpth/qp.py"", line 65, in forward      p, _ = expandParam(p_, nBatch, 2)    File ""/usr/local/lib/python2.7/dist-packages/qpth/util.py"", line 50, in expandParam      raise RuntimeError(""Unexpected number of dimensions."")  RuntimeError: Unexpected number of dimensions.  `  I use python 2.7 instead of 3.x, but I do update some part of the code to support python 2.7...  Do you know how can I solve these issues?    Meanwhile, the link to the denoising data seems broken.    thanks a lot!"
"> /usr/local/lib/python3.6/site-packages/IPython/core/ultratb.py(1268)__call__()  -> self.handler((etype, evalue, etb))  (Pdb)   ---------------------------------------------------------------------------  AssertionError                            Traceback (most recent call last)  /Users/gaopeng/Desktop/tensorflow/project/optnet/denoising/main.py in  ()      229       230 if __name__=='__main__':  --> 231     main()          global main =      /Users/gaopeng/Desktop/tensorflow/project/optnet/denoising/main.py in main()      109     elif args.model == 'optnet':      110         if args.learnD:  --> 111             model = models.OptNet_LearnD(nFeatures, args)          model = undefined          global models.OptNet_LearnD =            nFeatures = 100          args = Namespace(Dpenalty=0.01, batchSz=150, cuda=False, eps=0.0001, learnD=True, model='optnet', nEpoch=50, no_cuda=False, save='work/optnet.eps=0.0001.learnD.0.01', testBatchSz=100, testPct=0.1, tvInit=False)      112         else:      113             model = models.OptNet(nFeatures, args)    /Users/gaopeng/Desktop/tensorflow/project/optnet/denoising/models.py in __init__(self=OptNet_LearnD (  ), nFeatures=100, args=Namespace(Dpenalty=0.01, batchSz=150, cuda=False....01', testBatchSz=100, testPct=0.1, tvInit=False))      113       114         # self.fc1 = nn.Linear(nFeatures, nHidden)  --> 115         self.M = Variable(torch.tril(torch.ones(nHidden, nHidden)).cuda())          self.M = undefined          global Variable =            global torch.tril =            global torch.ones =            nHidden = 199          nHidden.cuda = undefined      116       117         Q = 1e-8*torch.eye(nHidden)    /usr/local/lib/python3.6/site-packages/torch/_utils.py in _cuda(self=      1     0     0  ...      0     0     0      1...     1     1  [torch.FloatTensor of size 199x199]  , device=-1, async=False)       63         else:       64             new_type = getattr(torch.cuda, self.__class__.__name__)  ---> 65             return new_type(self.size()).copy_(self, async)          new_type =            self.size.copy_ = undefined          self =       1     0     0  ...      0     0     0      1     1     0  ...      0     0     0      1     1     1  ...      0     0     0         ...          â‹±          ...             1     1     1  ...      1     0     0      1     1     1  ...      1     1     0      1     1     1  ...      1     1     1  [torch.FloatTensor of size 199x199]            async = False       66        67     /usr/local/lib/python3.6/site-packages/torch/cuda/__init__.py in __new__(cls= , *args=(torch.Size([199, 199]),), **kwargs={})      275       276     def __new__(cls, *args, **kwargs):  --> 277         _lazy_init()          global _lazy_init =        278         # We need this method only for lazy init, so we can remove it      279         del _CudaBase.__new__    /usr/local/lib/python3.6/site-packages/torch/cuda/__init__.py in _lazy_init()       87         raise RuntimeError(       88             ""Cannot re-initialize CUDA in forked subprocess. "" + msg)  ---> 89     _check_driver()          global _check_driver =         90     assert torch._C._cuda_init()       91     assert torch._C._cuda_sparse_init()    /usr/local/lib/python3.6/site-packages/torch/cuda/__init__.py in _check_driver()       54 def _check_driver():       55     if not hasattr(torch._C, '_cuda_isDriverSufficient'):  ---> 56         raise AssertionError(""Torch not compiled with CUDA enabled"")          global AssertionError = undefined       57     if not torch._C._cuda_isDriverSufficient():       58         if torch._C._cuda_getDriverVersion() == 0:    AssertionError: Torch not compiled with CUDA enabled  > /usr/local/lib/python3.6/site-packages/IPython/core/ultratb.py(1269)__call__()  -> try:  "
I tried running the sudoku example. My command line arguments are as follows:         I get errors arising from `models.py`:          Did I miss something?
"Hello,  I am using matlab2015 and windows 10. While I tried to run the following command in command window, I am getting some errors:   mex chol2invchol.c -lgsl -lblas  Error:  Error using mex  MEX cannot find library 'gsl' specified with the -l option.   MEX looks for a file with one of the names:   libgsl.lib   gsl.lib   Please specify the path to this library with the -L option.  Can you guide me what to do?"
"Hi Zi Wang,    I found that there is no 'decompose kernel' when I am running example_batch_addgp.m.  I could run example_addgp.m    And I could not find where is the 'decompose kernel' function."
"Dear Dr. Wang,  I found your paper about MES really interesting. I have a question about it: since I'm working on discrete and scattered search spaces with ""real"" black box functions, is your code suitable for this as it is or do you suggest to adapt it? In particular, I refer to all the optimization scripts: can I avoid all the gradient-based searches with **fmincon**, just computing _effectively_ the acquisition function in all the domain points?   Thank you in advance."
"Hello,    is it possible, that you can upload a mex file for windows for chol2invchol.c.  I tried several ways to mex it by myself, but I was not successful.    Thanks in advance.    Roman"
"In the section 5.2, do you ask for the maximum or minimum of the three function(eggholder, shekel, michalewicz)?"
"     What's the use for this line? `D` itself is already the eigendecomposition of `Z^t Z + \sigma^2 I`, it looks to me that `R=1./D`. Can you please clarify this ? "
"Just wondering if the code can be used with open source products?  Best,  Andrew"
"Hi Luca,  Were you able to find a solution to display pdfs using the Iframe class in the display module of IPython?   I am trying out the same, but the pdf file is not loaded if a file path is given as value for the **src** parameter of Iframe. It only works in case of urls as the value for the parameter.    PS: Also there is a function called display_pdf which can be used to display pdf from a file, although in that case it seems like in the case of customization a pdf object will have to be formed.   Please let me know if you were able to find a solution.  "
"Hi,  for an university project I'm using your code and trying to recreate the experiment in the file mint_deep_ho.py. I've changed the optimizer to Gradient Descent and now want to add very small numbers to w during gradient descent. Unfortunately, I always get back the error: ValueError: None values not supported. I've checked and saw that it is a Merged Variable with functions to access it but I have been so far unable to do so.   How can I access the weights w to add a very small number before the function grad = tf.gradients(loss, #MergedVariable.get_tensor(w), w)[0] is called?    Thanks in advance!"
"I am trying to integrate meProp into my work, but getting such error. Do you have any idea about this?     "
"Hi lancopku,    I'm currently implementing your meProp code to understand the flow of the architecture in detail.    However, I couln't see the improved acceleration speed of meprop compared to that of conventional   MLP.     In the table 7 and 8 of paper  , pytorch based GPU computation can achieve more faster back-propagation procedure.     Could you please let me know how to implement meprop to show faster backprop computation?    Best,  Seul-Ki"
"Have you tried on deeper models?    Since each step of backprops, gradients are removed with specific portions(like 5%), Will not the gradient vanish in a deeper neural network model?    Any thoughts?"
"!   now i use my dataset train the mcl model,but as you can see that why top-1 error is so high,and how to use tensorboard to see the Training error process  @chhwang "
"now ,i want to use your method for the (UC Merced Land Use Dataset) dataset. How to convert my data set into your data formï¼Ÿ @chhwang "
"Hi,   I find that the dataset links are invalid now. Please check it. Thanks."
"I'm trying to run an experiment with `python3 train-adaptive-nets hybrid-cr-tree-dynkcpt`, and there's error which doesn't occur in `train-nets` experiments.    I'm using tensorflow-gpu==1.12, numpy==1.15.4, matplotlib==3.0.3, seaborn==0.9.0, scipy==1.2.1    The error log goes as follows:       Looking forward to your rely."
None
"In the equation in the paper, there is no entropy term in the SIL policy loss, how come in the code there is one?    self.loss = self.pg_loss - entropy * self.w_entropy"
"Hello.    I firstly change the policy in   by:    `parser.add_argument('--policy', help='Policy architecture', choices=['cnn', 'lstm', 'lnlstm'], default='lstm')`    Then I run A2C+SIL on Atari games  :    `python baselines/a2c/run_atari_sil.py --env BreakoutNoFrameskip-v4`    I got error:         What can I do to fix this? Thank you very much!"
"I do not see a way to replicate grid world experiment from the paper using code that is available in the repository. Is there a way and if not, could you please publish the code?  "
"Thanks for this paper.    In the third part  (the last line on the right of the second page), you say that     > $\pi_{\theta}, V_{\theta}(s)$ are the policy (i.e. actor) and the value function parameterized by $\theta$ .    !     I want to know how the policy and the value function use the same parameters $\theta$.    Looking forward to your answers. Thanks in advance."
"In the paper, sil value loss is defined as 0.5 * max(0, (R-V))^2. Howerver in the code, the value loss is defined as below  `self.vf_loss = tf.reduce_sum(self.W * v_estimate * tf.stop_gradient(delta)) / self.num_samples`  which means that the value loss is 0.5 * V * clip((V-R), -5, 0).  What's the advantage of this implementation. Thanks  "
"Is there a reason that SIL requires using the `np.sign(reward)` to do all of the training, rather than the raw rewards themselves?  "
"gumbel_hard = tf.cast(tf.equal(gumbel_softmax, tf.reduce_max(gumbel_softmax, 1, keep_dims=True)), tf.float32)  mask = tf.stop_gradient(gumbel_hard - gumbel_softmax) + gumbel_softmax    For the above code, it seems to choose the maximal index marked with ""1"" and all other indexes set to 0?, it means you only allow 1 element go through the gradient descent, which is too strict...?"
"Hello, I tried to reproduce the results of T-net on 5w1s miniimagenet experiment.  I set parameters as   --metatrain_iterations=60000     --meta_batch_size=4 --update_batch_size=1     --num_updates=5 --logdir=logs/miniimagenet5way     --update_lr=.01 --meta_lr=0.001 --resume=True --num_filters=32 --max_pool=True     --use_T=True  But I can only get accuraty about 48.8%, which is not as good as 50.86% writen ny you paper.   Maybe you can tell me your parameters setting if you still have it.  Thank you."
"Dear Authors,    In the code:    sampled_character_folders = random.sample(folders, self.num_classes)  random.shuffle(sampled_character_folders)                          labels_and_images = get_images(sampled_character_folders, range(self.num_classes), nb_samples=self.num_samples_per_class, shuffle=False)    One of random labels (1 to 5 class) are assigned to each folder of random selected 5 folders. It seems that there are label conflict between two batches, for example:  batch1:  label 1: folder 11, label 2: folder 12, label 3: folder 13, label 4: folder 14;  batch2:  label 1: folder 12, label 2: folder 13, label 3: folder 14, label 4: folder 15;  In this case, in two different batch1, and batch 2, folder 12 is assgined to label 1 and label 2.  In my understanding, each folder with images belong to one object should be assigned a unique label."
"It's annoying but print needs parenthesis in Python 3; you have a few without the parenthesis.     In MAML.py, you have        for k, v in weights.iteritems():    You should update it to        for k, v in weights.items():    That's all. The code runs, good work.  "
"Hello,    I'm trying to reproduce the result reported in your paper.  However, by running the default script for omniglot 20way 1shot,  `  python main.py      --datasource=omniglot --metatrain_iterations=40000      --meta_batch_size=16 --update_batch_size=1      --num_classes=20 --num_updates=1 --logdir=logs/omniglot20way      --update_lr=.1 --use_T=True --use_M=True --share_M=True  `  gives 90.X accuracy on test time.  The result was not different even if I changed `num_updates` to 5 on train time.    Could you give suggestion for reproducing the performance?"
"Hi, yoohholee,        MT-net is a fantastic idea.         I ran this code but only got 50.04% for miniImagenet 1-shot classification.          Do you have any suggestions or recommendations to reproduce the results in the paper?        Such as data preprocessing, validation, tensorflow and python version or maybe other tricks?           Thanks so much!    "
"Thanks for the great paper and very clear code!    I'm a mathematics student without too much prior coding experience trying to run the notebooks. Figures 1-4 are easy to replicate, but when I run the fifth figure I get the following error:    AttributeError: module 'torch.nn' has no attribute 'ModuleDict'    I have followed the instructions for creating the environment. What is causing this?  "
"Hi, when i run the code, there is an error.   the posion of error is :  def get_orthogonal_weight(name, shape, gain=1.):      # Can also use tf.contrib.framework.convolutional_orthogonal_2d      return tf.get_variable(name, shape=shape,                             initializer=tf.contrib.framework.convolutional_delta_orthogonal(gain=gain))    the information of error:  initializer=tf.contrib.framework.convolutional_delta_orthogonal(gain=gain))  AttributeError: module 'tensorflow.contrib.framework' has no attribute 'convolutional_delta_orthogonal'"
When running the experiments IMDB-word I tend to get significantly lower post-hoc accuracy than the one listed in Table 4 in the paper (0.908). Could you tell me if I might have to change some hyperparameters etc.?    Thanks!
"For the IMDB-sent experiments, the test data (data/testData.tsv) seems to be missing. Is it possible to get the file?    Thanks!"
"Hello and congratulations for your great work.  I tried to implement the L2X explanations with hierarchical LSTM on a fake news dataset, as shown in the imdb_sent example. However, the explainer always selects the first sentence as an explanation. Do you have any idea why is this happening?   I would also like to ask you (sorry if it is a stupid question), as the method is model agnostic, why do you use different neural network architectures for each imdb example?"
Any plans to finish the code for reproducing the MNIST results? Thanks!
Can it explain 3D convolutional NN for 3D images classification? Thanks.
"It seems that this question is not relevant to the codes, but may I ask why the lower bound of the conditional expectation of log Pm(Y | x) is the conditional expectation of log Qs(Y | x)? I think applying the Jensenâ€™s inequality yields the lower bound to a function of Pm. I can't find any relationship between Pm and Qs. "
"Hi, it is really a nice work, and the task proposed in your paper is a valuable research issue.  But I have a question about MaskNet:  according to chapter 4.1 and fig3.B of your paper, the proposals are generated from one-hot vectors and target encoder outputs, and scene encoder outputs are not used to generate proposals. However, in you codes, the function mask_net(codes are showed below) generates proposals from one-hot vectors and the output of function matching_filter which takes both targets_encoded and images_encoded as inputs. Is there any thing wrong?  ``     `        def mask_net(targets, images, labels=None, feature_maps=24, training=False, threshold=0.3, rgb_mean=127, rgb_std=127):          # encode target          targets_encoded, _ = encoder(targets,                                       feature_maps=feature_maps,                                       dilated=False, reuse=False,                                       scope='clean_encoder')                images_encoded, images_encoded_end_points = encoder(images,                                                              feature_maps=feature_maps,                                                              dilated=True, reuse=False,                                                              scope='clutter_encoder')                # calculate crosscorrelation          # target_encoded has to be [batch, 1, 1, fmaps] for this to work          matched = matching_filter(images_encoded, targets_encoded, mode='standard')          matched = matched * targets_encoded                # Get size of matching          isz = images.get_shape().as_list()          ix = isz[1]          iy = isz[2]          tsz = targets.get_shape().as_list()          tx = tsz[1]          ty = tsz[2]          msz = matched.get_shape().as_list()          batch_size = msz[0]          mx = msz[1]          my = msz[2]                # Generate proposals          # There are 3 modes:          # Training the encoder and decoder          # Training the discriminator          # Evaluation          if training != False:                    # Get center of mass of labels to determine fg proposals              comx, comy = center_of_mass(labels)              comx = comx / (ix / mx)              comy = comy / (iy / my)                    # Initialize fg & bgindices              fg_index = [0 for x in range(4)]              # select the 4 locations around the label com as fg proposals              fg_index[0] = tf.cast(tf.floor(comx) + my * tf.floor(comy), tf.int32)              fg_index[1] = tf.cast(tf.ceil(comx) + my * tf.floor(comy), tf.int32)              fg_index[2] = tf.cast(tf.floor(comx) + my * tf.ceil(comy), tf.int32)              fg_index[3] = tf.cast(tf.ceil(comx) + my * tf.ceil(comy), tf.int32)              # Draw random indices for bg proposals              bg_index = [tf.expand_dims(tf.random_shuffle(tf.range(0, mx * my)), axis=1) for x in range(batch_size)]              bg_index = tf.concat(bg_index, axis=1)                    if training == 'encoder_decoder':                  # Generate 4 foreground and 4 background proposals                  num_proposals = 8                  proposal_range = range(num_proposals)                        # create index                  index = [0 for x in proposal_range]                  for i in range(4):                      index[i] = fg_index[i]                  # Select 4 random locations for bg proposals                  for i in range(4, num_proposals):                      index[i] = bg_index[i, :]                      if any([index[i] == index[j] for j in range(4)]):                          index[i] = bg_index[i + (num_proposals - 4), :]                    elif training == 'discriminator':                  # Generate x foreground and y background proposals                  num_proposals = 4                  proposal_range = range(num_proposals)                        fg_index = tf.stack(fg_index, axis=0)                  random_fg_index = tf.random_shuffle(tf.range(0, 4))                        index = [0 for x in proposal_range]                  index[0] = fg_index[random_fg_index[0], :]                  for l in range(num_proposals - 1):                      index[l + 1] = bg_index[l, :]                        tensor_index = tf.expand_dims(tf.stack(index, axis=0), axis=-1)                  tensor_labels = tf.expand_dims(tf.stack([1, 0, 0, 0], axis=0), axis=1)                  tensor_labels = tf.expand_dims(tf.tile(tensor_labels, [1, batch_size]), axis=-1)                        shuffled_index_and_labels = tf.random_shuffle(tf.concat([tensor_index, tensor_labels], axis=2))                        index = shuffled_index_and_labels[..., 0]                  labels = tf.cast(tf.transpose(shuffled_index_and_labels[..., 1]), tf.float32)                else:              proposal_range = range(mx * my)              index = [tf.ones(batch_size, dtype=tf.int32) * q for q in proposal_range]                # Run Decoder with proposals          proposed_segmentations = [0 for x in proposal_range]          scores = [0 for x in proposal_range]          for q in proposal_range:                    # To share weights between proposals they have to be inititalized              # for the first proposal and reused afterwards              if q == 0:                  reuse = False              else:                  reuse = True                    # Generate the one-hot proposal              one_hot_proposal = tf.one_hot(index[q], mx * my)              mask = tf.reshape(one_hot_proposal, [batch_size, mx, my, 1])              # Apply the proposal              masked = matched * mask              ......    `"
"In your UCI experiments, you set your objective function to a log loss (and likewise the BO objective). Surely one wants to perform VI with VADAM and should optimize an ELBO?"
"<img width=""688"" alt=""screen shot 2019-02-22 at 3 45 02 pm"" src=""   <img width=""688"" alt=""screen shot 2019-02-22 at 3 45 02 pm"" src=""   "
     See also:        Make sure that the same mini-batch should be used here.  One way to achieve that is to use `state['step']` as the seed of the random number generator for generating mini-batches. I did that in my Autograd implementation 
"This should work for any model, right? Not just for RL? I'm a bit confused for why the ActorNetworkEpsilon in the Tensorflow implementation contains code to optimise the policy network.     I'm trying to use it for StyleGAN-based image reconstruction, i.e. find a distribution over low-dimensional latents z such that G(z) match a given input image. The network is already trained and fixed, it's just the latent that I want to optimise.    Also, why is the design such that the critic network is also exposed to the outside DDPG.py? That is also something RL-specific, right?      Thanks,  Raz"
"According to Yarin Gal's repo  , there was test set contamination due to the bayesian optmisation of hypers:  > Update (2018) We replaced the Bayesian optimisation implementation (which was used to find hypers) with a grid-search over the hypers. This is following feedback from @capybaralet who spotted test-set contamination (some train-set points, used to tune hypers which were shared across all splits, were used as test-set points in later splits). The new implementation iterates over the 20 splits, and for each train-test split it creates a new train-val split to tune hypers. These hypers are discarded between different train-test splits.    Yet, it seems the UCI regression experiments here use the same splits as in the   repo. Was the bug fixed or is it still there and the reason for doing so here was the reviewers' request?"
"In results_display.py, all_results = pd.read_pickle(WDIR + '/results/cluster_results.pkl'). But there's no cluster_results.pkl in the data file. "
"Hello,    Thanks for making your code public on Github. I was wondering if there was something wrong with `param_dict` you defined in `LSTM2.py`:         When I run your code, I got following mistakes:         When I set `target_cols=['default']`, it worked and produced result similar to that you proposed in the paper. Could you please tell me what do you mean by setting `target_cols=[1]`?"
"Hello,    Your paper was an interesting read and thank you for making available the code. After following example.ipynb, I was wondering how I can print/visualize the weightings of the different 'candidate predictors'. It should be the case that B and D have high weightings for target variable E since E is a sum of last 10 values of B multiplied by D.   Which variable in SOCNN.py is the weighting frame? And do you have any suggestions what the best way is to print this?     Thank you in advance"
"I am running     TF = 1.6 , Tensorboard = 1.5.0  and Keras 2.1.5 but i am getting this error  below:       Developer@DEEPGPU:~/Downloads/nntimeseries-master/nnts/models# python ./CNN.py --dataset=artificial  ....  ...  ...  ..  ...  Total model parameters: 3451  WARNING:tensorflow:From /home/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.  Instructions for updating:  Use the retry module or similar alternatives.  Epoch 1/1000  ---current learning rate: 0.00100000  124/124 [==============================] - 13s 103ms/step - loss: 1.2770 - val_loss: 2.7545    ---current best val_loss: 2.75454  960/960 [==============================] - 0s 113us/step    --- test_loss: 2.290810   Traceback (most recent call last):    File ""./CNN.py"", line 109, in        runner.run(CNNmodel, log=log, limit=1)    File ""/root/Downloads/nntimeseries-master/nnts/utils.py"", line 130, in run      model_results, nn = model.run()    File ""/root/Downloads/nntimeseries-master/nnts/utils.py"", line 287, in run      verbose=self.verbose    File ""/home/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""/home/anaconda3/lib/python3.6/site-packages/keras/engine/training.py"", line 2262, in fit_generator      callbacks.on_epoch_end(epoch, epoch_logs)    File ""/home/anaconda3/lib/python3.6/site-packages/keras/callbacks.py"", line 77, in on_epoch_end      callback.on_epoch_end(epoch, logs)    File ""/home/anaconda3/lib/python3.6/site-packages/keras/callbacks.py"", line 820, in on_epoch_end      raise ValueError('If printing histograms, validation_data must be '  ValueError: If printing histograms, validation_data must be provided, and cannot be a generator.  "
"Is there a GPU version of the code. i have tried running it my Intel Xeon server with 4 x Titan X Pascal GPUs , unfortunately it only uses CPU .  Can it be run on Amazon AWS  or on GPUs ,    "
"There was a typo also on line 159 of `SOCNN.py` when trying the `shared_final_weights` option    `out = TimeDistributed(Dense(output_length ... `    that I changed to    `out = TimeDistributed(Dense(self.output_length ... `    And got the training started with:         But then the following error happend then:         I guess I have to explicitly set shape for the `sig` of `sig = TimeDistributed(Activation('softmax'), name='softmax')(sig)` ?"
None
"Thanks for sharing the code @mbinkowski of your interesting paper. However I had some issues trying to make the model run for the household data    For example, I got the following:       And on line 110 of `utils.py`, there is an attempt to read some results from pickled file that I don't have as I have never run the model yet successfully?    I replaced that with empty dictionary: `self.cresults = {}`, and had to add `WDIR` manually to some files, and then hit that when putting `limit` higher than 1:       Any idea of the errors?"
"hi,i am trying to reproduce the battle game (red army use MFQ and blue army use DQN.) i trained the model  and find both algorithums has a negative reward.and the agents number of two side has not decreased in the battle which is confused.can u help me?    !     "
"!     é—®é¢˜çš„åˆè¡·ï¼š  æˆ‘æƒ³è¦æŠŠç”¨MFACè®­ç»ƒå¥½çš„æ¨¡åž‹å’Œç”¨ACè®­ç»ƒå¥½çš„æ¨¡åž‹è¿›è¡Œå¯¹æ¯”ï¼Œè®©è¿™ä¸¤ä¸ªç®—æ³•è¿›è¡Œäº¤å‰æˆ˜æ–—å®žéªŒã€‚åœ¨ç»ˆç«¯è¿è¡Œå¦‚ä¸‹ï¼špython battle.py --algo mfac --oppo ac --idx 10 10    ä»ç„¶æŠ›å‡ºå¼‚å¸¸ï¼š  ValueError: The passed save_path is not a valid checkpoint: /home/lzh/postgraduate project/mfrl-master/data/models/mfac-0/mfac_1    ç–‘æƒ‘çš„ç‚¹ï¼š  1ï¼‰å¦‚ä½•è¿è¡Œbattle.py  2)  å‚æ•°--idx å¦‚ä½•è®¾ç½®parser.add_argument('--idx', nargs='*', required=True)   !   "
"Efficient Ridesharing Order Dispatching with  Mean Field Multi-Agent Reinforcement Learning    æ‚¨å¥½ï¼Œè¿™ç¯‡è®ºæ–‡çš„ä»£ç ï¼Œæ–¹ä¾¿å‘æˆ‘ä¸€ä»½å—ï¼Ÿ  2647790445@qq.com    Hello, can you send me a copy of the code for this paper?"
"Efficient Ridesharing Order Dispatching with  Mean Field Multi-Agent Reinforcement Learning    æ‚¨å¥½ï¼Œè¿™ç¯‡è®ºæ–‡çš„ä»£ç ï¼Œæ–¹ä¾¿å‘æˆ‘ä¸€ä»½å—ï¼Ÿ  2647790445@qq.com    Hello, can you send me a copy of the code for this paper?"
"In senario_battle.py  def play():  acts[i] = models[i].act(state=state[i], prob=former_act_prob[i], eps=eps)    In base.py  def act():  self.temperature = kwargs['eps']    However, self.temperature is not put into feed_dict. I've noticed that it is a float instead of a tensor (i.e. tf.placeholder).  Is it just enough to be a float? What is the difference between float and placeholder in this specific case?"
how install mfrl in windows?  
"Running this project on macOS, Clion. Meet these errors. Any suggestions?    ====================[ Build | render | Debugllvm ]==============================  /Applications/CLion.app/Contents/bin/cmake/mac/bin/cmake --build /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/cmake-build-debugllvm --target render -- -j 9  Scanning dependencies of target render  [ 14%] Building CXX object CMakeFiles/render.dir/src/render/backend/data.cc.o  [ 28%] Building CXX object CMakeFiles/render.dir/src/render/backend/render.cc.o  [ 57%] Building CXX object CMakeFiles/render.dir/src/render/backend/utility/logger.cc.o  [ 71%] Building CXX object CMakeFiles/render.dir/src/render/backend/utility/config.cc.o  [ 71%] Building CXX object CMakeFiles/render.dir/src/render/backend/text.cc.o  [ 85%] Building CXX object CMakeFiles/render.dir/src/render/backend/websocket.cc.o  In file included from /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/data.cc:1:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/fstream:183:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/ostream:138:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/ios:214:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/iosfwd:95:  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:137:77: error: use of undeclared identifier 'wcschr'  wchar_t* __libcpp_wcschr(const wchar_t* __s, wchar_t __c) {return (wchar_t*)wcschr(__s, __c);}                                                                              ^  In file included from /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/render.cc:1:  In file included from /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/server.h:4:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/fstream:183:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/ostream:138:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/ios:214:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/iosfwd:95:  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:137:77: error: use of undeclared identifier 'wcschr'  wchar_t* __libcpp_wcschr(const wchar_t* __s, wchar_t __c) {return (wchar_t*)wcschr(__s, __c);}                                                                              ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:144:87: error: use of undeclared identifier 'wcspbrk'  wchar_t* __libcpp_wcspbrk(const wchar_t* __s1, const wchar_t* __s2) {return (wchar_t*)wcspbrk(__s1, __s2);}                                                                                        ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:151:78: error: use of undeclared identifier 'wcsrchr'; did you mean 'wcschr'?  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:144:87: error: use of undeclared identifier 'wcspbrk'  wchar_t* __libcpp_wcsrchr(const wchar_t* __s, wchar_t __c) {return (wchar_t*)wcsrchr(__s, __c);}                                                                               ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:139:16: note: 'wcschr' declared here  wchar_t* __libcpp_wcspbrk(const wchar_t* __s1, const wchar_t* __s2) {return (wchar_t*)wcspbrk(__s1, __s2);}                                                                                        ^  const wchar_t* wcschr(const wchar_t* __s, wchar_t __c) {return __libcpp_wcschr(__s, __c);}                 ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:158:86: error: use of undeclared identifier 'wcsstr'; did you mean 'wcschr'?  wchar_t* __libcpp_wcsstr(const wchar_t* __s1, const wchar_t* __s2) {return (wchar_t*)wcsstr(__s1, __s2);}                                                                                       ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:141:16: note: 'wcschr' declared here        wchar_t* wcschr(      wchar_t* __s, wchar_t __c) {return __libcpp_wcschr(__s, __c);}                 ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:158:86: error: no matching function for call to 'wcschr'  wchar_t* __libcpp_wcsstr(const wchar_t* __s1, const wchar_t* __s2) {return (wchar_t*)wcsstr(__s1, __s2);}                                                                                       ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:141:16: note: candidate disabled:          wchar_t* wcschr(      wchar_t* __s, wchar_t __c) {return __libcpp_wcschr(__s, __c);}                 ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:158:93: error: cannot initialize a parameter of type 'wchar_t *' with an lvalue of type 'const wchar_t *'/usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:  151:78: error: use of undeclared identifier 'wcsrchr'; did you mean 'wcschr'?  wchar_t* __libcpp_wcsstr(const wchar_t* __s1, const wchar_t* __s2) {return (wchar_t*)wcsstr(__s1, __s2);}                                                                                              ^~~~  wchar_t* __libcpp_wcsrchr(const wchar_t* __s, wchar_t __c) {return (wchar_t*)wcsrchr(__s, __c);}                                                                               ^/usr/local/opt/llvm/bin/../include/c++/v1/wchar.h  :141:38: note: /usr/local/opt/llvm/bin/../include/c++/v1/wchar.hpassing argument to parameter '__s' here:139  :16: note: 'wcschr' declared here        wchar_t* wcschr(      wchar_t* __s, wchar_t __c) {return __libcpp_wcschr(__s, __c);}                                       ^  const wchar_t* wcschr(const wchar_t* __s, wchar_t __c) {return __libcpp_wcschr(__s, __c);}                 ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:165:60: error: unknown type name 'size_t'  wchar_t* __libcpp_wmemchr(const wchar_t* __s, wchar_t __c, size_t __n) {return (wchar_t*)wmemchr(__s, __c, __n);}                                                             ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h/usr/local/opt/llvm/bin/../include/c++/v1/wchar.h::158167::8657::  errorerror: : use of undeclared identifier 'wcsstr'; did you mean 'wcschr'?unknown type name 'size_t'    const wchar_t* wmemchr(const wchar_t* __s, wchar_t __c, size_t __n) {return __libcpp_wmemchr(__s, __c, __n);}                                                          ^  wchar_t* __libcpp_wcsstr(const wchar_t* __s1, const wchar_t* __s2) {return (wchar_t*)wcsstr(__s1, __s2);}                                                                                       ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:141:16: note: 'wcschr' declared here        wchar_t* wcschr(      wchar_t* __s, wchar_t __c) {return __libcpp_wcschr(__s, __c);}                 ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:158:86: error: no matching function for call to 'wcschr'  wchar_t* __libcpp_wcsstr(const wchar_t* __s1, const wchar_t* __s2) {return (wchar_t*)wcsstr(__s1, __s2);}                                                                                       ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:141:16: note: candidate disabled:    /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:169:57: error: unknown type name 'size_t'        wchar_t* wcschr(      wchar_t* __s, wchar_t __c) {return __libcpp_wcschr(__s, __c);}                 ^        wchar_t* wmemchr(      wchar_t* __s, wchar_t __c, size_t __n) {return __libcpp_wmemchr(__s, __c, __n);}                                                          ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:158:93: error: cannot initialize a parameter of type 'wchar_t *' with an lvalue of type 'const wchar_t *'  wchar_t* __libcpp_wcsstr(const wchar_t* __s1, const wchar_t* __s2) {return (wchar_t*)wcsstr(__s1, __s2);}In file included from   /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/utility/config.cc:                                                                                            ^~~~  1:  /usr/local/opt/llvm/bin/../include/c++/v1/string.h/usr/local/opt/llvm/bin/../include/c++/v1/wchar.h::73141::6438::  errornote: : passing argument to parameter '__s' here  use of undeclared identifier 'strchr'        wchar_t* wcschr(      wchar_t* __s, wchar_t __c) {return __libcpp_wcschr(__s, __c);}                                       ^  char* __libcpp_strchr(const char* __s, int __c) {return (char*)strchr(__s, __c);}                                                                 ^  In file included from /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/text.cc:4:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/math.h:308:  /usr/local/opt/llvm/bin/../include/c++/v1/stdlib.h:142:34: error: /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:165unknown type name 'ldiv_t':60  : error: unknown type name 'size_t'  wchar_t* __libcpp_wmemchr(const wchar_t* __s, wchar_t __c, size_t __n) {return (wchar_t*)wmemchr(__s, __c, __n);}inline _LIBCPP_INLINE_VISIBILITY ldiv_t div(long __x, long __y) _NOEXCEPT {                                                               ^                                 ^    /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:167:57: error: unknown type name 'size_t'  /usr/local/opt/llvm/bin/../include/c++/v1/stdlib.h:143:12: error: no member named 'ldiv' in the global namespace    return ::ldiv(__x, __y);           ~~^  const wchar_t* wmemchr(const wchar_t* __s, wchar_t __c, size_t __n) {return __libcpp_wmemchr(__s, __c, __n);}                                                          ^  /usr/local/opt/llvm/bin/../include/c++/v1/string.h:80:75: error: use of undeclared identifier 'strpbrk'  char* __libcpp_strpbrk(const char* __s1, const char* __s2) {return (char*)strpbrk(__s1, __s2);}                                                                            ^  In file included from In file included from /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/data.cc/Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/websocket.cc::1:  1In file included from :  /usr/local/opt/llvm/bin/../include/c++/v1/fstream:In file included from 183/Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/websocket.h:  :In file included from 4/usr/local/opt/llvm/bin/../include/c++/v1/ostream:  :In file included from 138/usr/local/include/websocketpp/config/asio_no_tls_client.hpp:  :In file included from 31/usr/local/opt/llvm/bin/../include/c++/v1/ios:  :In file included from 214/usr/local/include/websocketpp/config/core_client.hpp:  :38:  /usr/local/opt/llvm/bin/../include/c++/v1/iosfwdIn file included from :/usr/local/include/websocketpp/concurrency/basic.hpp189::3114:  :In file included from  /usr/local/include/websocketpp/common/thread.hpp:59error:  : In file included from /usr/local/opt/llvm/bin/../include/c++/v1/thread:use of undeclared identifier 'mbstate_t'86:    In file included from /usr/local/opt/llvm/bin/../include/c++/v1/iosfwd:95:  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:137:77: typedef fpos     streampos;/usr/local/opt/llvm/bin/../include/c++/v1/stdlib.h  :error146             ^: :  34/usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:: 169use of undeclared identifier 'wcschr':57  error::  unknown type name 'lldiv_t'error:   unknown type name 'size_t'  inline _LIBCPP_INLINE_VISIBILITY lldiv_t div(long long __x,                                   ^        wchar_t* wmemchr(      wchar_t* __s, wchar_t __c, size_t __n) {return __libcpp_wmemchr(__s, __c, __n);}wchar_t* __libcpp_wcschr(const wchar_t* __s, wchar_t __c) {return (wchar_t*)wcschr(__s, __c);}                                                            ^                                                                            ^    /usr/local/opt/llvm/bin/../include/c++/v1/stdlib.h:148:12: /usr/local/opt/llvm/bin/../include/c++/v1/iosfwd:error190: :14no member named 'lldiv' in the global namespace  : error: use of undeclared identifier 'mbstate_t'  return ::lldiv(__x, __y);             ~~^  typedef fpos     wstreampos;               ^  /usr/local/opt/llvm/bin/../include/c++/v1/iosfwd:195:14: error: use of undeclared identifier 'mbstate_t'  typedef fpos     u16streampos;               ^  /usr/local/opt/llvm/bin/../include/c++/v1/string.h:87:65: error: use of undeclared identifier 'strrchr'; did you mean 'strchr'?  char* __libcpp_strrchr(const char* __s, int __c) {return (char*)strrchr(__s, __c);}                                                                  ^  /usr/local/opt/llvm/bin/../include/c++/v1/string.h:75:13: note: 'strchr' declared here  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:144:87: error: use of undeclared identifier 'wcspbrk'  const char* strchr(const char* __s, int __c) {return __libcpp_strchr(__s, __c);}              ^  wchar_t* __libcpp_wcspbrk(const wchar_t* __s1, const wchar_t* __s2) {return (wchar_t*)wcspbrk(__s1, __s2);}                                                                                        ^  /usr/local/opt/llvm/bin/../include/c++/v1/iosfwd:196:14: error: use of undeclared identifier 'mbstate_t'  typedef fpos     u32streampos;               ^  /usr/local/opt/llvm/bin/../include/c++/v1/string.h:94:49: error: unknown type name 'size_t'  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:151:78: error: use of undeclared identifier 'wcsrchr'; did you mean 'wcschr'?  void* __libcpp_memchr(const void* __s, int __c, size_t __n) {return (void*)memchr(__s, __c, __n);}                                                  ^  wchar_t* __libcpp_wcsrchr(const wchar_t* __s, wchar_t __c) {return (wchar_t*)wcsrchr(__s, __c);}                                                                               ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:139:16: note: 'wcschr' declared here  const wchar_t* wcschr(const wchar_t* __s, wchar_t __c) {return __libcpp_wcschr(__s, __c);}                 ^  /usr/local/opt/llvm/bin/../include/c++/v1/string.h:96:46: error: unknown type name 'size_t'  In file included from /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/render.ccconst void* memchr(const void* __s, int __c, size_t __n) {return __libcpp_memchr(__s, __c, __n);}:  1:                                               ^In file included from   /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/server.h:4:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/fstream:183:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/ostream:138:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/ios:214:  /usr/local/opt/llvm/bin/../include/c++/v1/iosfwd:189:14: error: use of undeclared identifier 'mbstate_t'  typedef fpos     streampos;               ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:158:86: error: use of undeclared identifier 'wcsstr'; did you mean 'wcschr'?  wchar_t* __libcpp_wcsstr(const wchar_t* __s1, const wchar_t* __s2) {return (wchar_t*)wcsstr(__s1, __s2);}                                                                                       ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:141:16: note: 'wcschr' declared here  /usr/local/opt/llvm/bin/../include/c++/v1/string.h:98:46:       wchar_t* wcschr(      wchar_t* __s, wchar_t __c) {return __libcpp_wcschr(__s, __c);}  error:                ^  unknown type name 'size_t'        void* memchr(      void* __s, int __c, size_t __n) {return __libcpp_memchr(__s, __c, __n);}  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h                                             ^:  158:86: error: no matching function for call to 'wcschr'  /usr/local/opt/llvm/bin/../include/c++/v1/iosfwd:190:wchar_t* __libcpp_wcsstr(const wchar_t* __s1, const wchar_t* __s2) {return (wchar_t*)wcsstr(__s1, __s2);}14  :                                                                                      ^  error: use of undeclared identifier 'mbstate_t'  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:141:16: notetypedef fpos     wstreampos;:   candidate disabled:               ^          wchar_t* wcschr(      wchar_t* __s, wchar_t __c) {return __libcpp_wcschr(__s, __c);}                 ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:158:93: error: cannot initialize a parameter of type 'wchar_t *' with an lvalue of type 'const wchar_t *'  wchar_t* __libcpp_wcsstr(const wchar_t* __s1, const wchar_t* __s2) {return (wchar_t*)wcsstr(__s1, __s2);}                                                                                              ^~~~  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:141:38: note: passing argument to parameter '__s' here        wchar_t* wcschr(      wchar_t* __s, wchar_t __c) {return __libcpp_wcschr(__s, __c);}                                       ^  /usr/local/opt/llvm/bin/../include/c++/v1/string.h:101:74: error: use of undeclared identifier 'strstr'; did you mean 'strchr'?  char* __libcpp_strstr(const char* __s1, const char* __s2) {return (char*)strstr(__s1, __s2);}                                                                           ^/usr/local/opt/llvm/bin/../include/c++/v1/iosfwd  :195:14: /usr/local/opt/llvm/bin/../include/c++/v1/string.h:77error:: 13: use of undeclared identifier 'mbstate_t'  note: 'strchr' declared here  typedef fpos     u16streampos;               ^        char* strchr(      char* __s, int __c) {return __libcpp_strchr(__s, __c);}              ^  /usr/local/opt/llvm/bin/../include/c++/v1/string.h:101:74: error: no matching function for call to 'strchr'  char* __libcpp_strstr(const char* __s1, const char* __s2) {return (char*)strstr(__s1, __s2);}                                                                           ^  /usr/local/opt/llvm/bin/../include/c++/v1/string.h:77:13: note: candidate disabled:          char* strchr(      char* __s, int __c) {return __libcpp_strchr(__s, __c);}              ^  /usr/local/opt/llvm/bin/../include/c++/v1/string.h:101:81: error: cannot initialize a parameter of type 'char *' with an lvalue of type 'const char *'  char* __libcpp_strstr(const char* __s1, const char* __s2) {return (char*)strstr(__s1, __s2);}                                                                                  ^~~~  /usr/local/opt/llvm/bin/../include/c++/v1/string.h:77:/usr/local/opt/llvm/bin/../include/c++/v1/iosfwd32::196 :14:note : passing argument to parameter '__s' hereerror:   use of undeclared identifier 'mbstate_t'  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:165:60: error: unknown type name 'size_t'typedef fpos     u32streampos;                 ^        char* strchr(      char* __s, int __c) {return __libcpp_strchr(__s, __c);}                                 ^  wchar_t* __libcpp_wmemchr(const wchar_t* __s, wchar_t __c, size_t __n) {return (wchar_t*)wmemchr(__s, __c, __n);}                                                             ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:167:57: error: unknown type name 'size_t'  const wchar_t* wmemchr(const wchar_t* __s, wchar_t __c, size_t __n) {return __libcpp_wmemchr(__s, __c, __n);}                                                          ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:169:57: error: unknown type name 'size_t'        wchar_t* wmemchr(      wchar_t* __s, wchar_t __c, size_t __n) {return __libcpp_wmemchr(__s, __c, __n);}                                                          ^  In file included from /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/websocket.cc:1:  In file included from /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/websocket.h:4:  In file included from /usr/local/include/websocketpp/config/asio_no_tls_client.hpp:31:  In file included from /usr/local/include/websocketpp/config/core_client.hpp:38:  In file included from /usr/local/include/websocketpp/concurrency/basic.hpp:31:  In file included from /usr/local/include/websocketpp/common/thread.hpp:59:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/thread:86:  /usr/local/opt/llvm/bin/../include/c++/v1/iosfwd:189:14: error: use of undeclared identifier 'mbstate_t'  typedef fpos     streampos;               ^  In file included from /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/data.cc:1:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/fstream:183:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/ostream:138:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/ios:215:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/__locale:14:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/string:506:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/string_view:175:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/__string:57:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/algorithm:639:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/initializer_list:46:  /usr/local/opt/llvm/bin/../include/c++/v1/cstddef:49:9: error: no member named 'ptrdiff_t' in the global namespace  using ::ptrdiff_t;        ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/iosfwd:190:14: error: use of undeclared identifier 'mbstate_t'  typedef fpos     wstreampos;               ^  /usr/local/opt/llvm/bin/../include/c++/v1/iosfwd:195:14: error: use of undeclared identifier 'mbstate_t'  typedef fpos     u16streampos;               ^  /usr/local/opt/llvm/bin/../include/c++/v1/cstddef:50:9: error: no member named 'size_t' in the global namespace  using ::size_t;        ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/iosfwd:196:14: error: use of undeclared identifier 'mbstate_t'  typedef fpos     u32streampos;               ^  /usr/local/opt/llvm/bin/../include/c++/v1/cstddef:53:9: error: no member named 'max_align_t' in the global namespace  using ::max_align_t;        ~~^  In file included from /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/utility/config.cc:2:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/iostream:37:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/ios:214:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/iosfwd:95:  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:137:77: error: use of undeclared identifier 'wcschr'  wchar_t* __libcpp_wcschr(const wchar_t* __s, wchar_t __c) {return (wchar_t*)wcschr(__s, __c);}                                                                              ^  In file included from /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/data.cc:1:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/fstream:183:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/ostream:138:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/ios:215:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/__locale:14:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/string:506:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/string_view:175:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/__string:57:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/algorithm:639:  /usr/local/opt/llvm/bin/../include/c++/v1/initializer_list:61:5: error: unknown type name 'size_t'      size_t    __size_;      ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:144:87: error: use of undeclared identifier 'wcspbrk'  wchar_t* __libcpp_wcspbrk(const wchar_t* __s1, const wchar_t* __s2) {return (wchar_t*)wcspbrk(__s1, __s2);}                                                                                        ^  /usr/local/opt/llvm/bin/../include/c++/v1/initializer_list:65:38: error: unknown type name 'size_t'      initializer_list(const _Ep* __b, size_t __s) _NOEXCEPT                                       ^  /usr/local/opt/llvm/bin/../include/c++/v1/initializer_list:73:13: error: unknown type name 'size_t'      typedef size_t    size_type;              ^  In file included from /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/websocket.cc:1:  In file included from /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/websocket.h:4:  In file included from /usr/local/include/websocketpp/config/asio_no_tls_client.hpp:31:  In file included from /usr/local/include/websocketpp/config/core_client.hpp:38:  In file included from /usr/local/include/websocketpp/concurrency/basic.hpp:31:  In file included from /usr/local/include/websocketpp/common/thread.hpp:59:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/thread:87:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/__functional_base:14:  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.hIn file included from :/usr/local/opt/llvm/bin/../include/c++/v1/type_traits151::41778:  : /usr/local/opt/llvm/bin/../include/c++/v1/cstddef:error: 49:9use of undeclared identifier 'wcsrchr'; did you mean 'wcschr'?:   error: no member named 'ptrdiff_t' in the global namespace  using ::ptrdiff_t;wchar_t* __libcpp_wcsrchr(const wchar_t* __s, wchar_t __c) {return (wchar_t*)wcsrchr(__s, __c);}          ~~^                                                                             ^    /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:139:16: note: 'wcschr' declared here  const wchar_t* wcschr(const wchar_t* __s, wchar_t __c) {return __libcpp_wcschr(__s, __c);}                 ^  fatal error: too many errors emitted, stopping now [-ferror-limit=]  /usr/local/opt/llvm/bin/../include/c++/v1/cstddef:50:9: error: no member named 'size_t' in the global namespace  using ::size_t;        ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:158:86: error: use of undeclared identifier 'wcsstr'; did you mean 'wcschr'?  In file included from /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/render.cc:1:  In file included from /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/server.h:4:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/fstream:183:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/ostream:138:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/ios:215:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/__locale:14:  In file included from wchar_t* __libcpp_wcsstr(const wchar_t* __s1, const wchar_t* __s2) {return (wchar_t*)wcsstr(__s1, __s2);}/usr/local/opt/llvm/bin/../include/c++/v1/string  :506                                                                                     ^:    In file included from /usr/local/opt/llvm/bin/../include/c++/v1/string_view:175:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h/usr/local/opt/llvm/bin/../include/c++/v1/__string::14157::  16In file included from :/usr/local/opt/llvm/bin/../include/c++/v1/algorithm :639:  noteIn file included from : /usr/local/opt/llvm/bin/../include/c++/v1/initializer_list:'wcschr' declared here46:    /usr/local/opt/llvm/bin/../include/c++/v1/cstddef:49:9: error: no member named 'ptrdiff_t' in the global namespace  using ::ptrdiff_t;        ~~^        wchar_t* wcschr(      wchar_t* __s, wchar_t __c) {return __libcpp_wcschr(__s, __c);}                 ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:158:86: error: no matching function for call to 'wcschr'  wchar_t* __libcpp_wcsstr(const wchar_t* __s1, const wchar_t* __s2) {return (wchar_t*)wcsstr(__s1, __s2);}                                                                                       ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:141:16: note: /usr/local/opt/llvm/bin/../include/c++/v1/cstddefcandidate disabled:  :53  :9: error: no member named 'max_align_t' in the global namespace  using ::max_align_t;        ~~^        wchar_t* wcschr(      wchar_t* __s, wchar_t __c) {return __libcpp_wcschr(__s, __c);}                 ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:158:93: error: cannot initialize a parameter of type 'wchar_t *' with an lvalue of type 'const wchar_t *'  wchar_t* __libcpp_wcsstr(const wchar_t* __s1, const wchar_t* __s2) {return (wchar_t*)wcsstr(__s1, __s2);}                                                                                              ^~~~  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:141:38: note: passing argument to parameter '__s' here  /usr/local/opt/llvm/bin/../include/c++/v1/cstddef:50:9: error: no member named 'size_t' in the global namespace  using ::size_t;        ~~^        wchar_t* wcschr(      wchar_t* __s, wchar_t __c) {return __libcpp_wcschr(__s, __c);}                                       ^  /usr/local/opt/llvm/bin/../include/c++/v1/cstddef:53:9: error: no member named 'max_align_t' in the global namespace  using ::max_align_t;        ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:165:60: error: unknown type name 'size_t'  wchar_t* __libcpp_wmemchr(const wchar_t* __s, wchar_t __c, size_t __n) {return (wchar_t*)wmemchr(__s, __c, __n);}                                                             ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:167:57: error: unknown type name 'size_t'  const wchar_t* wmemchr(const wchar_t* __s, wchar_t __c, size_t __n) {return __libcpp_wmemchr(__s, __c, __n);}                                                          ^  In file included from /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/render.cc:1:  In file included from /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/server.h:4:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/fstream:183:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/ostream:138:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/ios:215:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/__locale:14:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/string:506:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/string_view:175:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/__string:57:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/algorithm:639:  /usr/local/opt/llvm/bin/../include/c++/v1/initializer_list:61:5: error: unknown type name 'size_t'      size_t    __size_;      ^  /usr/local/opt/llvm/bin/../include/c++/v1/wchar.h:169:57: error: unknown type name 'size_t'        wchar_t* wmemchr(      wchar_t* __s, wchar_t __c, size_t __n) {return __libcpp_wmemchr(__s, __c, __n);}                                                          ^  /usr/local/opt/llvm/bin/../include/c++/v1/initializer_list:65:38: error: unknown type name 'size_t'      initializer_list(const _Ep* __b, size_t __s) _NOEXCEPT                                       ^  /usr/local/opt/llvm/bin/../include/c++/v1/initializer_list:73:13: error: unknown type name 'size_t'      typedef size_t    size_type;              ^  fatal error: too many errors emitted, stopping now [-ferror-limit=]  In file included from /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/utility/config.cc:2:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/iostream:37:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/ios:214:  /usr/local/opt/llvm/bin/../include/c++/v1/iosfwd:189:14: error: use of undeclared identifier 'mbstate_t'  typedef fpos     streampos;               ^  fatal error: too many errors emitted, stopping now [-ferror-limit=]  In file included from /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/websocket.cc:1:  In file included from /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/websocket.h:4:  In file included from /usr/local/include/websocketpp/config/asio_no_tls_client.hpp:31:  In file included from /usr/local/include/websocketpp/config/core_client.hpp:38:  In file included from /usr/local/include/websocketpp/concurrency/basic.hpp:31:  In file included from /usr/local/include/websocketpp/common/thread.hpp:59:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/thread:87:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/__functional_base:14:  /usr/local/opt/llvm/bin/../include/c++/v1/type_traits:1553:32: error: use of undeclared identifier 'size_t'; did you mean 'sizeof'?      : public integral_constant  {};                                 ^  /usr/local/opt/llvm/bin/../include/c++/v1/type_traits:1553:38: error: expected expression      : public integral_constant  {};                                       ^  /usr/local/opt/llvm/bin/../include/c++/v1/type_traits:1555:32: error: use of undeclared identifier 'size_t'; did you mean 'sizeof'?      : public integral_constant ::value + 1> {};                                 ^  fatal error: too many errors emitted, stopping now [-ferror-limit=]  In file included from /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/utility/logger.cc:1:  In file included from /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/utility/logger.h:4:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/mutex:190:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/__mutex_base:14:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/chrono:829:  In file included from /usr/local/opt/llvm/bin/../include/c++/v1/ratio:81:  /usr/local/opt/llvm/bin/../include/c++/v1/cstdint:157:8: error: no member named 'uint8_t' in the global namespace  using::uint8_t;       ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/cstdint:158:8: error: no member named 'uint16_t' in the global namespace  using::uint16_t;       ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/cstdint:159:8: error: no member named 'uint32_t' in the global namespace  using::uint32_t;       ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/cstdint:160:8: error: no member named 'uint64_t' in the global namespace  using::uint64_t;       ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/cstdint:162:8: error: no member named 'int_least8_t' in the global namespace  using::int_least8_t;       ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/cstdint:163:8: error: no member named 'int_least16_t' in the global namespace  using::int_least16_t;       ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/cstdint:164:8: error: no member named 'int_least32_t' in the global namespace  using::int_least32_t;       ~~^  In file included from /Users/lindsaymorgan/Desktop/mfrl/examples/battle_model/src/render/backend/text.cc:4:  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:771:93: error: no member named 'acosf' in the global namespace; did you mean 'acos'?  inline _LIBCPP_INLINE_VISIBILITY float       acos(float __lcpp_x) _NOEXCEPT       {return ::acosf(__lcpp_x);}                                                                                            ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:771:46: note: 'acos' declared here  inline _LIBCPP_INLINE_VISIBILITY float       acos(float __lcpp_x) _NOEXCEPT       {return ::acosf(__lcpp_x);}                                               ^  /usr/local/opt/llvm/bin/../include/c++/v1/cstdint:165:8: error: no member named 'int_least64_t' in the global namespace  using::int_least64_t;       ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:772:93: error: no member named 'acosl' in the global namespace; did you mean 'acos'?  inline _LIBCPP_INLINE_VISIBILITY long double acos(long double __lcpp_x) _NOEXCEPT {return ::acosl(__lcpp_x);}                                                                                            ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:771:46: note: 'acos' declared here  inline _LIBCPP_INLINE_VISIBILITY float       acos(float __lcpp_x) _NOEXCEPT       {return ::acosf(__lcpp_x);}                                               ^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:778:38: error: call to 'acos' is ambiguous  acos(_A1 __lcpp_x) _NOEXCEPT {return ::acos((double)__lcpp_x);}                                       ^~~~~~  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:771:46: note: candidate function  inline _LIBCPP_INLINE_VISIBILITY float       acos(float __lcpp_x) _NOEXCEPT       {return ::acosf(__lcpp_x);}                                               ^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:772:46: note: candidate function  inline _LIBCPP_INLINE_VISIBILITY long double acos(long double __lcpp_x) _NOEXCEPT {return ::acosl(__lcpp_x);}                                               ^  /usr/local/opt/llvm/bin/../include/c++/v1/cstdint:167:8: error: no member named 'uint_least8_t' in the global namespace  using::uint_least8_t;       ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:783:93: error: no member named 'asinf' in the global namespace; did you mean 'asin'?  inline _LIBCPP_INLINE_VISIBILITY float       asin(float __lcpp_x) _NOEXCEPT       {return ::asinf(__lcpp_x);}                                                                                            ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:783:46: note: 'asin' declared here  inline _LIBCPP_INLINE_VISIBILITY float       asin(float __lcpp_x) _NOEXCEPT       {return ::asinf(__lcpp_x);}                                               ^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:784:93: error: no member named 'asinl' in the global namespace; did you mean 'asin'?  inline _LIBCPP_INLINE_VISIBILITY long double asin(long double __lcpp_x) _NOEXCEPT {return ::asinl(__lcpp_x);}                                                                                            ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:783:46: note: 'asin' declared here  inline _LIBCPP_INLINE_VISIBILITY float       asin(float __lcpp_x) _NOEXCEPT       {return ::asinf(__lcpp_x);}                                               ^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:790:38: error: call to 'asin' is ambiguous  asin(_A1 __lcpp_x) _NOEXCEPT {return ::asin((double)__lcpp_x);}                                       ^~~~~~  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:783:46: note: candidate function  inline _LIBCPP_INLINE_VISIBILITY float       asin(float __lcpp_x) _NOEXCEPT       {return ::asinf(__lcpp_x);}                                               ^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:784:46: note: candidate function  inline _LIBCPP_INLINE_VISIBILITY long double asin(long double __lcpp_x) _NOEXCEPT {return ::asinl(__lcpp_x);}                                               ^  /usr/local/opt/llvm/bin/../include/c++/v1/cstdint:168:8: error: no member named 'uint_least16_t' in the global namespace  using::uint_least16_t;       ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:795:93: error: no member named 'atanf' in the global namespace; did you mean 'atan'?  inline _LIBCPP_INLINE_VISIBILITY float       atan(float __lcpp_x) _NOEXCEPT       {return ::atanf(__lcpp_x);}                                                                                            ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:795:46: note: 'atan' declared here  inline _LIBCPP_INLINE_VISIBILITY float       atan(float __lcpp_x) _NOEXCEPT       {return ::atanf(__lcpp_x);}                                               ^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:796:93: error: no member named 'atanl' in the global namespace; did you mean 'atan'?  inline _LIBCPP_INLINE_VISIBILITY long double atan(long double __lcpp_x) _NOEXCEPT {return ::atanl(__lcpp_x);}                                                                                            ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:795:46: note: 'atan' declared here  inline _LIBCPP_INLINE_VISIBILITY float       atan(float __lcpp_x) _NOEXCEPT       {return ::atanf(__lcpp_x);}                                               ^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:802:38: error: call to 'atan' is ambiguous  atan(_A1 __lcpp_x) _NOEXCEPT {return ::atan((double)__lcpp_x);}                                       ^~~~~~  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:795:46: note: candidate function  inline _LIBCPP_INLINE_VISIBILITY float       atan(float __lcpp_x) _NOEXCEPT       {return ::atanf(__lcpp_x);}                                               ^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:796:46: note: candidate function  inline _LIBCPP_INLINE_VISIBILITY long double atan(long double __lcpp_x) _NOEXCEPT {return ::atanl(__lcpp_x);}                                               ^  /usr/local/opt/llvm/bin/../include/c++/v1/cstdint:169:8: error: no member named 'uint_least32_t' in the global namespace  using::uint_least32_t;       ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:807:116: error: no member named 'atan2f' in the global namespace; did you mean 'atan2'?  inline _LIBCPP_INLINE_VISIBILITY float       atan2(float __lcpp_y, float __lcpp_x) _NOEXCEPT             {return ::atan2f(__lcpp_y, __lcpp_x);}                                                                                                                   ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:807:46: note: 'atan2' declared here  inline _LIBCPP_INLINE_VISIBILITY float       atan2(float __lcpp_y, float __lcpp_x) _NOEXCEPT             {return ::atan2f(__lcpp_y, __lcpp_x);}                                               ^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:808:116: error: no member named 'atan2l' in the global namespace; did you mean 'atan2'?  inline _LIBCPP_INLINE_VISIBILITY long double atan2(long double __lcpp_y, long double __lcpp_x) _NOEXCEPT {return ::atan2l(__lcpp_y, __lcpp_x);}                                                                                                                   ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:807:46: note: 'atan2' declared here  inline _LIBCPP_INLINE_VISIBILITY float       atan2(float __lcpp_y, float __lcpp_x) _NOEXCEPT             {return ::atan2f(__lcpp_y, __lcpp_x);}                                               ^  /usr/local/opt/llvm/bin/../include/c++/v1/cstdint:170:8: error: no member named 'uint_least64_t' in the global namespace  using::uint_least64_t;       ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:830:93: error: no member named 'ceilf' in the global namespace; did you mean 'ceil'?  inline _LIBCPP_INLINE_VISIBILITY float       ceil(float __lcpp_x) _NOEXCEPT       {return ::ceilf(__lcpp_x);}                                                                                            ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:830:46: note: 'ceil' declared here  inline _LIBCPP_INLINE_VISIBILITY float       ceil(float __lcpp_x) _NOEXCEPT       {return ::ceilf(__lcpp_x);}                                               ^  /usr/local/opt/llvm/bin/../include/c++/v1/cstdint:172:8: error: no member named 'int_fast8_t' in the global namespace  using::int_fast8_t;       ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:831:93: error: no member named 'ceill' in the global namespace; did you mean 'ceil'?  inline _LIBCPP_INLINE_VISIBILITY long double ceil(long double __lcpp_x) _NOEXCEPT {return ::ceill(__lcpp_x);}                                                                                            ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:830:46: note: 'ceil' declared here  inline _LIBCPP_INLINE_VISIBILITY float       ceil(float __lcpp_x) _NOEXCEPT       {return ::ceilf(__lcpp_x);}                                               ^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:837:38: error: call to 'ceil' is ambiguous  ceil(_A1 __lcpp_x) _NOEXCEPT {return ::ceil((double)__lcpp_x);}                                       ^~~~~~  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:830:46: note: candidate function  inline _LIBCPP_INLINE_VISIBILITY float       ceil(float __lcpp_x) _NOEXCEPT       {return ::ceilf(__lcpp_x);}                                               ^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:831:46: note: candidate function  inline _LIBCPP_INLINE_VISIBILITY long double ceil(long double __lcpp_x) _NOEXCEPT {return ::ceill(__lcpp_x);}                                               ^  /usr/local/opt/llvm/bin/../include/c++/v1/cstdint:173:8: error: no member named 'int_fast16_t' in the global namespace  using::int_fast16_t;       ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:842:92: error: no member named 'cosf' in the global namespace; did you mean 'cos'?  inline _LIBCPP_INLINE_VISIBILITY float       cos(float __lcpp_x) _NOEXCEPT       {return ::cosf(__lcpp_x);}                                                                                           ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/math.h:842:46: note: 'cos' declared here  inline _LIBCPP_INLINE_VISIBILITY float       cos(float __lcpp_x) _NOEXCEPT       {return ::cosf(__lcpp_x);}                                               ^  /usr/local/opt/llvm/bin/../include/c++/v1/cstdint:174:8: error: no member named 'int_fast32_t' in the global namespace  using::int_fast32_t;       ~~^  fatal error: too many errors emitted, stopping now [-ferror-limit=]  /usr/local/opt/llvm/bin/../include/c++/v1/cstdint:175:8: error: no member named 'int_fast64_t' in the global namespace  using::int_fast64_t;       ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/cstdint:177:8: error: no member named 'uint_fast8_t' in the global namespace  using::uint_fast8_t;       ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/cstdint:178:8: error: no member named 'uint_fast16_t' in the global namespace  using::uint_fast16_t;       ~~^  /usr/local/opt/llvm/bin/../include/c++/v1/cstdint:179:8: error: no member named 'uint_fast32_t' in the global namespace  using::uint_fast32_t;       ~~^  fatal error: too many errors emitted, stopping now [-ferror-limit=]  20 errors generated.  make[3]: *** [CMakeFiles/render.dir/src/render/backend/utility/config.cc.o] Error 1  make[3]: *** Waiting for unfinished jobs....  20 errors generated.  make[3]: *** [CMakeFiles/render.dir/src/render/backend/utility/logger.cc.o] Error 1  20 errors generated.  make[3]: *** [CMakeFiles/render.dir/src/render/backend/text.cc.o] Error 1  20 errors generated.  make[3]: *** [CMakeFiles/render.dir/src/render/backend/data.cc.o] Error 1  20 errors generated.  make[3]: *** [CMakeFiles/render.dir/src/render/backend/websocket.cc.o] Error 1  20 errors generated.  make[3]: *** [CMakeFiles/render.dir/src/render/backend/render.cc.o] Error 1  make[2]: *** [CMakeFiles/render.dir/all] Error 2  make[1]: *** [CMakeFiles/render.dir/rule] Error 2  make: *** [render] Error 2  "
"Hello, I'm watching your paper ""Efficient Ride Sharing Order Dispatching with Mean Field Multi-Agent Reinforcement Learning"", and I'm very interested. Can you mention relevant codes and data sets? I promise to quote your paper in future works and look forward to your reply"
"Hi,when I run python3 train_battle.py --algo mfac, it occurs that:   File ""train_battle.py"", line 48, in        env.set_render_dir(os.path.join(BASE_DIR, 'examples/battle_model', 'build/render'))    File ""/home/lyl/miniconda3/envs/lyl/lib/python3.6/site-packages/magent/gridworld.py"", line 423, in set_render_dir      os.mkdir(name)  FileNotFoundError: [Errno 2] No such file or directory: '/mnt/d/try/mfrl-master/examples/battle_model/build/render'    Can you  tell me how to solve it?  Thanks.    "
I want to take a look at the battle screens I trained. What should I do? I found the index.html page file. How should I use this? Looking forward to your answer
"Hello, I recently looked at your code, I donâ€™t understand the meaning of â€˜minimapâ€™, and I want to know what â€˜minimap_modeâ€™ is? I look forward to hearing from you."
"Hello, I am reading this paper recently to ask you a question. Is it possible to join a new team in the battle game?I hope to get your reply."
"I use Ubuntu System, and when I try run ""python3 train_battle.py --algo mfac"", I got an Error ""ModuleNotFoundError: No module named 'magent"",  and ""AttributeError: 'GridWorld' object has no attribute 'game'"".   I'm a new learner,  I don't know how to fixt it, I hope someone can help me to solve this problem.Thanks  "
"Hi, recently I am trying to reproduce your work and feel a little confused about following questionï¼š  1. How to get average action during testï¼Ÿ During trainingï¼Œ we can obtain average action. However, during test, such action cannot be observed. I have carefully read your code, and found that you have adopted the average action in last step?  2. How you calculate the average action among neighbors? In your implementation, it seems that each node can observe all other players, thus the average is taken over all players?  It's really strange.  "
"Hi, recently I am trying to reproduce your work and feel a little confused when implementing MF-AC. According to the algorithm at somewhere the MF-Value (10) should be calculated, where it seems it involves many computations to enumerate all possible mean-field actions and their probabilities. I took a look at you MF-AC implementation in battle-game, but it appears to me (please correct me if i am wrong) here the MF-values are substituted with the returns from the sampled trajectory? Could you explain more about how to calculate the MF-value eq(10), for both MF-AC and MF-Q? Thanks"
"Hi I recently get some confusion when trying to reproduce your work, particular about experiment (1) on gaussian squeezing. According to my understanding in order to implement MAA2C algorithm as described in the DeepMind's NeurIPS 17 paper, the critic network should represent the Q-value function which takes joint action of the players into input. However, it seems that gaussian squeeze task is a stateless environment. According to your implementation details, there is a discount factor \gamma for AC methods but not for Q-learning method. So how do you define the state for gaussian squeezing? And if it is stateless, how can one use A2C methods?"
"Recently, I have run the train_battle.py and battle.py. However, I found the performance of one algorithms may vary quite obviously in the battle game. I consider this is quite normal. As the performance of models saved in the 2000th generation may not be the best. Besides, with different random seeds, final performance of models obtained after the self-play process should be different. In this case, I wonder how you evaluate their performance and gain results in Fig.8.  How many independent runs you have made for each algorithm in the comparative battles. "
there are some minor typos on ./examples/battle_model/algo/ac.py  tf.dense( >tf.layers.dense(  unit > units  action > activation    thanks for your effort :)
"It's not a PyTorch implementation of MF-Q and MF-AC, but it uses tensorflow."
None
None
None
"File: `examples/battle_model/src/gridworld/GridWorld.cc`.  Line: 423, 432, 447.    Is the `type.move_base` a replication mistake?  According to my understanding, this should be replaced by `type.turn_base`.    Thanks!"
"I have read the paper Mean Field Multi-Agent Reinforcement Learning,agent's state is global or individualï¼Ÿwould you mind explaining me the battle game'markov state.Thxï¼"
"I have read the paper: Mean Field Multi-Agent Reinforcement Learning. However, I wonder how you define the observation of agents, since I did find corresponding description in the paper. Do the agents are able to get a full observation or local observation, especially in the battle game?"
"I plan to apply this algorithm in the project. It will be appreciated if you can share the source code that used for battle game, Thanks."
"I found the value function in the code is a Q table and is defined as (Q = np.zeros((n_agents, dim_Q_state, n_actions))).   However, the Q function in the paper is defined as (state,action1,action2). Why they are different?"
"Hi, weakly-supervised disentanglement is a nice work. But I am still confused about some concept. Looking forward to your explanation.  - in the paper at page 5, you compare the work with prior work. ""Our approach critically differs in the sense that S is not known and needs to be estimated for every pair of images."" May I ask you about the impletation of S estimation? It seems that I have ignore some crucial details.  - What is the meaning of `Rnd`? Is it an abbreviations?"
"I have noticed a bug when trying to use ScreamDsprites:     Raises an error:  `Traceback (most recent call last):    File "" "", line 1, in      File ""disentanglement_lib/disentanglement_lib/data/ground_truth/dsprites.py"", line 183, in __init__      scream.thumbnail((350, 274, 3))    File "".venv/disentanglement_lib/lib/python3.6/site-packages/PIL/Image.py"", line 2299, in thumbnail      x, y = map(math.floor, size)  ValueError: too many values to unpack (expected 2)  `    The issue seems to be that thumbnail size is taking a tuple of 3 values instead of 2.  I have modified   from `scream.thumbnail((350, 274, 3))` to `scream.thumbnail((350, 274))` and this is working properly.    I am using Pillow 8.0.1, but the thumbnail   so this is not a compatibility issue.    I should add that    "
"!   This is Figure 7 MIG distribution on dSprites. The MIG score should be non-negative, and the scores are low. I analyze the pre-trained models and get the following results.  !     I don't know why they are so different, and the other metrics are different too."
"I have been examining the implementation of generating the weakly supervised dataset from the paper Weakly-Supervised Disentanglement Without Compromises, however I think there is a discrepancy between the function `simple_dynamics` in `disentanglement_lib/methods/weak/train_weak_lib` and what is described in Section 5 from version 3 the Arxiv paper.    ## Examined Function         ## Excerpt & Corresponding Code    The following is an excerpt from the Experimental Setup subsection from Section 5 of the paper Weakly-Supervised Disentanglement Without Compromises, split into sections that I assume should correspond to the above code:    1. > To create data sets with weak supervision from the existing disentanglement data sets, we first sample from the discrete z according to the ground-truth generative model (1)â€“(2).             2. > Then, we sample k factors of variation that should not be shared by the two images...             3. > ... and re-sample those coordinates to obtain zËœ. This ensures that each image pair differs in at most k factors of variation.             4. > For k we consider the range from 1 to d âˆ’ 1. This last setting corresponds to the case where all but one factor of variation are re-sampled. We study both the case where k is constant across all pairs in the data set and where k is sampled uniformly in the range [d âˆ’ 1] for every training pair (k = Rnd in the following). Unless specified otherwise, we aggregate the results for all values of k.    ## Problem With 2?    From the above excerpt there seems to be a problem with line:       In particular the expression `random_state.choice([1, k_observed])`. Instead of keeping `k` fixed half of the time `k` will be set to 1.    I may be misunderstanding things from the excerpt, but to me this seems odd that this is happening.    ### Fix?    Based on this, should lines 48 and 49 not be the following?       ## Problem With 4?    Based on the following excerpt it seems as though factors in the sampled pairs should always differ.    > ...We study both the case where k is constant across all pairs in the data set...           However, based on lines 52-53 this is not the case. There is a chance for the re-sampled factor to be the same. It is not guaranteed to be different.    This probability of being the same will only increase if the ground truth dimensionality/size of that factor is small.    ### Fix?  Sampling with the original value for the particular differing z removed from the range.    Untested possible code for 1 input factor:   "
None
The main limitation of beta vae metric is that it is sensitive to hyper parameters . This is mentioned in disentangling by factorizing. By not tuning the learning rate of Logistic  regressior we need not get the true beta vae metric
As I understand they were not realeased because the 3dshapes dataset itself wasn't at that time. But it has become public now. If the corresponding pretrained models on it can be realeased too it would be really great.
"Running dlib_reproduce - model dir and output dir outputs all stationary visualizations, but any animations/GIFS are just grey boxes. The same problem also occurs when running directly using the visualize function.    TensorFlow: 1.14.0 "
"I might be misunderstanding (e.g. perhaps all the ground-truth factors are meant to be discretized), but it seems like all of the   require latent factors to be discrete, e.g. using a classifier rather than a regressor for DCI. This is a bit inconvenient for users who would like to use your library to evaluate disentanglement solely given matrices of continuous latents and ground truth factors (without using the `representation_function` API). I know it's a design choice rather than a bug, but I'd still like to make the suggestion that it would be helpful for future users if you supported this use case!"
"Is the encoder architecture ( `conv_encoder` ) from   the one used in the experiments in the paper?   It seems to be a discrepancy in terms of the size of conv. kernels used in the 3rd and 4th layers.  The code uses 2x2 convolutions there, while, according to Table 2 from the Appendix of the   4x4 convolutions are used all the way."
"Does a lookup index exist for the 10'800 pretrained disentanglement_lib modules? I wanted to quickly reproduce specific experiments (e.g. btcvae, beta=4, dsprites), but I am not sure how to find that pretrained model.    Thanks!"
"I downloaded the pretrained model with index 0. To start playing with the model, I want to run dlib_postprocess. I need to give values to the following four flags:  1) model_dir: ""Directory to take trained model from.""  2) output_dir: ""Directory to save extracted representation to.""  3) gin_config: ""List of paths to the config files.""  4) gin_bindings: ""Newline separated list of Gin parameter bindings.""    The first two are obvious:     --model_dir=../0/model/tfhub  --output_dir=wherever_I_want_to_save_the_output_of_postprocessing    The last two are less clear to me. For gin_config I tried:    --gin_config= ../0/postprocessed/mean/results/gin/postprocess.gin    And I got the following error:    `Traceback (most recent call last):    File ""/home/mencia/Internship/Projects/Disentanglement_Locatello/env/bin/dlib_postprocess"", line 45, in        app.run(main)    File ""/home/mencia/Internship/Projects/Disentanglement_Locatello/env/lib/python3.5/site-packages/absl/app.py"", line 299, in run      _run_main(main, args)    File ""/home/mencia/Internship/Projects/Disentanglement_Locatello/env/lib/python3.5/site-packages/absl/app.py"", line 250, in _run_main      sys.exit(main(argv))    File ""/home/mencia/Internship/Projects/Disentanglement_Locatello/env/bin/dlib_postprocess"", line 41, in main      FLAGS.gin_bindings)    File ""/home/mencia/Internship/Projects/Disentanglement_Locatello/env/lib/python3.5/site-packages/disentanglement_lib/postprocessing/postprocess.py"", line 55, in postprocess_with_gin      gin.parse_config_files_and_bindings(gin_config_files, gin_bindings)    File ""/home/mencia/Internship/Projects/Disentanglement_Locatello/env/lib/python3.5/site-packages/gin/config.py"", line 1634, in parse_config_files_and_bindings      parse_config_file(config_file, skip_unknown)    File ""/home/mencia/Internship/Projects/Disentanglement_Locatello/env/lib/python3.5/site-packages/gin/config.py"", line 1602, in parse_config_file      raise IOError(err_str.format(config_file, prefixes))  OSError: Unable to open file: ~/Internship/Projects/Disentanglement_Locatello/pretrained_models/0/postprocessed/sampled/results/gin/postprocess.gin. Searched config paths: [''].`    I also tried importing sweep to dlib_postprocess and defining:    `gin_bindings, gin_config = get_model_config(model_num=0)`    But this did not work either.    How can I run dlib_postprocess?      "
"Hi guys, I am having this kind or error when I try the test command     > dlib_tests    after the pip install command.  `pip install .[tf_gpu]`    ======================================================================  ERROR: test_train_model(['/home/phong/data/Code_hay/disentanglement_lib/disentanglement_lib/config/tests/methods/unsupervised/train_test.gin'], ['model.model = @vae()', 'vae.beta = 10.', 'reconstruction_loss.loss_fn = @l2_loss', ""reconstruction_loss.activation = 'logits'"", ""reconstruction_loss.activation = 'tanh'"", 'encoder.encoder_fn = @conv_encoder', 'decoder.decoder_fn = @deconv_decoder', 'encoder.num_latent = 10']) (__main__.TrainTest)  test_train_model(['/home/phong/data/Code_hay/disentanglement_lib/disentanglement_lib/config/tests/methods/unsupervised/train_test.gin'], ['model.model = @vae()', 'vae.beta = 10.', 'reconstruction_loss.loss_fn = @l2_loss', ""reconstruction_loss.activation = 'logits'"", ""reconstruction_loss.activation = 'tanh'"", 'encoder.encoder_fn = @conv_encoder', 'decoder.decoder_fn = @deconv_decoder', 'encoder.num_latent = 10']) (__main__.TrainTest)  test_train_model(['/home/phong/data/Code_hay/disentanglement_lib/disentanglement_lib/config/tests/methods/unsupervised/train_test.gin'], ['model.model = @vae()', 'vae.beta = 10.', 'reconstruction_loss.loss_fn = @l2_loss', ""reconstruction_loss.activation = 'logits'"", ""reconstruction_loss.activation = 'tanh'"", 'encoder.encoder_fn = @conv_encoder', 'decoder.decoder_fn = @deconv_decoder', 'encoder.num_latent = 10'])  ----------------------------------------------------------------------  Traceback (most recent call last):    File ""/home/phong/miniconda3/envs/deep/lib/python3.7/site-packages/absl/testing/parameterized.py"", line 265, in bound_param_test      test_method(self, *testcase_params)    File ""/home/phong/data/Code_hay/disentanglement_lib/disentanglement_lib/methods/unsupervised/train_test.py"", line 92, in test_train_model      gin_bindings)    File ""/home/phong/data/Code_hay/disentanglement_lib/disentanglement_lib/methods/unsupervised/train.py"", line 53, in train_with_gin      train(model_dir, overwrite)    File ""/home/phong/miniconda3/envs/deep/lib/python3.7/site-packages/gin/config.py"", line 1073, in gin_wrapper      utils.augment_exception_message_and_reraise(e, err_str)    File ""/home/phong/miniconda3/envs/deep/lib/python3.7/site-packages/gin/utils.py"", line 49, in augment_exception_message_and_reraise      six.raise_from(proxy.with_traceback(exception.__traceback__), None)    File "" "", line 3, in raise_from    File ""/home/phong/miniconda3/envs/deep/lib/python3.7/site-packages/gin/config.py"", line 1050, in gin_wrapper      return fn(*new_args, **new_kwargs)    File ""/home/phong/data/Code_hay/disentanglement_lib/disentanglement_lib/methods/unsupervised/train.py"", line 123, in train      steps=training_steps)    File ""/home/phong/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 2876, in train      rendezvous.raise_errors()    File ""/home/phong/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 131, in raise_errors      six.reraise(typ, value, traceback)    File ""/home/phong/miniconda3/envs/deep/lib/python3.7/site-packages/six.py"", line 693, in reraise      raise value    File ""/home/phong/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 2871, in train      saving_listeners=saving_listeners)    File ""/home/phong/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 367, in train      loss = self._train_model(input_fn, hooks, saving_listeners)    File ""/home/phong/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1158, in _train_model      return self._train_model_default(input_fn, hooks, saving_listeners)    File ""/home/phong/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1192, in _train_model_default      saving_listeners)    File ""/home/phong/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1484, in _train_with_estimator_spec      _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])    File ""/home/phong/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 754, in run      run_metadata=run_metadata)    File ""/home/phong/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1252, in run      run_metadata=run_metadata)    File ""/home/phong/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1353, in run      raise six.reraise(*original_exc_info)    File ""/home/phong/miniconda3/envs/deep/lib/python3.7/site-packages/six.py"", line 693, in reraise      raise value    File ""/home/phong/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1338, in run      return self._sess.run(*args, **kwargs)    File ""/home/phong/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1411, in run      run_metadata=run_metadata)    File ""/home/phong/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1169, in run      return self._sess.run(*args, **kwargs)    File ""/home/phong/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 950, in run      run_metadata_ptr)    File ""/home/phong/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1173, in _run      feed_dict_tensor, options, run_metadata)    File ""/home/phong/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1350, in _do_run      run_metadata)    File ""/home/phong/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1370, in _do_call      raise type(e)(node_def, op, message)  tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.    (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.           [[node encoder/e1/Conv2D (defined at /data/Code_hay/disentanglement_lib/disentanglement_lib/methods/shared/architectures.py:173) ]]           [[loss/_497]]    (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.           [[node encoder/e1/Conv2D (defined at /data/Code_hay/disentanglement_lib/disentanglement_lib/methods/shared/architectures.py:173) ]]  0 successful operations.  0 derived errors ignored.    Errors may have originated from an input operation.  Input Source operations connected to node encoder/e1/Conv2D:   IteratorGetNext (defined at /data/Code_hay/disentanglement_lib/disentanglement_lib/methods/unsupervised/train.py:155)    Input Source operations connected to node encoder/e1/Conv2D:   IteratorGetNext (defined at /data/Code_hay/disentanglement_lib/disentanglement_lib/methods/unsupervised/train.py:155)    Original stack trace for 'encoder/e1/Conv2D':    File ""/miniconda3/envs/deep/lib/python3.7/runpy.py"", line 193, in _run_module_as_main      ""__main__"", mod_spec)    File ""/miniconda3/envs/deep/lib/python3.7/runpy.py"", line 85, in _run_code      exec(code, run_globals)    File ""/data/Code_hay/disentanglement_lib/disentanglement_lib/methods/unsupervised/train_test.py"", line 96, in        absltest.main()    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/absl/testing/absltest.py"", line 1909, in main      _run_in_app(run_tests, args, kwargs)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/absl/testing/absltest.py"", line 2026, in _run_in_app      app.run(main=main_function)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/absl/app.py"", line 299, in run      _run_main(main, args)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main      sys.exit(main(argv))    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/absl/testing/absltest.py"", line 2024, in main_function      function(argv, args, kwargs)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/absl/testing/absltest.py"", line 2295, in run_tests      argv, args, kwargs, xml_reporter.TextAndXMLTestRunner)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/absl/testing/absltest.py"", line 2265, in _run_and_get_tests_result      test_program = unittest.TestProgram(*args, **kwargs)    File ""/miniconda3/envs/deep/lib/python3.7/unittest/main.py"", line 101, in __init__      self.runTests()    File ""/miniconda3/envs/deep/lib/python3.7/unittest/main.py"", line 271, in runTests      self.result = testRunner.run(self.test)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/absl/testing/_pretty_print_reporter.py"", line 87, in run      return super(TextTestRunner, self).run(test)    File ""/miniconda3/envs/deep/lib/python3.7/unittest/runner.py"", line 176, in run      test(result)    File ""/miniconda3/envs/deep/lib/python3.7/unittest/suite.py"", line 84, in __call__      return self.run(*args, **kwds)    File ""/miniconda3/envs/deep/lib/python3.7/unittest/suite.py"", line 122, in run      test(result)    File ""/miniconda3/envs/deep/lib/python3.7/unittest/suite.py"", line 84, in __call__      return self.run(*args, **kwds)    File ""/miniconda3/envs/deep/lib/python3.7/unittest/suite.py"", line 122, in run      test(result)    File ""/miniconda3/envs/deep/lib/python3.7/unittest/case.py"", line 676, in __call__      return self.run(*args, **kwds)    File ""/miniconda3/envs/deep/lib/python3.7/unittest/case.py"", line 628, in run      testMethod()    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/absl/testing/parameterized.py"", line 265, in bound_param_test      test_method(self, *testcase_params)    File ""/data/Code_hay/disentanglement_lib/disentanglement_lib/methods/unsupervised/train_test.py"", line 92, in test_train_model      gin_bindings)    File ""/data/Code_hay/disentanglement_lib/disentanglement_lib/methods/unsupervised/train.py"", line 53, in train_with_gin      train(model_dir, overwrite)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/gin/config.py"", line 1050, in gin_wrapper      return fn(*new_args, **new_kwargs)    File ""/data/Code_hay/disentanglement_lib/disentanglement_lib/methods/unsupervised/train.py"", line 123, in train      steps=training_steps)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 2871, in train      saving_listeners=saving_listeners)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 367, in train      loss = self._train_model(input_fn, hooks, saving_listeners)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1158, in _train_model      return self._train_model_default(input_fn, hooks, saving_listeners)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1188, in _train_model_default      features, labels, ModeKeys.TRAIN, self.config)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 2709, in _call_model_fn      config)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1146, in _call_model_fn      model_fn_results = self._model_fn(features=features, **kwargs)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 2967, in _model_fn      features, labels, is_export_mode=is_export_mode)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 1549, in call_without_tpu      return self._call_model_fn(features, labels, is_export_mode=is_export_mode)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 1867, in _call_model_fn      estimator_spec = self._model_fn(features=features, **kwargs)    File ""/data/Code_hay/disentanglement_lib/disentanglement_lib/methods/unsupervised/vae.py"", line 43, in model_fn      z_mean, z_logvar = self.gaussian_encoder(features, is_training=is_training)    File ""/data/Code_hay/disentanglement_lib/disentanglement_lib/methods/unsupervised/vae.py"", line 93, in gaussian_encoder      input_tensor, is_training=is_training)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/gin/config.py"", line 1050, in gin_wrapper      return fn(*new_args, **new_kwargs)    File ""/data/Code_hay/disentanglement_lib/disentanglement_lib/methods/shared/architectures.py"", line 53, in make_gaussian_encoder      is_training=is_training)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/gin/config.py"", line 1050, in gin_wrapper      return fn(*new_args, **new_kwargs)    File ""/data/Code_hay/disentanglement_lib/disentanglement_lib/methods/shared/architectures.py"", line 173, in conv_encoder      name=""e1"",    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 324, in new_func      return func(*args, **kwargs)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/layers/convolutional.py"", line 424, in conv2d      return layer.apply(inputs)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1479, in apply      return self.__call__(inputs, *args, **kwargs)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/layers/base.py"", line 537, in __call__      outputs = super(Layer, self).__call__(inputs, *args, **kwargs)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 634, in __call__      outputs = call_fn(inputs, *args, **kwargs)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py"", line 146, in wrapper      ), args, kwargs)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py"", line 446, in converted_call      return _call_unconverted(f, args, kwargs)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py"", line 253, in _call_unconverted      return f(*args, **kwargs)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py"", line 196, in call      outputs = self._convolution_op(inputs, self.kernel)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 1079, in __call__      return self.conv_op(inp, filter)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 635, in __call__      return self.call(inp, filter)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 234, in __call__      name=self.name)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 1953, in conv2d      name=name)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 1071, in conv2d      data_format=data_format, dilations=dilations, name=name)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper      op_def=op_def)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func      return func(*args, **kwargs)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3616, in create_op      op_def=op_def)    File ""/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 2005, in __init__      self._traceback = tf_stack.extract_stack()      In call to configurable 'model' ( )    ----------------------------------------------------------------------  Ran 13 tests in 52.660s    FAILED (errors=4)  "
Also the `.npz` file listed in    is not to be found anywhere on the web.
The newest tf 1.14 release removes tensorflow.python.ops.linalg.linear_operator_util.matmul_with_boradcast.  This causes the tests to fail and I can't run the example code. 
which modification should I make?
"In the official TensorFlow blog posts, it was said that `tf.layers` would be depreciated and `tf.keras.layers` would be the preferred method. However in _architectures.py_  `tf.layers` has been used everywhere to create the encoder and decoder models. It would be great if these could be modifed to use `tf.keras.layers` since others would be able to reuse the code without worry.    #feature request"
     The Beta-VAE paper states that all the convolutional kernels are of size 4x4. I am not sure if it is intended that the 3rd and 4th convolutional layers in the `conv_encoder` function reduce these kernel sizes to 2x2?           
"I am sorry to say so, but I am struggling to modify the codes."
"Dear Google research group:    Thanks for the great implement, I have a related question.    In DCI metric, dose the dimention D of code C (in 2 THEORETICAL FRAMEWORK of the paper) affect the output number? If so, how it affect disentanglement, completeness and informativeness? Can I use this metric to compare two latent codes with different length?    Thank you very much for your help.    Best Wishes,    Alex"
It seems that the latent traversal animations depend on the ground truth data means for sampling â€” why is that? Why not sample latents directly and then use the mean representation (from postprocessing) without relying on ground truth statistics?    I might not be completely understanding that portion. Thanks!     
"I got a error in running`pip install .[tf_gpu]`:  `tensorboard 1.14.0 has requirement setuptools>=41.0.0, but you'll have setuptools 40.8.0 which is incompatible.`  what should I do?"
"I'm having trouble installing everything. Using setup.py installs tf2.0, which gives me several errors when I run `dlib_tests`. Can the authors please share a requirements.txt file / fix the setup.py file?"
"Hello,    I am writing about the  . Should the two values in `discr_loss` be negative? It seems like the `probs_z[:, 0]` is the probability that the sample comes from the joint distribution, but the line that I linked would encourage the values in `probs_z[:, 0]` to go towards 0 instead of 1."
"Hi,    the latest changes in v1.1 introduced an assertion that prevents Factor-VAE to be evaluated if the number of dimensions of the representation is not equal to the number of latent variables of the dataset:         I am not 100% sure, but I think the metric should also be computable in the other case.    Again, this is problematic for the disentanglement challenge. As it ends this Friday, a quick fix would be appreciated :-)"
"When models are evaluated with factor-vae metric, `num_variance_estimate` (default 10000) samples are fed to `representation_function` (  So, users having ready-made NVIDIA GPUs cannot deal with 10000x64x64x3 inputs as a single batch. I tested with GTX TITAN Xp with 12GB memory and Tensorflow 1.13.1. Please modify the code to extract representations with `batch_size` and concatenate them."
"Hello,   when I try to run your code with the default arguments:   `python main.py --expID 004 --td --bu --morphologies hopper`    I get the following error:  `    raise error.NameNotFound(message)  gym.error.NameNotFound: Environment 'environments:hopper_3' doesn't exist.  `  Do you know what could cause this issue?"
"Very interesting work! Thanks for sharing the code.     I run into an issue when setting the max_timesteps=20000,     To reproduce it:    ` python main.py --expID 002 --td --bu --morphologies walker_7_main --max_timesteps 20000`    It looks the training is finished, but an error was produced at the end:   "
"Hi, Wenlong. Thanks for sharing your code.    When I reproduced your code, I found that only one CPU was used for training. The training speed was a bit slow. Can your code train with multiple CPUs in parallel? I can't find the corresponding option in your code configuration."
"Hello Wenlong,  Thanks for sharing your interesting work!    I have run some experiments with your code, but I think there are some typo in   in environment .py files.  Specifically, for humanoid++, the   is assigned incorrectly, i.e. every limbs are assigned to  .    In humanoid xmls, the name of limb   belongs to {'torso', '(left/right) shoulder', '(left/right) thigh', '(left/right) shin', '(left/right) upper arm', '(left/right) lower arm'}, but the limb type assignment condition compares the name with {'hip', 'knee', 'shoulder', 'elbow'} which are the names of   in your code.    I'm wondering if this is intended and would like to hear from you if this makes some difference in model performance.    Thank you        "
"SubprocVecEnv uses multiprocessing which means utils.makeEnvWrapper will fail in     def helper():          e = gym.make(""%s-v0"" % env_name)          e.seed(seed)          return wrappers.ModularEnvWrapper(e, obs_max_len)    gym.make can't find the registered env. Because registry in gym.envs.registration will initialize again in subprocess."
"Hi @huangwl18, @pathak22! Thanks for the code release.    The paper does not specify which environments were used for training and for zero-shot evaluation. For instance, Humanoid++ has 8 environments out of which the two were used for zero-shot evaluation. Can you tell me which?    Can you, please, provide the full list for all of the environments used in the paper?"
"How many steps does it take to train a 3D model in this way? Your idea is very innovative, and I want to use this to improve my policy."
What is the advantage of using torchfold for trainning?
"Hi,    I'm wondering if you can provide an example of how to use early stopping in the training. I'n not sure what data type or class type is needed to pass into the   argument in   method. I do not fully understand what line 227 in   do. Thanks!"
"Your Cox-STG model is not compatible with some methods in the super class.    For example, if I train a Cox-STG model and want to get the risk prediction (for clinical usage or compute downstream survival curves with Breslow estimator). neither     nor     works.    This problem can be reproduced by adding these two lines at the end of the   file.      I use the following temporary code to predict the risk score:     but I still would be very appreciate if you can fix this problem"
"Hi,    Could please please update the survival dataset (and the corresponding function 'load_cox_gaussian_data()') to the latest pip version?    And also, I see there is a key called 'hr' in the gaussian_survival_data.h5 other than the three standard survival components 'x', 't', and 'e'. What does this ""hr"" stand for?     Best,  Shi-ang"
   could not get the same result when I repeat only  above code. 
Hello!  I wanted to ask if by any chance you have pre-trained model files available that you are willing to share.  Thanks!
None
"Trying to run the command   python3 train_taskonomy.py -d=/taskonomy-sample-model-1-master/ -a=xception_taskonomy_new -j 4 -b 96 -lr=.1 --fp16 -sbn --tasks=dnerac -r    but  found 0 training instances.  <img width=""482"" alt=""image"" src=""     Kindly help me out"
"Dear authors,    Thanks for your great work!    I am trying to replicate your results but encounter the problem that the training procedure encounters unexpected exits due to the following lines of codes.         The training will stop if the learning rate is too low.  While typically, we only conduct such an early stopping based on the validation metric instead of the learning-rate decay.    I am wondering whether there is high risk of producing underfit models.    Thanks,"
"Thanks for such a great paper.    I cannot reproduce the relative performance gains shown in the tables of your paper with ""results_xxx.txt"" files in      I wonder whether it is because I do not use the single-task loss obtained on 1/2-SNT networks.  Do you release those losses?"
Hi could you please share the script using which you visualised the error maps in the paper? Thanks in advance!
"When I tried to train with the following command     I got the following error:     After searching the error on Google, I found the problem is due to the wrong dataset path.      I got taskonomy dataset by     rename the folder name from taskonomy-sample-model-1 to taskonomy  and move the folder inside taskgrouping folder, so the folder structure showed as below:     Do I miss any steps? or using the wrong dataset?"
"Hi, and thanks for the great paper! Will you be releasing all the 31 models for all the possible combinations of the 5 tasks? It'll be extremely useful for my research.    "
"""REMEMBER to set your image size to 3x299x299 for both test and validation"" in      Is it still applicable? I didn't find the hints to set such image size in source code or README."
"Hi, thank you for the work. Are there any specific reasons that the network selection is implemented in C++?"
"Hi, @c-hofer ,    After generating the experiment configurations file, I run the following command as guided:         However, it stays here without any further output. The GPU usage is around 3GB.    Any hints to fix this issue?    Thanks~  "
"Hello. I understand that you move the source text vector to either 1 std, 2 std and so on based on the type selected. But could you please let me know how do you make sure that the manipulation lands the sentence to the target style?"
"Hi,   I'm trying to run the repo code with the Amazon and Yelp datasets before trying it on some of my own. I am running into the following error with the Amazon dataset and the baseline model. (I set `torch.autograd.set_detect_anomaly(True)` beforehand.)       I am trying to debug it on my end, but any help would be appreciated!   (P.S: I get the same error with the baseline and CP-VAE models on my own datasets as well)    Thank you!"
"I'm interested in this work. But I found that the \gamma parameter of the RBF kernel has a big impact on the approximation performance of the RBF kernel. When reducing the parameter sc in run_kernel_approx.m to 1.0, the performance of polytensorsketch is poor. Did I misunderstand something?  But looking at the curve of \gamma in Figure 1(a) in the paper, it shows this phenomenon.   "
If possible can you also include the code used to compute the bleu score of the model? Such as those produced to replicate the results in Figure 3 and 4 in the paper.
"Hi, are there weights available somewhere for some or all of these trained models?    Thanks!"
bus error (core dumped)   
"Hi, thank you for such a great implementation. I found a little error:     It's easy to fix and I've already fix it locally. So, please, fix it in the next commit. Thank you!"
I read some papers related to the Language model.   And I saw some models use KenLM for calculating normal/reverse perplexity  i.g.      But you use your model to calculate perplexity .       So I think we cannot calculate reverse perplexity only with this repository.  Can you give me some instructions to calculate reverse perplexity?
"The docstring of `TransformerSenderReinforce` mentions that the `max_len` parameter includes the EOS token:      However, the transformer can create a message of `max_len` (without EOS) and the EOS token will be appended afterwards:      So, to my understanding, the `max_len` parameter does actually not include the EOS token?"
Plot function doesn't work for   cause it returns n images where          ## Steps to Reproduce  1. run   untill cell # 26    ## Possible Implementation  Taking last image in   would help     
When using a minimal logging strategy no fields except aux are defined in the generated interaction. If running a distributed training this causes gathering interaction to fail      ## Steps to Reproduce  Run a distributed training with a minimal logging strategy
"def find_lengths(messages: torch.Tensor) -> torch.Tensor:      """"""      :param messages: A tensor of term ids, encoded as Long values, of size (batch size, max sequence length).      :returns A tensor with lengths of the sequences, including the end-of-sequence symbol   (in EGG, it is 0).      If no   is found, the full length is returned (i.e. messages.size(1)).    This leads to counterintuitive behaviour in which, if max_len is 3, [1, 2, 3] and [1, 2, 0] have the same length."
"max_len sets max length without considering the extra mandatory EOS at the end, however the length field in the Interaction object stores average length considering EOS. Consequently, if messages are always of maximum length, we have a somewhat surprising behaviour in which given max_len==N, the average length in the length field might be N+1"
"If I understand correctly, the   command line parameter defines the number of symbols that the agents use for communication, _including_ the EOS symbol (0). Is this intentional? Say, we want agents to communicate about 100 distinct values (and multiple attributes), intuitively we'd need  to set the vocab size to minimally 101 to have allow the agents to communicate successfully? (given max_len = # attributes)."
The function to dump interaction does not return all of them but just last one due to a typo in a variable name      This function should be somehow merged with the static method in this callback      (h/t @davidelct)
Typo in filename      (h/t @davidelct)
None
Adding option for one-hot standard attributes-values    ### Describe the solution you'd like to have implemented  Having a boolean option to have reconstruction and discrimination datasetas as one-hotified.  Standard game using attributes-values dataset use one-hot vector rather than discrete categorical input.  We would need to support such features with our standard attrValues EGG dataset
"When storing the interaction the current callback does not the case where interactions are not aggregated and training happens with multiple GPUs       ## Expected Behavior  Both single- and multi-gpu training setup should work well when storing interactions    ## Current Behavior  Right know, if interactions are not aggregated and InteractionSaver is used only the leader process will save data, causing all the date in the other gpu to be lsot.  "
## Expected Behavior  No errors    ## Detailed Description  When using the   with   the  training raises a runtime error:       Where 11 is my max_len.  This is due to the additional dimension concatenated   which brings `x` to be of dimension 12 while `t` is still of dimension `max_len=11`.    What is the reason for that additional zero at the end of the sequence?  
"### Is your proposal related to a problem?    Discriminating between few objects is a standard language emergence setup: Sender gets an object, sends a message, and then Receiver has to choose it among a set of distractors, based on a message.     At the same time, a somewhat similar idea is used in self-supervised learning: an Encoder net learns a representation of an image. Then another, smaller net (projection head), projects that learned representation to some space and in this space, a discriminative loss is applied. SimCLR is close to this setup (modulo applying augmentations).    In this issue I propose to implement SimCLR as an EGG game on images, say as `EGG/zoo/simclr`.    ### Describe the solution you'd like to have implemented    We probably have no need for data augmentation at the beginning.    * v0: SimCLR on MNIST. At the first step, Sender would get an image from MNIST, send a message and Receiver would need to chose it, using the rest of the batch as negatives. Here we'd have three options for the channel optimisation: using continuous messages (expect that agents would successfully overfit),  Gumbel-Softmax, and REINFORCE.  MNIST autoencoder can be used as an starting code      The only (moderately) tricky part would be implementing the contrastive loss.    We can use prediction head's accuracy as an evaluation metric here.     Probably, it will work better if Sender has some form of conv. architecture (LeNet3)?    We'd expect that (a) agents can overfit perfectly with continuous channel (in the absence of augmentations), (b) achieve some non-chance accuracy with Gumbel-Softmax and REINFORCE.    * v1: ImageNet instead of MNIST. Linear probes as an evaluation. ResNet as an underlying architecture.  * v2: Augmentations. Approaching the actual SimCLR in performance with continuous channel.      ### Useful resources ###  [1] SimCLR    [2] Some implementation of SimCLR on github     for inspiration; a blogpost    "
"## Expected Behavior  If the user didn't install the tensorbard lib correctly, then the   should raise an error.     ## Current Behavior  Right now the function prints an warning, but if you use  the  , at the end of the training epoch   will trow an error since `self.writer` is None.   For long epochs this is a waste of time     ## Possible Implementation  Either change    with a raise error or add an assertion of the kind      in the    "
### Is your proposal related to a problem?    It would be nice to have to option to log metrics as the epoch proceeds during the training. this is useful for those epochs which requires hours to complete.    ### Describe the solution you'd like to have implemented  Maybe adding something of this kind after         And add it to the   as:       
### Is your proposal related to a problem?  I'm working with the coco dataset which uses as label a dictionary of elements.  I would like to pass such dicts to Sender/Receiver and logger (with aux_info).      ### Describe the solution you'd like to have implemented    The solution is quite easy. Just add kwargs    as:   
### Is your proposal related to a problem?         Merging posdis and bos might be useful to reduce the number of callbacks to pass to the trainer (h/t @eugene-kharitonov)    ### Describe the solution you'd like to have implemented         Have a singel Disent callback (optionally) computing both metrics     ### Additional context         It has a very low priority. Good as a first issue for someone that wants to understand the logic behind the callbacks
"## Is your proposal related to a problem?       I am working with the    and the   architecture in a similar fashion to the  .  Both my sender and receiver get as input an image of size  , which saves the required infos for every batch of an epoch.  This causes the memory to quickly fill with images and explode.    In my case with:  - 32GB of RAM  - Batch size of 32    It takes around 20 batches to slow my machine down and 25 to be killed by the system.      ## Describe the solution you'd like to have implemented         Since the interaction saving is done in the  , inside the  , it can be tackled in two ways which I will now consider.    ### Inside the batch loop  The easiest fix would be discard the interaction inside the batch loop according to some metrics which can be either stochastic, e.g. discard interaction _i_ with probability _p_, or batch dependent, i.e. keep interaction _i_ every _n_ batches.    ### With the  LoggingStrategy  Although the previous fix sounds easier, it is not modular since it does not use the LoggingStrategy class.  So a much cleaner way is to allow the user to define a custom LoggingStrategy and use it to filter the interaction in the  .  In this case the only available solution would be the stochastic one, since no information about the current batch id is passed to the forward pass.    #### Necessary modification  To achieve the above mentioned result the LoggingStrategy class should change the `store_*` attributes from bool to functions.  These function can be either:  - passed to the class at `init` time if no other information are needed, e.g. the stochastic approach.  - set to be equal to some custom methods otherwise  "
"### Is your proposal related to a problem?    REINFORCE works best with large batch sizes; however, if you have a small GPU or use a Google Colab one, your CUDA-mem is limited.    ### Describe the solution you'd like to have implemented    Support training / eval in fp16.    ### Additional context  v0 implementation is as easy as calling `.half()` on the models. The trickier part is to make sure that there is no loss underflow in optimizers. Fresh pytorch supports auto-scaling of losses in optimizers when working with fp16 (which was before done with nvidia's mp). We'd need to double-check we use that and potentially bump pytorch version used.  "
"### Is your proposal related to a problem?    Discriminative emergence games are very close to the idea of contrastive predictive coding in computer vision, and it would be cool to support code for comparative experiments in EGG, in particular along the lines of      ### Describe the solution you'd like to have implemented    Add code to support experiments with learning image representations using a discrete channel communication game.  "
"  ## Expected Behavior  $ pytest   ...  all tests passed, no warning    ## Current Behavior  $pytest  ...  /private/home/mbaroni/anaconda3/envs/the_veritable_egg/lib/python3.7/site-packages/torchvision/io/video.py:2    /private/home/mbaroni/anaconda3/envs/the_veritable_egg/lib/python3.7/site-packages/torchvision/io/video.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses      import imp    -- Docs:    ========================================================================= 45 passed, 1 warning in 81.34s (0:01:21) =========================================================================    ## Steps to Reproduce  Run  $ pytest  in EGG root directory      ## Possible Implementation  avoid ""import imp""?    "
"### Is your proposal related to a problem?    Right now, computation of edit distance relies on editdistance pkg: we would like to get rid of this dependency.    ### Describe the solution you'd like to have implemented    Implement our own edit distance function.    ### Additional context    Remember this is a textbook example of a problem to be solved with dynamic programming.  "
Implement batch as a namedtuple/dataclass object (allows more flexibility for optional fields in a batch -- use cases: task embedding and potential additional info like round embedding in a multi-round setup).  
Implement bag of words disentanglement from Chaabouni et al 2020
Add small progress bar that monitorsÂ train progress when using command line interface-based training in EGG.
"In the trainer file a validationÂ loop refers to validation data etc, in the callbacks we have a method called on_test_end. Maybe it should be named on_validation_end?  "
"### Is your proposal related to a problem?    Right now, the useful logging information stored in the Interaction class is only available as a Python object. It would be good to have a standard text format for information exchange.    ### Describe the solution you'd like to have implemented    Provide function printing/reading Interaction information from text file.    "
"### Is your proposal related to a problem?    Right now, lots of work is repeated every time NEST output is post-processed.    ### Describe the solution you'd like to have implemented    Design an integrated tool, nest-aggregate, that collects results and stats from egg experiments --> a.k.a. automating all the boring text file processing through python script that we do after every nest launch)  "
"### Is your proposal related to a problem?    More than a problem, a minor functionality that would be handy to have: the possibility to track vocabulary usage during training.  "
Support for games involving multiple exchanges between the agents.
Support the possibility of playing games with multiple senders and receivers.  
"### Is your proposal related to a problem?    Right now, language analysis is implemented with callback functions that operate as part of the training process. It would be cleaner and more efficient to decouple it, performing language analysis on trained agents.    ### Describe the solution you'd like to have implemented    After training, it's possible to save Sender and Receiver and query them with new data for language analysis. The language analysis functions generically operate on data structures such as message and sender_input, independently of their source.    ### Additional context    Solving this would probably also solve issue #148 (test set): merge?    "
"Both as a baseline and to consider communication more generally, we should implement a third class of wrappers to play a game where the agents directly send a continuous channel."
"### Is your proposal related to a problem?    EGG seamlessly integrate validation, but there is no straightforward way to include a test set.    ### Describe the solution you'd like to have implemented    My favorite solution would be to have an easy way to save the trained agents, so that after training we can feed them arbitrary test sets.    ### Describe alternatives you've considered    Perhaps the simpler alternative would be to have a test option in the training, taking a further test set as option. However, it seems like a waste of resources to have to do a full re-training every time we change the test set.    "
"Passing an-already computed lengths of a message is a micro-optimization.     Apparently, it is lost in here:       Thanks @robertodessi for spotting!"
"Hi, I was trying to make Signaling Game work but encountered a lot of problems trying to do so. I have the original data. Is it working? If it does, which versions of Python Torch, or other dependencies should I use? "
Hi all!    I've been trying to play with the MNIST example in Google Colab but it seems EGG assumes Python 3.7 or higher and Google Colab is still 3.6 :(    I'll try with an older version of the repo.     Thanks
"Something like a custom name for model checkpointing is currently not well supported in EGG.       can be achieved with a CheckpointSaver callback when creating the Trainer instance and passing parameters `checkpoint_path=opts.checkpoint_dir` and `prefix=f'{custom_name}'` to the CheckpointSaver instance.    However, if `--preemptable` is set, another CheckpointSaver with the same `checkpoint_path` parameter will save checkpoints in the same `--checkpoint_dir` folder (see  ), resulting in duplicate checkpoints that have the different names in the same folder.    I was thinking maybe we could have CheckpointSaver NOT created by Trainer by default when `--checkpoint_dir` is set but only when it is set together with `--preemptable`, and have users define CheckpointSaver in their own game. This could still result in duplicate checkpoints in the same folder if `--preemptable` is set AND the users define their own CheckpointSaver.    Another possible solution is to have a check when initializing Trainer: if an instance of CheckpointSaver is present in the list of callbacks, then regardless of the `--preemptable` flag another CheckpointSaver will NOT be generated.    I find the latter simpler and cleaner and am happy to do it if you agree @eugene-kharitonov   "
Datasets representing a cartesian product of n_attributes having n_values are used quite often. We could provide a general implementation.
Is the dim_dateset ever used in the channel game?      It seems like we could safely remove it
After refactoring the code with Callbacks as classes for the Trainer it seems like the channel game wasn't updated properly:       That line raises an exception as a trainer doesn't have a save_checkpoint method. It is needed only if we want to save the model with a custom name rather than XX.tar (with XX the number of epochs).    
the argument `checkpoint_dir ./models/mymodel` does not work if the `models` directory doesn't exist 
enabling `force_eos` breaks code when using `max_len=1` in `RnnSenderGS` (and maybe other wrappers too) since it decreases the `max_len` to `0`. Although ideally `max_len` should be `>1` when using `RNnnSenderGS`.
The `device` param in `core/util` should be a `torch.device` object rather than an `str` since it breaks the functionality in `core/trainers.py`.
`TensorboardLogger` callback is not added to the `core/trainers.py` file so enabling it doesn't add stuff to the `SummaryWriter`.
The example command here does not seem to work:     with the possible tuples less than the total size of the dataset. `train_samples` should be changed to 1e5 or less.
"#13 this PR is not compatible with the changes done in #11 .  In particular, #11 expects the loss value as a tensor (which shouldn't be the case) but passing float value breaks it."
All the GS wrappers other than Gumbel Softmax are broken if temperature updating is enabled.   
"Hi,  I ran into a problem of loading the h5 dataset. Previously I tried and it worked. But after I ""pip install -e . "", it has a problem as below. I am NOT sure if that's the reason for it, just FYI.       Traceback (most recent call last):    File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py"", line 193, in _run_module_as_main      ""__main__"", mod_spec)    File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py"", line 85, in _run_code      exec(code, run_globals)    File ""/Users/usr/Documents//Emergent Communication/EGG-master/egg/zoo/signal_game/train.py"", line 87, in        dataset = ImageNetFeat(root=data_folder)    File ""/Users/usr/Documents/Emergent Communication/EGG-master/egg/zoo/signal_game/features.py"", line 120, in __init__      labels = pickle.load(f)  _pickle.UnpicklingError: invalid load key, '\x00'."
"Trainer calls a global CLI handler on initialization:       In the course my work with EGG I found this design choice to be very limiting: it effectively couples the `Trainer` to pre-defined common arguments that are not universally applicable (e.g. for some complex games the experiment consists of several trainer instances arguments like `num_epochs` are misleading). It would also be nice to be able to set own defaults. I would like EGG to give me the possibility of taking care of hyperparameters on my own and setting them per trainer. More concretely, could we pass configuration like checkpointing frequency, path, validation frequency as argument to `Trainer.__init__`? Is it compatible with the way you would like to see EGG being developed? If so, I'd be happy to prepare a pull request.  "
"Hi, I am totally new to this project and I encountered an issue when I try to run signal_game.train. The issue remains even after I specify vocabulary size. Can anybody give me some instructions?    `(env) pengfeihe@D-10-18-202-82 EGG-master % python -m egg.zoo.signal_game.train  Traceback (most recent call last):    File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py"", line 193, in _run_module_as_main      ""__main__"", mod_spec)    File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py"", line 85, in _run_code      exec(code, run_globals)    File ""/Users/pengfeihe/Library/Mobile Documents/com~apple~CloudDocs/Programming Language/Python/Emergent Communication/EGG-master/egg/zoo/signal_game/train.py"", line 85, in        opts = parse_arguments()    File ""/Users/pengfeihe/Library/Mobile Documents/com~apple~CloudDocs/Programming Language/Python/Emergent Communication/EGG-master/egg/zoo/signal_game/train.py"", line 41, in parse_arguments      opt = core.init(parser)    File ""/Users/pengfeihe/Library/Mobile Documents/com~apple~CloudDocs/Programming Language/Python/Emergent Communication/EGG-master/egg/core/util.py"", line 96, in init      arg_parser = _populate_cl_params(arg_parser)    File ""/Users/pengfeihe/Library/Mobile Documents/com~apple~CloudDocs/Programming Language/Python/Emergent Communication/EGG-master/egg/core/util.py"", line 55, in _populate_cl_params      help='Number of symbols (terms) in the vocabulary (default: 10)')    File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/argparse.py"", line 1367, in add_argument      return self._add_action(action)    File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/argparse.py"", line 1730, in _add_action      self._optionals._add_action(action)    File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/argparse.py"", line 1571, in _add_action      action = super(_ArgumentGroup, self)._add_action(action)    File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/argparse.py"", line 1381, in _add_action      self._check_conflict(action)    File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/argparse.py"", line 1520, in _check_conflict      conflict_handler(action, confl_optionals)    File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/argparse.py"", line 1529, in _handle_conflict_error      raise ArgumentError(action, message % conflict_string)  argparse.ArgumentError: argument --vocab_size: conflicting option string: --vocab_size`    "
Isn't the step loss calculated the first time the agent produces an eos token? and then everything after that is masked.  What this is doing is checking if eos was emitted in just the last timestep.     
None
Do you assume here that the input/output sequence length is the same as the message length? or bounded by it     
Can we get up with a GUI so that game developers can easily use this especially the Windows users?  It's just an idea and we can discuss that.
"I found two bugs in the objects game:    1. Temperature updating does not work, because the `callback` variable is assigned a `None` here.       #17 will fix this.    2. `_fill_split` falls into infinite loop for some inputs, e.g. `perceptual_dimensions = [10, 10]`.   "
"I really love the library, find it extremely well-designed and started using it for some of my own research. One thing that bothers me is lack of extensibility of trainer with respect to monitoring  and learning rate scheduling. I'm thinking, for instance, custom monitors and loggers other than Tensorboard. The `_message` function in `core.Trainer.train` is not accessible outside of the `train` method and the `epoch_callback` is of little use for monitoring because the losses and `rest` dicts accessible via `self`.    Have you thought about implementing a simple callback logic, similar to the callback API found in Keras, fastai or ignite? Would a pull request adding such a thing be welcome? Or do you have some other direction for developing EGG in mind?"
Hi there.    I have tried running this code on one of my machine with two RTX3090 GPUs (GPU memory 24GB for each)    python -m torch.distributed.launch --nproc_per_node=2 script/run.py -c config/fb15k237.yaml --gpus      !     Could you help me solve this problem? Is there something wrong with my setups?     
"Hi! This work is excellent so I want to reproduce the codes as soon as possible. However, when I try to reproduce the codes, I find there are some bugs such as the parameter mismatch when loading checkpoint:    `checkpoint['model']['graph']` is expected to be a tensor or tensor-like but get a `graph.Graph`    After trying, I thought maybe it was related to **the version of PyTorch**. After rolling back the PyTorch version from 1.12.1 to 1.10.1 (and reinstalling the torch-scatter), the bug is solved.  I believe if you are willing to test this issue and give some instructions, it can help more people who love this job like me.        "
"hi, thanks for the excellent work.  but i really met some problem on reproducing the code    the error is:       it has puzzled me for several days, i see in the NBFnet issues, other people also met the ninja issue on a linux machine.     In my situation, I am on a windows10 rtx3080 machine. with torchdrug fixed to 0.1.13 i have tried different  version of python, torch, but met the same ninja problem.    wishing for some advice to help me out*-*"
"It looks like the only learnable part of your proposed algorithm is the mapping operation, the GNN model. The logical operations use fuzzy logic and do not require learning. So, why not use the trained GNN model directly in the GNN-QE framework?"
  I get an OOM error with the visualize script:    RuntimeError: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 22.20 GiB total capacity; 19.53 GiB already allocated; 4.06 MiB free; 20.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF    Running via:  ` python script/visualize.py -c config/fb15k237_visualize.yaml --checkpoint ~/scratch/experiments/LogicalQuery/FB15k237LogicalQuery/GNN-QE/2022-09-18-13-34-00/model_epoch_10.pth`    I was able to run training ok after adjusting the batch_size in the config.    Any advice on adjusting config for the visualize script to avoid the OOM error?  I'm testing this in an AWS environment with 24G GPUs.            
"Hello,  I was going through your paper and through the paper of SGHMC and I understand that :  - SGD + noise equates to SGLD  - SGD + momentum + noise equates to SGHMC  However, I don't understand what Adam + noise which is what you used in your code equates to? in the paper you said that you were going to use SGHMC. Is it reasonable to assume that Adam + noise is also equivalent to SGHMC? if so, can you please say why?    Thanks  "
"Hello,  I want to run the unsupervised BGAN with MNIST dataset using this command:  `./run_bgan.py --data_path datasets --dataset mnist --num_mcmc 2 --out_dir ./results/ --train_iter 75000 --save_samples --n_save 100`  But I got an error:     I printed d_losses  and got [None]. Any help?  Thank you"
"Hello, I might be missing something obvious but I've been looking at it in every way possible and I can't grasp what I would be missing.     In the guide you submitted, a script, which is quite central to most of the things we can do with your repository, seems to be missing. It's the bayesian_gan_hmc script.    Do you know where I could find it? It would be of a great help to my problem.    Thank you."
"In the testing set, basically what you are doing is first to find what images are not fake and then find the accuracy when they are classified as real.    So the accuracy can be really high when there is only a small number of images classified as real, which is kind of like cheating."
None
"If I have generated numpy matrices as my ""real data"", do I specify that that as 'self.imgs = np.load('matrices.npy')'?   I intend to run this in unsupervised mode, so do I need to supply the other parameters such as self.labels, self.test_imgs and self.test_labels?   Sorry if this is a basic question, I am fairly new to GANs. "
In lines 67-70:   `  I believe what you intend to have here is dcgan.d_optims and dcgan.g_optims
None
"I am struggeling to run your semi-supervised code on CIFAR. I have followed the README and set up tensorflow in conda accordingly with correct versions. When I run (as close as possible from the README):    ~~~bash  ./run_bgan_semi.py --data_path ./datasets/ --dataset cifar --num_gen 10 --num_mcmc 2 --out_dir cifar_out --train_iter 100000 --N 4000 --lr 0.0005  ~~~    I get:    ~~~bash  Iter 100  d_losses: [None]  disc_info: [None, None, 7.3618059, 7.3462958]  Traceback (most recent call last):    File ""./run_bgan_semi.py"", line 419, in        b_dcgan(dataset, args)    File ""./run_bgan_semi.py"", line 214, in b_dcgan      print ""Disc losses = %s"" % ("", "".join([""%.2f"" % dl for dl in d_losses]))  TypeError: float argument required, not NoneType  ~~~    This only happens at the 100th iteration (and I have printed the respective variables to show that there are unexpected Nones in there), so I guess the None types in the d_losses are not a problem before. Any ideas? Thanks for any help :)."
"I am using tf1.3ï¼Œit shows as follow, how to solve this problem?  Traceback (most recent call last):    File ""I:/python3/bayesgan-master/run_bgan_semi.py"", line 413, in        b_dcgan(dataset, args)    File ""I:/python3/bayesgan-master/run_bgan_semi.py"", line 141, in b_dcgan      num_classes=dataset.num_classes)    File ""I:\python3\bayesgan-master\bgan_semi.py"", line 78, in __init__      self.build_bgan_graph()    File ""I:\python3\bayesgan-master\bgan_semi.py"", line 274, in build_bgan_graph      self.K, disc_params)    File ""I:\python3\bayesgan-master\bgan_semi.py"", line 401, in discriminator      w=disc_params[""d_h%i_W"" % layer], biases=disc_params[""d_h%i_b"" % layer]), train=train))    File ""I:\python3\bayesgan-master\dcgan_ops.py"", line 46, in __call__      scope=self.name)    File ""D:\ProgramData\tensorflow\lib\site-packages\tensorflow\contrib\framework\python\ops\arg_scope.py"", line 181, in func_with_args      return func(*args, **current_args)    File ""D:\ProgramData\tensorflow\lib\site-packages\tensorflow\contrib\layers\python\layers\layers.py"", line 592, in batch_norm      scope=scope)    File ""D:\ProgramData\tensorflow\lib\site-packages\tensorflow\contrib\layers\python\layers\layers.py"", line 373, in _fused_batch_norm      collections=moving_mean_collections)    File ""D:\ProgramData\tensorflow\lib\site-packages\tensorflow\contrib\framework\python\ops\arg_scope.py"", line 181, in func_with_args      return func(*args, **current_args)    File ""D:\ProgramData\tensorflow\lib\site-packages\tensorflow\contrib\framework\python\ops\variables.py"", line 262, in model_variable      use_resource=use_resource)    File ""D:\ProgramData\tensorflow\lib\site-packages\tensorflow\contrib\framework\python\ops\arg_scope.py"", line 181, in func_with_args      return func(*args, **current_args)    File ""D:\ProgramData\tensorflow\lib\site-packages\tensorflow\contrib\framework\python\ops\variables.py"", line 217, in variable      use_resource=use_resource)    File ""D:\ProgramData\tensorflow\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 1203, in get_variable      constraint=constraint)    File ""D:\ProgramData\tensorflow\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 1092, in get_variable      constraint=constraint)    File ""D:\ProgramData\tensorflow\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 425, in get_variable      constraint=constraint)    File ""D:\ProgramData\tensorflow\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 394, in _true_getter      use_resource=use_resource, constraint=constraint)    File ""D:\ProgramData\tensorflow\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 742, in _get_single_variable      name, """".join(traceback.format_list(tb))))  ValueError: Variable discriminator/d_bn1/moving_mean already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:      File ""D:\ProgramData\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 1470, in __init__      self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access    File ""D:\ProgramData\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 2956, in create_op      op_def=op_def)    File ""D:\ProgramData\tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper      op_def=op_def)"
"Dear authors,  As can be seen from generated samples in figure 2, 6, 7 and 8 mode collapse is a serious problem in Bayesian GAN. Every generator has mode collapse and different generators collapse to the same modes.   In figure 6, for example, generator 1 and 4 both have mode collapse and they collapse to the same mode (row 2, col 3 of generator 1 and row 3, col 3 of generator 4). If we consider the mode count method based on birthday paradox (Arora et al. 2017) then when mode collapse happens with high probability then the number of mode in the model distribution is about the same as the batch size.  Mode collapse happens with batch size of only 16, that implies that each generator captures only tens of modes. The total capacity of 10 generators is, therefore, much smaller than a single generator trained with normal method.   This is contrast to your claim that Bayesian GAN explore a broader region of the target distribution. In my opinion, the current setting for Bayesian GAN makes mode collapse worse. "
"Dear authors, great work on the BayesGAN paper and code, congratulations! I had a question about your code - is there currently support for multiple discriminator MC samples (J_d > 1)? If not, is there any reason why it's left out? Thanks!"
"I'm just wondering, shouldn't we use independent batch_norm variables for each generation of the generator ? "
"Hi Andrew,  I've just run bayesian_gan_hmc.py under tensorflow 1.3.0ï¼Œhere is the error:  `  Traceback (most recent call last):    File ""/home/jqh/jiangqiuhua/eclipse/plugins/org.python.pydev_6.2.0.201711281614/pysrc/pydevd.py"", line 1621, in        main()    File ""/home/jqh/jiangqiuhua/eclipse/plugins/org.python.pydev_6.2.0.201711281614/pysrc/pydevd.py"", line 1615, in main      globals = debugger.run(setup['file'], None, None, is_module)    File ""/home/jqh/jiangqiuhua/eclipse/plugins/org.python.pydev_6.2.0.201711281614/pysrc/pydevd.py"", line 1022, in run      pydev_imports.execfile(file, globals, locals)  # execute the script    File ""/home/jqh/jiangqiuhua/Tensorflow/bayesgan-master/bayesian_gan_hmc.py"", line 442, in        b_dcgan(dataset, args)    File ""/home/jqh/jiangqiuhua/Tensorflow/bayesgan-master/bayesian_gan_hmc.py"", line 141, in b_dcgan      num_classes=dataset.num_classes if args.semi_supervised else 1)    File ""/home/jqh/jiangqiuhua/Tensorflow/bayesgan-master/bgan_models.py"", line 357, in __init__      self.build_bgan_graph()    File ""/home/jqh/jiangqiuhua/Tensorflow/bayesgan-master/bgan_models.py"", line 144, in build_bgan_graph      self.generation[""generators""].append(self.generator(self.z, gen_params))    File ""/home/jqh/jiangqiuhua/Tensorflow/bayesgan-master/bgan_models.py"", line 415, in generator      h0 = tf.nn.relu(self.g_bn0(self.h0, reuse=reuse))  ValueError: Variable generator/g_bn0/moving_mean already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:  `  Can I run this under tensorflow 1.3.0?"
"Hi Andrew,  I've just done some experiments with WGAN with Gradient Penalty (Improved Training of Wasserstein GANs, Gulrajani et al.) and found that it can converge to a reasonable solution on the synthetic dataset. Although WGANGP does not converge as fast as bayesgan, I think it would be nice if you could add WGANGP to the baselines in your experiments.  Here is the output of my (very bad) implementation of WGANGP after 8000 iterations       !     "
"In attempting run the MNIST example (with the below, most basic command) I ran into the below error.          My environment is compiled with the environment.yaml and my command to run the code is:     bayesgan/bayesian_gan_hmc.py --data_path /home/jlandesman --dataset mnist --out_dir results/mnist --save_samples --n_save 100    The synth data appears to work well.    Any thoughts?     Many thanks for your help and congratulations on the paper."
"To learn on custom data (not images), are the shapes hard-coded to 8x8 or I can change them? I essentially have flat vectors.  "
"If I run using the default config.cfg file (with N4_relu.znn) except with outsz changed from 1,100,100 to 2,2,2 I get a segfault. I checked a couple other networks which seem to have the same issue.  Here is a dump from gdb:  /home/jkopinsky/ZNN/python/test.py:9: RuntimeWarning: to-Python converter for boost::shared_ptrznn::v4::parallel_network::network already registered; second conversion method ignored.   from core import pyznn reading config parameters... net file not exist:  ../experiments/ac3/net_current.h5 initialize a new network... parse_net file: ../networks/VD2D.znn construct the network class using the edges and nodes... field of view:  (1, 109, 109) setting up the network...  create train samples...  create input image class... /home/jkopinsky/ZNN/python/tifffile.py:1898: UserWarning: failed to import _tifffile.decodepackbits   warnings.warn(""failed to import %s"" % module_function) /home/jkopinsky/ZNN/python/tifffile.py:1898: UserWarning: failed to import _tifffile.decodelzw   warnings.warn(""failed to import %s"" % module_function) /home/jkopinsky/ZNN/python/tifffile.py:1898: UserWarning: failed to import _tifffile.unpackints   warnings.warn(""failed to import %s"" % module_function) boundary mirror...  create label image class... /home/jkopinsky/ZNN/python/tifffile.py:1995: UserWarning: decodelzw encountered unexpected end of stream (code 514)   ""decodelzw encountered unexpected end of stream (code %i)"" % code)  create test samples... start training... start from  1 save as  ../experiments/ac3/net_init.h5 stdpre:  /processing/znn/train/statistics/  Program received signal SIGSEGV, Segmentation fault. znn::v4::get_segmentation (affs=..., threshold=threshold@entry=0.5) at ../../src/include/flow_graph/computation/get_segmentation.hpp:94 94                      if ( zaff[z][y][x] > threshold ) "
"Hi,  I encountered the following error when trying to open my previously trained models, is there something happened somwhere inside or outsize the `znn-release` that the models are not compatible anymore?     or with Anaconda2    "
"When training on labels that may not have any positive classes in the field of view, normalization causes an NaN to propagate through the network. Would it be possible to add an epsilon to the denominator to prevent this gift that keeps on giving?   Sachin and Nick get around this (amongst other reasons) by limiting their samples to those containing a positive class. "
"- create a   or  , it is a kind of persistent spot instance managed by AWS. define the training command by `user_data` of the spot instance. - read all the data and configuration files in S3, save network to S3. This can be implemented using `boto3` and remove the complex dependency of `starcluster`. - whenever there is an update of saved network file, plot the learning curve online using Plotly. "
@sachinravi14 just found that the current development branch is not backward compatible with old configuration format. @sachinravi14 could you post the error message here? 
Sachin found that current pixel error computation do not consider the mask:    will fix this later. 
Mask is being applied to both output activation and target label in cost functions. Applying mask to the resulting gradient (  is redundant. 
"If there exists a mask, it should be used when computing the rebalancing weight mask. Currently rebalancing doesn't take into account the mask. "
"### Problem  Currently **softmax** is applied in the frontend. There is even a separate function   in the _forward.py_. This is super ugly. We do have   in the backend, but this is nearly useless because we always need to use the **softmax** in conjunction with the **cross-entropy loss** when training. Because of this, when running _forward.py_, it applies **softmax** if the type of loss is **softmax_loss**. But what if we want to output the scores (i.e. before applying **softmax**) instead of the probabilities? There is no way except for manually changing the type of loss. Super ugly.  ### Solution 1. (quick, easy but not intuitive) Differentiate the behavior of **softmax_edges**, depending on the phase (TRAIN/TEST). In TRAIN, it's equivalent to  . In TEST, it does its job. Remove _run_softmax_. 2. (general,intuitive but takes time) Implement general flow graph. **softmax_loss** in the topmost part should be able to handle both TRAIN and TEST. ### Priority  This is not urgent.  "
"I expect that exception handling is usually supported by a C++ program. I wonder why your function "" "" does not contain   so far.  How do you think about recommendations by Matthew Wilson in  ?  Would you like to adjust the implementation if you consider   like they are described by Danny Kalev? "
"whenever there is an error of network configuration file, we get a `segmentation fault`, and have no idea what is the problem. It would be better to have some checking of the `networks/XXX.znn` file in the backend or frontend.  "
center aligned system constrained the data implicitly and is not clear for other users. Offset system is explicit and easy to understand. We could have auto center align function for offset coordinate system. This is not urgent.  
"We could add more descriptive error output in cases where we know the training will fail. For example, if the loaded network doesn't have the right number of volumes for the given output type (say 1 volume for 3d affinity output) we should explicitly tell that to the user.  This would also make it easier for us to diagnose mysterious errors, since we can rule out the obvious ones "
"when I tried to run the make command at the znn-release root dir, some error came out:    /usr/bin/ld: cannot find /usr/local/lib: File format not recognized  collect2: error: ld returned 1 exit status  make: *** [test] Error 1    the whole output in terminal is like this:    Makefile:33: warning: overriding commands for target `malis'  Makefile:22: warning: ignoring old commands for target `malis'  g++ -o ./bin/test src/cpp/malis_test.cpp -g -I./src/include -I./zi -I. /usr/local/lib -DNDEBUG -O3 -std=c++1y -DZNN_CUBE_POOL_LOCKFREE -DZNN_USE_FLOATS  -lfftw3 -lfftw3f -lpthread -pthread -lrt  In file included from src/cpp/malis_test.cpp:6:0:  ./src/include/cube/cube_io.hpp: In function â€˜znn::v4::cube_p  znn::v4::read(const string&, const vec3i&) [with F = double; T = float; znn::v4::cube_p  = std::shared_ptr  >; std::string = std::basic_string ; znn::v4::vec3i = zi::vl::vec ]â€™:  ./src/include/cube/cube_io.hpp:37:17: warning: ignoring return value of â€˜size_t fread(void*, size_t, size_t, FILE*)â€™, declared with attribute warn_unused_result [-Wunused-result]                   static_cast (fread(&v, sizeof(F), 1, fvol));                   ^  ./src/include/cube/cube_io.hpp: In function â€˜znn::v4::cube_p  znn::v4::read(const string&, const vec3i&) [with F = double; T = int; znn::v4::cube_p  = std::shared_ptr  >; std::string = std::basic_string ; znn::v4::vec3i = zi::vl::vec ]â€™:  ./src/include/cube/cube_io.hpp:37:17: warning: ignoring return value of â€˜size_t fread(void*, size_t, size_t, FILE*)â€™, declared with attribute warn_unused_result [-Wunused-result]  /usr/bin/ld: cannot find /usr/local/lib: File format not recognized  collect2: error: ld returned 1 exit status  make: *** [test] Error 1    Looking forwards to your reply! "
compiled locally and get the following error:     the first errors were gone after changing string types into char\* in C++ code.    
"I know your framework must support multi-stride convolution like the first layer in Alexnet, where the stride is set to 4 other than 1.   However, by skimming your project, I cannot understand the way you use to compute the convolution layer.  For instance, in Convolve_mkl.hpp, these configurations are only applicable for single-stride convolution.     There are two questions: 1. Can you help me understand your method in multi-stride convolution? You can present files involving the operation or some docs for further explanation. 2. Can you explain how this framework select between fft and naive method by auto-tuning?  Thank you very much! "
"I'm receiving the following error message when attempting to train a network: <img width=""558"" alt=""screen shot 2016-05-05 at 1 30 08 pm"" src=""   Switching the specified network in the config file solves the problem. Thus there's something about the network file that is likely causing the error. I've also verified that the intended network file is being called by train.py. Network spec file below. Please advise.     "
"currently, we need to download the network file to plot learning curve. It is not quite convenient, especially when we do large network training in the cloud. We need to check the learning curve and make a decision whether reduce learning rate or not.  it would be nice to plot the learning curve in terminal. here is some potential tools:    or we can use plotly to plot online, then we can check the learning curve in browser.  btw, this is totally not emergent. "
"when I use `optimize` mode in training, I get a `segmentation fault` error.      BTW, the network seed was not loaded correctly. I have created a branch to fix this but still get this error! "
"Currently, we use uniform probability among different datasets. This works fine with similar size training volume. In case the size of training datasets differs a lot, this sampling may not be appropriate. At least, we should have a choice to balance the probability of patch source according to the number of candidate locations.  To support the mask function, we should directly get probability according to number of candidate locations among different training datasets. "
Should save a config file for training along with the net instances for record keeping. 
"When seeding a network, an initial instance should be saved. Currently it is blocked by the following code:   "
"We have to set the `train_load_net` and `train_seed_net` at the same time to make seeding works, which do not make sense. We should only need to set the `train_seed_net`. "
"I pulled your changes yesterday, and previously the forward pass was working just fine, but now I got the following during the processing:     Any idea of what is going on there? "
"When trying the segmentation for 2-PM data with MALIS weighing on, I get the following error:     The output for `rand_errors` indeed seems empty from that call in `train.py`:     But how should that be defined then so that I get some rand errors? `malis_weights` and `grdts` contained values with the `output` key "
parallel_network::network::fov() is currently not working properly for the networks having multiple branches.  
"## Description  Output data is not guaranteed to be correct due to the data being deallocated by the time it is accessed in python. ## Example code    1.   creates a smart pointer (`std::shared_ptr`) to the newly allocated memory on the heap.  This location for memory is the destination for copying from the input cubelist `clist`. 2. we create a np::ndarray object that points to the memory location of the heap   3. after we leave the scope of this function, the smart pointer `tqp` is destroyed, thus deallocating the memory that we copied the data to.  Talked to @jingpengwu and @torms3 as well reading   ## why haven't seen any issues?  @torms3 suggests that it is because we have been ""lucky"".  Mainly because we are using a custom memory pool manager that will not destroy the data soon after deallocating.  But it looks like it will be a bigger problem in the future when we add more outputs ## how to fix? - @torms3 suggests that maybe we could have the output of python be the direct memory access to the output data saved in the output node groups. - Copy the raw data into ndarray - Pass the memory management into python (ndarray owner?) - ...? "
"I have noticed that with some datasets the training error goes very fast to zero and eventually to NaN as well with the following warning when creating label class:     So I commented the following line 516 from `ZNN_Dataset.py` fixing at least the quick approach to zero/NaN:     As I was not using your framework for EM segmentation but rather on ""easier"" structures obtained from 2-photon microscope, could the `utils.fill_boundary_holes()` just work poorly in some non-EM cases? "
I get the following error with train.py when saving the network:    
both icc and g++ do not compile for python interface. It seems that the `cube pool` have some problem. @zlateski     
"   In output patch-based rebalancing, we should explicitly handle divide-by-zero when computing class-specific weights. It's possible that an output patch inclues one of the classes exclusively, and the probability of this increases as the output patch size decreases. "
"to test the initalization, it would be better to be able to run forward pass using an initialized network without loading existing .h5 network. "
"current parameters are in program, which is not quite `clean` for different users. "
"currently, forward pass seems write data after all the samples were processed. if one sample got some error, we have to reprocess all the samples. It would be better to write out the sample output when one sample went through the forward pass. "
None
None
"It might be helpful to have the training (and possibly the forward-pass) record a log of the samples passed to the network. That way, if we get an obscure error (segfault, etc.), we can more easily replicate the bug. Logging the samples could be as easy as identifying the sample filename and the deviance from the middle. "
"In recursive training, we need to load part of first stage network. This function has not been tested yet. Not sure whether it could work out of the box. "
"in the [sample] section, `input=1` is not quit clear to correspond to [image1]. Since each layer can only correspond to one image or label stack, we can assign the section name explicitly using `input=image1`. "
add an option to force all the edges to use fft. 
"right now, the rebalance weight is computed in the whole label image stack. We may need to test the performance using rebalance weight based on each output patch. "
"for each sample, the input and output is not specific for image or label. should be modified to  [sample1] input = image1 output = label1 "
None
None
secs/ iteration 
None
"if we use softmax in cost function, we should also use this in forward pass. "
"most of the time, we use `optimeze` in training, but do not need optimize for forward pass. forward pass is fast and optimize takes a big part of processing time. "
None
None
"In forward pass, we should be able to process new data without label image. "
None
"@torms3 I merged my branch and the network could not be initialized any more. I tried to use old version of `src`, it works. Did you changed any related network initialization code? "
"ubuntu@ip-172-31-46-238:~/znn-release$  ./bin/znn --options=""train.config"" FFT Threads initialized  [options] postprocess  [options] path_check Config path [./networks/N3.spec] Load path   [empty] terminate called after throwing an instance of 'std::invalid_argument'   what():  Non-existent save path [./experiments/] Aborted (core dumped) ubuntu@ip-172-31-46-238:~/znn-release$ mkdir experiments ubuntu@ip-172-31-46-238:~/znn-release$  ./bin/znn --options=""train.config"" FFT Threads initialized  [options] postprocess  [options] path_check Config path [./networks/N3.spec] Load path   [empty] Save path   [./experiments/] Hist path   [empty] train_range [1] test_range  [2]  [net_builder] operable: 1 [net] initialize [net] intialize_weight  [INPUT] Node size:      [ 164, 164, 1 ] x 1  [INPUT_C1] Kernel size:        [ 3, 3, 1 ] x 48  [C1] Node size:      [ 162, 162, 1 ] x 48 Filter size:        [ 3, 3, 1 ] Filter stride:      [ 3, 3, 1 ] Receive FFT: 0  [C1_C2] Kernel size:        [ 4, 4, 1 ] x 2304 Sparseness:     [ 3, 3, 1 ] Real filter size:   [ 10, 10, 1 ]  [C2] Node size:      [ 151, 151, 1 ] x 48 Filter size:        [ 2, 2, 1 ] Filter stride:      [ 2, 2, 1 ] Sparseness:     [ 3, 3, 1 ] Real filter size:   [ 4, 4, 1 ] Receive FFT: 0  [C2_C3] Kernel size:        [ 4, 4, 1 ] x 2304 Sparseness:     [ 6, 6, 1 ] Real filter size:   [ 19, 19, 1 ]  [C3] Node size:      [ 130, 130, 1 ] x 48 Filter size:        [ 2, 2, 1 ] Filter stride:      [ 2, 2, 1 ] Sparseness:     [ 6, 6, 1 ] Real filter size:   [ 7, 7, 1 ] Receive FFT: 0  [C3_FC] Kernel size:        [ 3, 3, 1 ] x 4800 Sparseness:     [ 12, 12, 1 ] Real filter size:   [ 25, 25, 1 ]  [FC] Node size:      [ 100, 100, 1 ] x 100 Receive FFT: 0  [FC_OUTPUT] Kernel size:        [ 1, 1, 1 ] x 200 Sparseness:     [ 12, 12, 1 ] Real filter size:   [ 1, 1, 1 ]  [OUTPUT] Node size:      [ 100, 100, 1 ] x 2 Receive FFT: 0  [network] load_input Loading [./dataset/ISBI2012/spec/batch1.spec] Input sizes:  [ 164, 164, 1 ]  Output sizes: [ 100, 100, 1 ] [ 100, 100, 1 ]   Loading input [INPUT1] Preprocessing [standard2D] completed. (Elapsed time: 0.0455943 secs)  Loading label [LABEL1] Preprocessing [binary_class] completed. (Elapsed time: 0.0177708 secs)  Loading mask [MASK1]  [volume_data_provider] collect_valid_locations Upper corner: [ 82, 82, 0 ] Lower corner: [ 175, 175, 30 ] Number of valid samples: 259470 Completed. (Elapsed time: 0.00870137 secs)  [network] load_input Loading [./dataset/ISBI2012/spec/batch2.spec] Input sizes:  [ 164, 164, 1 ]  Output sizes: [ 100, 100, 1 ] [ 100, 100, 1 ]   Loading input [INPUT1] Preprocessing [standard2D] completed. (Elapsed time: 0.0448642 secs)  Assertion out_szs_.size() == lbls_.size() failed file: src/core/../front_end/data_provider/volume_data_provider.hpp line: 223 Aborted (core dumped) "
"Minor thing, but in the README.md it links to fftw-dev, but it references the aptitude package fftw-dev. afaik that package does not exist: it should be fftw3-dev "
"Is the use of exogenous variables available in the code? It seems like there is a way to input them, but I've run into errors and, looking at the source code, various comments indicate that they may not actually be implemented yet."
"Dear BOCPD-MS Team,    I want to use your algorithm in an online way in order to react when a change point is detected.  I had to change some code in my local repo as the dimensions of some attributes are specified during initialization.    Now I am wondering how I can figure out that a change point was detected at a certain time, if I do not process the whole time series but want to react immediately if a change point is detected. There is the CPs attribute of Detector, but the change points stored there are changing.   Can I say that every time the length of the element stored in this list changes, a CP is detected?    Thanks and kind regards,  Florian"
"Hello, thanks for the code. I would appreciate it if you answer my question. If I have data to test, how do I run your code to get change points?"
"The code for the NIPS paper will soon be stored in a separate repository,  . This repo will now only be used to store the code associated with the ICML paper, so we can:  - [ ] Remove all files used exclusively for the NIPS paper  - [x] Revert main classes back to the versions used in the ICML paper"
Integrate with Travis CI to automatically run tests.
"The air pollution script isn't in the repo at the moment, so let's add that back in and then make a similar set of changes to those in other branches.    - [x] Add main script  - [ ] Make sure the data source is listed  - [ ] Add command line arguments for parameters, with defaults set to those used in the ICML paper  - [ ] Output a list of the parameters used  - [ ] Put data load process into a separate function so it can be reused when generating the figure  - [ ] General tidy up of main script and the figure generating script"
"Three scripts to check here: `30portfolio_ICML18.py`, `paper_pictures_30Portfolios_comparison_ICML18.py` and `paper_pictures_30Portfolios_comparison_fincancialcrisis.py`."
"Similar to #3 - let's check that the script for the snowfall dataset, `whistler_ICML18.py` generates the values needed for Table 1. There isn't an associated `paper_pictures...` script for this dataset as the results are only used for the table.    - [x] Check paths to data.  - [x] Output error measures for Table 1."
"The Nile dataset is used to generate Figure 5 and several entries from Table 1 in the paper _Spatio-temporal Bayesian On-line Changepoint Detection with Model Selection_.    In this issue, we will check that we can regenerate the figure and values included in the table by first running `nile_ICML18.py`, and then `paper_pictures_nileData.py`.    Initial to do list:  - [x] Ensure that `nile_ICML18.py` generates the results file needed by `paper_pictures_nileData.py`  - [x] Update the file paths given in `paper_pictures_nileData18.py`"
A couple of things to add to the README and tutorial:  - [ ] Link Binder button directly to demo file  - [ ] Fix links in toc  - [ ] Add note about different versions of code for ICML and NIPS papers
"Onto the next script - like snowfall, we only need the MSE, NLL and error values as there isn't a figure for this dataset."
"There aren't any unit tests for the BOCPDMS project at the moment, so let's start adding them before making any changes to the core classes. We could also do with a couple of end-to-end tests, based on the smallest examples."
"We need to add some general information to the README file. Things to cover are:  - [x] A brief introduction to the BOCPDMS project  - [x] The Reproducible Research Champions project  - [x] Links to paper(s)  - [ ] ~~Instructions for installing and running the code in this project~~    The final point may make the first page rather long, so I'll make a start on the first three and then we can decide whether to include the instructions on the main page or whether to put them somewhere separate.  "
"Hello!  I am doing my research related to understanding long-term dependencies using transformer-based models. I came to this repo when I was searching for the pathfinder dataset. Could you please provide the links to dataset files, since they are currently unavailable?"
"Can you supply your dataset? I have researched your code, but I have some doubt about the dataset structure. Can you supply your dataset link for me to help my research? Thank you very much."
What are the steps involved to use PointCNN for another dataset?
"Thanks for your work!  When i use your code extract scannet dataset for 3D classification , I get 12060 examples for training and 3416 for testing. It seems that scannetv2 is different from scannetv1. However , when i train it with pointnet and pointnet++, I respectively get the best performance 89.34 and 90.35! It is higher than your paper's result.  Do anyone have the same founding with me?  Is it normal phenomenon due to the Scannetv2 is easy to classify than Scannetv1(i guess that in 2018 , Scannet just release V1) or i make some mistakes which cause my dataset wrong?    Thanks for anyone's answer.  Best regards. â•°(*Â°â–½Â°*)â•¯"
"Dear author,    I am interested in finding out the model size for PointCNN part segmentation, but I could only find the information for PointCNN classification network. Could you please report what is GMac for PointCNN part segmentation network?  Thank you very much in advance."
"Hi. Thank you for the repository.    I wanted to ask if it is possible to get any guidance regarding the custom dataset. I have my own dataset i.e. X, Y, Z points with labels (4 columns in total) in txt files. I wanted to know how I can create a data loader for custom data and train the network."
"Hi everyone,    I am pretty new in Neural Networks, can anyone share with me which parameters do you use? In which formats are (number, dictionary, array)? How do you run the class PointCNN?    Thank you in advance!"
I am trying to create (and plot) a confusion matrix for a multilabel dataset that has the predicted results structured in the same way the S3DIS results. I'm having trouble with creating the confusion matrix from the results and I am wondering if anyone has figured out how to do this. Any help would be greatly appreciated!
"Hi,     Many thanks for the shared code.   I am starting to work on the code, but I am unable to understand the flow and the details like which function is doing what. There is no commented section that highlights certain important steps of the Xconv. Thus it is very difficult for me to understand what is going on where. I guess many like me who is not a complete professional in 3D CNN architecture, are facing this issue. I request to kindly give a detailed walkthrough video of the code, by explaining the major sections. In addition, if the code is shared with comments, it will benefit many.     Thanks and Regards,  Vaibhav"
"Hi, I'm having trouble download the s2dis dataset. I'm getting a proxy error for      Does anybody know of an alternative source for the dataset?    I noticed that it is possible to use   but the dataset seems to be in a different format."
"hello,  when i visualized the result of pointcnn. I found that the segmented model's color is almost black. so i want to know how could i change the color of it. Thanks  !     "
"hello!  In train_val_vls.py,there is a xforms,which multipies points_sampled,what is the meaning of xforms?Why is it not mentioned in the paperï¼Ÿ"
"Hiï¼Œthanks for your great job. But I have a question in this paper. I note the sentence in page4 â€œNote that S is an unordered setâ€. In my opinion, N has been pertumed in the KNN by the distance from the representative pointsï¼Œwhy not keep the same order in S? And we donâ€˜t have to learn an X-conv to pertume the neighbors. Looking forward to your answer."
"Hello,    I tried to run the testing on the ShapeNet pre-trained segmentation model that you provided. I followed all the instructions and ran the scripts to download and prepare the data. I also compiled the custom `FarthestPointSampling` operator successfully.    Then I used this command below to run the testing from `pointcnn_seg` directory:     Needless to say, I put the checkpoint files in this directory (relative to `pointcnn_seg`):       The model apparently fails to load, giving errors regarding FPS and stating that:         I do not understand. Could it be a problem with the provided model? Or am I doing anything wrong?  I would appreciate any insight on this.    Thank you.    **Full stack trace:**   "
"I want to do custom object detection, for the same I have captured the PCD images using the 3D camera and labeled it using ""semantic-segmentation-editor"" but how can I create a dataset with this for training and evaluate the same."
"Hi team,    I was wondering if anyone can suggest ideas for doing some hyperparameter tuning for the framework?     Cheers  Dan"
Hi    I am having some trouble calculating IoU on a per class basis for the semantics3d dataset.    Hoping someone might be able to give me some pointers regarding how I might go about this :)
"Hi team,     I am looking to train using my own data and wanted to get some basic explanation on the relevance of the parrameters     sample_num = 2048    batch_size = 12    num_epochs = 256    In my settings file i have :    num_class = 2    sample_num = 2048    batch_size = 12    num_epochs = 256    label_weights = []  for c in range(num_class):      label_weights.append(1.0)    learning_rate_base = 0.001  decay_steps = 20000  decay_rate = 0.7  learning_rate_min = 1e-6    step_val = 500    weight_decay = 0.0    Trying to train on aerial data with 8 points per sq meter density and i have a training dataset of 800 000 000 million points.    I am seing some over-fitting very early on with these values:    num_class = 2    sample_num = 12288    batch_size = 6    num_epochs = 8096    label_weights = []  for c in range(num_class):      label_weights.append(1.0)    learning_rate_base = 0.001  decay_steps = 20000  decay_rate = 0.7  learning_rate_min = 1e-6    step_val = 500    weight_decay = 0.0    jitter = 0.0  jitter_val = 0.0    rotation_range = [0, math.pi/32., 0, 'u']  rotation_range_val = [0, 0, 0, 'u']  rotation_order = 'rxyz'    scaling_range = [0.0, 0.0, 0.0, 'g']  scaling_range_val = [0, 0, 0, 'u']    sample_num_variance = 1 // 8  sample_num_clip = 1 // 4    Any advice?   "
Does anyone can explain the definition of block size and grid size in _prepare_semantic3d_data.py_ line 24-25     What is the effect when we use bigger or smaller size to the accuracy?  I'd really appreciate any help. Thank you.
I use './train_val_modelnet.sh -g 0 -x modelnet_x3_l4' to train my own dataset  What should I do if I want to terminate the program?
"Hi, thank you for your teams' persistent maintennance for this issue community!    For evaluating the trained model, I follow the Readme to run the script '/test_s3dis.sh'. However, I find it takes several hours, which completely disobeys my expectation about fast evaluation for a trained model. I think fast evaluation is very important for develop a desirable neural networks, not excepting PointCNN. How can I conduct a fast evaluation for the trained model on S3DIS?"
"As above, I want to get the features of every input instead of the classification results.  "
"Hello,     I really like your paper and thanks for sharing the code. However, I have some questions about the evaluation metric used for ShapNet Part Segmentation. Looking closely into the evaluation metric used in PointNet++(  and PointCNN(   I feel the numbers are not comparable.    I think (mpIoU, 84.6 in PointCNN) is same as (mean, 85.1 in Pointnet++), as in both cases the IOU is calculated for each sample (part-averaged) and then averaged over the dataset. Further, pIOU metric does not exist in PointNet++. The (mean shape IOU, 81.9 in PointNet++) is averaged across the shape categories and not individual parts.     Please correct me on this if I am missing something. I would very much appreciate some clarifications here.    Best,  Ankit     "
"Instruction for classification of mnist dataset contains the line:  python3 ./prepare_mnist_data.py -f ../../data/mnist    However, required files cannot be found in folder data/mnist, because they are in data/mnist/zips. The following command works much better:  python3 ./prepare_mnist_data.py -f ../../data/mnist/zips   Cheers"
"æˆ‘æŒ‰ç…§githubçš„è¯´æ˜Žè¿›è¡Œæ“ä½œåŽï¼Œä½¿ç”¨ModelNetæ•°æ®é›†å‡ºçŽ°ä¸‹åˆ—é”™è¯¯ï¼Œæˆ‘æ„Ÿè§‰è¿™æ˜¯è·¯å¾„å‡ºçŽ°äº†é—®é¢˜ï¼Œä½†æ˜¯ä¸çŸ¥é“åœ¨å“ªé‡Œè¿›è¡Œä¿®æ”¹ã€‚éžå¸¸æ„Ÿè°¢æ‚¨å¸®æˆ‘çœ‹ä¸€ä¸‹ã€‚    /home/chenkun/PointCNN-master/data_utils.py:128: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.    data = h5py.File(os.path.join(folder, filename))  Traceback (most recent call last):    File ""/home/chenkun/PointCNN-master/train_val_cls.py"", line 344, in        main()    File ""/home/chenkun/PointCNN-master/train_val_cls.py"", line 72, in main      data_train, label_train, data_val, label_val = setting.load_fn(args.path, args.path_val)    File ""/home/chenkun/PointCNN-master/data_utils.py"", line 139, in load_cls_train_val      data_train, label_train = grouped_shuffle(load_cls(filelist))    File ""/home/chenkun/PointCNN-master/data_utils.py"", line 132, in load_cls      points.append(data['data'][...].astype(np.float32))    File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper    File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper    File ""/home/chenkun/.local/lib/python3.6/site-packages/h5py/_hl/group.py"", line 264, in __getitem__      oid = h5o.open(self.id, self._e(name), lapl=self._lapl)    File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper    File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper    File ""h5py/h5o.pyx"", line 190, in h5py.h5o.open  KeyError: ""Unable to open object (object 'data' doesn't exist)"""
"Hi, is it possible to do multigpu training for segmentation of pointcnn, if yes how?"
"Good morning,    I'm trying to run some of my own data and I am at the evaluation section.  I'm calculating a confusion matrix and the numbers seem to be a bit strange so I just want to make sure I'm doing it correctly or figure out what I'm doing wrong.  Here is a snippet of my code.         Now, when I look at  ,  , and  , I would have thought that  , however  .  Is this to be expected?  Because the   is the maximum index point in the test set, so I'm not sure how the total points tested can be greater than that unless there are repeats.  "
å°Šæ•¬çš„æŽè€å¸ˆ  æ‚¨å¥½ï¼  å¾ˆæŠ±æ­‰æ‰“æ‰°æ‚¨ï¼Œæ‚¨åœ¨githubä¸Šæä¾›çš„é¢„è®­ç»ƒçš„è¿žæŽ¥å¥½åƒå´©äº†æ— æ³•ä¸‹è½½ï¼Œè¯·é—®å¯ä»¥éº»çƒ¦ä½ æä¾›ç»™æˆ‘ä»¬é¢„è®­ç»ƒå¥½çš„æ¨¡åž‹ä¹ˆ  
"Hi,    I am trying to extract the prediction per point cloud being fed for inference one by one.    Forgive me for my ignorance, but I cannot see how to do this from the validation block of the code  Can you give me some pointer on how to get from where I am so far:  `            saver = tf.train.import_meta_graph(args.load_ckpt+'.meta')          saver.restore(sess, args.load_ckpt)          print('{}-Checkpoint loaded from {}!'.format(datetime.now(), args.load_ckpt))          data_val_placeholder = tf.placeholder(data_val.dtype, data_val.shape, name='data_val')          label_val_placeholder = tf.placeholder(tf.int64, label_val.shape, name='label_val'                    for cloud_path in filenames:            if os.path.isfile(cloud_path):                            #application specific on how to get ground truth for each pointcloud here                                            pc = pypcd.PointCloud.from_path(cloud_path)                               if (pc.width==num_points):                  label= label_dict[str(label)]                                    for j in range(0, num_points):                    pc_arr[j] = [pc.pc_data['x'][j], pc.pc_data['y'][j], pc.pc_data['z'][j], pc.pc_data['normal_x'][j], pc.pc_data['normal_y'][j], pc.pc_data['normal_z'][j]]                                                          data_val=np.expand_dims(pc_arr,axis=0)                  label_val=np.expand_dims(label,axis=0)                  sess.run(iterator_val.initializer, feed_dict={                      data_val_placeholder: data_val,                      label_val_placeholder: label_val,                  })  `    I was wondering if there was some line you could use similar to   `prediction = sess.run(get_prediction ...'    Any help would be appreciated.  "
"Hello,thank you for your work,can your model apply on vkitti dataset to implement point cloud senmatic segmentation?"
"@yangyanli Hi, thank you for your sharing this code firstly.  when I run the command bash ""un7z_semantic3d.sh"", I encountered this error:       Can you help me?  "
"Thank you for sharing the code. I' m trying to follow the steps in Readme for testing S3DIS. When I used the command './test_s3dis.sh -g 0 -x s3dis_x8_2048_k16_fps -a 1 -l ../../models/seg/s3dis_x8_2048_fps_k16_xxxx/ckpts/iter-xxxxx -r 4  cd ../evaluation.', I got a mistake like this  ! . I find that the code in test_s3dis.sh use the test_general_seg.py with a parameter '-f', but there is no such selection in  test_general_seg.py. The same proplem occurs in test_scannet.sh.     In addition, I want to segment my own data, do I need to train all the Areas?   "
"Thank you for making this code available under MIT license.     I tried running as-is with latest version of tensor-flow (2.0.0). However, there are major breaking changes between 1.6 and 2.0, code did not run at all.   Moreover, 1.6 is also no longer available for download. So tried with version 1.15 and did not work either.    So have been attempting to port to TensorFlow 2.0. The work-in-progress is available at       So far have been able to define and run the model to the end.     Have not been able to figure out how to implement the optimizer and loss functions, so using the out-of-the-box loss and optimizer functions for now.    My model runs but prediction scores are hopeless.    5/5 [==============================] - 217s 43s/step - loss: 5900.1611 - accuracy: 0.1079 - categorical_accuracy: 0.1079 - val_loss: 5365.4263 - val_accuracy: 0.3203 - val_categorical_accuracy: 0.3203    Any suggestions from the original authors would be welcome."
"Hi,  I attempted to retrain semantic segmentation on s3dis. When train on area 1,2,3,4,6 and test on area 5, I got same result in table 3 (mIoU 57.14).  But, 6 folder cross validation , mIoU just 61.43. I use the same parameter in you paper. I don't know which area result lower than your. Could you give me your train result at every area, or your pretraining models.  Best regards,  G. Shi "
None
"I recently updated   to include support for PLY, and using it as a drop-in replacement for plyfile would increase the number of supported mesh formats by a great deal. Perhaps that's worth considering."
"Hello,    I would like to test the provided pre-trained model ""ScanNet"" you put on PoinCNN home page.  But I can not find any ""inference.py"" file.  So I would like to implement it on my own, but in order to freeze model, which tensors should I select in the model to do this ?     Thanks!"
ï¼¨ï½ï½— to ï½‡ï½…nerate the visulization of scene segment results on s3dis after s3dis_merge.py? I do not find .off files? looking forward your reply.
"I ran the PointCNN tensorflow code with my own dataset which was split into training, testing and validation datasets. Training and testing accuracies were tracked during training process. After enough training iterations,  both training and testing accuracies are high, close to 0.9. But when the trained model is applied to an independent validation dataset, the accuracy drops to 0.6. This is obviously an overfitting effect. I've used other voxel-based CNN or PointNet which does not encounter such effect. It is not the issue of the dataset.    I suspect the testing dataset actually participated in the training process, although the code turns ""is_training"" off. If PointCNN uses testing datasets as part of training samples, the high accuracy is not a surprise. I'd appreciate if anyone found the same overfitting problem with an independent validation dataset or just corrected me if wrong. "
"In the file: PointCNN/data_conversions/prepare_semantic3d_data.py   Line 35:39       This initialization code should be placed after line 46:       I.e.       Those arrays (e.g. indices_split_to_full) would be filled by the previous dataset data. If not initialized with zero again, and the previous values will be passed on to the next iteration of dataset. If next dataset is smaller, the arrays would be mixed with the previous ones. Please double check if this is a bug.            "
How do you visualize the extracted features?Can you send me your code about visualization of extracted features?My email is that 1442342449@qq.com.Think you very much.
"i succeed in compiling the file of .so, but when i run the program of train_val_seg.py, there is a warning that the tf_sampling_so.so could not be found for undefined symbol: _ZN10tensorflow12OpDefBuilder4AttrESs. i'm confused about the problem and i failed to find a way to solve it. So i wonder whether you know how to solve this problem?  i use tensorflow 1.13.1 cuda10.0, anaconda 10.130 and spyder 3.3.6.  Hoping for you reply, thanks!"
"Dear Yanganli,  I need only to create the features using pointcnn with the input as XYZ pointcloud in text format.  Could you please help me with some guiding step to run the code on pycharm in windows 10?    Best regards"
"After executed this 2 lines:     Several zips were downloaded, extracted, and made into .h5 files.    When I look at train_0.h5, data_num[0] is equal to len(data[0]), data_num[1] is equal to len(data[1]), and so on.    But when I look at train_5.h5, data_num[0] is not equal to len(data[0]).    This seems like incorrect correspondence.    I am running on Ubuntu 18.04.    Thanks."
"  1 æŸä¸ª issue æœ‰æåˆ°ï¼Œ""stress test on small number of input points"", ä½†æ˜¯æˆ‘çœ‹çš„è¿™ç¯‡è®ºæ–‡ä¸­æ²¡æœ‰è¿™å—çš„å†…å®¹ï¼Œè¯·é—®é™¤äº† arxiv çš„é‚£ç¯‡è®ºæ–‡ï¼Œè¿˜æœ‰å¦å¤–ä¸€ç¯‡è®ºæ–‡å—ï¼Ÿ  2 æˆ‘æƒ³åœ¨kittiä¸Šåº”ç”¨åˆ†ç±»çš„ç½‘ç»œï¼Œä½†æ˜¯ç‰©ä½“çš„ç‚¹æ•°é‡å·®åˆ«å¾ˆå¤§ï¼Œå¯èƒ½æœ€å°çš„ç‰©ä½“åªæœ‰4,5ä¸ªç‚¹ï¼Œè¿™æ ·çš„æƒ…å†µæ˜¯å¦éœ€è¦å¯¹ç‚¹çš„æ•°é‡è¿›è¡Œè¡¥å……ï¼Ÿ  3 XçŸ©é˜µæ˜¯æ€Žä¹ˆä¿è¯å¯ä»¥å­¦ä¹ çš„åˆ°è¿™æ ·ä¸€ä¸ªæ—‹è½¬çŸ©é˜µçš„ï¼Ÿ æ€Žä¹ˆä½¿å¾—è®ºæ–‡ä¸­ä½œè€…çš„è½¬ç½®çŸ©é˜µçš„æ„å›¾å¾—åˆ°å®žçŽ°çš„ï¼Ÿ ä¸€ä¸ªè®­ç»ƒå¥½çš„è½¬ç½®çŸ©é˜µåº”è¯¥åªèƒ½å¯¹ä¸€ç§é¡ºåºè¿›è¡Œè°ƒæ•´ï¼Œå®ƒæ˜¯æ€Žä¹ˆåšåˆ°å¯¹å¤šç§é¡ºåºï¼Œå¤šç§ä»£è¡¨ç‚¹çš„æƒ…å†µéƒ½èƒ½é€‚ç”¨çš„ï¼Ÿ     æœŸå¾…æ‚¨çš„å›žå¤ï¼"
"how to train on my own data,just have a txt file    x,y,z"
"Hi,   Thanks for your sharing,it really helps.The ideas in this net are really great.    I wonder which parameters I may set again if I want to use this net in a new situation.and what's more,can you share some experiences in the process of giving out this wonderful net?Thank you.   I'm new hand in this field,forgive me for any unreasonable request."
"Thank you for your brilliant work.   Recently, I've tried to reproduce pointcnn on a pointcloud classification work. I did what the README instruction said about the dataset preparation.  But, when I run the train_val_dataset.sh, the training log always report the error as below:      File ""../train_val_cls.py"", line 344, in        main()    File ""../train_val_cls.py"", line 174, in main      net = model.Net(points=points_augmented, features=features_augmented, is_training=is_training, setting=setting)    File ""/home/amax/PointCloud/PointCNN/pointcnn_cls.py"", line 12, in __init__      PointCNN.__init__(self, points, features, is_training, setting)    File ""/home/amax/PointCloud/PointCNN/pointcnn.py"", line 116, in __init__      depth_multiplier, sorting_method, with_global)    File ""/home/amax/PointCloud/PointCNN/pointcnn.py"", line 23, in xconv      nn_fts_from_pts_0 = pf.dense(nn_pts_local, C_pts_fts, tag + 'nn_fts_from_pts_0', is_training)    File ""/home/amax/PointCloud/PointCNN/pointfly.py"", line 346, in dense      reuse=reuse, name=name, use_bias=not with_bn)    File ""/home/amax/venv/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 324, in new_func      return func(*args, **kwargs)    File ""/home/amax/venv/lib/python3.5/site-packages/tensorflow/python/layers/core.py"", line 188, in dense      return layer.apply(inputs)    File ""/home/amax/venv/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1479, in apply      return self.__call__(inputs, *args, **kwargs)    File ""/home/amax/venv/lib/python3.5/site-packages/tensorflow/python/layers/base.py"", line 537, in __call__      outputs = super(Layer, self).__call__(inputs, *args, **kwargs)    File ""/home/amax/venv/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 586, in __call__      self.name)    File ""/home/amax/venv/lib/python3.5/site-packages/tensorflow/python/keras/engine/input_spec.py"", line 111, in assert_input_compatibility      layer_name + ' is incompatible with the layer: '  ValueError: Input 0 of layer xconv_1_nn_fts_from_pts_0 is incompatible with the layer: its rank is undefined, but the layer requires a defined rank.    It seems the input can't be loaded correctly and there are none issue said anything about this question. I would like to get some advice from you guys, thanks.  "
"Coud you offer pretrained model for shapenet part segmentation?  I have tried to trian a new model, but the mIoU of shapenet part segmentation is only 85.23% which is much lower than 86.14% in the paper.  Thanks!"
"Hi, @yangyanli   Firstly thans for your great work on PointCNN.  Recently, I have met the performance problem on Scannet Segmentation.  I trained the scannet dataset following your setting with the only difference on batch size(I set it to 8). I got the Per Voxel Acc  of 84.02%, lower than the accuracy on your paper(85.1%). I want to know the reason on it. Looking forward to your reply."
Thanks for sharing the code.  How can I apply PointCNN in Kitti 3D object detection?
"Hello,    Recently I was going through your xdconv section of the code and I think there's a bug but I fail to understand how it is still working. Let me try to explain where I think there should be an issue.    Let me e.g. run the code on these settings copied from **scannet_x8_2048_fps.py**:       Now in the **pointcnn.py** file, I have an issue with this part of the code:       So in this part of the code, the issue I have is when the _layer_idx == 1_   When that would happen the sizes of some variables would be:  pts = N x 384 x 3  fts = N x 128 x 3  qrs = N x 384 x 3    Now once we feed these into the **xconv** function, we will be able to calculate indices which would have size:  indices = N x 384 x K x 2    But then we use two lines of gather_nd in the xconv function that makes no sense. These lines are:  (1) `nn_pts = tf.gather_nd(pts, indices, name=tag + 'nn_pts')  # (N, P, K, 3)  `  (2) `nn_fts_from_prev = tf.gather_nd(fts, indices, name=tag + 'nn_fts_from_prev')  `    I understand how the (1) line would get us nn_pts of size N x 384 x K x 3  But I fail to understand, how the (2) line is giving us nn_fts_from_prev of size N x 384 x K x 3    I believe the (2) line should have prompted an error. Since it is not giving an error, it means there's a lapse on in my understanding of how it works. Since the indices have been calculated using pts and qrs, I believe the indices should range from 0-383. However, the fts size is only 128 and therefore it should give an out of bound error whenever an indice is called that is outside the size of fts.  I believe one indice would look something like [0, 300]. Which would be the 300th point on batch 0. However, this indice should not work with fts (line (2)) because fts is not large enough.    If someone can take just a few minutes to look into this and explain to me why it is working, it would be really helpful.    Thank you."
"When I evaluate your pretrained model of S3DIS, it seems that your checkpoint does not contain the key 'features_hd/kernel'. You did not use extra features when training S3DIS? Could you give me more details about how to evaluate your pretrained model on S3DIS? Thank you!"
"I ran the code of PointCNN on TU-Berlin (using a single Tesla V100 GPU), strictly following the code and hyper-parameter settings based on the released github version. The validation accuracy of PointCNN reached 60.% at 40K iteration, but was no longer increased, while the reported result is 70.57% in the NIPS version. I visualized the training/validation loss and training/validation accuracy as follows:  !   I found that the the loss did not converged to a sufficiently small value. I also tried several small initial learning rates such as 0.001, 0.0001, but got similar results. Do I have missed something that is important to the performance of PointCNN? @burui11087 "
"Hello,    in your paper you describe the use of several metrics for the segmentation task, two of those being the **overall accuracy** (OA) and **micro-averaged accuracy** (mAcc). My question is how you define those metrics and how you make a distinction between them, with my point being that these two metrics should essentially be the same. Overall accuracy **is** a case of micro-averaged accuracy.    Generally speaking, for the overall accuracy you would calculate the ratio of just how many points of all points have been classified correctly.   On the other hand, the micro-averaged accuracy is defined as the accuracy you get by equally weighting each point in your evaluation set, aggregating your prediction outcomes across all label classes and then computing the accuracy using the aggregate outcomes, *which would be exactly the same as the overall accuracy that I described above*. That's how I understand those metrics in the given context of point clouds.    So could you tell me your understanding of these metrics? Also, how did you calculate them for Table 2 and Table 3 of your appendix?   Did you perhaps mix up the terms of micro-averaged accuracy with macro-averaged accuracy (which would be the mean per class accuracy in this case)? Or is my understanding just wrong?    Best regards,  Thomas"
"@yangyanli, Hi thank you very much for releasing the code!  I have a question regarding the split of training/val/test dataset of shapenet.  In the paper, you have stated that 'ShapeNet Parts contains 16880 models (14006/2874 training/testing split) from 16 shape categories, each annotated with 2 to 6 parts and there are 50 different parts in total.' which means there is no validation dataset.    Also, in this code    it says that test data is used for validation. Does this mean that you have trained the train&validation dataset and then evaluated every epochs and picked the best performance model based on the test dataset?"
Hi  I am a student (not in programming) trying to use PointCNN to perform a semantic segmentation on my own data (urban scene with 4 classes).  Is it possible ? How should I proceed?    I can't find any file adaptable to other data than benchmarks.    Any help please? (quite urgent issue!)   
"Hi,      I followed the steps in the Semantic3d dataset and used a custom dataset to train. I was able to create .h5 and all steps were successful. But when I run,   ./train_val_semantic3d.sh -g 0 -x semantic3d_x4_2048_fps :     inside models/seg -> the log file shows the following error:  **tf_sampling_so.so: cannot open shared object file: No such file or directory**    I checked the existing issues (  and made changes to Pointcnn/sampling/tf_sampling_compiler.sh but still did not work.     I am using the TensorFlow version = 1.15, python 3.6,  conda environment(Used pip command to install tf as mentioned in one of the issues. Still didn't work)  Any help on how to resolve this issue?  Regards  Niranjan"
"A few days ago I run the Modelnet40 dataset and it worked well.  Now I want to run my own dataset(txt files)for binary classification,what should I do?   Which parameter should I change and and how to deal with my dataset?  "
None
"  !       When I try to run the code with GPU, tensorflow=1.14, it comes the error above. How can I solve it?"
"when i run 'README.MD' file,this commandâ€”â€” â€œ./train_val_shapenet.sh -g 0 -x shapenet_x8_2048_fpsâ€  an error comes outâ€”â€”  Train/Val with setting shapenet_x8_2048_fps on GPU 0!  and i donâ€˜t know why,could you help me to solve this problem?"
None
"  I want to do a classification work on 3D model, but my own data are .pcd file. How can I train them using Point CNN. Thank you!"
"Hi @burui11087, @yangyanli ,    I have been trying to use PointCNN for aerial lidar data segmentation, for which i had used sample_num as 12288 with batch size 4 successfully on 16GB V100 card. My versions were tensorflow 1.10.1 and cuda 9.2, i could compile the tf_compile then.    Now when i got by 32GB single V100 card i was trying to fit bigger batch size (12) with same sample_num, now i get this error as below: could you please suggest what i am missing?    However when i pass batch size of 4 the training starts well with same set-up.    My versions:   cuda toolkit 9.2  tensorflow 1.10.1  nvidia drivers 415.27    log of error:     E tensorflow/stream_executor/cuda/cuda_blas.cc:647] failed to run cuBLAS routine cublasGemmBatchedEx: CUBLAS_STATUS_NOT_SUPPORTED  2020-03-09 10:35:51.149051: E tensorflow/stream_executor/cuda/cuda_blas.cc:2510] Internal: failed BLAS call, see log for details  2020-03-09 10:35:51.149173: I tensorflow/stream_executor/stream.cc:4818] stream 0x5648deb70150 did not memzero GPU location; source: 0x7f657d7f89d0  2020-03-09 10:35:51.149236: I tensorflow/stream_executor/stream.cc:4818] stream 0x5648deb70150 did not memzero GPU location; source: 0x7f657d7f89f0  2020-03-09 10:35:51.153013: I tensorflow/stream_executor/stream.cc:4818] stream 0x5648deb70150 did not memzero GPU location; source: 0x7f655dff99d0  2020-03-09 10:35:51.153059: I tensorflow/stream_executor/stream.cc:4818] stream 0x5648deb70150 did not memzero GPU location; source: 0x7f655dff99f0  2020-03-09 10:35:51.153214: I tensorflow/stream_executor/stream.cc:4818] stream 0x5648deb70150 did not memzero GPU location; source: 0x7f657dff99d0  2020-03-09 10:35:51.153262: I tensorflow/stream_executor/stream.cc:4818] stream 0x5648deb70150 did not memzero GPU location; source: 0x7f657dff99f0  2020-03-09 10:35:51.153464: I tensorflow/stream_executor/stream.cc:4818] stream 0x5648deb70150 did not memzero GPU location; source: 0x7f655f7fc9d0  2020-03-09 10:35:51.153511: I tensorflow/stream_executor/stream.cc:4818] stream 0x5648deb70150 did not memzero GPU location; source: 0x7f655f7fc9f0  Traceback (most recent call last):    File ""/home/sayak_cowi/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1278, in _do_call      return fn(*args)    File ""/home/sayak_cowi/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1263, in _run_fn      options, feed_dict, fetch_list, target_list, run_metadata)    File ""/home/sayak_cowi/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1350, in _call_tf_sessionrun      run_metadata)  tensorflow.python.framework.errors_impl.InternalError: Blas xGEMMBatched launch failed : a.shape= ]]            ]]    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""train.py"", line 311, in        is_training: True,    File ""/home/sayak_cowi/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 877, in run      run_metadata_ptr)    File ""/home/sayak_cowi/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1100, in _run      feed_dict_tensor, options, run_metadata)    File ""/home/sayak_cowi/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1272, in _do_run      run_metadata)    File ""/home/sayak_cowi/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1291, in _do_call      raise type(e)(node_def, op, message)  tensorflow.python.framework.errors_impl.InternalError: Blas xGEMMBatched launch failed : a.shape= ]]            ]]    Caused by op 'xconv_1_fts_X', defined at:    File ""train.py"", line 132, in        net = model.Net(points_augmented, features_augmented, is_training, setting)    File ""/home/sayak_cowi/notebooks/PointCNN/pointcnn_seg.py"", line 11, in __init__      PointCNN.__init__(self, points, features, is_training, setting)    File ""/home/sayak_cowi/notebooks/PointCNN/pointcnn.py"", line 116, in __init__      depth_multiplier, sorting_method, with_global)    File ""/home/sayak_cowi/notebooks/PointCNN/pointcnn.py"", line 39, in xconv      fts_X = tf.matmul(X_2_KK, nn_fts_input, name=tag + 'fts_X')    File ""/home/sayak_cowi/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py"", line 1980, in matmul      a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)    File ""/home/sayak_cowi/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 1236, in batch_mat_mul      ""BatchMatMul"", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)    File ""/home/sayak_cowi/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper      op_def=op_def)    File ""/home/sayak_cowi/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func      return func(*args, **kwargs)    File ""/home/sayak_cowi/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3155, in create_op      op_def=op_def)    File ""/home/sayak_cowi/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1717, in __init__      self._traceback = tf_stack.extract_stack()    InternalError (see above for traceback): Blas xGEMMBatched launch failed : a.shape= ]]            ]]        -   "
"Hello, thank you for your code.  I am confused about how to upsample the points when doing the seg task. I am not able to find the  corresponding code in this repo because the code is a little complex to me. Is the upsampling method the same as that used by pointnet++?   looking forward to your reply."
"I am trying to visualize the labels returned in the ""results/*.txt"" from the semantic3d_merge.py. However, it seems that the labels do not correspond to the original order of the txt files used as the input to ""data_conversions/prepare_semantic3d_data.py"" . Can you give me some information about how to pair the final labels with their original x,y,z coordinates?"
"Good afternoon,    First of all, thank you for your code.  This is very good work.      I am trying to implement this in some of my own data.  However, when I am running it there seems to be an issue when I run into the ""net"" line.  I started implementing it line by line to see where the issue was because nothing was being produced in my log file.      Note I'm in troubleshooting mode, so the code is incomplete    **Code 1 (without net line)**     **Output in logfile**     Which is as expected.    Now, when I uncomment the last line   ` net = model.Net(points_augmented, features_augmented, is_training, setting)`  the log file is blank.  I get a long tensorflow warning message in the setting.txt file about TF2, which may be causing the issue?  I have no idea.  There are no error messages, just warnings.  I realize this is slightly different and I apologize for the long message.  Has anyone come across this?  Thank you!      Note:  I am using Tensorflow 1.14 inside a Docker container.  "
"Hello , thanks for great open source contribution, I would like to try out the network on my custom artificially created data. I only see guides that go through existing datasets and there is not many resources to guide for custom data training,  it would be great if you could provide some generic script or steps that explains process ,   Thank you very much for your time "
"I would like to use PointCNN to identify landmark, for the simplicity of the task, I had generated 2000 football shape (prolate spheroid) point cloud or random size with points counts from 2601 to 10201. 2 tips of the point clouds are the landmark that I am interested.    !     For illustration purpose, the 2 red points are the landmark that I am interested.    The labels are 0 and 1, with the 2 red points as 1, and other as 0.    My sampling is 2048 points, batch size of 2. Loss is improving, but almost stagnant after 500,000 iteration, or 150 epochs. mAcc is still often showing 0.5. Is that mean for every batch of training, the model only guess it right for 1 of the class, and failed for another class?    Since in every point clouds, there will be only 2 landmarks with label 1, so it means it cant identify the landmark? Or because the sampling size is too small to obtain any points which is landmark?    Thanks."
"     1.   nn_pts_local  is shape of (N, P, K, 3)ï¼Œwhy donâ€˜t reshape it to (N,PxKx3)?        2. shape question      if with_X_transformation: #                 ######################## X-transformation #########################          X_0 = pf.conv2d(nn_pts_local, K * K, tag + 'X_0', is_training, (1, K))          X_0_KK = tf.reshape(X_0, (N, P, K, K), name=tag + 'X_0_KK')       nn_pts_local is shape of (N, P, K, 3)   using (1,K) kernel get X_0  ,it's shape is (N, KxK, 1, 3),How can it reshaped to X_0_KK  (N, P, K, K)?    thanks !  "
"I want to use PointCNN to train my point cloud dataset, how can i modify the code?  Current situation: I have debugged the original author's modelnet classification network code, generated my own h5 file, changed the number of categories to my own category number, and changed the dataset path to my own dataset path. However, the program run incorrectly., I don't know why, I feel that there are some changes, I didn't find it, I hope you can give me some advice."
"Hello, I load the data and train the model as the process   ""cd data_conversions  python3 ./download_datasets.py -d tu_berlin  python3 ./prepare_tu_berlin_data.py -f ../../data/tu_berlin/ -a --create-train-test  cd ../pointcnn_cls   ./train_val_tu_berlin.sh -g 0 -x tu_berlin_x3_l4""  But, there is no error and the two txt files are blank. I do not know how to solve the problem. I try load the data many times and train the model many times. The result is the same. Have you ever faced the same problem?"
"When I run the ./train_val_quick_draw.sh -g 0 -x quick_draw_full_x2_l6, the log will stop when the data is loaded in 145. I do not what is the reason.    !     My second question is that how to output the traning process to the log file and  terminal? The training process now is in the log only."
"Hi,  I trained a classification model for ModelNet40 based on your code.  How can I make predictions now?    I can load the model as:       What is next?    thank you"
"Hi, I followed the README to try the Classification of ModelNet40, I input the command as below, but it didn't work.   `(ven3) ray@ray-MS-7B48:~/catkin_ws/src/PointCNN/pointcnn_cls$ ./train_val_modelnet.sh -g 0 -x modelnet_x3_l4  Train/Val with setting modelnet_x3_l4 on GPU 0!  (ven3) ray@ray-MS-7B48:~/catkin_ws/src/PointCNN/pointcnn_cls$   `  and I found some information in model/cls/pointcnn_cls_modelnet_x3_l4.txt  `2019-06-17 14:57:23.664508: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA  2019-06-17 14:57:23.677562: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_UNKNOWN  2019-06-17 14:57:23.677589: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: ray-MS-7B48  2019-06-17 14:57:23.677596: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: ray-MS-7B48  2019-06-17 14:57:23.677619: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 390.77.0  2019-06-17 14:57:23.677634: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:369] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  390.77  Tue Jul 10 18:28:52 PDT 2018  GCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.11)   """"""  2019-06-17 14:57:23.677644: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 390.77.0  2019-06-17 14:57:23.677650: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 390.77.0`      System environment:  Ubuntu 16  tensorflow-gpu==1.4  cuda==8.0  cudnn==6.0    Can you tell me how to fix this?  "
"Hi    Is there a possibility to define a fixed set for validation tasks and a fixed set for test tasks on classification? In ModelNet40 the training set is defined in `train_files.txt`, the test set in `test_files.txt`, it would be nice to define the validation set in a file like `val_files.txt` so that training and test sets can be different.  Is that possible?    Thanks a lot for your great work!    (edited)"
when I train the seg-scannetï¼Œi found the question:No moudle named 'scannet_x8_2048_k8_fps'?where I can find it?
"I find when I set save_ply_fn = True in modelnet_x3_l4.py then train_val_cls.py indicates error with  TypeError: 'bool' object is not callable. I found the error is line92: setting.save_ply_fn(data_sample, folder), and I cannot found this function save_ply_fn.      "
I have some questions when I train the Segmentation-ScanNet  l@l-THUNDEROBOT-PC:~/Desktop/PointCNN-master/pointcnn_seg$ ./train_val_scannet.sh -g 0 -x scannet_x8_2048_k8_fps   bash: ./train_val_scannet.sh: Permission denied    who can help me ?thanks!
None
"Hi, I'd want to know the time usage **0.031/0.012** you reported in the paper:    > As shown in Table 6, we summarize our running statistics based with the model for classification with batch size 16, 1024 input points on nVidia Tesla P100 GPU, in comparison with several other methods. PointCNN achieves 0.031/0.012 second per batch for training/inference on this setting.      is measured for a mini-batch size of 16 or 1/16 iteration within a mini-batch? I tried to reproduce your code based on a 2080-ti with TF 1.13 and CUDA 10.0, and the time usage in training phase was **0.34s** for a mini-batch (ModelNet40 using config file   ), which means **0.0213s** per 1/16 iteration within a mini-batch. Seems that the time usage you reported in the paper is calculated based on the latter criteria right?  "
"Thanks for this wonderful work. I had tried out your sample for classification and segmentation, and now I would like to perform landmarks extraction from human body point cloud, like identify shoulder point / waist level, and extract the distance between landmarks    How can this be done using segmentation? Or any better idea?    Thanks."
"Dear authors,    Hi! Thanks for your great work. I am trying to use your code for the S3DIS dataset recently. I notice that in [test_general_seg.py] there's a ""repeat_num"" flag. It is set to 1 by default. According to my experience on part segmentation, typically repeat_num > 1 will produce better results because of ensembling effects (actually you used repeat_num = 10 for partseg). I would like to know whether repeat_num > 1 is used for your reported S3DIS results (mIOU=65.39% across 6 areas and mIOU=57.26% for area 5). Thanks for your reply~    Best,  Ken"
"Hi,  I have noticed that in your NIPS paper you prepare the S3DIS and ScanNet dataset by slice room into 1.5m by 1.5m blocks with 0.3m padding on each side, and themselves are not linked to loss in the training phase, nor used for prediction in the testing phase. but I am not find something refer to it, am I wrong? can you tell me some points?"
Coud you offer pretrained model for modelnet classification?  Thanks!
"Hello,  Thank you for your good paper and source code.   Now I have a question about how to work on Semantic3D dataset. In your code, the input of the dataset dim is 7, and I think it means you have used the RGB information for segmentation. So how about the result of it? I have not seen the result in your paper.  Also I want to use the pretrained network to test on the other outdoor point cloud dataset, which dose not have RGB information. Could you tell me how to do it? I mean if the RGB information is necessary for train pointcnn, and if I do not have it, the result would be bad?    Thanks."
None
"When I want to test your pretrained model of part segmentation, it shows me that 'Key xdconv_5_X_0/kernel not found in checkpoint'. It seems that your checkpoint does not match the model defined in your codes."
"Hi,  Your work is so exciting that I spent some days reading your codes.  When I want to use T-SNE  for the visualization of X-conv features,just as what you wrote in the paperâ€”â€”'Visualization of X-Conv features'ï¼Œbut I could not find your codes regarding to this.  Did you implement such visualization via tensorboard's  projector or other ways,could you please give me some hints?  Looking forward to your answer!"
Coud you offer pretrained model for shapenet part segmentation?
"Hello, I really appreciate your work, I am wondering could you please provide the pre processed ScanNet data for 3D object classification? The original dataset is too large and it may takes weeks for me to download. Thank you very much!"
"Hi,   In your NIPS paper, you claim that the number of points for ModelNET 40 training is sampled from _N(1024,_ 128**2). Bust I found a fatal bug in your tensorflow code. The sample_num_variance and sample_num_clip are set to (1//8) and (1//4) respectively, which are two int zeros. In that setting, the number of points for training is still 1024.  I wish you could revise them.  Thanks."
"Hi,   I am reading your tensorflow code, but I found the batch size used in the code is different from that declared in your NIPS paper. In the code, batch size is 128 for ModelNet40, however, it becomes 16 in your paper. What is the real batch size?    I also found that epsilon in Adam optimizer is 0.01, but epsilon for Adam is usually very small like 1e-7 or 1e-8. The purpose of epsilon is to prevent from dividing by zero, isn't it? Is it possible or are there other motivations for you to set epsilon so large?    Thank you!"
"python3 ./prepare_mnist_data.py -f ../../data/mnist       (tensor16) zhengkai@zhengkai-ThinkStation-P920:~/PointCNN/data_conversions$ python3 ./prepare_mnist_data.py -f /home/zhengkai/data/mnist  Traceback (most recent call last):    File ""./prepare_mnist_data.py"", line 14, in        from mnist import MNIST  ImportError: cannot import name 'MNIST'          it seems that the Mnist data downloaded before could not work ,please help me ,thank you.  "
"Hi,   I could not run the code using the commands you give.    (pointcnn) rui@rui-donny:/mnt/Ubuntu/PointCNN-master/pointcnn_cls$ ./train_val_modelnet.sh -g 0 -x modelnet_x3_l4  Train/Val with setting modelnet_x3_l4 on GPU 0!  (pointcnn) rui@rui-donny:/mnt/Ubuntu/PointCNN-master/pointcnn_cls$     It echos ""Train/Val with setting modelnet_x3_l4 on GPU 0!"" but exits immediately. What's the problem?"
"Hi,   I previously saw the former version of your paper submitted to arXive. In that paper, the accuracy on ModelNet40 is 91.7%. However, the accuracy becomes 92.2% and 92.5% for unaligned and pre-aligned settings respectively, which is a big improvement. So, could you explain what upgrades did you do in your new version so that around 0.5% improvement was achieved? Moreover, is 91.7%  in unaligned or pre-aligned setting?  Another question: considering 5000 points and normal vectors are available, why don't you do experiments with 5000 points and normal vectors? In that case, it will convince people better.  "
"Hi, I run your code, but failed. When I check pointcnn_cls_modelnet_x3_l4.txt, it says    > 2019-03-07 10:01:40.397380: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA  2019-03-07 10:01:40.582576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:   name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.607  pciBusID: 0000:02:00.0  totalMemory: 10.92GiB freeMemory: 10.76GiB  2019-03-07 10:01:40.582606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0  2019-03-07 10:01:40.797216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:  2019-03-07 10:01:40.797253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0   2019-03-07 10:01:40.797260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N   2019-03-07 10:01:40.797473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10405 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)  2019-03-07 10:01:47.215384: E tensorflow/stream_executor/cuda/cuda_blas.cc:652] failed to run cuBLAS routine cublasGemmBatchedEx: CUBLAS_STATUS_NOT_SUPPORTED  2019-03-07 10:01:47.215450: E tensorflow/stream_executor/cuda/cuda_blas.cc:2574] Internal: failed BLAS call, see log for details  Traceback (most recent call last):    File ""/home/zjj/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call      return fn(*args)    File ""/home/zjj/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn      options, feed_dict, fetch_list, target_list, run_metadata)    File ""/home/zjj/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun      run_metadata)  tensorflow.python.framework.errors_impl.InternalError: Blas xGEMMBatched launch failed : a.shape= ]]     ]]    What happened ?  Thanks in advance"
"Hi,  I tried to segment point cloud using semantic3d dataset. But in ""./train_val_semantic3d.sh -g 0 -x semantic3d_x4_2048_fps"" step, I gave a error like in the image below    !     And on the terminal scene, ""Train/Val with setting semantic3d_x4_2048_fps on GPU 0!""  was written only. How can I solve this problem?    Thanks for your answers"
"Hello!  I was trying to run the modelnet classification, but after running   `./train_val_modelnet.sh -g 0 -x modelnet_x3_l4`  I get error that . is not recognized as command, so i tried running just the .sh script  `train_val_modelnet.sh -g 0 -x modelnet_x3_l4`  but it asked for a program with which it should open it, I used Python (installed 3.6 version) and it shows errors. First it shows     > gpu=  >        ^  > SyntaxError: invalid syntax    which is basically first line of code, and if I hid the line (commented), the next line would be error.  What am I doing wrong?    Thanks in advance!"
"I'm trying to reproduce semantic segmentation results on shapenet with pointcnn. My working environment is Windows7. I've successfully compiled tf_sampling_g.cu.o, then I tried to compile tf_sampling_so.so. I used mingw to run g++ commands below (from tf_sampling_compile.sh ):    > g++ -std=c++11 tf_sampling.cpp tf_sampling_g.cu.o -o tf_sampling_so.so -shared -fPIC -L ""C:/Program Files/Anaconda3/envs/env/lib/site-packages/tensorflow"" -l tensorflow_framework -I ""C:/Program Files/Anaconda3/envs/env/lib/site-packages/tensorflow/include/external/nsync/public/"" -I ""C:/Program Files/Anaconda3/envs/env/lib/site-packages/tensorflow/include"" -I ""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0/include"" -lcudart -L ""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0/lib/x64/"" -O2 -D_GLIBCXX_USE_CXX11_ABI=0    but get error like:     > cpu_info.h:20:20: fatal error: intrin.h: No such file or directory  > #include    > ^  > compilation terminated.    Has anybody successfully compiled tf_sampling_so.so on Windows? Many thanks for your help.  "
I got the numerous ply data when  i ran scannet dataset on pointCNN.I thought every file represented a separate object in the whole sceneï¼Œbut its not. I found that every ply file is just a tiny part of the whole scene. How to Understand those Numerous ply Filesï¼Ÿ OR how can i get each part that be partitionedï¼Ÿ
"The labels are stored in single 7z file for both Val and Train sets. Please extract the labels in both places, as currently they are extracted into Train folder only:   "
"The inference FLOPs in PointNet paper is 440M/sample,  but yours is 14.70B/16sample, both are with 1024 input points. It seems these two FLOPs are actually not equal, right? "
"It seems like you use the accuracy computed by labels_tile, not label.  It is not the same as the metric of other methods, and it will bring about 0.5% - 1% unreasonable gains in classification    in line 185 of PointCNN/train_val_cls.py :   `t_1_acc_op, t_1_acc_update_op = tf.metrics.accuracy(labels_tile, predictions)`  I am not sure if I am wrong or something else.****"
"Hi,  Thank you for sharing your work. My 3D face dataset contains large `ply` files. For example:     And i should do the classification basing on these ply files but I have no idea how to implement it using PointCNN. What should i do?    Thank you.  tsly  "
"Did you verify this  unofficial pytorch implementation? I have try to run it, but failed to get a reasonal result. That repo even does not provide a test script."
"Hello,    I am asking myself to what extend you have analyzed the impact of the class imbalance present in many indoor room datasets. Take the ScanNet dataset for example (which I am especially interested in) and  :         As you can see, wall and floor account for over 60% of the points in the dataset, while only about 0.2% of the points belong to objects labeled as sink.    I have noticed that while you provide a method to specifically tune weights for each label, you set them all to the same constant value in your settings ( , for example). This means that your cross-entropy loss with softmax is unweighted, potentially shifting the model's predictions towards the most common classes.    In your paper, you only provide the overall accuracy as a metric for the segmentation task on ScanNet which could be a problematic metric in the light of this unbalanced distribution. Just getting the most dominant classes right and neglecting the less common classes would suffice for the model to reach a relatively high accuracy. But interestingly, when looking at visualizations of segmentation results of ScanNet, your model nevertheless seems to do a good job e. g. in identifying the chairs, even though they only make up 1.6% of the total points. So this could be less of a problem than it might seem.    Still, table 3 of your paper's supplementary material seems to support my suspicions:    !     Here you can see that for the similar S3DIS dataset, most models tend to just get simple structures like the ceiling, floor and wall right, which should account for most of the samples present in the dataset. This again points to the unweighted cross-entropy loss with softmax being problematic when dealing with unbalanced class distributions in the training data.    So my question is, have you experimented with adjusting the label weights and did you see any improvements when trying to compensate for the class imbalance? I guess that you did try to use the weighting, because you have implemented it. Did you have any interesting insights?"
"I am interested in semantic3d segmentation work, and I wonder how to generate the label of each point of semantic3d. "
"Hi, I got a problem that if the parameters of segmentation task for scannet dataset is not correct. The result calculated by me is 11.5M but 4.4M in your paper(NIPS 2018)."
"How did you fix the benchmark files to reach 9,305/2,606 instances, as you mentioned in issue #29 ?    Can you update the benchmark files with the fixed files (maybe scannet-labels.combined.tsv and classes_ObjClassification-ShapeNetCore55.txt in ./data_conversions)? Thanks."
"Thanks for the great work.  I noticed the point data in modelnet40 has six dimensions, ect. ` [ 0.07231558  0.7900504  -0.13183269  0.          1.          0.        ]`. I know the first three are the coordinate and the last three are some kind of features, but what do they definitely mean?  "
"I find that you use precision as the evaluation metric, but other works (e.g. pointnet, dgcnn) use accuracy as the evaluation metric. Could you tell me why don't you use accuracy? "
"Based on the commands(cat) about tu_berlin in README, the result is about 60%, while 67.72% in the arXiv version and 70.57% in the NIPS version. "
"Hi,  I have been successful to train the model with Semantic3D dataset. Then I try to get the test result, but there are all classes as 1 in the files after using the /evaluation/semantic3d_merge.py. Is there problem in the test process or the merge process?"
"models_folder=""../../models/seg/""  train_files=""../../data/semantic3d/out_part/train_data_files.txt""  val_files=""../../data/semantic3d/out_part/val_data_files.txt""    æ‚¨å¥½ï¼Œæˆ‘çœ‹åˆ°åœ¨ä½ ä»¬çš„train_val_semantic3d.shå¼€å¤´éƒ¨åˆ†ä¸­æœ‰ä»¥ä¸Šçš„ä»£ç ï¼Œä½†æ˜¯æˆ‘è¿è¡Œæ—¶å€™å‡ºçŽ°äº†ä»¥ä¸‹é”™è¯¯ï¼š  IOError: [Errno 2] No such file or directory: '../../data/semantic3d/out_part/train_data_files.txt'  è¯·é—®ä»£ç ä¸­è¿™ä¸ªæ–‡ä»¶å¤¹æ˜¯ä»€ä¹ˆæ—¶å€™ç”Ÿæˆçš„å‘¢ï¼Ÿ    åœ¨ä½ ä»¬é¦–é¡µç»™å‡ºçš„è®­ç»ƒè¿è¡Œsemantic3dæ•°æ®é›†çš„æŒ‡ä»¤ä¸­ï¼Œå…¶ä¸­ä¸€æ¡æ˜¯ï¼š  python3 prepare_semantic3d_data.py  æˆ‘æ€€ç–‘è¿™ä¸€æ¡æ˜¯ä¸æ˜¯ç¼ºå°‘äº†ä¸€äº›å‚æ•°ï¼Ÿå¸Œæœ›æ‚¨å¯ä»¥å¸®æˆ‘è§£ç­”ï¼Œéžå¸¸æ„Ÿè°¢ã€‚"
è¯·é—®s3disåˆ†å‰²ä½ ä»¬ç”¨äº†å¤šå°‘ä¸ªç‚¹çš„ï¼Ÿå¥½åƒs3dis_x8_2048_fps.pyå¹¶æ²¡æœ‰ç‚¹æ•°çš„è®¾ç½®ï¼Œä½ ä»¬çš„ç‚¹æ•°æ˜¯æ˜¯åœ¨ç”Ÿæˆæ•°æ®é›†çš„æ—¶å€™ç¡®å®šçš„å—ï¼Ÿä½ ä»¬æ˜¯ç”¨8192ä¸ªç‚¹çš„å—ï¼Ÿ
"Sorry to bother you but I'm having problems in training. It seems that it stuck in line 306 in train_val_cls.py(sess.run()). I have checked the log that there is no error message, but the program seems not running when it reaches this line."
how many points do you use for s3dis?
" Hi, I am able to compile the sampling file but can't use it. I am using python 2.7 and Tensorflow 1.7.0  When I want to import sampling in pointcnn.py, I got the following error :     However, when I compile sampling, I got no error.  My tf_sampling_compile.sh file is :      I would be very grateful if you give me some suggestions ! Thanks!"
"In your paper, you mentioned that you sample points to training  !   But I can only find that you randomly sample 1024 points for classification training but not random drop out.  Since you said in your paper that it's crucial for training, so I want to make sure of it.  Thanks!  "
"Hello, thanks for sharing your great work with us here!  I just downloaded the ScanNet dataset for semantic segmentation task, and I have finished the training and testing. When I wanted to visualize the results, I found that every ply file is just a tiny part of the whole scene. How can I visualize the result of the whole scene? I have no idea which files belong to the same scene. Does every h5 file contain several different scenes?"
"when i run  ./test_shapenet.sh -g 0 -x shapenet_x8_2048_fps -l ../../models/seg/pointcnn_seg_shapenet_x8_2048_fps_xxxx/ckpts/iter-xxxxx -r 10,it tell me  tensorflow.python.framework.errors_impl.NotFoundError: /home/hbzhang/tensorflow/PointCNN/sampling/tf_sampling_so.so: undefined symbol: _ZN10tensorflow6StatusC1ENS_5error4CodeENS_11StringPieceE      "
!   
!   
è¯·é—®ä½¿ç”¨sacnnetè¿›è¡Œåˆ†ç±»ä»»åŠ¡çš„æ—¶å€™æ˜¯ä½¿ç”¨poinnet++ä¸Šä¸‹è½½çš„scannetåŽ‹ç¼©åŒ…ï¼ŒåŽç¼€å.pickleçš„æ–‡ä»¶ä¹ˆï¼Œæˆ‘ä»¬ä¸‹è½½è¯¥æ•°æ®é›†åŽï¼ŒæŒ‰ç…§readmeä¸­çš„è¯´æ˜Žä½¿ç”¨scannet_split.pyç”Ÿæˆäº†6ä¸ªå­æ–‡ä»¶å¤¹train_data ï¼Œtrain_label ï¼Œ train_tarns å’Œval_dataï¼Œval_labelï¼Œval_transï¼Œä¹‹åŽå†æ¬¡ä½¿ç”¨prepare_scannet_cls_data.pyå¤„ç†æ—¶ä¼šå‡ºçŽ°å„ç§é”™è¯¯ï¼Œæœ‰æ—¶å€™æ˜¯æ‰¾ä¸åˆ°æ–‡ä»¶ï¼Œæœ‰æ—¶å€™æ˜¯è¶…å‡ºç´¢å¼•ã€‚  è¯·é—®æˆ‘ä»¬è¦å¦‚ä½•è§£å†³  è°¢è°¢ï¼
"Hi,   I run ./train_val_modelnet.sh -g 0 -x modelnet_x3_l4 then I got the log.txtï¼š(last two lines)  2018-11-06 11:58:42.213172-[Val  ]-Average:      Loss: 0.3046  T-1 Acc: 0.9153  T-1 mAcc: 0.8782  2018-11-06 11:58:42.875714-Done!    However,in your paper,the accuracy shows in Table 1 is 88.8%(mA) and 92.5%(OA) for Pre-aligned data.  How can I get the same accuracy as in your paperï¼Ÿ    Thanks~ï¼"
Each testing point cloud is sampled multliple times to make sure all the points are evaluated at least r (r = 10 in our experiments) times at testing time.  what it means in your paper  It seems that it is different from the pointnet?  Thanks a lot! 
"When I test the semantic3d data set, my computer has 64GB of memory. Memory Error appears when I prepare data and train. How much memory does it need?"
"My point cloud data only x, y, Z, Intensity values, these four dimensionsã€‚ how to trainï¼Ÿ"
"Hi, it seems that there is no test.sh file in folder `./pointcnn_cls`, so how can we evaluate the classification model?  Thanks!"
"Hello,  I want to do research about road mark element recognition, such as arrows, lane mark.  Can it work with your network? Do you have any good suggestions?   Thank you!"
"After I read the code, I think for the ShapeNet Part segmentation task, you don't even use the point normals and class-label one-hot vectors as inputs. Is this correct? I think PointNet++ and many other methods use them as inputs as well for the network training. Do you think such additional information would help PointCNN to obtain even better results?    Thank you!"
"hello ,when I excute the commad:   ./train_val_modelnet.sh -g 0 -x modelnet_x3_l4  it just print ""Train/Val with setting modelnet_x3_l4 on GPU 0!"",but no any other action,why???"
"Hi,    We are very grateful to Teacher Li's team for this work. I am so interested in it. And I have several questions during the research process.    When I use modelnet_x3_l4_w_fts.py for train and value for modelnet calssification, it get better result.  Here are some settings:  data_dim = 6  use_extra_features = True  with_normal_feature = True  with_X_transformation = True  Maybe use_extra_features means with_normal_feature?  But input data has only coordinate Information. If I use other extra feature than normal feature, what should I do?     Thank you in advance!"
"Good morning,    I am working on 3D segmentation/detection based on point clouds, and wanted to try your model and understand it better.   I did an implementation using PyTorch, but the execution time seems very slow mainly due to the KNN inside of the network. On a GTX 1080 for a batch of size 5, I have approximately an forward time of 0.3 seconds for 4000 points, knowing that I work with a one million point Pointcloud.   What performances do you have with your implementation.   Other than for the known 3D dataset, what would you reckon as the ""best"" solution for to ""cut"" a point cloud in smaller pieces ?     Yours faithfully    Justin "
"Hello,  I find that the segmentation network and classification network are trained independently, I wonder if these two networks can be combined in the convolution stage, then the network can split for different tasks, just like the architecture of pointnet/pointnet++.  Thanks."
"when I compile tf_sampling_so.so file some warning happened:  nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).  but the  tf_sampling_so.so compiled successfully  then I run the command:  ./train_val_shapenet.sh -g 0 -x shapenet_x8_2048_fps  error messages in pointcnn_seg_shapenet_x8_2048_fps.txt like that:  Traceback (most recent call last):    File ""../train_val_seg.py"", line 295, in        main()    File ""../train_val_seg.py"", line 127, in main      net = model.Net(points_augmented, features_augmented, is_training, setting)    File ""/home/whf/ZYM/PointCNN/pointcnn_seg.py"", line 11, in __init__      PointCNN.__init__(self, points, features, is_training, setting)    File ""/home/whf/ZYM/PointCNN/pointcnn.py"", line 64, in __init__      from sampling import tf_sampling    File ""/home/whf/ZYM/PointCNN/sampling/tf_sampling.py"", line 15, in        sampling_module=tf.load_op_library(os.path.join(BASE_DIR, 'tf_sampling_so.so'))    File ""/home/whf/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py"", line 58, in load_op_library      lib_handle = py_tf.TF_LoadLibrary(library_filename, status)    File ""/home/whf/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__      c_api.TF_GetCode(self.status.status))  **tensorflow.python.framework.errors_impl.NotFoundError: /home/whf/ZYM/PointCNN/sampling/tf_sampling_so.so: undefined symbol: _ZN10tensorflow8internal21CheckOpMessageBuilder9NewStringEv**    My environment as followsï¼š  **gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04.4)   tensorflow 1.6.0**    "
"Namaste,    I ran into this error:  `FileNotFoundError: [Errno 2] No such file or directory: '../../data/mnist/train-labels-idx1-ubyte'`    Turned out the data was here:  `../../data/mnist/zip`    I copied to the proper folder. Poor man's solution :D    Idea: There might be some folders in the source-code that are a little off.    Best,  Tristan    P.S.: You can usually automatically check such things with unit-testing. Py PyPi ""unittests"" works like a charm.  "
"Namaste,    what I really like is the ""boxed"" approach in your code-base. Just plug in the data, create a settings-scenario and then train. This is excellent for many, many use-cases. Good work!    I wonder: How feasible would it be to open up to a certain degree? I am thinking about making the x-Convolution available as a Keras-layer. This seems to be the most powerful component of the overall approach. And I guess it would make sense to make it highly reusable. This would open the door to many more experiments. Representation learning, triplet-loss, auto-encoders, even GANs to a certain degree.    What do you think?    Best,  Tristan"
"Namaste,    I read in your paper, that you consider combining PointCNNs and CNNs. That is, predicting on both point-clouds and RGB-images in a multi-input-neural-net-scenario.    I have a good feeling that this is a great direction. Your paper is from January. Can you say anything about this great idea of yours?    Best,  Tristan"
"Namaste!    I your paper you mention a ""stress test on small number of input points"" and talk about some great results!    I am currently evaluating PointCNN against a use-case of mine: A regression on point-clouds. Predicting the height of measured ""objects"".     Currently we have point-clouds with 30k - 38k points. To the best of my knowledge PointCNN delivered great results with point-clouds with maximum size of 2048 points. We would have do downsample. Do you have any ideas/insights about proper downsampling strategies?    Best,  Tristan"
"I find that the parameter num_epochs(/pointcnn_seg/s3dis_x8_2048_fps.py) is 1024,but the parameter num_train(/train_val_seg.py) is calculated by a part training data rather than all training data."
"Thank you for your outstanding contribution! I've been working on 3D point clouds recently, but I found that there is no folder split_data, I would appreciate it very much if you could provide it. Or maybe you fogot update the file READEME.md(data_conversions/)? If it is, did some data path error is there between those data prepare files?"
"I'm trying to compile **tf_sampling_so.so** on Windows but no success.    I've compiled **tf_sampling_g.cu.o**, then I tried to compile **tf_sampling_so.so** with below command (taken from **tf_sampling_compile.sh** and change to the correct directories):    > g++ -std=c++11 tf_sampling.cpp tf_sampling_g.cu.o -o tf_sampling_so.so -shared -fPIC -L ""C:/Users/Ben/Anaconda3/envs/tensorflow/lib/site-packages/tensorflow"" -ltensorflow_framework -I ""C:/Users/Ben/Anaconda3/envs/tensorflow/Lib/site-packages/tensorflow/include/"" -I ""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0/include"" -lcudart -L ""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0/lib64/"" -O2 -D_GLIBCXX_USE_CXX11_ABI=0    but received this error:    > C:/Users/Ben/Anaconda3/envs/tensorflow/Lib/site-packages/tensorflow/include/tensorflow/core/platform/windows/cpu_info.h:20:20: fatal error: intrin.h: No such file or directory   #include                        ^  compilation terminated.    Really hope there is some guide for compiling and working on Windows (or an alternative method to do the Furthest Point Sampling process)."
cant find the tf_sampling_so.so  `NotFoundError: /home/yangll/PointCNN-master/sampling/tf_sampling_so.so: cannot open shared object file: No such file or directory  `
"block_data_root = ""../../data/scannet/scannet_split_dataset/val_data/01/""  block_label_root = ""../../data/scannet/scannet_split_dataset/val_label/01/""  block_trans_root = ""../../data/scannet/scannet_split_dataset/val_trans/01/""  block_pred_root = ""../../data/scannet/scannet_split_dataset/val_data/pred_4/01/""    save_voxel_re = True    seg_pred_out_root = ""../../data/scannet/scannet_split_dataset/val_data/pred_4/out/seg_pred_voxel/""  seg_label_out_root = ""../../data/scannet/scannet_split_dataset/val_data/pred_4/out/seg_label_voxel/""  pts_out_root = ""../../data/scannet/scannet_split_dataset/val_data/pred_4/out/pts_voxel/""    The origin code is here. I can't find scannet_split_dataset in my pc. I used scannet pickle files provided by pointnet++. Should I download the origin scannet? Or maybe it's an update bug that you missed?"
"sorry ,i cant find the filefold named 'split_data',is there the filefoldï¼Ÿ  `For big scene point cloud datasets like **Scannet** and **S3DIS**, we split them into small blocks for training:     `"
"during running the code about segmentating the semantic3d dataset,following problem comes up,i can you tell what is the reason and how to resolve?  `2018-09-22 12:27:32.108646-Loading ../../data/semantic3d/train/sg28_station4_intensity_rgb.txt...  Traceback (most recent call last):    File ""prepare_semantic3d_data.py"", line 241, in        main()    File ""prepare_semantic3d_data.py"", line 50, in main      xyzirgb = np.loadtxt(filename_txt)    File ""/usr/local/lib/python3.5/dist-packages/numpy/lib/npyio.py"", line 1092, in loadtxt      for x in read_data(_loadtxt_chunksize):    File ""/usr/local/lib/python3.5/dist-packages/numpy/lib/npyio.py"", line 1016, in read_data      % line_num)  ValueError: Wrong number of columns at line 75685977`"
None
"Hey Guys,     I got some question regarding to the X-Conv Operator. I have some hard issues to unerstand it correctly.  Here is my interpretation so far:    1. First you learn a X-transformation matrix from representative set of points. The representative points are sampled from the original point set.    2. You build n local regions around every representative point with k neighbors.    3. You aggregate a feature from each local region to create a feature-map F* that includes all the features from n local regions.    4. You weight and permute the whole feature map F* with the learnt X-transformation to bring it in some canonical order => output is a ordered feature-map Fx    5. Finally, typical convolution between K and Fx.    I think I did not get that right. Could you please explain me the meaning of K in the typical convolution part. I don't understand as well if you are building a feature-map per local region, order it with the X-transformation and then convolve it. Or, if you aggregate a big feature-map from all the extracted local region features, order it with X-transformation and convolve that.  Do you learn the X-Transformation from the representative set of points or the ""original"" points?    Thank you in advance!    BG Roman"
"hi, i met a problem, when I run the code as followed(ubuntu 16.04LTS, tensorflow 1.6)  ""python ./download_datasets.py -d modelnet""  then something was showed,  ""aise SSLError(e, request=request)  requests.exceptions.SSLError: HTTPSConnectionPool(host='shapenet.cs.stanford.edu', port=443): Max retries exceeded with url: /media/modelnet40_ply_hdf5_2048.zip (Caused by SSLError(SSLError(""bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')],)"",),))  (tensorflow) ""  what's wrong?"
"Hello,  When I'm trying to test segmentation following the instructions `./test_shapenet.sh -g 0 -x shapenet_x8_2048_fps -l ../../models/seg/pointcnn_seg_shapenet_x8_2048_fps_xxxx/ckpts/iter-xxxxx -r 10`, I get an error as shown below:     I didn't modify the test file, so is this a bug?  Thanks very much!"
"Hi Prof. Li,    I ran the code of PointCNN from your group recently, and discovered a small discrepancy between the implementation and your paper. It is mentioned in the paper that in X convolution, both MLP and MLP_delta(in algorithm 1) should not include batch normalization. However, I found in your implementation that batch normalization is actually turned on in all these modules and actually turning on such batch-norms do help me get the results claimed in the paper. So is it not that important to care about whether to have such batch norm layers or not? Thanks for your reply~    Ken"
"Hi, thanks for your work. I'm trying to run this package in my laptop.    While running **python3 ./download_datasets.py -d modelnet**, so many errors occurs.    Here is part of the error msg:  _Traceback (most recent call last):    File ""/anaconda3/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py"", line 441, in wrap_socket      cnx.do_handshake()    File ""/anaconda3/lib/python3.6/site-packages/OpenSSL/SSL.py"", line 1806, in do_handshake      self._raise_ssl_error(self._ssl, result)    File ""/anaconda3/lib/python3.6/site-packages/OpenSSL/SSL.py"", line 1546, in _raise_ssl_error      _raise_current_error()    File ""/anaconda3/lib/python3.6/site-packages/OpenSSL/_util.py"", line 54, in exception_from_error_queue      raise exception_type(errors)  OpenSSL.SSL.Error: [('SSL routines', 'ssl3_get_server_certificate', 'certificate verify failed')]_    I'm just wondering if this is the configuration problem or something wrong with my anaconda. Do I need to do minor changes on system files before running your package?      Sincerely,  Rowen"
æˆ‘åœ¨è°ƒè¯•pointcnnåœ¨mnistä¸Šçš„è¯†åˆ«ï¼Œä½†æ˜¯å‡†å¤‡mnistæ•°æ®çš„æ—¶å€™ï¼Œé‡åˆ°äº†ä¸€äº›é—®é¢˜ã€‚è¢«å›°åœ¨from mnist import MNISTè¿™å¥ä»£ç ä¸Šï¼Œæ‰¾ä¸åˆ°è¿™ä¸ªæ–‡ä»¶ï¼Œä¸çŸ¥æ‚¨æ˜¯å¦èƒ½æä¾›ï¼Ÿ
"Hi,  I notice that in prepare_s3dis_data.py and prepare_semantic3d_data.py, the coordinates of data are split an then concatenate as the order of x, z, y    `                    x, y, z = np.split(block_points, (1, 2), axis=-1)`  `                   block_xzyrgb = np.concatenate([x, z, y, block_rgb], axis=-1)`    Could you please tell me the purpose of using xzyrgb instead of xyzrgb in data preprocessing?"
"Hi,    I tried to reproduce the results for Scannet Dataset on Segmentation Task but I cannot reproduce the mentioned results. The accuracy just around 77.76% (Validation) and ~85% for training data.  I used the ""pointnet++ preprocessed data"" with provided scannet-seg hyper-parameters.     Can you give me a clue what I should do to reproduce the result? And also how long do you train the model to achieve the published result?      "
"Hi,    I'm a little curious about the training time on ModelNet40.    How many epochs it use to train the model on ModelNet40?    How long  it takes to train the model on ModelNet40?    Thank you."
"Hey Guys,     I already trained PointCNN with my own dataset where each pointcloud includes normals for part segmentation with the following settings:  `data_dim = 6 `                                                                                                                                                                                                                                         and  `use_extra_features = True`    I used the same format that is used for the shapenet dataset, so for example my .pts files from my test data have the following format per line:    `26.5564 1.52368 22.7281 0.315015 0.0 0.949087 ` with x, y, z, normal1, normal2, normal3     How can I make sure that PointCNN also uses normals from my testdata as additional information/features for the validation?   I used the same settings as for the training, and deleted the following line   `assert (point_num == len(coordinates))` from the test_shapenet_seg.py file.    Am I missing something? The result does not seem pretty different compared to the training and the validation of the same dataset without using normals for training.    BG Roman  "
"hi,  when i run the code with semantic3d dataset,there are many question.  eg.1.when i run ./train_val_semantic3d.sh -g 0 -x semantic3d_x8_2048_k16         show the probrom:/usr/bin/env: â€˜bash\râ€™: No such file or directory        2.and i find the file you provide have some missing,such as ""test_semantic3d.sh"" and so on  and may you provide the semantic3d dataset of ''.h5'' in the files? "
"Hello, I'm trying to run the example on windows, so I don't have an option to use bashfiles (at least the ones I tried failed).    It is my understanding that the last line of the file train_val_mnist.sh, is the one that actually puts everything together. So I have tried to set the values manually instead of using the bashfile code, however, the last parameter ""setting"" seems to come from a txt file.     ""CUDA_VISIBLE_DEVICES=$gpu python3 ../train_val_cls.py -t $train_files -v $val_files -s $models_folder -m pointcnn_cls -x $setting > $models_folder/pointcnn_cls_$setting.txt 2>&1 &""    How can I obtain the values from this txt file manually?    "
"Hello, can we use the PoinCNN for online classification? If so do you have any examples? classification from a data source that does not have multiple angles?  "
"  Hello,I  am a student from Shanghai Jiao Tong University. Our team is working on  refining your PointCNN's performance of classification task. However,  we have difficulties getting the results about classification task based  on data from Modelnet when running your codes in the command window  under the Ubuntu 16.04 system. It seems that the GPU is not working  although we implement your instructions in Github Readme.md.     File 1 shows our code in the command window. The modelnet files have  been downloaded successfully as shown in file 2. And we have put the  folder:PointCNN-master on the Desktop. Although the last line suggests that training task   has been implemented by GPU 0, we can see that this is not the case when watching its low utilization ratio as shown in file 3.     We would like to ask what is the reason of this phenomenon, and we  really appreciate your attention and feedback. Thank you for your time!  !   !   !     "
"Hello,I am a student from Shanghai Jiao Tong University. Our team is working on refining your PointCNN's performance of classification task. However, we have difficulties getting the results about classification task based on data from Modelnet when running your codes in the command window under the Ubuntu 16.04 system. It seems that the GPU is not working although we implement your instructions in Github Readme.md.    File 1 shows our code in the command window. The modelnet files have been downloaded successfully as shown in file 2. And we have put the folder:PointCNN-master on the Desktop.    Although the last line suggests that training task  !   !   !        has been implemented by GPU 0, we can see that this is not the case when watching its low utilization ratio as shown in file 3.    We would like to ask what is the reason of this phenomenon, and we really appreciate your attention and feedback. Thank you for your time!"
"Hello,  I have an outdoor dataset with about 32k points per pointcloud.  So sample_num = 32768 and I cannot decrease it anymore because I would be losing valuable information. The data is already downsampled.    The current architecture is this:           But it keeps running out of memory.    Could you please guide me as to what could be the most efficient method of decreasing the architecture size without degrading the results too much.    Thank you."
"I have in the past able to successfully compile the sampling file using python2 for PointNet++.    However, I am unable to compile it and use it with PointCNN.    This is the bash file:    `#/bin/bash  PYTHON=python3  CUDA_PATH=/usr/local/cuda  TF_LIB=$($PYTHON -c 'import tensorflow as tf; print(tf.sysconfig.get_lib())')  TF_PATH=$(python3 -c 'import tensorflow as tf; print(tf.sysconfig.get_include())')  $CUDA_PATH/bin/nvcc tf_sampling_g.cu -o tf_sampling_g.cu.o -c -O2 -DGOOGLE_CUDA=1 -x cu -Xcompiler -fPIC  g++ -std=c++11 tf_sampling.cpp tf_sampling_g.cu.o -o tf_sampling_so.so -shared -fPIC -L$TF_LIB -ltensorflow_framework -I $TF_PATH/external/nsync/public/ -I $TF_PATH -I $CUDA_PATH/include -lcudart -L $CUDA_PATH/lib64/ -O2 -D_GLIBCXX_USE_CXX11_ABI=0`    where the two commands give this output in the terminal    `$($PYTHON -c 'import tensorflow as tf; print(tf.sysconfig.get_lib())')  -bash: /home/aakhtar/.conda/envs/anique2/lib/python3.6/site-packages/tensorflow: Is a directory`  and  `$(python3 -c 'import tensorflow as tf; print(tf.sysconfig.get_include())')  -bash: /home/aakhtar/.conda/envs/anique2/lib/python3.6/site-packages/tensorflow/include: Is a directory`    so I know both these commands are working. The cuda folder is correct too.  When I compile this, it gives no errors.     However, when I run the train_val_seg.py file, I get the following error:    `Traceback (most recent call last):    File ""../train_val_seg.py"", line 268, in        main()    File ""../train_val_seg.py"", line 119, in main      net = model.Net(points_augmented, features_augmented, is_training, setting)    File ""/home/aakhtar/PointCNN_v1/pointcnn_seg.py"", line 12, in __init__      PointCNN.__init__(self, points, features, is_training, setting)    File ""/home/aakhtar/PointCNN_v1/pointcnn.py"", line 64, in __init__      from sampling import tf_sampling    File ""/home/aakhtar/PointCNN_v1/sampling/tf_sampling.py"", line 15, in        sampling_module=tf.load_op_library(os.path.join(BASE_DIR, 'tf_sampling_so.so'))    File ""/home/aakhtar/.conda/envs/anique2/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py"", line 58, in load_op_library      lib_handle = py_tf.TF_LoadLibrary(library_filename, status)    File ""/home/aakhtar/.conda/envs/anique2/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__      c_api.TF_GetCode(self.status.status))  tensorflow.python.framework.errors_impl.NotFoundError: /home/aakhtar/PointCNN_v1/sampling/tf_sampling_so.so: undefined symbol: _ZN10tensorflow8internal21CheckOpMessageBuilder9NewStringEv`    I am assuming, this error arises from the fact that the sampling file has not been compiled properly.  I am using a Conda environment.  outside the Conda environment, I have got python 3.4.8, and Tensorflow 1.6.0  however, inside the environment, I am using 3.6.6, and Tensorflow 1.6.0  The Cuda link is from outside the environment.    Can you please help me understand as to what is going wrong.  Thanks."
"I notice for the classification task, this project rotates data just along one axis.  Although data augment supports arbitrary rotation, you don't use it.    Will the arbitrary rotation further increase performance?  "
"Hi,    Thanks for sharing this great work!     I just need a small clarification on your code, specifically when you set the hyperparameters for your model.     # Classification Problems  Looking at the ModelZoo for ModelNet40 (Figure 8-a) in the paper), I see:  **Paper (Model Zoo for ModelNet )**          **Code (settings for ModelNet    )**  (from :         Looking at these two dictionaries, it all seems to be using almost all the values as shown in the paper,except, I don't understand the following parts:    1. From `xconv_params`, there's a multiplication by 3 (`x=3`) for C. Why? I guess the question is: what is the meaning of this variable?     2. Related to this `x`, where does 16, 32, 64, 128 (by this order) for C come from? It seems to be using half of what it described in the paper, for each of the layers, but each is multiplied by 3, so I'm a bit puzzled.     3. From `fc_params`, the final FC layers have 128 and 64 units respectively (I know it is also then followed by a FC layer with  40 units , it's just not part of the settings explicitly). Again, this seems to be half of what's shown in the paper.      # Segmentation Problems  Looking at the ModelZoo for ShapeNet Parts (Figure 8-e) in the paper), I see:        **Code (from  )**       Questions:    1. Why is x set to 8 for segmentation problems? As opposed to ModelNet40, multiplying x*[32,32,64,128] yields the sames values as shown in the paper, which makes perfect sense to be, so is it maybe a coincidence? It also make sense for `fc_params`    2. `xconv_params` makes sense to me, however `xdconv_params` seems to add an additional layer, resulting in 9 XConv (4+5) layers instead of 8, as shown in the paper. Is this related to 'pts_layer_idx', 'qrs_layer_idx' ?    Sorry for the long post, but I hope this will be beneficial to others as well :)    Many thanks in advance, and thanks once again for the great work!"
"I tried to train and test S3DIS on TITAN X of 12GB memory. And because of OOM, I changed the batchsize from 16 to 12.  The model is trained on Area2~6, and tester on Area1.  The result is listed as follows:    > Overall accuracy: 0.6220546181741131  IoU:  0.4692968354568871  0.6634375532984669  0.6585915255348683  0.6286324167941533  0.4089729327212604  0.4101586582534994  0.6679818077667772  0.5368703770020128  0.4935388803232025  0.47200222643714557  0.3127629041882209  0.38665093766294856  0.027841126074583687  mean IoU:  0.4720567831933866    The mean IoU of my result is 47.21%, which is much less than your result: segmentation mean IoU on S3DIS (62.74%).  I am confused that whether the difference between batchsize could bring such large difference.    Also, is it possible that I can train the model on two GPUs at the same time? In this way, I may set the batchsize as 16 and evaluate again."
"Hey Guys,     I am playing around with PointCNN on my own dataset which is pretty similar to shapenet. I may wonder if it could be possible to get a per point segmentation for each pointcloud? I just tested it with my own test.h5 files but it seems that I can only get the category (class) for each pointcloud.  Correct me if I am wrong :)    Many Thanks so far!    Greetings, R."
"I tried to train Shapenet on P40 with 20 GB memory but it still needs more memory.   The config is the default one which you provided with batch_size = 24 but the log file told me that the memory is not enough for creating new blocks.  By the way, could you please tell me about your training config?  Thank you!"
"Hiï¼      I tried to use the PointCNN to process ourdoor scene point clouds. I want to get the result of point clouds semantic segmentation in outdoor scene. I input my point clouds with labels, and after iterative train got the model. And I get this result:  !   I feel that this is a voxel processing of the point cloud data in the original scene, because the output results appear to be voxel.  The accuracy of the result is not high.  I used the S3DIS repo to evaluate.  I want to try semantic3D repo, but it seems like a file is missing(test_semantic3d.sh) Could you add this file?    "
"As the title says, could you offer the **pre-trained** model for **semantic segmentation** (on S3DIS and    Scannet )  prediction. We want to use the model as a prior for fulfilling follow-by other tasks.  Thanks !"
"line 113-114  In load_cls()     It seems if there are normal vectors in the file, coordinates will be used again as features for the the first X-Conv layer.    But in the paper, it says:     ""For input point cloud without any additional features, i.e., F is empty, the first X-Conv layer uses only FÎ´ .""    So does this step improve the performance?  If I concatenate  coordinates with normal vectors, will it help?"
There is error in test cifar10 when I follow `README.md`  Please change to       
"Hi ,thanks for your share!  Because my GPU doesn't satisfy the requirement of TensorFlow with GPU, I just use the CPU to do the training. It does work, but I find  it occupy 100%CPU and 98%RAM (CPU: i5, RAM: 8G) when I train the shapenet_x8_2048_fps for segmentation, even though I just load only one .h5 file ( which is only 162Mb).  There is anything wrong with my training? Or everything is OK?  Waiting for your reply. Thank you very much!"
"Hi ,thanks for your share!  When I try to train the shapenet_x8_2048_fps, I accept an error :  File ""../train_val_seg.py"", line 51, in main      step_val = setting.step_val  AttributeError: module 'shapenet_x8_2048_fps' has no attribute 'step_val'  I checked shapenet_x8_2048_fps.py and found it did not contain ""step_val "". What value should be set for step_val ?  Awaiting for your reply .Thank you very much!"
"Hi,     I'm having some issues with the preprocessing of ScanNet dataset for classification.    When I try to run the command:  python scannet_extract_obj.py -f ../../data/scannet/scannet_dataset_download/data/ -b ../../data/scannet/scannet_dataset_download/benchmark/ -o ../../data/scannet/cls/    The script crashes with this error:    Traceback (most recent call last):    File ""scannet_extract_obj.py"", line 375, in        main()    File ""scannet_extract_obj.py"", line 336, in main      sceneid = scene.strip().split(""scene"")[1]  IndexError: list index out of range    The scene variable when the crash occurs is with the string:  scene:  ../../data/scannet/scannet_dataset_download/data//h5_scannet    Has anyone had the same problem?  Thanks"
"HI,    I trying to train and evaluate for the ModelNet40 data but tensorflow seem to be coming up with the following error:        Any advice?    Thanks,    Thibault"
"Thanks for sharing your great work!    I am a beginner of deep learning models used on point clouds. I am very interested about your achievement. However, I have one question when conducting Classification operation on ModelNet40. You said performed ""/train_val_modelnet.sh -g 0 -x modelnet_x3_l4"", and it output ""Train/Val with setting modelnet_x3_l4 on GPU 0!"", then nothing happened. I would like to know how to achieve classification function, can you give the details steps after getting the above message? My operating environment is Ubuntu16.04 + python3.5  Thank you for your kind assistance.  "
"    parser = argparse.ArgumentParser()      parser.add_argument('--path', '-t', help='Path to data', default='/home/data/modelnet40_ply_hdf5_2048/train_files.txt')       parser.add_argument('--path_val', '-v', help='Path to validation data',default='/home/data/modelnet40_ply_hdf5_2048/test_files.txt')      parser.add_argument('--load_ckpt', '-l', help='Path to a check point file for load')      parser.add_argument('--save_folder', '-s', help='Path to folder for saving check points and summary', default='/home/data/save/')      parser.add_argument('--model', '-m', help='Model to use', default='pointcnn_cls')      parser.add_argument('--setting', '-x', help='Setting to use', default='modelnet_x3_l4_p032')      args = parser.parse_args()    i changed your code as above,and i got an error blow    InvalidArgumentError (see above for traceback): clip_value_min and clip_value_max must be either of the same shape as input, or a scalar. input shape:  ]]       thanks for your help!        "
"When I run test_s3dis.sh, I find that in the beginning, the contents of output "".seg"" files are normal, which are indexes of max point_seg_probs. But the output "".seg"" files corresponding to the files which are later processed are abnormal. The contents of these "".seg"" files are all ""-1"".    And I run test_general_seg.py in ""debug"" mode of PyCharm for several ""for"" loops, the output "".seg"" files are all normal, but it become ""-1"" again once using ""run"" mode of PyCharm for later processed files.    In test_general_seg.py, line 136-153:     The seg_idx write to file_seg is np.argmax(point_seg_probs), which can't be ""-1"".     I am really confused, could you please help me ?"
"Recently I studied the semantic segmentation of point cloud data.  I mainly process point clouds in outdoor scenes. I found that this repo has been changed very frequently. When I run test_s3dis.sh in S3DIS, I found some errors:  1. In your README.md, 162 line, ""./test_s3dis.sh -g 0 -x s3dis_x8_2048_fps_k16 -l ../../models/seg/s3dis_x8_2048_fps_k16_xxxx/ckpts/iter-xxxxx -r 4"", I notice the ""s3dis_x8_2048_fps_k16"" is wrong, because in ""pointcnn_seg"" folder, there is ""s3dis_x8_2048_k16_fps""   2. In test_s3dis.sh, 68 line, ""CUDA_VISIBLE_DEVICES=$gpu python3 ../test_geneal_seg.py -t ../../data/S3DIS/out_part_rgb/val1.txt -f ../../data/S3DIS/out_part_rgb/Area1_data/ -c ../../data/S3DIS/out_part_rgb/categories.txt -l $ckpt -m pointcnn_seg -x $setting -r $repeat $save_ply""  â€œtest_geneal_seg.pyâ€ miss ""r"" in ""general""  3. In ""test_general_seg.py"", 91 line, ""net = model.Net(points_sampled, features_sampled, num_class, is_training, setting)"" a parameter ""num_class"" is may be redundantï¼Œ because in pointcnn_seg.py, 10 line,  "" def __init__(self, points, features, is_training, setting):""  without ""num_class"" parameter.   I also found some other errors in the process of runningâ€¦â€¦  I would like to ask a few questions:  1. In ""#pointcnn-master#/data_conversions/split_data/s3dis_split.py"" , Can you describe the idea of dividing point cloud data into space volexs for quantitative statistics? And why do you want to downsampling? Can the final test result restore the original point cloud number?  I'm running point cloud data on servers in our lab and I hope to keep in touch with your team.  My e-mail: lybluoxiao@qq.com  "
"When I run this command""./test_shapenet.sh -g 0 -x shapenet_x8_2048_fps -l ../../models/seg/pointcnn_seg_shapenet_x8_2048_fps_xxxx/ckpts/iter-xxxxx -r 10""  The error occured in test_seg.py line 94:probs_op = net.probs.It tells me that no probs attribute in net.  The net is defined in pointcnn.py and I didn't find the self.probs in object point cnn.  And I found that in the previous version of point cnn.  If this is a bug, can you fix it?  Thank you! "
"I try to test shape net dataset on the trained model,  however, there is an error that when the code run into the pointcnn.py, the terminal tells me that the net has no attributes about probs(simply there is >>no net.probs in the test_seg.py ).  Could you tell me how to change the code to make it run?  Thanks a lot!"
"When I run the segmentation task on shapenet dataset, there is an error about max() is empty.  The code is in the prepre_partseg_data.py  55th line(category_label_seg_max_list is empty).  I am confused about it.  Thank you!  "
"/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.    from ._conv import register_converters as _register_converters  Traceback (most recent call last):    File ""../train_val_seg.py"", line 271, in        main()    File ""../train_val_seg.py"", line 126, in main      t_1_acc_op, t_1_acc_update_op = tf.metrics.precision_at_k(labels_sampled, logits, 1)  AttributeError: module 'tensorflow.python.ops.metrics' has no attribute 'precision_at_k'    And I find that the API 'precision_at_k' only exists in tensorflow 1.5 and above, how could you test the code in the environment tensorflow 1.4?    Thank you very much!"
"Hi, @yangyanli ,    According to these   in pointcnn.py, ""links"" are used there. Could you elaborate a bit on its function? Does it mean the connections/kernel_weights/feature_maps between two layers?     THX!"
"Hi, @yangyanli ,    I found that there are some   which uses separable convolution. What's the purpose on using it rather than conventional convolution operation? I didn't find any explanation on it in PointCNN paper.  Is the performance reason or memory cost reason on using separable convolution?    THX!  "
"Hi, authors,    In funtion   , there are many input variables. Could you help to clarify their meaning? According to PointCNN paper. I guess them as follows. Pls help to correct them if there's any mistake:         Moreover, what does ""P"" means ?    THX!      "
"1.I have tried your framework,it did works well.But now i want use it to detect fruits,such as orange.How can i fine tune the net to achieve my target?Should i make 3D datasets which just includes orange?Just like make VOC2007 datasets using 2D conv?I plan to use scannet datasets form to make my own datasets,is this proper?  2.ä½ å¥½ï¼Œæˆ‘è¯•ç”¨äº†ä½ ä»¬çš„ç½‘ç»œï¼Œç”¨çš„æ˜¯scannetæ•°æ®é›†è·‘çš„ï¼Œæ•ˆæžœç¡®å®žä¸é”™ã€‚æˆ‘çŽ°åœ¨æƒ³æ‹¿å®ƒæ¥è¯†åˆ«æŸ‘æ©˜ï¼Œå®žçŽ°è¿™ç§ç›®æ ‡éœ€è¦è°ƒä¼˜ç½‘ç»œå§ï¼Œé‚£ä¹ˆæ•°æ®é›†æ€Žä¹ˆåšå‘¢ï¼Œè‡ªå·±ä»¿é€ scannetæ•°æ®é›†å°±å¯ä»¥å—ï¼Œä¹‹å‰è‡ªåˆ¶VOC2007æ•°æ®é›†å·²ç»æ˜¯è½»è½¦ç†Ÿè·¯äº†ï¼Œä½†æ˜¯3Dæ•°æ®é›†è¿˜ä¸æ˜¯å¤ªäº†è§£ï¼Œæ„Ÿè§‰è¿™ä¸ªç½‘ç»œçš„hyper-paraæ¯”2Dçš„ç½‘ç»œè¦å¤šå•Šï¼Œä¼šä¸ä¼šå¾ˆéš¾è°ƒï¼ŸæœŸå¾…æ‚¨çš„å›žå¤ï¼"
"I downloaded the scannet dataset and run the code scannet extract object. However, there are only 7815 instances in the trainset and 2170 in the testset compared to 9035 and 2060 respectively stated in the PointCNN paper.     Also when i run the conversion code, I faced ""index out of range error"" when extract ShapeNetCore 55 labbel: ShapeNetCore55 = line_s[11] . I removed the strip function line_s = line.strip().split('\t') and it works but result in inconsistent train/test size.   Can you have a look of this issue. Thank you. "
"I follow the command:  > cd data_conversions  > python3 ./download_datasets.py -d modelnet  > cd ../pointcnn_cls  > ./train_val_modelnet.sh -g 0 -x modelnet_x2_l4    However, it will generate many model files iteratively. And the error is:    > File ""/home/bandary/.conda/envs/my_env/lib/python3.6/shutil.py"", line 359, in copytree  >     raise Error(errors)  > shutil.Error: [('/home/bandary/Documents/PointCNN/models/cls/pointcnn_cls_modelnet_x2_l4_6110_2018-04-13-10-58-25/PointCNN/models/cls/pointcnn_cls_modelnet_x2_l4_6110_2018-04-13-10-58-25/PointCNN/models/cls/pointcnn_cls_modelnet_x2_l4_6110_2018-04-13-10-58-25/PointCNN/models/cls/pointcnn_cls_modelnet_x2_l4_6110_2018-04-13-10-58-25/PointCNN/models/cls/pointcnn_cls_modelnet_x2_l4_6110_2018-04-13-10-58-25/PointCNN/models/cls/pointcnn_cls_modelnet_x2_l4_6110_2018-04-13-10-58-25/PointCNN/models/cls/pointcnn_cls_modelnet_x2_l4_6110_2018-04-13-10-58-25/PointCNN/models/cls/pointcnn_cls_modelnet_x2_l4_6110_2018-04-13-10-58-25/PointCNN/models/cls/pointcnn_cls_modelnet_x2_l4_6110_2018-04-13-10-58-25/PointCNN/models/cls/pointcnn_cls_modelnet_x2_l4_6110_2018-04-13-10-58-25/PointCNN/models/cls/pointcnn_cls_modelnet_x2_l4_6110_2018-04-13-10-58-25/PointCNN/models/cls/pointcnn_cls_modelnet_x2_l4_6110_2018-04-13-10-58-25/PointCNN/models/cls/pointcnn_cls_modelnet_x2_l4_"
"I am confused about line 95~99 in s3dis_upsampling.py:  `    for k, vid in enumerate(uvidx):`  `        voxel_label_dict[vid] = seg[k]`    `    for vid in vidx:`  `        ori_seg.append(voxel_label_dict[vid])`    Could you please tell me the function of these lines ?    And I trained the model on Area2ï½ž6 of S3DIS, then tested on Area1.  If I run s3dis_upsampling.py before running eval_s3dis.py, the output of eval_s3dis.py is:    > [741860, 1081219, 927535, 993140, 209411, 126037, 231132, 105981, 218610, 214055, 21996, 204034, 70819]  [339878, 330121, 640186, 811327, 88785, 99274, 197668, 58156, 122522, 143403, 10943, 116397, 2187169]  [65307, 102514, 164596, 287139, 11102, 36893, 44290, 15898, 16273, 24691, 1482, 35241, 4100]  Overall accuracy: 0.1573169259996786  IoU:  0.06425128710163307  0.07832515552105475  0.11730672605790646  0.18923990066748916  0.03867026130814298  0.19580401023256802  0.11518556084367117  0.10724573155512382  0.050092501669955276  0.07419906421009295  0.04711193057189179  0.12357025141133982  0.001819078853962575  0.09252472769267937    But if I don't run s3dis_upsampling.py, the output of eval_s3dis.py will be  greatly improved:    > [741860, 1081219, 927535, 993140, 209411, 126037, 231132, 105981, 218610, 214055, 21996, 204034, 70819]  [332462, 634309, 554291, 726372, 88278, 83955, 164272, 49907, 104344, 120826, 9476, 95388, 2181949]  [275670, 627944, 547055, 626761, 75240, 76869, 150398, 45148, 86119, 103163, 6392, 80861, 19842]  Overall accuracy: 0.5288675546738922  IoU:  0.3451691099502662  0.5773751728602112  0.5852288956332621  0.5735625041752421  0.33823483135460264  0.5774283932904156  0.6138543545872346  0.40769369694780566  0.36362446428948425  0.44520926298345403  0.25486443381180224  0.369969939742223  0.008886098330173056  0.4200847044581674    Have you ever met this problem?"
"When I run prepare_multiChannel_seg_data.py for S3DIS, I find that list data doesn't match data_num for Area2-Area6.  I don't know whether it will affect the follow-up operation, but I think it would be better if we put line77-80:  `    data = np.zeros((batch_size, max_point_num, args.channel_num))      data_num = np.zeros((batch_size), dtype=np.int32)      label = np.zeros((batch_size), dtype=np.int32)      label_seg = np.zeros((batch_size, max_point_num), dtype=np.int32)`  behind line 81:  `    for data_folder, label_folder in folders:`  Then the list data will match data_num."
"I want to try to make a classification for my point cloud data.    But you write: download [scannet_labelmap] from ScanNet, actually I can't find this file or folder, how can I operate to finish my classification?    Thanks for your attention! "
"In data_conversions/split_data/s3dis_split.py   line 73 and 75:     I think the aim of the code is to leave only one point in the same plane while removing other points in the plane. I don't know whether my understand is correct. And I wonder the feature of these planes, such as their shapes and the aim that you chose nvox[0], nvox[0] * nvox[1] as the coefficients.    I would be grateful if you could explain it to me!"
"    sorry,i want to know why the subvolume supervision can solve overfitting,i think,it only realizes the prediction of a whole through a part."
"When using eval_s3dis.py, the output shows that overall accuracy is 1.0 and IoU is 1.0.  I guess there is something wrong with the code in line25:  `pred_data_label_filenames = gt_label_filenames`  right?    Thanks!"
"For example,     Tensorflow's API use 'use_bias=True' as default."
"1. How to use eval_s3dis.py?   We split each room into several small blocks for testing, right?   so, do we need to combine the prediction of each block to get the prediction of the whole room?   since the groud-truth-files used in  ""eval_s3dis.py"" is about the whole room.  if so, can you provide the script for combining the predictions of each room blocks?    2. In s3dis_split.py.  we first ""split-to-blocks"" and then ""reblock"",   so is there any chance the number of points in the blocks still less than ""block_min_pnum""  after the ""reblock"" operation?  of course, I think it does not matter, so you can choose to ignore this question.    3. when using ""eval_shapenet_seg.py"" in the shapenet part segmentation task,   we have to change the following command :  python3 eval_shapenet_seg.py -g ../../data/shapenet_partseg/test_label -p ../../data/shapenet_partseg/test_data_pred_10  into:   python3 eval_shapenet_seg.py -g ../../data/shapenet_partseg/test_label -p ../../data/shapenet_partseg/test_data_pred_10 **-a**  right?    Thank you for your great work!"
Sincere thanks to your open code.   
"i have one or some CAD image(pcl, ply, obj, pcd....) of objects in real word around me ,how can I transfer it to h5 file ? then input this h5 file to pointCNN, thanks!"
"Thanks for your excellent work! I have noticed it extraordinary performance in sparse points (32 -> 80%+ in ModelNet40). Recently, I am trying to utilize it to detect objects. However, after grouping the raw points, I need to feed every group of points into en independent PointCNN. However, I had trouble in doing so.     This is because the the input is (Batch_size x Group_num x Points_of_Each_Group x XYZ), but the input of PointCNN is (Batch_size x Points_num x XYZ). More specifically, my input is (128 x 64 x 64 x 3) whose dimension is 4, but the dimension of PointCNN's expected input is 3.     I don't want to feed one group one by one, for it is inefficient so I am wondering whether the code of PointCNN (pointcnn.py for example) could be used in parallel computing of different group. I think it is plausible only by changing a few lines of code.  Really hope you can help me solve this problem.    Thank you in advance !!"
I want to learn about the algorithm furthest point sampling which is used in your paper. Do you have any recommended reference?
"when i train the modelnet with tensorflow 1.4, The process halt on and seems not running, should it read h5 file slowly or really does not run, but it has allocate GPU memory already?  Message halt on like these as following:   I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)  "
"line 120 in semantic3d_split.py, why need to multiply nvox[0] and nvox[1]?  line120: vidx = vidx[:, 0] + vidx[:, 1] * nvox[0] + vidx[:, 2] * nvox[0] * nvox[1]    I do not understand the downsample method. Does this method have a name or some reference?"
å¸Œæœ›äº†è§£PointCNNåœ¨semantic3Dæ•°æ®é›†ä¸Šçš„ç»“æžœï¼Ÿä»¥åŠè®­ç»ƒè¿™ä¸ªæ•°æ®é›†éœ€è¦çš„æ—¶é—´ï¼Ÿè°¢è°¢ï¼
Great Work. I ran most experiments. Smooth    One experiment for tu_berlin classification missing train&val file list after raw data conversion.    
Reopening #2 because of the update at charlesq34/pointnet2#21 
There are still bugs in prepare_multiChannel_seg_data.pyï¼Ÿ
"Can this architecture be used for segmentation on large areas of LIDAR point clouds? As an example, for the Semantic3D dataset."
Could someone write a simple explanation of how xconv_params and xdconv_params affect the training?
"Hi, authors,    What's the meaning of `with_fps`, which found in `modelnet_x2_l4.py`? It'll be much useful if more comments could be appended for each main parameters in `modelnet_x2_l4.py`.    P.S. I finally found the meaning of `fps`. That's ""Furthest point sampling"" from    , right?     THANKS"
"Hi ,thanks for your share!   when i  try to train the shapenet_x8_2048_fps,i accept a error which indicates that there is no ""label_weights"" in shapenet_x8_2048_fps.py. The code of train_val_seg.py does need the ""label_weights"" say `label_weights_list = setting.label_weights` .In your paper, it says   > ShapeNet Parts contains 16, 880 models (14, 006/2, 874 training/ testing split) from 16 shape categories, each annotated with 2 to 6 parts and there are 50 different parts in total.     so, should i add a vector of 16 dims which named label_weights ?  Awaiting for your reply .Thank you very much"
"Hi Yangyan,  I have compiled the tf_sampling module, however, when I try to import it, ""segmentation fault (core dumped)"" happens. Is there anything I need to pay attention of while compiling the module? (TF version, cuda,gcc version?)  Thanks,  Daxuan    "
"Hi, authors,    What's the purpose for  , which we could find many places referring to? e.g. line   , line  , line   in `train_val_cls.py`"
"     flake8 testing of      $ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__   "
æ‚¨å¥½ï¼å‡ºçŽ°ä»¥ä¸Šé—®é¢˜ï¼Œæ±‚è§£å†³æ–¹æ¡ˆ  
"Hi, it seems you did not provide the util.py file?  !   "
"Hi, when I run the SVHN/train.py, there is an error:    /usr/bin/python3.6 /home/zm/Can-we-Gain-More-from-Orthogonality-master/SVHN/train.py  Namespace(batch_size=50, cuda=False, data_augmentation=False, dataset='cifar10', epochs=200, learning_rate=0.1, length=16, model='resnet18', n_holes=1, no_cuda=True, ortho_decay=0.01, seed=0)  Files already downloaded and verified  Files already downloaded and verified  Epoch 0:   0%|          | 0/1000 [00:00       pred = cnn(images)    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 489, in __call__      result = self.forward(*input, **kwargs)    File ""/home/zm/Can-we-Gain-More-from-Orthogonality-master/SVHN/model/resnet.py"", line 146, in forward      x = self.avgpool(x)    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 489, in __call__      result = self.forward(*input, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/pooling.py"", line 565, in forward      self.padding, self.ceil_mode, self.count_include_pad)  RuntimeError: Given input size: (512x1x1). Calculated output size: (512x-5x-5). Output size is too small at /pytorch/aten/src/THNN/generic/SpatialAveragePooling.c:48    How to fix this error ?"
"Hi, when I run Can-we-Gain-More-from-Orthogonality-master\SVHN\train.py,  there is an error as follwos:  from util.misc import CSVLogger  ModuleNotFoundError: No module named 'util'.    Is the file of util  absent?        "
"     The comment says `l2_reg` caculates F-Norm of `W^TW-I`, but the code looks more likely to be power iteration for calculating dominant eigenvalue."
"Hi, I found in your readme it said only 'Restricted Isometry' has been implemented. But I found those two, one in  , the other one in  . Those two are different implementation, are those two different implementation for 'Restricted Isometry'? Thanks."
"Hi, it gave error at this line:      will you upload the whole folder for running your code? Thanks. "
"I hope to create the df data of other resolutions, e.g. 64. But I do not find any code about this. Could you share the corresponding codes?    And here is a detail about data, I can't understand. It seems that there is no minus value in df data. So what do you save in voxel grids? I used to think they are signed distance function values."
bash install.sh  report errorï¼šcuda available but nvcc not found. Please add nvcc to $PATH.    
"Hi guys,  I am now trying to replicate the results on RTX 2080Ti. I added -gencode arch=compute_75,code=sm_75 to the nvcc command to make it work on RTX 2080Ti. I did the bash install and downloaded the models. It goes fine upto this step. Then when I run: bash ./scripts/figures.sh 0 car df, then I get the following error:    ImportError: /home/user/Downloads/VON/render_module/vtn/vtn/_ext/vtn_lib/_vtn_lib.so: undefined symbol: __cudaRegisterFatBinaryEnd    I see that the _vtn_lib.so is created there. Could you please point me in the right direction? Thank you."
"Running into an error when feeding data into the model    I'm trying to train the model for top view car images, so after I downloaded the training data  -  created my own dataset from Google images, replaced /data/images/car  - updated imgs_car.txt  - updated pose_car.npz with my own viewpoints    However, I got the error below. Are there additional steps I'm missing in creating the training data?    Traceback (most recent call last):    File ""train.py"", line 22, in        for i, data in enumerate(dataset):    File ""/media/marcchoo/Data/marcj/Documents/VON/data/__init__.py"", line 84, in __iter__      for i, data in enumerate(self.dataloader):    File ""/home/marcchoo/anaconda3/envs/von/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 336, in __next__      return self._process_next_batch(batch)    File ""/home/marcchoo/anaconda3/envs/von/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 357, in _process_next_batch      raise batch.exc_type(batch.exc_msg)  IndexError: Traceback (most recent call last):    File ""/home/marcchoo/anaconda3/envs/von/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 106, in _worker_loop      samples = collate_fn([dataset[i] for i in batch_indices])    File ""/home/marcchoo/anaconda3/envs/von/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 106, in        samples = collate_fn([dataset[i] for i in batch_indices])    File ""/media/marcchoo/Data/marcj/Documents/VON/data/concat_dataset.py"", line 9, in __getitem__      return tuple(d[i] for d in self.datasets)    File ""/media/marcchoo/Data/marcj/Documents/VON/data/concat_dataset.py"", line 9, in        return tuple(d[i] for d in self.datasets)    File ""/media/marcchoo/Data/marcj/Documents/VON/data/images_dataset.py"", line 95, in __getitem__      return self.get_item(index)    File ""/media/marcchoo/Data/marcj/Documents/VON/data/images_dataset.py"", line 115, in get_item      mask = im_out[3, :, :]  IndexError: index 3 is out of bounds for dimension 0 with size 3"
"In test.py, `save_vox_to_obj(model.voxel.data.cpu().numpy(), 0.5 if not use_df else 0.85, obj_name)`. Why th parameter 'th' is 0.85 instead of 0.01 for default of ios_th.   `def save_vox_to_obj(voxel, th, save_path):      if len(voxel.shape) == 5:          voxel_iso = voxel[0, 0, :, :, :]      verts, faces, normals, values = measure.marching_cubes_lewiner(voxel_iso, th, spacing=(1 / 128, 1 / 128, 1 / 128),)      save_obj(verts - 0.5, faces, save_path)`  In the above usage, why `verts - 0.5`?  Beacuse in test_shape.py,   `v, f, n, _ = measure.marching_cubes_lewiner(output, opt.ios_th, spacing=(space, space, space))      save_obj(v, f, join(shape_dir, '%04d' % idx))`  I wonder the difference for two cases. Thanks!"
"How can i  train other objects? like  ships ,animal.  Can you list the process of  generating  train data in detail?  Thank you very  much  "
 visualizing generated 3D shapes.
"Add -gencode to match all the GPU architectures you have.  Check '  for list of architecture.  Check '  for GPU compilation based on architecture.  nvcc -c -o vtn_cuda_kernel_generic.cu.o vtn_cuda_kernel_generic.cu -x cu -Xcompiler -fPIC -std=c++11 -I /usr/local/anaconda3/lib/python3.7/site-packages/torch/lib/include/TH -I /usr/local/anaconda3/lib/python3.7/site-packages/torch/lib/include -I /usr/local/anaconda3/lib/python3.7/site-packages/torch/lib/include/THC -I /home/humaolin/papercode/VON/render_module/vtn/vtn/src         -gencode arch=compute_30,code=sm_30         -gencode arch=compute_35,code=sm_35         -gencode arch=compute_52,code=sm_52         -gencode arch=compute_61,code=sm_61   vtn_cuda_kernel_generic.cu:1:10: fatal error: THC.h: No such file or directory   #include              ^~~~~~~  compilation terminated.  "
"model.update_G()  model.update_D()    in standard GAN,  we need to update the parameter of D , then G."
Thanks a lot and waiting for your answer! 
I just try figure.sh with RTX 2080ti +CUDA9.0+Pytorch0.4    Error occurs RuntimeError: cuda runtime error (11) : invalid argument at /opt/conda/conda-bld/pytorch_1532582628513/work/aten/src/THC/THCGeneral.cpp:663    RTX 2080ti +CUDA8.0+Pytorch0.4    Error occurs  RuntimeError: CuDNN error: CUDNN_STATUS_EXECUTION_FAILED  RTX 2080ti + CUDA10+Pytorch 1.0    Error occurs  cant find module 'vtn'  I am  confused that does this job support RTX-GPUS?  
"When I train shapenet, the GPU utility is much too low, how to improve the training speed?"
"1.I have successed in executing `bash ./scripts/figures.sh 0` with df of car and chair. But when I try with voxel, it seems does have data. Your whole data is too big to download. Can you provide with a piece of data for test? The picture is for voxel of chair. It's same for voxel of car.  !     2.In executing `bash ./scripts/test_shape.sh 0`with df and voxel, I did some adjustment. In `test_shape.sh`. I remove epoch because of test. And is `--th 0.012` for `--ios_th 0.012`?  In `test_shape.py`, I also made two adjuxtment,   `# net_path = join(opt.checkpoints_dir, '%s_net_G_3D.pth' % opt.epoch)  #æ”¹åŠ¨  net_path = '/home/ljl/myproject/VON/final_models/models_3D/chair_df_G_3D.pth'`,   `# result_root = join(opt.checkpoints_dir, 'test_epoch_%s' % opt.epoch)  #æ”¹åŠ¨  result_root =  '/home/ljl/myproject/VON/results/test_shape_df_chair'`.  For the result, I have some questions. First, for df, the result of car has big holes in head and tail. Second, for voxel, it seems to be tattered.  !     !     I wander if I have something wrong in this.  3..In executing `bash ./scripts/test_texture.sh 0`with df and voxel, I met same problem for voxel just as 1. And for df, I met this and I haven't figure it out.  !     It seems a lot.Thank you for your considering."
"Hello,I have trouble in downloading your pretrained models and dataset.It's too slow.And I can not open your dropbox link.Can you provide Baidu SkyDrive link or some way easy to get in Chinese Mainland?Thank you!"
"Hi,  Thank you for the code and the detailed installation procedure.    I needed some information on the training steps for the car use case.  We have the Quadro RTX 4000 GPU with 8 GB GDDR6 GPU memory. But on running the training for 2D texture network, it quickly uses up the memory and i have CUDA out of memory issue.  I kept reducing the dataset and finally was able to give only 10 images for training.    What was the hardware that you used for training and did you do any extra config for training 2605 images for the texture network?  Also can you suggest any steps to increase dataset without hitting the GPU memory issue.  I'm using PyTorch 0.4.1 with CUDA92 within conda    Thank you"
"I've been trying to run the pipeline (only testing, no training) using a CPU only. Was this repository intended to do so?   In the readme it says that blender is optional and I interpreted it as blender being an alternative to the custom render module based on cuda. However, blender only seems to use whatever model.sample_2d() outputs and model.sample_2d() in turn is based on the custom render module.  Thanks in advance for commenting on this!"
"hello,  when i ran ""bash ./scripts/figures.sh 0 car df""  it occured errors:     + GPU_IDS=0  + CLASS=car  + DATASET=df  +++ dirname ./scripts/figures.sh  ++ cd ./scripts  ++ pwd -P  + SCRIPTPATH=/home/pf/VON/scripts  + ROOT_DIR=/home/pf/VON/scripts/..  + MODEL2D_DIR=/home/pf/VON/scripts/../final_models/models_2D/car_df/latest  + MODEL3D_DIR=/home/pf/VON/scripts/../final_models/models_3D/car_df  + RESULTS_DIR=/home/pf/VON/scripts/../results/fig_car_df/  + NUM_SHAPES=20  + NUM_SAMPLES=5  + python test.py --gpu_ids 0 --results_dir /home/pf/VON/scripts/../results/fig_car_df/ --model2D_dir /home/pf/VON/scripts/../final_models/models_2D/car_df/latest --model3D_dir /home/pf/VON/scripts/../final_models/models_3D/car_df --class_3d car --phase val --dataset_mode image_and_df --model test --n_shapes 20 --n_views 5 --reset_texture --reset_shape --suffix car_df  ----------------- Options ---------------                  D_norm_3D: none                                            G_norm_3D: batch3d                                      aspect_ratio: 1.0                                            batch_size: 12                                        checkpoints_dir: ../../results_texture/                           class_3d: car                                          color_jitter: False                                          crop_align: False                                           crop_size: 128                                              dataroot: None                                         dataset_mode: image_and_df                   [default: base]                 df_flipped: False                                            df_sigma: 8.0                                                 df_th: 0.9                                       display_winsize: 128                                                 epoch: latest                                           gan_mode: lsgan                                             gpu_ids: 0                                              init_param: 0.02                                            init_type: kaiming                                          input_nc: 1                                            interp_shape: False                                      interp_texture: False                                              ios_th: 0.01                                              isTrain: False                          [default: None]                  load_size: 128                                      max_dataset_size: inf                                                 model: test                           [default: base]                model2D_dir: /home/pf/VON/scripts/../final_models/models_2D/car_df/latest [default: None]                model3D_dir: /home/pf/VON/scripts/../final_models/models_3D/car_df [default: None]                   n_shapes: 20                             [default: 10]                    n_views: 5                              [default: 10]                       name: car_df                         [default: experiment_name]                        ndf: 64                                                 ndf_3d: 64                                                    nef: 64                                                   netD: multi                                             netD_3D: D0                                                   netE: adaIN                                                netG: resnet_cat                                        netG_3D: G0                                                    ngf: 64                                                 ngf_3d: 64                                                     nl: relu                                              no_flip: False                                          no_largest: False                                                norm: inst                                               num_Ds: 2                                             num_threads: 6                                                nz_shape: 200                                            nz_texture: 8                                               output_nc: 3                                                   phase: val                                            pose_align: False                                           pose_type: hack                                           print_grad: False                                        random_shift: False                                         random_view: False                                          real_shape: False                                        real_texture: False                                          render_25d: False                                           render_3d: False                                         reset_shape: True                           [default: False]              reset_texture: True                           [default: False]             resize_or_crop: crop_real_im                                  results_dir: /home/pf/VON/scripts/../results/fig_car_df/ [default: ../results/]                       seed: 0                                          serial_batches: False                                          show_input: False                                              suffix: car_df                         [default: ]                     use_df: False                                         use_dropout: False                                             verbose: False                                           voxel_res: 128                                             where_add: all                             ----------------- End -------------------  dataset [ImageAndDFDataset] was created  enable cudnn benchmark  initialization method [kaiming]  loading model from /home/pf/VON/scripts/../final_models/models_3D/car_df_G_3D.pth  initialization method [kaiming]  loading model from /home/pf/VON/scripts/../final_models/models_2D/car_df/latest_net_G_AB.pth  initialization method [kaiming]  loading model from /home/pf/VON/scripts/../final_models/models_2D/car_df/latest_net_E.pth  Traceback (most recent call last):    File ""/home/pf/VON/render_module/render_sketch.py"", line 2, in        from .vtn.vtn.functions import grid_sample3d, affine_grid3d    File ""/home/pf/VON/render_module/vtn/vtn/functions/__init__.py"", line 2, in        from .grid_sample3d import grid_sample3d    File ""/home/pf/VON/render_module/vtn/vtn/functions/grid_sample3d.py"", line 3, in        from .._ext import vtn_lib    File ""/home/pf/VON/render_module/vtn/vtn/_ext/vtn_lib/__init__.py"", line 3, in        from ._vtn_lib import lib as _lib, ffi as _ffi  ImportError: /home/pf/VON/render_module/vtn/vtn/_ext/vtn_lib/_vtn_lib.so: undefined symbol: __cudaPopCallConfiguration    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""test.py"", line 19, in        model = create_model(opt)    File ""/home/pf/VON/models/__init__.py"", line 59, in create_model      instance = model(opt)    File ""/home/pf/VON/models/test_model.py"", line 34, in __init__      self.setup_DR(opt)    File ""/home/pf/VON/models/base_model.py"", line 159, in setup_DR      from render_module.render_sketch import VoxelRenderLayer, CroppingLayer, GetRotationMatrix, FineSizeCroppingLayer    File ""/home/pf/VON/render_module/render_sketch.py"", line 4, in        from vtn.vtn.functions import grid_sample3d, affine_grid3d  ModuleNotFoundError: No module named 'vtn'      ------------  i don not install render ,is this the reason?  "
"Hi @ztzhang & @junyanz ,    Thanks for the great work with the great code.    I tried to use multi-GPU training recently, it seems that the training will down at the end of one epoch, as shown in the following picture  !     My script is as the following  `python -u train.py --gpu_ids 0,1 --display_id 1000 --dataset_mode df --model 'shape_gan' --class_3d car --checkpoints_dir ${CHECKPOINTS_DIR} --niter 125 --niter_decay 125 --batch_size 16 --G_norm_3D inst3d  --save_epoch_freq 10 --suffix {class_3d}_{model}_{dataset_mode} `    My guess is that the training data has 3457 samples and 3457%16 = 1, thus at the last iteration, only one GPU got the sample while another one did not.    I am not sure whether the guess is right,  so do you have any suggestions or solution to fix this bug?  "
I can not find your supplemental material neither on the project page nor on NIPS page? Where else can I refer to?
"Hi, I use the Dockerfile and train the shape gan, it runs just right at the start, but it stuck there without any log as follows, and the GPU utility stays 0%, could you give a hit about it?    !   It stucks at epoch 19 forever, but I don't know why."
"It shows   `ModuleNotFoundError: No module named 'vtn'`  However, the compilation succeeds. It only raises several WARNING but no errors in the compilation. And I run this code in the virtual env ""von"" initialized as the instruction."
"I met some problem when try to train VON,   1. how to generate .npz data from shapenet dataset? Which dataset are you using, the 2016 voxels data or the shapenetcore .obj data?  2. what is the training img for training texture gan? And how you generate it?    Can you show me a easy example with sample and code? Thank you very much.  BTW, could you upload you data.tar to google drive or OneDrive? The link in your script is much too slow. Thank you very much again."
How to change the voxel value to distance function values?  What is the distance function you are actually using?  Could you elaborate how to change the shapenet file to your training data format?
"The code seems hard to understand with complex structure, any guide please?   For example, where is the network architecture definition?"
"Thanks to your awesome work! Generating images from 3D is a great idea!    You demonstrated some images of distance function grids in your paper, but there seems no code doing this, sorry I'm new to 3D learning.      Iâ€˜m wondering how to get distance functions from a mesh like *.obj files, and how to render such a N x N x N distance grid in 3D view (just like your paper). Can you provide some tools or just tell me the name?"
"I run into several strange error in compiling the code, could you kindly provide a dockerfile of your system environment please?"
"I followed the steps in **README.md**, but got errors when I use this command:     The errors are as follows:     "
"Hello,  I can't find the usage of viewpoint code in your source code,  I am not sure whether I ignore something, can you tell me about it? thank you."
"Hi, I found it that during trianing, mult GPU didn't work.  For example     only GPU 0 is working, the others only takes 10MB memory and 0% GPU Utility       Thank you. "
"After sucessufully compiled `./install.sh`  I tried to reproduce`Generate 3D shapes, 2.5D sketches, and images` part by runing  `bash ./scripts/figures.sh 0 car df` and got an error       # here is my conifg:  - GeForce RTX 2080 Ti  - CUDA version 9.2.148  - ubuntu version16.04 LTS  - gcc version: 6.5.0  - conda environemnt python 3.6 with pytorch 0.4.1    **BTW I can only find gcc 6.5**    # `./install.sh` seems to be good            # here is the error          "
"I downloaded the model using `bash ./scripts/download_model.sh`.    But then when I try    ` bash ./scripts/figures.sh 0 car df`    to generate samples, I get the following output.     "
 I tried to run `./install.sh`   ## here is my conifg:  - GeForce RTX 2080 Ti  - CUDA version 9.2.148  - ubuntu version16.04 LTS  - gcc version: 5.4.0  - conda environemnt python 3.6 with pytorch 0.4.1    ## here is the error   
"I'm trying to reproduce Generate 3d shape 2d sketch images part by        below is the error        dataset [ImageAndDFDataset] was created      enable cudnn benchmark      initialization method [kaiming]      loading model from ./VON/scripts/../final_models/models_3D/car_df_G_3D.pth      initialization method [kaiming]      loading model from ./VON/scripts/../final_models/models_2D/car_df/latest_net_G_AB.pth      Traceback (most recent call last):        File ""./VON/render_module/render_sketch.py"", line 2, in            from .vtn.vtn.functions import grid_sample3d, affine_grid3d        File ""./VON/render_module/vtn/vtn/functions/__init__.py"", line 2, in            from .grid_sample3d import grid_sample3d        File ""./VON/render_module/vtn/vtn/functions/grid_sample3d.py"", line 3, in            from .._ext import vtn_lib      ModuleNotFoundError: No module named 'render_module.vtn.vtn._ext'            During handling of the above exception, another exception occurred:            Traceback (most recent call last):        File ""./test.py"", line 19, in            model = create_model(opt)        File ""./VON/models/__init__.py"", line 37, in create_model          instance.initialize(opt)        File ""./VON/models/test_model.py"", line 36, in initialize          self.setup_DR(opt)        File ""./VON/models/base_model.py"", line 161, in setup_DR          from render_module.render_sketch import VoxelRenderLayer, CroppingLayer, GetRotationMatrix, FineSizeCroppingLayer        File ""./VON/render_module/render_sketch.py"", line 4, in            from vtn.vtn.functions import grid_sample3d, affine_grid3d      ModuleNotFoundError: No module named 'vtn'              it seems that there is no such file `./render_module/vtn/vtn/_ext.py`  "
"Thank you so much for sharing your code. However, it seems that some of your scripts are missing. I cannot find `CreateDataLoader` module. And there is no `./data` subfolder. "
Could anyone please help with fixing the following error when trying to run cifar10_sobolev.py? Thanks a lot.           ModuleNotFoundError: No module named 'adler'  
"We are doing project on your paper. We've read your code for sobolev_filter and faced problems with understanding the trick you use for \xi grid. As far as we understood, the straight-forward way to do scaling is to make a meshgrid using range(sx) / sx and range(sy) / sy. But we found out that you do it in a symmetric-way, i.e. minimum(x, sx - x) / (sx // 2). We wonder why you do it like that? Are there any reasons like numerical stability, etc.? It is also interesting why one needs to limit |\xi| <= 5, does constant ""c"" in your code have any correlation with this constraint? And if yes, why it is not in the Sobolev-norm equation? "
I can't successfully run the .sh files used to download pre-trained model. detail is as follows:      and other .sh files in sniper/scripts have the same problemï¼Œmaybe is network problemï¼Ÿ  thsnks for any help.
"Greeting,    When running main_test for current_job = 1, I got the following output          Process finished with exit code 1     The shape of the cls_dets is always (0,5) while agg_dets is different and does not match.    Is there something that should be changed to make the code work?      "
"thanks for your impressive work, I would like to ask whether the focus mask branch will downgrade the performance of the detector in single scale train/inference setting ?     since the RCNN and focus branch are all built upon conv5 feature, the gradient from RCNN branch tends to highlight region of all objects, while the gradient of focus branch tends to hightlight only small object region, which seems a conflict and may bring negative impact on RCNN prediction."
I am stuck in this error...Can anybody help me? Thanks!
"It seems that the pretrained checkpoint ""soft"" (   ) is missing. Could you provide the link again? Thanks!"
"Thank you for your outstanding work ,but  there are some problems  to  consult.  The resolution of ImageNet dataset is 224*224.  When the weights of  object detection network pretrained on coco dataset loaded, should the resolution of pretrained dataset  set to 480*640 ï¼Ÿ"
"where can I find the COCO_train2017_rpn.pkl, thank you."
"I have been reproducing SNIPER according to the _INSTALLATION_  for 3 days, but I STILL cant run the demo.py and main_train.py successfully ,who can offer me  complete installation procedures or your configuration environment. Thx!"
None
"  !   When I ran Demo.py, I had this problem. If you ever had this problem, please give me some advice. Thank you very much  psï¼šmy python is 2.7."
What all changes are required to detect classes from openimages dataset for a sample image using model trained on openimages?
"cv::String::deallocate(), cv::String::deallocate(),  cv::String::deallocate() is not defined"
"I refer your code to my own dataset, and the bounding box is accurate, but the detection score is  low(I trained 7 epochs on my own dataset, but most of the detection score is lower than 0.5, rarely higher than 0.5), so I have to set a very low threshold.  Have you ever encountered this situation, can you give me some advice?  Thank you very much."
"To install mxnet, you need almost build every package from source like opeblas and opencv.  It's really a HEAVY environment."
"I tried training the SNIPER network on my own dataset and on the VOC dataset and encountered this error:    /home/pwh/FWWB/SNIPER-master/configs/faster/default_configs.py:181: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read   for full details.    exp_config = edict(yaml.load(f))  num_images 1286  PASCALVOC_2007_trainval gt roidb loaded from ./data/cache/PASCALVOC_2007_trainval_gt_roidb.pkl  appending ground truth annotations  Reading cached proposals after ***NMS**** from data/proposals/PASCALVOC_2007_trainval_rpn_after_nms.pkl  Done!  append flipped images to roidb  filtered 24 roidb entries: 2572 -> 2548  add bounding box regression targets  bbox target means:  [[0. 0. 0. 0.]   [0. 0. 0. 0.]]  [0. 0. 0. 0.]  bbox target stdevs:  [[0.1 0.1 0.2 0.2]   [0.1 0.1 0.2 0.2]]  [0.1 0.1 0.2 0.2]  Creating Iterator with 2548 Images  Total number of extracted chips: 11472  Done!  The Iterator has 11472 samples!  Initializing the model...  Traceback (most recent call last):    File ""main_train.py"", line 103, in        sym_inst.init_weight_rcnn(config, arg_params, aux_params)    File ""/home/pwh/FWWB/SNIPER-master/symbols/faster/resnet_mx_101_e2e.py"", line 468, in init_weight_rcnn      arg_params['conv_new_2_weight'] = mx.random.normal(0, 0.01, shape=self.arg_shape_dict['conv_new_2_weight'])  KeyError: 'conv_new_2_weight'  "
configs/faster/default_configs.py 181 shows warning       probably need to modify like below     
"I have built a docker container based on the Dockerfile supplied in   After confirmation of the output of demo.py, I tried to test the training processes.     This is the log file of this process.     I spent almost two days running this command. However, the output of this process was stopped at final line of the massage.    Does anyone have any idea to understand this state?    ---  Machine spec  CPU: i7-8700K  GPU: GeForce GTX 1080 "
!   
" detection_list = pool.map(detect_scale_worker, parallel_args)  !   "
!   what can i do?
"Hello,    I can't finish the compilation. Please, advice, what to check.    **Getting an error.**  ##########################################################################  1 error detected in the compilation of ""/tmp/tmpxft_0000aa11_00000000-12_dropout.compute_70.cpp1.ii"".  1 error detected in the compilation of ""/tmp/tmpxft_0000a9a4_00000000-12_deconvolution.compute_70.cpp1.ii"".  Makefile:396: recipe for target 'build/src/operator/nn/dropout_gpu.o' failed  make: *** [build/src/operator/nn/dropout_gpu.o] Error 1  Makefile:396: recipe for target 'build/src/operator/nn/deconvolution_gpu.o' failed  make: *** [build/src/operator/nn/deconvolution_gpu.o] Error 1  /usr/lib/gcc/x86_64-linux-gnu/6/include/avx512fintrin.h(12943): error: identifier ""__builtin_ia32_kmov16"" is undefined    1 error detected in the compilation of ""/tmp/tmpxft_0000aa79_00000000-12_sample_op.compute_70.cpp1.ii"".  Makefile:396: recipe for target 'build/src/operator/random/sample_op_gpu.o' failed  make: *** [build/src/operator/random/sample_op_gpu.o] Error 1  1 error detected in the compilation of ""/tmp/tmpxft_0000aa07_00000000-12_shuffle_op.compute_70.cpp1.ii"".  1 error detected in the compilation of ""/tmp/tmpxft_0000aaab_00000000-12_elemwise_sum.compute_70.cpp1.ii"".  1 error detected in the compilation of ""/tmp/tmpxft_0000aabd_00000000-12_init_op.compute_70.cpp1.ii"".  1 error detected in the compilation of ""/tmp/tmpxft_0000aa91_00000000-12_concat.compute_70.cpp1.ii"".  Makefile:396: recipe for target 'build/src/operator/random/shuffle_op_gpu.o' failed  make: *** [build/src/operator/random/shuffle_op_gpu.o] Error 1  Makefile:396: recipe for target 'build/src/operator/nn/concat_gpu.o' failed  make: *** [build/src/operator/nn/concat_gpu.o] Error 1  Makefile:396: recipe for target 'build/src/operator/tensor/init_op_gpu.o' failed  make: *** [build/src/operator/tensor/init_op_gpu.o] Error 1  Makefile:396: recipe for target 'build/src/operator/tensor/elemwise_sum_gpu.o' failed  make: *** [build/src/operator/tensor/elemwise_sum_gpu.o] Error 1  1 error detected in the compilation of ""/tmp/tmpxft_0000aac4_00000000-12_elemwise_binary_scalar_op_extended.compute_70.cpp1.ii"".  1 error detected in the compilation of ""/tmp/tmpxft_0000aa9c_00000000-12_requantize.compute_70.cpp1.ii"".  Makefile:396: recipe for target 'build/src/operator/tensor/elemwise_binary_scalar_op_extended_gpu.o' failed  make: *** [build/src/operator/tensor/elemwise_binary_scalar_op_extended_gpu.o] Error 1  1 error detected in the compilation of ""/tmp/tmpxft_0000aa66_00000000-12_softmax.compute_70.cpp1.ii"".  Makefile:396: recipe for target 'build/src/operator/quantization/requantize_gpu.o' failed  make: *** [build/src/operator/quantization/requantize_gpu.o] Error 1  1 error detected in the compilation of ""/tmp/tmpxft_0000aa1b_00000000-12_convolution.compute_70.cpp1.ii"".  Makefile:396: recipe for target 'build/src/operator/nn/softmax_gpu.o' failed  make: *** [build/src/operator/nn/softmax_gpu.o] Error 1  1 error detected in the compilation of ""/tmp/tmpxft_0000aae8_00000000-12_elemwise_unary_op_trig.compute_70.cpp1.ii"".  Makefile:396: recipe for target 'build/src/operator/nn/convolution_gpu.o' failed  make: *** [build/src/operator/nn/convolution_gpu.o] Error 1  Makefile:396: recipe for target 'build/src/operator/tensor/elemwise_unary_op_trig_gpu.o' failed  make: *** [build/src/operator/tensor/elemwise_unary_op_trig_gpu.o] Error 1  1 error detected in the compilation of ""/tmp/tmpxft_0000aada_00000000-12_elemwise_binary_op_basic.compute_70.cpp1.ii"".  Makefile:396: recipe for target 'build/src/operator/tensor/elemwise_binary_op_basic_gpu.o' failed  make: *** [build/src/operator/tensor/elemwise_binary_op_basic_gpu.o] Error 1  1 error detected in the compilation of ""/tmp/tmpxft_0000aae6_00000000-12_elemwise_binary_scalar_op_basic.compute_70.cpp1.ii"".  Makefile:396: recipe for target 'build/src/operator/tensor/elemwise_binary_scalar_op_basic_gpu.o' failed  make: *** [build/src/operator/tensor/elemwise_binary_scalar_op_basic_gpu.o] Error 1  1 error detected in the compilation of ""/tmp/tmpxft_0000ab15_00000000-12_elemwise_binary_op_extended.compute_70.cpp1.ii"".  1 error detected in the compilation of ""/tmp/tmpxft_0000aad3_00000000-12_cast_storage.compute_70.cpp1.ii"".  Makefile:396: recipe for target 'build/src/operator/tensor/elemwise_binary_op_extended_gpu.o' failed  make: *** [build/src/operator/tensor/elemwise_binary_op_extended_gpu.o] Error 1  Makefile:396: recipe for target 'build/src/operator/tensor/cast_storage_gpu.o' failed  make: *** [build/src/operator/tensor/cast_storage_gpu.o] Error 1  1 error detected in the compilation of ""/tmp/tmpxft_0000ab01_00000000-12_square_sum.compute_70.cpp1.ii"".  Makefile:396: recipe for target 'build/src/operator/tensor/square_sum_gpu.o' failed  make: *** [build/src/operator/tensor/square_sum_gpu.o] Error 1  1 error detected in the compilation of ""/tmp/tmpxft_0000aaf7_00000000-12_quantized_conv.compute_70.cpp1.ii"".  Makefile:396: recipe for target 'build/src/operator/quantization/quantized_conv_gpu.o' failed  make: *** [build/src/operator/quantization/quantized_conv_gpu.o] Error 1  1 error detected in the compilation of ""/tmp/tmpxft_0000ab38_00000000-12_dot.compute_70.cpp1.ii"".  Makefile:396: recipe for target 'build/src/operator/tensor/dot_gpu.o' failed  make: *** [build/src/operator/tensor/dot_gpu.o] Error 1  1 error detected in the compilation of ""/tmp/tmpxft_0000ab49_00000000-12_elemwise_unary_op_basic.compute_70.cpp1.ii"".  Makefile:396: recipe for target 'build/src/operator/tensor/elemwise_unary_op_basic_gpu.o' failed  make: *** [build/src/operator/tensor/elemwise_unary_op_basic_gpu.o] Error 1  1 error detected in the compilation of ""/tmp/tmpxft_0000ab07_00000000-12_elemwise_binary_broadcast_op_extended.compute_70.cpp1.ii"".  Makefile:396: recipe for target 'build/src/operator/tensor/elemwise_binary_broadcast_op_extended_gpu.o' failed  make: *** [build/src/operator/tensor/elemwise_binary_broadcast_op_extended_gpu.o] Error 1  1 error detected in the compilation of ""/tmp/tmpxft_0000ab65_00000000-12_elemwise_binary_scalar_op_logic.compute_70.cpp1.ii"".  Makefile:396: recipe for target 'build/src/operator/tensor/elemwise_binary_scalar_op_logic_gpu.o' failed  make: *** [build/src/operator/tensor/elemwise_binary_scalar_op_logic_gpu.o] Error 1  1 error detected in the compilation of ""/tmp/tmpxft_0000ab34_00000000-12_ordering_op.compute_70.cpp1.ii"".  Makefile:396: recipe for target 'build/src/operator/tensor/ordering_op_gpu.o' failed  make: *** [build/src/operator/tensor/ordering_op_gpu.o] Error 1  1 error detected in the compilation of ""/tmp/tmpxft_0000ab5e_00000000-12_broadcast_reduce_op_value.compute_70.cpp1.ii"".  Makefile:396: recipe for target 'build/src/operator/tensor/broadcast_reduce_op_value_gpu.o' failed  make: *** [build/src/operator/tensor/broadcast_reduce_op_value_gpu.o] Error 1  1 error detected in the compilation of ""/tmp/tmpxft_0000ab79_00000000-12_matrix_op.compute_70.cpp1.ii"".  Makefile:396: recipe for target 'build/src/operator/tensor/matrix_op_gpu.o' failed  make: *** [build/src/operator/tensor/matrix_op_gpu.o] Error 1  1 error detected in the compilation of ""/tmp/tmpxft_0000ab2d_00000000-12_indexing_op.compute_70.cpp1.ii"".  Makefile:396: recipe for target 'build/src/operator/tensor/indexing_op_gpu.o' failed  make: *** [build/src/operator/tensor/indexing_op_gpu.o] Error 1  ######################################################################    **Our config is:**    export CC = gcc  export CXX = g++  export NVCC = nvcc  DEV = 0  DEBUG = 1  USE_SIGNAL_HANDLER =  ADD_LDFLAGS =  ADD_CFLAGS =  USE_CUDA = 1  USE_CUDA_PATH = /usr/local/cuda  ENABLE_CUDA_RTC = 1  USE_CUDNN = 1  USE_NCCL = 1  USE_NCCL_PATH = /usr/local/cuda-10.0/targets/x86_64-linux  USE_OPENCV = 1  USE_LIBJPEG_TURBO = 0  USE_LIBJPEG_TURBO_PATH = NONE  USE_OPENMP = 1  USE_MKLDNN = 0  USE_NNPACK = 0  UNAME_S := $(shell uname -s)  USE_BLAS = openblas  USE_LAPACK = 0  USE_LAPACK_PATH =  USE_INTEL_PATH = NONE  ifeq ($(USE_BLAS), mkl)  USE_STATIC_MKL = 1  else  USE_STATIC_MKL = NONE  endif    ARCH := $(shell uname -a)  ifneq (,$(filter $(ARCH), armv6l armv7l powerpc64le ppc64le aarch64))    USE_SSE=0  else    USE_SSE=1  endif    USE_DIST_KVSTORE = 0    USE_HDFS = 0  LIBJVM=$(JAVA_HOME)/jre/lib/amd64/server    USE_S3 = 0    USE_OPERATOR_TUNING = 1    USE_GPERFTOOLS = 1    USE_JEMALLOC = 1    EXTRA_OPERATORS =    USE_CPP_PACKAGE = 0"
"Hi all,    if all my images have the same resolution (768x1024). How should I set the training scale?"
"rt, I trained it about 12 epochs. And got COCO score main_test.py from 7 to 12(epochs).  But when I try to have a sight of predictions like 'python demo.py --cfg configs/faster/sniper_res50_e2e.yml', the result image in ./data/demo/ contains no bbox."
"loading annotations into memory...  Done (t=16.02s)  creating index...  index created!  ('num_images', 82783)     COCO_train2014 gt roidb loaded from ./data/cache/COCO_train2014_gt_roidb.pkl  append flipped images to roidb  loading annotations into memory...  Done (t=9.06s)  creating index...  index created!  ('num_images', 40504)     COCO_val2014 gt roidb loaded from ./data/cache/COCO_val2014_gt_roidb.pkl  append flipped images to roidb  filtered 2138 roidb entries: 246574 -> 244436  add bounding box regression targets  bbox target means:  [[0. 0. 0. 0.]   [0. 0. 0. 0.]]  [0. 0. 0. 0.]  bbox target stdevs:  [[0.1 0.1 0.2 0.2]   [0.1 0.1 0.2 0.2]]  [0.1 0.1 0.2 0.2]  Creating Iterator with 244436 Images  Total number of extracted chips: 955689  Done!  The Iterator has 955776 samples!  Initializing the model...  Traceback (most recent call last):    File ""main_train.py"", line 91, in        sym = sym_inst.get_symbol_rpn(config) if config.TRAIN.ONLY_PROPOSAL else sym_inst.get_symbol_rcnn(config)    File ""/home/yuanyq/Detect_DL/SNIPER/symbols/faster/resnet_mx_101_e2e.py"", line 270, in get_symbol_rcnn      rois, label, bbox_target, bbox_weight = mx.sym.MultiProposalTarget(cls_prob=rpn_cls_prob, bbox_pred=rpn_bbox_pred, im_info=im_info,  AttributeError: 'module' object has no attribute 'MultiProposalTarget'"
"Hi all,    I've been searching now for a while, but I can't find which solver SNIPER is using, or at least it is not obvious to me.    Can anybody tell me where I can change the solver (e.g. gradient descent, Adam etc)?    Thanks for any hints."
"Hi,I meet compile chips error~"
"I successfully trained SNIPER on pascal with this config ""sniper_res101_e2e_pascal_voc.yml"" and I tried to train a new dataset with this but I have a problem as follow:  Traceback (most recent call last):    File ""main_train.py"", line 145, in        epoch_end_callback=epoch_end_callback, arg_params=arg_params, aux_params=aux_params)    File ""SNIPER-mxnet/python/mxnet/module/base_module.py"", line 520, in fit      self.update_metric(eval_metric, data_batch.label)    File ""SNIPER-mxnet/python/mxnet/module/module.py"", line 757, in update_metric      self._exec_group.update_metric(eval_metric, labels)    File ""SNIPER-mxnet/python/mxnet/module/executor_group.py"", line 616, in update_metric      eval_metric.update_dict(labels_, preds)    File ""SNIPER-mxnet/python/mxnet/metric.py"", line 304, in update_dict      metric.update_dict(labels, preds)    File ""SNIPER-mxnet/python/mxnet/metric.py"", line 132, in update_dict      self.update(label, pred)    File ""lib/train_utils/metric.py"", line 200, in update      cls = pred[keep_inds, label]  IndexError: index 144 is out of bounds for axis 1 with size 81    Although I have changed the NUM_CLASSES  and paths to dataset in ""sniper_res101_e2e_pascal_voc.yml"", the name of classes in pascal_voc.py. My dataset has 200 classes. So do I need to change anything in other files for training on a new dataset. I also trained with the command to generate proposals but after 2 epoch for proposals, it suffers from the above problem.   If someone has a glimpse of this note, please help me out. Thank you so much! "
could anyone give me a hand
"I have tried to train **SNIPER_Mobilenetv2**  with  my own datasets ,but I got this error:     This seems because the layers name  you setted in you code is not match with the downloaded pretrained model in **MXNet Zoo**. I find some people got the same problem, so could you please provide your pretrained **Mobilenetv2** model?"
I'm not familiar with Microsoft AI-frame and I only need the results to do some experiments (analyze its right and wrong cases). Can you provide the results json file?
" I met some bugs on running ""python demo.py"", It seems like the version of numpy in my anaconda is worng, is there anybody sucessfully running the demo and tell me the version of your numpy? thank you so much!!"
"  I have met this error when runing ""python demo.py"", have anybody meet the same problem and give some ideasï¼Ÿ thank you so much!     Traceback (most recent call last):    File ""demo.py"", line 118, in        main()    File ""demo.py"", line 114, in main      vis_name='{}_detections'.format(file_name), vis_ext=out_extension)    File ""lib/inference.py"", line 188, in aggregate      image_scores = np.hstack([all_boxes[j][i][:, -1] for j in range(1, self.num_classes)])    File ""lib/inference.py"", line 188, in        image_scores = np.hstack([all_boxes[j][i][:, -1] for j in range(1, self.num_classes)])  TypeError: list indices must be integers or slices, not tuple"
"@mahyarnajibi ï¼šthank you very much for you  share the code!  I have only 1 1050ti GPU ,    How many Settings  the  NUM_OF_PROCESS  are appropriate?"
"in lib/data_utils/data_workers.py:   line: 514  it seems that:  'chip_counter += 1' should be outside ""if len(neg_props_in_chips[chip_counter]) > 25 ..."",   In my understanding, when if condition is not satisfied, chip_counter does not change, and error comes.  "
"@bharatsingh430 @mahyarnajibi   I am trying to train sniper on xview dataset which has high resolution images (around 3000 * 3000) with a lot of gt_boxes per image (>900 in some cases). Its in tiff format (8 channels).    I converted the annotations into coco format and tried running the running the script -     While running the train_neg_props_and_sniper.sh script I get the following error         There is something to do with instances with magic number (100, 5)  e.g lib/data_utils/data_workers.py line 293   fgt_boxes = -np.ones((100, 5))  lib/iterators/MNIteratorE2E.py line 177     gt_boxes = -mx.nd.ones((n_batch, 100, 5))    It will be of great help if you can describe what is intended with this (100, 5)."
"Hi,have you compiled success ion windows? I want to  attempt it.    _Originally posted by @JeasonUESTC in      Have you compiled successfully on windows?"
None
"Performing inference for scale: (480, 512)  aaa  [03:37:32] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:107: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)  Traceback (most recent call last):    File ""/root/.pycharm_helpers/pydev/pydevd.py"", line 1758, in        main()    File ""/root/.pycharm_helpers/pydev/pydevd.py"", line 1752, in main      globals = debugger.run(setup['file'], None, None, is_module)    File ""/root/.pycharm_helpers/pydev/pydevd.py"", line 1147, in run      pydev_imports.execfile(file, globals, locals)  # execute the script    File ""/ssd/zyl/SNIPER/main_test.py"", line 66, in        main()    File ""/ssd/zyl/SNIPER/main_test.py"", line 63, in main      imdb_detection_wrapper(sym_inst, config, imdb, roidb, context, arg_params, aux_params, args.vis)    File ""lib/inference.py"", line 355, in imdb_detection_wrapper      roidb, imdb, arg_params, aux_params, vis]))    File ""lib/inference.py"", line 343, in detect_scale_worker      evaluate=False, cache_name='dets_scale_{}x{}'.format(scale[0],scale[1]))    File ""lib/inference.py"", line 229, in get_detections      scores, boxes, data, im_ids = self.detect(batch, scales)    File ""lib/inference.py"", line 100, in detect      print (gpu_out[self.rpn_output_names['rois']])    File ""SNIPER-mxnet/python/mxnet/ndarray/ndarray.py"", line 189, in __repr__      return '\n%s\n ' % (str(self.asnumpy()),    File ""SNIPER-mxnet/python/mxnet/ndarray/ndarray.py"", line 1876, in asnumpy      ctypes.c_size_t(data.size)))    File ""SNIPER-mxnet/python/mxnet/base.py"", line 149, in check_call      raise MXNetError(py_str(_LIB.MXGetLastError()))  mxnet.base.MXNetError: [03:37:33] src/operator/nn/./cudnn/cudnn_convolution-inl.h:689: Check failed: e == CUDNN_STATUS_SUCCESS (4 vs. 0) cuDNN: CUDNN_STATUS_INTERNAL_ERROR  "
@mahyarnajibi @bharatsingh430 using inference multiple times causes memory allocation problem
"I ran into the error when i tried to train SNIPER using python2. It is just fine when runing demo.py.  This is the traceback:  Traceback (most recent call last):    File ""main_train.py"", line 24, in        from lib.data_utils.load_data import load_proposal_roidb, merge_roidb, filter_roidb    File ""/home/qinlikun/SNIPER/lib/data_utils/load_data.py"", line 11, in        from lib.dataset import *    File ""/home/qinlikun/SNIPER/lib/dataset/__init__.py"", line 3, in        from coco import coco    File ""/home/qinlikun/SNIPER/lib/dataset/coco.py"", line 16, in        from mask.mask_voc2coco import mask_voc2coco    File ""lib/mask/mask_voc2coco.py"", line 10, in        from dataset.pycocotools.mask import encode as encodeMask_c    File ""lib/dataset/__init__.py"", line 3, in        from coco import coco    File ""lib/dataset/coco.py"", line 14, in        from .pycocotools.coco import COCO    File ""lib/dataset/pycocotools/coco.py"", line 58, in        import mask    File ""lib/dataset/pycocotools/mask.py"", line 3, in        import _mask as _mask    File ""_mask.pyx"", line 56, in init _mask  AttributeError: type object '_mask.RLEs' has no attribute '__reduce_cython__'    # packages in environment at /home/qinlikun/.conda/envs/sniper:  #  # Name                    Version                   Build  Channel  argparse                  1.4.0                        backports.functools-lru-cache 1.5                          ca-certificates           2019.1.23                     0  certifi                   2019.3.9                 py27_0  cloudpickle               1.0.0                        cycler                    0.10.0                       Cython                    0.29.7                       dask                      1.2.2                        decorator                 4.4.0                        easydict                  1.9                          futures                   3.2.0                        kiwisolver                1.1.0                        libedit                   3.1.20181209         hc058e9b_0  libffi                    3.2.1                hd88cf55_4  libgcc-ng                 8.2.0                hdf63c60_1  libstdcxx-ng              8.2.0                hdf63c60_1  matplotlib                2.2.4                        ncurses                   6.1                  he6710b0_1  networkx                  2.2                          numpy                     1.16.3                       opencv-python             4.1.0.25                     openssl                   1.1.1b               h7b6447c_1  Pillow                    6.0.0                        pip                       19.1.1                   py27_0  protobuf                  3.7.1                        pyparsing                 2.4.0                        python                    2.7.16               h9bab390_0  python-dateutil           2.8.0                        pytz                      2019.1                       PyWavelets                1.0.3                        PyYAML                    5.1                          readline                  7.0                  h7b6447c_5  scikit-image              0.14.2                       scipy                     1.2.1                        setuptools                41.0.1                   py27_0  six                       1.12.0                       sqlite                    3.28.0               h7b6447c_0  subprocess32              3.5.3                        tk                        8.6.8                hbc83047_0  toolz                     0.9.0                        tqdm                      4.32.1                       wheel                     0.33.2                   py27_0  zlib                      1.2.11               h7b6447c_3  "
"I want to make an adjustment based on my dataset but its not clear if the code uses the standard rgb, or works with opencv's convention (bgr)  "
"I want to use this model in android  platform, anyone who have some successful  experience  about this work please help me.  thanks very much !"
"Hi,     According to the R-FCN-3k paper, we can group classes into superclasses by clustering closely related classes. May I ask where in the R-FCN-3k code can I set the preferred # of superclasses? I can't seem to find it. Thank you. "
"Hi. I've tried demo image and found out that this SNIPER works very well.  I want to train this model with my own dataset.   Before trying, I wanted to check whether this training process works ok and I made a small datasets consists of 5 same images named, a1.jpg, a2.jpg, a3.jpg, a4.jpg, a5.jpg  And set annotation file as below which is in the same format to that of COCO.  However, error occurs when I run python main_train.py  ####################  /home/wonjae/SNIPER/configs/faster/default_configs.py:171: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read   for full details.    exp_config = edict(yaml.load(f))  loading annotations into memory...  Done (t=0.00s)  creating index...  index created!  num_images 5  COCO_train2014 gt roidb loaded from ./data/cache/COCO_train2014_gt_roidb.pkl  append flipped images to roidb  loading annotations into memory...  Done (t=0.00s)  creating index...  index created!  num_images 5  COCO_val2014 gt roidb loaded from ./data/cache/COCO_val2014_gt_roidb.pkl  append flipped images to roidb  filtered 2138 roidb entries: 246574 -> 244436  add bounding box regression targets  bbox target means:  [[0. 0. 0. 0.]   [0. 0. 0. 0.]]  [0. 0. 0. 0.]  bbox target stdevs:  [[0.1 0.1 0.2 0.2]   [0.1 0.1 0.2 0.2]]  [0.1 0.1 0.2 0.2]  Creating Iterator with 244436 Images  Total number of extracted chips: 955682  Done!  Traceback (most recent call last):    File ""main_train.py"", line 82, in        threads=config.TRAIN.NUM_THREAD, pad_rois_to=400)    File ""lib/iterators/MNIteratorE2E.py"", line 36, in __init__      super(MNIteratorE2E, self).__init__(roidb, config, batch_size, threads, nGPUs, pad_rois_to, False)    File ""lib/iterators/MNIteratorBase.py"", line 33, in __init__      self.get_batch()    File ""lib/iterators/MNIteratorE2E.py"", line 105, in get_batch      self.batch = self._get_batch()    File ""lib/iterators/MNIteratorE2E.py"", line 193, in _get_batch      processed_list = processed_list.get()    File ""/home/wonjae/anacondas/envs/py27/lib/python2.7/multiprocessing/pool.py"", line 572, in get      raise self._value  TypeError: 'NoneType' object has no attribute '__getitem__'    ######################  It seems that there are many others suffering from same problem.   I've tried the posted replies but it does not worksTT.  If anyone succeded in running his/her own datasets, can you please send the annotation file so that I can check out whether I am setting the annotation file right?  By the way, I've tried number of process/ number of thread to 8,4 but it did not work.   I will be looking forward for your reply.   "
"HI. Thanks for this wonderful SNIPER.    I wanted to train SNIPER using my own dataset and to do that I first had to do the installation following the guide you've mentioned.   make -j 1 USE_CUDA_PATH=/usr/local/cuda  when I type the code right above, it seems something is running but never stops.   It says, Makefile:240:Warning: Significatn performance increases can be achieved by installing and enabling gperftools or jemalloc development packages  Running CUDA_ARCH: -gencode arch=computw_30, code=sm_30 -gencode arch=compute_35, code=sm_35 gencode arch=compute50, code=sm_50 .......... This repeats forever.    Does it takes a lot of time to install mxnet?    If anyone who suffered from the same problem, I will be looking forward for reply.    "
"Hi, @bharatsingh430 @mahyarnajibi    I am trying to reproduce sniper under PyTorch, I have checked that my chip is cropped out similar to yours, and my training loss is under proper value. However when I use the trained model to do visualization, there will be a lot of part rois to be classified as objects. Could you please give me some suggestions? Thank you.    !   !   !   !     BTW, I have successfully trained your mxnet model successfully on the same dataset (VOC2007), your results are visually better in terms of suppression of false positives of parts.  !   "
"Hi,     I want to train the RFCN-3k network on a new dataset. My dataset is significantly different from ImageNet so I hope to re-train the detector and classifier. Do I have to follow the arrangement indicated in the 'Training' section in the cvpr3k readme?     I believe I can replicate this part:     data      |__ imagenet          |__ fall11_whole              |__ n04233124                  |__ xxx.JPEG                      ...                  ...          |__ fall11_whole_bbox              |__ n04233124                  |__ xxx.xml                      ...                  ...    but what about this? I only have the images and the bounding boxes so is it possible to bypass this altogether?  |__ ILSVRC2013_DET_val          |__ ILSVRC2013_DET_val_bbox          |__ ILSVRC2014_devkit              |__ data                  |__ 3kcls_1C_words.txt                  |__ 3kcls_cluster_interval1.txt                  |__ 3kcls_index.txt                  |__ wnid_name_dict.txt                  |__ 3kcls_cluster_result1.txt                  |__ meta_det.mat                  |__ det_lists                      |__ val.txt                          ...              |__ evaluation                  |__ eval_det_3k_1C.m                      ...                  |__ 3k_1C_pred                      |__ 3k_1C_matching.txt                  |__ cache"
"When I use ""bash scripts/compile.sh"",I found `error:`Traceback (most recent call last):    File ""setup_linux.py"", line 56, in        CUDA = locate_cuda()    File ""setup_linux.py"", line 51, in locate_cuda      for k, v in cudaconfig.iteritems():  AttributeError: 'dict' object has no attribute 'iteritems'``  Is the project use python2.7? Hope your reply soon. Thanks"
"hi    We evaluated the sniper's performance on our dataset, but one of classes FP is relatively high. We want to add some negative sample sets for this class. Specifically, we want to be on the negative sample set. Extract chips as negative chips to join the training. But in the current sniper code to extract chips, must be based on gtb, therefore, I want to know how to extract chips on the sample set without gtb? ?    thx"
"I'm trying to train on my own dataset (Visdrone2018), but according to ""Configuring the dataset"" from README.md, only coco and Pascal VOC dataset can be use as dataset for training, i'm looking a format for custom dataset.   Thanks for helps !"
I have tried to ran 'python main_test.py' for several times. but the program always stopped at 11376 or 11464. The GPUs did not run automatically.   !   
"@bharatsingh430 ,hello, if I want to output predict boundingbox, How should I modify?"
"Makefile:166: ""USE_LAPACK disabled because libraries were not found""  Running CUDA_ARCH: -gencode arch=compute_30,code=sm_30 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=[sm_70,compute_70] --fatbin-options -compress-all  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/usr/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=0 -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1 -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free -DUSE_JEMALLOC  -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/mkldnn/mkldnn_convolution.cc -o build/src/operator/nn/mkldnn/mkldnn_convolution.o  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/usr/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=0 -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1 -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free -DUSE_JEMALLOC  -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/mkldnn/mkldnn_act.cc -o build/src/operator/nn/mkldnn/mkldnn_act.o  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/usr/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=0 -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1 -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free -DUSE_JEMALLOC  -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/mkldnn/mkldnn_deconvolution.cc -o build/src/operator/nn/mkldnn/mkldnn_deconvolution.o  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/usr/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=0 -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1 -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free -DUSE_JEMALLOC  -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/mkldnn/mkldnn_fully_connected.cc -o build/src/operator/nn/mkldnn/mkldnn_fully_connected.o  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/usr/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=0 -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1 -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free -DUSE_JEMALLOC  -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/mkldnn/mkldnn_base.cc -o build/src/operator/nn/mkldnn/mkldnn_base.o  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/usr/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=0 -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1 -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free -DUSE_JEMALLOC  -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/mkldnn/mkldnn_softmax.cc -o build/src/operator/nn/mkldnn/mkldnn_softmax.o  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/usr/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=0 -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1 -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free -DUSE_JEMALLOC  -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/mkldnn/mkldnn_pooling.cc -o build/src/operator/nn/mkldnn/mkldnn_pooling.o  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/usr/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=0 -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1 -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free -DUSE_JEMALLOC  -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/mkldnn/mkldnn_sum.cc -o build/src/operator/nn/mkldnn/mkldnn_sum.o  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/usr/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=0 -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1 -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free -DUSE_JEMALLOC  -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/mkldnn/mkldnn_copy.cc -o build/src/operator/nn/mkldnn/mkldnn_copy.o  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/usr/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=0 -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1 -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free -DUSE_JEMALLOC  -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/mkldnn/mkldnn_concat.cc -o build/src/operator/nn/mkldnn/mkldnn_concat.o  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/usr/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=0 -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1 -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free -DUSE_JEMALLOC  -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/cudnn/cudnn_algoreg.cc -o build/src/operator/nn/cudnn/cudnn_algoreg.o  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/usr/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=0 -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1 -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free -DUSE_JEMALLOC  -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/cudnn/cudnn_batch_norm.cc -o build/src/operator/nn/cudnn/cudnn_batch_norm.o  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/usr/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=0 -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1 -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free -DUSE_JEMALLOC  -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/pooling.cc -o build/src/operator/nn/pooling.o  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/usr/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=0 -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1 -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free -DUSE_JEMALLOC  -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/softmax.cc -o build/src/operator/nn/softmax.o  In file included from include/mxnet/./resource.h:31:0,                   from include/mxnet/operator.h:39,                   from src/operator/nn/cudnn/./../convolution-inl.h:33,                   from src/operator/nn/cudnn/./cudnn_algoreg-inl.h:34,                   from src/operator/nn/cudnn/cudnn_algoreg.cc:26:  include/mxnet/./../../src/common/random_generator.h: In static member function â€˜static void mxnet::common::random::RandGenerator ::AllocState(mxnet::common::random::RandGenerator *)â€™:  include/mxnet/./../../src/common/random_generator.h:154:5: error: there are no arguments to â€˜CUDA_CALLâ€™ that depend on a template parameter, so a declaration of â€˜CUDA_CALLâ€™ must be available [-fpermissive]       CUDA_CALL(cudaMalloc(&inst->states_,       ^~~~~~~~~  include/mxnet/./../../src/common/random_generator.h:154:5: note: (if you use â€˜-fpermissiveâ€™, G++ will accept your code, but allowing the use of an undeclared name is deprecated)  include/mxnet/./../../src/common/random_generator.h: In static member function â€˜static void mxnet::common::random::RandGenerator ::FreeState(mxnet::common::random::RandGenerator *)â€™:  include/mxnet/./../../src/common/random_generator.h:159:5: error: there are no arguments to â€˜CUDA_CALLâ€™ that depend on a template parameter, so a declaration of â€˜CUDA_CALLâ€™ must be available [-fpermissive]       CUDA_CALL(cudaFree(inst->states_));       ^~~~~~~~~  In file included from src/operator/nn/mkldnn/../../linalg.h:31:0,                   from src/operator/nn/mkldnn/../fully_connected-inl.h:37,                   from src/operator/nn/mkldnn/mkldnn_fully_connected.cc:26:  src/operator/nn/mkldnn/../.././c_lapack_api.h:330:25: note: #pragma message: Warning: lapack usage not enabled, linalg-operators will not be available. Ensure that lapack library is installed and build with USE_LAPACK=1 to get lapack functionalities.        "" functionalities."")                           ^  In file included from src/operator/nn/mkldnn/../../linalg.h:31:0,                   from src/operator/nn/mkldnn/../convolution-inl.h:44,                   from src/operator/nn/mkldnn/mkldnn_convolution.cc:26:  src/operator/nn/mkldnn/../.././c_lapack_api.h:330:25: note: #pragma message: Warning: lapack usage not enabled, linalg-operators will not be available. Ensure that lapack library is installed and build with USE_LAPACK=1 to get lapack functionalities.        "" functionalities."")                           ^  In file included from src/operator/nn/cudnn/./../../linalg.h:31:0,                   from src/operator/nn/cudnn/./../convolution-inl.h:44,                   from src/operator/nn/cudnn/./cudnn_algoreg-inl.h:34,                   from src/operator/nn/cudnn/cudnn_algoreg.cc:26:  src/operator/nn/cudnn/./../.././c_lapack_api.h: At global scope:  src/operator/nn/cudnn/./../.././c_lapack_api.h:330:25: note: #pragma message: Warning: lapack usage not enabled, linalg-operators will not be available. Ensure that lapack library is installed and build with USE_LAPACK=1 to get lapack functionalities.        "" functionalities."")                           ^  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/usr/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=0 -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1 -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free -DUSE_JEMALLOC  -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/deconvolution.cc -o build/src/operator/nn/deconvolution.o  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/usr/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=0 -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1 -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free -DUSE_JEMALLOC  -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/activation.cc -o build/src/operator/nn/activation.o  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/usr/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=0 -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1 -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free -DUSE_JEMALLOC  -I/content/sample_data/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/batch_norm.cc -o build/src/operator/nn/batch_norm.o  Makefile:392: recipe for target 'build/src/operator/nn/cudnn/cudnn_algoreg.o' failed  make: *** [build/src/operator/nn/cudnn/cudnn_algoreg.o] Error 1  make: *** Waiting for unfinished jobs....    **Following are the configuration:**    I have tried   CUDA 10.1 with associated cUDNN  CUDA 10.0 with associated cUDNN  CUDA 9.2 with associated cUDNN    I am getting the same error in all the CUDA setup.    I am using google Colab. Its currently using Ubuntu 18.0.4    Kindly help me..  I am stuck on this for very long time."
"@mahyarnajibi ,hello, please tell me how to deal with this question? thank you!  aaa  Traceback (most recent call last):    File ""main_train.py"", line 145, in        epoch_end_callback=epoch_end_callback, arg_params=arg_params, aux_params=aux_params)    File ""SNIPER-mxnet/python/mxnet/module/base_module.py"", line 484, in fit      for_training=True, force_rebind=force_rebind)    File ""SNIPER-mxnet/python/mxnet/module/module.py"", line 430, in bind      state_names=self._state_names)    File ""SNIPER-mxnet/python/mxnet/module/executor_group.py"", line 265, in __init__      self.bind_exec(data_shapes, label_shapes, shared_group)    File ""SNIPER-mxnet/python/mxnet/module/executor_group.py"", line 361, in bind_exec      shared_group))    File ""SNIPER-mxnet/python/mxnet/module/executor_group.py"", line 639, in _bind_ith_exec      shared_buffer=shared_data_arrays, **input_shapes)    File ""SNIPER-mxnet/python/mxnet/symbol/symbol.py"", line 1524, in simple_bind      raise RuntimeError(error_msg)  RuntimeError: simple_bind error. Arguments:  bbox_target: (16, 84L, 32L, 32L)  im_info: (16, 3L)  valid_ranges: (16, 2L)  label: (16, 21504L)  gt_boxes: (16, 100L, 5L)  bbox_weight: (16, 84L, 32L, 32L)  data: (16, 3L, 512L, 512L)  [18:06:05] src/storage/storage.cc:65: Check failed: e == cudaSuccess || e == cudaErrorCudartUnloading CUDA: invalid device ordinal    "
"To speed up inference, I've tried to utilise the inbuilt parallel inference functionality of SNIPER. I have 5 images as my test images and have set CONCURRENT_JOBS to 5. But on running main_test.py, I'm unable get any result. The following is my output:       Using `nvidia-smi` I can see that the 5 processes are being spawned but the program execution seems to be stuck here. I tried to check what's the source of the problem and found that it seemed to originate from the forward function. It seems that the forward function is not being run correctly as the print(""Res"") command isn't executed. Below is the forward function.         Could anyone tell what's the reason behind this behaviour? "
"hello, I want to convert mxnet model to caffe model,any ideal for me? or, I can use c++ to load model to detect the image!  thank you!"
"I'm trying to optimise the inference time when dealing with a set of images. Using the main_test.py to process 5 images, takes about 15 seconds.         Is this the maximum speed attainable on a single GPU? For reference: I'm currently using a 12 GB Nvidia Titan XP. Do you have any suggestions in improving inference time for a batch of images? Is splitting the images between multiple GPUs the only way to speed up? If so, how can I go about doing that?        **Update**: The paper states that on a single Tesla V100 GPU you were able run inference on 5 images per second. I wanted to try this out for myself and the following is my result.         As you can see while there is some speedup, the overall process still takes about the same amount of time. Is this the maximum speed at which it can infer or are there additional modifications that could be made to make it faster?     You had mentioned in the paper that you had **run multiple processes in parallel during inference**. Could you tell me how that was done?  "
"As installing this is not as straightforward as it seems, would you please consider releasing a docker file? I think that in the long run this will probably save you dozens of issues"
"Hi there, thanks for sharing this awesome detector!    After going through the installation procedure step by step with no errors, I ran the `demo.py`. But it threw an error which looks like an issue installing MXNet.    Any ideas as to how I can resolve this?     "
"Thanks for the paper/code. I was wondering how fast the MobileNetv2 variant is and wasn't able to find it in this board/repo nor in the paper. So, did anyone try it? Perhaps original authors can answer? :)    Thanks"
hi budy how to resume train exit model in this project
"Hello,      I've noticed that you scale up the loss by cfg.TRAIN.scale and scale down the learning_rate, weight decay and warmup_lr for mixed precision training. But I found the common practice seems that just scaling down the rescale_grad is enough.  Could you please explain that? Thx!       btw, it's somewhat hard to understand that why you scale up the `weight decay`. Should not it be scale down either (same as lr and warmup_lr)?"
"when train with mobilenetv2,the errors occurred as following:  Traceback (most recent call last):    File ""/home/fenghan/SNIPER/main_train.py"", line 91, in        sym = sym_inst.get_symbol_rpn(config) if config.TRAIN.ONLY_PROPOSAL else sym_inst.get_symbol_rcnn(config)    File ""/home/fenghan/SNIPER/symbols/faster/mobilenetv2_e2e.py"", line 254, in get_symbol_rcnn      name='multi_proposal_target')    File "" "", line 107, in MultiProposalTarget    File ""SNIPER-mxnet/python/mxnet/_ctypes/symbol.py"", line 135, in _symbol_creator      s._compose(name=name, **kwargs)    File ""SNIPER-mxnet/python/mxnet/symbol/symbol.py"", line 481, in _compose      self.handle, name, num_args, keys, args))    File ""SNIPER-mxnet/python/mxnet/base.py"", line 149, in check_call      raise MXNetError(py_str(_LIB.MXGetLastError()))  mxnet.base.MXNetError: [11:34:07] src/core/symbolic.cc:72: Symbol.ComposeKeyword argument name crowd_boxes not found."
"I'm using VOC2007  dataset. It failing with the following error:    The Iterator has 20925 samples!  Initializing the model...  {'bbox_target': (45L, 84L, 32L, 32L), 'im_info': (45L, 3L), 'label': (45L, 21504L), 'valid_ranges': (45L, 2L), 'bbox_weight': (45L, 84L, 32L, 32L)                                                     , 'gt_boxes': (45L, 100L, 5L), 'data': (45L, 3L, 512L, 512L)}  ********************     infer_shape error. Arguments:    bbox_target: (45L, 84L, 32L, 32L)    im_info: (45L, 3L)    valid_ranges: (45L, 2L)    label: (45L, 21504L)    gt_boxes: (45L, 100L, 5L)    bbox_weight: (45L, 84L, 32L, 32L)    data: (45L, 3L, 512L, 512L)  Traceback (most recent call last):    File ""main_train.py"", line 106, in        sym_inst.infer_shape(shape_dict)    File ""/home/SNIPER/symbols/symbol.py"", line 44, in infer_shape      arg_shape, out_shape, aux_shape = self.sym.infer_shape(**data_shape_dict)    File ""SNIPER-mxnet/python/mxnet/symbol/symbol.py"", line 995, in infer_shape      res = self._infer_shape_impl(False, *args, **kwargs)    File ""SNIPER-mxnet/python/mxnet/symbol/symbol.py"", line 1125, in _infer_shape_impl      ctypes.byref(complete)))    File ""SNIPER-mxnet/python/mxnet/base.py"", line 149, in check_call      raise MXNetError(py_str(_LIB.MXGetLastError()))  mxnet.base.MXNetError: Error in operator cls_prob_reshape:  +0x5b) [0x7ff7f5da901b]  [bt] (1) /home/SNIPER/SNIPER-mxnet/python/mxnet/../../lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x28) [0x7ff7f5da9b88                                                     ]  [bt] (2) /home/SNIPER/SNIPER-mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::op::ReshapeShape(nnvm::NodeAttrs const&, std::vector  >*, std::vector  >*)+0xa1c) [0x7ff7f661552c]  [bt] (3) /home/SNIPER/SNIPER-mxnet/python/mxnet/../../lib/libmxnet.so(+0x2fd0ab1) [0x7ff7f821aab1]  [bt] (4) /home/SNIPER/SNIPER-mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::exec::InferShape(nnvm::Graph&&, std::vector  >&&, std::__cxx11::basic_string , std::allocator  > const&)+0x1ae7) [0x7ff7f821cc7                                                     7]  [bt] (5) /home/SNIPER/SNIPER-mxnet/python/mxnet/../../lib/libmxnet.so(MXSymbolInferShape+0xddf) [0x7ff7f87e0f8f]  [bt] (6) /home/.conda/envs/py2.7/lib/python2.7/lib-dynload/../../libffi.so.6(ffi_call_unix64+0x4c) [0x7ff840f7aec0]  [bt] (7) /home/.conda/envs/py2.7/lib/python2.7/lib-dynload/../../libffi.so.6(ffi_call+0x22d) [0x7ff840f7a87d]  [bt] (8) /home/.conda/envs/py2.7/lib/python2.7/lib-dynload/_ctypes.so(_ctypes_callproc+0x4de) [0x7ff8411918de]  [bt] (9) /home/.conda/envs/py2.7/lib/python2.7/lib-dynload/_ctypes.so(+0x9b31) [0x7ff841187b31]  "
"Traceback (most recent call last):    File ""demo.py"", line 118, in        main()    File ""demo.py"", line 79, in main      sym = sym_inst.get_symbol_rcnn(config, is_train=False)    File ""/home/shaoyidu/Program/guoleida/detection/SNIPER/symbols/faster/resnet_mx_101_e2e.py"", line 327, in get_symbol_rcnn      rois, _ = mx.sym.MultiProposal(cls_prob=rpn_cls_prob_reshape, bbox_pred=rpn_bbox_pred, im_info=im_info,  AttributeError: 'module' object has no attribute 'MultiProposal'  How to solve it? Thanks"
"when i train the model on coco dataset, i got this error.  !     Can you help me fix the problem?"
"I have removed cache from the VOCdevkit folder,and the pascal_voc.py have modified to my own label.         self.classes = ['__background__', #always index 0                          'DiaoChe','TaDiao', 'ShiGongJiXie', 'DaoXianYiWu', 'YanHuo']  When I run my data set, I always report an errorï¼š  Traceback (most recent call last):    File ""main_train.py"", line 72, in        for image_set in image_sets]    File ""lib/data_utils/load_data.py"", line 29, in load_proposal_roidb      roidb = imdb.gt_roidb()    File ""lib/dataset/pascal_voc.py"", line 105, in gt_roidb      gt_roidb = [self.load_pascal_annotation(index) for index in self.image_set_index]    File ""lib/dataset/pascal_voc.py"", line 168, in load_pascal_annotation      cls = class_to_index[obj.find('name').text.lower().strip()]  KeyError: 'yanhuo'  Can you give me a hand?"
Valid range for the two scales the author reported in paper: [1.667 and max size of 512]
"When I fix bn, the performance is so bad, I don't know why. @bharatsingh430 How about the your result using fix bn? In my experiment, batch-size is 8."
"when i run this train_main.py, the memory increase seriouslyï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼1  My memory is 128G"
"When I run the demo.py, I got the error:  Traceback (most recent call last):    File ""demo.py"", line 16, in        from iterators.MNIteratorTest import MNIteratorTest    File ""lib/iterators/MNIteratorTest.py"", line 4, in        from MNIteratorBase import MNIteratorBase    File ""lib/iterators/MNIteratorBase.py"", line 4, in        from concurrent.futures import ThreadPoolExecutor  ImportError: No module named concurrent.futures  (SNIP) t1@t1:/media/t1/0a33a0a0-b876-45c1-bb1c-f0d  Could anyone help me?"
"it maybe mxnet compile error, but i can not solve the problem"
"It's a problem when i run demo.py.  Traceback (most recent call last):    File ""demo.py"", line 16, in        from iterators.MNIteratorTest import MNIteratorTest    File ""lib/iterators/MNIteratorTest.py"", line 3, in        from data_utils.data_workers import im_worker    File ""lib/data_utils/data_workers.py"", line 10, in        from nms.nms import nms_wrapper    File ""lib/nms/nms.py"", line 3, in        from cpu_nms import cpu_nms, cpu_soft_nms  ImportError: No module named cpu_nms  Who can give me a help? Thank you."
None
"@bharatsingh430  hi,  When I enter ""make -j 1 USE_CUDA_PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\lib\x64"", but I got some error.  !     p.s. My OS is Windows10.  Can you explain why I got this error? Thank you very much.  "
"I use conda as my environment,  after I install CUDA 9.0, cuDNN 7.1.2 openblas 0.3.3 , I use this command :   make -j 1 USE_CUDA_PATH=./anaconda3/pkgs/cudatoolkit-9.2-0 USE_NCCL=1    t**he error shows like this:**  In file included from /home/huanhuan/SNIPER/SNIPER-mxnet/3rdparty/mshadow/mshadow/tensor.h:16:0,                   from include/mxnet/./base.h:32,                   from include/mxnet/io.h:34,                   from src/operator/nn/mkldnn/../convolution-inl.h:30,                   from src/operator/nn/mkldnn/mkldnn_convolution.cc:26:  **/home/huanhuan/SNIPER/SNIPER-mxnet/3rdparty/mshadow/mshadow/./base.h:155:23: fatal error: cblas.h: no such files**  compilation terminated.  Makefile:392: recipe for target 'build/src/operator/nn/mkldnn/mkldnn_convolution.o' failed  make: *** [build/src/operator/nn/mkldnn/mkldnn_convolution.o] Error 1    Could anyone give me same advise? Thanks"
"I train with the voc dataset ,however the performance is low, Is there any suggestions?  PS. I did negative chip mining with all-in-one script, `bash train_neg_props_and_sniper.sh --cfg configs/faster/sniper_res101_e2e_pascal_voc.yml`    AP for aeroplane = 0.8877  AP for bicycle = 0.8689  AP for bird = 0.8217  AP for boat = 0.7576  AP for bottle = 0.6952  AP for bus = 0.8733  AP for car = 0.8941  AP for cat = 0.8880  AP for chair = 0.7121  AP for cow = 0.8673  AP for diningtable = 0.7425  AP for dog = 0.8656  AP for horse = 0.8817  AP for motorbike = 0.8924  AP for person = 0.8699  AP for pottedplant = 0.5786  AP for sheep = 0.8156  AP for sofa = 0.8142  AP for train = 0.8564  AP for tvmonitor = 0.8102  Mean AP@0.5 = 0.8196  AP for aeroplane = 0.7999  AP for bicycle = 0.7931  AP for bird = 0.6629  AP for boat = 0.5752  AP for bottle = 0.5877  AP for bus = 0.8509  AP for car = 0.8673  AP for cat = 0.8202  AP for chair = 0.5877  AP for cow = 0.7890  AP for diningtable = 0.6201  AP for dog = 0.7486  AP for horse = 0.7710  AP for motorbike = 0.8105  AP for person = 0.7156  AP for pottedplant = 0.4109  AP for sheep = 0.7237  AP for sofa = 0.7352  AP for train = 0.7798  AP for tvmonitor = 0.7820  Mean AP@0.7 = 0.7216  All done!    I just change the sniper_res101_e2e_pascal_voc.yml a little to adjust them for my system.     "
"I am just trying to training SNIPER on mu own dataset, and the below is my config:     After few times iteration, I got 82% RPNacc and 95% RCNNacc, but the testing log is just show below i cant understand why     Any one can help me to fix it out? Maybe I think there is some bugs in the repo?     "
AttributeError: 'module' object has no attribute 'MultiProposal'    This is the error i receive when I try running demo.py on the SNIPER github repo code    I've built Mxnet from source (GPU version) and I am trying to run on Ubuntu 16.0.4 on a GTX 1070 GPU. 
"Traceback (most recent call last):    File ""demo_3k.py"", line 21, in        from bbox.bbox_transform import bbox_pred, clip_boxes    File ""lib/bbox/bbox_transform.py"", line 2, in        from bbox import bbox_overlaps_cython, ignore_overlaps_cython  ImportError: cannot import name bbox_overlaps_cython    This is the error I get after installing mxnet and other pre-requisites and running the demo_3k python executable.       Also I would like to know if there is a build process for running the R-FCN 3000 on a GPU machine"
"Thanks for open the SNIPER code.  Have yet selected SNIPER to add on my base model for another dataset. But I  only get 2-3 points improvement for mAP0.75, which is not satisfying.    So firstly I wonder if @mahyarnajibi  or anyone can confirm whether I have done the correct main change points or not.    Changes as following:  1st.The original image size is 1280*712, I have set the valid range ([-1, 70], [60, 180], [170, -1] ) and scales([ 3.0, 1.667, 1280.0]), and use the same code to generated chips.    2nd.I have made anchors on chips and set those  anchors with gt_box not in the valid range to -1 which is not being trained later.    Negative samples are not used currently, am I missing any important changes for SNIPER?    "
"Has anybody train this code on pascal dataset? My dataset was made in pascal dataset , so I want to know if this code can train on pascal dataset or not.And how to do it.  Hope some guidance, thank you so much!"
"Hello everyone:    I find the sublabel is always (gt's gt_subclassid - 1); so after training, the inference result is 1 less than gt_subclassid; anyone else the same?  "
"I am looking through the code for a script or function that allows me to infer with one single image each time. Did you provide such function already in the code. If not, do you have any plan to add this ?"
"If so,when will you release this version?"
None
"I refer your code to my own dataset, and the bounding box is very accurate, but the classification confidence is very low(I see the detections_val_results.json,the scores only 0.001,the threshold  is 0.5), so that I can't get the test detect result in the end.     Have you ever encountered this situation, can you give me some advice?  Thank you very much.  @mahyarnajibi "
"Hi,  Could you provide what version of gcc I should use for sniper-mxnet?  Currently, I built with gcc 5.5. 0, ubuntu18.04.1, cuda 9.0, conda 4.5.1. I got the following error :  usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11300): error: argument of type ""void *"" is incompatible with parameter of type ""long long *""  thanks in advance.    "
"When downloading the pre-trained model using"" bash scripts/download_sniper_detector.sh"", it failed at unzip the file. Is there something wrong? Thanks    2018-09-08 10:42:01 (26.8 KB/s) - â€˜./sniper_models.tar.gzâ€™ saved [237587099/237587099]    Done!  Unzipping the file...  output/  output/sniper_mobilenetv2_bn/  output/sniper_mobilenetv2_bn/sniper_mobilenetv2_e2e/  output/sniper_mobilenetv2_bn/sniper_mobilenetv2_e2e/train2014_val2014/  output/sniper_mobilenetv2_bn/sniper_mobilenetv2_e2e/train2014_val2014/SNIPER-0007.params    gzip: stdin: invalid compressed data--format violated  tar: Unexpected EOF in archive  tar: Unexpected EOF in archive  tar: Error is not recoverable: exiting now"
"loading annotations into memory...  Done (t=1.07s)  creating index...  index created!  num_images 18337  COCO_train2014 gt roidb loaded from /home/nd/jliangqiu/datasets/COCO_SINPER/cache/COCO_train2014_gt_roidb.pkl  append flipped images to roidb  filtered 0 roidb entries: 36674 -> 36674  add bounding box regression targets  bbox target means:  [[0. 0. 0. 0.]   [0. 0. 0. 0.]]  [0. 0. 0. 0.]  bbox target stdevs:  [[0.1 0.1 0.2 0.2]   [0.1 0.1 0.2 0.2]]  [0.1 0.1 0.2 0.2]  Creating Iterator with 36674 Images  Total number of extracted chips: 167952  Done!  The Iterator has 167952 samples!  Initializing the model...  Optimizer params: {'wd': 0.01, 'lr_scheduler':  , 'multi_precision': True, 'learning_rate': 0.00015, 'rescale_grad': 1.0, 'clip_gradient': None, 'momentum': 0.9}  checkpointing.............................  fit.......................................  [('data', (16L, 3L, 512L, 512L)), ('valid_ranges', (16L, 2L)), ('im_info', (16L, 3L))]  3  a  aaa  [10:45:40] src/kvstore/././comm.h:628: only 0 out of 2 GPU pairs are enabled direct access. It may affect the performance. You can set MXNET_ENABLE_GPU_P2P=0 to turn it off  [10:45:40] src/kvstore/././comm.h:637: ..  [10:45:40] src/kvstore/././comm.h:637: ..  [10:45:40] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:107: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)  Image Resize Failed!  Image Resize Failed!  Image Resize Failed!  Image Resize Failed!  Image Resize Failed!  Image Resize Failed!  Image Resize Failed!  "
"Dear Author:    Thanks for this awesome work. Currently, I'm using this framework tranined on my own dataset. However, during training, it is stucked in following place for quite a few hours. Do you know what is going on and have any advice? Thanks a lot!    ! "
i just can get the resnet-101 model here
"Traceback (most recent call last):    File ""main_train.py"", line 64, in        for image_set in image_sets]    File ""lib/data_utils/load_data.py"", line 29, in load_proposal_roidb      roidb = imdb.gt_roidb()    File ""lib/dataset/coco.py"", line 135, in gt_roidb      roientry,flag = self._load_coco_annotation(index)    File ""lib/dataset/coco.py"", line 192, in _load_coco_annotation      if obj['area'] > 0 and x2 >= x1 and y2 >= y1:  KeyError: 'area'    Who have met the same question?Please help me, thanks!!"
Thanks for your code very much.  Could you report the human AP on the coco test-dev dataset ?
"I've noticed that you replace the last few rois with gt_boxes in  , which means that there are accurate proposals for each RoI batch.    1) How much does this replacement benefit the overall accuracy?  2) Should we disable these lazy RoIs for END2END training?"
"i run the demo.py with mobilenetv2 model on the P40 ,about 150ms,the speed is slow,something wrong?  i never change the config.    Tester: 1/1, Detection: **0.1813s**, Post Processing: 0.001738s  Tester: 1/1, Detection: **0.1461s**, Post Processing: 0.001268s  Tester: 1/1, Detection: **0.1313s**, Post Processing: 0.001222s  "
"Hello,  I have read the SNIPER paper and something is not clear to me.  Is this training/inference method compatible only with 2 stage detectors like Faster RCNN?  Or can it also be applied to SSD?    Thank you"
"Hello,    First of all thank you for putting the code here on github so we can all use and experiment with it.  I am trying to train on my own dataset,but I am having a little bit of trouble.  More specifically, I have  this error:  File ""lib/iterators/MNIteratorE2E.py"", line 194, in _get_batch      processed_list = processed_list.get()  File ""/home/radud/anaconda3/envs/mxnetpy2/lib/python2.7/multiprocessing/pool.py"", line 572, in get      raise self._value  TypeError: 'NoneType' object has no attribute '__getitem__'    Any help would be appreciated.  Thank you.    "
"1. there are three branch, i want to know theirs relations, is the model structure is the same, but only train on different data set?  2. as the master branch i try demo.py(output/sniper_res101_bn/sniper_res101_e2e/train2014_val2014/SNIPER-0007.params) which is  coco 81 class, the result is very well especially for small object, so i want to try the openimage image v4's 600 class, i download the pretrained model by scripts, how can i modify the config file and demo.py?"
"Hi , I tried to use SNIPER on a new dataset but the perfomance is not satisfying. I think the problem is that my own neg-chips are not as good as yours in coco dataset. Could you please release the SNIP code so that I can train my dataset on SNIP and compare their performances?    Thanks a lot"
"Hello, @mahyarnajibi ! It is possible to test your pretrained model to detect 600 classes? I check branch `openimages2` and in config see ` pretrained: ""./data/pretrained_model/CRCNN"" ` row, but `bash scripts/download_pretrained_model.sh` doesn't download  anything, but only `resnet_mx_101_open-0000`(and imagenet ofc). This is what we need, this model is pretraied for 600 classes? and we need only move(or replace config's row with path) model to CRCNN folder and run `python demo.py` with custom config?"
When I try bash scripts/compile.sh   Here are bugs:    x86_64-linux-gnu-gcc: error: unrecognized command line option â€˜-Wdate-timeâ€™  x86_64-linux-gnu-gcc: error: unrecognized command line option â€˜-fstack-protector-strongâ€™  error: command 'x86_64-linux-gnu-gcc' failed with exit status 1    My solution is as follows:  sudo rm -r /usr/bin/gcc  sudo ln -s /usr/bin/gcc-5 /usr/bin/gcc  sudo ln -s /usr/bin/gcc-5 /usr/bin/gcc  sudo  rm /usr/bin/x86_64-linux-gnu-gcc  sudo ln -s /usr/bin/gcc-5 /usr/bin/x86_64-linux-gnu-gcc       
Great work. I think it would be really great if there is an inference example script for running openimagev4 pretrained model just similar the current demo.py (COCO). 
"Hi;    I noticed `ms < (self.chip_size - self.chip_stride - 1) / im_scale` is served as a constraint to choose the valid gt_boxes id(please see below), which however is unmentioned in you paper.  Could you please explain the idea behind this constraint? Thank youï¼     "
"Hi, I try to train SNIPER on my own dataset, but the performance always keep low as bellow,     Loss and accuracy seem just fine as bellow,     Here is the yaml config,     I have struggled for several days but couldn't figure it out.  Hope you can give some advice.  Thanks :)"
"If I change rpn_post_nms_top_n, the metric will get wrong at     pred_label = pred.asnumpy().reshape(-1, last_dim).argmax(axis=1).astype('int32')              "
"Hi, I got an index out of bound error when training my own model.    > loading annotations into memory...  Done (t=0.35s)  creating index...  index created!  num_images 11500  COCO_train2014 gt roidb loaded from ./data/cache/COCO_train2014_gt_roidb.pkl  appending ground truth annotations  Reading cached proposals after ***NMS**** from data/proposals/COCO_train2014_rpn_after_nms.pkl  Done!  append flipped images to roidb  filtered 7928 roidb entries: 23000 -> 15072  add bounding box regression targets  bbox target means:  [[0. 0. 0. 0.]   [0. 0. 0. 0.]]  [0. 0. 0. 0.]  bbox target stdevs:  [[0.1 0.1 0.2 0.2]   [0.1 0.1 0.2 0.2]]  [0.1 0.1 0.2 0.2]  Creating Iterator with 15072 Images  Total number of extracted chips: 72201  Done!  The Iterator has 72201 samples!  Initializing the model...  Optimizer params: {'wd': 0.01, 'lr_scheduler':  , 'multi_precision': True, 'learning_rate': 0.00015, 'rescale_grad': 1.0, 'clip_gradient': None, 'momentum': 0.9}  aaa  [00:32:22] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:107: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)  Exception in thread Thread-8:  Traceback (most recent call last):    File ""/usr/lib/python2.7/threading.py"", line 801, in __bootstrap_inner      self.run()    File ""/usr/lib/python2.7/threading.py"", line 754, in run      self.__target(*self.__args, **self.__kwargs)    File ""lib/iterators/PrefetchingIter.py"", line 61, in prefetch_func      self.next_batch[i] = self.iters[i].next()    File ""lib/iterators/MNIteratorBase.py"", line 90, in next      if self.iter_next():    File ""lib/iterators/MNIteratorBase.py"", line 87, in iter_next      return self.get_batch()    File ""lib/iterators/MNIteratorE2E.py"", line 99, in get_batch      self.batch = self._get_batch()    File ""lib/iterators/MNIteratorE2E.py"", line 136, in _get_batch      nids = processed_roidb[i]['props_in_chips'][cropid]  IndexError: index 12 is out of bounds for axis 0 with size 12    How can I fix this?"
"`loading annotations into memory...  Done (t=0.11s)  creating index...  index created!  num_images 8100  COCO_train2014 gt roidb loaded from ./data/cache/COCO_train2014_gt_roidb.pkl  append flipped images to roidb  loading annotations into memory...  Done (t=0.01s)  creating index...  index created!  num_images 900  COCO_val2014 gt roidb loaded from ./data/cache/COCO_val2014_gt_roidb.pkl  append flipped images to roidb  filtered 0 roidb entries: 18000 -> 18000  add bounding box regression targets  bbox target means:  [[0. 0. 0. 0.]   [0. 0. 0. 0.]]  [0. 0. 0. 0.]  bbox target stdevs:  [[0.1 0.1 0.2 0.2]   [0.1 0.1 0.2 0.2]]  [0.1 0.1 0.2 0.2]  Creating Iterator with 18000 Images  Total number of extracted chips: 30025  Done!  Traceback (most recent call last):    File ""main_train.py"", line 78, in        threads=config.TRAIN.NUM_THREAD, pad_rois_to=400)    File ""lib/iterators/MNIteratorE2E.py"", line 32, in __init__      super(MNIteratorE2E, self).__init__(roidb, config, batch_size, threads, nGPUs, pad_rois_to, False)    File ""lib/iterators/MNIteratorBase.py"", line 33, in __init__      self.get_batch()    File ""lib/iterators/MNIteratorE2E.py"", line 100, in get_batch      self.batch = self._get_batch()    File ""lib/iterators/MNIteratorE2E.py"", line 195, in _get_batch      processed_list = processed_list.get()    File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 567, in get      raise self._value  TypeError: 'NoneType' object has no attribute '__getitem__'`    could you help me to fix it?"
"Hi authors, Great work! And I know that you're migrating caffe SSH to this repo which will be re-write by MXnet. I have done such work these days but can not achieve the same accuracy as it was by Caffe. For example, single scale 79.1% mAP on widerface-hard, vs reported 81.4% from caffe SSH.  Do you have any tips on it?     Thanks."
"Hi, I see that in openimages2 branch, you train for 14 epochs. What are the results?"
"Really awesome work!  Do you plan to release an inference script for the `openimages2` branch, as well as a pretrained model?"
"Hi,    Thank you for your repo and paper. This is an awesome work!    I want to ask that in your paper      You mentioned an ensemble operation. You said that:  > Since proposals are shared across all networks, we  average the scores and box-predictions for each RoI. During  flipping we average the detection scores and bounding  box predictions.     How to average box predictions for each RoI?     How did you implemented in your code?    Thank you!"
"What are the CRCNN-0007 weights trained on?    For context, it fails to detect people in images."
"I'm using a custom dataset. But it's asking for RPN weights, is there any way to bypass that. If I load the weights, it failing with the following error:   "
"Hello, @mahyarnajibi, I am new to mxnet and when I run the cmake command, it turns out:    -- The C compiler identification is GNU 4.8.4  -- The CXX compiler identification is GNU 4.8.4  -- Check for working C compiler: /usr/bin/cc  -- Check for working C compiler: /usr/bin/cc -- works  -- Detecting C compiler ABI info  -- Detecting C compiler ABI info - done  -- Detecting C compile features  -- Detecting C compile features - done  -- Check for working CXX compiler: /usr/bin/c++  -- Check for working CXX compiler: /usr/bin/c++ -- works  -- Detecting CXX compiler ABI info  -- Detecting CXX compiler ABI info - done  -- Detecting CXX compile features  -- Detecting CXX compile features - done  -- CMake version '3.11.4' using generator 'Unix Makefiles'  -- The CUDA compiler identification is NVIDIA 8.0.26  -- Check for working CUDA compiler: /usr/local/cuda-8.0/bin/nvcc  -- Check for working CUDA compiler: /usr/local/cuda-8.0/bin/nvcc -- works  -- Detecting CUDA compiler ABI info  -- Detecting CUDA compiler ABI info - done  -- Performing Test SUPPORT_CXX11  -- Performing Test SUPPORT_CXX11 - Success  -- Performing Test SUPPORT_CXX0X  -- Performing Test SUPPORT_CXX0X - Success  -- Performing Test SUPPORT_MSSE2  -- Performing Test SUPPORT_MSSE2 - Success  CMake Deprecation Warning at 3rdparty/mkldnn/CMakeLists.txt:21 (cmake_policy):    The OLD behavior for policy CMP0048 will be removed from a future version    of CMake.      The cmake-policies(7) manual explains that the OLD behaviors of all    policies are deprecated and that a policy should be set to OLD only under    specific short-term circumstances.  Projects should be ported to the NEW    behavior and not rely on setting a policy to OLD.      CMake Deprecation Warning at 3rdparty/mkldnn/CMakeLists.txt:22 (cmake_policy):    The OLD behavior for policy CMP0054 will be removed from a future version    of CMake.      The cmake-policies(7) manual explains that the OLD behaviors of all    policies are deprecated and that a policy should be set to OLD only under    specific short-term circumstances.  Projects should be ported to the NEW    behavior and not rely on setting a policy to OLD.      -- CMAKE_BUILD_TYPE is unset, defaulting to Release  -- Detecting Intel(R) MKL: trying mklml_intel  -- Detecting Intel(R) MKL: trying mklml  -- Detecting Intel(R) MKL: trying mkl_rt  CMake Warning at 3rdparty/mkldnn/cmake/MKL.cmake:178 (message):    Intel(R) MKL not found.  Some performance features may not be available.    Please run scripts/prepare_mkl.sh to download a minimal set of libraries or    get a full version from    Call Stack (most recent call first):    3rdparty/mkldnn/cmake/OpenMP.cmake:25 (include)    3rdparty/mkldnn/CMakeLists.txt:57 (include)      -- Found OpenMP_C: -fopenmp (found version ""3.1"")   -- Found OpenMP_CXX: -fopenmp (found version ""3.1"")   -- Found OpenMP: TRUE (found version ""3.1"")    -- Could NOT find Doxygen (missing: DOXYGEN_EXECUTABLE)   -- VTune profiling environment is unset  -- Looking for pthread.h  -- Looking for pthread.h - found  -- Looking for pthread_create  -- Looking for pthread_create - found  -- Found Threads: TRUE    -- Could NOT find MKL (missing: MKL_INCLUDE_DIR MKLML_GNU_LIBRARY MKLDNN_LIBRARY)   --  MKL not found  -- Found CUDA: /usr/local/cuda-8.0 (found version ""8.0"")   -- Could NOT find MKL (missing: MKL_INCLUDE_DIR MKLML_GNU_LIBRARY MKLDNN_LIBRARY)   -- Found OpenBLAS libraries: /usr/lib/libopenblas.so  -- Found OpenBLAS include: /usr/include  -- Could NOT find Gperftools (missing: GPERFTOOLS_LIBRARIES GPERFTOOLS_INCLUDE_DIR)   -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.26"")   -- Could NOT find Jemalloc (missing: JEMALLOC_LIBRARY JEMALLOC_INCLUDE_DIR)   --  OpenCV_LIBS=opencv_core;opencv_highgui;opencv_imgproc  -- OpenCV found (/usr/share/OpenCV)  -- Found OpenMP_C: -fopenmp    -- Found OpenMP_CXX: -fopenmp    -- Found OpenMP: TRUE     CMake Error at SNIPER/SNIPER-mxnet/build/3rdparty/openmp/runtime/CMakeFiles/CMakeTmp/CMakeLists.txt:3 (project):    Generator        Unix Makefiles      does not support toolset specification, but toolset        cuda=,host=x64      was specified.      CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage  CMake Error at 3rdparty/openmp/runtime/cmake/LibompGetArchitecture.cmake:57 (try_run):    Failed to configure test project build system.  Call Stack (most recent call first):    3rdparty/openmp/runtime/CMakeLists.txt:40 (libomp_get_architecture)      -- Configuring incomplete, errors occurred!    Would you please be so kind as to help me out? Thanks."
"Hi, thanks for sharing the codes, I failed to download the model due to the network accessability, log looks like the following:     would you please provide some other download links like google drive? Thanks"
"Hi, it seems a broken link to ImageNet pre-trained checkpoint :          Could you fix the link to this file? thanks."
"when  I run ` python main_train.py --cfg configs/faster/sniper_res101_e2e_pascal_voc.yml`, it reported an error, how to work it?  "
!   
I was not able to build SNIPER-mxnet when disabling CUDA. Following error raises  src/operator/multi_proposal.cc:35:10: fatal error: thrust/sort.h: No such file or directory   #include              ^~~~~~~~~~~~~~~  compilation terminated.  Makefile:392: recipe for target 'build/src/operator/multi_proposal.o' failed  make: *** [build/src/operator/multi_proposal.o] Error 1  make: *** Waiting for unfinished jobs....
"Compile the provided MXNet , success  but ,RuntimeError: Cannot find the MXNet library  List of candidates:  /home/znzx/xjp/SNIPER/SNIPER-mxnet/python/mxnet/libmxnet.so  /home/znzx/xjp/SNIPER/SNIPER-mxnet/python/mxnet/../../lib/libmxnet.so  /home/znzx/xjp/SNIPER/SNIPER-mxnet/python/mxnet/../../build/libmxnet.so  ../../../libmxnet.so  "
"error: â€˜class dmlc::Streamâ€™ has no member named â€˜WriteArrayâ€™     strm->WriteArray(tensor->shape, ndim);  error: â€˜DMLC_IO_NO_ENDIAN_SWAPâ€™ was not declared in this scope     if (DMLC_IO_NO_ENDIAN_SWAP &&  error: â€˜ByteSwapâ€™ is not a member of â€˜dmlcâ€™         dmlc::ByteSwap(dmlc::BeginPtr(bytes), type_bytes, num_elems);   error: â€˜class dmlc::Streamâ€™ has no member named â€˜ReadArrayâ€™       CHECK(strm->ReadArray(&shape[0], ndim))  ..."
"> Initializing the model...  > Traceback (most recent call last):  >   File ""main_train.py"", line 90, in    >     sym_inst = eval('{}.{}'.format(config.symbol, config.symbol))(n_proposals=500, momentum=args.momentum)  >   File "" "", line 1, in    > NameError: name 'resnet_mx_50_e2e' is not defined  > configs/faster/default_configs.py:171: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read   for full details.  >   exp_config = edict(yaml.load(f))  > Extracting proposals on train2014 for negative chip mining...  > ./SNIPER/configs/faster/default_configs.py:171: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read   for full details.  >   exp_config = edict(yaml.load(f))  > loading annotations into memory...  > Done (t=2.66s)  > creating index...  > index created!  >     Current version is seem like not fit with resnet-50, would you like to provide one?"
"Why use thread_pool in processing image but use multiprocessing.pool in process labels?  I tried both use multiprocessing.pool ,that would be slower  ,and i don't know whether it is safe..."
"I tried training the SNIPER network on my own dataset and on the COCO dataset and encountered this error.     Traceback (most recent call last):    File ""main_train.py"", line 117, in        sym_inst.init_weight_rpn(config, arg_params, aux_params)    File ""/sniper/symbols/faster/resnet_mx_101_e2e_mask.py"", line 643, in init_weight_rpn      arg_params['stage4_unit1_offset_weight'] = mx.nd.zeros(shape=self.arg_shape_dict['stage4_unit1_offset_weight'])  KeyError: 'stage4_unit1_offset_weight'    So the arg_shape_dict doesn't contain the key 'stage4_unit1_offset_weight'. In fact the arg_params dict has 332 keys and arg_shape_dict has only 288 keys which I'm assuming shouldn't be the case.  "
"i train SNIPER model with Negative Chip Mining .     what's num of recommend cycle ? just 2 is ok ? when set to 4,  is better than 2 ?  "
"  # Number of images per GPU    BATCH_IMAGES: 8  # Number of images per gpu for each scale    BATCH_IMAGES:    - 2    - 2    - 4  !     there're 6500 images in the train dataset. Log shows that It'd been trained over 6,700 batches, but still not an epoch.  And there is only a log file in the output, still not any weights-file.  (And strangely, the log has not been updated for an hour.)    !   "
"after   in /usr/bin/python (and I ban anaconda already), demo.py is ok, results show good.  !       but while I try to train my coco-style dataset. It comes out:    > xx/SNIPER$ python main_train.py   > Traceback (most recent call last):  >   File ""main_train.py"", line 24, in    >     from data_utils.load_data import load_proposal_roidb, merge_roidb, filter_roidb  >   File ""lib/data_utils/load_data.py"", line 11, in    >     from dataset import *  >   File ""lib/dataset/__init__.py"", line 3, in    >     from coco import coco  >   File ""lib/dataset/coco.py"", line 14, in    >     from .pycocotools.coco import COCO  >   File ""lib/dataset/pycocotools/coco.py"", line 54, in    >     from skimage.draw import polygon  >   File ""/usr/local/lib/python2.7/dist-packages/skimage/__init__.py"", line 167, in    >     from .util.dtype import (img_as_float32,  >   File ""/usr/local/lib/python2.7/dist-packages/skimage/util/__init__.py"", line 6, in    >     from .apply_parallel import apply_parallel  >   File ""/usr/local/lib/python2.7/dist-packages/skimage/util/apply_parallel.py"", line 8, in    >     import dask.array as da  >   File ""/usr/local/lib/python2.7/dist-packages/dask/array/__init__.py"", line 5, in    >     from .core import (Array, block, concatenate, stack, from_array, store,  >   File ""/usr/local/lib/python2.7/dist-packages/dask/array/core.py"", line 29, in    >     from . import chunk  >   File ""/usr/local/lib/python2.7/dist-packages/dask/array/chunk.py"", line 73, in    >     nancumprod = npcompat.nancumprod  > AttributeError: 'module' object has no attribute 'nancumprod'  >     I had have numpy==1.16.4, is it too high? I downgrade it to 1.13.3(min version for scipy 1.3.0), it still comes out.    !   "
"**python main_train.py --set TRAIN.USE_NEG_CHIPS False**    /home/xu/new/SNIPER/configs/faster/default_configs.py:171: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read   for full details.    exp_config = edict(yaml.load(f))  loading annotations into memory...  Done (t=0.04s)  creating index...  index created!  num_images 9626  COCO_train2014 gt roidb loaded from /home/xu/new/SNIPER/data/cache/COCO_train2014_gt_roidb.pkl  append flipped images to roidb  loading annotations into memory...  Done (t=0.01s)  creating index...  index created!  num_images 1571  COCO_val2014 gt roidb loaded from /home/xu/new/SNIPER/data/cache/COCO_val2014_gt_roidb.pkl  append flipped images to roidb  filtered 0 roidb entries: 22394 -> 22394  add bounding box regression targets  bbox target means:   +0x5b) [0x7f31a0b9f92b]  [bt] (1) /home/xu/new/SNIPER/SNIPER-mxnet/python/mxnet/../../lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x28) [0x7f31a0ba0498]  [bt] (2) /home/xu/new/SNIPER/SNIPER-mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::storage::GPUPooledStorageManager::Alloc(mxnet::Storage::Handle*)+0x154) [0x7f31a35c5c14]  [bt] (3) /home/xu/new/SNIPER/SNIPER-mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::StorageImpl::Alloc(mxnet::Storage::Handle*)+0x5d) [0x7f31a35c7bdd]  [bt] (4) /home/xu/new/SNIPER/SNIPER-mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::NDArray::CheckAndAlloc() const+0x1b1) [0x7f31a0c854b1]  [bt] (5) /home/xu/new/SNIPER/SNIPER-mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::exec::StorageFallbackOpExecutor::PreFCompute(bool)+0x14b5) [0x7f31a3029725]  [bt] (6) /home/xu/new/SNIPER/SNIPER-mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::exec::FComputeExecutor::Run(mxnet::RunContext, bool)+0x19) [0x7f31a302a079]  [bt] (7) /home/xu/new/SNIPER/SNIPER-mxnet/python/mxnet/../../lib/libmxnet.so(+0x2fb0f26) [0x7f31a2ff1f26]  [bt] (8) /home/xu/new/SNIPER/SNIPER-mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::engine::ThreadedEngine::ExecuteOprBlock(mxnet::RunContext, mxnet::engine::OprBlock*)+0x8e5) [0x7f31a35a9c15]  [bt] (9) /home/xu/new/SNIPER/SNIPER-mxnet/python/mxnet/../../lib/libmxnet.so(void mxnet::engine::ThreadedEnginePerDevice::GPUWorker (mxnet::Context, bool, mxnet::engine::ThreadedEnginePerDevice::ThreadWorkerBlock *, std::shared_ptr  const&)+0xeb) [0x7f31a35c046b]    have this problemï¼Œhow can i solve itï¼Ÿ"
"/home/xu/anaconda3/envs/mxnet/bin/python3.5 /home/xu/SNIPER/main_train.py  Error in sitecustomize; set PYTHONVERBOSE for traceback:  NameError: name 'modules_list' is not defined  /home/xu/SNIPER/configs/faster/default_configs.py:171: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read   for full details.    exp_config = edict(yaml.load(f))  loading annotations into memory...  Done (t=0.04s)  creating index...  index created!  num_images 9626  COCO_train2014 gt roidb loaded from ./data/cache/COCO_train2014_gt_roidb.pkl  appending ground truth annotations  Reading cached proposals after ***NMS**** from data/proposals/COCO_train2014_rpn_after_nms.pkl  Traceback (most recent call last):    File ""/home/xu/SNIPER/main_train.py"", line 72, in        for image_set in image_sets]    File ""/home/xu/SNIPER/main_train.py"", line 72, in        for image_set in image_sets]    File ""/home/xu/SNIPER/lib/data_utils/load_data.py"", line 31, in load_proposal_roidb      roidb = eval('imdb.' + proposal + '_roidb')(roidb, append_gt,proposal_path=proposal_path)    File ""/home/xu/SNIPER/lib/dataset/imdb.py"", line 144, in rpn_roidb      rpn_roidb = self.load_rpn_roidb(gt_roidb,proposal_path)    File ""/home/xu/SNIPER/lib/dataset/imdb.py"", line 132, in load_rpn_roidb      return self.create_roidb_from_box_list(box_list, mapping_list, gt_roidb)    File ""/home/xu/SNIPER/lib/dataset/imdb.py"", line 158, in create_roidb_from_box_list      assert len(box_list) == self.num_images, 'number of boxes matrix must match number of images'  AssertionError: number of boxes matrix must match number of images  Done!    what problemï¼Ÿ who can help meï¼Ÿ"
"ximgproc -lopencv_calib3d -lopencv_features2d -lopencv_highgui -lopencv_videoio -lopencv_flann -lopencv_xobjdetect -lopencv_imgcodecs -lopencv_objdetect -lopencv_xphoto -lopencv_imgproc -lopencv_core -llapack -lcudnn  -lcufft -lcuda -lnvrtc -L/usr/local/cuda/lib64/stubs  /usr/bin/ld: warning: libjpeg.so.9, needed by /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.so, not found (try using -rpath or -rpath-link)  /usr/bin/ld: warning: libpng16.so.16, needed by /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.so, not found (try using -rpath or -rpath-link)  /usr/bin/ld: warning: libjasper.so.4, needed by /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.so, not found (try using -rpath or -rpath-link)  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jas_image_writecmptâ€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_set_interlace_handling@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_set_IHDR@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_get_io_ptr@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jpeg_set_quality@LIBJPEG_9.0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jpeg_quality_scaling@LIBJPEG_9.0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jas_image_encodeâ€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jpeg_resync_to_restart@LIBJPEG_9.0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jas_image_decodeâ€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jas_cleanupâ€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_set_longjmp_fn@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jpeg_set_defaults@LIBJPEG_9.0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_set_gray_to_rgb@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jas_image_strtofmtâ€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_set_bgr@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jpeg_stdio_src@LIBJPEG_9.0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jpeg_finish_compress@LIBJPEG_9.0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jas_image_createâ€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_set_rgb_to_gray@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jpeg_calc_output_dimensions@LIBJPEG_9.0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jpeg_std_error@LIBJPEG_9.0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_set_swap@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_destroy_read_struct@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jas_initâ€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_set_palette_to_rgb@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_get_tRNS@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jpeg_write_scanlines@LIBJPEG_9.0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jpeg_simple_progression@LIBJPEG_9.0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jpeg_read_header@LIBJPEG_9.0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jas_cmprof_createfromclrspcâ€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jas_stream_fopenâ€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jpeg_start_compress@LIBJPEG_9.0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jpeg_destroy_compress@LIBJPEG_9.0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_set_packing@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_read_end@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_set_read_fn@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_write_end@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jas_matrix_destroyâ€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_set_expand_gray_1_2_4_to_8@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_set_write_fn@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jas_image_destroyâ€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_create_write_struct@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_error@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_destroy_write_struct@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jpeg_stdio_dest@LIBJPEG_9.0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_set_strip_16@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jas_stream_closeâ€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_create_read_struct@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jpeg_finish_decompress@LIBJPEG_9.0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jas_image_readcmptâ€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jpeg_destroy_decompress@LIBJPEG_9.0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_set_tRNS_to_alpha@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jpeg_CreateDecompress@LIBJPEG_9.0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jpeg_alloc_huff_table@LIBJPEG_9.0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_set_compression_level@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jpeg_CreateCompress@LIBJPEG_9.0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_set_filter@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jpeg_default_qtables@LIBJPEG_9.0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_init_io@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jas_image_getcmptbytypeâ€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jas_cmprof_destroyâ€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_get_IHDR@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jpeg_start_decompress@LIBJPEG_9.0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_set_compression_strategy@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_write_info@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jas_matrix_createâ€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jpeg_read_scanlines@LIBJPEG_9.0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_create_info_struct@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_read_update_info@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_write_image@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_read_image@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_read_info@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜jas_image_chclrspcâ€™æœªå®šä¹‰çš„å¼•ç”¨  /home/xu/anaconda3/envs/mxnet/lib/libopencv_imgcodecs.soï¼šå¯¹â€˜png_set_strip_alpha@PNG16_0â€™æœªå®šä¹‰çš„å¼•ç”¨  collect2: error: ld returned 1 exit status  Makefile:456: recipe for target 'bin/im2rec' failed  make: *** [bin/im2rec] Error 1    compile mxnet have problrmï¼Œhow can i doï¼Ÿ"
"cd to SNIPER-mxnetï¼Œi usemake -j 8 USE_CUDA_PATH=/user/local/cuda-9.0 but have some problemã€‚ï¼ˆcudnã€cudnnã€OpenBLAS are installedï¼‰ï¼Œbut opencv use pip  to install in the virtal environment ï¼ˆopencv-pythonï¼‰ï¼Œand mxnet not installï¼Œwhat should i do next ï¼Ÿplease ï¼Œfresh manã€‚    Makefile:166: ""USE_LAPACK disabled because libraries were not found""  Makefile:240: WARNING: Significant performance increases can be achieved by installing and enabling gperftools or jemalloc development packages  Running CUDA_ARCH: -gencode arch=compute_30,code=sm_30 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=[sm_70,compute_70] --fatbin-options -compress-all  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/user/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=1 -I/usr/include/opencv -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1  -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/mkldnn/mkldnn_convolution.cc -o build/src/operator/nn/mkldnn/mkldnn_convolution.o  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/user/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=1 -I/usr/include/opencv -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1  -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/mkldnn/mkldnn_concat.cc -o build/src/operator/nn/mkldnn/mkldnn_concat.o  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/user/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=1 -I/usr/include/opencv -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1  -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/mkldnn/mkldnn_act.cc -o build/src/operator/nn/mkldnn/mkldnn_act.o  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/user/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=1 -I/usr/include/opencv -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1  -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/mkldnn/mkldnn_softmax.cc -o build/src/operator/nn/mkldnn/mkldnn_softmax.o  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/user/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=1 -I/usr/include/opencv -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1  -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/mkldnn/mkldnn_copy.cc -o build/src/operator/nn/mkldnn/mkldnn_copy.o  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/user/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=1 -I/usr/include/opencv -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1  -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/mkldnn/mkldnn_fully_connected.cc -o build/src/operator/nn/mkldnn/mkldnn_fully_connected.o  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/user/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=1 -I/usr/include/opencv -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1  -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/cudnn/cudnn_algoreg.cc -o build/src/operator/nn/cudnn/cudnn_algoreg.o  g++ -std=c++11 -c -DMSHADOW_FORCE_STREAM -Wall -Wsign-compare -O3 -DNDEBUG=1 -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/mshadow/ -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/dmlc-core/include -fPIC -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/nnvm/include -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/dlpack/include -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/nnvm/tvm/include -Iinclude -funroll-loops -Wno-unused-parameter -Wno-unknown-pragmas -Wno-unused-local-typedefs -msse3 -mf16c -I/user/local/cuda/include -DMSHADOW_USE_CBLAS=1 -DMSHADOW_USE_MKL=0 -DMSHADOW_RABIT_PS=0 -DMSHADOW_DIST_PS=0 -DMSHADOW_USE_PASCAL=0 -DMXNET_USE_OPENCV=1 -I/usr/include/opencv -fopenmp -DMXNET_USE_OPERATOR_TUNING=1 -DMSHADOW_USE_CUDNN=1  -I/home/xu/SNIPER/SNIPER-mxnet/3rdparty/cub -DMXNET_ENABLE_CUDA_RTC=1 -DMXNET_USE_NCCL=0 -DMXNET_USE_LIBJPEG_TURBO=0 -MMD -c src/operator/nn/cudnn/cudnn_batch_norm.cc -o build/src/operator/nn/cudnn/cudnn_batch_norm.o  In file included from /home/xu/SNIPER/SNIPER-mxnet/3rdparty/mshadow/mshadow/tensor.h:16:0,                   from include/mxnet/./base.h:32,                   from include/mxnet/operator.h:38,                   from src/operator/nn/mkldnn/mkldnn_act.cc:28:  /home/xu/SNIPER/SNIPER-mxnet/3rdparty/mshadow/mshadow/./base.h:155:23: fatal error: cblas.h: æ²¡æœ‰é‚£ä¸ªæ–‡ä»¶æˆ–ç›®å½•  compilation terminated.  Makefile:392: recipe for target 'build/src/operator/nn/mkldnn/mkldnn_act.o' failed  make: *** [build/src/operator/nn/mkldnn/mkldnn_act.o] Error 1  make: *** æ­£åœ¨ç­‰å¾…æœªå®Œæˆçš„ä»»åŠ¡....  In file included from /home/xu/SNIPER/SNIPER-mxnet/3rdparty/mshadow/mshadow/tensor.h:16:0,                   from include/mxnet/./base.h:32,                   from include/mxnet/io.h:34,                   from src/operator/nn/mkldnn/../convolution-inl.h:30,                   from src/operator/nn/mkldnn/mkldnn_convolution.cc:26:  /home/xu/SNIPER/SNIPER-mxnet/3rdparty/mshadow/mshadow/./base.h:155:23: fatal error: cblas.h: æ²¡æœ‰é‚£ä¸ªæ–‡ä»¶æˆ–ç›®å½•  compilation terminated.  In file included from /home/xu/SNIPER/SNIPER-mxnet/3rdparty/mshadow/mshadow/tensor.h:16:0,                   from include/mxnet/./base.h:32,                   from include/mxnet/operator.h:38,                   from src/operator/nn/cudnn/./../batch_norm-inl.h:31,                   from src/operator/nn/cudnn/./cudnn_batch_norm-inl.h:33,                   from src/operator/nn/cudnn/cudnn_batch_norm.cc:27:  /home/xu/SNIPER/SNIPER-mxnet/3rdparty/mshadow/mshadow/./base.h:155:23: fatal error: cblas.h: æ²¡æœ‰é‚£ä¸ªæ–‡ä»¶æˆ–ç›®å½•  compilation terminated.  Makefile:392: recipe for target 'build/src/operator/nn/mkldnn/mkldnn_convolution.o' failed  make: *** [build/src/operator/nn/mkldnn/mkldnn_convolution.o] Error 1  Makefile:392: recipe for target 'build/src/operator/nn/cudnn/cudnn_batch_norm.o' failed  make: *** [build/src/operator/nn/cudnn/cudnn_batch_norm.o] Error 1  In file included from /home/xu/SNIPER/SNIPER-mxnet/3rdparty/mshadow/mshadow/tensor.h:16:0,                   from include/mxnet/./base.h:32,                   from include/mxnet/operator.h:38,                   from src/operator/nn/mkldnn/../fully_connected-inl.h:30,                   from src/operator/nn/mkldnn/mkldnn_fully_connected.cc:26:  /home/xu/SNIPER/SNIPER-mxnet/3rdparty/mshadow/mshadow/./base.h:155:23: fatal error: cblas.h: æ²¡æœ‰é‚£ä¸ªæ–‡ä»¶æˆ–ç›®å½•  compilation terminated.  Makefile:392: recipe for target 'build/src/operator/nn/mkldnn/mkldnn_fully_connected.o' failed  make: *** [build/src/operator/nn/mkldnn/mkldnn_fully_connected.o] Error 1  In file included from /home/xu/SNIPER/SNIPER-mxnet/3rdparty/mshadow/mshadow/tensor.h:16:0,                   from include/mxnet/./base.h:32,                   from include/mxnet/operator.h:38,                   from src/operator/nn/mkldnn/../concat-inl.h:30,                   from src/operator/nn/mkldnn/mkldnn_concat.cc:25:  /home/xu/SNIPER/SNIPER-mxnet/3rdparty/mshadow/mshadow/./base.h:155:23: fatal error: cblas.h: æ²¡æœ‰é‚£ä¸ªæ–‡ä»¶æˆ–ç›®å½•  compilation terminated.  In file included from /home/xu/SNIPER/SNIPER-mxnet/3rdparty/mshadow/mshadow/tensor.h:16:0,                   from include/mxnet/base.h:32,                   from src/operator/nn/mkldnn/../../mxnet_op.h:30,                   from src/operator/nn/mkldnn/../softmax-inl.h:30,                   from src/operator/nn/mkldnn/mkldnn_softmax.cc:26:  /home/xu/SNIPER/SNIPER-mxnet/3rdparty/mshadow/mshadow/./base.h:155:23: fatal error: cblas.h: æ²¡æœ‰é‚£ä¸ªæ–‡ä»¶æˆ–ç›®å½•  compilation terminated.  In file included from /home/xu/SNIPER/SNIPER-mxnet/3rdparty/mshadow/mshadow/tensor.h:16:0,                   from include/mxnet/base.h:32,                   from src/operator/nn/mkldnn/../../mxnet_op.h:30,                   from src/operator/nn/mkldnn/../softmax-inl.h:30,                   from src/operator/nn/mkldnn/mkldnn_copy.cc:26:  /home/xu/SNIPER/SNIPER-mxnet/3rdparty/mshadow/mshadow/./base.h:155:23: fatal error: cblas.h: æ²¡æœ‰é‚£ä¸ªæ–‡ä»¶æˆ–ç›®å½•  compilation terminated.  Makefile:392: recipe for target 'build/src/operator/nn/mkldnn/mkldnn_concat.o' failed  make: *** [build/src/operator/nn/mkldnn/mkldnn_concat.o] Error 1  Makefile:392: recipe for target 'build/src/operator/nn/mkldnn/mkldnn_softmax.o' failed  make: *** [build/src/operator/nn/mkldnn/mkldnn_softmax.o] Error 1  Makefile:392: recipe for target 'build/src/operator/nn/mkldnn/mkldnn_copy.o' failed  make: *** [build/src/operator/nn/mkldnn/mkldnn_copy.o] Error 1  In file included from src/operator/nn/cudnn/./../../../common/cuda_utils.h:31:0,                   from src/operator/nn/cudnn/./cudnn_algoreg-inl.h:33,                   from src/operator/nn/cudnn/cudnn_algoreg.cc:26:  /home/xu/SNIPER/SNIPER-mxnet/3rdparty/mshadow/mshadow/base.h:155:23: fatal error: cblas.h: æ²¡æœ‰é‚£ä¸ªæ–‡ä»¶æˆ–ç›®å½•  compilation terminated.  Makefile:392: recipe for target 'build/src/operator/nn/cudnn/cudnn_algoreg.o' failed  make: *** [build/src/operator/nn/cudnn/cudnn_algoreg.o] Error 1  "
"Hi   I'm hoping to get some clarity on what happens in the function `worker()` in `data_workers.py`.   I trained successfully with my own data set, and then decided to reduce the size of the physical images and train again, and since I started with the smaller images I keep getting the ""Image Resize Failed!"" error message, and I can't understand what's going wrong.     Initially, I thought that the crop size was bigger than the image (being set to 512 by default), so I reduced the crop size, but it didn't help.    Next I attempted to remove the crop altogether since there is an `else` clause to handle that. This is where things got weird. There is a comment which says "" Compute scale based on config"" and that fails as well.    note that in the if statement `max_size` is a list with 2 terms, but in the else `max_size` is `data[1]` which is a totally different data structure of the form   `[array([1963.584,  467.712, 4027.968, 2528.064]), 0.24801587301587302, 512, 512, 0]`  which means that  `rim = np.zeros((3, max_size[0], max_size[1]), dtype=np.float32)` will fail.    What is intended here? They are not the image dimensions, nor bounding box coordinates?                     "
"I see that in the paper and in the readme the mAP score is published for the pascal and coco datasets, but I can't see how to generate the scores for my own dataset."
"hello,  When I enter "" make -j 16 USE_CUDA_PATH=/usr/local/cuda"", but I got some error.    /usr/bin/ld: warning: libcudart.so.8.0, needed by /usr/local/opencv-3.2/lib/libopencv_core.so, not found (try using -rpath or -rpath-link)  but I can find libopencv_core.so in /usr/local/opencv-3.2/lib    Can you explain why I got this error? Thank you very much."
"hello,    When I enter "" make -j 16 USE_CUDA_PATH=/usr/local/cuda"", but I got some error.    src/nnvm/tvm_bridge.cc:37:37: fatal error: tvm/runtime/packed_func.h: No such file or directory  compilation terminated.  Makefile:392: recipe for target 'build/src/nnvm/tvm_bridge.o' failed  make: *** [build/src/nnvm/tvm_bridge.o] Error 1  make: *** Waiting for unfinished jobs....    Can you explain why I got this error? Thank you very much."
"Hey all,    I currently fail to run training for my own dataset. I used the `train_neg_props_and_sniper.sh` script which works fine until the actual training starts. I always get `OSError: [Errno 12] Cannot allocate memory`, even though I tried to reduce `NUM_PROCESS` and `NUM_THREAD` to 2 as well as increasing `CHIPS_DB_PARTS` to 10000 and decreasing to 2. I'm not sure where else I can reduce memory consumption.     Setup:  32 GB Ram  2x Geforce 1080 Ti    Dataset:  21510 images in 1280x720 with 187 classes    Config file:       Error log:       I would be glad for any suggestion."
"As far as I can tell I followed the instructions perfectly. `make -j 8 USE_CUDA_PATH=/usr/local/cuda-8.0/` completed successfully, I compiled the bash scripts and installed the requirements.txt. When I try the demo.py or main_train.py I get the the following stacktrace:   `Traceback (most recent call last):    File ""main_train.py"", line 14, in        from iterators.MNIteratorE2E import MNIteratorE2E    File ""lib/iterators/MNIteratorE2E.py"", line 9, in        import mxnet as mx    File ""SNIPER-mxnet/python/mxnet/__init__.py"", line 25, in        from . import engine    File ""SNIPER-mxnet/python/mxnet/engine.py"", line 23, in        from .base import _LIB, check_call    File ""SNIPER-mxnet/python/mxnet/base.py"", line 113, in        _LIB = _load_lib()    File ""SNIPER-mxnet/python/mxnet/base.py"", line 105, in _load_lib      lib = ctypes.CDLL(lib_path  and I ran `make clean_all` from the SNIPER-mxnet/ dir, but it didn't solve the issue. Also I ran objdump -TC /usr/local/cuda-8.0/libmxnet.so and in order to search manually for the symbol and found it in the output as expected.  "
None
"Hello, @mahyarnajibi,   I am new to mxnet and thanks for this wonderful SNIPER.    I have some issue when I compile the SNIPER code.    I successfully compiled SNIPER-mxnet,  (Ubuntu18.04, CUDA10.0, Python2.7, g++/gcc 5.5.0)  But, when I compile: bash scripts/compile.sh  I got the following error msg:        Compiling NMS module...      running build_ext      skipping 'cpu_nms.c' Cython extension (up-to-date)      building 'cpu_nms' extension      p++6 -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/test/anaconda3/envs/SNIPER-py27/lib/python2.7/site-packages/numpy/core/include -I/home/test/anaconda3/envs/SNIPER-py27/include/python2.7 -c cpu_nms.c -o build/temp.linux-x86_64-2.7/cpu_nms.o -Wno-cpp -Wno-unused-function      unable to execute 'p++6': No such file or directory      error: command 'p++6' failed with exit status 1      Compiling bbox module...      running build_ext      skipping 'bbox.c' Cython extension (up-to-date)      building 'bbox' extension      p++6 -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/test/anaconda3/envs/SNIPER-py27/lib/python2.7/site-packages/numpy/core/include -I/home/test/anaconda3/envs/SNIPER-py27/include/python2.7 -c bbox.c -o build/temp.linux-x86_64-2.7/bbox.o -Wno-cpp -Wno-unused-function      unable to execute 'p++6': No such file or directory      error: command 'p++6' failed with exit status 1      Compiling chips module...      running build_ext      skipping 'chips.cpp' Cython extension (up-to-date)      building 'chips' extension      p++6 -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/test/anaconda3/envs/SNIPER-py27/lib/python2.7/site-packages/numpy/core/include -I/home/test/anaconda3/envs/SNIPER-py27/include/python2.7 -c chips.cpp -o build/temp.linux-x86_64-2.7/chips.o      unable to execute 'p++6': No such file or directory      error: command 'p++6' failed with exit status 1      Compiling coco api...      running build_ext      building '_mask' extension      p++6 -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/test/anaconda3/envs/SNIPER-py27/lib/python2.7/site-packages/numpy/core/include -I/home/test/anaconda3/envs/SNIPER-py27/include/python2.7 -c _mask.c -o build/temp.linux-x86_64-2.7/_mask.o -Wno-cpp -Wno-unused-function -std=c99      unable to execute 'p++6': No such file or directory      error: command 'p++6' failed with exit status 1      All Done!    why it says that ""p++6""?  I grep this keyword in all files, but never found it.  Or am I doing some stupid mistakes?  kindly help me, thanks."
"HI. Thanks for this wonderful SNIPER.    I wanted to train SNIPER using my own dataset and to do that I first had to do the installation following the guide you've mentioned.  make -j 8 USE_CUDA_PATH=/usr/local/cuda  but it says that :  !     1. configuration:  Ubuntu 18.0.4  CUDA 10.0 with associated cUDNN    2. I'm a mxnet novice. I had install openBLAS, mxnet by using the following command:  sudo apt-get install libopenblas-dev  pip install mxnet-cu100      Kindly help me..  I am stuck on this for very long time."
"Hi, i have successfully trained and tested SNIPER on my own dataset. However, after running  , I cannot seem to find the 'Summary Metrics' results (similar to the one shown below, taken from issue #77) in any of the output files. It also does not get printed in the terminal. Does   not automatically produce this? If not, how can I get it? Thanks.            Edit: resolved the issue by renaming my test file. originally it had the word 'test' in it but there is an   statement somewhere in the   code that bypasses this kind of evaluation if the input is a test set. Solved by replacing the 'test' with 'val' in my annotations filename."
"Hi team,     I was finally able to successfully run the first section (training the proposal network) of `train_neg_props_and_sniper.sh`. However, my code runs to an error during extraction of proposals for negative mining on the training set.     Specifically, my error code is as follows:    `Done! Saving proposals into: output/sniper_neg_proposals/props_scale_1400x2000/proposals.pkl  Performing inference for scale: (800, 1280)  aaa  Performing inference for scale: (800, 1280)  aaa  Traceback (most recent call last):    File ""main_test.py"", line 66, in        main()    File ""main_test.py"", line 61, in main      imdb_proposal_extraction_wrapper(sym_inst, config, imdb, roidb, context, arg_params, aux_params, args.vis)    File ""lib/inference.py"", line 441, in imdb_proposal_extraction_wrapper      proposal_list = pool.map(proposal_scale_worker, parallel_args)    File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 251, in map      return self.map_async(func, iterable, chunksize).get()    File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 567, in get      raise self._value  mxnet.base.MXNetError: [01:50:55] src/operator/nn/./cudnn/cudnn_convolution-inl.h:744: Failed to find any forward convolution algorithm.  `    Btw, I am using CUDA 9.0. Can anyone help me understand why this happens and how to resolve it? Thank you.    Edit: It seems to be coming from a memory error. Tried reducing the batch size and it worked."
"Hi, I'm trying to train SNIPER on my own dataset. I first ran bash train_neg_props_and_sniper.sh --cfg [PATH_TO_CFG_FILE] as per the instructions in the readme because I was hoping to retrain the proposal network first. However, I run into this error:    AssertionError: Path does not exist: ./data/myowndataset/images/train/COCO_train_000000xxxxxx.jpg    But I am not using coco images but instead my own dataset. Currently, my files are named like this: train_000xxx.jpg. Does this mean I have to rename my files in the the format COCO_train_000000xxxxxx.jpg? Is there any way to let the program automatically accept whatever filename I use?  "
" I build mxnet from the cvprk3k branch of this repo with the following command: make  -j 8 USE_CUDA_PATH=/usr/local/cuda. I have OpenBlas, CUDA, and CUDNN already installed.    /usr/local/lib/libopenblas.so: undefined reference to `_gfortran_concat_string'  /usr/local/liblibopenblas.so: undefined reference to `_gfortran_etime'  collect2: error: ld returned 1 exit status  Makefile:454: recipe for target 'bin/im2rec' failed  make: *** [bin/im2rec] Error 1  make: *** Waiting for unfinished jobs....    Edit 1:    It seems that I have successfully built the code by adding -lgfortran in ADD_LFLAGS in config.mk. However, when I start running python demo.py, I get the error ImportError: No module named mxnet.    Can somebody help with this? Thank you.  "
I've read that main_test.py can be used to do inference on multiple images but it seems that you must provide annotations for those images as it looks for instances_{testfolder}.json. Perhaps I'm mistaken but how do you go about doing inference on images without annotations?
The possible reason is that the optimizer parameter changes are only for begin_epoch = 0 .
"     First, thanks for sharing your impressive work. I want to know why the grad_scale in bbox_loss is divided by (188 * 16). What does the meaning of number?"
"I want to train the mbv2,so i change the parament cfg = 'configs/faster/sniper_mobilenetv2_e2e.yml',but it has no attribute: get_symbol_rpn!"
"weiliu@server:~/pycode/SNIPER$ python demo.py   Traceback (most recent call last):    File ""demo.py"", line 11, in        import mxnet as mx    File ""SNIPER-mxnet/python/mxnet/__init__.py"", line 25, in        from . import engine    File ""SNIPER-mxnet/python/mxnet/engine.py"", line 23, in        from .base import _LIB, check_call    File ""SNIPER-mxnet/python/mxnet/base.py"", line 113, in        _LIB = _load_lib()    File ""SNIPER-mxnet/python/mxnet/base.py"", line 104, in _load_lib      lib_path = libinfo.find_lib_path()    File ""SNIPER-mxnet/python/mxnet/libinfo.py"", line 71, in find_lib_path      'List of candidates:\n' + str('\n'.join(dll_path)))  RuntimeError: Cannot find the MXNet library.  List of candidates:  /usr/local/cuda-9.0/lib64/libmxnet.so  /usr/local/cuda-9.0/lib64/libmxnet.so  /home/weiliu/pycode/SNIPER/SNIPER-mxnet/python/mxnet/libmxnet.so  /home/weiliu/pycode/SNIPER/SNIPER-mxnet/python/mxnet/../../lib/libmxnet.so  /home/weiliu/pycode/SNIPER/SNIPER-mxnet/python/mxnet/../../build/libmxnet.so  ../../../libmxnet.so  "
None
"Extract script download model, prompt extract file failure, can be sent to Google or baidu cloud disk, thank youï¼ï¼  `gzip: stdin: invalid compressed data--format violated  tar: å½’æ¡£æ–‡ä»¶ä¸­å¼‚å¸¸çš„ EOF  tar: å½’æ¡£æ–‡ä»¶ä¸­å¼‚å¸¸çš„ EOF  tar: Error is not recoverable: exiting now  Cleaning up...  All done!  `"
"Excuse me, I have tried many times, but I cannot compile MXNet from you.  Faults as follows:  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dgemoveT'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_cgemv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zgemoveT'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_strscal'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zhemv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ctrmm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_sscal'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_strmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_scasum'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dtrmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ssqtrans'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_drotg'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zsqtrans'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_cmoveConj'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_sgeset'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dgbmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dspr2'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dsbmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ctrmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dsymv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_strmm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dtpmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ssyreflect'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zhbmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_izamax'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_srot'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zger2u'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ztpmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_sgemoveT'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_cGetNB'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ctrscal'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_srotg'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_scopy'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ztpsv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_cger2u'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dgemv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zgemaxnrm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_cgemaxnrm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dscal'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_cgezero'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_cher2k'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_cscalConj'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_strsv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zsymm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zhpr'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zger2c'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dgezero'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dgemm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_csymm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ctrsm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_cger2c'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_sspr'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_cher2'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_stbmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dgemaxnrm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_cherk'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_icamax'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_drotmg'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zhpr2'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_cher'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dspr'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dtbmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_stpmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zsyr2k'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_sasum'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_sswap'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dsyrk'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_srotmg'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ccopyConj'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_srotm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_csqtrans'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ccplxdivide'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_drotm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ssyr2k'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zherk'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_cswap'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ccplxinvert'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_chemm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dger'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_cdotc_sub'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dsyr2'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dger2'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zcplxinvert'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_sger2'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_cgeru'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ssymv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_chemv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zGetNB'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ztrmm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_sgescal'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zaxpy'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dtrscal'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_stbsv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dtrsm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dtbsv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zhemm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ssymm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dset'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zgemv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dtrsv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ctbmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zgeset'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zgbmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_sger'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_sgezero'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_sdsdot'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zher2k'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ssbmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_sspr2'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_chbmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dznrm2'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dsymm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_sgemm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_chpr2'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_sgemaxnrm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dsdot'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_daxpy'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_sgbmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zgerc'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_strsm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_sgemv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dswap'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zscalConj'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zher'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_saxpby'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_sGetNB'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_cdotu_sub'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_csrot'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_cgbmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ztrsv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_saxpy'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_chpr'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zgeru'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zrotg'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_cgemoveT'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zcplxdivide'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dsyr2k'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zset'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dtrmm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_xerbla'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zsyrk'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ztrsm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_stpsv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dzasum'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dsyr'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zmoveConj'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ddot'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ctpmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ztbsv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zdotu_sub'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ctrsv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dGetNB'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dsyreflect'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ztbmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_csyr2k'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ctpsv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zcopy'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_idamax'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zgezero'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_cgeset'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dtpsv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dgeset'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dcopy'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zaxpby'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_sdot'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_cgemm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zscal'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ssyrk'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_daxpby'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zsyreflect'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_isamax'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dasum'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_chpmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dspmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ssyr2'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_caxpby'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_sset'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zdrot'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_csyreflect'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zher2'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_cset'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_drot'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_csyrk'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_scnrm2'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zgemm'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_snrm2'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zswap'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dgescal'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_crotg'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ccopy'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zdotc_sub'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zhpmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_caxpy'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dnrm2'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_zcopyConj'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_cscal'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_cgerc'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ztrscal'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_sspmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ctbsv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ztrmv'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_dsqtrans'  /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/liblapack.so: undefined reference to `ATL_ssyr'  collect2: error: ld returned 1 exit status  can you help me?"
"Acconding to readme, I run git clone --recursive   i failed.  The error message is as follows:  Submodule path 'SNIPER-mxnet/3rdparty/ps-lite': checked out 'a6dda54604a07d1fb21b016ed1e3f4246b08222a'  Failed to recurse into submodule path 'SNIPER-mxnet/3rdparty/nnvm'  Failed to recurse into submodule path 'SNIPER-mxnet'    I try to run git submodule update --init --recursive, Still can't solve the problem.  The error message is as follows:  fatal: reference is not a tree: fdba6cc9bd3bec9ccd0592fa3900b7fe25d6cb97  Unable to checkout 'fdba6cc9bd3bec9ccd0592fa3900b7fe25d6cb97' in submodule path 'SNIPER-mxnet/3rdparty/nnvm/tvm'  Failed to recurse into submodule path 'SNIPER-mxnet/3rdparty/nnvm'  Failed to recurse into submodule path 'SNIPER-mxnet'    Thank you for your attention."
"Hi.Thanks for your sharing code.  I want to use your model to get features(ndarray type) of the image in each bounding box,but I don't know how to modify your code to get that after checking your code carefully.  I mean,I want to get outputs before the classify layer for the small image in each bounding box.  Could you give me some help?"
"[10:03:52] src/kvstore/././comm.h:628: only 0 out of 2 GPU pairs are enabled direct access. It may affect the perfoet MXNET_ENABLE_GPU_P2P=0 to turn it off  [10:03:52] src/kvstore/././comm.h:637: ..  [10:03:52] src/kvstore/././comm.h:637: ..  [10:03:52] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:107: Running performance tests to find the best convolutio can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)  Exception in thread Thread-15:  Traceback (most recent call last):    File ""/usr/lib/python2.7/threading.py"", line 810, in __bootstrap_inner      self.run()    File ""/usr/lib/python2.7/threading.py"", line 763, in run      self.__target(*self.__args, **self.__kwargs)    File ""lib/iterators/PrefetchingIter.py"", line 64, in prefetch_func      self.next_batch[i] = self.iters[i].next()    File ""lib/iterators/MNIteratorBase.py"", line 90, in next      if self.iter_next():    File ""lib/iterators/MNIteratorBase.py"", line 87, in iter_next      return self.get_batch()    File ""lib/iterators/MNIteratorE2E.py"", line 99, in get_batch      self.batch = self._get_batch()    File ""lib/iterators/MNIteratorE2E.py"", line 171, in _get_batch      all_labels = self.pool.map(self.anchor_worker.worker, worker_data)    File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 251, in map      return self.map_async(func, iterable, chunksize).get()    File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 558, in get      raise self._value  ValueError: could not broadcast input array from shape (101,5) into shape (100,5)  "
"Hi,there  can SNIPER be compiled on windows ?  I tried and failed, would you like to give some tips ?  thanks"
"Hi,I am training my model in openimages dataset. I see in your openimage config file that you used a pretrained model called CRCNN-1epoch.But I download your pretrain model with bash scripts/download_pretrained_model.sh ,but there is no file named CRCNN-0001.Can you tell me where is  this file to download."
"Hi,    I am currently trying to improve my performance for a certain dataset, and I am not using negative chips to train my algorithms. From the visualization it looks fine, however there are some false positives that I do want to avoid. I am pretty sure that is because of the fact that I don't do negative chip training.    Can you specify more on how to get the negative chips? From the paper it says:       And in the code, I found that there's a setting called `TEST.EXTRACT_PROPOSALS` - but reading thru the code, I am not sure if it is doing what's described in the paper.     Any pointers will be appreciated! Thanks!"
"Hi,In openimages.py ,i want to know the reason why you make gt_roidb's max_overlaps and max_classes only equal boxes' class but not include crowed' class."
"i test the coco pretrained model, the result is really very good, as openimage v4 has 600 classes, can this pretrained model can share?"
"Hi,    Which specific model did you use for Resnet 50? Can you provide the link to download the pre-trained resnet50 on imagenet? Looks like it's not available in the mxnet model zoo...    Thanks!"
!   !   
!   
None
"Hi, @mahyarnajibi ,, I am new to mxnet and when I run the command,:python demo.py.  it turns out:  mxnet.base.MXNetError:  +0x1bc) [0x7f8cdb156b5c]  [bt] (1) /usr/local/lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x28) [0x7f8cdb157f08]  [bt] (2) /usr/local/lib/libmxnet.so(void mxnet::op::BatchNormForwardImpl (mshadow::Stream *, mxnet::OpContext const&, mxnet::op::BatchNormParam const&, std::vector  > const&, std::vector  > const&, std::vector  > const&, std::vector  > const&)+0x31e) [0x7f8cdcd2a05e]  [bt] (3) /usr/local/lib/libmxnet.so(void mxnet::op::BatchNormForward (mxnet::OpContext const&, mxnet::op::BatchNormParam const&, std::vector  > const&, std::vector  > const&, std::vector  > const&, std::vector  > const&)+0x301) [0x7f8cdcd2a841]  [bt] (4) /usr/local/lib/libmxnet.so(void mxnet::op::BatchNormCompute (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector  > const&, std::vector  > const&, std::vector  > const&)+0x852) [0x7f8cdcd1fcc2]  [bt] (5) /usr/local/lib/libmxnet.so(mxnet::exec::FComputeExecutor::Run(mxnet::RunContext, bool)+0x50) [0x7f8cdb2a0660]  [bt] (6) /usr/local/lib/libmxnet.so(+0xc55ba8) [0x7f8cdb2a3ba8]  [bt] (7) /usr/local/lib/libmxnet.so(mxnet::engine::ThreadedEngine::ExecuteOprBlock(mxnet::RunContext, mxnet::engine::OprBlock*)+0x429) [0x7f8cdcb149c9]  [bt] (8) /usr/local/lib/libmxnet.so(void mxnet::engine::ThreadedEnginePerDevice::GPUWorker (mxnet::Context, bool, mxnet::engine::ThreadedEnginePerDevice::ThreadWorkerBlock *, std::shared_ptr  const&)+0xeb) [0x7f8cdcb18c4b]  [bt] (9) /usr/local/lib/libmxnet.so(std::_Function_handler ), mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)::{lambda()#3}::operator()() const::{lambda(std::shared_ptr )#1}>::_M_invoke(std::_Any_data const&, std::shared_ptr &&)+0x4e) [0x7f8cdcb18ece]      Would you please to help me?Thank you."
"Hi, I use SNIPER to train my own dataset. When training, I got nan loss as bellow,     It just occurs every several batches. My batch size is 4, and every 100 batches, it occurs. Other time everything just looks fine like bellow,     I trying to decrease `lr` to 0.0005, but not work."
"Hi, I have a quick question regarding model inference. Let's say I've trained a model using SNIPER, for a regular object detection task, can the model be served:    - using the official MXNet build?  - using CPU?    The reason is that for some reason, we need to inference the model on CPU. I guess it might be possible since the NN is still standard resnet + faster RCNN, but I am not sure about this (since I do see a few updates in the SNIPER-mxnet module which might be relevant to accomplish the goal above).    Thanks!"
"Hi, I got a problem which really confused me for several days, and I cannot solve. If you may help, that will be great.  The error message shows bellow:   "
"Hi, I am trying to clone the openimage branch by using command:    `git clone -b openimages2 --recursive      and I got a fatal error stating that I cannot read from SNIPER-mxnet remote module. The complete error message is as follow:    > Cloning into 'SNIPER'...  > remote: Counting objects: 1663, done.  > remote: Compressing objects: 100% (89/89), done.  > remote: Total 1663 (delta 45), reused 96 (delta 34), pack-reused 1540  > Receiving objects: 100% (1663/1663), 22.00 MiB | 9.45 MiB/s, done.  > Resolving deltas: 100% (1009/1009), done.  > Checking connectivity... done.  > Submodule 'SNIPER-mxnet' (git@github.com:mahyarnajibi/SNIPER-mxnet.git) registered for path 'SNIPER-mxnet'  > Cloning into 'SNIPER-mxnet'...  > The authenticity of host 'github.com (192.30.255.112)' can't be established.  > RSA key fingerprint is SHA256:nThbg********************************6E5SY8.  > Are you sure you want to continue connecting (yes/no)? yes  > Warning: Permanently added 'github.com,192.30.255.112' (RSA) to the list of known hosts.  > Permission denied (publickey).  > fatal: Could not read from remote repository.  >   > Please make sure you have the correct access rights  > and the repository exists.  > fatal: clone of 'git@github.com:mahyarnajibi/SNIPER-mxnet.git' into submodule path 'SNIPER-mxnet' failed  > "
"Hey,   Sniper is an awesome work. However ,FPN is a standard component  of object detection for scales with slightly computation increased. Have you tried to combine Sniper with FPN ? "
"when FLIP is set to true, running the repository against coco will produce some error like below:         And changing FLIP to false will eliminate the error. I suspect there's something wrong with the flipping logic (didn't dig deep though)."
What would be the best way to setup a python script to get a detection from a single image from a trained R-FCN-3k model? Thanks!
"hi, I've tried to run the 'demo.py' file, then met the error like:   mxnet.base.MXNetError: [22:56:26] /home/vis/xuxiaqing/code/code_my/video_detection/SNIPER/SNIPER-mxnet/src/operator/nn/batch_norm.cu:527: Check failed: err == cudaSuccess (7 vs. 0) Name: BatchNormalizationUpdateOutput ErrStr:too many resources requested for launch    My cuda version is 8.0, the cudnn version is 5.0.  Are there any tips for solving this problem? many thanks~      "
"when i train use main_train.py, appear the following question.    Optimizer params: {'wd': 0.01, 'lr_scheduler':  , 'multi_precision': True, 'learning_rate': 0.00015, 'rescale_grad': 1.0, 'clip_gradient': None, 'momentum': 0.9}  aaa  [00:33:49] /home/czn/scc/SNIPER/SNIPER-mxnet/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:107: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)  CUDA error: too many resources requested for launch"
"HI.      Thinks for provide this nice project.       When I compile this repo,I met some problems, and I solve it with compile OpenBLAS and lapcak,link lapack/liblapack.a to /usr/lib/liblapack.a. And then,i run the demo.py,But it not response after print two line 'aaa' and no any other message.Then i change the default config to fit my double 1080ti GPU.I just change the gpus from '0,1,2,3,4,5,6,7' to '0,1',and change the lr from '0.015' to '0.04'.Download pretrained_model and proposals,run the main_train.py script.But the process exhaust all my 32G memory and the computer not responsed.I don't know how to solve this problem.   Env: ubuntu 16.04.04 (kernel 4.4), CUDA9.0/cudnn 7.1.4, double Nvidia 1080 ti,python 2.7  Can you give me some abvise."
"Hi team,    Another question I have is how to avoid resizing the output image (i.e. output the same image resolution as the input)? One particular example would be for the image below (some random image on Internet), the input resolution is 1023 * 968, however the output resolution is 640 * 480, plus additional white edges. BTW I tried to modify the TEST.SCALES option in the yaml file to a large number but with no luck.    SCALES:    - !!python/tuple      output:  !   "
"Thanks for open sourcing the code, it looks great!    One question I have is how to train new proposals? Since I want to use SNIPER in a very different dataset, I assume the best way for me is to re-calculate the proposals? In that case, how do I re-train the proposal network?    Thanks!"
"Installation and all compilations went smoothly. However, when run ""python demo.py"", encountered following error:     Further inspection shows a simple test command ""import mxnet"" gives same error.    Env: ubuntu 16.04.04 (kernel 4.4), CUDA9.0/cudnn 7.0, Nvidia 1080 ti.    Tried another machine with ubuntu 16.04.04 (kernel 4.13) with different CPUs and builds, has exact same error.    Wondering anyone sees same error?    Thanks.    ps: same env runs several mxnet projects without any issue, including Deformable-ConvNet project."
"The first line of the README contains one typo: ""instance-level segmentataion"" instead of ""segmentation"""
"Hi, what is the cuda version? python version "
"/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.    from ._conv import register_converters as _register_converters  Traceback (most recent call last):    File ""demo.py"", line 123, in        main()    File ""demo.py"", line 87, in main      convert=True, process=True)    File ""lib/train_utils/utils.py"", line 90, in load_param      arg_params, aux_params = load_checkpoint(prefix, epoch)    File ""lib/train_utils/utils.py"", line 56, in load_checkpoint      save_dict = mx.nd.load('%s-%04d.params' % (prefix, epoch))    File ""/home/czn/scc/SNIPER/SNIPER-mxnet/python/mxnet/ndarray/utils.py"", line 175, in load      ctypes.byref(names)))    File ""/home/czn/scc/SNIPER/SNIPER-mxnet/python/mxnet/base.py"", line 149, in check_call      raise MXNetError(py_str(_LIB.MXGetLastError()))  mxnet.base.MXNetError:  +0x56) [0x7ff817364586]  [bt] (1) /home/czn/scc/SNIPER/SNIPER-mxnet/python/mxnet/../../build/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x28) [0x7ff817364d38]  [bt] (2) /home/czn/scc/SNIPER/SNIPER-mxnet/python/mxnet/../../build/libmxnet.so(mxnet::NDArray::Load(dmlc::Stream*, std::vector  >*, std::vector , std::allocator  >, std::allocator , std::allocator  > > >*)+0x204) [0x7ff817586e44]  [bt] (3) /home/czn/scc/SNIPER/SNIPER-mxnet/python/mxnet/../../build/libmxnet.so(MXNDArrayLoad+0xc1) [0x7ff81754c471]  [bt] (4) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7ff82fa2ee40]  [bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x2eb) [0x7ff82fa2e8ab]  [bt] (6) /usr/lib/python2.7/lib-dynload/_ctypes.x86_64-linux-gnu.so(_ctypes_callproc+0x48f) [0x7ff82fc3e3df]  [bt] (7) /usr/lib/python2.7/lib-dynload/_ctypes.x86_64-linux-gnu.so(+0x11d82) [0x7ff82fc42d82]  [bt] (8) python(PyEval_EvalFrameEx+0x578f) [0x4c15bf]  [bt] (9) python(PyEval_EvalCodeEx+0x306) [0x4b9ab6]  "
"Hi,    First of all, thanks for providing this nicely organized repo.  I have successfully run SNIPER demo without any problem.  However, for R-FCN 3K, I am not able to run the demo. There is strange error, which originates from asnumpy() function.   The error message I got is   SNIPER/CRCNN-mxnet/3rdparty/nnvm/include/nnvm/tuple.h:438: Check failed: dim == static_cast (ndim()) (2 vs. 1) dimension do not match target dimension 2 vs 1    I am using CUDA 9.1 with cuDNN 7.02 on a P3.x16 machine.    I have tried diffrent configures on the cmake, e.g., with/withou mkl, but I still cannot figure out what is the problem.    I can easily convert other NDArray to numpy array without any problem. I just cannot convert the output of the R-FCN network.    Please advise.    "
Thank You for sharing code.  fyi cannot perform git clone --recursive on SNIPER:    _fatal: repository '  not found  fatal: clone of '  into submodule path 'SNIPER-mxnet' failed_  
"Hi  @eladhoffer @itayhubara,   I see that in the quantize.py file, self.weight is left unchanged and qweight is only used to compute gradients. This results in the use of full precision weights for gradient update step (optimizer.step) which acts as error feedback and hence a less accuracy drop.   When I add self.weight=qweight to QConv2d class's forward function, I see an accuracy drop of 15-20% for ResNet20 on CIFAR10. Does that mean we need a copy of the full precision weights and full precision weight update step or am I missing something? Any help would be highly appreciated!    Thanks,  Aparna "
"Hi!   It seems to me as a cheat when preforming '+' operation at the end of residual block in quantized ResNet implementation. It requires 16 bit accumulator to get the outputs of sum, also input tensors ought to be quantized. What we get is that this op inputs are 32 bit (res input) and 16 bit (after last qconv) and the result is 32 bit, so, the accuracy doesn't fall at all. "
"if my input is torch.autograd.Variable, how to correct the code. I met the error:   File ""example/mpii.py"", line 352, in        main(parser.parse_args())    File ""example/mpii.py"", line 107, in main      train_loss, train_acc = train(train_loader, model, criterion, optimizer, args.debug, args.flip)    File ""example/mpii.py"", line 153, in train      output = model(input_var)    File ""/home/wangmeng/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py"", line 224, in __call__      result = self.forward(*input, **kwargs)    File ""/home/wangmeng/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py"", line 58, in forward      return self.module(*inputs[0], **kwargs[0])    File ""/home/wangmeng/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py"", line 224, in __call__      result = self.forward(*input, **kwargs)    File ""/home/wangmeng/pytorch-pose-quantized/pose/models/hourglass_quantized.py"", line 172, in forward      x = self.conv1(x)    File ""/home/wangmeng/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py"", line 224, in __call__      result = self.forward(*input, **kwargs)    File ""/home/wangmeng/pytorch-pose-quantized/pose/models/modules/quantize.py"", line 188, in forward      qinput = self.quantize_input(input)    File ""/home/wangmeng/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py"", line 224, in __call__      result = self.forward(*input, **kwargs)    File ""/home/wangmeng/pytorch-pose-quantized/pose/models/modules/quantize.py"", line 165, in forward      min_value * (1 - self.momentum))  TypeError: add_ received an invalid combination of arguments - got (Variable), but expected one of:   * (float value)        didn't match because some of the arguments have invalid types: (Variable)   * (torch.cuda.FloatTensor other)        didn't match because some of the arguments have invalid types: (Variable)   * (torch.cuda.sparse.FloatTensor other)        didn't match because some of the arguments have invalid types: (Variable)   * (float value, torch.cuda.FloatTensor other)   * (float value, torch.cuda.sparse.FloatTensor other)  "
"It seems that in *rangBN* that scale_fix should be  as in paper,   but here scale_fix becomes   scale_fix =    "
"TRAINING - Epoch: [0][410/446]  Time 0.602 (0.622)      Data 0.000 (0.005)      Loss 4.0999 (5.5282)    Prec@1 2.344 (3.435)    Prec@5 19.531 (14.536)  TRAINING - Epoch: [0][420/446]  Time 0.602 (0.622)      Data 0.000 (0.005)      Loss 4.1251 (5.4952)    Prec@1 3.906 (3.459)    Prec@5 20.312 (14.664)  TRAINING - Epoch: [0][430/446]  Time 0.611 (0.621)      Data 0.000 (0.005)      Loss 4.0770 (5.4635)    Prec@1 3.125 (3.478)    Prec@5 24.219 (14.813)  TRAINING - Epoch: [0][440/446]  Time 0.600 (0.621)      Data 0.000 (0.005)      Loss 4.0965 (5.4331)    Prec@1 7.031 (3.515)    Prec@5 19.531 (14.948)  Traceback (most recent call last):    File ""main.py"", line 305, in        main()    File ""main.py"", line 187, in main      train_loader, model, criterion, epoch, optimizer)    File ""main.py"", line 293, in train      training=True, optimizer=optimizer)    File ""main.py"", line 249, in forward      output = model(inputs)    File ""/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__      result = self.forward(*input, **kwargs)    File ""/workspace/pytorch-quantization/quantized.pytorch/models/resnet_quantized.py"", line 148, in forward      x = self.layer3(x)    File ""/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__      result = self.forward(*input, **kwargs)    File ""/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/nn/modules/container.py"", line 91, in forward      input = module(input)    File ""/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__      result = self.forward(*input, **kwargs)    File ""/workspace/pytorch-quantization/quantized.pytorch/models/resnet_quantized.py"", line 56, in forward      out = self.bn1(out)    File ""/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__      result = self.forward(*input, **kwargs)    File ""/workspace/pytorch-quantization/quantized.pytorch/models/modules/quantize.py"", line 272, in forward      y = y.view(C, self.num_chunks, B * H * W // self.num_chunks)  RuntimeError: invalid argument 2: size '[256 x 16 x 134]' is invalid for input with 551936 elements at ../src/TH/THStorage.cpp:40  "
"I noticed that you don't cancel gradient of the large values, when using straight through estimator  .     In QNN paper it was claimed ""Not cancelling the gradient when r is too large significantly worsens performance"".     Does it only matter for low precision quantization (e.g. binary?)"
"Hi, thank you for posting your code!     I think there's a mismatch of the argument order between   and  .         Among other potential issues, this causes stochastic arg to take the value of num_chunks, sometimes making it true, and leading to ""stochastic"" rounding. "
"Hi,    I am trying to run prediction but hitting a roadblock with CUDA not supporting Byte tensor:       Any thoughts how can I directly use a quantized model?  "
is there any method for inference and test model?
"I wanna find the 'parameters' method in any model, but I couldn't. how can I fix that?"
"Hi, How can I print the gradient before and after quantization?  I want to make sure that it was quantized"
None
Hi very interesting work!    I would like to clarify one point on the model generalization. Does the trained model only work on the training example x from domain A or could it be tested on any images in A?
"hello,    I went through implementation of meta learning, where they compute second order derivative, I saw a paper where they combined meta learning with gans to achieve good results on one shot image generation, they generated different facial expressions from one image.    does meta learning apply to this repository also?    thanks"
"Hi, I have been working on another image translation task and I am interested in the metrics in your paper: **perceptual distance** & **style distance**. Can you provide some details about the evaluation? Cause if I use the  , then the input will be in range [0, 255], so the style distance will be around 10k -20k, I am wandering did you use a customized vgg whose input is in range [0, 1] so that the distance could be around 1-10?  Thanks"
"Hi, thanks for your wonderful work.     In your code, you directly use 'mu_2 = torch.pow(mu, 2)' to compute the KL loss between encoded feature and N(0, 1).    However, in traditional, we first compute 'mu' and 'logvar' by the encoder, and compute the KL loss by 'KL = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())'.    Can you explain the difference between these two ways for computing the KL loss?    Thank you very much!"
test.py  --dataroot=./datasets/facades/ --name=facades_ost --model=ost --load_dir=facades_ost --no_dropout --n_downsampling=2 --num_unshared=2 --start=0 --max_items_A=12
"E:\Users\Raytine\Anaconda3\python.exe F:/zhaiyao/OneShotTranslation-master/drawing_and_style_transfer/ptest.py --dataroot=./datasets/facades/ --name=facades_ost --model=ost --no_dropout --n_downsampling=2 --num_unshared=2 --start=0 --max_items_A=1  ------------ Options -------------  A: A  B: B  aspect_ratio: 1.0  batchSize: 1  checkpoints_dir: ./checkpoints  dataroot: ./datasets/facades/  dataset_mode: unaligned  display_id: 1  display_port: 8097  display_server:    display_winsize: 256  fineSize: 256  gpu_ids: [0]  how_many: 50  init_type: normal  input_nc: 3  isTrain: False  loadSize: 286  load_dir: ./checkpoints  max_items_A: 1  max_items_B: -1  model: ost  nThreads: 2  n_downsampling: 2  n_layers_D: 3  name: facades_ost  ndf: 64  ngf: 64  no_dropout: True  no_flip_and_rotation: False  norm: instance  ntest: inf  num_res_blocks_shared: 6  num_res_blocks_unshared: 0  num_unshared: 2  output_nc: 3  phase: train  resize_or_crop: resize_and_crop  results_dir: ./results/  rotation_degree: 7  serial_batches: False  start: 0  which_direction: AtoB  which_epoch: latest  which_model_netD: basic  which_model_netG: resnet_9blocks  -------------- End ----------------  CustomDatasetDataLoader  dataset [UnalignedDataset] was created  ost  E:\Users\Raytine\Anaconda3\lib\site-packages\torchvision\transforms\transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.    ""please use transforms.Resize instead."")  initialization method [normal]  initialization method [normal]  F:\zhaiyao\OneShotTranslation-master\drawing_and_style_transfer\models\networks.py:22: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.    init.normal(m.weight.data, 0.0, 0.02)  initialization method [normal]  initialization method [normal]  initialization method [normal]  initialization method [normal]  Traceback (most recent call last):    File ""F:/zhaiyao/OneShotTranslation-master/drawing_and_style_transfer/ptest.py"", line 24, in        model = create_model(opt)    File ""F:\zhaiyao\OneShotTranslation-master\drawing_and_style_transfer\models\__init__.py"", line 17, in create_model      model.initialize(opt)    File ""F:\zhaiyao\OneShotTranslation-master\drawing_and_style_transfer\models\ost.py"", line 89, in initialize      self.load_network(self.netEnc_a, 'Enc_a', which_epoch)    File ""F:\zhaiyao\OneShotTranslation-master\drawing_and_style_transfer\models\base_model.py"", line 54, in load_network      network.load_state_dict(torch.load(save_path))    File ""E:\Users\Raytine\Anaconda3\lib\site-packages\torch\serialization.py"", line 356, in load      f = open(f, 'rb')  FileNotFoundError: [Errno 2] No such file or directory: './checkpoints\\./checkpoints\\latest_net_Enc_a.pth'"
"The readme instructions give parameters for the dataset locations (e.g) --dataroot=./datasets/facades/trainB  but folder ./datasets does not exist.  Could you add it, or provide instructions on how to set it up?"
"Hi, the code in this repository clearly copies code from the   - as evidenced by the â€œCredit to GPflowâ€ remarks. However, GPflowâ€™s Apache 2.0 license explicitly states:    > Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: You must give any other recipients of the Work or Derivative Works a copy of this License; and You must cause any modified files to carry prominent notices stating that You changed the files; and You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work [...]     It would be great if you could fix this and add correct attribution of code taken from GPflow to your codebase. (It would also have been best scientific practice to actually cite the GPflow paper, as referenced in the GPflow README.md, in your publication on SGHMC for DGPs.)"
"Dear @benjaminaubin :          I am a PhD student at the University of Chinese Academy of Sciences. I'm doing research on Bayesian deep learning.  I've been reading this paper of yours. The derivation of AMP algorithm in the paper is very detailed, which is of great help to me. Thanks a lot. There is one point in this paper that I never understood, and I don't seem to see the instructions in it. In section4.2 (Page 11), q = qd * IKÃ—K + (qa/K)*1K*1K' , where does this formula come from? And what do q, qd and qa mean (the overlaps? I gauss )? If possible, could you give me some detailed description. Best wishes!"
"@benjaminaubin  Thanks for your sharing. And it helps me a lot! Could you explain the function of Zout_K2 in gout_sign_sign.py. I'm very curious how does it implement Zout's formula in the article. In particular, the decomposition of V , the meaning of omega_tilde and the meaning of lower ,upper limit of integration bother me for a long time."
" @mrjel Hi,thanks for sharing this wonderful repo.I'm a beginner to the Capsule Network.    When I run the affnist.py,I got something wrong.  It was about the mismatching of the data format.(The function space_to_depth in pooling_capsule_layer.py)    The wrong statement:  ""t_t.contiguous().view(batch_size, d_height, 1, d_depth, s_posev)  RuntimeError: shape '[48, 2, 1, 256, 1]' is invalid for input of size 18432""    I have found the value of t_t when the error occured,which was ""[48,3,2,64,1]"".It's obviously that there got something wrong.  But I didn't change any code while I got the wrong result,and I don't know how to fix it.  Could you help me?Thank you very much!~"
"Hi. After some workarounds, when I tried using your code, I found that the splineconv declaration does not accept the norm argument.  I received the following error: __init__() got an unexpected keyword argument 'norm'  for the conv1 member.  Do clarify this."
"Hello @mrjel :  I cloned your code to my cpu-only computer. I follow the instructions in your readme.md. when I run the mnist example, I got a RuntimeError. Here is the error:     The negative part's absolute value is *quite small*, but it is NOT 0. ...  I do not know where to fix it.     Yours,   @wmf1997  "
"Hi, thanks for sharing this wonderful repo. I found that there is a function space_to_depth defined as follows:     def space_to_depth(input, block_size):       block_size = int(block_size)       block_size_sq = block_size * block_size       output = input       (batch_size, s_height, s_width, s_depth, s_posev) = output.size()       d_depth = s_depth * block_size_sq       d_height = int((s_height + (block_size - 1)) / block_size)       t_1 = output.split(block_size, 2)       stack = [           t_t.contiguous().view(batch_size, d_height, 1, d_depth, s_posev)           for t_t in t_1       ]       output = torch.cat(stack, 2)       return output    **Wha's the exact meaning of this function to process pose and aggrement in the forward pass?**        def forward(self, x, a, pose, size):          pooled_size = (size[0], int((size[1] + 1) / (self.pool_length)),                         int((size[2] + 1) / (self.pool_length)))            a = a.view(*size, self.in_channels)          pose = pose.view(*size, self.in_channels, 2)            a = space_to_depth(a.unsqueeze(-1), self.pool_length).squeeze(-1)          pose = space_to_depth(pose, self.pool_length)            pose = pose.view(*pooled_size, self.pool_size, self.in_channels, 2)          a = a.view(*pooled_size, self.pool_size, self.in_channels)  "
"When I run setup.py, it gives the message that installation is complete. However, import does not work. I get the message    File ""examples/mnist.py"", line 10, in      from group_capsules.utils import grid, grid_cluster, make_batch, spread_loss   ModuleNotFoundError: No module named 'group_capsules'"
None
"Hi @SimonKohl, did you change your project to support the higher version of tf. Actually, I am working on a project and cannot downgrade my tf 2.2.0 (due to conflict with other work). Any help from your side will be really appreciated. Hope to get a reply from you soon. "
"I am working on probabilistic Unet. However, there is dependency problem among installed versions. I am using the Linux platform with a GPU cluster. May you please tell me your installed version of TensorFlow GPU, Keras, python, and others.  In the requirement file, some are given, but not all. Following is my error stack, anybody please reply (if encountered the same issue):    a00636.science.domain  0,1  WARNING:tensorflow:From /home/bdp954/PU_Project1/probabilistic_unet/utils/training_utils.py:181: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.  Instructions for updating:  `normal` is a deprecated alias for `truncated_normal`  Logging to /home/bdp954/PU_Project1/probabilistic_unet/model/train.log  Assembling file dict from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train.  Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/jena  Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/stuttgart  Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/aachen  Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/hanover  Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/dusseldorf  Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/hamburg  Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/zurich  Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/erfurt  Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/cologne  Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/krefeld  Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/strasbourg  Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/weimar  Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/bremen  Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/bochum  Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/tubingen  train set comprises 2701 files.  Assembling file dict from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train.  Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/monchengladbach  Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/ulm  Reading from /home/bdp954/PU_Project1/gtFine_trainvaltest/output/quarter/train/darmstadt  train set comprises 274 files.  INFO:tensorflow:Building U-Net.  Building U-Net.  INFO:tensorflow:Building ConvGaussian.  Building ConvGaussian.  INFO:tensorflow:Building ConvGaussian.  Building ConvGaussian.  INFO:tensorflow:encoder scale 0: (?, 27, 256, 512)  encoder scale 0: (?, 27, 256, 512)  INFO:tensorflow:encoder scale 1: (?, 32, 256, 512)  encoder scale 1: (?, 32, 256, 512)  INFO:tensorflow:encoder scale 2: (?, 64, 128, 256)  encoder scale 2: (?, 64, 128, 256)  INFO:tensorflow:encoder scale 3: (?, 128, 64, 128)  encoder scale 3: (?, 128, 64, 128)  INFO:tensorflow:encoder scale 4: (?, 192, 32, 64)  encoder scale 4: (?, 192, 32, 64)  INFO:tensorflow:encoder scale 5: (?, 192, 16, 32)  encoder scale 5: (?, 192, 16, 32)  INFO:tensorflow:encoder scale 6: (?, 192, 8, 16)  encoder scale 6: (?, 192, 8, 16)      Failed to import TensorFlow. Please note that TensorFlow is not installed by default when you install TensorFlow Probability. This is so that users can decide whether to install the GPU-enabled TensorFlow package. To use TensorFlow Probability, please install the most recent version of TensorFlow, by following instructions at        Traceback (most recent call last):    File ""train_prob_unet.py"", line 188, in        train(cf)    File ""train_prob_unet.py"", line 62, in train      prob_unet(x, y, is_training=True, one_hot_labels=cf.one_hot_labels)    File ""/home/bdp954/miniconda3/envs/PU_env/lib/python3.6/site-packages/sonnet/python/modules/base.py"", line 389, in __call__      outputs, subgraph_name_scope = self._template(*args, **kwargs)    File ""/home/bdp954/miniconda3/envs/PU_env/lib/python3.6/site-packages/tensorflow/python/ops/template.py"", line 359, in __call__      return self._call_func(args, kwargs)    File ""/home/bdp954/miniconda3/envs/PU_env/lib/python3.6/site-packages/tensorflow/python/ops/template.py"", line 310, in _call_func      result = self._func(*args, **kwargs)    File ""/home/bdp954/miniconda3/envs/PU_env/lib/python3.6/site-packages/sonnet/python/modules/base.py"", line 246, in _build_wrapper      output = self._build(*args, **kwargs)    File ""/home/bdp954/PU_Project1/probabilistic_unet/model/probabilistic_unet.py"", line 410, in _build      self._q = self._posterior(img, seg)    File ""/home/bdp954/miniconda3/envs/PU_env/lib/python3.6/site-packages/sonnet/python/modules/base.py"", line 389, in __call__      outputs, subgraph_name_scope = self._template(*args, **kwargs)    File ""/home/bdp954/miniconda3/envs/PU_env/lib/python3.6/site-packages/tensorflow/python/ops/template.py"", line 359, in __call__      return self._call_func(args, kwargs)    File ""/home/bdp954/miniconda3/envs/PU_env/lib/python3.6/site-packages/tensorflow/python/ops/template.py"", line 310, in _call_func      result = self._func(*args, **kwargs)    File ""/home/bdp954/miniconda3/envs/PU_env/lib/python3.6/site-packages/sonnet/python/modules/base.py"", line 246, in _build_wrapper      output = self._build(*args, **kwargs)    File ""/home/bdp954/PU_Project1/probabilistic_unet/model/probabilistic_unet.py"", line 340, in _build      return tfd.MultivariateNormalDiag(loc=mu, scale_diag=tf.exp(log_sigma))    File ""/home/bdp954/miniconda3/envs/PU_env/lib/python3.6/site-packages/tensorflow_probability/python/internal/lazy_loader.py"", line 57, in __getattr__      module = self._load()    File ""/home/bdp954/miniconda3/envs/PU_env/lib/python3.6/site-packages/tensorflow_probability/python/internal/lazy_loader.py"", line 41, in _load      self._on_first_access()    File ""/home/bdp954/miniconda3/envs/PU_env/lib/python3.6/site-packages/tensorflow_probability/python/__init__.py"", line 41, in _validate_tf_environment      import tensorflow.compat.v1 as tf  ModuleNotFoundError: No module named 'tensorflow.compat.v1'  "
"Hello,    the download link for the cropped LIDC data is no longer publically available. It instead leads to a SSO application for google employees.       From the Readme file:  > The **LIDC data** can be downloaded as pngs, cropped to size 180 x 180 from Google Cloud Storage, see here:  .    Is this intentional? Where can we find the preprocessed data?"
Why do you subtract segmentation ground truth by 0.5 before concatenating with the image to feed into the posterior net?
ModuleNotFoundError: No module named 'batchgenerators'    the  'batchgenerators' at the location of '\data\cityscapes\data_loader.py'
"Thanks for awsome codes and paper. However, I cannot formulate the first term in ELBO as the cross-entropy function which used in your loss function.     What's more, it is a little strange to add the cross-entropy related to the segmentation result from Z_q. Because the Q is generated from the ground-truth, and the S_q is from Z_q. Is it meaningful to only calculate the CE(Y, S_q) rather than CE(Y, S_p)? I mean that the model has gotten the ground-truth in training phase, it is unfair to calculate CE for this model."
How do I do the inference on a single image using the provided pretrained weights?
"Hi!  First of all, thank you very much for sharing your code.    I would like to know if I could use it with single channel MR images.    Thank you!"
"I have successfully ran the training on the Cityscapes dataset while only changing the paths in the config files and fixing issue #2 and executed the evaluation scripts.    However, the results were completely different than those presented in the readme. In fact, after investigating, it seems that the trained model only outputs a single number across all images - 3, which seems to be the id for the class 'wall'. This shows that the model failed to learn any task, even though all the learning parameters were preserved from your code.    Do you have any pointers you could give as to what might be causing such a failure and how could I go about making the model actually learn something?"
"Hello Simon! Tank you for sharing this code, it helps me with my research on semantic segmentation!     I have a fairly stupid issue: I would like to use the pertained model and I downloaded the weights from the zenodo.org. However, after running the --write samples  function, I get this error in conda prompt:   ""AttributeError: module 'model.pretrained_weights' has  no attribute __file__ It comes to the problem of extracting weights, i.e. I guess I don't know how to edit exp_dir path and use weights from ckpt files. Would anyone please help me understand how to edit this path and use the data?     I tried extracting the ckpt data with tf.Session but I encountered other issues and I assume there's simpler solution to this?    Thank you!"
"I installed all the required dependancies and tried hard to run the code but stuck at following error. Please help    Reading from /home/gpu3/prob_unet/output/quarter/train/stuttgart  Reading from /home/gpu3/prob_unet/output/quarter/train/weimar  Reading from /home/gpu3/prob_unet/output/quarter/train/jena  Reading from /home/gpu3/prob_unet/output/quarter/train/aachen  Reading from /home/gpu3/prob_unet/output/quarter/train/bochum  Reading from /home/gpu3/prob_unet/output/quarter/train/hanover  Reading from /home/gpu3/prob_unet/output/quarter/train/tubingen  Reading from /home/gpu3/prob_unet/output/quarter/train/erfurt  Reading from /home/gpu3/prob_unet/output/quarter/train/bremen  Reading from /home/gpu3/prob_unet/output/quarter/train/zurich  Reading from /home/gpu3/prob_unet/output/quarter/train/hamburg  Reading from /home/gpu3/prob_unet/output/quarter/train/dusseldorf  Reading from /home/gpu3/prob_unet/output/quarter/train/cologne  Reading from /home/gpu3/prob_unet/output/quarter/train/strasbourg  Reading from /home/gpu3/prob_unet/output/quarter/train/krefeld  train set comprises 2701 files.  Traceback (most recent call last):    File ""train_prob_unet.py"", line 188, in        train(cf)    File ""train_prob_unet.py"", line 39, in train      data_provider = get_train_generators(cf)    File ""/home/gpu3/probabilistic_unet/training/data/cityscapes/data_loader.py"", line 335, in get_train_generators      n_batches=cf.n_train_batches)    File ""/home/gpu3/probabilistic_unet/training/data/cityscapes/data_loader.py"", line 300, in create_data_gen_pipeline      mirror_transform = MirrorTransform(axes=(3,))    File ""/home/gpu3/probabilistic_unet/training/batchgenerators/transforms/spatial_transforms.py"", line 192, in __init__      raise ValueError(""MirrorTransform now takes the axes as the spatial dimensions. What previously was ""  ValueError: MirrorTransform now takes the axes as the spatial dimensions. What previously was axes=(2, 3, 4) to mirror along all spatial dimensions of a 5d tensor (b, c, x, y, z) is now axes=(0, 1, 2). Please adapt your scripts accordingly.  "
"I have checked evaluation with pretrained weights and test plots and they work. I tried to train the network but the result was this error. Do you have any suggestion about the error? Thank you very much.    Traceback (most recent call last):    File ""train_prob_unet.py"", line 190, in        train(cf)    File ""train_prob_unet.py"", line 41, in train      data_provider = get_train_generators(cf)    File ""/home/tpv/probabilistic_unet/data/cityscapes/data_loader.py"", line 335, in get_train_generators      n_batches=cf.n_train_batches)    File ""/home/tpv/probabilistic_unet/data/cityscapes/data_loader.py"", line 300, in create_data_gen_pipeline      mirror_transform = MirrorTransform(axes=(3,))    File ""/home/tpv/batchgenerators/batchgenerators/transforms/spatial_transforms.py"", line 200, in __init__      raise ValueError(""MirrorTransform now takes the axes as the spatial dimensions. What previously was ""  ValueError: MirrorTransform now takes the axes as the spatial dimensions. What previously was axes=(2, 3, 4) to mirror along all spatial dimensions of a 5d tensor (b, c, x, y, z) is now axes=(0, 1, 2). Please adapt your scripts accordingly."
"Hello, i am trying to load the pretrained weights inside model/pretrained_weights . Do you know if I missing something?      File ""cityscapes_eval_config.py"", line 91, in        exp_dir = '/'.join(os.path.abspath(pretrained_weights.__file__).split('/')[:-1])  AttributeError: module 'model.pretrained_weights' has no attribute '__file__'  "
"During training why do you sample latent space from Posterior Net instead of from prior net, to be the same as test procedure ? "
Anyone have a working dockerfile for the gpu implementation?
Are there any speed benchmarks with the GPU implementation and other popular methods of training AI's such as Muzero?    Also would there be any advantage in paring this with Muzero?
"so while trying to run the ga code for frostbite environment I run into this issue  `ValueError: kth(=-5) out of bounds (5)`  the source of error is this   `idx = np.argpartition(returns_n2, (-population_size, -1))[-1:-population_size-1:-1] `    Anybody knows how to resolve this?"
"module 'gym' has no attribute 'undo_looger_setup'    When I run '. scripts/local_run_exp.sh ga configurations/frostbite_ga.json', it occur that error.    <img width=""731"" alt=""á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2020-06-26 á„‹á…©á„’á…® 6 10 33"" src=""   "
"Issue 1:  At line:     The rollout function returns 3 objects, due to which we getting an error:  `ValueError: too many values to unpack (expected 2)`  It could be solved by replacing line 230 with:  `eval_rews, eval_length, _ = policy.rollout(env)`  Issue 2:  At line:     It should be `lengths_n2 = np.concatenate([r.lengths_n2 for r in curr_task_results])` very much similar to    Otherwise `lengths_n2.mean()` and` lengths_n2.sum()` will have multiple dimension."
"When I try to visualize the snapshots generated by games like Frostbite, i just run the command        This works like a charm, but when I change the game, so like SpaceInvaders, after the training and the generation of the snapshots files, when I run:     It doesn't work as it expects inputs of shape (1,84,84,4) but SpaceInvaders' inputs are (1, 210, 160, 3).    Its like the script viz.py has stubbed the expected input shape and can't handle different games from those with (1,84,84,4) tensors.     Did anyone run into the same issue?    Details of the error:  Input command:     Error reported:   "
"Hi, I ran into trouble when running the command "". scripts/local_run_exp.sh"", where the cygwin terminal gives ""-bash: tmux: command not found"". Seems like nobody have ran into this problem before, I hope someone can give a help. Thanks in advance!"
"I got the following error:    [2019-07-03 17:08:04,453 pid=11687] Could not connect to {'unix_socket_path': '/tmp/es_redis_master.sock'}. Retrying after 7.11 sec (2/300). Error: Error 2 connecting to unix socket: /tmp/es_redis_master.sock. No such file or directory.    Could you please tell me how to resolve it? Thanks."
"Hello People,    I managed to find some error, when I tested the ES algorithm.         Most of these errors were related to `es_distributed/tf_util.py` file which was originated from the `tf.concat` function or method.    Below were some of the changes:    1) `def concatenate(arrs, axis=0)` function at line 30 - 31    from:         to:         2) `def flatgrad(loss, var_list)` function at line 219 - 222    from:         to:         3) `def __init__(self, var_list)` function at line 243 -244    from:          to:          My environment information:  Ubuntu 18.04 x64  Python 3.6.8  tensorflow 1.13.1  Click 7.0  atari-py 0.1.15  numpy 1.16.3  gym 0.12.1  baselines 0.1.5    Thanks"
"Hello People,    I am getting the following error when I run `. scripts/local_run_exp.sh rs configurations/frostbite_ga.json`, which is the RS experiment         and I managed to fixed it by changing the following code:    from    deep-neuroevolution/es_distributed/rs.py, line 190:    `eval_rews, eval_length = policy.rollout(env) # eval rollouts don't obey task_data.timestep_limit`    to:    deep-neuroevolution/es_distributed/rs.py, line 190:    `eval_rews, eval_length, _ = policy.rollout(env) # eval rollouts don't obey task_data.timestep_limit`    My environment information:  Ubuntu 18.04 x64  Python 3.6.8  tensorflow 1.13.1  Click 7.0  atari-py 0.1.15  numpy 1.16.3  gym 0.12.1  baselines 0.1.5    Thanks"
"Hello People,    I am getting the following error when I run `. scripts/local_run_exp.sh ga configurations/frostbite_ga.json`, which is the GA experiment         and I managed to fixed it by changing the following code:    from    deep-neuroevolution/es_distributed/ga.py, line 230:    `eval_rews, eval_length = policy.rollout(env) # eval rollouts don't obey task_data.timestep_limit`    to:    deep-neuroevolution/es_distributed/ga.py, line 230:    `eval_rews, eval_length, _ = policy.rollout(env) # eval rollouts don't obey task_data.timestep_limit`      My environment information:  Ubuntu 18.04 x64  Python 3.6.8  tensorflow  1.13.1  Click  7.0  atari-py 0.1.15  numpy 1.16.3   gym 0.12.1  baselines  0.1.5    Thanks"
"06/09/2019 08:36:22 PM {      ""episode_cutoff_mode"": 5000,      ""game"": ""frostbite"",      ""l2coeff"": 0.005,      ""model"": ""ModelVirtualBN"",      ""mutation_power"": 0.02,      ""num_test_episodes"": 200,      ""num_validation_episodes"": 30,      ""optimizer"": {          ""args"": {              ""stepsize"": 0.01          },          ""type"": ""adam""      },      ""population_size"": 5000,      ""return_proc_mode"": ""centered_rank"",      ""timesteps"": 250000000.0  }  06/09/2019 08:36:22 PM Logging to: /tmp/tmphfvloc5w  2019-06-09 20:36:22.309953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0  2019-06-09 20:36:22.310085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 73 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)  No ROM File specified or the ROM file was not foun    I got this problem when runing es.py with the config file es_atari_config.json. anyone else encountered the same issue? It seems that the progaram has successfully loaded the config file, but failed to load rom file.      "
"When I used nvidia 1080ti, I was able to compile gym_tensorflow.so and run the exp. The env is tensorflow-gpu 1.8.0 and cuda version is 9.0. But when I switch to 2080ti, the exp run into  trouble as follow:      2019-06-02 18:03:10.727082: E tensorflow/stream_executor/cuda/cuda_blas.cc:654] failed to run cuBLAS routine cublasSgemmBatched: CUBLAS_STATUS_EXECUTION_FAILED  2019-06-02 18:03:10.727109: E tensorflow/stream_executor/cuda/cuda_blas.cc:2413] Internal: failed BLAS call, see log for details  Exception in thread Thread-1:  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1322, in _do_call      return fn(*args)    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1307, in _run_fn      options, feed_dict, fetch_list, target_list, run_metadata)    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1409, in _call_tf_sessionrun      run_metadata)  tensorflow.python.framework.errors_impl.InternalError: Blas xGEMMBatched launch failed : a.shape= ]]            ]]    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""/usr/lib/python3.6/threading.py"", line 916, in _bootstrap_inner      self.run()    File ""/usr/lib/python3.6/threading.py"", line 864, in run      self._target(*self._args, **self._kwargs)    File ""/root/hz/deep-neuroevolution/gpu_implementation/neuroevolution/concurrent_worker.py"", line 94, in _loop      rews, is_done, _ = self.sess.run(  , but still no luck after I take action to Install patchs for cuda9,  I wonder if there is a solution."
"Hi,    It is not entirely clear to me how you guys handle elitism. In your paper ( ) you mention ""The Nth individual is an **unmodified** copy of the best individual from the previous generation,..."". However, if I understand correctly, in the ga and ga_modified algorithms your workers still evolve the parameters if they sample the first (or last) individual in the population.    In fact I don't immediately see where any of the mentioned 'evaluate top 10 individuals from previous generation on 30 additional episodes' takes place.     I'm just curious because I'm not sure how to handle elitism in a project myself. Thanks in advance :)"
"When I run '. scripts/local_run_exp.sh es configurations/frostbite_es.json', I got such error 'AttributeError: 'list' object has no attribute 'ndim'' .     Part of log:  'env_id': 'FrostbiteNoFrameskip-v4',   'optimizer': {'args': {'stepsize': 0.01}, 'type': 'adam'},   'policy': {'args': {}, 'type': 'ESAtariPolicy'}}  ********** Iteration 0 **********  Traceback (most recent call last):    File ""/home/gaoli/tools/Python3.6/lib/python3.6/runpy.py"", line 193, in _run_module_as_main      ""__main__"", mod_spec)    File ""/home/gaoli/tools/Python3.6/lib/python3.6/runpy.py"", line 85, in _run_code      exec(code, run_globals)    File ""/home/gaoli/CODE/deep-neuroevolution/es_distributed/main.py"", line 90, in        cli()    File ""/home/gaoli/CODE/deep-neuroevolution/env/lib/python3.6/site-packages/click/core.py"", line 722, in __call__      return self.main(*args, **kwargs)    File ""/home/gaoli/CODE/deep-neuroevolution/env/lib/python3.6/site-packages/click/core.py"", line 697, in main      rv = self.invoke(ctx)    File ""/home/gaoli/CODE/deep-neuroevolution/env/lib/python3.6/site-packages/click/core.py"", line 1066, in invoke      return _process_result(sub_ctx.command.invoke(sub_ctx))    File ""/home/gaoli/CODE/deep-neuroevolution/env/lib/python3.6/site-packages/click/core.py"", line 895, in invoke      return ctx.invoke(self.callback, **ctx.params)    File ""/home/gaoli/CODE/deep-neuroevolution/env/lib/python3.6/site-packages/click/core.py"", line 535, in invoke      return callback(*args, **kwargs)    File ""/home/gaoli/CODE/deep-neuroevolution/es_distributed/main.py"", line 61, in master      algo.run_master({'unix_socket_path': master_socket_path}, log_dir, exp)    File ""/home/gaoli/CODE/deep-neuroevolution/es_distributed/es.py"", line 246, in run_master      assert (result.noise_inds_n.ndim == 1 and  AttributeError: 'list' object has no attribute 'ndim'  "
"Getting the following errors from a fresh git clone following README instructions:  SIDE NOTE: I'm installing this on an Ubuntu OS using Windows Subsystem for Linux       NOTE: I do have slightly update version of some of the python packages, but I don't think that's the errors I'm hitting. Here is the pip list anyways:   "
None
"I have been getting this error, upon running the command,  . scripts/local_run_exp.sh es configurations/frostbite_es.json.    "
"When running  `. scripts/local_run_exp.sh es configurations/frostbite_es.json `    I get a Value Error/Dimension Mismatch. Here is the full output of the redis master:         How do I fix this and get the example up and running? I'd be grateful for any help!    Cheers,  Marvin"
None
in visual_inspector/figure_base/load_data.py    the line:     will result in a nan error when maxfit == minfit    I suggest:   
"In ES, compute_ranks() does an argsort, which will give different ranks to individuals with the same fitness.  This introduces a noise in the gradient estimate. This is not a big issue since the expected value of the noise is zero, but it can slow down convergence.   This is really only a problem in environments, where rewards are sparse, so a lot of individuals will have the same fitness.  Solution: Average ranks for individuals with equal fitness.    "
"I lunch any of the local experiment after setting up redis server for master and relay.   I get stuck in iteration 0 for the master while the relay keep retrying   2018-11-19 15:07:39,067 pid=31344] [master] Declared experiment {'config': {'calc_obstat_prob': 0.0,        `      'episode_cutoff_mode': 5000,              'episodes_per_batch': 5000,              'eval_prob': 0.01,              'l2coeff': 0.005,              'noise_stdev': 0.005,              'return_proc_mode': 'centered_rank',              'snapshot_freq': 20,              'timesteps_per_batch': 10000},   'env_id': 'FrostbiteNoFrameskip-v4',   'num_elites': 1,   'policy': {'args': {'nonlin_type': 'relu'}, 'type': 'GAAtariPolicy'},   'population_size': 10}  ********** Iteration 0 **********  `  `[2018-11-19 15:10:58,121 pid=31372] es:exp not set. Retrying after 4.89 sec (41/300)  `  I am not sure why the workers cannot connect to the master. "
"Hi, what is the meaning of ""timesteps"" parameters in ga_atari_config.json? Is it the total frames or Forward Passes in the paper?  Thanks."
"Hello,    First of all, I think you did some really great work! I am trying to apply the idea of using GAs to train models for some of my own work and have been looking to your work and code for some inspiration. I am wondering if you could help me find out how the weight calculated by the GA are fed into the tensorflow model archictecture? I have been looking through the repo for a while but am struggling to find the link between the GA and the model run.    Thanks!"
I notice Novelty Seeking is not implemented for gpu implementation.  Is there any issue that might block Novelty Seeking implementation using gpu?
"WorkerClient.get_current_task() doesn't return the new task_id when the master's work_id changes, but instead keeps returning the old task_id. The workers cached_task_id is thus never updated.  What can be the reason for this?"
"Hi,   I am trying to run gpu-implementation es.py using gym enviroment gym.CartPole-v1  I edited the configuration file es_atari_config.json by changing ""frostbrite"" to ""gym.CartPole-V1"" and encounter error below             I notice that the GymEnv in tf_env.py having property of discrete_action that will only return False which is triggering the assert in the get_ref_batch    `  class GymEnv(PythonEnv):  ...      @property      def discrete_action(self):          return False   `      Is openai gym's enviroment ready for run?"
I have got the following error message when I tried to launch sample GA experiment:     and the following error message when trying to launch sample RS experiment:     Thanks
"Hello,   To be able to visualize the cloud plot using vine for the Atari domain, you said that we need to capture the final emulator RAM state ( integer-valued vectors of length 128 that capture all the state variables in a game) as the BC, I am wondering what format of data should we be entering as input to the ""process_bc"" script for dimensionality reduction ?  Thank you!"
You have a minor typo in:  `gpu_implementation/README.md`    This:   `python ga.py ra_atari_config.json`  Should be this:  `python ga.py rs_atari_config.json`    Unless the filename is mis-named.
"__flake8 testing of   on Python 3.6.3__    $ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__   "
"Thank you for opening code.  Along with your Readme.md, I did    ` . scripts/local_run_redis.sh `    error happens     *** FATAL CONFIG FILE ERROR ***  Reading the configuration file, at line 80  >>> 'protected-mode yes'  Bad directive or wrong number of arguments    What is protected-mode? and what should I do to do program?"
"in pose_rnn.py, L80:   "
"Hi,  It is really an interested and wonderful work!  I just want to know how you decompose the components of the video? I have read the paper carefully, but I still can not quite  figure it out.  And another question is that how long it takes to complete a Moving MNIST experiment on your device?"
"Dear authors and maintainers,    would you please be so kind, as to add a license to the repo, such that terms of usage are clear?     Thank you in advance!"
"Hi,    I cannot run the bouncing ball experiment. I only created a new data set of 500 sequences and updated the paths and file names related to the data set. Could you take a look at what's going on?      "
"Hi! Thanks a lot for the ATOMO paper and sharing your code over here. I just had a look at your implementation of 1-bit QSGD compression, and was wondering where the Elias coding is implemented. I would be curious if you have any thoughts on doing this efficiently. I found `utils.compress`, which I thought could be related, but I cannot find a use of this function in this repo.  "
"helloï¼Œ Iâ€˜m trying to test this code on imagenet, but I find that when the program runs to `self.comm.Bcast([self.model_recv_buf.recv_buf[layer_idx], MPI.DOUBLE], root=0)` in function `async_fetch_weights_bcast` in distributed_worker.py at the first step, it thrown an error that is `MPI_ERR_TRUNCATE: message truncated  ` , but I check the memory size in Bcast and it works when the program ran on Cifar10/100, have u encountered this problem?     And another issue: then I replaced the Pytorch0.3.0 with Pytorch0.4/1.1, the proceeding time on decode of QSGD is significantly higher than 0.3.0, almost 10 times than it, have u tried this?  "
"Dear @kirthevasank,    Thanks for your great implementation. I try to run the code with **demos/demo_cnn.py**. But I see X1, X2 is empty to compute the distance, leading to a bug in computation.  In file **opt/nasbot.py**  init_gp_fitter = NNGPFitter([], [], self.domain.get_type(), tp_comp=self.tp_comp,                                  list_of_dists=None, options=self.options,                                  reporter=self.reporter)     Could you help to solve this issue? Thanks so much."
"Hi!  Just as your paper, the Cifar-10's error rate curves could be plot as this.  !   However, When I tried to run your code, I got this curves.  !   I want to know if there is something wrong "
SliceLocalization. P is unreadable  and always reports errorsï¼š  ValueError: insecure string pickle.  It was already dumped. Forget to close when savingï¼š   Can you give a new one ?
"Hi!,@kirthevasank, I have forked the project and run it in the cifar10 dataset, all the architectures searched by the Nasbot have low accuracy compare to the SOTA architectures. I have changed some HyperParameters, but it's still not work, can you guess the reason for the ""lower"" accuracy ? Thanks!! "
"Hi,     If we want to find architectures on big datasets such as ImageNet, how to change our code to support it?    Thanks"
"Thanks for releasing all of your code for this.  Could you make a `setup.py` so that this is installable via pip?  It seems pretty close.      FYI I'm looking to use the OTMANN distance in a separate project, but the local references are making it difficult for me to include just the distance without copying over all of the files and updating the paths."
"The code size is too large, not easy to read, how to use random search in the source code?Can you give an example? Thank you."
"hi, I meet the problem when i follow the step 'bash make_direct.sh' & ' python simple_direct_test.py '  I have installed both python2 and python3 on my computer, after the 'bash make_direct.sh',    'ImportError: No module named direct' is occured when I use 'python2 simple_direct_test.py',    but it's work right ""python3 simple_direct_test.py"", what should I try to make it work on python2? Thanks!"
"I'm trying out the code in BOCPDviaSMC_Hawkes on a sensor dataset and am getting an error at this line `rld = stats.rv_discrete( values = ( np.arange( min(t_+1, self.rmax) ), self.rlp ) )` in the `class BOCPD`. I printed out `self.rlp` :           Any possible reason why I'd be getting nans (or any way to solve this issue)?    Thanks!"
"Never mind! The shape does not so depend on the observation, and both a=0.0 and 1.0 works similarly.  ----EDIT-----  I want to reproduce your toy experiment for double banana in the paper in python. It seems like the shape of the posterior would heavily depend on your observation y. Could you share which value you take to get those figure? (In code it is just a random variable)    By the way, how is it possible to get a symmetric shape with $x_1 = 0$ in that example, if you take a = 1.0? Is it a typo that we instead should take a = 0.0?"
"I believe that there are some files missing to run this. Specifically, I tried to run the double banana file (by first executing load_dir_double_banana.m and then running script_double_banana.m . It couldn't find the function SVN_H. I'm guessing that those files are in the folder ""../samplers/stein"", which isn't posted on the repository?"
"although the code natively supports the gdb13 dataset, choosing it as the dataset option in the code raises an exception after a few iterations.  it raises an error on molecule env in the get_observation method line 548  ``        F[0,n:n+n_shift,:] = auxiliary_atom_features``  which it tried to broadcast shape (5,5) to shape (4,5), this occurs because the last list of the array is from 16 to 21 but the array is only size 20.  can you please share a fix so it could also run on the dataset you already support?"
"Hi,   Just wondering what the function load_conditional() does in gym_molecule/envs/molecule.py.    Sorry I'm very new in this field.    Thanks.    Cheers,  Lesley"
"Hello, thank you very much for sharing the code. But, I have a question in line 525 of ppo1.pposgd_simple_gcn.py, passing the parameter pi to the function traj_segment_generator. However, I think oldpi should be passed to the function instead of pi. Is it true? Please let me know. Thank you, look forward to your reply.    Best wishes!  Anny"
"Hi,    I am trying to run this model on new dataset. I know one of the data file I have to give as input is SMILES as by default is ZINC 250K dataset.  When I checked the code it also requires, other files too, such as opt.test.logP-SA/zinc_plogp_sorted.csv. These files contains 800 molecules with I guess logP values but I am not sure how to generate such files from any other dataset. Did you providing any helper function or could you please suggest me how to do that and also how to get the penalized logP values?    Also what other data files are required to run the model?    In the code there are some hard coded values such as for the normalization, do I need to change according to new dataset and also is there any components needs to be updated in the code?    Thanks."
" read your paper, i am interested in the thoughts,but when i deploy the project ,i met too many problems , the first command i got the problem ,so can you supply the detail of env,such as the version of python, mujoco and some else .    i got too many problems, and debug several days got fail"
How long will it take for this command `mpirun -np 8 python run_molecule.py 2>/dev/null` to terminate? It is running on my machine for more than 2 days. I am running the program on GeForce GTX 1080 Ti along with 4 CPU cores.
"Hello,  thanks you for sharing your code. I am very interested in your model but there is no way to make it working.  I have fixed a lot of bugs (I cannot summarize all since there are a lot) but there are many other that would require to re-write some parts.  Do you have an up-to-date version that works?  Thanks again."
"After running for 12 days with 64 CPU's and few available GPU's (which I guess were not used by this method) finally this method finished training. It has generated a number of molecules over each iteration. When trying to generate the results using given evaluation code (which first needed to fixed for test data, authors just used sample of training data for evaluation), the results are different from training set with no improvement and less than input data statistics.  Tried to get the values for penalized logP (trained model with conditions=True and has_scaffold=True) using given reward_penalized_logp function in molecule.py and checked the top 3 largest values which has maximum of 3.34 and when checked minimum that is -89.43 which are nowhere near as reported in paper.  Either full code is not given for training or the evaluation is done differently for which code is not provided. I don't know how I can get the same results in in paper."
"Hello, I want to ask a problem about this special action space, and I hope I can get some help from here.     In fact, in my own environment, I try to train an agent using ppo algorithm (without supervised learning in this paper), but I think t**he training fails because my action is similar to the paper** ï¼ˆ**MultiCatCategoricalPd**ï¼‰. I wonder   that if  an action A is composed of [A1, A2, A3], then another action B is [B1, B2, B3],  how to calculate the loss between them (**neglogp**, **kl**)?  In the code, it seems that we just treat each part of subaction as independent ones, but if the A1 is not the same with B1, there is no meaning to calcalute the similarity between A2 and B2. I mean **if the second subaction is conditioned on the first, how can we perform calculation without the need to care for the first subaction** .      More importantly, my **second subaction space will be masked based on the first subaction**, so if A1 != B1, the valid space of A2 and B2 is not the same (I have to set the logits of invalid actions to -1e6), so c**alculating the neglogp between two actions under the same state will be huge if the first sub-action is different for these two actions**.   This issue has been around me about seveal months, so if you can help with this, I will appreciate that a lot.    Thanks in advance!"
"I directly ran the script   with no modification, but only 3614 molecules are generated. Is this the normal case? "
Here is the error log     
"Hello,    Thank you for the code.   Could you specify the preprocessing methods you apply in the original datasets (e.g. mnist)? Apart from dequantization, logit and all the functions which are already in the code."
I'm not an expert but I've been working hard to run this code on google colab. Looks like this is not working with the latest python package. Please make those small changes that are required to run on python3 so that people like me can run this code. Nice paper by the way! 
"When doing density estimation on the UCI datasets HEPMASS and MiniBooNE, I saw in the appendix D.2 of the article that several dimensions of the raw data were removed since certain real values are reoccurring too frequently. This does make sense to me since such densities would involve Dirac delta distributions being problematic when trying to estimate them with continuous densities. However, when I checked the code I stumbled upon the following lines:        They seem to compute the maximum over the counts of each real value but when implementing it myself this is not the case. The `sorted` function is sorting the array based on the first entry, which is the real value corresponding to the count and not the count itself. I demonstrate this problem in the following notebook:     It also showcases the consequences. For the HEPMASS dataset there is coincidentally no difference between the features that get dropped and the features that would be dropped when `max_count` is computed correctly, i.e. by using     On the other side, for MiniBooNE there are some dimension which are drop although `max_count` is only moderately high, e.g. 6, while dimensions with values reoccurring 3434 times are kept.    This might be a minor issue but since the version of the MiniBooNE dataset you made publicly available has been used numerous times by others as a benchmark for density estimation I think it is an issue which requires our attention."
"I am trying to run your code on POWER, GAS datasets.  The data I download from the link is 'txt' files.  However, in your code, you read from a file called 'data.npy'.       Could you please provide the code to preprocess the data and generate npy files?  Thanks."
"Hi!    Thanks for sharing amazing work!    I'm trying to port your code to PyTorch (for further use in my research).    I have a question regarding your implementation of Batch Norm. As you mention in the paper, it's implemented using global batch statistics. Could you please provide pointers to the lines where it is implemented exactly? My knowledge of Theano is a little bit rusty.  "
     The tf.sum term should be tf.sum(self.u ** 2 * tf.exp(self.logp) - self.logp)) 
"README links to the datasets at   , the link throws 404"
I have tried running your code but got the following error message (MNIST experiments):         It looks like the model is not getting the data properly. Could this be caused by changes in theano version ?
"It's unclear how `every attribute with a Pearson correlation coefficient greater than 0.98` are eliminated. As correlation is calculated in pairs, how do you decide which attribute to eliminate?    Thanks."
"Hi,  I am not sure I understand the log-likelihood expression for the Gaussian MADE:       is this correct?  I assumed that the log -likelihood would be that of a univariate gaussian with mean mu and var alpha? is my understanding wrong?"
"Hello LMescheder, I currently read your paper and interested in your method. I have a question about your model design. Is it only allow for convolution layer? Did you try other model like fully connected layer for your method?    Thank you for your attention"
None
"Hi @junyanz,  While testing the model I would like to have some control over the diversity of the generated images. Is there anyway to interpolate the latent vector to regulate the outputs? Something similar to StyleGAN?"
"When I define the option ""--netD"" or ""-netD2"" as 'basic_128' or 'basic_256',  the function ""define_D"" should use the parameter ""nl_layer"", but it report an error.    <img width=""620"" alt=""1657181742947"" src=""     The condition judge part of function ""define_D"" is like this:   "
"Hello, the training uses shoe sketches of different thicknesses (half each), and it is found that the diversity of the generated images is reduced. Do you have any suggestions to improve the diversity?"
"I change the load_size and crop_size to 512 in train_facades.sh, and get error.  I want to train on large image size, how to do that?       "
"Hi Juyanz,    How are you !hope all is fine .    I want to train this algo on different images ,I was thinking how to train them on self images ,  1)What pre-processing steps do i need to take here .Can you please guide me .?  2)The model is not saving to any directory ,Can you please let know where the trained model saves(for our own images) ?    "
I get similar-ish results when using the same discriminator for both cVAE-GAN and cLR-GAN.  Is this a happy coincidence and a strike of luck? Is there some theoretical backing for having 2 discriminators?
"code from line 112, models/bicycle_gan_model.py  `self.fake_B_random = self.netG(self.real_A_encoded, self.z_random) `    Why do we generate the fake_B_random using real_A_encoded instead of real_A_random?   I know that it does not really matter since there is no L1 loss in this part (cLR-GAN).    Doing it in this way means that half of the A input data is unused in training, or do I have some misconception here."
"As introduced in the paper, we want the Encoder to recover the latent vector from the output image in the cLR-GAN (Conditional Latent Regressor GAN). But we only update parameters in Generator. Why did you design the network like this ?"
"Hi!   I'd like to reproduce the metric reporting, specially LPIPS and VGG-16 mentioned in the paper but can't see it in the repo, are the metrics implementation available anywhere?  thanks!"
     BicycleGAN$ python ./scripts/test_before_push.pybash    ` ./datasets/download_cyclegan_dataset.sh mini_pix2pix  bash: ./datasets/download_cyclegan_dataset.sh: No such file or directory`    
"'''  ../BycycleGAN/models/bicycle_gan_model.py, line 188, in backward_G_alone  self.loss_z_L1.backword()  RuntimeError: one of the variables needed for gradient compution has been modified by an inpalace operation:[torch.cuda.FloatTensor [258,8]],which is output 0 pf TBackward, is at version 2; expected version 1 instaed.   '''    1: When I run the script ""train_edages2shoes.sh"", I have encountered the error above and it seems that there is something wrong with the calculation diagram you defined. Note:  I did not make any changes to the scripts in the Models folder.  2:  when set the ""self.opt.conditional_D "" is True,   I want to know why you chose to use (self.real_a_encoded, self.fake_b_random) to build fake_data_random instead of (self.real_a_random, self.fake_b_random).    `          # generate fake_B_random          self.fake_B_random = self.netG(**self.real_A_encoded**, self.z_random)              if self.opt.conditional_D:   # tedious conditoinal data  input_nc+output_nc              self.fake_data_encoded = torch.cat([self.real_A_encoded, self.fake_B_encoded], 1)              self.real_data_encoded = torch.cat([self.real_A_encoded, self.real_B_encoded], 1)                self.fake_data_random = torch.cat([**self.real_A_encoded**, self.fake_B_random], 1)              self.real_data_random = torch.cat([self.real_A_random, self.real_B_random], 1)          else:              self.fake_data_encoded = self.fake_B_encoded              self.fake_data_random = self.fake_B_random              self.real_data_encoded = self.real_B_encoded              self.real_data_random = **self.real_B_random**  `    "
"> Note that the encoder **E** here is producing a point estimate for z', whereas the encoder in the previous section was predicting a Gaussian distribution.    I could not clearly understand what is the difference between the two latent z's being predicted. Both are trying to close to a normal distribution so both of them should be giving a point estimate as an output. @junyanz Sir, kindly correct me if I am wrong in my understanding. It would help me understand this wonderful paper even better. Thanks in advance :-)"
"Thank you for the amazing job!  I've run the experiment of LPIPS distance metric and found that my LPIPS distance is larger than the one in your paper. Specific speaking:  1) for random pairs of ground truth real images in the B, my average score is approximately 0.609 (in your paper it's the upperbound 0.262)  2) for 100 test images, 19 pairs of generated outputs for each single input image A (I've read the issues before, and do the experiments according to what you have spoken), my average result is 0.261 (in your paper it's 0.110).    In my experment, I just use the pretrained weight of your maps (map photo -> aerial photo) model, where input size is [1*3*256*256]. I use :   'python compute_dists_dirs.py -d0 imgs/ex_dir0 -d1 imgs/ex_dir1 -o imgs/example_dists.txt --use_gpu'  command in the above form to match the corresponding images in two dictionaries, just following the repository '  had said. The model options I use are 'net-lin','alex'.    I find my results are approximately in proportion like yours (0.609/0.262 approxiamtes 0.261/0.110), but I don't know why mine are larger than yours. I'm confused about this and looking forward to your reply, thank you very much!"
None
"Also ,I can not find the ""train_night2day.sh"" in the scripts dictionary.For I want to reproduce the training process."
"Hi,I am doing the research on night images to day images.But when I apply a pre-trained model(pix2pix),I could not find the model ""night2day.pth""from the website""  forward your reply,thanks!"
"when I run the code, although I use multi-gpu, only gpu 0 is used.  I use config below:  python ./train.py \  --gpu_ids 0,1,2,3  --batchsize 16  ............"
"I find the calculation of KL-loss  is different from VAE.   Because:  latent_loss = 0.5 * tf.reduce_sum(ã€€tf.square(z_mean) + tf.square(z_stddev)   ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€- tf.log(tf.square(z_stddev)) - 1,ã€€1ã€€)  I think the   ""self.mu.pow(2) ""  of line-154 in bicycle_gan_model.py  should be ""self.mu.pow(-2)"".   "
"<img width=""387"" alt=""1635407195(1)"" src=""   "
"Hi, I believe there is a little mistake in the way the use_same_D option is processed. In case this option is enabled, in bicycle_gan_model.py, self.backward_D is called twice, both on self.netD. Since the backward call is done twice then, without retain_graph=True, the first backward pass is overwritten, thus D is only updated on self.real_data_random and self.fake_data_random, not on self.real_data_encoded and self.fake_data_encoded. Either retain_graph=True should be used, or the backward pass should incorporate both losses like, (self.loss_D + self.loss_D2).backward()     "
"Hello,    I do not manage to test your GAN with single images ie not paired.    I did that :   python test.py --num_test 180 --dataroot datasets/Translation/val --name Translation --model bicycle_gan  --results_dir ./results/ --dataset_mode single --direction AtoB      And I've got the following results :    self.real_B = input['B' if AtoB else 'A'].to(self.device)    Thanks you for your time    "
"In bicycle_gan_model.py there is a section that enables the conditional discriminator which is as following.  `              self.fake_data_encoded = torch.cat([self.real_A_encoded, self.fake_B_encoded], 1)              self.real_data_encoded = torch.cat([self.real_A_encoded, self.real_B_encoded], 1)              self.fake_data_random = torch.cat([self.real_A_encoded, self.fake_B_random], 1)              self.real_data_random = torch.cat([self.real_A[half_size:], self.real_B_random], 1)  `    Why is self.real_A_encoded added for some and only self.real_A[half_size:] for the last one? Could you please clarify? Would help a lot, thank you!"
"Dear authors,  thanks for sharing your amazing work.    Here is my second question.   I found that your paper said you use 70*70 and 140*140 PatchGAN as D. To my knowledge, 140*140 means that each cell has a 140*140 perception field in D output.  I've calculated the perception field by myself, the 70*70 PatchGAN is exactly right, but I came out with a 141*141 perception field in 140*140 PatchGAN. To my knowledge, we can calculate perception field in this way:  !   So   l_0=1  l_1=3  l_2=3+(4-1)*2=9  l_3=9+(4-1)*2*2=21  l_4=21+(4-1)*2*2*2=45  l_5=45+(4-1)*2*2*2*2=93  l_6=93+(4-1)*2*2*2*2*1=141"
"Dear authors,  thanks for sharing your amazing work.    I've read the code, and found two questions. Here comes the first one.   In bicycle_gan_model, the last row of function 'backward_EG' specifies that we should retain graph when backwarding, I change the 'retain graph' option to false and no error occurs. I would like to ask whether we should specifies 'retain graph' option to be true?"
"Hi Junyan,    Thanks for your impressive work. Recently, I apply BicycleGAN-like framework into my GAN model, hoping it learn to generate diverse results. However, my model seems not sensitve to the latent code at all. The outputs look almost same with differnt latent code, both over training and validation stages, even I enlarge the weights of KL-loss and l1-loss on latent code. My KL_loss is becoming very small during training, but the l1-loss can only be converaged to around 0.7. Do you remember what are converaged values of these losses during your training? And kindly do you have any idea or advice of it?  Thanks you so muchï¼    Looking forward to your reply.     Best,  Lai    "
"Hi,    I think it is really an excellent work. Here I have a question about the evaluation metrics of diversity. What the paper says is that ""we compute the average distance between 1900 pairs of randomly generated output"" . Does it mens the distance is calculated by each output and the corresponding groud truth, or by each output and the input image, or by any two outputs? If the former is the caseï¼Œ what if the datasets is unpaired or have no ground truth?     Thanks "
"Dear @junyanz,  Thank you very much for the great share. I have size mismatch error when training for other than 256x256 image size. Here is the command for training the model that fails.  `python train.py --dataroot /data/random_tiles_1536 --name denoising_final_2 --model bicycle_gan --direction BtoA --netG unet_256 --norm instance --input_nc 1 --output_nc 1 --load_size 1536 --crop_size 256 --display_winsize 1536`    if I change --crop_size to any other value i.e. 1024 then shape mismatch"
"Dear @junyanz ,    Thank you very much for great paper. I have question related to distribution of latent code Z. In your code use a normal distribution (torch.randn(batch_size, nz). What happen if I use other distribution (ex: MultivariateNormal of pytorch)?"
"Hi thanks for the excellent code.    I want to use z as an extra label other than A, by initialising it with a given vector and removing the VAE loss. Some other modifications might involve using NON-VAE mode in z generations and adding extra loss between input_z and encoded_z.  Does it make any sense to you?    Thanks again."
"Hi,    I wonder whether the implementation of class D_NLayersMulti is correct. I got two problems I have trouble understanding.    1. Defining --num_ds as 2 actually uses a single discriminator since ""for i in range(1, num_D)"" in networks.py starts at 1, and thus only executes the loop a single time.  2. When using multiple discriminators, the number of filters is varied, not the amount of layers. Changing the amount of filters does nothing to the patch size, only to the complexity of the network. I believe from   this is not how using multiple discriminators is intended.    Could this please be clarified? Thanks in advance."
"Thanks for your reamarkable work first and your paper. It seems I learned a lot from your paper.    I am thinking about why cLR-GAN cause same results mentioned in your paper.  And by the way, do your results show that cVAE-GAN don't have similar result because your paper does talk about it.    According to my knowledge, cLR-GAN should have better variation but poor image fidelity in that this method has no L1 loss in target domain B. In this way, the generator in cLR-GAN would explore better in latent space and therefore it has better diversity. Becase of no L1 loss, output tend to have artifacts metntioned in pix2pix paper.    Thanks in advance."
"Hi, Junyanz    I have a little question for the L1-loss on latent code z (see the link:    Is there any reseaon why using mu2 for l1 loss, but not using the encoded z vector like that in self.encode(), which considers the logvar as well?     I would appreciate much if you could help.    Best,  Lai  "
"Dear Junyanz,  Currently, I really ambiguous about objective in your paper. The output is many styles or many images that have same only one style.    Thank you very much."
"Hi Junyan, have you trained bicycleGAN on cityscapes(Label2Photo like pix2pix)? How does it go? Can the model also generate various realistic images from label? Since I don't see any results on cityscapes dataset from your paper and this repo. Also ,could you provide recommended network params for cityscapes? Thanks!"
"Currently running the train_edges2shoes.sh script on an overclocked GTX 1070. How long did it take for you to finish and on which GPU? At current speed it will take about 15 days for me, not sure if I am doing something wrong here. GPU is active, but nowhere near fully loaded. All defaults settings used as provided by the script."
"To my understanding, the paper does not use dropout in the experiments. I feel like dropout in the generator could be useful for pix2pix for instance, to allow multiple different outputs for a single given input. All the scripts in this project use dropout by default, but I wonder why. Isn't the one-to-many mapping problem already solved by adding the latent vector?    Thanks in advance."
"I trained a task of coloration use my own dataset about 22000,last time,i can't get good result both fake_B_encoded and fake_B_random.now I add nosie into middle layer and nz=512,although fake_B_encoded is good,checkerboard artifacts of fake_B_random is very serious,I attemp to use upsampling+conv,but the effect has not improved.Is it a problem of too few datasets?,and the task of coloration is complex.so it lead the Encoder can not encode pictrue in a continuous space.could you give me some suggestion?  !   !   "
"Hi, thanks for the awesome code.  When updating E and G, only netD is set to require grad==false, but why not set netD2.  Thanks in advance."
"I trained a task of coloration use my own dataset about 22000,but the result was not very good.specially,when input random_z,the result is very bad.I've already trained about 130 epoch,should I train more epochï¼ŸCan we improve the results if we increase the weight of kl?.Could you give me some suggestion, thanks.  !   !   !         "
"Hello,    I am writing you because i had tried to use your repository named 'BicycleGAN'. Nevertheless, it has an error 'import Error: can not import name ABC'. I followed step by step the readme code but i still have the same problem. Could you please tell how can i solve it, please?    Thank you.    Yeimy  "
$ bash ./datasets/download_testset.sh edges2shoes  ./datasets/download_testset.sh: line 5: wget: command not found  mkdir: cannot create directory â€˜./datasets/edges2shoes/â€™: File exists  tar (child): ./datasets/edges2shoes.tar.gz: Cannot open: No such file or directory  tar (child): Error is not recoverable: exiting now  tar: Child returned status 2  tar: Error is not recoverable: exiting now  rm: cannot remove './datasets/edges2shoes.tar.gz': No such file or directory
None
" Hello , sorry to bother you. Could you please help me to fix this problem? When I run the command that bash ./scripts/test_edges2shoes.sh  I got the following error:    File ""/home/ysgx/.local/lib/python3.6/site-packages/torch/serialization.py"", line 368, in load      return _load(f, map_location, pickle_module)    File ""/home/ysgx/.local/lib/python3.6/site-packages/torch/serialization.py"", line 532, in _load      magic_number = pickle_module.load(f)  EOFError: Ran out of input    !   Does that mean there is something wrong with my installation of Pytorch?  I look forward to receiving your reply as soon as possible.  Thank you very much."
"Hi folks, could you try the latest commit? Hopefully, it can address your issues.    _Originally posted by @junyanz in      Yes, now its working great!! Thank you."
"Hello!      During my using timeï¼Œi found a confused bugï¼Œthat is        `RuntimeError: output with shape [1, 256, 256] doesn't match the broadcast shape [3, 256, 256]`      I don`t known why this error appearance, and can you help me to fix itï¼Ÿ      whateverï¼Œi should thank you for you great projectï¼"
!     I cann't train and use a pre-trained model because of this reason? Have any one fix it ï¼Ÿ
The way you inject noise in `UnetBlock_with_z` is possibly ineffective.     Let's consider an example:       `y` doesn't depend on `z` if `p` is equal to zero.  If `p` is equal to 1 and `s` is large (~128) then `y` changes in some weird way.      I am talking about this part of your code:   
"Hi, I am getting the following error when running the code. I am using python3.6 and have installed the latest pytorch version(1.0).  !       Could you suggest the issue involved ?  "
"First, thanks for releasing a clean and well trained code for your paper!  I tried retraining the edges2shoes model using the ""scripts/train_edges2shoes.sh"" script. However, the validation results generated by my retrained model look clearly inferior to results from your released trained model. So, are there any extra tricks used for the released trained model (e.g. longer training, different hyper parameter or loss settings, ... etc)?  Details: I trained using a single Quadro P6000 GPU (with cuda 9.0 and python 2.7) for the full 60 epochs without changing your code/scripts."
"When testing my own network trained with BicycleGAN, sometimes there appears an error termed ""LeakyReLU object has no attribute conv"". What is the reason for this behaviour and which part of the code should be adapted to alleviate this error? I suppose that there might be a mistake when defining the paths in the testing script.    Additionally, how should the testing script be adapted when the input images are 512x512 pixels?"
"I tried to classify some images with pix2pix + CycleGAN and with BicycleGAN. In pix2pix the best results occured for the batch size of 1. However, in BicycleGAN I get an error that the batch size should be even. When removing the corresponding assertion, I get a cuDNN error due to a bad parameter. Is the assertion necessary for aligned datasets?"
"As far as I understood the paper, it is possible to add images and additional information to G and D in BicycleGAN. Is this possible to achieve this by using the add_to_input function? If so, where do I set the number and the path of the additional information? Otherwise, which function should be used for this purpose?    Thank you in advance."
"I have an issue regarding the execution of BicycleGAN on the CPU. As far as I understood the code, the parameter gpu_ids has to be set to '-1'. How can I achieve this by running the script train_edges2shoes.sh?    In other words, is there a command similar to: ""bash ./scripts/train_edges2shoes.sh --gpu_ids=-1"" (without quotation marks) which allows us to run BicycleGAN on the CPU?    Thank you in advance."
"I noted that there is a function cal_gradient_penalty in your code, but it seems that you didn't use it?"
"When ran the code on a single GPU, I got an error:     I have checked the installation and configuration, both of them are correct.  So, do you have any ideas for the problem?"
I am using training dataset of 203 samples and validation of 70 samples.  Is this error due to very less amount of samples?  !   
"We are trying to install your package but run into a couple problems.  What we did is that we install vscode and pyhton plugins. and we made sure its run on conda.  it couldnt process bash command ('bash' is not recognized as an internal or external command, operable program or batch file.)  . So we insstall  Cygwin, and enable linux subsystem feature, enable bash, download libusbwin32 and use C:\WINDOWS\system32>lxrun /install in cmd prompt.   Now when i try to bash install_conda VScode prompts me ""/bin/bash: /scripts/install_conda.sh: No such file or directory"" even though im sure im putting the correct path for forementioned script"
"Hi,    In Section 4 Implementation Details   > **Training details** ...  > We also find that not conditioning the discriminator D on input A leads to  > better results (also discussed in [34]), ...    Does this means that the discriminator has no information to ensure the generated image to be conditioned on image A?   Say we are generating shoes from edges. Being unconditioned on the input image (edges), the discriminator should only be able to tell if the generated shoes are real/fake, but not able to tell if the generated shoes doesn't match the conditions (edge)?  Or some other mechanism is working on the condition part?    Looking forward to your reply, thanks! Nice work!    "
"Hi, thanks for your amazing work.  I have trained the model with my own dataset, and generated multi-style samples corresponding input with different latent code z.    However, I felt it is difficult to generate the specific style sample. It is very painful that read the mapping of latent code z.  Is it possible to disentangle latent code z by adding mutual information?  Or there is any method to solve this?"
"I wonder why you used relu as a non-linear function as the default option. I know many models are using leaky relu, is there any reason why relu is the default option? Has the experiment proved better results?"
"Hi, in your commit     there are lots of `import pdb; pdb.set_trace()` debugger statement within code       I am not sure if this is intended, or just forgot to remove them after debug?"
"Hi, thanks for your amazing work. I have a problem with the multi-gpu support. The original batchsize is 2 for a real image and another random. In fact it's one pair. If we want to use multi-gpu for faster training, the batch size should be larger. But simply changing the batchsize causes error. May I know is it easy to modify the code for this purpose? "
"Dear authors,  thanks for your amazing work.     I have a question about the value of the lambdas in the loss function. I understood that they define a certain balance between converging to several images close to the original B, and diverging a bit from the original B.    By default, you chose:    lambda_L1=10.0  lambda_GAN=1.0  lambda_GAN2=1.0  lambda_z=0.5  lambda_kl=0.01    I trained the bicycle gan model with the default parameters. My dataset consist of 90 000 images. Meanwhile some result are impressive, the majority remain a bit blurry (they don t follow the same probabilty distribution as in my original dataset). I guess the lambda_L1 is not high enough to force the network to generate images that follow the original probability distribution. What result could you get if you choose a lambda_L1=100 such as in pix2pix ?"
"Hi, thanks for your amazing work.  I train the network with my own database, it works well.  However, I don't have so many data for training(just about 700 images).  My question is   1. How many data should I put in the val folder? Or, the val folder is just for testing the network?   2. Besides the paired data, I also have some unpaired data, for example, (Many B but lack of A, training direction: A2B), let me know if you are consider an algorithm for such semi-supervised situation.     Thanks."
"Hi!    Thank you for sharing this wonderful work.  I have one simple question.  In the result visualizations, what is the meaning of  fake_B_random and fake_B_encoded?    Thank you"
"Hi,   Thanks for sharing this amazing work!  I have tried to apply BicycleGAN into MRI image translation tasks, and it works well!  Now, I am trying to change the network as a multiple inputs network. My idea is by given multiple corresponding inputs, the output will be more realistic and accurate.   Do you think this is possible to achieve based on bicycleGAN?"
"Hi,   Thank you very much for sharing this fantastic work!  I would like to train a model,  cLR-GAN or cVAE-GAN.  I think the default setting is MODEL='bicycle_gan'.  Is there any way to train each model?    Thank you!      "
"Hey Jun-Yan, thanks for putting this repo together.  I'm trying to train it on piano roll data and have been seeing unexpected behavior: the generator outputs the same image even though the conditions, i.e. noise vector and real A, change.    Any thoughts on what it could be? I've added the loss log, options, output images during training and output images during inference.            Model outputs during training(fake_b_encoded, fake_b_random, real_a_encoded,real_b_encoded)  !  !  !  !        Model outputs during inference(real_a, real_b, fake_b)    !   "
I'm getting a size mismatch when trying to load a saved model. Training works fine.  I compared training and test OPTs and can't find any difference that would produce the error below.             
"Your work gets surprising results and I expect to reproduce the data of LPIPS distance that you list in Figure6. Given one input image, you sample 19 outputs. For every input(maps), do you calculate the LPIPS distance between the given image(maps) and corresponding 19 samples(satellite) ? After that, you sum those 19 groups of data and have a average? Is it the same to other 99 input images in your experiment ? I'm confused about this and looking forward to your reply, thank you!"
"For the line below, as the batch size =2. So the two images in **self.real_B** are different.    self.real_B_random = self.real_B[**half_size:**]              it seems that the D tries to distinguish between a fake encoded B and a real image, but from a different image file in the code above.    so why not use self.real_B_random = self.real_B[**0:half_size**] which is the same image instead.    if the 2 images are very random: the 1st encoded image is from a ""sneaker"" and the 2nd real random image is a ""high heel"", then such a way is benefit to improve D?      "
"I have a question about the code of discriminator.  In Class D_NLayersMulti  it uses parallel_forward.    But I can't get the point of why using pareallel_forward. Besides, the paper said it used Patch-GAN, however, I could not find any codes to create Patch-GAN."
"I sent a email to the pylaffont@cs.brown.edu, but failed to get the link of the database ,can you send a link of this database to my email  andyliu281@gmail.com ,Thank you"
"Dear sir,When I run the script:bash ./scripts/train_edges2shoes.sh,the following RuntimeError occurs.  (epoch: 1, iters: 49400, time: 0.311) , z_encoded_mag: 0.577, G_total: 4.293, G_L1_encoded: 2.367, z_L1: 0.259, KL: 0.076, G_GAN: 1.002, D_GAN: 0.498, G_GAN2: 0.589, D_GAN2: 0.988  (epoch: 1, iters: 49600, time: 0.322) , z_encoded_mag: 0.409, G_total: 2.001, G_L1_encoded: 0.385, z_L1: 0.302, KL: 0.069, G_GAN: 0.794, D_GAN: 0.960, G_GAN2: 0.450, D_GAN2: 1.137  (epoch: 1, iters: 49800, time: 0.311) , z_encoded_mag: 0.939, G_total: 3.441, G_L1_encoded: 1.597, z_L1: 0.373, KL: 0.079, G_GAN: 0.833, D_GAN: 0.774, G_GAN2: 0.560, D_GAN2: 1.015  skip this point data_size = 1  Traceback (most recent call last):    File ""./train.py"", line 28, in        model.update_G()    File ""/home/rharad/junyanz/BicycleGAN/models/bicycle_gan_model.py"", line 148, in update_G      self.backward_EG()    File ""/home/rharad/junyanz/BicycleGAN/models/bicycle_gan_model.py"", line 114, in backward_EG      self.loss_G.backward(retain_graph=True)    File ""/home/rharad/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py"", line 167, in backward      torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)    File ""/home/rharad/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 99, in backward      variables, grad_variables, retain_graph)  RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.    How can I solve this problem....."
when I run command that bash ./scripts/train_edges2shoes.sh  I got following  error  RuntimeError: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1518243271935/work/torch/lib/THC/generic/THCStorage.cu:58  how to solve this problem?  
"Is possible to generate labels from facades images?  We tried both --which_direction B2A and --which_direction A2B, but the results are the same, is there anything I missed?  Thanks a lot!"
"`python train.py --dataroot ./datasets/maps --name maps_bicyclegan --model bicycle_gan --display_id 0 --nThread 0 --loadSize 512 --fineSize 512 --display_winsize 512 --which_direction 'AtoB' --use_dropout --gpu_ids 0`         tested 256x256 works fine, but any other size would raise the exception above   I am not good enough to fix this on my own after hours ...  "
Can I use my own data for training?
"line 682 and 683 in class E_ResNet in models/networks.py :  input_ndf = ndf * min(max_ndf, n)  # 2**(n-1)  output_ndf = ndf * min(max_ndf, n+1)  # 2**n    The code and code comment are not consistent. The multiplier of the number of first filters is not growing exponentially."
In this   could you explain why ` torch.mean(torch.abs(self.mu2 - self.z_random))` not ` torch.mean(torch.abs(self.z_predict - self.z_random))`?
None
"Hi, I am really shocked by the awesome work you guys achieved, but yet I get confused about ""How BicycleGAN's result differs from pix2pix's result and cyclegan's""    1. Does BicycleGAN give better result at edge2shoes than pix2pix with same training sets and epochs ?  2. Does BicycleGAN trains faster using bidirectional cycle-consistency losses ?  3. In what condition BicycleGAN is better than ... others , and when does not ?"
"Hello! When I run the test_edges2handbags.sh, I met an error. What's the problem?    model [BiCycleGANModel] was created  Loading model bicycle_gan  process input image 000/010  Traceback (most recent call last):    File ""./test.py"", line 41, in        model.set_input(data)    File ""/home/jaheimlee/gitrep/BicycleGAN/models/base_model.py"", line 193, in set_input      self.input_A.resize_(input_A.size()).copy_(input_A)  RuntimeError: calling resize_ on a tensor that has non-resizable storage. Clone it first or create a new tensor instead.  "
"I've noticed that `self.fc = nn.Sequential(*[nn.Linear(output_ndf, output_nc)])` or `self.fcVar = nn.Sequential(*[nn.Linear(output_ndf, output_nc)])` is the last layer of E_ResNet, depending on vaeLike is True or not. So why doesn't it need an activation function before being output?"
It seems that D_NLayersMulti has two discriminators while D_NLayers has only one. But how do two discriminators work?
"It is also ""Unsupervised or Unpaird Image to Image translation""  like CycleGAN ?   When I read the paper roughly, I thought it was supervised.    but I read the code, It is like unsupervised ... "
"If yes, please let us know how to."
"     Hi, Junyan,  I am confused why the latent L1 loss is only used to optimize G only, but not used to optimize E?    By the way     ` self.fake_data_random = torch.cat([self.real_A_encoded, self.fake_B_random], 1)`   should be   `self.fake_data_random = torch.cat([self.real_A_random, self.fake_B_random], 1)`.  Right?"
hello junyanz  Can you share day-night for me?  thank you very much
"Tried training (based on my experience with pytorch-CycleGAN-pix2pix), my pytorch setup is based on python 3.6. Got error ""slice indices must be integers or None or have an __index__ method"" because of the division at line      I suggest adding explicit conversion to int at this line."
"flake8 testing of   on Python 2.7.14    $ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__     "
!   How to solve this problemï¼Œthank you very much. 
"Hi there, I download this code and adapted them for my semi-supervised segmentation (Pytorch version). And thanks for this genius code you provided!    But I have a question is that I know that mean-teacher model contains two loss, one is for unsupervised loss for paired labeled data and ground-truth, and the other is for contrast loss.     And here is the contrast loss I calculated:  1. get the consistency weight by ` 10 * sigmoid_rampup(epoch, 5)` at each epoch  2. compute the logits from student and teacher's output    And then I got contrast loss up to **thousands** and it seems not right. Is it normal or some bug in my code?  Could you give me some advice if you have some idea? Thanks!"
None
I would be very grateful if someone show me how to use this code
"I want to transfer the MT framework to a NLP task but I don't understand how to train it with unlabeled data. I have got the idea of the paper, but i'm confusing about the implementation.            if isinstance(model_out, Variable):              assert args.logit_distance_cost  = 0:              class_logit, cons_logit = logit1, logit2              res_loss = args.logit_distance_cost * residual_logit_criterion(class_logit, cons_logit) / minibatch_size              meters.update('res_loss', res_loss.data[0])          else:              class_logit, cons_logit = logit1, logit1              res_loss = 0            class_loss = class_criterion(class_logit, target_var) / minibatch_size          meters.update('class_loss', class_loss.data[0])            ema_class_loss = class_criterion(ema_logit, target_var) / minibatch_size          meters.update('ema_class_loss', ema_class_loss.data[0])            if args.consistency:              consistency_weight = get_current_consistency_weight(epoch)              meters.update('cons_weight', consistency_weight)              consistency_loss = consistency_weight * consistency_criterion(cons_logit, ema_logit) / minibatch_size              meters.update('cons_loss', consistency_loss.data[0])          else:              consistency_loss = 0              meters.update('cons_loss', 0)    I notice that the `TwoStreamBatchSampler` divides the dataset into labeled part and unlabeled part, but the code above seems handles both labeled and unlabeled data in a universal way. I think only the labeled part of `model_out` should be used to calculate the `class_loss`. Did I get it wrong?"
"Hi, The dataset preparation command in pytroch version of mean-teacher:  `pip install git+ssh://git@github.com/pytorch/vision@c31c3d7e0e68e871d2128c8b731698ed3b11b119`   is no longer exist.  Hope you would update the dataset link later.  Thank you!"
"I am planning to apply mean-teacher for my problem of token classification. Since adding different noise for teacher and student is really important for the approach, i am confused about how to calculate consistency cost as length of active logits would differ. for e.g. if i use synonym noise then it can happen that it increases the length of the sentence (some tokens maybe replaces by synonym of len 2) when given to teacher model and same augmentation may generate different sentence(of different length) when given to student model."
None
"When I tried to run this code on CIFAR-10, I discovered several problems caused by version incompatibilty, for example, the parameter 'async = true' cannot be set; the function .tomsgpack() didn't work cuz msgpack has already been removed in recent pandas.  Could anyone plz post a list of the version(s) of packages used in the project? (e.g. python, pandas, pytorch) Thanks~~"
"For reference,  <img width=""787"" alt=""Screenshot 2021-06-11 at 10 54 24 PM"" src=""   "
"Thank you for sharing this project.     I run the cifar10_test.py with the default configuration, and the accuracy of training data is about 50% for top 1and 90% for top 5.     However, for validation data, the accuracy is very low, and the accuracy is 0.154% for top1/5. Please have a look for the following figure.     !     Any suggestion is appreciated!"
"In the paper, it is said alpha should be 0.99 at the beginning (when global_step is small) and should be 0.999 at the end (when global_step is large), however, in the code:    alpha = min(1 - 1 / (global_step + 1), alpha)    following this, alpha is 0 when global_step is small, and is alpha (this is set as 0.99 from parameters) when global_step is >99. The code seems different what the paper presented. The paper indicates a code of     alpha = max(1 - 1 / (global_step + 1), alpha)    does anyone find issues here?"
"Hello, I am reproducing your work as a baseline now.  I come up with a problem that the precision (top1&top%) in the test process is always zero.  During training, the performance of this model is nomal, like  _""INFO:main:Epoch: [13][170/226]  Time 0.329 (0.339)      Data 0.000 (0.006)      Class 0.2847 (0.2877)   Cons 0.0917 (0.0787)    Prec@1 60.000 (58.871)  Prec@5 62.000 (61.994)""_  But in test, it performs like  _""INFO:main:Test: [10/20] Time 0.100 (0.242)      Data 0.000 (0.143)      Class 1.8608 (1.7321)   Prec@1 0.000 (0.000)    Prec@5 0.000 (0.000)""_    Cloud you plz help me to solve this problem?Thank you so much."
"Exception in thread Thread-1:  Traceback (most recent call last):    File ""/home/anaconda3/lib/python3.6/threading.py"", line 916, in _bootstrap_inner      self.run()    File ""/home//anaconda3/lib/python3.6/threading.py"", line 864, in run      self._target(*self._args, **self._kwargs)    File ""/home/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py"", line 139, in _serve      signal.pthread_sigmask(signal.SIG_BLOCK, range(1, signal.NSIG))    File ""/home/anaconda3/lib/python3.6/signal.py"", line 60, in pthread_sigmask      sigs_set = _signal.pthread_sigmask(how, mask)  ValueError: signal number 32 out of range    I want to know what version of python you are using.  "
"I do not know why it happend.    /home/lts/.conda/envs/PCL/bin/python /home/lts/PycharmProject/mean-teacher/pytorch/main.py  INFO:main:=> creating model 'cifar_shakeshake26'  INFO:main:=> creating EMA model 'cifar_shakeshake26'  INFO:main:  List of model parameters:  =========================  module.conv1.weight                            16 * 3 * 3 * 3 =         432  module.layer1.0.conv_a1.weight                96 * 16 * 3 * 3 =      13,824  module.layer1.0.bn_a1.weight                               96 =          96  module.layer1.0.bn_a1.bias                                 96 =          96  module.layer1.0.conv_a2.weight                96 * 96 * 3 * 3 =      82,944  module.layer1.0.bn_a2.weight                               96 =          96  module.layer1.0.bn_a2.bias                                 96 =          96  module.layer1.0.conv_b1.weight                96 * 16 * 3 * 3 =      13,824  module.layer1.0.bn_b1.weight                               96 =          96  module.layer1.0.bn_b1.bias                                 96 =          96  module.layer1.0.conv_b2.weight                96 * 96 * 3 * 3 =      82,944  module.layer1.0.bn_b2.weight                               96 =          96  module.layer1.0.bn_b2.bias                                 96 =          96  module.layer1.0.downsample.0.weight           96 * 16 * 1 * 1 =       1,536  module.layer1.0.downsample.1.weight                        96 =          96  module.layer1.0.downsample.1.bias                          96 =          96  module.layer1.1.conv_a1.weight                96 * 96 * 3 * 3 =      82,944  module.layer1.1.bn_a1.weight                               96 =          96  module.layer1.1.bn_a1.bias                                 96 =          96  module.layer1.1.conv_a2.weight                96 * 96 * 3 * 3 =      82,944  module.layer1.1.bn_a2.weight                               96 =          96  module.layer1.1.bn_a2.bias                                 96 =          96  module.layer1.1.conv_b1.weight                96 * 96 * 3 * 3 =      82,944  module.layer1.1.bn_b1.weight                               96 =          96  module.layer1.1.bn_b1.bias                                 96 =          96  module.layer1.1.conv_b2.weight                96 * 96 * 3 * 3 =      82,944  module.layer1.1.bn_b2.weight                               96 =          96  module.layer1.1.bn_b2.bias                                 96 =          96  module.layer1.2.conv_a1.weight                96 * 96 * 3 * 3 =      82,944  module.layer1.2.bn_a1.weight                               96 =          96  module.layer1.2.bn_a1.bias                                 96 =          96  module.layer1.2.conv_a2.weight                96 * 96 * 3 * 3 =      82,944  module.layer1.2.bn_a2.weight                               96 =          96  module.layer1.2.bn_a2.bias                                 96 =          96  module.layer1.2.conv_b1.weight                96 * 96 * 3 * 3 =      82,944  module.layer1.2.bn_b1.weight                               96 =          96  module.layer1.2.bn_b1.bias                                 96 =          96  module.layer1.2.conv_b2.weight                96 * 96 * 3 * 3 =      82,944  module.layer1.2.bn_b2.weight                               96 =          96  module.layer1.2.bn_b2.bias                                 96 =          96  module.layer1.3.conv_a1.weight                96 * 96 * 3 * 3 =      82,944  module.layer1.3.bn_a1.weight                               96 =          96  module.layer1.3.bn_a1.bias                                 96 =          96  module.layer1.3.conv_a2.weight                96 * 96 * 3 * 3 =      82,944  module.layer1.3.bn_a2.weight                               96 =          96  module.layer1.3.bn_a2.bias                                 96 =          96  module.layer1.3.conv_b1.weight                96 * 96 * 3 * 3 =      82,944  module.layer1.3.bn_b1.weight                               96 =          96  module.layer1.3.bn_b1.bias                                 96 =          96  module.layer1.3.conv_b2.weight                96 * 96 * 3 * 3 =      82,944  module.layer1.3.bn_b2.weight                               96 =          96  module.layer1.3.bn_b2.bias                                 96 =          96  module.layer2.0.conv_a1.weight               192 * 96 * 3 * 3 =     165,888  module.layer2.0.bn_a1.weight                              192 =         192  module.layer2.0.bn_a1.bias                                192 =         192  module.layer2.0.conv_a2.weight              192 * 192 * 3 * 3 =     331,776  module.layer2.0.bn_a2.weight                              192 =         192  module.layer2.0.bn_a2.bias                                192 =         192  module.layer2.0.conv_b1.weight               192 * 96 * 3 * 3 =     165,888  module.layer2.0.bn_b1.weight                              192 =         192  module.layer2.0.bn_b1.bias                                192 =         192  module.layer2.0.conv_b2.weight              192 * 192 * 3 * 3 =     331,776  module.layer2.0.bn_b2.weight                              192 =         192  module.layer2.0.bn_b2.bias                                192 =         192  module.layer2.0.downsample.conv.weight       192 * 96 * 1 * 1 =      18,432  module.layer2.0.downsample.conv.bias                      192 =         192  module.layer2.0.downsample.bn.weight                      192 =         192  module.layer2.0.downsample.bn.bias                        192 =         192  module.layer2.1.conv_a1.weight              192 * 192 * 3 * 3 =     331,776  module.layer2.1.bn_a1.weight                              192 =         192  module.layer2.1.bn_a1.bias                                192 =         192  module.layer2.1.conv_a2.weight              192 * 192 * 3 * 3 =     331,776  module.layer2.1.bn_a2.weight                              192 =         192  module.layer2.1.bn_a2.bias                                192 =         192  module.layer2.1.conv_b1.weight              192 * 192 * 3 * 3 =     331,776  module.layer2.1.bn_b1.weight                              192 =         192  module.layer2.1.bn_b1.bias                                192 =         192  module.layer2.1.conv_b2.weight              192 * 192 * 3 * 3 =     331,776  module.layer2.1.bn_b2.weight                              192 =         192  module.layer2.1.bn_b2.bias                                192 =         192  module.layer2.2.conv_a1.weight              192 * 192 * 3 * 3 =     331,776  module.layer2.2.bn_a1.weight                              192 =         192  module.layer2.2.bn_a1.bias                                192 =         192  module.layer2.2.conv_a2.weight              192 * 192 * 3 * 3 =     331,776  module.layer2.2.bn_a2.weight                              192 =         192  module.layer2.2.bn_a2.bias                                192 =         192  module.layer2.2.conv_b1.weight              192 * 192 * 3 * 3 =     331,776  module.layer2.2.bn_b1.weight                              192 =         192  module.layer2.2.bn_b1.bias                                192 =         192  module.layer2.2.conv_b2.weight              192 * 192 * 3 * 3 =     331,776  module.layer2.2.bn_b2.weight                              192 =         192  module.layer2.2.bn_b2.bias                                192 =         192  module.layer2.3.conv_a1.weight              192 * 192 * 3 * 3 =     331,776  module.layer2.3.bn_a1.weight                              192 =         192  module.layer2.3.bn_a1.bias                                192 =         192  module.layer2.3.conv_a2.weight              192 * 192 * 3 * 3 =     331,776  module.layer2.3.bn_a2.weight                              192 =         192  module.layer2.3.bn_a2.bias                                192 =         192  module.layer2.3.conv_b1.weight              192 * 192 * 3 * 3 =     331,776  module.layer2.3.bn_b1.weight                              192 =         192  module.layer2.3.bn_b1.bias                                192 =         192  module.layer2.3.conv_b2.weight              192 * 192 * 3 * 3 =     331,776  module.layer2.3.bn_b2.weight                              192 =         192  module.layer2.3.bn_b2.bias                                192 =         192  module.layer3.0.conv_a1.weight              384 * 192 * 3 * 3 =     663,552  module.layer3.0.bn_a1.weight                              384 =         384  module.layer3.0.bn_a1.bias                                384 =         384  module.layer3.0.conv_a2.weight              384 * 384 * 3 * 3 =   1,327,104  module.layer3.0.bn_a2.weight                              384 =         384  module.layer3.0.bn_a2.bias                                384 =         384  module.layer3.0.conv_b1.weight              384 * 192 * 3 * 3 =     663,552  module.layer3.0.bn_b1.weight                              384 =         384  module.layer3.0.bn_b1.bias                                384 =         384  module.layer3.0.conv_b2.weight              384 * 384 * 3 * 3 =   1,327,104  module.layer3.0.bn_b2.weight                              384 =         384  module.layer3.0.bn_b2.bias                                384 =         384  module.layer3.0.downsample.conv.weight      384 * 192 * 1 * 1 =      73,728  module.layer3.0.downsample.conv.bias                      384 =         384  module.layer3.0.downsample.bn.weight                      384 =         384  module.layer3.0.downsample.bn.bias                        384 =         384  module.layer3.1.conv_a1.weight              384 * 384 * 3 * 3 =   1,327,104  module.layer3.1.bn_a1.weight                              384 =         384  module.layer3.1.bn_a1.bias                                384 =         384  module.layer3.1.conv_a2.weight              384 * 384 * 3 * 3 =   1,327,104  module.layer3.1.bn_a2.weight                              384 =         384  module.layer3.1.bn_a2.bias                                384 =         384  module.layer3.1.conv_b1.weight              384 * 384 * 3 * 3 =   1,327,104  module.layer3.1.bn_b1.weight                              384 =         384  module.layer3.1.bn_b1.bias                                384 =         384  module.layer3.1.conv_b2.weight              384 * 384 * 3 * 3 =   1,327,104  module.layer3.1.bn_b2.weight                              384 =         384  module.layer3.1.bn_b2.bias                                384 =         384  module.layer3.2.conv_a1.weight              384 * 384 * 3 * 3 =   1,327,104  module.layer3.2.bn_a1.weight                              384 =         384  module.layer3.2.bn_a1.bias                                384 =         384  module.layer3.2.conv_a2.weight              384 * 384 * 3 * 3 =   1,327,104  module.layer3.2.bn_a2.weight                              384 =         384  module.layer3.2.bn_a2.bias                                384 =         384  module.layer3.2.conv_b1.weight              384 * 384 * 3 * 3 =   1,327,104  module.layer3.2.bn_b1.weight                              384 =         384  module.layer3.2.bn_b1.bias                                384 =         384  module.layer3.2.conv_b2.weight              384 * 384 * 3 * 3 =   1,327,104  module.layer3.2.bn_b2.weight                              384 =         384  module.layer3.2.bn_b2.bias                                384 =         384  module.layer3.3.conv_a1.weight              384 * 384 * 3 * 3 =   1,327,104  module.layer3.3.bn_a1.weight                              384 =         384  module.layer3.3.bn_a1.bias                                384 =         384  module.layer3.3.conv_a2.weight              384 * 384 * 3 * 3 =   1,327,104  module.layer3.3.bn_a2.weight                              384 =         384  module.layer3.3.bn_a2.bias                                384 =         384  module.layer3.3.conv_b1.weight              384 * 384 * 3 * 3 =   1,327,104  module.layer3.3.bn_b1.weight                              384 =         384  module.layer3.3.bn_b1.bias                                384 =         384  module.layer3.3.conv_b2.weight              384 * 384 * 3 * 3 =   1,327,104  module.layer3.3.bn_b2.weight                              384 =         384  module.layer3.3.bn_b2.bias                                384 =         384  module.fc1.weight                                    10 * 384 =       3,840  module.fc1.bias                                            10 =          10  module.fc2.weight                                    10 * 384 =       3,840  module.fc2.bias                                            10 =          10  ===========================================================================  all parameters                                   sum of above =  26,197,316    /home/lts/.conda/envs/PCL/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.    warnings.warn(warning.format(ret))  /home/lts/PycharmProject/mean-teacher/pytorch/main.py:224: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.    ema_input_var = torch.autograd.Variable(ema_input, volatile=True)  INFO:main:Epoch: [0][0/22000] Time 10.247 (10.247) Data 3.054 (3.054) Class 2.2228 (2.2228) Cons 0.0015 (0.0015) Prec@1 3.000 (3.000) Prec@5 51.000 (51.000)  Traceback (most recent call last):    File ""/home/lts/PycharmProject/mean-teacher/pytorch/main.py"", line 426, in        main(RunContext(__file__, 0))    File ""/home/lts/PycharmProject/mean-teacher/pytorch/main.py"", line 105, in main      train(train_loader, model, ema_model, optimizer, epoch, training_log)    File ""/home/lts/PycharmProject/mean-teacher/pytorch/main.py"", line 311, in train      **meters.sums()    File ""/home/lts/PycharmProject/mean-teacher/pytorch/mean_teacher/run_context.py"", line 34, in record      self._record(step, col_val_dict)    File ""/home/lts/PycharmProject/mean-teacher/pytorch/mean_teacher/run_context.py"", line 45, in _record      self.save()    File ""/home/lts/PycharmProject/mean-teacher/pytorch/mean_teacher/run_context.py"", line 38, in save      df.to_msgpack(self.log_file_path, compress='zlib')    File ""/home/lts/.conda/envs/PCL/lib/python3.6/site-packages/pandas/core/generic.py"", line 1320, in to_msgpack      **kwargs)    File ""/home/lts/.conda/envs/PCL/lib/python3.6/site-packages/pandas/io/packers.py"", line 154, in to_msgpack      writer(fh)    File ""/home/lts/.conda/envs/PCL/lib/python3.6/site-packages/pandas/io/packers.py"", line 150, in writer      fh.write(pack(a, **kwargs))    File ""/home/lts/.conda/envs/PCL/lib/python3.6/site-packages/pandas/io/packers.py"", line 691, in pack      use_bin_type=use_bin_type).pack(o)    File ""pandas/io/msgpack/_packer.pyx"", line 230, in pandas.io.msgpack._packer.Packer.pack (pandas/io/msgpack/_packer.cpp:3642)    File ""pandas/io/msgpack/_packer.pyx"", line 232, in pandas.io.msgpack._packer.Packer.pack (pandas/io/msgpack/_packer.cpp:3484)    File ""pandas/io/msgpack/_packer.pyx"", line 191, in pandas.io.msgpack._packer.Packer._pack (pandas/io/msgpack/_packer.cpp:2605)    File ""pandas/io/msgpack/_packer.pyx"", line 220, in pandas.io.msgpack._packer.Packer._pack (pandas/io/msgpack/_packer.cpp:3178)    File ""pandas/io/msgpack/_packer.pyx"", line 191, in pandas.io.msgpack._packer.Packer._pack (pandas/io/msgpack/_packer.cpp:2605)    File ""pandas/io/msgpack/_packer.pyx"", line 220, in pandas.io.msgpack._packer.Packer._pack (pandas/io/msgpack/_packer.cpp:3178)    File ""pandas/io/msgpack/_packer.pyx"", line 227, in pandas.io.msgpack._packer.Packer._pack (pandas/io/msgpack/_packer.cpp:3348)  TypeError: can't serialize tensor(62, device='cuda:0')    Process finished with exit code 1  "
"Hi! Awesome work by the way.  I am curious from which part of your code let you get (input, ema_input).  The TwoStreamBatchSampler seem to return a single batch (not as a tuple of two batches). I am just wondering from which part of the code let the DataLoader return a tuple of (input, ema_input)  Looking forward to your reply.  Thanks in advance!"
"Hello,  Why the models have two fc layers and two outputs? I don't think it's necessary.  Consistency_loss can also be calculated by class_logit and ema_logit.  What's the difference between class_logit and cons_logit?"
"Hello!  There is one function that I don't understand in /mean_teacher/utils.py  `def export(fn):      mod = sys.modules[fn.__module__]      if hasattr(mod, '__all__'):          mod.__all__.append(fn.__name__)      else:          mod.__all__ = [fn.__name__]      return fn`    Look forward for your reply, thank you so much."
python 3.6  cuda9.0  pytorch0.4
"Hi, i am wonder about this loss:  ` class_loss = class_criterion(class_logit, target_var) / minibatch_size `  since this loss ignore some samples(no_lable), why here still use the minibatch_size not the labeled_size?"
"Thanks for your codes. I had to admit it's a wonderful strategy.   However, when I use this package on the action recognition dataset Stanford40, I encounter the loss explosion problem, so I am thinking about using pre-trained model.   I had decreased the classes from 40 to 10. and turned the mode to fully supervised learning with exclude_unlabled as 'True'. Hope you have time to give a reply even a little hint.   Here I print out the loss at each step until loss explosion. The Res Loss increase like crazy.   AssertionError: Loss explosion: 226970.828125  0 batch  class Variable containing:   2.3374  const Variable containing:  1.00000e-02 *    2.4998  res Variable containing:  1.00000e-02 *    1.0730  1 batch  class Variable containing:   12.6847  const Variable containing:   275.5649  res Variable containing:  1.00000e+05 *    2.2668"
"Hi, the code will be update for a new version of pytorch?    I'm trying to do this by my own, but I'm new to pytorch and finding some issues. After changing the .data to item, I try to run the code but receive RuntimeError: cuDNN error: CUDNN_STATUS_BAD_PARAM"
"I trained the ResNet architecture (cifar_shakeshake26 in Pytorch version) on cifar-10 dataset with 1000 unlabeled images and 44000 labeled images (the resting 5000 images are used for validation) for about 180 epochs, setting the bach-size 256, labeled batch-size 62.   But I observed that the validation precision (top 1) would first rise from 43% up to 50% and then fall to only 13% (began to fall after about 10 epochs) along the training process. I was so puzzled about this phenomenon. Besides, the precision in training always rise and never fall, why the validation precision would fall??"
mean-teacher has not   b as given in Original   and its   that you have  . Is it intentional?
"I am talking about  `self.mean_cons_cost_mt, self.cons_costs_mt = consistency_costs(  self.cons_logits_1, self.class_logits_ema, self.cons_coefficient, consistency_mask, self.hyper['consistency_trust'])`   What is the difference between using 'self.class_logits_ema' and using  'self.cons_logits_ema' as consistency targets?"
"Great paper!    Tensorflow documentation says the EMA variables are created with (trainable=False) and added to the GraphKeys.ALL_VARIABLES collection. Now as they are not trainable they wont have the gradient applied on them, i understand that. But, as they depend upon the current trainable variables of the graph, and hence so do the predictions of the teacher network; an additional gradient will flow to the trainable variables because of ema being dependent upon them. Is  this correct understnading of implementation?"
"Hi, I found that the teacher model's weights seem to be not updated as it performed as bad as it was first initialized.     `alpha = min(1 - 1 / (global_step + 1), alpha)    for ema_param, param in zip(ema_model.parameters(), model.parameters()):    ema_param.data.mul_(alpha).add_(1 - alpha, param.data)`    Shouldn't this be `ema_param.data.mul_(alpha).add_((1 - alpha)*param.data)` ?    here are the parameters printed out during training:  ('teacher_p: ', Parameter containing:  tensor([ 0.0007, -0.0006, 0.0046, -0.0033, 0.0004, 0.0262, 0.0153, -0.0259,  -0.0115, -0.0015, -0.0117, -0.0060, 0.0161, 0.0104, 0.0080, -0.0015,  -0.0116, -0.0160, 0.0247, -0.0227, 0.0077, 0.0052, 0.0217, 0.0111,  -0.0036, -0.0176, -0.0188, 0.0026, -0.0163, 0.0155],  device='cuda:0'))  ('student_p: ', Parameter containing:  tensor([-0.0322, -0.0153, 0.0206, -0.0212, -0.0274, 0.0293, 0.0225, -0.0279,  -0.0272, -0.0282, -0.0272, -0.0261, 0.0275, 0.0261, 0.0274, -0.0251,  0.0014, -0.0285, 0.0296, -0.0296, 0.0105, -0.0209, 0.0123, 0.0227,  -0.0162, -0.0081, -0.0079, -0.0233, -0.0145, 0.0030],  device='cuda:0', requires_grad=True))  ('(after) teacher_p: ', Parameter containing:  tensor([ 0.0007, -0.0006, 0.0046, -0.0033, 0.0004, 0.0262, 0.0153, -0.0259,  -0.0115, -0.0016, -0.0117, -0.0060, 0.0161, 0.0104, 0.0080, -0.0015,  -0.0116, -0.0160, 0.0247, -0.0227, 0.0077, 0.0052, 0.0217, 0.0111,  -0.0036, -0.0176, -0.0187, 0.0026, -0.0163, 0.0155],  device='cuda:0'))"
"Hi, thank you for your great project!    Iâ€™m stuck with two problems while trying to test the mean teacher idea as described in your NIPS 2017 presentation with a MNIST dataset and a simple convnet from official Pytorch examples using your Pytorch code:    1. Loss is defined as:    `loss = class_loss + consistency_loss + res_loss`    where      but default value of `args.consistency` is None, so `consistency_loss=0` by default    Similarly,     but  `args.logit_distance_cost=-1` by default    So using the default values switches the mean teacher off and just an ordinary supervised model remains? Should these losses be complimentary or interchanged?    2. Training a mean teacher model on MNIST with some consistency weight without res_loss with fixed hyperparameters (  gives significantly lower test accuracy (~78% with 1000 labels) compared to setting the consistency weight to zero (~92%).    Iâ€™d greatly appreciate any comments or hints.  "
"Dear authors,    We tried to reproduce mean teacher with shakeshake26 network on 1000 labels on CIFAR-10.  We failed to reproduce the error rate 10.08+/-0.41 reported in the appendix A for CIFAR-10 on 1000 labels.  Is the number correct?    Thank you,  Hai"
"i get the idea of the paper. but how can i use the unlabeled data to train the model.  i see  a word in the paper. if i use the unlabeled data  just keep the consistency loss ,because there is no classification loss . am i right?"
"I am trying to reproduce the result on pytorch with the traditional 13 layers convnet, but the result is bad.  It maybe some mistakes in my settings. I would appreciate it if you could offer help."
  -- This link seems to be broken (from the README.md) .. getting a 404
"hi, i have already achieve ~94% with 4000 labels on cifar10. But for my own three classification task, i have 160k labelled data and unlabelled data. i can not get expected results( worse than using the labelled data to train directly ). is lr strategy sensitive to it? here is my setting (finetuning on mobilenet-v1 and using 4 gpus): thanks in advance      defaults = {             # Technical details          'workers': 20,          'checkpoint_epochs': 20,          'evaluation_epochs': 5,            # Data          'dataset': 'my dataset',          'train_subdir': 'train',          'eval_subdir': 'test',            # Data sampling          'base_batch_size': 100,          'base_labeled_batch_size': 50,            # Architecture          'arch': 'mnet1',            # Costs          'consistency_type': 'mse',          'consistency_rampup': 5,          'consistency': 20.0,          'logit_distance_cost': .01,          'weight_decay': 2e-4,            # Optimization          'lr_rampup': 0,          'base_lr': 0.001,          'nesterov': True,      }"
"hi , firstly thanks for your great work for ssl. But when i refer many resnext nets of pytorch, there are no ShiftConvDownsample layer?  what is the function of it? And mean teacher didn't use this layer  in the experiment of cifar10 and imagenet, right?  And the two fc layers after avepooling correspond to student and teacher?  thanks in advance..."
I have no idea why have 2 input in train_loader
         I don't understand the useful of the export function?
"Hi,  I ran the code on cifar-10 and it worked great!  As this method is the state-of-the-art semi-supervised method for image recognition, I was curious to check its performance on cifar-100.   I changed the parts of the code loading cifar-10, to load cifar-100 (Tesorflow version). The code runs and prints indications of training, but I got 98-100% error, So I guess I am doing something wrong.  An example of one of the output lines:    > **INFO:main:step    60:   train/error/1: 100.0%,  train/class_cost/1:        nan,  train/cons_cost/mt:        nan**    Do you have an idea what else should I do to apply it to cifar-100? "
"File ""./main.py"", line 166, in main  train(train_loader, train_loader_len, model, ema_model, ema_model, optimizer, epoch, training_lo  File ""./main.py"", line 492, in train  assert not (np.isnan(loss.data[0]) or loss.data[0] > 1e5), 'Loss explosion: {}'.format(loss.data  AssertionError: Loss explosion: 1088561.875      I trained with 8 GPUs which should be close enough to the 10 GPU setting in the provided configuration. Is this expected?"
The PyTorch code currently crashes if the model has only one output. See comments on issue #7 .
"Thanks for your inspiring idea and the corresponding code.    I try to run the tensorflow code train_cifar10.py.  But it takes more than 2 hours to construct the computational graph and I'm still stuck here.  The screen does not print any thing.  If I replace the CNNs in the original code with a plain ResNet-32 (without Weight Normalization or other tricks), the whole code goes on well.    Do you know what might be wrong?    Thanks."
"Hi, I am very impressed with your research.    It may seem like a stupid, but I'm wondering what should I use the model for validating.  I'm confusing with some knowledge distillation (KD) methodologies, which use the terms, teacher and student model.  At first I thought those have different meaning (i.e., the word teacher in mean teacher and KD).  However, I'm wondering why the EMA model (teacher model) has better performance than the student model, which is supervised-learned with ground truths. (and the   also tells that the teacher model leads the student model.)  Indeed, the experiments results show that the teacher model has better performances than the students.    1) How can I approach that the EMA-weighted model has better performance than the student model?  2) So, is it correct that the teacher model is using at the final system?    Thanks for reading."
"Hello! I tried using your code, I executed the tensorflow-based script via `python train_cifar10.py`, but there's an error while searching for the data itself: `FileNotFoundError: [Errno 2] No such file or directory: 'data\\images\\cifar\\cifar10\\cifar10_gcn_zca_v2.npz'`. I looked through the repository and couldn't find it. Am I missing something and this data is created inside the code?"
"I'm a novice. I won't use the trained model of pytorch to test. Do I use the ""test_data. Py"" file to testï¼Ÿ"
"Traceback (most recent call last):    File ""main.py"", line 423, in        main(RunContext(__file__, 0))    File ""main.py"", line 104, in main      train(train_loader, model, ema_model, optimizer, epoch, training_log)    File ""main.py"", line 310, in train      **meters.sums()    File ""/home/lbl/work/mean-teacher-master/pytorch/mean_teacher/run_context.py"", line 34, in record      self._record(step, col_val_dict)    File ""/home/lbl/work/mean-teacher-master/pytorch/mean_teacher/run_context.py"", line 45, in _record      self.save()    File ""/home/lbl/work/mean-teacher-master/pytorch/mean_teacher/run_context.py"", line 38, in save      df.to_msgpack(self.log_file_path, compress='zlib')    File ""/home/lbl/miniconda3/lib/python3.7/site-packages/pandas/core/generic.py"", line 5274, in __getattr__      return object.__getattribute__(self, name)  AttributeError: 'DataFrame' object has no attribute 'to_msgpack'        my pandas version is 1.0.1 and this function may be removed earlier, so what can i do?"
"I found that the loss plot of **cifar_shakeshake26** trained on 1000-labeled dataset was unusual (but it is normal if trained on 4000-labeled or 45000-labeled dataset). I made a 50-epoch training, and training plots are as following.    !   !   !   !   !   !   !   !         Please take a look at the test loss plot, it started from a small value, but raised dramatically with training epochs increasing. This situation doesnâ€™t happen while trained on 4000-labeled or 45000-labeled datasets.  You can reproduce this with running following command:         I don't know how to explain this, hope you guys could help. Thanks a lot :D"
"Hi,sir. When I use `python train_cifar10.py` it seems not work  In windows the filesname cannot include':' but in run_context.py the path created includes it."
None
I can not find where is ZCA preprocessing being applied to CIFAR10 data in pytorch implementation. Am I missing something?
May I know the version you were using?  The code is not compatible with latest pytorch. Thank yoU!
"Thanks for your inspiring idea and the corresponding code.    I have run the cifar10 experiments in your code on the AWS cloud.  After i trained the network, the data logs were saved in the cloud.  Then I downloaded the results files, such as,  training.msgpack, but i don't know how to unpack it  to show the training logs.    I have google and searched at stackoverflow. But i still have not find a way to show the logs.    Would you please show me how to unpack the .msgpack file and show the logs?    Thanks."
"The results of running **python main.py ......** are showed as following:    > Traceback (most recent call last):  >   File ""main.py"", line 424, in    >     main(RunContext(__file__, 0))  >   File ""main.py"", line 104, in main  >     train(train_loader, model, ema_model, optimizer, epoch, training_log)  >   File ""main.py"", line 274, in train  >     meters.update('top1', prec1[0], labeled_minibatch_size)  >   File ""/home/gzx/Meanteacher/mean-teacher/pytorch/mean_teacher/utils.py"", line 53, in update  >     self.meters[name].update(value, n)  >   File ""/home/gzx/Meanteacher/mean-teacher/pytorch/mean_teacher/utils.py"", line 86, in update  >     self.sum = self.sum +(val * n)  > RuntimeError: Expected object of type torch.cuda.FloatTensor but found type torch.cuda.LongTensor for argument #2 'other'    Is this because I install pytorch by using conda? The pytorch version is 0.4.1"
"I used the suggested command 'python main.py \      --dataset cifar10 \      --labels data-local/labels/cifar10/1000_balanced_labels/00.txt \      --arch cifar_shakeshake26 \      --consistency 100.0 \      --consistency-rampup 5 \      --labeled-batch-size 62 \      --epochs 180 \      --lr-rampdown-epochs 210    I'm using ubuntu 18.04 python3.6, pytorch 0.3.0. numpy 1.14.2, and cuda8.0.and 2 gtx1080Ti  When I run the main.py, it can start training but there is no output information(epochs ,accuracy) during the training process (in a few hours), the only outputs are like this:      INFO:main:=> creating model 'cifar_shakeshake26'  INFO:main:=> creating EMA model 'cifar_shakeshake26'  INFO:main:  List of model parameters:  =========================  module.conv1.weight                            16 * 3 * 3 * 3 =         432  module.layer1.0.conv_a1.weight                96 * 16 * 3 * 3 =      13,824  module.layer1.0.bn_a1.weight                               96 =          96  module.layer1.0.bn_a1.bias                                 96 =          96  .....  module.fc2.weight                                    10 * 384 =       3,840  module.fc2.bias                                            10 =          10  ===========================================================================  all parameters                                   sum of above =  26,197,316    I have checked the results folder and there is no checkpoint file in it."
"Hi, we'd like to release some code which uses parts of this codebase.  However, your code has no license listed.  Do you mind adding a license, so that others can use your code?   "
"Hey,   I guess input and ema_input transformed versions of the same images, right ?(     If so, did you guys experiment with using the same input for both model and ema_model ? Does using the same input lead to drop in performance ?    Thanks !"
"Hello  I have read the pytorch code of mean-tearcher, and  in the `update_ema_variables` function, only parameters are updated. But as for batchnorm layer, mean and var buffers should also be computed except the weight and bais params.  In the current implementition, batchnorm of EMA model may be using the default 0-mean and 1-var.... am I right ?"
"Hello,    Our team is interested in testing an implementation of the mean-teacher Resnet in PyTorch for a few image classification problems we are working on.     However, we are having difficulty adapting the network to our image dimensions.     If I resize our images to 32x32 it runs without error. But, if I change to something else, I get:         Which makes sense. We're just a little unfamiliar with PyTorch and, speaking for myself, Resnet. So, I thought I would post this question while I was looking into this to see if someone might post an obvious hint that may not be obvious to find.     Thank you in advance,  Tommy"
"Hi, I ran your tensorflow code (file train_svhn.py) and the final accuracy was only around 90%. I did not change anything in the code. I ran it as is ! Do you have any suggestions why I do not get the expected 96% ? By the way, I ran it on one GPU."
"Hi, thanks for the great work!  I am wondering whether you could upload the instruction to reproduce the ImageNet results. It seems the data preparation for ImageNet is missing. Thanks."
"Hi, thanks that you keep pushing semi-supervised ML forward!    I have recently tried pytorch version and apparently my gpu capacity is not enough (6GB).   I had issues with tensorflow as well but I have already seen that older version of tensorflow may help.    Which brings two suggestions:  - Can you please dockernize your environment so it is easier to reproduce your results?  - Can you add MNIST example. I know it is boring but it will most likely run even on single gpu?    "
"Hi, I'm wondering why we need a 2 layer loop in the aggregate function?  Here is the pseudo code from the paper:  !   From my understanding, the aggregation is a one way process, from the bottom layer(with the most nodes) to the top layer(original batch nodes), which does not need 2 loops."
"I have a question on the feature sets. As the dimensions of c1, c3, c7 are very large, how did you represent each protein using 50-dimensional vectors?    Thanks in advance!    _Originally posted by @zcwang0702 in  "
Hi is my understanding correct?     during inference on new node do we have to build a new graph with this new node.?    or how to do testing on new node which added?    
"Hi, is there any mapping files about correspondence between protein name and ID in dataset?"
"Hi,    Can you use a graphsage model for a weighted graph? I know you can do that for a regular GNN model. "
"N(v) ,the fixed-size of neighbors, which is mentioned in the paper. I think this is to make sure that the aggregate function can work correctly. But what if a node with neighbors less than N(v). How dose the model sample and train?  And what would happen if there is an isolated node in train or inference stage?"
æˆ‘å»ºç«‹äº†ä¸€ä¸ªæœ‰ä¸‰ç§é¡¶ç‚¹å’Œä¸‰ç§è¾¹çš„å¼‚æž„å›¾å½“æˆ‘å°†è¿™ä¸ªå¼‚æž„å›¾è½¬æˆäºŒéƒ¨å›¾æ—¶å‡ºçŽ°é—®é¢˜ã€‚  !   ç„¶åŽæˆ‘æ”¹æˆ  !   è¿˜æ˜¯æœ‰é—®é¢˜  
"if a node only have 1 top neighbors,should i have to remove the node before i train the sage model? or the model just train the node only use the 1 top neighbors?"
"Since the dot product similarity z_u*z_v can be affected by the norm of the node representation vectors; i.e., z_u*z_v = cos *|z_u|*|z_v|. If You use dot product to approximate the (cosine) similarity of the 2 vectors z_u and z_v, how can you make sure that the 2 vectors have similar norms?"
I am confused about the difference between GraphSAGE-GCN and GraphSAGE-mean. In the paper there is no explanation about the aggregation function of GraphSAGE-GCN; how should it work?
"Is there any example notebooks of using this code?    Thanks,    Bo Peng"
"I wanna to train a dataset unsupervisedly with tensorflow 1.15(just want to train with multiple GPUs).  About 42,000,000+ random walks edge pairs and 7,000,000 nodes.   And `ValueError: Tried to convert 'value' to a tensor and failed. Error: Cannot create a tensor proto whose content is larger than 2GB.` happened. So I change the code like that      It works. that's the change of the origin colde.    But then, I encounter another problem, the logs of error is       There are few related solutions online. Do you have any advice? Thanks a lot!  My appeal is actually to use multi-GPUs parallel computing to accelerate the computation of GraphSAGE.  "
"Hi,   Thank you for this great code. I am trying to running this code on google colab with python version 2.7. I am running this file:  `!bash /content/drive/MyDrive/GraphSAGE-master/example_supervised.sh`  And it is giving me following error:  /usr/bin/python2: No module named graphsage    And when I try this command:  `!python -m /content/drive/MyDrive/GraphSAGE-master/graphsage/unsupervised_train.py --train_prefix ./example_data/ppi --model graphsage_mean --max_total_steps 1000 --validate_iter 10`    Then I  am getting following error:  `/usr/bin/python2: Import by filename is not supported.`    Please help me how can I run the code in google colab. I would be very thankful to your for your help and support."
"Excuse me, I wonder why to pad features with zero vectorï¼Ÿ  *************************************************************      if not features is None:          # pad with dummy zero vector          features = np.vstack([features, np.zeros((features.shape[1],))])  *********************************************************************"
"Hi, I was trying to create a GraphSAGE model for directed graph, based on  , but there is no description of the aggregator.    In GraphSAGE model, Mean aggregator, LSTM aggregator, and Pooling aggregator can be used.   So I was wondering which aggregator is used in Directed GraphSAGE model.    "
"Hello,    I have been saving my model via :            save_path = saver.save(sess, model_path + model_name)          print(""Model saved in path: %s"" % save_path)    But when it comes to restoring model :        saver = tf.compat.v1.train.Saver()        saver = tf.compat.v1.train.import_meta_graph(model_path + '/model.meta')      saver.restore(sess, tf.train.latest_checkpoint(model_path + '/'))        It restores but when i try to run it, it gives me following error:    Input to reshape is a tensor with 646076700 values, but the requested shape has 371140200    Help is much appreciated, maybe the way i am saving is wrong    Looking forward to hear back"
"Forgive my lack of knowledge, but I have a simple question regarding the provided demo for node representation learning with GraphSAGE and the use of an unsupervised sampler:    > Given a large set of positive (+) node pairs (generated from random walks performed on the graph), and an equally large set of negative (-) pairs that are randomly selected from the graph    What do you mean by positive and negative node pairs? Could you point me to some useful resources to better understand this concept?   "
I have a graph with 40 million nodes and 2.5 billion edges. How long will it take with unsupervised graphsage?
"Excuse me, I would like to ask why the positive samples does not get similar embeddings after training?"
"Hey,   I am pretty new with GraphSAGE. How can I use a trained model to generate new labels for a new graph ( same structure as the one used in training just with new features)?  Thanks a lot! "
Does it mean expectation?
"As __inif__ in line 210 of models.py defined:     The last statement should be `raise Exception(""Unknown aggregator: "", aggregator_type)`?"
"Hi, I have used this model to train an unsupervised model. However the results are all NaN. WHY?  "
"Didn't find 3.2.0 for futures, as shown below.        I replaced it to 3.1.1 in requirements.txt. Let's see what happens."
"**so , run example_unsupervised.sh ,and use hinge loss, the trend of loss is like**    Epoch: 0001  Iter: 0000 train_loss= 2.14502 train_mrr= 0.22259 train_mrr_ema= 0.22259 time= 1.17050  Iter: 0050 train_loss= 2.42778 train_mrr= 0.17273 train_mrr_ema= 0.21027 time= 0.08527  Iter: 0100 train_loss= 2.35766 train_mrr= 0.18598 train_mrr_ema= 0.20077 time= 0.07464  Iter: 0150 train_loss= 2.28739 train_mrr= 0.19115 train_mrr_ema= 0.19425 time= 0.07058  Iter: 0200 train_loss= 1.84928 train_mrr= 0.24786 train_mrr_ema= 0.19023 time= 0.06793  Iter: 0250 train_loss= 2.13344 train_mrr= 0.20242 train_mrr_ema= 0.18814 time= 0.06637  Iter: 0300 train_loss= 2.33268 train_mrr= 0.19176 train_mrr_ema= 0.18753 time= 0.06522  Iter: 0350 train_loss= 2.27680 train_mrr= 0.20565 train_mrr_ema= 0.18688 time= 0.06435  Iter: 0400 train_loss= 2.37595 train_mrr= 0.19267 train_mrr_ema= 0.18856 time= 0.06373  Iter: 0450 train_loss= 2.18717 train_mrr= 0.18997 train_mrr_ema= 0.18781 time= 0.06335  Iter: 0500 train_loss= 1.97818 train_mrr= 0.21563 train_mrr_ema= 0.18772 time= 0.06307  Iter: 0550 train_loss= 2.04497 train_mrr= 0.20323 train_mrr_ema= 0.18829 time= 0.06280  Iter: 0600 train_loss= 2.14987 train_mrr= 0.19552 train_mrr_ema= 0.18920 time= 0.06247  Iter: 0650 train_loss= 2.08283 train_mrr= 0.18723 train_mrr_ema= 0.18896 time= 0.06223  Iter: 0700 train_loss= 2.01260 train_mrr= 0.20139 train_mrr_ema= 0.18837 time= 0.06199    **And  run example_unsupervised.sh ,and use xent loss, the trend of loss is like:**  Epoch: 0001  Iter: 0000 train_loss= 18.96014 train_mrr= 0.22259 train_mrr_ema= 0.22259 time= 1.25577  Iter: 0050 train_loss= 18.76009 train_mrr= 0.17460 train_mrr_ema= 0.21034 time= 0.08849  Iter: 0100 train_loss= 18.55639 train_mrr= 0.18638 train_mrr_ema= 0.20079 time= 0.07623  Iter: 0150 train_loss= 17.97637 train_mrr= 0.18695 train_mrr_ema= 0.19437 time= 0.07148  Iter: 0200 train_loss= 17.25497 train_mrr= 0.23274 train_mrr_ema= 0.19028 time= 0.06911  Iter: 0250 train_loss= 17.02276 train_mrr= 0.19407 train_mrr_ema= 0.18802 time= 0.06776  Iter: 0300 train_loss= 16.97402 train_mrr= 0.19147 train_mrr_ema= 0.18748 time= 0.06685  Iter: 0350 train_loss= 16.58705 train_mrr= 0.20570 train_mrr_ema= 0.18639 time= 0.06603  Iter: 0400 train_loss= 16.23674 train_mrr= 0.19959 train_mrr_ema= 0.18713 time= 0.06545  Iter: 0450 train_loss= 15.92830 train_mrr= 0.18490 train_mrr_ema= 0.18618 time= 0.06496  Iter: 0500 train_loss= 15.54055 train_mrr= 0.21570 train_mrr_ema= 0.18709 time= 0.06473  Iter: 0550 train_loss= 15.47228 train_mrr= 0.20580 train_mrr_ema= 0.18705 time= 0.06442  Iter: 0600 train_loss= 15.25948 train_mrr= 0.18497 train_mrr_ema= 0.18760 time= 0.06424  Iter: 0650 train_loss= 15.14235 train_mrr= 0.17513 train_mrr_ema= 0.18732 time= 0.06409    the result seems good, but if anyone take a close look, you will find  tf.reduce_sum(true_xent), as pos_loss, increase!  and  tf.reduce_sum(negative_xent) as neg_loss decrease.  if set  'neg_sample_size' variableâ€˜s value  to 1, you can find  pos_loss decrease and neg_loss increase!    so , can author explain this?  "
None
the trend of loss:   who can tell me why????    Epoch: 0001  Iter: 0000 train_loss= 2.28088 train_mrr= 0.39205 train_mrr_ema= 0.39205 val_loss= 2.36228 time= 1.15651  Iter: 0050 train_loss= 2.37790 train_mrr= 0.07485 train_mrr_ema= 0.35128 val_loss= 2.36228 time= 0.02849  Iter: 0100 train_loss= 2.31257 train_mrr= 0.30818 train_mrr_ema= 0.31404 val_loss= 2.36228 time= 0.01703  Iter: 0150 train_loss= 2.34312 train_mrr= 0.11648 train_mrr_ema= 0.29781 val_loss= 2.36228 time= 0.01318  Iter: 0200 train_loss= 2.34605 train_mrr= 0.07353 train_mrr_ema= 0.27681 val_loss= 2.28311 time= 0.01132  Iter: 0250 train_loss= 2.26551 train_mrr= 0.12962 train_mrr_ema= 0.27216 val_loss= 2.28311 time= 0.01010  Iter: 0300 train_loss= 2.20238 train_mrr= 0.31675 train_mrr_ema= 0.26658 val_loss= 2.28311 time= 0.00932  Iter: 0350 train_loss= 2.14746 train_mrr= 0.22286 train_mrr_ema= 0.25806 val_loss= 2.28311 time= 0.00872  Iter: 0400 train_loss= 2.17668 train_mrr= 0.26786 train_mrr_ema= 0.25654 val_loss= 2.21059 time= 0.00834  Iter: 0450 train_loss= 2.17388 train_mrr= 0.17841 train_mrr_ema= 0.24381 val_loss= 2.21059 time= 0.00796  Iter: 0500 train_loss= 2.15469 train_mrr= 0.33229 train_mrr_ema= 0.24230 val_loss= 2.21059 time= 0.00768  Iter: 0550 train_loss= 2.13787 train_mrr= 0.20762 train_mrr_ema= 0.25058 val_loss= 2.21059 time= 0.00743  Iter: 0600 train_loss= 2.05377 train_mrr= 0.34222 train_mrr_ema= 0.24788 val_loss= 2.15021 time= 0.00726  Iter: 0650 train_loss= 2.16601 train_mrr= 0.15316 train_mrr_ema= 0.23802 val_loss= 2.15021 time= 0.00707  Iter: 0700 train_loss= 2.03879 train_mrr= 0.37843 train_mrr_ema= 0.25796 val_loss= 2.15021 time= 0.00693  Iter: 0750 train_loss= 2.10829 train_mrr= 0.27148 train_mrr_ema= 0.23796 val_loss= 2.15021 time= 0.00678  Iter: 0800 train_loss= 2.12745 train_mrr= 0.12650 train_mrr_ema= 0.24822 val_loss= 2.12446 time= 0.00667  Iter: 0850 train_loss= 2.06832 train_mrr= 0.47053 train_mrr_ema= 0.25520 val_loss= 2.12446 time= 0.00656  Iter: 0900 train_loss= 2.10655 train_mrr= 0.13009 train_mrr_ema= 0.24280 val_loss= 2.12446 time= 0.00649  Iter: 0950 train_loss= 2.05646 train_mrr= 0.39455 train_mrr_ema= 0.24797 val_loss= 2.12446 time= 0.00642
"Hi authors,    I saw it in the paper that you are using positional gene sets, motif gene sets and immunological signatures as features. I wonder how you put three dataset into one list. Additionally, I wonder what is the connection between features and labels.    Hope to hear you soon.     Regards,  Mengtao Zhang"
"Could anyone please share the given code revise to suit with **networkx==_2.3_**, with the input data format remaining the same?"
"Hi, I was wondering if there is a way to leverage edge attributes when processing graphs with GraphSAGE?    I have a network where the links between nodes are not expressed as binary or a single real number, but the relationship between nodes is described with both, link and several attributes for that link (the attributes are real numbers).    I would like to use this edge attribute information in unsupervised setting with GraphSAGE, have you perhaps worked with a similar setting? Is there a known way how to incorporate edge attributes and not just node attributes?    Thank you!    Ira"
Generating random walk by the given method on Big-Graph (~5 Mn Nodes) is taking too long.  Could someone please share an better optimized version of the same?    Code Given:     
"Dear authors:  I just want to obtain the embeddings for all nodes that have no features in a small  BA network(just 100 nodes) ,after that I use these embeddings to reconstruct a new graph  that has the same number of edges as the original network. The edges are added according to the similarity of these embeddingsã€‚But the new graph just has a few edges that belongs to the original network. And I also change the minibatch size ,the results are also wired .Is it reasonable?"
pip3 install -r requirements.txt --user  no longer works as it attempts to install version numbers that are no longer available.  For example  futures==3.2.0 and tensorflow==1.8.0 do not exist.
"Dear Authors:    I got an error in using the utils.load_data.    On line 64, there is ""id_map[n] for n in G.nodes()"", which indicates that  the n is int, but in your example_data, the keys for id_map are all strings, not int.    Hope this helps."
"I read the code, and i do not know why not use the train adj to reserve more information for all nodes embeddgins:       If only use val adj info, the train nodes adj info is not used, i do not know why the embedding results for the train node are right."
"I set the identity_dim parameter to 64, and get error:     "
Is the tow nodes of 'train_removed' edge must be in 'test' or 'val' set?
"Hello    There is cition data with word embedding preprocessing presented in your paper.  If possible, could you please releast it?    Thx"
"it seems that reddit-G.json only has 231443 nodesï¼Œwhich means the node features should have 231443 rows and several colsã€‚However,  reddit-feats.npy has the shape of (232965, 602)ï¼Œhow to explain it?"
"not all of the samples are labeled,   how can i use it as a semi-supervised style?"
Do not support multi-relational edges?  
"In supervised_train.py, why to pad features with zero vector?    ""if not features is None:          # pad with dummy zero vector          features = np.vstack([features, np.zeros((features.shape[1],))])"""
"For me, it's a complex code,I don't know what to changeï¼Ÿplease helpï¼Œ3ks    "
"When generating train_edges in _remove_isolated() function, the legal edges seem like:      node(train)     node(train)      node(train)     node(test)      node(valid)     node(test)      node(test)     node(test)  node(train) represents the node with train symbol.  Why those 4 types of undirected edges are uesd to train the unsupervised model?   Looking forward to the answers!@RexYing @williamleif @bkj "
"Hello, I have four questions:  1. About PPI data, what functions does it prove? What does graph classification mean? Is it the similarity between the prediction graph and the training graph?  2. How can 24 graphs not be displayed in PPI database?  3. What is the difference between features and labelsï¼Ÿ  4. Can features and labels be more than 0 or 1ï¼Ÿ  Thank you for your reply"
I want to save embeddings while in supervised mode. I tried adding the save_val_embed function but to no avail. I read the issue num: 26 but couldn't understand how to resolve. Can anyone explain?
"Hi, I was wondering that GraphSage, which will look_up table and then get the embedding vector of input.    My question is i don't see stop_gradient in the embedding process,      so i suppose that the embedding will update with training process, but i think in the paper of GraphSage, the embedding is the input(don't update).  !     It's a bug?"
!   
None
"Hiï¼ŒI don't know how to build a JSON file like toy-ppi-G.json, etc. Can you provide pre-processing code for example_data?"
"Hello,    I noticed that the process of getting embeddings in unsupervised mode essentially involves feeding self-loops to the model, i.e. inputs1==inputs2 as seen in incremental_embed_feed_dict.    How then does one generalize to new unseen nodes? Feed all the edge connections of those nodes to the model and then average their ""output1"" vectors?    There is this post:   the reply is not clear to me. When inferring a new node, it would not exist in the id2idx dictionary as seen in the code. In such case, it would not be possible to pass it through the model naively using the minbatch iterator. How then can we go about ""aggregating its neighbors"" so to speak?    Thank You,    Kuhan"
"Do not support edge weights?  If so, how to add weights? How to modify to codes."
"Hi, @williamleif       The code uses `{}-feats.npy` as `tf.constant` which is fine when `feats` array with small dimension, but it's not ok to deal large dimension `feats`. It will cause `cannot create a tensor proto whose content is larger than 2GB`.       So, my solution is to take `input_features` as a `placeholder`. Although it passes, it costs a lot of memory and runs slow.       Since, my `feats` are sparse array. So I treat the `feats` as a `SparseTensor`, and to gather the particular rows of `feats` in `SampleAndAggregate.aggregate` instead of `embedding_lookup`.But the `SparseTensor` usage is also time-consuming with low memory.       Do you have any suggestions about large dimension `feats`.  And may I commit the `SparseTensor` usage to the repo.    Thank you !  Sincerely~"
"Hello,    What is the best way to create the various input files necessary from a SNAP Network with attributes for unsupervised learning and supervised learning?    Is the best practice to go from SNAP Network -> NetworkX Object -> GraphSAGE JSON files or SNAP Network -> GraphSAGE JSON files?    Thank You"
"Dear Authors:    May I know why you use link_pred_layer.loss? Is it the same as what you describe in the paper? Does it use hinge loss?    self.loss += self.link_pred_layer.loss(self.outputs1, self.outputs2, self.neg_outputs)     Can you explain a little more about the loss function?    Thanks."
Is there anyway to find out how much the graph data has contributed to prediction results?  I mean how do you know the performance gains are from using the graph structure and not because of CNN model?   Is there anyway to try GraphSAGE model or something close to it on features only?
"I am trying to create input data files for the model but can't understand how to creat feat.npy file if I have two kinds of node in my graph (Let's say User and Item). Since I have 20 user attributes for every user and 50 item attributes for every item, how to create feat matrix for such graph? I would really appreciate your help on this."
"Hi,  I apologize if my question is naive. However, I have a simple example that I want to implement:  5 Computers in a network, each has a vector of only two features `f = [CPU, RAM]`.  Each computer has no particular class, and my goal is to predict ahead the CPU and RAM usage of a certain (or all) computer in future after a certain time (i.e. `t + 1` ) based on the CPU and RAM readings history and the device topology (assume computers are connected differently every certain time).    -------------------------    **1. Is it doable with this library ?**    **2. Do ye recommend Supervised or Unsupervised type?**    **3. If possible please some hints about the general approach that I should follow, particularity, how to invoke the predict method? Because for example, say in Scikit-learn library, there is an explicit `predict()` function that one can use after training the model.**    -------------------------    Any help is very much appreciated,  Many Thanks."
"Dear authors,    Recently, I noticed the Reddit and Citation dataset  you used in your paper ""Inductive Representation Learning on Large Graphs"". And I found that you have released the Reddit dataset here:      I have some questions about this, could help me and give me some advice?  1. The feature of Reddit dataset is composed of 4 parts: ""For features, we use off-the-shelf 300-dimensional GloVe CommonCrawl word vectors [27]; for each post, we concatenated (i) the average embedding of the post title, (ii) the average embedding of all the postâ€™s comments (iii) the postâ€™s score, and (iv) the number of comments made on the post."" So I am wondering if you could tell me the specific meaning of the feature dimension since I only want the feature of the (i) part.    2. About the Citation dataset, it seems that you do not release this dataset. So I am wondering if it is ok for public usage. If it's ok, could you give me this dataset, and the corresponding feature dimension meaning of this dataset like Reddit.    Thank you.    I will really appreciate that."
"hello,because I don't know the description of the dataset,and I don't find any description of dataset online,could you release the description of dataset?"
"Hi there,  Thank you for your implementation of this paper. I have a question of the data preparation.   If I have a network without any feature nodes, just want to get unsupervised network embedding of these nodes (like in node2vec). What kind of data I need to prepare for GraphSage.   Previously, I randomly select 20% of nodes as validation and 80% as the training. I only include all the edges between training data points and all the edges between test data points in the files. But the embedding result is pretty weird. The validation loss is not changed over time. What exactly I need to do for the next step? Randomly drop some edges? Look forward to your help!"
"Hi,  Thanks for your code.    I'm doing research related to dynamic graphs. Datasets with information including node attributes, node labels and dynamic links are needed.  In the SNAP website, I canot find datasets meet my needs. There seems no download link in   where reddit dataset sourced from. Is there any way I can obtain those datasets?     Thanks in advance."
"When I was following the unsupervised training code, I found   train edge separation. Here,  Train edges = All edges - (edges between a valid node to a valid node) - (edges between a valid node to a train node without a train neighbourhood )  Can you please clarify the reason for those edge set deduction?"
I would like to know what is the exact dataset used to generate the toy_ppi dataset in example_data folder (input files used to generate the pre-processed dataset). What are the node features available in the dataset? May I know the link where the dataset is available?
"I try graphsage with my own code and data.  it works well when not use tf.nn.l2_normalize after encoder and before decoder. but when i use tf.nn.l2_normalize to normlize the output vector to length of 1, the model just cannot converge and the loss does not decrease. what is the cause of this problem."
    I find the tf.nn.fixed_unigram_candidate_sampler() function outputs negative samples including some positive samples.The outputs of negative sampling should not include any positive samples(included in true_classes). I am confused about this.      
"When I run `pip install requirements.txt`, I got the error `No matching distribution found for futures==3.2.0 (from -r requirements.txt (line 8))`. After I manually change the version to 3.1.1, it works. I don't understand why and apparently other people are having this issue      "
"self.id2idx always has keyError. it is a dict whose keys are of str type, but you always use int key to access it. e.g. self.id2idx[neigibor], where neighbor is int not str.  How to resolve that?"
"Hey,  Thanks for the code.     I have tried GraphSage approach on my dataset which is a bi-partite graph of two entities (for e.g. users and movies) . I am try to use the embeddings of nodes generated using GraphSage for link prediction task (what users will watch next).  I have an out of time dataset (which is not biased) on what users actually did (for example watched which movie).    I am trying to evaluate the performance of GraphSage and compare against the performance of standard matrix factorization on this task. However, what I have repeated found is following:  1. If users end of watching the same movie again in the validation set, Matrix Factorization performs way better than GraphSage in terms of Precision@K metric.  2. If users watch a movie which they have never watched, GraphSage performs better.    I have also consistently observed that GraphSage underfits the data (even after changing the samples_1 and samples_2 parameters). Is it by design or there is more to it?  "
"Hi Will/Rex/Anyone have the answer,    Since I have the case when the data generates a disjointed graph, how should I specify the parameters in {'name': 'disjoint_union( ,  )'} for G['graph']. Do I need separate them with networkx first?     Thank you in advance!  Phil  "
"Hi ï¼ŒI have encountered some troubles. I did not find a specific description of the LSTM part in the paper, so I have several problemsï¼š  `ï¼ˆ1ï¼‰ï¼š`initial_state = self.cell.zero_state(batch_size, tf.float32)          used = tf.sign(tf.reduce_max(tf.abs(neigh_vecs), axis=2))          length = tf.reduce_sum(used, axis=1)          length = tf.maximum(length, tf.constant(1.))          length = tf.cast(length, tf.int32)`    I canâ€˜t really understand why length is defined by these codeï¼ŒEspecially the first two lines  ï¼ˆ2ï¼‰ï¼š`index = tf.range(0, batch_size) * max_len + (length - 1)          flat = tf.reshape(rnn_outputs, [-1, out_size])          neigh_h = tf.gather(flat, index)`  These lines of code also bother me, and I can't understand them very well  I hope to get your answer      "
"Hi! I'm reading this paper and try to exploit GraphSAGE to other application.  I have two questions about input data format.    1. feature data  In the example data, both toy-ppi-G.json and toy-ppi-feats.npy have features for each node.  In toy-ppi-G.json, is feature attribute is needed?    2. test/val attribute for  -G.json   Is the attribute of test/val for each node mandatory? If so, how can I add the attribute by networkx library? such as nx.set_node_attributes(G, 'test', true)?    In tutorial on WWW2018 (p.34), the preprocessing is little bit explained, but I cannot understand because I'm new to graph analysis.   "
"In the paper, the current node's vector is concatenated with the aggregated neighbors' vector, then it is fed into a fully connected (dense) layer. However, reversely, in the code (aggregators.py), the current node's vector and aggregated neighbors' vector are fed into their own dense layers  respectively, then results of them are concatenated. Could you tell me why? Thank you in advance."
"Hi,      I am trying to apply Graphsage to my dataset for node classification. I tried to format my data to what Graphsage needs. However, I found two problems:      1. the identity of nodes must be integers otherwise there will be an error. This may be caused by the version of networkx. I don't know if I am doing it wrong.      2.In readme, the order of features is followed by the id_map.json, where there is a dict in the file, which is unordered.       Looking forward to your reply!"
"Hi everybody,    I checked all issues, there are two three question about getting embedding for unseen nodes but they are not clear for me. what I am doing is I have a big graph and i use unsupervised learning for that, it results all embedded vectors for all nodes. The thing that I want is saving the trained model and use it for predicting embedded vector for unseen nodes. Could you please explain and guide me step by step what I should do in this case?    Thanks in advance!    Best,  Vahid"
"Can I ask why n2v model just skip the ""while structure"" when running random walks?"
"During training, each graphsage layer takes uniform samples from each node's neighbours, a new sample at each training iteration. With a large number of samples taken during the training, the entire neighbourhood of each node is (probably) covered, and the model learns node embeddings from node own features and the aggregated features of essentially entire node neighbourhoods.    But what about predictions? If at prediction time you again take random samples of a node's neighbours, then repeated predictions by the same model on the same node will give different results (due to different node neighbours sampled each time - see Fig. 1 of the paper), i.e., predictions will be stochastic. Do you observe this stochasticity of predictions? If not, how do you sample neighbours at prediction time? "
"Hello,    I would like to test on the citation dataset, but it seems not open to public. In the SNAP website, I found below:  The Web of Science citation data used in the paper can be made available to groups or individuals with valid WoS licenses.    How can I get WoS licenses and then ask for the dataset?    "
"This might be a dumb question, but could somebody kindly tell me the difference between `features in  -G.json` and` features in  -feats.npy`?"
I am wondering if it makes sense to use GraphSAGE on graphs with no features? Does it still have an advantage over DeepWalk? Why is GraphSAGE without features experiment is missing in the paper?
"Hi,    I have a multi-label classification problem, where one node can have multi labels, Do I need to change the code for multi-class classification?   thanks."
"Hi,    I am trying to use graphsage to train embeddings in completely unsupervised way. But when I prepare the data in -G.json format, if I have 0 nodes with 'val' attribute set to True, then I get a crash when the model starts to train. What is the significance of validation in unsupervised setting? Ideally for unsupervised setting 'val' and 'test' attributes shouldn't be needed at all.    Also does it make sense to use the graphsage embeddings obtained in unsupervised way for some other task like link prediction?    Thanks."
"Hi, I download your code and tried the unsupervised training. When I run the ppi_eval.py strictly following the instructions, it output some F1 Score far different from the scores shown on your paper, for examples, some of them look like these  F1 score 0.6524257784214338  F1 score 0.7677407675597393  F1 score 0.7941708906589428  F1 score 0.7668356263577119  F1 score 0.8696596669080376  F1 score 0.8081100651701665  F1 score 0.7624909485879797  the only thing I changed in the code is that I replaced dict.iteritems() to dict.items(), which I think won't be the real problem. I wonder if there is something wrong? Are the scores ""Micro F1"" or ""Macro F1"" on your paper?    default parameters + mean aggregator + unsupervised training on ppi dataset(not the toy)   "
"Hi,    I downloaded a dataset and, from it, I need to create the files in example_data folder. Should I create my own code to generate it or is there code ready for this?    Tanks in advance"
"hi  Rex  Ying,  I  am  confused  about  the  mini-batch  operation in  algorithm2  in  your  paper.  So  why  do  not  we  just  sample  a  mini-batch  samples  and  find  the  corresponding  neighbours  to  train.  Instead,  you  use  a more  complex  operation  to  define  this?  rhank  you  very  much."
"Hi:  I used this command :   `python -m graphsage.supervised_train --train_prefix ./example_data/ppi --model graphsage_seq --sigmoid`  and I get this result:  `Optimization Finished!  Full validation stats: loss= 0.52820 f1_micro= 0.46528 f1_macro= 0.23076 time= 0.33810  Writing test set stats to file (don't peak!)`  Is default hyperparameters the optimal parametersï¼ŸThe result(f1_micro= 0.46) is far from the result(f1_micro=0.61) listed in the paper.  I set epoch=50, and I get the results range from 0.60 to 0.64. So, what are the optimal parameters for supervised LSTM model on the PPI?  "
"Hello,    On Page 4 of the paper   there is a normalization step in line 7 of algorithm 1. However, I cannot find this step in your code. The algorithm in aggregators.py implements until line 5 of the algorithm (activation) and return the output to line 326 in models.py for the next iteration.    Is there something I missed or you intentionally skip the normalization line to for other benefits?    Thank you very much,  Kai  "
"Hi ! I have a question about Lemma 3 in the appendices of the paper.     The proof of Lemma 3 says:    !     However,  nodes that co-occur in a certain node's 3-hop neighborhood actually may not be adjacent in A^3, so the chromatic number cannot guarantee that these nodes have different colors. From another aspect of view, if nodes that co-occur in any node's 3-hop neighborhood are assigned different colors, then the maximum degree of all nodes in the A^3 graph is no more than the chromatic number, which is certainly not a general case.     If I'm wrong please correct me. Thank you.  "
Thank the authors for the paper and the code. We are going to train the model on multiple GPU. I also know that you did some experiments on multiple GPU.   Is the provided code can run directly with multiple GPU? Or we have to modify this code? Please help us with some explanation on doing with multiple GPU.
"isn't it wrong to simply assign model.context_embeds with update_nodes+no_update_nodes, where no_update_nodes = tf.stop_gradient(...), for which you meant to stop the gradient for the already trained nodes? because the old model.context_embeds is still there in the tf graph, used by model.opt_op, right? plz correct me if I'm wrong, thx."
"Hi,    I wanted to use GraphSage for unsupervised training but it seems to need the class membership information ( -class_map.json) although I want to run it in unsupervised mode. Did I overlook something or is this class information always needed?    Thanks in advance! "
"Hi,  I want to do cross validation using the node embedding as input of a classifier, but I do not known how to gain the embedding vectors of new nodes, would you like to introduce me which your function or file have this function? Thank you very much!    "
"Hi,    In the paper, the F1 on the PPI data set for the supervised setting is 0.598 (for GraphSAGE-mean). The default hyperparameters yield an F1 of around 0.576 on the full data set.     Can I know whether the difference is due to the different hyperparameter choices or just some randomness from stochastic sampling? If the former, please share with me the concrete choice of parameters if possible.     Thanks!    Jianbo"
"Hello! I'm reading this article recently and come across some problems. I'm not clear what the file ""x-class_map.json"" stands for and want to know which method in ""networkx"" module can produce the file. Hope for your answer. Thanks~"
"Hi, does this model can be used to unseen nodes if we don't know adj_info of these nodes? That's mean, we only know the features of these nodes.  "
"I am quiet new to graphs and am trying to translate my datasets (in HDFS, which I can read using scala/python/hive) to networkX graphs of the format -G.json   The datasets are obviously not in graph format but as guest transactions, which I can translate to graph.    First, is there any utiliity to do that. Secondly, what is the use of features and label in the graph description. How are these features different from -feat.npy features for nodes. Is label only for supervised learning?    Third, is there any talk or detailed slides about the implementation? I got some hints from the paper, but a talk makes it easier to follow i guess (I have seen Jure's recent talks on this but they are overview talks, I was looking to detailed ones)."
"Hi, could you please kindly offer more details on training embedding on simple static graphs with no features? Thanks"
None
"LSTM is usually used to sequence task, What is the reason that LSTM could success to aggretate neighbor features which has no sequence dependence? "
None
"Hello,    I would like to use GraphSAGE on a massive streaming graph where I'm receiving new nodes and edges on, say, a Kafka queue. We want to generate the node embedding on the fly as data flows in.    Is there anyway to integrate GraphSAGE to be used on a stream of edges as opposed to a fixed dataset, in such a set-up?"
None
Can this graphsage model support bipartite graph training? Like user-item graph?
"I run the code and the embeddings are saved as file val.txt and val.npy.  I read the source code, it seems that the saved file are total embeddings of the node, including train and val nodes. SO, why the file is named by 'val'?    Am i wrong about the source code?"
"In the paper Experiments section, it says:       I don't know why the nodes can be predicted if it is not visible in training process. How to get its embedding if it is not presented during training?  And even 'unseen graphs' can be tested, where to get the trained embeddings for the nodes in unseen graphs?      "
"when i run example_unsupervised.sh , i got and error:   No such file or directory: './example_data/ppi-G.json'     Where to get the data file?"
"What's the meaning of label in G.json? Does it indicate the link between nodes?  If so,  the G.json file will be extreamly large when we get millions of nodes."
Difference of the model design.    It seems the difference is that GraphSAGE sample the data.    But what is the difference in model architecture.    Thank you very much.    @williamleif 
"It seems in GNN(graph neural network), in transductive situation, we input the whole graph and we mask the label of valid data and predict the label for the valid data.    But is seems in inductive situation, we also input the whole graph(but sample to batch) and mask the label of the valid data and predict the label for the valid data.    Thank you very much.  @RexYing @williamleif "
"Hi. I have some own documents that I really wanna use them as my dataset. How can I convert them into type that it will fit with utils.py. I try to see what's inside your example_data folder. However, there are some many complicated things I can't understand :(("
"Hi there,    Background: my understanding for `outputs1` and `outputs2` in `SampleAndAggregate` in `model.py` (unsupervised learning) is that they are used to denote neighbor node embeddings. They are coming from `inputs1`/`inputs2` or `batch1`/`batch2` from `minibatch.py`. They are used in the Eq (1) implemention in `prediction.py`.    In `SampleAndAggregate` (`model.py`):  - `outputs1` is coming from only the first layer embedding (line 350)  - `outputs2` is coming from the two layers (final) embedding (line 352)  - `neg_outputs` is coming from final embedding (line 358)    Should both `outputs1` and `outputs2` be final embeddings to use in Eq (1)?    Thanks,  Xiaokui"
"Based on my understanding, it seems that train nodes are [1,2,3,4,5] and valid nodes are [6,7]. We are training on [1,2,3,4,5], then how does [6,7] get the embedding for downstream model?    Thank you very much!    "
"I am trying to run the unsupervised file to get embeddings from a graph. But, I am getting the following exception all the time. I have searched for a lot. Please help!    DuplicateFlagError: The flag 'log_device_placement' is defined twice. First from C:/Users/KSE9/GraphSAGE/unsupervised_train.py, Second from C:/Users/KSE9/GraphSAGE/unsupervised_train.py.  Description from first occurrence: Whether to log device placement.    This happens for all flags if I try to comment out anyone."
"Hi, I'm new to graph cnn and doing some  . They uses GraphSAGE to train the model. And I ran the GraphSAGE Cora Node Classification Example, `graphsage-cora-example.py`.    This is the part of the model summary.     `  I have two questions.  1. Why are there multiple input layers?  2. What are these numbers of output shape indicates?  I know the number 1433 is come from unique words of cora dataset.(right?)    I read the original paper of  , but still I don't understand..."
"Hi,    could you tell me what machine you run your code on and how long does it take to run the supervised_training.py as it is?"
"I wish got **embedding 250dim** , but Even if  I have set `--identity_dim 250 ` ï¼Œalways output 256d.    This is the command I ranï¼š     `python -m graphsage.unsupervised_train --train_prefix ./example_data/ppi --model graphsage_mean --max_total_steps 1000 --validate_iter 10 --identity_dim 250 `    and embedding shape ,I have printed.:    <img width=""298"" alt=""image"" src=""     **Is there any way to solve it**"
"Thank you for your paper and this implementation.    I'm reading your   about GraphSAGE and I don't get Eq.3 about MaxPooling.      **Why is it using h^k instead of h^(k-1) on the right hand in the Eq.3?**    Because, the MaxPooling function seems using the previous layer's hidden vector, which is h^(k-1), to calculate the current layer's hidden vector h^k.   That is, my question is that **why didn't use the following equation?:**    **h^k = max( {sigma( Wpool*h^(k-1) + b) )} )**  (I omit set definition of h to avoid messy)    Thank you in advance!"
"In the paper, there are two paragraphs as following:   > The mean aggregator is nearly equivalent to the convolutional propagation rule used in the transductive GCN framework  ,  GCN can be directly applied in an inductive setting.  Once trained on a graph, to learn another unseen graph, we can just feed another graph Laplacian matrix into GCN.    Why original GCN aggregator is transductive while the mean aggregator is inductive?  "
"I am trying to create input data files for the model but can't understand how to creat feat.npy file if I have two kinds of node in my graph (Let's say User and Item). Since I have 20 user attributes for every user and 50 item attributes for every item, how to create feat matrix for such graph? I would really appreciate your help on this. "
"Hello,    I was going through the code of the GraphSAGE module where I came across the method construct_test_adj() in the MinibatchIterator classes where I was expecting the adjacency matrix to be constructed only for the nodes labeled test (and/or validation) but I see that all the nodes were used. Since I am planning on implementing a supervised version of GraphSAGE, I was expecting the evaluation to be performed only the test nodes. Could you please explain the rationale behind this, is this because the test graphs could be disjointed?    Also please excuse me if my question is too trivial or does not make any sense.  "
"In the paper, line 7 of Algorithm 1 and line 13 of Algorithm 2 indicates there should be an l2 normalization after every layer. However, looking at the network generated by `example_supervised.sh` in Tensorboard, I only see a single l2_normalize node at the last layer. Am I misunderstanding the algorithm? Or is there likely to be something wrong with how I'm running the example?"
None
"Hi,     I was looking for a tool to introduce a ML on a knowledge graph stored in RDF store with a SPARQL endpoint. I noticed GraphSAGE can do more or less everything I am looking i.e. identify a possible graph pattern and apply additional label to enrich the knowledge graph. However I am unable to figure out how to use this tool to work on a graph represented in an RDF database with a SPARQL endpoint.    I am new to ML as well as RDF, so any pointers that suggests if its possible or not will be greatly appreciated.    Regards  Kiran    "
"Hi, I tried to use `save_val_embeddings` to dump the embedding of the nodes into a .npy file. But it seems that there is no guarantee that the nodes generated by `incremental_embed_feed_dict` are in certain order. So how do I know the node_id of each row in the dumped embedding matrix?               "
"Hi,    I found there is three graphs in your example data and in your paper show that we can apply multi graphs in the code.  However, I found that the data in the example_data file is single graph, could you pls show an example for multi graphs usage?    thank you and best regards."
"In File 'aggregators.py'  Line 159: shape of 'bias' is [self.output_dim]  Line 189: shape of 'output' is [self.output_dim*2] if flag 'Concat' is Ture  If flag 'self.bias' is True, Line 193 will raise an exception."
Hi    My question is as in the title. I am in a situation where I have a partially labeled graph on which I want to predict the nodes with the missing labels. Is it necessary that every nodes has a label in  -class_map.json    Hope you can help me.
"Hi there,    I have a question on sample numbers in layer 1 and 2.     I'm wondering which layer contains the 1-hop neighbors of the center node?  I first thought center node is 1-hop away from nodes in layer 1, and 2-hop away from nodes in layer 2.  That means the sampled result should be [1 (center node), 25 (1-hop nodes), 25*10 (2-hop nodes)].  But it turns out `samples` from `SampleAndAggregate.sample()` to have the shape [1, 10, 250]. I'm pretty confused here.    Thanks,  Serena  "
"Hi there,    I'm not sure how the identity features for the nodes are initialized.  I saw it's from   `self.embeds = tf.get_variable(""node_embeddings"", [adj.get_shape().as_list()[0], identity_dim])`  Does it mean it initializes from some uniform distribution for a given range using `glorot_uniform_initializer`?    Thanks,  Serena  "
"In the method of _build() in the class of SampleAndAggregate, there are codes of      Note that we only take the output of 'sampled_candidates' when call tf.nn.fixed_unigram_candidate_sampler, the parameter of  'true_classes' seem not use, so that, why we need 'labels' in here? And the output of 'sampled_candidates' also contains the elements in the 'labels', it means that 'self.neg_samples' not only contain the elements of disparate nodes but also contains the neighbor nodes, the sample rule is only base on the degrees of each node. But in the paper of ""Inductive Representation Learning on Large Graphs"", what it said is ""The graph-based loss function encourages nearby nodes to have similar representations while enforcing that the representations of disparate nodes are highly distinct"", I think it means the 'neg_samples' in code can only contain the disparate nodes but not the neighbor nodes. Or maybe neighbor nodes should be highly distinct when they are high degrees??"
"Hi there,    I'm trying to understand the model but am a little bit confused here.  `          self.inputs1 = placeholders[""batch1""]  self.inputs2 = placeholders[""batch2""]  `  I'm wondering why there are two inputs and two outputs? I saw the final node embeddings are save in model.output1. So then what are output2?    Thanks,  Serena"
"What can I do to apply graphSAGE in graph classification?  Certain pooling will be required I guess but how can I train them?  Intuitively, that requires every sample in a batch to come from the same graph.  Am I right? or is there any better way of doing this"
The val.txt has a lot of number.Does these number represent the id of nodes?   
"I think line 480 in models.py:     neg_aff = tf.matmul(self.outputs2, tf.transpose(self.neg_outputs)) + self.neg_outputs_bias    should be self.outputs1 instead of outputs2.     Is it a mistake? "
"hi, I don't understand the meaning of the second 'for' loop in    models.py  --class SampleAndAggregate(GeneralizedModel):  ----def aggregate(...)    It seems that it is not consistent with pseudo-code in your paper.    (By the way, your code is a little hard to follow T_T)"
"Hi, can I ask you a simple question?  When I tried to run the image in docker image, I met a problem shown as follow:  !   I have tried my best to fix it, but it doesn't work. Could you give me any suggestion about fixing it?  Thank you.  "
"Hi William,    I am trying to replicate your results on Reddit data. However, I would like to get understanding of source data. Can you specify what query you fired on Google Big Query for retrieving source data?     Thanks,  Ayush"
"I would like to test GraphSAGE on a static simple network with no features and class labels. All we know about the network is the connection of nodes. The input file is similar to:      which means node 0 is connected to 1 and 2, and the output is just embeddings based on the proximity information.    What do i need to modify the input data for getting those embeddings? thanks for your details! "
"Hi!    Great work!   Is there a simple way to output the embeddings also for the supervised version of graphSage? I was trying to add `save_val_embeddings` function, but got stuck. "
"Hi authors,    I have a question about the Node2Vec(DeepWalk) model you implemented. It seems that it only applies SkipGram model on a window size of 1, which means it only predicts context embeddings of immediate neighbors. Is this true?    Best,  Minjie"
In this line:         The loss is re-assigned to the link prediction loss. It should be `self.loss += ...`    (It doesn't affect anything since the weight decay is set to 0 anyways)
"I am trying to save the model obtained by running GraphSAGE on a graph with a large number of nodes (in the order of few million nodes). I would like to use the model later on data not in the original training, validation or test sets. I tried using a   object as follows, in `supervised_train.py`:     This worked nicely for some of the small examples I tried, but now I am facing an error message saying the GraphDef to be written is too large:       I tried to cut down on the amount of data to save by specifying `tf.Saver` to only write the trainable parameters:     However, the error persists.    Any ideas how I could save the necessary parts of the computational graph to be able to later load it from disk and apply it to new data?"
"Hi,  I am a little confused by this line.     what does `+ 1` mean? Is this intentional for any boundary conditions?    Thanks.    "
"Hi William,    Are there example code and data for the across learning example described in section 4.2 in your paper?  Currently, the PPI data and evaluation code seem like a single graph example.  Thank you very much for sharing the resources!    Cheers,  Oh-Hyun"
"     In ppi_eval.py,  your f1_score evaluation seems wired which is different from your implementations in citation_eval.py and reddit_eval.py.  According to my understanding,  `test_labels ` and `log.predict(test_embeds)` are of the shape [batch_size(5124), num_classes(121)],  so you actually calculate the per-class f1_score. Does this satisfy your expectation and why?"
"In the input-format description, id_map.json should change to class_map.json   -id_map.json -- A json-stored dictionary mapping the graph node ids to classes."
"Are you able to provide some explanation about the definition of the XENT loss?         That doesn't (appear to) follow Equation 1 in the paper -- specifically, where's the 0.01 constant coming from?  "
"In the unsupervised model, I notice that it use the second layers's num_samples as the number of neighbor nodes for the first layers. This lead to samples shape for next aggregate function become this:    - batch_size x       -      -      Thank you, I learned a lot from the code"
"In the unsupervised model, you do this operation twice:     on the following lines:     -     -      Is the repetition intentional?  Or is this a bug?"
"Can you clarify the expected behavior of the unsupervised example script?  I'm getting something like:         So the `train_loss` is decreasing, but the `val_loss` appears to be exactly the same.  Any thoughts?    Thanks  Ben"
"Do you happen to have the parameters for the supervised LSTM model on the Reddit dataset that yields 0.954? I'm running          on my machine, and getting results closer to  ~0.92.      Similar question, do you have any sense of the statistical significance in the differences between the various supervised GraphSAGE models? `mean`, `LSTM` and `pool` appear to outperform `gcn`, and certainly outperform the baseline models, but is there strong evidence that `LSTM` performs better than `mean`, given different random starts, etc?  (I ask because `LSTM` is significantly slower, so it'd be nice if I didn't have to use it...)    Thanks"
This still appears to be broken due to the api changes to dict.items/dict.iteritems etc. 
"Some specific versions of certain packages are needed, and we should make this easier for users.    In particular, we need networkx<2.0. "
"Hi        I am reading your code( now only mean aggregate).  I am confused why the sampling method is in a reversed layer order?     def sample(self, inputs, layer_infos, batch_size=None):          """""" Sample neighbors to be the supportive fields for multi-layer convolutions.            Args:              inputs: batch inputs              batch_size: the number of inputs (different for batch inputs and negative samples).          """"""                    if batch_size is None:              batch_size = self.batch_size          samples = [inputs]          # size of convolution support at each layer per node          support_size = 1          support_sizes = [support_size]          for k in range(len(layer_infos)):              t = len(layer_infos) - k - 1              support_size *= layer_infos[t].num_samples              sampler = layer_infos[t].neigh_sampler              node = sampler((samples[k], layer_infos[t].num_samples))              samples.append(tf.reshape(node, [support_size * batch_size,]))              support_sizes.append(support_size)          return samples, support_sizes"
"Hi, dear authors    In algorithm 1 line 4 of your paper, the aggregator is operating on the neighbors of each node. But I notice in your code, you have the adj_info and UniformNeighborSampler, which is used to generate the list of neighbor of each node. My question is are you sampling the same node in the neighborhood for multiple times and feed it to the aggregator?     Another question is that during the training,  with different Weight , we can correspondingly update h, just in the first iteration you initialized h as x. Do we need to update h based on algorithm 1,  in each iteration? If so, isn't this computationally expensive?     The last question is that, I assume the mini batch data is used for the following unsupervised or supervised task, not for algorithm 1?    Thanks a lot!"
"For small/non-dynamic graphs, we should incorporate an option to use one-hot identity vectors as features, since this will give better predictive performance (at the cost of much slower runtime). We will need to tweak the code to deal with the sparsity though. I think the best solution would be to use tf.nn.embedding_lookup, rather than materializing the sparse one-hot vectors. "
"Hello, I have 7 millions nodes. When I train the model, the use of memory is big,and tensorflow throws a error when create the variable of adj_info,so how to feed datas when the number of nodes is big? thanks!   Tensorflow throws a error like this:  >ValueErrorTraceback (most recent call last)    in  ()  ----> 1 train(train_data)    >  in train(train_data, test_data)      149                                       max_degree=FLAGS.max_degree,      150                                       context_pairs=context_pairs)  --> 151     adj_info = tf.Variable(tf.constant(minibatch.adj, dtype=tf.int32), trainable=False, name=""adj_info"")      152       153     if FLAGS.model == 'graphsage_mean':    >/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.pyc in constant(value, dtype, shape, name, verify_shape)      100   tensor_value = attr_value_pb2.AttrValue()      101   tensor_value.tensor.CopyFrom(  --> 102       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))      103   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)      104   const_tensor = g.create_op(    >/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.pyc in make_tensor_proto(values, dtype, shape, verify_shape)      431     if nparray.size * nparray.itemsize >= (1   433           ""Cannot create a tensor proto whose content is larger than 2GB."")      434     tensor_proto.tensor_content = nparray.tostring()      435     return tensor_proto    >ValueError: Cannot create a tensor proto whose content is larger than 2GB.      "
"Hi,I use the save method of the model to keep well-trained models,but there were the following mistakesï¼š  >  raise ValueError(""No variables to save"")  > ValueError: No variables to save    So how do I keep well-trained models ,thanks!"
"The NLL loss function in the newsvendor task is wrongly used.   NLLLoss() (in PyTorch) expects a log-probability as input. But your code sends the raw probability into it. So it is actually minimizing -sum pi. instead of minimizing - sum log(pi). Though this weird loss can still kind of maximize the likelihood, it is different from your description.   This happens in your task_net and mle_net.     Another bug: your main.py for  ""# Nonlinear MLE net"" actually is doing linear MLE net. You need to send the parameter ""is_nonlinear=True""."
"When I train the task_net from the power scheduling problem (modified to work with my data)  the SQP solving process takes forevor. While it runs, my GPU (Tesla K80) utilization hovers at only ~3%. I'm not sure if that is normal or what the bottleneck may be. However, this step significantly impacts training time. Training the rmse nets is extremely quick, of course.  "
I wonder where the dataset of electricity price and load  is from ? I hope to use and refer the dataset in my work.
"Hello I am trying to run the main.py file of battery_storage  and I am getting size issue as attached below.    `C:\Users\AppData\Local\Programs\Python\Python37\Scripts\battery_storage>main.py --save   C:\Users\AppData\Local\Programs\Python\Python37\Scripts\battery_storage  --nRuns 1  --paramSet 1  C:\Users\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\loss.py:445: UserWarning: Using a target size (torch.Size([500, 24])) that is different to the input size (torch.Size([500, 24])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.    return F.mse_loss(input, target, reduction=self.reduction)  Traceback (most recent call last):    File ""C:\Users\AppData\Local\Programs\Python\Python37\Scripts\battery_storage\main.py"", line 234, in        main()    File ""C:\Users\AppData\Local\Programs\Python\Python37\Scripts\battery_storage\main.py"", line 73, in main      model_rmse = nets.run_rmse_net(model_rmse, loaders_task, params, tensors_task)    File ""C:\Users\AppData\Local\Programs\Python\Python37\Scripts\battery_storage\nets.py"", line 55, in run_rmse_net      total_train_loss += train_loss.data[0] * X_train_.size(0)  IndexError: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item ()` in C++ to convert a 0-dim tensor to a number`"
"Hello  I am trying to reproduce the code for power scheduling. But I don't have access to GPU , so I am running codes on a windows system with CPU.In your code,I have removed cuda() wherever it was written.But I am getting errors which I am not able to resolve.Your support in solving issues will be highly appreciated.  I have attached modified main.py and nets.py files.    For power scheduling,I am getting error in main file as            `AttributeError                            Traceback (most recent call last)    in    ----> 1 model_rmse = nets.run_rmse_net(model_rmse, variables_rmse, X_train, Y_train)    ~\Untitled Folder 1\nets.py in run_rmse_net(model, variables, X_train, Y_train)       42         model.train()       43         train_loss = nn.MSELoss()(  ---> 44             model(variables['X_train_']), variables['Y_train_'])       45         train_loss.backward()       46         opt.step()    c:\users\appdata\local\programs\python\python37\lib\site-packages\torch\nn\modules\module.py in _call_impl(self, *input, **kwargs)      720             result = self._slow_forward(*input, **kwargs)      721         else:  --> 722             result = self.forward(*input, **kwargs)      723         for hook in itertools.chain(      724                 _global_forward_hooks.values(),    c:\users\appdata\local\programs\python\python37\lib\site-packages\torch\nn\modules\loss.py in forward(self, input, target)      443       444     def forward(self, input: Tensor, target: Tensor) -> Tensor:  --> 445         return F.mse_loss(input, target, reduction=self.reduction)      446       447     c:\users\appdata\local\programs\python\python37\lib\site-packages\torch\nn\functional.py in mse_loss(input, target, size_average, reduce, reduction)     2633                 mse_loss, tens_ops, input, target, size_average=size_average, reduce=reduce,     2634                 reduction=reduction)  -> 2635     if not (target.size() == input.size()):     2636         warnings.warn(""Using a target size ({}) that is different to the input size ({}). ""     2637                       ""This will likely lead to incorrect results due to broadcasting. ""    AttributeError: 'tuple' object has no attribute 'size'  ` "
"mldl@mldlUB1604:~/ub16_prj/e2e-model-learning/power_sched$ python3 main.py --save .  setGPU: Setting GPU to: 0  0 0.07094819098711014 0.01747439242899418  1 0.0608951672911644 0.015597431920468807  2 0.04586026072502136 0.012867853045463562  3 0.03554176539182663 0.011823796667158604  ..................................  997 0.002964276820421219 0.010087293572723866  998 0.0029753427952528 0.010150546208024025  999 0.0028919086325913668 0.011296983808279037  ERROR:root:An unexpected error occurred while tokenizing input  The following traceback may be corrupted or invalid  The error message is: ('EOF in multi-line string', (48, 66))    ---------------------------------------------------------------------------  TypeError                                 Traceback (most recent call last)  /usr/local/lib/python3.5/dist-packages/qpth-0.0.6-py3.5.egg/qpth/solvers/pdipm/batch.py in pre_factor_kkt(Q=  ( 0  ,.,.) =     1055.5512     0.0000     0.0000...ch.cuda.DoubleTensor of size 2553x24x24 (GPU 0)]  , G=  ( 0  ,.,.) =      1  -1   0  ...    0   0   0    ...ch.cuda.DoubleTensor of size 2553x46x24 (GPU 0)]  , A=[torch.cuda.DoubleTensor with no dimension]  )      357     try:  --> 358         Q_LU = Q.btrifact(pivot=False)          Q_LU = undefined          Q.btrifact =            global pivot = undefined      359     except:    TypeError: btrifact received an invalid combination of arguments - got (pivot=bool, ), but expected one of:   * ()        didn't match because some of the keywords were incorrect: pivot   * (torch.cuda.IntTensor info)      During handling of the above exception, another exception occurred:    RuntimeError                              Traceback (most recent call last)  /home/mldl/ub16_prj/e2e-model-learning/power_sched/main.py in  ()      151       152 if __name__=='__main__':  --> 153     main()          global main =      /home/mldl/ub16_prj/e2e-model-learning/power_sched/main.py in main()       71         model_rmse = nets.run_rmse_net(       72             model_rmse, variables_rmse, X_train, Y_train)  ---> 73         nets.eval_net(""rmse_net"", model_rmse, variables_rmse, params, save_folder)          global nets.eval_net =            model_rmse = Net (    (lin): Linear (149 -> 24)    (net): Sequential (      (0): Linear (149 -> 200)      (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True)      (2): ReLU ()      (3): Dropout (p = 0.2)      (4): Linear (200 -> 200)      (5): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True)      (6): ReLU ()      (7): Dropout (p = 0.2)      (8): Linear (200 -> 24)    )  )          variables_rmse = {'X_train_': Variable containing:  -4.0186e-01  7.3033e-02  3.3559e-02  ...  -1.6573e-01  4.8663e-02 -6.3155e-01   4.1893e-01  4.7020e-01  5.5965e-01  ...  -1.6573e-01  7.2976e-02 -6.3155e-01   9.7893e-01  1.0885e+00  1.3457e+00  ...  -1.6573e-01  9.7268e-02 -6.3155e-01                  ...                   â‹±                   ...                  -1.0027e+00 -9.6148e-01 -9.6799e-01  ...  -1.3273e-01 -1.4598e-01  1.5815e+00  -6.2014e-01 -6.1378e-01 -6.5050e-01  ...  -1.3273e-01 -1.2164e-01  1.5815e+00  -8.7676e-01 -8.8478e-01 -8.8041e-01  ...  -1.3273e-01 -9.7281e-02 -6.3233e-01  [torch.cuda.FloatTensor of size 2553x149 (GPU 0)]  , 'X_test_': Variable containing:  -0.1722 -0.0922 -0.0812  ...  -0.1327 -0.0729 -0.6323   0.0751  0.1430  0.2965  ...  -0.1327 -0.0485 -0.6323   0.5183  0.4600  0.7180  ...  -0.1327 -0.0241 -0.6323            ...             â‹±             ...            -1.0261 -1.1711 -1.2253  ...  -0.1327 -1.4622 -0.6323  -1.2920 -1.3552 -1.4059  ...  -0.1327 -1.4635 -0.6323  -1.0121 -1.1967 -1.2472  ...  -0.1327 -1.4645 -0.6323  [torch.cuda.FloatTensor of size 639x149 (GPU 0)]  , 'Y_test_': Variable containing:   1.5750  1.5000  1.4730  ...   1.8880  1.8380  1.7480   1.6700  1.5620  1.5500  ...   1.8470  1.8250  1.7090   1.6560  1.5600  1.5430  ...   1.7330  1.6720  1.5990            ...             â‹±             ...             1.2820  1.2070  1.1620  ...   1.6300  1.5460  1.4420   1.3420  1.2380  1.1910  ...   1.5690  1.4880  1.3880   1.2810  1.2810  1.2810  ...   1.2810  1.2810  1.2810  [torch.cuda.FloatTensor of size 639x24 (GPU 0)]  , 'Y_train_': Variable containing:   1.6384  1.5479  1.5014  ...   2.0454  2.0098  1.8751   1.7482  1.6577  1.6313  ...   2.0735  2.0128  1.9111   1.7923  1.7135  1.6666  ...   1.9348  1.9248  1.8024            ...             â‹±             ...             1.4260  1.3520  1.3000  ...   1.5640  1.5180  1.4510   1.3710  1.2990  1.2580  ...   1.6990  1.6590  1.6090   1.5220  1.4540  1.4040  ...   1.8120  1.7540  1.6820  [torch.cuda.FloatTensor of size 2553x24 (GPU 0)]  }          params = {'c_ramp': 0.4, 'n': 24, 'gamma_over': 0.5, 'gamma_under': 50}          save_folder = './0'       74        75         # Randomly construct hold-out set for task net training.    /home/mldl/ub16_prj/e2e-model-learning/power_sched/nets.py in eval_net(which='rmse_net', model=Net (    (lin): Linear (149 -> 24)    (net): Seque...opout (p = 0.2)      (8): Linear (200 -> 24)    )  ), variables={'X_test_': Variable containing:  -0.1722 -0.0922 -0.0812  .....[torch.cuda.FloatTensor of size 639x149 (GPU 0)]  , 'X_train_': Variable containing:  -4.0186e-01  7.3033e-02  3....torch.cuda.FloatTensor of size 2553x149 (GPU 0)]  , 'Y_test_': Variable containing:   1.5750  1.5000  1.4730  .....  [torch.cuda.FloatTensor of size 639x24 (GPU 0)]  , 'Y_train_': Variable containing:   1.6384  1.5479  1.5014  .....[torch.cuda.FloatTensor of size 2553x24 (GPU 0)]  }, params={'c_ramp': 0.4, 'gamma_over': 0.5, 'gamma_under': 50, 'n': 24}, save_folder='./0')      142       143     # Eval model on task loss  --> 144     Y_sched_train = solver(mu_pred_train.double(), sig_pred_train.double())          Y_sched_train = undefined          solver = SolveScheduling (  )          mu_pred_train.double =            sig_pred_train.double =        145     train_loss_task = task_loss(      146         Y_sched_train.float(), variables['Y_train_'], params)    /usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py in __call__(self=SolveScheduling (  ), *input=(Variable containing:   1.6559  1.5848  1.5492  .....torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]  , Variable containing:  1.00000e-02 *   1.9104  2.32...torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]  ), **kwargs={})      204       205     def __call__(self, *input, **kwargs):  --> 206         result = self.forward(*input, **kwargs)          result = undefined          self.forward =            input = (Variable containing:   1.6559  1.5848  1.5492  ...   2.0905  2.0244  1.8988   1.7428  1.6612  1.6278  ...   2.0502  1.9847  1.8801   1.7963  1.7084  1.6635  ...   1.9366  1.8893  1.7936            ...             â‹±             ...             1.4143  1.3457  1.3031  ...   1.6152  1.5751  1.5050   1.3843  1.3180  1.2787  ...   1.6053  1.5626  1.4952   1.5086  1.4384  1.4004  ...   1.8398  1.7719  1.6712  [torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]  , Variable containing:  1.00000e-02 *   1.9104  2.3216  2.4676  ...   6.8909  6.7232  6.3667   1.9104  2.3216  2.4676  ...   6.8909  6.7232  6.3667   1.9104  2.3216  2.4676  ...   6.8909  6.7232  6.3667            ...             â‹±             ...             1.9104  2.3216  2.4676  ...   6.8909  6.7232  6.3667   1.9104  2.3216  2.4676  ...   6.8909  6.7232  6.3667   1.9104  2.3216  2.4676  ...   6.8909  6.7232  6.3667  [torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]  )          kwargs = {}      207         for hook in self._forward_hooks.values():      208             hook_result = hook(self, input, result)    /home/mldl/ub16_prj/e2e-model-learning/power_sched/model_classes.py in forward(self=SolveScheduling (  ), mu=Variable containing:   1.6559  1.5848  1.5492  .....torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]  , sig=Variable containing:  1.00000e-02 *   1.9104  2.32...torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]  )      150             d2g = GQuadraticApprox(self.params[""gamma_under""],       151                 self.params[""gamma_over""])(z0, mu0, sig0)  --> 152             z0_new = SolveSchedulingQP(self.params)(z0, mu0, dg, d2g)          z0_new = undefined          global SolveSchedulingQP =            self.params = {'c_ramp': 0.4, 'n': 24, 'gamma_over': 0.5, 'gamma_under': 50}          z0 = Variable containing:   1.6559  1.5848  1.5492  ...   2.0905  2.0244  1.8988   1.7428  1.6612  1.6278  ...   2.0502  1.9847  1.8801   1.7963  1.7084  1.6635  ...   1.9366  1.8893  1.7936            ...             â‹±             ...             1.4143  1.3457  1.3031  ...   1.6152  1.5751  1.5050   1.3843  1.3180  1.2787  ...   1.6053  1.5626  1.4952   1.5086  1.4384  1.4004  ...   1.8398  1.7719  1.6712  [torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]            mu0 = Variable containing:   1.6559  1.5848  1.5492  ...   2.0905  2.0244  1.8988   1.7428  1.6612  1.6278  ...   2.0502  1.9847  1.8801   1.7963  1.7084  1.6635  ...   1.9366  1.8893  1.7936            ...             â‹±             ...             1.4143  1.3457  1.3031  ...   1.6152  1.5751  1.5050   1.3843  1.3180  1.2787  ...   1.6053  1.5626  1.4952   1.5086  1.4384  1.4004  ...   1.8398  1.7719  1.6712  [torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]            dg = Variable containing:  -24.7500 -24.7500 -24.7500  ...  -24.7500 -24.7500 -24.7500  -24.7500 -24.7500 -24.7500  ...  -24.7500 -24.7500 -24.7500  -24.7500 -24.7500 -24.7500  ...  -24.7500 -24.7500 -24.7500             ...               â‹±              ...              -24.7500 -24.7500 -24.7500  ...  -24.7500 -24.7500 -24.7500  -24.7500 -24.7500 -24.7500  ...  -24.7500 -24.7500 -24.7500  -24.7500 -24.7500 -24.7500  ...  -24.7500 -24.7500 -24.7500  [torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]            d2g = Variable containing:   1054.5512   867.7901   816.4561  ...    292.3651   299.6580   316.4354   1054.5512   867.7901   816.4561  ...    292.3651   299.6580   316.4354   1054.5512   867.7901   816.4561  ...    292.3651   299.6580   316.4354                ...                  â‹±                 ...                  1054.5512   867.7901   816.4561  ...    292.3651   299.6580   316.4354   1054.5512   867.7901   816.4561  ...    292.3651   299.6580   316.4354   1054.5512   867.7901   816.4561  ...    292.3651   299.6580   316.4354  [torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]        153             solution_diff = (z0-z0_new).norm().data[0]      154             print(""+ SQP Iter: {}, Solution diff = {}"".format(i, solution_diff))    /usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py in __call__(self=SolveSchedulingQP (  ), *input=(Variable containing:   1.6559  1.5848  1.5492  .....torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]  , Variable containing:   1.6559  1.5848  1.5492  .....torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]  , Variable containing:  -24.7500 -24.7500 -24.7500 ...torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]  , Variable containing:   1054.5512   867.7901   816...torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]  ), **kwargs={})      204       205     def __call__(self, *input, **kwargs):  --> 206         result = self.forward(*input, **kwargs)          result = undefined          self.forward =            input = (Variable containing:   1.6559  1.5848  1.5492  ...   2.0905  2.0244  1.8988   1.7428  1.6612  1.6278  ...   2.0502  1.9847  1.8801   1.7963  1.7084  1.6635  ...   1.9366  1.8893  1.7936            ...             â‹±             ...             1.4143  1.3457  1.3031  ...   1.6152  1.5751  1.5050   1.3843  1.3180  1.2787  ...   1.6053  1.5626  1.4952   1.5086  1.4384  1.4004  ...   1.8398  1.7719  1.6712  [torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]  , Variable containing:   1.6559  1.5848  1.5492  ...   2.0905  2.0244  1.8988   1.7428  1.6612  1.6278  ...   2.0502  1.9847  1.8801   1.7963  1.7084  1.6635  ...   1.9366  1.8893  1.7936            ...             â‹±             ...             1.4143  1.3457  1.3031  ...   1.6152  1.5751  1.5050   1.3843  1.3180  1.2787  ...   1.6053  1.5626  1.4952   1.5086  1.4384  1.4004  ...   1.8398  1.7719  1.6712  [torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]  , Variable containing:  -24.7500 -24.7500 -24.7500  ...  -24.7500 -24.7500 -24.7500  -24.7500 -24.7500 -24.7500  ...  -24.7500 -24.7500 -24.7500  -24.7500 -24.7500 -24.7500  ...  -24.7500 -24.7500 -24.7500             ...               â‹±              ...              -24.7500 -24.7500 -24.7500  ...  -24.7500 -24.7500 -24.7500  -24.7500 -24.7500 -24.7500  ...  -24.7500 -24.7500 -24.7500  -24.7500 -24.7500 -24.7500  ...  -24.7500 -24.7500 -24.7500  [torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]  , Variable containing:   1054.5512   867.7901   816.4561  ...    292.3651   299.6580   316.4354   1054.5512   867.7901   816.4561  ...    292.3651   299.6580   316.4354   1054.5512   867.7901   816.4561  ...    292.3651   299.6580   316.4354                ...                  â‹±                 ...                  1054.5512   867.7901   816.4561  ...    292.3651   299.6580   316.4354   1054.5512   867.7901   816.4561  ...    292.3651   299.6580   316.4354   1054.5512   867.7901   816.4561  ...    292.3651   299.6580   316.4354  [torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]  )          kwargs = {}      207         for hook in self._forward_hooks.values():      208             hook_result = hook(self, input, result)    /home/mldl/ub16_prj/e2e-model-learning/power_sched/model_classes.py in forward(self=SolveSchedulingQP (  ), z0=Variable containing:   1.6559  1.5848  1.5492  .....torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]  , mu=Variable containing:   1.6559  1.5848  1.5492  .....torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]  , dg=Variable containing:  -24.7500 -24.7500 -24.7500 ...torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]  , d2g=Variable containing:   1054.5512   867.7901   816...torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]  )      118         h = self.h.unsqueeze(0).expand(nBatch, self.h.size(0))      119   --> 120         out = QPFunction(verbose=False)(Q, p, G, h, self.e, self.e)          out = undefined          global QPFunction =            global verbose = undefined          Q = Variable containing:  ( 0  ,.,.) =     1055.5512     0.0000     0.0000  ...      0.0000     0.0000     0.0000       0.0000   868.7901     0.0000  ...      0.0000     0.0000     0.0000       0.0000     0.0000   817.4561  ...      0.0000     0.0000     0.0000                 ...                  â‹±                 ...                      0.0000     0.0000     0.0000  ...    293.3651     0.0000     0.0000       0.0000     0.0000     0.0000  ...      0.0000   300.6580     0.0000       0.0000     0.0000     0.0000  ...      0.0000     0.0000   317.4354    ( 1  ,.,.) =     1055.5512     0.0000     0.0000  ...      0.0000     0.0000     0.0000       0.0000   868.7901     0.0000  ...      0.0000     0.0000     0.0000       0.0000     0.0000   817.4561  ...      0.0000     0.0000     0.0000                 ...                  â‹±                 ...                      0.0000     0.0000     0.0000  ...    293.3651     0.0000     0.0000       0.0000     0.0000     0.0000  ...      0.0000   300.6580     0.0000       0.0000     0.0000     0.0000  ...      0.0000     0.0000   317.4354    ( 2  ,.,.) =     1055.5512     0.0000     0.0000  ...      0.0000     0.0000     0.0000       0.0000   868.7901     0.0000  ...      0.0000     0.0000     0.0000       0.0000     0.0000   817.4561  ...      0.0000     0.0000     0.0000                 ...                  â‹±                 ...                      0.0000     0.0000     0.0000  ...    293.3651     0.0000     0.0000       0.0000     0.0000     0.0000  ...      0.0000   300.6580     0.0000       0.0000     0.0000     0.0000  ...      0.0000     0.0000   317.4354   ...     (2550,.,.) =     1055.5512     0.0000     0.0000  ...      0.0000     0.0000     0.0000       0.0000   868.7901     0.0000  ...      0.0000     0.0000     0.0000       0.0000     0.0000   817.4561  ...      0.0000     0.0000     0.0000                 ...                  â‹±                 ...                      0.0000     0.0000     0.0000  ...    293.3651     0.0000     0.0000       0.0000     0.0000     0.0000  ...      0.0000   300.6580     0.0000       0.0000     0.0000     0.0000  ...      0.0000     0.0000   317.4354    (2551,.,.) =     1055.5512     0.0000     0.0000  ...      0.0000     0.0000     0.0000       0.0000   868.7901     0.0000  ...      0.0000     0.0000     0.0000       0.0000     0.0000   817.4561  ...      0.0000     0.0000     0.0000                 ...                  â‹±                 ...                      0.0000     0.0000     0.0000  ...    293.3651     0.0000     0.0000       0.0000     0.0000     0.0000  ...      0.0000   300.6580     0.0000       0.0000     0.0000     0.0000  ...      0.0000     0.0000   317.4354    (2552,.,.) =     1055.5512     0.0000     0.0000  ...      0.0000     0.0000     0.0000       0.0000   868.7901     0.0000  ...      0.0000     0.0000     0.0000       0.0000     0.0000   817.4561  ...      0.0000     0.0000     0.0000                 ...                  â‹±                 ...                      0.0000     0.0000     0.0000  ...    293.3651     0.0000     0.0000       0.0000     0.0000     0.0000  ...      0.0000   300.6580     0.0000       0.0000     0.0000     0.0000  ...      0.0000     0.0000   317.4354  [torch.cuda.DoubleTensor of size 2553x24x24 (GPU 0)]            p = Variable containing:  -1772.5911 -1401.5665 -1291.1130  ...   -638.0358  -633.3941  -627.4944  -1864.3215 -1468.0045 -1355.4388  ...   -626.1972  -621.4645  -621.5749  -1920.8175 -1508.9659 -1384.5890  ...   -592.8813  -592.7801  -594.1101                ...                  â‹±                 ...                 -1517.6264 -1193.8847 -1090.0132  ...   -498.5796  -498.3313  -502.4959  -1485.9994 -1169.8336 -1070.0472  ...   -495.6922  -494.5639  -499.3930  -1617.1337 -1274.4051 -1169.5335  ...   -564.4880  -557.4778  -555.2510  [torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]            G = Variable containing:  ( 0  ,.,.) =      1  -1   0  ...    0   0   0     0   1  -1  ...    0   0   0     0   0   1  ...    0   0   0       ...       â‹±       ...        -0  -0  -0  ...    1  -0  -0    -0  -0  -0  ...   -1   1  -0    -0  -0  -0  ...   -0  -1   1    ( 1  ,.,.) =      1  -1   0  ...    0   0   0     0   1  -1  ...    0   0   0     0   0   1  ...    0   0   0       ...       â‹±       ...        -0  -0  -0  ...    1  -0  -0    -0  -0  -0  ...   -1   1  -0    -0  -0  -0  ...   -0  -1   1    ( 2  ,.,.) =      1  -1   0  ...    0   0   0     0   1  -1  ...    0   0   0     0   0   1  ...    0   0   0       ...       â‹±       ...        -0  -0  -0  ...    1  -0  -0    -0  -0  -0  ...   -1   1  -0    -0  -0  -0  ...   -0  -1   1   ...     (2550,.,.) =      1  -1   0  ...    0   0   0     0   1  -1  ...    0   0   0     0   0   1  ...    0   0   0       ...       â‹±       ...        -0  -0  -0  ...    1  -0  -0    -0  -0  -0  ...   -1   1  -0    -0  -0  -0  ...   -0  -1   1    (2551,.,.) =      1  -1   0  ...    0   0   0     0   1  -1  ...    0   0   0     0   0   1  ...    0   0   0       ...       â‹±       ...        -0  -0  -0  ...    1  -0  -0    -0  -0  -0  ...   -1   1  -0    -0  -0  -0  ...   -0  -1   1    (2552,.,.) =      1  -1   0  ...    0   0   0     0   1  -1  ...    0   0   0     0   0   1  ...    0   0   0       ...       â‹±       ...        -0  -0  -0  ...    1  -0  -0    -0  -0  -0  ...   -1   1  -0    -0  -0  -0  ...   -0  -1   1  [torch.cuda.DoubleTensor of size 2553x46x24 (GPU 0)]            h = Variable containing:   0.4000  0.4000  0.4000  ...   0.4000  0.4000  0.4000   0.4000  0.4000  0.4000  ...   0.4000  0.4000  0.4000   0.4000  0.4000  0.4000  ...   0.4000  0.4000  0.4000            ...             â‹±             ...             0.4000  0.4000  0.4000  ...   0.4000  0.4000  0.4000   0.4000  0.4000  0.4000  ...   0.4000  0.4000  0.4000   0.4000  0.4000  0.4000  ...   0.4000  0.4000  0.4000  [torch.cuda.DoubleTensor of size 2553x46 (GPU 0)]            self.e = Variable containing:[torch.cuda.DoubleTensor with no dimension]        121         return out      122     /usr/local/lib/python3.5/dist-packages/qpth-0.0.6-py3.5.egg/qpth/qp.py in forward(self= , Q_=  ( 0  ,.,.) =     1055.5512     0.0000     0.0000...ch.cuda.DoubleTensor of size 2553x24x24 (GPU 0)]  , p_=  -1772.5911 -1401.5665 -1291.1130  ...   -638.03...torch.cuda.DoubleTensor of size 2553x24 (GPU 0)]  , G_=  ( 0  ,.,.) =      1  -1   0  ...    0   0   0    ...ch.cuda.DoubleTensor of size 2553x46x24 (GPU 0)]  , h_=   0.4000  0.4000  0.4000  ...   0.4000  0.4000  ...torch.cuda.DoubleTensor of size 2553x46 (GPU 0)]  , A_=[torch.cuda.DoubleTensor with no dimension]  , b_=[torch.cuda.DoubleTensor with no dimension]  )       89        90         if self.solver == QPSolvers.PDIPM_BATCHED:  ---> 91             self.Q_LU, self.S_LU, self.R = pdipm_b.pre_factor_kkt(Q, G, A)          self.Q_LU = undefined          self.S_LU = undefined          self.R = undefined          global pdipm_b.pre_factor_kkt =            Q =   ( 0  ,.,.) =     1055.5512     0.0000     0.0000  ...      0.0000     0.0000     0.0000       0.0000   868.7901     0.0000  ...      0.0000     0.0000     0.0000       0.0000     0.0000   817.4561  ...      0.0000     0.0000     0.0000                 ...                  â‹±                 ...                      0.0000     0.0000     0.0000  ...    293.3651     0.0000     0.0000       0.0000     0.0000     0.0000  ...      0.0000   300.6580     0.0000       0.0000     0.0000     0.0000  ...      0.0000     0.0000   317.4354    ( 1  ,.,.) =     1055.5512     0.0000     0.0000  ...      0.0000     0.0000     0.0000       0.0000   868.7901     0.0000  ...      0.0000     0.0000     0.0000       0.0000     0.0000   817.4561  ...      0.0000     0.0000     0.0000                 ...                  â‹±                 ...                      0.0000     0.0000     0.0000  ...    293.3651     0.0000     0.0000       0.0000     0.0000     0.0000  ...      0.0000   300.6580     0.0000       0.0000     0.0000     0.0000  ...      0.0000     0.0000   317.4354    ( 2  ,.,.) =     1055.5512     0.0000     0.0000  ...      0.0000     0.0000     0.0000       0.0000   868.7901     0.0000  ...      0.0000     0.0000     0.0000       0.0000     0.0000   817.4561  ...      0.0000     0.0000     0.0000                 ...                  â‹±                 ...                      0.0000     0.0000     0.0000  ...    293.3651     0.0000     0.0000       0.0000     0.0000     0.0000  ...      0.0000   300.6580     0.0000       0.0000     0.0000     0.0000  ...      0.0000     0.0000   317.4354   ...     (2550,.,.) =     1055.5512     0.0000     0.0000  ...      0.0000     0.0000     0.0000       0.0000   868.7901     0.0000  ...      0.0000     0.0000     0.0000       0.0000     0.0000   817.4561  ...      0.0000     0.0000     0.0000                 ...                  â‹±                 ...                      0.0000     0.0000     0.0000  ...    293.3651     0.0000     0.0000       0.0000     0.0000     0.0000  ...      0.0000   300.6580     0.0000       0.0000     0.0000     0.0000  ...      0.0000     0.0000   317.4354    (2551,.,.) =     1055.5512     0.0000     0.0000  ...      0.0000     0.0000     0.0000       0.0000   868.7901     0.0000  ...      0.0000     0.0000     0.0000       0.0000     0.0000   817.4561  ...      0.0000     0.0000     0.0000                 ...                  â‹±                 ...                      0.0000     0.0000     0.0000  ...    293.3651     0.0000     0.0000       0.0000     0.0000     0.0000  ...      0.0000   300.6580     0.0000       0.0000     0.0000     0.0000  ...      0.0000     0.0000   317.4354    (2552,.,.) =     1055.5512     0.0000     0.0000  ...      0.0000     0.0000     0.0000       0.0000   868.7901     0.0000  ...      0.0000     0.0000     0.0000       0.0000     0.0000   817.4561  ...      0.0000     0.0000     0.0000                 ...                  â‹±                 ...                      0.0000     0.0000     0.0000  ...    293.3651     0.0000     0.0000       0.0000     0.0000     0.0000  ...      0.0000   300.6580     0.0000       0.0000     0.0000     0.0000  ...      0.0000     0.0000   317.4354  [torch.cuda.DoubleTensor of size 2553x24x24 (GPU 0)]            G =   ( 0  ,.,.) =      1  -1   0  ...    0   0   0     0   1  -1  ...    0   0   0     0   0   1  ...    0   0   0       ...       â‹±       ...        -0  -0  -0  ...    1  -0  -0    -0  -0  -0  ...   -1   1  -0    -0  -0  -0  ...   -0  -1   1    ( 1  ,.,.) =      1  -1   0  ...    0   0   0     0   1  -1  ...    0   0   0     0   0   1  ...    0   0   0       ...       â‹±       ...        -0  -0  -0  ...    1  -0  -0    -0  -0  -0  ...   -1   1  -0    -0  -0  -0  ...   -0  -1   1    ( 2  ,.,.) =      1  -1   0  ...    0   0   0     0   1  -1  ...    0   0   0     0   0   1  ...    0   0   0       ...       â‹±       ...        -0  -0  -0  ...    1  -0  -0    -0  -0  -0  ...   -1   1  -0    -0  -0  -0  ...   -0  -1   1   ...     (2550,.,.) =      1  -1   0  ...    0   0   0     0   1  -1  ...    0   0   0     0   0   1  ...    0   0   0       ...       â‹±       ...        -0  -0  -0  ...    1  -0  -0    -0  -0  -0  ...   -1   1  -0    -0  -0  -0  ...   -0  -1   1    (2551,.,.) =      1  -1   0  ...    0   0   0     0   1  -1  ...    0   0   0     0   0   1  ...    0   0   0       ...       â‹±       ...        -0  -0  -0  ...    1  -0  -0    -0  -0  -0  ...   -1   1  -0    -0  -0  -0  ...   -0  -1   1    (2552,.,.) =      1  -1   0  ...    0   0   0     0   1  -1  ...    0   0   0     0   0   1  ...    0   0   0       ...       â‹±       ...        -0  -0  -0  ...    1  -0  -0    -0  -0  -0  ...   -1   1  -0    -0  -0  -0  ...   -0  -1   1  [torch.cuda.DoubleTensor of size 2553x46x24 (GPU 0)]            A = [torch.cuda.DoubleTensor with no dimension]         92             zhats, self.nus, self.lams, self.slacks = pdipm_b.forward(       93                 Q, p, G, h, A, b, self.Q_LU, self.S_LU, self.R,    /usr/local/lib/python3.5/dist-packages/qpth-0.0.6-py3.5.egg/qpth/solvers/pdipm/batch.py in pre_factor_kkt(Q=  ( 0  ,.,.) =     1055.5512     0.0000     0.0000...ch.cuda.DoubleTensor of size 2553x24x24 (GPU 0)]  , G=  ( 0  ,.,.) =      1  -1   0  ...    0   0   0    ...ch.cuda.DoubleTensor of size 2553x46x24 (GPU 0)]  , A=[torch.cuda.DoubleTensor with no dimension]  )      362 Please make sure that your Q matrix is PSD and has      363 a non-zero diagonal.  --> 364 """""")          global Factor = undefined          global the = undefined          global U22 = undefined          global block = undefined          global that = undefined          global we = undefined          global can = undefined          global only = undefined          global do = undefined          global after = undefined          global know = undefined          global D = undefined      365       366     # S = [ A Q^{-1} A^T        A Q^{-1} G^T          ]    RuntimeError:   qpth Error: Cannot perform LU factorization on Q.  Please make sure that your Q matrix is PSD and has  a non-zero diagonal.    > /usr/local/lib/python3.5/dist-packages/qpth-0.0.6-py3.5.egg/qpth/solvers/pdipm/batch.py(364)pre_factor_kkt()      362 Please make sure that your Q matrix is PSD and has      363 a non-zero diagonal.  --> 364 """""")      365       366     # S = [ A Q^{-1} A^T        A Q^{-1} G^T          ]    ipdb>   "
"mldl@mldlUB1604:~/ub16_prj/e2e-model-learning/newsvendor$ python3 main.py --save .  setGPU: Setting GPU to: 0  316.580809054  501.94985305  TEST SET RESULTS:                      Average loss: 1226.7804  Epoch: 0 [100/100 (100%)] Loss: 986.8513  986.851318359375 1226.7803955078125  TEST SET RESULTS:                      Average loss: 1225.4849  Epoch: 1 [100/100 (100%)] Loss: 979.9048  979.9048461914062 1225.48486328125    ...........................................    Epoch: 999 [100/100 (100%)] Loss: 191.7679  191.7679443359375 528.0696411132812  528.0696411132812  /home/mldl/ub16_prj/e2e-model-learning/newsvendor/plot.py:36: MatplotlibDeprecationWarning: The set_axis_bgcolor function was deprecated in version 2.0. Use set_facecolor instead.    ax.set_axis_bgcolor(""none"")  /usr/local/lib/python3.5/dist-packages/numpy/core/numeric.py:531: UserWarning: Warning: converting a masked element to nan.    return array(a, dtype, copy=False, order=order)  /usr/local/lib/python3.5/dist-packages/matplotlib/axes/_base.py:2903: UserWarning: Attempting to set identical left==right results  in singular transformations; automatically expanding.  left=100.0, right=100.0    'left=%s, right=%s') % (left, right))  *** Error in `pdonti..linear': free(): invalid pointer: 0x00007ff6f29b4ac0 ***  ======= Backtrace: =========  /lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7ff6f7bde7e5]  /lib/x86_64-linux-gnu/libc.so.6(+0x7fe0a)[0x7ff6f7be6e0a]  /lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7ff6f7bea98c]  /usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZNSt15basic_stringbufIcSt11char_traitsIcESaIcEE8overflowEi+0x181)[0x7ff6df542fa1]  /usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZNSt15basic_streambufIcSt11char_traitsIcEE6xsputnEPKcl+0x89)[0x7ff6df599e79]  /usr/local/lib/python3.5/dist-packages/torch/lib/libshm.so(_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l+0x1c5)[0x7ff6f2729235]  /usr/local/lib/python3.5/dist-packages/matplotlib/ttconv.cpython-35m-x86_64-linux-gnu.so(_ZN14TTStreamWriter6printfEPKcz+0xd2)[0x7ff69c3384a2]  /usr/local/lib/python3.5/dist-packages/matplotlib/ttconv.cpython-35m-x86_64-linux-gnu.so(_ZN12GlyphToType3C2ER14TTStreamWriterP6TTFONTib+0x2e2)[0x7ff69c3370f2]  /usr/local/lib/python3.5/dist-packages/matplotlib/ttconv.cpython-35m-x86_64-linux-gnu.so(_Z17tt_type3_charprocR14TTStreamWriterP6TTFONTi+0x18)[0x7ff69c337438]  /usr/local/lib/python3.5/dist-packages/matplotlib/ttconv.cpython-35m-x86_64-linux-gnu.so(_Z17get_pdf_charprocsPKcRSt6vectorIiSaIiEER20TTDictionaryCallback+0x384)[0x7ff69c335d84]  /usr/local/lib/python3.5/dist-packages/matplotlib/ttconv.cpython-35m-x86_64-linux-gnu.so(+0x7209)[0x7ff69c333209]  pdonti..linear(PyCFunction_Call+0x77)[0x4e9bc7]  pdonti..linear(PyEval_EvalFrameEx+0x614)[0x524414]  pdonti..linear[0x52d82f]  pdonti..linear(PyEval_EvalFrameEx+0x5532)[0x529332]  pdonti..linear[0x52d82f]  pdonti..linear(PyEval_EvalFrameEx+0x5532)[0x529332]  pdonti..linear(PyEval_EvalFrameEx+0x4a14)[0x528814]  pdonti..linear(PyEval_EvalFrameEx+0x4a14)[0x528814]  pdonti..linear(PyEval_EvalCodeEx+0x13b)[0x52e12b]  pdonti..linear[0x4ebdd7]  pdonti..linear(PyObject_Call+0x47)[0x5b7167]  pdonti..linear(PyEval_EvalFrameEx+0x24af)[0x5262af]  pdonti..linear(PyEval_EvalCodeEx+0x13b)[0x52e12b]  pdonti..linear[0x4ebdd7]  pdonti..linear(PyObject_Call+0x47)[0x5b7167]  pdonti..linear(PyEval_EvalFrameEx+0x24af)[0x5262af]  pdonti..linear[0x52d2e3]  pdonti..linear(PyEval_EvalFrameEx+0x5532)[0x529332]  pdonti..linear(PyEval_EvalFrameEx+0x4a14)[0x528814]  pdonti..linear(PyEval_EvalFrameEx+0x4a14)[0x528814]  pdonti..linear[0x52d2e3]  pdonti..linear(PyEval_EvalCode+0x1f)[0x52dfdf]  pdonti..linear[0x5fd2c2]  pdonti..linear(PyRun_FileExFlags+0x9a)[0x5ff76a]  pdonti..linear(PyRun_SimpleFileExFlags+0x1bc)[0x5ff95c]  pdonti..linear(Py_Main+0x456)[0x63e7d6]  pdonti..linear(main+0xe1)[0x4cfe41]  /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7ff6f7b87830]  pdonti..linear(_start+0x29)[0x5d5f29]  ======= Memory map: ========  00400000-007a8000 r-xp 00000000 08:22 53875828                           /usr/bin/python3.5  009a8000-009aa000 r--p 003a8000 08:22 53875828                           /usr/bin/python3.5  009aa000-00a41000 rw-p 003aa000 08:22 53875828                           /usr/bin/python3.5  00a41000-00a72000 rw-p 00000000 00:00 0   0136b000-52c8d000 rw-p 00000000 00:00 0                                  [heap]  200000000-200100000 rw-s 11782c000 00:06 552                             /dev/nvidiactl  200100000-200104000 rw-s 116faf000 00:06 552                             /dev/nvidiactl  200104000-200120000 ---p 00000000 00:00 0   "
"Based on the default, Im wondering it takes too much time to train the model.  Can you tell me how many number of rounds, batch sizes, and gpus you've used for training to get the final recall value that you wrote at the Readme.md?"
"Hello, thanks for sharing.  I met a problem when I try to download the pre-trained model. When I visit your link for that, the website shows ""This shared file or folder link has been removed or is unavailable to you."".   I would be grateful if you can provide the new link or methods to download your pre-trained model!  "
"hi @anewell   I want to reproduce the  same results as in your paper. Currentï¼Œthe program has been run throughï¼Œ  but I don't know what the documents ( e.g. `imscales, im_to_roi_idx, num_rois, rpn_rois,rpn_scores` those are included in proposals.h) are respectively?  I also looked up the code base that generated this fileï¼Œbut, unfortunately, not found . If you can tell me the answer of the question, I will be very grateful.  "
"Hi    I followed the steps, it showed that it started training but unfortunately the model didn't train. Anyway I could know where it went wrong?    Even though I am getting good loss curves.  "
"I find your code for counting rel score_loss(graph.py/539) is:  score_loss = calc.bce_loss(tf.squeeze(rel_scores_[:opt.obj_slots*num_objs],1),                                             rel_scores_gt[:opt.obj_slots*num_objs])  I think opt.obj_slots and  num_objs  should be replaced by  opt.rel_slots and num_rels."
"Hi, thanks for sharing your codes.  And would you mind telling me the number of training iterations?"
"I've noticed that in your model.hg  ""  #cnv = layers.res_block  cnv = lambda x,y: layers.cnv(x,y,3)  ""  So does it mean that in your px2graph network, the hourglass structure uses a single conv to replace res_block and it improves the performance ? "
"hello,I am a beginner of this region.I dont know why should we use munkres-tensorflow.  And,I can't download the pretrained model now,may be the superlink is dead."
"Hi, thanks for sharing the code.    I have a question about the training settings. To train the model, we need to specify the sg_task to one of three options [PR|CL|SG]. I am not familiar with tensorflow. Maybe I am wrong, but I am wondering why it is needed to specify the sg_task during training. As far as I know about the scene graph generation task, the three options are merely specified during evaluation, which means that the training should be the same to different settings, right?    thanks in advance for your reply."
"I don't know if you'll see this article, but in the load_prediction function, can you explain me more details about obj.score and rel.score? And is it plausible to think that object class index is the highest index in obj.class_score?"
"Hi Alejandro,  First off, thanks for sharing your code. However there seems to be some issue with it. I cloned your repo as is, put the data in required `data/` dir but when I run it, i get the following stacktrace.       Any Idea why this could be happening?        Using the following setup:  tensorflow version: 1.3.0  munkres-tensorflow compiled as per instructions.  python: 3.6  "
"I train the model with sg_task as SG ,but the results are too bad.  train: python main.py -e second --sg_task=SG --batchsize=20 --gpu_choice='0,1'  predict: python main.py --branch second -e my_results --predict valid               python eval.py my_results/valid_preds  R@50: 0.03502271674069844  R@100: 0.04519653756966068  It' OK?"
"I want to test with other images, not samples in visual genome dataset,  how to modify the code?"
"I ran the training codes twice. The first time it got stuck at the 2nd `2000` iteration, and the second time it got stuck at the 3rd `2000` iteration.         I am not sure why it happened."
"Hi apologies @daniellevy, @tachim,  @kuleshov, @llonchj   ,  I am getting errors on running some of the examples.     (myenv) [andrewcz@andrewcz-pc examples]$ python nice_synthetic.py   Traceback (most recent call last):    File ""nice_synthetic.py"", line 13, in        from a_nice_mc.objectives.bayes_logistic_regression.synthetic import Synthetic  ModuleNotFoundError: No module named 'a_nice_mc.objectives'    Im not sure what im doing wrong.  Many thanks,  Best,  Andrew"
"First of all, very nice work! @jiamings  I'm not sure where to ask this question so I'm posting it here. My main concern is that feed forward neural networks are bad at extrapolation. Could fitting the proposal distribution to certain samples, e.g. the bootstrapped samples described in the paper, hampers ergodicity? "
"Hi,    I want to try to use a-nice-mc to sample from the posterior weights of a neural net but in my initial experiments for a relatively small net I find that I'm not able to learn to draw reasonable samples even after 60000 training iterations.    I'm training a one-layer feedforward net with a hidden width of 50 units to perform regression on the Boston housing data-set.    I think the issue is that bootstrapping from the randomly initialised model is inadequate for such a high dimensional space. I'm going to try to bootstrap instead from samples drawn using HMC.    Do you have any advice or thoughts on this before I embark? Would you like to include the bootstrap in the repo?    thanks,    Raza"
"   For different step, it seems that you use the same stepsize. I guess `stepsize` should be replaced with `s`.  @jiamings "
"The dataset being used to run the program - small.mat file, I am unable to open and view its contents in my local machine. Can you provide some other file that allows me to view the content for understanding the code better?    Kindly help me out.  @gingsmith "
"Hi,    I wanna run the experiments on 3 datasets but I just see only small.mat in data folder. Could you help how can I run 3 dataset in the papers?    Thanks"
_Originally posted by @kudhru in  
"Hi    Are the   and   functions used for Mocha implementation, same as those mentioned in the   (Equation 1 and Equation 3) ? I was wondering if the L2 regularization term is missing in the code as mentioned in the MTL regularizer (Equation 2 in the paper).     Thanks  Dhruv"
"     Hi     Should the above be   `primal_new = mean(max(0.0, 1.0 - preds)) + lambda * (w' * w);`   instead?"
"Currently, I want to run grid-search and cross-validation on your code, but I did not know how to adjust those parameters. Would you please provide some suggestions here about parameter selection? Thank you!"
"I get this error:         I dwonload it from  , then get this error:       "
None
"Hi ,thanks for your code! I want to know what I should to do if I use  pytorch_structure2vec do this work(TSP instance)?"
"Excuse me!Thank you very much for your excellent workã€‚When I try to run this code,I could not find appropriate intel mkl Download link with graphnnâ€˜s environment""   Could you help me ? Thank you very much ."
"Hi!  I'm training another combinatorial problem and it works well. But, sometimes when I run  `./run_nstep_dqn.sh` this is stopped without error message.     In the system log appears this:     `kernel: traps: python[7276] trap divide error ip:7f96bd2cea0b sp:7fffbb710190 error:0 in libcapmds.so[7f96bd202000+106000]`    I think that occur because is trying divide a floating-point by zero, but what could be the cause of it?  Thanks."
"If I want to test the tsp instances, do I have to re-train the dqn model?"
"I was tested in real-world data using Infonet data that uploaded in readme link.  I already compiled mvc_lib and so much warning arise.  And when i try to run realworld_s2v_mvc these error come up.   Please help me.       File ""/home/juseong/graph_comb_opt/code/realworld_s2v_mvc/../memetracker/meme.py"", line 50, in build_full_graph      for edge in g.edges_iter(data=True):  AttributeError: 'Graph' object has no attribute 'edges_iter'    "
"Hi,   Thank you for sharing your code.    I am trying to understand how to write the equation for Q in s2v_mvc, given the additional auxiliary input. Following the notation in the paper, is it like this :    <img width=""271"" alt=""q_function"" src=""     Where _a^{T}_ is the auxiliary input and _Î¸_{8}_ is 3-dimensional. Am I right? If I am not, can you explain how to write the equation for s2v_mvc?     Thank you!"
!   !   
when I run ./run_nstep_dqn.sh in s2v_tsp2d  it returns   !       and some problems also happen when I run ./run_nstep_dqn in s2v_mvc  !     can you help me ??
"I find it is not easy to understand how does the nn works, e.g., the func 'QNet::SetupGraphInput()' and ' QNet::BuildNet()', so can you give more detailed instructions about the code? thanks a lot!"
"When I run the first command,  `[..]> git clone --recursive    I get this error, which seems to be related to outdated ssh keys online?   "
"Great work! I have been trying to use the trained model. I figured that only a few of the arguments are needed to initialize MvcLib so I tried `api = MvcLib(['test.py', '-dev_id', 0, '-mem_size', 5000, '-num_env', 10, '-max_n', 20, '-batch_size', 64, '-n_step', 5])` to initiate `api`. However, it seems to be throwing a Segmentation Fault.     On running the evaluation script - `./run_eval.sh`, its throwing this error:       I have followed the instructions to build graphnn and maxcut/ and mvc builds. My training scripts worked and created a results folder with model_name.model files in there. I want to load a model and use it for my problem. Is it possible to have a convinient way to use the models?"
"Hello,    I was going through the code for s2v_tsp2d and was wondering if you could clarify something. When you call the predict function you use `arg_max` to select the best node even though it is a minimization problem.  I also tried setting `int sign = -1;`  in tsp2d_env and still got minimum tour lengths. Could you explain where the minimization is occurring ?    Thank you for your time !"
Could you tell me how to build graphnn?  And I want to know whether use pytorch version graphnn?
None
"I'm able to run the generate synthetic data and train sessions. I use python 2.7.18 for both training and testing. I use cPickle load. The evaluate.py can't load the model generated from the training session. The error is:    Traceback (most recent call last):    File ""evaluate.py"", line 64, in        g = cp.load(f)    File ""/usr/lib/python2.7/pickle.py"", line 1384, in load      return Unpickler(file).load()    File ""/usr/lib/python2.7/pickle.py"", line 864, in load      dispatch     File ""/usr/lib/python2.7/pickle.py"", line 892, in load_proto      raise ValueError, ""unsupported pickle protocol: %d"" % proto  ValueError: unsupported pickle protocol: 5    How to fix this issue? Thanks for your help in advance.  "
"Hello, I have a question about the decay factor (discount factor) in N-step DQN (paper & implementation)  I found the following equation in the paper(page 6; first paragraph):  !   As far as I know, discount factor should be applied as following:  !   I checked the implementation of tsp2d(code/s2v_tsp2d/tsp2d_lib/src/tsp2d_lib.cpp), and in the `Fit()` function, former version of equation is implemented.       I'm currently studying the DRL, so I might be wrong... But since I couldn't also found the reason why you used discount factor in that way, I write this issue.  Thank you for providing nice implementation! I've really learned a lot from it!"
"Hi,   Thanks for the code. I am trying to run the MaxCut algorithm, but it seems that the loss is always 0. Any idea what could be going wrong?  Thanks"
"When do the Test, how to chose the S0 (first state)? If you make the wrong choice in the first step, you're going to end up with a bad result.  I can't understand how the selection of the first state in the program.    _Originally posted by @NH333 in  "
"/tmp/ccXHgDI5.o: In function `LoadModel':  /home/anqichen/graph_comb_opt/code/s2v_mvc/mvc_lib/src/mvc_lib.cpp:29: undefined reference to `gnn::ParamSet ::Load(std::__cxx11::basic_string , std::allocator  >)'  /tmp/ccXHgDI5.o: In function `SaveModel':  /home/anqichen/graph_comb_opt/code/s2v_mvc/mvc_lib/src/mvc_lib.cpp:36: undefined reference to `gnn::ParamSet ::Save(std::__cxx11::basic_string , std::allocator  >)'  build/lib/nn_api.o: In function `Predict(std::vector , std::allocator  > >&, std::vector  >*, std::allocator  >*> >&, std::vector  >*, std::allocator  >*> >&)':  /home/anqichen/graph_comb_opt/code/s2v_mvc/mvc_lib/src/lib/nn_api.cpp:31: undefined reference to `gnn::FactorGraph::FeedForward(std::vector , std::allocator  > >, std::map , std::allocator  >, void*, std::less , std::allocator  > >, std::allocator , std::allocator  > const, void*> > >, gnn::Phase, unsigned int)'  build/lib/nn_api.o: In function `Fit(std::vector , std::allocator  > >&, std::vector  >*, std::allocator  >*> >&, std::vector  >&, std::vector  >&)':  /home/anqichen/graph_comb_opt/code/s2v_mvc/mvc_lib/src/lib/nn_api.cpp:80: undefined reference to `gnn::FactorGraph::FeedForward(std::vector , std::allocator  > >, std::map , std::allocator  >, void*, std::less , std::allocator  > >, std::allocator , std::allocator  > const, void*> > >, gnn::Phase, unsigned int)'  build/lib/qnet.o: In function `void __gnu_cxx::new_allocator ::construct , std::allocator  >&>(gnn::GraphVar*, std::__cxx11::basic_string , std::allocator  >&)':  /NIRAL/work/jprieto/install/include/c++/5.3.0/ext/new_allocator.h:120: undefined reference to `gnn::GraphVar::GraphVar(std::__cxx11::basic_string , std::allocator  >)'  build/lib/qnet.o: In function `void __gnu_cxx::new_allocator  >::construct , std::__cxx11::basic_string , std::allocator  >&>(gnn::TensorVarTemplate *, std::__cxx11::basic_string , std::allocator  >&)':  /NIRAL/work/jprieto/install/include/c++/5.3.0/ext/new_allocator.h:120: undefined reference to `gnn::TensorVarTemplate ::TensorVarTemplate(std::__cxx11::basic_string , std::allocator  >)'  build/lib/qnet.o: In function `void __gnu_cxx::new_allocator  >::construct , std::__cxx11::basic_string , std::allocator  >&>(gnn::TensorVarTemplate *, std::__cxx11::basic_string , std::allocator  >&)':  /NIRAL/work/jprieto/install/include/c++/5.3.0/ext/new_allocator.h:120: undefined reference to `gnn::TensorVarTemplate ::TensorVarTemplate(std::__cxx11::basic_string , std::allocator  >)'  build/lib/qnet.o: In function `gnn::TensorVarTemplate ::TensorVarTemplate(std::__cxx11::basic_string , std::allocator  >, std::vector  >)':  /home/anqichen/graph_comb_opt/code/s2v_mvc/mvc_lib/../../../graphnn/include/nn/variable.h:204: undefined reference to `gnn::TensorVarTemplate ::TensorVarTemplate(std::__cxx11::basic_string , std::allocator  >, std::vector  >)'  build/lib/qnet.o: In function `void __gnu_cxx::new_allocator  >::construct , std::__cxx11::basic_string , std::allocator  >&>(gnn::MatMul *, std::__cxx11::basic_string , std::allocator  >&)':  /NIRAL/work/jprieto/install/include/c++/5.3.0/ext/new_allocator.h:120: undefined reference to `gnn::MatMul ::MatMul(std::__cxx11::basic_string , std::allocator  >, gnn::Trans, gnn::Trans, gnn::PropErr)'  build/lib/qnet.o: In function `void __gnu_cxx::new_allocator  >::construct , std::__cxx11::basic_string , std::allocator  >&>(gnn::ReLU *, std::__cxx11::basic_string , std::allocator  >&)':  /NIRAL/work/jprieto/install/include/c++/5.3.0/ext/new_allocator.h:120: undefined reference to `gnn::ReLU ::ReLU(std::__cxx11::basic_string , std::allocator  >, gnn::PropErr)'  build/lib/qnet.o: In function `void __gnu_cxx::new_allocator  >::construct , std::__cxx11::basic_string , std::allocator  >&>(gnn::ElewiseAdd *, std::__cxx11::basic_string , std::allocator  >&)':  /NIRAL/work/jprieto/install/include/c++/5.3.0/ext/new_allocator.h:120: undefined reference to `gnn::ElewiseAdd ::ElewiseAdd(std::__cxx11::basic_string , std::allocator  >, std::vector  >, gnn::PropErr)'  build/lib/qnet.o: In function `void __gnu_cxx::new_allocator  >::construct , std::__cxx11::basic_string , std::allocator  >&>(gnn::ConcatCols *, std::__cxx11::basic_string , std::allocator  >&)':  /NIRAL/work/jprieto/install/include/c++/5.3.0/ext/new_allocator.h:120: undefined reference to `gnn::ConcatCols ::ConcatCols(std::__cxx11::basic_string , std::allocator  >, gnn::PropErr)'  build/lib/qnet.o: In function `void __gnu_cxx::new_allocator  >::construct , std::__cxx11::basic_string , std::allocator  >&>(gnn::SquareError *, std::__cxx11::basic_string , std::allocator  >&)':  /NIRAL/work/jprieto/install/include/c++/5.3.0/ext/new_allocator.h:120: undefined reference to `gnn::SquareError ::SquareError(std::__cxx11::basic_string , std::allocator  >, gnn::PropErr)'  build/lib/qnet.o: In function `void __gnu_cxx::new_allocator  >::construct , std::__cxx11::basic_string , std::allocator  >&>(gnn::ReduceMean *, std::__cxx11::basic_string , std::allocator  >&)':  /NIRAL/work/jprieto/install/include/c++/5.3.0/ext/new_allocator.h:120: undefined reference to `gnn::ReduceMean ::ReduceMean(std::__cxx11::basic_string , std::allocator  >, int, bool, gnn::PropErr)'  build/lib/qnet.o: In function `gnn::Node2NodeMsgPass ::Node2NodeMsgPass(std::__cxx11::basic_string , std::allocator  >, bool)':  /home/anqichen/graph_comb_opt/code/s2v_mvc/mvc_lib/../../../graphnn/include/nn/msg_pass.h:71: undefined reference to `gnn::IMsgPass ::IMsgPass(std::__cxx11::basic_string , std::allocator  >, bool)'  build/lib/qnet.o: In function `gnn::SubgraphMsgPass ::SubgraphMsgPass(std::__cxx11::basic_string , std::allocator  >, bool)':  /home/anqichen/graph_comb_opt/code/s2v_mvc/mvc_lib/../../../graphnn/include/nn/msg_pass.h:180: undefined reference to `gnn::IMsgPass ::IMsgPass(std::__cxx11::basic_string , std::allocator  >, bool)'  build/lib/qnet.o:(.data.rel.ro._ZTVN3gnn17TensorVarTemplateINS_3CPUENS_5DENSEEfEE[_ZTVN3gnn17TensorVarTemplateINS_3CPUENS_5DENSEEfEE]+0x58): undefined reference to `non-virtual thunk to gnn::TensorVarTemplate ::ZeroGrad()'  build/lib/qnet.o:(.data.rel.ro._ZTVN3gnn17TensorVarTemplateINS_3CPUENS_5DENSEEfEE[_ZTVN3gnn17TensorVarTemplateINS_3CPUENS_5DENSEEfEE]+0x60): undefined reference to `non-virtual thunk to gnn::TensorVarTemplate ::OnesGrad()'  ../../../graphnn/build_cpuonly/lib/libgnn.a(cpu_dense_tensor.o): In function `tbb::interface7::task_arena::initialize()':  /home/anqichen/intel/tbb/include/tbb/task_arena.h:250: undefined reference to `tbb::interface7::internal::task_arena_base::internal_initialize()'  ../../../graphnn/build_cpuonly/lib/libgnn.a(cpu_dense_tensor.o): In function `tbb::interface7::task_arena::terminate()':  /home/anqichen/intel/tbb/include/tbb/task_arena.h:281: undefined reference to `tbb::interface7::internal::task_arena_base::internal_terminate()'  ../../../graphnn/build_cpuonly/lib/libgnn.a(cpu_dense_tensor.o): In function `tbb::interface7::task_arena::current_thread_index()':  /home/anqichen/intel/tbb/include/tbb/task_arena.h:369: undefined reference to `tbb::interface7::internal::task_arena_base::internal_current_slot()'  ../../../graphnn/build_cpuonly/lib/libgnn.a(cpu_dense_tensor.o): In function `void tbb::interface7::task_arena::execute_impl (tbb::flow::interface10::graph::wait_functor const&)':  /home/anqichen/intel/tbb/include/tbb/task_arena.h:213: undefined reference to `tbb::interface7::internal::task_arena_base::internal_execute(tbb::interface7::internal::delegate_base&) const'  collect2: error: ld returned 1 exit status  "
"Hello, I'm going to run `s2v_tsp2d`. However, I didn't find data generator for TSP problem. So the program cannot run without data file. Is that my mistake?   !     !   "
"GNU gdb (Ubuntu 8.1-0ubuntu3.2) 8.1.0.20180409-git  ......  **(gdb) b simlulator.cpp:57  No source file named simlulator.cpp.**  This message comes up when I'm debugging, i don't know how to deal with it.  Could you give me some advice on debugging?   "
"Hi, thanks for the great paper and sharing your code!     I really liked your paper and currently trying to re-implement it in pytorch / deep graph library (  I would be grateful if you could help me out with some of my questions regarding the code (specifically s2v_mvc).    For the n-step q net fitting in s2v_mvc folder,   1. it says n=5 for minimum vertex cover in the paper and ""evaluate.sh"", but the code ""run_nstep_dqn.sh"" use n=2 for training. Would I be able to obtain the results in the Figure D.2 in the appendix of the paper, if I switch to n=5 without changing other hyper-parameters?    For the implementation of q net, I am trying to understand the differences between the code and the paper (which I am totally happy with). Could you confirm my following understandings?  2. The implemented q_net takes input as an ""uncovered"" subgraph with respect to the currently selected nodes with node features=1.  3. The network takes an additional 3-dimensional input ""aux_feat"" containing a) ratio of covered nodes, b) ratio of covered edges and c) a bias term.    It would also be more than helpful if you could point out other ""mvc-specific"" implementation of the code.  Thanks very much in advance! "
Thank for this great work!    I tried to reproduce the results but encountered a problem when training with n-step Q learning.     I am running the code on Ubuntu 18.04 with Python 3.6.7 (anaconda). (I modified the Python2 code to run under Python 3). The compilation of GraphNN was successful and the examples are run correctly. The dynamic libraries are also built successfully with only some slight warnings. When I ran      I got     Would you please help me to solve this problem? Thanks in advance.
"I hope to read your code and understand some details of your algorithm. However, it seems like the neural network introduced in your paper is encapsulated in the self.lib = ctypes.CDLL('%s/build/dll/libtsp2d.so' % dir_path). I wonder how I may get more information."
"Hello,    When I try to build the graphNN library I get this error.     make: *** No rule to make target `build_cpuonly/objs/cxx/src//nn/hit_at_k.o', needed by `build_cpuonly/lib/libgnn.a'.  Stop.    Thanks!    "
"Hi Hanjun,  I downloaded the synthetic data for TSP from the Dropbox link you provided. However, the content in the .gz file is unreadable. It is neither a folder, nor a file of any know extension. I wonder if there is something wrong with the file, or there is a special way to extract it.     Thanks a lot!  Yuhan"
"when I enter the command:  ./run_nstep_dqn.sh  it returned:    Traceback (most recent call last):    File ""main.py"", line 77, in        api = MvcLib(sys.argv)    File ""/home/fanchangjun/Code/graph_comb_opt/code/s2v_mvc/mvc_lib/mvc_lib.py"", line 11, in __init__      self.lib = ctypes.CDLL('%s/build/dll/libmvc.so' % dir_path)    File ""/home/fanchangjun/anaconda2/lib/python2.7/ctypes/__init__.py"", line 366, in __init__      self._handle = _dlopen(self._name, mode)  OSError: /home/fanchangjun/Code/graph_comb_opt/code/s2v_mvc/mvc_lib/build/dll/libmvc.so: cannot open shared object file: No such file or directory    Do you know the reasons? Thanks!"
"I managed to execute run_nstep_dqn.sh of s2v_maxcut.  But, I got this error when executing run_eval.sh:  `Traceback (most recent call last):    File ""evaluate.py"", line 69, in        api.InsertGraph(g, is_test=True)    File ""/home/abcd/graph_comb_opt/code/s2v_maxcut/maxcut_lib/maxcut_lib.py"", line 45, in InsertGraph      n_nodes, n_edges, e_froms, e_tos, weights = self.__CtypeNetworkX(g)    File ""/home/abcd/graph_comb_opt/code/s2v_maxcut/maxcut_lib/maxcut_lib.py"", line 24, in __CtypeNetworkX      edges = list(g.edges_iter(data='weight', default=1))  AttributeError: 'Graph' object has no attribute 'edges_iter'`"
"I run the training, `run_nstep_dqn.sh` of s2v_mvc and got this error:  `File ""main.py"", line 77, in        api = MvcLib(sys.argv)    File ""/home/abcd/S2V-DQN/code/s2v_mvc/mvc_lib/mvc_lib.py"", line 11, in __init__      self.lib = ctypes.CDLL('%s/build/dll/libmvc.so' % dir_path)    File ""/home/abcd/anaconda2/envs/tensorflow-gpu/lib/python2.7/ctypes/__init__.py"", line 366, in __init__      self._handle = _dlopen(self._name, mode)  OSError: /home/abcd/S2V-DQN/code/s2v_mvc/mvc_lib/build/dll/libmvc.so: undefined symbol: _ZTVN3fmt11FormatErrorE`"
"In code/s2v_mvc/mvc_lib/src/lib/qnet.cpp, line 66, what is the ""aux_input"" used to do? And in code/s2v_mvc/mvc_lib/include/config.h, what are ""cfg::reg_hidden"" and ""cfg::aux_dim"" used to indicate?"
"Hi Chris,    Could you please help me check this? I have installed all additional packages, including `DIPHA`, `Persesus` and `Hera`. I use `chmod +x software_backends.cfg` as your suggestion in README with an entry to name of each package (except for `DIPHA` pointed to an subfolder)    Thanks   "
"file not found error  persistence_diagram_file_path = os.path.join(tmp_dir, ""persistence_diagram"")  with open(image_data_file_path, ""bw"") as f:       _ImageDataFile(filtrated_cubical_complex).write_to_binary_file(f)"
"I can not find day2night weights, I just want to use the pre-trained model. Can somebody tell me where can I find them? Thanks"
"Dear author, I am very interested in the model UNIT you proposed in NIPS-17 paper, but I am confused that I can not find the weight sharing part mentioned in your paper ""Unsupervised Image-to-Image Translation Networks"" in this code repo UNIT. Can you point out the weight sharing part? Thank you very much!"
@mingyuliutw Hi! I am trying to train summer to winter translation with my own dataset. The images haven been cropped to 256*256 and the number of images in the summer and winter sets are 8049 and 5477. But the program is always killed after 2 thousand iterations.  Have you encountered similar problems?
"  !     OK, so I followed this tutorial to the T:   And then I get this random error. Everything was fine, I used labelImg, I generated the tfrecords, I downloaded all the software, I organized the files and extracted all the files. I'm using an up to date version of tensorflow. And the images/labelmap.pbtxt exists in the images folder (maniacal laughter). Why can't it find this file? I added a path, that still doesn't work."
"When i run the test using pretrained model, i get error that cant import torch.utils.serialization"
"The result graph shown in the training obtained a good conversion effect, but the effect was poor when the image  is tested on the model trained to 500000. May I ask what factors could lead to this situation?"
"Hi there,    I was wondering on whether are there any alternatives to deal with the error caused by `import load_lua` ?    I tried both the Docker and conda method, both failed."
"Hello, first of all thank you for your work. I read the paper and tested the network with good results. However I have a theoretical question about the model. How is it possible that from the same latent code each generator is able to generate two images, the translated one and the reconstructed one, thus respecting the consistency of the cycle?  It is clear to me that each generator is able to reconstruct an image of a domain but it is not clear to me how it is possible to recreate two different images using the same generator if we start from the same encoding.  Is it possible for weight sharing?    thank you very much for your availability"
"Hi, thank you for your awesome work!    I notice that you removed the shared weight in the generator and discriminator in the latest implementation, which is organized with the format of MUNIT repo.    I'm wondering what is the necessity of this removal and how it will affect the final results, especially for the photorealistic transfer, e.g., summer to winter, day to night, and whether it will influence the clearness of the generated images. I assume that the change from the original implementation in NIPS to the one similar to MUNIT has some extra bonus since you mentioned in one closed issue that in many cases, the shared weights work well(""I found for some tasks, discriminator weight sharing is quite useful. For example, for the SVHN to MNIST domain adaptation, the two adversarial discriminators share weights for several layers. I also found that for the face image translation, discriminator weight-sharing is helpful too (The yaml file I released actually use this setting.). But when the domains are quite different and a patch-based discriminator is used, which often only have few layers. Discriminator weight sharing could hurt the performance.""  )  "
How do I test this code in a single image?
"Hi Sir,            I want to convert visual images to infrared images for my project.  I have paired infrared - visible images for training. (FLIR dataset and Kaist Dataset). Is it possible to do this task using this repo? "
None
"Hello,   Could you please share (anytime soon) the winter-to-summer or summer-to-winter pretrained model or the dataset that was used?    Thanks."
None
"Hello,  I would like to ask if there is a way to train the UNIT model with a lower precision? just to to give it a try if I can result in reducing training/inference time    Which part do I need to edit in the code?"
I am afraid that the image links in TUTORIAL.md are 404.  Is that because of my Internet connection or they're just not found?
"Here it is  !     *Anyway, great research!*"
"Hi,    I am working with translating images from the gta domain to the cityscapes one. I observed that you have incorporated data augmentation in your model in the form of horizontal flips and random crops. My doubt was that since we are training UNIT in order to translate full size images from one domain to the other, wouldn't it be better to show UNIT full size images while training (in contrast to random crops).    While carrying out your experiments, did you gather any empirical evidence of the importance of data augmentation, specifically the random crops?    Also I am relatively new to the field of image-to-image translation, so it would be great if you could share your insights on this matter, like the intuitive/theoretical reasons for using random crops and not showing UNIT a full image at a time.    Thank you :) "
"Hi,    Thank you very much for open sourcing your  . Currently, I am working on translating images from the GTA domain to the Cityscapes one. My first goal was to reproduce your results, by training UNIT from scratch, but due to time constraints I implemented a version of UNIT which works with **parallel GPUs**. Unfortunately, I am not seeing the same visually appeasing results that you got.    I edited and used the same config file that I suppose you used for training your gta2city model -  . I have attached the exact config file that I used and also some sample images comparing the results of your model and mine.       Results (after converting gta images to cityscapes domain)    Your model             |  My model  :-------------------------: | :-------------------------:  !  | !   !  | !   !  | !     As you can see my images don't seem to be semantically consistent (trees start to manifest here and there). Do you have any ideas why this would be happening?    It would be really helpful if you could let me know of the exact settings that you used like -   1. For how many iterations did you train your gta2city model? If I understand correctly, I am effectively training for batch_size * max_iter = 6 * 130000 = 780000 iterations.  2. What were the dataset sizes that you worked with? Did you use all of gta and cityscapes dataset i.e. 25k images each, or a part of them?  3. What were the image size parameters in your model (like the new_size, crop_size etc.). As you can see from my config, I had to reduce the crop_image_height to 256 due to memory issues.    And I noticed that since you worked with batch size 1, there is no need of batch normalization, but I increased the batch size to 6 to work with 6 GPUs, so do I need to add it, will it affect the final results?    Thank you so much for your time :)  "
"Hello, thank you very much for your work.    I find your the UNIT model so impressive so i use it in a school project, it returned a good results but the issue that I am having is a diversity problem, given an input image, the output is always the same even with using a VAE (in your case, it's a reduced vae). so what i did is that i added a new share layer that compute the log variance and changed the KL loss function, but i had problems with exploding gradients. so i want to ask if it is possible to make the model returns multiple outputs for the same input ?    Thank you in advance,  Regards        "
"Hello,     First of all, thanks you for making the code publicly available. Your work on Image-to-Image translation is really amazing.    I am trying to achieve domain translation between 2 datasets. For this I am working with the master branch of UNIT, and I noticed a difference between this branch and the version_02 branch. The latter is void of the VGG loss while the former has it. Regarding the same, I have several questions. (I tried going through the solved issues and apologize in advance if any of these have been answered before.)    1. What is the reason for adding a VGG loss to the master branch, and not to the version_02 one. Is it because the latter has explicit shared layers in the generator?, or is it due to some other reason?    2. How did you decide the weights of the various losses of the generator? Why is the default VGG loss weight so low compared to the cyclic and reconstruction loss?    3. While carrying out my experiments, I noticed that, without the vgg loss objects start to disappear in the translated images. Why is this so? I ask this because vgg loss seems to have a huge impact on the quality of the generated images, despite of its low weight.    4. While carrying out more experiments on a custom dataset, I noticed that the vgg loss for both the generators first decreases and then increases by a huge amount. More over vgg_a is much higher than vgg_b. It should be noted that my datasets were imbalanced with a ration of 7:1. Is this the reason behind the increase in loss values, if so, could you please explain your reasons for the same. Or do I need to change optimizer, learning rate, step size etc.     Thank you so much for taking your time to read this.  Any help is greatly appreciated :)"
"Hello,    One of the interesting parts in the CycleGAN implementation is the identity loss, which ensures that the network should act as an identity mapping not translation in case if it is fed from a real sample from the target domain.  Example:  Snow to Summer translation  if the network is fed with Snow, it should output a summer image  if it is fed with a summer image, ""using the same Encoder-Decoder for the Snow2Summer"", it should output the exact identical summer image with no translation    I wonder, if the UNIT is able to handle such scenario, or if there is something similar to the identity loss is implemented."
Could you please provide your trained model for sunny to rainy translation
"Hai,   I am trying to train day to night translation with my own data-set. The image dimension in both domains are 1280x720, but in the given config file the image is cropped to 256x256, after training with 256x256 random cropping the output images generated are of dimension 420x256. I need to train this model with 512x512 and I have changed in the data_loader also since i am having a cuda:out_of_memory issue after 100 iteration. why this happens like this..??why we need to do training only with 256x256 random cropping only..??This makes my output video to blur and breaking of pixels. I need to get rid of that.  How should I train with the whole image dimension and the generated output images are same as that of the input image..??  "
"Hi,    I tried to recreate results for cat2tiger problem with linked on README model weights. I did everything as instructed but my results are far different from results on README   !     What could possibly go wrong?    Regards,  Marta  "
"Hi, there, i'm trying to train dataset and stucked with some learning problems    **Example** of 10000 iterations day 2 night (train)  !   **Example** of 10000 iterations night 2 day (train)  !     I think that it's `unit_day2night.yaml` configuration problem. Can you share yours config file?    ---    Another option, that it's **dataset's split** issue.  I used **50/50** for train and test. And both divided into two equal domains A and B.    "
"Hello,     I want to try this code on my custom dataset, and based from the   I can use sample script from `gta2city` as template    But I don't know how to organize the dataset folder (eg. directory structure for each domain images). Can you show me how a proper dataset folder can be used in this code?"
I have images with high resolution and I am running the training on GTX 1080p with 12 GB Ram. I reduced the random cropped images to 64x64 to let the training proceed. The GPU seems to run out of memory when the crop size is any greater than 64x64. Hence was wondering if it would be run on multiple GPU to speed up the training? 
"Hiï¼Œ I am confuse about  **Discriminator Loss**, in your paper ,     > We use the LSGAN objective proposed by Mao et al.    I think your code function is :            loss  = L2(out0 - 0) + L2( out1 -1 )    but  in pix2pixHD  code is      I think his code function is :        loss = 1/2 * L1(  out0 - out1 )    Is it difference ??"
"I am trying to test my trained model on an image with [720, 1280] and I run out of memory.  My GPU is P100 with 16GB, but still runs out of memory after around 6 images    How can I used multiple GPUs in the test phase ?"
"hello,where could I get the pretrained model or train dataset of Street Scene Image Translation? you got a very nice result in that sample. I'm trying to do the same, thanks a lot ."
Can you share the code that made the youtube video?
Could UNIT be used for language translation? If yes can you give please some hint?  Thank you in advance
"Hi, I want to train UNIT on CelebA, but I didn't find your configs on this dataset."
"The paper showed that weights of the last few layers of encoders and first few layers of decoders are shared. However, I have not found its implementations in the source code. It seems to be that two encoders (two decoders) use separate weights.    Maybe my limitation knowledge in Pytorch library makes me do not know where is the implementation of shared weights. Can you please indicate the implementation of shared weights constraint?    Thank you very much."
 æˆ‘æƒ³è¯·æ•™ä¸€ä¸‹ï¼Œunitæ¨¡åž‹å¦‚ä½•è·‘é©¬å’Œæ–‘é©¬çš„æ•°æ®é›†ï¼Ÿ
"Hello,  I would like to know how does the code handle different sizes of datasets in both domains  Let's say I have 20K images in domainA, and 15K in domainB    Does this mean, that the training will be done on only 15K images, or will it load use the biggest size from both domains and repeat the images ?  Like we have 20K of minibatches, and for the difference of these 5K, they are repeated again in the training ?"
"Hello,  I am using the UNIT on my own dataset, but the resulting are not that satisfactory. I believe the dataset contain very complex feature classes to learn.  I though of trying to play with the architecture or hyperparameters to adapt my dataset, but I am not sure exactly from where to start ? like we already have 6 subnets with 3 main components, E, G, and D. I am not sure what can be the start, like maybe changing number of filters, or adding more layers or so. I just want to try.    Do you have any idea/recommendation from where can I start playing around ?"
"Hello,  I downloaded the vgg16.t7 manually, when I ran""load_lua(r""D:\Progarm Files\eclipse\preferences\UNIT\vgg16.t7"")"",  I got this error message:""unknown type id 1056626884. The file may be corrupted.""  Why is this problem? I had downloaded it several times.    Thank you for your answer."
"Hi,    I am a student who tries to re-implement the USPS->MNIST experiment.    Could you give the hyper-parameters of that experiment?    Thanks"
"Hello,    I notice that the images which are saved during the training process, or even after x iterations, it is always the same image, it doesn't change, and actually, it is the first image found in the directory, either trainA or trainB    Does this mean that the model only loads one image and train on ? or it loads all images, but saves only one image (same for every iteration, but the 4 columns are same image but flipped)"
"Hello,  I followed your   on getting the network running on my own dataset.  However, I receive an error         Any idea what can be the problem here ?"
"Hi Mingyu,    Thanks for you novel work, it is very interesting. I have some questions. During the Sample process. The code is (in the file `trainer.py` line 320 to line 336):              h_a, _ = self.gen_a.encode(x_a[i].unsqueeze(0))              h_b, _ = self.gen_b.encode(x_b[i].unsqueeze(0))              x_a_recon.append(self.gen_a.decode(h_a))              x_b_recon.append(self.gen_b.decode(h_b))    However, in the training process, the code is (in the file `trainer.py` line 267 to line 274):          h_a, n_a = self.gen_a.encode(x_a)          h_b, n_b = self.gen_b.encode(x_b)          # decode (within domain)          x_a_recon = self.gen_a.decode(h_a + n_a)          x_b_recon = self.gen_b.decode(h_b + n_b)          # decode (cross domain)          x_ba = self.gen_a.decode(h_b + n_b)          x_ab = self.gen_b.decode(h_a + n_a)    Why don't you add n_a and n_b in the sample process?    Thanks,"
"Dear Sir,    I'm trying to recreate this project, but I am getting an error that says CUDA is not sufficent enough. Is there a way to run this code without having an Nvidia card. Is it possible to use your already trained models running only in the CPU? If so what part of the code would I need to change to make it run?    Many thanks for any assistance you can give.    Mohamed"
I am trying to run UNIT on my own data but receiving the error below. It seems related to the vgg model. Please help..     
"Hi,     I am trying to implement the network on my own data-set, but received the error. I don't know where does it come from. Please help. Thank you a lot!       "
Could you please release the code for Domain Adaptation that you mentioned in your paper?  Thanks in advanced.
"Hi,When I testing ï¼Œthe pycharm found an error:No such file or directory: './models\\vgg16.t7',  then I opened URL:  that I can not download vgg16.t7."
"Hi Mr. Liu,  I'm interested in your work and tried your code on svhn2mnist task. Here are several generated images and I want to ask why the a2b and  b2a from decode_b(ca, sb) decode_a(cb, sa) are bad?  !   !   But decode_b(ca, noise) decode_a(cb, noise) are good but not relavant to the original class label?  !     !   The first ten labels of svhn images are 8, 9, 1, 4, 3, 5, 3, 8, 5, 3 ....          "
There aren't style_dim parameters in config files.
"Traceback (most recent call last):    File ""cocogan_translate_one_image.py"", line 81, in        main(sys.argv)    File ""cocogan_translate_one_image.py"", line 44, in main      trainer.gen.load_state_dict(torch.load(opts.weights))  NameError: name 'trainer' is not defined    I get this error when I run     python cocogan_translate_one_image.py --config ../exps/unit/cat2tiger.yaml --a2b 1 --weights ../outputs/unit/cat2tiger/cat2tiger_gen_00500000.pkl --image_name ../images/cat001.jpg --output_image_name ../results/cat2tiger_cat001.jpg  self.snapshot_save_iterations=5000  self.image_save_iterations=2500  self.image_display_iterations=100  self.display=1  self.snapshot_prefix='../outputs/unit/cat2tiger/cat2tiger'  self.hyperparameters={'trainer': 'COCOGANTrainer', 'lr': 0.0001, 'll_direct_link_w': 100, 'kl_direct_link_w': 0.1, 'll_cycle_link_w': 100, 'kl_cycle_link_w': 0.1, 'gan_w': 10, 'batch_size': 1, 'max_iterations': 500000, 'gen': {'name': 'COCOResGen', 'ch': 64, 'input_dim_a': 3, 'input_dim_b': 3, 'n_enc_front_blk': 3, 'n_enc_res_blk': 3, 'n_enc_shared_blk': 1, 'n_gen_shared_blk': 1, 'n_gen_res_blk': 3, 'n_gen_front_blk': 3}, 'dis': {'name': 'COCODis', 'ch': 64, 'input_dim_a': 3, 'input_dim_b': 3, 'n_layer': 6}}  self.datasets={'train_a': {'channels': 3, 'scale': 1.2, 'crop_image_height': 216, 'crop_image_width': 216, 'class_name': 'dataset_imagenet_image', 'root': '../images', 'folder': './', 'list_name': 'image_list.txt'}, 'train_b': {'channels': 3, 'scale': 1.2, 'crop_image_height': 216, 'crop_image_width': 216, 'class_name': 'dataset_imagenet_image', 'root': '../images', 'folder': './', 'list_name': 'image_list.txt'}}  Traceback (most recent call last):    **File ""cocogan_translate_one_image.py"", line 81, in        main(sys.argv)    File ""cocogan_translate_one_image.py"", line 44, in main      trainer.gen.load_state_dict(torch.load(opts.weights))  NameError: name 'trainer' is not defined**      Any ideas? I'm using python 3.6    Thanks!"
"Dr. Ming-Yu Liu åŠ‰æ´ºå ‰    Hi. I am graduated student of Osaka University. Thanks for your great paper and open source.  I am trying to use it on my own data, but I found a mistake in file cocogan_nets.py, at line 121.  The code is written as `w = x2.size(2)`, but it seems to be `w = x2.size(3)`.   If it doesn't have mistake can you explain it to me?    Thank you."
"Hi,     Is there a pre trained model available for day to night translation? I checked the google drive folder, and they were cat2tiger based models."
"Hi, mingyu,    I am training the model on 5 different attributes from celeba, but on testing i want to export classification accuracy from the discriminator to know if the translated image is classified properly to the target domain or not!  How can i got this classification score?    I made the following changes, are they right?  In cocogan_net.py, I added these lines to class COCOSharedDis:       In cocogan_translate_one_image, i edited these lines         And I added these lines:     out_translated_a  is a [torch.cuda.FloatTensor of size 4 (GPU 0)], what do these four values mean? how can i got one value from them?      Sorry for my bad English.  Thanks in advance  "
"Have you experimented with a dataset that does not have a center crop?  What was the result?    When you see the results of the paper, you always have the face in the center, and the background is almost invisible.  Have you tried it in a dataset with lots of variants? For example, the conversion between a cat and a dog."
Could you give me day2night datasets? Thank you! My email is songcan.wust@gmail.com
Where can I get your night street scenes datasets?
How to train on my own data?
"Hi,  Thanks for your great paper and open source.  Could you offer the image list for cat2tiger experiment?   Thanks a lot!"
"Dear M. Liu,    I am studying your paper that I found very interesting, thank you for sharing your research !  I read the other issues/questions asked, where you pointed that some explanations were not given in the paper, about that I would like to ask you more details about the domain adaptation experiment.    Could you explain the discriminator update rule and more precisely when you compute that please ?  feature_loss_a = self._compute_ll_loss(fake_feat_ab - fake_feat_aa, dummy_variable)  feature_loss_b = self._compute_ll_loss(fake_feat_ba - fake_feat_bb, dummy_variable)  (cocogan_trainer_da.py lines 102-103)    Also, my experiments focus more on domain translation than domain adaptation.  For that purpose, is this update rule still relevant ?    Thank you for your answer.  Adrien"
"I am running version_01, and I am facing problems with the summary writer.     1.   throws the following error:    > AttributeError: 'module' object has no attribute 'FileWriter'    So, I replaced it with the following line:        train_writer = tf.summary.FileWriter(""%s/%s"" % (opts.log,os.path.splitext(os.path.basename(opts.config)) .    The error it throws is:     >  in add_summary for value in summary.value:  AttributeError: 'Tensor' object has no attribute 'value'    Can anybody please point out the change I need to make.   "
"Hi, thanks for making the code public available. It is really an amazing work.  After read the paper and the source code, I have several questions.    1. In file cocogan_trainer.py, from line 73 to line 82, those member variables, self.gen_xx_loss_xx.  I only find their definition, but I haven't find where they are used. So, what's these variables for?  2. I try to train CelebA blondhair, and I find the batch size was set to 1, could the batch size be bigger than 1?  3. I find the generated image of CelebA blondhair is a little blurred. Is this phenomenon normal?  "
"Hi !   *** Q1 ***  When using Image translation, did you **not use augmentation?** I want to know about this, but I can not find it in your code    *** Q2 ***  For example, suppose I want to convert from domain_A (dog) to domain_B (cat). (domainA -> domain_B)    Assume that the number of dog images in `domain_A is 1000`, and the number of cat images in `domain_B is 1500`. (That means # domainA < # domainB)    If so, how do I train? I think the **learning imbalance** will happen because the number of data in two domains is different.  Did you make the number of images in both domains the same?    *** Q3 ***  Generator_loss = G_A_loss + G_B_loss  Discriminator_loss = D_A_loss + D_B_loss    When you train Generator A and B, why did you train with `sum of two loss(Generator_loss)`, instead of training about G_A_loss and G_B_loss, respectively?  Likewise, why did the Discriminator do so?"
"Hi I couldnt find in the paper or understand from the code what you are doing during test time in the next manner:    On train time you insert noise and you compute the mu and sd and represent z as a random vector.    What happens during test time, cause i dont see you do the same, do you use the average mu and sd or something else? i would love to know.    thanks,  Gal."
None
"Hi !   I am reproducing your code with tensorflow.  But I do not know the current hyperparameter information. (batch size, input size, dropout rate, etc.)  Could you tell me which code I can check?"
"In the file ./exps/unit_local/synthia2cityscape.yaml, the sentence ""root: /cosmo/datasets/cityscape_1024x512/"" is error? I found that it should set as ""root: ../datasets/cityscape"" for ""python cocogan_translate_one_image.py --config ../exps/unit_local/synthia2cityscape.yaml --a2b 0 --weights ../outputs/unit/street_scene/synthia2cityscape_gen_00250000.pkl --image_name ../images/freiburg_000000_000021_leftImg8bit.png --output_image_name ../results/synthetic_freiburg_000000_000021_leftImg8bit.png"""
"Why did not the weight of discriminators be shared?  Or maybe you tried, but the results were not good?"
"In the code, the maximum epochs to train the SHVN->MNIST model is 100,000.    Do you remember what was the epoch that produced the result stated in the paper i.e. 90.53% accuracy?"
"In appendix B, under the paragraph about SVHN->MNIST, it is written:    >  We also found spatial context information was useful. For each input image, we created a 5-channel variant where the first three channels were the original RGB images and the last two channels were the normalized x and y coordinates.    It sounds like this paragraph is saying that the MNIST image, which is originally in grayscale, gets converted to RGB so that when we add the spatial features, the total number of channels for the MNIST image is 5. But in the code, the channel of the MNIST image after adding the spatial feature is only 3.    Did I misread the paragraph?"
"I seem to be getting this error. AFAIK I correctly did the CelebA pre-processing by first extracting the original CelebA data and running the resize/crop script which generates the `img_align_crop_resize_celeba` directory.    Here is the error:         I think I know what the error means: it seems to be expecting 3/4 channels for a particular image (is it trying to convert a b/w image to RGB??), but I'm just wondering why I'm getting this issue in the first place since nobody else has raised it here!    I'm using the OpenCV suggested in USAGE.md: `conda install -y -c menpo opencv`"
I am getting low accuracy. May I know why ?    $python cocogan_train_domain_adaptation.py --config ../exps/unit/svhn2mnist.yaml --log ../logs        Iteration: 00000010/00200000  Iteration: 00000020/00200000  Iteration: 00000030/00200000  Iteration: 00000040/00200000  Iteration: 00000050/00200000  Iteration: 00000060/00200000  Iteration: 00000070/00200000  Iteration: 00000080/00200000  Iteration: 00000090/00200000  Iteration: 00000100/00200000  Classification accuracy for Test_B dataset: 0.1296  Iteration: 00000110/00200000  Iteration: 00000120/00200000  Iteration: 00000130/00200000  Iteration: 00000140/00200000  Iteration: 00000150/00200000  Iteration: 00000160/00200000  Iteration: 00000170/00200000  Iteration: 00000180/00200000  Iteration: 00000190/00200000  Iteration: 00000200/00200000  Classification accuracy for Test_B dataset: 0.1032  Iteration: 00000210/00200000  Iteration: 00000220/00200000  Iteration: 00000230/00200000  Iteration: 00000240/00200000  Iteration: 00000250/00200000  Iteration: 00000260/00200000  Iteration: 00000270/00200000  Iteration: 00000280/00200000  Iteration: 00000290/00200000  Iteration: 00000300/00200000  Classification accuracy for Test_B dataset: 0.1084  Iteration: 00000310/00200000  Iteration: 00000320/00200000  Iteration: 00000330/00200000  Iteration: 00000340/00200000  Iteration: 00000350/00200000  Iteration: 00000360/00200000  Iteration: 00000370/00200000  Iteration: 00000380/00200000  Iteration: 00000390/00200000  Iteration: 00000400/00200000  Classification accuracy for Test_B dataset: 0.0826  Iteration: 00000410/00200000  Iteration: 00000420/00200000  Iteration: 00000430/00200000  Iteration: 00000440/00200000  Iteration: 00000450/00200000  Iteration: 00000460/00200000  Iteration: 00000470/00200000  Iteration: 00000480/00200000  Iteration: 00000490/00200000  Iteration: 00000500/00200000  Classification accuracy for Test_B dataset: 0.0984  Iteration: 00000510/00200000  Iteration: 00000520/00200000  Iteration: 00000530/00200000  Iteration: 00000540/00200000  Iteration: 00000550/00200000  Iteration: 00000560/00200000  Iteration: 00000570/00200000  Iteration: 00000580/00200000  Iteration: 00000590/00200000  Iteration: 00000600/00200000  Classification accuracy for Test_B dataset: 0.0912  Iteration: 00000610/00200000  Iteration: 00000620/00200000  Iteration: 00000630/00200000  Iteration: 00000640/00200000  Iteration: 00000650/00200000  Iteration: 00000660/00200000  Iteration: 00000670/00200000  Iteration: 00000680/00200000  Iteration: 00000690/00200000  Iteration: 00000700/00200000  Classification accuracy for Test_B dataset: 0.0819  
In which file you have defined loss function such as  mean squared error (MSE) and L2 for domain adaptation ?
I have a question about the `_compute_kl` function in the class `COCOGANDAContextTrainer`. The following are the relevant parts of the code:       This function was used in `gen_update`:       My question is how did you derive the formula to compute the KL divergence term?    I thought it was based on the   paper which has the following parts:    !     and in Appendix B:    !     I note the following differences between the code and the paper (Auto-Encoding Variational Bayes):    1. The KL divergence term is multiplied by 2 instead of 1/2.  I guess this does not matter much since it just rescales the loss.    2. There is no `- 1` in the `encoding_loss`. Did you choose not to include this term because it will not change the optimum point anyway?    
"In page 7 of the paper, it says:    > Also, for a pair of generated images in different domains, we minimized the L1 distance between the features extracted by the highest layer of the discriminators...    And in the code in cocogan_trainer_da,py, this is implemented as follows:         Isn't this L2 loss because `self._compute_ll_loss` is implemented using` torch.nn.MSELoss()`?"
"Hi Ming Yu,      I see the cat2tiger samples having cat and tiger facing left / right, does that mean this project work with the face rotation by itself? No need to annotate or list which cat image facing left, which cat image facing right? Thanks!    Best Wishes,  Chi Kiu SO"
"In Appendix B, in the paragraph about SVHN -> MNIST, it is written:    >  For each input image, we created a 5-channel variant where the first three channels were the original RGB images and the last two channels were the normalized x and y coordinates.    It seems to me that this is implement in cocogan_trainer_da.py:         The way the code is written, the contents of `xy` is just -1 or 0 because the environment is Python 2. Did you mean to do this? I thought the values should be between -1 and 1.  "
"In paper, you use `cartoon dataset`    i want to get this dataset...   how can I download it ?     "
"In the GaussianVAE2D class definition in the file common_net.py, there is a method named sample.    This is how its defined:       I don't understand why noise is defined as a Variable because we do not need to differentiate the loss function with respect to it, do we?   "
"Hi! Thank you very much for sharing your code. I encountered some error when training from scratch with celeba blond hair translation using python 3 according to the usage. The itertool.izip in python2 is changed to the built-in zip in python3:    `Traceback (most recent call last):    File ""cocogan_train.py"", line 88, in        main(sys.argv)    File ""cocogan_train.py"", line 56, in main      for it, (images_a, images_b) in enumerate(zip(train_loader_a,train_loader_b)):    File ""/home/fox/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 301, in __iter__      return DataLoaderIter(self)    File ""/home/fox/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 171, in __init__      self._put_indices()    File ""/home/fox/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 210, in _put_indices      indices = next(self.sample_iter, None)    File ""/home/fox/anaconda3/lib/python3.5/site-packages/torch/utils/data/sampler.py"", line 115, in __iter__      for idx in self.sampler:    File ""/home/fox/anaconda3/lib/python3.5/site-packages/torch/utils/data/sampler.py"", line 50, in __iter__      return iter(torch.randperm(len(self.data_source)).long())  RuntimeError: invalid argument 1: must be strictly positive at /opt/conda/conda-bld/pytorch_1503968623488/work/torch/lib/TH/generic/THTensorMath.c:2033  `  How can I fix this problem?    "
hello mingyliutw  could you share pretrained models(Day2Nightã€Snowy2Summery)  thank you very much
"Probably worth mentioning that sometimes the images are obtained with inverted colors, the same behavior of the network I observed in the CycleGAN. Helps restart the training."
"Hi, I tried to run the training with CelebA dataset, and got the following error when I started train :     Traceback (most recent call last):    File ""cocogan_train.py"", line 6, in        from tools import *    File ""/home/paperspace/Downloads/UNIT/src/tools/__init__.py"", line 6, in        from net_config import *  ModuleNotFoundError: No module named 'net_config'    Do you have any hints I could solve it? thanks!    "
"Hi guys, I tried to train on GTX1080Ti, Ubuntu 16.04, cuda 8, cudnn 6 with image sizes 640x480, and config:  train:    snapshot_save_iterations: 5000 # How often do you want to save trained models    image_save_iterations: 2500 # How often do you want to save output images during training    image_display_iterations: 100    display: 1 # How often do you want to log the training stats    snapshot_prefix: ../outputs/unit/night2day/ # Where do you want to save the outputs    hyperparameters:      trainer: COCOGANTrainer      lr: 0.0001             # learning rate      ll_direct_link_w: 100  # weight on the self L1 reconstruction loss      kl_direct_link_w: 0.1 # weight on VAE encoding loss      ll_cycle_link_w: 100   # weight on the cycle L1 reconstruction loss      kl_cycle_link_w: 0.1  # weight on the cycle L1 reconstruction loss      gan_w: 10              # weight on the adversarial loss      batch_size: 1          # image batch size per domain      max_iterations: 500000 # maximum number of training epochs      gen:        name: COCOResGen        ch: 64               # base channel number per layer        input_dim_a: 3        input_dim_b: 3        n_enc_front_blk: 3        n_enc_res_blk: 3        n_enc_shared_blk: 1        n_gen_shared_blk: 1        n_gen_res_blk: 3        n_gen_front_blk: 3      dis:        name: COCODis        ch: 64        input_dim_a: 3        input_dim_b: 3        n_layer: 6    datasets:      train_a: # Domain 1 dataset        channels: 3       # image channel number        scale: 1       # scaling factor for scaling image before processing        crop_image_height: 480 # crop image size        crop_image_width: 640  # crop image size        class_name: dataset_image       # dataset class name        root: ../datasets/sg/      # dataset folder location        folder: night/        list_name: lists/night.txt  # image list      train_b: # Domain 2 dataset        channels: 3       # image channel number        scale: 1        # scaling factor for scaling image before processing        crop_image_height: 480 # crop image size        crop_image_width: 640  # crop image size        class_name: dataset_image        root: ../datasets/sg/        folder: sunny/        list_name: lists/sunny.txt      However, I encountered out-of memory error as follows:  self.display=1  dataset_image  dataset=dataset_image(conf)  dataset_image  dataset=dataset_image(conf)  Iteration: 00000001/00500000  THCudaCheck FAIL file=/opt/conda/conda-bld/pytorch_1503966894950/work/torch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory  Traceback (most recent call last):    File ""cocogan_train.py"", line 88, in        main(sys.argv)    File ""cocogan_train.py"", line 64, in main      image_outputs = trainer.gen_update(images_a, images_b, config.hyperparameters)    File ""/media/ml3/Volume/UNIT/src/trainers/cocogan_trainer.py"", line 71, in gen_update      total_loss.backward()    File ""/home/ml3/.conda/envs/torch/lib/python2.7/site-packages/torch/autograd/variable.py"", line 156, in backward      torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)    File ""/home/ml3/.conda/envs/torch/lib/python2.7/site-packages/torch/autograd/__init__.py"", line 98, in backward      variables, grad_variables, retain_graph)    File ""/home/ml3/.conda/envs/torch/lib/python2.7/site-packages/torch/autograd/function.py"", line 91, in apply      return self._forward_cls.backward(self, *args)    File ""/home/ml3/.conda/envs/torch/lib/python2.7/site-packages/torch/autograd/_functions/basic_ops.py"", line 210, in backward      return grad_output.mul(ctx.constant).mul(var.pow(ctx.constant - 1)), None    File ""/home/ml3/.conda/envs/torch/lib/python2.7/site-packages/torch/autograd/variable.py"", line 339, in mul      return Mul.apply(self, other)    File ""/home/ml3/.conda/envs/torch/lib/python2.7/site-packages/torch/autograd/_functions/basic_ops.py"", line 48, in forward      return a.mul(b)  RuntimeError: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1503966894950/work/torch/lib/THC/generic/THCStorage.cu:66    Thanks for your comments!  "
I am so interested in the _Dog Breed Image Translation_ and the _Cat Species Image Translation_. Are the cropped cats and dogs datasets used in the experiments available?   
Can you upload the street images? I am very interested in the street scene image translation tasks.
Is there a pre-trained model for converting rgb to ir and ir to rgb like mentioned in the paper?
"In the paper, the author assumes that the image in two different domain can be coded into a common latent representation. What I what to know is that if the ""Shared Latent Representation""  assumption works for unaligned data. Because in the training process, the model must pick arbitrary image from each distribution, and if the data is unaligned, how to make the two images coded in to a common representation? In other words, whether the shared latent space assumption requires the aligned data?   "
"  def  gen_update( self , images_a, images_b, hyperparameters):    self .gen.zero_grad()   x_aa, x_ba, x_ab, x_bb, shared =  self .gen(images_a, images_b)   x_bab, shared_bab =  self .gen.forward_a2b(x_ba)   x_aba, shared_aba =  self .gen.forward_b2a(x_ab)   outs_a, outs_b =  self .dis(x_ba,x_ab)    for  it, (out_a, out_b)  in   enumerate (itertools.izip(outs_a, outs_b)):     outputs_a = nn.functional.sigmoid(out_a)     outputs_b = nn.functional.sigmoid(out_b)     all_ones = Variable(torch.ones((outputs_a.size( 0 ))).cuda( self .gpu))      if  it== 0 :       ad_loss_a = nn.functional.binary_cross_entropy(outputs_a, all_ones)       ad_loss_b = nn.functional.binary_cross_entropy(outputs_b, all_ones)      else :       ad_loss_a += nn.functional.binary_cross_entropy(outputs_a, all_ones)       ad_loss_b += nn.functional.binary_cross_entropy(outputs_b, all_ones)     The code above is a part of code in cocogan_trainer.py.   I think the   ` all_ones = Variable(torch.ones((outputs_a.size(0))).cuda(self.gpu))`  should be   ` all_zeros = Variable(torch.zeros((outputs_a.size(0))).cuda(self.gpu))`  Because it calculates the loss when the inputs of Discriminator are fakeA and fakeB.  Is my understanding right?   Do I misunderstand it?"
"hello mingyuliu's teacher  When I saw your essay, I was very interested. I now use Cycle gan to train from day to night translation, but my day2night data werehave limited . Can you share your data with me? My email is: cuimiao188@163.com    thank you very much"
"Hi,  I had a question about how tying the weights affects the discriminator's ability to differentiate between domains when training for domain adaptation (training source domain's discriminator with target and source weights tied for higher layers, and also minimizing l1 loss between features output by them).  Intuitively it feels like that will prevent the discriminators from being good at telling the domains apart and seems counterproductive to the adversarial part of the training (even though it may be helpful for extracting similar features from two domains to get similar results on whatever task is being performed at the end of it all)?   Just wanted to ask why this is not true(or if this is true)?  Thanks! "
"As far as I understand, in the current version only image sets consisting of images where width == height are properly supported. Current workaround would be to pre- and post-scale aspect ratios or add padding. Niether is ideal."
"Hi MingYu,    Instead of training celeba (face attributes dataset), I would like to test Snowy2Summery.  May I know whether I need to find a whole lot of snow images if I would like to train that model myself?    Thank you very much"
"Hi,    I am trying to run the ""Cat to Tiger Translation"" test with `python cocogan_translate_one_image.py --config ../exps/unit/cat2tiger.yaml --a2b 1 --weights ../outputs/unit/cat2tiger/cat2tiger_gen_00500000.pkl --image_name ../images/cat001.jpg --output_image_name ../results/cat2tiger_cat001.jpg` and get the following error:    `AttributeError: 'list' object has no attribute 'gen'`     By inspecting the source code in `cocogan_translate_one_image.py:42` (  I find that trainer is here indeed first defined as as list. The `exec` command in the next line seems to turn this into a dict (?) so it seems that this is the source of the error.    What to do? (I am using Python3)  Thx        "
"Hi,mingyuliu,thanks for your contribution which inspires me a lot!  But I have one question that during translation,whether CNN detects the facial marks/features of the image and just inpaint the color and texture to the target domain while preserving the outline of the original image?  The image below shows that when transfering from a cat to a tiger,ont only the facial features are changed ,but also the face becomes bigger,so can you tell me the reason?  !   "
"1. Is there any reason (either empirical or theoretical) to choose Instance normalization instead of Batch normalization? The paper refer 'resnet' paper for explaining RESBLK but I think 'resnet' does not have instance normalization layer by default.    2. Additionally, is there any reason to choose RESBLK only in few layers of encoder & decoder?"
Links of pre-trained models for couple of tasks are available in your git. Do you plan on sharing links to other pre-trained models as well? 
"Hi Dr. Liu,    I ran the code of training attributed-based face images translation.     When the iteration is about 100, the training will end and encounter a segmentation fault problem.     i.e.,       the stack-trace information       "
"Hi, Mingyu.   I'm studying your amazing work about UNIT, and I'm very interested in UNIT's    attractive poteintial of IR/RGB conversion. So I tried to repeat the experiment. Since the detailed parameters of training are not provided, I just tried borrowing settings in synthia2cityscape.yaml for the training. But the results were not satisfying.  So I'm wondering is it right for me to do so? If it's not right to borrow settings in synthia2cityscape.yaml, is there any possibility that settings for IR/RGB conversion may be released?  Thanks for your attention and amazing work:)"
I like this paper a lot! Do you have the pretrained model or the data available for the street images/day2night examples?  
"@mingyuliutw Nice work! BTW, how can i get the cropped amimal face image to train your model?  I don't want to use the pretrained model. I want to train the model from scratch with the animal data.     Thanks in advance."
"I was trying to implement UNIT to tensorflow code    So I read about paper and pytorch code and I have some questions about code.    1. In the paper, there are 4 convolution layers and single FC layer for domain classification in discriminator of domain adaptation from SVHN to MNIST, but code uses dropout layers among convolution layers. Why dropout layers are used in code?    2. In code, feature_losses that computes l1_loss between two features(fake_feat_ab, fake_feat_aa) are used in discriminator loss, but there is no explanation about that loss in paper. Can you explain why these losses are used?    Maybe I missed point in paper, but I can't find out"
"thank you for the opening-source code and I follow the instruction in  `readme` and run into this error:    > Traceback (most recent call last):    File ""train.py"", line 232, in        main(sys.argv)    File ""train.py"", line 182, in main      for it, (images_a, images_b, images_a2, images_b2) in enumerate(itertools.izip(train_loader_a, train_loader_b, train_loader_a2, train_loader_b2)):    File ""/home/lz/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py"", line 179, in __next__      batch = self.collate_fn([self.dataset[i] for i in indices])    File ""/home/lz/UNIT-master/src/datasets/dataset_image.py"", line 37, in __getitem__      crop_img = self._load_one_image(self.images[index])    File ""/home/lz/UNIT-master/src/datasets/dataset_image.py"", line 62, in _load_one_image      crop_img = img[y_offset:(y_offset + self.image_size), x_offset:(x_offset + self.image_size), :]  TypeError: only integer scalar arrays can be converted to a scalar index    can you hint me where i can make this mistake ,thanks a lot "
Hello and thanks for open-sourcing the code!    I was wondering if you could provide other lists in `datasets/celeba/lists` to train with. In particular `Sunglasses_ON.txt` /  `Sunglasses_OFF.txt`.    Thanks!
"Thanks for your source code for domain adaptation! However, I found the training set used in SVHN-MNIST experiment, you choose the 'extra' split of SVHN but many of other paper's settings are the 'train' split with 73257 images. And I change the split into 'train', the training process is much slower than before. It's accuracy on MNIST test set is 0.3773 in 58k iteration. Have you tried the 'train' split? Will it take more time to train? Thanks a lot!"
None
Could you please provide the source code of domain adaption from svhn to mnist? I guess there are some differences with the trick training face attribute transformation or other tasks. Thanks very much!
"Hi, thank you very much for your work.    I have downloaded the pretrained model and tried to run the testing in the README:  `./translate_one_image.py --config ../exps/celeba_blond_hair.yaml --image_name ../images/ian.jpg --output_image_name ../results/ian_to_eyeglasses.jpg --weights ../snapshots/celeba_eyeglasses_gen_00500000.pkl --a2b 0  `    And I face this error:       Can you tell me what did I do wrong?  Thank you beforehand. "
When running the C++ unit tests I get errors in the KG algorithm. Does that mean that the KG is not implemented correctly and you cannot use it with this library?     
"Hi,   When trying to run any of the synthetic functions or my own functions using the observations of fun. derivatives,  I'm getting the following error:     Traceback (most recent call last):    File ""/home/User/ENV_CMOE/lib/python3.6/site-packages/emcee/ensemble.py"", line 505, in __call__      return self.f(x, *self.args, **self.kwargs)    File ""/home/User/Cornell-MOE/moe/optimal_learning/python/cpp_wrappers/log_likelihood_mcmc.py"", line 310, in compute_log_likelihood      cpp_utils.cppify(noise),  TypeError: No registered converter was able to produce a C++ rvalue of type int from this Python object of type numpy.int64    I'm changing the self._observations to numpy.arange(self._dim) instead of []. I've noticed that a similar issue is mentioned   in Issue #63 , but unfortunately, a proposed workaround with [float(x) for x in ...] doesn't work. Has anybody encountered a similar problem or knows how to fix this?"
"Hi Wu Jian:    I am a beginner of bayesian optimization and I find this exciting soft ware is a good resource to learn.  Yet, I only have a computer that has Window10 and python3, how can I implement Cornell-MOE on my local computer? I hope you can give some directions, thanks !    Best regards! "
"Hi,   Thanks for the great package!   I was trying to understand the default prior placed on noise standard deviation (I suppose that is what the last few hyper-parameters mean in the   ?  When I played around with the scale parameter of the implemented horseshoe prior, I think the distribution and the generated sample do not seem to match.  Here are the codes to generate some plots and you can see while the peak of the historgram of the generated sample is between -3 and -2, the log probability of the distribution still peaks at 0.         I am worried this may influence the behavior of the algorithm.  Any thought or any suggestion for alternative prior for the noise variance?"
"Hi,    Is there an example or any instruction to show how to run the batch of sample points in parallel for real problem which involves lots of file reading and writing. Since GP is designed for expensive problems while most of the expensive problems need to run external (third party) executable programs and they usually require to save the module setup file and simulation file in different directories.     So run use Cornell-MOE in parallel on these problems it might request a unit variable to differenate each function evaluation in one iteration.     Could you please provide some suggestions or instructions on how to do this? Does the current version evaluate the batch samples of one iteration in real parallel?    Thanks,  Xia Wei.      "
"Version used:      What works fine:  - Ubuntu 18.04, Python 3.6.8, Boost 1.65.1    What fails:  - MacOS 10.14.5, Python 3.6.8, Boost 1.66.0    Linking fails         *Config*       Looks like the method signature is not correct...       Boost installed via MacPorts       Any ideas? "
"Which dependencies are required to use the python library? The setup.py file lists a ton of requirements for `install_requires` in   but most of these appear to have to do with the documentation and/or website. Which of them are actually required to run the software, and which of them are optional (or obsolete)?     Thanks!"
"Hi,     Thanks for sharing the code.    Is there ant simple ""getting started"" document on setting up like a basic EI or KG with your tool.  1- I have seen the main.py in the ""example"" folder, it is good but it is for a very complete case, not for simple sampling mode.  2- I have tried the MOE getting started document (available in the ""doc"" folder) but as the function names and procedures are changed in this repo they are not useful.    So a getting started page, or **_a few more examples of simple cases_** would be very beneficial. Some description of what each function (wrapper) is doing is also beneficial    Thanks a lot,  Vahid"
"in `struct GradientDescentParameters` of moe/optimal_learning/cpp/gpp_optimizer_parameters.hpp,  there are two parameter `num_multistarts` and `max_num_restarts`, how are they different?         "
"seems by default the prior mean function of the GP is zero, which is inappropriate if, e.g., the target function is always positive.   Is there a way to specify a constant mean or other mean functions?"
"line 120: for the TopHat prior, why is it not log(1/(max - min))? (I guess this one doesn't matter since it's a constant anyway?)  line 351, for the NormalPrior, the comment says it returns the log probablity but it seems returning the PDF value, instead of log(pdf). "
"when i set the search_domain of learning_rate ,it can not work.       cholesky matrix singular -8.577649091442408837E-06 Thread 3 of 8 failed on iteration 8 of 20. Message:  SingularMatrixException: 8 x 8 matrix is singular; 7-th leading minor is not SPD.  GP-Variance matrix singular. Check for duplicate points_to_sample/being_sampled or points_to_sample/being_sampled duplicating points_sampled with 0 noise. void optimal_learning::ExpectedImprovementEvaluator::ComputeGradExpectedImprovement(optimal_learning::ExpectedImprovementEvaluator::StateType*, double*) const (/home/test_data/zhaozhangyun/Cornell-MOE-master/moe/optimal_learning/cpp/gpp_math.cpp: 2066)  cholesky matrix singular -4.816341968136730829E-05 Thread 5 of 8 failed on iteration 14 of 20. Message:  SingularMatrixException: 8 x 8 matrix is singular; 8-th leading minor is not SPD.  GP-Variance matrix singular. Check for duplicate points_to_sample/being_sampled or points_to_sample/being_sampled duplicating points_sampled with 0 noise. void optimal_learning::ExpectedImprovementEvaluator::ComputeGradExpectedImprovement(optimal_learning::ExpectedImprovementEvaluator::StateType*, double*) const (/home/test_data/zhaozhangyun/Cornell-MOE-master/moe/optimal_learning/cpp/gpp_math.cpp: 2066)  Traceback (most recent call last):    File ""main.py"", line 183, in        cpp_sgd_params_kg, num_to_sample, num_mc=2 ** 10)    File ""/home/test_data/zhaozhangyun/Cornell-MOE-master/examples/bayesian_optimization.py"", line 31, in gen_sample_from_qei      max_num_threads=8))    File ""/home/test_data/auto_ml/venv2.7/lib/python2.7/site-packages/moe/optimal_learning/python/cpp_wrappers/expected_improvement.py"", line 101, in multistart_expected_improvement_optimization      status,  moe.build.GPP.SingularMatrixException: SingularMatrixException: 8 x 8 matrix is singular; 7-th leading minor is not SPD.  GP-Variance matrix singular. Check for duplicate points_to_sample/being_sampled or points_to_sample/being_sampled duplicating points_sampled with 0 noise. void optimal_learning::ExpectedImprovementEvaluator::ComputeGradExpectedImprovement(optimal_learning::ExpectedImprovementEvaluator::StateType*, double*) const (/home/test_data/zhaozhangyun/Cornell-MOE-master/moe/optimal_learning/cpp/gpp_math.cpp: 2066)  "
It would be great to have a wrapper class that hide a lot of the machinery needed to generate proposal points. I'm going to try to write up something for myself and can propose pulling this in. Just thought I'd raise the issue to see if anyone thinks it's a particularly bad idea.
"When running the example in Python 3.6, I get the following error:     I'm curious if this could be caused by using the newer version of numpy, as the older version listed in the requirements was giving me errors when I attempted to pip install."
"when I was trying to run many jobs on a server, it results in an error ""fork: retry: Resource temporarily unavailable"" when over 30 jobs are submitted. I googled about it, the reason seems to be the limit of max number of processes is exceeded (by default on that server is 4096). Then I checked the number of lightweight processes of the job ""python main.py Hartmann3 EI 4 1"", using the command ""ps -o nlwp  "", it turns out the process has 134 lightweight processes, so I can only run about 30 jobs. Is this because of some parameter setting?   (FYI: I tried to change the ""max_num_threads"" parameter in line 55 from 8 to 1, but didn't make a difference. I also tested a simple for loop python program, it's only 1 lightweight process) "
"Hello Jian Wu,    I am currently exploring the use of different surrogates, as a Gaussian alternative and am considering building upon your gradients' work in Cornell-MOE to do so.    I have been reading your ""Bayesian Optimization with Gradients"" paper from NIPS 2017 - congratulations on an excellent paper. In Equation 3.3, you list the multi-output formula in terms of the Gaussian mean and covariance functions.     Assume the probabilistic model then changes, from say, Gaussian Processes (GPs), to multivariate Student-t, with a different mean and covariance function for the underlying Student-t process (STP).     Would Equation 3.3 remain the same (but with different STP mean and covariance inputs)?    OR    Does Equation 3.3 have to change, each time the underlying surrogate model changes, to reflect the properties of the new probabilistic model? For example, how would going from GPs to Random Forests work?    I would obviously then look to program any amendments into my own code, using Cornell-MOE as my starting-point (with citation).    Thank you.    Regards,  Conor Clare"
None
None
It would be nice to have a discussion and list of links to other BayesOpt packages in our README file.  That would help people figuring out which BayesOpt package to use.
"It would be very nice to have additional algorithms be implemented in Cornell MOE.  This would make it easier for ML researchers doing comparisons against existing methods, and also would be useful for people using Cornell-MOE for real problems.  Two algorithms to be added that seem relatively high in priority for solving real problems, and that don't seem to be present in many other packages, are:  - The ability to specify expensive-to-evaluate constraints  - Support for high-dimensional Bayesian optimization, e.g., using random embeddings or other recently published methods    A nice-to-have for ML researchers doing benchmarking would be implementations of entropy search methods."
"Right now, performance evaluation using our preferred method for KG acquisition functions is confusing and hard to do.  When we measure performance using KG acquisition functions, we should use the point with the best posterior mean.  To make this easier and less confusing, we should change our text output to make it more clear which points are being evaluated and which ones should be used for evaluation.  The current text uses ""suggested"" and ""recommended"" to mean these two different things, which isn't very clear.  Also, we should return the vector of points that should be used for evaluation, to make it easier to do this evaluation.  We should also make sure that there are examples where we do the evaluation correctly."
None
I got this error when trying to implementing moe on windows  ImportError: No module named build.GPP    What could be the reason for this issue?
"moe/optimal_learning/python/cpp_wrappers/expected_improvement_mcmc.py    class ExpectedImprovementMCMC   def evaluate_at_point_list()   line 219 says:   :type points_to_evaluate: array of float64 with shape (num_to_evaluate, self.dim)  so the shape here should be a tuple of 2.    but in line 246:   num_to_evaluate, num_to_sample, _ = points_to_evaluate.shape  it's expected tuple of 3.    Is there something wrong?    "
"I have followed the installation instructions strictly, tried both with python 2 and python 3,  but when it completes, I get the following error message when running the example (main.py) from the 'examples' folder:    Traceback (most recent call last):    File ""main.py"", line 113, in        cpp_gp_loglikelihood.train()    File ""/home/USER/ENV_NAME/lib/python2.7/site-packages/moe/optimal_learning/python/cpp_wrappers/log_likelihood_mcmc.py"", line 207, in train      rstate0=self.rng)    File ""/home/USER/ENV_NAME/lib/python2.7/site-packages/emcee/ensemble.py"", line 384, in run_mcmc      for results in self.sample(initial_state, iterations=nsteps, **kwargs):    File ""/home/USER/ENV_NAME/lib/python2.7/site-packages/emcee/ensemble.py"", line 254, in sample      ""Initial state has a large condition number. ""  ValueError: Initial state has a large condition number. Make sure that your walkers are linearly independent for the best performance    I have tried several inputs with different objective functions, like:   python main.py Branin KG 4 1  python main.py Hartmann6 KG 4 1 HeSBO 3  etc.    Has anybody encountered a similar problem?   "
"In all of my applications I do not actually know the true function to be evaluated. In the objective I simply set `evaluate` and `evaluate_true` to be the same noisy function. However, what do I set `_sample_var` to? I would like for the GP regression model to estimate the sample variance on each step, but it is hard to tell how to set this option."
"Hi,    I followed the instruction and tried to install the package. When I run `python setup.py install` I face the following error. I would appreciate if you help me with this. Here is the part of the output in which the errors start:     "
"Apologies in advance if this is the wrong place to post questions.    I'm interested in applying this package on some data where the input data lies on a simple curved discrete surface (2D triangulated mesh embedded in 3D space), i have two questions with respect to this application:    1. Does the package handle discrete search domains, if so, how?   2. If that software does not handle discrete domains. Suppose I were to approximate my discrete surface with some bounded polynomial surface such that it is continuous, are there ways to specify this constraint (data must lie on polynomial plane)?     I'm a little wary of using a bounding box search space because the true theoretical optimal solution is outside the parametric surface I've defined but it is unfeasible physically. "
None
"This is an error resulted from executing the example: python main.py Hartmann3 KG 4 1  After installation (which seems to be successful since no error was resulted, only a bunch of warnings), under moe/ there are these files: build  __init__.py  __init__.pyc  optimal_learning  tests, but no ""views"".  Anyone know how to fix this? Thanks    Traceback (most recent call last):    File ""main.py"", line 5, in        from moe.optimal_learning.python.cpp_wrappers.domain import TensorProductDomain as cppTensorProductDomain    File ""/home/shali/virtual_env/local/lib/python2.7/site-packages/moe/optimal_learning/python/cpp_wrappers/domain.py"", line 11, in        from moe.optimal_learning.python.constant import TENSOR_PRODUCT_DOMAIN_TYPE, SIMPLEX_INTERSECT_TENSOR_PRODUCT_DOMAIN_TYPE    File ""/home/shali/virtual_env/local/lib/python2.7/site-packages/moe/optimal_learning/python/constant.py"", line 6, in        import moe.views.constant as views_constant  ImportError: No module named views.constant  "
I was following your instructions to install and try an example. But I didn't find main.py...
"Without emcee, the following error shows up:   python main.py Hartmann3 4 1  Traceback (most recent call last):    File ""main.py"", line 7, in        from moe.optimal_learning.python.cpp_wrappers.log_likelihood_mcmc import GaussianProcessLogLikelihoodMCMC as cppGaussianProcessLogLikelihoodMCMC    File ""/home/ls66635/anaconda2/lib/python2.7/site-packages/moe/optimal_learning/python/cpp_wrappers/log_likelihood_mcmc.py"", line 57, in        import emcee  ImportError: No module named emcee    It turns out that  emcee is imported while not in requirement.txt    import copy    import numpy  import emcee  from scipy import optimize    import moe.build.GPP as C_GP    Then I install 2.21 through pip install emcee (2.2.1)    Now another error shows up    examples git:(master) python main.py Branin KG 4 1000 1   emcee: Exception while calling your likelihood function:    params: [  9.15380851e-03  -1.27299406e-01  -1.89707753e+00  -1.84206807e+01]    args: []    kwargs: {}    exception:  Traceback (most recent call last):    File ""/home/ls66635/anaconda2/lib/python2.7/site-packages/emcee/ensemble.py"", line 519, in __call__      return self.f(x, *self.args, **self.kwargs)    File ""/home/ls66635/anaconda2/lib/python2.7/site-packages/moe/optimal_learning/python/cpp_wrappers/log_likelihood_mcmc.py"", line 308, in compute_log_likelihood      cpp_utils.cppify(noise),  ArgumentError: Python argument types in      moe.build.GPP.compute_log_likelihood(list, list, int, int, LogLikelihoodTypes, list, list, int, list)  did not match C++ signature:      compute_log_likelihood(boost::python::list, boost::python::list, int, int, optimal_learning::LogLikelihoodTypes, boost::python::list, boost::python::list)  Traceback (most recent call last):    File ""main.py"", line 78, in        cpp_gp_loglikelihood.train()    File ""/home/ls66635/anaconda2/lib/python2.7/site-packages/moe/optimal_learning/python/cpp_wrappers/log_likelihood_mcmc.py"", line 199, in train      rstate0=self.rng)    File ""/home/ls66635/anaconda2/lib/python2.7/site-packages/emcee/sampler.py"", line 172, in run_mcmc      **kwargs):    File ""/home/ls66635/anaconda2/lib/python2.7/site-packages/emcee/ensemble.py"", line 198, in sample      lnprob, blobs = self._get_lnprob(p)    File ""/home/ls66635/anaconda2/lib/python2.7/site-packages/emcee/ensemble.py"", line 382, in _get_lnprob      results = list(M(self.lnprobfn, [p[i] for i in range(len(p))]))    File ""/home/ls66635/anaconda2/lib/python2.7/site-packages/emcee/ensemble.py"", line 519, in __call__      return self.f(x, *self.args, **self.kwargs)    File ""/home/ls66635/anaconda2/lib/python2.7/site-packages/moe/optimal_learning/python/cpp_wrappers/log_likelihood_mcmc.py"", line 308, in compute_log_likelihood      cpp_utils.cppify(noise),  Boost.Python.ArgumentError: Python argument types in      moe.build.GPP.compute_log_likelihood(list, list, int, int, LogLikelihoodTypes, list, list, int, list)  did not match C++ signature:      compute_log_likelihood(boost::python::list, boost::python::list, int, int, optimal_learning::LogLikelihoodTypes, boost::python::list, boost::python::list)  "
Would it be possible to port to python 3?  The work seems to have been done on the original yelp/MOE depot but never reached master:   
It would be nice to support Python 3 by default. Can you shortly write down the things required for a Python 3 port? Is it trivial via `futurize` or `six` packages?
moe/optimal_learning/python/base_prior.py:198: RuntimeWarning: divide by zero encountered in log    return np.log(np.log(1 + 3.0 * (self.scale / np.exp(theta)) ** 2))
The new implementations encourage the computations' sharing.
Unify the code of the square-exponential kernel and the Matern kernels with derivative observations.
None
None
"In particular, I'm referring to the analytical formula provided by ""Fast Computation of the Multi-Points Expected Improvement with Applications in Batch Selection"" (Chevalier 12), the comparison to which I couldn't find in your paper, in terms of etc. speed and accuracy. It seems that for smaller values of q, it's pretty fast."
A bug reported by a user.
"Hi,  I get the following numbers when I run test_SUSY, which are different than the ones reported in the paper (19.6%,  0.877):==>   C_ERR = 0.1972; AUC = 0.8754    Could it be due to different versions of the matlab I am using, if yes, please can you provide the matlab version you used ?"
"Very great work!  I want to ask, is your RGB images rendered by yourself? Is there color information?  If I want to get a dataset which includes RGB images and point clouds of each single object, is there such a data set?  Looking forward to your reply!  Thank you!"
"Hi, I'm trying to replicate your projection work with PyTorch. But I'm confused about the definition of ""focal length"" and ""disf"" in your code. That is:  ___________________________________________________  focal_length = math.sqrt(3)/2 ?   dmin = 1/(focal_length + math.sqrt(3))  dmax = 1/(focal_length)  for k=1,depth do      disf = dmin + (k-1)/(depth-1) * (dmax-dmin)  baseGrid[k][i][j][1] = 1/disf  ___________________________________________________  Please forgive my offense, I have followed your advice and read the appropriate books. I still don't understand how focal length is defined. And in the paper your point that ""the minimum and maximum disparity in the camera frame are denoted as dmin and dmax"", So who is a disparty, disf or 1/disf? Your help means a lot to me."
Thank you for what you have done!  I'm confused about the translate matrix .here are the specific question:  (1)Is the original focal length=1(i.e. if elevation=0deg) ?  (2)Why the translate matrix set to be as follows at first?  !     could you please explain it?  I'll appreciate for you reply!
"I am sorry to bother you . Recently  i want to reproduct the PTN with tensorflow implementation . I want to use my own data , but having problems in making tfrecords format data. Could you please show me the codes to make  tfrecords format data with the following features(float representations) : image , mask, vox . Looking forward to your reply , thanks."
None
None
"Hi, @xcyan,    The   of your tensorflow implementation is broken, could you fix it?    THX"
"Hi,  I was just evaluating the pretrained model and encountered the following issue regarding the memory space of the gpu,  THCudaCheck FAIL file=/tmp/luarocks_cutorch-scm-1-1355/cutorch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory  /home/sbasavaraju/torch/install/bin/luajit: ...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:351: cuda runtime error (2) : out of memory at /tmp/luarocks_cutorch-scm-1-1355/cutorch/lib/THC/generic/THCStorage.cu:66  stack traceback:   [C]: in function 'read'   ...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:351: in function     [C]: in function 'read'   ...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:351: in function 'readObject'   ...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:369: in function 'readObject'   /home/sbasavaraju/torch/install/share/lua/5.1/nn/Module.lua:192: in function 'read'   ...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:351: in function 'readObject'   ...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:369: in function 'readObject'   ...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:369: in function 'readObject'   /home/sbasavaraju/torch/install/share/lua/5.1/nn/Module.lua:192: in function 'read'   ...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:351: in function 'readObject'   ...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:369: in function 'readObject'   ...e/sbasavaraju/torch/install/share/lua/5.1/torch/File.lua:409: in function 'load'   scripts/eval_quant_test.lua:63: in main chunk   [C]: in function 'dofile'   ...raju/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk   [C]: at 0x00406670    I have Ubuntu 14,04 with graphics model NVIDIA GeForce GTX 470 1 GB , is the graphics not sufficient to run the program ?    Thanks "
"Hi Doc. @xcyan, I encountered the following problem during running ""eval_models.sh"".   `torch/install/share/lua/5.1/torch/File.lua:351: read error: read 39866875 blocks instead of 46656000 at torch/pkg/torch/lib/TH/THDiskFile.c:356`  I thought it is because of the data corruption. Could you please share the checksum of the pretrained models for the data validation? Thank you. "
"In    in the expression math.min(data:size() * opt.nview / 2 , opt.ntrain), I don't understand where the division by two is coming. data:size() is the number of trainings scenes you have, 4744 in this case, and you have 24 views per scene, so multiplication makes sense, but why divide it by two. Maybe I missed something, but it seems that only half of the views/data is used?"
"Hello,  after training the encoder(CNN-Vol) and the perspective transformer (PTN-Proj) I test the final model by changing the lines:  `base_loader = torch.load(opt.checkpoint_dir .. 'arch_PTN_singleclass_nv24_adam1_bs6_nz512_wd0.001_lbg(0,1)_ks24_vs32/net-epoch-100.t7')  encoder = base_loader.encoder  base_voxel_dec = base_loader.voxel_dec    unsup_loader = torch.load(opt.checkpoint_dir .. 'arch_PTN_singleclass_nv24_adam1_bs6_nz512_wd0.001_lbg(1,0)_ks24_vs32/net-epoch-100.t7')  unsup_voxel_dec = unsup_loader.voxel_dec    sup_loader = torch.load(opt.checkpoint_dir .. 'ptn_comb.t7')  sup_voxel_dec = sup_loader.voxel_dec`    The results on the testset are:  cat [chair]: CNN-VOL IOU = 0.459553 PTN-COMB IOU = 0.162989 PTN-PROJ IOU = 0.472389     which are 4 to 5 points lower than reported in the paper. What could be the reasons for it? Also I noticed the pretrained ptn-comb model has some problems (0.16) when evaluated with my encoder (instead of the pretrained encoder). What is the reason for this?  "
"Hi,  I trained the single class encoder with ./demo_pretrain_singleclass.sh. Now I wanted to evaluate the trained models. So in eval_quant_test.lua I just changed the name of the loaded file (cnn_vol.t7) to the last trained model:  base_loader = torch.load(opt.checkpoint_dir .. 'arch_rotatorRNN_singleclass_nv24_adam2_bs8_nz512_wd0.001_lbg10_ks16/net-epoch-20.t7')  encoder = base_loader.encoder  base_voxel_dec = base_loader.voxel_dec    When I run the testcript eval_models.sh I get following error:  /home/meeso/torch/install/bin/luajit: scripts/eval_quant_test.lua:90: attempt to index global 'base_voxel_dec' (a nil value)  stack traceback:   scripts/eval_quant_test.lua:90: in main chunk   [C]: in function 'dofile'   ...eeso/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk   [C]: at 0x00406670    Any idea how I fix this?"
"Hi,  I downloaded the pretrained models and after running run ./eval_models.sh and got these numbers  CNN-VOL IOU = 0.500177 PTN-COMB IOU = 0.509016 PTN-PROJ IOU = 0.503761   they are not far off, but still don't correspond to the paper numbers, any idea why I am getting different results?"
There might be a typo in `scripts/eval_quant_test.lua`:  `require 'stn'     ------> require 'ptn'`
"I am getting a     > clojure.lang.Compiler$CompilerException: java.lang.RuntimeException: Method code too large!, compiling:(C:\Users\Boris\Boris\home\projects\methodlarge\src\methodlarge\core.clj:13:1)    error when feeding BOPP too many priors and a primitive procedure. Here is a minimal example with BOPP 0.1.5 with java version 1.8.0_191, in which the first defopt example spits the method code too large error. The problem doesn't seem to be with anglican, but with BOPP. Is there any way around it?         What I can find on the internet, is that some functions in clojure can generate errors like that. See:   "
"I am trying to figure out how to correctly provide a list of initial points in BOPP. Am I overlooking something simple?    Here are the details. The doopt, the output of BOPP, and the defopt.       And from the output I get before the thing crashes is that BOPP doesn't like the format I use for initial-points. One, it reformats the map into a collcetion of [keyword values], and two, it drops the last keyword-value pair ({:beta_p 0.16744317384178953} in this case)    >:intial-points ([:intro_day 35] [:intro_duration 1] [:distancing 0.6322107370456067] [:beta_b 0.05537357023136269] [:beta_bS 0.19621440146596547] [:beta_lice 0.16614719314835572] {:intro_day 32, :intro_duration 1, :distancing 1.024606244767797, :beta_b 0.06818428961187084, :beta_bS 0.1391747830274707, :beta_lice 0.14780650814426455, :beta_p 0.27853257654947244}      If I do not specify any initial points, BOPP runs fine, and feeds the run-epidemic function with the sampled maps of priors that the run-epidemic function expects.     "
Please could you tell me where to download the dataset from?
"Hey Matthew!    First of all, thanks for the awesome ideas and work.    I have a question about the way you're computing `pgm_natgrad`. Specifically  . I'm copying the relevant lines:         If I understand correctly, the dropped term is this:    !     Which in the paper you mention ""is computed automatically as part of the backward pass for computing the gradients with respect to the other parameters"".    Can you clarify why that term is dropped? Also, I don't understand the minus sign, right in the beginning of the assignment, line 33.    Again, congrats on the awesome work!    ---    On a side note, I think I spotted 2 errors in the paper:    1) In section 4.2 (and then again in the appendix), where you define \eta_x to be a partial local optimizer of the surrogate objective:  !    I believe this should be argmax, rather than argmin. Can you confirm?    2) In the second expression of proposition 4.2:  !   I think the gradient should be w.r.t. \theta, rather than x. Is that correct?"
"  should refer to  , but actually refers to a function in the same file with the same name  . This leads to a bug when  . The  ) should be a 1-D ndarray, but it is a scalar.      "
"Hi, I've been going through this code for the last week and I'm really excited about its potential. However, I'm currently stuck with the `lds_svae_dots.py` example. I've tried many different hyper-parameter initializations and I cannot reproduce the corresponding figure from the paper (not even with the ones reported in the paper). What you see below is kind of the closest I can get:         For those who are trying as well and if the code is slow then please go to the `svae.optimizers.py` file and unindent the callback such that the plotting function is called just once for a corresponding batch. You might also need to comment out the line with `plt.close('all')` in `lds_svae_dots.py`.    What follows is just a short report of what I've found out. The VAE part seems to work as the input is correctly reconstructed whereas the inference is usually off (see fig above). I ran the tests and realized that some of them were failing for the newest version of pylds. I therefore installed an older version of pylds (73fceec2215347e0a0e35a5f116e69aa719b2efc) which made the tests pass on a commit from April 2016 (a89e886acf9c8f4afaf0b19b8debe102a3b153c1).    Unfortunately this does not fix the issue with reproducing the LDS result so I am wondering if you are aware of what might be the underlying reason for why the model doesn't converge to a good solution? Could there be a bug in the inference part of the code which causes this behavior?    Thank you for making the code publicly available!    Cheers,    Haffi"
"Hi,    It seems that there has been a great deal of rearranging of this code since publication.  Which commit should I clone to reproduce the figures shown in the paper?    Brian"
"I was trying to import svae.svae and got the error:    Traceback (most recent call last):    File "" "", line 1, in      File ""svae/svae.py"", line 5, in        from util import split_into_batches, get_num_datapoints    File ""svae/util.py"", line 12, in        from autograd.container_types import TupleNode, ListNode  ImportError: cannot import name TupleNode    and checked autograd.container_types, this TupleNode is not available there, or did I miss something? Thanks for any help."
"Hi, I have the correct set up to reproduce the gmm experiments and am now trying to reproduce the lds dots experiment via the script in this file:      But I'm having trouble getting the code to run (attached the last error). Any help is appreciated, thanks!  !    "
"I saw you fixed the from test_util (change to svae.util), but somehow it reverted on trunk?  perhaps flesh out the setup.py a bit.   Here is what I did (version requirements are just whatever  I had available on my Ubuntu 16.04 box).  I did not see the other experiments/ that wuaalb was mentioning. Perhaps they are on a branch?          diff --git a/setup.py b/setup.py      index c841313..01b893c 100644      --- a/setup.py      +++ b/setup.py      @@ -3,6 +3,16 @@ import numpy as np      from Cython.Build import cythonize          setup(      +    name='svae',      +    version='0.0.0',      +    description='structure variational auto-encoder',      +    install_requires=['autograd>=1.1.7', 'numpy>=1.11.0', 'scipy>=0.17.0', 'Cython>=0.25.1'      +                      , 'pyhsmm>=0.1.6', 'toolz>=0.8.1'],      +    keywords=['autoencoder', 'machine learning', 'optimization'      +              , 'neural networks', 'Python', 'Numpy', 'Scipy'],      +    url='       +    packages=['svae', 'svae.distributions', 'svae.hmm', 'svae.lds', 'svae.models'],      +           ext_modules=cythonize('**/*.pyx'),           include_dirs=[np.get_include(),],       )      diff --git a/tests/test_gaussian.py b/tests/test_gaussian.py      index 7b7e560..2fa0daa 100644      --- a/tests/test_gaussian.py      +++ b/tests/test_gaussian.py      @@ -5,8 +5,7 @@ from autograd import grad              from svae.distributions.gaussian import logZ, expectedstats, \           pack_dense, unpack_dense      -from test_util import rand_psd      -      +from svae.util import rand_psd              def rand_gaussian(n):           J = rand_psd(n) + n * np.eye(n)        "
I will do it. 
Any instructions on how to install svae? A similar to autograd and simple install like   pip install svae or pip install    would be appreciated.    Thanks 
"Hi! Thanks for sharing the code and congrats for this amazing article!    I have a particular doubt about the natural parameterisation of the NIW distribution and I saw that in your code there is a function to re-parameterise it (`standard_to_natural()` in `svae/distributions/niw.py`). In particular, I don't exactly see where the outer product in parameter `S`. Do you know any reference where I can check out the natural parameterisation for the NIW distribution (I couldn't find any)?    Many thanks in advance!"
"Hi, i tried to re-produce the results in paper using this code, but i meet the following confusing errors:  i run instruction `python setup.py build_ext --inplace` on ubuntu 14.04 with python 2.7.6, scipy, numpy and gcc 4.7.3 correctly installed. Could anyone help me? Thanks!   So sorry for placing so long errors here.    - andrew@ubuntu1:~/xu/svae$ python setup.py build_ext --inplace  - `missing cimport in module 'scipy.linalg.cython_lapack': ./svae/cython_linalg_grads.pxd  - missing cimport in module 'scipy.linalg.cython_blas': ./svae/cython_linalg_grads.pxd  - missing cimport in module 'scipy.linalg.cython_lapack': ./svae/cython_util.pxd  - missing cimport in module 'scipy.linalg.cython_blas': ./svae/cython_util.pxd  - missing cimport in module 'scipy.linalg.cython_lapack': ./svae/lds/cython_gaussian_grads.pxd  - missing cimport in module 'scipy.linalg.cython_blas': ./svae/lds/cython_gaussian_grads.pxd  - missing cimport in module 'scipy.linalg.cython_lapack': svae/lds/cython_lds_inference.pyx  - missing cimport in module 'scipy.linalg.cython_blas': svae/lds/cython_lds_inference.pyx  - missing cimport in module 'scipy.linalg.cython_blas': svae/hmm/cython_hmm_inference.pyx  - Compiling tests/test_cython_linalg_grads.pyx because it changed.  - Compiling tests/test_cython_gaussian_grads.pyx because it changed.  - Compiling svae/lds/cython_lds_inference.pyx because it changed.  - Compiling svae/hmm/cython_hmm_inference.pyx because it changed.  - Cythonizing svae/hmm/cython_hmm_inference.pyx  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  - # cython: boundscheck=False, nonecheck=False, wraparound=False, cdivision=True  -   - import numpy as np  - cimport numpy as np  - import numpy.random as npr  - from scipy.linalg.cython_blas cimport ddot, dgemm, dgemv  - ^  - ------------------------------------------------------------  -   - svae/hmm/cython_hmm_inference.pyx:7:0: 'scipy.linalg.cython_blas.pxd' not found  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  - # cython: boundscheck=False, nonecheck=False, wraparound=False, cdivision=True  -   - import numpy as np  - cimport numpy as np  - import numpy.random as npr  - from scipy.linalg.cython_blas cimport ddot, dgemm, dgemv  - ^  - ------------------------------------------------------------  -   - svae/hmm/cython_hmm_inference.pyx:7:0: 'ddot.pxd' not found  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  - # cython: boundscheck=False, nonecheck=False, wraparound=False, cdivision=True  -   - import numpy as np  - cimport numpy as np  - import numpy.random as npr  - from scipy.linalg.cython_blas cimport ddot, dgemm, dgemv  -                                      ^  - ------------------------------------------------------------  -   - svae/hmm/cython_hmm_inference.pyx:7:38: Name 'ddot' not declared in module 'scipy.linalg.cython_blas'  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  - # cython: boundscheck=False, nonecheck=False, wraparound=False, cdivision=True  -   - import numpy as np  - cimport numpy as np  - import numpy.random as npr  - from scipy.linalg.cython_blas cimport ddot, dgemm, dgemv  - ^  - ------------------------------------------------------------  -   - svae/hmm/cython_hmm_inference.pyx:7:0: 'dgemm.pxd' not found  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  - # cython: boundscheck=False, nonecheck=False, wraparound=False, cdivision=True  -   - import numpy as np  - cimport numpy as np  - import numpy.random as npr  - from scipy.linalg.cython_blas cimport ddot, dgemm, dgemv  -                                            ^  - ------------------------------------------------------------  -   - svae/hmm/cython_hmm_inference.pyx:7:44: Name 'dgemm' not declared in module 'scipy.linalg.cython_blas'  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  - # cython: boundscheck=False, nonecheck=False, wraparound=False, cdivision=True  -   - import numpy as np  - cimport numpy as np  - import numpy.random as npr  - from scipy.linalg.cython_blas cimport ddot, dgemm, dgemv  - ^  - ------------------------------------------------------------  -   - svae/hmm/cython_hmm_inference.pyx:7:0: 'dgemv.pxd' not found  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  - # cython: boundscheck=False, nonecheck=False, wraparound=False, cdivision=True  -   - import numpy as np  - cimport numpy as np  - import numpy.random as npr  - from scipy.linalg.cython_blas cimport ddot, dgemm, dgemv  -                                                   ^  - ------------------------------------------------------------  -   - svae/hmm/cython_hmm_inference.pyx:7:51: Name 'dgemv' not declared in module 'scipy.linalg.cython_blas'  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  - # cython: boundscheck=False, nonecheck=False, wraparound=False, cdivision=True  -   - from scipy.linalg.cython_blas cimport dsymv, ddot, dgemm, dgemv  - ^  - ------------------------------------------------------------  -   - svae/cython_util.pxd:3:0: 'dsymv.pxd' not found  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  - # cython: boundscheck=False, nonecheck=False, wraparound=False, cdivision=True  -   - from scipy.linalg.cython_blas cimport dsymv, ddot, dgemm, dgemv  -                                      ^  - ------------------------------------------------------------  -   - svae/cython_util.pxd:3:38: Name 'dsymv' not declared in module 'scipy.linalg.cython_blas'  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  - # cython: boundscheck=False, nonecheck=False, wraparound=False, cdivision=True  -   - from scipy.linalg.cython_blas cimport dsymv, ddot, dgemm, dgemv  -                                             ^  - ------------------------------------------------------------  -   - svae/cython_util.pxd:3:45: Name 'ddot' not declared in module 'scipy.linalg.cython_blas'  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  - # cython: boundscheck=False, nonecheck=False, wraparound=False, cdivision=True  -   - from scipy.linalg.cython_blas cimport dsymv, ddot, dgemm, dgemv  -                                                   ^  - ------------------------------------------------------------  -   - svae/cython_util.pxd:3:51: Name 'dgemm' not declared in module 'scipy.linalg.cython_blas'  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  - # cython: boundscheck=False, nonecheck=False, wraparound=False, cdivision=True  -   - from scipy.linalg.cython_blas cimport dsymv, ddot, dgemm, dgemv  -                                                          ^  - ------------------------------------------------------------  -   - svae/cython_util.pxd:3:58: Name 'dgemv' not declared in module 'scipy.linalg.cython_blas'  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  - # cython: boundscheck=False, nonecheck=False, wraparound=False, cdivision=True  -   - from scipy.linalg.cython_blas cimport dsymv, ddot, dgemm, dgemv  - from scipy.linalg.cython_lapack cimport dtrtrs, dpotrf, dpotrs, dpotri  - ^  - ------------------------------------------------------------  -   - svae/cython_util.pxd:4:0: 'scipy.linalg.cython_lapack.pxd' not found  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  - # cython: boundscheck=False, nonecheck=False, wraparound=False, cdivision=True  -   - from scipy.linalg.cython_blas cimport dsymv, ddot, dgemm, dgemv  - from scipy.linalg.cython_lapack cimport dtrtrs, dpotrf, dpotrs, dpotri  - ^  - ------------------------------------------------------------  -   - svae/cython_util.pxd:4:0: 'dtrtrs.pxd' not found  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  - # cython: boundscheck=False, nonecheck=False, wraparound=False, cdivision=True  -   - from scipy.linalg.cython_blas cimport dsymv, ddot, dgemm, dgemv  - from scipy.linalg.cython_lapack cimport dtrtrs, dpotrf, dpotrs, dpotri  -                                        ^  - ------------------------------------------------------------  -   - svae/cython_util.pxd:4:40: Name 'dtrtrs' not declared in module 'scipy.linalg.cython_lapack'  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  - # cython: boundscheck=False, nonecheck=False, wraparound=False, cdivision=True  -   - from scipy.linalg.cython_blas cimport dsymv, ddot, dgemm, dgemv  - from scipy.linalg.cython_lapack cimport dtrtrs, dpotrf, dpotrs, dpotri  - ^  - ------------------------------------------------------------  -   - svae/cython_util.pxd:4:0: 'dpotrf.pxd' not found  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  - # cython: boundscheck=False, nonecheck=False, wraparound=False, cdivision=True  -   - from scipy.linalg.cython_blas cimport dsymv, ddot, dgemm, dgemv  - from scipy.linalg.cython_lapack cimport dtrtrs, dpotrf, dpotrs, dpotri  -                                                ^  - ------------------------------------------------------------  -   - svae/cython_util.pxd:4:48: Name 'dpotrf' not declared in module 'scipy.linalg.cython_lapack'  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  - # cython: boundscheck=False, nonecheck=False, wraparound=False, cdivision=True  -   - from scipy.linalg.cython_blas cimport dsymv, ddot, dgemm, dgemv  - from scipy.linalg.cython_lapack cimport dtrtrs, dpotrf, dpotrs, dpotri  - ^  - ------------------------------------------------------------  -   - svae/cython_util.pxd:4:0: 'dpotrs.pxd' not found  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  - # cython: boundscheck=False, nonecheck=False, wraparound=False, cdivision=True  -   - from scipy.linalg.cython_blas cimport dsymv, ddot, dgemm, dgemv  - from scipy.linalg.cython_lapack cimport dtrtrs, dpotrf, dpotrs, dpotri  -                                                        ^  - ------------------------------------------------------------  -   - svae/cython_util.pxd:4:56: Name 'dpotrs' not declared in module 'scipy.linalg.cython_lapack'  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  - # cython: boundscheck=False, nonecheck=False, wraparound=False, cdivision=True  -   - from scipy.linalg.cython_blas cimport dsymv, ddot, dgemm, dgemv  - from scipy.linalg.cython_lapack cimport dtrtrs, dpotrf, dpotrs, dpotri  - ^  - ------------------------------------------------------------  -   - svae/cython_util.pxd:4:0: 'dpotri.pxd' not found  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  - # cython: boundscheck=False, nonecheck=False, wraparound=False, cdivision=True  -   - from scipy.linalg.cython_blas cimport dsymv, ddot, dgemm, dgemv  - from scipy.linalg.cython_lapack cimport dtrtrs, dpotrf, dpotrs, dpotri  -                                                                ^  - ------------------------------------------------------------  -   - svae/cython_util.pxd:4:64: Name 'dpotri' not declared in module 'scipy.linalg.cython_lapack'  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  -     for t in range(T):  -         themax = max_vector(node_params[t])  -         for i in range(N):  -             alpha[t,i] = in_potential[i] * exp(node_params[t,i] - themax)  -         lognorm += log(normalize_inplace(alpha[t])) + themax  -         dgemv('T', &N, &N, &one, &pair_params[0,0], &N, &alpha[t,0], &inc,  -             ^  - ------------------------------------------------------------  -   - svae/hmm/cython_hmm_inference.pyx:85:13: undeclared name not builtin: dgemv  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  -     for t in range(T):  -         themax = max_vector(node_params[t])  -         for i in range(N):  -             alpha[t,i] = in_potential[i] * exp(node_params[t,i] - themax)  -         lognorm += log(normalize_inplace(alpha[t])) + themax  -         dgemv('T', &N, &N, &one, &pair_params[0,0], &N, &alpha[t,0], &inc,  -                   ^  - ------------------------------------------------------------  -   - svae/hmm/cython_hmm_inference.pyx:85:19: Cannot convert 'int *' to Python object  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  -     for t in range(T):  -         themax = max_vector(node_params[t])  -         for i in range(N):  -             alpha[t,i] = in_potential[i] * exp(node_params[t,i] - themax)  -         lognorm += log(normalize_inplace(alpha[t])) + themax  -         dgemv('T', &N, &N, &one, &pair_params[0,0], &N, &alpha[t,0], &inc,  -                       ^  - ------------------------------------------------------------  -   - svae/hmm/cython_hmm_inference.pyx:85:23: Cannot convert 'int *' to Python object  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  -     for t in range(T):  -         themax = max_vector(node_params[t])  -         for i in range(N):  -             alpha[t,i] = in_potential[i] * exp(node_params[t,i] - themax)  -         lognorm += log(normalize_inplace(alpha[t])) + themax  -         dgemv('T', &N, &N, &one, &pair_params[0,0], &N, &alpha[t,0], &inc,  -                           ^  - ------------------------------------------------------------  -   - svae/hmm/cython_hmm_inference.pyx:85:27: Cannot convert 'double *' to Python object  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  -     for t in range(T):  -         themax = max_vector(node_params[t])  -         for i in range(N):  -             alpha[t,i] = in_potential[i] * exp(node_params[t,i] - themax)  -         lognorm += log(normalize_inplace(alpha[t])) + themax  -         dgemv('T', &N, &N, &one, &pair_params[0,0], &N, &alpha[t,0], &inc,  -                                 ^  - ------------------------------------------------------------  -   - svae/hmm/cython_hmm_inference.pyx:85:33: Cannot convert 'double *' to Python object  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  -     for t in range(T):  -         themax = max_vector(node_params[t])  -         for i in range(N):  -             alpha[t,i] = in_potential[i] * exp(node_params[t,i] - themax)  -         lognorm += log(normalize_inplace(alpha[t])) + themax  -         dgemv('T', &N, &N, &one, &pair_params[0,0], &N, &alpha[t,0], &inc,  -                                                    ^  - ------------------------------------------------------------  -   - svae/hmm/cython_hmm_inference.pyx:85:52: Cannot convert 'int *' to Python object  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  -     for t in range(T):  -         themax = max_vector(node_params[t])  -         for i in range(N):  -             alpha[t,i] = in_potential[i] * exp(node_params[t,i] - themax)  -         lognorm += log(normalize_inplace(alpha[t])) + themax  -         dgemv('T', &N, &N, &one, &pair_params[0,0], &N, &alpha[t,0], &inc,  -                                                        ^  - ------------------------------------------------------------  -   - svae/hmm/cython_hmm_inference.pyx:85:56: Cannot convert 'double *' to Python object  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  -     for t in range(T):  -         themax = max_vector(node_params[t])  -         for i in range(N):  -             alpha[t,i] = in_potential[i] * exp(node_params[t,i] - themax)  -         lognorm += log(normalize_inplace(alpha[t])) + themax  -         dgemv('T', &N, &N, &one, &pair_params[0,0], &N, &alpha[t,0], &inc,  -                                                                     ^  - ------------------------------------------------------------  -   - svae/hmm/cython_hmm_inference.pyx:85:69: Cannot convert 'int *' to Python object  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  -         themax = max_vector(node_params[t])  -         for i in range(N):  -             alpha[t,i] = in_potential[i] * exp(node_params[t,i] - themax)  -         lognorm += log(normalize_inplace(alpha[t])) + themax  -         dgemv('T', &N, &N, &one, &pair_params[0,0], &N, &alpha[t,0], &inc,  -               &zero, &in_potential[0], &inc)  -              ^  - ------------------------------------------------------------  -   - svae/hmm/cython_hmm_inference.pyx:86:14: Cannot convert 'double *' to Python object  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  -         themax = max_vector(node_params[t])  -         for i in range(N):  -             alpha[t,i] = in_potential[i] * exp(node_params[t,i] - themax)  -         lognorm += log(normalize_inplace(alpha[t])) + themax  -         dgemv('T', &N, &N, &one, &pair_params[0,0], &N, &alpha[t,0], &inc,  -               &zero, &in_potential[0], &inc)  -                     ^  - ------------------------------------------------------------  -   - svae/hmm/cython_hmm_inference.pyx:86:21: Cannot convert 'double *' to Python object  -   - Error compiling Cython file:  - ------------------------------------------------------------  - ...  -         themax = max_vector(node_params[t])  -         for i in range(N):  -             alpha[t,i] = in_potential[i] * exp(node_params[t,i] - themax)  -         lognorm += log(normalize_inplace(alpha[t])) + themax  -         dgemv('T', &N, &N, &one, &pair_params[0,0], &N, &alpha[t,0], &inc,  -               &zero, &in_potential[0], &inc)  -                                       ^  - ------------------------------------------------------------  -   -   - svae/hmm/cython_hmm_inference.pyx:86:39: Cannot convert 'int *' to Python object  - Traceback (most recent call last):  -   File ""setup.py"", line 6, in    -     ext_modules=cythonize('**/*.pyx'),  -   File ""/usr/lib/python2.7/dist-packages/Cython/Build/Dependencies.py"", line 798, in cythonize  -     cythonize_one(*args[1:])  -   File ""/usr/lib/python2.7/dist-packages/Cython/Build/Dependencies.py"", line 915, in cythonize_one  -     raise CompileError(None, pyx_file)  - Cython.Compiler.Errors.CompileError: svae/hmm/cython_hmm_inference.pyx`"
"Hi Matt,  I'm not sure if this is right place to ask question about your paper?  But anyway, I implemented a version of latent Gaussain Mixture model (normal-gamma prior for the gaussian). I found that the gradient from the KL loss term ( E_q[ KL(q(x)||P(x|theta))] ) make the result worse. Well, it makes the latent space looks like a gaussian and the generator network wouldn't be able to learn to reconstruct at all. I manage to make it works sometime but all of the time without the KL loss.  What I am wondering is, is this the behaviour you see on your experiments?   Sub-question, I'm using learning rate around 0.1- 0.2 for updating global parameters with natural gradient and I'm using 0.001 for the neural network recogniser and generator...Do you optimise the two parts seperately or together? It seems like in the paper the theory suggest same learning rate but I'm not sure. Sorry, I should have read through your code to answer these question but I'm a little bit clueless reading code in general.  my code if anyone interested    Thanks alot in advance, Nat "
"After running the script gmm_svae_synth.py (around one and a half hours on a Mac desktop computer), the results seem not reasonable (running_gmm_2000.png). So I just wondering if  there is anything wrong with the script?  !  "
Hey Matty --- I'm seeing an assertion error during the `resnet_decode` step when running the `gmm_svae_synth.py` example as is:     It looks like the param sizes passed to add have shapes     so the second set of params coming from `linear_decode` has a last dimension twice as big as the second set of params coming from `mlp_decode`.    Is one passing back a dense covariance and the other passing back a diagonal covariance? 
"Hi, do you plan to release the code for hierarchical VI modules at some point?"
"Hi,    Could you please let me know why the obstacles are not contained in the rollout testing phase?  Since the code only considers the transitions to free cells not the valid transition under a particular action      Thanks"
"Hi,     why l_q equals 10 rather than 8?  Thanks"
"From my view of the paper, examples shown in the main paper were mainly aimed at supervised learning (imitation learning), though there are some examples using reinforcement learning. So the question is does VIN naturally work with RL? In addition, almost all examples involve extracting some high level grid world representation of the state space, it is not clear how this model may be applied to a more realistic domain where representing all states may be infeasible? "
"Hi, could you release the code for gridworld with reinforcement learning? Thanks a lot!"
"Hi! Recently, we tried to reproduce the experiment, but at the visualization part (script_viz_policy.m) has to use MATLAB call python code (vin.py). In this part, the code imports theano module (which I have already installed). But it reported an err:  Undefined variable ""py"" or class ""py.vin.vin"".    Error in script_viz_policy (line 3)  tmp = py.vin.vin; clear tmp;  % to load Python    And I followed instruction of Matlab Online Help, used command py.importlib.import_module('vin') for test, and still reported an err:    Error using theano_utils>  (line 3)  Python Error: ImportError: No module named theano    Error in vin>  (line 3)  from theano_utils import *    Error in __init__>import_module (line 37)      __import__(name)    But I can import theano in python,    >>> import theano  >>> import theano.tensor as T  >>> theano.test()  Theano version 0.8.2  theano is installed in //anaconda/lib/python2.7/site-packages/theano  NumPy version 1.11.1  NumPy relaxed strides checking option: False  NumPy is installed in //anaconda/lib/python2.7/site-packages/numpy  Python version 2.7.12 |Anaconda 4.2.0 (x86_64)| (default, Jul  2 2016, 17:43:17) [GCC 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2336.11.00)]  nose version 1.3.7    I couldn't figure out where is the problem. Thank you in advance!  "
"Hi,  Could you release the code for web navigation experiments as well??   Thanks!"
"In your `conv2d_keep_shape` function which I copied below what is the purpose of the `image_shape` and `filter_shape` args?       The parameters exist when you call the function every time you want to do a convolution, but they aren't being used in anyway. For example:         Are they intended to be used somehow or is this a remnant of old code being refactored? I ask because I implemented this architecture in TensorFlow and am attempting to replicate the results."
"Is it also ok to release the code for Mars Navigation dataset, models and particularly the one for continuous control via Mujoco simulator. Did you use OpenAI Gym for it by the way ?    In addition, there are other continuous control task in OpenAI Gym, e.g. LunarLander, CarRacing, BipedalWalker, do you think we can try to use VIN on these tasks ? Perhaps even for some games in OpenAI Universe ?"
"It is  waste of time to learn 2 libraries especially damn MATLAB. I have not used MATLAB since i left University 6 years ago and never come across any company in real world using  MATLAB . In some die-hard colleges that don't want to let go still use MATLAB but majority have moved on to python .  There are Tensorflow , Theano and others that would accomplish the same task probably in easier way and help majority of people adopt your research "
"Hi! I want to do some experiments on larger domains such as 100*100 gridworld. But when I run the data generation file ""make_data_girdworld_nips.m"",a missing function called ""shortest_paths()"" causes code to fail.`[~, pred] = shortest_paths(G_inv,goal_s,options);  `  Could you please release the code of function `""shortest_paths()""?Thank you very much!  "
"I've encountered some issues in reproducing results from the VIN paper. I've run commands from the   file, but I was able to reproduce only results for the 8x8 grid world. I've trained 6 times network for 16x16 grid world problem, but the best results that I've gotten was with loss 0.1006 and accuracy: 95.7% (loss 0.11 and accuracy 95% on average). In addition, I've trained network for 28x28 grid world problem, but my loss after 120 epochs was 0.26 and accuracy 89.37%.    Is there anything that I can do to get the same (or similar) accuracy as in the paper?"
"When I run the program with the provided data, I met shape mismatch error. The command I used is:       The error is caused by taking out the q values (see  ).          I am not good at debugging Theano (much harder than TensorFlow). I guess it was caused by the `:` indexing. Could you please help on this?"
"I tried to produce the grid_world data wth the script of `script_make_data.m`. However, I obtained          I print out `dom` and check the variables of `n_obs` and `add_border_res`, found out that `dom` is always 1, and `add_border_res` makes `n_obs == 0 || add_border_res` true.    Could you please help on this? Thanks!"
"Hello Ed,    First of all, thank you for this amazing work. I was wondering if you could answer the following questions, it'd be really appreciated.    1. How do you think RETAIN handles with class-imbalance situation? I personally saw it handled well in my dataset, but couldn't follow why it worked great.    2. Patients can have common medical codes like hypertension or diabetes, which can be regarded as noise in terms of predicting other disease. Can RETAIN capture such codes with less weight even though they appear a lot?    Best Regards,"
"Hi Retain team,    I am interested in using retain for my research. I wanted to know how to prepare data in the presence of more features like vital signs or medication apart from diagnostic codes? Do I need to concatenate all the feature together? For example for a patient with diagnostic codes (c1 to c3) and vital signs (v1 to v3) , should the input be like [c1,c2,c3,v1,v2,v3] for a single visit? "
"Hello retain team,    Great job ! Thank you for sharing it.  Do you have explanation why you use 2 sets of attention weights (visits and variables) instead of only one for variables ?  With this set you can still get a visit contribution using aggregating method, average or sum of the variable weights of each visit for instance   Thanks in advance for your help"
"Hi,    My query is more like a question than an issue. I am a bit confused about the following foot note in your paper (image attached).    !     My query is that why feeding the sequence in forward order will generate the same e_1 and beta_1 as they depend on learnable parameters also ?    Regards,    Usama."
"Hi Edward,    This is more of a question on Retain paper and not on the actual code.  I have a usecase where RETAIN can be a good possible solution however, the problem is multi-class(5 classes) in nature.  I am interested to hear your initial thoughts on my approach -      if I implement attention mechanism in the same way explained in RETAIN paper but the model architecture is enhanced to predict multi-class as output (i.e. I will have one set of alpha, beta and context however,  5 nodes in output layer).     Do you expect attention mechanism and interpretation to still hold good with my above approach? Does having just one context vector for five classes make sense?    Thanks in advance!  -Priya"
"Hi, I've just started using this repo and got the following error,         The code thought seems to train fine using only,         "
"Hi Edward,    This is more of a question than an actual issue so I will close it immediately:     What do you think about the use of Bidirectional LSTM instead of just the reversed order? Do you think it impedes the interpretable spirit of the RETAIN or is it fine to use as long as it provides the boost in performance? I've been using concatenated Bidirectional lstm with moderate increase in AUC - I think due to concatenation of the outputs it is totally plausible that the network will select the order that provides more value but I am not entirely sure that this makes sense from clinical perspective.     Do you have any thoughts on this?    Thanks,  Tim"
"Hello Edward,    Thank you for sharing the code for the Retain paper! I have been looking through it and found that in attention_step function of build_model you've divided the GRUs' outputs by 2. What is the reasoning behind this operation? Relevant code:     "
"The usage file and another paper I found mentions the possibility of using the architecture for node classification. I wondered how the signal classification example differs from node classification with multi-feature nodes V=(number of nodes, node features) and A = (number of nodes, number of nodes), specifically:    1. For node classification I'd assume we need the final layer to output: (Batch size=1, Number of classes, Nodes) to get a prediction per class just set the last fully connected layer=number of classes, but how can we get a prediction for each node when the node pooling decreases number of nodes so the final layers number of nodes is V/(pool_size_1*pool_size_2).  2. The Notebook example expands input since features=1, so first pass to gconv is N, M, F=1, i.e. graph signals in a batch, number of nodes, and features=1. For this case would rank still be=2 since data only has number of nodes in batch and features, and should it be expanded? It doesn't seem like there should be any M if not dealing with graph signals.    Thank you for the great article!"
"Hi!    First of all thanks for creating such a nice reproducible example for your repo, you made your algorithm way more accessible.  I have a question regarding accessing the output of the final, fully connected layer in your architecture (I'm not a TF expert). After training the model with my own network and data, I'd like to see how the embeddings encoded in the final layer look like but I don't know where to start.    Can you point me in the right direction? :)    Best,  Piotr"
"I noticed line 139 in ""lib/utils.py"" should be modified  from `model = gensim.models.Word2Vec(Sentences(self.documents), size)`  to `model = gensim.models.Word2Vec(sentences=Sentences(self.documents), size=size)`  I guess the parameter list of the Word2Vec constructor changed..."
"Hi,   I am trying to reproduce the mnist results on images data of my own. They are simple parabolas (almost like the ones in the tutorial on jupyter notebook) and I am classifying them into 4 classes based on their curvature shape. I got good results using CNN with the same number of conv filters and FC layers as shown in the attached photo. Yet I am not sure on what to set for the polynomial orders in the parameters, as well as the coarsening level and number of edges in the arguments. I tried different values but my loss and accuracy remain the same in every epoch (attached as well). Any advice would be appreciated. thank you in advance !       !     !   "
I have an MNIST-like dataset for image classification using a Graph Convolutional Neural Network. Could you maybe suggest on how to make the adjacency matrix for such a dataset ?     It would be great. Please let me know. 
"I have been trying to run `usage.ipynb` notebook. Even if I fix the error in #46 as described in #46 , the line `accuracy, loss, t_step = model.fit(X_train, y_train, X_val, y_val)`  under  **3 Graph ConvNet** header gives the below error:       I changed the below line (  to fix index is out of bounds error from      `indices.extend(np.random.permutation(train_data.shape[0]))` to   `indices.extend(np.random.permutation(self.batch_size))`.     This gives the error below:        "
"At the following line,   you have the assertion `assert len(indices[0] == M)`, but this is always true, because `indices[0]` is not empty and `indices[0] == M` produces another list of the same size as `indices[0]`, so `len(indices[0] == M)` is always greater than zero."
"In the module  , you are calculating a variable `sigma2` as follows `sigma2 = np.mean(dist[:, -1])**2`. However, this is ""mu squared""  (not the variance) of the last column of the distance matrix. Why not simply using `dist.var()`?"
"First off, thanks for developing and sharing this interesting package. I've forked the repo and all test cases work fine. This is probably related to #14, but I've made a new issue because I have data.     I have an adjacency matrix from a large directed graph. The dimensions of the adjacency matrix are `(7919711, 7116242)` and the structure is extremely sparse, number of non-zero elements are `2732656`.     When I try to run the pooling on a subset `(10000x10000)` of my own data that   (5.07 KB file) I can produce errors with the flavour (ran on sparse_adj_subset)        And if I rerun, I get        showing that the index changes.     I am running with `graphs, perm = coarsening.coarsen(ajd_sparse_ss, levels=3, self_connections=False)` but setting `self_connections=True` gives similar problems. "
"When approximate the initial filters kernel gÎ¸(Î›) = diag(Î¸) with a polynomial   !   why we canâ€˜t simply combine the eigenvector and eigenvalue matrix together like  !   because   !  *  The paper instead, use Chebyshev polynomials and also said that   !   I'm quite confused here. I don't know why the Chebyshev is needed?  "
"Hello @mdeff ,    MNIST data are defined on 2D grid. Hence, we build graph on MNIST by supposing that each pixel is a node and the max number edges per node is 8. Hence, we have  a regular and fixed graphs.    The Fourier basis is obtained by computing the Laplacian of  the graph. Since the graphs are regulars, you pick at random a graph encoding an exemple of MNIST and compute its laplacian. This latter is used as the Fourier basis of MNIST.    1) Why we don't compute the laplacian of each training exemple ?  2) How do you explain that taking any Laplacian  of MNIST example represents the Fourier Basis of the whole data ? Is there any effect on the stability of the Fourier Basis ? Does it  apply also to irrigular graphs ?    Thank you for your answer.    "
"Hi,  I'm wondering if there exists or if your are planning to implement any version of your work in Keras.    Thank you  Andrea"
"Hello @mdeff     Let me thank you for this notebook showing different graph convolution implementation.         l'm wondering if you have an optimized implementation of a convolution in a full spectral domain filter (instead of chebyshev expansion) ?    `x âˆ— G g = U.T ( diag(w_g)Ux) ` (link : page3    such that :   `x `: input  spectral multipliers` w_g = ( w_1 ,...,w_N )`  `U `: eigenvectors  `U.T `: transposed eigenvectors     Here is what l've tried                        Please correct me.  Thank you,  "
"## Hi there! I've tried for a while to run your code, but still have some flag problems, so here what I get:  **_this is what I get if I just 'make' in nips2016:_**         _**I tried to solve it myself and what I found was, that tensorflow uses now absl for flags, so I tried to do this:**_        **_instead of_**       **_but still get another error_**         **_So, maybe I just overlook something, but still stuck in it._**"
"I appreciate your implementation. In the introduction of usage, an example of data matrix X=[x1,...,xn]^T is used. xi is a vector containing signals on all nodes in the graph. The signal on each node is a scalar. My question is if the signal on each node is a vector i.e., xi is a matrix, will the original implementation work? Thank you!"
It seems that it's doing signal classification / regression in the notebook 'usage' . What change should I make to let it do node classification / regression? Only change the matrix A?
"Hello MichaÃ«l,    My colleague told me when I use the `graph.laplacian` function, I have to assign 0.0 to the diagonal elements of the `L` matrix on the fake nodes. The current function will return 1.0 on those elements.     My colleague said on fake nodes   weights `W` = 0.0 so `d` will be 0.0.   `d+=np.spacing(0.0)` will have a value very close to 0.0 (e.g. 4.9e-324).  `d = 1 / np.sqrt(d)` will have a large value (e.g. 4.5e+161).  `D*W*D` will be 0.0 (b/c 0.0 times a large number (but not infinity) is 0.0).  `L=I-D*W*D` will be 1.0.    The original Laplacian definition `L=D-W ` tell us the original `L` will have 0.0 on fake nodes (b/c D and W both are 0.0 on them). Normalizing (squashing) the original `L` by `D` shouldn't change the value 0.0 to 1.0.    Is my colleague right on this?    Thanks.  Maosi Chen"
"Thanks you for share this implementation.    Suppose each data has different graph structure...(same node number but different edges and edge weights)    In this case... can I feed different graph for each data? this implementation looks like that graph structure is fixed (grid or random...) for datasets    Thanks.    regards, Sangyun Lee"
I have the same problem as #36 because my graph is disconnected. Was wondering how to change the code to make this work for disconnected graphs?
"!       In the paper, the theta is said to be a vector of polynomial coefficients.  Hence, I understood that theta_k should be a constant.    However, it seems that theta_k is a matrix in complemented codes.  Is there any reason to complement in this way?  Or theta should be revised to matrices in R^KxMxN?"
"_Hi!  I encountered this issue while I was trying to feed my data._    **My input shape:**    -   - X_train(967680,64)  - train_labels (967680,1)  - X_test(1075020,64)  - test_labels(1075020,1)      **Paraneters**  params['batch_size']     = 1024         python 3.6  tensorflow 1.13.1"
"Hi,  I'm trying to obtain results for image classification on CIFAR-10. However much I change the model parameters, it seems to give a random classification output of ~10% test accuracy. By any chance, have you run any experiments on larger (and more complex) image datasets? Thanks in advance."
"I have been trying to run `usage.ipynb` notebook. Second cell of the notebook gives `name 't' is not defined` error at line `t = np.tanh(t)`. I guess it should be `t = np.tanh(w)`, Is that right ?"
None
"I'm just curious. This is coming from someone with a naive understanding of your methodology. I was reading through the STGCN paper, and came across your method. Seems like parameter reduction and filter localization is done through restriction of the kernel to a polynomial.    Is it possible, in anyway, to replace this filter with, and again I am a bit naive here, with a neural ordinary differential equation? That is, representing the filter as a diffeq rather than a polynomial and learn those set of diffeq parameters? Would such parameters help reduce the complexity of the model?    This is a point of curiosity, just looking for enlightenment. If this is possibly, than it seems the neural ODE framework could be extended to GCN."
"Hi, thanks for opensource!     I found that in your MNIST example, spatial information (or node ordering), is used implicitly. This is due to that a set of nodes are treated as a list and fed into fully contented layer (in  , [N, M, F] reshaped into [N, M*F]). This seems to implicitly use something more than just the grid graph, as for graph this set of nodes should be treated in a permutation-invariant way. A consequence seems to be that during test time, if the input is rotated 90 degree, the model will fail since data are not truly treated as graph.    Could you please kindly confirm this?"
"Hi,    First of all, Thanks a lot for sharing this package.    I notice that the current package only a single GPU during model training. Is it possible to distribute the training process to multi-gpu, which will accelerate the training process?    Thanks again.  Best,     Yu."
   l would like to know if only :    ` y = tf.nn.relu(y)`    which concerned by backpropagation or also :                 ?      Thank you a lot for your answer.
"Hi,    I was wondering if there is a code modification where the model is used to train for regression rather than classification. I assume that the loss function would be changed, as well as the last layer of the model.    Thank you"
"hi, modeff  Many thanks for you work.   I have a dataset of 3d pointCloud, can I apply this model to 3d object/face recognition or shape dense correspondence?"
None
None
"Hello,    Let me first thank you for your work.    l would like to ask you whether your  `graph_conv_cheby()` can be applied to an arbitrary graph  such as an `irregular graph`  or just on a grid graph ?    Thank you "
None
None
"I am attempting to classify whole correlational graphs using this code (in particular, adapting code from the `usage.ipynb` file. A previous inquiry about this was closed by the person who asked the question, and another time this came up, it was recommended that the user edit the `base_model` and `cgcnn` classes in `lib/models.py`. This is also referred to as ""whole graph classification"" in `usage.ipynb`.    How would we edit `lib/models.py` to accommodate whole graph classification?"
"common['p']  cannot be larger than 1, otherwise, InternalError: cudnn PoolBackward launch failed    "
"Thank you so much for sharing the idea and code.  I have tried to use the code for my own dataset.  but I got this error and I do not know what the problem is.      , line 131, in        accuracy, loss, t_step = model.fit(X_train, y_train, X_val, y_val)  ,  line 117, in fit      string, accuracy, f1, loss = self.evaluate(val_data, val_labels, sess)  , line 73, in evaluate      predictions, loss = self.predict(data, labels, sess)  ,line 44, in predict      batch_labels[:end-begin] = labels[begin:end]  ValueError: could not broadcast input array from shape (100,1) into shape (100)    Which 100 here is the number of batches.  "
"#Thank you for sharing your code. I really enjoy it (as well as your paper)!    For the bias + relu process (e.g. `b1relu`), I'm concerned about the effects of bias on those padded vertices (fake nodes) especially when there are many consecutive ones. For any layer before the last (cnn_graph)  layer, since next layer's L is padded with 0 on the corresponding positions the effects of bias may be eliminated to some extend (`x0` in `chebyshev5` is still affected). However, for the last cnn_graph layer those bias on padded vertices will be kept and fed into the fully connected layer.    My question is: should we mask those padded vertices (i.e. set them to 0) after adding the bias to the output of convolution and before relu?    Thanks!  Maosi Chen      "
"comment  in the rcv1.ipynb said that:  ""This left us with 50 classes and 402,738 documents. We divided the documents into equal-sized training and test sets randomly. Each document was represented using the 2000 most frequent non-stopwords in the dataset.""    but I find that  dataset.data_info() :  N = 420065 documents, M = 47236 words, sparsity=0.0000%    According to the comment in the script(rcv1.ipynb), N should be 402738 and M should be 2000. but in fact it is not match the description    Is there any wrong in  rcv1.ipynb ? You have a rcv1 code here, but you don't  mention rcv1 dataset in the paper , why?"
None
"I've forked the repo, and am attempting to run through the 'usage.ipynb' jupyter notebook, replacing the random data, targets, and graph with some real data, targets, and graph I have. Doing so Makes the following snippet:     Occasionally error out with a 'index 563 is out of bounds for axis 0 of size 563' error. This happens sporadically for the same inputs."
"Can I use graph CNN on multiple graph?  for example, for images, I model each image as a graph, other than a node of graph. So In the training phase, actually I am training multiple graphs. "
"The error messages are:      A = graph.adjacency(dist, idx).astype(np.float32)    File ""lib/graph.py"", line 74, in adjacency      W.setdiag(0)    File ""/usr/lib/python2.7/dist-packages/scipy/sparse/base.py"", line 684, in setdiag      max_index = min(M, N-k, len(values))  TypeError: object of type 'int' has no len()      When I execute the code  from the usage.ipynb, the above error message promptedã€‚  my python version is 2.7.6"
I have a few libraries that require python2 so having an __init__.py in the lib folder would be really nice so that I can automatically import the library without additional changes.
"Hi, MichaÃ«l    Thanks for the great work and sharing the codes.     I have node classification problem in a graph where the link is built upon additional information, not on features. For example, in a social network, each node is a person who has a set of features, and their connections are created by their social interaction. I want to classify the node with different social roles. In this case, the graph is not built with the features as you proposed in the paper. I am thinking to use your framework. One approach is to pass the features of all nodes through the Laplacian graph like in this approach:   However, this is not going to scale up, since I have to use all data points to do the computation.  I have trouble to figure out how to do this in batch fashion. Do you have any recommendation or suggestions?    Thank you very much!"
"Hi,  I have read the paper  "" Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering"" and the usage notebook for figuring out the use of this models.  But I find that the tasks in your paper and the usage notebook are all aiming for classification.  Dose this project provide an interface for regression task?  Or do I have to write this interface by myself?    Thanks.  "
As the title says.
"Hi,    Great project, Is there any plan to generalize the code to AutoEncoders ? especially implementing deconvolution in fourrier domain .."
"Hi, i am experimenting some of my own data, and I think it is relevant to your description about ""node regression"" setting, but I am not so sure.  So as I understand, (correct me if I am wrong.), take mnist experiment as example,  Each sample feed into the network is a digit image, conveted to a graph with 28x28 nodes. The goal is to classify this **entire graph** to a label, say, 8.  What I am looking for is that, I have my own data which is a bigger graph, and there is a value or a vector of values on each node. And i am thinking if it's possible to: 1.  given a node, only feed in it's neighbors into network 2. the corresponding output of this node input is it's associate value.    I think it is more like I want to use a subgraph as training....  I am not so sure if this is the same as you mentioned in the usage.ipynb node classification/regression.. Do you have any hint or suggestion of how to do that?  Thx in advance~! "
"Hi, I am trying to run the sample code on mnist dataset. (i.e. in the python notebook ""mnist"")  I have encounter an issue when it runs the following line of code: `L, perm = coarsen(A, FLAGS.coarsening_levels)` The error is : `IndexError: index 783 is out of bounds for axis 0 with size 783` at line 119 in function ""mestis_one_level"" in coarsening.py...  I am wondering if you encounter the same thing?  thx! "
"I was trying to run the mnist.ipynb notebook but encountered the following error:  ---  > IndexError                                Traceback (most recent call last) >   in  () >      44 A = grid_graph(28, corners=False) >      45 A = graph.replace_random_edges(A, 0) > ---> 46 L, perm = coarsen(A, FLAGS.coarsening_levels) >      47 print('Execution time: {:.2f}s'.format(time.process_time() - t_start)) >      48 graph.plot_spectrum(L) >  >   in coarsen(A, levels) >      17 >      18 def coarsen(A, levels): > ---> 19     graphs, parents = coarsening.metis(A, levels) >      20     perms = coarsening.compute_perm(parents) >      21 >  > /home/nils/scripts/graph_cnn/cnn_graph-master/coarsening.py in metis(W, levels, rid) >      51         vv = val >      52 > ---> 53         cluster_id = metis_one_level(cc,rr,vv,rid,weights) # cc is ordered >      54 >      55         parents.append(cluster_id) >  > /home/nils/scripts/graph_cnn/cnn_graph-master/coarsening.py in metis_one_level(rr, cc, vv, rid, weights) >     118             for jj in range(rowlength[tid]): >     119                 nid = cc[rs+jj] > --> 120                 if marked[nid]: >     121                     tval = 0.0 >     122                 else: >  > IndexError: index 783 is out of bounds for axis 0 with size 783  ---  When I was trying to find the error, I noticed that something with the assignment of the row and column indices in the metis() function seems strange since the row indices are assigned to the variable ""cc"" and the column indices are assigned to the variable ""rr"". When the indices are passed to the function metis_one_level(), I noticed that metis_one_level() takes the variables in the following order: metis_one_level(rr,cc,vv,rid,weights). So here it seems as if the rows and columns are in the wrong order. In any way, when I change that, the mnist script runs smoothly without the index error. Maybe you know more and could shed some light on that since I don't know if changing rows and columns is a good idea.  ---     ---  Additionally, I've tried to insert my own data which is a correlation matrix (which basically is an adjacency matrix) but am not exactly sure at which point in the code I should do that. My idea was to build the grid graph, just changing the size of it from 28 to 160 and then continue with the script. Of course I would exchange the MNIST data with mine. Does that make sense? Do I need to build the grid even if I already have an adjacency matrix? Thanks a lot for your help! "
"When trying to run one of your jupyter notebooks on GPU (`mnist.ipynb` for example), I get the following error when the tensorflow session is started in your model fitting routine:     Have you experienced this kind of problem? I think this might be due to importing/running other jupyter notebooks from within the notebook using magic commands like `%run -n models.ipynb`. "
"Hi Matthieu,    thanks for your BinaryConnect paper and implementation of it here, which is really inspiring and helpful. I got a concern about updating the trained weights. Empirically, you know, for each backpropagation, parameter changes after gradient descent are tiny, which is also illustrated in your paper.  with these tiny changes on weights, after binarization of each epoch,  it is possible that these binarized weights remain unchanged, for example, it is hard to change from 1 to -1 due to the tiny changes on weights.  And based on that, after the forward pass, each epoch of result may be similar, which in turn result in more tiny changes on weights. Hince, after several epochs of training, the weights are hardly updated and it is still far away from optimization.  How do you solve this issue?    another question is:    !   In your figure, after training, the distribution of weights are around -1 and 1. I don't know why but my training weights seems like a little bit random? do you know why?    thanks.  "
"Does anyone know the version of python,theano and so on?Thank you !"
"I am trying to implement BinaryConnect for Tensorflow, I have been unsuccessful thus far. Will the authors of the paper extend support for BinaryConnect to Tensorflow anytime soon?"
"Building the CNN... /home/eli/anaconda2/lib/python2.7/site-packages/lasagne/layers/conv.py:489: UserWarning: The `image_shape` keyword argument to `tensor.nnet.conv2d` is deprecated, it has been renamed to `input_shape`.   border_mode=border_mode) Traceback (most recent call last):   File ""cifar10.py"", line 301, in       W_grads = binary_connect.compute_grads(loss,cnn)   File ""/home/eli/work/b2/BinaryConnect/binary_connect.py"", line 163, in compute_grads     grads.append(theano.grad(loss, wrt=layer.Wb)) AttributeError: 'Conv2DLayer' object has no attribute 'Wb' "
"Hello Sir !  I am a student from a university in China and I am trying to reproduce your experimental results on Windows10   with the newest version of theano. I came with several errors and solved them. Maybe there are several changes which should be made  to run BinaryConnect with the newest version of theano ?  [1] As the newest version of theano changed  ""signal.downsample.max_pool_2d"", line 415 in layer.py should be changed to   ""z = T.signal.pool.pool_2d(input=z, ds=self.pool_shape, st=self.pool_stride, mode='max')""  ...I hope I am right...  [2]I came with a problem between ""float32"" and ""float64"" and this finally worked  Chnage line 272 to 276 in trainer.py to:          self.shared_x.set_value(float32(set.X[start:(size+start)]))          self.shared_y.set_value(float32(set.y[start:(size+start)]))  Maybe adding float32 to it can me the program stronger?  "
"Hi,    I am reading you code and try to understand any single piece. For backward, I got confused on calculating the gradient respect to binary weight. What is the definition of gradient to the discrete value? And also, does theano do this automaticly?    Best regards,    Tong "
See main issue here -->      I am having a hard time generating the .h5 datasets needed to run the SVHN example in Binary Connect. Has anyone tried to do this recently that can guide me to success?    'h5/splitted_train_32x32.h5'  'h5/valid_32x32.h5'  'h5/test_32x32.h5'
Do you will include code also for   
Error: no module named 'cle.cle'  But I have installed the module 'cle'.How to solve it?
"Hi, read-me file says, ""The original wave files have been read by numpy and saved into '.npz' format. There is a function that reads the numpy formatted files and generate a hdf5 format file."" I can't find the script which converts to .npz formatted files, or to hdf5.     I also wanted to know as to what do you mean by ""full_blizzard"" and ""blizzard_tbptt"" while reading the blizzard data in the file ""blizzard.py"". "
"Hi,    I started the training the iamondb script and wanted to know if there is a test script where I could test the output of the model?    I do not see a test script in the package. It would be a great help if you could point me to a script where I could test the code.    Regards,  Vivek"
"> Entering main loop > Traceback (most recent call last): >   File ""vrnn_gauss.py"", line 371, in   >     main(params) >   File ""vrnn_gauss.py"", line 349, in main >     mainloop.run() >   File ""cle/cle/train/**init**.py"", line 83, in run >     while self.run_epoch(): >   File ""cle/cle/train/**init**.py"", line 89, in run_epoch >     for batch in self.data: >   File ""cle/cle/data/**init**.py"", line 90, in **iter** >     yield self.data.slices(idx, idx + self.batch_size) >   File ""../../../nips2015_vrnn/datasets/iamondb.py"", line 97, in slices >     mask = self.create_mask(batches[0].swapaxes(0, 1)) > ValueError: bad axis2 argument to swapaxes  Hi, I run the model of iamondb ,and got the eroor message , how can I fix it ? thx ! "
I'm trying to train your Blizzard 2013 models and it looks like there are some preprocessing steps to go from the Blizzard dataset dumps to the HD5 and .npy files read by the models.  Could you let me know what needs to be done to reproduce? Many thanks! 
We can get it from Kratarth's repo - unless you forgot to include the files and have them locally 
Im trying to run the IAMOnDB example.   I get an error when i run fetch_iamondb(...)     I downloaded and unpacked 1. ascii-all.tar.gz 2. lineImages-all.tar.gz 3. lineStrokes-all.tar.gz 4. original-xml-all.tar.gz 5. original-xml-part.tar.gz  Can you point me in the right direction?   I also had some trouble figuring out from which folder i should execute the files such that CLE is properly imported? 
Trying to download blizzard 2013 datasets. Link down?  www.synsig.org/index.php/Blizzard_Challenge_2013
"Hello, has TensorNet accelerated during training? Hope you can share the results of the training speed, thank you"
"Hi,  I am new to matlab. When I tried to run vl_test_ttlayers, I got the following error:          Can somebody help me with above error ?"
"Hello sir,  Have you developed tensornet for CIFAR-10 dataset?  I am following your libraries for developing them. If you have implemented it would be nice for me to get some idea about it."
"Hello,  I've been having a bit of trouble getting a simple example up and running using pre-annotated data. After following the tutorial in the README, I'm getting a NullPointerException:     My code is  .  Any help would be greatly appreciated. "
"envy@ub1404:~/os_pri/github/imageqa-public/src$ GNUMPY_USE_GPU=yespython python train.py -model ../models/2_vis_blstm.model.yml -output ../results -data ../data/cocoqa -config ../config/train.yml Initializing weights for imgFeat Reading ../data/hidden_oxford_mscoco.h5 (123287, 4096) Epoch size: 70854 Batch size: 100 Step size: 709 Examples per step: 70854 Steps per epoch: 1 Trainer 2_vis_blstm-v-20160323-095803 .Traceback (most recent call last):   File ""train.py"", line 307, in       validInputWeights)   File ""train.py"", line 156, in trainValid     validInputWeights=validInputWeights)   File ""/home/envy/os_pri/github/imageqa-public/src/nn/trainer.py"", line 264, in train     self.model.backward(dEdY)   File ""/home/envy/os_pri/github/imageqa-public/src/nn/container.py"", line 130, in backward     self.stages[s].graphBackward()   File ""/home/envy/os_pri/github/imageqa-public/src/nn/stage.py"", line 144, in graphBackward     dEdX = self.backward(self.dEdY)   File ""/home/envy/os_pri/github/imageqa-public/src/nn/lut.py"", line 96, in backward     if X[n] != 0: ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all() envy@ub1404:~/os_pri/github/imageqa-public/src$  "
"     Since dists is transformed once, there is no need apply sqrt on the median.  If X and Y in example.m is changed by multiplying 1000, I can see the outcome of tests is changed.    Should the second line be         Once I change the second line, the test becomes immune to the scale of X or Y."
Hey there    There seems to be a bug on example code `mondrianforest_demo.py` with `PLOT=True` :          `pred_forest_test` only has one key `'pred_prob'`. The code tries to access both `pred_mean` and `pred_var`.    Manuel
"Could you provide a class that implements fit and predict methods, like in scikit-learn?  We met at MLSS Kyoto two years ago ;-) "
"Great project, have been looking at possibilities to modify this code to support streaming learning on the fly. One question I have has to do with what happens to training examples from previous iterations:  It seems that after _mf.fit()_ is run and _mf.partial_fit()_ is called, the nodes are still holding references (indicies) to previous training samples:  _self.train_ids[node_id] = np.append(self.train_ids[node_id], train_ids_new)_  in the method _add_training_points_to_node_  When I try to replace (rather than concatenate) _data[""x_train""]_ with new training samples, I get an error in _get_data_min_max(data, train_ids)_ which is caused by _train_ids_ being empty. "
Is the bagging capability functional? I've run the code with settings.bagging=1 and settings.bagging=0 and I get the same answer both times. So I was wondering if there is a problem with the bagging? 
"For leaf nodes, `max_split_costs` is large, which often leads to `discount = 0` if `discount_parameter` is large (>10). Due to this, predictive distributions at the leaves tend not to be smooth.  For example, the following are the predictive distributions at the leaves when testing with the provided toy dataset and default parameters (for a single tree):     Is this the intended behaviour? "
"I ran the demo code `./mondrianforest.py --draw_mondrian 1 --save 1 --n_mondrians 10 --dataset toy-mf --store_every 1 --n_mini 6 --tag demo` and it terminated on:     A `mkdir` fixed that. Maybe add the folder to the repository? (With a dummy file in, since git doesn't understand empty folders.) Or create the folder from python. "
"Hi, nice to see the code published! I tried running the demo command `./mondrianforest.py --draw_mondrian 1 --save 1 --n_mondrians 10 --dataset toy-mf --store_every 1 --n_mini 6 --tag demo` on my ubuntu machine and got:     That's the only dependency missing from your readme, for an ubuntu machine, I think. If I install the dvipng package then it works. "
"Hi Fabien,     I want to use the mask-rcnn predictions for EPIC-Kitchen dataset. Can you elaborate how to interpret the pickle files provided as Complementary data-masks. For beginning, I would like to know a way to overlay mask and bounding boxes on the corresponding frames. Any tutorial or readme for doing so would be highly appreciated    Thanks,  Nirat"
"here have something questions.  1ã€I want to use this project on ""Something-something"", but i haven't this mask data.  2ã€The dataset of ""something-something"" with 174 classes in 20bn. but I find only 157 classes in your paper. so I don't know that whether ""SS"" in 20bn ."
"Hi Fabien,  thank you for your works!  the paper doesn't mention how to train a mask-r-cnn since VLOG dataset does has any mask annotation, i wonder if your team has annotate the mask by yourself or use other open datasets to train mask-r-cnn?   thanks again!"
"Hello,I run your project,but i can only get maskRCNN__png. I can not get other that you show your result in your article.I want to know how i need to do."
"Thank you for releasing this repo. Would it be possible to use this model to do a demo on my own video? What would the steps to do so be?    Thanks,"
"Hi Fabien,    May I ask for the average training and inference time of an iteration with batch size 8, and what GPU did you use?    Thank you very much!"
"Hi Fabien,    Thanks so much for putting up the code! May I ask if you could also provide the loader file for EPIC kitchen please?"
about the performance variation
"Hello,    Could you provide ResNet-18 pretrained weights with GroupNorm?"
"using the pretrained models, how can I run detection on any image?    **_python imagenet-resnet-gn.py --data C:\yolo\resnet\GroupNorm-reproduce\ImageNet-ResNet-TensorFlow\image\ -d 101 --eval --load ImageNet-R101-GroupNorm32-AlignPadding.npz_**    In the image folder I have .png files on which I want to perform detection. But it keeps on giving the error of     _File ""imagenet-resnet-gn.py"", line 128, in        ds = get_imagenet_dataflow(args.data, 'val', batch)    File ""C:\yolo\resnet\GroupNorm-reproduce\ImageNet-ResNet-TensorFlow\imagenet_utils.py"", line 95, in get_imagenet_dataflow      ds = dataset.ILSVRC12Files(datadir, name, shuffle=False)    File ""C:\Users\paul90\AppData\Local\Continuum\anaconda3\envs\yolov3\lib\site-packages\tensorpack\dataflow\dataset\ilsvrc.py"", line 164, in __init__      assert os.path.isfile(fname), fname  AssertionError: C:\yolo\resnet\GroupNorm-reproduce\ImageNet-ResNet-TensorFlow\image\val\ILSVRC2012_val_00000001.JPEG_    can anyone please tell me how to run the detection on any image?"
"Hi,    Is it possible or did you try training the resnet101 on ImageNet? Could you release the pretrained weights?    Thx"
"nn.moments returns variance instead of std.    Best,  Huiyu"
Would it be possible for you to share pretrained weights for resnet GN? Transferring directly from the pytorch BN weights gives very bad accuracy.
None
With pytorch1.1 & cuda9.0 & python3
"Thx for the excellent work for the community! I have two confusions hoping to be answered:  1. When reading the paper, I found the 2 stream network released in NIPS was used. But the TSN is used, when I read the code here. So what do we use?  2. I found that the process is different between THUMOS14 and ActivityNet in  . Could you sent me the code on THUMOS14 please? My email is  rainbowdream1991@gmail.com"
"Hello, Mr. Lin, very thanks, the code is useful for me.  Sorry to be a bother.  When I try to run it, the TEM testing loss is almost equal from epoch 1 to 20, the module learned nothing.  What maybe cause this question? Is there any advises?  Looking forward to your reply, thank you very much!  "
"Hi,  Thanks for the code! I just wanted to ask if BSN(Boundary sensitive network) as well as BMN(Boundary matching network) can be applied for real time action detection in videos?"
"Hi Tianwei @wzmsltw     Thanks for your awesome work! I'm also trying to run BSN with some customized video data. However, I'm confused with a part of your code: it'  . I also noticed that you mentioned 16 in your paper in multiple places and I have no idea about the 16*16 in your code means.    Would you mind helping me with this question?    Thanks!"
"Great work! Thank you for sharing!    This work is for generating action proposals.   You mentioned in your paper, for temporal action detection, on activityNet-1.3, you adopt top-1 video-level classification results generated by method of [Zhao, Y., et al., CUHK & ETHZ & SIAT Submission to ActivityNet Challenge 2017], and use confidence scores of our proposals for detection results retrieving. I did not get how you do it. Could you please explain my following questions:     1. In the paper of Zhao, Y., et al., CUHK & ETHZ & SIAT Submission to ActivityNet Challenge 2017, it mentioned different methods for different tasks. Which method is adopted by you? Is it SSN(Structure Segment Networks)?    2. BSN will generate at least 100 proposals. Will you chose all proposals for action classification?    3. Assume you only choose top-k (such as k = 2, 3, 4, 5) proposals output from BSN for action classification. For each selected proposal generated from BSN, you do video-level action classification. Does it mean for a proposal ""started on frame m, and ended on frame n"", you will generate only one action label?     4. I wonder if it is possible to release your detection demo. That will be awesome!   "
"@cinjon how is your progress to try this code on THUMOS?     If I understand this correctly I have to extract the snippet level features using TSN (  But the anet2016-cuhk is pretrained on activity net so you first have to finetune the network on THUMOS and then extract the snippet level features from THUMOS and do the TEM, PGM & finally the PEM training? Is this correct?    _Originally posted by @tobiascz in  "
Hiï¼Œthx for the brilliant workï¼  I encounter a problem for using sliding window method in THUMOS14 dataset and I notice your reply in  . I would be appreciate if you can send me codes of BSN in THUMOS14? My email address is rainbowdream1991@gmail.com. Thx again!
"Hi, do you have the results for Table 2 (  for ActivityNet? I am speaking of the @50, @100, ..., @1000 proposal recall numbers. I only see those reported for Thumos. Thanks."
None
"Great job for action proposal.            1, By reading your paper, i am wondering whether conv2d is better than conv1d in TEM module for action score regression?           2, Can overlaping of slide windows in TEM feature sequence give us better scores?    looking forward for your reply!"
"Training on Thumos seems to go extremely quickly given the data loader that you sent out. There appears to be only ~2500 examples in the resulting dataset, which only takes ~3-5 minutes to train to 20 epochs. Is this correct?"
"Here are what mine look like using the provided code for Thumos dataset, but converted to Pytorch. I am trying to debug this because I don't think these are correct given the poor test results.     Training curve. The line to the right is the same as the line to the left, except using 4 gpus and a corresponding 4x batch size and 4x learning rate.  !     Test curve. Note how this never goes down; it only shows overfitting to Train.  !   "
"In this repo, it is .01. In the tensorflow rep, it is .1. Which (of either) did you actually use?"
"When training, at approximately what level of loss for each of the parts does the model start to become reasonable? And what kind of curves should we expect?"
"The weight_init code in models.py TEM is:         However, there are no nn.Conv2d modules in TEM, so this is never triggered. What is this supposed to be?"
"My video has different scenes.    First, I want to cut my video to different parts , then i want to do scene classification on different parts of video . Can I use BSN to finish my first step ?"
"Hi there, I am trying to train this on another dataset and am getting a bit stuck trying to figure out how exactly you extracted the features using TSN.    I am using the mmaction repository (which is what the authors of the TSN library you suggest ... suggest using) and the approach in that repository is to oversample by first computing the crops and flips and then run that through the model.     I noticed in #3 that you said you don't remember if you used oversample or not. Has that changed by any chance? It would save me a lot of time if you can remember that.    Also, I noticed that the size of the features in the provided CSV were each of size 400. That seems small given that the TSN outputs features of size 1024 out of the box. Is there some other setting you used to get size 400 features?    Thanks for your help."
"As far as I can tell, the training procedure with this repo is to first train the TEM, then generate the PGMs, then use those to train the PEM. Do you then repeat or do you do it just once?"
"Hi there, thanks for releasing your code. I've went through it with the intention of adding a new dataset and, as far as I can tell, the main thing that needs to be done is to generate the video_anno file, which is a large json consisting of:    >  : {'duration_second', 'duration_frame', 'annotations', 'feature_frame'}.    I understand that the annotations field is meant to be a list of {'label':  , 'segment': [start, end]}, but can you verify what the other three are meant to be? It's not clear if duration_second is according to a normalized FPS or if it's just the timestamp in the video. It's also unclear what the difference is between duration_frame and feature_frame.    In what units is the start and end of segment, i.e. is it relative to the actual time in the video or a normalized time?    Additionally, I will not be modifying the video to be 100 frames each. It seems like you did that for ActivityNet but the paper doesn't mention anything similar for Thumos. What was your strategy for Thumos?    Finally, what's the story with video_df in _get_base_data? It seems like it loads the full data in every time. That's 11G uncompressed. Is this right?"
"Hello, in models.py line 28 (   you used initialization in conv2d. But here it is conv1d. Also in PEM part the same issue.    "
"Is there a way (or let's say a snippet) of getting predictions for a custom video?    Example signature:  `start_points, end_points = BSN(path_to_dir_with_video_frames)`"
Thanks for your impressive work. I note that the feature extracted by TSN is 3072-dim but your default input dimension is 400. Have you used PCA for dimensionality reduction?
"In the opts.py, the 'pem_low_iou_thres' default value is 2.2. I suppose this is a small mistake. Do you mean 0.2 for this?     ####code#######  parser.add_argument(          '--pem_low_iou_thres',          type=float,          default=2.2)  ###############    Thanks."
None
"When I train `PEM` module, an error occurs as followed.  But the first time I trained this module around a week ago, it works well.  Could someone solve this??    ------     "
It works well until run this step.     Here's the output result:     
"Thank you for your work!   When I used TSN to extract features, I found oversample was used in the code, which results in the output of 'fc-action' being 10*200 (input being an image).  So I have to ask the following questions.  1, Whether oversample is used for feature extraction? If so, is the average taken at the end?  2, Or center-crop(w: 224 h: 224 c: 3) is adopted when extracting features in an image(340 256 3) or flow stack(340 256 10)?"
"Hi,  I want to know if the evaluation program is the same on the THUMOS14 dataset?"
"Hi TianWei,    How many optical flow do you stack in a snippet(as described in your paper) s around center frame. Is it related to the length of snippet(delta)? "
"Have you tested the results of the last two categories. How accurate is it.  In other words, how accurate is your model to predict whether two image blocks are correctly sampling the same image or different images."
"Hello, I read your code and find that it seems like there's no EXIF-consistency function provided.  When I tried to use all the EXIF tags in the run_vote_no_threads() , then it shows that the CustomRunner object has no attribute ""tags"".     Besides,  in the demo code if you set dense=True you actually will not run the dense method."
"when i try to run the demo.py file i am getting the following error in windows.      ForkingPickler(file, protocol).dump(obj)  TypeError: can't pickle _thread.RLock objects"
"For version later than tensorflow 2.0,the service for tensorflow.config has been stopped .That means tensorflow.config cannot be use.  Then how can I deal with the program with Python 3 and tensorflow 2.0?It's a really hard thing for undergraduate.  PLEASE HELP!!!  å¯¹äºŽtensorflow 2.0ä¹‹åŽçš„ç‰ˆæœ¬ï¼Œtensorflow.configæœåŠ¡å·²åœæ­¢ï¼Œè¿™æ„å‘³ç€tensorflow.configæ— æ³•ä½¿ç”¨ã€‚  ç„¶åŽæˆ‘è¯¥å¦‚ä½•ä½¿ç”¨Python 3å’Œtensorflow 2.0å¤„ç†è¯¥ç¨‹åº?  è¯·å¸®å¿™ï¼ï¼ï¼  "
"Thanks your works for every researchers.    the detection result (Cluster w/ MeanShift) of demo.py is same as your showing,  but i get a wrong result (Segment with NCusts) when i run ncuts_demo.py.  the error message is 'Intel MKL ERROR: Parameter 6 was incorrect on entry to DLASWP'.  i tried to find answer for this question, but i failed.  could you give me some solving suggestions?   thanks again.  @minyoungg @andrewowens @Eschew @AndrewHLiu "
I think  it is a great work !Can you share your train code and dataset?  Thank you very much!
"win10, py3.5  error:      AttributeError: Can't pickle local object 'EfficientBenchmark.update_queue_runner. .new_cr'"
Thanks for posting your code! The work seems very interesting. The google drive link (   ) for the pretrained model seems to be broken. Could it be fixed? 
"Hello, I am trying to load the PASCAL3D+ dataset with the tools provided in this repository but the link to Google Drive is not found :( is there any chance that this could be restored?"
"Hi,    Great work and nice meeting you at ECCV.  Looking at the infinite_mixture model in the repo, _mc_loss can easily be used also for the single mode von mises model - kappa is predicted by the basis network, correct?    Also, to model an unbounded regression task, like size or location, one can simply replace the von Mises distribution by a Normal distribution and use the same pipeline, correct?    Cheers,  Francesco"
"Hi @mzolfaghari ,  Thanks very much for this great work. It is not like an issue but just a general question.     Could you please suggest if there is any replacement option for 3D convolution (could I use depthwise or group convolution by manipulating them?    I look forward to hearing from you. Thanks."
"Thanks for your work!  i test your code in PyTorch, but in fact, inference speed is always 10x slower than what you reported in your paper when using the same machine and setting   for example  eco lite | SampledFrames |    Tian X | yours (videos per second)                 4                           15.2     (142.9)                16                          4.1       (35.3)    please, could you provide some information about your inference speed in  PyTorch? beyond that, what do you think are the causes?  Looking forward to your reply @mzolfaghari "
"The presented method setting the number of segments seems not right.     layer { name: ""reshape_fc_st2"" type: ""Reshape"" bottom: ""global_pool2D"" top: ""reshape_fc_st2"" reshape_param { shape { dim: [-1, 1, 16, 1024] } } }     layer { name: ""segment_consensus_st2"" type: ""Pooling"" bottom: ""reshape_fc_st2"" top: ""pool_fusion_st2"" pooling_param { pool: AVE kernel_h: 16 kernel_w: 1 } }    it looks like that the parameters in the above two layers need to be changed accordingly."
"After  I install caffe successfullyï¼Œwhen I try to import caffe in python ï¼Œone error occursï¼š  `Traceback (most recent call last):    File ""online_recognition.py"", line 4, in        import caffe    File ""/root/ECO4/caffe_3d/python/caffe/__init__.py"", line 1, in        from .pycaffe import Net, SGDSolver    File ""/root/ECO4/caffe_3d/python/caffe/pycaffe.py"", line 13, in        from ._caffe import Net, SGDSolver  ImportError: /root/ECO4/caffe_3d/python/caffe/_caffe.so: invalid ELF header`    before I compile caffeï¼ŒI changed Makefile.config about PYTHONPATH and  the path of protobuf,and I have added the following line:   `export  PYTHONPATH=/root/ECO2/caffe_3d/python:$PYTHONPATH`  I google it and someone says I installed the wrong versionï¼Œcan anyone help meï¼Ÿ thxï¼"
"Determining if the pthread_create exist failed with the following output:  Change Dir: /home/lijie/ECO-efficient-video-understanding/caffe_3d/build/CMakeFiles/CMakeTmp    Run Build Command:""/usr/bin/make"" ""cmTC_35b4f/fast""  /usr/bin/make -f CMakeFiles/cmTC_35b4f.dir/build.make CMakeFiles/cmTC_35b4f.dir/build  make[1]: Entering directory '/mnt/raid3/home/lijie/ECO-efficient-video-understanding/caffe_3d/build/CMakeFiles/CMakeTmp'  Building C object CMakeFiles/cmTC_35b4f.dir/CheckSymbolExists.c.o  /usr/bin/cc    -o CMakeFiles/cmTC_35b4f.dir/CheckSymbolExists.c.o   -c /home/lijie/ECO-efficient-video-understanding/caffe_3d/build/CMakeFiles/CMakeTmp/CheckSymbolExists.c  Linking C executable cmTC_35b4f  /usr/local/lib/python3.5/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/cmTC_35b4f.dir/link.txt --verbose=1  /usr/bin/cc      -rdynamic CMakeFiles/cmTC_35b4f.dir/CheckSymbolExists.c.o  -o cmTC_35b4f  CMakeFiles/cmTC_35b4f.dir/CheckSymbolExists.c.o: In function `main':  CheckSymbolExists.c:(.text+0x16): undefined reference to `pthread_create'  collect2: error: ld returned 1 exit status  CMakeFiles/cmTC_35b4f.dir/build.make:86: recipe for target 'cmTC_35b4f' failed  make[1]: *** [cmTC_35b4f] Error 1  make[1]: Leaving directory '/mnt/raid3/home/lijie/ECO-efficient-video-understanding/caffe_3d/build/CMakeFiles/CMakeTmp'  Makefile:121: recipe for target 'cmTC_35b4f/fast' failed  make: *** [cmTC_35b4f/fast] Error 2    File /home/lijie/ECO-efficient-video-understanding/caffe_3d/build/CMakeFiles/CMakeTmp/CheckSymbolExists.c:  /* */  #include      int main(int argc, char** argv)  {    (void)argv;  #ifndef pthread_create    return ((int*)(&pthread_create))[argc];  #else    (void)argc;    return 0;  #endif  }    Determining if the function pthread_create exists in the pthreads failed with the following output:  Change Dir: /home/lijie/ECO-efficient-video-understanding/caffe_3d/build/CMakeFiles/CMakeTmp    Run Build Command:""/usr/bin/make"" ""cmTC_403cc/fast""  /usr/bin/make -f CMakeFiles/cmTC_403cc.dir/build.make CMakeFiles/cmTC_403cc.dir/build  make[1]: Entering directory '/mnt/raid3/home/lijie/ECO-efficient-video-understanding/caffe_3d/build/CMakeFiles/CMakeTmp'  Building C object CMakeFiles/cmTC_403cc.dir/CheckFunctionExists.c.o  /usr/bin/cc   -DCHECK_FUNCTION_EXISTS=pthread_create   -o CMakeFiles/cmTC_403cc.dir/CheckFunctionExists.c.o   -c /usr/local/lib/python3.5/dist-packages/cmake/data/share/cmake-3.12/Modules/Chec  kFunctionExists.c  Linking C executable cmTC_403cc  /usr/local/lib/python3.5/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/cmTC_403cc.dir/link.txt --verbose=1  /usr/bin/cc  -DCHECK_FUNCTION_EXISTS=pthread_create    -rdynamic CMakeFiles/cmTC_403cc.dir/CheckFunctionExists.c.o  -o cmTC_403cc -lpthreads  /usr/bin/ld: cannot find -lpthreads  collect2: error: ld returned 1 exit status  CMakeFiles/cmTC_403cc.dir/build.make:86: recipe for target 'cmTC_403cc' failed  make[1]: *** [cmTC_403cc] Error 1  make[1]: Leaving directory '/mnt/raid3/home/lijie/ECO-efficient-video-understanding/caffe_3d/build/CMakeFiles/CMakeTmp'  Makefile:121: recipe for target 'cmTC_403cc/fast' failed  make: *** [cmTC_403cc/fast] Error 2      @mzolfaghari  when I run `cmake .. `ï¼ŒI met this problem aboveï¼Œplease helpï¼Œthanksï¼"
"thanks for your great work,this really help me.  I have capture 10 classes action, stand, sitdown, cameback, goaway ....  I want fintuning on your kinetics pre-model, I don't know can you share it for me, thanks        good luck to you , look forward your reply"
"Dear author, I am fortunate to read your thesis. I have some questions about the speed of recognition to ask you.  The speed of I3D in your paper is only 0.9 clip/s.  but the speed I tested reached 40 clip/s ( Titan V).   Is the method we tested different? I hope to know more details."
"great work for action recongnition.  i am trying to change your network to FCN to fit big scene with multi actions.  change the input from 224*224 to 512*512 we got 100*51 output with the kinetcs model.  with total black input the 100 output of 1*51 differs from the same output as we expected.  can you help me with this problem?  bad english writing, hope you can understand my word.  looking for your reply.  "
"Thanks for your excellent work!   I want to reproduce your paperï¼ŒBut I have some questions about Initialization models and accuracy@1   Q1: does the models use Initialization models in your paper in ECO-pytorch if you just set args.pretrained_parts == ""both"", In other words,  weight_url_2d='  and models/C3DResNet18_rgb_16F_kinetics_v1.pth.tar  is your Initialization models in your paper  ECO-pytorch  Q2: is it normal that baseline  is 53.4% when i set args.pretrained_parts == ""scratch"" and num_segments=16 , could you provide some information about your model accuracy@1 when you set pretrained_parts='scratch', '2D', '3D', 'both','finetune' and segments=16  i am new learner in this, If you can help me, I will be very grateful.  Looking forward to your reply @mzolfaghari "
"Firstly thank your great contribution. I have a  question about the testing.  when test the .caffemodel ""ECOfull_UCF101_16F.caffemodel"", with the input of validation video, .e.g., ""v_PlayingDhol_g04_c02.avi"", the result changes sevaral times, but, I find there no correct labels. I have test a lot of times with other vailidation video, the conclution seems not to be changed.     can you give me some suggestion about testing ? Thank you very much"
"Hi @mzolfaghari,    Thanks for sharing your code!    I tried to download the pre-trained ECO_(full|Lite)_UCF101.caffemodel but I couldn't. Similarly, I couldn't download pre-trained models on HMDB51.  e.g.  Download id: 1FMFjtzt_sBWRUyBV86d1cuwHAlnw7HpB       If possible, could you please check and share the correct download ids?    Thanks!"
"I tried to change 2D tensor into 3D tensor, so it can be feed into 3D Conv. The code are shown as follows:  y = self.con1x1(x) # 2D tensor  y = y.view((-1, 96, 16) + y.size()   Thank you so much for you kind help."
"nice work in video understanding, i am very intrested in your work.   as google is blocked in China, i can not download the pretrained models you uploaded to google driver. so,  can you share you pretrained models by Baidu Yun for us. thanks a lot"
"** Although I am testing PyTorch implementation, I guess it might happen to the Caffe implementation as well. So I am asking if there is an answer to my question.     Hello,    I am Youngkyoon Jang, who is a postdoc in the University of Bristol. Thanks for sharing the PyTorch implementation of ECO network for testing new dataset, EPIC-KITCHEN. But I am facing the problem when testing the model to calculate the accuracy.     I am currently using PyTorch implementation for ECO network. And I noticed that the model always predicts the first index with the highest score when using a single input instead of multiple mini-batch samples in testing. Did you know this problem? Is there a correct way to get a consistent output?     When I put a different number of batch sample in a mini-batch, the model predicts a different score for the same sample depending on the number of mini-batch.     I look forward to your reply.     Best,   Young"
"Hi.    In the paper, it is mentioned that you get 64.4% accuracy on Kinetics database for ECO-lite 16 frame. To get this result, do you use the protocol of Kinetics evaluation, with randomly sampling clips of 16 frames from every video and averaging over the results of classification for each inference, or do you use a single inference with 16 frames randomly sampled from the 16 segments of the video?     I tried to reproduce this results using the weights eco_lite_rgb_16F_kinetics_v3.pth.tar and I get only 58.5% on a single inference.     Thanks"
"Dear sir,    I just ust the online_recognition.py method to test the speed, and I found that I wiil use about 50ms for one forward with input shape(16*3*224*224),    I think I can't achieve the hight VPS in the paper, thank you !"
"sir,    when I run online_recognition.py , when loading the model, the problem occurs such below:     blob.hpp:141] Check failed: num_axes()   4 axes    does the caffe_3d branch support C3D?    thank you very much!    And I just compile caffe by make$make pycaffe but not cmake."
"Hi, using the pytorch code provided.  I only obtain 38.3 accuracy on Something-something V1 validation set.  Which is worse than reported, any suggestions?"
"@mzolfaghari Thank you for your excellent work. Your paper mentioned that     > We initialize the weights of the 2D-Net weights with the BN-Inception architecture [31] pre-trained on Kinetics, as provided by [33].    >     I wonder how you train a 2D-Net on Kinetics.  "
"Could you report ECO-pytorch Performance on Kinetics validation set (ECO-Lite)?    I evaluated Kinetics-pretrained ECO-Lite model from ECO-pytorch on Kinetics validation set,  only got 62.1% which is not as good as 64.4% in paper. (ECO-Lite, number of segment = 16)"
"Hiï¼Œ  Thank you for your wonderful job! I use the UCF101 pretrained model downloaded by the script to test the split1 of UCF101, but i only got 88.9% (lite model) and 89.74% (full model) compare to the 91.6% and 92.8% in the paper. The segment is set to 16 and no parameter is changed.    Thx.       "
"I have finished the steps before make && make install,when I conducted make && make install,it had a error about    No implicit rule found for 'install'.   Finished prerequisites of target file 'install'.  Must remake target 'install'.  make: *** No rule to make target 'install'ã€‚ åœæ­¢ã€‚  Can you help me solve the problem?Thank you in advance    "
"Thanks for your excellent repo and paper.  From your paper I learnt that you used kinetic pertained model to extract features for video captioning task. Could you explain more about how you extract ECO features for MSVD dataset? (for instance, from which layer you extract features,  how to set num_class for MSVD dataset)    Thank you in advance."
"I basically reproduce the result on hmdb51,  but I can't reproduce same result on something dataset.  Can you tell me how to set dropout, learning rate or other training trick?    Thank u very muchï¼"
"Hi ,I am very interested in your outstanding work.  But I have a problem when i run online recognition.py  I set the model paths in online_recognition.py as :    mean_file = ""/home/gs/Desktop/ECO/caffe_3d/action_matlab/rgb_mean.mat""  model_def_file = '/home/gs/Desktop/ECO/Models/ECO_Lite_kinetics.caffemodel'  model_file = '/home/gs/Desktop/ECO/models_ECO_Lite/kinetics/deploy.prototxt'    when i run ""python online_recognition.py""  And i got error:      WARNING: Logging before InitGoogleLogging() is written to STDERR  W1224 10:08:03.607410  2554 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface  W1224 10:08:03.607450  2554 _caffe.cpp:140] Use this instead (with the named ""weights"" parameter):  W1224 10:08:03.607463  2554 _caffe.cpp:142] Net('/home/gwl/Desktop/test/model/deploy.prototxt', 1, weights='/home/gwl/Desktop/test/model/ECO_Lite_kinetics.caffemodel')  [libprotobuf ERROR google/protobuf/text_format.cc:245] Error parsing text-format caffe.NetParameter: 51:12: Message type ""caffe.LayerParameter"" has no field named ""bn_param"".  F1224 10:08:03.625159  2554 upgrade_proto.cpp:90] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: /home/gwl/Desktop/test/model/deploy.prototxt  *** Check failure stack trace: ***      Did I set it wrong?  Please help me,thanks!"
"can you show specific implements  of ""Online video understading"" codesï¼Œand by the way,is this alogrithm use in training or testing model?"
"I was able to make the changes and train the PyTorch version of ECO on the Moments in Time dataset. Could you please guide me to the changes to be made to train the Caffe version of ECO on the Moments in Time dataset? Thanks,"
"When I recognize a video,    How to count how many times this action has taken place??    thank you very much    "
"Hi, thank you for releasing a code for the paper.    I have a question on the implementation.  It is written in the paper that you have obtained the best performance on something-something from an ensemble of networks with {16, 20, 24, 32} number of frames.    I wonder how was this ensemble implemented?  Did you train one single model (e.g. taking 16 frames as an input) and test by making that model to take various number of frames {16, 20, 24, 32} (it could be possible because the model performs global average pooling at the end of 3DConvNet so temporal dimension goes away),  or  train multiple models with different number of frames (\e.g. one model takes 16 frames on both train/test, another model takes 20 frames on both train/test, .. , the other takes 32 frames on both train/test)?    Thank you"
"name: ""ECOLite""  ####################################### data #######################################  input: ""data""  input_dim: 80    why the input_dim is 80 ?    is  batchsize  5   ?16 * 5 =80 ,is that right ?"
"when i finetune my own dataset ,loss=0  why?  I1115 13:17:15.940382 25582 solver.cpp:241] Iteration 20, loss = 0  I1115 13:17:15.940572 25582 solver.cpp:256]     Train net output #0: loss = 0 (* 1 = 0 loss)  I1115 13:17:15.940580 25582 solver.cpp:665] Iteration 20, lr = 0.001  I1115 13:18:19.726904 25582 solver.cpp:241] Iteration 40, loss = 0  I1115 13:18:19.727092 25582 solver.cpp:256]     Train net output #0: loss = 0 (* 1 = 0 loss)  I1115 13:18:19.727100 25582 solver.cpp:665] Iteration 40, lr = 0.001  I1115 13:19:23.873339 25582 solver.cpp:241] Iteration 60, loss = 0  I1115 13:19:23.873425 25582 solver.cpp:256]     Train net output #0: loss = 0 (* 1 = 0 loss)  I1115 13:19:23.873433 25582 solver.cpp:665] Iteration 60, lr = 0.001  I1115 13:20:29.114048 25582 solver.cpp:241] Iteration 80, loss = 0  I1115 13:20:29.114857 25582 solver.cpp:256]     Train net output #0: loss = 0 (* 1 = 0 loss)  I1115 13:20:29.114867 25582 solver.cpp:665] Iteration 80, lr = 0.001  I1115 13:21:34.218885 25582 solver.cpp:241] Iteration 100, loss = 0  I1115 13:21:34.218997 25582 solver.cpp:256]     Train net output #0: loss = 0 (* 1 = 0 loss)  I1115 13:21:34.219003 25582 solver.cpp:665] Iteration 100, lr = 0.001  I1115 13:21:35.014933 25582 solver.cpp:273] Maximum resident set size is: 1352040 kb  I1115 13:21:35.015017 25582 solver.cpp:282] cuda meminfo: used 3957 MB, of 4037 MB  I1115 13:22:39.643602 25582 solver.cpp:241] Iteration 120, loss = 0  I1115 13:22:39.643775 25582 solver.cpp:256]     Train net output #0: loss = 0 (* 1 = 0 loss)  I1115 13:22:39.643785 25582 solver.cpp:665] Iteration 120, lr = 0.001  I1115 13:23:45.116634 25582 solver.cpp:241] Iteration 140, loss = 0  I1115 13:23:45.116822 25582 solver.cpp:256]     Train net output #0: loss = 0 (* 1 = 0 loss)  I1115 13:23:45.116832 25582 solver.cpp:665] Iteration 140, lr = 0.001  I1115 13:24:50.277351 25582 solver.cpp:241] Iteration 160, loss = 0  I1115 13:24:50.277442 25582 solver.cpp:256]     Train net output #0: loss = 0 (* 1 = 0 loss)  I1115 13:24:50.277449 25582 solver.cpp:665] Iteration 160, lr = 0.001  I1115 13:25:55.094033 25582 solver.cpp:241] Iteration 180, loss = 0  I1115 13:25:55.094105 25582 solver.cpp:256]     Train net output #0: loss = 0 (* 1 = 0 loss)  I1115 13:25:55.094113 25582 solver.cpp:665] Iteration 180, lr = 0.001  I1115 13:26:59.386559 25582 solver.cpp:241] Iteration 200, loss = 0  I1115 13:26:59.386631 25582 solver.cpp:256]     Train net output #0: loss = 0 (* 1 = 0 loss)  I1115 13:26:59.386636 25582 solver.cpp:665] Iteration 200, lr = 0.001  I1115 13:28:04.782306 25582 solver.cpp:241] Iteration 220, loss = 0  I1115 13:28:04.782454 25582 solver.cpp:256]     Train net output #0: loss = 0 (* 1 = 0 loss)  I1115 13:28:04.782464 25582 solver.cpp:665] Iteration 220, lr = 0.001  I1115 13:29:09.191298 25582 solver.cpp:241] Iteration 240, loss = 0  I1115 13:29:09.191468 25582 solver.cpp:256]     Train net output #0: loss = 0 (* 1 = 0 loss)  I1115 13:29:09.191490 25582 solver.cpp:665] Iteration 240, lr = 0.001  I1115 13:30:13.419653 25582 solver.cpp:241] Iteration 260, loss = 0  I1115 13:30:13.419807 25582 solver.cpp:256]     Train net output #0: loss = 0 (* 1 = 0 loss)  "
"Hi ,I am very interested in your outstanding work.  But I have a problem when i run online recognition.py  I set the  model paths in online_recognition.py as :     rgb_mean.mat: /home/ra/ECO-efficient-video-understanding-master/caffe_3d/action_matlab  deploy.prototxt: /home/ra/ECO-efficient-video-understanding-master/models_ECO_Lite/kinetics  ECO_Lite_kinetics.caffemodel:  download  from Google Drive      And i got error:  python online_recognition.py   Setting device 0  WARNING: Logging before InitGoogleLogging() is written to STDERR  I1105 17:18:41.087724  7888 net.cpp:46] Initializing net from parameters:   name: ""ECOLite""  input: ""data""  input_dim: 80  input_dim: 3  input_dim: 224  input_dim: 224  .  .  .  I1105 17:19:27.531430  7888 net.cpp:551] Collecting Learning Rate and Weight Decay.  I1105 17:19:27.531447  7888 net.cpp:300] Network initialization done.  I1105 17:19:27.531451  7888 net.cpp:301] Memory required for data: 3935844160  GLib-GIO-Message: 17:19:43.243: Using the 'memory' GSettings backend.  Your settings will not be saved or shared with other applications.  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]  Traceback (most recent call last):    File ""online_recognition.py"", line 122, in        online_predict(mean_file,model_def_file,model_file,classes_file,num_categories)    File ""online_recognition.py"", line 92, in online_predict      net.blobs['data'].data[...] = np.transpose(rgb[:,:,:,:], (3,2,1,0))  ValueError: could not broadcast input array from shape (16,3,224,224) into shape (80,3,224,224)    Did I set it wrong?  Please help me,thanks!"
"@mzolfaghari Thank you for your excellent work and the public-available modelï¼  After reading your paper and running through the code(ECO-pytorch), I have some question about the training strategy. I followed the training parameter in the training script to finetune the ECO-lite model on UCF101 using 16 frames, but I failed to get the same result as in the paper which is 91.6% on the 3 splits of UCF101.   Then I tried to finetune the ECO-full model using the same strategy, but I got the nearly same result as ECO-lite, which is approximately 88%. It seems that ECO-full does not improve the performance. So I wonder if there is any difference between the training parameters of lite and full model.   Thank you!"
solved
"Hello, I intend to use the ECO PyTorch code to train the model on the moments in time dataset. I have the frames extracted and the training and validation lists prepared.     I'd greatly appreciate any guidance on the changes to be made to the code (main.py) to train the model on the moments dataset (or any dataset other than kinetics).    Thank you,"
"Dear sir, when I lately train the ECO network based on UCF101 dataset, the loss is always 87.3365 from the begining even I changed the learning rate and some other hyper parameters, also when I use the ECO_Lite_UCF101.caffemodel for tesing, it predicts all the objects to the same one class. I wonder whether you can give me any advice to solve the problem. Thank you."
Did you have the pytorch pretrained models on Kinetics? I would really appreciate it if you can share it
"I'm fine tuning the pretrained ECO Full architecture on my own dataset. The inference (deploy.protoxt) runs with no issues. The only difference in the two files is the last layer and the data layer.    I noticed in the first forward pass, res3b_bn blows up (inf). I'm wondering if this is an issue someone has seen before. I've attached the log file.         "
"Hello,    In the script 'create_list_kinetics.m', you have the following path:    path_DB_rgb='/datasets/kinetics/train/db_frames//'    I'm assuming this folder contains the frames for the kinetics videos. Are the frames for the videos available online somewhere, or is there a script available to split the videos into the frames?    I tried running ''main.py' of your pytorch implementation and got the following error:    when running     ---> 20     for i, (input, target) in enumerate(train_loader):    ......    FileNotFoundError: [Errno 2] No such file or directory: '/kinetics/pumping_gas/ib5PzcBeYIc_000004_000014/0004.jpg'    Thanks,    "
"Hello,    Thank you for making the PyTorch implementation available. What are the steps to using a trained model for activity recognition on some videos in my folder?    I have downloaded the ECO Lite model using gd_download.py and have the model downloaded. I have a folder of videos in .avi format.    Thanks,"
"Hi: @mzolfaghari    I was training ucf101 dataset and set ECO_Lite.prototxt parameters  `# ----- video/label input ----- for training  layer {    name: ""data""    type: ""VideoData""    top: ""data""    top: ""label""    video_data_param {      source: ""/home/ww/ECO-efficient-video-understanding/datasets/ucf101/train_rgb_split1.txt""      batch_size: 10      new_length: 1      num_segments: 16      modality: RGB      shuffle: true      name_pattern: ""image_%04d.jpg""    }    transform_param{      crop_size: 224      mirror: true      fix_crop: true      more_fix_crop: true      multi_scale: true      max_distort: 1      scale_ratios:     Can you give me some advice, thanks!"
None
"@mzolfaghari Hi, I plan to train the custom dataset with the different ""num_segments"" for the speed & accuracy.  And, I change the following information:  (1)  layer {    name: ""data""    type: ""VideoData""    top: ""data""    top: ""label""    video_data_param {      #source: "".../train.txt""      source: ""./train_videofolder_new.txt""      batch_size: 1 #8 #16      new_length: 1      new_width: 320      new_height: 240      num_segments: 8 #16      modality: RGB      shuffle: true      name_pattern: ""%05d.jpg""    }    transform_param{      crop_size: 224      mirror: true      fix_crop: true      more_fix_crop: true      multi_scale: true      max_distort: 1      scale_ratios:[1,.875,.75, .66]      is_flow: false      mean_value: [104]      mean_value: [117]      mean_value: [123]            mean_value: [104]      mean_value: [117]      mean_value: [123]            mean_value: [104]      mean_value: [117]      mean_value: [123]            mean_value: [104]      mean_value: [117]      mean_value: [123]            mean_value: [104]      mean_value: [117]      mean_value: [123]            mean_value: [104]      mean_value: [117]      mean_value: [123]            mean_value: [104]      mean_value: [117]      mean_value: [123]        mean_value: [104]      mean_value: [117]      mean_value: [123]          }    include: { phase: TRAIN }  }    (2)  #=================== 3D network ===========================================  #layer {  #  name: ""r2Dto3D""  #  type: ""Reshape""  #  bottom: ""inception_3c_double_3x3_1_bn""  #  top: ""res2b_bn_pre""  #  reshape_param{  #    shape { dim: -1 dim: 16 dim: 96 dim: 28 dim: 28 }   #  }  #}  layer {    name: ""r2Dto3D""    type: ""Reshape""    bottom: ""inception_3c_double_3x3_1_bn""    top: ""res2b_bn_pre""    reshape_param{      shape { dim: -1 dim: 8 dim: 96 dim: 28 dim: 28 }     }  }    (3)  #layer { name: ""reshape_fc_st2"" type: ""Reshape"" bottom: ""global_pool2D"" top: ""reshape_fc_st2"" #reshape_param { shape { dim: [-1, 1, 16, 1024] } } }   #layer { name: ""segment_consensus_st2"" type: ""Pooling"" bottom: ""reshape_fc_st2"" top: ""pool_fusion_st2"" #pooling_param { pool: AVE kernel_h: 16 kernel_w: 1 } }    layer { name: ""reshape_fc_st2"" type: ""Reshape"" bottom: ""global_pool2D"" top: ""reshape_fc_st2"" reshape_param { shape { dim: [-1, 1, 8, 1024] } } }   layer { name: ""segment_consensus_st2"" type: ""Pooling"" bottom: ""reshape_fc_st2"" top: ""pool_fusion_st2"" pooling_param { pool: AVE kernel_h: 8 kernel_w: 1 } }      However, I got the following error;  I0823 11:47:52.993273    22 net.cpp:170] Top shape: 1 1024 (1024)  I0823 11:47:52.993278    22 layer_factory.hpp:74] Creating layer global_pool  I0823 11:47:52.993285    22 net.cpp:99] Creating Layer global_pool  I0823 11:47:52.993289    22 net.cpp:479] global_pool   global_pool  I0823 11:47:52.993306    22 net.cpp:163] Setting up global_pool  F0823 11:47:52.993738    22 blob.cpp:32] Check failed: shape[i] >= 0 (-1 vs. 0)  *** Check failure stack trace: ***      @     0x7f3e01c5c5cd  google::LogMessage::Fail()      @     0x7f3e01c5e433  google::LogMessage::SendToLog()      @     0x7f3e01c5c15b  google::LogMessage::Flush()      @     0x7f3e01c5ee1e  google::LogMessageFatal::~LogMessageFatal()      @     0x7f3e01ff7878  caffe::Blob ::Reshape()      @     0x7f3e020f5a7b  caffe::PoolingLayer ::Reshape()      @     0x7f3e0206deec  caffe::CuDNNPoolingLayer ::Reshape()      @     0x7f3e02135abd  caffe::Net ::Init()      @     0x7f3e0213821c  caffe::Net ::Net()      @           0x4118a3  time()      @           0x40e932  main      @     0x7f3e0066f830  __libc_start_main      @           0x40ef79  _start      @              (nil)  (unknown)  Aborted (core dumped)    Looking forward to any replies.  Thanks"
"@mzolfaghari Hi, I plan to train the custom dataset with the different ""num_segments"" for the speed & accuracy.  And, I change the following information:  (1)  layer {    name: ""data""    type: ""VideoData""    top: ""data""    top: ""label""    video_data_param {      #source: "".../train.txt""      source: ""./train_videofolder_new.txt""      batch_size: 1 #8 #16      new_length: 1      new_width: 320      new_height: 240      num_segments: 8 #16      modality: RGB      shuffle: true      name_pattern: ""%05d.jpg""    }    transform_param{      crop_size: 224      mirror: true      fix_crop: true      more_fix_crop: true      multi_scale: true      max_distort: 1      scale_ratios:[1,.875,.75, .66]      is_flow: false      mean_value: [104]      mean_value: [117]      mean_value: [123]            mean_value: [104]      mean_value: [117]      mean_value: [123]            mean_value: [104]      mean_value: [117]      mean_value: [123]            mean_value: [104]      mean_value: [117]      mean_value: [123]            mean_value: [104]      mean_value: [117]      mean_value: [123]            mean_value: [104]      mean_value: [117]      mean_value: [123]            mean_value: [104]      mean_value: [117]      mean_value: [123]        mean_value: [104]      mean_value: [117]      mean_value: [123]          }    include: { phase: TRAIN }  }    (2)  #=================== 3D network ===========================================  #layer {  #  name: ""r2Dto3D""  #  type: ""Reshape""  #  bottom: ""inception_3c_double_3x3_1_bn""  #  top: ""res2b_bn_pre""  #  reshape_param{  #    shape { dim: -1 dim: 16 dim: 96 dim: 28 dim: 28 }   #  }  #}  layer {    name: ""r2Dto3D""    type: ""Reshape""    bottom: ""inception_3c_double_3x3_1_bn""    top: ""res2b_bn_pre""    reshape_param{      shape { dim: -1 dim: 8 dim: 96 dim: 28 dim: 28 }     }  }    (3)  #layer { name: ""reshape_fc_st2"" type: ""Reshape"" bottom: ""global_pool2D"" top: ""reshape_fc_st2"" #reshape_param { shape { dim: [-1, 1, 16, 1024] } } }   #layer { name: ""segment_consensus_st2"" type: ""Pooling"" bottom: ""reshape_fc_st2"" top: ""pool_fusion_st2"" #pooling_param { pool: AVE kernel_h: 16 kernel_w: 1 } }    layer { name: ""reshape_fc_st2"" type: ""Reshape"" bottom: ""global_pool2D"" top: ""reshape_fc_st2"" reshape_param { shape { dim: [-1, 1, 8, 1024] } } }   layer { name: ""segment_consensus_st2"" type: ""Pooling"" bottom: ""reshape_fc_st2"" top: ""pool_fusion_st2"" pooling_param { pool: AVE kernel_h: 8 kernel_w: 1 } }      However, I got the following error;  I0823 11:47:52.993273    22 net.cpp:170] Top shape: 1 1024 (1024)  I0823 11:47:52.993278    22 layer_factory.hpp:74] Creating layer global_pool  I0823 11:47:52.993285    22 net.cpp:99] Creating Layer global_pool  I0823 11:47:52.993289    22 net.cpp:479] global_pool   global_pool  I0823 11:47:52.993306    22 net.cpp:163] Setting up global_pool  F0823 11:47:52.993738    22 blob.cpp:32] Check failed: shape[i] >= 0 (-1 vs. 0)  *** Check failure stack trace: ***      @     0x7f3e01c5c5cd  google::LogMessage::Fail()      @     0x7f3e01c5e433  google::LogMessage::SendToLog()      @     0x7f3e01c5c15b  google::LogMessage::Flush()      @     0x7f3e01c5ee1e  google::LogMessageFatal::~LogMessageFatal()      @     0x7f3e01ff7878  caffe::Blob ::Reshape()      @     0x7f3e020f5a7b  caffe::PoolingLayer ::Reshape()      @     0x7f3e0206deec  caffe::CuDNNPoolingLayer ::Reshape()      @     0x7f3e02135abd  caffe::Net ::Init()      @     0x7f3e0213821c  caffe::Net ::Net()      @           0x4118a3  time()      @           0x40e932  main      @     0x7f3e0066f830  __libc_start_main      @           0x40ef79  _start      @              (nil)  (unknown)  Aborted (core dumped)    Looking forward to any replies."
"@mzolfaghari Thank you  for providing such good ideas and articles. I really want to reproduce this code. But I don't have the dataset, for example Kinetics dataset and so on. Can you upload on GitHub soon? Thank you very much!"
"when I train the network, I got the information as following:  (I build the caffe-training environment under the     You are running caffe compiled with MPI support. Now it's running in non-parallel model  what's meaning?    What's more, I can't use the multi-gpus to train my model.    Looking forward to any advice."
" @mzolfaghari   Thank you very much for providing such good ideas and articles. I can't wait to reproduce this code. But I encountered difficulties in configuring the environment. I use bvlc/caffe's image. In the container, follow the steps you suggested to enter the caffe3d folder and execute make all -j8. This error occurs:  `root@ecdc968b55d9:/data/ECO-efficient-video-understanding-master/caffe_3d# make all -j8  /bin/sh: 1: bc: not found  CXX src/caffe/internal_thread.cpp  CXX src/caffe/layer_factory.cpp  CXX src/caffe/syncedmem.cpp  CXX src/caffe/data_transformer.cpp  CXX src/caffe/solver.cpp  CXX src/caffe/common.cpp  CXX src/caffe/net.cpp  CXX src/caffe/blob.cpp  In file included from ./include/caffe/blob.hpp:9:0,                   from ./include/caffe/layer.hpp:8,                   from src/caffe/layer_factory.cpp:3:  .build_release/src/caffe/proto/caffe.pb.h:12:2: error: #error This file was generated by a newer version of protoc which is   #error This file was generated by a newer version of protoc which is    ^  .build_release/src/caffe/proto/caffe.pb.h:13:2: error: #error incompatible with your Protocol Buffer headers. Please update   #error incompatible with your Protocol Buffer headers.  Please update    ^  .build_release/src/caffe/proto/caffe.pb.h:14:2: error: #error your headers.   #error your headers.    ^  In file included from ./include/caffe/blob.hpp:9:0,                   from ./include/caffe/layer.hpp:8,                   from src/caffe/layer_factory.cpp:3:  .build_release/src/caffe/proto/caffe.pb.h:23:35: fatal error: google/protobuf/arena.h: No such file or directory  compilation terminated.  In file included from ./include/caffe/blob.hpp:9:0,                   from ./include/caffe/net.hpp:10,                   from src/caffe/solver.cpp:7:  .build_release/src/caffe/proto/caffe.pb.h:12:2: error: #error This file was generated by a newer version of protoc which is   #error This file was generated by a newer version of protoc which is    ^  .build_release/src/caffe/proto/caffe.pb.h:13:2: error: #error incompatible with your Protocol Buffer headers. Please update   #error incompatible with your Protocol Buffer headers.  Please update    ^  .build_release/src/caffe/proto/caffe.pb.h:14:2: error: #error your headers.   #error your headers.    ^  In file included from ./include/caffe/blob.hpp:9:0,                   from ./include/caffe/net.hpp:10,                   from src/caffe/solver.cpp:7:  .build_release/src/caffe/proto/caffe.pb.h:23:35: fatal error: google/protobuf/arena.h: No such file or directory  compilation terminated.  In file included from ./include/caffe/blob.hpp:9:0,                   from ./include/caffe/layer.hpp:8,                   from src/caffe/net.cpp:9:  .build_release/src/caffe/proto/caffe.pb.h:12:2: error: #error This file was generated by a newer version of protoc which is   #error This file was generated by a newer version of protoc which is    ^  .build_release/src/caffe/proto/caffe.pb.h:13:2: error: #error incompatible with your Protocol Buffer headers. Please update   #error incompatible with your Protocol Buffer headers.  Please update    ^  .build_release/src/caffe/proto/caffe.pb.h:14:2: error: #error your headers.   #error your headers.    ^  In file included from ./include/caffe/blob.hpp:9:0,                   from ./include/caffe/layer.hpp:8,                   from src/caffe/net.cpp:9:  .build_release/src/caffe/proto/caffe.pb.h:23:35: fatal error: google/protobuf/arena.h: No such file or directory  compilation terminated.  Makefile:516: recipe for target '.build_release/src/caffe/layer_factory.o' failed  make: *** [.build_release/src/caffe/layer_factory.o] Error 1  make: *** Waiting for unfinished jobs....  Makefile:516: recipe for target '.build_release/src/caffe/solver.o' failed  make: *** [.build_release/src/caffe/solver.o] Error 1  Makefile:516: recipe for target '.build_release/src/caffe/net.o' failed  make: *** [.build_release/src/caffe/net.o] Error 1  In file included from ./include/caffe/blob.hpp:9:0,                   from src/caffe/blob.cpp:4:  .build_release/src/caffe/proto/caffe.pb.h:12:2: error: #error This file was generated by a newer version of protoc which is   #error This file was generated by a newer version of protoc which is    ^  .build_release/src/caffe/proto/caffe.pb.h:13:2: error: #error incompatible with your Protocol Buffer headers. Please update   #error incompatible with your Protocol Buffer headers.  Please update    ^  .build_release/src/caffe/proto/caffe.pb.h:14:2: error: #error your headers.   #error your headers.    ^  In file included from ./include/caffe/blob.hpp:9:0,                   from src/caffe/blob.cpp:4:  .build_release/src/caffe/proto/caffe.pb.h:23:35: fatal error: google/protobuf/arena.h: No such file or directory  compilation terminated.  Makefile:516: recipe for target '.build_release/src/caffe/blob.o' failed  make: *** [.build_release/src/caffe/blob.o] Error 1  In file included from ./include/caffe/blob.hpp:9:0,                   from ./include/caffe/data_transformer.hpp:6,                   from src/caffe/data_transformer.cpp:7:  .build_release/src/caffe/proto/caffe.pb.h:12:2: error: #error This file was generated by a newer version of protoc which is   #error This file was generated by a newer version of protoc which is    ^  .build_release/src/caffe/proto/caffe.pb.h:13:2: error: #error incompatible with your Protocol Buffer headers. Please update   #error incompatible with your Protocol Buffer headers.  Please update    ^  .build_release/src/caffe/proto/caffe.pb.h:14:2: error: #error your headers.   #error your headers.    ^  In file included from ./include/caffe/blob.hpp:9:0,                   from ./include/caffe/data_transformer.hpp:6,                   from src/caffe/data_transformer.cpp:7:  .build_release/src/caffe/proto/caffe.pb.h:23:35: fatal error: google/protobuf/arena.h: No such file or directory  compilation terminated.  Makefile:516: recipe for target '.build_release/src/caffe/data_transformer.o' failed  make: *** [.build_release/src/caffe/data_transformer.o] Error 1  `    Can you help me analyze where I am wrong? Then how do I need to solve it?"
"@mzolfaghari   Thanks for your excellent repo and paper.  video_data_param {      source: "".../train.txt""      batch_size: 16      new_length: 1      new_width: 320      new_height: 240      num_segments: 16      modality: RGB      shuffle: true      name_pattern: ""%05d.jpg""    }    I want to use your model to train my custom datasets, so, would you mind telling us the format in your train.txt. And, what's more, as for the num_segments: 16,  there are 16      {mean_value: [104]      mean_value: [117]      mean_value: [123]  }  the number of the num_segments is the same as the {mean_value}?"
"@mzolfaghari   Thanks for your excellent idea, paper and repo. And, after reading your paper, I'm a little bit confused about the weight sharing in your architecture overview. Would you mind telling me more details about it. I thought the N frames are related to the N number of Inception_3c of 2D-Net, so what's the meaning about the weight sharing.  Thanks.  Looking forward to any replies."
"Thanks for your excellent idea, paper and repo. And, after reading your paper, I'm a little bit confused about the weight sharing in your architecture overview. Would you mind telling me more details about it. I thought the N frames are related to the N number of Inception_3c of 2D-Net, so what's the meaning about the weight sharing.  Thanks.  Looking forward to any replies."
"Hi @mzolfaghari     Thanks for your excellent work! In updated repo, I found that the dataset file lists are not provided (kinetics_train_frm.txt  for example). Can you provide these files for reference? Thanks so much!"
"hi, @mzolfaghari ! Thanks for your excellent work, and I'm glad you will release your code here~  But I have some questions about the ECO Lite arch & training details:    1. The 2D-Net use BN-Inception Arch(until inception-3c layer) and the output channel is 96, does it mean that only select one branch(output channel: 96) of the inception-3c layer?    2. For the original 3D-Resnet18, down-sampling is performed by conv3_1, conv4_1 and conv5_1. I noticed that in the supplementary material Table 1, the output size of conv3_x layer is ""28x28xN"", it seems that down-sampling is not performed by conv3_1, and conv3_1 use stride ""1x1x1""?    3. I try to implement the ECO Network using PyTorch, I initialize the weights of the 2D-Net with the BN-Inception arch pretrained on Kinetics, which provided by  . And I initialize 3D-Net with the Kinetics-pretrained model of 3D-Resnet18 provided by  . But when I trained on UCF101, the training loss dropped well, but the test loss performed badly, it seems like overfitting.   (I noticed that in page 7 of the paper, after initializing the weights from different pretrained models, you train ECO and ECO Lite on the Kinetics dataset for 10 epochs, but I didn't do this, can this cause overfitting?)"
"Hi @mzolfaghari   Thanks for your paper and the upcoming code! The performance of ECO is very impressive, especially for the early recognition task. According to your paper, ECO can achived an accuracy of more than 0.8 when the observation ratio is only 0.1. Since ECO is trained with sparse sampling to capture long-term dependencies, I'm wondering why it performs so well for early recognition, in which the input is obtained by dense sampling. Any intuitive idea about that?  By the way, have you tried performing inference by dense sliding window, as typical 3D CNN does?  "
"Hi,  Thanks for making open-source your submission that is very useful. Would it be possible for you to upload the supplementary material on arxiv?  In your paper you refer to the supplementary material concerning the architecture.  I am especially interested in your 2D_Nets_s and 3D_Net architecture.    Thanks and well done for your impressive work :)    Fabien  "
I have a question: how to determine the prune ratio of each layer? by try-and-error? 
æˆ‘å†™äº†ä¸€ä¸ªç®€å•çš„pytorchå®žçŽ°ï¼Œä½†æ˜¯ç»“æžœå¾ˆå¥‡æ€ªï¼Œæœ€è¿‘æ‚¨æ˜¯å¦æœ‰ç©ºç»™æˆ‘ä¸€ç‚¹æŒ‡å¯¼ï¼Ÿ
"When I change your code to pytorch, I found the loss doesn't convergence, and the conditions W-Z is  also doesn't convergence."
Is a PyTorch implementation going to be provided?
"I read the codes in it and find that they just set the connection weights as zeros. And then use the whole matrix to calculate the output. I assume that when training and testing the network, the memory footprint(gpu memory) would not change.    Therefore, how pruning benefit the CNN when applying in resource limited machine?"
None
Hi!   I am trying to integrate physics intuition during robot training. I am currently using Mujoco and that's why I think this project would be relatively easy to integrate.    How difficult would it be to extend this project to different tasks/physic events?   
"Hi, thanks for open source such a great dataset. I have downloaded the RGB images and unzip it, but for one data point, I only find frame0 images `xxx-mono-0.png` with different camera views. Does that mean the whole falling videos are provided? Would be possible to upload them for download? I understand the dataset should be very large and it's been 4 years since the dataset release, so completely fine if not possible  "
"I came across your dataset, it looks cool! You might be interested in some additional real stacking data which I recently released:       feel free to close this issue"
"Hi @ogroth and thanks for releasing your code. It works well! Is it possible to determine the mass of an object (cuboid, rectangle, ...) when I create a new scenario? And the gravity of the world?  Thanks"
" some questions about   ""class PartialConv2d(nn.Conv2d): ""  firstly, what's the purpose of  ""   self.mask_ratio = torch.mul(self.mask_ratio, self.update_mask)""? mabe the is the update_mask which is use for the next layer,so,why, torch.mul(self.mask_ratio, self.update_mask)  secendly,  what's the purpose of ""output = torch.mul(output, self.update_mask)"" the next feature layer is the feature with mask?  lastly, there is no computition about  filter weight matrix and X (WX).so when type ""class PartialConv2d(nn.Conv2d)"",the code will first jump  to nn.Conv2d ï¼Ÿ"
"Hello! In partialconv2d.py :       if 'multi_channel' in kwargs:              self.multi_channel = kwargs['multi_channel']              kwargs.pop('multi_channel')          else:              self.multi_channel = False     Why should we set the args: multi-channel to be ""True"" if we want use Pconv for image inpainting? I think it has no effect when we use it for image inpainting  if multi-channel is set to False. It seems like a waste, but it doesn't make any difference to the result.    Please correct me if I make some wrongs."
"When i import modules, i take error that   `import models as models_partial # partial conv based padding`  > `ModuleNotFoundError: No module named 'models'`"
"I did your usage guide for image inpainting.    PartialConv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False, **multi_channel=True, return_mask=True**)    and started to run main.py  but after partialconv, the code error is occured with this message     AttributeError: 'tuple' object has no attribute 'dim'    How can I solve this error?  "
"Hi. Has anyone tried to reproduce the painting results on CelebA-HQ dataset? This model is trained on a randomly select subset of CelebA-HQ of size 27k for 3 days on a GPU and fine-tuned for half of a day using batch size 6. And here are some outputs I get, where the inputs come from my screenshot of the paper:  !     For comparison, here are the results shown in the paper (Fig 8):  <img width=""643"" alt=""sample"" src=""     For my code, the inpainted regions are blurry when the masked areas are big and the masked facial features are not recovered. I wonder if anyone has encountered similar issues or has any guess on what might be the reason. Thanks!        "
Hello. Has anyone tried to save the model with partial convolutions as pytorch script? I got this error when calling `pytorch.jit.script`:   
Is the online demo down for good or will it be coming back online?
"Thank you for your great project!           I am sorry,Because I don't found the website of the mask training dataset from the paper,Do you provide the website? If you do I will  appreciate it!          Looking forward to your reply.    "
"Thanks for your awesome job. I load the test mask file from your project (  However, all the masks with different ratios  are in a directory.  Are them in a rigid sequance? For examaple, the masks with ratio of (0.01, 0.1] are form 0.png to 999.png.  If not, how can i find the masks with specific ratio?"
"Hi,  I really like your work - thank you for sharing the code.    I have some remarks on the code of PartialConv2d:    ### 1. Using buffers instead of simple ""member tensors"":  The class attribute `self.weight_maskUpdater` is defined as a ""plain"" tensor:     As a result, when the model is transferred to GPU, or data type is changing you need to explicitly check for it and change it:       A more elegant way is to use  :     This way `self.weight_maskUpdater` will be affected by any `.to(...)` / `.cuda()` / `.cpu()` invoked on the model hosting this layer, making the condition on line 49 redundent.    ### 2. Use of    Instead of `torch.ones(...).to(...)`     You can use `torch.ones_like` which is simpler and easier to read:       ### 3. No need to import `Variable`  You import `Variable`, but never use it.       **BTW** All these comments are applicable to the code of `PartialConv3d`."
"Hello, i can't see about train details in this project. To train inpainting model, input : batch image and mask is one or batch? Whether this difference will mislead the final result? Thanks!"
"First, congratulations to the PartialConv team, it is a fascinating contribution in many ways.    Also, thank you for making pdresnet checkpoints available, and for the insightful comparison with the other padding modes!  Would it also be possible to have pre-trained weights for REFLECT and REPLICATE?  Thank you!"
"Hello,    I was trying the web base bersion of your toolbox, but without sucess. (     I have a very sparse pixel image, which I want to inpaint (see attached image).   Basically I want to inpaint in the convex hull, where I draw a mask and use the scipy inpaint, with good results. When I try this toolbox, it returns the same image as the input.  I understand that the missing pixels are much-much more than the actual information, but I wonder if I can tweak some parameters to make it work.    Can you share a hint where shall I start searching?  Thanks          !   "
"Hi, i want to use and modify your code for image inpainting. I have a dataset with thousands of pics.  Here are some questions,hope someone can help  me: @liuguilin1225   1.if I want to fill a hole in image A with  more content from image A and less imaginary content from other images. What should i do? Is it right to modify total loss function? Is so, which part of loss should be modified?  2. If I modify loss function, does it work if I train my own dataset both with the pretrained model this repo provided and the modified loss function?  "
None
"now i'm trying to make some codes to build a model for video inpainting, not image inpainting.  on researching inpainting models, i found partial convolution works well   so i would like to build a model with partial convolution for 3D but there is no standard codes or codes working well that i can follow    so do your team have a plan to do or doing some works or research for it?  so is there any reference to read some codes, not papers?    what i mentioned)      "
"Thank you for your genius work. However, there's a little problem that puzzles me.  For image inpainting task,  when the pixel values the missing area of the input image are set to 0,   I think that ordinary convolution(bias=False and no use BN) and partial convolution are the same? Under this circumstance,  the result of the ordinary convolution(bias=False and no use BN) is 0 naturally,  at the missing area (if all piexls in this slide windows are 0) . And the gradients at these missing areas is 0, it is the same as parital convolution.  I'm confused about this small question. Can you tell me the essential difference between them? Thanks!  @liuguilin1225 "
"Hi,  Thanks a lot for your brilliant work on inpainting with partial conv. It helped me a lot. :)    I'm a bit puzzled at your comparison experiment with Globally and Locally Consistent Image Completion. According to the paper, you are using the pretrained model of GL, which was trained on Place2 dataset, and you compared the results on both Place2 and ImageNet (figure 5 & 6).   I just wonder why you decided to test it on ImageNet? Do you mean that the images in Place2 and ImageNet are similiar so we can just use the latter to validate models trained on the former?    Looking forward to your reply. Thanks again!"
"I am a postgraduate student in China, I find a new partial-conv based (mask based ) convolution method for some special inpainting application. I have been re-implementing your inpainting model for a long time. But it doesn't get a good result as yours. Could u help me to publicize the official model code or send to me? I am hurrying to publish my first paper by modifying your method. Thanks very much"
"I was impressed with your efficient padding method. So I used your method in my code.  But I have a problem.    error :       raw_out = super(PartialConv2d, self).forward(torch.mul(input, mask)  TypeError: mul() received an invalid combination of arguments - got (Tensor, NoneType), but expected one of:   * (Tensor input, Tensor other, Tensor out)   * (Tensor input, float other, Tensor out)  ---------------------------------------------------------------------------------------------------------------  The first iteration does not produce an error, but at the second iteration this error occurs.  I think the error is due to the mask value being none.  That's why I  I used self.last_size = (None, None) on line 44 or I used the mask by replacing it with the self variable.    If you have any other way to fix this error, can you recommend it?    Thank you for opening the code."
Hello!  Greaty results! How to generalization this layer for resneXt network with `groups != 1` in conv layers inside Bottleneck block?
"You mentioned in the previous issue we can load pretrained and convert conv2d to partialconv. How would you change it as the model structure is fixed in pretrained models? My model is          Using this makes your model train much slower than normal conv2d layers with fine tunning both off and on. How can i change all layers to PartialConv2d in the model above?    Also is there any plans to release a PartialConv3d? I would be interested in medical segmentation for that?    Thanks,  Rahul"
"Hi,    In your inpainting paper, you've mentioned that in the decoder part of  Unet, you add skip connections for both features and mask by concatenating them. I assume you concatenate them along feature channels. But if I give this concatenated mask as input to the Pconv layer, it throws the following error. So, it looks like I can't pass the concatenated mask to this layer directly. What should I concatenate the masks?     "
None
"Hi,   I have a question about arch design, I find in the paper in Section 3.2 which is:  > replacing all convolutional layers with partial convolutional layers and using nearest neighbor up-sampling in the decoding stage    The input have inpainting mask, so the partial conv is necessary.    But on the middle feature maps, do it be ordinary conv is also okay?      "
"The demo at   is not working.    After uploading the image, erasing and clicking ""Apply Model"", the things are loading forever: ""Creating Inpainted Image  Analyzing masks...""."
"hello, I have a question,  Is the model  input = image * mask or input = image*mask +1-maskï¼Ÿ Is the hole white or black in imageï¼Ÿ  "
"Hey! Again, great work!  I was wondering why you subtract the bias here before adding it again?       "
"Hello! First off thanks for your paper and code, it's great insight.     I wanted to try using PartialConv2D to replace nn.Conv2D for an upscaling task, to test if that would help reduce the artifacts that appear at the image edges (which I guess is caused by the padding), but when I tried to do it, the first loss of the network became extremely large and on the second iteration the loss tensor was just NaN.    Do I have to take anything else into account when testing replacing Conv2D for partial convolution based padding?    Cheers!"
"!     When I was reading the paper reference 11 is Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with conditional adversarial networks. arXiv preprint (2017).    Are you using a conditional adversarial network?  "
Duplicate
"The ""main.py"" is only training a model. How can I write a test.py to test the image inpainting? Input args?"
"When I check the implementation of `PartialConv2d`, I find you apply updated mask after a normal convolution rather than apply mask to input first:         I'm curious about whether these two ways have difference? It seems applying mask first is faster in intuition."
Really simple and interesting work.    I am wondering when it will be convenient for you to share the imagenet pretrained models.    I hope to try your pd_resnet on the scene parsing tasks.    
"Hi all,    Are the pre-trained models from this study available anywhere? I need the Pretrained 2d pose estimation with resnet50 backbone. If yes, where can I find them?  I would appreciate it if you could help me with this.    Thanks,"
I would like to run the model in C++. Do you have plan to write example how to inference in C++? Or is there any suggestion to config the model and run it in C++.   Thank you
skipping 'cpu_nms.c' Cython extension (up-to-date)  skipping 'gpu_nms.cpp' Cython extension (up-to-date)  
"Based on the paper, this method uses optical flow to propagate joints coordinates set, which part of code that does the method?"
What is the use of  flip_pairs ?
"Traceback (most recent call last):    File ""/home/zhengxin/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/multiprocessing/spawn.py"", line 59, in _wrap      fn(i, *args)    File ""/home/zhengxin/humanpose.pytorch/tools/train.py"", line 223, in main_worker      perf_indicator = validate(    File ""/home/zhengxin/humanpose.pytorch/tools/../lib/core/function.py"", line 204, in validate      name_values, perf_indicator = val_dataset.evaluate(    File ""/home/zhengxin/humanpose.pytorch/tools/../lib/dataset/mpii.py"", line 100, in evaluate      preds = preds[:, :, 0:2]  + 1.0  TypeError: unhashable type: 'slice'    how can i fix it? thanks."
äººä½“æ£€æµ‹å™¨åœ¨å“ªé‡Œï¼Ÿ
ä¸¤ä¸ªé…ç½®æ–‡ä»¶æœ‰ä»€ä¹ˆä¸åŒï¼Ÿåº”è¯¥ä½¿ç”¨å“ªä¸€ä¸ªï¼Ÿ
None
How can I get the correct PCK if I rotate the test_set in MPII? How to deal with the gt.matï¼Ÿ
"when I use   !python3 pose_estimation/valid.py \      --cfg experiments/coco/resnet50/256x192_d256x3_adam_lr1e-3.yaml \      --flip-test \      --model-file models/pytorch/pose_coco/pose_resnet_50_256x192.pth.tar\      --workers 2     to valid the pretrained model,the following error happens!    => classes: ['__background__', 'person']  => num_images: 5000  => load 6352 samples  => fail to read data/coco/images/val2017/000000397133.jpg  Traceback (most recent call last):    File ""pose_estimation/valid.py"", line 168, in    => fail to read data/coco/images/val2017/000000476258.jpg      main()    File ""pose_estimation/valid.py"", line 164, in main      final_output_dir, tb_log_dir)    File ""/content/drive/MyDrive/simplepose /human-pose-estimation.pytorch/pose_estimation/../lib/core/function.py"", line 108, in validate      for i, (input, target, target_weight, meta) in enumerate(val_loader):    File ""/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py"", line 521, in __next__      data = self._next_data()    File ""/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py"", line 1203, in _next_data      return self._process_data(data)    File ""/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py"", line 1229, in _process_data      data.reraise()    File ""/usr/local/lib/python3.7/dist-packages/torch/_utils.py"", line 434, in reraise      raise exception  ValueError: Caught ValueError in DataLoader worker process 0.  Original Traceback (most recent call last):    File ""/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py"", line 287, in _worker_loop      data = fetcher.fetch(index)    File ""/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py"", line 49, in fetch      data = [self.dataset[idx] for idx in possibly_batched_index]    File ""/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py"", line 49, in        data = [self.dataset[idx] for idx in possibly_batched_index]    File ""/content/drive/MyDrive/simplepose /human-pose-estimation.pytorch/pose_estimation/../lib/dataset/JointsDataset.py"", line 80, in __getitem__      raise ValueError('Fail to read {}'.format(image_file))  ValueError: Fail to read data/coco/images/val2017/000000397133.jpg"
"æˆ‘æŒ‰ç…§ä½œè€…çš„è®¾ç½®è‡ªå·±åœ¨MPIIè®­ç»ƒé›†ä¸Šè®­ç»ƒï¼Œä½†ç»“æžœè¿œä½ŽäºŽä½œè€…ç»™å‡ºçš„ç»“æžœï¼Œè¿™æ˜¯æ€Žä¹ˆå›žäº‹å•Šï¼Ÿ  2021-09-30 17:41:40,782 | Arch | Head | Shoulder | Elbow | Wrist | Hip | Knee | Ankle | Mean | Mean@0.1 |  2021-09-30 17:41:40,782 |---|---|---|---|---|---|---|---|---|---|  2021-09-30 17:41:40,782 | 256x256_pose_resnet_50_d256d256d256 | 96.589 | 95.109 | 88.171 | 82.355 | 87.243 | 82.610 | 78.413 | 87.806 | 33.620 |"
å¤§ä½¬ï¼Œèƒ½å¦å°†æ£€æµ‹çš„ä»£ç å‘å¸ƒå‡ºæ¥å‘¢
     æ‚¨å¥½ï¼Œè¯·é—®ä¸‹ï¼Œä»£ç ä¸­centerï¼Œ scaleéƒ¨åˆ†ã€‚  è¯·é—®ä¸‹ï¼š    1.è¿™é‡Œçš„centeræ˜¯æ ‡æ³¨äººçš„ç›®æ ‡æ¡†çš„ä¸­å¿ƒåæ ‡å—ï¼Ÿ  2.å–‚å…¥ç½‘ç»œæ¨¡åž‹çš„è¾“å…¥æ˜¯æ•´å¼ å›¾è¿˜æ˜¯æ ¹æ®ç›®æ ‡æ¡†è£å‰ªå¥½çš„å›¾å‘¢ï¼Ÿ     - å¦‚æžœå–‚å…¥çš„æ˜¯æ•´å¼ å›¾ï¼Œé‚£ä¹ˆcenteræ˜¯æ•´å¼ å›¾çš„ä¸­å¿ƒï¼Œ è¿˜æ˜¯ç›®æ ‡æ¡†å¯¹åº”çš„ä¸­å¿ƒå‘¢ï¼Ÿ     - å¦‚æžœæ˜¯è£å‰ªå¥½çš„å›¾ï¼Œæ˜¯å¦æ˜¯ç›®æ ‡æ¡†å¯¹åº”ä¸­å¿ƒï¼Ÿ     
æ‚¨å¥½ï¼Œè¯·é—®SimpleBaselineæ¨¡åž‹çš„é€Ÿåº¦å¯ä»¥è¾¾åˆ°å¤šå°‘FPSï¼Ÿæ„Ÿè°¢~  
"I am working on another project for which I need to understand the pose of the subject. I hope to use the pre-trained model given, but I can't load the model. Can anyone please guide me?"
"ä½œè€…æ‚¨å¥½ï¼Œå…³äºŽæ•°æ®å¢žå¼ºï¼Œæˆ‘æƒ³è¯·æ•™ä¸‹ã€‚     é…ç½®æ–‡ä»¶ä¸­ï¼Œ`ROT_FACTOR: 40`ã€`SCALE_FACTOR: 0.3`ã€‚  ç„¶åŽæ•°æ®é›†å¤„ç†è¿‡ç¨‹ä¸­     `s = s * np.clip(np.random.randn()*sf + 1, 1 - sf, 1 + sf)`  `r = np.clip(np.random.randn()*rf, -rf*2, rf*2) \                  if random.random() <= 0.6 else 0`    è¯·é—®è¿™é‡Œæ˜¯å¦å°±æ˜¯æŒ‡çš„æ˜¯æ•°æ®éšå³ç¼©æ”¾ï¼ˆ1-0.3, 1+0.3ï¼‰ å³ éšæœºç¼©æ”¾ï¼ˆ[0.7, 1.3]ï¼‰ï¼ŒåŒæ—¶éšæœºæ—‹è½¬ï¼ˆ[-80Â°, 80Â°]ï¼‰å‘¢ï¼Ÿ"
"ä½œè€…æ‚¨å¥½ï¼Œå…³äºŽæ•°æ®å¢žå¼ºï¼Œæˆ‘æƒ³è¯·æ•™ä¸‹ã€‚     é…ç½®æ–‡ä»¶ä¸­ï¼Œ`ROT_FACTOR: 40`ã€`SCALE_FACTOR: 0.3`ã€‚  ç„¶åŽæ•°æ®é›†å¤„ç†è¿‡ç¨‹ä¸­     `s = s * np.clip(np.random.randn()*sf + 1, 1 - sf, 1 + sf)`  `r = np.clip(np.random.randn()*rf, -rf*2, rf*2) \                  if random.random() <= 0.6 else 0`    è¯·é—®è¿™é‡Œæ˜¯å¦å°±æ˜¯æŒ‡çš„æ˜¯æ•°æ®éšå³ç¼©æ”¾ï¼ˆ1-0.3, 1+0.3ï¼‰ å³ éšæœºç¼©æ”¾ï¼ˆ[0.7, 1.3]ï¼‰ï¼ŒåŒæ—¶éšæœºæ—‹è½¬ï¼ˆ[-80Â°, 80Â°]ï¼‰å‘¢ï¼Ÿ"
Is there any mobile implementation? Androis / iOS?   Thanks
"Hi,     I am confusing about heatmap generation.     1. From COCO dataset, the valid of joint are 0 for not in the image, 1 for in the image but not visible, and 2 for in the image and visible. How do you encode this into heatmap? If valid is 0, it will be zero matrix? if it's 1 or 2, it will be heatmap with 1 peak? Am I right?    2. The loss function when training, It's MSE between predicted heatmaps and targeted heatmaps according to the paper. Do I have to multiply with valid? or it's not needed?    All the best,  Than"
"   what is the meaning of ""Mean@0.1"" at the very right of the table"
"Hello, I used the network model in this article to estimate the gaze point, but the loss function is already very small during the training process, but there is no output heat map but the output is more and more like the contour of the original input picture. How is this going"
"Hi,  I download the weight xxx.pth.tar from google drive but when I untar it. It said that it is now archive.    !       Any recommendation here?    Thanks"
Why the scores is higher than that in the paper when I used your code and model? And how do I test the model on COCO test-dev dataset? (Sorry for my terrible English)
None
"è®ºæ–‡4.2å°èŠ‚ä¸­æåˆ°    > It contains 514 videos including 66,374 frames in total, split into 300, 50 and 208 videos for training, validation and test set respectively.     è¯·é—®ä½ ä»¬ä½¿ç”¨çš„æ˜¯posetrack2017çš„æ•°æ®é›†å—ï¼Œè€Œä¸æ˜¯2018çš„ï¼Œæ±‚è§£ç­”ã€‚"
è¯·é—®å¤§å®¶æœ‰äººçŸ¥é“target_weightè®¾ç½®æˆFalseå’ŒTrueçš„åŒºåˆ«æ˜¯ä»€ä¹ˆä¹ˆï¼Ÿ  æˆ‘çœ‹ä»£ç ä¸­è®¾ç½®ä¸ºTureçš„è¯ï¼Œå°±æ„å‘³ç€ä¸è®¡ç®—not visibleçš„å…³èŠ‚ç‚¹çš„é¢„æµ‹æŸå¤±ï¼Œä½†æ„Ÿè§‰åº”è¯¥ä¸æ˜¯è¿™ä¸ªæ„æ€  å¸Œæœ›æœ‰äººå¯ä»¥å¸®åŠ©æˆ‘è§£å†³ä¸€ä¸‹è¿™ä¸ªå›°æƒ‘ï¼Œè°¢è°¢ï¼ï¼ï¼
"I noticed these in the code:     I guessed you've test resNet18. But It seems that there's no pretrained models for resNet18.  So if you have pretrained models for resNet18, can I get one?"
Is it possible to train the network using hand data sets?  To achieve hand posture estimation?
"By following the instruction,  pip install -r requirements.txt    ERROR: Could not find a version that satisfies the requirement EasyDict==1.7 (from -r requirements.txt (line 1)) (from versions: none)  ERROR: No matching distribution found for EasyDict==1.7 (from -r requirements.txt (line 1))"
"Hey, first of all, thanks for making the code available.     I am having a hard time understanding the calculation done on the variable 'scale' at different places. I am implementing the pose estimation of something other than human. Do I still have to use the pixel_std=200? What would happen if my object is smaller than 200px?    What is the purpose of the ""get_affine_transform"" function? from visualization, I can see that it aligns the center of the object with the image. but its dependency on 'scale' is something I am not able to understand. I would appreciate the help here."
"      dst_dir = np.array([0, dst_w * -0.5], np.float32)     dst[1, :] = np.array([dst_w * 0.5, dst_h * 0.5]) + dst_dir      I can get `dst[1, 1]  ==  (dst_h-dst_w)*0.5`  but Why??    why not `dst_dir == np.array([0, dst_h * -0.5], np.float32)`??  "
"By following the instruction,   ` pip install -r requirements.txt`    I still ran into following error:      > ""ERROR: Could not find a version that satisfies the requirement opencv-python==3.4.1.15 (from -r requirements.txt (line 2)) (from versions: 3.4.8.29, 3.4.9.31, 3.4.9.33, 4.1.2.30, 4.2.0.32, 4.2.0.34)  ERROR: No matching distribution found for opencv-python==3.4.1.15 (from -r requirements.txt (line 2))""    It seems that there is no specific version required by the ""requirement.txt""      "
Dose anyone know why divided by 10 here:     
"I checked the  MPII JSON annotations you posted in Google Drive, however, i found there're many annotations are missing from original MPII matlab version. Here're some examples:    original MPII annotations: (red is visible, blue is occluded)  !     your json annotations: (red is visible)  !     original MPII annotations: (red is visible, blue is occluded)  !     your json annotations: (red is visible)  !     Those missing annotation from train / val annotations results in a different PCKh result when evaluate. Could you help to explain this strange difference? thanks.    "
"Hello, can you tell me what the headboxes_src in gt_valid.mat file are supposed to be? Are they the coordinates for the heads keypoints in the dataset?"
"As the subject states: the instructions to fetch and make this repo are simply not geared towards Win10 users, and are both incorrect and very frustrating. There needs to be a section specifically for Windows 10 users, and should have 2 sets of instructions for those that use Git-Bash or Mingw32/64, and those that use PowerShell.    I personally use Git-Bash, and after 25 minutes here still have not been able to 'make' the library.  In my case:       Which is funny because from that same folder:       Which clearly shows that nvcc is accessible, and therefore there are adequate env vars.       This is the kind of thing that causes users to abandon this and move on to another project.  Please address this."
please help me~
"Thanks for great repo!    I noticed the spatial resolution is reduced by large factor (in one network it's 32) with MaxPooling and Conv(stride>1) which decreases the spatial resolution to less than the output map resolution, this then ""upsampled"" with transposed convolution. What is the reasoning behind this? Was this method induced purely from empirical methods?  In practice one forces the network to store spatial resolution in the feature dimension, why not just downsample less with maxpooling and less stride instead of adding transposed convolution at the end?"
"@leoxiaobin   first thank you for your job,but i try to use warpAffine & gaussian_filter & maximum_filter to draw the result ,show offset error,could you tell how to convert the hm to point(i just show in warpAffine img ),i try to :  new_img = input = cv2.warpAffine(              data_numpy,              trans,              (int(self.image_size[0]), int(self.image_size[1])),              flags=cv2.INTER_LINEAR)            for i in range(self.num_joints):              if joints_vis[i, 0] > 0.0:                  print(affine_transform(joints[i, 0:2], trans))                  cv2.circle(new_img, center=(int(affine_transform(joints[i, 0:2], trans)[0]), int(affine_transform(joints[i, 0:2], trans)[1])), color=(0, 0, 255), radius=5)                  joints[i, 0:2] = affine_transform(joints[i, 0:2], trans)     to show the gt ,is right"
why pred * 4? in function.py   
"by the peopleo bbox,crop the people from the original imgs???   and only put the people in the dnn??"
"    File ""C:\Users\Jackson\Anaconda3\envs\human_pose_estimation\lib\site-packages\torchvision\models\segmentation\segmentation.py"", line 1, in        from .._utils import IntermediateLayerGetter      File ""C:\Users\Jackson\Anaconda3\envs\human_pose_estimation\lib\site-packages\torchvision\models\_utils.py"", line 7, in        class IntermediateLayerGetter(nn.ModuleDict):    AttributeError: module 'torch.nn' has no attribute 'ModuleDict'"
"I try to train simplebaseline(resnet18) in caffe framework. However, the loss decreases so slowly and the prediction is a little bad.    "
"Hi, thank you for your well done jobs. I want test the caffe-style model on val2017. where I can download  the caffe-style model you have trained ,such as ""256x192_pose_resnet_50_caffe "".    Thank you for your help"
None
**Please Update the readme and highlight the validataion method whether use the GT bbox during the training**.  This bothers lot of people.       
"# When I excuted the following cmd  `python3 pose_estimation/train.py     --cfg experiments/mpii/resnet50/256x256_d256x3_adam_lr1e-3.yaml  `  # But got error:    => loading pretrained model models/pytorch/imagenet/resnet50-19c8e357.pth  Traceback (most recent call last):    File ""pose_estimation/train.py"", line 206, in        main()    File ""pose_estimation/train.py"", line 112, in main      writer_dict['writer'].add_graph(model, (dump_input, ), verbose=False)    File ""/home/rememberoh/anaconda3/envs/HPE/lib/python3.6/site-packages/tensorboardX/writer.py"", line 738, in add_graph      self._get_file_writer().add_graph(graph(model, input_to_model, verbose, **kwargs))    File ""/home/rememberoh/anaconda3/envs/HPE/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py"", line 255, in graph      list_of_nodes, node_stats = parse(graph, args)    File ""/home/rememberoh/anaconda3/envs/HPE/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py"", line 211, in parse      nodes_py.append(NodePyIO(node, 'input'))    File ""/home/rememberoh/anaconda3/envs/HPE/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py"", line 63, in __init__      super(NodePyIO, self).__init__(node_cpp, methods_IO)    File ""/home/rememberoh/anaconda3/envs/HPE/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py"", line 58, in __init__      setattr(self, m, getattr(node_cpp, m)())  AttributeError: 'torch._C.Value' object has no attribute 'uniqueName'    # I dont know what was happening    # And here is full output:  /home/rememberoh/anaconda3/envs/HPE/lib/human-pose-estimation.pytorch/pose_estimation/../lib/core/config.py:161: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read   for full details.    exp_config = edict(yaml.load(f))  => creating output/mpii/pose_resnet_50/256x256_d256x3_adam_lr1e-3  => creating log/mpii/pose_resnet_50/256x256_d256x3_adam_lr1e-3_2019-08-13-13-32  Namespace(cfg='experiments/mpii/resnet50/256x256_d256x3_adam_lr1e-3.yaml', frequent=100, gpus=None, workers=None)  {'CUDNN': {'BENCHMARK': True, 'DETERMINISTIC': False, 'ENABLED': True},   'DATASET': {'DATASET': 'mpii',               'DATA_FORMAT': 'jpg',               'FLIP': True,               'HYBRID_JOINTS_TYPE': '',               'ROOT': 'data/mpii/',               'ROT_FACTOR': 30,               'SCALE_FACTOR': 0.25,               'SELECT_DATA': False,               'TEST_SET': 'valid',               'TRAIN_SET': 'train'},   'DATA_DIR': '',   'DEBUG': {'DEBUG': False,             'SAVE_BATCH_IMAGES_GT': True,             'SAVE_BATCH_IMAGES_PRED': True,             'SAVE_HEATMAPS_GT': True,             'SAVE_HEATMAPS_PRED': True},   'GPUS': '0',   'LOG_DIR': 'log',   'LOSS': {'USE_TARGET_WEIGHT': True},   'MODEL': {'EXTRA': {'DECONV_WITH_BIAS': False,                       'FINAL_CONV_KERNEL': 1,                       'HEATMAP_SIZE': array([64, 64]),                       'NUM_DECONV_FILTERS': [256, 256, 256],                       'NUM_DECONV_KERNELS': [4, 4, 4],                       'NUM_DECONV_LAYERS': 3,                       'NUM_LAYERS': 50,                       'SIGMA': 2,                       'TARGET_TYPE': 'gaussian'},             'IMAGE_SIZE': array([256, 256]),             'INIT_WEIGHTS': True,             'NAME': 'pose_resnet',             'NUM_JOINTS': 16,             'PRETRAINED': 'models/pytorch/imagenet/resnet50-19c8e357.pth',             'STYLE': 'pytorch'},   'OUTPUT_DIR': 'output',   'PRINT_FREQ': 100,   'TEST': {'BATCH_SIZE': 32,            'BBOX_THRE': 1.0,            'COCO_BBOX_FILE': '',            'FLIP_TEST': False,            'IMAGE_THRE': 0.0,            'IN_VIS_THRE': 0.0,            'MODEL_FILE': '',            'NMS_THRE': 1.0,            'OKS_THRE': 0.5,            'POST_PROCESS': True,            'SHIFT_HEATMAP': True,            'USE_GT_BBOX': False},   'TRAIN': {'BATCH_SIZE': 32,             'BEGIN_EPOCH': 0,             'CHECKPOINT': '',             'END_EPOCH': 140,             'GAMMA1': 0.99,             'GAMMA2': 0.0,             'LR': 0.001,             'LR_FACTOR': 0.1,             'LR_STEP': [90, 120],             'MOMENTUM': 0.9,             'NESTEROV': False,             'OPTIMIZER': 'adam',             'RESUME': False,             'SHUFFLE': True,             'WD': 0.0001},   'WORKERS': 4}  => init deconv weights from normal distribution  => init 0.weight as normal(0, 0.001)  => init 0.bias as 0  => init 1.weight as 1  => init 1.bias as 0  => init 3.weight as normal(0, 0.001)  => init 3.bias as 0  => init 4.weight as 1  => init 4.bias as 0  => init 6.weight as normal(0, 0.001)  => init 6.bias as 0  => init 7.weight as 1  => init 7.bias as 0  => init final conv weights from normal distribution  => init 8.weight as normal(0, 0.001)  => init 8.bias as 0  => loading pretrained model models/pytorch/imagenet/resnet50-19c8e357.pth  Traceback (most recent call last):    File ""pose_estimation/train.py"", line 206, in        main()    File ""pose_estimation/train.py"", line 112, in main      writer_dict['writer'].add_graph(model, (dump_input, ), verbose=False)    File ""/home/rememberoh/anaconda3/envs/HPE/lib/python3.6/site-packages/tensorboardX/writer.py"", line 738, in add_graph      self._get_file_writer().add_graph(graph(model, input_to_model, verbose, **kwargs))    File ""/home/rememberoh/anaconda3/envs/HPE/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py"", line 255, in graph      list_of_nodes, node_stats = parse(graph, args)    File ""/home/rememberoh/anaconda3/envs/HPE/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py"", line 211, in parse      nodes_py.append(NodePyIO(node, 'input'))    File ""/home/rememberoh/anaconda3/envs/HPE/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py"", line 63, in __init__      super(NodePyIO, self).__init__(node_cpp, methods_IO)    File ""/home/rememberoh/anaconda3/envs/HPE/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py"", line 58, in __init__      setattr(self, m, getattr(node_cpp, m)())  AttributeError: 'torch._C.Value' object has no attribute 'uniqueName'"
"Thanks for your code.   When I use the backbone pose_resnet to get the feature map, I found the output feature map size(96x72) is four times smaller than the input(384x288) for the backbone.  When I count the stride, the conv stride 2x2 is more than deconv's stride.  Is the any error I made, or if I want the feature map size is same as the input size, What should I do?    @leoxiaobin "
I plan to implement the code myself and wanted to ask if there was any information about inference time. I couldn't find anything about it.
"Should I follow the guidelines in the paper ""Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour""? "
"when i try to test on test2017 dataset, i got the following information:   => num_images: 20288  => load 0 samples  => Writing results json to output/coco/pose_resnet_50/256x192_d256x3_adam_lr1e-3/results/keypoints_test_results.json  | Arch | Null |  |---|---|  | 256x192_pose_resnet_50_d256d256d256 | 0.000 |   what can i do?"
"When I run the training code, only gpu '0' is loaded.    How can I run with all 4 gpus?"
"/home/gyc/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.    warnings.warn(warning.format(ret))  => load 2958 samples  => fail to read data/mpii/images/005808361.jpg  => fail to read data/mpii/images/080936116.jpg  => fail to read data/mpii/images/024240973.jpg  => fail to read data/mpii/images/027405522.jpg  Traceback (most recent call last):  => fail to read data/mpii/images/068556529.jpg    File ""/home/gyc/POSE_ROOT/pose_estimation/valid.py"", line 166, in    => fail to read data/mpii/images/000933162.jpg  => fail to read data/mpii/images/018143835.jpg      main()    File ""/home/gyc/POSE_ROOT/pose_estimation/valid.py"", line 162, in main      final_output_dir, tb_log_dir)    File ""/home/gyc/POSE_ROOT/pose_estimation/../lib/core/function.py"", line 108, in validate  => fail to read data/mpii/images/043052934.jpg  => fail to read data/mpii/images/007896916.jpg      for i, (input, target, target_weight, meta) in enumerate(val_loader):    File ""/home/gyc/anaconda3/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 336, in __next__      return self._process_next_batch(batch)    File ""/home/gyc/anaconda3/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 357, in _process_next_batch      raise batch.exc_type(batch.exc_msg)  ValueError: Traceback (most recent call last):    File ""/home/gyc/anaconda3/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 106, in _worker_loop      samples = collate_fn([dataset[i] for i in batch_indices])    File ""/home/gyc/anaconda3/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 106, in        samples = collate_fn([dataset[i] for i in batch_indices])    File ""/home/gyc/POSE_ROOT/pose_estimation/../lib/dataset/JointsDataset.py"", line 79, in __getitem__      raise ValueError('Fail to read {}'.format(image_file))  ValueError: Fail to read data/mpii/images/005808361.jpg"
Hi. Thanks for your great work  it seems that  your pretrained model have to be used with GPU for inference time as well. I'm wondering if it is possible to use pretrained model for inference only with CPU?
"I just follow the step to test coco dataset,the output is person's keypoints rather than connected joints.I have no too much experience about pose estimation. Don't you put out the code of pose estimation ? or where should I get the right output of pose estimation?  Thanks for a lot.  This is one of the keypoints output:  !   "
"Hi all, i have a problem when connect cuda with the codes by running ""make"" in (ROOT)/lib.    They search cuda path at the host (e.g. /usr/local/cuda-9.0/), but in anaconda environment, we dont have the path at the host. The simplest way to solve this problem is just installing cuda at the host, but i don't want that.     Is there any tips to connect cuda in anaconda when running this codes??"
"How can I disable cudnn for batch_norm in Windows 10?  I tried to run the code with out doing it, I got this output:   File ""C:\Users\pc\Desktop\pose-project\human-pose-estimation.pytorch-master\pose_estimation\..\lib\core\function.py"", line 52, in train      loss.backward()    File ""C:\Users\pc\Anaconda3\envs\tensorflow\lib\site-packages\torch\tensor.py"", line 102, in backward      torch.autograd.backward(self, gradient, retain_graph, create_graph)    File ""C:\Users\pc\Anaconda3\envs\tensorflow\lib\site-packages\torch\autograd\__init__.py"", line 90, in backward      allow_unreachable=True)  # allow_unreachable flag  RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn     In the ""main"" function I added -""with torch.no_grad():"" to solve a memory error.  Maybe that is the problem?   Any help will be appreciated."
"Your test.json contains 7247 test samples while the original .mat file contains 11731 test samples. Can you explain the discrepancy? In effect, can you tell me how to start from this codebase and make a submission to leader board for single person pose?"
"How can I use this pose estimation model trained on COCO to get keypoint coordinates of my own dataset?   In other words, so just use pre-trained model trained on COCO dataset to perform inferencing."
"Hi,    I have encountered with this error when trying to make the libs:    error: command '/usr/bin/nvcc' failed with exit status 1      I guess it has something to do with CUDA, so could you please identify your CUDA, cudnn, and gcc versions?     Thanks in advance."
"I was trying to reproduce your algorithm of the tracking part. But there's a really specific detail that I can't figure out how to manage to complete. You got the bounding boxes from joints propagation and from the detector like R-FCN. However when you do the NMS operation, where do you get the confidence score of the bounding boxes from joints propagation?"
"/root/anaconda3/bin/python /home/dell/ä¸‹è½½/human-pose-estimation.pytorch-master/pose_estimation/valid.py --cfg ../experiments/mpii/resnet152/256x256_d256x3_adam_lr1e-3.yaml --flip-test --model-file ../models/pytorch/pose_mpii/pose_resnet_152_256x256.pth.tar  => creating output/mpii/pose_resnet_152/256x256_d256x3_adam_lr1e-3  => creating log/mpii/pose_resnet_152/256x256_d256x3_adam_lr1e-3_2019-03-20-21-12  Namespace(cfg='../experiments/mpii/resnet152/256x256_d256x3_adam_lr1e-3.yaml', coco_bbox_file=None, flip_test=True, frequent=100, gpus=None, model_file='../models/pytorch/pose_mpii/pose_resnet_152_256x256.pth.tar', post_process=False, shift_heatmap=False, use_detect_bbox=False, workers=None)  {'CUDNN': {'BENCHMARK': True, 'DETERMINISTIC': False, 'ENABLED': True},   'DATASET': {'DATASET': 'mpii',               'DATA_FORMAT': 'jpg',               'FLIP': True,               'HYBRID_JOINTS_TYPE': '',               'ROOT': 'data/mpii/',               'ROT_FACTOR': 30,               'SCALE_FACTOR': 0.25,               'SELECT_DATA': False,               'TEST_SET': 'valid',               'TRAIN_SET': 'train'},   'DATA_DIR': '',   'DEBUG': {'DEBUG': False,             'SAVE_BATCH_IMAGES_GT': True,             'SAVE_BATCH_IMAGES_PRED': True,             'SAVE_HEATMAPS_GT': True,             'SAVE_HEATMAPS_PRED': True},   'GPUS': '0',   'LOG_DIR': 'log',   'LOSS': {'USE_TARGET_WEIGHT': True},   'MODEL': {'EXTRA': {'DECONV_WITH_BIAS': False,                       'FINAL_CONV_KERNEL': 1,                       'HEATMAP_SIZE': array([64, 64]),                       'NUM_DECONV_FILTERS': [256, 256, 256],                       'NUM_DECONV_KERNELS': [4, 4, 4],                       'NUM_DECONV_LAYERS': 3,                       'NUM_LAYERS': 152,                       'SIGMA': 2,                       'TARGET_TYPE': 'gaussian'},             'IMAGE_SIZE': array([256, 256]),             'INIT_WEIGHTS': True,             'NAME': 'pose_resnet',             'NUM_JOINTS': 16,             'PRETRAINED': 'models/pytorch/imagenet/resnet152-b121ed2d.pth',             'STYLE': 'pytorch'},   'OUTPUT_DIR': 'output',   'PRINT_FREQ': 100,   'TEST': {'BATCH_SIZE': 32,            'BBOX_THRE': 1.0,            'COCO_BBOX_FILE': '',            'FLIP_TEST': True,            'IMAGE_THRE': 0.0,            'IN_VIS_THRE': 0.0,            'MODEL_FILE': '../models/pytorch/pose_mpii/pose_resnet_152_256x256.pth.tar',            'NMS_THRE': 1.0,            'OKS_THRE': 0.5,            'POST_PROCESS': True,            'SHIFT_HEATMAP': True,            'USE_GT_BBOX': False},   'TRAIN': {'BATCH_SIZE': 32,             'BEGIN_EPOCH': 0,             'CHECKPOINT': '',             'END_EPOCH': 140,             'GAMMA1': 0.99,             'GAMMA2': 0.0,             'LR': 0.001,             'LR_FACTOR': 0.1,             'LR_STEP': [90, 120],             'MOMENTUM': 0.9,             'NESTEROV': False,             'OPTIMIZER': 'adam',             'RESUME': False,             'SHUFFLE': True,             'WD': 0.0001},   'WORKERS': 4}  => loading model from ../models/pytorch/pose_mpii/pose_resnet_152_256x256.pth.tar  /root/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.    warnings.warn(warning.format(ret))  Traceback (most recent call last):    File ""/home/dell/ä¸‹è½½/human-pose-estimation.pytorch-master/pose_estimation/valid.py"", line 165, in        main()    File ""/home/dell/ä¸‹è½½/human-pose-estimation.pytorch-master/pose_estimation/valid.py"", line 148, in main      normalize,    File ""/home/dell/ä¸‹è½½/human-pose-estimation.pytorch-master/pose_estimation/../lib/dataset/mpii.py"", line 33, in __init__      self.db = self._get_db()    File ""/home/dell/ä¸‹è½½/human-pose-estimation.pytorch-master/pose_estimation/../lib/dataset/mpii.py"", line 45, in _get_db      with open(file_name) as anno_file:  FileNotFoundError: [Errno 2] No such file or directory: 'data/mpii/annot/valid.json'    Process finished with exit code 1"
"This code is in core/loss.py:  `heatmaps_pred = output.reshape((batch_size, num_joints, -1)).split(1, 1)`    I was just wondering what is the `split(1,1)` for? I From my understanding it splits the tensor into equally sized tensors along the 1 axis, but I am not understanding why split into tensors of size 1? Hope somebody can help. Thanks"
"hello,can you sharing ctracking code?"
"First of all , thank for you awesome project.       For to use the pose estimator on real-time, I insert YOLOv3 to get human bbox, now I want test the AP accuracy of  human-pose-estimation with new human detector's pose.     but when I set the confidence of YOLOv3 to 0.0, I get 687810 bboxes, like this          when set confidence to 0.1 , like this    | Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |  |---|---|---|---|---|---|---|---|---|---|---|  | 384x288_pose_resnet_50_d256d256d256 | 0.107 | 0.137 | 0.112 | 0.047 | 0.192 | 0.113 | 0.145 | 0.116 | 0.050 | 0.199 |            I can't understand why you can get 104125 bboxes, how do you set the threconfidence?     could you please help me ? Thanks in advance."
"Hi,    We have re-implemented the estimation model in  . We also achieve slightly higher AP by using pre-trained ResNet with higher accuracy:       Besides, you mentioned in   that the tracking code is implemented with MXNet. Do you think there's a chance that you will release it? GluonCV is also based on MXNet. With your tracking implementation as a starting point, we may provide a smooth and unified pipeline of pose tracking in GluonCV.    Best,  Tong"
"   I review the code and find the pixel_std represents the std of human bbox area, right?  But why we need to normalize the bbox scale and set it to 200? "
"Hi, I recently try to re-implement the flow-based propagation section in your paper.  I use the FlowNet2S you mentioned, and I get the flow information between two adjacent frames on posetrack2018 val dataset. The question is the inference size must be divisible by 64. Therefore, the shape of flow is not corresponding to the shape of images. In order to get the same results reported, I wonder how to solve this problem and the inference time for ""With Joint Propagation"" on posetrack2018 val dataset.  Thanks a lot!"
"I am planning to tune the pretrained coco model on a tagged custom dataset. In the .yaml file, I can see we can give resent50 as pretrained model. Is there any way I can use the coco pretrained model to do fine tuning on my local dataset?"
"@leoxiaobin  Hi, when I was training along with validation under my own dataset, when do validation occurs this kind of errors:  AssertionError: Results do not correspond to current coco set  Is there some wrong with my own data?"
"Hi,  I am trying to perform inference by using the pre-trained model (resnet150_coco). Need help on the following:  1. I am not able to map key-points to the original body part labels in the coco dataset format. Can you tell me the output format that is produced from the model (for the keypoints) so that I can Map keypoints at inference to their corresponding body part label.  2. Do we need to compulsorily fix bounding boxes for each person to get the keypoints during inference.  3. For multi-person keypoint generation, do we have to infer the frame multiple times to get the key-points or can we get them only in one inference of the image. If one inference is required then what is the data format the model outputs to differentiate people and their key-points.     Any help is greatly appreciated."
"I experiment your framework with different backbone. Surprisingly, the performance of resnet34 is quite close to resnet50. Renest34 have mAP of about 0.705 (and 0.893 AP@0.5) on 2017 coco keypoints for 256x192, tested using ground truth box as input and without flip.    However, for mobilenetv1,v2 and mnasnet, their results are very bad, even worse then resnet18. Do you have any idea why this is so?     mobilenetv1,v2 and mnasnet has only mAP of about 0.648 on coco keypoint. Resnet18 is about 0. 657.    On imagenet, mobilenetv2(top-1 28%) and mnasnet  (top-1 26%) has performance much better than resnet18 (top-1 30%), and is comparable to Resnet 34(top-1 27%) . Resnet50 is top-1 24%  "
"here is log of error:  --------------------------------------------  /home/boyun/anaconda3/envs/tc0.40/bin/python /home/boyun/PycharmProjects/Humanpose/MultiPose/MSRA_BASELINE/human-pose-estimation.pytorch/pose_estimation/train.py  => creating output/coco/pose_resnet_101/384x288_d256x3_adam_lr1e-3  Namespace(cfg='experiments/coco/resnet101/384x288_d256x3_adam_lr1e-3.yaml', frequent=5, gpus=None, workers=None)  => creating log/coco/pose_resnet_101/384x288_d256x3_adam_lr1e-3_2019-01-24-14-54  {'CUDNN': {'BENCHMARK': True, 'DETERMINISTIC': False, 'ENABLED': True},   'DATASET': {'DATASET': 'coco',               'DATA_FORMAT': 'jpg',               'FLIP': True,               'HYBRID_JOINTS_TYPE': '',               'ROOT': 'data/coco/',               'ROT_FACTOR': 40,               'SCALE_FACTOR': 0.3,               'SELECT_DATA': False,               'TEST_SET': 'val2017',               'TRAIN_SET': 'train2017'},   'DATA_DIR': '',   'DEBUG': {'DEBUG': True,             'SAVE_BATCH_IMAGES_GT': True,             'SAVE_BATCH_IMAGES_PRED': True,             'SAVE_HEATMAPS_GT': True,             'SAVE_HEATMAPS_PRED': True},   'GPUS': '0',   'LOG_DIR': 'log',   'LOSS': {'USE_TARGET_WEIGHT': True},   'MODEL': {'EXTRA': {'DECONV_WITH_BIAS': False,                       'FINAL_CONV_KERNEL': 1,                       'HEATMAP_SIZE': array([72, 96]),                       'NUM_DECONV_FILTERS': [256, 256, 256],                       'NUM_DECONV_KERNELS': [4, 4, 4],                       'NUM_DECONV_LAYERS': 3,                       'NUM_LAYERS': 101,                       'SIGMA': 3,                       'TARGET_TYPE': 'gaussian'},             'IMAGE_SIZE': array([288, 384]),             'INIT_WEIGHTS': True,             'NAME': 'pose_resnet',             'NUM_JOINTS': 17,             'PRETRAINED': 'models/pytorch/imagenet/resnet101-5d3b4d8f.pth',             'STYLE': 'pytorch'},   'OUTPUT_DIR': 'output',   'PRINT_FREQ': 5,   'TEST': {'BATCH_SIZE': 1,            'BBOX_THRE': 1.0,            'COCO_BBOX_FILE': 'data/coco/person_detection_results/COCO_val2017_detections_AP_H_56_person.json',            'FLIP_TEST': False,            'IMAGE_THRE': 0.0,            'IN_VIS_THRE': 0.2,            'MODEL_FILE': '',            'NMS_THRE': 1.0,            'OKS_THRE': 0.9,            'POST_PROCESS': True,            'SHIFT_HEATMAP': True,            'USE_GT_BBOX': True},   'TRAIN': {'BATCH_SIZE': 6,             'BEGIN_EPOCH': 0,             'CHECKPOINT': '',             'END_EPOCH': 140,             'GAMMA1': 0.99,             'GAMMA2': 0.0,             'LR': 0.001,             'LR_FACTOR': 0.1,             'LR_STEP': [90, 120],             'MOMENTUM': 0.9,             'NESTEROV': False,             'OPTIMIZER': 'adam',             'RESUME': False,             'SHUFFLE': True,             'WD': 0.0001},   'WORKERS': 4}  => init deconv weights from normal distribution  => init 0.weight as normal(0, 0.001)  => init 0.bias as 0  => init 1.weight as 1  => init 1.bias as 0  => init 3.weight as normal(0, 0.001)  => init 3.bias as 0  => init 4.weight as 1  => init 4.bias as 0  => init 6.weight as normal(0, 0.001)  => init 6.bias as 0  => init 7.weight as 1  => init 7.bias as 0  => init final conv weights from normal distribution  => init 8.weight as normal(0, 0.001)  => init 8.bias as 0  => loading pretrained model models/pytorch/imagenet/resnet101-5d3b4d8f.pth  Traceback (most recent call last):    File ""/home/boyun/PycharmProjects/Humanpose/MultiPose/MSRA_BASELINE/human-pose-estimation.pytorch/pose_estimation/train.py"", line 208, in        main()    File ""/home/boyun/PycharmProjects/Humanpose/MultiPose/MSRA_BASELINE/human-pose-estimation.pytorch/pose_estimation/train.py"", line 114, in main      writer_dict['writer'].add_graph(model, (dump_input, ), verbose=False)    File ""/home/boyun/anaconda3/envs/tc0.40/lib/python3.6/site-packages/tensorboardX/writer.py"", line 566, in add_graph      self.file_writer.add_graph(graph(model, input_to_model, verbose))    File ""/home/boyun/anaconda3/envs/tc0.40/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py"", line 171, in graph      from torch.onnx.utils import OperatorExportTypes  ImportError: cannot import name 'OperatorExportTypes'    Process finished with exit code 1  ----------------------------------------------------------------------------  when i remove `writer_dict['writer'].add_graph(model, (dump_input, ), verbose=False)`ï¼Œthe train well run. so i guess that the tensorboardX maybe exist some bug. my config is ""tensorboardX=1.6"". but i find the requirement of this repo tensorboradx only need to bigger 1.2ã€‚ what should i do to debug?   Any suggestion will be appreciated."
"I would like to ask you if your proposed smiple baselines network is top-down or bottom-up. I think it should be top-down. I don't know if my understanding is correct. If it is correct, I would like to ask you how to Change it to bottom-up, which is the same joint point of multiple human bodies in a heatmap  .Thank you "
"I use python 3.6.    In step Valid on COCO val2017 using pretrained models,  I did      sudo python3 pose_estimation/valid.py --cfg experiments/coco/resnet50/256x192_d256x3_adam_lr1e-3.yaml --flip-test --model-file models/pytorch/pose_coco/pose_resnet_50_256x192.pth.tar    and the error occurs    **=> creating output/coco/pose_resnet_50/256x192_d256x3_adam_lr1e-3  => creating log/coco/pose_resnet_50/256x192_d256x3_adam_lr1e-3_2019-01-14-10-45  Namespace(cfg='experiments/coco/resnet50/256x192_d256x3_adam_lr1e-3.yaml', coco_bbox_file=None, flip_test=True, frequent=100, gpus=None, model_file='models/pytorch/pose_coco/pose_resnet_50_256x192.pth.tar', post_process=False, shift_heatmap=False, use_detect_bbox=False, workers=None)  {'CUDNN': {'BENCHMARK': True, 'DETERMINISTIC': False, 'ENABLED': True},   'DATASET': {'DATASET': 'coco',               'DATA_FORMAT': 'jpg',               'FLIP': True,               'HYBRID_JOINTS_TYPE': '',               'ROOT': 'data/coco/',               'ROT_FACTOR': 40,               'SCALE_FACTOR': 0.3,               'SELECT_DATA': False,               'TEST_SET': 'val2017',               'TRAIN_SET': 'train2017'},   'DATA_DIR': '',   'DEBUG': {'DEBUG': True,             'SAVE_BATCH_IMAGES_GT': True,             'SAVE_BATCH_IMAGES_PRED': True,             'SAVE_HEATMAPS_GT': True,             'SAVE_HEATMAPS_PRED': True},   'GPUS': '0',   'LOG_DIR': 'log',   'LOSS': {'USE_TARGET_WEIGHT': True},   'MODEL': {'EXTRA': {'DECONV_WITH_BIAS': False,                       'FINAL_CONV_KERNEL': 1,                       'HEATMAP_SIZE': array(   "
"this is not an issue but rather an observation. What i did:    - use ground truth box in json as input instance box  - disable flip test  - disable oks_nms instance re-scoring  - simply set output=target in def validate(config, val_loader, val_dataset, ...) in function.py    i get the results:     Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.975   Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.990   Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.988   Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.971   Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.980   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.988   Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.999   Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.996   Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.984   Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.996      ---  this represent results of a perfect convolution net which have predictions same as the target ground truth. Even so, there is a drop of 0.025 in AP.    This shows that there is some still room of improvement if you have better post processing methods or better target definition.          "
"@leoxiaobin       Hi, I've found that you exchanged the stride size of conv1 and conv2 in the first Bottleneck of layer2,  layer3, layer4, just to make sure with you(since, a paper also refers this operation), thanks"
"Hi, thanks for your code!  But I have a question about flip test. I don't know why here you do a shift on output_flipped.  The comments says that ""feature is not aligned, shift flipped heatmap for higher accuracy"", what's the meaning of this opreration?    "
"Thanks for your excellent works!  I have a question about the result you show in the Fig3. in the paper ""Simple Baselines for Human Pose Estimation and Tracking.""    In the lower tracking result. person with ID 12 is occluded by the ID 4 in the frame 5,how it can be tracked with the same ID after it's occlusion? Did you mentioned the mechanism in the paper?   Is it in the paragraph 3.3?  <img width=""658"" alt=""20190102142258"" src=""     thanks~"
"Hi,    Is there a way to disable cudnn for batch norm on pytorch 1.0?"
"I can't run the code on windows and I also can't install Linux right now due to personal issues, I am currently just backtracking everything in your code right now trying to rewrite it so it would work for my system. I am having a hard time figuring out the input data tensor shape and the correct output shape. I have managed to recreate the model in pytorch and I assumed the input data shape for the 50 layer resnet is [B, 3, 384, 288] for coco and [B, C, 256, 256] with mpii is this correct? If not can you please tell me what the input and output tensor shapes are?  "
"I want like to evaluate the accuracy of the pretrained model with a subset of images of MPII (let's call these images subset A). I have the annotations of subset A in a json file.    I'm a bit confused on how to specifically evaluate the accuracy of the pretrained model on subset A . Where should I place the image files and the annotation json of the subset I want to evaluate? What lines of code elsewhere should I modify?    Right now, I have the following file structure inside ${POSE_ROOT}/data in which I have added the 'subsetA' folder with its corresponding images and annotations (in subsetA/annot/ there only exists a valid.json file the annotations).    data  â””â”€â”€ subsetA  â”‚   â”œâ”€â”€ annot   ------------valid.json  â”‚   â””â”€â”€ images  ------------(subsetA images are here)  â””â”€â”€ mpii      â”œâ”€â”€ annot      â””â”€â”€ images    Specific instructions would be very helpful and greatly appreciated!"
I am looking to use this for a project. Would we be able to interface this via ros?
"I followed the instructions and I have ""make"" on windows 10 from GNU I believe. Anyways the error I got is        Not sure what file I am missing."
"Hi, @leoxiaobin   Thanks for sharing your excellent work! It have very good results.I am curious about which your b**ounding box detector.**     I evaluate your valided bbox results, it's get **56.8** performance in detection task, and I get **49.5** with maskrcnn detector. **Would you give your refered codes or repo, and have you train faster RCNN codes?**    Thanks a lot!     "
"In your paper, you used pretrained model on COCO and then fine-tuned on Posetrack. But their points format are different, so there must be a conversion otherwise the model could not be used. How did you convert COCO format to Posetrack 17 in your experiment?"
Hello! Thank you for good job. Could you tell me the detect speech on COCO test dataset per image?
"Traceback (most recent call last):    File ""pose_estimation/valid.py"", line 32, in        import dataset    File ""/home/liangjian/pose/pose_estimation/../lib/dataset/__init__.py"", line 12, in        from .coco import COCODataset as coco    File ""/home/liangjian/pose/pose_estimation/../lib/dataset/coco.py"", line 23, in        from nms.nms import oks_nms    File ""/home/liangjian/pose/pose_estimation/../lib/nms/nms.py"", line 13, in        from .cpu_nms import cpu_nms  ImportError: dynamic module does not define module export function (PyInit_cpu_nms)      any clue?"
"Hi ,   Thanks for your great paper,  I am also interested in your tracking code,  How can we access to that?    Thanks,"
"I read your code carefully, and implement with following code.  But I still get the wrong result.  Could you help me?                        (256, 256, 3)      (16, 64, 64)                 (64, 64, 3)      10 46      8 37      27 29      13 37      33 7      30 7      25 18      17 31      31 22      29 21      15 32      12 51      23 15      36 18      13 40      12 41         !          "
This is my code about using your model to predict pose of my image.  Is there any wrong with it ?  Can you provide me a right one ?                    torch.Size(   
"Thanks for you pose estimation code. According to your paper,your method of tracking is also great, so will you release the tracking code?"
There are important files that Microsoft projects should all have that are not present in this repository. A pull request has been opened to add the missing file(s). When the pr is merged this issue will be closed automatically.  Microsoft teams can   within the open source guidance available internally.    
  from .cpu_nms import cpu_nms  ModuleNotFoundError: No module named 'nms.cpu_nms'  how to deal with ?
"Hi, thank for your excellent job!  But, I realize that time for post processing is too large. With card geforce 940MX, time an image forward to model is about 0.009s but time for post processing is about 0.033s. How can I modify the post processing for reducing inference time (accept decrease accuracy and retrain model).  Thanks very much!"
"From what I can understand, the paper ""Integral Human Pose Regression"" is also your group's work. According to that paper, Integral regression has a better result than heatmaps. So, why don't you use it on this paper or HRNet?"
"I want to test model on test-dev set, but the code pose_estimation/valid.py is only for testing on validation set (5000 images):    So, I have a question: How to test the model on COCO test-dev set 2017 to submit the json file to codalab server?  I'm looking forward from you. Thank you for your great work!"
"Can I ask why the input channel issue??   **RuntimeError: weight of size [64, 3, 7, 7], expected input[32, 256, 192, 3] to have 3 channels, but got 256 channels instead**     "
"My env setting is Pytorch 0.4.0 + CUDA 9.0, I disabled the CUDNN batch norm. However, it seems that there is a GPU memory leak. Does anyone encounter this problem? "
"  !     thx for your nice work! I'm wondering what's the meaning of 'time'.And how could i calculate the FPS by your log information, thx a lot!"
How to do itï¼Ÿ
"I meet the same question postde in #44 but little different.  > Traceback (most recent call last):    File ""pose_estimation/train.py"", line 37, in        import dataset    File ""/home/gyh/human-pose-estimaion/pose_estimation/../lib/dataset/__init__.py"", line 12, in        from .coco import COCODataset as coco    File ""/home/gyh/human-pose-estimaion/pose_estimation/../lib/dataset/coco.py"", line 23, in        from nms.nms import oks_nms    File ""/home/gyh/human-pose-estimaion/pose_estimation/../lib/nms/nms.py"", line 14, in        from .gpu_nms import gpu_nms  ImportError: libcudart.so.9.0: cannot open shared object file: No such file or directory    When I follow the step 5, the output is   > cd nms; python setup.py build_ext --inplace; rm -rf build; cd ../../  running build_ext  skipping 'cpu_nms.c' Cython extension (up-to-date)  skipping 'gpu_nms.cpp' Cython extension (up-to-date)    I'm not sure whether it's correct or not, or there are some problems with the vision of CUDA, which is 8.0.  "
"I use pretrained model to test with bbox but I got this error.    index created!  => classes: ['__background__', 'person']  => num_images: 5000  => Total boxes: 378148  => Total boxes after fliter low score@0.0: 378148  => load 378148 samples  => fail to read data/coco/images/val2017/000000466947.jpg  => fail to read data/coco/images/val2017/000000491543.jpg  => fail to read data/coco/images/val2017/000000106516.jpg  => fail to read data/coco/images/val2017/000000565319.jpg  => fail to read data/coco/images/val2017/000000098438.jpg  => fail to read data/coco/images/val2017/000000251235.jpg  => fail to read data/coco/images/val2017/000000295029.jpg  Traceback (most recent call last):  => fail to read data/coco/images/val2017/000000221348.jpg    File ""pose_estimation/valid.py"", line 172, in    => fail to read data/coco/images/val2017/000000327859.jpg      main()    File ""pose_estimation/valid.py"", line 168, in main      final_output_dir, tb_log_dir)    File ""/home/lxt/project/human-pose-estimation.pytorch/pose_estimation/../lib/core/function.py"", line 108, in validate      for i, (input, target, target_weight, meta) in enumerate(val_loader):    File ""/home/lxt/torch1.0/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 819, in __next__      return self._process_data(data)    File ""/home/lxt/torch1.0/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 846, in _process_data      data.reraise()    File ""/home/lxt/torch1.0/lib/python3.6/site-packages/torch/_utils.py"", line 369, in reraise      raise self.exc_type(msg)  ValueError: Caught ValueError in DataLoader worker process 0.  Original Traceback (most recent call last):    File ""/home/lxt/torch1.0/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py"", line 178, in _worker_loop      data = fetcher.fetch(index)    File ""/home/lxt/torch1.0/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch      data = [self.dataset[idx] for idx in possibly_batched_index]    File ""/home/lxt/torch1.0/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in        data = [self.dataset[idx] for idx in possibly_batched_index]    File ""/home/lxt/project/human-pose-estimation.pytorch/pose_estimation/../lib/dataset/JointsDataset.py"", line 84, in __getitem__      raise ValueError('Fail to read {}'.format(image_file))  ValueError: Fail to read data/coco/images/val2017/000000466947.j"
"Dose anyone have the right test.json that contains 11731 samples? I would appreciate it if you could send it to me. My email address is 464383828@qq.com ,Thank you!"
"@leoxiaobin       I think in this condition, we have to change `ul` by `br` because `ul[0]` and `ul[1]` will never exceed the size of the heatmap. The same for `br[0]` and `br[1]` will never be less than 0 (we add positive quantity)"
"Hi ,  Could u help me ?I found some ways to sovle it but failed  => load 6352 samples  ->loss  tensor(0.0014, device='cuda:0', grad_fn= )  Traceback (most recent call last):    File ""train.py"", line 261, in        main()    File ""train.py"", line 180, in main      final_output_dir, tb_log_dir, writer_dict)    File ""H:\paper_coding\human-pose-estimation.pytorch-master\lib\core\function.p  y"", line 56, in train      loss.backward( ) # allow_unreachable=True)  #retain_graph=True    File ""E:\Python36\lib\site-packages\torch\tensor.py"", line 107, in backward      torch.autograd.backward(self, gradient, retain_graph, create_graph)    File ""E:\Python36\lib\site-packages\torch\autograd\__init__.py"", line 93, in b  ackward      allow_unreachable=True)  # allow_unreachable flag  RuntimeError: cuDNN error: CUDNN_STATUS_BAD_PARAM      "
"I was just wondering if the joint coordinates are 0-based or 1-based. I know this will probably not have a huge impact on the performance, but I would like to get this right.    So, in   there is no subtraction to align the joints. Also, the `id`s are 0-based. Did you confirm from the authors that the indices are 1-based or are you just assuming so?"
"I meet OOM problem both train and valid,  I decrease the batch size but it still happen, the problems seems happen when load pretrained models.  the log of valid mpii dataset with resnet50 model is below. I can't find where the problem is.    => creating output/mpii/pose_resnet_50/256x256_d256x3_adam_lr1e-3  => creating log/mpii/pose_resnet_50/256x256_d256x3_adam_lr1e-3_2019-04-11-14-26  Namespace(cfg='experiments/mpii/resnet50/256x256_d256x3_adam_lr1e-3.yaml', coco_                                                                                                             bbox_file=None, flip_test=True, frequent=100, gpus=None, model_file='models/pyto                                                                                                             rch/pose_mpii/pose_resnet_50_256x256.pth.tar', post_process=False, shift_heatmap                                                                                                             =False, use_detect_bbox=False, workers=None)  {'CUDNN': {'BENCHMARK': True, 'DETERMINISTIC': False, 'ENABLED': True},   'DATASET': {'DATASET': 'mpii',               'DATA_FORMAT': 'jpg',               'FLIP': True,               'HYBRID_JOINTS_TYPE': '',               'ROOT': 'data/mpii/',               'ROT_FACTOR': 30,               'SCALE_FACTOR': 0.25,               'SELECT_DATA': False,               'TEST_SET': 'valid',               'TRAIN_SET': 'train'},   'DATA_DIR': '',   'DEBUG': {'DEBUG': False,             'SAVE_BATCH_IMAGES_GT': True,             'SAVE_BATCH_IMAGES_PRED': True,             'SAVE_HEATMAPS_GT': True,             'SAVE_HEATMAPS_PRED': True},   'GPUS': '8',   'LOG_DIR': 'log',   'LOSS': {'USE_TARGET_WEIGHT': True},   'MODEL': {'EXTRA': {'DECONV_WITH_BIAS': False,                       'FINAL_CONV_KERNEL': 1,                       'HEATMAP_SIZE': array([64, 64]),                       'NUM_DECONV_FILTERS': [256, 256, 256],                       'NUM_DECONV_KERNELS': [4, 4, 4],                       'NUM_DECONV_LAYERS': 3,                       'NUM_LAYERS': 50,                       'SIGMA': 2,                       'TARGET_TYPE': 'gaussian'},             'IMAGE_SIZE': array([256, 256]),             'INIT_WEIGHTS': True,             'NAME': 'pose_resnet',             'NUM_JOINTS': 16,             'PRETRAINED': 'models/pytorch/imagenet/resnet50-19c8e357.pth',             'STYLE': 'pytorch'},   'OUTPUT_DIR': 'output',   'PRINT_FREQ': 100,   'TEST': {'BATCH_SIZE': 32,            'BBOX_THRE': 1.0,            'COCO_BBOX_FILE': '',            'FLIP_TEST': True,            'IMAGE_THRE': 0.0,            'IN_VIS_THRE': 0.0,            'MODEL_FILE': 'models/pytorch/pose_mpii/pose_resnet_50_256x256.pth.tar                                                                                                             ',            'NMS_THRE': 1.0,            'OKS_THRE': 0.5,            'POST_PROCESS': True,            'SHIFT_HEATMAP': True,            'USE_GT_BBOX': False},   'TRAIN': {'BATCH_SIZE': 32,             'BEGIN_EPOCH': 0,             'CHECKPOINT': '',             'END_EPOCH': 140,             'GAMMA1': 0.99,             'GAMMA2': 0.0,             'LR': 0.001,             'LR_FACTOR': 0.1,             'LR_STEP': [90, 120],             'MOMENTUM': 0.9,             'NESTEROV': False,             'OPTIMIZER': 'adam',             'RESUME': False,             'SHUFFLE': True,             'WD': 0.0001},   'WORKERS': 4}  => loading model from models/pytorch/pose_mpii/pose_resnet_50_256x256.pth.tar  Traceback (most recent call last):    File ""pose_estimation/valid.py"", line 165, in        main()    File ""pose_estimation/valid.py"", line 123, in main      model.load_state_dict(torch.load(config.TEST.MODEL_FILE))    File ""/home1/yuzijian19/voice/lib/python3.6/site-packages/torch/serialization.py"", line 367, in load      return _load(f, map_location, pickle_module)    File ""/home1/yuzijian19/voice/lib/python3.6/site-packages/torch/serialization.py"", line 538, in _load      result = unpickler.load()    File ""/home1/yuzijian19/voice/lib/python3.6/site-packages/torch/serialization.py"", line 504, in persistent_load      data_type(size), location)    File ""/home1/yuzijian19/voice/lib/python3.6/site-packages/torch/serialization.py"", line 113, in default_restore_location      result = fn(storage, location)    File ""/home1/yuzijian19/voice/lib/python3.6/site-packages/torch/serialization.py"", line 95, in _cuda_deserialize      return obj.cuda(device)    File ""/home1/yuzijian19/voice/lib/python3.6/site-packages/torch/_utils.py"", line 76, in _cuda      return new_type(self.size()).copy_(self, non_blocking)    File ""/home1/yuzijian19/voice/lib/python3.6/site-packages/torch/cuda/__init__.py"", line 496, in _lazy_new      return super(_CudaBase, cls).__new__(cls, *args, **kwargs)  RuntimeError: CUDA error: out of memory"
@leoxiaobin  Thanks for sharing your work.     Could you please explain how to find **feature is not aligned**.         
"Hello, thank you for the great code!    I like the simplicity of the loss, however I don't understand one specific case when:  - the joint is not visible in the object patch AND there is a wrong prediction (FP), then the loss will be zero because the `target_weight=0`. Shouldn't one want to penalize such a case of FPs?    Why wouldn't you not use target_weight, then both FPs and FNs would be penalized which is good."
hi!  I want to output the inference results on input images. Is there the code? thanks!
"hi!   I disable cudnn for batch_norm. If I use other code such as mask rcnn, will it influences the result of mask rcnn?  thanks!"
None
None
"I use valid.py, the result is: (on my own datasets)     Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.000   Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.000   Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.000   Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000   Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = -1.000   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.000   Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.000   Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.000   Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000   Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = -1.000   Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.000   Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.000   Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.000   Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000   Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = -1.000   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.000   Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.000   Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.000   Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000   Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = -1.000  | Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |  |---|---|---|---|---|---|---|---|---|---|---|  | 256x192_pose_resnet_50_d256d256d256 | 0.000 | 0.000 | 0.000 | -1.000 | -1.000 | 0.000 | 0.000 | 0.000 | -1.000 | -1.000 |"
hi!  if I use detection bbox('COCO_val2017_detections_AP_H_56_person.json')     the acc is 0.000.  so how to use the file `COCO_val2017_detections_AP_H_56_person.json` ?  thanks!
"Hello, I am  trying to finetune your model on posetrack. I read your paper. You said the  learning rate strategy  was different and other settings were the same as those in the training COCO part.  I tried to finetune the model like what you said. But problem is COCO and Posetrack have 2 different keypoints -- ""**head_top**"" and ""**head_bottom**"" in posetrack, ""**left_eye**"", ""**right_eye**"" in COCO.  Before finetuning, I made a big COCO-format annotation file with those Posetrack json files. And I simply treated the ""head_top"" and ""head_bottom"" keypoints  as ""left_eye"", ""right_eye"". But it seemed like the model can't specify these 2 points very well.    So can you tell me how you handle these 2 points? "
"firstly, i will appreciate this repo.  when i run the          `python pose_estimation/valid.py\       --cfg experiments/coco/resnet101/384x288_d256x3_adam_lr1e-3.yaml \       --flip-test \       --model-file models/pytorch/pose_coco/pose_resnet_101_384x288.pth.tar`    i can get the result as  fallows   Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.755   Average Precision  (AP) @[ IoU=0.50         | area=   all | maxDets= 20 ] = 0.925   Average Precision  (AP) @[ IoU=0.75         | area=   all | maxDets= 20 ] = 0.826   Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.724   Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.808   Average Recall       (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.784   Average Recall       (AR) @[ IoU=0.50         | area=   all | maxDets= 20 ] = 0.936   Average Recall       (AR) @[ IoU=0.75         | area=   all | maxDets= 20 ] = 0.845   Average Recall       (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.749   Average Recall       (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.838    this result is better your repo, my  configuration include : Python3.6 pytorch0.4.0  How to interpret this phenomenon? I do not modify anything about code, However, My valid result is better your repo result and the differenc is up to 1.9%  "
None
"Hi,     I'm re-implementing your algorithm, and would love to compare the training logs for consistency.     So is there any plan to share the training logs?    Thanks!"
"i check your code and it seems that you are using opencv imread which will read image in bgr format.    but pytorch resnet models (  expect image in rgb format.            mean=[0.485, 0.456, 0.406] #rgb          std =[0.229, 0.224, 0.225]    This is a small issue  and will not affect results.    "
"Hi, I implemented your work with TensorFlow and uploaded in my git repo (   If you want, you can mention my git repo at your README markdown file."
"1. the size of the pose pool used in tracking.  2. the validation set is marked every other 4 frames. During testing, do you track all frames (including 4 unmarked frames between), or just those marked by the validation set?"
The pre-trained models have a `.pth.tar` extension but are not valid `.tar` files. The content is in Pickle format and extension should be `.pth`.
"Where is the core folder supposed to live? before running the code block of ""Valid on MPII using pretrained models""? Inside lib?    Because I get the following error when running that:    user@magic-vm:~/human-pose-estimation.pytorch$ python pose_estimation/valid.py   \  >     --cfg experiments/mpii/resnet50/256x256_d256x3_adam_lr1e-3.yaml \  >     --flip-test \  >     --model-file models/pytorch/pose_mpii/pose_resnet_50_256x256.pth.tar  Traceback (most recent call last):    File ""pose_estimation/valid.py"", line 25, in        from core.config import config  ImportError: No module named core.config"
None
"**Valid on COCO val2017 using pretrained models**    python pose_estimation/valid.py \      --cfg experiments/mpii/resnet50/256x256_d256x3_adam_lr1e-3.yaml \      --flip-test \      --model-file models/pytorch/pose_coco/pose_resnet_50_256x256.pth.tar    I think the first and third line, 256x256 has to be changed to 256x192.\  Or both directory must be changed to mpii"
I'm running valid.py for 256x192 resnet 50 network in COCO dataset.  I get different validation result from yours in readme.  This is your result    256x192_pose_resnet_50_d256d256d256 | 0.704 | 0.886 | 0.783 | 0.671 | 0.772 | 0.763 | 0.929 | 0.834 | 0.721 | 0.824    and this is what i get     Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.724   Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.915   Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.804   Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.697   Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.765   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.756   Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.930   Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.823   Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.723   Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.804  | Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |  |---|---|---|---|---|---|---|---|---|---|---|  | 256x192_pose_resnet_50_d256d256d256 | 0.724 | 0.915 | 0.804 | 0.697 | 0.765 | 0.756 | 0.930 | 0.823 | 0.723 | 0.804 |    I download all pretrained models from google drive and run command below    python3 pose_estimation/valid.py --cfg experiments/coco/resnet50/256x192_d256x3_adam_lr1e-3.yaml --flip-test --model-file models/pytorch/pose_coco/pose_resnet_50_256x192.pth.tar    Is there any update of model or weight?
Since this is a more convenient way to compare the results.
"In your paper , you achieved 70.4AP using 256x192_pose_resnet_50,  Results on COCO val2017 with detector having human AP of 56.4 on COCO val2017 dataset:  Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L)  -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --  256x192_pose_resnet_50_d256d256d256 | 0.704 | 0.886 | 0.783 | 0.671 | 0.772 | 0.763 | 0.929 | 0.834 | 0.721 | 0.824    while after using the same method and the same model, even after training with our own gpus, the AP achieved is   | Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |  2018-11-14 18:15:51,575 |---|---|---|---|---|---|---|---|---|---|---|  2018-11-14 18:15:51,575 | 256x192_pose_resnet_50_d256d256d256 | 0.723 | 0.925 | 0.794 | 0.697 | 0.765 | 0.755 | 0.932 | 0.820 | 0.723 | 0.802 |    Nearly 2percent higher .Wolud you be so kind to explian why?    Also,the validate command is not right. It should be:  python pose_estimation/valid.py \      --cfg experiments/coco/resnet50/256x192_d256x3_adam_lr1e-3.yaml \      --flip-test \      --model-file models/pytorch/pose_coco/pose_resnet_50_256x192.pth.tar    Rahter than:  python pose_estimation/valid.py \      --cfg experiments/mpii/resnet50/256x256_d256x3_adam_lr1e-3.yaml \      --flip-test \      --model-file models/pytorch/pose_coco/pose_resnet_50_256x256.pth.tar    The work is really remarkable, looking forward to your comment  "
     Is the copy.deepcopy() necessary here? Can we directly adopt the value of the index position?
!   !   As shown in the picture 
"hi, I have trained my own model, could you provide detections_person.json you used, so that i can compare to the COCO leaderboard. thank u ."
I tried to train the model by myself but it can not get the experiment results as you released no matter how I altered parameters.  Is there any trick in training? Or how to set the parameters?  Thank you!
Hi @leoxiaobin   I am confused about the following code:            coords      They plus 0.5 after getting the sub pixel coordinates.    Would you please explain the difference?  Thanks!  
Hi @leoxiaobin   Thanks for sharing your excellent work!  I am curious about following line in the select_data function in you code.        num_vis is the number of labelled joints.    Would you please explain how to calculate the metric?  Is 16 the number of symmetric joints in coco dataset?  Do you use the select_data only in COCO datasetï¼Ÿ  What is 0.45 ?  Why minus 0.2 / 16?  Why select those data which are greater than the metric?          
"Hi, I got the same issue as #17. Though I have disabled cudnn for batch_norm as mentioned in Installation, I got the same results. I'm using pytorch 0.5.0. Shall I do anything else to fix this problem? Thank you!"
"  Traceback (most recent call last):    File ""pose_estimation/train.py"", line 37, in        import dataset    File ""/home/sunbin/my_project/simple_baselines/human-pose-estimation/pose_estimation/../lib/dataset/__init__.py"", line 12, in        from .coco import COCODataset as coco    File ""/home/sunbin/my_project/simple_baselines/human-pose-estimation/pose_estimation/../lib/dataset/coco.py"", line 23, in        from nms.nms import oks_nms    File ""/home/sunbin/my_project/simple_baselines/human-pose-estimation/pose_estimation/../lib/nms/nms.py"", line 14, in        from .gpu_nms import gpu_nms  "
"Hello,    Thank you for releasing the code! Had a quick question what is the speed per image on a Nvidia GPU? At least approximately.    Thanks!"
"thanks for your release code.   I run you code for 140 epochs ,  just reach 71.6 map.  does this have something with random seed in dataset ?"
"There are two papers regarding posetrack. In paper ""PoseTrack: Joint Multi-Person Pose Estimation and Tracking"", the PCKh is 20% of head bounding box's diagonal. In paper ""PoseTrack: A Benchmark for Human Pose Estimation and Tracking"", the PCKh is 30% of head bounding box's diagonal. I'm wondering, which one you guys used? In PoseTrack GitHub site, their evaluation toolkit ""poseval"" uses 30% version."
My cards (Nvidia Titan XP) is unable to run on 384x288 (out of memory). I want to run on something less than 384x288 but more than 256x192. Thanks!
"During training, the DataLoader (i.e., torch.utils.data.DataLoader) always got stuck, resulting in the low GPU utilization ? Have you encountered such a situation ?"
"I did the validation via valid.py and put the json file generated in the process through the cocoapi. These two gave rather different results. The cocoapi's result is more than 20% lower than valid.py's. Do you have different coordinates system? If so, could you so kindly tell us the conversion formula between your system and cocoapi's? Thanks!"
"Is it because ResNet in torchvision is not up-to-date? Say, it may not have BN within the Residual Module?"
"I noticed that when I trained on my own dataset, your code produced three model: checkpoint.pth.tar, final_state.pth.tar, and model_best.path.tar. Unfortunately, I was unable to load the model_best.pth.tar in validation. It reports ""Missing Key(s) Erro"" in state_dict."
"Hi, I am unable to understand how OKS is calculated in experiments using COCO dataset.  In the train function in `lib/core/function.py` you seem to call `accuracy` from the file `lib/core/evaluate.py`. But that accuracy is PCKh right? So how do you calculate OKS.    Could you please explain the steps how can I calculate OKS given I use your dataloader?? Thanks alot in advance!!! "
Still kpt_score*bbox_score? or other strategies? Thanks!
I'm little confused here. It seems Detectron has a different stragety...
"When I try to run the training on COCO, as showed in README.md file, I encountered this error.       my cuda version is 8.0, my cudnn is 7.2.1, driver version is 381.22"
"Thanks for your excellent work. I would like to ask a problem (might not your code bug).  When training on coco dataset under your instruction by  `python pose_estimation/train.py \      --cfg experiments/coco/resnet50/384x288_d256x3_adam_lr1e-3.yaml`  At an unexpected moment (not always in epoch 1, this happened at any time before), process is interrupted unexpectedly, the log in terminal is as follow    **Epoch: [1][2000/4682] Time 0.499s (0.496s) Speed 64.1 samples/s Data 0.000s (0.007s) Loss 0.00067 (0.00070) Accuracy 0.648 (0.578)  Epoch: [1][2100/4682] Time 0.476s (0.496s) Speed 67.2 samples/s Data 0.000s (0.007s) Loss 0.00062 (0.00070) Accuracy 0.571 (0.580)  Epoch: [1][2200/4682] Time 0.479s (0.497s) Speed 66.8 samples/s Data 0.000s (0.007s) Loss 0.00068 (0.00070) Accuracy 0.624 (0.582)  THCudaCheck FAIL file=/pytorch/aten/src/THC/generated/../generic/THCTensorMathPointwise.cu line=265 error=4 : unspecified launch failure  Traceback (most recent call last):    File ""pose_estimation/train.py"", line 206, in        main()    File ""pose_estimation/train.py"", line 174, in main      final_output_dir, tb_log_dir, writer_dict)    File ""/home/vision01/Workspace/Python/Pose/human-pose-estimation.pytorch-master/pose_estimation/../lib/core/function.py"", line 53, in train      optimizer.step()    File ""/home/vision01/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py"", line 92, in step      exp_avg.mul_(beta1).add_(1 - beta1, grad)  RuntimeError: cuda runtime error (4) : unspecified launch failure at /pytorch/aten/src/THC/generated/../generic/THCTensorMathPointwise.cu:265  THCudaCheckWarn FAIL file=/pytorch/aten/src/THC/THCStream.cpp line=50 error=4 : unspecified launch failure**    I disabled cudnn for batch_norm as you instructed  My platform is Ubuntu 16.04, GTX1080TI, CUDA 9.0.176, cudnn 7.0.5, python3.6, pytorch0.4.0  Don`t think is a bug in your code, butI was confused a lot and still had no clue after google.  Hope your guys can help me, thanks a lot!"
"in main() function of 'train.py', line 92~94     I can't find the reference of the eval function and the functionality of the parenthesis after the eval().  In PyCharm, when I ""ctrl-click"" on the eval, it brings me to an eval definition that only has ""pass"" in it."
"Hi, I see it uses center and  scale when fetching validation dataset.   Where are the center and scale come from?  They are predicted by human detector or the gt information?   "
"hello,thank you for your perfect project, but I don't know how to use  your model to predict on my own dataset, I just use a file like predict.py which input only include model path and my dataset path to predict on my dataset.thank you"
"I've been trying to train on coco. The training loss seems normal. However, the validation performance is always 0. Any idea what is happening? "
I was not able to find a python file to test inference.  Could you please upload a `test.py` file to test inference on a batch of images/video?  The AP results in the paper look very promising.
I noticed that one of the requirements is installing cocoapi. So I'm courios your results on COCO val2017 is based on OKS[0.5~0.95] or PCKhï¼Ÿ
"When 'import tensorboardX', it reported an error: Segmentation fault (core dumped) . The same error occur when run the train.py. I tried tensorboardX1.2 and 1.4  by 'conda install'.  Environment:  pytorch: v0.4.0  python: v3.6 (anaconda3)  cuda: 8.0  cudnn: v6"
"Torch v0.5, requirements satisfied with `pip install -r requirements.txt` and trying to run `valid.py` with the suggested model/config for Coco         I saw in the paper that this was based on the latest Mask R-CNN and a similar issue was raised   so is the Facebook detectron required to run this code?"
"I noticed in the source code, your project support posetrack datasets. I plan to write a yaml file to run it on posetrack datasets. But I'm stumped on annotations' format. Do you support the original posetrack annotations (in matlab format)? or json verson of such annotations? or I have to turn it into coco format?    I'm sorry that I'm not very familiar with python, so I can't have those answers just by reading the source code.    Many thanks for any help you can offer."
"When I run ""python pose_estimation/valid.py --cfg experiments/coco/resnet50/256x192_d256x3_adam_lr1e-3.yaml --flip-test --model-file models/pytorch/pose_coco/pose_resnet_50_256x192.pth.tar"", it reported an error:  => loading model from models/pytorch/pose_coco/pose_resnet_50_256x192.pth.tar  Traceback (most recent call last):    File ""pose_estimation/valid.py"", line 165, in        main()    File ""pose_estimation/valid.py"", line 123, in main      model.load_state_dict(torch.load(config.TEST.MODEL_FILE))    File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 721, in load_state_dict      self.__class__.__name__, ""\n\t"".join(error_msgs)))  RuntimeError: Error(s) in loading state_dict for PoseResNet:          Missing key(s) in state_dict: ""bn1.num_batches_tracked"", ""layer1.0.bn1.num_batches_tracked"", ""layer1.0.bn2.num_batches_tracked"", ""layer1.0.bn3.num_batches_tracked"", ""layer1.0.downsample.1.num_batches_tracked"", ""layer1.1.bn1.num_batches_tracked"", ""layer1.1.bn2.num_batches_tracked"", ""layer1.1.bn3.num_batches_tracked"", ""layer1.2.bn1.num_batches_tracked"", ""layer1.2.bn2.num_batches_tracked"", ""layer1.2.bn3.num_batches_tracked"", ""layer2.0.bn1.num_batches_tracked"", ""layer2.0.bn2.num_batches_tracked"", ""layer2.0.bn3.num_batches_tracked"", ""layer2.0.downsample.1.num_batches_tracked"", ""layer2.1.bn1.num_batches_tracked"", ""layer2.1.bn2.num_batches_tracked"", ""layer2.1.bn3.num_batches_tracked"", ""layer2.2.bn1.num_batches_tracked"", ""layer2.2.bn2.num_batches_tracked"", ""layer2.2.bn3.num_batches_tracked"", ""layer2.3.bn1.num_batches_tracked"", ""layer2.3.bn2.num_batches_tracked"", ""layer2.3.bn3.num_batches_tracked"", ""layer3.0.bn1.num_batches_tracked"", ""layer3.0.bn2.num_batches_tracked"", ""layer3.0.bn3.num_batches_tracked"", ""layer3.0.downsample.1.num_batches_tracked"", ""layer3.1.bn1.num_batches_tracked"", ""layer3.1.bn2.num_batches_tracked"", ""layer3.1.bn3.num_batches_tracked"", ""layer3.2.bn1.num_batches_tracked"", ""layer3.2.bn2.num_batches_tracked"", ""layer3.2.bn3.num_batches_tracked"", ""layer3.3.bn1.num_batches_tracked"", ""layer3.3.bn2.num_batches_tracked"", ""layer3.3.bn3.num_batches_tracked"", ""layer3.4.bn1.num_batches_tracked"", ""layer3.4.bn2.num_batches_tracked"", ""layer3.4.bn3.num_batches_tracked"", ""layer3.5.bn1.num_batches_tracked"", ""layer3.5.bn2.num_batches_tracked"", ""layer3.5.bn3.num_batches_tracked"", ""layer4.0.bn1.num_batches_tracked"", ""layer4.0.bn2.num_batches_tracked"", ""layer4.0.bn3.num_batches_tracked"", ""layer4.0.downsample.1.num_batches_tracked"", ""layer4.1.bn1.num_batches_tracked"", ""layer4.1.bn2.num_batches_tracked"", ""layer4.1.bn3.num_batches_tracked"", ""layer4.2.bn1.num_batches_tracked"", ""layer4.2.bn2.num_batches_tracked"", ""layer4.2.bn3.num_batches_tracked"", ""deconv_layers.1.num_batches_tracked"", ""deconv_layers.4.num_batches_tracked"", ""deconv_layers.7.num_batches_tracked"".     Environment:  Pytorch version 0.5.0  CUDA 8.0 and cudnn 6      "
"When I followed the instructions on readme, running ""python pose_estimation/valid.py --cfg experiments/mpii/resnet50/256x256_d256x3_adam_lr1e-3.yaml --flip-test --model-file models/pytorch/pose_mpii/pose_resnet_50_256x256.pth.tar"", it reported an error:   ImportError: No module named 'nms.cpu_nms'    I've noticed there are cpu_nmn.pyx under 'human-pose-estimation.pytorch/lib/nms'. I ran the 'setup.py' under the same directory by typing 'python setup.py install'. Unfortunately, it didn't solve my problem. I also tried to add various path including cpu_nms.py,cpu_nms.so...in to the environment, no use."
"Hi, I met a question when training.  During training stage,  after evaluating the current results, training suspended. Log info as below:     Accumulating evaluation results...  DONE (t=0.06s).   Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.339   Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.665   Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.300   Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.329   Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.360   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.391   Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.693   Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.377   Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.370   Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.420  => coco eval results saved to output/coco/pose_resnet_50/256x192_d256x3_adam_lr1e-3/results/keypoints_val2017_results.pkl  Traceback (most recent call last):    File ""pose_estimation/train.py"", line 206, in        main()    File ""pose_estimation/train.py"", line 180, in main      writer_dict)    File ""/media/human-pose-estimation.pytorch/pose_estimation/../lib/core/function.py"", line 182, in validate      filenames, imgnums)    File ""/media/human-pose-estimation.pytorch/pose_estimation/../lib/dataset/coco.py"", line 329, in evaluate      name_value = OrderedDict(info_str)  TypeError: 'NoneType' object is not iterable  "
Thank you for releasing the code. The README reads that it is required to disable cudnn for batch_norm. Would you please explain why we should do this?
"For image `008734041.jpg`, the visible annotation is a little different.   Current:     Original:   "
"I run into error, no graph saved.  I download model from `         "
I got following error when I try to download pretrained model from OneDrive    Something went wrong  Please try again or refresh the page  
What is the purpose of having granularity as the input to model design?
"In > region_norm_ops.py,  line 55  `    # 3. normalization      bn_output = BatchNorm('bn', feed2bn, axis=3, gamma_initializer=gamma_initializer,                            internal_update=True)`    It seems that calling `BatchNorm` in tensorpack will cause this error. I've tried several versions of tensorpack without solving this bug"
ä½ å¥½ï¼        é¦–å…ˆæ„Ÿè°¢åˆ†äº«è¿™é¡¹æœ‰è¶£å·¥ä½œçš„ä»£ç ã€‚        1. å› ä¸ºæˆ‘æ˜¯ä»ŽMTLè·Ÿè¿‡æ¥çš„ï¼Œæ‰€ä»¥æˆ‘ä¸å¤§æ¸…æ¥šè¿™ä¸ªrepoä»£ç é‡Œçš„ResNet-12æŒ‡çš„å°±æ˜¯ECCV 20è®ºæ–‡é‡Œçš„ResNet-25å—ï¼Ÿå› ä¸ºæˆ‘çœ‹è¿™é‡Œçš„ç»“æžœè·Ÿè®ºæ–‡é‡ŒæŠ¥å‘Šçš„æ˜¯ä¸€æ ·çš„ï¼Œä½†æ˜¯ç½‘ç»œç»“æž„å†™äº†ä¸ªResNet-12ï¼›        2. æˆ‘çœ‹è¿™ä¸ªrepoé‡Œè¯´ç½‘ç»œç»“æž„æ˜¯è·ŸMetaOptNetä¸€æ ·ï¼Œæ„æ€æ˜¯MetaOptNetçš„ResNet-12è·ŸMTLè®ºæ–‡é‡Œçš„ResNet-12ä¸ä¸€æ ·å—ï¼Ÿå‰è€…æ›´æ·±å‚æ•°é‡æ›´å¤šï¼Ÿ          **æœ€åŽå†ä¸€æ¬¡æ„Ÿè°¢ä½ ä»¬çš„æºä»£ç ï¼Œ100%å¯å¤çŽ°æ²¡æ¯›ç—…ï¼ï¼ˆé™¤äº†hard taskè‡³ä»Šæ²¡æœ‰æ›´æ–°ï¼Œå“ˆå“ˆï¼‰**
"Hi, thanks for sharing your excellent work.  I am trying to pretrain the model in my own dataset and an error occurs as shown below.    Start pre-train phase.  Rest for 0.000 hour:  logs/pre_train/soybean-resnet12  Epoch 1, total loss=5.9711 acc=0.0000: 100%|â–ˆâ–ˆ| 600/600 [00:17       trainer.pre_train()    File ""/home/ubuntu/junwang/paper/bmvc/e3bm/trainer/meta_trainer.py"", line 376, in pre_train      (data_shot, data_query))     File ""/home/ubuntu/anaconda3/envs/bmvc/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 532, in __call__      result = self.forward(*input, **kwargs)    File ""/home/ubuntu/anaconda3/envs/bmvc/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 152, in forward      outputs = self.parallel_apply(replicas, inputs, kwargs)    File ""/home/ubuntu/anaconda3/envs/bmvc/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 162, in parallel_apply      return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])    File ""/home/ubuntu/anaconda3/envs/bmvc/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 85, in parallel_apply      output.reraise()    File ""/home/ubuntu/anaconda3/envs/bmvc/lib/python3.7/site-packages/torch/_utils.py"", line 394, in reraise      raise self.exc_type(msg)  TypeError: Caught TypeError in replica 0 on device 0.  Original Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/envs/bmvc/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 60, in _worker      output = module(*input, **kwargs)    File ""/home/ubuntu/anaconda3/envs/bmvc/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 532, in __call__      result = self.forward(*input, **kwargs)    File ""/home/ubuntu/junwang/paper/bmvc/e3bm/model/meta_model.py"", line 158, in forward      return self.pretrain_forward(inputs)    File ""/home/ubuntu/junwang/paper/bmvc/e3bm/model/meta_model.py"", line 166, in pretrain_forward      return self.fc(self.encoder(input))    File ""/home/ubuntu/anaconda3/envs/bmvc/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 532, in __call__      result = self.forward(*input, **kwargs)    File ""/home/ubuntu/junwang/paper/bmvc/e3bm/model/resnet12.py"", line 113, in forward      x = self.layer1(x)    File ""/home/ubuntu/anaconda3/envs/bmvc/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 532, in __call__      result = self.forward(*input, **kwargs)    File ""/home/ubuntu/anaconda3/envs/bmvc/lib/python3.7/site-packages/torch/nn/modules/container.py"", line 100, in forward      input = module(input)    File ""/home/ubuntu/anaconda3/envs/bmvc/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 532, in __call__      result = self.forward(*input, **kwargs)    File ""/home/ubuntu/junwang/paper/bmvc/e3bm/model/resnet12.py"", line 49, in forward      out = self.conv1(x)    File ""/home/ubuntu/anaconda3/envs/bmvc/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 532, in __call__      result = self.forward(*input, **kwargs)    File ""/home/ubuntu/anaconda3/envs/bmvc/lib/python3.7/site-packages/torch/nn/modules/conv.py"", line 345, in forward      return self.conv2d_forward(input, self.weight)    File ""/home/ubuntu/anaconda3/envs/bmvc/lib/python3.7/site-packages/torch/nn/modules/conv.py"", line 342, in conv2d_forward      self.padding, self.dilation, self.groups)  TypeError: conv2d(): argument 'input' (position 1) must be Tensor, not tuple    In conclusion, the error is that the model takes tensor as input but a tuple (data_shot, data_query) are given.    In addition, I fould another problem:  When choosing the paratemers to be optimized in the optmizer, the ''self.model' seems incorrect as error occurs, I change 'self.model' to 'self.model.module', it works.   These errors may caused by the multi-gpu scenerio.  However, when I use only one gpu, an error below occurs,    Start pre-train phase.  Rest for 0.000 hour:  logs/pre_train/soybean-resnet12  Epoch 1, total loss=5.4935 acc=0.0000: 100%|â–ˆâ–ˆ| 600/600 [00:12       trainer.pre_train()    File ""/home/ubuntu/junwang/paper/bmvc/e3bm/trainer/meta_trainer.py"", line 376, in pre_train      (data_shot, data_query))     File ""/home/ubuntu/anaconda3/envs/bmvc/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 532, in __call__      result = self.forward(*input, **kwargs)    File ""/home/ubuntu/junwang/paper/bmvc/e3bm/model/meta_model.py"", line 161, in forward      return self.meta_forward(data_shot, data_query)    File ""/home/ubuntu/junwang/paper/bmvc/e3bm/model/meta_model.py"", line 220, in meta_forward      grad = torch.autograd.grad(loss, fast_weights)    File ""/home/ubuntu/anaconda3/envs/bmvc/lib/python3.7/site-packages/torch/autograd/__init__.py"", line 157, in grad      inputs, allow_unused)  RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn     I am grateful if you can provide any support to figure out the problem."
"Hi, thanks for the great work and releasing your code!    In the Readme, the results on resnet-12 under transductive setting are given for E3bm. However, I cannot find those numbers in the published paper. So I am just wondering is something wrong here?    Thanks!"
"Hi, I am trying to replicate the performance of 3D-Resnet-18 on HVU dataset by training for scene labels.     In the paper (  Table 3, the MAP is listed as 50.6%.    Is it possible to share the training details for 3D-Resnet-18?     I am getting a MAP of 37% on 251,794 videos by training using the following configuration:    1. **Learning rate**: 1e-4    2. **Scheduler**: LR scheduler on the plateau (decrease the learning rate by 0.1 when the validation MAP does not improve for 3 epochs)    3. **Optimizer**: Adam    4. **Batch size**: 64    5. Temporal Random crop as augmentations for training followed by scaling and normalization using mean and std of kinetics dataset.    6. Center crop as augmentations for validation followed by scaling and normalization using mean and std of kinetics dataset.    Thanks in advance "
"Hi author, may I know the trimmed video can be still downloaded by filling the form in the README ?"
The tags are organized according to WordNet ontology. Will the ontology be released?
"Dear Author,          I have run some experiments on the HVU Dataset and find that there are some performance gaps from the reported numbers in the paper. For example, I only got around 25% mAP for action tags, while the reported performance is 50%+, but for other tags categories, the mAP I got is similar to the reported numbers. Would you please share more details about evaluation (like the evaluation scripts you used or the scores generated by your models on the validation set)?  Thanks a lot! "
"Thanks for releasing the HVU Dataset. In your paper, you say that you use 'softmax loss for the action recognition branch'. However, there are multiple action labels for some videos in the HVU dataset. So I wonder how u use softmax loss for action recognition. "
"Hi, I'm every interested in your work. Could you provide a trimmed video dataset where the length of each video is 10s? I can not download all the entire videos and trim them because of some limitted of my computer. Thank you very much if you could help me."
"I try to download the dataset videos but when the downloading finished I found that some videos are not downloaded, so is these videos are private or permanently deleted from youtube."
"I was working to integrate the HVU Dataset to Tensorflow Datasets. I tried running this script   both directly from the terminal as well as by using the code in my Kaggle Notebook and what happens is that some files **(almost ~10%)** are not properly downloaded and are around 252 Bytes each only.   On the other hand, if i use the script code for the individual video IDs that were not downloaded properly, there is no issue.    What do you think could be causing this?  I think it has something to do with the **joblib.parallel** library which was used for parallelisation in the script.  I'm attaching a screenshot of my Kaggle Notebook output showcasing the improperly downloaded files.    !     "
I can't tell which tags in the train/val labels map to which categories. Would you be able to publish a mapping that matches tags to categories? Thanks!
Hi @alidiba67   Thanks for creating the HVU dataset. I am wondering when the dataset will be available? 
"Thanks for open-sourcing your work, I have been trying to use CMC on my custom toy dataset which has 2 views (RGB, Depth). The number of RGB and Depth images is 4000. I have 2 dataloader, one for RGB data and another for depth. And use corresponding RGB and depth to be positive pair.  When I use NCECriterion, the reslut is like,  !   If use Softmax-CE, the result becomes,  !     Could you please give me some suggestion about train with dual-modal datasets or small datasets ?    Thanks a lot!"
"Hi, Could you share the pre-trained model for video action recognition task? i.e. the pre-trained model who's result are shown in Table 2"
"I read the codes and maybe it's for only two views. So, How can it be used for more than two views datasets?"
"Hi, Thanks a lot for sharing this great code.  In `NCEAverage.py`, the code is shown as the following:     Select `batchsize * (K + 1)` features from memory bank and then calculate the similarity matrix. How to determine a positive pair and K negative pairs for each sample?     In MoCo, the aug views of the same image, v1 and v2, are a positive pairs, besidesï¼Œv1 and other k views from the memory bank of the `encoder_k` are the k negative pairs. In CMC, this seems to be K + 1 negative pairs, because the memory bank is updated late and it seems that it is not guaranteed to obtain the positive pair view of each sample in the same batch."
"when you implement CMC on cifar100, did you resize input image to be 256 by 256, or keep it in its original size"
"Hi,   Thanks for your open-source code. I was training the imagenet on Lab views by nce default setting in your code.  I found that the CPU memory is increasing while training and it won't be released after killing the job. I wonder if you encounter this issue and any possible reason?    Thank you!"
Can anybody tell me how to use the pretrained MoCo?
"Hello, thanks for your great work.  I find that there is no pre-trained model of ResNet50v1 in DropBox. Could you please provide it?"
"Hi,  Thanks for your excellent work. I have a question about implementation of MoCo.    It seems that shuffle_ids from line 440 to line 442   does not work. In other work, feat_k  get the same results with or without shuffle_ids and reverse_ids."
Sorry to interrupt. I notice that the training epochs of ResNet-50/AlexNet on ImageNet-1K is 240 (default setting of your code). But I find the training epochs is 240 in your paper. So I wonder which setting is correct for ResNet-50/AlexNet on ImageNet-1K.
"With regards to the mean and std values required for normalization: How do the values arise for LAB and YCbCr respectively? My assumption was, that the training images were transformed into the new color space and then statistics across channels are computed which are then used for normalization. That would explain YCbCr, but what does the formula for LAB color space mean? Is it the min and max values averaged?    `def get_train_loader(args):      """"""get the train loader""""""      data_folder = os.path.join(args.data_folder, 'train')        if args.view == 'Lab':          mean = [(0 + 100) / 2, (-86.183 + 98.233) / 2, (-107.857 + 94.478) / 2]          std = [(100 - 0) / 2, (86.183 + 98.233) / 2, (107.857 + 94.478) / 2]          color_transfer = RGB2Lab()      elif args.view == 'YCbCr':          mean = [116.151, 121.080, 132.342]          std = [109.500, 111.855, 111.964]          color_transfer = RGB2YCbCr()`"
"Hello,    Thank you for this great work and well-written paper.    When training the linear classifier (e.g. Alexnet with ImageNet), how do you perform the train/test split? 1. Do you use all of the classes in your test set?  2. How do you choose views for each sample?    Many thanks,  Jonny"
"Hi, Thanks a lot for sharing this great code.  I have a question about data augmentation and the memory bank. If we use data augmentation, the features in the memory bank are not update for this issue. Especially for the positive examples which we using from the memory bank.  Have you thought about it?"
"     I think the purpose of using NCE is to avoid expensive summation over entire vector in softmax. But in your implementation, there is still summation over entire `log_D0` which confused me. I'll appreciate it if you explain this.   I'm new to this field, and hope you point out my misunderstanding if there is."
"It appears to me that shuffle-bn has no effect, when run on a single GPU.    Example:            I guess another approach is necessary on single-GPU. Any thoughts?    Thanks for releasing this code.  "
"Hi there    Thanks a lot for this great repo!    I am trying out MoCo on my own dataset (I also added additional augmentations). Training appears to have converged, but the max value I get for `ins_prob` is about `13.35`, and the lowest value I get for loss is about `0.2422`.    I am wondering what metrics you got when training on Imagenet? Am not sure what a ""good"" score should look like.    Here are screenshots from training progress in tensorboard (ignore the multiple lines at the start of training).    !     !     Thanks,  Liam"
"Hi,  Thanks for your released code. I want to check something puzzling me.  Does 'resnes50v2' represent 'ResNet-50' in Table 2 in the paper?  Does 'resnes50v3' represent 'ResNet-50 x2' in Table 2 in the paper?  If the answers are true, I want to know if you have trained 'resnet50v1' on ImageNet. Could you please share the results?    Thanks."
"When I evaluated the result on ImageNet (not the subset), I got the bug as follows:    THCudaCheckWarn FAIL file=/opt/conda/conda-bld/pytorch_1524586445097/work/aten/src/THC/THCStream.cpp line=50 error=59 : device-side assert triggered    Does anyone have any thought about the issue?"
"Could you please release the code of using other views instead of only ""l"" and ""ab"" in the training CMC process?"
"     This one-time estimation is problematic, especially if the dictionary is not random noise. Computing Z as a moving average of this would give a more reasonable result."
"I saw your note and it seems rather unusual to use such a large learning rate:    Note: When training linear classifiers on top of ResNets, it's important to use large learning rate, e.g., 30~50.    Is there something I'm missing? I can't imagine how you get stable gradient descent with such high learning rates. "
"I enjoyed reading the paper. Thanks for open sourcing the code.    Please let me know if I can train CMC model [resnet50 variant] by loading pretrained resnet-50 trained on ImageNet.    Also if I want to train with custom dataset with custom number of classes, please suggest what change is required in hyperparams?"
Hi     Thank you for sharing this project with us! I am curious why did you change the default `AdaptiveAvgPool2d` of ResNet to `AvgPool2d`. How does this change affect the performance?    Your `AvgPool2d` layer:       Pytorch's `AdaptiveAvgPool2d` layer:   
"Hi there,     This is a bit of a meta-question.     I noticed that your   uses the original AlexNet parameters i.e. with convolutions 96,256,384,384,256 vs. the one weird trick paper 64,192,384,256,256 that is the standard in the official PyTorch  .     In comparison, Feng et al. at CVPR 2019 use the smaller version of AlexNet in their  .     I was wondering if there was a standard for which version of AlexNet should be used in the self-supervised literature, and if it even makes a difference?     Thanks  "
"Hi,  thanks for your repo.  It would be nice if you could provide the code / the input pipeline which you used to run the NYU RGB-D experiments as well (similar to #4 ). To me it is not entirely clear, how you added the different modalities.  Best,"
"I noticed in your paper you used a modified Alexnet version for STL-10 whose images have size 64x64.  Please, I would like to know the modification you applied in your HalfAlexNet class.  Is the code present in this repo?    Thanks!"
"Hi all,    I was training and evaluating the model when I noticed something weird. On line 223 in train_CMC.py, the resnet model output two feature tensors, each of size (batch size, 128) as expected. However, once I save that resnet model and run it again in LinearProbing.py, the feature tensors (line 233 in LinearProbing.py) suddenly become (batch size, 1024, 7, 7) of size. I'm sure the models and checkpoints are consistent since I never changed them. Is there something I am missing here? Thanks!"
"Dear authors,    Hope this question finds you well. As I was extending CMC to CIFAR10, I noticed that the model doesn't seem to be training, and l_p and ab_p numbers don't seem to be changing either. This is strange, as I never changed the training code. Please see below for a screenshot of this problem. What might be a cause for this? I am aware of #50, but this problem persists even after tuning the softmax and sampling parameters.    <img width=""566"" alt=""Screen Shot 2021-01-31 at 11 49 54 AM"" src=""   "
"not sure if I missed something but it seems to me that if you train on multiple gpus with current implementation, the `AliasMethod`  puts the index on default gpu. The `memory_l` and `memory_ab` are on the correct gpu using the `register_buffer`. Then the `torch.index_select(self.memory_l, 0, idx.view(-1)).detach()` would gives `arguments are located on different GPUs ` error. "
"Hi,  Could you provide the format of the training set and validation set ? "
"     Hi~    The above sample method has the chance to get positive samples, but if the dataset is big enough, the chance to get a positive sample is little.  Am I right?    Thank you!"
"Hi, thanks for sharing the code.  I notice that if I use 8 GPU, due to NCEAverage.cuda(), the first gpu will use like 10500MB while other gpu will use like only 4000MB.   Can't run a ResNet width 2 CMC on an eight 2080Ti machine.   Any idea to balance? So that I can use deeper model and higher dimension."
"Hi! Congrats on the work!     I may find something weird here.          nChannels here should be 512 * width instead of 2048 * width? For example, for ResNet18v1, each view has 256 channel, so that's 512 in total  "
"Hi @HobbitLong, I am trying to implement CMC on CIFAR-10 with a shallow ResNet. However, the accuracy only reaches 60%~70%. I have tried to tune the batch size from 64 to 512 and learning rate from 0.01 to 0.12. In addition, I also tuned the nce_k from 8192 to 65536. Unfortunately, it is not improved yet. I am writing to ask do you have any suggestions on tuning parameters on small datasets like CIFAR-10? Thank you very much."
"Hi @HobbitLong , I am trying to reproduce MoCo v2 on ImageNet 1k. Have you tried to replace the Linear projection head to MLP? Do you think it is necessary that add the batch normalization layer or bias for the fully connected layer? I keep all the hyper-param same as the paper but only could get 61.4~ acc with 4 gpu 256 batch size.     Would you kindly share with me the specific configurations based on your codebase for reproducing MoCov2-ResNet-50?    Thanks a lot!"
"Hi, it seems that you are using the dot product between vectors from two views as a proxy for unknown distribution denoted as p d  in your paper  . In other words, your h Î¸  is the dot product. Theoretically any h Î¸  can work so it's all good.     But doesn't it force the two representations to be similar? I understand the two representations should have high mutual information. But it is not the same as having the two vectors in similar directions.    Obviously it worked out pretty well. But do you think having a parameterized `NCEAverage` loss would have allowed for more representations with not so similar directions but still having high MI?    Thank you again!"
"Hey there, thanks for the well-documented code!    Quick question: Am I correctly assuming that in order to evaluate the model on the 1,000-class ImageNet validation dataset one has to train the linear classifier first (using `LinearProbing.py`)? If so, would it be possible to release pre-trained weights for the classifier as well, such that one can use `classifier.load_state_dict(checkpoint['classifier'])`?"
"Hi,    I see your code for NCESoftmaxLoss as follows:    #########  class NCESoftmaxLoss(nn.Module):      """"""Softmax cross-entropy loss (a.k.a., info-NCE loss in CPC paper)""""""      def __init__(self):          super(NCESoftmaxLoss, self).__init__()          self.criterion = nn.CrossEntropyLoss()        def forward(self, x):          bsz = x.shape   You have one positive for each sample.     So is something missed here??    Thanks.  "
"Dear authors,    I just read your paper recently, and I think it is really interesting and significant.  So I want to see some details about the method by running the code.    I mainly focus on graph representation learning, recommendation, and ML.  I am not familiar with the image dataset and processing.     Could you provide me the datasets to run the code? Or where can I download the Imagenet 100 and STL-100 dataset??    Thanks.    Xu Chen"
"Hello,    Thanks for making this code available.  I am trying to run the pertained alexnet model (downloaded from the Dropbox link) with the following command:    python LinearProbing.py --dataset imagenet --data_folder /share/ctn/users/jwl2182/imagenet_data --save_path . --model_path /home/jwl2182/CMC/CMC_alexnet.pth --model alexnet --learning_rate 0.1 --layer 5 --tb_path /home/jwl2182/CMC/tb --gpu 0      But I get the following error.  Any ideas what might be happening?    RuntimeError: Error(s) in loading state_dict for MyAlexNetCMC:          Unexpected key(s) in state_dict: ""encoder.module.l_to_ab.conv_block_1.1.num_batches_tracked"", ""encoder.module.l_to_ab.conv_block_2.1.num_batches_tracked"", ""encoder.module.l_to_ab.conv_block_3.1.num_batches_tracked"", ""encoder.module.l_to_ab.conv_block_4.1.num_batches_tracked"", ""encoder.module.l_to_ab.conv_block_5.1.num_batches_tracked"", ""encoder.module.l_to_ab.fc6.1.num_batches_tracked"", ""encoder.module.l_to_ab.fc7.1.num_batches_tracked"", ""encoder.module.ab_to_l.conv_block_1.1.num_batches_tracked"", ""encoder.module.ab_to_l.conv_block_2.1.num_batches_tracked"", ""encoder.module.ab_to_l.conv_block_3.1.num_batches_tracked"", ""encoder.module.ab_to_l.conv_block_4.1.num_batches_tracked"", ""encoder.module.ab_to_l.conv_block_5.1.num_batches_tracked"", ""encoder.module.ab_to_l.fc6.1.num_batches_tracked"", ""encoder.module.ab_to_l.fc7.1.num_batches_tracked"".  "
"Hi @HobbitLong , thank you for releasing the code. I wanted to ask a few questions regarding the implementation of `NCEAverage.py`. I understand some of them might be pretty basic questions but hopefully the answers will also help others to understand the code + implementation better.    * What is the purpose of `T=0.07` and why do `out_l` and `out_ab` need to be divided by `T`?    ~~* Is there any advantage of starting out with unit vectors (on average) by implementing `stdv = 1. / math.sqrt(inputSize / 3)`  . I say this because `out_l` and `out_ab` need to be normalized anyway as is done  .~~    ~~* Is this correct that you use a moving average (MA) to update `weight_l` and `weight_ab` (instead of just copying the values directly) because the model itself is learning and the values `l` and `ab` can be noisy? Using a MA reduces variance.~~    ~~* As a follow up, how would this implementation be possible if you were not using memory banks? Is this an incidental advantage of using a memory bank?~~    * ** .~~    Thank you again."
"Hi Yonglong,    Thanks a lot for the great work and sharing the code. I am trying to reproduce the results of MoCo on ImageNet-1k, with ResNet 50. Did you reproduce the results on Kaiming's paper on the full ImageNet? Would you kindly share me the specific configurations for reproducing MoCo-ResNet-50?    Thanks a lot!"
"     Hi, I have a question about using softmax instead of NCE loss.  In that function, every label is set zero including the critic value of positive sample, which has index 0 of the batch.  I want to know the reason. My take on this is that the label should be [1, 0, 0, 0, ...]. Isn't it?"
"Is there any ten crop results?  As I know,  some methods will improve a lot with ten crop, but some may only improve a little. I wonder how much improvement can be get with ten crop in CMC."
"Hi @HobbitLong,    Thanks for such a clean and readable code.    I am interested in using the pre-trained weights that you were kind enough to provide. I downloaded the pre-trained weights CMC_resnet50v2.pth and MoCo_softmax_16384_epoch200.pth. Then, I ran the linear evaluation code with the following commands, but couldn't reproduce the accuracies. The accuracies at the final, 60th, epoch for CMC and MoCo are 62.0% and 57.3% respectively. The accuracies should be 64.1% (from the CMC paper) and 59.4% (from  ).         Have I missed something? Do I need to change the default hyperparameters to get the reported numbers?    Thanks"
"     If I understand the inner work of `eval_moco_ins.py` correctly, the code seems training the downstream task (single FC) using augmented images (train_transform == 'CJ').    This augmentation process not only slows down the training speed of the downstream task but also seems to violate the purpose of evaluation (_Then we freeze the features and train a supervised linear classiï¬er_, said in MoCo paper).    Isn't it right to save the center-cropped average pooled features and perform FC training on those fixed features?"
Why initialize memory value this way? Thanks!   
"Hi, thank you for sharing the code! I am curious about the effect of the data augmentation, concretely the RandomResizedCrop in train_moco_ins.py.   In your codes, the minimum crop scale is 0.2 for most choices but 0.08 for imagenet full dataset with ResNet, however the parameter in other papers such as non parametric instance discrimination is also set to 0.2 when using ResNet as backbone. So I am curious about the choice(0.08 as default torchvision parameter). Is this smaller scale work better in full imagenet? Have you validated the performance on imagenet between 0.08 and 0.2 with a ResNet backbone?  "
Could you tell me the details about how the imagenet100 subset was created?
"Hi @HobbitLong,    Thanks for your nice paper and publlic code!    I have reproduced results of MoCo and InsDIS on ImageNet100 following your steps.  I got 67.44 for MoCo and 66.02 for InsDIS, which are worse than the expected 73.4 and 69.1.    Could you please help me about this?    Best  Mengyuan"
"How to prevent an element in the enqueue come from the same sample as the queryï¼Œ especially when the dataloaderâ€˜s param ""shuffle"" is True?    Thank you     "
"After 126 epochs for training, the loss still seems huge. And the probs for ""L"",""ab"" are only about 0.007.  We set learning rate, batch size to 6e-2 and 1024 (8 Tesla-V100).    Train: [126][930/1252] BT 0.827 (0.953) DT 0.001 (0.234) loss 6.161 (6.071) l_p 0.007 (0.007) ab_p 0.006 (0.006)  torch.Size([1024, 16385, 1])  Train: [126][940/1252] BT 0.630 (0.951) DT 0.001 (0.232) loss 5.945 (6.071) l_p 0.007 (0.007) ab_p 0.006 (0.006)  torch.Size([1024, 16385, 1])    I don't know what's wrong with our experiment setting. Could you share the curves of training loss and probs of 'L' and 'ab'?    "
"Hi,  I'm confused how can I get the imagenet100 dataset online, since I can't find any corresponding link for downloading.  Could you please share the linkï¼Ÿ    Thank you.  "
"Hi,  Would you please share the subset of ImageNet(ImageNet100) you used?  I want to train the MoCo model and compare it with your results!  Thanks!"
"Hi, @HobbitLong, could you please supply the classes you use for the imagenet 100 dataset? thanks. Is the imagenet 100 the same as imagenet except for the class number?"
"Hi @HobbitLong, thanks for your great work and also sharing the code. I guess the ImageNet-100 is not a conventional subset so I wonder if you can share the list since we also don't have enough resources to run on the full ImageNet ==."
"Hi,   I want to use CMC in my own experiment, but the loss is strange. At each epoch, the loss decays as normal (like from 20 to 11). But at the next epoch, the loss becomes nearly the same as begining (the loss is 20 again). I wonder if it is 'normal' in CMC.     Thanks."
Would you please share the MoCo pre-trained weights?
"Hi,    Is it possible to get download link for AlexNet weights trained on ImageNet?    Thanks for sharing the code and ResNet weights."
"     Hi!    While reading your code, I've noticed that for loops in the initialization function of AliasMethod causes a lot of computation.    However, the only entry (  instantiating the class is passing torch.ones, which results in ones self.prob and zeros self.alias in AliasMethod.    What could go wrong if I let them just ones and zeros instead of running for loops while initializing AliasMethod?    Thanks for sharing the code :) (and RepDistiller too!)"
"Hi! Thanks for your code!    I have some questions about your implement. I notice that for negative samples we use memory bank cause 4096 is too large for 1 batch. But for positive samples, why still use memory bank rather than the feature calculated by this batch? Is there any harm for doing this?    Thanks"
"Hi,     Thanks for open-sourcing your work, I have been trying to use CMC on my custom toy dataset which has 2 views (Image (3D), Sensor view (3D)) I'm able to run the model successfully but the Z for view 1 and view 2 is being set to 119973150195712.    I made sure to use L2 norm around the final features from each of the alexnet halfs but I'm really not sure why the Z values are being initialized to such a high value. I kept the nce_m,nce_k and nce_t to the same as that of your code.     Please, can you help me with the same? Thank you"
"Hi, thank you for your code. Would you please provide a download link of ImageNet-trained resnet50/resnet101 weights? "
"Hi,     Thanks for your code. I just wonder how to visualize the AB channel of images in the code as shown in your paper. I could visualize L channel using TensorboardX, but that doesn't work for AB channel."
"Does the version of the torchvision impact the experimental setting, and which version should be used in the experiments? "
"Hi     Thank you for sharing this great work with us.     I saw that you have `spawn` in the code, thus I am wondering your plan to release the code for supporting `DistributedDataParallel`. In particular, I am curious how do you sync the memory bank for _L_ and _ab_, _e.g.,_ in `self.register_buffer('memory_ab')` during training.    Thank you :) "
I enjoyed reading the paper and thanks for uploading the code.     Quick question - would it be possible to also upload the scripts to run the STL-10 eval?     Thanks!
"Hello,  Thank for providing the code !  I am trying to your code on Pytorch 1.1.0 and I receive this error.  Do you have this kind of error on other Pytorch version ?   !   "
None
"Hi, thanks for open-sourcing the code. I wanted to know as to when will you enable the support for resnet to be used as a backbone."
"I've managed to render several of the fitted scenes on Ubuntu 22.04 LTS, with an NVIDIA GeForce 940MX GPU (2 GB RAM). However, I faced several challenges while doing so. Therefore, I am going to document these challenges with their solutions here in case someone finds them helpful.    _Note: this guide was written based on commit  _.    # Step 0    Before starting, make sure you have `gcc` and OpenGL tools installed. Installing these will save you from dealing with a lot of problems later. This can be done by running:     More details on how to install OpenGL can be found  .    Next, make sure you have an appropriate NVIDIA driver installed, and that you have CUDA installed. These can be installed via a quick google search.    Finally, make sure you have Anaconda installed.    # Step 1    Clone the repository     then run `install_deps.sh`     # Step 2  Download the fitted scenes and the rendering network weights as described  . Place the `downloads` folder in the root directory (where the `README.md` file is located).    # Step 3  As described in the `README.md` file, try running the `Person 1` fitted scene:     This will likely not work, and you will observe an `AttributeError`:     To fix this `AttributeError`, run     Try to run the `Person 1` fitted scene again:     However, this will not work again, and you will observe the following error:     The solution to this error can be found  . To fix this error, run     Again, try to run the `Person 1` fitted scene:     Unfortunately, this will still throw an error:     The solution to this error can be found in  . To fix this error, run     One more time: try to run the `Person 1` fitted scene:     Still not quite there yet. You will observe the following error:     The solution to this `RuntimeError` can be found in issue #12. To fix this error, replace the code in `viewer.py` with the code given in the gist in  . In other words, here is the code to put into `viewer.py`:     **FINALLY**, try to run the `Person 1` fitted scene again:     Depending on how much RAM your GPU has, you may see a `CUDA out of memory` error:     In this case, change the `viewport` parameter from `2000,1328` to something smaller, like `500,500`:     This should now work. In case you see the message:     This just means that it is taking some time to load the render. Click on ""Wait"" and the render should load soon."
"Hi, thank you for the great work! I have a doubt about the code. During training, you have set eval mode to true. I'd be grateful if you could help me understand why that's so and whether there is a difference in inference results if we set the model to train mode instead.    Thanks in advance"
"Can we train the scene model on remote servers without Xorg ?  By the way, I have checked the docker you provided and tested whether glfw can work fine, but still got below :  !   "
"Hi, thanks for your work, which is definitly amazing. I want know whether the model can apply to another scene without fine-tune?"
"I try to generate data for training data on a server, but not work.  I run the command:     and log:     Do you have any code to generate data on servers?  Thank you!"
"I want to train the model with my own dataset from scratch, and also want to fine-tune the model with 2 different scenes datasets as you did in paper about scene editing. Could you tell me how to modify the train.py or release the training code from scratch?Looking forward to your reply.Thank you."
Thanks so much for your work.  But I got an error for GLX: Failed to create context.  How can I perform rendering on a remote server without UI?
"in ubuntu 18.04    (npbg) vig-titan2@vigtitan2-System-Product-Name:~/PycharmProjects/npbg/metashape-pro$ LD_LIBRARY_PATH=""python/lib:$LD_LIBRARY_PATH"" ./python/bin/python3.5 -m pip install pillow  Collecting pillow    Using cached        Complete output from command python setup.py egg_info:      /home/vig-titan2/PycharmProjects/npbg/metashape-pro/python/bin/python3.5: error while loading shared libraries: libpython3.5m.so.1.0: cannot open shared object file: No such file or directory            ----------------------------------------  Command ""python setup.py egg_info"" failed with error code 127 in /tmp/pip-build-2xuxzoaq/pillow/      It seems that LD_LIBRARY_PATH is not set properly. can you help me?"
"Hello,    you mentioned that you can render a full HD image in 60 ms, which is impressive. Do you just render with PyTorch or do you use something like TensorRT for inference?    Thanks :)"
"I saw the descriptors are stored in PointTexture class as nn.Parameters, and they are rasterized to image through `index_select`. I am wondering how to get these indices?"
"Do you have any idea regarding what kind of information the neural descriptors at each point represent?     I've read the paper but it was not that clear about the information that are learned in these descriptors.     Also do you have any idea how lighting information (e.g. light sources position, intensity, distribution) could be possibly added as extra parameters?    Furthermore, to my understanding by using your model someone could render any map other than just the RGB values is that correct? For example imagine that someone has the irrandiance map of the scene the in principle he could render the corresponding irrandiance values for unseen views right?"
I try to train a large-scale scene with large number of point clouds.  But I received a CUDA out of memory error at the start of training.    Is there any way to train the model on Multi-GPUs?
None
"Because I possess an AMD GPU rather than an NVIDIA, CUDA-supporting one, I've replaced pytorch with pytorch-directml and offloaded unsupported function calls to the CPU while doing most of the work on the GPU. However, despite having modified no viewer code, it seems to render half of the image in a skewed triangle transposed across the diagonal.    Does anyone know why this is?    NOTE: I used the numpy-based viewer pasted   and my own fork  .    !   "
"hi, thanks for sharing good work.  I would like to know how to set the value of num_samples in train_example.yaml, and how is it implemented in the code. "
"Hello, I want to get the RGB rendered picture of points descriptors rasterizer result. Can U tell me how to get it?  !    "
"Hi, thanks for sharing the good work.  it's difficult to rotate the view in the viewer, blur due to too much rotation.  I wonder if there is a view_matrix that can rotate 360 degrees in a certain position, Or is there another way to customize the view's trajectory? (It's not the same trajectory as training)"
"Hi, thanks for the good work.  When I trained the scene, the PSNR was up to 25, which was clear to see on the Tensorboard. Why is it not as good as the one seen in the viewer during training, and the PSNR obtained is less than 20? (the same pose and point cloud) Does the viewer have any tricks to save the image and reduce distortion?"
"Hi,    Thanks for your great work! I tried training the network from scratch and despite my best efforts, a bug seems to persist on my side. It seems the in render.py (/npbg/npbg/gl/render.py), lines 53 and 83 (self.fbo.activate() and self.fbo.deactivate() respectively) seem to create a problem. I get the following error :      File ""/media/shubhendujena/3E08E5152BF3ABC4/nvs_experiments/npbg_try/npbg/gl/render.py"", line 54, in render      self.fbo.activate()    File ""/home/shubhendujena/anaconda3/envs/npbg/lib/python3.6/site-packages/glumpy/gloo/globject.py"", line 95, in activate      self._activate()    File ""/home/shubhendujena/anaconda3/envs/npbg/lib/python3.6/site-packages/glumpy/gloo/framebuffer.py"", line 375, in _activate      gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, self._handle)    File ""src/errorchecker.pyx"", line 53, in OpenGL_accelerate.errorchecker._ErrorChecker.glCheckError  OpenGL.error.GLError: GLError(          err = 1282,          description = b'invalid operation',          baseOperation = glBindFramebuffer,          cArguments = (GL_FRAMEBUFFER, 1)  )  deleting buffers...  Exception ignored in:  >  Traceback (most recent call last):    File ""/media/shubhendujena/3E08E5152BF3ABC4/nvs_experiments/npbg_try/npbg/gl/programs.py"", line 287, in __del__    File ""/media/shubhendujena/3E08E5152BF3ABC4/nvs_experiments/npbg_try/npbg/gl/programs.py"", line 292, in delete    File ""src/latebind.pyx"", line 32, in OpenGL_accelerate.latebind.LateBind.__call__    File ""src/wrapper.pyx"", line 311, in OpenGL_accelerate.wrapper.Wrapper.__call__    File ""/home/shubhendujena/anaconda3/envs/npbg/lib/python3.6/site-packages/OpenGL/platform/baseplatform.py"", line 401, in __call__    File ""/home/shubhendujena/anaconda3/envs/npbg/lib/python3.6/site-packages/OpenGL/platform/baseplatform.py"", line 381, in load  ImportError: sys.meta_path is None, Python is likely shutting down  deleting buffers...  Exception ignored in:  >  Traceback (most recent call last):    File ""/media/shubhendujena/3E08E5152BF3ABC4/nvs_experiments/npbg_try/npbg/gl/programs.py"", line 287, in __del__    File ""/media/shubhendujena/3E08E5152BF3ABC4/nvs_experiments/npbg_try/npbg/gl/programs.py"", line 292, in delete    File ""src/latebind.pyx"", line 32, in OpenGL_accelerate.latebind.LateBind.__call__    File ""src/wrapper.pyx"", line 311, in OpenGL_accelerate.wrapper.Wrapper.__call__    File ""/home/shubhendujena/anaconda3/envs/npbg/lib/python3.6/site-packages/OpenGL/platform/baseplatform.py"", line 401, in __call__    File ""/home/shubhendujena/anaconda3/envs/npbg/lib/python3.6/site-packages/OpenGL/platform/baseplatform.py"", line 381, in load  ImportError: sys.meta_path is None, Python is likely shutting down  deleting buffers...  Exception ignored in:  >  Traceback (most recent call last):    File ""/media/shubhendujena/3E08E5152BF3ABC4/nvs_experiments/npbg_try/npbg/gl/programs.py"", line 287, in __del__    File ""/media/shubhendujena/3E08E5152BF3ABC4/nvs_experiments/npbg_try/npbg/gl/programs.py"", line 292, in delete    File ""src/latebind.pyx"", line 32, in OpenGL_accelerate.latebind.LateBind.__call__    File ""src/wrapper.pyx"", line 311, in OpenGL_accelerate.wrapper.Wrapper.__call__    File ""/home/shubhendujena/anaconda3/envs/npbg/lib/python3.6/site-packages/OpenGL/platform/baseplatform.py"", line 401, in __call__    File ""/home/shubhendujena/anaconda3/envs/npbg/lib/python3.6/site-packages/OpenGL/platform/baseplatform.py"", line 381, in load  ImportError: sys.meta_path is None, Python is likely shutting down  deleting buffers...  Exception ignored in:  >  Traceback (most recent call last):    File ""/media/shubhendujena/3E08E5152BF3ABC4/nvs_experiments/npbg_try/npbg/gl/programs.py"", line 287, in __del__    File ""/media/shubhendujena/3E08E5152BF3ABC4/nvs_experiments/npbg_try/npbg/gl/programs.py"", line 292, in delete    File ""src/latebind.pyx"", line 32, in OpenGL_accelerate.latebind.LateBind.__call__    File ""src/wrapper.pyx"", line 311, in OpenGL_accelerate.wrapper.Wrapper.__call__    File ""/home/shubhendujena/anaconda3/envs/npbg/lib/python3.6/site-packages/OpenGL/platform/baseplatform.py"", line 401, in __call__    File ""/home/shubhendujena/anaconda3/envs/npbg/lib/python3.6/site-packages/OpenGL/platform/baseplatform.py"", line 381, in load  ImportError: sys.meta_path is None, Python is likely shutting down    It goes away if I comment out these lines but then I get a bunch of windows opening every few steps. I'd be grateful if you could give me some tips on how I can go about resolving this.    Thanks in advance "
"Hello, I try to run docker/tools/build.sh, however, I get such error:  Can u help me?     "
"Hi,   I am trying to fit the scene ""scene0000_00"" from the ScanNet dataset. The regression loss is always 0. Do you have an idea on where could be the problem?"
"Hi,  I am trying to fit a scene and I have a problem with the dataloader. At the second epoch, even though the dataset is loaded, the scene.program seems to be None. Do you have an idea on where could the problem ?      Here is error message :    ### EPOCH 1  > TRAIN  EVAL MODE IN TRAIN  model parameters: 1928771  running on datasets [0]  proj_matrix was not set  total parameters: 76715531  Traceback (most recent call last):    File ""train.py"", line 517, in        train_loss = run_train(epoch, pipeline, args, iter_cb)    File ""train.py"", line 253, in run_train      return run_epoch(pipeline, 'train', epoch, args, iter_cb=iter_cb)    File ""train.py"", line 228, in run_epoch      run_sub(dl, extra_optimizer)    File ""train.py"", line 118, in run_sub      for it, data in enumerate(dl):    File ""C:\Users\user\.conda\envs\npbg\lib\site-packages\torch\utils\data\dataloader.py"", line 517, in __next__      data = self._next_data()    File ""C:\Users\user\.conda\envs\npbg\lib\site-packages\torch\utils\data\dataloader.py"", line 557, in _next_data      data = self._dataset_fetcher.fetch(index)  # may raise StopIteration    File ""C:\Users\user\.conda\envs\npbg\lib\site-packages\torch\utils\data\_utils\fetch.py"", line 44, in fetch      data = [self.dataset[idx] for idx in possibly_batched_index]    File ""C:\Users\user\.conda\envs\npbg\lib\site-packages\torch\utils\data\_utils\fetch.py"", line 44, in        data = [self.dataset[idx] for idx in possibly_batched_index]    File ""C:\Users\user\.conda\envs\npbg\lib\site-packages\torch\utils\data\dataset.py"", line 219, in __getitem__      return self.datasets[dataset_idx][sample_idx]    File ""C:\Users\user\Documents\npbg\npbg\datasets\dynamic.py"", line 246, in __getitem__      input_ = self.renderer.render(view_matrix=view_matrix, proj_matrix=proj_matrix)    File ""C:\Users\user\Documents\npbg\npbg\datasets\dynamic.py"", line 68, in render      self.scene.set_camera_view(view_matrix)    File ""C:\Users\user\Documents\npbg\npbg\gl\programs.py"", line 366, in set_camera_view      self.program['m_view'] = inv(m).T"
"Hello, thank you for your excellent work and the open-source code!  I have finetuned your pre-trained model, however, I can not find the way about inference.  I saw inference related settings in your config files, but I canâ€™t use them correctly.  Thank you for your reply!  "
"Hi  I printed a trained data of epoch 20 by using viewer.py, but there's an issue  As shown in the image below, it doesn't have any color  I think there should be something wrong with the process of fitting descriptor  Is there any solution for this?  Thanks.    !   !   !   "
"Hi, with your help, i some how managed to solve problems including pytorch rtx 30 series compatibility problem.  I'm trying to apply this npbg to 3d point cloud of our school building.   The dataset consists of 708 photos and I successfully built the required files with metashape.   However, when i try to run the following command,   `python train.py --config configs/train_example.yaml --pipeline npbg.pipelines.ogl.TexturePipeline --dataset_names  `    I get the error below.     `multiprocessing.pool.MaybeEncodingError: Error sending result (entire message is at the bottom): [( ,  )]'. Reason: 'error(""'i' format requires -2147483648 <= number <= 2147483647"",)'`    Here is the dataset built with metashape. Give it a try, it won't take that long.        When i google the message, it says it's something related to python 3.5~3.6's problem of sending and receiving large data. Do you have any solution to this?   Thanks            Entire printed message of above command :               `      "
"Hi, currently testing the code with sample data but there seems to be an error loading numpy_formathandler accelerator.   After calling python viewer.py --config downloads/person_1.yaml --viewport 2000,1328 --origin-view, the view.py window freezes with above error.  I can't figure out what's wrong here. Any idea?   Thanks!    !         "
"Hello! Thanks for sharing the code! I just want to test the code by viewing some results, but I encounter some problems with the following line.       And the error message is as follows:       I tested the code on RTX 2080Ti with CUDA 10.0.130."
"Hello,  I would like to understand more about your descriptors. So if you train your network on 100 ScanNet scenes then you would have a set of 100 descriptors (one for each scene) and each descriptor would contains a set of N vectors (N points in the pointcloud). I wonder if my understand is correct or not.    Also I have another question about the two-stage learning. So in the pretraining stage, u guys use a set of scenes (set A) to training both descriptors and rendering network. Then in the fine-tuning stage, u guys zero out the descriptors and fine-tuned the rendering network to fit a new set of scene (set B), right ? My question is: after the fine-tuning stage, it is obvious the network can render novel view in set B but I wonder if the network can generalize to set A anymore ? "
"First of all this work is impressive. Thanks for sharing the codes with the wider community.    My question pertains mainly to the cost function used for converging the model. While the perceptual cost function alone has proven to be effective in achieving good results(both in the paper and in my own validation experiments), I wonder if the feasibility of alternative cost functions such as the L1 has been investigated as well ? Or perhaps some combination scheme of multiple cost functions?  What is the core motivation behind training with the VGG cost alone ?    Apologies if this has been answered elsewhere but I found no pertinent discussions in the paper or anywhere in the existing issues.   "
"Hi guys,    I would be interested to know whether it is possible to fit different scenes based on other existing datasets. For example I would like to understand and get an idea how to fit the redwood dataset (  I have read the README regarding how to fit our own scenes but it is not clear to me how to do the same with the e.g. the redwood data.    Thus, would be easy to give some feedback here how to do that.    Thanks."
"Hi,    I am trying to run the example code from the readme file but I am getting the following error:        any idea what could be the cause?"
"  Hello, thank you for your excellent work and the open source code!    Following the guild in readme, I can successfully fit a new scene by building the reconstruction with Agisoft Metashape Pro and then fitting descriptors.    However, when I directly use the reconstructions (eg.   provided in ScanNet dataset, I found that I can not fitting descriptors correctlly then.    Are there some differences between the pointcloud built by Agisoft Metashape Pro and the pointcloud provided by ScanNet dataset? And how can I fit a scene in Scannet dataset such as scene0000_00 with the pointcloud provided by ScanNet dataset?    Thank you for your reply!"
"I am trying to train a new scene and am running into memory issues with training.    I have tried with a single Titan RTX (24GB) card, and a multi GPU setup (4 x Tesla T4, each 16GB)  With both I am receiving a CUDA out of memory error on the first epoch of training.    I was wondering which GPU set ups you used for training? As I would assume these should be sufficiently big to train the model."
"Each ScanNet scene contains RGB-D images, so I can project 2D pixels to 3D point cloud and save them to a `.ply` file.   But how to modify `path_example.yaml` and `train_example.yaml` to fit the descriptors on this `ply` file?    Any guidelines or suggestions?"
As highlighted on the scalismo forum (  there is an error in the current computation of the transition probability. The parameter update proposal is not correctly projected into the posterior model.     Current version:     example of correct projection:     Testing needs to be performed before included in the project.
This is coming and nothing on internet is helping. pls solve this issue or let us know the solution
"I was trying to use the code to visualize the mesh 3D. I noticed that the code only allow me to visualize the scene using the panorama images. Is it possible instead to visualize the scene only using the prospective images instead of the panorama.   For example given this 2D image visualize the 3D mesh of it, using only this view     !   "
"Hello,    I followed your advice in Issue #13 to infer the semantic labels associated with the bounding boxes, but have only succeeded in inferring 77% of them from the instance.png and semantic.png images of each scene (for 438311 bounding boxes, only 337345 could be inferred, that is more exactly 76.9647579002%).    This missing quarter of information is quite big, and seems as I feared in my message in Issue #13, to be due to object occlusion in the images. Therefore, it seems that it is impossible for end users to infer the missing data. Would you please consider filling in the gap =) ?    Wish you a nice day !"
"Hello again !    I succeeded in associating the 3D bounding boxes with semantic labels, and started digging around the dataset, and while exploring the first scene, I realized that the ceiling lamps were misidentified as sofas (you can check by simply using a color picker to compare the color of the sofas in the background with the ceiling lamps in image Structured3D/scene_00000/2D_rendering/485142/panorama/full/semantic.png). I'll try to put in place some automatic detectors and report back if I find any other incongruity.    Wish you a pleasant day."
"Hi,    I noticed that when downloading the dataset from OneDrive, the operation fails(always), at least for my Ubuntu 16 system. I guess the problem is with unstable OneDrive server.    I was able to solve this issue by using a download manager."
hi   when i execute any visualizer of Structured3D file i'm facing a issue i have mention below for all python file    ### Visualize 3D Annotation    visualize_3d.py: error: argument --scene: invalid int value: 'scene_id'    Visualize 3D Textured Mesh    visualize_mesh.py: error: argument --scene: invalid int value: 'scene_id'    Visualize 2D Layout    visualize_layout.py: error: argument --scene: invalid int value: 'scene_id'    Visualize 3D Bounding Box    visualize_bbox.py: error: argument --scene: invalid int value: 'scene_id'    please help me out    Thank you      
"Hi,    Thanks for the great work! I am trying to generate point cloud from the dataset. I follow this   and generate the point cloud for scene_00000 based on the Structured3D_perspective_full_00 dataset. However, there are some missing parts obviously. Do you think this is normal or is there a way to generate a more complete point cloud? Thanks in advance!    !     Best,  Yue        "
"Hi JiaZhangï¼Œ    Thanks for your excellent work!  I have tried many times to download the dataset, but none of them worked, and I check this downlading on different PCs of different locations and ISPs.   There are a lot of files that use Google Cloud as a hub, and users can download very quickly, so maybe you could consider using Google Drive or Google Cloud as an option.  "
"Hi,    I'm trying to create full point clouds from the RGB and depth images from the full perspective folders. I'm using Open3D to first create RGBD images and then use the intrinsics and extrinsics derived from the ""parse_camera_info"" function in utils to create the point clouds. However, when I try to merge two point clouds from different images of the same room they don't align properly.     <img width=""651"" alt=""Screenshot 2022-02-09 at 13 33 14"" src=""     The colors also don't seem to work properly, but this is not as much of a problem than the two point clouds not aligning. I was wondering if I could get some help with my problem. I'll attach my code so it's easier to see where it could go wrong.     Thanks!        import open3d as o3d      import numpy as np      import cv2      import torch      import matplotlib.pyplot as plt            def normalize(vector):          return vector / np.linalg.norm(vector)            def parse_camera_info(camera_info, height, width):          """""" extract intrinsic and extrinsic matrix          """"""          lookat = normalize(camera_info[3:6])          up = normalize(camera_info[6:9])                W = lookat          U = np.cross(W, up)          V = -np.cross(W, U)                print(""W"", W)          print(""U"", U)          print(""V"", V)                rot = np.vstack((U, V, W))                trans = camera_info[:3]          # myorder = [1, 2, 0]          # trans = [trans[i] for i in myorder]                xfov = camera_info[9]          yfov = camera_info[10]                K = np.diag([1, 1, 1])                K[0, 2] = width / 2          K[1, 2] = height / 2                K[0, 0] = K[0, 2] / np.tan(xfov)          K[1, 1] = K[1, 2] / np.tan(yfov)                return rot, trans, K            def create_pcd_perspective(rgb_image_path, depth_image_path, camera_path):          color_raw = o3d.io.read_image(rgb_image_path)          depth_raw = o3d.io.read_image(depth_image_path)          rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(              color_raw, depth_raw, depth_scale=10000, convert_rgb_to_intensity=False)                with open(camera_path) as f:              lines = f.readlines()[0].split("" "")                # plt.subplot(1,2,1)          # plt.imshow(np.array(rgbd_image.color))          # plt.subplot(1,2,2)          # plt.imshow(np.array(rgbd_image.depth))          # plt.show()                camera_info = [float(x) for x in lines]                rot, trans, K = parse_camera_info(camera_info, np.array(color_raw).shape[0], np.array(color_raw).shape[1])          print(""rot"", rot)          print(""trans"", np.array(trans))          print(""K"", K)                trans = np.array(trans) / 10000                w = np.array(color_raw).shape[0]          h = np.array(color_raw).shape[1]          fx = K[0][0]          fy = K[1][1]          cx = K[0][2]          cy = K[1][2]                extrinsic = np.vstack((np.hstack((np.array(rot), np.array([trans]).T)), np.array([0, 0, 0, 1])))          # extrinsic = np.linalg.inv(extrinsic)          print(""ex"", extrinsic)                intrinsic = o3d.camera.PinholeCameraIntrinsic(w, h, fx,fy, cx, cy)          cam = o3d.camera.PinholeCameraParameters()          cam.intrinsic = intrinsic          cam.extrinsic = extrinsic          pcd = o3d.geometry.PointCloud.create_from_rgbd_image(              rgbd_image, cam.intrinsic, cam.extrinsic)          pcd.transform([[-1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]])          print(""--------"")          return pcd            if __name__ == '__main__':          rgb_image_path = ""Structured3D_perspective_full_00/scene_00000/2D_rendering/485142/perspective/full/0/rgb_rawlight.png""          depth_image_path = ""Structured3D_perspective_full_00/scene_00000/2D_rendering/485142/perspective/full/0/depth.png""          camera_path = ""Structured3D_perspective_full_00/scene_00000/2D_rendering/485142/perspective/full/0/camera_pose.txt""                rgb_image_path_1 = ""Structured3D_perspective_full_00/scene_00000/2D_rendering/485142/perspective/full/1/rgb_rawlight.png""          depth_image_path_1 = ""Structured3D_perspective_full_00/scene_00000/2D_rendering/485142/perspective/full/1/depth.png""          camera_path_1 = ""Structured3D_perspective_full_00/scene_00000/2D_rendering/485142/perspective/full/1/camera_pose.txt""                pcd = create_pcd_perspective(rgb_image_path, depth_image_path, camera_path)          # o3d.visualization.draw_geometries([pcd])                pcd_1 = create_pcd_perspective(rgb_image_path_1, depth_image_path_1, camera_path_1)          # o3d.visualization.draw_geometries([pcd_1])                pcd_combined = pcd + pcd_1                o3d.visualization.draw_geometries([pcd_combined])                o3d.io.write_point_cloud(""test.ply"", pcd_combined)          "
"Hey,    I am currently trying to use Structured3D for a top-down layout estimation task. Is there a way to generate top-down layout maps like the ones below for Structured 3D? I think a 3D reconstruction using the panoramas would be quite sparse. Note that the top down layout includes information about obstacles, making it different from a floorplan. Any help would be appreciated. Thanks!    !   !     "
"I tried to download the dataset, but found both `Structured3D_perspective_full_00.zip` and `Structured3D_perspective_full_01.zip` to be corrupted, so I stopped the downloading.    When trying to unzip the archives, I get the following error:      I tried multiple zip tools, but it is the same for all of them. Are the files corrupted, or is this not a zip archive, or compressed with a special algorithm?"
"Hi,  Thanks for your excellent work!  I noticed that in addition to the roomã€windowã€door categories, there is also undefined in semantic field in annotation_3d.json file, what does undefined mean? Does that mean a room but the function is not clear or just has no semantic label?  And I find that some ID in in semantic field in annotation_3d.json file has no relative image, eg, scene_00147: 2113 which is labeled as balcony, scene_00092: 234 which is labeled as bathroom.  !   "
"Hi,    Thanks for your wonderful dataset.    I download the dataset you provide from the link you shared with nobodypeng@bupt.edu.cn. However, I found that   and   are the same. They are all from   to  . And md5 sum of both of them is      Scenes between   and   seems missing. It seems that   should start from   but start from    Could you please share the missing part? Thanks a lot.  "
"Hi,    The paper says ""At the time of rendering, a panoramic or pin-hole camera is placed at random  locations not occupied by objects in the room"".  Is the camera height also random or a fixed height?    Thanks,  Bing"
"Hi,    Thank you for the dataset. I would like to ask though whether it would be possible to get/extract the complete ground truth 3D mesh with the objects, as well as the light sources properties (e.g. position, dimensions, possibly IES data) used within the scenes.    Thanks."
"Hi,    I want to reproduce the perspective images in Blender and I want to know what is the focal length and sensor width of the cameras you used to render these images. Can you please provide me with this information?     Thanks,    Micael"
"Hi, great dataset! I was wondering if the object semantic annotations, currently only in semantic.png files it seems, could somehow be inferred from the 3D bounding box id's in bbox_3d.json, or in any other 3D data? If not, could this easily be added? Thank you!!!"
"Hi,    I would like to know if you can provide instance annotations for the Panoramas? Otherwise, is there a way to convert instance annotations from the perspectives to the Panoramas?    Thanks,    Micael"
"Hi,    Thanks again for this great dataset. I want to generate a point cloud of a scene from rgbd perspective images and camera poses but the point cloud is distorted as shown below. The point cloud below is obtained from scene: 00000 room_id: 485142 and position_id: 0. I used the parse_camera_info function from utils to obtain the camera intrinsics and extrinsics and then generated the point cloud using   function from the link above as you mentioned that you adapted some of your code from there. Any idea as to what might be causing the distortion? Are there any changes to the rgbd_to_world function I need to do in order to get a more accurate point cloud?    <img width=""519"" alt=""Screen Shot 2020-04-07 at 12 59 10 PM"" src=""   "
"Hi,    Thanks for this great dataset. I found out that the basis of the 3D bounding boxes (obtained from bbox_3d.json) of the two opposite facing chairs in Scene_00000 with instance ID's->(59,60) gotten from instance.png are the same. Could this be an error in the annotation?    "
"Hi,    First of all, I want to thank you for this great dataset.  I try to generate point clouds based on the panorama images, however, only camera xyz location are provided. I'm wondering where can I find the camera rotation parameters?  If I assume the camera has the same rotation, the point clouds generated are miss stitched.    Thanks,    Daxuan    !   "
"Hello, I read your paper, but I didn't understand how to generate photo-realistic 2D images. Can you help meï¼Ÿ"
"In the readme.md, it says the semantic.png stores an 8-bit number indicating the nyuv2 label-ids. But actually in the dataset, the semantic.png stores a 4-channel color for each pixel. It seems that one color denotes a certain category, but could you please provide the color-category mapping list? Thank you!"
When trying to download the 3D dataset the downloads always fail part way through. Often times the page does not load and gives an error message:    
"It doesn't seem to be written anywhere, but the Structured3D seems to be using the ScanNet class to color mappings as provided here:       Would be great if this could be documented, and if some code that could convert this to a 1...40 representation."
"Thank you for your outstanding work!    I have a question about available plane labels. I notice that in Figure 1 (b) Planes of your paper, several walls are labeled. So I wonder have you just labeled walls, or your dataset actually provides planar labels of all kinds of objects?    !     Thank you."
Thank you for sharing the useful dataset!  I have trained   on Structured3D for general layout.  I found some confusing example during testing:  !   This is a snapshot from my  .    The top-down view of the ground truth (green solid lines) is **self-intersecting**?  I put the estimated result (red dot lines) for your reference.
Where is the code of attention normalization? I cannot find it in the whole project.
"Error(s) in loading state_dict for AOGNet:   Missing key(s) in state_dict: ""stem.0."
"I use 4 P40 GPUs, so I change the configs about GPUs like these:  GPUS=0,1,2,3  NUM_GPUS=4  NUM_WORKERS=4  remove --fp16    batch_size: 128  lr_scale_factor: 128    the logs is bellow:  PyTorch VERSION: 1.1.0  CUDA VERSION: 9.0.176  CUDNN VERSION: 7501  GPU TYPE: Tesla P40  Warning:  if --fp16 is not used, static_loss_scale will be ignored.  Warning:  if --fp16 is not used, static_loss_scale will be ignored.  Warning:  if --fp16 is not used, static_loss_scale will be ignored.  Warning:  if --fp16 is not used, static_loss_scale will be ignored.  => creating aognet  => Params (double-check): 12.373355M  Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.  Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.  Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.  Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.  => ! Weight decay applied to FeatNorm parameters  Epoch: [0][0/2503]      Time 7.958 (7.958)      Speed 64.334 (64.334)   Data 0.623 (0.623)      Loss 6.9367394447 (6.9367)      Prec@1 0.000 (0.000)    Prec@5 0.391 (0.391)    lr 0.000032  Epoch: [0][10/2503]     Time 1.237 (1.862)      Speed 414.005 (274.955) Data 0.001 (0.057)      Loss 6.9408278465 (6.9327)      Prec@1 0.000 (0.053)    Prec@5 0.586 (0.550)    lr 0.000352  Epoch: [0][20/2503]     Time 1.235 (1.565)      Speed 414.680 (327.186) Data 0.000 (0.030)      Loss 6.9295492172 (6.9330)      Prec@1 0.000 (0.074)    Prec@5 0.391 (0.502)    lr 0.000671  Epoch: [0][30/2503]     Time 1.235 (1.461)      Speed 414.436 (350.407) Data 0.001 (0.021)      Loss 6.9239211082 (6.9295)      Prec@1 0.195 (0.082)    Prec@5 0.391 (0.498)    lr 0.000991  Epoch: [0][40/2503]     Time 1.233 (1.408)      Speed 415.177 (363.574) Data 0.000 (0.016)      Loss 6.9220929146 (6.9269)      Prec@1 0.000 (0.091)    Prec@5 0.586 (0.557)    lr 0.001310  Epoch: [0][50/2503]     Time 1.235 (1.376)      Speed 414.457 (371.963) Data 0.000 (0.013)      Loss 6.9215707779 (6.9268)      Prec@1 0.000 (0.084)    Prec@5 0.195 (0.532)    lr 0.001630  Epoch: [0][60/2503]     Time 1.261 (1.355)      Speed 406.069 (377.859) Data 0.001 (0.011)      Loss 6.9190740585 (6.9267)      Prec@1 0.195 (0.090)    Prec@5 0.586 (0.525)    lr 0.001950  Epoch: [0][70/2503]     Time 1.234 (1.340)      Speed 414.789 (382.103) Data 0.000 (0.009)      Loss 6.9427552223 (6.9263)      Prec@1 0.000 (0.091)    Prec@5 0.195 (0.506)    lr 0.002269  Epoch: [0][80/2503]     Time 1.264 (1.328)      Speed 404.973 (385.508) Data 0.001 (0.008)      Loss 6.9220204353 (6.9271)      Prec@1 0.000 (0.084)    Prec@5 0.195 (0.482)    lr 0.002589  Epoch: [0][90/2503]     Time 1.264 (1.319)      Speed 404.948 (388.145) Data 0.001 (0.007)      Loss 6.9311232567 (6.9266)      Prec@1 0.195 (0.084)    Prec@5 0.195 (0.489)    lr 0.002909  Epoch: [0][100/2503]    Time 1.262 (1.311)      Speed 405.781 (390.479) Data 0.001 (0.007)      Loss 6.9314498901 (6.9261)      Prec@1 0.000 (0.091)    Prec@5 0.391 (0.493)    lr 0.003228  Epoch: [0][110/2503]    Time 1.263 (1.305)      Speed 405.252 (392.227) Data 0.000 (0.006)      Loss 6.9336290359 (6.9266)      Prec@1 0.195 (0.090)    Prec@5 0.391 (0.489)    lr 0.003548      I also run the BN version, it's also not convergent after 20 epoches."
"close fp16, change batch to 128.    the log is:  PyTorch VERSION: 1.0.0  CUDA VERSION: 9.0.176  CUDNN VERSION: 7401  GPU TYPE: Tesla P40  Warning:  if --fp16 is not used, static_loss_scale will be ignored.  => creating aognet  => Params (double-check): 12.373355M  Warning:  if --fp16 is not used, static_loss_scale will be ignored.  Warning:  if --fp16 is not used, static_loss_scale will be ignored.  Warning:  if --fp16 is not used, static_loss_scale will be ignored.  => ! Weight decay applied to FeatNorm parameters   Traceback (most recent call last):    File ""/home/zzt/AOGNets/AOGNet-v2/scripts/../tools/main_fp16.py"", line 774, in        main()    File ""/home/zzt/AOGNets/AOGNet-v2/scripts/../tools/main_fp16.py"", line 340, in main      cfg.dataaug.mixup_rate, cfg.dataaug.labelsmoothing_rate)    File ""/home/zzt/AOGNets/AOGNet-v2/scripts/../tools/main_fp16.py"", line 475, in train      output = model(input)    File ""/usr/local/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__      result = self.forward(*input, **kwargs)    File ""/usr/local/python3/lib/python3.6/site-packages/apex/parallel/distributed.py"", line 560, in forward      result = self.module(*inputs, **kwargs)    File ""/usr/local/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__      result = self.forward(*input, **kwargs)    File ""/home/zzt/AOGNets/AOGNet-v2/scripts/../tools/../models/aognet/aognet.py"", line 643, in forward  Traceback (most recent call last):    File ""/home/zzt/AOGNets/AOGNet-v2/scripts/../tools/main_fp16.py"", line 774, in        main()    File ""/home/zzt/AOGNets/AOGNet-v2/scripts/../tools/main_fp16.py"", line 340, in main      cfg.dataaug.mixup_rate, cfg.dataaug.labelsmoothing_rate)    File ""/home/zzt/AOGNets/AOGNet-v2/scripts/../tools/main_fp16.py"", line 475, in train      output = model(input)    File ""/usr/local/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__      result = self.forward(*input, **kwargs)    File ""/usr/local/python3/lib/python3.6/site-packages/apex/parallel/distributed.py"", line 560, in forward      result = self.module(*inputs, **kwargs)    File ""/usr/local/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__      y = self.stage0(y)    File ""/usr/local/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__      result = self.forward(*input, **kwargs)    File ""/home/zzt/AOGNets/AOGNet-v2/scripts/../tools/../models/aognet/aognet.py"", line 643, in forward      result = self.forward(*input, **kwargs)    File ""/usr/local/python3/lib/python3.6/site-packages/torch/nn/modules/container.py"", line 92, in forward      input = module(input)    File ""/usr/local/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__      y = self.stage0(y)    File ""/usr/local/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__      result = self.forward(*input, **kwargs)    File ""/usr/local/python3/lib/python3.6/site-packages/torch/nn/modules/container.py"", line 92, in forward      result = self.forward(*input, **kwargs)    File ""/home/zzt/AOGNets/AOGNet-v2/scripts/../tools/../models/aognet/aognet.py"", line 223, in forward      input = module(input)    File ""/usr/local/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__      tnode_output = getattr(self, op_name)(tnode_tensor_op)      result = self.forward(*input, **kwargs)    File ""/home/zzt/AOGNets/AOGNet-v2/scripts/../tools/../models/aognet/aognet.py"", line 223, in forward    File ""/usr/local/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__      tnode_output = getattr(self, op_name)(tnode_tensor_op)    File ""/usr/local/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__      result = self.forward(*input, **kwargs)    File ""/home/zzt/AOGNets/AOGNet-v2/scripts/../tools/../models/aognet/operator_singlescale.py"", line 132, in forward      result = self.forward(*input, **kwargs)    File ""/home/zzt/AOGNets/AOGNet-v2/scripts/../tools/../models/aognet/operator_singlescale.py"", line 132, in forward      y = self.conv_norm_ac_2(y)    File ""/usr/local/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__      y = self.conv_norm_ac_2(y)    File ""/usr/local/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__      result = self.forward(*input, **kwargs)    File ""/home/zzt/AOGNets/AOGNet-v2/scripts/../tools/../models/aognet/operator_singlescale.py"", line 91, in forward      result = self.forward(*input, **kwargs)    File ""/home/zzt/AOGNets/AOGNet-v2/scripts/../tools/../models/aognet/operator_singlescale.py"", line 91, in forward      y = self.conv_norm(x)    File ""/usr/local/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__      y = self.conv_norm(x)    File ""/usr/local/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__      result = self.forward(*input, **kwargs)    File ""/home/zzt/AOGNets/AOGNet-v2/scripts/../tools/../models/aognet/operator_singlescale.py"", line 73, in forward      result = self.forward(*input, **kwargs)      y = self.conv_norm(x)    File ""/usr/local/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__    File ""/home/zzt/AOGNets/AOGNet-v2/scripts/../tools/../models/aognet/operator_singlescale.py"", line 73, in forward      y = self.conv_norm(x)    File ""/usr/local/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__      result = self.forward(*input, **kwargs)    File ""/usr/local/python3/lib/python3.6/site-packages/torch/nn/modules/container.py"", line 92, in forward      input = module(input)    File ""/usr/local/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__      result = self.forward(*input, **kwargs)    File ""/usr/local/python3/lib/python3.6/site-packages/torch/nn/modules/container.py"", line 92, in forward      result = self.forward(*input, **kwargs)    File ""/home/zzt/AOGNets/AOGNet-v2/scripts/../tools/../models/aognet/operator_basic.py"", line 176, in forward      input = module(input)    File ""/usr/local/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__      y = self.attention_weights(x) # bxk # or use output as attention input    File ""/usr/local/python3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__      result = self.forward(*input, **kwargs)    File ""/home/zzt/AOGNets/AOGNet-v2/scripts/../tools/../models/aognet/operator_basic.py"", line 176, in forward      result = self.forward(*input, **kwargs)    File ""/home/zzt/AOGNets/AOGNet-v2/scripts/../tools/../models/aognet/operator_basic.py"", line 147, in forward      var = torch.var(x, dim=(2, 3)).view(b, c, 1, 1)  TypeError: var() received an invalid combination of arguments - got (Tensor, dim=tuple), but expected one of:   * (Tensor input, bool unbiased)        didn't match because some of the keywords were incorrect: dim   * (Tensor input, int dim, bool unbiased, bool keepdim, Tensor out)  "
None
"Hello, thanks for your work!    I have a question about the outputs of the pre-trained model. We acquired the pre-trained model from this  . However, our outputs are not the same as those presented in the paper. Unlike the presented results, our results don't generate neck parts of the 3D faces.     Is there any additional processing part to add neck parts to our results?    I am attaching one of our results and a result from the paper.  <img width=""250"" alt=""output"" src=""   <img width=""250"" alt=""paper-result"" src=""     Thank you."
"Hello, thanks for your work!    I'm doing experiments for my graduation project, and I'd like to make sure they're consistent with your research. Do you have input vectors or seed values for the faces on your paper?    Thank you."
Hello! Thanks for sharing the code and your work.    Can you please point out the code to cylindrically unwrap the mean face for creating a UV representation corresponding to a specific mesh topology.    Thanks!
"Hi, I'm following your work! And I wonder if your network adopts some training strategies like ProGAN does, such as progressive growth."
"Hiï¼Œwhen I run python test.py, it has problem like following picture. How to solve this? @barisgecer   !   "
"Hello, thank you for releasing the code for this amazing research. Do you plan or releasing the weights from your training?"
"Hi, thanks for your great job!     However, I am a little confused about the identity-generic maps. As mentioned in the paper, the detailed normal map for skin pores, one of the generic maps, should be added on the identity-specific map for improved rendering. How can we get this detailed normal map? Is there any ground truth for the skin pores?    Looking forward for your reply, thanks a lot!"
Hi! I would love to try this outï¼Œ When do you target releasing the code?
"Hi, I have a question about normal UVmap after reading your paper. The vertices of face can obtain directly from shape UVmap, so the normal UVmap is for recalculating the vertice coordinates to improve the face mesh precision or it is only used to render face obj?"
ä½œè€…ä½ å¥½ï¼Œæˆ‘çœ‹è®ºæ–‡ä¸­æ˜¯twolineçš„è¿‡ç¨‹ï¼Œè€Œä»£ç æ˜¯onelineï¼Œä¹Ÿæ²¡æœ‰H_ab*H_ba-Içš„loss. æ˜¯æˆ‘çœ‹é”™äº†ï¼Œè¿˜æ˜¯ä»€ä¹ˆæƒ…å†µï¼Ÿ
None
! 
"Hello, as for the realization of mask M () network mentioned in your paper, I still don't understand how you realize the function mentioned in your paper through the mask M () module after reading the code. I wonder if you could explain it to me again. Thank you."
"Hi,  Thank you very much for sharing your code.    I tried to reproduce the results for **SIFT** with **RANSAC** but they are:  {'RE': 1.7144430284550183, 'LT': 1.8459903052573097, 'LL': 2.4626396163105597, 'SF': 1.8192603742769062, 'LF': 1.799048634072144}  In the paper (Table 1-a row 5) the results are matching for **RE** and **SF** and close to **LF**. However, very far from **LT** and **LL**.     Could you please help us with that concern?    Thank you,  Kerim"
"Hi I face some errors,        Oneline-DLTv1/resnet.py"", line 288, in forward      feature_loss = torch.sum(torch.mul(feature_loss_mat, mask_ap)) / sum_value  RuntimeError: The size of tensor a (315) must match the size of tensor b (560) at non-singleton dimension 3    when training:  python train.py --gpus 2 --cpus 8 --lr 0.0001 --batch_size 32          Could you please share me how to solve it thx!  "
æ¨¡åž‹ä¸­çš„å‚æ•°ä½¿ç”¨çš„æ˜¯ 'models/freeze-mask-first-fintune.pth'é‡Œé¢çš„æ•°æ®ï¼Œä¸ºä»€ä¹ˆä¸æ˜¯ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰è®­ç»ƒå¥½çš„æ¨¡åž‹å•¦ï¼Ÿ
"Hi,  Firstly,  thanks for sharing your code!  And I have a question with the training process.  As training the model from scratch, I found that for most cases the mask(displayed in tensorboard) was nearly black with only a few bright point and it was normal for scenes  with low texture(like below).    !     !     I want know why should this be happening and whether the same case occured in your training process?    "
"A: hi!    Thank you for your contribution. If I need to annotate my data set, what tools should I use to annotate it? In other words, how do I generate the (.npy) file and what is in the (.npy) file? Can you show me the full contents of the npy file?    Thank you and I am looking forward to your reply."
"Hi,  First of all thanks for sharing your code.    I want to test the model on my own dataset.    What changes will be needed in the test file to work?    1. Am I limited to a specific image size?  2. What values should I use for --w_patch and --h_patch ?  3. Should the chosen coroners need to be with respect to the original image coordinate system or with respect to the patched image?    Thanks,  Yuval"
"I have a question about the dimensions of the feature_loss_mat calculated within resnet.forward.    ~~~  feature_loss_mat = triplet_loss(patch_2, pred_I2_CnnFeature, patch_1)    feature_loss = torch.sum(torch.mul(feature_loss_mat, mask_ap)) / sum_value  ~~~    feature_loss_mat is calculated using TripletMarginLoss, and the dimensions of the input are (64, 1, H, W). (if batch size is 64)  At this time, the calculated dimension of feature_loss_mat is (64, H, W).    And finally, to calculate the loss, it is multiplied by mask_ap, and the dimension of mask_ap is (64,1,H,W).    In my opinion, the dimensions of feature_loss_mat and mask_ap should be the same. Is the difference between the two dimensions intended?"
How is the .npy file generated in the Coordinate folder?
"why use these mean and std values? And not normalise it to [-1, 1] by using transforms"
None
"Does anyone encounter this error:""malloc_consolidate(): invalid chunk size"" without more error information?  Every time I train the model for 1 to 4 epochs it happens. "
"Im trying to use the output of the net (H -> homography matrix) to create an optical flow map   the pred_full is correct and the interpolate returns the correct frame, however when is use the formula the create the map from the matrix i get worng flow values     code use to calc the map:      * new cord is the cord for frame 2     * H is homo matrix               What am i doing wrong???"
"Hello,  I'm working with this code and have a question concerning data processing involved. In the following line for TrainData loader, what are these mean and std values? Where are they calculated from? I was wondering if I can use some normalization functions from OpenCV or Numpy for this purpose.        Thank you  Warm regards,  Dharm"
"Hai @JirongZhang,    When I try to run test.py I encountered this error can you please help me what is that I'm missing here ?.  Pytorch version: 1.6.0   "
"Hi, thanks for open source the code.    Can I ask where is the baseline method proposed by the paper _Deep Image Homography Estimation_ ?  The original model is pretty small, I hope to try a bigger baseline model first. Looking forward to your reply, thanks"
None
"Hi @JirongZhang ,    those two statements seem to be mutually exclusive. Could you please elaborate on that? Is using the triplet loss beneficial or not?     Readme:    > The ""Oneline"" model can produce almost comparable performance as ""Doubleline"" model, but much easier to optimize.      Paper:    > Triplet loss. We further exam the effectiveness of our triplet loss by removing the term of Eq. 5 from Eq. 6. As shown in Table 2(c) â€œw/o. triplet lossâ€, the triplet loss decreases errors over 50%, especially beneficial in LT (118.42%lower error) and LL (70.10% lower error) scenes, demonstrating that it not only avoids the problem of obtaining trivial solutions,  but also facilitates a  better optimization.  "
" Hi Zhang,   as you said in the paper, 'a sub-network m(Â·) learns to produce an inlier probability map or mask, highlighting the content in the feature maps that contribute much for the homography estimation. '  I know clearly what is the function of this predictor but could you pls show me in detail what is the sub-network like and how does it work to reject the outlier? Thank you."
"In your project's homepage, the differences between the source images and the target images are small, most are the translation. I wonder whether your method could deal with the large transform such as random affine transform."
" i indicates a pixel location in the masks and feature maps. Here we utilize spatial transform network [18] to achieve the warping operation.    Eg5     i   indicates a pixel location ?   what the pixel loacation mean ?   image location in  batch?     how to understand the  ""Here we utilize spatial transform network [18] to achieve the warping operation.""      thx~ "
"Hi,Mr.Zhang ï¼Œi have some questions about the paperï¼Œcan you explain it ï¼Ÿ thanks~  1ã€image input size is  315 Ã— 560 ï¼Ÿ  2ã€How to get Ga with Ma  and  Fa ?  3ã€And then ï¼Œ np.dstack  Ga and Gb   to  Resnet34 ï¼Ÿ  4ã€Training data is  imageA imageB and Homography matrix  from imageA to imageB ?  5ã€Can the training set be generated like ã€ŠDeep image homography estimationã€‹?    I want to try to implement your paper, but I feel that there will be some difficulties in the loss function part, can I get your help?"
It's a great job. Thanks for your contributions! Can you provide a pre-trained model for application and testing?
   è¯·é—®ä¸€ä¸‹ï¼Œä¸ºä»€ä¹ˆè¿™ä¸ªç½‘å€æ‰“ä¸å¼€å•¦ï¼Œä¸çŸ¥é“æ€Žä¹ˆæ‰èƒ½ä¸‹è½½  
could you give the link of the pretrained model?
"Hi @JirongZhang,    I am absolutely happy you manage to open the code! I would probably ask you about the code in the following days, and my first question is about mask normalization.    This is the function:       And this is the invocation:       So, as far as I understand, for each mask prediction, you normalize it with the half of the maximum value. This will result in normalizing the mask in the range (0, 2), but in the last line, you clamp the mask values to be the range of (0, 1). And my question is: **why?** I have some naive intuition behind it, but I am not fully convinced by this intuition. And, why didn't you mention it in the paper? I don't see it as a simple implementation trick."
Nice work! can you provide a trained model so i can test on my image directly.
None
"Hi @JirongZhang ,    I've seen you've updated the paper on ArXiv. It looks even better than before, good job!    Do you use pretrained ResNet34 weights or you're training from scratch? Are your feature extraction and mask estimation models using the same ResNet-block as the ResNet34 backbone or they use standard conv2d?    Best,  Daniel"
Congratulations! I saw your paper has been accepted by ECCV 2020. Hope you public the code soon.
None
"Besides taking the video frames as described in the paper, did you also warp one of the pair along the lines of DeTone? Otherwise the generalization on heavily skewed pairs is surprising, considering videos never seem to have that much distortion."
Code is unavailable
amazing work!!! any news on when the code will be availableÂ¿? thanks in advance...
hi ï¼Œi had trained a model with 128128  than i want to use this model to 19201080  i croped 128128 from 19201080 than estimate a Hï¼›  but has tow problemsï¼š  aã€transform the 19201080 and crop the same place 128128 looks goodï¼Œbut the whole image is worse  bã€i choose different 128*128 area to predict get very different Hã€‚    what should i to to use this model in my project ï¼Ÿ
Is there an approximate time point for open source datasets?
"outputs = Mode(input)  outputs.shape   is   (4,2)  because the outputs is the  offsets, so  i  need to do  like this ï¼š  h, status = cv2.findHomography(np.add(orf,outputs),orf)    when i calculate the loss ,what is the  orf (source points )ï¼Ÿ"
Amazing Workï¼   And   When can we see the codeï¼Ÿ Thanksï¼
rt
Can't wait to test our algorithm on the real image dataset.
Very interesting paper.  I see the point in predicting zero activations which reduces the MAC. And central to this argument is the widespread usage of ReLU.    Will ZAP be beneficial if my CNN is using Leaky Relu and thus i expect not many activations will be zero?    Thankyou
Thanks for the cool tool!     Could you please give me some detail descriptions of the code?  And something user should pay special attention to?    
"Hello,   What software should i use to creat my own dataset for training? are Labelme's json files supported?     thank you."
"I want to train segfix on my own dataset with script ""scripts/cityscapes/segfix/run_hx_20_d_2_segfix_trainval.sh"", but it seems that it needs file like *.mat, how to solve this problem? thank you.    !   "
It seems that the currently released checkpoints ( for SegFix models) are still trained with Pytorch-0.4.1.
"Thanks for your work!     I download LIP dataset from  , and get dataset folder structure as below:       that is different from the structure you mentioned in  :       could you please provide the scripts to preprocess LIP dataset? Thanks for a lot!"
"hi, i want to try SegFix by using a pretrained SegFix model,where the  model?"
"From my understanding, the two open source (  &  ) doesn't differ greatly.     So I applied SegFix to results generated from HRNet-Semantic-Segmentation. The original mIoU is like below.     <img width=""836"" alt=""image"" src=""     Obviously, I assumed that the final mIoU after applying SegFix would increase. However, that's not the case. mIoU actually decreased to 80.29.    I applied SegFix the way described in MODEL_ZOO.md (below)    <img width=""699"" alt=""image"" src=""     Is this the correct way to apply SegFix? Or is there any other way to apply SegFix? "
"how do you calculate the flops in the figure 4 if I want to calculate input size of 512 * 97 * 97? i use the underline formula but the result is much larger than expectation.   <img width=""627"" alt=""æ•èŽ·"" src=""   "
"I have downloaded   and  , but I can not reproduce 81.6 mIoU result by using `scripts/cityscapes/segfix.py`.    Am I missing something?    refinement command as follows:   "
"When I follow the SegFix Pipelines and apply the segfix in my labels, my result of label is white. So how to fix it?  !   "
"FSSlowOhemCELoss is instantiated in the class FSAuxOhemCELoss, but that class is not defined anywhere."
Hi!  I meet the following error while training. Can anyone help me?          I followed the guidelines    and run   `bash scripts/cityscapes/hrnet/run_h_48_d_4_ocr.sh train 1` . I  have not changed any code in `configs/cityscapes/H_48_D_4.json` and `scripts/cityscapes/hrnet/run_h_48_d_4_ocr.sh`
"Hi ,   I am training segfix model and my mask loss is 0.0 for most of iterations. Also the loss doesnt converge.     Another question i have is what is relevance of dt_num_classes? is it related to the actual total number of classes?"
when i runbash scripts/cityscapes/segfix/run_h_48_d_4_segfix.sh segfix_pred_val 1 to get predict offsets but i got Error:[Errno 2] No such file or directory: './data/cityscapes/val/offset_gt/dt_offset/frankfurt_000000_000294_leftImg8bit.mat'
"Dose somebody else has the same problem ?  I run 'bash ./scripts/cityscapes/hrnet/run_h_48_d_4_ocr.sh val 1'  but i got the error the title mentioned. Seems the shell can't perform inference ,i.e I can't get the predictions."
"Thank you for all your works ! Sorry about that I have a question  in train process i get the the following metrics with val datas  2021-10-09 04:18:34,676 INFO    [base.py, 84] Performance 0.7751076951173586 -> 0.7788474201729588  2021-10-09 04:18:39,755 INFO    [trainer.py, 403] Test Time 16.872s, (0.148) Loss 0.32007823  2021-10-09 04:18:39,755 INFO    [base.py, 33] Result for seg  2021-10-09 04:18:39,756 INFO    [base.py, 49] Mean IOU: 0.7788474201729588  2021-10-09 04:18:39,756 INFO    [base.py, 50] Pixel ACC: 0.9112432591266407    But when i use the same val datas in the inference process , i get the the following metrics   2021-10-11 15:45:30,445 INFO    [ade20k_evaluator.py, 46] Evaluate 228 images  2021-10-11 15:45:30,445 INFO    [ade20k_evaluator.py, 47] mIOU: 0.47010236649173026  2021-10-11 15:45:30,445 INFO    [ade20k_evaluator.py, 48] Pixel ACC: 0.6231233362565961    I feel very strange , could you help me , thank you very much !!!"
"Thank you for your sharing. I use 2 3090GPUS and meet the following error:     Traceback (most recent call last):    File ""/openseg.pytorch-pytorch-1.7/main.py"", line 524, in        model.train()    File ""openseg.pytorch-pytorch-1.7/segmentor/trainer.py"", line 416, in train      self.__val()    File ""openseg.pytorch-pytorch-1.7/segmentor/trainer.py"", line 361, in __val      loss = self.pixel_loss(outputs[i], targets[i].unsqueeze(0))    File ""/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl      result = self.forward(*input, **kwargs)    File ""/data1/Segmentation/2021Q1/openseg.pytorch-pytorch-1.7/lib/loss/loss_helper.py"", line 273, in forward      aux_loss = self.ce_loss(aux_out, targets)    File ""/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl      result = self.forward(*input, **kwargs)    File ""/data1/Segmentation/2021Q1/openseg.pytorch-pytorch-1.7/lib/loss/loss_helper.py"", line 135, in forward      target = self._scale_target(targets[0], (inputs.size(2), inputs.size(3)))    File ""/data1/Segmentation/2021Q1/openseg.pytorch-pytorch-1.7/lib/loss/loss_helper.py"", line 145, in _scale_target      targets = F.interpolate(targets, size=scaled_size, mode=""nearest"")    File ""/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py"", line 3052, in interpolate      'Input is {}D, size is {}'.format(dim, len(size)))  ValueError: size shape must match input shape. Input is 3D, size is 2"
"Hi,Do you have instructions for segfix?"
"Hi, what does w/ mean?   What's the difference between w/ and w/o?  thank you!"
"Is this repo design both for semantic segmentation and for segfix ?  What about if I only want to train segfix?  As the model zoo says:  ""  For whom want to try SegFix by training a new SegFix model, you should:     1. Generate ground truth offsets.   2. Download ImageNet pretrained model to `pretrained_model/`.   3. Run the training script.   4. Run the prediction script to predict offsets for Cityscapes val / test set.   5. Run the refinement script to refine any labels with the offsets.   ""  and I want to know how to generate ground truth offsets."
"I have been trying to reproduce the results with pretrained weights using ""bash scripts/cityscapes/hrnet/run_h_48_d_4_ocr.sh val"".  Inside the vis folder that is generated, the outputs are fine. But inside the label folder, every pixel value is 0.    After trying segfix, label_w_segfix folder is generated. Every pixel for every image in that folder is 255. Help me out."
"hi,@PkuRainBow     I trained a segfix model and as you state in   I want to refine my segmentation result.  So as you said, first get the offset prediction and then refine.  But for the offset prediction, it reports:  openseg.pytorch-master/lib/datasets/tools/collate.py"", line 108, in collate      assert pad_height >= 0 and pad_width >= 0  it seems to be the input image size larger than the size in configuration, some minus number occurs    ""test"": {        ""batch_size"": 16,        ""mode"": ""ss_test"",        ""out_dir"": ""/msravcshare/dataset/seg_result/cityscapes"",        ""data_transformer"": {          ""size_mode"": ""fix_size"",          ""input_size"": [2048, 1024],          ""align_method"": ""only_pad""        }      },  "
"@hsfzxjy ,really thanks for your prompt and detailed reply!!!  I got your meaning . Because mIou is a metric insensitive to the class imbalance problem in the ground truth label, right? so at the begining, I thought miou is a better metric than accuray...  in addiction thanks for your training trick advise, may I ask why adopting a smaller crop than segmentation model is benificial?  ---------------  #30  "
"Hello,  Thank you for the great work.    I am trying to reproduce results using the pytorch 1.7 branch using,         In the  , I made the following changes:  - PRETRAINED_MODEL=""./pretrained_model/hrnetv2_w48_imagenet_pretrained.pth""  - MAX_ITERS=40000  - BATCH_SIZE=16  - TEST_INTERVAL=999999, memory error when evaluating while training.    Please find the training log here using 8 GPUs:        I finally evaluted the 'latest' checkpoint using  .    However, the predictions are all ""sidewalk"" as shown below and I get poor IoU score overall.    !     I would really appreciate your help.  Thank you.  "
THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=663 error=11 : invalid argument    Has anyone encountered this problemï¼Ÿ
"hi @PkuRainBow @hsfzxjy @LayneH     now I am using my own data to train a segfix model to do the post processing. as for the torch version ,   in your config files, you use the inplace_abn for all bn . and in   the BN implementation, it seems only 0.4,1.0.1.1,1.2 torch version are qualified. Can I use torch 1.5 or higher.  another question is ""syncbn"" is ok for the segfix model? have you compared it with the inplace_abn. "
"Hello, I have downloaded the VOC dataset and the context annotations. However, there is not a directory named ""PytorchEncoding"".   Could you tell me what's the ""PytorchEncoding/train.pth"" and """"PytorchEncoding/val.pth"" in pascal_context_generator.py ?  Thanks!"
"I run this command for inference on an image:-    python main.py --configs configs/coco_stuff/H_48_D_4_TEST.json --pretrained models_downloaded/hrnet_w48_ocr_hrnet48_ohem_2_latest.pth --test_img test.jpg --out_dir ./output/    And get this crash:-  Unexpected key(s) in state_dict: ""config_dict"", ""state_dict"".    Any help please? Thanks!  "
None
"I want to train segfix on my custom dataset. The dataset contains 10 classes and the image size is 2048X2048. While going through the script, I found this ""export dt_num_classes=8"" in run_h_48_d_4_segfix.sh. Do I need to change this to 10?   Also there's this line ""assert num_classes in (4, 8, 16, 32,)"" in offset_helper.py. What parameters do I need to change and where?      I tried to change some of these parameters to 10 but I recieved the error       Requirement already satisfied: yacs in /home/workspace/projects/env_aseg/lib/python3.6/site-packages (0.1.8)  Requirement already satisfied: PyYAML in /home/workspace/projects/env_aseg/lib/python3.6/site-packages (from yacs) (5.4.1)  Requirement already satisfied: torchcontrib in /home/workspace/projects/env_aseg/lib/python3.6/site-packages (0.0.2)  Requirement already satisfied: pydensecrf in /home/workspace/projects/env_aseg/lib/python3.6/site-packages (1.0rc3)  dt_max_distance: 5  dt_num_classes: 8  offset_dir: offset_gt/dt_offset  Logging to ./log/cityscapes/segfix_hrnet_hrnet48_segfix_loss_1.log  2021-07-17 17:54:46,324 INFO         File ""/home/workspace/projects/segfix_falcon/openseg.pytorch/lib/models/nets/segfix.py"", line 29, in __init__      self.backbone = BackboneSelector(configer).get_backbone()    File ""/home/workspace/projects/segfix_falcon/openseg.pytorch/lib/models/backbones/backbone_selector.py"", line 34, in get_backbone      model = HRNetBackbone(self.configer)(**params)    File ""/home/workspace/projects/segfix_falcon/openseg.pytorch/lib/models/backbones/hrnet/hrnet_backbone.py"", line 769, in __call__      bn_momentum=0.1)    File ""/home/workspace/projects/segfix_falcon/openseg.pytorch/lib/models/backbones/hrnet/hrnet_backbone.py"", line 313, in __init__      self.bn1 = ModuleHelper.BatchNorm2d(bn_type=bn_type)(64, momentum=bn_momentum)  TypeError: 'NoneType' object is not callable"
  notes that the supplied checkpoints in the   are trained using 0.4.1 and that 1.7 will be released soon. Is there a timeline to release the checkpoints for Pytorch-1.7?
"Thank you for your work.  Some of the model links in the model zoo are broken. (say COCO-stuff, OCR + RMI).  Is it possible to fix this? Thanks.    !   "
"In MODEL_ZOO.md, the link of  offset_semantic.zip in SegFix do not work:    SegFix  On Cityscapes, we can use SegFix scheme to further refine the boundary of segmentation results. To apply SegFix, you should first download **_offset_semantic.zip_** to $DATA_ROOT/cityscapes, then unzip the archive. Take HRNet-W48 based OCR as an example. To refine the results on Cityscapes val set, you should first run bash scripts/cityscapes/hrnet/run_h_48_d_4_ocr.sh val 1 to obtain the baseline results, then run bash scripts/cityscapes/hrnet/run_h_48_d_4_ocr.sh segfix 1 val to apply SegFix.  Thanks for your early reply~  "
could you provide the code that loads the `.pdparams` file of paddleclas to pytorch-implemented model?
"hi,  @PkuRainBow  @hsfzxjy  @LayneH     for the purpose of training the segfix on  the cityscapes dataset, it seems you used the coarse label file to train the model see belowï¼ˆi.e. model for predicting the boundary and direction)? but In my understanding , this model should be trained by the fine labeled mask?  it is right? if not , how can the trained segfix model be able to give accurate boundary and direction , therefore seems impossible to get the good fixed result on the coarse mask prediction by some segmenation model.      in   Data Preparation part shows:  python lib/datasets/preprocess/cityscapes/cityscapes_generator.py --coarse True \    --save_dir   --ori_root_dir    python lib/datasets/preprocess/pascal_context/pascal_context_generator.py \    --save_dir   --ori_root_dir    python lib/datasets/preprocess/coco_stuff/coco_stuff_generator.py \    --save_dir   --ori_root_dir      the ' --coarse True ' indicates  coarse label used.  "
"hi: @hsfzxjy  @PkuRainBow     now ,i am reading your code for segfix. some confusion to ask.   1. in   deg_reduce = 2, the computed direction degree is divided by the param deg_reduce . can you explain it ? in the paper , you said : ""Considering that there might be some \fake"" interior pixels 6 when the  boundary is thick, we propose two different schemes as following: (i) re-scaling  all the offsets by a factor, e.g., 2."". but it seems the 2 in paper is not related to deg_reduce  here? the re-scaling in the paper can be interpreted as the concept stride or step?  2. in    180 is added to computed degree ,why?   3.  lastly, after visulizing the sobel_x,sobel_y, and the distance map(i.e. the result of distrance transformation), I think the direction should be like this ï¼Ÿ In this example, the direction along x is from outside to interior, but the direction along y is oppsite ,right? I think it should be from outside to interior as well along y ,right? Can you explain it ? thanks      !   "
"Thank you for open-source the project.  I follow the instruction in GETTING_STARTED.md but the program break down when the log shows ""   ](url)  "
"**Data Preparation**  You need to download Cityscapes, LIP, PASCAL-Context or COCO-Stuff 10k v1.1 datasets.  We arrange images and labels in another way. You could preprocess the files by running:    `python lib/datasets/preprocess/cityscapes/cityscapes_generator.py --coarse True \    --save_dir   --ori_root_dir    python lib/datasets/preprocess/pascal_context/pascal_context_generator.py \    --save_dir   --ori_root_dir    python lib/datasets/preprocess/coco_stuff/coco_stuff_generator.py \    --save_dir   --ori_root_dir  `    After I ran these 3 types of code, I always received an error:  **/bin/sh: 1:Syntax error: end of file unexpected.**    Do you know how to handle it and go ahead to run your entire code? Thank you!"
"I cannot find any class-id mapping in README or the config file. Just like Road in the ground truth with a label of 0 and traffic light is 1, the unlabeled is 255, etc.    could you provide the mapping for v1.2 of mapillary?"
I have run the lib/datasets/preprocess/cityscapes/dt_offset_generator.py and got the mat filesï¼Œbut the mat dict is as follows:  !   I don't konw how to get the Visualized images as in the paper ï¼š  !   could somebody give me some advice?
"!     As shown in the picture, I didn't understand why the training suddenly ends. Could you provide some advices? Thanks."
"Hi, I want to reproduce the results of ocr paper, specially for pascal context and ade20k. Should I use the HRNet-OCR repo or this repo? In fact, I follow the default settings of HRNet-OCR and just replace HRNet with resnet101, but I can not reproduce the results on pascal context (54.8%mIoU) and ade20k (45.3%mIoU)."
model/log links for ocr+rmi dont work in model zoo:      
Thank you for awesome repo.  Could you provide config of PolyTransform model ?  Thank you so much.
"Thanks for your excellent work. I want to reproduce your experimental results in the comparison with DenseCRF in the SegFix paper. However, I can not find an appropriate group of parameters. It will be very helpful if you can provide the parameters used in your experiments. Thanks a lot!"
"Thanks for your nice work!  When i'm trying to use the branch pytorch1.7, i meet some problems.    For example  `bash scripts/cityscapes/hrnet/run_h_48_d_4_ocr.sh val 1`    I got  `Logging to ./log/cityscapes/hrnet_w48_ocr_1.log  2021-02-18 13:51:54,123 INFO    [offset_helper.py, 54] engery/max-distance: 5 engery/min-distance: 0  2021-02-18 13:51:54,123 INFO    [offset_helper.py, 61] direction/num_classes: 8 scale: 1  2021-02-18 13:51:54,123 INFO    [offset_helper.py, 66] c4 align axis: False  2021-02-18 13:51:54,175 INFO    [module_runner.py, 44] BN Type is torchsyncbn.  2021-02-18 13:52:06,206 INFO    [module_runner.py, 84] Loading checkpoint from ./checkpoints/cityscapes/hrnet_w48_ocr_1_latest.pth...  Traceback (most recent call last):    File ""main.py"", line 217, in        model = Tester(configer)        File ""/home/v-tihang/code_base/MyCodeBase/torch-learning/openseg.pytorch/segmentor/tester.py"", line 69, in __init__      self._init_model()    File ""/home/v-tihang/code_base/MyCodeBase/torch-learning/openseg.pytorch/segmentor/tester.py"", line 73, in _init_model      self.seg_net = self.module_runner.load_net(self.seg_net)    File ""/home/v-tihang/code_base/MyCodeBase/torch-learning/openseg.pytorch/segmentor/tools/module_runner.py"", line 104, in load_net      self.load_state_dict(net.module, checkpoint_dict, self.configer.get('network', 'resume_strict'))    File ""/home/v-tihang/code_base/MyCodeBase/torch-learning/openseg.pytorch/segmentor/tools/module_runner.py"", line 156, in load_state_dict      raise RuntimeError(err_msg)  RuntimeError: unexpected key in source state_dict: conv3x3.1.weight, conv3x3.1.bias, conv3x3.1.running_mean, conv3x3.1.running_var, ocr_distri_head.object_context_block.f_pixel.1.weight, ocr_distri_head.object_context_block.f_pixel.1.bias, ocr_distri_head.object_context_block.f_pixel.1.running_mean, ocr_distri_head.object_context_block.f_pixel.1.running_var, ocr_distri_head.object_context_block.f_pixel.3.weight, ocr_distri_head.object_context_block.f_pixel.3.bias, ocr_distri_head.object_context_block.f_pixel.3.running_mean, ocr_distri_head.object_context_block.f_pixel.3.running_var, ocr_distri_head.object_context_block.f_object.1.weight, ocr_distri_head.object_context_block.f_object.1.bias, ocr_distri_head.object_context_block.f_object.1.running_mean, ocr_distri_head.object_context_block.f_object.1.running_var, ocr_distri_head.object_context_block.f_object.3.weight, ocr_distri_head.object_context_block.f_object.3.bias, ocr_distri_head.object_context_block.f_object.3.running_mean, ocr_distri_head.object_context_block.f_object.3.running_var, ocr_distri_head.object_context_block.f_down.1.weight, ocr_distri_head.object_context_block.f_down.1.bias, ocr_distri_head.object_context_block.f_down.1.running_mean, ocr_distri_head.object_context_block.f_down.1.running_var, ocr_distri_head.object_context_block.f_up.1.weight, ocr_distri_head.object_context_block.f_up.1.bias, ocr_distri_head.object_context_block.f_up.1.running_mean, ocr_distri_head.object_context_block.f_up.1.running_var, ocr_distri_head.conv_bn_dropout.1.weight, ocr_distri_head.conv_bn_dropout.1.bias, ocr_distri_head.conv_bn_dropout.1.running_mean, ocr_distri_head.conv_bn_dropout.1.running_var, aux_head.1.weight, aux_head.1.bias, aux_head.1.running_mean, aux_head.1.running_var    missing keys in source state_dict: backbone.stage3.1.branches.0.0.bn1.num_batches_tracked, backbone.stage4.0.fuse_layers.3.2.0.1.num_batches_tracked, backbone.stage4.0.branches.3.2.bn1.num_batches_tracked, backbone.stage3.0.branches.0.1.bn1.num_batches_tracked, backbone.stage4.1.branches.0.1.bn1.num_batches_tracked, backbone.stage3.0.branches.1.3.bn1.num_batches_tracked, backbone.stage4.1.branches.3.2.bn1.num_batches_tracked, backbone.stage3.2.fuse_layers.2.0.0.1.num_batches_tracked, backbone.stage3.1.branches.2.2.bn1.num_batches_tracked, backbone.stage4.0.branches.3.1.bn2.num_batches_tracked, backbone.stage3.3.branches.1.2.bn2.num_batches_tracked, backbone.layer1.0.downsample.1.num_batches_tracked, backbone.stage4.1.branches.2.1.bn1.num_batches_tracked, backbone.stage4.2.fuse_layers.0.1.1.num_batches_tracked, backbone.stage4.0.branches.1.0.bn2.num_batches_tracked, backbone.layer1.0.bn2.num_batches_tracked, backbone.stage4.1.branches.1.2.bn1.num_batches_tracked, backbone.stage3.2.branches.2.1.bn2.num_batches_tracked, backbone.stage2.0.branches.0.2.bn1.num_batches_tracked, aux_head.1.0.running_var, backbone.stage3.0.branches.0.3.bn2.num_batches_tracked, backbone.stage4.1.branches.1.0.bn2.num_batches_tracked, backbone.stage4.0.fuse_layers.3.0.2.1.num_batches_tracked, backbone.stage4.2.branches.2.0.bn1.num_batches_tracked, backbone.stage3.3.fuse_layers.2.0.1.1.num_batches_tracked, backbone.stage3.0.branches.2.0.bn1.num_batches_tracked, backbone.stage4.2.branches.2.3.bn2.num_batches_tracked, backbone.stage3.1.branches.2.0.bn2.num_batches_tracked, backbone.stage2.0.branches.1.2.bn1.num_batches_tracked, backbone.layer1.3.bn2.num_batches_tracked, backbone.stage3.3.branches.0.2.bn1.num_batches_tracked, backbone.stage4.1.branches.1.2.bn2.num_batches_tracked, backbone.stage3.2.branches.1.0.bn2.num_batches_tracked, backbone.stage3.0.fuse_layers.2.1.0.1.num_batches_tracked, backbone.stage3.1.branches.0.2.bn1.num_batches_tracked, backbone.stage3.1.branches.1.1.bn1.num_batches_tracked, backbone.stage4.1.branches.3.3.bn1.num_batches_tracked, backbone.layer1.3.bn1.num_batches_tracked, backbone.stage3.2.branches.2.2.bn1.num_batches_tracked, backbone.stage4.2.branches.0.3.bn1.num_batches_tracked, backbone.stage4.2.fuse_layers.3.1.1.1.num_batches_tracked, backbone.layer1.1.bn1.num_batches_tracked, backbone.stage2.0.branches.1.1.bn2.num_batches_tracked, backbone.stage3.2.branches.2.0.bn1.num_batches_tracked, conv3x3.1.0.weight, backbone.stage4.1.fuse_layers.3.2.0.1.num_batches_tracked, ocr_distri_head.object_context_block.f_down.1.0.bias, backbone.stage3.0.branches.0.3.bn1.num_batches_tracked, backbone.stage3.2.branches.1.1.bn1.num_batches_tracked, backbone.stage3.3.branches.1.3.bn2.num_batches_tracked, aux_head.1.0.num_batches_tracked, backbone.stage4.2.branches.2.1.bn1.num_batches_tracked, backbone.stage4.0.branches.0.2.bn1.num_batches_tracked, backbone.stage4.0.branches.3.3.bn2.num_batches_tracked, backbone.stage4.1.branches.2.0.bn1.num_batches_tracked, backbone.stage4.1.branches.2.3.bn1.num_batches_tracked, backbone.stage4.0.branches.1.0.bn1.num_batches_tracked, backbone.stage3.1.fuse_layers.2.0.0.1.num_batches_tracked, ocr_distri_head.conv_bn_dropout.1.0.bias, backbone.stage2.0.branches.1.0.bn2.num_batches_tracked, backbone.stage4.1.branches.0.3.bn2.num_batches_tracked, backbone.stage4.2.fuse_layers.3.0.1.1.num_batches_tracked, backbone.stage3.1.fuse_layers.2.1.0.1.num_batches_tracked, backbone.stage4.0.branches.0.1.bn1.num_batches_tracked, backbone.stage4.1.fuse_layers.0.3.1.num_batches_tracked, backbone.stage3.2.branches.0.1.bn1.num_batches_tracked, backbone.stage3.2.branches.0.2.bn1.num_batches_tracked, backbone.stage4.2.branches.2.0.bn2.num_batches_tracked, backbone.stage3.3.fuse_layers.0.1.1.num_batches_tracked, backbone.stage3.3.branches.1.3.bn1.num_batches_tracked, backbone.stage3.0.branches.2.1.bn2.num_batches_tracked, backbone.stage4.2.branches.0.1.bn1.num_batches_tracked, backbone.stage4.1.branches.0.0.bn2.num_batches_tracked, backbone.stage3.0.fuse_layers.2.0.1.1.num_batches_tracked, backbone.stage4.0.fuse_layers.2.3.1.num_batches_tracked, backbone.stage4.0.fuse_layers.3.1.1.1.num_batches_tracked, backbone.stage4.2.branches.3.0.bn1.num_batches_tracked, backbone.stage4.0.branches.3.1.bn1.num_batches_tracked, backbone.stage4.1.branches.0.2.bn1.num_batches_tracked, backbone.stage3.3.fuse_layers.2.1.0.1.num_batches_tracked, backbone.stage4.2.branches.3.3.bn2.num_batches_tracked, backbone.stage3.1.fuse_layers.1.2.1.num_batches_tracked, backbone.stage4.1.fuse_layers.0.1.1.num_batches_tracked, backbone.stage4.1.fuse_layers.2.1.0.1.num_batches_tracked, backbone.stage3.2.branches.0.1.bn2.num_batches_tracked, backbone.stage4.2.branches.1.0.bn2.num_batches_tracked, backbone.stage4.0.branches.2.2.bn2.num_batches_tracked, ocr_distri_head.object_context_block.f_up.1.0.weight, backbone.stage2.0.branches.1.3.bn1.num_batches_tracked, backbone.stage3.1.branches.1.0.bn1.num_batches_tracked, backbone.stage3.3.branches.2.1.bn1.num_batches_tracked, backbone.stage2.0.branches.1.3.bn2.num_batches_tracked, backbone.stage3.2.fuse_layers.1.2.1.num_batches_tracked, backbone.stage3.2.fuse_layers.1.0.0.1.num_batches_tracked, backbone.stage3.2.branches.1.1.bn2.num_batches_tracked, ocr_distri_head.object_context_block.f_pixel.3.0.num_batches_tracked, backbone.stage4.1.branches.2.2.bn2.num_batches_tracked, backbone.stage3.1.branches.1.3.bn1.num_batches_tracked, backbone.stage3.2.branches.2.3.bn2.num_batches_tracked, backbone.stage4.0.fuse_layers.2.0.1.1.num_batches_tracked, backbone.stage3.1.branches.1.0.bn2.num_batches_tracked, ocr_distri_head.object_context_block.f_up.1.0.num_batches_tracked, backbone.stage4.1.fuse_layers.3.0.2.1.num_batches_tracked, ocr_distri_head.object_context_block.f_object.1.0.num_batches_tracked, backbone.stage4.2.branches.0.2.bn2.num_batches_tracked, backbone.stage3.1.branches.0.3.bn2.num_batches_tracked, backbone.stage3.2.branches.1.0.bn1.num_batches_tracked, backbone.stage3.3.branches.0.1.bn2.num_batches_tracked, backbone.stage4.0.fuse_layers.3.0.1.1.num_batches_tracked, backbone.stage4.1.branches.3.1.bn1.num_batches_tracked, backbone.stage4.2.branches.1.2.bn1.num_batches_tracked, backbone.stage4.2.fuse_layers.1.2.1.num_batches_tracked, backbone.stage4.2.branches.0.0.bn2.num_batches_tracked, backbone.stage2.0.branches.0.0.bn2.num_batches_tracked, backbone.stage4.2.fuse_layers.1.3.1.num_batches_tracked, ocr_distri_head.object_context_block.f_pixel.1.0.running_var, backbone.stage4.0.branches.1.1.bn1.num_batches_tracked, backbone.stage3.0.branches.0.2.bn1.num_batches_tracked, backbone.stage4.2.branches.0.2.bn1.num_batches_tracked, backbone.stage3.0.branches.0.1.bn2.num_batches_tracked, backbone.stage4.2.branches.1.3.bn1.num_batches_tracked, backbone.stage3.2.branches.2.1.bn1.num_batches_tracked, backbone.stage3.0.branches.2.3.bn1.num_batches_tracked, backbone.stage3.1.branches.2.2.bn2.num_batches_tracked, backbone.stage3.1.branches.0.1.bn2.num_batches_tracked, backbone.stage3.2.branches.1.2.bn1.num_batches_tracked, backbone.stage3.0.branches.1.1.bn2.num_batches_tracked, backbone.stage4.1.branches.1.3.bn2.num_batches_tracked, ocr_distri_head.object_context_block.f_pixel.1.0.running_mean, backbone.stage3.1.branches.1.3.bn2.num_batches_tracked, backbone.stage4.1.fuse_layers.2.0.1.1.num_batches_tracked, ocr_distri_head.conv_bn_dropout.1.0.weight, backbone.transition3.3.0.1.num_batches_tracked, backbone.stage4.1.branches.3.0.bn1.num_batches_tracked, ocr_distri_head.object_context_block.f_down.1.0.running_var, backbone.stage4.1.branches.2.0.bn2.num_batches_tracked, backbone.stage3.2.fuse_layers.2.0.1.1.num_batches_tracked, backbone.stage4.1.branches.3.0.bn2.num_batches_tracked, ocr_distri_head.object_context_block.f_pixel.1.0.weight, aux_head.1.0.weight, backbone.layer1.1.bn2.num_batches_tracked, backbone.stage3.1.branches.2.3.bn1.num_batches_tracked, backbone.stage4.2.branches.3.0.bn2.num_batches_tracked, ocr_distri_head.object_context_block.f_pixel.3.0.weight, ocr_distri_head.object_context_block.f_object.3.0.num_batches_tracked, backbone.stage3.2.fuse_layers.0.2.1.num_batches_tracked, backbone.stage4.1.branches.3.1.bn2.num_batches_tracked, backbone.stage3.2.branches.0.2.bn2.num_batches_tracked, backbone.stage2.0.branches.0.1.bn2.num_batches_tracked, backbone.layer1.0.bn3.num_batches_tracked, backbone.stage3.3.branches.2.3.bn2.num_batches_tracked, backbone.stage4.1.branches.3.2.bn2.num_batches_tracked, backbone.stage4.2.branches.3.1.bn1.num_batches_tracked, backbone.stage3.2.branches.0.0.bn1.num_batches_tracked, backbone.stage4.1.fuse_layers.3.1.1.1.num_batches_tracked, backbone.stage4.1.branches.1.1.bn2.num_batches_tracked, backbone.stage4.0.branches.2.3.bn2.num_batches_tracked, backbone.stage4.2.branches.3.2.bn2.num_batches_tracked, ocr_distri_head.object_context_block.f_down.1.0.weight, backbone.stage4.1.branches.2.1.bn2.num_batches_tracked, backbone.stage4.1.fuse_layers.1.0.0.1.num_batches_tracked, backbone.stage4.0.branches.1.2.bn2.num_batches_tracked, backbone.stage3.3.branches.0.0.bn1.num_batches_tracked, backbone.stage4.0.fuse_layers.3.1.0.1.num_batches_tracked, backbone.transition1.1.0.1.num_batches_tracked, backbone.stage4.2.branches.3.2.bn1.num_batches_tracked, backbone.stage4.2.fuse_layers.2.0.1.1.num_batches_tracked, backbone.stage2.0.branches.0.0.bn1.num_batches_tracked, backbone.layer1.0.bn1.num_batches_tracked, backbone.stage3.0.fuse_layers.1.2.1.num_batches_tracked, backbone.stage3.3.branches.1.2.bn1.num_batches_tracked, ocr_distri_head.object_context_block.f_up.1.0.running_var, backbone.stage4.1.branches.0.2.bn2.num_batches_tracked, backbone.stage3.3.fuse_layers.1.0.0.1.num_batches_tracked, backbone.stage4.2.branches.3.3.bn1.num_batches_tracked, backbone.stage3.3.fuse_layers.1.2.1.num_batches_tracked, backbone.stage3.3.branches.2.2.bn2.num_batches_tracked, backbone.stage3.0.branches.1.2.bn1.num_batches_tracked, backbone.stage3.0.fuse_layers.0.1.1.num_batches_tracked, backbone.stage4.0.branches.3.3.bn1.num_batches_tracked, backbone.transition2.2.0.1.num_batches_tracked, backbone.stage4.2.branches.1.1.bn2.num_batches_tracked, backbone.stage4.2.branches.2.2.bn2.num_batches_tracked, backbone.stage4.0.branches.2.0.bn1.num_batches_tracked, backbone.stage4.1.fuse_layers.3.0.0.1.num_batches_tracked, ocr_distri_head.object_context_block.f_pixel.1.0.num_batches_tracked, backbone.stage4.2.fuse_layers.3.2.0.1.num_batches_tracked, backbone.stage4.0.fuse_layers.2.1.0.1.num_batches_tracked, conv3x3.1.0.bias, backbone.stage3.1.fuse_layers.2.0.1.1.num_batches_tracked, aux_head.1.0.running_mean, backbone.stage4.2.fuse_layers.2.3.1.num_batches_tracked, ocr_distri_head.object_context_block.f_pixel.3.0.running_mean, backbone.stage4.0.branches.2.1.bn1.num_batches_tracked, backbone.stage3.3.branches.0.0.bn2.num_batches_tracked, backbone.stage4.1.branches.1.3.bn1.num_batches_tracked, backbone.stage3.0.branches.2.0.bn2.num_batches_tracked, aux_head.1.0.bias, backbone.stage3.3.branches.1.1.bn2.num_batches_tracked, backbone.stage3.3.fuse_layers.0.2.1.num_batches_tracked, backbone.stage4.0.branches.0.0.bn1.num_batches_tracked, backbone.stage4.1.fuse_layers.2.3.1.num_batches_tracked, backbone.layer1.3.bn3.num_batches_tracked, backbone.stage4.1.branches.0.3.bn1.num_batches_tracked, ocr_distri_head.object_context_block.f_object.1.0.bias, backbone.stage3.1.fuse_layers.0.2.1.num_batches_tracked, backbone.stage3.0.fuse_layers.2.0.0.1.num_batches_tracked, ocr_distri_head.object_context_block.f_pixel.3.0.running_var, backbone.stage3.3.branches.2.1.bn2.num_batches_tracked, backbone.stage4.1.fuse_layers.1.3.1.num_batches_tracked, ocr_distri_head.object_context_block.f_object.3.0.weight, ocr_distri_head.object_context_block.f_pixel.3.0.bias, backbone.stage3.3.branches.1.0.bn2.num_batches_tracked, backbone.stage4.1.fuse_layers.2.0.0.1.num_batches_tracked, backbone.stage4.0.branches.0.2.bn2.num_batches_tracked, ocr_distri_head.object_context_block.f_object.3.0.bias, backbone.stage4.0.branches.2.2.bn1.num_batches_tracked, backbone.stage3.3.branches.1.0.bn1.num_batches_tracked, ocr_distri_head.conv_bn_dropout.1.0.running_mean, ocr_distri_head.object_context_block.f_object.3.0.running_mean, backbone.stage4.1.fuse_layers.3.1.0.1.num_batches_tracked, conv3x3.1.0.running_var, backbone.stage3.0.branches.2.2.bn2.num_batches_tracked, backbone.stage3.3.branches.0.3.bn2.num_batches_tracked, backbone.stage3.0.branches.1.0.bn1.num_batches_tracked, backbone.stage4.1.branches.1.1.bn1.num_batches_tracked, backbone.stage4.0.fuse_layers.0.1.1.num_batches_tracked, backbone.stage4.2.branches.1.0.bn1.num_batches_tracked, backbone.stage4.0.branches.3.0.bn2.num_batches_tracked, ocr_distri_head.object_context_block.f_object.1.0.weight, backbone.stage4.1.fuse_layers.0.2.1.num_batches_tracked, backbone.stage3.1.branches.1.2.bn2.num_batches_tracked, backbone.stage3.0.fuse_layers.1.0.0.1.num_batches_tracked, backbone.stage4.0.branches.2.1.bn2.num_batches_tracked, backbone.stage3.1.branches.2.1.bn2.num_batches_tracked, backbone.stage2.0.branches.1.0.bn1.num_batches_tracked, backbone.stage4.2.fuse_layers.3.1.0.1.num_batches_tracked, backbone.stage4.2.fuse_layers.2.1.0.1.num_batches_tracked, backbone.stage4.2.branches.2.1.bn2.num_batches_tracked, ocr_distri_head.object_context_block.f_object.1.0.running_mean, backbone.stage4.2.fuse_layers.3.0.2.1.num_batches_tracked, backbone.stage3.0.fuse_layers.0.2.1.num_batches_tracked, backbone.stage4.0.branches.1.2.bn1.num_batches_tracked, backbone.stage4.1.branches.0.1.bn2.num_batches_tracked, backbone.stage3.3.branches.2.0.bn2.num_batches_tracked, backbone.stage2.0.branches.0.3.bn1.num_batches_tracked, backbone.stage3.3.branches.0.3.bn1.num_batches_tracked, backbone.stage3.0.branches.2.3.bn2.num_batches_tracked, backbone.stage3.2.branches.1.3.bn2.num_batches_tracked, backbone.stage3.1.fuse_layers.1.0.0.1.num_batches_tracked, backbone.stage3.1.branches.2.3.bn2.num_batches_tracked, backbone.stage3.2.branches.1.2.bn2.num_batches_tracked, backbone.stage3.3.branches.2.2.bn1.num_batches_tracked, backbone.stage2.0.branches.0.1.bn1.num_batches_tracked, backbone.stage3.1.branches.1.1.bn2.num_batches_tracked, backbone.stage3.3.branches.0.1.bn1.num_batches_tracked, backbone.stage3.1.branches.0.2.bn2.num_batches_tracked, backbone.layer1.2.bn1.num_batches_tracked, backbone.stage3.2.branches.0.0.bn2.num_batches_tracked, backbone.stage4.0.fuse_layers.0.3.1.num_batches_tracked, backbone.stage4.0.branches.0.0.bn2.num_batches_tracked, backbone.stage4.0.branches.0.3.bn2.num_batches_tracked, backbone.stage4.2.branches.0.3.bn2.num_batches_tracked, backbone.stage4.0.branches.3.0.bn1.num_batches_tracked, backbone.stage3.3.fuse_layers.2.0.0.1.num_batches_tracked, backbone.stage4.2.fuse_layers.2.0.0.1.num_batches_tracked, backbone.layer1.2.bn3.num_batches_tracked, backbone.stage2.0.branches.0.3.bn2.num_batches_tracked, backbone.stage4.0.fuse_layers.1.3.1.num_batches_tracked, backbone.stage2.0.fuse_layers.1.0.0.1.num_batches_tracked, backbone.stage3.1.branches.2.0.bn1.num_batches_tracked, backbone.stage4.2.branches.2.3.bn1.num_batches_tracked, backbone.stage3.2.fuse_layers.0.1.1.num_batches_tracked, backbone.layer1.1.bn3.num_batches_tracked, backbone.stage3.1.branches.0.3.bn1.num_batches_tracked, ocr_distri_head.object_context_block.f_down.1.0.running_mean, backbone.stage4.1.fuse_layers.1.2.1.num_batches_tracked, backbone.stage4.2.branches.0.1.bn2.num_batches_tracked, backbone.layer1.2.bn2.num_batches_tracked, backbone.stage3.1.branches.2.1.bn1.num_batches_tracked, backbone.stage4.0.branches.2.3.bn1.num_batches_tracked, backbone.stage3.1.branches.1.2.bn1.num_batches_tracked, backbone.stage3.2.branches.0.3.bn2.num_batches_tracked, backbone.stage3.1.branches.0.1.bn1.num_batches_tracked, backbone.stage4.2.branches.1.3.bn2.num_batches_tracked, backbone.stage4.1.branches.0.0.bn1.num_batches_tracked, conv3x3.1.0.num_batches_tracked, backbone.stage3.0.branches.0.2.bn2.num_batches_tracked, ocr_distri_head.object_context_block.f_up.1.0.running_mean, backbone.stage4.2.fuse_layers.0.3.1.num_batches_tracked, backbone.stage4.1.branches.2.2.bn1.num_batches_tracked, backbone.stage3.1.fuse_layers.0.1.1.num_batches_tracked, backbone.stage4.1.branches.3.3.bn2.num_batches_tracked, backbone.stage3.2.branches.0.3.bn1.num_batches_tracked, backbone.stage2.0.fuse_layers.0.1.1.num_batches_tracked, backbone.stage3.3.branches.1.1.bn1.num_batches_tracked, backbone.stage3.0.branches.0.0.bn2.num_batches_tracked, backbone.stage4.0.fuse_layers.1.0.0.1.num_batches_tracked, ocr_distri_head.object_context_block.f_object.3.0.running_var, backbone.stage4.2.branches.3.1.bn2.num_batches_tracked, backbone.stage4.0.branches.3.2.bn2.num_batches_tracked, ocr_distri_head.object_context_block.f_down.1.0.num_batches_tracked, backbone.stage4.0.branches.0.3.bn1.num_batches_tracked, backbone.stage4.2.branches.2.2.bn1.num_batches_tracked, backbone.stage4.0.branches.1.1.bn2.num_batches_tracked, backbone.stage3.3.branches.2.0.bn1.num_batches_tracked, backbone.stage4.2.branches.0.0.bn1.num_batches_tracked, backbone.stage4.1.branches.1.0.bn1.num_batches_tracked, backbone.stage2.0.branches.1.2.bn2.num_batches_tracked, backbone.stage3.3.branches.2.3.bn1.num_batches_tracked, backbone.stage3.0.branches.1.3.bn2.num_batches_tracked, backbone.stage4.1.fuse_layers.3.0.1.1.num_batches_tracked, backbone.stage3.2.fuse_layers.2.1.0.1.num_batches_tracked, ocr_distri_head.conv_bn_dropout.1.0.running_var, backbone.stage4.2.fuse_layers.1.0.0.1.num_batches_tracked, backbone.bn2.num_batches_tracked, backbone.stage3.0.branches.2.1.bn1.num_batches_tracked, backbone.bn1.num_batches_tracked, backbone.stage3.1.branches.0.0.bn2.num_batches_tracked, conv3x3.1.0.running_mean, backbone.stage4.0.branches.0.1.bn2.num_batches_tracked, backbone.stage4.0.branches.2.0.bn2.num_batches_tracked, backbone.stage4.0.fuse_layers.2.0.0.1.num_batches_tracked, backbone.stage4.2.branches.1.1.bn1.num_batches_tracked, ocr_distri_head.conv_bn_dropout.1.0.num_batches_tracked, ocr_distri_head.object_context_block.f_up.1.0.bias, ocr_distri_head.object_context_block.f_pixel.1.0.bias, backbone.stage4.1.branches.2.3.bn2.num_batches_tracked, backbone.stage4.0.fuse_layers.1.2.1.num_batches_tracked, backbone.stage2.0.branches.1.1.bn1.num_batches_tracked, backbone.stage3.2.branches.2.3.bn1.num_batches_tracked, backbone.stage3.0.branches.1.1.bn1.num_batches_tracked, backbone.stage4.0.branches.1.3.bn2.num_batches_tracked, backbone.stage3.0.branches.1.2.bn2.num_batches_tracked, backbone.stage3.0.branches.2.2.bn1.num_batches_tracked, backbone.stage4.0.branches.1.3.bn1.num_batches_tracked, backbone.stage4.0.fuse_layers.0.2.1.num_batches_tracked, backbone.stage4.2.fuse_layers.3.0.0.1.num_batches_tracked, backbone.stage3.2.branches.2.2.bn2.num_batches_tracked, backbone.transition1.0.1.num_batches_tracked, backbone.stage3.3.branches.0.2.bn2.num_batches_tracked, backbone.stage3.0.branches.1.0.bn2.num_batches_tracked, backbone.stage4.2.fuse_layers.0.2.1.num_batches_tracked, backbone.stage3.2.branches.1.3.bn1.num_batches_tracked, backbone.stage3.2.branches.2.0.bn2.num_batches_tracked, backbone.stage2.0.branches.0.2.bn2.num_batches_tracked, ocr_distri_head.object_context_block.f_object.1.0.running_var, backbone.stage3.0.branches.0.0.bn1.num_batches_tracked, backbone.stage4.0.fuse_layers.3.0.0.1.num_batches_tracked, backbone.stage4.2.branches.1.2.bn2.num_batches_tracked    evaluate the result...  ERROR: Cannot find any ground truth images to use for evaluation. Searched for: /cityscapes/val/label/*.png`"
"in branch 1.7 maybe should be >= 1.7.0  pip install --upgrade torch, torchvision"
Pytorch0.4 is such an old version and very inconvenient to be used in a new machine. Is there any plan to transplant segfix to pytorch1.x ?
why not release segfix weights pretrained on ade20k dataset ?    I can't find it in the MODEL_ZOO page.  
thank you 
!     Does current framework support training with 8 gpus?  Pytorch version == 0.4.1
"In loss_heleper.py  In the calculation of loss function, the input is two tensors[1,8,128,128] /[1,2,128,128], and the corresponding label of single is three tensors.[1,512,512],[1,512,512],[1,512,512]    targets=targets_.clone().unsqueeze(1).float()  AttributeError:'list' object has no attribute 'clone'"
"Hiï¼Œyour work are so awesomeï¼Congratulationï¼  I want to progress my prediction ,would you give a simple tutorial to use Segmix directlyï¼Ÿ  Thanks for your help!  æ‚¨å¥½ï¼Œæˆ‘æƒ³æŠŠSegmixåº”ç”¨åˆ°æˆ‘çš„æ¨¡åž‹ï¼Œè¯·é—®èƒ½å¦å‡ºä¸ªå…³äºŽSegmixä»£ç çš„ç®€å•ä½¿ç”¨æ•™ç¨‹å—ï¼Ÿ  ä¸‡åˆ†æ„Ÿè°¢ï¼"
æ‚¨å¥½ï¼Œæˆ‘æƒ³ç›´æŽ¥è°ƒç”¨segfixæ¥ä¼˜åŒ–ä¸‹æˆ‘çš„ç»“æžœï¼Œè¯·é—®æˆ‘åº”è¯¥è°ƒç”¨å“ªä¸ªç¨‹åºï¼Ÿ  æœŸå¾…æ‚¨çš„å›žå¤ï¼Œè°¢è°¢ï¼
"my dataset image size is 256*256ï¼Œand i dont know how to modifiy the json file     here is my json file, and when i try to train my dataset, there is such sizemisbatch error...like:  !   !   and so on,  environment should be satisfiedï¼š  !         this is my val error:  !   and the config.profile:  !   this is my log file screenshot:  !   !   !   !         "
i cant open the model.zoo link about model.  like this:  !   just the ISA Resnet101 can be opened and downloaded.  could u provide the link again? thank u
"I met OOM problem when validate during training phase.  Here is the log:  World size: 4  ['--configs', 'configs/cityscapes/R_101_D_8.json', '--drop_last', 'y', '--phase', 'train', '--gathered', 'n', '--loss_balance', 'y', '--log_to_file', 'n', '--backbone', 'deepbase_resnet101_dilated8', '--model_name', 'base_ocnet', '--gpu', '0', '1', '2', '3', '--distributed', '--data_dir', './dataset/cityscapes', '--loss_type', 'fs_auxce_loss', '--max_iters', '40000', '--checkpoints_name', 'base_ocnet_deepbase_resnet101_dilated8_20201029', '--pretrained', './pretrained_model/resnet101-imagenet.pth']  *****************************************  Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.  *****************************************  2020-10-30 23:23:59,797 INFO    [offset_helper.py, 54] engery/max-distance: 5 engery/min-distance: 0  2020-10-30 23:23:59,797 INFO    [offset_helper.py, 61] direction/num_classes: 8 scale: 1  2020-10-30 23:23:59,797 INFO    [offset_helper.py, 66] c4 align axis: False  2020-10-30 23:23:59,800 INFO    [offset_helper.py, 54] engery/max-distance: 5 engery/min-distance: 0  2020-10-30 23:23:59,800 INFO    [offset_helper.py, 61] direction/num_classes: 8 scale: 1  2020-10-30 23:23:59,800 INFO    [offset_helper.py, 66] c4 align axis: False  2020-10-30 23:23:59,808 INFO    [module_runner.py, 44] BN Type is inplace_abn.  2020-10-30 23:23:59,808 INFO    [__init__.py, 17] Using evaluator: StandardEvaluator  2020-10-30 23:23:59,811 INFO    [module_runner.py, 44] BN Type is inplace_abn.  2020-10-30 23:23:59,811 INFO    [__init__.py, 17] Using evaluator: StandardEvaluator  2020-10-30 23:24:00,360 INFO    [module_helper.py, 136] Loading pretrained model:./pretrained_model/resnet101-imagenet.pth  2020-10-30 23:24:00,361 INFO    [module_helper.py, 136] Loading pretrained model:./pretrained_model/resnet101-imagenet.pth  2020-10-30 23:24:00,752 INFO    [offset_helper.py, 54] engery/max-distance: 5 engery/min-distance: 0  2020-10-30 23:24:00,752 INFO    [offset_helper.py, 61] direction/num_classes: 8 scale: 1  2020-10-30 23:24:00,752 INFO    [offset_helper.py, 66] c4 align axis: False  2020-10-30 23:24:00,759 INFO    [offset_helper.py, 54] engery/max-distance: 5 engery/min-distance: 0  2020-10-30 23:24:00,760 INFO    [offset_helper.py, 61] direction/num_classes: 8 scale: 1  2020-10-30 23:24:00,760 INFO    [offset_helper.py, 66] c4 align axis: False  2020-10-30 23:24:00,763 INFO    [module_runner.py, 44] BN Type is inplace_abn.  2020-10-30 23:24:00,763 INFO    [__init__.py, 17] Using evaluator: StandardEvaluator  2020-10-30 23:24:00,771 INFO    [module_runner.py, 44] BN Type is inplace_abn.  2020-10-30 23:24:00,771 INFO    [__init__.py, 17] Using evaluator: StandardEvaluator  2020-10-30 23:24:01,344 INFO    [module_helper.py, 136] Loading pretrained model:./pretrained_model/resnet101-imagenet.pth  2020-10-30 23:24:01,349 INFO    [module_helper.py, 136] Loading pretrained model:./pretrained_model/resnet101-imagenet.pth  2020-10-30 23:24:06,815 INFO    [trainer.py, 78] Params Group Method: None  2020-10-30 23:24:06,816 INFO    [trainer.py, 78] Params Group Method: None  2020-10-30 23:24:06,816 INFO    [trainer.py, 78] Params Group Method: None  2020-10-30 23:24:06,816 INFO    [trainer.py, 78] Params Group Method: None  2020-10-30 23:24:06,817 INFO    [optim_scheduler.py, 66] Use lambda_poly policy with default power 0.9  2020-10-30 23:24:06,817 INFO    [data_loader.py, 131] use the DefaultLoader for train...  2020-10-30 23:24:06,818 INFO    [optim_scheduler.py, 66] Use lambda_poly policy with default power 0.9  2020-10-30 23:24:06,818 INFO    [optim_scheduler.py, 66] Use lambda_poly policy with default power 0.9  2020-10-30 23:24:06,818 INFO    [data_loader.py, 131] use the DefaultLoader for train...  2020-10-30 23:24:06,818 INFO    [optim_scheduler.py, 66] Use lambda_poly policy with default power 0.9  2020-10-30 23:24:06,818 INFO    [data_loader.py, 131] use the DefaultLoader for train...  2020-10-30 23:24:06,818 INFO    [data_loader.py, 131] use the DefaultLoader for train...  2020-10-30 23:24:06,855 INFO    [data_loader.py, 164] use DefaultLoader for val ...  2020-10-30 23:24:06,855 INFO    [data_loader.py, 164] use DefaultLoader for val ...  2020-10-30 23:24:06,856 INFO    [data_loader.py, 164] use DefaultLoader for val ...  2020-10-30 23:24:06,857 INFO    [data_loader.py, 164] use DefaultLoader for val ...  2020-10-30 23:24:06,861 INFO    [loss_manager.py, 54] use loss: fs_auxce_loss.  2020-10-30 23:24:06,861 INFO    [loss_manager.py, 54] use loss: fs_auxce_loss.  2020-10-30 23:24:06,861 INFO    [loss_manager.py, 39] use distributed loss  2020-10-30 23:24:06,861 INFO    [loss_manager.py, 39] use distributed loss  2020-10-30 23:24:06,863 INFO    [loss_manager.py, 54] use loss: fs_auxce_loss.  2020-10-30 23:24:06,863 INFO    [loss_manager.py, 54] use loss: fs_auxce_loss.  2020-10-30 23:24:06,863 INFO    [loss_manager.py, 39] use distributed loss  2020-10-30 23:24:06,863 INFO    [loss_manager.py, 39] use distributed loss  2020-10-30 23:24:07,060 INFO    [data_helper.py, 119] Input keys: ['img']  2020-10-30 23:24:07,061 INFO    [data_helper.py, 120] Target keys: ['labelmap']  2020-10-30 23:24:07,115 INFO    [data_helper.py, 119] Input keys: ['img']  2020-10-30 23:24:07,115 INFO    [data_helper.py, 120] Target keys: ['labelmap']  2020-10-30 23:24:07,117 INFO    [data_helper.py, 119] Input keys: ['img']  2020-10-30 23:24:07,117 INFO    [data_helper.py, 120] Target keys: ['labelmap']  2020-10-30 23:24:07,126 INFO    [data_helper.py, 119] Input keys: ['img']  2020-10-30 23:24:07,126 INFO    [data_helper.py, 120] Target keys: ['labelmap']  2020-10-30 23:24:18,010 INFO    [trainer.py, 219] Train Epoch: 0 Train Iteration: 10 Time 11.147s / 10iters, (1.115) Forward Time 3.996s / 10iters, (0.400) Backward Time 6.918s / 10iters, (0.692) Loss Time 0.029s / 10iters, (0.003) Data load 0.203s / 10iters, (0.020318)  Learning rate = [0.00999797497721687, 0.00999797497721687] Loss = 3.54437590 (ave = 3.62284539)    2020-10-30 23:24:18,808 INFO    [trainer.py, 259] 0 images processed    2020-10-30 23:24:19,588 INFO    [trainer.py, 259] 0 images processed    2020-10-30 23:24:19,825 INFO    [trainer.py, 259] 0 images processed    2020-10-30 23:24:19,840 INFO    [trainer.py, 259] 0 images processed    Traceback (most recent call last):    File ""/home/kururu/github/openseg.pytorch/main.py"", line 227, in        model.train()    File ""/home/kururu/github/openseg.pytorch/segmentor/trainer.py"", line 365, in train      self.__train()    File ""/home/kururu/github/openseg.pytorch/segmentor/trainer.py"", line 240, in __train      self.__val()    File ""/home/kururu/github/openseg.pytorch/segmentor/trainer.py"", line 308, in __val      outputs = self.seg_net(*inputs)    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__      result = self.forward(*input, **kwargs)    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 442, in forward      output = self.module(*inputs[0], **kwargs[0])    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__      result = self.forward(*input, **kwargs)    File ""/home/kururu/github/openseg.pytorch/lib/models/nets/ocnet.py"", line 58, in forward      x = self.oc_module(x)    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__      result = self.forward(*input, **kwargs)    File ""/home/kururu/github/openseg.pytorch/lib/models/modules/base_oc_block.py"", line 153, in forward      priors = [stage(feats) for stage in self.stages]    File ""/home/kururu/github/openseg.pytorch/lib/models/modules/base_oc_block.py"", line 153, in        priors = [stage(feats) for stage in self.stages]    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__      result = self.forward(*input, **kwargs)    File ""/home/kururu/github/openseg.pytorch/lib/models/modules/base_oc_block.py"", line 89, in forward      sim_map = (self.key_channels**-.5) * sim_map  RuntimeError: CUDA out of memory. Tried to allocate 4.10 GiB (GPU 0; 10.91 GiB total capacity; 5.16 GiB already allocated; 3.28 GiB free; 229.69 MiB cached)  Traceback (most recent call last):    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/runpy.py"", line 193, in _run_module_as_main      ""__main__"", mod_spec)    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/runpy.py"", line 85, in _run_code      exec(code, run_globals)    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/site-packages/torch/distributed/launch.py"", line 253, in        main()    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/site-packages/torch/distributed/launch.py"", line 249, in main      cmd=cmd)  subprocess.CalledProcessError: Command '['/home/kururu/anaconda3/envs/kururudev-torch-1-0/bin/python', '-u', '/home/kururu/github/openseg.pytorch/main.py', '--local_rank=3', '--configs', 'configs/cityscapes/R_101_D_8.json', '--drop_last', 'y', '--phase', 'train', '--gathered', 'n', '--loss_balance', 'y', '--log_to_file', 'n', '--backbone', 'deepbase_resnet101_dilated8', '--model_name', 'base_ocnet', '--gpu', '0', '1', '2', '3', '--distributed', '--data_dir', './dataset/cityscapes', '--loss_type', 'fs_auxce_loss', '--max_iters', '40000', '--checkpoints_name', 'base_ocnet_deepbase_resnet101_dilated8_20201029', '--pretrained', './pretrained_model/resnet101-imagenet.pth']' returned non-zero exit status 1.  Traceback (most recent call last):    File ""main.py"", line 178, in        handle_distributed(args_parser, os.path.expanduser(os.path.abspath(__file__)))    File ""/home/kururu/github/openseg.pytorch/lib/utils/distributed.py"", line 56, in handle_distributed      cmd=command_args)  subprocess.CalledProcessError: Command '['/home/kururu/anaconda3/envs/kururudev-torch-1-0/bin/python', '-u', '-m', 'torch.distributed.launch', '--nproc_per_node', '4', '/home/kururu/github/openseg.pytorch/main.py', '--configs', 'configs/cityscapes/R_101_D_8.json', '--drop_last', 'y', '--phase', 'train', '--gathered', 'n', '--loss_balance', 'y', '--log_to_file', 'n', '--backbone', 'deepbase_resnet101_dilated8', '--model_name', 'base_ocnet', '--gpu', '0', '1', '2', '3', '--distributed', '--data_dir', './dataset/cityscapes', '--loss_type', 'fs_auxce_loss', '--max_iters', '40000', '--checkpoints_name', 'base_ocnet_deepbase_resnet101_dilated8_20201029', '--pretrained', './pretrained_model/resnet101-imagenet.pth']' returned non-zero exit status 1.  Traceback (most recent call last):    File ""/home/kururu/github/openseg.pytorch/main.py"", line 227, in        model.train()    File ""/home/kururu/github/openseg.pytorch/segmentor/trainer.py"", line 365, in train      self.__train()    File ""/home/kururu/github/openseg.pytorch/segmentor/trainer.py"", line 240, in __train      self.__val()    File ""/home/kururu/github/openseg.pytorch/segmentor/trainer.py"", line 308, in __val      outputs = self.seg_net(*inputs)    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__      result = self.forward(*input, **kwargs)    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 442, in forward      output = self.module(*inputs[0], **kwargs[0])    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__      result = self.forward(*input, **kwargs)    File ""/home/kururu/github/openseg.pytorch/lib/models/nets/ocnet.py"", line 58, in forward      x = self.oc_module(x)    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__      result = self.forward(*input, **kwargs)    File ""/home/kururu/github/openseg.pytorch/lib/models/modules/base_oc_block.py"", line 153, in forward      priors = [stage(feats) for stage in self.stages]    File ""/home/kururu/github/openseg.pytorch/lib/models/modules/base_oc_block.py"", line 153, in        priors = [stage(feats) for stage in self.stages]    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__      result = self.forward(*input, **kwargs)    File ""/home/kururu/github/openseg.pytorch/lib/models/modules/base_oc_block.py"", line 89, in forward      sim_map = (self.key_channels**-.5) * sim_map  RuntimeError: CUDA out of memory. Tried to allocate 4.10 GiB (GPU 3; 10.92 GiB total capacity; 5.46 GiB already allocated; 1019.50 MiB free; 3.89 GiB cached)  Traceback (most recent call last):    File ""/home/kururu/github/openseg.pytorch/main.py"", line 227, in        model.train()    File ""/home/kururu/github/openseg.pytorch/segmentor/trainer.py"", line 365, in train      self.__train()    File ""/home/kururu/github/openseg.pytorch/segmentor/trainer.py"", line 240, in __train      self.__val()    File ""/home/kururu/github/openseg.pytorch/segmentor/trainer.py"", line 308, in __val      outputs = self.seg_net(*inputs)    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__      result = self.forward(*input, **kwargs)    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 442, in forward      output = self.module(*inputs[0], **kwargs[0])    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__      result = self.forward(*input, **kwargs)    File ""/home/kururu/github/openseg.pytorch/lib/models/nets/ocnet.py"", line 58, in forward      x = self.oc_module(x)    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__      result = self.forward(*input, **kwargs)    File ""/home/kururu/github/openseg.pytorch/lib/models/modules/base_oc_block.py"", line 153, in forward      priors = [stage(feats) for stage in self.stages]    File ""/home/kururu/github/openseg.pytorch/lib/models/modules/base_oc_block.py"", line 153, in        priors = [stage(feats) for stage in self.stages]    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__      result = self.forward(*input, **kwargs)    File ""/home/kururu/github/openseg.pytorch/lib/models/modules/base_oc_block.py"", line 89, in forward      sim_map = (self.key_channels**-.5) * sim_map  RuntimeError: CUDA out of memory. Tried to allocate 4.10 GiB (GPU 2; 10.92 GiB total capacity; 5.46 GiB already allocated; 1023.50 MiB free; 3.89 GiB cached)  Traceback (most recent call last):    File ""/home/kururu/github/openseg.pytorch/main.py"", line 227, in        model.train()    File ""/home/kururu/github/openseg.pytorch/segmentor/trainer.py"", line 365, in train      self.__train()    File ""/home/kururu/github/openseg.pytorch/segmentor/trainer.py"", line 240, in __train      self.__val()    File ""/home/kururu/github/openseg.pytorch/segmentor/trainer.py"", line 308, in __val      outputs = self.seg_net(*inputs)    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__      result = self.forward(*input, **kwargs)    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 442, in forward      output = self.module(*inputs[0], **kwargs[0])    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__      result = self.forward(*input, **kwargs)    File ""/home/kururu/github/openseg.pytorch/lib/models/nets/ocnet.py"", line 58, in forward      x = self.oc_module(x)    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__      result = self.forward(*input, **kwargs)    File ""/home/kururu/github/openseg.pytorch/lib/models/modules/base_oc_block.py"", line 153, in forward      priors = [stage(feats) for stage in self.stages]    File ""/home/kururu/github/openseg.pytorch/lib/models/modules/base_oc_block.py"", line 153, in        priors = [stage(feats) for stage in self.stages]    File ""/home/kururu/anaconda3/envs/kururudev-torch-1-0/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__      result = self.forward(*input, **kwargs)    File ""/home/kururu/github/openseg.pytorch/lib/models/modules/base_oc_block.py"", line 89, in forward      sim_map = (self.key_channels**-.5) * sim_map  RuntimeError: CUDA out of memory. Tried to allocate 4.10 GiB (GPU 1; 10.92 GiB total capacity; 5.46 GiB already allocated; 523.50 MiB free; 4.38 GiB cached)"
None
Which is required for LIP/R_101_D_16.json
"!   !   !   Coarse Label Mapï¼ŒOffset Mapï¼ŒRefined Label Mapï¼ŒDistance Mapï¼Œ Direction Map and the last oneï¼ŒHow to draw themã€‚Which drawing software is used, which is a program, what is the name of the software, and can the program be open source?I want to apply Figure 2 and Figure 3 to my own grayscale map. If it can be open sourced, will it be possible in the near future?Thanks you very much.  æ‚¨å¥½ï¼ŒæŠ±æ­‰æˆ‘çš„è‹±è¯­å¤ªæ¸£äº†ï¼Œæƒ³äº†è§£ä¸€ä¸‹è¿™3å¼ å›¾æ˜¯å¦‚ä½•åˆ¶ä½œçš„ã€‚å“ªäº›å›¾ç”¨äº†ç”»å›¾è½¯ä»¶ï¼Œæ˜¯ä»€ä¹ˆè½¯ä»¶ï¼Œå“ªäº›ç”¨äº†ç¨‹åºï¼Œç¨‹åºå¯ä»¥å¼€æºå—ã€‚æˆ‘æƒ³æŠŠå›¾2å’Œå›¾3åº”ç”¨åˆ°è‡ªå·±çš„ç°åº¦å›¾ä¸Šï¼Œå¦‚æžœå¯ä»¥å¼€æºï¼Œè¿‘æœŸå¯ä»¥å—ï¼Ÿè°¢è°¢å„ä½å¤§ä½¬ï¼Œä¸‡åˆ†æ„Ÿè°¢ã€‚"
"run ""bash scripts/cityscapes/hrnet/run_h_48_d_4_ocr.sh val 1"". will get error as follows:    in segmentor/tester.py#L243(     ""Default process group has not been initialized""  AssertionError: Default process group is not initialized"
".pth files dont match .sh scripts, will raise RuntimeError when load_state_dict, such as:    ocr/Cityscapes/hrnet_w48_ocr_1_latest.pth  does not math checkpoints/cityscapes,  will raise RuntimeError when load_state_dict in segmentor/tools/module_runner.py#L156   "
ä½ å¥½ï¼Œä»Žä»£ç æ¥çœ‹ï¼Œé€šè¿‡é•¿çŸ­ä¸¤æ­¥ï¼Œè™½ç„¶æ¯ä¸€æ­¥çš„å›¾ç‰‡çš„å°ºå¯¸å˜å°äº†ï¼Œä½†æ˜¯ä»–çš„batchsizeå˜å¤§äº†å•Šï¼Œé‚£ä¹ˆä»–æ˜¯æ€Žä¹ˆå‡å°äº†è®¡ç®—é‡å’Œå‚æ•°é‡çš„å‘¢ï¼Œéžå¸¸ä¸ç†è§£è¿™ç‚¹ï¼Œè¯·å¤§ä½¬è§£é‡Šä¸€ä¸‹
None
"     probs = F.softmax(self.scale * probs, dim=2)# batch x k x hw    Quick question: dim should be 1 or 2? In my opinion, k represents the number of object classes. Maybe I misunderstood some detailed parts of the proposed method."
"I met this RuntimeError: Ninja is required to load C++ extensions when  the program running, could you pls help me to solve it?  Thanks a lot!  Ps, I've installed ninja and its version is 1.10.1  "
"Dear Authors,    Could you clarify the difference between the ""HRNet-OCR"" at  (1.    and the ""HRNet-OCR"" at (2.      - Is the difference very minor and can be ignored?   - Or the difference is just about the pytorch version (e.g., version 0.41 for 1. and version 1.1 for 2. in the training using the Cityscapes dataset)?  - What's your advice (which one to start with) for the beginners?    Many thanks!      "
"I was excited to try segfix training on my own data.    I could produce the mat files for train and val data.  Training works with run_h_48_d_4_segfix.sh and loss convergences. But on the validation the IoU is more or less random (I have 2 classes)    2020-08-20 10:47:41,932 INFO    [base.py, 32] Result for mask  2020-08-20 10:47:41,932 INFO    [base.py, 48] Mean IOU: 0.7853758111568029  2020-08-20 10:47:41,933 INFO    [base.py, 49] Pixel ACC: 0.9692584678389714  2020-08-20 10:47:41,933 INFO    [base.py, 54] F1 Score: 0.7523384841507573 Precision: 0.7928424176432377 Recall: 0.7157718538603068  2020-08-20 10:47:41,933 INFO    [base.py, 32] Result for dir (mask)  2020-08-20 10:47:41,933 INFO    [base.py, 48] Mean IOU: 0.5390945167184129  2020-08-20 10:47:41,933 INFO    [base.py, 49] Pixel ACC: 0.7248566725097775  2020-08-20 10:47:41,933 INFO    [base.py, 32] Result for dir (GT)  2020-08-20 10:47:41,934 INFO    [base.py, 48] Mean IOU: 0.41990305666871003  2020-08-20 10:47:41,934 INFO    [base.py, 49] Pixel ACC: 0.6007717101395131      to investigate the issue further I tried to analyse the predicted mat files with  bash scripts/cityscapes/segfix/run_h_48_d_4_segfix.sh segfix_pred_val 1    with         ""input_size"": [640, 480] this exception happens:   File ""/home/rsa-key-20190908/openseg.pytorch/lib/datasets/tools/collate.py"", line 108, in collate      assert pad_height >= 0 and pad_width >= 0  after fixing it more or less, iv got similar results as val during training   They were around 3Kb instead of ~70kb  btw, it took ""input_size"": [640, 480]  config from ""test"": { leave instead ""val"": {    is it possible validation only works with ""input_size"": [2048, 1024],?  Can you give me any hints how to manually verify the .mat files of there correctness? Currently I'm diving into 2007.04269.pdf and the code of dt_offset_generator.py to get an understanding.    "
None
Thank you for your excellent algorithm.  Could you please provide the script that transfer the original coco-stuff dataset to the format for trainingï¼Ÿï¼ˆtrain/imageï¼Œtrain/labelï¼Œval/imageï¼Œval/labelï¼‰Because I just found the scripts for other datasetï¼ˆeg.cityscapes/LIPï¼‰
"Hello,   in the requirements.txt it is recommended to use     versions. But are the newer versions of pytorch with CUDA 10 support supported?"
"Hi, noted that for spatial_ocr_block.pyï¼Œthis implementation is diffrent with that of your HRNet+OCR in the sub module f_object,f_pixel,f_down,f_up and so on. All the convolution layer in those sub modules are followed with a bias in this implementation, while the option 'bias' is set to 'False' in all convolution layer of those sub modules in the implementation of your HRNet+OCR. What's the motivation or different effect of the two implementation? "
"Dear Author,    Thank you for your excellent work, but some errors are reported for backbones.        checkpoint names:      checkpoints/cityscapes/hrnet_w48_ocr_1_latest.pth          commands:      (for HRNet-W48:)      python -u main.py --configs configs/cityscapes/H_48_D_4.json --drop_last y --backbone hrnet48 --model_name hrnet_w48_ocr --checkpoints_name hrnet_w48_ocr_1 --phase test --gpu 0 --resume ./checkpoints/cityscapes/hrnet_w48_ocr_1_latest.pth --loss_type fs_auxce_loss --test_dir input_images --out_dir output_images      Error messages:    2020-07-15 21:00:10,470 INFO         File ""/home/dai/code/semantic_segmentation/9/openseg.pytorch-master/lib/models/nets/hrnet.py"", line 105, in __init__      self.backbone = BackboneSelector(configer).get_backbone()    File ""/home/dai/code/semantic_segmentation/9/openseg.pytorch-master/lib/models/backbones/backbone_selector.py"", line 34, in get_backbone      model = HRNetBackbone(self.configer)(**params)    File ""/home/dai/code/semantic_segmentation/9/openseg.pytorch-master/lib/models/backbones/hrnet/hrnet_backbone.py"", line 598, in __call__      bn_momentum=0.1)    File ""/home/dai/code/semantic_segmentation/9/openseg.pytorch-master/lib/models/backbones/hrnet/hrnet_backbone.py"", line 307, in __init__      self.bn1 = ModuleHelper.BatchNorm2d(bn_type=bn_type)(64, momentum=bn_momentum)  TypeError: 'NoneType' object is not callable          Could you please tell me what is wrong? thank you.  "
Is the released model the same as the one achieved mIOU 84.5?
"Hi :     I can not download this paper, could you offer me this paper pdf format ?  or linklist also ok.  Thanks a lot"
"Thanks for sharing this wonderful work with us!    I have a problem with the computing of similarity map in the OCR module.  In **line 131** in lib/models/seg_hrnet_orc.py          `sim_map = (self.key_channels**-.5) * sim_map`   Why multiply a small value (`self.key_channels**-.5`) to sim_map before softmax?    During validation, I have printed the final result of `sim_map` and I found all values in this map are very close to 0.0526 (equals to 1/19), which means the probabilities of a pixel `i` belong to different classes` k` are almost equal.  Is this contradicting the assumption that the similarity map should represent the relation between the _i_th pixel and the _k_th object region?    #######################    Your former answer:    - Multiplying the small value is following the original self-attention scheme. Please refer to the last paragraph of 3.2.1 in the paper ""Attention Is All You Need"". However, we find this small factor does not influence the segmentation performance.    - As the final result of the sim_map, we do not understand why all the values are almost the same in your case. What checkpoints are you testing? How about the performance of the used checkpoint? Please provide more information so that we can help you.    #########################    Thanks a lot for your reply!  I used the checkpoint posted on HRNet-OCR. The segmentation performance is good ad the mIoU is 81.6, too.  !   In inference, I have printed 10 random rows in the `sim_map` like below:  !   All values in this map are very close to 0.0526 (equals to 1/19).  "
"SegFix is just used to citiscapes, is right? Because my own dataset hasnot the *.mat offset files."
"Dear Author,    I am trying to use the pretrained models (ResNet-101 or HRNet-W48 backbones) in my work, but similar errors are reported for both backbones.     1. checkpoint names:  checkpoints/cityscapes/hrnet_w48_ocr_1_latest.pth  checkpoints/cityscapes/spatial_ocrnet_deepbase_resnet101_dilated8_1_latest.pth    2. commands:   (for **HRNet-W48**:)  python -u main.py --configs configs/cityscapes/H_48_D_4.json --drop_last y --backbone hrnet48 --model_name hrnet_w48_ocr --checkpoints_name hrnet_w48_ocr_1 --phase test --gpu 0 --resume ./checkpoints/cityscapes/hrnet_w48_ocr_1_latest.pth --loss_type fs_auxce_loss --test_dir input_images --out_dir output_images  (for **ResNet101**:)  python -u main.py --configs configs/cityscapes/R_101_D_8.json --drop_last y --backbone deepbase_resnet101_dilated8 --model_name spatial_ocrnet --checkpoints_name spatial_ocrnet_deepbase_resnet101_dilated8_1 --phase test --gpu 0  --resume ./checkpoints/cityscapes/spatial_ocrnet_deepbase_resnet101_dilated8_1_latest.pth --loss_type fs_auxce_loss --test_dir input_images --out_dir output_images    3. environments:  python                    3.7.3                h33d41f4_1    conda-forge  pytorch                   1.1.0           py3.7_cuda10.0.130_cudnn7.5.1_0    PyTorch  torchcontrib              0.0.2                        torchvision               0.3.0           py37_cu10.0.130_1    pytorch  gcc (GCC) 7.2.0  cuda 10.0    4. Error messages:   RuntimeError: unexpected key in source state_dict: conv_3x3.1.weight, conv_3x3.1.bias, conv_3x3.1.running_mean, conv_3x3.1.running_var, spatial_ocr_head.object_context_block.f_pixel.1.weight, spatial_ocr_head.object_context_block.f_pixel.1.bias, spatial_ocr_head.object_context_block.f_pixel.1.running_mean, spatial_ocr_head.object_context_block.f_pixel.1.running_var, spatial_ocr_head.object_context_block.f_pixel.3.weight, spatial_ocr_head.object_context_block.f_pixel.3.bias, spatial_ocr_head.object_context_block.f_pixel.3.running_mean, spatial_ocr_head.object_context_block.f_pixel.3.running_var, spatial_ocr_head.object_context_block.f_object.1.weight, spatial_ocr_head.object_context_block.f_object.1.bias, spatial_ocr_head.object_context_block.f_object.1.running_mean, spatial_ocr_head.object_context_block.f_object.1.running_var, spatial_ocr_head.object_context_block.f_object.3.weight, spatial_ocr_head.object_context_block.f_object.3.bias, spatial_ocr_head.object_context_block.f_object.3.running_mean, spatial_ocr_head.object_context_block.f_object.3.running_var, spatial_ocr_head.object_context_block.f_down.1.weight, spatial_ocr_head.object_context_block.f_down.1.bias, spatial_ocr_head.object_context_block.f_down.1.running_mean, spatial_ocr_head.object_context_block.f_down.1.running_var, spatial_ocr_head.object_context_block.f_up.1.weight, spatial_ocr_head.object_context_block.f_up.1.bias, spatial_ocr_head.object_context_block.f_up.1.running_mean, spatial_ocr_head.object_context_block.f_up.1.running_var, spatial_ocr_head.conv_bn_dropout.1.weight, spatial_ocr_head.conv_bn_dropout.1.bias, spatial_ocr_head.conv_bn_dropout.1.running_mean, spatial_ocr_head.conv_bn_dropout.1.running_var, dsn_head.1.weight, dsn_head.1.bias, dsn_head.1.running_mean, dsn_head.1.running_var      _It looks like that the checkpoint model and the running model does not match at some layers. Could you take a look please? Thank you very much!_  "
None
"Hi,    Would you consider releasing the resnet50 pretrained model?"
"Hello,    Thanks for making the code and the pre-trained models available!    I would like to know to reproduce your results on the Cityscapes test set (mIoU 84.5/84.2 with/without SegFix) from your provided pre-trained model.    Should I take the 80000-iteration OCR HRNet-W48 weights that you listed in the  ?    Thank you in advance for your response."
"Dear Author    Hello. Thank you for sharing the code about it.  I can get some insight into your code to solve my problem.  I have some questions about your code.  First, in the ocrnet.py, you apply the feature network after that use the OCR block after that you use the F.interpolat 2 times. However, I am wondering why you have two returns about the first interpolation result and the second interpolation result. Usual segmentation network uses the last segmentation as the final segmentation result.     Second, I want to use your segfix algorithm about my problem. However, I can not find the independent algorithm about it. I also can not find the paper. The readme only mentions that it is similar about the PointRend scheme. I am wondering is it  CNN approach or use the extra others?   Thank you."
Could you please improve the doc about how to use the segfix methodï¼ŸI'm a little bit confused about how to generate the offset file.
"Could you please, improve the documentation about how can we use the library with pre-trained model  ?    I would like to use it on my own dataset if possible.  Thanks"
"Under the Cityscapes Semantic Segmentation section in model zoo following is written:  To apply SegFix, you should first down the offset files offset_instance.zip to $DATA_ROOT/cityscapes, and then extract the archive.    where offset_instance.zip is linked to offset_semantic.zip.    I was wondering whether you have released the instance offset and the link is wrong or it's just a typo?  In the case of a typo, can you provide the link for instance offsets?"
"For comparison in our paper, we are looking for the detailed test set results (class IoUs) of these prediction files that you shared:    Do you happen to have a snapshot of the submission results obtained with these predictions?  Thank you for your consideration."
"Hi!    Thanks for your nice work. It is really impressive. I'm interested in the SegFix algorithm.  Could you send a copy of the paper ""SegFix: Model-Agnostic Boundary Refinement for Segmentation"", since I cannot find it on arXiv.    Best,  David"
Hello. I'm trying to reproduce your CityScapes results for our BMVC paper.    after I followed the data directory format in the config.profile file and running `bash ./scripts/cityscapes/hrnet/run_h_48_d_4_ocr.sh val 1` I get this error:    ``ERROR: Found no prediction for ground truth /home/arash/openseg.pytorch/dataset/cityscapes/val/label/munster_000027_000019_gtFine_labelIds.png``    could you explain how did you prepare the data?  Thanks
"Thanks for your job!  elif torch_ver == '1.2':                  from inplace_abn import InPlaceABNSync                  return InPlaceABNSync(num_features, **kwargs)  I cant't find the file of inplace_abn for torch_ver==1.2  and i want to know if i use this file if i need to install first"
"Hi thanks for releasing the code, first thing was to try it by myself!    All worked very well, i successfully trained and validated with own images and own label files.  But what I don't get is, how I can generate own *.mat files to run the segfix. You provided only mat files for cityscape but how to generate them for an own dataset?    When starting val with segfix (scripts/cityscapes/hrnet/run_h_48_d_4_ocr.sh segfix 3 val) I recieve:    FileNotFoundError: [Errno 2] No such file or directory: openseg.pytorch/data/cityscapes/val/offset_pred/semantic/offset_hrnext/is-03-08-2019-normal-98-001089.mat'        "
None
As the title.  And any config details or suggestions about pretraining on Mapillary will be appreciated!    Thanks!
None
"Paper explains how you combine ResNet with OCR. The output stride of (dilated) ResNet is 8 and you use the last two stages as inputs for OCR.    However, HRNetV2 has outputs at 4 different scales (output stride = [4, 8, 16, 32]). Can you explain how you combine them?    In addition, section 3.2 in paper states that the output size of stage 3&4 are H Ã— W. Is this 1/8 of original input image size since the output stride is 8?"
when release the OCR module.
"Hi, have you release any checkpoints about OCNet"
"Hello, would you like share your source code?"
Could you share the train/valid and test list of UTKFace dataset?
"Hi, grateful for your excellent work! Reading your paper make me to have a more clear understanding for neural networks.     I try to run ""nce+_eval.py"" but failed. The file with path ""/root/ebms_regression/1dregression/2/gt_x_values_2_scores.pkl"" is needed but I did not found it. Could you offer the file as a download link?"
"Hi, grateful for your excellent work!  I want to ask a question that when the code of **head-pose estimation and age estimation** could be updated? Thanks a lot!"
"I applied your NCE+ method to super_dimp.  As written in the paper, only the loss function was modified.  And both training and inference used super_dimp's method as it is.  However, it is performing worse than super_dimp.  (GOT-10K AUC - 0.801 vs 0.759)  Are there any other modifications not mentioned in the paper?  (Are the same values â€‹â€‹for the loss weight and learning rate used as the super_dimp method?)  "
"hiï¼Œi'm grateful for your excellent work. I want to ask a question about code for dimp-nce+, when the code could be uploaded in pytracking? Thanks."
"While running 'bash scripts/download_nlvr2.sh $PATH_TO_STORAGE',  When downloading and decompressing training data(nlvr2_train.tar, nlvr2_train.tar.1), the following error occurs. Where can I get error-free data?    ===========================================================  nlvr2_train/  nlvr2_train/feat_th0.2_max100_min10/  nlvr2_train/feat_th0.2_max100_min10/data.mdb  tar: Unexpected EOF in archive  tar: rmtlseek not stopped at a record boundary  tar: Error is not recoverable: exiting now"
"Hi, thank you for the great job.   I just have a question this.  Do you plan to release the pretrained model which is just trained in-domain dataset.  "
"When I launched the 2nd stage training using :     An error occured, which is:       How to fix this problem? Thanks."
"Hi, as title says, I do not find decoder in UniterModel. But the paper indicates Uniter has a decoder. Do you release the code of decoder?"
"WARNING: Detected NVIDIA NVIDIA A100-PCIE-40GB GPU, which is not yet supported in this version of the container    How to deal with it ?"
How to measure the loss weight of different pre-training tasks? Which task's loss determines the model training convergence?
"I encountered the following error on GPU 3090, but it can run successfully on Titan Xp. I suspect that the Cuda version in the container is relatively low. So has anyone tried to build an environment to run uniter without using the official container environment? Or does anyone have a better solution?  !   "
how to generate image feature from ground true?  the code just provide the stript that generates the image from detected bbx instead of ground true. 
**This is more of a question than issue.**    In model.py at line number 330 gather index has been used to re-orient text and image embedding after concatenation. I am trying to understand what is this gather index conceptually. I am trying to find the same in the paper. From the paper it seems image and text features are concatenated. So it will be helpful to know what these gather index vectors represents and how to create them for a custom dataset.    Thanks.
"Could you provide ids for the train/val splits for SBU? I do not need the full db, just want to ensure I am using the same splits.    Also, was wondering if these could be provided for CC and VG as well, to make things easier.    Thank you!"
"Hello,    I know that both of `re_coco_det` and `re_coco_gt` have the following two fields  - `img_feat`  - `img_bb`    And `img_bb` in `re_coco_gt` is ground-truth bounding boxes while in `re_coco_det`, it is predicted bounding boxes from Faster-RCNN. What about `img_feat`? How is `img_feat` calculated in `re_coco_gt` and `re_coco_det`, respectively?    Thanks for any input!"
"Hi, thank you very much for your great work. I'm wondering if I need to reimplement it for referring expression task, is it doable? How long will it take?  "
"Hi!    I met this error when reproducing VQA task, could you please have a loook and give me some suggestion based on your experience? Thanks a lot!      0%|          | 0/6000 [00:00 :08/15/2021 10:08:00 - INFO - __main__ -   ***** Running training with 4 GPUs *****  [1,0] :08/15/2021 10:08:00 - INFO - __main__ -     Num examples = 471128  [1,0] :08/15/2021 10:08:00 - INFO - __main__ -     Batch size = 1024  [1,0] :08/15/2021 10:08:00 - INFO - __main__ -     Accumulate steps = 5  [1,0] :08/15/2021 10:08:00 - INFO - __main__ -     Num steps = 6000  [1,0] :[1a62e574072d:00334] *** Process received signal ***  [1,0] :[1a62e574072d:00334] Signal: Bus error (7)  [1,0] :[1a62e574072d:00334] Signal code: Non-existant physical address (2)  [1,0] :[1a62e574072d:00334] Failing at address: 0x7f246888f00a  [1,0] :[1a62e574072d:00334] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x11390)[0x7f25b870a390]  [1,0] :[1a62e574072d:00334] [ 1] /opt/conda/lib/python3.6/site-packages/lmdb/cpython.cpython-36m-x86_64-linux-gnu.so(+0x128e0)[0x7f25aa5988e0]  [1,0] :[1a62e574072d:00334] [ 2] /opt/conda/lib/python3.6/site-packages/lmdb/cpython.cpython-36m-x86_64-linux-gnu.so(+0x12b74)[0x7f25aa598b74]  [1,0] :[1a62e574072d:00334] [ 3] /opt/conda/lib/python3.6/site-packages/lmdb/cpython.cpython-36m-x86_64-linux-gnu.so(+0x14ba5)[0x7f25aa59aba5]  [1,0] :[1a62e574072d:00334] [ 4] /opt/conda/lib/python3.6/site-packages/lmdb/cpython.cpython-36m-x86_64-linux-gnu.so(mdb_get+0xbc)[0x7f25aa59b40c]  [1,0] :[1a62e574072d:00334] [ 5] [1,0] :/opt/conda/lib/python3.6/site-packages/lmdb/cpython.cpython-36m-x86_64-linux-gnu.so(+0x9d9d)[0x7f25aa58fd9d]  [1,0] :[1a62e574072d:00334] [ 6] python(_PyCFunction_FastCallDict+0x154)[0x55f2e08c1744]  [1,0] :[1a62e574072d:00334] [ 7] [1,0] :python(+0x19842c)[0x55f2e094842c]  [1,0] :[1a62e574072d:00334] [ 8] python(_PyEval_EvalFrameDefault+0x30a)[0x55f2e096d38a]  [1,0] :[1a62e574072d:00334] [ 9] [1,0] :python(_PyFunction_FastCallDict+0x11b)[0x55f2e0942bab]  [1,0] :[1a62e574072d:00334] [10] python(_PyObject_FastCallDict+0x26f)[0x55f2e08c1b0f]  [1,0] :[1a62e574072d:00334] [11] [1,0] :python(_PyObject_Call_Prepend+0x63)[0x55f2e08c66a3]  [1,0] :[1a62e574072d:00334] [12] python(PyObject_Call+0x3e)[0x55f2e08c154e]  [1,0] :[1a62e574072d:00334] [13] [1,0] :python(+0x16b50a)[0x55f2e091b50a]  [1,0] :[1a62e574072d:00334] [14] python(_PyEval_EvalFrameDefault+0x877)[0x55f2e096d8f7]  [1,0] :[1a62e574072d:00334] [15] [1,0] :python(_PyFunction_FastCallDict+0x11b)[0x55f2e0942bab]  [1,0] :[1a62e574072d:00334] [16] python(_PyObject_FastCallDict+0x26f)[0x55f2e08c1b0f]  [1,0] :[1a62e574072d:00334] [17] python(_PyObject_Call_Prepend+0x63)[0x55f2e08c66a3]  [1,0] :[1a62e574072d:00334] [18] [1,0] :python(PyObject_Call+0x3e)[0x55f2e08c154e]  [1,0] :[1a62e574072d:00334] [19] python(+0x16b50a)[0x55f2e091b50a]  [1,0] :[1a62e574072d:00334] [1,0] :[20] python(_PyEval_EvalFrameDefault+0x877)[0x55f2e096d8f7]  [1,0] :[1a62e574072d:00334] [21] [1,0] :python(+0x19253b)[0x55f2e094253b]  [1,0] :[1a62e574072d:00334] [22] python(+0x198505)[0x55f2e0948505]  [1,0] :[1a62e574072d:00334] [23] [1,0] :python(_PyEval_EvalFrameDefault+0x30a)[0x55f2e096d38a]  [1,0] :[1a62e574072d:00334] [24] python(+0x191a76)[0x55f2e0941a76]  [1,0] :[1a62e574072d:00334] [25] python(_PyFunction_FastCallDict+0x1bc)[0x55f2e0942c4c]  [1,0] :[1a62e574072d:00334] [26] [1,0] :python(_PyObject_FastCallDict+0x26f)[0x55f2e08c1b0f]  [1,0] :[1a62e574072d:00334] [27] python(_PyObject_Call_Prepend+0x63)[0x55f2e08c66a3]  [1,0] :[1a62e574072d:00334] [28] [1,0] :python(PyObject_Call+0x3e)[0x55f2e08c154e]  [1,0] :[1a62e574072d:00334] [29] python(+0x16b50a)[0x55f2e091b50a]  [1,0] :[1a62e574072d:00334] *** End of error message ***  [1,2] :[1a62e574072d:00336] *** Process received signal ***  [1,2] :[1a62e574072d:00336] Signal: Bus error (7)  [1,2] :[1a62e574072d:00336] Signal code: Non-existant physical address (2)  [1,2] :[1a62e574072d:00336] Failing at address: 0x7f56e824f00a  [1,3] :[1a62e574072d:00337] *** Process received signal ***  [1,3] :[1a62e574072d:00337] Signal: Bus error (7)  [1,3] :[1a62e574072d:00337] Signal code: Non-existant physical address (2)  [1,3] :[1a62e574072d:00337] Failing at address: 0x7fd82888f00a  [1,3] :[1a62e574072d:00337] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x11390)[0x7fd96b06d390]  [1,3] :[1a62e574072d:00337] [ 1] [1,3] :/opt/conda/lib/python3.6/site-packages/lmdb/cpython.cpython-36m-x86_64-linux-gnu.so(+0x128e0)[0x7fd955ee38e0]  [1,3] :[1a62e574072d:00337] [ 2] /opt/conda/lib/python3.6/site-packages/lmdb/cpython.cpython-36m-x86_64-linux-gnu.so(+0x12b74)[0x7fd955ee3b74]  [1,3] :[1a62e574072d:00337] [ 3] /opt/conda/lib/python3.6/site-packages/lmdb/cpython.cpython-36m-x86_64-linux-gnu.so(+0x14ba5)[0x7fd955ee5ba5]  [1,3] :[1a62e574072d:00337] [ 4] [1,3] :/opt/conda/lib/python3.6/site-packages/lmdb/cpython.cpython-36m-x86_64-linux-gnu.so(mdb_get+0xbc)[0x7fd955ee640c]  [1,3] :[1a62e574072d:00337] [ 5] /opt/conda/lib/python3.6/site-packages/lmdb/cpython.cpython-36m-x86_64-linux-gnu.so(+0x9d9d)[0x7fd955edad9d]  [1,3] :[1a62e574072d:00337] [ 6] [1,2] :[1a62e574072d:00336] [ 0] [1,3] :python(_PyCFunction_FastCallDict+0x154)[0x5600587e3744]  [1,3] :[1a62e574072d:00337] [ 7] python(+0x19842c)[0x56005886a42c]  [1,3] :[1a62e574072d:00337] [ 8] [1,2] :/lib/x86_64-linux-gnu/libpthread.so.0(+0x11390)[0x7f582a139390]  [1,2] :[1a62e574072d:00336] [ 1] /opt/conda/lib/python3.6/site-packages/lmdb/cpython.cpython-36m-x86_64-linux-gnu.so(+0x128e0)[0x7f58106a88e0]  [1,2] :[1a62e574072d:00336] [ 2] /opt/conda/lib/python3.6/site-packages/lmdb/cpython.cpython-36m-x86_64-linux-gnu.so(+0x12b74)[0x7f58106a8b74]  [1,2] :[1a62e574072d:00336] [ 3] /opt/conda/lib/python3.6/site-packages/lmdb/cpython.cpython-36m-x86_64-linux-gnu.so(+0x14ba5)[0x7f58106aaba5]  [1,2] :[1a62e574072d:00336] [ 4] /opt/conda/lib/python3.6/site-packages/lmdb/cpython.cpython-36m-x86_64-linux-gnu.so(mdb_get+0xbc)[0x7f58106ab40c]  [1,2] :[1a62e574072d:00336] [ 5] /opt/conda/lib/python3.6/site-packages/lmdb/cpython.cpython-36m-x86_64-linux-gnu.so(+0x9d9d)[0x7f581069fd9d]  [1,2] :[1a62e574072d:00336] [ 6] [1,3] :python(_PyEval_EvalFrameDefault+0x30a)[0x56005888f38a]  [1,3] :[1a62e574072d:00337] [ 9] python(_PyFunction_FastCallDict+0x11b)[0x560058864bab]  [1,3] :[1a62e574072d:00337] [10] [1,2] :python(_PyCFunction_FastCallDict+0x154)[0x558e13464744]  [1,2] :[1a62e574072d:00336] [ 7] python(+0x19842c)[0x558e134eb42c]  [1,2] :[1a62e574072d:00336] [ 8] [1,3] :python(_PyObject_FastCallDict+0x26f)[0x5600587e3b0f]  [1,3] :[1a62e574072d:00337] [11] [1,2] :python(_PyEval_EvalFrameDefault+0x30a)[0x558e1351038a]  [1,2] :[1a62e574072d:00336] [ 9] python(_PyFunction_FastCallDict+0x11b)[0x558e134e5bab]  [1,2] :[1a62e574072d:00336] [10] [1,3] :python(_PyObject_Call_Prepend+0x63)[0x5600587e86a3]  [1,3] :[1a62e574072d:00337] [12] python(PyObject_Call+0x3e)[0x5600587e354e]  [1,3] :[1a62e574072d:00337] [13] [1,2] :python(_PyObject_FastCallDict+0x26f)[0x558e13464b0f]  [1,2] :[1a62e574072d:00336] [11] [1,3] :python(+0x16b50a)[0x56005883d50a]  [1,3] :[1a62e574072d:00337] [14] [1,2] :python(_PyObject_Call_Prepend+0x63)[0x558e134696a3]  [1,2] :[1a62e574072d:00336] [12] [1,3] :python(_PyEval_EvalFrameDefault+0x877)[0x56005888f8f7]  [1,3] :[1a62e574072d:00337] [15] [1,2] :python(PyObject_Call+0x3e)[0x558e1346454e]  [1,2] :[1a62e574072d:00336] [13] [1,3] :python(_PyFunction_FastCallDict+0x11b)[0x560058864bab]  [1,3] :[1a62e574072d:00337] [16] [1,2] :python(+0x16b50a)[0x558e134be50a]  [1,2] :[1a62e574072d:00336] [14] [1,3] :python(_PyObject_FastCallDict+0x26f)[0x5600587e3b0f]  [1,3] :[1a62e574072d:00337] [17] python(_PyObject_Call_Prepend+0x63)[0x5600587e86a3]  [1,3] :[1a62e574072d:00337] [18] [1,2] :python(_PyEval_EvalFrameDefault+0x877)[0x558e135108f7]  [1,2] :[1a62e574072d:00336] [15] python(_PyFunction_FastCallDict+0x11b)[0x558e134e5bab]  [1,3] :python(PyObject_Call+0x3e)[0x5600587e354e]  [1,3] :[1a62e574072d:00337] [19] [1,2] :[1a62e574072d:00336] [16] python(_PyObject_FastCallDict+0x26f)[0x558e13464b0f]  [1,2] :[1a62e574072d:00336] [17] [1,3] :python(+0x16b50a)[0x56005883d50a]  [1,3] :[1a62e574072d:00337] [20] [1,2] :python(_PyObject_Call_Prepend+0x63)[0x558e134696a3]  [1,2] :[1a62e574072d:00336] [18] [1,3] :python(_PyEval_EvalFrameDefault+0x877)[0x56005888f8f7]  [1,3] :[1a62e574072d:00337] [21] [1,2] :python(PyObject_Call+0x3e)[0x558e1346454e]  [1,2] :[1a62e574072d:00336] [19] [1,3] :python(+0x19253b)[0x56005886453b]  [1,3] :[1a62e574072d:00337] [22] python(+0x198505)[0x56005886a505]  [1,3] :[1a62e574072d:00337] [23] [1,2] :python(+0x16b50a)[0x558e134be50a]  [1,2] :[1a62e574072d:00336] [20] python(_PyEval_EvalFrameDefault+0x877)[0x558e135108f7]  [1,2] :[1a62e574072d:00336] [21] [1,3] :python(_PyEval_EvalFrameDefault+0x30a)[0x56005888f38a]  [1,3] :[1a62e574072d:00337] [24] python(+0x191a76)[0x560058863a76]  [1,3] :[1a62e574072d:00337] [25] [1,2] :python(+0x19253b)[0x558e134e553b]  [1,2] :[1a62e574072d:00336] [22] [1,3] :python(_PyFunction_FastCallDict+0x1bc)[0x560058864c4c]  [1,3] :[1a62e574072d:00337] [26] [1,2] :python(+0x198505)[0x558e134eb505]  [1,2] :[1a62e574072d:00336] [23] [1,3] :python(_PyObject_FastCallDict+0x26f)[0x5600587e3b0f]  [1,3] :[1a62e574072d:00337] [27] [1,2] :python(_PyEval_EvalFrameDefault+0x30a)[0x558e1351038a]  [1,2] :[1a62e574072d:00336] [24] python(+0x191a76)[0x558e134e4a76]  [1,3] :python(_PyObject_Call_Prepend+0x63)[0x5600587e86a3]  [1,3] :[1a62e574072d:00337] [28] [1,2] :[1a62e574072d:00336] [25] python(_PyFunction_FastCallDict+0x1bc)[0x558e134e5c4c]  [1,2] :[1a62e574072d:00336] [26] [1,3] :python(PyObject_Call+0x3e)[0x5600587e354e]  [1,3] :[1a62e574072d:00337] [29] python(+0x16b50a)[0x56005883d50a]  [1,2] :python(_PyObject_FastCallDict+0x26f)[0x558e13464b0f]  [1,2] :[1a62e574072d:00336] [27] python(_PyObject_Call_Prepend+0x63)[0x558e134696a3]  [1,2] :[1a62e574072d:00336] [28] [1,3] :[1a62e574072d:00337] *** End of error message ***  [1,2] :python(PyObject_Call+0x3e)[0x558e1346454e]  [1,2] :[1a62e574072d:00336] [29] [1,2] :python(+0x16b50a)[0x558e134be50a]  [1,2] :[1a62e574072d:00336] *** End of error message ***  [1,1] :[1a62e574072d:00335] *** Process received signal ***  [1,1] :[1a62e574072d:00335] Signal: Bus error (7)  [1,1] :[1a62e574072d:00335] Signal code: Non-existant physical address (2)  [1,1] :[1a62e574072d:00335] Failing at address: 0x7fd41730e00a  [1,1] :[1a62e574072d:00335] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x11390)[0x7fd5591ad390]  [1,1] :[1a62e574072d:00335] [ 1] /opt/conda/lib/python3.6/site-packages/lmdb/cpython.cpython-36m-x86_64-linux-gnu.so(+0x128e0)[0x7fd44f81f8e0]  [1,1] :[1a62e574072d:00335] [1,1] :[ 2] /opt/conda/lib/python3.6/site-packages/lmdb/cpython.cpython-36m-x86_64-linux-gnu.so(+0x12b74)[0x7fd44f81fb74]  [1,1] :[1a62e574072d:00335] [ 3] /opt/conda/lib/python3.6/site-packages/lmdb/cpython.cpython-36m-x86_64-linux-gnu.so(+0x14ba5)[0x7fd44f821ba5]  [1,1] :[1a62e574072d:00335] [ 4] /opt/conda/lib/python3.6/site-packages/lmdb/cpython.cpython-36m-x86_64-linux-gnu.so(mdb_get+0xbc)[0x7fd44f82240c]  [1,1] :[1a62e574072d:00335] [ 5] /opt/conda/lib/python3.6/site-packages/lmdb/cpython.cpython-36m-x86_64-linux-gnu.so(+0x9d9d)[0x7fd44f816d9d]  [1,1] :[1a62e574072d:00335] [ 6] [1,1] :python(_PyCFunction_FastCallDict+0x154)[0x55e2b8f02744]  [1,1] :[1a62e574072d:00335] [ 7] [1,1] :python(+0x19842c)[0x55e2b8f8942c]  [1,1] :[1a62e574072d:00335] [ 8] [1,1] :python(_PyEval_EvalFrameDefault+0x30a)[0x55e2b8fae38a]  [1,1] :[1a62e574072d:00335] [ 9] [1,1] :python(_PyFunction_FastCallDict+0x11b)[0x55e2b8f83bab]  [1,1] :[1a62e574072d:00335] [10] [1,1] :python(_PyObject_FastCallDict+0x26f)[0x55e2b8f02b0f]  [1,1] :[1a62e574072d:00335] [11] [1,1] :python(_PyObject_Call_Prepend+0x63)[0x55e2b8f076a3]  [1,1] :[1a62e574072d:00335] [12] [1,1] :python(PyObject_Call+0x3e)[0x55e2b8f0254e]  [1,1] :[1a62e574072d:00335] [13] [1,1] :python(+0x16b50a)[0x55e2b8f5c50a]  [1,1] :[1a62e574072d:00335] [14] [1,1] :python(_PyEval_EvalFrameDefault+0x877)[0x55e2b8fae8f7]  [1,1] :[1a62e574072d:00335] [15] [1,1] :python(_PyFunction_FastCallDict+0x11b)[0x55e2b8f83bab]  [1,1] :[1a62e574072d:00335] [16] [1,1] :python(_PyObject_FastCallDict+0x26f)[0x55e2b8f02b0f]  [1,1] :[1a62e574072d:00335] [17] [1,1] :python(_PyObject_Call_Prepend+0x63)[0x55e2b8f076a3]  [1,1] :[1a62e574072d:00335] [18] [1,1] :python(PyObject_Call+0x3e)[0x55e2b8f0254e]  [1,1] :[1a62e574072d:00335] [19] [1,1] :python(+0x16b50a)[0x55e2b8f5c50a]  [1,1] :[1a62e574072d:00335] [20] [1,1] :python(_PyEval_EvalFrameDefault+0x877)[0x55e2b8fae8f7]  [1,1] :[1a62e574072d:00335] [21] [1,1] :python(+0x19253b)[0x55e2b8f8353b]  [1,1] :[1a62e574072d:00335] [22] [1,1] :python(+0x198505)[0x55e2b8f89505]  [1,1] :[1a62e574072d:00335] [23] [1,1] :python(_PyEval_EvalFrameDefault+0x30a)[0x55e2b8fae38a]  [1,1] :[1a62e574072d:00335] [24] [1,1] :python(+0x191a76)[0x55e2b8f82a76]  [1,1] :[1a62e574072d:00335] [25] [1,1] :python(_PyFunction_FastCallDict+0x1bc)[0x55e2b8f83c4c]  [1,1] :[1a62e574072d:00335] [26] [1,1] :python(_PyObject_FastCallDict+0x26f)[0x55e2b8f02b0f]  [1,1] :[1a62e574072d:00335] [27] [1,1] :python(_PyObject_Call_Prepend+0x63)[0x55e2b8f076a3]  [1,1] :[1a62e574072d:00335] [28] [1,1] :python(PyObject_Call+0x3e)[0x55e2b8f0254e]  [1,1] :[1a62e574072d:00335] [29] [1,1] :python(+0x16b50a)[0x55e2b8f5c50a]  [1,1] :[1a62e574072d:00335] *** End of error message ***  --------------------------------------------------------------------------  Primary job  terminated normally, but 1 process returned  a non-zero exit code. Per user-direction, the job has been aborted.  --------------------------------------------------------------------------  --------------------------------------------------------------------------  mpirun noticed that process rank 0 with PID 0 on node 1a62e574072d exited on signal 7 (Bus error).  "
"(base) root@e3ff8e900e504011eb09ab60a9de5c5cd03e-wangx3189-0:/userhome/UNITER/UNITER# rm -rf output && python pretrain.py --config config/pretrain-alldata-base-8gpu.json --output_dir /userhome/UNITER/UNITER/output/  --------------------------------------------------------------------------  [[48719,1],0]: A high-performance Open MPI point-to-point messaging module  was unable to find any relevant network interfaces:    Module: OpenFabrics (openib)    Host: e3ff8e900e504011eb09ab60a9de5c5cd03e-wangx3189-0    Another transport will be used instead, although this may result in  lower performance.    NOTE: You can disable this warning by setting the MCA parameter  btl_base_warn_component_unused to 0.  --------------------------------------------------------------------------  07/15/2021 08:44:23 - INFO - __main__ -   device: cuda:0 n_gpu: 1, rank: 0, 16-bits training: True  07/15/2021 08:44:24 - INFO - __main__ -   Waiting on git info....  07/15/2021 08:44:24 - INFO - __main__ -   Git branch: master  07/15/2021 08:44:24 - INFO - __main__ -   Git SHA: 1dbfa62bb8ddb1f2b11d20775ce324c5e45a3ab4    0%|                                                                                                                                                            | 0/200000 [00:00       main(args)    File ""pretrain.py"", line 206, in main      opts.train_datasets, True, opts)    File ""pretrain.py"", line 125, in create_dataloaders      img_db = [all_img_dbs[path] for path in dset['img']]    File ""pretrain.py"", line 125, in        img_db = [all_img_dbs[path] for path in dset['img']]    File ""/userhome/UNITER/UNITER/data/data.py"", line 311, in __getitem__      self.min_bb, self.num_bb, self.compress)    File ""/userhome/UNITER/UNITER/data/data.py"", line 75, in __init__      readahead=not _check_distributed())  lmdb.Error: /userhome/UNITER/UNITER/img/coco_train2014/all: No such file or directory  Exception ignored in:  >  Traceback (most recent call last):    File ""/userhome/UNITER/UNITER/data/data.py"", line 98, in __del__      self.env.close()  AttributeError: 'DetectFeatLmdb' object has no attribute 'env'    (base) root@e3ff8e900e504011eb09ab60a9de5c5cd03e-wangx3189-0:/userhome/UNITER/UNITER#   (base) root@e3ff8e900e504011eb09ab60a9de5c5cd03e-wangx3189-0:/userhome/UNITER/UNITER#   (base) root@e3ff8e900e504011eb09ab60a9de5c5cd03e-wangx3189-0:/userhome/UNITER/UNITER#   (base) root@e3ff8e900e504011eb09ab60a9de5c5cd03e-wangx3189-0:/userhome/UNITER/UNITER# rm -rf output && python pretrain.py --config config/pretrain-indomain-base-8gpu.json --output_dir /userhome/UNITER/UNITER/output/      --------------------------------------------------------------------------  [[47594,1],0]: A high-performance Open MPI point-to-point messaging module  was unable to find any relevant network interfaces:    Module: OpenFabrics (openib)    Host: e3ff8e900e504011eb09ab60a9de5c5cd03e-wangx3189-0    Another transport will be used instead, although this may result in  lower performance.    NOTE: You can disable this warning by setting the MCA parameter  btl_base_warn_component_unused to 0.  --------------------------------------------------------------------------  07/15/2021 08:49:38 - INFO - __main__ -   device: cuda:0 n_gpu: 1, rank: 0, 16-bits training: True  07/15/2021 08:49:38 - INFO - __main__ -   Waiting on git info....  07/15/2021 08:49:38 - INFO - __main__ -   Git branch: master  07/15/2021 08:49:38 - INFO - __main__ -   Git SHA: 1dbfa62bb8ddb1f2b11d20775ce324c5e45a3ab4    0%|                                                                                                                                                            | 0/200000 [00:00       main(args)    File ""pretrain.py"", line 206, in main      opts.train_datasets, True, opts)    File ""pretrain.py"", line 125, in create_dataloaders      img_db = [all_img_dbs[path] for path in dset['img']]    File ""pretrain.py"", line 125, in        img_db = [all_img_dbs[path] for path in dset['img']]    File ""/userhome/UNITER/UNITER/data/data.py"", line 311, in __getitem__      self.min_bb, self.num_bb, self.compress)    File ""/userhome/UNITER/UNITER/data/data.py"", line 75, in __init__      readahead=not _check_distributed())  **lmdb.Error: /userhome/UNITER/UNITER/img/coco_train2014/all: No such file or directory**  Exception ignored in:  >  Traceback (most recent call last):    File ""/userhome/UNITER/UNITER/data/data.py"", line 98, in __del__      self.env.close()  **AttributeError: 'DetectFeatLmdb' object has no attribute 'env'**     ============================================   Many bugs in this code. Any plan for the update of this code?           "
"(UNITER) wangxiao@wx:~/Documents/UNITER$ python ./pretrain_vcr.py  Traceback (most recent call last):    File ""./pretrain_vcr.py"", line 27, in        from data import (TokenBucketSampler,  ImportError: cannot import name 'MlmDatasetForVCR' from 'data' (/media/wangxiao/Samsung_P2/UNITER/data/__init__.py)"
"Now, the process reports `F0625 08:25:59.166216   119 syncedmem.cpp:71] Check failed: error == cudaSuccess (2 vs. 0)  out of memory` on my 11GB 2080ti. But that's all we have."
"Hi,   â€¨I was able to run the feature extraction script, `scripts/ extract_imgfeat.sh` with the Docker image from      It processed ~750 images before I encountered this:    `/src/tools/../lib/rpn/proposal_layer.py:184: RuntimeWarning: invalid value encountered in greater_equal`  `keep = np.where((ws >= min_size) & (hs >= min_size))[0] `  `F0607 05:45:37.953456   123 roi_pooling_layer.cu:87] Check failed: error == cudaSuccess (9 vs. 0)  invalid configuration argument`    Do you have any pointers on what I could possibly try to fix this? Thank you!  "
"Due to domain conflict, I wonder if you could also release model checkpoints pretrained on out-of-domain data (CC, SBU) only?  Thanks a lot!"
"Hello.  I want to pre-train myself.    I try running the CMD here:            (after launching the docker)    But I receieve:  `[1,0] :RuntimeError: cuda runtime error (35) : CUDA driver version is insufficient for CUDA runtime version at ../torch/csrc/cuda/Module.cpp:33`     I use Cuda 10.2.    Do you know how can I solve it?     I addition, I would like to ask how long should the pretraining take?    Thank you"
"Hi,    Thank you for your gret job.    if you are going to provid the code about extract feature from a image? as current code is no easy to be used in other dataset.    Thank you!"
"Hello, I watched that you use two image features (fname_gt, fname)   At first, I thought, img_gt features for image masking tasks like MRM, MRFC etc.   But I see it is used at MLM task, so I'm not sure that you use double features even gt features have different number of bounding box objects.  Thank you  "
"when I run python train_re.py --config config/train-refcoco-base-1gpu.json  --output_dir $RE_EXP    I just get this result.    03/11/2021 11:44:06 - INFO - __main__ -   Loading Train Dataset /txt_db/refcoco_train.db, /img_db/re_coco_gt  /img_db/re_coco_gt  /txt_db/refcoco_train.db  Traceback (most recent call last):    File ""train_re.py"", line 461, in        main(args)    File ""train_re.py"", line 129, in main      ReDataset, re_collate, opts)    File ""train_re.py"", line 46, in create_dataloader      num_bb, opts.compressed_db)    File ""/src/data/data.py"", line 75, in __init__      readahead=not _check_distributed())  lmdb.Error: /img_db/re_coco_gt/feat_numbb100: No such file or directory  Exception ignored in:  >  Traceback (most recent call last):    File ""/src/data/data.py"", line 98, in __del__      self.env.close()  AttributeError: 'DetectFeatLmdb' object has no attribute 'env'    Does anyone know this?     "
"Hello, thanks for open-sourcing the amazing work!   Do you mind sharing more experiment details for image feature extraction?   For example, what are the score threshold, NMS threshold, and the number of detections to keep?   For every image, do you always generate the same amount of detections so that the pooled ROI feature dimension will be the same for all images? Thank you again for your time!"
Hi.    Why did you decide to use a BCE loss on the ITM pretraining text and a ranking loss on the ITM downstream task? Is there any intuition behind this? Why not use a ranking loss on both?    Thanks for the great work.    -Caleb
"After reproducing paper results, I am interested in the model's (ITM) performance on different datasets. To achieve that I have to run the preprocessing pipeline on my own images but I am getting errors on all different GPU setups I have attempted it on (K-80 | V100 | A100) because of CUDA version issues. Could you please help me troubleshoot? Perhaps switching versions of nvidia/cuda on the host would help?    You can see the stacktrace below, this particular host machine has 8 V100 GPUS, CUDA 10.2 and nvidia-driver 440.     "
"Hi @ChenRocks , I see the location features for each region is a 7-dimensional vector, so could you please tell me the order of coordinates? Is it [x1, y1, x2, y2, w, h, w âˆ— h] or [y1, x1, y2, x2, h, w, w âˆ— h]?  Thank you!"
Thanks for your great workï¼  Could you please share the code for create img / txt db for CC / SBU ï¼Ÿ  Thanks againï¼
"Hi all.    I'm Henry Lee, a graduate student at Seoul National University.    Thanks for sharing your great work for future research.    If you don't mind, could you share the fine-tuned weights of COCO image-text retrieval?    As you mentioned in the repo, it's really heavy so I could not fine-tune with the GPUs I can use in our university.    I do not need the exact same parameters that are reported in the paper, I just want to use them for other tasks.    Thank you in advance.  Henry"
None
"Hi @ChenRocks @linjieli222,  It is still ok even if it is not used by   ,  , and  .    - I have to handle 1,102,076+ personal images and have to find a fast one even the performance might be lower -- it is fine NOT to be any of  ,  , and  , but **better faster to implement**.    For example, by comparison with one GPU (cpu-only is presumably  ), for NLVR2 107,292 images,    takes 5-6 hours to extract faster-rcnn features by  .  I also follow visualBERT's   and  ,  ,  .   "
"Hey, I've downloaded the source data, but there's only data.mdb and lock.mdb file. But, I can't access them for lacking of the database's information.  something like this:  # connStr = (  #     r'Driver={SQL Sever};'  #     r'Server=sqlserver;'  #     r'Database=bill;'  #     r'UID=sa;'  #     r'PWD=passwd'  # )    I would like to know if txt_db stores sentences and img_db stores npz files?  @lichengunc @linjieli222 "
"When I try to run the pretrain.py file, and set the work=8, the following error will appear. The training is very slow when the worker=4, the gpu utilization is very low,  How do I solve it? thanks  !         "
"hi, the uniter-large.pt released is trained on in-domain & out-of-domain data or only in-domain data?"
What vocab file does the uniter model use? cased or uncased? 
"i want to fintune the uniter model in my own dataset, how to generate the lmdb dataset for images and text? i generate the image features from faster rcnn, but how to convert the text content and image features into uniter input format? is there any code in your github show? thanks!"
"In the pretrain.py code,  why is the value of IMG_ LABEL_ DIM is 1601?"
"Hi,  Can you please upload the pre-processing code (Used to convert SNLI-VE to txt dB and IMDB) for the SNLI-VE dataset"
"Hi, I met the following problem recently:    W /tmp/pip-install-fzrlm1c4/horovod/horovod/common/stall_inspector.cc:105] One or more tensors were sub  mitted to be reduced, gathered or broadcasted by subset of ranks and are waiting for remainder of ranks for more than 60 seconds. This may indicate that different ranks   are trying to submit different tensors or that only subset of ranks is submitting tensors, which will cause deadlock    It happed when broadcast the model using train_vpa.  I run the code on two gpus. Do you know how to solve it? Thanks  "
Hi!    I want visualize result like your paper's Figure 7(Additional text-to-image attention visualization example).    Could you provide visualization code?    thank you:)
1. Is the vocabulary available that takes the words of the questions and converts them to 'input_ids'?  2. Is there a function that does this for an input question?  3. Is there a code that take a single image-question pair and predicts the answer?
"Thank you so much for providing the code for UNITER, great work!  The README mentions the following ""NOTE: Image-Text Retrieval is computationally heavy, especially on COCO.""    Do you have a rough estimation in mind of how many hours (or days) does it take to run the full text-to-image retrieval on COCO on 1 GPU (eg. Tesla V100).    Thanks for your help!  "
"Thanks for your contribution!  I run your code and can not reproduce the performance in your paper.  There my results in two different settingï¼š  Finetuning Image-text retrieval with ""train-itm-flickr-base-8gpu.json""   <img width=""417"" alt=""å›¾ç‰‡"" src=""     Finetuning Image-text retrieval with""train-itm-flickr-base-16gpu-hn.json""  <img width=""464"" alt=""å›¾ç‰‡"" src=""       There is a gap between the results and those in your paper.  What's the difference between the experiment you do and the code you released? For example,  training steps and learning rate. Thanks!"
None
"I cannot find the code tools/generate_npz.py,  i don't know how to generate the image feature file. Could you  please upload your code? Thanks!"
None
"Hi, I have a problem in training.    1. The last elements of location features (w x h in [x1, y1, x2, y2, w, h, w Ã— h]) will cause overflow in fp16 training mode. So how do you train in fp16?  2. And what is the meaning of ""normalized"" in the location feature? Could you provide an example?    Thanks!"
None
"In the feature extraction code, I notice that although `Image` is used to get `img`, but `im` read from `cv2` is used later. It's confusing that `img = Image.open(im_file).convert('RGB')` is used, but no  `cv2.cvtColor(im_cv, cv2.COLOR_BGR2RGB)`.       So, is RGB or BGR that we need to use for this step?"
The following code for downloading the image dbs of vg is missed in the `download_indomain.sh`      
Hi could you also release `prepro.py` for VQA?   I have my own alternative questions and answers for VQA images and I want to test UNITER (and VILLA) on these.      It seems that `prepro.py` only has a `process_nlvr2()` function -- but not for VQA
"Hi I'm getting these errors for all extractions (line 22, 28) for the dataset . Unable to run either training or inference because of this    $ tar -xvf nlvr2_dev.tar  nlvr2_dev/  nlvr2_dev/feat_th0.2_max100_min10/  nlvr2_dev/feat_th0.2_max100_min10/data.mdb  tar: Unexpected EOF in archive  tar: rmtlseek not stopped at a record boundary  tar: Error is not recoverable: exiting now"
"Hi, I don't quite understand the code here.         `self._size_mul` is used for partitioning, then why we need to add it when checking if the full token length is exceeded?"
"Hello,  I have some questions regarding RefCOCO/+/g training / evaluation details.    1. Are you going to upload RefCOCO/+/g training/evaluation codes?  2. Which boxes did you finetune UNITER on?  3. Which boxes did you use to evaluate on val, test, val^d, and test^d evaluation respectively? Did you use  ?    Table from UNITER  !     It seems ViLBERT-MT authors finetuned their model on 100 BUTD boxes + Mask R-CNN boxes from MattNet->  .  Then they used 100 BUTD boxes during evaluation ->      I calculated oracle scores on RefCOCOg val split: ""if there exists a candidate box with iou(candidate,target) > 0.5 => correct""    Mask R-CNN boxes from MAttNet -> 86.10%  MS COCO GT boxes -> 99.6%  VilBERT-MT's 100 BUTD boxes on RefCOCOg -> 96.53%    Since BUTD boxes have better coverage on Mask R-CNN boxes from MAttNet, I don't think this is fair comparison to MattNet. Also this is not consistent with the ViLBERT-MT paper.    Paragraph from ViLBERT-MT  !     ViLBERT-MT authors compared ViLBERT-MT and UNITER on test^d. I wonder which boxes you used for UNITER finetuning and evaluation.    Table from ViLBERT-MT  !                     "
Would it be possible to release the code or references for performing the second stage pretraining for VCR?
None
"[1,2] :  File ""train_itm.py"", line 410, in    [1,2] :    main(args)  [1,2] :  File ""train_itm.py"", line 221, in main  [1,2] :    LOGGER.info(f""image retrieval R1: ""  [1,2] :KeyError: 'img_r1'  [1,1] :Traceback (most recent call last):  [1,1] :  File ""train_itm.py"", line 410, in    [1,1] :    main(args)  [1,1] :  File ""train_itm.py"", line 221, in main  [1,1] :    LOGGER.info(f""image retrieval R1: ""  [1,1] :KeyError: 'img_r1'  [1,3] :Traceback (most recent call last):  [1,3] :  File ""train_itm.py"", line 410, in    [1,3] :    main(args)  [1,3] :  File ""train_itm.py"", line 221, in main  [1,3] :    LOGGER.info(f""image retrieval R1: ""  [1,3] :KeyError: 'img_r1'  [1,0] :  [1,0] :10/24/2020 15:08:26 - INFO - __main__ -   evaluation finished in 565 seconds  [1,0] :10/24/2020 15:08:26 - INFO - __main__ -   image retrieval R1: 63.73,  [1,0] :image retrieval R5: 87.61,  [1,0] :image retrieval R10: 93.51  [1,0] :text retrieval R1: 74.95,  [1,0] :text retrieval R5: 92.70,  [1,0] :text retrieval R10: 97.14  [1,0] :10/24/2020 15:08:26 - INFO - __main__ -   ==================================================================  [1,0] :Traceback (most recent call last):  [1,0] :  File ""train_itm.py"", line 410, in    [1,0] :    main(args)  [1,0] :  File ""train_itm.py"", line 176, in main  [1,0] :    all_reduce_and_rescale_tensors(grads, float(1))  [1,0] :  File ""/root/paddlejob/workspace/env_run/utils/distributed.py"", line 35, in all_reduce_and_rescale_tensors  [1,0] :    hvd.allreduce_(buffer_t[:offset])  [1,0] :  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/horovod/torch/mpi_ops.py"", line 252, in allreduce_  [1,0] :    handle = allreduce_async_(tensor, average, name, op)  [1,0] :  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/horovod/torch/mpi_ops.py"", line 228, in allreduce_async_  [1,0] :    return _allreduce_async(tensor, tensor, name, op)  [1,0] :  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/horovod/torch/mpi_ops.py"", line 127, in _allreduce_async  [1,0] :    name.encode() if name is not None else _NULL, true_op)  [1,0] :RuntimeError: Horovod has been shut down. This was caused by an exception on one of the ranks or an attempt to allreduce, allgather or broadcast a tensor after one of the ranks finished execution. If the shutdown was caused by an exception, you should see the exception in the log before the first shutdown message.  [1,0] :   10%|#         | 2000/20000 [34:10<5:07:33,  1.03s/it]-------------------------------------------------------  Primary job  terminated normally, but 1 process returned  a non-zero exit code. Per user-direction, the job has been aborted.  -------------------------------------------------------  --------------------------------------------------------------------------  mpirun detected that one or more processes exited with non-zero status, thus causing  the job to be terminated. The first process to do so was      Process name: [[10998,1],3]    Exit code:    1"
"I can't use your docker so I installed the library manually, I got this error, can you give some advice? thank you very much.    "
A great work!     I find it is difficult to prepare the pretraining data for the code. Could you please release a sample of pretraining data? It would be better if you could provide the code or procedures for data pre-processing.    Thanks a lot!
"Hi, I find that you use dense captions with all the box features. Can I concate them?"
"Hello,thanks for your codeï¼š          when I run the second step""Launch the Docker container for running the experiments"",it downloads some image slowly,how can I download it locally?thanks againï¼  !   !   "
"Hello I am just onboarding this repo and am stuck at the following step:    >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  4. Run inference for the NLVR2 task and then evaluate.    # inference  python inf_nlvr2.py --txt_db /txt/nlvr2_test1.db/ --img_db /img/nlvr2_test/ \      --train_dir /storage/nlvr-base/ --ckpt 6500 --output_dir . --fp16  >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>.    I first got an hvd error that I resolved by adding a hvd.init() in the /data/data.py after importing hvd.    But now I get the error below. Can you please let me know what I could be doing incorrectly?  >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  Traceback (most recent call last):    File ""inf_nlvr2.py"", line 138, in        main(args)    File ""inf_nlvr2.py"", line 71, in main      results = evaluate(model, eval_dataloader, device)    File ""/opt/conda/lib/python3.6/site-packages/torch/autograd/grad_mode.py"", line 43, in decorate_no_grad      return func(*args, **kwargs)    File ""inf_nlvr2.py"", line 92, in evaluate      scores = model(**batch, targets=None, compute_loss=False)    File ""/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__      result = self.forward(*input, **kwargs)    File ""/opt/conda/lib/python3.6/site-packages/apex/amp/_initialize.py"", line 177, in new_fwd      **applier(kwargs, input_caster))  TypeError: forward() got an unexpected keyword argument 'input_ids'    >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  "
"hi!  i try convert vcr dataset to itm task.  ( i make data [CLS] question [SEP] answer [SEP] or [CLS] question [SEP] answer [SEP] rationale [SEP])  but i got error below.  the error start at ""model/UniterTextEmbeddings"".( maybe ""words_embeddings = self.word_embeddings(input_ids)"")  Have you seen this error?  i tried to solve this problem during three days, but i can't find why this error occured.    thank you :)     "
hi     i run inf_vcr.py for original model.  i got this result  Do you know what the problem is?   
None
during training on VCR i got this error.  have you seen this error?    thank you     
"one stage pre-training approach is using all task(MLM+ITM+MRC-kl+MRFR+WRA).    two stage pre-training is using  MLM, MRFR , MRC task. is that right?    Thank you for always kind reply :) "
"Hi everyone,    I am creating a UNITER model for a classification task, but after a few steps of training it launches the error     > RuntimeError: CUDA error: device-side assert triggered         Have you encounter this error before?    If I try to make a forward pass before training the outputs are correct       The model     "
"  i got this during vcr training.  (""Warning: NaN or Inf found in input tensor"" and ""Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0"")  Is the problem caused by hardware?    thank you!   "
i got this problem.  how do i do?    thank you   
can it train without horovodrun?    thank you!
  i got this error!  would you check   ... server?  thank you:)     
I am facing the same problem as #2. The download breaks in the middle and restarts. Any alternative method available?
"Hi,  Currently, there is only the in-domain pre-training config in this repo. Could you please provide the config files for the out-of-domain and the out-of-domain&in-domain pre-training process? Since the detection scripts and the docker image have been already provided, we can try to generate the out-of-domain feature files and do pre-train by ourselves.  "
Hi. Could you provide the code for downloading the out-of-domain dataset (Conceptual Captions & SBU captions).     thank you!
"Hi,    Thank you for releasing the code and checkpoints to reproduce the results of UNITER. We notice that the features of vg has still not beed uploaded yet, which is needed to run VQA fine-tuning. Could you please provide this file? Thank you very much!"
"     From the paper, I think there only one layernorm in the last step, but I notice there are more layernorms."
"While recreating the preprocessing pipeline, I am not able to locate the file generate_npz.py for image feature extraction. This file is referenced in scripts/extract_imgfeat.sh for your reference."
"Thank you for the great method UNITER (both code and the paper), but I really got confused when I read the detials of WRA, I'm so thankful if you are convenient to release the code of the parts of WRA loss functions.    Best wishes and appreciations anyhow."
"The following does not work -      The correct weights are not getting loaded by the model. I suspect it happens because the file `uniter-base.pt` has keys for UNITER weights which are of the form `uniter.encoder.*`, where as the model's `state_dict` expects keys to be of the form `encoder.*`.     A quick fix for it was to modify the loaded weights' keys to conform to the `model.state_dict().keys()`"
"Hi,    Can you share your pretraining code as well ?     Thanks.  "
"Hi, thanks for the great repo! I wonder would you be able to share the codes/models for reproducing the image retrieval results? Specifically, could you specify the architecture for the ITM layer?    We've encountered an issue in reproducing your results of image retrieval task on Flickr30k dataset.    I finetuned the image retrieval tasks (on Flickr30k) on your conceptual caption pretrained model (uniter-base.pt). Specifically, I finetuned on all architectures except the ITM head, as I don't know the specific architecture of the ITM head.    With the same hyperparameter in your paper, the best results for image retrieval on Flickr30k Recall@1 I could achieve is 63, which is far from the results in the paper: Table 4 IR Flickr Recall@1 73.    I wonder could you share the codes/models/architectures for image retrieval tasks?    Thanks a lot."
"Thanks for your great workï¼I wanna ask :   In the pre-training of the second stage of the VCR task, what is the format of pre-training dataï¼Ÿ  Thanks again!  "
Thanks for your great workï¼  Could you please share the code on VCR task including the pre-training code on VCR ï¼Ÿ  Thanks againï¼
"Just as the title, could you please also provide the processed data(image feature, location information) used in SNLI-VE task?"
"Thank you for your great work.  Could you please provide the codes of the pretraining process in detail? Specifically, the MLM, MRM, and ITM.  "
"I noticed that in the paper, the input of the transformer is illustrated through the model image. The image is on the left while text is on the right.     !       However, in the code    it's the other way around.    Just curious, I guess the performance is not affected."
"Hi,   I want to get an understanding of what happened during the pretraining, especially the train and validation loss change.  Thank you."
"Hi there,    Thank you for sharing of this great repo. Do you have a plan to release a version without docker requirement?    Best,"
"Hi, Thank you for sharing nice work.  It seems your work is really good at vision-language multi-grounding.  I wanna check the performance with other tasks.  What should i follow, if i wanna pre-train the model with other dataset?  Thank you."
"Hi,  Thanks for the great repo! In the pretrained model, there are some weights named as ITM.weight ITM.bias. I wonder are those weights for image-text matching? If so, could you share the architecture for image-text matching classifier?  Thanks a lot!"
"When I trying to reproduce preprocessing, I find this file is missing. Can you upload that file? Thanks."
"Like the above, Could you please provide the environment requirements? I'd like to build the environment on my server without docker. Thanks!"
"Like the above, we cannot download the nlvr2 dataset very well. The download process can be break easily, so we can't download the training dataset for that it's big, maybe it's the server problem?  Hope your response, thanks!"
i want to train vcr data!  Could you provide the code for the VCR dataset?    thank you!:)
"How this model handles the scenario where two similar dressed up persons cross over or single person changing the appearance in the course of Video stream,it might lose the tracking object  in either of the cases"
"When I finish training my data set, I get **latest.pt**, errors are reported when tracking with this weight as follows:  File ""E:\python_workspace\JDE\track.py"", line 203, in        main(opt,    File ""E:\python_workspace\JDE\track.py"", line 139, in main      nf, ta, tc = eval_seq(opt, dataloader, data_type, result_filename,    File ""E:\python_workspace\JDE\track.py"", line 90, in eval_seq      online_targets = tracker.update(blob, img0)    File ""E:\python_workspace\JDE\tracker\multitracker.py"", line 318, in update      track.activate(self.kalman_filter, self.frame_id)    File ""E:\python_workspace\JDE\tracker\multitracker.py"", line 63, in activate      self.mean, self.covariance = self.kalman_filter.initiate(self.tlwh_to_xyah(self._tlwh))  ZeroDivisionError: division by zero    How to solve this problemï¼Ÿ"
Thanks for your awesome research!  I have a quick question about this project.  can I use the model with real-stream like camera?
èƒ½å¦è¿›è¡Œå¤šç±»åˆ«çš„è·Ÿè¸ªï¼Ÿ
"ä½œè€…æ‚¨å¥½ï¼Œå¾ˆæœ‰å¹¸èƒ½æ‹œè¯»æ‚¨çš„ä»£ç ã€‚ä½†æ˜¯æˆ‘æœ‰ä¸€ä¸ªå°é—®é¢˜æƒ³å‘æ‚¨è¯·æ•™ï¼Œâ€œself.lost_stracks = sub_stracks(self.lost_stracks, self.removed_stracks)â€ï¼Œ è¿™æ˜¯åœ¨æ‚¨çš„ä»£ç mutitrackerä¸­çš„ç¬¬336è¡Œã€‚è¿™é‡Œæ‚¨é‡‡ç”¨å°†ä¸¤ä¸ªè½¨è¿¹é›†åˆç›¸å‡çš„æ–¹æ³•ä»¥åŽ»é™¤self.lost_stracksè½¨è¿¹é›†åˆä¸­çš„removedçš„è½¨è¿¹ï¼Œä½†æ˜¯åœ¨æ­¤æ—¶ self.removed_stracksä¸­å¹¶æ²¡æœ‰å¯¹å½“å‰å¸§çš„removedçš„è½¨è¿¹è¿›è¡Œæ·»åŠ ï¼Œè¿™æ ·ä¼šä¸ä¼šé€ æˆself.lost_stracksè½¨è¿¹é›†åˆä¸­æ— æ³•åŽ»é™¤å½“å‰å¸§æ–°ç”Ÿæˆçš„removedè½¨è¿¹ï¼Ÿ"
"Hello   @Zhongdao   Why the embedding representation of appearance is D*H*W, how can so many representations distinguish the representation of positive and negative samplesï¼Ÿ"
"Hi. Thank you for your great work.    I just want to ask if you did minus the center_x and center_y by 1. Because the MOT dataset's coordinates are 1-based indexing, but YOLO (I think) assumes 0-based indexing from the COCO dataset.  Not much of difference but still I would like to ask.    Thank you."
"Hello, thanks for your excellent work    When I training in VisDrone dataset, the conf during training will decrease to around 0.002 after 35 Epochs, and the Precision is good but Recall is a bit low(around 50%)    is this normalï¼Ÿ  "
Hello! Can anyone help me understand what these training values mean? Thank you.
æˆ‘æƒ³é—®ä¸€ä¸‹ï¼Œæ€Žä¹ˆæ‰èƒ½æŸ¥çœ‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±æ›²çº¿å‘¢ï¼Ÿç®—æ³•ä»£ç é‡Œé¢æœ‰æä¾›è¿™ä¸ªåŠŸèƒ½å—ï¼Ÿè¿˜æ˜¯è¯´è¦è‡ªå·±å¦å¤–å†™ä»£ç ï¼Ÿè°¢è°¢ã€‚  ~~  I want to know how can I check loss curve after training? Does your repo provide this function? Or I should write the code by myslef? Thank you 
"ä¸ºä»€ä¹ˆè®­ç»ƒå®Œè‡ªå·±çš„æ•°æ®é›†åŽæ²¡æœ‰ï¼Œä½¿ç”¨æƒé‡æµ‹è¯•ä½†æ˜¯æ²¡æœ‰å¾—åˆ°æ£€æµ‹ç»“æžœï¼Œæˆ‘è¾“å‡ºzhuganç½‘ç»œçš„é¢„æµ‹ç»“æžœé™¤äº†frameæœ‰è¾“å‡ºï¼Œtlwh,å’Œidéƒ½æ²¡æœ‰è¾“å‡ºã€‚  !   "
"Namespace(cfg='cfg/yolov3_1088x608.cfg', conf_thres=0.5, input_video='.\\assets\\IMG_0055.gif', iou_thres=0.5, min_box_area=200, nms_thres=0.4, output_format='video', output_root='.\\d  ata', track_buffer=30, weights='jde.1088x608.uncertainty.pt')    2022-03-01 02:40:36 [INFO]: Starting tracking...  Lenth of the video: 129 frames  2022-03-01 02:40:37 [INFO]: Torch not compiled with CUDA enabled  ffmpeg version 2022-02-28-git-7a4840a8ca-full_build-www.gyan.dev Copyright (c) 2000-2022 the FFmpeg developers    built with gcc 11.2.0 (Rev7, Built by MSYS2 project)      configuration: --enable-gpl --enable-version3 --enable-static --disable-w32threads --disable-autodetect --enable-fontconfig --enable-iconv --enable-gnutls --enable-libxml2 --enable-g  mp --enable-bzlib --enable-lzma --enable-libsnappy --enable-zlib --enable-librist --enable-libsrt --enable-libssh --enable-libzmq --enable-avisynth --enable-libbluray --enable-libcaca   --enable-sdl2 --enable-libdav1d --enable-libdavs2 --enable-libuavs3d --enable-libzvbi --enable-librav1e --enable-libsvtav1 --enable-libwebp --enable-libx264 --enable-libx265 --enable-l  ibxavs2 --enable-libxvid --enable-libaom --enable-libopenjpeg --enable-libvpx --enable-mediafoundation --enable-libass --enable-frei0r --enable-libfreetype --enable-libfribidi --enable  -libvidstab --enable-libvmaf --enable-libzimg --enable-amf --enable-cuda-llvm --enable-cuvid --enable-ffnvcodec --enable-nvdec --enable-nvenc --enable-d3d11va --enable-dxva2 --enable-l  ibmfx --enable-libshaderc --enable-vulkan --enable-libplacebo --enable-opencl --enable-libcdio --enable-libgme --enable-libmodplug --enable-libopenmpt --enable-libopencore-amrwb --enab  le-libmp3lame --enable-libshine --enable-libtheora --enable-libtwolame --enable-libvo-amrwbenc --enable-libilbc --enable-libgsm --enable-libopencore-amrnb --enable-libopus --enable-lib  speex --enable-libvorbis --enable-ladspa --enable-libbs2b --enable-libflite --enable-libmysofa --enable-librubberband --enable-libsoxr --enable-chromaprint    libavutil      57. 21.100 / 57. 21.100    libavcodec     59. 21.100 / 59. 21.100    libavformat    59. 17.102 / 59. 17.102    libavdevice    59.  5.100 / 59.  5.100    libavfilter     8. 27.100 /  8. 27.100    libswscale      6.  5.100 /  6.  5.100    libswresample   4.  4.100 /  4.  4.100    libpostproc    56.  4.100 / 56.  4.100  [image2 @ 000001af85bc9740] Could find no file with path '.\data\frame/%05d.jpg' and index in the range 0-4  .\data\frame/%05d.jpg: No such file or directory      Hi,, I encoutered this error while trying to run the demo.py by PyCharm on Windows 10. Can anyone help me to solve this error?  "
"I download the Cityperson from the  . However, I tried to unzip it. It comes with error.          Does anyone know what's going on."
"eval_seq(opt, dataloader, 'mot', result_filename,                   save_dir=frame_dir, show_image=False, frame_rate=frame_rate)    æˆ‘æŠŠ show_image=Falseæ”¹æˆshow_image=Trueï¼Œä½†æ˜¯æ— æ³•æ˜¾ç¤ºå›¾ç‰‡ï¼Œè¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Œå°±æ˜¯å¼¹å‡ºä¸€ä¸ªçª—å£ï¼Œç„¶åŽæç¤ºæœªå“åº”"
"I have run the demo.py successfully on colab. However, the result.mp4 cannot be played.   Is there method to resolve this ?   Thank you"
"hi there, thanks for your work!  i just train the model on the cuhksysu dataset, and modify the ccmcpe.json as below    > {      ""root"":""H:/my_datasets/DataZoo"",      ""train"":      {          ""cuhksysu"":""./data/cuhksysu.train""      },      ""test_emb"":      {          ""caltech"":""./data/caltech.10k.val"",          ""cuhksysu"":""./data/cuhksysu.val"",          ""prw"":""./data/prw.val""      },      ""test"":      {          ""caltech"":""./data/caltech.val"",          ""citypersons"":""./data/citypersons.val""      }  }    after 30 epochs , i got a weight.  but i run the track.py and test on mot16 train dataset, i got   !     could you give me some adviceï¼Ÿ thanks a lotï¼   "
None
None
"Hi,   I am unable to download the MOT17 data. When I unzip it, it comes that file is a bad zip file. Can you give me the new link to it"
     Why not use appearance affinity to associate unmatched detections in the current frame with unconfirmed tracklets which were created as new tracks in the last frame?    Why use different thresholds of box IoU in Step 3 and Step 4?
"     Line 336 should be `self.lost_stracks = sub_stracks(self.lost_stracks, removed_stracks)` given the context of the code.  Or swap Line 336 and Line 337."
"# matching.py    from scipy.spatial.distance import cdist    def embedding_distance(tracks, detections, metric='cosine'):      cost_matrix = np.zeros((len(tracks), len(detections)), dtype=np.float)      if cost_matrix.size == 0:          return cost_matrix      det_features = np.asarray([track.curr_feat for track in detections], dtype=np.float)      track_features = np.asarray([track.smooth_feat for track in tracks], dtype=np.float)      cost_matrix = np.maximum(0.0, cdist(track_features, det_features))       # cdisté»˜è®¤æ˜¯æ±‚æ¬§å¼è·ç¦»      return cost_matrix"
"hi,I want to implement ship tracking  How to customize the datasetï¼Ÿ"
"I finally managed to get the docker image run on windows. When I tried to run demo.py in the container I got this error:    `root@9354e09d581a:/Towards-Realtime-MOT# python /Towards-Realtime-MOT/demo.py --input-video /Towards-Realtime-MOT/raw/MOT16-01-raw.gif --weights /Towards-Realtime-MOT/cfg/darknet53.conv.74 --output-format video --output-root /Towards-Realtime-MOT/  Namespace(cfg='cfg/yolov3_1088x608.cfg', conf_thres=0.5, input_video='/Towards-Realtime-MOT/raw/MOT16-01-raw.gif', iou_thres=0.5, min_box_area=200, nms_thres=0.4, output_format='video', output_root='/Towards-Realtime-MOT/', track_buffer=30, weights='/Towards-Realtime-MOT/cfg/darknet53.conv.74')    2021-08-08 20:53:24 [INFO]: Starting tracking...  Traceback (most recent call last):    File ""/Towards-Realtime-MOT/demo.py"", line 84, in        track(opt)    File ""/Towards-Realtime-MOT/demo.py"", line 52, in track      dataloader = datasets.LoadVideo(opt.input_video, opt.img_size)    File ""/Towards-Realtime-MOT/utils/datasets.py"", line 94, in __init__      self.w, self.h = self.get_size(self.vw, self.vh, self.width, self.height)    File ""/Towards-Realtime-MOT/utils/datasets.py"", line 98, in get_size      wa, ha = float(dw) / vw, float(dh) / vh  ZeroDivisionError: float division by zero  `    It happened no matter what video input I chose (I tried both gif and webm formats) and with a few trained models (e.g. darknet53.conv.74 that was in the link here, or jde_darknet53_30e_1088x608.pdparams that was in the baidu repo, I also tried inputting one of the cfg files but now I'm pretty sure it was nonsense).  What else am I doing wrong?  "
None
"Thanks for the excellent work!!! I have a question regrading the scaling in preprocess.     During inference time, the image is scale to values between 0 and 1.          However, during the training time, I don't find any scaling on image. Is that scaling not important for training?     Many thanks!"
None
è¯·é—®æ˜¯å¦éœ€è¦å°†å›¾ç‰‡å’Œæ ‡æ³¨æŒ‰ç…§è§†é¢‘åºåˆ—ä»Žå‰åˆ°åŽé¡ºåºé€å…¥ç½‘ç»œå­¦ä¹ ï¼Ÿå¦‚æžœshuffleä¸€ä¸‹ï¼Œå¯¹reidåˆ†æ”¯çš„è®­ç»ƒæœ‰å½±å“å—ï¼Ÿ
"Hello, @Zhongdao  first of all, thanks for sharing your work here with us.  I was trying to run track.py as you mentioned in the readme file, and I downloaded the dataset you provided MOT16 but I have this weird error, I'm not sure what is the problem!  I try to run the following:  python track.py --cfg cfg/yolov3_1088x608.cfg --weights weights/jde.1088x608.uncertainty.pt     and I get this error msg:  2020-06-27 00:21:22      When I add --test-mot16 to the command line,the results are correct  "
"when i run train.py ,there are not any output for a long time."
None
"Hi@Zhongdao, thanks for you great job,  I want to know  the C++ implementation of JDETracker.  if there is no C++ version, it is nice for you to tell me which project would helps to do this?   "
"Hi, I got a problem with loading the models. I cloned and installed all the things listed in the README.md files. However, when I try to run the demo file:       It cannot run and raise these error       I found that it happened at load function     More concretely, it happend at       I try with both python3.6 and python3.7 and it stills the same.    So, what happened with the model files? Please help! Thank you"
Firstï¼ŒI found the jde.1088x608.uncertainty.pt is at epoch 60ï¼Œso I started a training process for 60 epochsã€‚All settings are original in the code includes the dataset settingã€‚At lastï¼ŒI get a result with mAP=0.7888 while jde.1088x608.uncertainty.pt can attach mAP = 0.8209. Because of the gap in mAPï¼Œ it also gets a lower MOTA metricã€‚Dose anyone also  take the experiment as mineï¼Ÿ
None
We just got MOTA of 73.1 70.1 63.5ï¼Œcan not reproduce your results MOTA of 74.8 70.8 63.7 in README.md.   Please give more details.
"I download the pre-trained model and test on the MOT-16 training set, only get the result with MOTA 73.1.  How can I get 74.8 MOTA described in readme.md? Does the type of GPU mattersï¼Ÿor the dataset contains other dataï¼Ÿ              IDF1   IDP   IDR  Rcll  Prcn  GT  MT  PT ML   FP    FN  IDs    FM  MOTA  MOTP IDt IDa IDm  MOT16-02 55.6% 60.9% 51.2% 75.3% 89.5%  54  26  23  5 1581  4406  285   480 64.8% 0.197 113  77  11  MOT16-04 72.2% 79.8% 65.9% 78.6% 95.3%  83  45  24 14 1856 10158  158   367 74.4% 0.169  57  62   2  MOT16-05 76.3% 80.2% 72.9% 85.9% 94.5% 125  85  34  6  339   961  172   203 78.4% 0.192  90  58  25  MOT16-09 56.8% 62.2% 52.3% 78.6% 93.4%  25  15  10  0  290  1127  136   143 70.5% 0.175  66  36   7  MOT16-10 66.3% 68.9% 63.8% 82.2% 88.7%  54  30  24  0 1288  2198  295   520 69.3% 0.217 122  72   3  MOT16-11 75.6% 76.5% 74.8% 91.5% 93.6%  69  54  14  1  577   780   82   165 84.3% 0.154  28  37   5  MOT16-13 73.8% 79.3% 69.0% 81.2% 93.3% 107  69  36  2  662  2158  184   467 73.8% 0.212 102  77  35  OVERALL  68.9% 74.4% 64.1% 80.3% 93.1% 517 324 165 28 6593 21788 1312  2345 73.1% 0.183 578 419  88"
  !       like this.  Could anyone tell me? It is OK?  
"Hello, Thanks for your wonderful works. I meet some problems when I training with my custom dataset, the loss is nan, Is this normal, if not, could u give me some advice?  !   "
"Hi guys  Thanks so much for this great pkg.    I ran it on some of my lab data, and the results are quite poor:         Any advice is appreciated.    Thanks"
"  Namespace(cfg='cfg/yolov3_1088x608.cfg', conf_thres=0.5, input_video='G:\\datasets\\webm\\MOT16-03.mp4', iou_thres=0.5, min_box_area=200, nms_thres=0.4, output_format='video', output_root='results', track_buffer=30, weights='G:/datasets/jde.uncertainty.pt')    ----------------------------------------------------------------------------------------------------------------------------------------------    2020-11-17 09:22:07 [INFO]: Starting tracking...  Lenth of the video: 1500 frames  2020-11-17 09:23:07 [INFO]: Torch not compiled with CUDA enabled  ffmpeg version 4.3.1-2020-11-02-essentials_build-www.gyan.dev Copyright (c) 2000-2020 the FFmpeg developers    built with gcc 10.2.0 (Rev3, Built by MSYS2 project)    configuration: --enable-gpl --enable-version3 --enable-static --disable-w32threads --disable-autodetect --enable-fontconfig --enable-iconv --enable-gnutls --enable-libxml2 --enable-gmp --enable-lzma --enable-zlib --enable-libsrt --enable-libssh --enable-libzmq --enable-avisynth --enable-sdl2 --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxvid --enable-libaom --enable-libopenjpeg --enable-libvpx --enable-libass --enable-libfreetype --enable-libfribidi --enable-libvidstab --enable-libvmaf --enable-libzimg --enable-amf --enable-cuda-llvm --enable-cuvid --enable-ffnvcodec --enable-nvdec --enable-nvenc --enable-d3d11va --enable-dxva2 --enable-libmfx --enable-libgme --enable-libopenmpt --enable-libopencore-amrwb --enable-libmp3lame --enable-libtheora --enable-libvo-amrwbenc --enable-libgsm --enable-libopencore-amrnb --enable-libopus --enable-libspeex --enable-libvorbis --enable-librubberband    libavutil      56. 51.100 / 56. 51.100    libavcodec     58. 91.100 / 58. 91.100    libavformat    58. 45.100 / 58. 45.100    libavdevice    58. 10.100 / 58. 10.100    libavfilter     7. 85.100 /  7. 85.100    libswscale      5.  7.100 /  5.  7.100    libswresample   3.  7.100 /  3.  7.100    libpostproc    55.  7.100 / 55.  7.100  [image2 @ 000001fde918da40] Could find no file with path 'results\frame\%05d.jpg' and index in the range 0-4  results\frame\%05d.jpg: No such file or directory    ----------------------------------------------------------------------------------------------------------------------------------------------    åœ¨windowsä¸‹cpuå§‹ç»ˆæ— æ³•è¿è¡Œï¼Œæœ‰äººå’Œæˆ‘æœ‰ä¸€æ ·çš„é—®é¢˜å—ï¼Ÿ"
"I already have a well-trained yolov3 weight. Is it possible that this weight is used for JDE training? Something like load this weight for JDE detection part and freeze them, then  just train the id parts?"
Can anyone show me how to run with camera or webcam? Many thanks.
"RuntimeError: Error(s) in loading state_dict for Darknet:          size mismatch for classifier.weight: copying a param with shape torch.Size([519, 512]) from checkpoint, the shape in current model is torch.Size([14455, 512]).          size mismatch for classifier.bias: copying a param with shape torch.Size([519]) from checkpoint, the shape in current model is torch.Size([14455]).  "
"where can I download the weight----""weights/latest.pt"""
"@Zhongdao Appreciate your work :+1: ,  can you provide training loss logs for MOT datasets for reference?  Thank you in advance."
æŒ‰ç…§é“¾æŽ¥   æœ€ç»ˆå¾—åˆ°çš„dataæ–‡ä»¶å¤¹ä¸‹åªæœ‰imagesï¼ˆæ–‡ä»¶å¤¹ï¼‰å’Œannotation.jsonã€‚  è¯·é—®æ˜¯éœ€è¦è‡ªå·±å†™ç¨‹åºæ¥ç”Ÿæˆå¯¹åº”çš„TXTæ–‡ä»¶å—ï¼Ÿä½ é‚£è¾¹æ–¹ä¾¿æä¾›å—    è°¢è°¢
Could you public your test results of all evaluation metric? Thank you!
None
"Hello, nice work by the way.  Can this work be trained for multi-class-multi-object tracking such as tracking multiple cars, multiple persons and multiple dogs at the same time?"
"Traceback (most recent call last):   File ""E:/jpf/Towards-Realtime-MOT/train.py"",   line 221, in   opt=opt,     File ""E:/jpf/Towards-Realtime-MOT/train.py"",   line 77, in train load_darknet_weights(model, osp.join(weights_from, 'darknet53.conv.74'))    File ""E:\jpf\Towards-Realtime-MOT\models.py"",   line 346, in load_darknet_weights bn_b = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.bias)    RuntimeError: shape '[512]' is invalid for input of size 0     help,help,help"
Can we use Efficientdet instead of Yolo to improve the results?
"Thanks for your open source code!  I have a question:  in the line336-line337 of multitracker.py  :     self.lost_stracks = sub_stracks(self.lost_stracks, self.removed_stracks)   self.removed_stracks.extend(removed_stracks)    I think it should first extend removed_stacks then sub_stracks, so just change the turn of the two lines.     self.removed_stracks.extend(removed_stracks)   self.lost_stracks = sub_stracks(self.lost_stracks, self.removed_stracks)    "
"Thanks for your so great work!    I use   to covert your pytorch model to caffemodel and eliminate the yolo layer ï¼›  !     but the inference results is wrong ,the sigmoid(objectness score) all are 0.99xxx,so I use pytorch2onnx,onnx2caffe ,get the caffemode,the results are the same wrong,I saved the pytorch weights and caffe weights to the txt file ,compiled the weight ,the weights are same except the BN layer     "
ä½ å¥½ï¼Œéžå¸¸æ„Ÿè°¢å…¬å¼€è¿™ä¹ˆå¥½çš„æ–¹æ³•ï¼Œéº»çƒ¦é—®ä¸€ä¸‹æˆ‘å¦‚æžœæœ‰å¤šä¸ªè§†é¢‘æ•°æ®ï¼ˆåªæ£€æµ‹ä¸€ç§ç±»åž‹çš„æ±½è½¦ï¼Œæ¯”å¦‚åªæ£€æµ‹å…¬äº¤è½¦ï¼‰ï¼ŒåŒä¸€ä¸ªè§†é¢‘æ ‡æ³¨ä¸åŒçš„identityï¼Œé‚£ä¸åŒçš„è§†é¢‘ï¼Œidentityæ˜¯ä»Ž0å¼€å§‹ï¼Œè¿˜æ˜¯ä»Žä¸Šä¸€ä¸ªè§†é¢‘çš„identity+1å¼€å§‹ï¼Ÿ
None
"MOT16-02 40.7% 54.9% 32.4% 49.5% 84.0%  54  10  32  12 1682  9006  245   384 38.7% 0.265 142  64  12  MOT16-04 62.7% 76.0% 53.4% 66.4% 94.5%  83  22  44  17 1831 15968  204   603 62.1% 0.240  86  74   8  MOT16-05 64.9% 76.5% 56.4% 66.0% 89.6% 125  37  67  21  522  2315  111   182 56.8% 0.243  66  58  27  MOT16-09 54.1% 64.1% 46.7% 67.6% 92.8%  25   9  14   2  277  1701   49    98 61.4% 0.205  23  25   6  MOT16-10 53.6% 64.5% 45.8% 61.2% 86.1%  54  14  32   8 1222  4779  214   514 49.5% 0.255  72  76   5  MOT16-11 70.9% 80.4% 63.3% 72.3% 91.9%  69  28  25  16  588  2539   50   135 65.4% 0.195  15  28   8  MOT16-13 47.2% 67.4% 36.3% 45.7% 84.9% 107  18  55  34  932  6216  217   556 35.7% 0.279  97  75  39  OVERALL  57.3% 70.8% 48.1% 61.5% 90.6% 517 138 269 110 7054 42524 1090  2472 54.1% 0.242 501 400 105          {      ""root"":""F:"",      ""train"":      {          ""mot17"":""./data/mot17.train"",          ""caltech"":""./data/caltech.train"",          ""citypersons"":""./data/citypersons.train"",          ""cuhksysu"":""./data/cuhksysu.train"",          ""prw"":""./data/prw.train"",          ""eth"":""./data/eth.train""      },      ""test_emb"":      {          ""cuhksysu"":""./data/cuhksysu.val"",          ""prw"":""./data/prw.val""      },      ""test"":      {          ""caltech"":""./data/caltech.val"",          ""citypersons"":""./data/citypersons.val""      }  }  "
"hi,thanks for your work!  I have a problem of 'train'. When I train the model with my own datasets, It occurs LOSS is 'NAN'.  eg: 2020-09-11 13:58:45 [INFO]:     0/29   7000/8353       nan       nan       nan       nan      3.46    0.0786  Could you help me solve the problem?  best regards!"
"python demo.py --input-video /mnt/d/object_tracking/crowd.mp4 --weights /mnt/d/object_tracking/yolov4-person_best.weights --output-format video --output-root /mnt/d/object_tracking/tracking_video  Namespace(cfg='cfg/yolov3_1088x608.cfg', conf_thres=0.5, input_video='/mnt/d/object_tracking/crowd.mp4', iou_thres=0.5, min_box_area=200, nms_thres=0.4, output_format='video', output_root='/mnt/d/object_tracking/tracking_video', track_buffer=30, weights='/mnt/d/object_tracking/yolov4-person_best.weights')    2020-09-09 14:28:49 [INFO]: Starting tracking...  Lenth of the video: 341 frames  2020-09-09 14:28:50 [INFO]: invalid load key, '\x00'.  ffmpeg version 4.2.4-1ubuntu0.1 Copyright (c) 2000-2020 the FFmpeg developers    built with gcc 9 (Ubuntu 9.3.0-10ubuntu2)    configuration: --prefix=/usr --extra-version=1ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared    libavutil      56. 31.100 / 56. 31.100    libavcodec     58. 54.100 / 58. 54.100    libavformat    58. 29.100 / 58. 29.100    libavdevice    58.  8.100 / 58.  8.100    libavfilter     7. 57.100 /  7. 57.100    libavresample   4.  0.  0 /  4.  0.  0    libswscale      5.  5.100 /  5.  5.100    libswresample   3.  5.100 /  3.  5.100    libpostproc    55.  5.100 / 55.  5.100  [image2 @ 0x7fffc5377300] Could find no file with path '/mnt/d/object_tracking/tracking_video/frame/%05d.jpg' and index in the range 0-4  /mnt/d/object_tracking/tracking_video/frame/%05d.jpg: No such file or directory    could anyone help me?  Thanks"
!   
Hi @Zhongdao     Excellent work mate. But I am only getting 7 FPS on Windows using the JDE-864x480 weights.    How to speed things up?  
"why the init of kalman filter's std  is:  std = [              2 * self._std_weight_position * measurement[3],              2 * self._std_weight_position * measurement[3],              1e-2,              2 * self._std_weight_position * measurement[3],              10 * self._std_weight_velocity * measurement[3],              10 * self._std_weight_velocity * measurement[3],              1e-5,              10 * self._std_weight_velocity * measurement[3]]    could you tell some reason or source ?"
"(Env) C:\ds-projects\Experiments\Towards-Realtime-MOT>python demo.py --input-video C:\ds-projects\Experiments\Towards-Realtime-MOT\videos\MOT16-03.mp4 --weights C:\ds-projects\Experiments\Towards-Realtime-MOT\model\all_dla34.pth --output-root C:\ds-projects\Experiments\Towards-Realtime-MOT    Namespace(cfg='cfg/yolov3_1088x608.cfg', conf_thres=0.5, input_video='C:\\ds-projects\\Experiments\\Towards-Realtime-MOT\\videos\\MOT16-03.mp4', iou_thres=0.5, min_box_area=200, nms_thres=0.4, output_format='video', output_root='C:\\ds-projects\\Experiments\\Towards-Realtime-MOT', track_buffer=30, weights='C:\\ds-projects\\Experiments\\Towards-Realtime-MOT\\model\\all_dla34.pth')    2020-08-14 18:09:42 [INFO]: Starting tracking...  Lenth of the video: 1500 frames  2020-08-14 18:09:43 [INFO]: 'model'  ffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers    built with gcc 9.2.1 (GCC) 20200122    configuration: --enable-gpl --enable-version3 --enable-sdl2 --enable-fontconfig --enable-gnutls --enable-iconv --enable-libass --enable-libdav1d --enable-libbluray --enable-libfreetype --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libopus --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libtheora --enable-libtwolame --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libzimg --enable-lzma --enable-zlib --enable-gmp --enable-libvidstab --enable-libvorbis --enable-libvo-amrwbenc --enable-libmysofa --enable-libspeex --enable-libxvid --enable-libaom --enable-libmfx --enable-amf --enable-ffnvcodec --enable-cuvid --enable-d3d11va --enable-nvenc --enable-nvdec --enable-dxva2 --enable-avisynth --enable-libopenmpt    libavutil      56. 31.100 / 56. 31.100    libavcodec     58. 54.100 / 58. 54.100    libavformat    58. 29.100 / 58. 29.100    libavdevice    58.  8.100 / 58.  8.100    libavfilter     7. 57.100 /  7. 57.100    libswscale      5.  5.100 /  5.  5.100    libswresample   3.  5.100 /  3.  5.100    libpostproc    55.  5.100 / 55.  5.100  [image2 @ 0000023776b09bc0] Could find no file with path 'C:\ds-projects\Experiments\Towards-Realtime-MOT\frame\%05d.jpg' and index in the range 0-4  C:\ds-projects\Experiments\Towards-Realtime-MOT\frame\%05d.jpg: No such file or directory    I have checked others' errors in the issues, but it seems slightly different? Mine is not no memory, it writes model?"
Can the program output the path of pedestrian and display in the output video?
I'm interested in loading Towards-Realtime-MOT using TorchScript scripting/tracing. Is this possible?
"Hello, I'm wondering how to get the ./data/mot17.train file mentioned in .../src/lib/cfg/data.json? It seems it does not exist in the MOT17 dataset? Or do I just need to produce one by myself?   Thank you~ "
"thx a lot for share your work, I want to know the what the difference the trick(fix bn) will lead to? fast converge? or higher mAP?"
æ‚¨å¥½ï¼Œè¯·é—®æ‚¨æœ‰ä»£ç çš„æ–‡æ¡£å—ï¼Ÿè·Ÿè¸ªé‚£éƒ¨åˆ†çš„ä»£ç çœ‹çš„å®žåœ¨æœ‰ç‚¹éš¾å—
"Hi, @Zhongdao     Thank you for your code.    I have successfully installed your repo together with essential requirements. It runs very well but I just want to check one specific detail with you. Hope you can give some advice.    The performance results you shared in README.md file is tested on the MOT-16 training set by using Nvidia Titan Xp GPU.    I do exactly what you did but get a slightly different result. For example, using JDE-1088x608, I get 73.1 on MOTA instead of 74.8. I'm not sure whether this small difference comes from the different GPU, may I know your opinion on this? And is it normal?    Thank you for your great help!  "
"my system is win10, python 3.6, cython was installed.   log:      (mot) C:\Users\bujingyun>pip install cython_bbox  Collecting cython_bbox    Using cached cython_bbox-0.1.3.tar.gz (41 kB)  Building wheels for collected packages: cython-bbox    Building wheel for cython-bbox (setup.py) ... error    ERROR: Command errored out with exit status 1:     command: 'd:\tool\anaconda\envs\mot\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\bujingyun\\AppData\\Local\\Temp\\pip-install-tk9l15e7\\cython-bbox\\setup.py'""'""'; __file__='""'""'C:\\Users\\bujingyun\\AppData\\Local\\Temp\\pip-install-tk9l15e7\\cython-bbox\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d 'C:\Users\bujingyun\AppData\Local\Temp\pip-wheel-7w02hctf'         cwd: C:\Users\bujingyun\AppData\Local\Temp\pip-install-tk9l15e7\cython-bbox\    Complete output (11 lines):    running bdist_wheel    running build    running build_ext    building 'cython_bbox' extension    creating build    creating build\temp.win-amd64-3.6    creating build\temp.win-amd64-3.6\Release    creating build\temp.win-amd64-3.6\Release\src    C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -Id:\tool\anaconda\envs\mot\lib\site-packages\numpy\core\include -Id:\tool\anaconda\envs\mot\include -Id:\tool\anaconda\envs\mot\include ""-IC:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\ATLMFC\include"" ""-IC:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include"" ""-ID:\Windows Kits\10\include\10.0.17763.0\ucrt"" ""-ID:\Windows Kits\10\include\10.0.17763.0\shared"" ""-ID:\Windows Kits\10\include\10.0.17763.0\um"" ""-ID:\Windows Kits\10\include\10.0.17763.0\winrt"" ""-ID:\Windows Kits\10\include\10.0.17763.0\cppwinrt"" /Tcsrc/cython_bbox.c /Fobuild\temp.win-amd64-3.6\Release\src/cython_bbox.obj -Wno-cpp    cl: å‘½ä»¤è¡Œ error D8021 :æ— æ•ˆçš„æ•°å€¼å‚æ•°â€œ/Wno-cppâ€    error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\bin\\HostX86\\x64\\cl.exe' failed with exit status 2    ----------------------------------------    ERROR: Failed building wheel for cython-bbox    Running setup.py clean for cython-bbox  Failed to build cython-bbox  Installing collected packages: cython-bbox      Running setup.py install for cython-bbox ... error      ERROR: Command errored out with exit status 1:       command: 'd:\tool\anaconda\envs\mot\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\bujingyun\\AppData\\Local\\Temp\\pip-install-tk9l15e7\\cython-bbox\\setup.py'""'""'; __file__='""'""'C:\\Users\\bujingyun\\AppData\\Local\\Temp\\pip-install-tk9l15e7\\cython-bbox\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\bujingyun\AppData\Local\Temp\pip-record-8esbxua6\install-record.txt' --single-version-externally-managed --compile --install-headers 'd:\tool\anaconda\envs\mot\Include\cython-bbox'           cwd: C:\Users\bujingyun\AppData\Local\Temp\pip-install-tk9l15e7\cython-bbox\      Complete output (11 lines):      running install      running build      running build_ext      building 'cython_bbox' extension      creating build      creating build\temp.win-amd64-3.6      creating build\temp.win-amd64-3.6\Release      creating build\temp.win-amd64-3.6\Release\src      C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -Id:\tool\anaconda\envs\mot\lib\site-packages\numpy\core\include -Id:\tool\anaconda\envs\mot\include -Id:\tool\anaconda\envs\mot\include ""-IC:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\ATLMFC\include"" ""-IC:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include"" ""-ID:\Windows Kits\10\include\10.0.17763.0\ucrt"" ""-ID:\Windows Kits\10\include\10.0.17763.0\shared"" ""-ID:\Windows Kits\10\include\10.0.17763.0\um"" ""-ID:\Windows Kits\10\include\10.0.17763.0\winrt"" ""-ID:\Windows Kits\10\include\10.0.17763.0\cppwinrt"" /Tcsrc/cython_bbox.c /Fobuild\temp.win-amd64-3.6\Release\src/cython_bbox.obj -Wno-cpp      cl: å‘½ä»¤è¡Œ error D8021 :æ— æ•ˆçš„æ•°å€¼å‚æ•°â€œ/Wno-cppâ€      error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\bin\\HostX86\\x64\\cl.exe' failed with exit status 2      ----------------------------------------  ERROR: Command errored out with exit status 1: 'd:\tool\anaconda\envs\mot\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\bujingyun\\AppData\\Local\\Temp\\pip-install-tk9l15e7\\cython-bbox\\setup.py'""'""'; __file__='""'""'C:\\Users\\bujingyun\\AppData\\Local\\Temp\\pip-install-tk9l15e7\\cython-bbox\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\bujingyun\AppData\Local\Temp\pip-record-8esbxua6\install-record.txt' --single-version-externally-managed --compile --install-headers 'd:\tool\anaconda\envs\mot\Include\cython-bbox' Check the logs for full command output.    "
None
"Thanks for sharing your codes.  In your code, for ReID part (multitracker.py ''' Step 2: First association, with embedding'''), the last bbox of the lost_tracklet is used to carry out the matching process with the detections in the current frame. If the mentioned bbox has significant overlap with the bboxes of the adjacent IDs  when tracklet is lost, the result of ReID is poor.  I suggest to save the bbox for each tracklet in which the overlap with the other bboxes is minimum. Then do ReID for the saved selected bbox. I Think it significantly improve the ReID performance.    "
None
"Command :   `python demo.py --input-video /path/to/input/video.mp4 --weights /path/to/weights/jde_576x320_uncertainty.pt --output-format video --cfg cfg/yolov3_576x320.cfg`    Output/Error:  Namespace(cfg='cfg/yolov3_576x320.cfg', conf_thres=0.5, input_video='/path/to/input/video.mp4', iou_thres=0.5, min_box_area=200, nms_thres=0.4, output_format='video', output_root='results', track_buffer=30, weights='/path/to/weights/jde_576x320_uncertainty.pt')    2020-05-29 17:42:12 [INFO]: Starting tracking...  Lenth of the video: 743 frames  2020-05-29 17:42:13 [INFO]:   The NVIDIA driver on your system is too old (found version 9010).  Please update your GPU driver by downloading and installing a new  version from the URL:    Alternatively, go to:   to install  a PyTorch version that has been compiled with your version  of the CUDA driver.  ffmpeg version 4.0 Copyright (c) 2000-2018 the FFmpeg developers    built with gcc 7.2.0 (crosstool-NG fa8859cb)    configuration: --prefix=/anaconda/envs/TowardsRealtimeMOT --cc=/opt/conda/conda-bld/ffmpeg_1531088893642/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --disable-doc --enable-shared --enable-static --enable-zlib --enable-pic --enable-gpl --enable-version3 --disable-nonfree --enable-hardcoded-tables --enable-avresample --enable-libfreetype --disable-openssl --disable-gnutls --enable-libvpx --enable-pthreads --enable-libopus --enable-postproc --disable-libx264    libavutil      56. 14.100 / 56. 14.100    libavcodec     58. 18.100 / 58. 18.100    libavformat    58. 12.100 / 58. 12.100    libavdevice    58.  3.100 / 58.  3.100    libavfilter     7. 16.100 /  7. 16.100    libavresample   4.  0.  0 /  4.  0.  0    libswscale      5.  1.100 /  5.  1.100    libswresample   3.  1.100 /  3.  1.100    libpostproc    55.  1.100 / 55.  1.100  [image2 @ 0x5635f21de440] Could find no file with path 'results/frame/%05d.jpg' and index in the range 0-4  results/frame/%05d.jpg: No such file or directory    Request you to please let me know if any other information is needed."
"Hi,  I wonder that would you be willing to share your training log file? Thank you!"
"helloï¼Œmy training loss becomes negative suddenly,can you tell me how can I fix it ,thank you very much!  !   "
None
"åœ¨rtx2080tiä¸Š,æµ‹è¯•JDE-576x320æ¨¡åž‹,ä½¿ç”¨çš„æ˜¯æ‘„åƒå¤´è¯»å–çš„å›¾åƒ,ä½†æ˜¯é€Ÿåº¦åªæœ‰8fps.  è¿˜æœ‰ä»€ä¹ˆå¯ä»¥åŠ é€Ÿçš„å—?    `def track(opt):        cfg_dict = parse_model_cfg(opt.cfg)      opt.img_size = [int(cfg_dict[0]['width']), int(cfg_dict[0]['height'])]        # run tracking      timer = Timer()      accs = []      n_frame = 0      tracker = JDETracker(opt, frame_rate=30)      cap= cv2.VideoCapture(3)      cap.set(3,1920)      cap.set(4,1080)      # ç›®æ ‡å°ºåº¦å¤„ç†      vw,vh = [int(cfg_dict[0]['width']), int(cfg_dict[0]['height'])]      wa, ha = float(cap.get(3)) / vw, float(cap.get(4)) / vh      a=min(wa,ha)      w,h=int(vw * a), int(vh * a)      cv2.namedWindow('show',0)      while 1:          timer.tic()          ret,img0 = cap.read()          if ret:              img = cv2.resize(img0, (w, h))              img, _, _, _ = letterbox(img, height=vh, width=vw)              # Normalize RGB              img = img[:, :, ::-1].transpose(2, 0, 1)              img = np.ascontiguousarray(img, dtype=np.float32)              img /= 255.0              blob = torch.from_numpy(img).cuda().unsqueeze(0)              online_targets = tracker.update(blob, img0)              if len(online_targets)>0:                  print('ok')              print(len(online_targets))              online_tlwhs = []              online_ids = []              for t in online_targets:                  tlwh = t.tlwh                  tid = t.track_id                  vertical = tlwh[2] / tlwh[3] > 1.6                  if tlwh[2] * tlwh[3] > opt.min_box_area and not vertical:                      online_tlwhs.append(tlwh)                      online_ids.append(tid)              timer.toc()              online_im = vis.plot_tracking(img0, online_tlwhs, online_ids, frame_id=1,                                                fps=1. / timer.average_time)              cv2.imshow('show', online_im)              cv2.waitKey(1)`"
helloï¼ŒI want to draw everyone's trajectory instead of the bounding box. What should I doï¼ŸThx
Hello! I wonder what's the meaning of emb_scale and how to set its value? I dont understand this code:    self.emb_scale = math.sqrt(2) * math.log(self.nID-1) if self.nID>1 else 1
"I use the jde code to generate the MOT results. And an error occurred when submitting to MOT Challenge. The Error was:  ""Writing to db failed!""  I don't know what the real problem is. Can anyone help me? Thank you very much!    "
"   hello there, an error happens here, did someone know how to fix it?Just delete the unexpected keyword(hier image_size and nID) in the test() function?   Thanks in advance"
"Hi,  I wonder what is the meaning of the emb_scale ??  Thank you very much"
"Hi,   Thanks for great work!   I visualized MOT17 and it looks like completely occluded (e.g. identities 2 and 3 in the picture) and really far-away persons are included as labels. Do you think dropping some occluded and far-away detections would help in general case?  Also, classes like persons on vehicles or persons sitting are excluded. Is there a reason for that?    <img width=""882"" alt=""Screen Shot 2020-04-26 at 11 11 14 PM"" src=""   "
"when i run track.py,first frame has no result? it begins in second frame .what should i do? In first frame ,i hope it has result.     2,1,1285.7813158920205,282.4268737792969,76.7357324737712,216.04457397460936,0.9999936819076538,0  2,2,344.44428190536524,59.48232650756836,50.008097566222645,138.97880859375,0.9999473094940186,0  2,3,347.46524395352105,223.17556152343752,90.68783484686418,232.00208129882813,0.9998770952224731,0  2,4,472.2209626931699,230.5023193359375,74.43901943787897,195.14400634765624,0.9999018907546997,0  2,5,531.9139656713292,206.4421447753906,75.68522783702925,203.72283935546875,0.9999454021453857,0  2,6,1085.463814813034,290.28294067382814,97.32302955361897,238.89238891601562,0.9998795986175537,0  2,7,809.184698873678,128.71075897216798,61.70667647139386,172.25555419921875,0.9997780919075012,0"
I got the following error   `Found no NVIDIA driver on your system. Please check that you  have an NVIDIA GPU and installed a driver from       when executed at  ` with torch.no_grad():  pred = self.model(im_blob)` at multitrackers.py-JDETracker-update()    but i deleted the '.cuda()' and edited the init() as   
"I want check confidence of pedestrian in result text,  what i should do?"
"Hello,  I ve managed to run the demo, but it seems that it can not track cars. Actually it seems that the demo can only track persons.  Is that intented? Any suggestions as to what I can do to track vehicles and other objects?"
"In the `Dataset Zoo` I see     > The field [class] should be 0. Only single-class multi-object tracking is supported in this version.    means only support single-class, **Can I tracking person and car in the meantime ï¼Ÿ**"
"when I run demo.py,it shows  ModuleNotFoundError: No module named 'cython_bbox'  I have tried pip install cython_bbox,but :    ERROR: Command errored out with exit status 1:     command: 'C:\ProgramData\Anaconda3\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\AppData\\Local\\Temp\\pip-install-prez7ymw\\cython-bbox\\setup.py'""'""'; __file__='""'""'C:\\Users\\AppData\\Local\\Temp\\pip-install-prez7ymw\\cython-bbox\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d 'C:\Users\AppData\Local\Temp\pip-wheel-uyp17uhy' --python-tag cp37         cwd: C:\Users\AppData\Local\Temp\pip-install-prez7ymw\cython-bbox\    Complete output (11 lines):    running bdist_wheel    running build    running build_ext    building 'cython_bbox' extension    creating build    creating build\temp.win-amd64-3.7    creating build\temp.win-amd64-3.7\Release    creating build\temp.win-amd64-3.7\Release\src    C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.22.27905\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MT -IC:\ProgramData\Anaconda3\lib\site-packages\numpy\core\include -IC:\ProgramData\Anaconda3\include -IC:\ProgramData\Anaconda3\include ""-IC:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.22.27905\ATLMFC\include"" ""-IC:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.22.27905\include"" ""-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.7.2\include\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\cppwinrt"" /Tcsrc/cython_bbox.c /Fobuild\temp.win-amd64-3.7\Release\src/cython_bbox.obj -Wno-cpp    cl: å‘½ä»¤è¡Œ error D8021 :æ— æ•ˆçš„æ•°å€¼å‚æ•°â€œ/Wno-cppâ€    error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.22.27905\\bin\\HostX86\\x64\\cl.exe' failed with exit status 2    ----------------------------------------    ERROR: Failed building wheel for cython-bbox    Running setup.py clean for cython-bbox  Failed to build cython-bbox  Installing collected packages: cython-bbox    Running setup.py install for cython-bbox ... error      ERROR: Command errored out with exit status 1:       command: 'C:\ProgramData\Anaconda3\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\AppData\\Local\\Temp\\pip-install-prez7ymw\\cython-bbox\\setup.py'""'""'; __file__='""'""'C:\\Users\\AppData\\Local\\Temp\\pip-install-prez7ymw\\cython-bbox\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\AppData\Local\Temp\pip-record-peuju9_m\install-record.txt' --single-version-externally-managed --compile           cwd: C:\Users\AppData\Local\Temp\pip-install-prez7ymw\cython-bbox\      Complete output (11 lines):      running install      running build      running build_ext      building 'cython_bbox' extension      creating build      creating build\temp.win-amd64-3.7      creating build\temp.win-amd64-3.7\Release      creating build\temp.win-amd64-3.7\Release\src      C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.22.27905\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MT -IC:\ProgramData\Anaconda3\lib\site-packages\numpy\core\include -IC:\ProgramData\Anaconda3\include -IC:\ProgramData\Anaconda3\include ""-IC:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.22.27905\ATLMFC\include"" ""-IC:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.22.27905\include"" ""-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.7.2\include\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\cppwinrt"" /Tcsrc/cython_bbox.c /Fobuild\temp.win-amd64-3.7\Release\src/cython_bbox.obj -Wno-cpp      cl: å‘½ä»¤è¡Œ error D8021 :æ— æ•ˆçš„æ•°å€¼å‚æ•°â€œ/Wno-cppâ€      error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.22.27905\\bin\\HostX86\\x64\\cl.exe' failed with exit status 2      ----------------------------------------  ERROR: Command errored out with exit status 1: 'C:\ProgramData\Anaconda3\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\AppData\\Local\\Temp\\pip-install-prez7ymw\\cython-bbox\\setup.py'""'""'; __file__='""'""'C:\\Users\\AppData\\Local\\Temp\\pip-install-prez7ymw\\cython-bbox\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\AppData\Local\Temp\pip-record-peuju9_m\install-record.txt' --single-version-externally-managed --compile Check the logs for full command output.    So,what can I do?Please help me.  btw:my system is WIN10  Thaks."
"I download the jde.1088x608.uncertainty.pt and use it to evaluate on the MOT16 train set. While the MOTA looks good, the MOTP is strangely low. Did anyone encounter the same issue?    !   "
"@Zhongdao  result of txt file as the following picture shows,  !   !     mAP value is always 0, all of test results are also always 0 or nan as the following picture shows  !     Sincerely hope to receive your reply. how to write first frame data into according txt file, meanwhile also write mAP value into according txt file. thank you very much.  "
"@Zhongdao hello, according to your requirementï¼Œ, I have deleted the ""Cuda()"" function in related files. However, it also occurs the problem.  could you explain to me and how to solve the problem, thank you, hope to receive your answer.  !   "
"Hi @Zhongdao ,    I tried to train the model on Google Colab. However, I always come across the error below. There is a folder that is not being created. I solved it by adding a folder creation routine.    Thanks.    > ================================================================================  dataset summary  OrderedDict([('caltech', 848.0)])  total # identities: 849  start index  OrderedDict([('caltech', 0)])  ================================================================================  2020-04-06 07:23:31 [INFO]:    Epoch       Batch       box      conf        id     total  nTargets      time  2020-04-06 07:23:37 [INFO]:     0/29       0/300      0.22      2.05      13.5       129         2      6.89  2020-04-06 07:23:50 [INFO]:     0/29      40/300     0.197      2.04      10.1       111       2.8     0.157  2020-04-06 07:24:03 [INFO]:     0/29      80/300     0.158      1.83      10.8       102      2.81     0.154  2020-04-06 07:24:16 [INFO]:     0/29     120/300     0.129      1.39      10.6      78.6      2.83     0.159  2020-04-06 07:24:29 [INFO]:     0/29     160/300     0.108      1.08      9.58      58.6      2.84     0.152  2020-04-06 07:24:41 [INFO]:     0/29     200/300    0.0936     0.871      8.87      45.4      2.86     0.154  2020-04-06 07:24:54 [INFO]:     0/29     240/300    0.0871     0.732      8.29      36.3      2.86     0.147  2020-04-06 07:25:07 [INFO]:     0/29     280/300    0.0859      0.63      7.84      29.5      2.85     0.146  Traceback (most recent call last):    File ""train.py"", line 218, in        opt=opt,    File ""train.py"", line 160, in train      copyfile(cfg, weights_to + '/cfg/yolo3.cfg')    File ""/usr/lib/python3.6/shutil.py"", line 121, in copyfile      with open(dst, 'wb') as fdst:  FileNotFoundError: [Errno 2] No such file or directory: 'weights/run06_04_07_23/cfg/yolo3.cfg'    In addition, it seems like the test function's signature changed as well:    > Traceback (most recent call last):    File ""train.py"", line 223, in        opt=opt,    File ""train.py"", line 181, in train      print_interval=40, nID=dataset.nID)  TypeError: test_emb() got an unexpected keyword argument 'nID'"
"When I train the model with datasets. No matter how small the batch_size and img-resolution I set ,error always raise like:    >RuntimeError: CUDA out of memory. Tried to allocate 1.73 GiB (GPU 0; 4.00 GiB total capacity; 1.04 GiB already allocated; 1.97 GiB free; 6.95 MiB cached)    and   may help. It says I should use `with torch.no_grad:` or `inputs = Variable(inputs, volatile=True)` in validation_step. Could u tell where exactly I should place these codes plz? THX A LOT!"
None
"I'm trying to use this code on tracking football players, and the problem is when two or more players cross from near each other, their bounding box may transfer or take a new id. is there any way to improve performance?  "
"Hi,    I'm a big fan of your work ""Towards-Realtime-MOT"" / ""JDE"" and I would like to know if you are planning on doing the MOT20 challenge and if you are working on reducing the IDS ?    Best regards,    Samy H."
None
also please provide processed image files for caaltech dataset  
"As i use the MOT17 dataset, but the original data from official website have a different instruction with yours  described in data zoo, so, how can i convert it?"
"Hello, thanks for the great work. I wanted to train it from scratch. And i have downloaded caltech dataset and i wanted to convert it to the format according to this link   . But i am confused with default dataset of caltech dataset. is there any way to convert it?"
"I convert to onnx model from .pt model you provided,but when inference on opencv, has an error:  > self.model = cv2.dnn.readNetFromONNX(onnx_path)  > cv2.error: OpenCV(4.2.0) /io/opencv/modules/dnn/include/opencv2/dnn/dnn.inl.hpp:349: error: (-204:Requested object was not found) Required argument ""starts"" not found into dictionary in function 'get'"
"I wonder what is the intuition behind replacement of cosine similarity with euclidean distance, does it more accurate ? or faster ? "
the demo only track the personï¼ŒI want to track the car ï¼Œhow can I do itï¼Ÿ
"Hi,    I needed to convert your .pt model to onnx. Upon conversion, my onnx output is different for the bbox coordinates as compared to the output from .pt model. All other values (embeddings and confidence) match exactly between the two models. Any reason why this happens? Have you applied any specific logic to convert model values to bbox coordinates?    **Actual output (.onnx):**  pred11 = [[[ 0.0429871 0.10708108 -0.15831867 ... -0.05099011 0.04215946 0.03529864]    [ 0.15205492 0.14381851 -0.4134867 ... -0.02547888 0.02003362 0.05678646]  [ 0.11788099 0.21649244 -0.49855745 ... -0.03875189 -0.00112192 0.05244121]  ...  [-0.02988242 -0.02718411 0.17555043 ... -0.05482839 -0.03526874 -0.01705099]  [-0.02175785 -0.01682113 0.20047602 ... -0.04242673 0.00226362 0.00717632]  [-0.01177015 -0.02307639 0.05427436 ... -0.03097095 0.00717672 0.00656337]]] 218 float32    **Expected output (.pt):**  pred22 = [[[ 4.3859973e+00 3.2662495e+01 8.7064598e+01 ... -5.0997593e-02 4.2160533e-02 3.5292290e-02]  [ 4.7508678e+01 4.3868427e+01 6.7454163e+01 ... -2.5484795e-02 2.0031789e-02 5.6778252e-02]  [ 7.6021790e+01 6.6031555e+01 6.1953438e+01 ... -3.8754154e-02 -1.1225204e-03 5.2439131e-02]  ...  [ 3.9146216e+02 4.0653235e+02 2.1454182e+01 ... -5.4829646e-02 -3.5270575e-02 -1.7050508e-02]  [ 3.9960834e+02 4.0709204e+02 2.1995722e+01 ... -4.2428117e-02 2.2626489e-03 7.1746223e-03]  [ 4.0778818e+02 4.0675439e+02 1.9004044e+01 ... -3.0972414e-02 7.1776402e-03 6.5612365e-03]]] 218 float32"
"Hello,    Thanks for the great work and make it open-source!  In the paper, it claims that the task-independent uncertainty is modeled as learnable parameter. However, they are constants in the repo.  I notice a similar question #54, but it is still confusing to me. Could you please classify this part? Thanks!"
Can we extend this model to track objects from multiple cameras. Like a person moving in the floor need to be tracked across multiple cameras.
After running track.py on MOT16 evaluation set with pretrained weights we get 0 and Nan on MOT metrics.            IDF1  IDP  IDR Rcll Prcn GT MT PT ML     FP FN IDs  FM  MOTA MOTP  MOT16-01 0.0% 0.0% nan% nan% 0.0%  0  0  0  0   4657  0   0   0 -inf%  nan  MOT16-03 0.0% 0.0% nan% nan% 0.0%  0  0  0  0  89459  0   0   0 -inf%  nan  MOT16-06 0.0% 0.0% nan% nan% 0.0%  0  0  0  0   9091  0   0   0 -inf%  nan  MOT16-07 0.0% 0.0% nan% nan% 0.0%  0  0  0  0  13491  0   0   0 -inf%  nan  MOT16-08 0.0% 0.0% nan% nan% 0.0%  0  0  0  0   9453  0   0   0 -inf%  nan  MOT16-12 0.0% 0.0% nan% nan% 0.0%  0  0  0  0   6697  0   0   0 -inf%  nan  MOT16-14 0.0% 0.0% nan% nan% 0.0%  0  0  0  0  11503  0   0   0 -inf%  nan  OVERALL  0.0% 0.0% nan% nan% 0.0%  0  0  0  0 144351  0   0   0 -inf%  nan  The video outputs appear to look normal.   
None
"When testing demo.py, the input size can only be 1088 * 608, and the model cannot predict after modification."
@Zhongdao Thank you for your sharing .Could you tell me your CSDN or blog? And Could you tell me some researchers in MOT tracking? I am naive in MOT tracking. I want to find some researchers to follow.Thank you very much!!
"Traceback (most recent call last):    File ""train.py"", line 194, in        opt=opt,    File ""train.py"", line 126, in train      loss, components = model(imgs.cuda(), targets.cuda(), targets_len.cuda())    File ""/home/arena/anaconda3/envs/mask_JDE/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 547, in __call__      result = self.forward(*input, **kwargs)    File ""/home/arena/anaconda3/envs/mask_JDE/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 152, in forward      outputs = self.parallel_apply(replicas, inputs, kwargs)    File ""/home/arena/anaconda3/envs/mask_JDE/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 162, in parallel_apply      return parallel_apply(replicas, inputs, kwargs, self.device_ids     File ""/home/arena/anaconda3/envs/mask_JDE/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 547, in __call__      result = self.forward(*input, **kwargs)    File ""/media/arena/D/huangw/Project/Towards-Realtime-MOT-master (1)/models.py"", line 132, in forward      create_grids(self, img_size, nGh, nGw)    File ""/media/arena/D/huangw/Project/Towards-Realtime-MOT-master (1)/models.py"", line 268, in create_grids      assert self.stride == img_size[1] / nGh  AssertionError  "
the download is really slow for baidu disk
"Hi, when I try to train the network with the provided data, the mAP is always zero! Is it normal?  How many iterations does it take to reach the higher mAP in the training phase?  "
"Traceback (most recent call last):    File ""demo.py"", line 6, in        import motmetrics as mm    File ""/home/nisarg/.local/lib/python2.7/site-packages/motmetrics/__init__.py"", line 2, in        from .mot import MOTAccumulator    File ""/home/nisarg/.local/lib/python2.7/site-packages/motmetrics/mot.py"", line 12, in        from motmetrics.lap import linear_sum_assignment    File ""/home/nisarg/.local/lib/python2.7/site-packages/motmetrics/lap.py"", line 171, in        init_standard_solvers()    File ""/home/nisarg/.local/lib/python2.7/site-packages/motmetrics/lap.py"", line 149, in init_standard_solvers      from importlib import util  ImportError: cannot import name util    I am unable to import util even though the importlib is installed in the PC  Please help!!  "
"@Zhongdao Hi, Thank you for your sharing .I have been confused about too little information on multi-object tracking. Could you introduce your blog and some researchers or blog about multi-object tracking? Thank you very much!"
"Hi all,  I spent two week to run demo.py. And I have a little bit experience . If you need to help, please feel free to contact me via skype: quangthanh1987.  Thanks and Best Regards,"
"When i run the demo.py,  I am facing issue:  2019-10-30 14:06:51 [INFO]: start tracking...  Lenth of the video: 43190 frames  2019-10-30 14:06:58 [INFO]: Processing frame 0 (100000.00 fps)  Segmentation fault (core dumped)    Anybody help me"
"I run the follow command, but server errors happen.     python demo.py --input-video ./results/test.mp4  --weights ./jde.1088x608.uncertainty.pt  --output-format video --output-root ./results/    Unable to init server: Could not connect: Connection refused  Unable to init server: Could not connect: Connection refused    (demo.py:14943): Gdk-CRITICAL **: 01:48:57.208: gdk_cursor_new_for_display: assertion 'GDK_IS_DISPLAY (display)' failed  Namespace(cfg='./cfg/yolov3.cfg', conf_thres=0.5, img_size=(1088, 608), input_video='./results/test.mp4', iou_thres=0.5, min_box_area=200, nms_thres=0.4, output_format='video', output_root='./results/', track_buffer=30, weights='./jde.1088x608.uncertainty.pt')    2019-10-28 01:48:57 [INFO]: start tracking...  Lenth of the video: 1500 frames  2019-10-28 01:48:57 [INFO]: 'module' object is not callable  ffmpeg version 3.4.6-0ubuntu0.18.04.1 Copyright (c) 2000-2019 the FFmpeg developers    built with gcc 7 (Ubuntu 7.3.0-16ubuntu3)    configuration: --prefix=/usr --extra-version=0ubuntu0.18.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared    libavutil      55. 78.100 / 55. 78.100    libavcodec     57.107.100 / 57.107.100    libavformat    57. 83.100 / 57. 83.100    libavdevice    57. 10.100 / 57. 10.100    libavfilter     6.107.100 /  6.107.100    libavresample   3.  7.  0 /  3.  7.  0    libswscale      4.  8.100 /  4.  8.100    libswresample   2.  9.100 /  2.  9.100    libpostproc    54.  7.100 / 54.  7.100  [image2 @ 0x55815dee38c0] Could find no file with path './results/frame/%05d.jpg' and index in the range 0-4  ./results/frame/%05d.jpg: No such file or directory"
"i can run the demo.py sucessfully , but the result.mp4 is the same as the input video ,  there is no detection and tracking. why is this phenomenon happensã€‚ "
"When I run demo.py. I have an issue like this:  Traceback (most recent call last):    File ""demo.py"", line 8, in        from tracker.multitracker import JDETracker    File ""/home/thanhpham/PycharmProjects/HumanDetection01/venv/MOT/tracker/multitracker.py"", line 13, in        from models import *    File ""/home/thanhpham/PycharmProjects/HumanDetection01/venv/MOT/models.py"", line 8, in        from utilss.syncbn import SyncBN  ImportError: cannot import name 'SyncBN'    ******************************************  From the author of paper. I changed the name file utils to utilss my code:  The code of demo.py file    from tracker.multitracker import JDETracker  from utilss import visualization as vis  from utilss.utilss import *  from utilss.io import read_results  from utilss.log import logger  from utilss.timer import Timer  from utilss.evaluation import Evaluator  import utilss.datasets as datasets  import torch  from track import eval_seq  **************************  When run command line:  bash compile.sh to install syncbn  I face issue like this:    Traceback (most recent call last):    File ""setup.py"", line 2, in        from torch.utilss.cpp_extension import CUDAExtension, BuildExtension  ModuleNotFoundError: No module named 'torch'  ~/PycharmProjects/HumanDetection01/venv/MOT/utilss/syncbn  Although, I installed torch already.  *****************************    Please give me some advices          "
"Since the project requires maskrcnn-benchmark, it only works on GPU, will there be a CPU version?"
"!   My weight file uses darknet53.conv.74; The cfg file uses yolov3 _ 1088x608.cfg and renamed it yolov3.cfg because I did not find the file named yolov3.cfg. Is there any problem with this?    There are other problems. The mot17 data set is decompressed incorrectly, and the mot16 data set has no label_ with_ IDS, so I used the PRW dataset but got Nan.Can you help me?Thanks."
"!     i use this command:   python track.py --cfg ./cfg/yolov3_1088x608.cfg --weights weights/latest.pt --test-mot16 --save-images      and i check the detected image, is right.  !   "
"I got error when using yolov3-tiny.conv.15 (  and yolov3-tiny.cfg (  likes below:         Actually, I modified yolov3-tiny.cfg with adding `embedding_dim=512` in iine 10 and change anchors to `anchors = 14,10,  27,23,  58,37,  82,81,  169,135,  319,344` in line 135 and 177.    Are there any suggestions? Thanks.  "
"   file_name: matching.py  method: fuse_motion    Query    1. track - Previously detected tracker object  2. Measurements - it gives bounding boxes of all the objects in current frames.  3. gating distance - quantifies the distance between on such previously identified box vs all the boxed in current frame.  ex:  [ 0.00027311 58.948 207.32]  This operation is purely done by kalman filter.  4. cost_matrix - It is the confusion_matrix created by the 128 dim embedding generated by the network. Dot product between embedding of tracks vs current_boundingbox_detections  ex:  [[0.01109,0.41579,1.10747]  [0.42742,0.03098,0.84188]]  tracked objects from previous frames (2 objects) vs current frames object (3 objects)    5. Here in line 3 instead of finding the min of cosine distance from confusion_matrix they have indexed it using the gating_distance(provided by kalman filter) thus rendering the distance metric useless.    6. Here apart from the places indexed by gating_distance everything will be infinity  ex:  cost_matrix = [[0.01109,inf,inf]                          [inf,0.03098,inf]]  Since values are set to inf the linear_assignment task will always provides the obvious outputs.    Am is missing something ? It would be great if you could shed some clarity on this. Thanks"
None
"Hi  I am trying to train on my custom dataset. The shape of the images is (864x480).   When I run the train.py file (with config file specified as cfg/yolov3_864x480.cfg), I get the following assertion error:     "
I am working with RetinaNet to detect lung cancer in CT images. I would like try your code to compare the results. Is it possible or it works with video input?    Regards.
"I train on mot16 and get a weights, but when I test it, I meet this:   RuntimeError: Error(s) in loading state_dict for Darknet:   size mismatch for classifier.weight: copying a param with shape torch.Size([519, 512]) from checkpoint, the shape in current model is torch.Size([14455, 512]).   size mismatch for classifier.bias: copying a param with shape torch.Size([519]) from checkpoint, the shape in current model is torch.Size([14455])."
"I ran train.py but no response...    There's no indication after the state below    !     At this time, the status of GPU confirmed by nvidia-smi is as follws.  !     Of the 11GB GPU, only 1.5GB is used. Is this normal??"
"Hey, thanks for your great work!  But I have a question about  the **Automatic Loss Balancing Function**.    According to  , **continuous output** and **softmax output** have different coefficients: continuous one contains 1/2 while softmax does not. But I notice both in your paper and code give coefficient 1/2 to all three losses.    Would you mind giving some further explanations?     "
"Has anyone tested `AP` and `TPR` for the pre-trained model  `jde.uncertainty.pt`? The results of my test are `AP=80.54` and `TPR@FAR =84`, which are  **quite different** from those of `AP=83.0` and `TPR=90.4` claimed in the paper. Except the `batch_size=8` All other hyperparameters were set as default.  The GPU used for this result is TITAN V.  Part of the output for Detection test:  !     Part of the output for Embeding  test:  !   "
"First of all, congratulations on the work. I have been making modifications to the code and I have some concerns about the datasets used in the training.     My doubt is if the provided models (**JDE-1088x608, JDE-864x480, JDE-576x320**) are trained with all the datasets of the DATASET_ZOO.md file except the MOT16 for validation.     If so, that would be a problem since the **MOT17** and **MOT16** videos are the same ones, so the training and validation would overlap.     I assume that the MOT17 is not part of the training, but I would like someone to confirm this if you would be so kind.        Thank you for your response. Keep up the good work.  "
"Hello, @Zhongdao   first of all, huge thanks for sharing your work here with us.  I was trying to run **track..py** as you mentioned in the readme file, and I downloaded the dataset you provided MOT16 but I have this weird error, I'm not sure what is the problem!  I try to run the following:  **python track.py --weights weights/jde.1088x608.uncertainty.pt --cfg cfg/yolov3_1088x608.cfg**    and I get this error msg:  2020-06-27 00:21:22      thanks in advance :)  "
"Thanks for your great work.    I created a custom image and label to try to track the chicken.    I configured the folder and ccmcpe.json file as shown in the following pictures.  !   !     However, it returns 'no .train file' as shown in the following pircure. I think the train file is output after training, how can I solve this problem?? What parts should I fix??  !   "
"Thank you for your great research. I have two questions about creating a custom datasets.  1. Do you have tools for annotation and labeling in new videos(images) that can be applied to your code? My annotation tool cannot enter [identity].  2. Does [identity] mean the same person in a video? If 40 people appear in one video, is it correct that 0 to 39 are used for [identity]?    I'm eager to train with my own data."
"Hello, Thank you for great work and for making it an open-source.     I have a problem when I use the code for training in my Custom Dataset.     I generate the data by following procedure as mentioned on the model zoo, and arranging. However, when I train, I got FileNotFoundError as shown below;    I run the code on window 10 with NVIDIA GPU 1080    _set CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 & python train.py_    Traceback (most recent call last):    File ""train.py"", line 218, in        opt=opt,    File ""train.py"", line 50, in train      dataset = JointDataset(dataset_root, trainset_paths, img_size, augment=True, transforms=transforms)    File ""C:\Users\Nurya\Desktop\TRACKING\JDE\utils\datasets.py"", line 365, in __init__      lb = np.loadtxt(lp)    File ""C:\Users\Nurya\Anaconda3\envs\cartracking-pytorch\lib\site-packages\numpy\lib\npyio.py"", line 955, in loadtxt      fh = np.lib._datasource.open(fname, 'rt', encoding=encoding)    File ""C:\Users\Nurya\Anaconda3\envs\cartracking-pytorch\lib\site-packages\numpy\lib\_datasource.py"", line 266, in open      return ds.open(path, mode, encoding=encoding, newline=newline)    File ""C:\Users\Nurya\Anaconda3\envs\cartracking-pytorch\lib\site-packages\numpy\lib\_datasource.py"", line 622, in open      encoding=encoding, newline=newline)  FileNotFoundError:      This is how my data file organized    !     And this is where you can find car.train and car. val file as you can see below in attached file     !     Below also is my ccmcpe.json file as a config file    !       Any help on how to clear the problem.  Thank you  "
"self.tracked_stracks = [t for t in self.tracked_stracks if t.state == TrackState.Tracked]  self.tracked_stracks = joint_stracks(self.tracked_stracks, activated_starcks)  self.tracked_stracks = joint_stracks(self.tracked_stracks, refind_stracks)  è€Œ  def joint_stracks(tlista, tlistb):      exists = {}      res = []      for t in tlista:          exists[t.track_id] = 1          res.append(t)      for t in tlistb:          tid = t.track_id          if not exists.get(tid, 0):              exists[tid] = 1              res.append(t)      return res  æ±‚å–tlista, tlistbçš„è·Ÿè¸ªå™¨å¹¶é›†ï¼Œå…ˆä¿å­˜tlistaä¸­å…¨éƒ¨è·Ÿè¸ªå™¨ï¼Œå†ä¿å­˜tlistbä¸­ä¸å«tlistaçš„track_idçš„è·Ÿè¸ªå™¨ï¼Œé‚£ä¹ˆactivated_starcksä¸­å·²ç»update è·Ÿè¸ªå™¨ä¸å°±æ”¾å¼ƒæŽ‰äº†å—ï¼Ÿ  "
"when testing on MOT20 train dataset,there is an error:  Traceback (most recent call last):    File ""track20.py"", line 173, in        save_videos=opt.save_videos)    File ""track20.py"", line 98, in main      dataloader = datasets.LoadImages(osp.join(data_root, seq, 'img1'), opt.img_size)  #todo MOT20æ—¶æ²¡æœ‰fileå±žæ€§    File ""G:\programming practice\Towards-Realtime-MOT\utils\datasets.py"", line 25, in __init__      self.nF = len(self.files)  # number of image files  AttributeError: 'LoadImages' object has no attribute 'files'  how to save this?"
None
"Thanks for your work ,could you please tell me how to generate the file in the data, such as .train and .val "
"I try  to decompress cityperson dataset:  `cat Citypersons.z01 Citypersons.z02 Citypersons.z03 Citypersons.zip > c.zip  unzip c.zip`    But I got the following error:  `Archive: c.zip  error: End-of-centdir-64 signature not where expected (prepended bytes?)  (attempting to process anyway)  warning [c.zip]: zipfile claims to be last disk of a multi-part archive;  attempting to process anyway, assuming all parts have been concatenated  together in order. Expect ""errors"" and warnings...true multi-part support  doesn't exist yet (coming soon).  warning [c.zip]: 9437184000 extra bytes at beginning or within zipfile  (attempting to process anyway)  file #1: bad zipfile offset (local header sig): 9437184004  (attempting to re-compensate)`    I can only get partially unzipped files:  `ls Citypersons/images/train/  tubingen ulm weimar zurich`    Can you provide the checksum of all part of Citypersons? "
"Great works !    I followed instruction at:     However, with Caltech Pedestrian dataset, I downloaded all *.tar file from:     but I don't know how to convert *.seq file to sequence of .png file (or .jpg)   With City Person dataset, I downloaded from Gdrive link but it seems Citypersons.z03 file is not final part zip     Could you pls share link download Cityperson & Caltech_pedestrian dataset (have images & label_with_ids)    One more question, do your code support change custom detector instead of Yolov3 ?  Thank you so much !"
"Hello everyone,  After installing all the required libraries ( i think), i get the following error when running the demo :    2020-04-14 18:04:55 [INFO]: Starting tracking...  Lenth of the video: 462 frames  2020-04-14 18:04:55 [INFO]: invalid load key, '\x00'.  'ffmpeg' is not recognized as an internal or external command,operable program or batch file.    I am pretty sure i installed ffmpeg package.    Any inputs?  Thank you"
after i run `python track.py --cfg ./cfg/yolov3_1088x608.cfg --weights ./jde.uncertainty.pt`  there is an error: AttributeError: 'LoadImages' object has no attribute 'files'  if i miss some steps to test it on mot16 challenge?  hope to get your reply
"the model output = Bx(nAx6+512)xGxG, 512 is embedding feature length, nA is anchors num, this seems if one grid has two bbox output, they shared the same embedding features.  When two person is close enough, the center points if the two person fall into same grid, the embedding features seem to be ambiguous"
"Hi guys, I got the problem ""  Error(s) in loading state_dict for Darknet:          size mismatch for classifier.weight: copying a param with shape torch.Size([7, 512]) from checkpoint, the shape in current model is torch.Size([14455, 512]).          size mismatch for classifier.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([14455]).  "" while I run the demo.py,  Where is the location of 14455?  "
"Hi,    When I read your source code about yololayer,    if output of  network is xywh, when it transformed to actural xywh, you used the formulation:  xy = xy * anchor_size + grid_offset * stride  wh = exp(wh) * anchor_size    But the formulations of raw yolo paper is:  xy = sigmoid(xy) * stride + grid_offset * stride  wh = exp(wh) * anchor_size    I have some concern about this  when use sigmoid, the raw output is scaled to (0,1), so  the center point  of detected box, is up to scale with the stride.  But in your solution, xy is up to scale with anchor_size, but the anchor size is various    Best regards  "
"Hello there,  I'm trying to run the demo but i have this issue, (i'm using windows 10), I'm still new to all this tracking so if anyone could help with detail would be much appreciated,   thanks.  !       (realTimeMOT) C:\Users\Tamam Jerbi\PycharmProjects\Graduation_proj\Towards_real_time_MOT\Towards-Realtime-MOT>python demo.py --input-video ppl2.mp4 --weights weights\jde.1088x608.uncertainty.pt  Namespace(cfg='cfg/yolov3_1088x608.cfg', conf_thres=0.5, input_video='ppl2.mp4', iou_thres=0.5, min_box_area=200, nms_thres=0.4, output_format='video', output_root='results', track_buffer=30, weights='weights\\jde.1088x608.uncertainty.pt')    2020-03-22 18:16:41 [INFO]: Starting tracking...  Lenth of the video: 2054 frames  2020-03-22 18:16:41 [INFO]: __init__() got multiple values for argument 'nID'    ffmpeg version 4.2 Copyright (c) 2000-2019 the FFmpeg developers    built with gcc 9.1.1 (GCC) 20190807    configuration: --disable-static --enable-shared --enable-gpl --enable-version3 --enable-sdl2 --enable-fontconfig --enable-gnutls --enable-iconv --enable-libass --enable-libdav1d --enable-libbluray --enable-libfreetype --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libopus --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libtheora --enable-libtwolame --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libzimg --enable-lzma --enable-zlib --enable-gmp --enable-libvidstab --enable-libvorbis --enable-libvo-amrwbenc --enable-libmysofa --enable-libspeex --enable-libxvid --enable-libaom --enable-libmfx --enable-amf --enable-ffnvcodec --enable-cuvid --enable-d3d11va --enable-nvenc --enable-nvdec --enable-dxva2 --enable-avisynth --enable-libopenmpt    libavutil      56. 31.100 / 56. 31.100    libavcodec     58. 54.100 / 58. 54.100    libavformat    58. 29.100 / 58. 29.100    libavdevice    58.  8.100 / 58.  8.100    libavfilter     7. 57.100 /  7. 57.100    libswscale      5.  5.100 /  5.  5.100    libswresample   3.  5.100 /  3.  5.100    libpostproc    55.  5.100 / 55.  5.100  [image2 @ 000001b5d11d19c0] Could find no file with path 'results\frame/%05d.jpg' and index in the range 0-4  results\frame/%05d.jpg: No such file or directory"
None
"I am getting the following error:  builtins.RuntimeError: Error(s) in loading state_dict for Darknet:  size mismatch for classifier.weight: copying a param with shape torch.Size([14455, 512]) from checkpoint, the shape in current model is torch.Size([30, 512]).  size mismatch for classifier.bias: copying a param with shape torch.Size([14455]) from checkpoint, the shape in current model is torch.Size([30]).    It looks like the weights are not consistent with the model architecture.  I noticed the weights are ""jde.*"" while the cfg is ""yolov3*"" - is there a jde*.cfg I need to use?    The cfg I am using looks like:  Namespace(cfg='cfg/yolov3_1088x608.cfg', conf_thres=0.5, input_video='../yolov3/videos/amazon-1088x608.mp4', iou_thres=0.5, min_box_area=200, nms_thres=0.4, output_format='video', output_root='./output/', track_buffer=30, weights='./models/jde.1088x608.uncertainty.pt')    The stack trace is:  File ""/home/denver/work/Towards-Realtime-MOT/demo.py"", line 84, in      track(opt)  File ""/home/denver/work/Towards-Realtime-MOT/demo.py"", line 59, in track    save_dir=frame_dir, show_image=False, frame_rate=frame_rate)  File ""/home/denver/work/Towards-Realtime-MOT/track.py"", line 79, in eval_seq    tracker = JDETracker(opt, frame_rate=frame_rate)  File ""/home/denver/work/Towards-Realtime-MOT/tracker/multitracker.py"", line 164, in __init__    self.model.load_state_dict(torch.load(opt.weights, map_location='cpu')['model'], strict=False)  File ""/home/denver/work/venvs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 830, in load_state_dict    self.__class__.__name__, ""\n\t"".join(error_msgs)))    builtins.RuntimeError: Error(s) in loading state_dict for Darknet:  size mismatch for classifier.weight: copying a param with shape torch.Size([14455, 512]) from checkpoint, the shape in current model is torch.Size([30, 512]).  size mismatch for classifier.bias: copying a param with shape torch.Size([14455]) from checkpoint, the shape in current model is torch.Size([30])."
"Dear @Zhongdao ,  First of all, thank you for sharing this great work!    I'm just trying to run the demo examples from the script below with your pretrained weights.       My running code:  _python demo.py --input-video /mnt/choong/Database/infiniq2018_PeopleCounting/videos_selected/PC_088_CAM3_2/videos/PC_088_CAM3_2_60sec.mp4 --weights /mnt/choong/Towards-Realtime-MOT/weights/jde.1088x608.uncertainty.pt --output-format video --output-root /mnt/choong/Towards-Realtime-MOT/output_    And I have below error messages...  It looks like something is wrong with Numba..but I'm not quite sure what's the problem.  Can you please point me out some clue?  Thank you in advance.      ----ERROR MESSAGE-----  Namespace(cfg='cfg/yolov3_1088x608.cfg', conf_thres=0.5, input_video='/mnt/choong/Database/infiniq2018_PeopleCounting/videos_selected/PC_088_CAM3_2/videos/PC_088_CAM3_2_60sec.mp4', io  u_thres=0.5, min_box_area=200, nms_thres=0.4, output_format='video', output_root='/mnt/choong/Towards-Realtime-MOT/output', track_buffer=30, weights='/mnt/choong/Towards-Realtime-MOT/  weights/jde.1088x608.uncertainty.pt')    2020-03-19 11:42:05 [INFO]: Starting tracking...  Lenth of the video: 1799 frames  Start eval_seq  2020-03-19 11:42:11 [INFO]: Processing frame 0 (100000.00 fps)  /mnt/choong/Towards-Realtime-MOT/tracker/multitracker.py:140: NumbaWarning:   Compilation is falling back to object mode WITH looplifting enabled because Function ""tlbr_to_tlwh"" failed type inference due to: non-precise type pyobject  [1] During: typing of argument at /mnt/choong/Towards-Realtime-MOT/tracker/multitracker.py (143)    File ""tracker/multitracker.py"", line 143:      def tlbr_to_tlwh(tlbr):          ret = np.asarray(tlbr).copy()          ^      @staticmethod  /opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function ""tlbr_to_tlwh"" was compiled in object mode without forceobj=True.    File ""tracker/multitracker.py"", line 142:      @jit      def tlbr_to_tlwh(tlbr):      ^      state.func_ir.loc))  /opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning:   Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.    For more information visit      File ""tracker/multitracker.py"", line 142:      @jit      def tlbr_to_tlwh(tlbr):      ^      state.func_ir.loc))  /mnt/choong/Towards-Realtime-MOT/tracker/multitracker.py:116: NumbaWarning:   Compilation is falling back to object mode WITH looplifting enabled because Function ""tlbr"" failed type inference due to: non-precise type pyobject  [1] During: typing of argument at /mnt/choong/Towards-Realtime-MOT/tracker/multitracker.py (122)    File ""tracker/multitracker.py"", line 122:      def tlbr(self):                     """"""          ret = self.tlwh.copy()          ^      @property  /opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function ""tlbr"" was compiled in object mode without forceobj=True.    File ""tracker/multitracker.py"", line 118:      @jit      def tlbr(self):      ^      state.func_ir.loc))  /opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning:   Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.    For more information visit      File ""tracker/multitracker.py"", line 118:      @jit      def tlbr(self):      ^      state.func_ir.loc))  /mnt/choong/Towards-Realtime-MOT/tracker/multitracker.py:103: NumbaWarning:   Compilation is falling back to object mode WITH looplifting enabled because Function ""tlwh"" failed type inference due to: non-precise type pyobject  [1] During: typing of argument at /mnt/choong/Towards-Realtime-MOT/tracker/multitracker.py (109)    File ""tracker/multitracker.py"", line 109:      def tlwh(self):                     """"""          if self.mean is None:          ^      @property  /opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function ""tlwh"" was compiled in object mode without forceobj=True.    File ""tracker/multitracker.py"", line 105:      @jit      def tlwh(self):      ^      state.func_ir.loc))  /opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning:   Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.    For more information visit      File ""tracker/multitracker.py"", line 105:      @jit      def tlwh(self):      ^      state.func_ir.loc))  2020-03-19 11:42:12 [INFO]: name 'logger' is not defined    "
"I saw you mentioned MOT16 training data as default for evaluating, but what is the format of its labels? Is it like the ones used for training the tracker or like the default ones given by the challenge?"
This is part of the error msg:   ERROR: Failed building wheel for cython-bbox    Running setup.py clean for cython-bbox  Failed to build cython-bbox  Installing collected packages: cython-bbox      Running setup.py install for cython-bbox ... error      ERROR: Command errored out with exit status 1:    
I think there have bug in    Darknet init        where is `img_size` pramer?
!     Can i know what exactly these two steps in screenshots will do. i had confusion with these two steps.    Thanks in Advance 
"Hi, can you please correct the link for the model JDE-576x320 in google drive. It appears to be missing. Thanks a lot in advance!"
"I have just upgraded CUDA from 9.1 to 10.2, and the following error is encountered:    THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=50 error=30 : unknown error  2020-03-04 14:11:47 [INFO]: cuda runtime error (30) : unknown error at /pytorch/aten/src/THC/THCGeneral.cpp:50  "
"Hi, I'm trying to benchmark your jde.1088x608.uncertainty pre-trained model on CVPR19 dataset. I noticed that by default, your JDETracker initialize the Darknet model with nID  = 14455. I have 3 questions:   - Where does this value come from? is it the total number of object identities in the whole joint dataset that you used for training the model? because seemingly there are not that many identities in any of the datasets you mentioned in DATASET_ZOO.  - Is the value important to the performance of the model, since I could still run the tracking with fairly good result (~60% MOTA) with the same default nID value of 14455?  - If I want to train this model on a different dataset, how should I set this value for the model initialization?"
"On what objects(i.e pedestrian only, or general objects) is the darknet model trained on? If I want to train the whole system on some other object, say birds, can I train it on its dataset without changing darknet weights?"
">>(Towards-Realtime-MOT) [huangpan@logos-server Towards-Realtime-MOT]$ python demo.py --input-video  test/MOT16-12.mp4  --weights weights/jde_864x480_uncertainty.pt  --output-format video --output-root results/  Traceback (most recent call last):    File ""demo.py"", line 34, in        from tracker.multitracker import JDETracker    File ""/home/huangpan/git_download/Towards-Realtime-MOT/tracker/multitracker.py"", line 15, in        from tracker import matching    File ""/home/huangpan/git_download/Towards-Realtime-MOT/tracker/matching.py"", line 7, in        import lap  ModuleNotFoundError: No module named 'lap'    my conda info show as below:      >>(Towards-Realtime-MOT) [huangpan@logos-server Towards-Realtime-MOT]$ conda info         active environment : Towards-Realtime-MOT      active env location : /home/huangpan/anaconda3/envs/Towards-Realtime-MOT              shell level : 2         user config file : /home/huangpan/.condarc  >> populated config files : /home/huangpan/.condarc            conda version : 4.8.1      conda-build version : 3.15.1           python version : 3.7.0.final.0         virtual packages : __cuda=10.2                            __glibc=2.30    >>     base environment : /home/huangpan/anaconda3  (writable)             channel URLs :                                                                                                                                                                                                                                                                                                                                                                                                                                                                 package cache : /home/huangpan/anaconda3/pkgs                            /home/huangpan/.conda/pkgs         envs directories : /home/huangpan/anaconda3/envs                            /home/huangpan/.conda/envs                 platform : linux-64               user-agent : conda/4.8.1 requests/2.22.0 CPython/3.7.0 Linux/5.4.8-arch1-1 arch/ glibc/2.30                  UID:GID : 1001:1002               netrc file : None             offline mode : False      when I use pip install lap , it still have the same problem!  How can I fixed  it?   any idea is appreciated!"
!   
"I trained with origin ccmcpe.json, theire is Error tips in test stage  Extracting pedestrain features...  Extracting 0/1750, # of instances 16, time 0.38 sec.  Extracting 40/1750, # of instances 740, time 0.15 sec.  Extracting 80/1750, # of instances 1562, time 0.16 sec.  Extracting 120/1750, # of instances 2167, time 0.15 sec.  Extracting 160/1750, # of instances 2625, time 0.16 sec.  Traceback (most recent call last):    File ""train.py"", line 188, in        opt=opt,    File ""train.py"", line 159, in train      test.test_emb(cfg, data_cfg, weights=latest, batch_size=batch_size, print_in                                                                                                                terval=40)    File ""/home/Towards-Realtime-MOT/test.py"", line 191, in test_emb      feat, label = out[:-1], out[-1].long()  IndexError: dimension specified as 0 but tensor has no dimensions  "
"I tried to download mot devkit using this link (  but it says not found 404 error. If you have it, could you please send it to samihasara@hotmail.com?  Thanks"
the cfg default value equals 'cfg/yolov3.cfg' setted in demo.py . but in fact there is no the file named yolov3.cfg in the cfg folder. maybe it's an exception ignored during updating several code versions . it still could be found two months ago. 
"Hi,    I am trying to run the demo with a test video from YouTube inside of Google Colab but I am getting an error similar to #38 although I am using the current weights and configs:       The Google Colab with the code can be found here:         Thanks for helping with this!"
I guess it is not comppressed properly  
I cannot open the data and results in the MOTChallenge website. Have you meet the same problem?
"How is your anchor calculated?    anchors = 8,24, 11,34, 16,48, 23,68,   32,96, 45,135, 64,192, 90,271,   128,384, 180,540, 256,640, 512,640   "
"  `python demo.py --input-video cctv.mp4 --weights darknet53.conv.74 --output-format video --output-root output1.mp4 --cfg cfg/yolov3_864*480.cfg  Namespace(cfg='cfg/yolov3_864x480.cfg', conf_thres=0.5, input_video='cctv.mp4', iou_thres=0.5, min_box_area=200, nms_thres=0.4, output_format='video', output_root='output1.mp4', track_buffer=30, weights='darknet53.conv.74')    2020-02-11 11:22:00 [INFO]: start tracking...  Traceback (most recent call last):    File ""demo.py"", line 66, in        track(opt)    File ""demo.py"", line 35, in track      dataloader = datasets.LoadVideo(opt.input_video, opt.img_size)    File ""/home/lzc/Projects/Towards-Realtime-MOT/utils/datasets.py"", line 91, in __init__      self.w, self.h = self.get_size(self.vw, self.vh, self.width, self.height)    File ""/home/lzc/Projects/Towards-Realtime-MOT/utils/datasets.py"", line 95, in get_size      wa, ha = float(dw) / vw, float(dh) / vh  ZeroDivisionError: float division by zero  `  I am wondering that is the resolution of the input video must be exactly 864.480 or it doesn't matter?  The video I use is 864.486."
Different objects detected by different anchors in a same grid have same embedded feature.     It will causes the near objects have totally same embedded feature when tracking.  I think it may be the main reason why ID switch (IDs) is so high of JDE.  !     Do you have good ideas to deal with this problem?          And could you provide the result of following setting?  | Method | Det | Emb | #box | #id |  | :-: | :-: | :-: | :-: | :-: |  | DeepSORT_2 | JDE | WRN | 270k | 1.2k |
"     I use cuhk-sysu Dataset for train,test_emb and test.         Loss has become negative. Is this normal?  "
"Hello,   Thanks for your work and the open-source. I have some questions:    why you use different functions to build the target, as you use build_target_thres function for regular training, but use build_target_max for embedding test.  I have gone into  the code, finding that:  1. **build_target_thres** will choose the max_iou gt for every anchor  2. **build_target_max** will choose the max_iou anchor for every gt    and then I check other implementations for yolov3, finding they keep the max_iou anchor for every gt in both training and inference procedures.   So could you please explain why you coded this way?"
"I trained the model with MOT15 dataset. The training processes looked successful (The loss decreased constantly). I didn't ran the test and test_embeddings code though (I commented it out for now). Now while using the self-trained weights, I am getting the following error:     !     I am not able to understand what went wrong. Please help."
"What should be the format of test data? Is it exactly same as training data. Also, what information is required in text files?"
What is the difference between TrackState.tracked and strack.is_activated intuitively? What does the first one used for?
How to make this code work for real-time object tracking? I want to take continuous sequence from web-cam and want to track people in it.
cannot import name 'SyncBN' from 'utils.syncbn' (unknown location)    I compiled and placed it under utils/syncbn. But still facing the same issue    Need help..    Thanks in Advance
The correct word should be  DEVICES
Whenever I run train.py file I get this error.    RuntimeError: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 4.00 GiB total capacity; 2.78 GiB already allocated; 3.35 MiB free; 147.37 MiB cached)    Can I train it on the CPU.    
"Thanks for your good works, but when I run the demo.py, I have met a error: Segmentation fault  when the code run at utils/utils.py(line457).  could you please give me some advices"
OSError: Towards-Realtime-MOT-master\PRW/labels_with_ids/c1s1_000151.txt not found.    The txt file is present in that folder but also it is showing it is not found.I'm using windows
"@Zhongdao Hi, I try to train on UA-DETRAC dataset for tracking vehicle, but I get no detection of the car.   As I follow by the data format, [class] [identity] [x_center] [y_center] [width] [height], I set the car class to zero and follow the data format.  But after training 5 epoch, I use the `test` code to check the map and always get the value of 0.  And I use the demo code, but the model can't get the detection of the car.  Should I try to change the yolov3.cfg anchor size? or other code or the setting should I revised but I ignored?      "
"Hi @Zhongdao,    I have two questions about STrack class:  1. Why in predict(), if state is not Tracked, you zero one of the state variables (velocity of h)?     2. In update_features() you normalize smooth_feat, but I can't find this kind of normalization for feat input?       Regards,"
">  CUDA_VISIBLE_DEIVCES=0,1,2,3,4,5,6,7 python train.py    â€œDEIVCESâ€ should be â€œDEVICESâ€  This confused me for half an hour to debug..."
"Hi,    Can someone help me about this issue ?    !     **First :**  ""Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from      **And also:**  ""[image2 @ 0000021d85b49580] Could find no file with path 'results\frame/%05d.jpg' and index in the range 0-4 results\frame/%05d.jpg: No such file or directory""      Indeed I don't have any NVIDIA GPU on my computer but I thought that it will be possible to run the demo.py script  "
I download the MOT17 dataset from the official website and I do not know how to generate the labels with ids folder. Could you please upload this code?
"Hey @Zhongdao     It is an excellent job. And I want to use the dataset you provided in this repo to train my model, but i am confusing whether the ignore area of annotation in origin dataset are considered here?  "
"When I download the Caltech dataset from this link:     The format of images are .seq and there is no .png, .jpg and .txt int it. Should I convert the dataset to the right format myself? "
"On line 39 of syncbn.py: ex_w, exs_w = ctx.master_queue.get (). A queue is blocked. How can I solve it? Thanks."
"Hi,     I'm learning multi-object tracking. I found that basically the tracked objects of MOT are pedestrians.  Can I use your project to track other objectsï¼ŸSuch as kiwi?  I want to use the eye-in-hand system to pick kiwi.So I need to track kiwi.    Can you help me?    Thanks in advance.    ps.å¤§ä½¬ï¼Œæˆ‘çš„ä¸­å¼è‹±è¯­æ˜¯ä¸æ˜¯æœ‰ç‚¹ä¸¢äºº..."
"1.when compute multiple weighted loss,  the init values set are -4.15, -4.85, -2.3, how to get these values?  2.in your code, use softmax loss to compute embeddings, but in papers use triplet loss, which loss compute embeddings?    "
"I think it should be specified that in the annotation files `[class]` should be set to 0, instead of saying   > The field `[class]` is not used in this project since we only care about a single class, i.e., pedestrian here.    because in `test.py` the predicted class is hardcoded to be      So if someone accidentally sets all classes = 1 in the annotation files they will have very bad scores and not know why. "
"Towards-Realtime-MOT says ""Pytorch >= 1.0.1"" while ""maskrcnn-benchmark"" says ""pytorch 1.0 from nightly release"". Can someone help me in which version of pytorch to install!"
"When I use   python demo.py --input-video a.mp4 --weights darknet53.conv.74 --output-format text  It shows  2019-11-16 16:36:15 [INFO]: start tracking...  Lenth of the video: 106 frames  2019-11-16 16:36:15 [INFO]: invalid load key, '\x00'.  Is anybody know how to solve it ? Thanks."
"Hi,  this is a wonderful work on MOT.  I have some questions to the architecture.  Paper indicate that network choose FPN as base architecture, and I study Feature Pyramid Network (FPN) (Lin et al. 2017) the backbone is ResNets. But the implement backbone seems to be YOLOv3.   Is that network choose FPN as base architecture just the concept or use the specific network ?  I check YOLOv3 has three different scale feature maps, are these means the FPN?  I can't find the feature map with the different scale fused by skip connection,  can somebody point where the code is?  And another question is RPN anchor choose from yolo layer with different scales?  thanks  "
"hi    I'm really thank you for this wonderful work.    I test on the MOT16-01 and find that the fps gradually increase, like 7.66 10.00 11.49 12.32....    Is it related to the fps computing method or some other reasonï¼Œdo you have any ideas?        Thank you for your any reply!  --------------------------------------------------------------------  2019-11-11 14:20:52    !   !   !   "
"hi, this is a wonderful work on MOT, but i run the demo.py, and only 3-4 fps, the video i run is TownCentreXVID.avi, this video is 1080p. i check the code, there is a resize, so i do not know what is the difference"
"Will you support for vehicle dataset too, such as UA-DETRAC?  btw I create UA-DETRAC to Caltech Pedestrian dataset here:   "
Is there a way somehow to change the tracking from pedestrian detection to vehicle detection without retraining the network?
"I get the result frames,but player can't open result.mp4"
2019-11-03 16:49:03 [INFO]: start tracking...  Lenth of the video: 900 frames  2019-11-03 16:49:05 [INFO]: CUDA error: out of memory  ffmpeg: error while loading shared libraries: libopencv_core.so.2.4: cannot open shared object file: No such file or directory      help!help ï¼ï¼  
"when i run the demo.py,i got this:  `from utils.cython_bbox import bbox_ious  ModuleNotFoundError: No module named 'utils.cython_bbox'`  Is anything I have to download or compile?  Wish u help"
"I meet the following mistake:   x/$ python demo.py --input-video input/MOT16-11.mp4 --weights weights/jde.1088x608.uncertainty.pt --output-format video --output-root output/  /home/x/anaconda3/envs/Towards_MOT/lib/python3.6/site-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.    DeprecationWarning)    Namespace(cfg='cfg/yolov3.cfg', conf_thres=0.5, img_size=(1088, 608), input_video='input/MOT16-11.mp4', iou_thres=0.5, min_box_area=200, nms_thres=0.4, output_format='video', output_root='output/', track_buffer=30, weights='weights/jde.1088x608.uncertainty.pt')    2019-10-28 20:05:44 [INFO]: start tracking...  The value: vw=960, vh=540 dw=1088 dh=608  Lenth of the video: 900 frames  2019-10-28 20:05:46 [INFO]: Processing frame 0 (100000.00 fps)  2019-10-28 20:05:47 [INFO]: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED  ffmpeg: error while loading shared libraries: libopencv_core.so.2.4: cannot open shared object file: No such file or directory    How can I solve this?Thank you!"
"i want to test the model by my video,so i run demo.py with my video, but no affect i see.  so could anybody gives me a tutorial about what's the order runing the code?"
None
"this error occur when i run demo.py,does anybody know why?"
"  testing of   on Python 3.8.0    $ __flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics__     __E901,E999,F821,F822,F823__ are the ""_showstopper_""   issues that can halt the runtime with a SyntaxError, NameError, etc. These 5 are different from most other flake8 issues which are merely ""style violations"" -- useful for readability but they do not effect runtime safety.  * F821: undefined name `name`  * F822: undefined name `name` in `__all__`  * F823: local variable name referenced before assignment  * E901: SyntaxError or IndentationError  * E999: SyntaxError -- failed to compile a file into an Abstract Syntax Tree  "
Thanks for your job. Do you have a plan to upload the cfg file and weight of yolov3-tiny?
"Hi there, excellent work with real time MOT!  how can we run demo.py using 864x408 images as input? do we need another trained model? something like JDE-864x408-uncertainty?"
"Traceback (most recent call last):    File ""demo.py"", line 8, in        from tracker.multitracker import JDETracker    File ""/home/fan60526/Towards-Realtime-MOT/tracker/multitracker.py"", line 10, in        from utils.utils import *    File ""/home/fan60526/Towards-Realtime-MOT/utils/utils.py"", line 13, in        import maskrcnn_benchmark.layers.nms as nms    File ""/home/fan60526/maskrcnn-benchmark/maskrcnn_benchmark/layers/__init__.py"", line 10, in        from .nms import nms    File ""/home/fan60526/maskrcnn-benchmark/maskrcnn_benchmark/layers/nms.py"", line 3, in        from maskrcnn_benchmark import _C  ImportError: /home/fan60526/maskrcnn-benchmark/maskrcnn_benchmark/_C.cpython-37m-x86_64-linux-gnu.so: undefined symbol: THCudaFree      Hello, I encountered the above error when running demo.py, request to help me solve, I am using python3.7 Cuda version 10.1"
"Hello, I would like to ask if your training results can be trained through the training data you published.thank  youï¼"
"""CUHKSYSU/images/s6933.jpg"" file path is not exist, however ""CUHKSYSU/labels_with_ids/s6933.txt"" is exist , casuse mismatch, can you slove the problem, please? "
"I think the loss function is not consider the multiple classes loss in the `YOLOLayer`, How can I achieve the multiple calsses trianinig? "
opencv-python   ffmpeg   scikit-learn  numba  motmetrics
"Hi, thanks for your great contribution, I write a simpler demo code, which is good for fresh, but I am wondering that is there any faster-rcnn detection model or other model which is better than yolo3?    "
"firstly, thanks for your great jobs, when l try demo.py i met this error, could you give me some advice?  thank you very much"
"Traceback (most recent call last):    File ""demo.py"", line 8, in        from tracker.multitracker import JDETracker    File ""/HDD/lq/data/Towards-Realtime-MOT/tracker/multitracker.py"", line 10, in        from utils.utils import *    File ""/HDD/lq/data/Towards-Realtime-MOT/utils/utils.py"", line 13, in        import maskrcnn_benchmark.layers.nms as nms    File ""/HDD/lq/data/Towards-Realtime-MOT/maskrcnn-benchmark/maskrcnn_benchmark/layers/__init__.py"", line 10, in        from .nms import nms    File ""/HDD/lq/data/Towards-Realtime-MOT/maskrcnn-benchmark/maskrcnn_benchmark/layers/nms.py"", line 5, in        from apex import amp    File ""/usr/local/lib/python3.6/dist-packages/apex/__init__.py"", line 18, in        from apex.interfaces import (ApexImplementation,    File ""/usr/local/lib/python3.6/dist-packages/apex/interfaces.py"", line 10, in        class ApexImplementation(object):    File ""/usr/local/lib/python3.6/dist-packages/apex/interfaces.py"", line 14, in ApexImplementation      implements(IApex)    File ""/usr/lib/python3/dist-packages/zope/interface/declarations.py"", line 485, in implements      raise TypeError(_ADVICE_ERROR % 'implementer')  TypeError: Class advice impossible in Python3.  Use the @implementer class decorator instead.  "
" python demo.py --input-video test/MOT16-11.mp4 --weights weights/jde.uncertainty.pt --output-format text --output-root results/  Namespace(cfg='cfg/yolov3.cfg', conf_thres=0.5, img_size=(1088, 608), input_video='test/MOT16-11.mp4', iou_thres=0.5, min_box_area=200, nms_thres=0.4, output_format='text', output_root='results/', track_buffer=30, weights='weights/jde.uncertainty.pt')    2019-10-15 10:35:17 [INFO]: start tracking...  Lenth of the video: 900 frames  2019-10-15 10:35:21 [INFO]: Processing frame 0 (100000.00 fps)  2019-10-15 10:35:21 [INFO]: too many indices for array  no result was genrated, why there is too many indices for array?  "
"Hi, thanks for you awesome code, but when I try demo.py, I met this error, and I found there is nothing in the syncbn folder."
"The author's pedestrian tracking algorithm is excellent, better than yolov3 + deep sort, but the detection rate does not seem to be highï¼Ÿ"
æ— æ³•æ‰“å¼€DATASET_ZOO.MD
"æ‚¨å¥½ï¼Œæˆ‘åŠ è½½æ¨¡åž‹æ—¶å‡ºçŽ°ä»¥ä¸‹é—®é¢˜ï¼š  2019-10-11 13:38:53 [INFO]: ""filename 'storages' not found""  ffmpeg version 2.8.15-0ubuntu0.16.04.1 Copyright (c) 2000-2018 the FFmpeg developers    built with gcc 5.4.0 (Ubuntu 5.4.0-6ubuntu1~16.04.10) 20160609    configuration: --prefix=/usr --extra-version=0ubuntu0.16.04.1 --build-suffix=-ffmpeg --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --cc=cc --cxx=g++ --enable-gpl --enable-shared --disable-stripping --disable-decoder=libopenjpeg --disable-decoder=libschroedinger --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmodplug --enable-libmp3lame --enable-libopenjpeg --enable-libopus --enable-libpulse --enable-librtmp --enable-libschroedinger --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxvid --enable-libzvbi --enable-openal --enable-opengl --enable-x11grab --enable-libdc1394 --enable-libiec61883 --enable-libzmq --enable-frei0r --enable-libx264 --enable-libopencv    libavutil      54. 31.100 / 54. 31.100    libavcodec     56. 60.100 / 56. 60.100    libavformat    56. 40.101 / 56. 40.101    libavdevice    56.  4.100 / 56.  4.100    libavfilter     5. 40.101 /  5. 40.101    libavresample   2.  1.  0 /  2.  1.  0    libswscale      3.  1.101 /  3.  1.101    libswresample   1.  2.101 /  1.  2.101    libpostproc    53.  3.100 / 53.  3.100  [image2 @ 0x9bf940] Could find no file with path '/media/xuemin/CE1E49B3007246A9/results/frame/%05d.jpg' and index in the range 0-4  /media/xuemin/CE1E49B3007246A9/results/frame/%05d.jpg: No such file or directory    è¯·æ•™ä¸‹æ‚¨çŸ¥é“å“ªé‡Œå‡ºé—®é¢˜äº†å—ï¼Ÿè°¢è°¢"
"Thank you for your work, I encountered some errors while running the demo.py, can you help me?     "
Can I reduce image size without retrain the model?
"thanks for this great job! when i run this demo ,i meet a error:  `python3 demo.py --input-video /home/lyp/Videos/deploy1-155175756,155175757.mp4 --weights /home/lyp/project/mot-project/towards-realtime-mot/Towards-Realtime-MOT/jde.uncertainty.pt --output-format video --output-root /home/lyp/project/mot-project/towards-realtime-mot/  /usr/local/lib/python3.5/dist-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.    DeprecationWarning)  Traceback (most recent call last):    File ""demo.py"", line 8, in        from tracker.multitracker import JDETracker    File ""/home/lyp/project/mot-project/towards-realtime-mot/Towards-Realtime-MOT/tracker/multitracker.py"", line 14, in        from tracker import matching    File ""/home/lyp/project/mot-project/towards-realtime-mot/Towards-Realtime-MOT/tracker/matching.py"", line 7, in        from utils.cython_bbox import bbox_ious  ImportError: cannot import name 'bbox_ious'  `  i guess my python version is not matching with the cython_bbox.cpython-36m-x86_64-linux-gnu.so.  i find a 3.5 version file from   but this version file do not have the bbox_ious fuction.  Can you tell me about the file dir about cython_bbox.cpython-36m-x86_64-linux-gnu.so? i want a version with 3.5,thanks!          "
   è¿è¡Œæ—¶æŠ¥äº†è¿™ä¸ªé”™ã€‚ã€‚ã€‚
"   When I run the demo, I get this error.I see that there is no such file under the directoryã€‚"
"I'm sorry, this would be very naive question but I couldnt run demo because utils.py couldnt properly import maskrcnn_benchmark.layers.nms as nms    $ python demo.py --input-video path/to/your/input/video --weights path/to/model/weights  Traceback (most recent call last):    File ""demo.py"", line 8, in        from tracker.multitracker import JDETracker    File ""/home/hoge/work/fisheye/Towards-Realtime-MOT/tracker/multitracker.py"", line 10, in        from utils.utils import *    File ""/home/hoge/work/fisheye/Towards-Realtime-MOT/utils/utils.py"", line 14, in        import maskrcnn_benchmark.layers.nms as nms  ModuleNotFoundError: No module named 'maskrcnn_benchmark'    I tried to install by following this install.md instruction     and I manage to install properly.  But how should I edit importing sentense like import maskrcnn_benchmark.layers.nms as nms?    my current working tree structure is like this.  dir(fisheye)/  â”œâ”€â”€Towards-Realtime-MOT (git cloned dir)  â”œâ”€â”€ maskrcnn-benchmark (git cloned dir)    in the case I tried to edit utils.py path for maskrcnn by myself with begginer python knowledge like below.  from ..maskrcnn-benchmark import maskrcnn_benchmark.layers.nms as nms    However I failed to import with this error massage  from ..maskrcnn-benchmark import maskrcnn_benchmark.layers.nms as nms  SyntaxError: invalid syntax    Can someone help me please?    "
Thanks for this code. I'm trying to run demo.py by following all the given instructions. I got following two errors. Can you please help me with this?    2019-10-09 11:56:11 [INFO]: start tracking...  Lenth of the video: 4706 frames  2019-10-09 11:56:11 [INFO]: **[Errno 2] No such file or directory: 'weights/latest.pt'**  ffmpeg version 4.2 Copyright (c) 2000-2019 the FFmpeg developers    built with gcc 7.3.0 (crosstool-NG 1.23.0.449-a04d0)    configuration: --prefix=******* --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1566210161358/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-gpl --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-libx264 --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame    libavutil      56. 31.100 / 56. 31.100    libavcodec     58. 54.100 / 58. 54.100    libavformat    58. 29.100 / 58. 29.100    libavdevice    58.  8.100 / 58.  8.100    libavfilter     7. 57.100 /  7. 57.100    libavresample   4.  0.  0 /  4.  0.  0    libswscale      5.  5.100 /  5.  5.100    libswresample   3.  5.100 /  3.  5.100    libpostproc    55.  5.100 / 55.  5.100  [image2 @ 0x55b2a58bdcc0] **Could find no file with path 'results/frame/%05d.jpg' and index in the range 0-4  results/frame/%05d.jpg: No such file or directory**
"Have error line: [INFO]: invalid load key, '\x00' when run demo.py  in JDETracker function to load pretrained weights    tracker = JDETracker(opt, frame_rate=frame_rate)    File ""tracker/multitracker.py"", line 158, in __init__      self.model.load_state_dict(torch.load(opt.weights, map_location='cpu')['model'], strict=False)    File ""/usr/local/lib/python3.6/dist-packages/torch/serialization.py"", line 386, in load      return _load(f, map_location, pickle_module, **pickle_load_args)    File ""/usr/local/lib/python3.6/dist-packages/torch/serialization.py"", line 563, in _load      magic_number = pickle_module.load(f, **pickle_load_args)  _pickle.UnpicklingError: invalid load key, '\x00'."
"Hey, Thanks for you excellent work, I want to train this repo on my own dataset, could you give instructions for training dataset composed? Expect to be answered!"
"import motmetrics as mm  ModuleNotFoundError: No module named 'motmetrics'    hello!while i am running demo.py,the above error occured, but i can not find the motmetrics module.why?"
None
"  def eval_seq(opt, dataloader, data_type, result_filename, save_dir=None, show_image=True, frame_rate=30):      if save_dir:          mkdir_if_missing(save_dir)      tracker = JDETracker(opt, frame_rate=frame_rate)      timer = Timer()      results = []      frame_id = 0      for path, img, img0 in dataloader:          if frame_id % 20 == 0:              logger.info('Processing frame {} ({:.2f} fps)'.format(frame_id, 1./max(1e-5, timer.average_time)))  dataloader can't be loop"
 I wonder what is the hardware specification were used on the experiment to get the fps the author claimed 
"Thanks for your work and in your project I see some import errors, such as SyncBN in models.py, _C in nms.py, and maskrcnn_benchmark in utils.py. Can you provide these files?"
æ‚¨å¥½ï¼å½“æˆ‘æƒ³è¿è¡Œdemoçš„æ—¶å€™ï¼Œå‘çŽ°æ²¡æœ‰æƒé‡æ–‡ä»¶ã€‚è¿™ä¸ªæƒé‡æ–‡ä»¶æ˜¯trainç”Ÿæˆçš„å—ï¼ŸçŽ°åœ¨æˆ‘å¯ä»¥é€šè¿‡è¿è¡Œtrainç”Ÿæˆæƒé‡æ–‡ä»¶ï¼Œç„¶åŽè¿è¡Œdemoå—ï¼Ÿ
"Total                                                                                               25,503,912  4,089,284,608     10,376,016  Model profiling with width mult 0.9x:  Item                                                                                                    params           macs       nanosecs  Total                                                                                               25,503,912  3,338,064,192     10,670,845  test_only  07/19 03:54:31 PM | Start testing.  07/19 03:54:34 PM | VAL 3.0s 1.0x-224 Epoch:-1/120 Loss:95113.9297 Acc:0.000  07/19 03:54:38 PM | VAL 6.2s 0.9x-224 Epoch:-1/120 Loss:313094.5938 Acc:0.000  07/19 03:54:41 PM | VAL 9.3s 1.0x-192 Epoch:-1/120 Loss:1569.4336 Acc:0.000  07/19 03:54:44 PM | VAL 12.2s 0.9x-192 Epoch:-1/120 Loss:7313.3169 Acc:0.000  07/19 03:54:47 PM | VAL 15.2s 1.0x-160 Epoch:-1/120 Loss:575.6573 Acc:0.000  07/19 03:54:50 PM | VAL 18.4s 0.9x-160 Epoch:-1/120 Loss:2952.8027 Acc:0.000  07/19 03:54:53 PM | VAL 21.2s 1.0x-128 Epoch:-1/120 Loss:1645.1794 Acc:0.000  07/19 03:54:55 PM | VAL 23.7s 0.9x-128 Epoch:-1/120 Loss:81482.6406 Acc:0.000      I use your pretrianed model resnet50 to test on a subset of ImageNet_val , but ACC is 0. "
"Hi, Its rly a nice work! Could u pls update the detection code?"
"     In this line, do you test the model with validation dataset? I don't think it's reasonable. :("
"Thanks for your great work about MutualNet especially your training strategy which inspires me a lot.    However, in test period, you test all these width-resolution configurations on a validation set from 1.0-224 to 0.25-128 and finally choose one best configuration according to all the results. In real time,we don't have time to test all these configurations on every sample, so I think this strategy will waste a lot of time.     Maybe the best way in test is to make the input dynamicly choose the best width-resolution to output directly. Just like the MSDNet meationed in your paper, the input can dynamicly find the most suitable classifier to output judged by a threshold.     So how to implement dynamic inference on MutualNet in test period?   Thanks.      "
None
None
"I use the windows to train model, but I get this problem.    --------------------------------------------------------------------------      File ""D:\æ¡Œé¢\ç«ç¾å¤§åˆ›\ç›¸å…³æ¨¡åž‹ä»£ç \MutualNet-ç›®æ ‡æ£€æµ‹ç­‰\MutualNet-master\ComputePostBN.py"", line 44, in ComputeBN      for batch_idx, (inputs, targets) in enumerate(postloader):    File ""E:\Anaconda3\envs\mutual\lib\site-packages\torch\utils\data\dataloader.py"", line 819, in __iter__      return _DataLoaderIter(self)    File ""E:\Anaconda3\envs\mutual\lib\site-packages\torch\utils\data\dataloader.py"", line 560, in __init__      w.start()    File ""E:\Anaconda3\envs\mutual\lib\multiprocessing\process.py"", line 105, in start      self._popen = self._Popen(self)    File ""E:\Anaconda3\envs\mutual\lib\multiprocessing\context.py"", line 223, in _Popen      return _default_context.get_context().Process._Popen(process_obj)    File ""E:\Anaconda3\envs\mutual\lib\multiprocessing\context.py"", line 322, in _Popen      return Popen(process_obj)    File ""E:\Anaconda3\envs\mutual\lib\multiprocessing\popen_spawn_win32.py"", line 65, in __init__      reduction.dump(process_obj, to_child)    File ""E:\Anaconda3\envs\mutual\lib\multiprocessing\reduction.py"", line 60, in dump      ForkingPickler(file, protocol).dump(obj)  AttributeError: Can't pickle local object 'get_cifar. . '    (E:\Anaconda3\envs\mutual) D:\æ¡Œé¢\ç«ç¾å¤§åˆ›\ç›¸å…³æ¨¡åž‹ä»£ç \MutualNet-ç›®æ ‡æ£€æµ‹ç­‰\MutualNet-master>Traceback (most recent call last):    File "" "", line 1, in      File ""E:\Anaconda3\envs\mutual\lib\multiprocessing\spawn.py"", line 99, in spawn_main      new_handle = reduction.steal_handle(parent_pid, pipe_handle)    File ""E:\Anaconda3\envs\mutual\lib\multiprocessing\reduction.py"", line 87, in steal_handle      _winapi.DUPLICATE_SAME_ACCESS | _winapi.DUPLICATE_CLOSE_SOURCE)  PermissionError: [WinError 5] æ‹’ç»è®¿é—®ã€‚    --------------------------------------------------------------------------    I have searched the internetï¼Œthey sayï¼š    Pytorchçš„DataLoaderæ¨¡å—ï¼Œå½“ num_workersè®¾ç½®ä¸ºéž0æ—¶ï¼Œåœ¨linuxä¸‹è¿è¡Œæ­£å¸¸ï¼Œä½†æ˜¯åœ¨Windowsä¸‹å‡ºçŽ°AttributeError: Canâ€™t pickle local object â€˜Sharpen..create_matricesâ€™ï¼Œå‡ºçŽ°è¯¥å¼‚å¸¸çš„åŽŸå› æ˜¯pickleæ¨¡å—ä¸èƒ½åºåˆ—åŒ–lambda functionã€‚  åœ¨Unix/Linuxä¸‹ï¼Œmultiprocessingæ¨¡å—å°è£…äº†forkï¼ˆï¼‰è°ƒç”¨ï¼Œæ˜¯æˆ‘ä»¬ä¸éœ€è¦å…³æ³¨forkï¼ˆï¼‰çš„ç»†èŠ‚ã€‚ç”±äºŽwindowsæ²¡æœ‰forkè°ƒç”¨ï¼Œå› æ­¤ï¼Œmultiprocessingéœ€è¦â€œæ¨¡æ‹Ÿâ€å‡ºforkçš„æ•ˆæžœï¼Œçˆ¶è¿›ç¨‹æ‰€æœ‰Pythonå¯¹è±¡éƒ½å¿…é¡»é€šè¿‡pickleåºåˆ—å·å†ä¼ åˆ°å­è¿›ç¨‹ä¸­åŽ»ã€‚æ‰€ä»¥ï¼Œå¦‚æžœmultiprocessingåœ¨Windowsä¸‹è°ƒç”¨å¤±è´¥äº†ï¼Œè¦å…ˆè€ƒè™‘æ˜¯ä¸æ˜¯pickleå¤±è´¥äº†ã€‚    --------------------------------------------------------------------------    I think the reason is multiprocessing , so I want to shut it downï¼Œ and I change â€˜data_loader_workers==1â€™ï¼Œbut it still not work  code belowï¼š  # data  #dataset: imagenet1k  #data_transforms: imagenet1k_mobile  ##dataset_dir: /home/ubuntu/yang/data/ImageNet/ILSVRC/Data/CLS-LOC  #dataset_dir: /home/ubuntu/yang/data/ImageNet/ILSVRC/Data/CLS-LOC  #data_loader_workers: 1  dataset: cifar100    # test pretrained resume  test_only: True  pretrained: '../models/mobilenetv1.pt'  resume: ''    As you can seeï¼ŒMy computer dont have enough memoryï¼Œso I want to use cifar100ï¼Œbut still not workã€‚  I have never used pytorch beforeï¼ŒI feel sorry  "
None
"Hi, I ran the inference with pretrained mobilenetv2 but encountered the following error. Any idea about how to solve it?   "
å¤§ä½¬ï¼Œæˆ‘åœ¨ä¸‹è½½æ‚¨çš„é¢„è®­ç»ƒæ¨¡åž‹æ—¶ï¼Œå‘çŽ°æ˜¯è°·æ­Œäº‘é“¾æŽ¥ã€‚ä¿ºä»¬æ‘é‡Œæ²¡æ³•å­ä¸‹è½½ã€‚è·ªæ±‚ä¸€ä»½å›½å†…å¯ä»¥ä¸‹è½½çš„é“¾æŽ¥ã€‚
è¯·é—®æ˜¯å¦å°è¯•è¿‡åŸºäºŽMobileNetçš„ç›®æ ‡æ£€æµ‹è®­ç»ƒï¼Ÿ
"Hi,  thanks for sharing your implementation. I have two questions about it:    1. Does it also work on tabular data?  2. Is it possible to identify the noisy instances (return the noisy IDs or the clean set)?    Thanks!"
"Hi there,    Recently, I have a great interest in this work aiming at addressing such a few-shot-like task with additional noisy data.  Nevertheless, some problems about data pre-processing raised. As stated in this repository `e.g. 13, 795 means that the 13th image of the YFCC100M dataset was used for the 795th class of Shot ImageNet Benchmark.`, I do not really understand what ""13th"" image exactly means. As I know, all images in YFCC100M are now stored in AWS s3 in a hierarchical manner, and there is no ""image id"" or something like that indicating ""n-th"" image.  May you have an instrument or clue to guide one to derive the ""image id"" for YFCC100M?"
"Hey,     Thank you for this awesome repo!    May I know if there are any plans to share code for the CORe50 dataset?   "
It's a great honor to be the first questioner of this project. I want to use incremental learning on a regression problem. I wonder if REMIND works well in this problem.  
"   In this part of ""make_numpy_imagenet_label_files.py"",  ""map"" list preserve the mapping from user order to default pytorch order. But when computing new labels of samples, the key input to the map is the default pytorch value. So I wonder if the map list should preserve the mapping from default label to user order ?  "
"Hi! I want to reproduce the performance of streaming ImageNet models, and follow the steps in README to run the run_imagenet_experiment.sh file( same ImageNet ordering), but i get this result after training, which seems not to match the performance in the paper.    !   "
I have gone through your code and as mentioned in the paper you have used ``faiss.ProductQuantizer`` for quantization of the feature vectors.   Can you tell me how to determine the memory usage of the product quantization like mentioned in your paper?         @tyler-hayes @erobic
"Hello, I would like to ask you what is the purpose of using a pooling layer to make the feature map of h*w into h'*w'? AdaptiveAvgPool2d((ws,ws)) in the code you posted, why output_size=kernel_size?"
"Nice work!  I wonna use context-gated-convolution in action recgnition. I noticed that you use torch.nn.Unfold in layer.py, and it needs a 4-D input(batch x channel x height x width). But I don't know how to do to extend context-gated-convolution to a 5-D input (batch x channel x frams x height x width) . Will you release your code for action recgnition in the paper?  Thanks!"
"Hello, I am getting the following error when I run your content gate conv. How do I fix this error?  return torch.matmul(out, x_un).view(b, self.oc, int(np.sqrt(l)), int(np.sqrt(l)))  RuntimeError: shape '[1, 64, 69, 69]' is invalid for input of size 306432   "
"Hello, I find your job very creative and utilize it on my network. However, the GPU consumption enlarges four times as the former network when training. Have you ever encountered this problem and how can I address it? Thank you very much for your reply!"
in layers.py  num_lat -> self.num_lat
"At line 19 in `context-gated-convolution/image-classification/models/layers.py`,     `self.ind` is always True and the layer works the same as traditional convolution.    Is `or True` redundant?"
"Hello,    this is not an issue, I just wanted to understand the forward pass, can you please tell me what do the argument represent and what are there expected shapes in this snippet in set_matching.py?      "
"Thank you for your wonderful work! I encounter some problems when I try to test the performance of ICP, FPFH+RANSAC and FGR. I use the Open3D library but do not know how to set hyper-parameters. Could you share the codes of ICP, FPFH+RANSAC and FGR with Open3d? "
"Hello, can't your one-to-one LOSS design method lead to the default correspondence according to index when testing? In this way, the loss during training will be very low, but the test results will be very poor."
"Dear authors,    Thank you for making the codes public. I am very intrested about your works, and I have some questions about your codes:  1. I found that in PRNet's code, transformations of test data are fixed in this way:     But I am fail to find these codes in your codes. So are the transformations of test data fixed?      2. You said that you used GNN as feature extractor, but the codes show that it is different from the GNN used in PRNet, in your codes, GNN is in this way:     But in this way, the idices will be fixed at first and not changed, and they are calculated by the Euclidean distance between coordinate points, instead of the L1 or L2 distance of the features. I think it is more like a variant of PointNet, not GNN.    I will be very appreciate if you can answer my doubts."
"As a master study project we tried to reproduce this paper for 35 hours. Our group and two other groups did not manage to replicate the results, since we could not get the code running after 35 hours. We describe our issues on this blog:      In there, we give general suggestions to prevent these sorts of issues, but also provide all the errors we came across. I believe it would help others if you could take another look at your readme trying to run on a clean linux environment such as we did in the blog. We believe your paper has great results, and many people would like to use it, but get discouraged due to the issues. We post this in case it may help to either help with the readme, or get others facing similar problems past some of them."
ä½œè€…æ‚¨å¥½ï¼Œ  æ ¹æ®ä½ çš„usageæ“ä½œå‘½ä»¤ï¼Œæ²¡æœ‰åŠžæ³•è¿›è¡Œè®­ç»ƒï¼Œè¯·é—®æ˜¯æ€Žæ ·è¿›è¡Œè®­ç»ƒå‘¢ï¼Ÿ
None
"hi,ä½œè€…ï¼Œå› ä¸ºç ”ç©¶è¿™ä¸€æ–¹å‘ä¸ä¹…ï¼Œä½ ä»¬æŒ‡çš„æ•°æ®é›†æ˜¯å“ªå‡ ä¸ªå‘¢ï¼Ÿ  !     Please be sure to consult the included readme.txt file for VID competition details. Additionally, the development kit includes    Overview and statistics of the data.  Meta data for the competition categories.  Matlab routines for evaluating submissions.   VID initial release. 22GB. MD5: b329300dd0cd4422171878970d30e1da     VID initial release snippets. 15GB. MD5: 4e8f46f7d507edec5a42e1c25de664c3    There are a total of 1952 snippets for training. The number of snippets for each synset ranges from 23 to 261. There are 281 validation snippets and 458 test snippets. All snippets are extracted into frames in JPEG format. Original snippets are also provided.     VID final releasenew 43GB. MD5: ec74f41fc65873eaa55abafc75db23b4     VID final release snippetsnew 7.8GB. MD5: f338ddd0765e0703097b7462a4fb3186    These are the new data. There are a total of 1910 snippets for training. There are 274 validation snippets and 479 test snippets. All snippets are extracted into frames in JPEG format. Original snippets are also provided.     VIDnew 86GB. MD5: 5feb88ac3345e5b5eae71f6ec8a91325    æˆ‘ä¸€ä¸ªä¸‹è½½å“ªä¸ªè§†é¢‘æ•°æ®é›†å‘¢ï¼Ÿ  2ã€‚  !    DET dataset. 49GB. MD5: 676c745e4329b0592cd855ec0fbbae94    The training and validation dataset is unchanged from ILSVRC2014. There are a total of 456567 images for training. The number of positive images for each synset (category) ranges from 461 to 67513. The number of negative images ranges from 42945 to 70626 per synset. There are 20121 validation images. All images are in JPEG format.     DET test dataset. 6GB. MD5: 51e04a189e97dece83a025d69c3888e8    The test dataset is refreshed by adding 11142 new images. The file contains 51294 (40152 + 11142) images and the test.txt file.   DET test dataset (new). 914MB. MD5: 9d7685ed0f26281ac87e31d82f036be1    The file only contains 11142 new images and the test.txt file.    DETæ•°æ®é›†æœ‰ä¸‰ä¸ªï¼Œæˆ‘åº”è¯¥ä¸‹è½½å“ªä¸ªå‘¢ï¼Ÿ      å¸Œæœ›å¾—åˆ°æ‚¨çš„å›žå¤ã€‚thanksï¼    "
"Hi,     Nice work, I wonder when you will release the full version of code ?    Thanks"
ä½œè€…æ‚¨å¥½ï¼Œæˆ‘ä»Žä¼šè®®ä¸­äº†è§£åˆ°äº†æ‚¨è¿™ç¯‡è®ºæ–‡ï¼Œå¾ˆæƒ³å­¦ä¹ ï¼Œæƒ³è¯·é—®ä¸€ä¸‹æ‚¨è¿™ç¯‡è®ºæ–‡çš„ä»£ç è¿˜æœ‰å¤šä¹…ä¼šå…¬å¸ƒå‘¢
RT
"Thanks for your work!  I have some questions about the sampling operation.     - Does every pixel has an individual sampling pattern to learn?   - And once the training is over, are all the sampling pattern fixed and just used to align the features of the test frames?    Very thanks for your help!"
"Hi,    Thanks for your work. I was wondering how do you deal with gradient updates on the non-searchable stages of the model.  The searchable layers will only be updated once, but multiple forward and backward passess would then go through the tail/stem and the detection head. Would you perhaps average the gradients ? or perhaps freeze the parameters of the non-searchable stages ?"
Another paper EVALUATING THE SEARCH PHASE OF NEURAL ARCHITECTURE SEARCH tested FairNAS on NASBench101 but get the Kendall Tau of -0.23.     FairNAS using 13 models to evaulate the rank and get the Kendall Tau of 0.9487.    I think the number of models used in FairNAS is the way too little and can not really reflect the rank ability of FairNAS
None
ä½ å¥½ï¼Œ  æˆ‘å¯¹FairNASçš„ç†è§£æ˜¯ï¼Œåœ¨è®­ç»ƒè¶…ç½‘çš„æ—¶å€™ï¼Œæ¯ä¸ªbatchæ˜¯ç­‰å¾…æ‰€æœ‰è·¯å¾„ åå‘ä¼ æ’­ æ¢¯åº¦ç›¸åŠ ä¹‹åŽï¼Œç»Ÿä¸€è¿›è¡Œå‚æ•°æ›´æ–°ã€‚ æˆ‘çš„é—®é¢˜æ˜¯ï¼Œå¯¹äºŽè¶…ç½‘ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹ï¼Œå®ƒåªå­˜åœ¨äºŽä¸€æ¡è·¯åŠ²ä¸­ï¼Œæ‰€ä»¥åªä¼šæŽ¥æ”¶åˆ°ä¸€æ¬¡æ¢¯åº¦ï¼Œæ²¡æœ‰ç›¸åŠ çš„è¿‡ç¨‹ï¼Œä¹Ÿæ²¡æœ‰å¿…è¦ç­‰æ‰€æœ‰æ¢¯åº¦åä¼ ä¹‹åŽä¸€èµ·æ›´æ–°å‚æ•°ï¼Œè¯·é—®ç®—æ³•ä¸­æåˆ°çš„æ¢¯åº¦ç›¸åŠ æ˜¯æŒ‡ä»€ä¹ˆï¼Ÿ  å¦å¤–ï¼ŒFariNASè™½ç„¶è§£å†³äº†å¾ˆå¤šå…¬å¹³æ€§çš„é—®é¢˜ï¼Œä½†æ˜¯æ˜¯å¦ä¾ç„¶å­˜åœ¨è·¯å¾„å…ˆåŽé—®é¢˜ï¼Ÿå°±æ˜¯è¯´å¯¹äºŽæœ‰ç›¸åŒèŠ‚ç‚¹noda Pçš„è·¯å¾„L1å’ŒL2ï¼Œå…ˆè®­ç»ƒL1çš„æ—¶å€™ï¼ŒèŠ‚ç‚¹På·²ç»è¢«æ”¹å˜ï¼Œå†è®­ç»ƒL2çš„æ—¶å€™ï¼Œè¯¥èŠ‚ç‚¹æ˜¯å¦ä¼šå½±å“åˆ°L2çš„æ•ˆæžœï¼Ÿ  è°¢è°¢ï¼
I want to know your strategy for retraining the network. Can you release training network code?
the paper didn't offer the speed of the network compared with other lightweight networkï¼Ÿ
"Hello!    Thanks for your amazing work! I was wondering if you will release the searching code in future, for I wish to build new model on my own dataset.    Best,"
"You paper say 'In order to be consistent with the previous works, we donâ€™t employ any other tricks  like dropout [21], cutout [6] or mixup [28], although they can further improve the scores on the test set.'     Is there any misunderstandingï¼Ÿ"
$ conda-env create -n medirl -f environment.yml  Collecting package metadata (repodata.json): done  Solving environment: failed    ResolvePackageNotFound:    - torch
anaconda-project.yml which mentioned in the README file is not exist. So environment regeneration is not easy
In /attention/attention.py you import a file which is not present?    `import maximumEtropyDeepIRL.py as deep_maxent`
Where can i get the eyecar video data?
æ‚¨å¥½ï¼Œæ‚¨ICCVè®ºæ–‡ä¸­æåˆ°å°æ¨¡åž‹å¯ä»¥åœ¨é€€åŒ–åŽçš„REDS4ä¸Šå¯ä»¥PSNRè¾¾åˆ°29.2ï¼Œå¤§æ¨¡åž‹PSNRå¯ä»¥è¾¾åˆ°30.2ï¼Œæˆ‘æŒ‰ç…§æ‚¨å¤§æ¨¡åž‹çš„é…ç½®æœ€åŽåªå¾—åˆ°äº†29.33ï¼Œè®­ç»ƒäº†å°†è¿‘500epochï¼Œå¤§æ¦‚300epochçš„æ—¶å°±å·²ç»æ˜¯29.3äº†ï¼Œå¯èƒ½æ˜¯å·²ç»æ”¶æ•›äº†ã€‚è¯·é—®æ‚¨å¯ä»¥åˆ†äº«ä¸€ä¸‹è®­ç»ƒç­–ç•¥å—ï¼Ÿ
Could you explain the loss function in the ICCV paper?
"I tested your code with --quick_test Gaussian_Vid4 and got different results:    The rfft (& irfft) function is deprecated and the torch version is very old, not compatible with my python and CUDA versions. I replaced that with:  def rfft(t):  # Real-to-complex Discrete Fourier Transform, onesided=False  t = torch.fft.fft2(t)  return torch.stack(       What can cause the different performance?  Thx!"
"I tried to test your code by this command :   python inference.py --input_path ../LR_videos --gt_path ../GT_videos --model_path ../pretrain_models/gaussian_e1r3.pt  which was mentioned in the repo with Pytorch=0.4.1 and 1.2.0 and always get this error :   input=correlation.FunctionCorrelation(tensorFirst=tensorFirst, tensorSecond=tensorSecond),  AttributeError: module 'correlation' has no attribute 'FunctionCorrelation'    How can i solve it?  Thanks for your work!"
æ„Ÿè°¢æ‚¨å¼€æºè¿™ä¹ˆå¥½çš„å·¥ä½œï¼Œæˆ‘ä¹‹å‰åœ¨ç½‘ç»œæœ‰çœ‹åˆ°å¦å¤–ä¸€ç¯‡æ½˜è€å¸ˆçš„Deep Blind Video Super-resolutionæ–‡ç« ï¼Œblogè¿žæŽ¥å¦‚ä¸‹: 
"<img width=""938"" alt=""image"" src=""   In google drive, there is no pretrain_models.pt  where is it???????  I cannot find"
"<img width=""703"" alt=""image"" src=""     I updated PyTorch version to 0.4.1 as your experiment setting and tried to run pytorch-pwc code for checking     However, it said pytorch-pwc need PyTorch version later than 1.3.0.  If then, the fore mentioned  in my issue, the PSNR and SSIM measure would be the same in my previous issue.....  How did you build   with pytorch version 0.4.1.???????  I need help"
"!   I have a question about test    In the paper DBVSR in ICCV,(       The PSNR in the benchmark datasets((REDS) dataset) with unknown Gaussian kernels is 29.18  However, when I tested it on unknown Gaussian kernel, average PSNR was 29.087...  is there any problem with my programming environment??  I am using torch 1.7.1 and python 3.8.12"
May I get the kernel_x4.pt for video sr?
train_dataset: mini-imagenet  train_dataset_args: {split: train}  tval_dataset: mini-imagenet  tval_dataset_args: {split: test}  val_dataset: mini-imagenet  val_dataset_args: {split: val}
"Hello  Thank you for your awesome work.  When I'm trying to run train_classifier.py code with miniImageNet dataset, I'm receiving the followed error:    ./save\classifier_mini-imagenet_resnet12 exists, remove? ([y]/n): n  mini-imagenet  ./materials\mini-imagenet  train dataset: torch.Size([3, 80, 80]) (x38400), 64  mini-imagenet  ./materials\mini-imagenet  val dataset: torch.Size([3, 80, 80]) (x18748), 64  mini-imagenet  ./materials\mini-imagenet  fs dataset: torch.Size([3, 80, 80]) (x12000), 20  num params: 8.0M  Traceback (most recent call last):  File ""train_classifier.py"", line 281, in  main(config)  File ""train_classifier.py"", line 148, in main  for data, label in tqdm(train_loader, desc='train', leave=False):  File ""C:\Users\nusra\AppData\Local\Programs\Python\Python37\lib\site-packages\tqdm\std.py"", line 1171, in iter  for obj in iterable:  File ""C:\Users\nusra\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\utils\data\dataloader.py"", line 352, in iter  return self._get_iterator()  File ""C:\Users\nusra\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\utils\data\dataloader.py"", line 294, in _get_iterator  return _MultiProcessingDataLoaderIter(self)  File ""C:\Users\nusra\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\utils\data\dataloader.py"", line 801, in init  w.start()  File ""C:\Users\nusra\AppData\Local\Programs\Python\Python37\lib\multiprocessing\process.py"", line 112, in start  self._popen = self._Popen(self)  File ""C:\Users\nusra\AppData\Local\Programs\Python\Python37\lib\multiprocessing\context.py"", line 223, in _Popen  return _default_context.get_context().Process._Popen(process_obj)  File ""C:\Users\nusra\AppData\Local\Programs\Python\Python37\lib\multiprocessing\context.py"", line 322, in _Popen  return Popen(process_obj)  File ""C:\Users\nusra\AppData\Local\Programs\Python\Python37\lib\multiprocessing\popen_spawn_win32.py"", line 89, in init  reduction.dump(process_obj, to_child)  File ""C:\Users\nusra\AppData\Local\Programs\Python\Python37\lib\multiprocessing\reduction.py"", line 60, in dump  ForkingPickler(file, protocol).dump(obj)  AttributeError: Can't pickle local object 'MiniImageNet.init..convert_raw'  Traceback (most recent call last):  File """", line 1, in  File ""C:\Users\nusra\AppData\Local\Programs\Python\Python37\lib\multiprocessing\spawn.py"", line 105, in spawn_main  exitcode = _main(fd)  File ""C:\Users\nusra\AppData\Local\Programs\Python\Python37\lib\multiprocessing\spawn.py"", line 115, in _main  self = reduction.pickle.load(from_parent)  EOFError: Ran out of input    The config as follows:    {'train_dataset': 'mini-imagenet', 'train_dataset_args': {'split': 'train', 'augment': 'resize'}, 'val_dataset': 'mini-imagenet', 'val_dataset_args': {'split': 'train_phase_val'}, 'fs_dataset': 'mini-imagenet', 'fs_dataset_args': {'split': 'test'}, 'eval_fs_epoch': 5, 'model': 'classifier', 'model_args': {'encoder': 'resnet12', 'encoder_args': {}, 'classifier': 'linear-classifier', 'classifier_args': {'n_classes': 64}}, 'batch_size': 8, 'max_epoch': 100, 'optimizer': 'sgd', 'optimizer_args': {'lr': 0.1, 'weight_decay': 0.0005, 'milestones': [90]}, 'save_epoch': 5, 'visualize_datasets': True}    I would appreciate it if you give me any suggestions.  Thank you"
"When I load train_labels.pkl, I can only know which category is between 0-351 as above ,but I need to know the true category like(cat ,bird etc.) and their ids in Imagenet (like n02119789).  So how do I get these informations?"
"i tryed this in my code:    model = models.load(torch.load(config['load'], map_location='cpu'))    but next i get this error:    Traceback (most recent call last):                                                                                                                                            File ""test_few_shot.py"", line 125, in        main(config)    File ""test_few_shot.py"", line 77, in main      logits = model(x_shot, x_query).view(-1, n_way)    File ""/hdd/anaconda3/envs/fsl_last/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__      result = self.forward(*input, **kwargs)  TypeError: forward() takes 2 positional arguments but 3 were given    pls tell me how to run 'test_few_show' on cpu?"
"I can get goog result when i use ""cos"" distance, but get bad result when i use ""sqr"" distance. "
"1. How do you divide mini-imagenet dataset for training, validating and testing ?   2. Your code of prototypical network using the same partition  ?  "
None
"When I tried to run the train_meta_baseline.py using ""max_epoch=100"", it always stopped at epoch 83...  ã€""tval  .................... 0%""ã€‘  it seems that something is wrong with ""tval"""
"Hi authors,    thanks for your paper. As mentioned in your paper, your baseline classifier method is quite similar to baseline++. I was wondering why the method outperforms baseline/baseline++ in ""a closer look at few-shot learning""? What is the difference between your methods and baseline/baseline++ that make your paper perform pretty well?    I would appreciate it if I can get your reply!    Thanks!"
" Hello, I have download datasets and put it in the appropriate folder, but when I run the ""python train_classifier.py --config configs/train_classifier_mini.yaml"" , this problem of ""FileNotFoundError: [Errno 2] No such file or directory: 'miniImageNet_category_split_train_phase_train.pickleâ€™ occured"". I can't find a way to solve this problem. Would you like to know how to solve this problem? Thank you for reading this problem and I look forward to receiving your reply. "
"Thank you for sharing, I really appreciate it.     I have tried your code and I find it easy to use the encoder you provided to get the expected result on mini-imagenet. However, when I use  your classifier script, it is difficult to train an encoder which can get the expected result. So can you tell me some details or tips for how to use the classifier script to get such a great encoder. And how did you use your classifier script to get the encoder you provided?     Thank you~"
"I think your idea is very brilliant, so I want to run it.   But when I download the datasets like 'mini-imagenet',ran the code as you did ' python train_classifier.py --config configs/train_classifier_mini.yaml' something bad occured--   ' FileNotFoundError: [Errno 2] No such file or directory: './materials/mini-imagenet/miniImageNet_category_split_train_phase_train.pickle'  --Could you help me to deal with it?I will be grateful!thx"
None
è¯·é—®å¦‚ä½•è®­ç»ƒè‡ªå·±çš„æ•°æ®é›†å‘¢ï¼Œåœ¨baseline_classfiy é‡Œæœ‰éœ€è¦åŠ è½½åºåˆ—åŒ–æ–‡ä»¶ï¼Œæ˜¯ä¸æ˜¯ä¹Ÿéœ€è¦å°†è‡ªå·±çš„æ–‡ä»¶å­˜å‚¨æˆmini_imagnenetçš„åºåˆ—åŒ–å½¢å¼ï¼Œå†è¿›è¡ŒåŠ è½½å‘¢ï¼Ÿ
"Thank you for sharing, I really appreciate it. I recently used your code to reproduce the experimental results in your paper, and I had no problem with the results on mini-imagenet, but when I used the tiered-resnet12.pth file from the provided pre-trained model to train on the tiered-imagenet dataset, I found that with the 5way-1shot setting, I could only achieve about 63% and could not achieve 68% of the results in the paper."
"Dear author,  I have some questions about the meta-test stage.  In your paper, you apply `consistent sampling` and `the same 800 testing tasks are sampled for estimating the performence`. But in the `test_few_shot.py`, I find the random seed is fixed as `0` and the default value of the parameter `test-epochs` is `10`. Do you mean runing the same 800 testing tasks for ten times and computing the average acc?  If so, since the meta-test stage is non-parametric, why different test-epochs get different acc?  Looking forward to your replyï¼"
None
"when I execute `python train_classifier.py --config configs/train_classifier_mini.yaml --gpu 0,1,2,3` , the error reported as the following:   !   Have you ever run into the same problem? Thanks for your help! @yinboc "
"Hi there,    Thanks for the great work!     I followed the README.md instructions on training classifier/multi-classifier on meta-Dataset. However when using multi-GPUs, the error reported as the following:    Traceback (most recent call last):    File ""train_classifier.py"", line 195, in        main(config)    File ""train_classifier.py"", line 107, in main      loss = F.cross_entropy(logits, label)    File ""/home/anaconda3/envs/fewshot_torch17/lib/python3.7/site-packages/torch/nn/functional.py"", line 2468, in cross_entropy      return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)    File ""/home/anaconda3/envs/fewshot_torch17/lib/python3.7/site-packages/torch/nn/functional.py"", line 2264, in nll_loss      ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)  RuntimeError: Assertion `THCTensor_(checkGPU)(state, 4, input, target, output, total_weight)' failed. Some of weight/gradient/input tensors are located on different GPUs. Please move them to a single one. at /opt/conda/conda-bld/pytorch_1603729006826/work/aten/src/THCUNN/generic/ClassNLLCriterion.cu:28    No changes are made on the original code. I am wondering if you have any clue on this?"
"Hello  Thank you for your awesome work.   When I'm trying to run train_classifier.py code with miniImageNet dataset, I'm receiving the followed error:      ./save\classifier_mini-imagenet_resnet12 exists, remove? ([y]/n): n  mini-imagenet  ./materials\mini-imagenet  train dataset: torch.Size([3, 80, 80]) (x38400), 64  mini-imagenet  ./materials\mini-imagenet  val dataset: torch.Size([3, 80, 80]) (x18748), 64  mini-imagenet  ./materials\mini-imagenet  fs dataset: torch.Size([3, 80, 80]) (x12000), 20  num params: 8.0M  Traceback (most recent call last):    File ""train_classifier.py"", line 281, in        main(config)    File ""train_classifier.py"", line 148, in main      for data, label in tqdm(train_loader, desc='train', leave=False):    File ""C:\Users\nusra\AppData\Local\Programs\Python\Python37\lib\site-packages\tqdm\std.py"", line 1171, in __iter__      for obj in iterable:    File ""C:\Users\nusra\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\utils\data\dataloader.py"", line 352, in __iter__      return self._get_iterator()    File ""C:\Users\nusra\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\utils\data\dataloader.py"", line 294, in _get_iterator      return _MultiProcessingDataLoaderIter(self)    File ""C:\Users\nusra\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\utils\data\dataloader.py"", line 801, in __init__      w.start()    File ""C:\Users\nusra\AppData\Local\Programs\Python\Python37\lib\multiprocessing\process.py"", line 112, in start      self._popen = self._Popen(self)    File ""C:\Users\nusra\AppData\Local\Programs\Python\Python37\lib\multiprocessing\context.py"", line 223, in _Popen      return _default_context.get_context().Process._Popen(process_obj)    File ""C:\Users\nusra\AppData\Local\Programs\Python\Python37\lib\multiprocessing\context.py"", line 322, in _Popen      return Popen(process_obj)    File ""C:\Users\nusra\AppData\Local\Programs\Python\Python37\lib\multiprocessing\popen_spawn_win32.py"", line 89, in __init__      reduction.dump(process_obj, to_child)    File ""C:\Users\nusra\AppData\Local\Programs\Python\Python37\lib\multiprocessing\reduction.py"", line 60, in dump      ForkingPickler(file, protocol).dump(obj)  AttributeError: Can't pickle local object 'MiniImageNet.__init__. .convert_raw'  Traceback (most recent call last):    File "" "", line 1, in      File ""C:\Users\nusra\AppData\Local\Programs\Python\Python37\lib\multiprocessing\spawn.py"", line 105, in spawn_main      exitcode = _main(fd)    File ""C:\Users\nusra\AppData\Local\Programs\Python\Python37\lib\multiprocessing\spawn.py"", line 115, in _main      self = reduction.pickle.load(from_parent)  EOFError: Ran out of input       The config as follows:    {'train_dataset': 'mini-imagenet', 'train_dataset_args': {'split': 'train', 'augment': 'resize'}, 'val_dataset': 'mini-imagenet', 'val_dataset_args': {'split': 'train_phase_val'}, 'fs_dataset': 'mini-imagenet', 'fs_dataset_args': {'split': 'test'}, 'eval_fs_epoch': 5, 'model': 'classifier', 'model_args': {'encoder': 'resnet12', 'encoder_args': {}, 'classifier': 'linear-classifier', 'classifier_args': {'n_classes': 64}}, 'batch_size': 8, 'max_epoch': 100, 'optimizer': 'sgd', 'optimizer_args': {'lr': 0.1, 'weight_decay': 0.0005, 'milestones': [90]}, 'save_epoch': 5, 'visualize_datasets': True}    I would appreciate it if you give me any suggestions.  Thank you    "
"Thanks for the great work!    For tiered imagenet, I am confused that why using ""resize"" augmentation during pre-training (classifier-baseline) but using default one (directly resize to final resolution 80?) during meta fine-tuning? Is there any specific reason behind this inconsistency?    Thanks!    "
"Hi, thanks for the amazing code!    I'm running the training script for meta-dataset, but found the GPU utilization rate to be very low. I'm not very familiar with tensorflow, could you help me point out how to speed up the dataloading? Thanks a lot!"
"What a great work!   I have some questions about the training from scratch in table 5.   Do you mean training the second stage(encoder + cosine similarity) without the first stage? Just like change the `load_encoder` and `load` to `None` in the `train_meta_mini.yaml` ? If so, what is the details about optimizer? Is it the same as the first stage?"
"Helloï¼ŒI have a Q.During the run, this problem appeared:  No such file or directory: './materials\\mini-imagenet\\miniImageNet_category_split_train_phase_train.pickle'  Expect your help!"
"Great work! The image size of the mini dataset  in your code is setted 3*80*80, so accuracy in your paper is 3*80*80? Why don't you   use 3*84*84? Expect your help!"
"Hi, thanks for the amazing and well-documented code!    I directly ran your code to try to reproduce the results on mini-ImageNet. However, compared to the accuracies reported in the paper, I observed higher performance for classifier-baseline but slightly lower performance for meta-baseline. Here are my results for 2 runs with random seed 1 and 2. I wonder if you have observed similar variance across different runs? Thank you so much for your help!    | | classifier | classifier | meta | meta |  |--|---------|---------|------|------|  | | 1-shot | 5-shot | 1-shot | 5-shot |  |run1|  60.3 | 78.47 | 62.75 | 79.01 |  |run2| 60.69 | 78.32 |   62.99  |   79.24 |"
"in your paper:  Scaling the cosine similarity. Since cosine similarity has  the value range of [1; 1], when it is used to compute the  logits, it is important to scale the value before applying Softmax  function during training. We add a learnable parameter  r similar to recent works (Gidaris & Komodakis, 2018;  Qi et al., 2018; Oreshkin et al., 2018), that the predicted  probability in training becomes:    I can't find this learnable parameter r in your code . Your classifier seems to use cosine similarity directly?"
None
"excuse me ,i wonder whether the encoder part of the model is training and update in the meta-learning stage?"
"Hello!  In the train stage of minIageNet datasets, there are 64 types of samples, each of which has 600 samples, which means 38400 samples. If you use the 5-way 1-shot experimental setting, it means that one task requires 80 images and your batch has 200 tasks,  your epoch has 4 batches, which means that 64000 pictures are used, and you used data amplification? Or is the sample reused in one epoch? thank you for your patience!"
"Just out of curiosity, since my own dataset is quite small, with only 51 classes and each class has less than 100 examples. Do you think it is possible to use a pretrained model, say ResNet18, as the start point to engage the Classifier-Baseline training, then use the trained model for Meta-Baseline training and so on?    I've looked into the code and set the `pretrained=True` in `def resnet18(pretrained=True, progress=True, **kwargs)`. However, the result did not seem to be any difference when `pretrained=False`. Base on the `train_classifier_im800.yaml`, the   `model_args` is    -  `encoder: resnet18`  - `encoder_args: {}`  - `classifier: linear-classifier `  - `classifier_args: {n_classes: 800}`      It just enlists `resnet18` as the backbone encoder. Can I set `encoder_args` like `encoder_args: {pretrained: true}` or so?"
"I was testing the code with ""python train_classifier.py --config configs/train_classifier_mini.yaml"", and I got this error message. The output in my powershell console is:    set gpu: 0  ./save\classifier_mini-imagenet_resnet12 exists, remove? ([y]/n): y  train dataset: torch.Size([3, 80, 80]) (x38400), 64  val dataset: torch.Size([3, 80, 80]) (x18748), 64  fs dataset: torch.Size([3, 80, 80]) (x12000), 20  num params: 8.0M  train:   0%|                                                                                  | 0/1200 [00:00       main(config)    File ""train_classifier.py"", line 147, in main      for data, label in tqdm(train_loader, desc='train', leave=False):    File ""C:\Users\Administrator\anaconda3\lib\site-packages\tqdm\std.py"", line 1107, in __iter__      for obj in iterable:    File ""C:\Users\Administrator\anaconda3\lib\site-packages\torch\utils\data\dataloader.py"", line 279, in __iter__      return _MultiProcessingDataLoaderIter(self)    File ""C:\Users\Administrator\anaconda3\lib\site-packages\torch\utils\data\dataloader.py"", line 719, in __init__      w.start()    File ""C:\Users\Administrator\anaconda3\lib\multiprocessing\process.py"", line 112, in start      self._popen = self._Popen(self)    File ""C:\Users\Administrator\anaconda3\lib\multiprocessing\context.py"", line 223, in _Popen      return _default_context.get_context().Process._Popen(process_obj)    File ""C:\Users\Administrator\anaconda3\lib\multiprocessing\context.py"", line 322, in _Popen      return Popen(process_obj)    File ""C:\Users\Administrator\anaconda3\lib\multiprocessing\popen_spawn_win32.py"", line 89, in __init__      reduction.dump(process_obj, to_child)    File ""C:\Users\Administrator\anaconda3\lib\multiprocessing\reduction.py"", line 60, in dump      ForkingPickler(file, protocol).dump(obj)  AttributeError: Can't pickle local object 'MiniImageNet.__init__. .convert_raw'  PS D:\few-shot-meta-baseline-master> Traceback (most recent call last):    File "" "", line 1, in      File ""C:\Users\Administrator\anaconda3\lib\multiprocessing\spawn.py"", line 105, in spawn_main      exitcode = _main(fd)    File ""C:\Users\Administrator\anaconda3\lib\multiprocessing\spawn.py"", line 115, in _main      self = reduction.pickle.load(from_parent)  EOFError: Ran out of input    Have anyone run into the same problem under windows 10?"
"Hi, thanks for your impressive work!   I am wondering what is the usage of the folder `meta-dataset`, since it is an independent folder. How and when should I use it?  Should I run ""2. Training Meta-Baseline python train_meta.py --config configs/train_meta_mini.yaml"" in the `meta-dataset` folder, or in the `few-shot-meta-baseline folder`? It is a little confusing."
Can you share the code or script on how to prepare a given dataset into your pickle-style dataset? Thanks a lot!!!
Hi!  Thanks for this code.  I was wondering if it would be possible to share pretrained models for the Meta-baseline (not just the Classifier-baseline) on ImageNet-800
the miniImageNet dataset used in your paper:  miniImageNet_category_split_train_phase_train.pickle  miniImageNet_category_split_train_phase_val.pickle  miniImageNet_category_split_train_phase_test.pickle  miniImageNet_category_split_val.pickle  miniImageNet_category_split_test.pickle  I know ï¼š  â‘ miniImageNet_category_split_val.pickle  is the validation set of miniImageNet which has 16 classes and Each class has 600 images  â‘¡miniImageNet_category_split_test.pickle a test set of miniImageNet which has 20 classes and Each class has 600 images.  â‘¢miniImageNet_category_split_train_phase_train.pickle a train set of miniImageNet which has 64 classes and Each class has 600 images.  but I don't know the detail of miniImageNet_category_split_train_phase_val.pickle and  miniImageNet_category_split_train_phase_test.pickle.??????????????  Is there any overlap between these two data sets and the data inminiImageNet_category_split_train_phase_train.pickle or something else?  
"Hello, I wonder if the classifier baseline branch can be adapted in a few shot learning task?"
"Hi authors, thanks very much for the amazing work. You say that mixup/label smoothing are used in your experiments, but I can't find where it is, would you mind show me the position? If it's not the latest version, I would be appreciate if you can commit your change."
"Thanks for your code! I have the question about your code in meta-dataset ,How to use it and I can't find your pre-trained model Can you tell me the meaning of this folder code and the code file location of the pre training model, and I look forward to your replyï¼"
"Hey!  I have trained the model as you mentioned in the repo. I do have the weights file now. Im now stuck at how to evaluate or test my model.How do i feed my query and target image to the model. I ran the test_one_shot.py file as you said.It gave an error like this:  set gpu: 0  dataset: torch.Size([3, 80, 80]) (x12000), 20  num params: 0.0K  Traceback (most recent call last):                                                  File ""test_few_shot.py"", line 125, in        main(config)    File ""test_few_shot.py"", line 77, in main      logits = model(x_shot, x_query).view(-1, n_way)    File ""/home/malesh/anaconda3/envs/att_2/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 489, in __call__      result = self.forward(*input, **kwargs)    File ""/mnt/dash/Alpha_Share/LOGO_POC/RnD/few_shot/few-shot-meta-baseline-master/models/meta_baseline.py"", line 31, in forward      x_tot = self.encoder(torch.cat([x_shot, x_query], dim=0))  TypeError: 'NoneType' object is not callable    How do i rectify this and please provide us with more details on evaluating the model."
"thanks for your good paper and code. i have a little confusion about base and novel class generalization in figure3 and figure 1b. if i understand correctly, the gap between base and novel class generalization happens in meta-baseline stage after classifier-baseline stage. And the novel class generalization, which is also the performance we care about most, reach the peak at the first epoch. So, does this mean that we have no need to continue training after classifier-baseline stage ? And i find the meta-learning stage in meta-baseline only have effects in miniImageNet, i.e. inceasting training epoch can get performance improvement, but no effect in tieredImageNet and ImageNet-800.    if there is something wrong, please correct me."
Would it be possible to release the config files for cos-metric based training? Thanks.
"In train_meta.py, when the average train/val loss and accuracy get updated, it seems like the argument n in add() should be set to the number of episodes per mini-batch."
"HI,  im a newbie to deep learning.i want to train my own dataset and evaluate it.Can u elaborate on how it can be done and how do i pass the model path,my query and target images to the evaluation script? "
"Hi,    Thanks for your work.    Recently I am reading the paper ""Prototypical Networks"". I found your idea is quite similar to that one. Could you help explain a little bit about the main difference between your idea and that one?    What will the accuracy of ProtoNets on miniImageNet be if the backbone function is changed to ResNet that used in your paper?"
"Hi, awesome work! Could you please tell us how to use the pretrained models offered by youï¼ŸIs it just like epoch-last.pth? "
I can not run the codes due to the lack of miniImageNet_category_split_train_phase_train.pickle. How can I get it?
Do you plan to release the pretained models?  Not everyone can afford pretraining on 8 GPUs.  Thanks.
None
"Hi, I'm very interested in this and have already generated adversarial masks to the images. However, after I got the generated image, I don't know how to evaluate if this method works? Is there any way for me to do the evaluation?    Thank you!"
"I would like to reproduce the results in the paper. However, the dataset in the paper is selected from the public dataset by you. It's hard for me to reproduce the results in the paper without the dataset that you have selected.    Thanks a lot."
"æˆ‘å°è¯•ä½¿ç”¨LFW(500å¼ å›¾(one image per identity)ä½œä¸ºattack_images, 10å¼ ä½œä¸ºtargets)å¤çŽ°ï¼Œä½†æ˜¯å½“æˆ‘æŠŠgammaè®¾ç½®æˆ0.1æ—¶ï¼Œç”Ÿæˆçš„adv_imageså°±ä¸èƒ½æˆåŠŸä¿æŠ¤idäº†ï¼Œè€Œè®ºæ–‡ä¸­å¯ä»¥è®¾ç½®åˆ°2.5ã€‚è¿™æ˜¯ä¸ºä»€ä¹ˆï¼Ÿ"
Thanks.
Thanks a lot.
Thanks.
pairs.txt is initial for training but it is missed in the repo.
None
"Hello I am a computer student, currently reproduced your code on the voc2012 dataset, but the result of the BIM attack is 2 is 33, which is very different from the result in the paper, I encountered this problem during the test, please ask you also encountered it. If you can, you can ask for some suggestions for this problem.Thank you very much.  !       "
"Hi,    I saw this message after downloading your provided pre-trained model from Google Drive and type the `unzip pretrained_model.zip` command.         I wonder whether it is a corrupt file so that I cannot correctly decode it.  Thanks.    Best,  Tsung-Han Wu"
"Hi,    First, thanks for your excellent work.  I am a master student major in CS who is now working on adversarial training. I would like to ask whether your provided config is able to reach the comparable result as your paper report.    Specifically,   1. In  , is it reasonable to set `test_h = 449, test_w = 449` while testing?  2. In your README, you stated that ""For multiprocessing training, we use apex, tested with pytorch 1.0.1."". However, it seems that your   set `opt_level: 'O0'` and does not use float16 during training.     I would be very grateful if you could provide me any suggestions or further explanation.  Thank you very much.    Best,  Tsung-Han Wu."
"Hi, @xiaogang00 Thanks for your work.But I do not find the code of C&W and DeepFool in test code,are the code at another position?  Could you supplement the code of C&W and DeepFool code if possible?  Thanks a lot!"
None
None
"`    python tools/train_net_step_rel.py --dataset vg8k --cfg configs/vg8k/e2e_relcnn_VGG16_8_epochs_vg8k_y_loss_only_hubness100k.yaml --nw 8 --use_tfboard --seed 3`        INFO train_net_step_rel.py: 663: Save ckpt on exception ...      INFO train_net_step_rel.py: 150: save model: Outputs/e2e_relcnn_VGG16_8_epochs_vg8k_y_loss_only_hubness100k/vg8k/Apr02-00-17-05_bock_step_with_prd_cls      _v3/ckpt/model_step0.pth      INFO train_net_step_rel.py: 665: Save ckpt done.      Traceback (most recent call last):        File ""tools/train_net_step_rel.py"", line 604, in main          net_outputs = maskRCNN(**input_data)        File ""/home/wentaoy2/anaconda3/envs/vr/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 477, in __call__          result = self.forward(*input, **kwargs)        File ""/home/wentaoy2/LTVRR/lib/nn/parallel/data_parallel.py"", line 108, in forward          outputs = [self.module(*inputs[0], **kwargs[0])]        File ""/home/wentaoy2/anaconda3/envs/vr/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 477, in __call__          result = self.forward(*input, **kwargs)        File ""/home/wentaoy2/LTVRR/lib/modeling/model_builder_rel.py"", line 513, in forward          return self._forward(data, im_info, dataset_name, roidb, use_gt_labels, include_feat, **rpn_kwargs)        File ""/home/wentaoy2/LTVRR/lib/modeling/model_builder_rel.py"", line 553, in _forward          assert len(roidb) == 1    AssertionError"
"On the command for evaluating the **Visual Genome dataset** there is a file best.pth is missing, could you help us locate it?  `python tools/test_net_rel.py --dataset vg8k --cfg configs/vg8k/e2e_relcnn_VGG16_8_epochs_vg8k_y_loss_only_hubness.yaml --do_val --load_ckpt Outputs/e2e_relcnn_VGG16_8_epochs_gvqa_y_loss_only_hubness/vg8k/Mar11-07-01-07_gpu210-18_step_with_prd_cls_v3/ckpt/best.pth  --use_gt_boxes --use_gt_labels --seed 0`         FileNotFoundError: [Errno 2] No such file or directory: 'Outputs/e2e_relcnn_VGG16_8_epochs_gvqa_y_loss_only_hubness/vg8k/Mar11-07-01-07_gpu210-18_step_with_prd_cls_v3/ckpt/best.pth'  "
"Yes, I also got the CUDA version issue,  the error message I am getting is         ImportError: /home/wentaoy2/LTVRR/lib/model/roi_pooling/_ext/roi_pooling/_roi_pooling.so: undefined symbol: __cudaRegisterFatBinaryEnd        Which CUDA version are you using?  And the version of CUDA and pytorch I am on is        $ pip list| grep torch     torch           0.4.1.post2             torchvision     0.1.8      $ nvcc --version     nvcc: NVIDIA (R) Cuda compiler driver     Copyright (c) 2005-2021 NVIDIA Corporation  Built on Mon_Oct_11_21:27:02_PDT_2021  Cuda compilation tools, release 11.4, V11.4.152  Build cuda_11.4.r11.4/compiler.30521435_0  (vr) wentaoy2@bock:~/LTVRR$       I also tried CUDA-9.0 and got simiar issures:             File ""/home/wentaoy2/LTVRR/lib/model/roi_pooling/functions/roi_pool.py"", line 3, in           from .._ext import roi_pooling      File ""/home/wentaoy2/LTVRR/lib/model/roi_pooling/_ext/roi_pooling/__init__.py"", line 3, in           from ._roi_pooling import lib as _lib, ffi as _ffi      ImportError: /home/wentaoy2/LTVRR/lib/model/roi_pooling/_ext/roi_pooling/_roi_pooling.so: undefined symbol: __cudaRegisterFatBinaryEnd            $ nvcc --version          nvcc: NVIDIA (R) Cuda compiler driver          Copyright (c) 2005-2017 NVIDIA Corporation          Built on Fri_Sep__1_21:08:03_CDT_2017          Cuda compilation tools, release 9.0, V9.0.176"
None
None
"â‘ The challenge page  shows the important data  10th October, 2021: The challenge deadline for both the benchmarks.  (   â‘¡But the Competition page shows the Competition Ends : Never   (     I want to confirm further if the challenge is a long-term competitions without deadline?  Thank you!  "
How to get the 'rel_annotations_train.csv' file? What's format does it have?
"Thank you for your work.  I want to know whether some codes on the 2D-CNN network have been adjusted, such as MobileNetV2-TAM mentioned by erwangccc. I want to know how TAM is embedded in this type of network."
Thanks for the great work TAM!  If you can provide a small demo dataset.I think it would more easy for others to learn your wonderful work.  @liu-zhy 
Thank you for your work.   And do you implement MobileNetV2-TAM arch? Could you release those code?    Thanks!
n_segmentæ˜¯è§†é¢‘åºåˆ—ä¸­å¸§çš„ä¸ªæ•°ï¼Œä½†å¦‚æžœä¸ç¡®å®šè§†é¢‘åºåˆ—å¸§æ•°åº”è¯¥æ€Žä¹ˆåŠžå‘¢ï¼Œè¿™é‡Œä¸èƒ½è‡ªé€‚åº”è°ƒæ•´å˜›ï¼Ÿ
TAMä¸­çš„n_segmentè¡¨ç¤ºçš„è¾“å…¥è§†é¢‘çš„å¸§æ•°å—ï¼Ÿ
"Hi Zhaoyang,     thanks for sharing the nice implementations!  I have a question regarding the data processing of Something-something v2.   I notice that for data on Something-something v2, you hard code `label_transforms` to swap the labels for 3 groups of classes: 86 and 87, 93 and 94, 166 and 167 (line 458 in `ops/models.py`). However this is only done for training, not for validation or test. I wonder if this means that there are errors in the annotation of training data of Something-something v2.     Looking forward to your reply and thanks for the efforts.    Best,    Wei"
"Hi, thanks for your great work. I tried the pretrained somthingv1-8f and somethingv1-16f checkpoints and only got 0.5% test accuracy. Maybe there are some mistakes in these ckpts. Could you please check that? Or is there anything I need to do with the ssthv1 frames? I'm using the original frames without resizing them before being loaded."
"Hi, thanks for your awesome work in video recognition and also the release.    I run the test command  but get errors.     My envsï¼š python3.7, torch 1.6.0ï¼Œ cuda version 11.0  error log:     So could you please help me to figure it out? thx    "
"Thanks for open source such great work.  I notice that all the learning rate of linear layers are x5, even in all the temporal adaptive module. I know that normally for the last fully connected layer, larger learning rate would bring better performance. Is this a mistake? Or it can produce better result?"
Thanks for your great works!  I want to know that how much time did you take to train on Kinetics400 and what GPUs did you use?
We have tested your several related works on our own large real dataset and the result is exciting. Respect bro.
sorry to put into wrong repo. closed.
ä»£ç ä½•æ—¶èƒ½å¼€æºï¼Ÿ
"Hi @NJUHuoJing, thanks for sharing your great code and congratulations on your amazing work.   I have a question about the encoder that you applied in your framework, I can get similar results as yours when I use your code with your provided encoder (vgg_r51.pth). However, when I use the pre-trained VGG-19   provided by pytorch-torchvision, I cannot get any reasonable result. I have tried the torchvision VGG-19 with the exact same layers as you. For instance, in the `test_artistic.py` code, I used the `r41`,`r31`,`r21`, and I defined the following code for the encoder.        Then, I defined the encoder as the following line and added it in `multi_level_test` by using just the `r41`,`r31`,`r21` layers:      I am wondering how did you train your encoder and what is the difference between your VGG-19 model and the VGG-19 model from torchvision since both of them are trained on ImageNet.    I would appreciate a lot any help that you could provide regarding this issue.  Thank you very much."
"Hi @NJUHuoJing , I have read your paper carefully but I don't understand how to have equation 4 from equation 2. Can you explain it for me?"
"Hi,  Thanks for sharing your code.   I ran you test code with success and results looks good.  Do you have plans to share your train code?  Thanks.  Ofer"
"Hi,  thanks for sharing the code!  I can't find your pre-trained models. can you please share it?  also - do you have plans to share your train code?  Thanks.  Ofer"
"Hi, great work!!  Is there any plan releasing the training & testing code or pertained models?"
"Hi, i would like to enquire if there might be more branches (under different titles) of this work going on? I did a Google on Adaptive Model Streaming and only this paper came up."
Hi! Are you able to share the checkpoint files for the pre-trained models (both teacher and student models) for the task described in your paper? Thank you! 
"Hello :)    I have a question about figure 4. in this paper.    In the paper, the ffhq results were generated by using averaged style vectors of each domain.    However, I checked that the style vector is generated globally. There is only one style vector that is shared by all the domains.    Could you explain about ""averaged style vectors of each domain""?    Thank you :)"
"I trained to 65 epochs and found an errorï¼šâ€œfid_ema, fid_ema_mean is not definedâ€"
Hello.    Do you have any plan to upload other pre-trained weights of another setting that were used in your paper?    Unless can you upload only trained food10 weights of which the number of domain is 10.    Thanks.
"Could you please explain that why the gradients of `d_adv_real, d_gp, and d_adv_fake` are calculated separately when training the Discriminator? Whereas in G only the summation of all loses i.e. `g_loss` is used for calculating the gradients?     I am referring to the following section of code.   "
"I was wondering that have you tried using TUNIT for the few-shot generation task like FUNIT?     Given few images of an unseen class, can TUNIT transfer its style (from the guiding network) to a seen content during inference?   OR  it will require fine-tuning the network on the new class? "
"Hello. Could you please give some explanation and advise for the scenario explained below.    I am not using TUNIT but a very similar architecture where I have a GAN jointly trained with a style encoder. The style encoder is trained as a classifier in a supervised setting and its middle layer features are injected as a style code in the generator. During training the validation results are pretty good (which I assume is down to the L1 loss I am using because of the paired supervision).   During inference the style encoder maps all images to a same style code.     The exact same scenario is mentioned in section 3.3. of your paper where you define the style contrastive loss for the generator. Quoting from your paper ""This loss guides the generated image G(x, Ëœs) to have a style similar to the reference  image xËœ and dissimilar to negative (other) samples. By doing so, we avoid the degenerated solution where the encoder  maps all the images to the same style code of the reconstruction loss [5] based on L1 or L2 norm."" Where reference [5] is the starGAN-v2.    In my case I am also using a style classification loss on the style image for the generator however it seems to completely ignore it during inference.      Could you please explain  1) Why your referred to StarGAN-v2 for this particular scenario?   2) Can you give an advise so that my style encoder doesn't ignore the style code at inference?"
"Hi. I couldn't find the explanation of the following question on your paper.     While training the Generator, could you please explain why you update the gradients of the guiding network C? (  Even though we are not backpropagating for the guiding network?      "
"In Figure 17 (a), style vectors are averaged for domain translation. I tried to find this in your code but haven't found yet. Could you please let me know the code where you have done this? or how to generate averaged style vector translated images? "
"HI,sorry to bother you.  But I have some questions when I try to reproduce the quantitive result in Food -10 dataset.  I add the food-10 dataset in code and use the same settings as 'animal faces' dataset.    When I run the code  of 'animal faces'.I can get the best mFIDï¼š49.1 when paper gave the mFID as 47.7.  But when I run the code  of 'food-10'.I can get the best mFIDï¼š69.50 when paper gave the mFID as 52.2  I use pytorch 1.9.0.   I think the difference mFID between animal faces is reasonable.But the 'food-10' is strange.  So I am wondering that if you use different settings in 'food-10' dataset?  Thanks  "
"Hi,sorry to bother you  But I  use the validation coda to validate my training model   ""python main.py --gpu 3 --validation --load_model GAN_20210928-031155 --dataset animal_faces --data_path 'data'""  but it get totally error result.    It gather the all 50 val dataset to one class   I draw the TSNE,it seems like that  !   and print "" (i, len(cluster_grid     "
"Hi,when I use your actual example in Train:  python main.py --gpu 0 --dataset afhq_cat --output_k 10 --data_path '../data' --p_semi 0.2    will have error.    Traceback (most recent call last):    File ""main.py"", line 524, in        main()    File ""main.py"", line 201, in main      main_worker(args.gpu, ngpus_per_node, args)    File ""main.py"", line 257, in main_worker      train_loader, val_loader, train_sampler = get_loader(args, {'train': train_dataset, 'val': val_dataset})    File ""main.py"", line 448, in get_loader      train_sup_dataset = train_dataset['SUP']  KeyError: 'SUP'    Is the code fit for all the dataset? and What's the reason you deal  different dataset with different way?  Thanks  "
"#### It seems that the following code needs to be modified.     '''  validation.py  #val_dataset = data_loader['TRAINSET'] if args.dataset in ['animal_faces', 'lsun_car', 'ffhq'] else data_loader['VALSET']  val_dataset = data_loader['TRAINSET'] if args.dataset in ['u own dataset', 'animal_faces', 'lsun_car', 'ffhq'] else data_loader['VALSET']  '''  #### Otherwise you will get the following errorï¼š  â€˜â€™â€˜  AttributeError: â€˜Subsetâ€™ object has no attribute â€˜targetsâ€™  â€™â€˜â€™"
torchvision==0.3.0  sklearn  is required to train model.    Thanks for sharing your code!
"Hi,     is it possible to train on images with a different vertical and horizontal resolution?    Is it possible to train on images which are not the power of 2?     Thanks, Fabian"
"Hey, I'm trying to get the tunit algorithm running with FFHQ (thumbnails) which are size 128x128. Just for context (although I don't think this is relevant) I had downloaded it from the Drive link (found  ). The way that this code had been set up for FFHQ wasn't quite correct as it didn't take into account the folder structure so I modified the relvant sections everything from `tot_targets` on line 327 in datasetgetter.py to:        Now, when I go to train, the places errors get thrown are very confusingly not the same. Occasionally I will get a stack-trace pointing to `for _, (data, _) in enumerate(train_loader):` at line 142 (initialize_queue) in ops.py. More commonly though, I will get a stack-trace pointing to `imgs, _ = next(train_it)` at line 62 (trainGAN_UNSUP) in train_unsupervised.py:       Normally this error gets thrown at the 0th iteration. However,  I've seen the tqdm bar (at times) move up to 4 iterations before having the error thrown ðŸ¤·â€â™‚ï¸:  !     Been at this for hours now but the code isn't trivial to understand so I'm hoping one of the maintainers has some intuition behind this. Using Pytorch v1.2.0 on an Ubuntu machine. Error is there whether I'm using multiple GPUs or not  "
could you please provide the ten classes' names that used in Appendix?
"Hello!  I'm interested in the experiment of AnimalFaces Dataset, but I didn't find a pure AnimalFaces in public work like FUNIT, the cost of dowmloading the original ImageNet is too large for me, so I wander if you have uploaded the AnimalFaces Dataset (not afhq) to some online network location? I believe it will be very helpful if they have already been uploaded.  Best!"
"Hi, thanks for sharing your code.  I have several questions about your design choice and looking forward to your reply.  1) Data augmentation:  I find the augmentation operation contains     `transforms.ColorJitter(0.4, 0.4, 0.4, 0.125)`  operation.    Since the style information always includes the color, why you involve the ColorJitter operation and regard this transformation sample as the positive sample of the original image?    Will that influence the final results?    2) queue samples use the transformed image  You use   `     x_k = data[1]â€¨    `  in `def initialize_queue(model_k, device, train_loader, feat_size=128)`  This means you use a transformed image to extract style vector rather than the original image, why?    Looking forward to your reply, thanks!"
"Hi ,    I was trying to train summer2winter and utilize the command as below       I have encountered an issue relating to OOM and you can see the log as below.         Any suggestion for this ?? Thanks"
"hiï¼ŒFriedRonaldo     in your code , the weight and the bias of the 'AdaIN' layer is from the Fully connected    example : 128-style code  ---FC---->  256  ----FC---->  2*feature_num    the bias and weight are get from the ""2*feature_num""    can i use the 128-style code to comput mu and sigma, and then comput the 'AdaIN' layer ?    example : mu = mean(128-style code)         sigma = variance(128-style code)    what is the difference?"
"the example in paper shows that four reference images are used to extract the average style code. but in code,only one reference image is used. "
Hello.  I would like to see the results visualized with T-SNE.  The current dataset is summer2winter.    I want to know how to use it in main.  Thank you.
"Thank you for the awesome work.    I am now learning custom data in an unsupervised way. But there was a problem. The cuda out of memory occurs the moment you move to epoch 69->70.    'RuntimeError: CUDA out of memory. Tried to allocate 20.61 GiB (GPU 0; 23.65 GiB total capacity; 3.54 GiB already allocated; 17.74 GiB free; 1.55 GiB cached)'    The command I executed is:'python main.py --gpu 0 --dataset summer2winter --output_k 2 --data_path'../data' --p_semi 0.0 --img_size 256 --batch_size 1 --ddp'    Also, my gpu is TITAN RTX 1.    Thanks for letting me know about this issue."
"I have two folder summer and winter and i have modified dataset structure as per guidelines(like animal_faces), I started supervised approach since I have proper labelled dataset,  after the successful completion of training for 200 epochs the results are same as original input image with some noises added to it. not sure where the things are going wrong.  how to solve this??"
"Hi @FriedRonaldo ,    First of all, thank you for the awesome work.    My question is, do you have any pointers for training on a GPU with less RAM? Like my 2070 8GB?    For example, I keep getting CUDA out of memory messages when training with two classes/labels. Even when decreasing batch_size and val_batch significantly.    Many thanks in advance."
"Hi, I used the following code load the checkpoint:    G = DataParallel(Generator(img_size=512,sty_dim=128))  C = DataParallel(GuidingNet(128))  load_file = ""./ologs/GAN_20200630-174700/model_42.ckpt""  checkpoint = torch.load(load_file, map_location=""cuda:0"")  G.load_state_dict(checkpoint[""G_EMA_state_dict""]) #G_EMA_state_dict  C.load_state_dict(checkpoint['C_EMA_state_dict'])  G.to(device=""cuda"")  C.to(device=""cuda"")  G.eval()  C.eval()  c_src = G.cnt_encoder(x_src)  s_ref = C.moco(x_ref)  x_res = G.decode(c_src, s_ref)    i am getting the following error:  `  AttributeError: 'DataParallel' object has no attribute 'cnt_encoder'`    since i don't have great experience with pytorch i am not able to figure how to actually load without dataparallel. let me know how i can solve and run the inference part,  Thanks"
"how can we train in on custom domains??? when i try i am getting error  `Traceback (most recent call last):    File ""main.py"", line 524, in        main()    File ""main.py"", line 201, in main      main_worker(args.gpu, ngpus_per_node, args)    File ""main.py"", line 257, in main_worker      train_loader, val_loader, train_sampler = get_loader(args, {'train': train_dataset, 'val': val_dataset})    File ""main.py"", line 478, in get_loader      pin_memory=True, sampler=train_sampler, drop_last=False)    File ""/home/mia/anish/experiments/tunit/tunitenv/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 213, in __init__      sampler = RandomSampler(dataset)    File ""/home/mia/anish/experiments/tunit/tunitenv/lib/python3.6/site-packages/torch/utils/data/sampler.py"", line 94, in __init__      ""value, but got num_samples={}"".format(self.num_samples))  ValueError: num_samples should be a positive integer value, but got num_samples=0  `"
"Hi. I'm so glad to see your model! But I have some issue.   I'm going to learn from custom data, But I met    Traceback (most recent call last):    File ""main.py"", line 524, in        main()    File ""main.py"", line 201, in main      main_worker(args.gpu, ngpus_per_node, args)    File ""main.py"", line 327, in main_worker      add_logs(args, logger, 'STATEMA/G_EMA{}/FID'.format(idx_fid), fid_ema[idx_fi      d], epoch + 1)  UnboundLocalError: local variable 'fid_ema' referenced before assignment    this issue.   How can I Solve this Problem? Thanks!  "
Hi~  Is it possible to use custom datasets for training? Thanks!    
Hi so far i started training using        But even at 10th epoch D and G have 0 losses. Is this expected or abnormal?  `Epoch: [10/200] [500/1000] MODE[ONLYCLS] Avg Loss: D[0.00] G[0.00] C[2.61]`    I am not seeing any items in the corresponding results folder for the run even after 9 epochs.... At what point does it give results?
Please find the stacktrace below. Can you let me know what I am doing wrong?   
"Hi.    This paper is simply great!     I was actually looking for a TensorFlow implementation, specifically TensorFlow-compatible checkpoints/frozengraphs/SavedModel. My plan is to convert it to a TensorFlow Lite model and create a demo mobile application it.     Looking forward to hearing from you. "
Love this implementation. For starters it might help some to try it out on Google Colab.    If I have time I try to implement it and open a pull request ;)
"Thanks for your great work. The results of Labels moothing reported in your paper is surprisingly high. I wonder do you reproduce such results or copied from existing works? If former, could you share the code of it?    Thanks a lot!"
"Hi @lgcnsai @ByeongMoonJi,    Thanks for your excellent work!    I have seen a hyperparameter for ImageNet in the main paper, which is a batch size of 256, while in supplementary paper used 512.    - What exactly batch size value that you used in Table 4?   - One more thing, could you provide me the alpha value for ImageNet dataset that used in Table 4?  - Lastly, could you upload the ResNet-152 pre-trained model (Baseline, with LS,  with  CS, with  TF, and with PS-KD) to available on GitHub ?    Best Regards,  Chakkrit"
"We note that the current code only works with CIFAR-100 and ImageNet datasets, could you please share the code running on IWSLT 15?"
"Hello, what is the machine configuration trained in this article? I train on my machine: the Resnet18 network structure of CIFAR100 in the environment of Nvidia 2080 and pytorch1.6 can only achieve an accuracy of 78.600."
"               Hi~ When reading Eq. (6) in the authors' interesting paper, I have a question whether the gradient should be calculated only for P_t(x), or for both P_{t-1}(x) and P_{t}(x).  It seems that the theoretical support (in the paper) is  presented based on the former, but the code is implemented following the latter.     Specifically, in the referred code Line 481, softmax_output is assigned to all_predictions[input_indices] without detach(). In the next epoch, all_predictions[input_indices] is used to calculated the soft_targets (see the referred code Line 437). Then, the loss is calculated by loss = criterion_CE_pskd(outputs, soft_targets), so loss.backward() will compute the gradient for both outputs and soft_targets, which correspond to P_{t}(x) and and  (1-\alpha)y+\alpha P_{t-1}(x) in the paper, respectively.    Is my understanding correct? or I have missed something?"
"Hi @hendrycks     Can i choose `choices=['edsr', 'cae']` both datasets at once while training? I am getting an error that you cannot choose two datasets"
"Hi,     Thank you for the amazing work.     I am just wondering what does aurra and rms calib error represent?  These two values are printed on the terminal by running the eval.py script.     Thank you :) "
"Hi,    It seems that at   of train_noise2net.py the ``pretrained`` argument of ResNet50 is always set to ``True``.    Is this simply a bug or something we should take into account? As far as I understand, the results of the paper do not consider an ImageNet pre-trained classifier, right?    Thanks,  Apostolos"
Thanks for sharing code with public.  Is the link to blurred image still good?  I am not able to access the lnk.    Thanks  C. Chien
"Dear authors,    Can you make the resnet-50 pretrained weights available ? Extracting fails in every case.    Thank you in advance."
"Hi, thank you for sharing the great implementation. I want to run a `DeepAugment + Augmix` experiment on ImageNet, but it seems that the   does not have the option to enable `--augmix`. The transformations are just basic `RandomResizedCrop ` and `RandomHorizontalFlip`. So before I start the training, I just want to make sure it could reproduce the performance on ImageNet-C as reported in the paper (53.6 mce). Thank you very much. Please let me know if I miss anything.     Another question is, when you perform augmix, do you do it on ImageNet images alone? Or do it on EDSR and CAE images as well? Thanks."
"for compactor_param, mask in compactor_mask_dict.items():####################å•ç‹¬è®¾ç½®compactorçš„æ¢¯åº¦ä¿¡æ¯,åŠ ä¸Šlasso_gradæ¢¯åº¦          compactor_param.grad.data = mask * compactor_param.grad.data          lasso_grad = compactor_param.data * ((compactor_param.data ** 2).sum(dim=(1, 2, 3), keepdim=True) ** (-0.5))###########è¿™ä¸ªmaskæ˜¯ä¹˜ä»¥çš„lossç¬¬äºŒé¡¹ï¼Œå’Œè®ºæ–‡ä¸åŒ          compactor_param.grad.data.add_(resrep_config.lasso_strength, lasso_grad)        if not if_accum_grad:          if gradient_mask_tensor is not None:##################gradient_mask_tensorä¸€ç›´ä¸ºNone              for name, param in net.named_parameters():                  if name in gradient_mask_tensor:                      param.grad = param.grad * gradient_mask_tensor[name]          optimizer.step()###############æ¯æ¬¡åªæœ‰ç¬¬äºŒé¡¹ä¼šmask          optimizer.zero_grad()      acc, acc5 = torch_accuracy(pred, label, (1,5))"
Hi @DingXiaoH  :       thanks to your great masterpiece .         and as i wanna check the finished_converted.hdf5 that if the acc has dropped after folding conv and fusing bn ?  so i need to load finished_converted.hdf5 to a engine or model then do inference to eval .       do u have any idea ?      
could you offer your pruned resnet-50.hdf5 file and not pruned one. it saves a lot of time
"Dear author, thank u for your excellent work and code. Recently when I read the convert code, I cannot completely understand the logic behand the beta compensate, can you explain it or point out the reference?    BTW, why we only need to compensate beta for pointwise layer rather than both the depthwise and pointwise layers?     "
The definition of **Conv2dReLU** in builder.py.    Line 87: Maybe you wanna return the **result** not the **conv**.  
"Hi author, thank you for your grate work.      I implemented the resrep pruner in tensorflow environment according to your paper and code. below is what  I did:  firstly I add compactors to the conv layers to be pruned. and then I restore the parameter from original model be to pruned. I confirmed the restored model have the same output with the original model.  I also implemented others works such as adding regularization to compactor's grad , mask  schedule, etc.      after that  I begin to train the model, but I found the model can't  convergence, the loss is become bigger as the training on going( the mask is not set yet as the steps is not enough)ï¼Œit seems like grad explosionï¼Ÿ    did you meet the same issue beforeï¼Ÿ    my environmentï¼š  tf2.2  network: mobilenetv2 + fpn + centernet. (the compactors only added to the mobilenetv2 backbone)      thank youï¼           "
!     Should the parameter `groups=num_features`  in the compactor?          
"Dear author,    Thanks for you contribution!    When I was reading the code and trying to run my own custom ResNet18 with rr/exp_resrep.py, I am confused by some variables.    For example,  in rr/exp_resrep.py, there is a variable named `pacesetter_dict`, which is from the function of `resnet_bottleneck_follow_dict`.  `pacesetter_dict` is fed to `resrep_config`. However, when I check the `ResRepConfig` and  `ResRepBuilder`, I only see that the `resrep_config.target_layers` is explicitly used. I may miss some parts but I have not found how is `pacesetter_dict` used.    So I want to ask   1) does it matter I give a random `pacesetter_dict` when I implement other models?  2) What is the meaning of `pacesetter_dict` and `resnet_bottleneck_follow_dict`?"
"Hello, I'm a pruning newbie.  Thanks for sharing your great research.      I have a few questions.    - Many open source pruning codes use the public classification dataset. but, I want to pruning a mobilenet based object detection model using a custom dataset. Isn't this a problem for using this ResRep?    - If that's possible, I'd really appreciate it if you could give me an example of how to proceed.    Thank you sincerely."
"what should I do to get a pruned architecture? or after pruning, run a test ?"
How does ResRep automatically select the pruning rate for each layer without layer crashing? Could I ask where the implementation of this part of the code is? Thank you!
"Thanks for your great work, could  you please provide a script to change the trained ""finish_converted.hdf5"" to pytorch model?"
"Thank you for sharing this great repo and paper.  I wonder if you examined the affects of ResRep pruning on the latency or throughput of a model?    It is not uncommon that pruning, even at a large scale doesn't yield much improvement on the model's inference speed, was ResRep able to have a profound impact on the inference speed?    Thanks in advanced."
ä½ å¥½ï¼Œè¯·é—®ä¸€ä¸‹ï¼Œå·²ç»ä½¿ç”¨è¿‡DBBè®­ç»ƒæ–¹å¼ï¼Œå†ä½¿ç”¨ResRepæ•ˆæžœè¿˜æ˜Žæ˜¾å—ï¼Ÿ
the size of finish.hdf5 was biger than original model's.
"Thank you for sharing the code !  The idea of the paper is awesome and I want to use the pruning method on other networks.  However, when I try to read the codes, I don't know the meaning of those three variables.  **Pacesetter, target_layers, succeeding strategy**     I am wondering how can I set those variables if I want to use other network like VGG or shuffleNet  Thank you so much !"
Hi Xiaohan! This is in the inspiring re-param series and it's a really nice work. And I wonder if it is available to know how the reviewers of model compression community react to this paper?
None
Firstly thank you for the amazing work  If you could provide a short breakdown of the code and how the succeeding strategy is decided.  It'll prove to be a great help.
"After the experiment is reproduced, check the saved experiment data, Loss, Top_1 accuracy. Open the saved file in txt format, and show errors. How to view experimental data?"
"Hi! Thanks for your great work and I have a few small questions:  1) ""sgd_optimizer"" function in rr/resrep_train.py, the weight_decay of bias and bn are set to weight_decay_biasï¼ˆi.e 0ï¼‰and apply_lr of bias is set to 2 * lr , I  confuse that such a setting is noly for resnet50 training  or  a general trick for resrep pruning?  if the resrep method is applied to other model ,  such weight_decay and ir seetings  will still be necessaryï¼Ÿ  2ï¼‰why introduce the channel selection limit channels and whether exists a criteria for its value setting?  "
"when I run the train.py with dvs128gesture dataset, it says:    Traceback (most recent call last):    File ""codes/train.py"", line 283, in        number_layer=number_layer).to(device)    File ""C:\Users\DDX\OneDrive\BATC_DDX\ICCV2021_DVS128\codes\models.py"", line 280, in __init__      use_max_pool=use_max_pool, alpha_learnable=alpha_learnable, detach_reset=detach_reset)    File ""C:\Users\DDX\OneDrive\BATC_DDX\ICCV2021_DVS128\codes\models.py"", line 36, in create_conv_sequential      PLIFNode(init_tau=init_tau, surrogate_function=surrogate.ATan(alpha = alpha_learnable), detach_reset=detach_reset) if   use_plif else LIFNode(tau=init_tau, surrogate_function=surrogate.ATan(alpha = alpha_learnable), detach_reset=detach_rese  t),    File ""C:\Users\DDX\OneDrive\BATC_DDX\ICCV2021_DVS128\codes\models.py"", line 11, in __init__      super().__init__(v_threshold, v_reset, surrogate_function, detach_reset, monitor_state)  TypeError: __init__() takes from 1 to 5 positional arguments but 6 were given    Seems like something goes wrong with the PIFNode class  "
è¿™æ®µä»£ç è²Œä¼¼æœ‰ç‚¹é—®é¢˜ã€‚ckpt_timeåº”è¯¥è¦é‡æ–°èµ‹å€¼ã€‚   
"Hi, thanks for your interesting work!    can you tell me the requirments that you used?  I train your work on cifar10-dvs with `torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1`, it generates an error as            "
"Can you please help me understand the transform added to some dataset when training? E.g.     if dataset_name == 'MNIST':          transform_train = transforms.Compose([              transforms.RandomAffine(degrees=30, translate=(0.15, 0.15), scale=(0.85, 1.11)),              transforms.ToTensor(),              transforms.Normalize(0.1307, 0.3081),          ])    I tried to train without the RandomAffine and it seemed that the performance was not that good as with this transform."
"This is the script that I am using:  `python ./codes/train_val.py -init_tau 2.0 -use_max_pool -device cuda:0 -dataset_name DVS128Gesture -log_dir_prefix ./pretrained_train_val -T 20 -max_epoch 1024  -detach_reset -channels 128 -number_layer 5 -split_by number -normalization None -use_plif`    Here is what you guys logged in ./logs/A/dvsgesture.txt :   `train dataset len 803, val dataset len 133`  Here is what I get from the dataset downloaded from official IBM website:  `train dataset len 923, val dataset len 157`    "
AttributeError: 'PLIFNode' object has no attribute 'spiking'  æç¤ºåœ¨models.pyä¸­ç¬¬22è¡Œç¼ºå°‘spiking()å‡½æ•°ï¼Œè¯·é—®è¿™ä¸ªå‡½æ•°æ˜¯åœ¨å“ªä¸ªæ–‡ä»¶é‡Œå¯ä»¥è°ƒç”¨ï¼Ÿ
"è°ƒç”¨NMNISTæ•°æ®é›†çš„åº“çš„æ—¶å€™å‡ºçŽ°ä¸‹é¢çš„æŠ¥é”™ï¼Œæˆ‘æ˜¯æŒ‰ç…§è¿™ä¸ªé¡¹ç›®æç¤ºè£…çš„spikingjelly  >>> from spikingjelly.datasets.n_mnist import NMNIST  Traceback (most recent call last):    File "" "", line 1, in      File "" "", line 259, in load_module    File ""/home/liwei/.conda/envs/liwei/lib/python3.8/site-packages/spikingjelly-0.0.0.0.1-py3.8.egg/spikingjelly/datasets/__init__.py"", line 5, in      File "" "", line 259, in load_module    File ""/home/liwei/.conda/envs/liwei/lib/python3.8/site-packages/spikingjelly-0.0.0.0.1-py3.8.egg/spikingjelly/datasets/nav_gesture.py"", line 11, in    ModuleNotFoundError: No module named 'loris'  "
"Hello~    I wanted to ask if it is possible to get only pre-trained CIFAR-10 weights?    I've been training for more than epoch=410 (several days) but current max_test_accuracy=0.8755, it hasn't increased since epoch=320.  Should we train to 1024 to get the same accuracy on paper?    Thank you for your consideration       "
"Hi, thanks for the great work.     I wanted to ask a question about Visual Encoder.     !     In this Figure (taken from the paper), Could you please explain what do channels mean 45, 75 and 76?     I've taken the following sentences from Spiking Jelly documentations:  Note that the size of the image input to the network is ``[1, 1, 28, 28]``,â†’the 0th dimension is ``batch``, and the first dimension is ``channel`` therefore, when calling ``imshow``, first use ``squeeze()`` to change the â†’size to ``[28, 28]``    What is the difference between these two ""channels""?     The second question is ""Did you use Visual Encoder for both static and neuromorphic datasets(N-MNIST, DVS, etc.)?"" Aren't neuromorphic datasets already in the spike train form?     Thank you very much for your time and considerations  "
"Hi,    We have trained N-MNIST using this repository but for the DVS128Gesture dataset, the codes gives the following cuda memory error:    - RuntimeError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 10.74 GiB total capacity; 9.36 GiB already allocated; 28.31 MiB free; 9.38 GiB reserved in total by PyTorch)    We are using the parameters given in this github respository for the DVS128Gesture dataset with train_val.py file. Is there any way of reducing the required memory size? "
"Hi!    I tried running train_val with DVS128Gesture Dataset but the codes returns the following error:    > train dataset len 0, val dataset len 0  > Traceback (most recent call last):  >   File ""./codes/train_val.py"", line 232, in    >     train_data_loader = torch.utils.data.DataLoader(  >   File ""/home/asgr5/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 213, in __init__  >     sampler = RandomSampler(dataset)  >   File ""/home/asgr5/.local/lib/python3.8/site-packages/torch/utils/data/sampler.py"", line 93, in __init__  >     raise ValueError(""num_samples should be a positive integer ""  > ValueError: num_samples should be a positive integer value, but got num_samples=0    The dataset was downloaded from   and stored in the path home/username/datasets/DVS128Gesture. This DVS128Gesture folder contains all files with initials as user* and does not have any seperate folders for train and test. I used the following command to run the simulations:    > python ./codes/train_val.py -init_tau 2.0 -use_max_pool -device cuda:0 -dataset_name DVS128Gesture -log_dir_prefix ./codes/plif_test/logsd -T 20 -max_epoch 1024 -detach_reset -channels 128 -number_layer 5 -split_by number -normalization No  > ne -use_plif    It seems that there is a problem in finding the dataset but it exists already in the given path. Is there any pre-processing required?"
"Hi,     The latest version of spikingjelly does not seem to be compatible with this code. It does not have the accelerating package. It would be great if you could mention the version of the spikingjelly used with this code? Also, please mention the other requirements like pytorch version etc. "
"Hi, thank you for sharing these wonderful works!    I found a problem in loading the pre-trained file 'vg-faster-rcnn.tar'.  The anchor ratios and anchor scales in neural-motifs are inconsistent with the `torchvision.models.detection`   motifs  anchor ratios: (0.23232838, 0.63365731, 1.28478321, 3.15089189); scales: (2.22152954, 4.12315647, 7.21692515, 12.60263013, 22.7102731)  torchvision  anchor ratios: (0.5, 1.0, 2.0); scales: (32, 64, 128, 256, 512).  Thus the pre-trained weights 'vg-faster-rcnn.tar' mismatch the torchvision in `rpn.head.bbox_pred` (120, 512, 1, 1) vs (60, 512, 1, 1).    I don't know if my analysis above is correct and if this will affect the performance of rpn."
Is there any way to decrease the size of the training set? (specifically when using GQA)  
"## Your research is very good, and it is very helpful to me. Thank you for always.      ### I would like to proceed with creating a scenegraph for my custom image.  Can you help me how I can do it?    I would like to modify the Jupiter notebook source code provided by you, if possible, to give a custom image as input.    * I mean, I don't add learning data, I just want to see the scene graph as the image I specified!"
None
!   
"### Your project is very helpful to me. Thank you for always.    Let me explain my issue.  I solved the 4th issue below and encountered a problem.  #4     When I run the last source code of the Jupiter notebook file you provided, I get the following error.  I want to solve this problem.       - ERROR   "
!   
Hello! Thanks for the great job! I wanted to ask if you have anywhere version which also predicts attributes for GQA? 
Thanks for the amazing work! I am trying to reproduce your work but encountered the following error. I guess it is probably because the data link is not accessible:   
"Hi, thanks for sharing your excellent work. Can you release the experiment related (such as verifying Robustness) code for reproducing? @ningyu1991 "
"Hi, I want to reproduce your code but there are too many issue running the nvidia stylegan.    Could you provide the source code of running the fingerprintted generator?"
Hiï¼Œ  I'm trying to produce some fingerprints using your code but i don't know how to validate my fingerprint detection accuracy.  Could you tell me how to use arguments '--check' to validate fingerprint detection accuracyï¼Ÿ  thanks  
"Hi, I'm trying to produce some fingerprints using your code.  I've been experimenting with AFHQ (cats only) but to no avail.    First of all, the bitwise accuracy stays at 50% for most runs and occasionally it converges to sth like 90%+. But when it does converge, the fidelity of the output images is virtually nonexistent.    I'm running the autoencoder training as `python3 train.py --data_dir data/afhq/train/cat/ --image_resolution 128 --output_dir ./output_cat --fingerprint_length 100 --batch_size 32 --num_epochs 30`.    Followed by `python3 embed_fingerprints.py --encoder_path output_cat/checkpoints/stegastamp_100_28102021_10:00:00_encoder.pth --data_dir data/afhq/train/cat --image_resolution 128 --output_dir output_cat_encoded --batch_size 16`.    Am I missing something?"
"Hi,   I downloaded your StyleGAN2 trained on 150k fingerprinted CelebA 128x128, and its size is 8.9G;   The official styleGAN2 pretrained models' sizes are all ~300M;  I follow the guide on   run   python run_generator.py generate-images --network=PATH TO StyleGAN2 trained on 150k fingerprinted CelebA 128x128 \    --seeds=6600-6625 --truncation-psi=0.5  It does not work.    How should I use your pretrained StyleGAN2 model?  Thank you!    "
"Thanks for sharing the code of this work. Recently I am trying to run the code with default setting, and find before starting, the code will load the pretrained models under `pretrained_models` folder in default by this  . When I try to take the original ImageNet pretraining weight of torch, I found the results on Real-Clipart 3 shot varies obviously.    When training with provided pretrained model, the results ended up with test acc `77.4` after 20000 steps of training, while when training from an ImageNet pretraining model, the results can only achieve `70.1`.    I noticed that in the implementation detail of the paper, the two networks are first pretrained with labeled data, but did not find detailed information about the pretraining process, can you provide any suggustion on the setting of pretraining (e.g. starting model, training steps, learning rate), thanks~"
"Hello, thank you for your good job. I wonder if we can improve the model to process some bad cases, like self-contact or occlusions, by adding MTP and DSC datasets? I find that they have some similar situations. Will you consider adding it to the ROMP training model if it can?  Thank you."
"Thanks for your great work!  I wanna ask how to get the 3d position of the SMPL template in a image like this in README, for example, I want to draw the SMPL of the person on the right bottom of the image, I think I need to use the `beta`, `pose` and `cam_trans`? Maybe I should use smpl model to get vertex first, or I only need to translate the `global_orient` by `trans`? I don't know clearly how to transform to the camera coordinate system, can you show me the code about this part?   !       BTW, I noticed the code here in `convert2fbx.py` seemed to calculate the absolute 3D position of the global_orient, can you explain why the indexs are `1,2,0` instead of `0,1,2`? and what the `100` scaler mean?       "
"Thanks for your great work ,  æˆ‘æƒ³è«‹å•ä¸‹åœ–åˆ©ç”¨é¼ æ¨™è½‰å‹•æ¨¡åž‹çš„æ–¹æ³•æ˜¯ç”¨ä»€éº¼å¹³å°åšå‡ºä¾†çš„?  æˆ‘æœ‰åŽ»ç¿»é–±vis_humanè£¡çš„codeï¼Œä½†ä¸¦æ²’æœ‰ç™¼ç¾ç›¸é—œfunction.  è¬è¬~  !   "
æ‚¨å¥½ï¼Œä»“ä¸­çš„romp\lib\visualizationæ˜¯ä¸æ˜¯å¯è§†åŒ–çš„éƒ¨åˆ†ï¼Ÿæˆ‘çŽ°åœ¨æƒ³æŠŠåœ¨3DPWä¸Šé¢„æµ‹çš„meshæ¸²æŸ“ä¸€ä¸‹æŠ•å½±åˆ°å›¾ç‰‡ä¸Šï¼Œè¯·é—®æœ‰æ²¡æœ‰å¯ä»¥ç›´æŽ¥è¿è¡Œçš„å¯åŠ¨æŒ‡ä»¤ï¼Ÿ  ä¸‡åˆ†æ„Ÿè°¢~!
Hello! An error occurred while running the following code.  python -m romp.predict.image --inputs=demo/images --output_dir=demo/image_results --show_mesh_stand_on_image  --interactive_vis  Can't find ROMP/model_data/parameters/J_regressor_extra.npy. 
"Hello,    1. I am trying to train the model starting from pretrained resent on the cmu_panoptic dataset. However, I get the following error:     Do you know what I need to do to avoid this error?    2. Also, does the cmu_panoptic have the 2d pose annotation for all the people appearing in every image?    I would appreciate it if you could help me with this,  Thanks,"
What are the difference between the output .  !     Could you please answer my question? Thanks!    
"I used the demo video:  used ""bev -m video -i ... -o ... --save_video"". Then I used blender and applpy convert2fbx.py,but  I was encountered with a problem that :  >     File ""/Text"", line 275, in process_poses  KeyError: 'bpy_prop_collection[key]: key ""f_avg_Pelvis"" not found'    "
"Thanks so much for sharing the code and solving problems.  I've encounterd a similiar problem as #116 but not the same. I have already downloaded the fine tuned model ROMP_HRNet32_V1.pkl  but it still cannot process my images.      python -m romp.predict.image --inputs /root/autodl-tmp/data/my_video/output/images --output_dir /root/autodl-tmp/data/my_video/output/smpl_pred      No configs_yml is set, set it to the default --configs_yml=configs/image.yml  yaml_timestamp  /root/autodl-tmp/neuman/preprocess/ROMP/active_configs/active_context_2022-11-01_21_11_05.yaml  Loading the configurations from configs/image.yml  INFO:root:{'tab': 'hrnet_cm64_process_images', 'configs_yml': 'configs/image.yml', 'inputs': '/root/autodl-tmp/data/my_video/output/images', 'output_dir': '/root/autodl-tmp/data/my_video/output/smpl_pred', 'interactive_vis': False, 'show_largest_person_only': False, 'show_mesh_stand_on_image': False, 'soi_camera': 'far', 'make_tracking': False, 'temporal_optimization': False, 'save_dict_results': True, 'save_visualization_on_img': True, 'fps_save': 24, 'character': 'smpl', 'renderer': 'pytorch3d', 'f': None, 'model_return_loss': False, 'model_version': 1, 'multi_person': True, 'new_training': False, 'perspective_proj': False, 'FOV': 60, 'focal_length': 443.4, 'lr': 0.0003, 'adjust_lr_factor': 0.1, 'weight_decay': 1e-06, 'epoch': 120, 'fine_tune': True, 'GPUS': 0, 'batch_size': 64, 'input_size': 512, 'master_batch_size': -1, 'nw': 4, 'optimizer_type': 'Adam', 'pretrain': 'simplebaseline', 'fix_backbone_training_scratch': False, 'backbone': 'hrnet', 'model_precision': 'fp32', 'deconv_num': 0, 'head_block_num': 2, 'merge_smpl_camera_head': False, 'use_coordmaps': True, 'hrnet_pretrain': '/root/autodl-tmp/neuman/preprocess/ROMP/trained_models/pretrain_hrnet.pkl', 'resnet_pretrain': '/root/autodl-tmp/neuman/preprocess/ROMP/trained_models/pretrain_resnet.pkl', 'loss_thresh': 1000, 'max_supervise_num': -1, 'supervise_cam_params': False, 'match_preds_to_gts_for_supervision': False, 'matching_mode': 'all', 'supervise_global_rot': False, 'HMloss_type': 'MSE', 'eval': False, 'eval_datasets': 'pw3d', 'val_batch_size': 4, 'test_interval': 2000, 'fast_eval_iter': -1, 'top_n_error_vis': 6, 'eval_2dpose': False, 'calc_pck': False, 'PCK_thresh': 150, 'calc_PVE_error': False, 'centermap_size': 64, 'centermap_conf_thresh': 0.25, 'collision_aware_centermap': False, 'collision_factor': 0.2, 'center_def_kp': True, 'local_rank': 0, 'distributed_training': False, 'distillation_learning': False, 'teacher_model_path': '/export/home/suny/CenterMesh/trained_models/3dpw_88_57.8.pkl', 'print_freq': 50, 'model_path': 'trained_models/ROMP_HRNet32_V1.pkl', 'log_path': '/root/autodl-tmp/neuman/preprocess/log/', 'learn_2dpose': False, 'learn_AE': False, 'learn_kp2doffset': False, 'shuffle_crop_mode': False, 'shuffle_crop_ratio_3d': 0.9, 'shuffle_crop_ratio_2d': 0.1, 'Synthetic_occlusion_ratio': 0, 'color_jittering_ratio': 0.2, 'rotate_prob': 0.2, 'dataset_rootdir': '/root/autodl-tmp/neuman/preprocess/dataset/', 'dataset': 'h36m,mpii,coco,aich,up,ochuman,lsp,movi', 'voc_dir': '/root/autodl-tmp/neuman/preprocess/dataset/VOCdevkit/VOC2012/', 'max_person': 64, 'homogenize_pose_space': False, 'use_eft': True, 'smpl_mesh_root_align': False, 'Rot_type': '6D', 'rot_dim': 6, 'cam_dim': 3, 'beta_dim': 10, 'smpl_joint_num': 22, 'smpl_model_path': '/root/autodl-tmp/neuman/preprocess/ROMP/model_data/parameters', 'smpl_J_reg_h37m_path': '/root/autodl-tmp/neuman/preprocess/ROMP/model_data/parameters/J_regressor_h36m.npy', 'smpl_J_reg_extra_path': '/root/autodl-tmp/neuman/preprocess/ROMP/model_data/parameters/J_regressor_extra.npy', 'smpl_uvmap': '/root/autodl-tmp/neuman/preprocess/ROMP/model_data/parameters/smpl_vt_ft.npz', 'wardrobe': '/root/autodl-tmp/neuman/preprocess/ROMP/model_data/wardrobe', 'mesh_cloth': 'ghostwhite', 'nvxia_model_path': '/root/autodl-tmp/neuman/preprocess/ROMP/model_data/characters/nvxia', 'track_memory_usage': False, 'adjust_lr_epoch': [], 'kernel_sizes': [5], 'collect_subdirs': False, 'save_mesh': True, 'save_centermap': False}  INFO:root:------------------------------------------------------------------  visualize in gpu mode  INFO:root:start building model.  Using ROMP v1  Confidence: 0.25  INFO:root:using fine_tune model: trained_models/ROMP_HRNet32_V1.pkl  INFO:root:missing parameters of layers:['_result_parser.params_map_parser.smpl_model.betas', '_result_parser.params_map_parser.smpl_model.faces_tensor', '_result_parser.params_map_parser.smpl_model.v_template', '_result_parser.params_map_parser.smpl_model.shapedirs', '_result_parser.params_map_parser.smpl_model.J_regressor', '_result_parser.params_map_parser.smpl_model.J_regressor_extra9', '_result_parser.params_map_parser.smpl_model.J_regressor_h36m17', '_result_parser.params_map_parser.smpl_model.posedirs', '_result_parser.params_map_parser.smpl_model.parents', '_result_parser.params_map_parser.smpl_model.lbs_weights', '_result_parser.params_map_parser.smpl_model.vertex_joint_selector.extra_joints_idxs']  INFO:root:Train all layers, except: ['_result_parser.params_map_parser.smpl_model.betas']  visualize in gpu mode  Initialization finished!  Processing /root/autodl-tmp/data/my_video/output/images, saving to /root/autodl-tmp/data/my_video/output/smpl_pred  INFO:root:gathering datasets  Loading 23 images to process  Processed 0 / 23 images      Then it terminates. I really have no idea what to do. Could you please help me out of the situation?"
"ä½œè€…ä½ å¥½ï¼Œæˆ‘è®­ç»ƒåˆ°ä¸€åŠï¼Œä¼šæŠ¥é”™å¦‚ä¸‹ï¼š  python: /opt/conda/conda-bld/magma-cuda113_1619629459349/work/interface_cuda/interface.cpp:899: void magma_queue_create_from_cuda_internal(magma_device_t, cudaStream_t, cublasHandle_t, cusparseHandle_t, magma_queue**, const char*, const char*, int): Assertion `queue->dBarray__ != __null' failed.  æŠ¥é”™é˜¶æ®µå¾ˆéšæœºï¼Œæœ‰æ—¶å€™è®­ç»ƒå¼€å§‹ä¸ä¸ä¹…å°±æŠ¥é”™ï¼Œæœ‰æ—¶å€™è®­ç»ƒäº†ä¸€æ®µæ—¶é—´æ‰æŠ¥é”™ï¼Œ  è¯·é—®è¿™ä¸ªå¯èƒ½æ˜¯ä»€ä¹ˆåŽŸå› é€ æˆçš„å‘¢ï¼Ÿ  @Arthur151 "
"Hello, thank you for your excellent job. I find that the animation sequence imported into UE is lying down, which has 90 degrees from the correct result. How can I fix this problem? Thank you."
"I'm trying to reproduce BEV training results, and I have a few questions.  1. The source tree references a ""posetrack"" dataset. It is not mentioned in the paper, its official web site is dead (as in, the domain name does not even resolve), and last activity in its maintainers' Twitter account was two years ago. For now, I've deleted it from the configs, but is there any other place where I can download it?  2. The paper says that training was done on 4x V100; was that 16 GB or 32 GB? I am currently training on 2x 1080Ti (12 GB each), and I can only go up to batch size 18 without running out of GPU memory. I assume that you can do 64 because you have twice as many GPUs (so, your 64 is 16 per GPU)? Should I adjust the learning rate to compensate?  3. The paper describes a two-step training strategy: ""We first learn monocular 3D pose and shape estimation for 120 epochs on basic training datasets. Then we add the weak annotations of RH to training samples and train for 120 epochs."" Does the v6_train.sh script correspond to the first step or the second step?  4. How can I tell if it is training correctly? What kind of ""Losses"" should I see after one or two epochs? How long did it take to train?  5. Do I understand correctly that the model assumes that all humans are seen at fairly low field-of-view angles (the paper mentions 60 degrees)? I tried the pretrained checkpoint on some wide-angle photos with 90 degree FOV and the results weren't very satisfactory.  6. Is the ""SMIL infant template"" the same one that's available for download on the AGORA web site? (I've managed to import that one into Blender and to have a look at it, but I can't figure out how to do the same with your SMPLA_NEUTRAL.pth.) If it is, are you aware of its defects? Most notably, its hands are clenched into fists rather than flat, lips and eyeballs are messed up, and feet are way too small (I'd say they need to be about 50% larger.)"
"Hello,    Looking at the datasets  , seems like you have a customized annotation for each dataset so that it can be used for your code. If I want to annotate another dataset to be run with your code, is there any guidance or code that I can look into?    I would appreciate it if you could guide me with this.  Thanks,  "
"batch_orth_proj(j3d_preds, cam_preds) as i understand, j3d_preds is world coords,the reason is cv2. solvePnPRansac to get world translate vec, cam_preds is the camara info  of Camera space from other closed issueï¼Œ from convert_cam_to_3d_trans2() we can know the pj3d is  , need View transform . but cam_preds =(s, tx, ty), and in function convert_cam_to_3d_trans, depth, dx, dy = 1.0 / s, tx / s, ty / s which is trans3d, and is outputs['cam_trans'], although it can replace by trans caculate from cv2. solvePnPRansac.why depth, dx, dy is trans(transform by convert_cam_to_3d_trans as input cam from  output of romp), and batch_orth_proj(j3d_preds, cam_preds, mode=""2d"") can do projection transform?   1. what the means of cam_preds from romp   2. function batch_orth_proj in poset_parser.py is Camera transform and Orth Projection transform? or its meanning  3. convert_cam_to_3d_trans can transform romp's cam to tranlate vec as world trans vec? or its meanning  4. thx."
"In your supply materials, you give the results on MuPoTS and 3dpw dataset.  could you public the codes of evaluating BEV on 3dpw/human3.6 datasets? "
"Hey Arthur151, here is a question to ask about the extraction of animation data and trans3d.    By following the Simple_ROMP guide, the code works amazingly well: romp --mode=webcam --show  I would like to extract the location of bones/poses and the translation info like the webcam.py part:                  trans = np.array([convert_cam_to_3d_trans(cam) for cam in cams])                  poses = np.array([result['poses'] for result in results[frame_id]])  How to print the trans and poses, and which file to edit?   (After installation: python setup.py install, the romp command is in the virtual environment /romp/bin/romp, should I edit something and reinstall romp or directly edit the file to print the trans and poses)  "
"Hi, I using `convert_fbx.py` to convert model outupiut to fbx animation, the SMPL model works OK, but using other mixamo characters the result looks not right.    I have tried many differenct mixamo models, they have same problems:    !   !   "
## Problem  **Simple ROMP** has very poor performance on my machine:   - around 10 FPS (standalone: `romp --mode=webcam --show -t`)   - around 7 FPS (as an module: `from romp import ROMP`)    But my hardware utilization on my GPU with CUDA is still low:  !     ## Steps to reproduce       
__Steps to reproduce__       __Error message__     But the file `D:\miniconda3\envs\romp\lib\site-packages\onnxruntime\capi\onnxruntime_providers_tensorrt.dll` exists. ðŸ¤”     __Full error message__       __System__   - OS: Windows 10   - Cuda:      
"Hello, I use ROMP to estimate the SMPL and use it to render novel view pictures. But I found that if using the intrinsics and extrinsics estimated by ROMP can correctly generate the bounding box in 2D picture, but using intrinsics and extrinsics calibrated by the camera ourselves will be wrong, which is really inexplicable. Why can't I use the  intrinsics and extrinsics calibrated by the camera? Do you know the possible reason? Thank you very much!"
æ‚¨å¥½ï¼Œæˆ‘åœ¨ç¿»ä¸€äº›æ•°æ®é›†(æ¯”å¦‚coco)çš„æ•°æ®å¤„ç†çš„æ—¶å€™å‘çŽ°æ‚¨å¥½åƒæ²¡æœ‰ä¼ å…¥ç›¸æœºå†…å‚ã€‚è¿™è®©æˆ‘æœ‰ç‚¹ç–‘æƒ‘ï¼Œæƒ³ç¡®è®¤ä¸€ä¸‹è¿™æ˜¯å¦æ˜¯æˆ‘çœ‹é”™äº†ã€‚  å¦‚æžœæ‚¨åœ¨é¢„æµ‹æ—¶ç¡®å®žæ²¡ç”¨å†…å‚ï¼Œé‚£ä¹ˆæˆ‘æƒ³è¯¢é—®ä¸€ä¸‹å°±æ˜¯ç…§ç‰‡ä¸Šç›¸åŒå¤§å°çš„äººï¼Œåœ¨ä¸åŒå†…å‚ä¸‹ï¼Œæ·±åº¦æ˜¯ä¸ä¸€æ ·çš„ã€‚ä½†BEVä¼¼ä¹Žæ˜¯å¯ä»¥ç›´æŽ¥é¢„æµ‹æ·±åº¦çš„ï¼Œè¯·é—®æ‚¨æ˜¯æ€Žä¹ˆè§£å†³è¿™ä»¶äº‹æƒ…çš„ï¼Ÿ
"I have..  ## Some dependency questions   1.  , wich  . Why you not also recommend this  ?      and..  ## Some `pip` issues   1. `pip install simple-romp` requires also `numpy` and `cython` but `pip` does not resolve it properly, so i need to use `pip install simple-romp cython numpy` instead.   2. If i am using the `-t` parameter for temporal optimization,  . `simple-romp` works as expected but i get the message:       Full error message:       __How to reproduce__       __Installed packages__         and..  ## Some `conda`, or to be more clear, `cython` issues  I get: `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf`.    Full error message:       __How to reproduce__       __Installed packages__         ## My system   * Conda: `4.12.0`   * OS: Windows 10 Enterprise `Version 21H2 (Build 19044.1826)`"
æ‚¨å¥½ï¼Œæˆ‘åœ¨åšé®æŒ¡è¯„ä¼°çš„æ—¶å€™é‡åˆ°äº†ä¸€äº›é—®é¢˜    å‰æï¼šæˆ‘çš„æ¨¡åž‹åŠ å…¥äº†3dpw-train setè®­ç»ƒè€Œæˆ    æˆ‘è¯„ä¼°äº†æˆ‘è®­ç»ƒçš„æ¨¡åž‹ç»“æžœå¦‚ä¸‹ï¼š     åŒæ—¶è¯„ä¼°äº†ROMP_HRNet32_V1.pklï¼Œç»“æžœå¦‚ä¸‹ï¼š       è¿™ä¸ªç»“æžœå’Œè®ºæ–‡ä¸­çš„ç»“æžœä¸€ç‚¹ä¹Ÿä¸ä¸€æ ·  !     è¯·é—®è¿™ä¸ªç»“æžœè¿˜éœ€è¦å†è¿›è¡Œä¸€æ­¥è®¡ç®—æ‰èƒ½å¾—åˆ°è®ºæ–‡ä¸­çš„ç»“æžœå—ï¼Ÿ  
"### Question for dataset checking  Hi, Doc. Sun. After evaluation on BEV, now I want to train a model by myself from scratch. But when I run ` python -m romp.lib.dataset.h36m --configs_yml='configs/v1.yml'` to test my dataset installation, a bug pops out like:     `AssertionError: Path /home/gpu/content/ROMP0/model_data/parameters/SMPL_FEMALE.pkl does not exist!`    I wonder where can I get the SMPL_FEMALE.**pkl**. I can only find SMPL_FEMALE.**pth** in the model_data.zip. Hope for your answer : -)   "
"I'm using bev to  infer a video. After adding -t -sc 5.0 when inferring a video, I got some models at the far end shifting randomly. Is it likely causing by the OneEuro algorithm or the calculation of cam_trans?            "
No such file or directory    mpi_inf_3dhp/S1/Seq1/annot.mat    mpi_inf_3dhp in google drive has only annots.npz    How can i find annot.mat file ?? @Arthur151 
"Hi Yu,    we used your work to extract 3D motion from 2D videos. In general, it works quite well for single-person scenarios. Nice work! However, when I tried with multiperson cases, I have two issues:  1. regarding exporting motion to fbx/bvh. It seems to me that convert2fbx.py only exports the motion for one character. I looked into .npz file, and the 3D joint position data for all person are there. So this is not really critical since I still can extract 3D motion data.  2. another issue is a little bit more annoying to me. I noticed that the tracking result seems to be inconsistent for multi-person. You can see an example below. There are two consecutive frames. The estimated poses look fine for me. However, the colors are swapped. I assume the color is the character's identity. And the extract 3D motion is corresponding to the colored skeleton, which is flipped.  This might not be a big problem for 3D pose estimation in the video since every person in the scene still has an estimated pose attached. But for the 3D motion, the motion is a mix of two-person and not consistent in time, so you will see a sudden jump at some point in time.     !   !   It would be great if you can help us with the mentioned issues. Thank you very much in advance.  "
"Hi Yu, thanks for your work and such an organized repo!     I'm now using ROMP to get SMPL poses and would like to visualize the meshes via a perspective camera. I usually use a similar way as   to convert the `s`, `t_x`, and `t_y` along with a human bbox to the pinhole camera parameters and it does work on VIBE output. However, it seems like I cannot easily get the parameters with the output from ROMP .npz outputs (I can get a rough bbox from `pj2d_org` ). I found that the scaling factor `s` is quite different between the VIBE and ROMP estimation for the same input image (~1.14 in VIBE and ~0.58 in ROMP). Could you please point out how I can quickly obtain (estimate) the camera intrinsic and extrinsic? Thanks! "
"Hi, I got another bug when I tried to evaluate the model.  Here is my command     Here is the bug report   "
ä½œè€…ä½ å¥½ï¼Œæˆ‘ä½¿ç”¨.npzæ–‡ä»¶è½¬.fbxçš„æ—¶å€™ï¼Œè®¾ç½®äº†fps_sourceï¼Œfps_targetè¿™ä¸¤ä¸ªå‚æ•°ä¸º30ï¼Œä½†è½¬æ¢åŽçš„fbxç»“æžœå¸§çŽ‡è¿˜æ˜¯24ï¼Œå¦‚å›¾æ‰€ç¤º  !   è¯·é—®è¿™æ˜¯ä»€ä¹ˆé—®é¢˜
å¤§ä½¬æ‚¨å¥½ï¼Œéžå¸¸æ„Ÿè°¢æ‚¨çš„å·¥ä½œ~ æˆ‘åœ¨å¤çŽ°è®ºæ–‡ç»“æžœçš„è¿‡ç¨‹ä¸­é‡åˆ°äº†ä¸€äº›é—®é¢˜    æˆ‘é‡‡ç”¨äº†v1.ymlæ¥è¿›è¡Œè®­ç»ƒï¼Œç”±äºŽæ˜¾å¡åŽŸå› ï¼ˆæˆ‘çš„ç”µè„‘ä¸€å¼ 3070æ˜¾å¡ï¼Œ8Gæ˜¾å­˜ï¼‰æˆ‘çš„batchè°ƒå°åˆ°äº†8ï¼Œå…¶ä»–åŸºæœ¬éƒ½æ˜¯é»˜è®¤ï¼Œä»¥ä¸‹æ˜¯æˆ‘çš„ymlæ–‡ä»¶       æˆ‘è®­ç»ƒäº†5ä¸ªepochä¹‹åŽï¼Œå‘çŽ°Lossè™½ç„¶åœ¨ä¸‹é™ï¼Œä½†æ˜¯ä¸‹é™çš„å¾ˆæ…¢ï¼Œä¸‹é¢æ˜¯æˆ‘çš„log           æˆ‘ä¹‹å‰æœ‰å°è¯•è¿‡å°†3DPWçš„æ•°æ®é›†åŠ å…¥åˆ°è®­ç»ƒä¹‹ä¸­ï¼Œå‘çŽ°å¾ˆå¿«å°±è¾¾åˆ°äº†ä¸é”™çš„ç»“æžœï¼Œä¸‹é¢æ˜¯åŠ å…¥äº†3DPWä¹‹åŽçš„log         æˆ‘æƒ³ç¡®è®¤ä¸‹æˆ‘çš„è®­ç»ƒæ˜¯å¦æœ‰é—®é¢˜ï¼Œå› ä¸ºçœ‹èµ·æ¥è™½ç„¶ä¸‹é™çš„æ¯”è¾ƒæ…¢ï¼Œä½†æ˜¯è¿˜æ˜¯åœ¨ä¸‹é™çš„  1ã€æƒ³è¯·å¤§ä½¬å¸®å¿™çœ‹ä¸‹æ—¥å¿—ï¼Œèƒ½å¦ç¡®è®¤æˆ‘çš„è®­ç»ƒæ˜¯æœ‰é—®é¢˜çš„çŠ¶æ€ï¼Ÿ  2ã€å¤§ä½¬èƒ½å¦å‘ä¸‹æ‚¨çš„è®­ç»ƒæ—¥å¿—ï¼ˆä½¿ç”¨äº†pretrian/pretrain_hrnet.pklï¼‰ï¼Œç„¶åŽæˆ‘è‡ªå·±å¯¹æ¯”ä¸€ä¸‹    PSï¼šæˆ‘æ²¡æœ‰ä½¿ç”¨æœ€æ–°çš„ä»£ç ï¼Œå› ä¸ºçœ‹èµ·æ¥å’Œä»£ç å¹¶æ²¡æœ‰å…³ç³»
"Hi, long time no see. Glad to see your BEV's contribution. I have been playing with ROMP+blender now and developing something interesting. But the translation along the depth direction is always shaking, we can only set the camera right in front of the character, otherwise, it's a bit ugly.    I have two questions now:    1. In ROMP, I remember that you said you determine the depth order by the scale. But I remember that you just transform add cam_trans to the verts then directly go into render process. I didn't see the code to ""determine the depth order"". I guess maybe you actually regard the translation recovery process: `verts += cam_trans` as the ""determine depth order"" process?    2. In BEV, do we have a more stable 3D translation now? Will it still shake along the depth direction? I haven't tried BEV with Blender yet. I'll try this later.  "
"Hi,    Really impressive work with ROMP and BEV!    I was wondering generally around both implementations - why do you decide to create a dense parameter map for things like smpl rotations / body center translations and then sample from them based on the estimated body center indices instead of sampling from the final feature map based on your inferred centers and then predicting smpl rotations / centers based on the sampled feature vector?    Have you done any experimentation around this with respect to model training? Can you think of any advantages / disadvantages based on either of those methodologies?    Thank you and amazing work!  "
"  i need to use the following code for using it as an module in 'image' mode:     But if iam using it in the 'video' mode, i need to do the following:     According to here:      My question:  If ROMP get's in both cases ('video' and 'image' mode) only an single image as input, then why i need to change the `mode` from `image` to `video`?  "
"hello,The loss function converges relatively high, and I want to know how much the loss function converges to a better model."
"æˆ‘æƒ³è¿è¡Œwebcamå’Œwebcam_blenderï¼Œç”¨æ¥åœ¨blenderé‡Œé©±åŠ¨3Dçš„æ¨¡åž‹(     ä½†æ˜¯åœ¨è¿è¡Œwebcam.pyæ—¶æŠ¥é”™äº†ï¼ŒæŠ¥é”™ä¿¡æ¯å¦‚ä¸‹ï¼š  TypeError: forward() missing 1 required positional argument: 'meta_data'ï¼Œå¹¶ä¸”æ¯æ¬¡åœ¨print ""Reseting Mesh 5""ä¹‹åŽå‡ºçŽ°è¿™ä¸ªé”™è¯¯ã€‚    æˆ‘çœ‹åˆ°ä¹‹å‰æœ‰ä¸€ä¸ªissueè¯´æŒ‡å®šCUDA_VISIBLE_DEVICESï¼Œä½†è¿™ä¸ªæ–¹æ³•åœ¨è¿™ä¸ªbugå‰æ˜¯ä¸èµ·æ•ˆçš„    åŒæ—¶ï¼Œæˆ‘å°è¯•äº†predict/videoå’Œpredict/imageï¼Œä»–ä¿©éƒ½æ˜¯å¯ä»¥æ­£å¸¸å·¥ä½œçš„ã€‚    ä¸çŸ¥é“å­™åšä½ åœ¨å¹³æ—¶ä½¿ç”¨çš„è¿‡ç¨‹ä¸­æ˜¯å¦é‡åˆ°è¿‡ç±»ä¼¼çš„é—®é¢˜ï¼Œæˆ–è€…ä½ å»ºè®®ä»Žå“ªäº›æ–¹é¢å¯ä»¥å®šä½bugå’Œè¿›è¡Œæ”¹è¿›ï¼Ÿ"
"Tested with the following command:     And get the following result at frame `215` of  :   !     Why does it evaluate not only the largest person (`--show_largest`), according to the  ?:  `  by:       My `pip freeze` output:     System:   - Python: v`3.9.12`   - OS: Windows 10 Enterprise (64-bit, Version `21H2`, Build `19044.1706`)  "
"Invocation without any arguments:     Invocation with explicit `--help` argument:     I recommend the following solution (not tested):       My `pip freeze` output:     System:   - Python: v`3.9.12`   - OS: Windows 10 Enterprise (64-bit, Version `21H2`, Build `19044.1706`)  "
æˆ‘ä»¬æƒ³é€šè¿‡è’¸é¦çš„æ–¹å¼è®­ç»ƒä¸€ä¸ªæ¨¡åž‹ï¼Œçœ‹åˆ°æ‚¨åšè¿‡ç›¸å…³çš„å·¥ä½œï¼š    è¿˜æœ‰ä¸€ç§ç®€æ´é«˜æ•ˆçš„æ€è·¯ï¼Œå°±æ˜¯æ¨¡åž‹è’¸é¦ï¼Œç›´æŽ¥è’¸å¤§æ¨¡åž‹çš„è¾“å‡ºå°±å¥½äº†ï¼Œæˆ‘è¯•è¿‡è’¸å°æ¨¡åž‹ï¼Œå¾ˆæœ‰æ•ˆï¼Œå¾ˆå¿«å°±èƒ½è®­å¥½ã€‚    è¯·é—®æ‚¨ç”¨çš„å“ªä¸ªæ¨¡åž‹ä½œä¸ºteacheræ¨¡åž‹ä»¥åŠè’¸é¦ç›¸å…³çš„èŠ±è´¹æ–¹ç¨‹å¦‚ä½•è®¾ç½®ä¼šæ¯”è¾ƒå¥½ï¼Ÿ
"I'm training ROMP model with your code  It saves every validation process   but, I test the results with ""python -m romp.test"" code I got the different result as evaluation   I think they are same process, do you know why ?  Or did I do something wrong?   "
"Hi,    I am making changes to the ROMP and I would like to run the code to see the results. I do `python -m torch.utils.bottleneck romp.train --configs_yml='configs/v1_hrnet_3dpw_ft.yml'`.        When I do `python -m romp.train --configs_yml='configs/v1_hrnet_3dpw_ft.yml'` it works fine. But adding `torch.utils.bottleneck `causes error. How can I avoid this error?    Thanks,"
Hello. I am running your demo webcam.sh and I generated a cartoon character. I wonder are there any apis for changing the background?
"Hi,     I am trying to run Romp in distributed mode. I follow this  . Since there is no folder called `core` in the repository I replaced it with `romp`. However, when I run the code it raises the error that there is no file called `train.py`. How can I avoid this error?    Thanks"
"I have run some tests with different dancing video materials.  Sometimes it recognizes the wrong body size at some frames, because of difficult movements.  That doesn't look pretty.    So how about an calibration option to:  detect the body size at the beginning of an video or webcam session and set it fixed until the end."
"Hi team    first of all Â´great job !      I am currently trying to do some body part swaps project and came across your project.     I would like to know if its possible to get the body parts as shown in the following pictures.       !     I want to not only have a ""3D avatars"" but also have the body regions itself to cut away the regions I am not interested in for post processing     Is there any way to get them as shown in the pictures ? It doesn't have to be so detailed but it would be great if it was.   "
"Hello. Thanks for your work.  I successfully ran your demo webcam.sh, which generates a cartoon character like this.  <img width=""527"" alt=""Screenshot 2022-04-30 at 12 06 12"" src=""   I wonder how to generate such a character, and can I change it? Any there any references about it?    "
æ„Ÿè°¢æ‚¨ä¹‹å‰çš„æŒ‡å¯¼ï¼Œæˆ‘å·²ç»å¯ä»¥æ ¹æ®å•äººè§†é¢‘çš„å‚æ•°åœ¨unityä¸­é©±åŠ¨å•äººæ¨¡åž‹äº†~  å¯¹äºŽäººæ•°ä¸å˜çš„è§†é¢‘ï¼Œæˆ‘å‘çŽ°single_batch_resultsæ¯å¸§è¾“å‡ºçš„äººç‰©çš„å‚æ•°çš„æ•°ç»„æ˜¯å¯¹åº”çš„ï¼Œç¬¬ä¸€å¸§çš„single_batch_results[0]å’Œç¬¬äºŒå¸§çš„single_batch_results[0]æ˜¯åŒä¸€ä¸ªäººã€‚  ä½†æ˜¯å¯¹äºŽäººæ•°å˜åŒ–çš„è§†é¢‘ï¼Œè¯·é—®æˆ‘åº”è¯¥æ€Žä¹ˆçŸ¥é“æ¯å¸§è§†é¢‘ä¸­äººç‰©æ˜¯å¦‚ä½•å¯¹åº”çš„å‘¢~
"ä½ å¥½ï¼Œ  import numpy as np  import torch  test = np.load('./00000000.npz',allow_pickle=True)  print(test.files)  print(test['results']['smpl_thetas'])æˆ‘æƒ³ç”¨è¿™ä¸ªå–å€¼ï¼Œä¸ºä½•æŠ¥äº†è¿™ä¸ªé”™  IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices  "
"Hey!  Thanks for your work!  I have a question: can I continue training the model from the epoch before which the model was trained?    I mean, I started training from scratch, after the training was interrupted, and after a while, can I continue training (maybe using existed .pkl file)?    I'm trying to connect mobilenet_v2 as a backbone, but it needs distillation and experimentation to improve. But the training process is long, so I want to be able to train up to epoch 10 (for example), and after other experiments, continue training from epoch 10 (the same model and with the same source code as before)"
"I try to run the code, but when i want to run smpl_parser there're no key values ""outputs['smpl_betas'], outputs['smpl_thetas']"",do I miss somthing? thanks for the answer"
ä½ å¥½ï¼Œæœ‰æ‰¹å¤„ç†è§†é¢‘çš„æ–‡ä»¶å—
ä½ å¥½ï¼Œæ€Žä¹ˆè§£å†³è¿™ä¸ªé—®é¢˜ï¼Ÿ
"I tried to test the dataset loading process as described in    `python -m romp.lib.dataset.lsp`    It raised an error as below:  `visualizer = Visualizer(resolution = (512,512,3), result_img_dir=save_dir,with_renderer=True)  TypeError: __init__() got an unexpected keyword argument 'with_renderer'`    This error occured in the line      Initialization function of visualizer does not have such argument:      Is this an obsolete argument or did I miss something?"
"ä½œè€…æ‚¨å¥½ï¼Œæˆ‘åœ¨è¿è¡Œæ‚¨çš„æ¨¡åž‹æ—¶ï¼Œè¾“å‡ºmeshç­‰åŠŸèƒ½æ­£å¸¸ï¼Œä½†æ˜¯è¾“å‡ºcenter_confæ—¶ä¼šæŠ¥é”™ï¼Œè¯·é—®è¿™æ˜¯å¦å’Œå›¾ç‰‡æ ¼å¼æœ‰å…³ï¼Ÿæˆ‘å¯ä»¥æ€Žæ ·è°ƒæ•´åŽ»è§£å†³ï¼Ÿ  å…·ä½“æŠ¥é”™å¦‚ä¸‹ï¼š  (base) wangyang@wangyangdeMacBook-Pro ~ % romp --mode=image --calc_smpl --render_mesh --input=/Users/wangyang/Downloads/3323.JPG --save_path=/Users/wangyang/Downloads/893.JPG --show_items=center_conf  Using ROMP v1  /Users/wangyang/opt/anaconda3/lib/python3.8/site-packages/romp/post_parser.py:34: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').    topk_ys = (topk_inds.long() // w).float()  /Users/wangyang/opt/anaconda3/lib/python3.8/site-packages/romp/post_parser.py:39: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').    topk_clses = index.long() // K  /Users/wangyang/opt/anaconda3/lib/python3.8/site-packages/romp/post_parser.py:144: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').    parsed_results['centers_pred'] = torch.stack([flat_inds%64, flat_inds//64],1) * 512 // 64  Traceback (most recent call last):    File ""/Users/wangyang/opt/anaconda3/bin/romp"", line 8, in        sys.exit(main())    File ""/Users/wangyang/opt/anaconda3/lib/python3.8/site-packages/romp/main.py"", line 179, in main      outputs = romp(image)    File ""/Users/wangyang/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1110, in _call_impl      return forward_call(*input, **kwargs)    File ""/Users/wangyang/opt/anaconda3/lib/python3.8/site-packages/romp/utils.py"", line 655, in wrap_func      result = func(*args, **kwargs)    File ""/Users/wangyang/opt/anaconda3/lib/python3.8/site-packages/romp/main.py"", line 167, in forward      outputs = rendering_romp_bev_results(self.renderer, outputs, image, rendering_cfgs)    File ""/Users/wangyang/opt/anaconda3/lib/python3.8/site-packages/vis_human/main.py"", line 101, in rendering_romp_bev_results      cv2.putText(result_image[1], '{:.3f}'.format(outputs['top_score'][ind]), tuple(kp.astype(int)), cv2.FONT_HERSHEY_COMPLEX,1,(255,0,255),1)    IndexError: list index out of range"
"æˆ‘è®¤ä¸ºè¯¥å‚æ•°æ˜¯æ–¹ä¾¿æœåŠ¡å™¨å¯ç”¨severè°ƒç”¨æœ¬åœ°æ‘„åƒå¤´ï¼Œè¯¥å‚æ•°éœ€è¦ä½¿ç”¨   æ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶å†…      run_server(server_host, server_port)  # first, run this function only in server      # run_client(server_host, server_port)  # then, run this function only in clientï¼Œè¯·é—®åœ¨å®¢æˆ·ç«¯æ˜¯ç›´æŽ¥å•ç‹¬è¿è¡Œè¯¥.pyæ–‡ä»¶å—ï¼Ÿæ˜¾ç¤ºæœ‰é”™è¯¯ï¼Œè¯¥å‡½æ•°æ‰¾ä¸åˆ°"
"  Hello, I am using coco dataset to train, after 19 epochs of training I get this error!   "
"ä½œè€…æ‚¨å¥½ï¼Œæˆ‘å°è¯•ç”¨center cropçš„å›¾åƒä½œä¸ºè®­ç»ƒçš„è¾“å…¥ã€‚æˆ‘æ”¹åŠ¨äº†å¦‚ä¸‹å†…å®¹ï¼š  1ï¼‰Image_baseä¸­get_item_single_frameï¼Œå¯¹ç›®æ ‡ä¸­å¿ƒåšcenter cropã€‚  2ï¼‰all_person_detected_maskï¼Œæ‰€æœ‰sampleéƒ½æ˜¯true  3ï¼‰max_personä¸º1ï¼Œmulti_personä¸ºfalse  4ï¼‰è®­ç»ƒé›†h36m,mpiinf,mpii,lsp,crowdpose,pw3d   ç”¨pretrained model finetune  lossä¸­MPJPEå’ŒPAMPJPEåŸºæœ¬ä¸é™(ç»´æŒåœ¨200å·¦å³)ï¼Œè®­ç»ƒå›¾ç‰‡å¯è§†åŒ–ä¸­ï¼Œèƒ½çœ‹åˆ°mpiinfé¢„æµ‹ç»“æžœè¶Šæ¥è¶Šå·®ï¼Œè®­ç»ƒå‡ ä¸ªepochåŽè¿™ä¸ªæ•°æ®é›†çš„ç»“æžœå°±å®Œå…¨ä¸å¯¹äº†ï¼Œä½†å…¶ä»–æ•°æ®é›†çœ‹ä¸ŠåŽ»æ²¡ä»€ä¹ˆé—®é¢˜ã€‚  ç„¶åŽï¼Œæˆ‘å°†mpiinfæ•°æ®é›†åŽ»æŽ‰è®­ç»ƒï¼Œlosså’Œå¯è§†åŒ–ç»“æžœçœ‹ä¸ŠåŽ»éƒ½æ­£å¸¸ã€‚    æ•´å›¾è®­ç»ƒï¼Œç”¨mpiinfï¼ŒMPJPEå’ŒPAMPJPEæ˜¯åœ¨ä¸‹é™çš„ï¼Œä½†æ˜¯cropå›¾æ—¶ï¼Œåªæœ‰è¿™ä¸ªæ•°æ®é›†é™ä¸ä¸‹æ¥ï¼Œæ‚¨çŸ¥é“åŽŸå› ä¸?  "
"How to get this .npy file, only find the .JSON file in the dataset. Is this automatically generated based on the code?"
None
è¯·é—®å¦‚æžœæˆ‘è‡ªå·±çš„æ•°æ®é›†ä¸­å¹¶ä¸æ˜¯æŒ‰ç…§SMPLçš„24ä¸ªå…³é”®ç‚¹ï¼Œæ¯”å¦‚æˆ‘æœ‰26ä¸ªå…³èŠ‚ï¼Œä¸ŽSMPLä¸­çš„24ä¸ªä¸åŒï¼Œå¯ä»¥ç”¨æ‚¨çš„æ–¹æ³•ç”Ÿæˆæœ€åŽçš„æ—‹è½¬å‚æ•°poseså—
çŽ°åœ¨512x512çš„å¤§æ¦‚cpuéœ€è¦100msï¼Œæˆ‘æƒ³å°è¯•ä¸€ä¸‹é™ä½Žåˆ†è¾¨çŽ‡ï¼Œæƒ³è¯·æ•™ä¸‹ä½œè€…æœ‰æ²¡æœ‰ç±»ä¼¼çš„å°è¯•ï¼Ÿ
"In image_dataset.py, if we activate shuffle_mode and then the image will be cropped. Now if a person is cropped out of the region, the 2d kps will mostly fall out of the image region, and these outlier 2d keypoints will be updated to (-2 , -2)    However, if there is one 2d keypoint still remaining inside the image, for example a head point like this:  !     Then the 2d keypoints of this person on the right will be updated to [[-2, -2], [-2, -2], ... [x_head, y_head], ...[-2, -2]]. So the 2d loss will only be calculated for head joint.    However, in the current code, the 3d keypoints is not synchronized with 2d joints, all of the 3d joints (whole body) will still be learned, is there any reason behind it? Are you meaning to give the network to infer the 'invisible' part of the body?  "
æ„Ÿè°¢æ‚¨æä¾›çš„ROMPæ–¹æ³•ï¼æˆ‘å°è¯•ä½¿ç”¨æä¾›çš„æ–¹æ³•ï¼Œæ•ˆæžœå¾ˆæ£’ï¼ä½†æ˜¯åœ¨æŸäº›åœºæ™¯ä¸‹å‡ºçŽ°äº†äººä½“æ— æ³•è¯†åˆ«çš„æƒ…å†µã€‚å¦‚å›¾ä¸­ï¼Œçº¢è‰²ç®­å¤´æ‰€æŒ‡è¢«è¯•è€…åœ¨è¿›è¡Œå¤§å¹…å‰å±ˆåŠ¨ä½œæ—¶ã€‚    ç”±äºŽæˆ‘ä¸æ˜¯è®¡ç®—æœºç›¸å…³ä¸“ä¸šçš„ï¼Œæ‰€ä»¥æƒ³è¯·é—®ä¸€ä¸‹ï¼Œè¿™æ˜¯ä»€ä¹ˆåŽŸå› å‘¢ï¼Ÿæ˜¯å¦æœ‰è§£å†³æ–¹æ³•ï¼Ÿ  !   
"I am missing an notice about the purpose of training `romp` by myself.  For example: Are there any benefits of usage, if i train it again by myself instead of using the attached dataset?    Would be great to get some guidelines about this question."
"a) I was not able to found an notice about the detailed differences and benefits of 'simple-romp' against 'romp'.  Only the name implies, that `simple-romp` might be easier to use.  In which situations should i use `romp` or `simple-romp`?    b) But how about dependecies like `pytorch3d` and all requirements?  How to use it with `conda env` instead of plain `pip`?    Please add further information, if possible."
"  In the paper, it says that fine tuned model shows the best performance at 120Epoch, but after running the v1_hrnet_3dpw_ft.yml file, it showed the best performance at 1 epoch. Did I do something wrong?"
æœ‰ä»€ä¹ˆåŽŸå› å’Œä¼˜åŒ–æ€è·¯å‘¢ï¼Ÿäººä½“æ¨¡åž‹é©±åŠ¨ç³»ç»Ÿé©±åŠ¨å‡ºæ¥çš„åŠ¨ä½œå°±æ¯”è¾ƒè‡ªç„¶ï¼Œæˆ‘å‘çŽ°å¾ˆå¤šsmplxç‰ˆæœ¬çš„æ•°å­—äººä½“ï¼Œå‡ºæ¥çš„fbxï¼ŒåŠ¨ç”»åŠ¨ä½œéƒ½ä¼šæœ‰æŠ–åŠ¨å’Œä¸è¿žè´¯ï¼Œä¸è¿žè´¯æ˜¯é‡‡æ ·é—®é¢˜ï¼ŸæŠ–åŠ¨åº”è¯¥æ˜¯æ²¡åšåŠ¨åŠ›å­¦é™åˆ¶ï¼Ÿ
"   å¯ä»¥å¸®å¿™çœ‹ä¸€ä¸‹è¿™ä¸ªmobilenetv3çš„è®­ç»ƒæ—¥å¿—å—ï¼Ÿ  ä»–çš„validation æŒ‡æ ‡ä¸‹é™åˆ°140&90é™„è¿‘å°±æ„Ÿè§‰å¡ä½äº†ï¼Œè€Œä¸”æˆ‘æŠŠ30epochçš„æ¨¡åž‹æ‹¿å‡ºæ¥æµ‹è¯•äº†ä¸€ä¸‹ï¼Œå‘çŽ°SMPL bodyä¼šé£˜åœ¨äººç‰©ä¸Šé¢ï¼Œæˆ‘çš„æƒ³æ³•æ˜¯è®­ç»ƒçš„æ—¶å€™body center heatmapåº”è¯¥æ˜¯æœ€å…ˆæ”¶æ•›çš„ï¼Œæ‰€ä»¥å³ä¾¿poseä¸Šè¯¯å·®æ¯”è¾ƒå¤§ï¼Œdet lossåº”è¯¥æ”¶æ•›ï¼Œä¹Ÿå°±æ˜¯ä¸ä¼šäº§ç”Ÿè¿™ç§é£˜åœ¨äººç‰©ä¸Šæ–¹çš„çŽ°è±¡ï¼Œä¸çŸ¥é“æ‚¨è®­ç»ƒçš„æ—¶å€™æœ‰æ²¡æœ‰è¿™ç§é—®é¢˜ï¼Œå¸Œæœ›æ‚¨èƒ½ç»™å‡ºå»ºè®®ï½ž    æˆ‘åœ¨resnetçš„åŸºç¡€ä¸Šå°†batch_size è°ƒæ•´ä¸ºäº†128ï¼Œlr ç›¸åº”x2 ä¸º 0.0001  å›¾ç¤ºï¼š  <img width=""735"" alt=""image"" src=""   "
"     Here, I guess you meant to calculate a bounding box for a single person, based on the visible kpts and then expend the box.    However, when you do `leftTop, rightBottom = np.clip(box       **You can reproduce the results very quick, by copying the code below and use this image, it's exactly the same as your code, you can put it directly into your main() in augments.py**, I pre-defined 5 kpts based for the person (red dots):  !   kps2d = np.array(       By the way, I think that we can choose full_kps within the range: `valid_range = (full_kp2ds[:, :, 2]>0).sum(-1) > 2` instead of randomly choosing one from full_kp2ds and then judging whether it has `if (kps_vis[:,2]>0).sum()>2` or not:     "
Helloï¼Œauthorï¼  I'm learning romp.I reconstructed a human 3D model from the image. Now I find a two-dimensional coordinate point on the person in the image. I want to know how to find the corresponding three-dimensional coordinate point on the human 3D model. I think it should be to obtain the camera parameters. I don't know how to obtain it on romp? Can you give me some adviceï¼Œthanksï¼
"Thank Arthur Sun for sharing this wonderful and great work!    ROMP shows great performance with Blender, and I just wonder that is there a way to drive characters in Unreal Engine other than Blender in real-time?    Thank you very much.  "
"Hi, I read your code for Panoptic evaluation and want to make sure that the mesh parameter map information is fetched directly from the GT and not from the center map information , right?  code in romp/lib/maps_utils/result_parser.py line 97-100"
æ‚¨æä¾›çš„h36mæ•°æ®é›†æ˜¯311124å¼ å›¾ç‰‡ç”¨æ¥è®­ç»ƒï¼Œ106648å¼ ç”¨æ¥æµ‹è¯•ï¼Œåœ¨imagesæ–‡ä»¶å¤¹ä¸­ä¸€å…±æ˜¯417772å¼ å›¾ç‰‡ã€‚  Q1ï¼šhuman3.6mçš„ ä¸­Fig2ï¼ˆaï¼‰ä¸­å¯è§ï¼Œtrainã€valã€testä¸€å…±æ˜¯3640788å¼ å›¾ç‰‡ã€‚è¯·é—®æ‚¨çš„417772å¼ å›¾ç‰‡æ˜¯å¦‚ä½•é€‰å–å’Œåˆ’åˆ†çš„ï¼Ÿ  Q2ï¼šhuman3.6mçš„ ä¸­çœ‹åˆ°ï¼Œåªæœ‰å¸¦è¡£æœçš„meshæ²¡æœ‰ä¸å¸¦è¡£æœçš„SMPL meshï¼Œè¯·é—®æ‚¨çš„meshæ•°æ®æ˜¯å¦‚ä½•ç”Ÿæˆçš„ï¼Ÿ
   Q1:è®­ç»ƒåˆ°ä¸€åŠçš„æ—¶å€™ï¼Œç”±äºŽOOMæˆ–å…¶ä»–åŽŸå› ï¼Œå¯¼è‡´è®­ç»ƒç»ˆæ­¢ï¼Œå¦‚ä½•è®¾ç½®å‚æ•°ï¼Œä½¿å¾—å¯ä»¥ç»§ç»­è®­ç»ƒï¼Ÿ  Q2ï¼šå³ä½¿æˆåŠŸè®­ç»ƒå®Œæˆ120ä¸ªepochï¼Œå¦‚æžœå‘çŽ°lossè¿˜æ²¡æœ‰æ”¶æ•›ï¼Œé‚£ä¹ˆå¦‚ä½•ç»§ç»­è®¾ç½®å‚æ•°ï¼Œä½¿å¾—å…¶ç»§ç»­è®­ç»ƒæ›´å¤šçš„epochï¼Ÿ  
ä½œè€…æ‚¨å¥½ï¼Œæ‚¨æä¾›çš„ROMP_HRNet32_V1.pklæ˜¯å¦æ˜¯é€šè¿‡v1.ymlåœ¨pretrain_hrnet.pklçš„åŸºç¡€ä¸Šè®­ç»ƒå¾—åˆ°çš„å—ï¼Ÿ
"Hi,I tried your code on some images obtained from web-cam as well as some images from yoga-82 dataset(on which mediapipe) was trained.  Since,the pose changes are extreme,it seems the model is not able to get a correct fit.  Any reaons on why that is happening  results of fitting romp   !   !   !     original images  !       !   m/34626942/148653266-6fa10108-7b54-42b9-b6cc-8de280b8992e.jpg)  ?  I hope you can give me some insight into what can be done to improve the fitting ,should i tune the parameters in the config file?"
None
"Hi, I have difficulty training for the 6 dataset(mpiinf, coco, mpii, lsp, muco, crowdpose). The training code can run successfully for a period of time (not more than some epoch) and then will encounter this error in the training logs file.  Can you give me a solution for this?    >6 epoch    In 6 epoch , the Losses can be output, but the ""INFO:root:Evaluation on pw3d"" is nan value.    !   !         >7 epoch    In 7 epoch, the log is "" PA_MPJPE calculation failed! svd_cuda: (Batch element 0): The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated singular values (error code: 55)"".    !       "
"Hi, brother, I run convert blender fbx script got this error:       why this error happens?"
"å¤§ä½¬æ‚¨å¥½ï¼Œæ„Ÿè°¢æ‚¨æ‰€åšçš„å·¥ä½œï¼Œç›®å‰é‡åˆ°äº›é—®é¢˜æƒ³å‘æ‚¨è¯·æ•™ï¼Œå¸Œæœ›æ‚¨æä¾›æ€è·¯ã€‚    å½“åªä½¿ç”¨save_dict_resultsé‡Œé¢ä¿å­˜çš„smpl 85ç»´å‚æ•°å®žçŽ°å¯è§†åŒ–æ—¶ï¼Œcamç›¸æœºç»“æžœä¸å‡†ï¼Œä½¿ç”¨ä»¥ä¸‹å€¼ï¼š    -  cam (3,) # 3 camera parameters of weak-perspective camera, (scale, tranlation_x, tranlation_y)    -  pose (72,) # 72 SMPL pose parameters.    -  betas (10,) # 10 SMPL shape parameters.    æ›´æ”¹smpl_mesh_root_align: Falseæ²¡ç”¨ï¼Œcamã€poseã€betaså€¼å¹¶æ²¡æœ‰æ”¹å˜ï¼Œæ‰€ä»¥ç»“æžœè¿˜æ˜¯ä¸å¯¹ã€‚    æ•ˆæžœå¦‚ä¸‹ï¼š               "
"!     Hi, default is 1960x1960, I make scaled to 1 rather than 2.  I want it to be rectangle window, how to controll it?"
"æ‚¨å¥½ï¼Œè¯·æ•™ä¸€ä¸‹å…³äºŽconvert_fbx.pyä¸­å†™å…¥ä½ç§»çš„ä»£ç   bones[bone_name_from_index[0]].location = Vector((100*trans[1], 100*trans[2], 100*trans[0])) - pelvis_position  è¿™é‡Œçš„é¡ºåºä¸ºä»€ä¹ˆæ˜¯trans[1][2][0]å‘¢ï¼Ÿä¸æ˜¯æŒ‰XYZçš„é¡ºåºå—"
    Could you provide a complete test code for convert_to_bvh.pyï¼ŸVery Thanks.
"Hi. I am trying to use your prediction to control the animator in blender. However, I met a problem when I am running ""webcam_blender.sh""    The ""webcam.sh"" demo can be run successfully. However, when I ran ""webcam_blender.sh"", the program keeps blocked in the accepting signal operation,      , and the program will not move forward.    It's multi-threading needed for accepting the signal?"
è¯·é—®ä½œè€…ä½¿ç”¨4å¼ p40gpuè®­ç»ƒå®Œæ•´çš„æ•°æ®é›†éœ€è¦å¤šä¹…ï¼Ÿ
"Dear Arthur151, when test video on your method, last frame always gets mistake, hope you reply.    this is issue:        pygame 2.0.0 (SDL 2.0.12, python 3.7.9)  Hello from the pygame community.    /home/neu307/miniconda3/envs/env_centerhmr/lib/python3.7/site-packages/quaternion/numba_wrapper.py:21: UserWarning:     !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  Could not import from numba, which means that some  parts of this code may run MUCH more slowly.  You  may wish to install numba.  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!      warnings.warn(warning_text)  INFO - 2021-11-16 15:56:39,929 - acceleratesupport - No OpenGL_accelerate module loaded: No module named 'OpenGL_accelerate'  INFO - 2021-11-16 15:56:40,399 - base - {'tab': 'hrnet_cm64_test', 'configs_yml': 'configs/video.yml', 'demo_image_folder': 'None', 'local_rank': 0, 'model_version': 1, 'multi_person': True, 'collision_aware_centermap': False, 'collision_factor': 0.2, 'kp3d_format': 'smpl24', 'eval': False, 'max_person': 64, 'input_size': 512, 'Rot_type': '6D', 'rot_dim': 6, 'centermap_conf_thresh': 0.25, 'centermap_size': 64, 'deconv_num': 0, 'model_precision': 'fp32', 'backbone': 'hrnet', 'gmodel_path': '../trained_models/ROMP_hrnet32.pkl', 'print_freq': 50, 'fine_tune': True, 'gpu': '0', 'batch_size': 64, 'val_batch_size': 1, 'nw': 4, 'calc_PVE_error': False, 'dataset_rootdir': '/home/neu307/wh/dataset/', 'high_resolution': True, 'save_best_folder': '/home/neu307/wh/checkpoints/', 'log_path': '/home/neu307/wh/log/', 'total_param_count': 85, 'smpl_mean_param_path': '/home/neu307/wh/ROMP-master/models/satistic_data/neutral_smpl_mean_params.h5', 'smpl_model': '/home/neu307/wh/ROMP-master/models/statistic_data/neutral_smpl_with_cocoplus_reg.txt', 'smplx_model': True, 'cam_dim': 3, 'beta_dim': 10, 'smpl_joint_num': 22, 'smpl_model_path': '/home/neu307/wh/ROMP-master/models', 'smpl_uvmap': '/home/neu307/wh/ROMP-master/models/smpl/uv_table.npy', 'smpl_female_texture': '/home/neu307/wh/ROMP-master/models/smpl/SMPL_sampleTex_f.jpg', 'smpl_male_texture': '/home/neu307/wh/ROMP-master/models/smpl/SMPL_sampleTex_m.jpg', 'smpl_J_reg_h37m_path': '/home/neu307/wh/ROMP-master/models/smpl/J_regressor_h36m.npy', 'smpl_J_reg_extra_path': '/home/neu307/wh/ROMP-master/models/smpl/J_regressor_extra.npy', 'kernel_sizes': [5], 'GPUS': 0, 'use_coordmaps': True, 'video_or_frame': True, 'input_video_path': '../demo/test_videos/C1/C1_single_2.mp4', 'webcam_mesh_color': 'LightCyan', 'save_mesh': False, 'save_centermap': False, 'save_dict_results': True, 'webcam': False, 'fps_save': 30, 'multiprocess': False}  INFO - 2021-11-16 15:56:40,399 - base - ------------------------------------------------------------------  INFO - 2021-11-16 15:56:40,399 - base - start building model.  Using ROMP v1  INFO - 2021-11-16 15:56:43,931 - train_utils - using fine_tune model: ../trained_models/ROMP_hrnet32.pkl  INFO - 2021-11-16 15:56:44,085 - base - finished build model.  Initialization finished!  Running the code on video  ../demo/test_videos/C1/C1_single_2.mp4  Processing video 0/164  Processing video 1/164  Processing video 2/164  Processing video 3/164  Processing video 4/164  Processing video 5/164  Processing video 6/164  Processing video 7/164  Processing video 8/164  Processing video 9/164  Processing video 10/164  Processing video 11/164  Processing video 12/164  Processing video 13/164  Processing video 14/164  Processing video 15/164  Processing video 16/164  Processing video 17/164  Processing video 18/164  Processing video 19/164  Processing video 20/164  Processing video 21/164  Processing video 22/164  Processing video 23/164  Processing video 24/164  Processing video 25/164  Processing video 26/164  Processing video 27/164  Processing video 28/164  Processing video 29/164  Processing video 30/164  Processing video 31/164  Processing video 32/164  Processing video 33/164  Processing video 34/164  Processing video 35/164  Processing video 36/164  Processing video 37/164  Processing video 38/164  Processing video 39/164  Processing video 40/164  Processing video 41/164  Processing video 42/164  Processing video 43/164  Processing video 44/164  Processing video 45/164  Processing video 46/164  Processing video 47/164  Processing video 48/164  Processing video 49/164  Processing video 50/164  Processing video 51/164  Processing video 52/164  Processing video 53/164  Processing video 54/164  Processing video 55/164  Processing video 56/164  Processing video 57/164  Processing video 58/164  Processing video 59/164  Processing video 60/164  Processing video 61/164  Processing video 62/164  Processing video 63/164  Processing video 64/164  Processing video 65/164  Processing video 66/164  Processing video 67/164  Processing video 68/164  Processing video 69/164  Processing video 70/164  Processing video 71/164  Processing video 72/164  Processing video 73/164  Processing video 74/164  Processing video 75/164  Processing video 76/164  Processing video 77/164  Processing video 78/164  Processing video 79/164  Processing video 80/164  Processing video 81/164  Processing video 82/164  Processing video 83/164  Processing video 84/164  Processing video 85/164  Processing video 86/164  Processing video 87/164  Processing video 88/164  Processing video 89/164  Processing video 90/164  Processing video 91/164  Processing video 92/164  Processing video 93/164  Processing video 94/164  Processing video 95/164  Processing video 96/164  Processing video 97/164  Processing video 98/164  Processing video 99/164  Processing video 100/164  Processing video 101/164  Processing video 102/164  Processing video 103/164  Processing video 104/164  Processing video 105/164  Processing video 106/164  Processing video 107/164  Processing video 108/164  Processing video 109/164  Processing video 110/164  Processing video 111/164  Processing video 112/164  Processing video 113/164  Processing video 114/164  Processing video 115/164  Processing video 116/164  Processing video 117/164  Processing video 118/164  Processing video 119/164  Processing video 120/164  Processing video 121/164  Processing video 122/164  Processing video 123/164  Processing video 124/164  Processing video 125/164  Processing video 126/164  Processing video 127/164  Processing video 128/164  Processing video 129/164  Processing video 130/164  Processing video 131/164  Processing video 132/164  Processing video 133/164  Processing video 134/164  Processing video 135/164  Processing video 136/164  Processing video 137/164  Processing video 138/164  Processing video 139/164  Processing video 140/164  Processing video 141/164  Processing video 142/164  Processing video 143/164  Processing video 144/164  Processing video 145/164  Processing video 146/164  Processing video 147/164  Processing video 148/164  Processing video 149/164  Processing video 150/164  Processing video 151/164  Processing video 152/164  Processing video 153/164  Processing video 154/164  Processing video 155/164  Processing video 156/164  Processing video 157/164  Processing video 158/164  Processing video 159/164  Processing video 160/164  Traceback (most recent call last):    File ""/home/neu307/wh/ROMP-master/src/core/test.py"", line 235, in        main()    File ""/home/neu307/wh/ROMP-master/src/core/test.py"", line 224, in main      demo.process_video(args.input_video_path)    File ""/home/neu307/wh/ROMP-master/src/core/test.py"", line 114, in process_video      outputs = self.single_image_forward(frame)    File ""/home/neu307/wh/ROMP-master/src/core/test.py"", line 98, in single_image_forward      meta_data = img_preprocess(image, '0', input_size=args.input_size, single_img_input=True)    File ""/home/neu307/wh/ROMP-master/src/core/../lib/models/../utils/demo_utils.py"", line 31, in img_preprocess      image = image[:,:,::-1]  TypeError: 'NoneType' object is not subscriptable    Process finished with exit code 1  "
"Hi and thanks for great work !    I would like to finetune the model on some custom 2D pose dataset. From what I read you trained on 3D and 2D pose datasets so that should be possible, right ?    Best  Alexander"
"Hi,     I am wondering if there is a way to use the results as training data for this model itself.     Thank you!"
"wonderful Project! thank you very much.  I found that you have vedo and open3d visualizer which have texture render , but for pytorch3d you only render a simple color. I found the pytorch3d official web and can not find a good way to how to use. I think maybe this is easier for you :     I think only need  to change the textures"
"1 romp/lib/dataset/image_base.py    ä½ å¥½ è¯·é—®ä¸‹ï¼Œvalid_masks   = np.zeros((self.max_person, 6), dtype=np.bool)  ç¬¬äºŒç»´åº¦ä¸€å…±æ˜¯6ç§ç±»åž‹ï¼Œä¸çŸ¥é“523è¡Œè¿™é‡Œä¸ºä»€ä¹ˆæœ‰åˆ¤æ–­ï¼Œå¹¶ä¸”ä¸‹æ ‡æ˜¯6ï¼Œè¶…è¿‡äº†valid_masksçš„èŒƒå›´ã€‚  !     !    "
"Thanks for sharing, there are some questions about the dataset.Some datasets, such as coco, LSP, use pseudo 3D annotations. Will you share these pseudo-annotations, such as data/eft_fit/ coco2014-all-ver01.json mentioned in the code"
"Thanks for your outstanding work and repo!    In your paper, you have noticed that AICH, UP and OH datasets are also included during training (and the dataloaders are provided). However, it seems that they are missing in the data preparation guide and cannot be downloaded.     Is it possible that you can provide these data as well?     Thanks again!"
How can 2-D pose estimation be realized in real timeï¼ŒWhether to use Openposeï¼ˆ  any other wayï¼ŒI found this when I was testing  !       What measures do I need to take to prevent this from happeningï¼ŒLooking forward to your reply
"Hi, thanks for your generous sharing of this wonderful work, and I have tried to test on my own video.  when only look the mesh that projected onto the image,  the results look good, but when converting to fbx and see in blender, although the pose looks normal, the root motion has big problems like drifting and jitter.    It seems this root motion problem are common in methods based on smpl, and I have noticed that the  and   have given some methods to improve the performance. but they are time-consuming.    I wonder if there may be a way to do some work combine these two works or you have some other ideas, thanks~"
"Hi authors,    First of all, I would like to thank you so much for your great work ROMP!    I am facing issues related to training. Could you share the code for generating the ground truths for training the models?    Thank you in advance!"
"Hello!     Firstly, greatly appreciate all the work you have put into this project.    I would have two questions for you:    1. I want to import an .fbx into Unity but the axis are wrong. The fbx was converted using the Blender python script you have provided. Even in Blender the fbx has wrong axis. If I try to rotate the Armature in Blender, when I play the animation, it gets back to the original wrong pose. In Unity it seems that it should be rotated with 90 degrees around the Z axis and in Blender it should be rotated 90 degrees around the Y Axis.     2. Also, is it possible for us to remove the camera distance from the FBX? As you can see in the Blender picture, the armature is translated with some distance from the center of the world, would it be possible to center it?     Thank you!   !   !   "
"Hello! I have recently run your work. I want to get one main person from my video. However there are some other persons showing up during the video sometimes. How should i set the configs? I have tried set ""multi_person"" to False, but seems it won't work.  It would be so nice if you could help me here. Thank you.  "
"Hello @Arthur151,  Congratulations on the great work and Thanks for making the code available for use.    I tried running the inference script on a custom video via:   CUDA_VISIBLE_DEVICES=0 python core/test.py --gpu=0 --configs_yml=configs/video.yml    But the result is strange..   Is this because of the camera view ??  If yes then, Is there any way to tune the model for such camera views??  ! "
The model runs perfectly but I noticed there is a lot of jittering in the output. Is there any way to remove the jittering?
"Hi, is it possible if you can post a step-by-step tutorial on how to run and install the program? Thank you in advance."
"I haven't read the source code yet.  I ran the demo, and found that the head pose estimation is not supported, which confirmed in the results of the experimental section of paper.  Would you like to know whether the support for head pose estimation will be increased in the future?  Thanks."
I'm trying to animate 3D character in Unity using SMPL 24 joints data. Everything is going well but the only problem is that there is different in Unity and SMPL final data axis.     Below axis are from SMPL data (I draw 3D skeleton using 24 joints from CenterHMR)  !     Below image is about axis in Unity  !     Seems that we need to change the Y and Z axis in SMPL results.     So suggest me that we should swap/change axis (Y and Z) in `j3d_smpl24` list that I got from CenterHMR from   or in Unity side?
  you can add a function to export to bvh and use in blender
"Hey @Arthur151,  I was able to understand the joint positions and order in SMPL24 and I know that the pose (72,) is the rotation values for 24 joints in SMPL.  I am trying to convert the output into a BVH file, but I don't understand, how to use these rotation values while converting into BVH. Do i need to change the format of the joint rotations. If yes, then to which format and how?  Can you please guide me on this?  "
First of all thanks @Arthur151. I'm able to generate 3D meshes for my videos however I'm curious to know if we can also run this on CPU. it would be very kind of you if you can guide me on this. 
"Poor results on some images, one of the examples is attached below:      !     How can we improve it"
"I removed Coordconv when training centermap task independently, the training loss similar to the network with Coordconv, but the centermap seems to get bad on the test set."
"We use 256 image input to your trained 512 input model, and adjust the coordconv size, but the result seems not right. Does your network need the same input size on train and test stage?"
"I would like to visualize the body landmarks projected onto the 2d image however I have some problems understanding how the values are stored. When I run the demo script I get a file called .npz and I can load it with numpy. Inside it I found f->results where two dictionaries are stored one for each person in the image. Inside each dictionary I can see 'cam' (3,), 'pose' (72,), 'betas' (10,), 'j3d_smpl24' (24, 3), 'j3d_op25' (25, 3), and 'verts' (6890, 3) variables (I put their corresponding shapes in parentheses). From your paper I thought 'pose' variable would be of length 132 (22 landmarks x 6D). Anyway, I assume you have saved 24 landmarks in 3D which makes an array of length 72. From the 'cam' variable (tx, ty = cam[1:]) I calculated the center for each people trivially since those numbers are normalized between -1 and 1 just as you mentioned in your paper. However when it comes to visualizing 'pose', 'j3d_smpl24' or 'verts' variables I ran into some problems. Could you explain how each of these variables store their data? Apparently they are not normalized between -1 and 1. Also I'm having some problems understanding the first number in 'cam' variable which should correspond to scale according to the paper. In the paper it is said that this variable ""reflects the size and depth of the human body to some extent."". How can this scale be used to visualize pose points? In my example I get 5.51 for one person and -5.156 for the other. Would you also explain what does a negative scale represent?"
"After runing the demo code, I can get the .obj and .npz files.   How to put these files in MAYA for further usage.    Thanks"
"Thanks for sharing the nice work!  This monocular method achieved impressive results, but in my opinion, multi-view will definitely improve the final performance.  Do you have any thoughts to upgrade ROMP to multi-view?  "
æ‚¨å¥½ï¼Œæƒ³æµ‹è¯•ç›¸å…³å·¥ä½œçš„å®žé™…è¡¨çŽ°ï¼Œæˆ‘ç”¨æ‰‹æœºç›´æŽ¥æ‹æ‘„äº†ä¸€äº›è§†é¢‘ï¼Œç”¨ROMPè¿›è¡Œé¢„ä¼°ï¼Œä½†æ˜¯æ¸²æŸ“å‡ºæ¥çš„ç»“æžœæŠ–åŠ¨æ¯”è¾ƒæ˜Žæ˜¾ï¼Œå¹¶ä¸”æ‰‹éƒ¨ä¸Žèº«ä½“æŽ¥è§¦çš„æ—¶å€™æœ‰ç©¿æ¨¡çš„çŽ°è±¡ï¼Œä½†äººä¾§é¢è¡Œèµ°çš„æ—¶å€™ï¼Œè¢«é®æŒ¡çš„æ‰‹ä¼šé¢„ä¼°é”™è¯¯ã€‚æƒ³è¯·é—®æ˜¯å¦è¾“å…¥çš„è§†é¢‘æœ‰ä¸€å®šçš„æ‹æ‘„è¦æ±‚ä»¥åŠé¢„å¤„ç†ï¼ˆæ¯”å¦‚å›¾åƒçš„å¤§å°ç­‰ï¼‰ï¼Œæˆ–è€…æ˜¯å¦éœ€è¦æ·»åŠ ä¸€äº›åŽå¤„ç†è¿›è¡Œæ”¹å–„ï¼Ÿ æ„Ÿè°¢ï¼ŒæœŸå¾…å›žå¤ï¼
"Hello, thanks for your excellent job. Your illustratement is quite helpful, especially about the training dataset. I want to train ROMP by myself. I wonder if the datasets in the directory structure are the whole training datasets. Since I found that UPã€AICHã€OH are listed on the paper, none of them appear in the directory. Need we download them by ourselves or they have been uploaded in the google drive you provide? Thank you for your help. "
"Hello, thank you for your great jobs. I want to drive some characters like yours in blender_character_driven-min.gif. Can you tell me how to realize it? Thank you."
ä¿æŒåˆ†è¾¨çŽ‡ä¸å˜ï¼Œä»…ä»…æ”¹å˜äººæ•°ï¼ˆä¸”æ•´ä¸ªè§†é¢‘æ—¶é•¿ä¸­çš„äººæ•°å„è‡ªä¿æŒä¸å˜ï¼‰çš„æµ‹è¯•è§†é¢‘ä¼¼ä¹Žä¸å¤ªå¥½æœé›†
"æ„Ÿè°¢æ‚¨çš„å·¥ä½œï¼Œåœ¨æˆ‘å°è¯•ä½¿ç”¨temporal optimizeæ—¶ï¼Œå‡ºçŽ°äº†å¦‚ä¸‹é—®é¢˜ã€‚  ä¸æ¸…æ¥šè¦æ€Žä¹ˆè§£å†³ï¼Œè¿˜è¯·æ‚¨æŒ‡æ•™    (ROMP) ubuntu18@ubuntu18:~/deep_learning/ROMP/simple_romp$ romp --mode=video --calc_smpl --render_mesh -i='/home/ubuntu18/deep_learning/ROMP/test/video1/sample_video.mp4'  -o='/home/ubuntu18/deep_learning/ROMP/test/video1/sample_video_out.mp4' --temporal_optimize --smooth_coeff=3 --show  Using ROMP v1  Extracting the frames of input /home/ubuntu18/deep_learning/ROMP/test/video1/sample_video.mp4 to /home/ubuntu18/deep_learning/ROMP/test/video1/sample_video_frames  Traceback (most recent call last):    File ""/home/ubuntu18/anaconda3/envs/ROMP/bin/romp"", line 33, in        sys.exit(load_entry_point('simple-romp==1.0.5', 'console_scripts', 'romp')())    File ""/home/ubuntu18/anaconda3/envs/ROMP/lib/python3.7/site-packages/simple_romp-1.0.5-py3.7-linux-x86_64.egg/romp/main.py"", line 191, in main      outputs = romp(image)    File ""/home/ubuntu18/anaconda3/envs/ROMP/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1190, in _call_impl      return forward_call(*input, **kwargs)    File ""/home/ubuntu18/anaconda3/envs/ROMP/lib/python3.7/site-packages/simple_romp-1.0.5-py3.7-linux-x86_64.egg/romp/utils.py"", line 725, in wrap_func      result = func(*args, **kwargs)    File ""/home/ubuntu18/anaconda3/envs/ROMP/lib/python3.7/site-packages/simple_romp-1.0.5-py3.7-linux-x86_64.egg/romp/main.py"", line 164, in forward      outputs = self.temporal_optimization(outputs, signal_ID)    File ""/home/ubuntu18/anaconda3/envs/ROMP/lib/python3.7/site-packages/simple_romp-1.0.5-py3.7-linux-x86_64.egg/romp/main.py"", line 146, in temporal_optimization      tracked_ids = get_tracked_ids(detections, tracked_objects)    File ""/home/ubuntu18/anaconda3/envs/ROMP/lib/python3.7/site-packages/simple_romp-1.0.5-py3.7-linux-x86_64.egg/romp/utils.py"", line 281, in get_tracked_ids      tracked_ids = [tracked_ids_out[np.argmin(np.linalg.norm(tracked_points-point[None], axis=1))] for point in org_points]    File ""/home/ubuntu18/anaconda3/envs/ROMP/lib/python3.7/site-packages/simple_romp-1.0.5-py3.7-linux-x86_64.egg/romp/utils.py"", line 281, in        tracked_ids = [tracked_ids_out[np.argmin(np.linalg.norm(tracked_points-point[None], axis=1))] for point in org_points]  IndexError: index 4 is out of bounds for axis 0 with size 4  "
"@Arthur151  è¯·é—®ä¸‹ï¼Œåœ¨è¯„æµ‹MuPoTS-3Dçš„MPJPE/PA-MPJPE/PCKçš„æ—¶å€™ï¼Œé¢„æµ‹çš„3dç‚¹ä»ŽSMPLå›žå½’å¾—åˆ°çš„ï¼Œ  è¿™é‡Œä½¿ç”¨çš„å›žå½’çŸ©é˜µåè®®æ˜¯ç”¨çš„h36m_joint_regressor.npy, è¿˜æ˜¯ç”¨çš„å…¶ä»–çš„åè®®ï¼Ÿ  "
"Hi, thanks for the great work!  I want to ask how to export fbx from a npz file of one image, not a video.   I checked the issues below.  #228  #270  and trying to export using Blender like #228.    But I'm facing some errors, and I cannot fix it.  Could you tell me how to export fbx from single image.    The error is below:         Following script is my script with error. I changed convert2fbx.py following to #228.   "
Why is the betas  of BEV output in 11 dimensions instead of 10 dimensions?   Which ten numbers represent the betas of SMPL? Thank you!
"Hi thanks for the wonderful work!  I am trying to replicate this nice work, in my environment. Before diving in to reproduction I have a few questions.     1. From the supplement material I know that BEV is trained with 4 V100 GPUs which is know to have 24 GB memory.  Since I can only utilize 3090 GPUs, that matches the required memory, I will have to use CUDA version 11.x.   Your installation.md make me to install cuda 10.2.. so will it work in 3090 GPUs (with just small modification maybe)?    2. How long will it takes to train from scratch using 3090? or with V100 how long did it take?      Thanks! :)"
"Thanks for your work and releasing the code, I think in your work y-axis is the upward axis. I was wondering how can I change the axis of the pelvis just from smpl-72 pose output? I tried rotating the pelvis with scipy.spatial.transform.Rotation but the global rotation is changing for each frame any idea what is the problem?"
"Hi, Yu, thank you for your great work!    I noticed that ROMP used 6D representation in the outputs[params_maps], but in the loss, still axis-angle representation to matrix? why not using the parameters in ['params_maps']?    Thank you!"
æˆ‘ç›´æŽ¥å¤åˆ¶äº†åˆ«äººçš„condaçŽ¯å¢ƒè¿‡æ¥ï¼Œæˆ‘ä»¬ä¿©çš„çŽ¯å¢ƒè·¯å¾„ä¸ä¸€æ ·ï¼ˆä¹‹å‰æ˜¯Usersè·¯å¾„ä¸‹ï¼Œæˆ‘çŽ°åœ¨æ˜¯ProgramDataè·¯å¾„ä¸‹ï¼‰ã€‚å‘çŽ°bevå‘½ä»¤è°ƒç”¨çš„ä»ç„¶æ˜¯ä¹‹å‰çš„è·¯å¾„çš„pythonã€‚  !   è¯·æ•™ä¸€ä¸‹ï¼Œè¿™ä¸ªbevè°ƒç”¨çš„pythonè·¯å¾„æ€Žä¹ˆèƒ½æ”¹æˆæˆ‘çŽ°åœ¨çš„condaçŽ¯å¢ƒè·¯å¾„ã€‚
"Using Google Colab get an error message:    !cd /content/ROMP  !python -m romp.predict.image --inputs=demo/images --output_dir=demo/image_results --renderer=pytorch3d --configs_yml=configs/image.yml    Output:  yaml_timestamp  /content/ROMP/active_configs/active_context_2022-10-18_19_57_42.yaml  Traceback (most recent call last):    File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main      ""__main__"", mod_spec)    File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code      exec(code, run_globals)    File ""/content/ROMP/romp/predict/image.py"", line 7, in        from .base_predictor import *    File ""/content/ROMP/romp/predict/base_predictor.py"", line 2, in        from utils.cam_utils import convert_cam_to_3d_trans  ****ImportError: cannot import name 'convert_cam_to_3d_trans' from 'utils.cam_utils'**** (/content/ROMP/romp/lib/models/../utils/cam_utils.py)"
æ‚¨å¥½ï¼Œæˆ‘åœ¨åšCMU_Panopticçš„æµ‹è¯•æ—¶ï¼Œå‘çŽ° çš„æ–‡ä»¶å¤¹æ²¡æœ‰æ‰¾åˆ°ã€‚æŒ‰ç…§ å¤„ç†å¥½åŽçš„æ–‡ä»¶ç»“æž„æ˜¯è¿™æ ·ã€‚è¯·é—®èƒ½æä¾›å¯¹åº”æ ‡æ³¨è§£å†³è¿™ä¸ªé—®é¢˜å˜›ï¼Ÿæˆ–è€…æ˜¯æ€Žä¹ˆä¿®æ”¹dataset.pyè¯»å–annots.npzçš„å†…å®¹ï¼Ÿ   
Where can I get the details about it? It seems to be very complex ndarray
"Hi, Is the old version of ROMP still available for webcam demo now?  I have just cloned the newest repo without any change at all and run by the old command of webcam demo:    python -u -m romp.predict.webcam --configs_yml ./configs/webcam_blender.yml    This new version of can not drive the character correctly (only pelvis moves correctly, other parts keep still as T-pose), and there are several mistakes happened:  1. key '`smpl_joints24`' no longer exists in the output of newest version of ROMP but it still remains in the base_predictor.py and webcam.py.  2. function '`convert_can_to_3d_trans()`' no longer exists in cam_utils.py.  3. The `detection flag and reorganize index` sometimes don't match and throw an error in that case:   !   4. args().smpl_model_path seems to be wrong. And There is no `SMPL_NEUTRAL.pkl ` and `J_regressir_extra` in the newest model_data.zip  !     I will get back to debugging later since now I'm too busy. But if this old one's going to be abandoned, just ignore these questions.  "
None
"Thanks a lot for this great and easy-to-use repo!    I'm trying to render the results using the weak perspective camera model. My question relates to these issues:   -     -     -      However none of these issues gave me the answer I was looking for. I am using the weak perspective camera parameters stored in `cam` and as suggested in   I multiply them with 2. I also pad the image to be square as mentioned  . I then convert the weak perspective camera model to a projection matrix the same way I used to do it for VIBE, which worked well there. However, for the ROMP output I'm still getting a slight misalignment, as you can see in the following screenshot. The light model is what I am rendering and the blue model in the background is the visualization output from ROMP. I think it's because I should somehow account for `cam_trans` but I don't know how exactly. Can you help me with this?    !   "
"hello!    I tried simple_romp video predict, but I got only 69 values for 'body_pose'.    What I expected was 72 values for 24 joints, but something is wrong.    Could you help with this problem? or is there anything that I missed out?"
## Problem     The webcam source might not be necessarily number '0'.  Especially if virtual webcam drivers are used.    ## Recommended solution  Add command line and configuration option to change the device number.
è¿è¡Œcodeçš„ç‰ˆæœ¬ä¿¡æ¯ï¼š  commit 02b9d1d8fe6a9797ea0c25ce75c83c975133dee8  Author: Yu Sun    Date:   Fri Jan 21 13:56:09 2022 +0800  
"## State of the art  The image processing library   uses the **RGB** (red, green, blue) pixel format.  In contrast the visual processing library   uses the **BGR** (blue, green, red) pattern.  Both libraries are popular.    ## ROMP implementation  Since ROMP uses mainly OpenCV:           There is of course an conversion, since the model requires the *RGB* pattern:       ## Example  Of cource i might do something like this..     But the part `cvtColor(..., COLOR_RGB2BGR)` is critical.  It's easy to make an mistake there, if the user doesn't take a look at the source code of ROMP.    ## Recommendation  I think the best solution might be to add an note to the README file of simple-romp, that it expects the **BGR** pattern for it's input data."
"Hi,     I am trying to train the network starting from `hrnet_pretrain.py`. When I run the code on a node with 4 GPUS it runs totally fine. When I run it on a node with 8 GPUS I get the following error at the end of first epoch when performing evaluation. I tested this with `val_batch_size` of 16 and `batch_size` of 16:       I think this error happens when dividing the data across processes. However, I am not really sure what would be the solution for this. Have you encountered this error before?    Thanks in advance,  "
"Hello,    I have several questions regarding the pretraining phase. I would like to use pre-trained `hrnet` or `resnet50` models.   1. the `pretrain_hrnet.pkl` mentioned in many previous issues is not in the repository. Could you please tell me where I can find it? I believe this is made by you.   2. Does `pretrain_hrnet.pkl` contain backbone parameters only?   3. The `pretrain_hrnet.pkl` path should be provided to hrnet_pretrain parameter in `v1.yml`. Is that correct? I am confused about this because in   issue you mention `resnet_pretrain` should be set while in the   you mention `model_path` should be set. `model_path` is for trained ROMP model. Is not it? So for pertaining either `hrnet_pretrain` or `resnet_pretrain` should be set.  4. Also in the `val_result` function you have the following line:     Why do you train the model during evaluation for resnet and not for hrnet?      I would really appreciate it if you could help me with this questions."
"In image_base.py, when you calculate occluded person, I think there is wrong condition.    full_kp2ds is normalized to [-1 ~ +1] before passed to detect_occluded_person() as belows:            Therefore, I think visible condition to be greater than '-1', not '0'  as belows:     "
"Hi @Arthur151   What is the meaning of following error which I encountered in google Colab? I think the yaml file is missing in the active_config directory. Please check the issue.    No configs_yml is set, set it to the default --configs_yml=configs/image.yml  yaml_timestamp  /content/ROMP/active_configs/active_context_2022-08-10_12_57_59.yaml  Traceback (most recent call last):    File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main      ""__main__"", mod_spec)    File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code      exec(code, run_globals)    File ""/content/ROMP/romp/predict/image.py"", line 7, in        from .base_predictor import *    File ""/content/ROMP/romp/predict/base_predictor.py"", line 1, in        from ..base import *    File ""/content/ROMP/romp/base.py"", line 16, in        from models import build_model    File ""/content/ROMP/romp/lib/models/__init__.py"", line 6, in        from .build import build_model    File ""/content/ROMP/romp/lib/models/build.py"", line 7, in        from models.romp_model import ROMP    File ""/content/ROMP/romp/lib/models/romp_model.py"", line 18, in        from loss_funcs import Loss    File ""/content/ROMP/romp/lib/models/../loss_funcs/__init__.py"", line 6, in        from .calc_loss import Loss    File ""/content/ROMP/romp/lib/models/../loss_funcs/calc_loss.py"", line 23, in        from maps_utils.centermap import CenterMap    File ""/content/ROMP/romp/lib/models/../maps_utils/__init__.py"", line 6, in        from .kp_group import HeatmapParser    File ""/content/ROMP/romp/lib/models/../maps_utils/kp_group.py"", line 13, in        from munkres import Munkres  ModuleNotFoundError: No module named 'munkres'"
"Hi.    I have a question about cpu usage and gpu usage.    When I excuted the model with command ""romp --mode=webcam --show, desktop's cpu usage was over 70% and gpu usage was under 5%.    I checked I used gpu-mode and cuda_is_available was true.    The cpu was i7-8700k and gpu was RTX 2060.    I'm wondering the result was right or I did something wrong.    Thank you very much!! :)"
I saw model_data.zip has changed on 21 Jun 2022. See:    Is it possible to have an old copy of it?  
"Hello,    I was looking into the following script: `  In this script, you are using `pretrain.yml` however there is no such file in the repository.    Thanks"
"In coarse2fine_localization function, there is code as belows:     I have some questions at this function.    > In Figure 2, 3D anchor map is added after calculating 3D Offset Map. But, in this function, 3D anchor map is added to CAM_MAPS_OFFSET first (orange box in Figure 2). Can you explain why?  I think right codes for this part is as below.       > I can't understand why (1) convert_cam_params_to_centermap_coords and (2) denormalize_center functions are required. Can you explain details about these functions .. :(   It is hard to know why cam3dmap_anchors is subtracted and using argmin ....     "
"In function ' coarse2fine_localization', for adjusting z-wise only, I think we use cam_maps_3d[:, :, :, :, 0], not [:, :, :, :, 2] because self.coordmap_3d is the result of concatenate (Z_map, Y_map, X_map).      "
"Hi,     Downloading the following file: `wget   I can only see pretrained HRNET model. Is the pretrained resnet available also? Where can I get it?    Thanks,"
æ‚¨å¥½ï¼Œè¿è¡Œ`romp --mode=video --calc_smpl -i=path/to/input -o=path/to/output`ï¼Œä½¿ç”¨çš„æ˜¯ROMPv1æ¨¡åž‹ï¼Œä½†æ˜¯æˆ‘æƒ³è¦æ›´æ¢ä¸ºROMP_HRNet32_V1.pklæ¨¡åž‹ï¼Œè¯·é—®ä»Žå“ªé‡Œä¸‹è½½ï¼Ÿåˆåœ¨å“ªé‡Œæ›´æ”¹ä»£ç å‘¢ï¼Ÿ
æ‚¨å¥½ï¼Œæ„Ÿè°¢æ‚¨ä¼˜ç§€çš„å·¥ä½œï¼  æˆ‘æ˜¯ç¬¬ä¸€æ¬¡æŽ¥è§¦è¿™ä¸ªé¢†åŸŸçš„æ–°æ‰‹ï¼Œæ‰€ä»¥å…³äºŽè®ºæ–‡ä¸­çš„å„ç§Protocolæœ‰ä¸€äº›ç–‘æƒ‘ã€‚  æ ¹æ®æˆ‘çš„è°ƒç ”ï¼Œåœ¨HMRé¢†åŸŸï¼Œå¤§å®¶ä¸€å¼€å§‹éƒ½æ˜¯åœ¨å„ç§æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒ(H36Mã€COCOç­‰ç­‰)ï¼Œç„¶åŽå†åœ¨3DPWä¸Šè¿›è¡Œæµ‹è¯•ã€‚  æ‰€ä»¥ï¼Œè®ºæ–‡ä¸­ä¸åŠ  * çš„å®žéªŒç»“æžœï¼Œæ„å‘³ç€æ˜¯åœ¨æ··åˆæ•°æ®é›†ï¼ˆä¸å«3dpw training setï¼‰ä¸‹è®­ç»ƒï¼Œåœ¨3dpw test setè¿›è¡Œæµ‹è¯•å¾—åˆ°çš„ç»“æžœï¼›  è€ŒåŠ  * çš„å®žéªŒç»“æžœåˆ™æ„å‘³ç€æ˜¯åœ¨æ··åˆæ•°æ®é›†ï¼ˆåŒ…å«3dpw training setï¼‰ä¸‹è®­ç»ƒï¼Œåœ¨3dpw test setè¿›è¡Œæµ‹è¯•å¾—åˆ°çš„ç»“æžœã€‚  å³ extra datasetsæŒ‡çš„æ˜¯3dpw training setã€‚  è€ŒfinetuneæŒ‡çš„æ˜¯åœ¨æ··åˆæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒä¹‹åŽï¼Œåœ¨å¾—åˆ°çš„å‚æ•°åŸºç¡€ä¸Šï¼Œåœ¨3dpw training setä¸Šé‡æ–°è®­ç»ƒä¸€éã€‚  ä¸çŸ¥è¿™æ ·çš„ç†è§£æ˜¯å¦æ­£ç¡®ï¼Ÿ
æ‚¨å¥½ï¼Œé€šè¿‡é˜…è¯»ä»£ç æˆ‘å‘çŽ°ä¸€æ¬§å…ƒæ»¤æ³¢ç”¨åœ¨äº†é™¤å…¨å±€æ—‹è½¬å¤–çš„å…¶å®ƒå…³èŠ‚è½´è§’ä¸Šï¼Œè¯·é—®ä¸€ä¸‹å¯¹äºŽå…¨å±€æ—‹è½¬çš„æŠ–åŠ¨é—®é¢˜è¯¥å¦‚ä½•è§£å†³å‘¢ï¼Ÿ
"Hello,    I have trained ROMP on the `pw3d` dataset for 150 epochs and from SCRATCH. Here are my hyperparameters:     The model seems to be well converged and here is the loss function:  <img width=""433"" alt=""Screen Shot 2022-07-19 at 5 29 03 PM"" src=""     However, when I look at the Center-map heatmaps it doe  <img width=""1033"" alt=""Screen Shot 2022-07-19 at 5 27 16 PM"" src=""   s not look good at all:      I expected to get a plot similar to below that I obtained by fine-tuning the ROMP with HRNet-32 as the backbone (V1_hrnet_3dpwft.sh):  <img width=""1234"" alt=""Screen Shot 2022-07-15 at 4 52 37 PM"" src=""     Any thought or advice on why this is the case and what I further need to do to improve the center map heatmap?    Thanks in advance,  "
æ‚¨å¥½ï¼Œæˆ‘ä½¿ç”¨æ—§ç‰ˆä»£ç æ˜¯å¯ä»¥æ­£å¸¸è®­ç»ƒçš„ï¼Œä½†æ˜¯ä½¿ç”¨æœ€æ–°ä»£ç å°±ä¼šæœ‰é—®é¢˜    configå¦‚ä¸‹ï¼š         é”™è¯¯æ—¥å¿—å¦‚ä¸‹ï¼š       å¾ˆå¥‡æ€ªï¼Œéº»çƒ¦å¤§ä½¬çœ‹ä¸€ä¸‹ä¸ºä»€ä¹ˆä¼šè¿™æ ·~
å¤§ä½¬æ‚¨å¥½ï¼Œæˆ‘åœ¨è¿›è¡ŒEvaluation on CMU Panopticçš„æ—¶å€™é‡åˆ°äº†é—®é¢˜ï¼Œæè¿°å¦‚ä¸‹ï¼š    è¿™æ˜¯æˆ‘çš„config       è¿™æ˜¯æˆ‘çš„æ•°æ®é›†  !   !          éº»çƒ¦çœ‹ä¸‹æ˜¯ä»€ä¹ˆåŽŸå› ~
"åœ¨ä¹‹å‰ä¸€ä¸ªç‰ˆæœ¬çš„æ•°æ®é›†ä»£ç ä¸­å¤šæ•°æ•°æ®é›†æœ‰ï¼š  kp3ds -= root_trans[:,None]  å°±æ˜¯æ‰€æœ‰çš„3dçš„åæ ‡æ ‡ç­¾éƒ½æ˜¯å‡åŽ»æ ¹èŠ‚ç‚¹ä¹‹åŽçš„åæ ‡ã€‚  è¿™æ ·å¤„ç†çš„è¯ï¼Œæ¨¡åž‹é¢„æµ‹çš„åæ ‡æ˜¯å¦å°±ä¸æ˜¯ä¸–ç•Œåæ ‡ç³»ä¸‹çš„åæ ‡äº†ï¼Œè€Œæ˜¯ä»¥æ ¹èŠ‚ç‚¹ï¼ˆå·¦ã€å³è‡€ä¸­ç‚¹ï¼‰ä¸ºé›¶ç‚¹çš„åæ ‡ï¼Ÿ    æœ€æ–°ç‰ˆæœ¬çš„æ•°æ®é›†æ˜¯ä¸æ˜¯å› ä¸ºè¿™ä¸ªåŽŸå› ï¼ŒæŠŠroot_transä¹Ÿä¼ å…¥äº†æ¨¡åž‹ï¼Ÿ"
å½“æˆ‘è¿è¡Œä»¥ä¸‹å‘½ä»¤æ—¶ï¼ŒæŠ¥å‡ºäº†ä»¥ä¸‹é”™è¯¯  `python main.py -m=video -i=/docker/data/6.MP4 -o=/docker/data/ROMP/demo/dance_6/bev_tj06.mp4 --save_video -t -sc=1.5 --renderer pyrender --show_largest --show_items mesh  `  !   ä¸æ‰§è¡Œ--show_largestå¯ä»¥æ­£å¸¸è¿è¡Œï¼Œè¯·é—®æ˜¯ä»€ä¹ˆåŽŸå› é€ æˆçš„
ä½œè€…å¤§å¤§ä½ å¥½  I have a question about the training codes. I have read through the codes and I couldn't find the methods of Gaussion kernel size and CAR calculation used for the training. Did I missed some part lol 
"valid_maskså¥½åƒæ˜¯ä»£è¡¨æŸäº›æ ‡ç­¾æ˜¯å¦å­˜åœ¨æˆ–è€…æ˜¯å¦å‡†ç¡®ã€‚  æ ¹æ®ä»£ç ï¼Œvalid_masks[3,4,5]ä¼¼ä¹Žåˆ†åˆ«ä»£è¡¨poseå‚æ•°çš„å‰3ä½ï¼Œå‰©ä½™ä½ä»¥åŠshapeå‚æ•°æ˜¯å¦æœ‰æ•ˆã€‚  é‚£valid_masksçš„å…¶ä½™å‡ ä½æ˜¯ä»€ä¹ˆå«ä¹‰å‘¢ï¼Ÿ"
å¤§ä½¬æ‚¨å¥½ï¼Œéžå¸¸æ„Ÿè°¢æ‚¨çš„å·¥ä½œ~    æˆ‘åŠ å…¥pw3dè®­ç»ƒå‡ºæ¥çš„æ¨¡åž‹ï¼Œåœ¨è¯„ä¼°Evaluation on 3DPW Challengeæ—¶å‡ºçŽ°å¦‚ä¸‹é”™è¯¯       æˆ‘çš„configå¦‚ä¸‹       æ‰§è¡Œå‘½ä»¤å¦‚ä¸‹ï¼š  python romp/lib/evaluation/collect_3DPW_results.py --configs_yml=configs/eval_3dpw_challenge.yml    éº»çƒ¦å¤§ä½¬å¸®å¿™çœ‹ä¸‹è¿™æ˜¯å“ªé‡Œçš„é—®é¢˜~  
å¤§ä½¬å¥½ï¼Œæˆ‘åœ¨é˜…è¯»è®ºæ–‡ä¸­æœ‰äº›å›°æƒ‘ï¼Œæˆ‘å¯¹äºŽ3DPWè¯„ä¼°ä¸­çš„ä¸‰ç§åè®®æ˜¯è¿™æ ·ç†è§£çš„    åè®®ä¸€ï¼šä¸ç”¨3dpw-trainæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¯¹æ•´ä¸ª3dpwæ•°æ®é›†è¿›è¡Œè¯„ä¼°  åè®®äºŒï¼šä¸ç”¨3dpw-trainæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¯¹3dpw-testè¿›è¡Œè¯„ä¼°  åè®®ä¸‰ï¼šä½¿ç”¨3dpw-trainæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¯¹3dpw-testè¿›è¡Œè¯„ä¼°    è¯·é—®æ˜¯è¿™æ ·å—ï¼Ÿ
å¤§ä½¬æ‚¨å¥½ï¼Œéžå¸¸æ„Ÿè°¢æ‚¨çš„å·¥ä½œ~    æˆ‘åŠ å…¥pw3dè®­ç»ƒå‡ºæ¥çš„æ¨¡åž‹ï¼Œåœ¨è¯„ä¼°Evaluation on 3DPW Challengeæ—¶å‡ºçŽ°å¦‚ä¸‹é”™è¯¯       æˆ‘çš„configå¦‚ä¸‹       æ‰§è¡Œå‘½ä»¤å¦‚ä¸‹ï¼š  python romp/lib/evaluation/collect_3DPW_results.py --configs_yml=configs/eval_3dpw_challenge.yml    éº»çƒ¦å¤§ä½¬å¸®å¿™çœ‹ä¸‹è¿™æ˜¯å“ªé‡Œçš„é—®é¢˜~  
"Hi, Doc. Sun. It's me again. :-). This time when I run BEV's test on AGORA, the BEV's module somehow encountered 'no attribute' error. More specificly, the `setting` of module `BEV` had no attribute called `show_items`. Detailed infomation shows as below when I run eval_AGORA.py.    `Using BEV.  Threshold for positive center detection: 0.15  Traceback (most recent call last):    File ""/home/gpu/content/ROMP/simple_romp/evaluation/eval_AGORA.py"", line 125, in        get_results_on_AGORA(set_name)    File ""/home/gpu/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py"", line 28, in decorate_context      return func(*args, **kwargs)    File ""/home/gpu/content/ROMP/simple_romp/evaluation/eval_AGORA.py"", line 110, in get_results_on_AGORA      model = BEV(default_eval_settings)    File ""/usr/local/anaconda3/lib/python3.8/site-packages/bev/main.py"", line 94, in __init__      self._initilization_()    File ""/usr/local/anaconda3/lib/python3.8/site-packages/bev/main.py"", line 111, in _initilization_      self.visualize_items = self.settings.show_items.split(',')  AttributeError: 'Namespace' object has no attribute 'show_items'  `    Hope for your answer. ðŸ‘ "
"Thank your works!    Your thesis is very impressive and wonderful.    I have 2 Questions.  I understand that the dataset below is used when training BEVs.   dataset: h36m,muco,coco,lsp,agora,pw3d,relative_human,posetrack   eval_datasets: 'relative,agora'    However, Google drive has only annots_train21_full.npz of posetrack data.  1.  Then after I change the dataset above as below,  h36m,muco,coco,lsp,agora,pw3d,relative_human,posetrack21  Then, should I change the dataset above as below and receive the image file from the official dataset site?    2.  Or should I change the code below to 21 on the data loader after using it as it is?  dataset: h36m,muco,coco,lsp,agora,pw3d,relative_human,posetrack"
"Hi, Doc. Sun. It works perfect when I run a BEV's evaluation on CMU-Panoptic dataset. However, when I use RH Dataset for evaluation , after 80 iterations like    `processing in crowd mode  BEV 'forward' executed in 1.8504s, FPS 0.5  [-                                       ]   71/1836 (  3%) 1765 to go  BEV 'forward' executed in 0.0923s, FPS 10.8  [-                                       ]   72/1836 (  3%) 1764 to go  BEV 'forward' executed in 0.1033s, FPS 9.7  [-                                       ]   73/1836 (  3%) 1763 to go  BEV 'forward' executed in 0.0940s, FPS 10.6  [-                                       ]   74/1836 (  4%) 1762 to go  BEV 'forward' executed in 0.0832s, FPS 12.0  [-                                       ]   75/1836 (  4%) 1761 to go  BEV 'forward' executed in 0.0859s, FPS 11.6  [-                                       ]   76/1836 (  4%) 1760 to go  BEV 'forward' executed in 0.0892s, FPS 11.2  [-                                       ]   77/1836 (  4%) 1759 to go  BEV 'forward' executed in 0.2021s, FPS 4.9  [-                                       ]   78/1836 (  4%) 1758 to go  BEV 'forward' executed in 0.2111s, FPS 4.7  [-                                       ]   79/1836 (  4%) 1757 to go  BEV 'forward' executed in 0.1024s, FPS 9.8  [-                                       ]   80/1836 (  4%) 1756 to go`    , a list of `No person detected!` occured like    `processing in crowd mode  No person detected!  No person detected!  No person detected!  No person detected!  No person detected!  No person detected!  `    , and then it pomps out error like     `File ""/usr/local/anaconda3/lib/python3.8/site-packages/bev/main.py"", line 138, in forward      outputs = self.process_long_image(image, show_patch_results=self.settings.show_patch_results)    File ""/usr/local/anaconda3/lib/python3.8/site-packages/bev/main.py"", line 213, in process_long_image      ch, cw = croped_images[cid].shape[:2]  IndexError: list index out of range`    It functions well in the beginning, so I wondered why the evaluation encountered error in the middle of the process. Hope for your answer :-)"
"Hello! ROMP is really a great work, and your document makes everything so easy to use.  But when I followed the step in google colab to install ROMP in my local machine, and run     `python -m romp.predict.image --inputs=demo/images --output_dir=demo/image_results --renderer=pytorch3d`    then I encountered bug report as:    `No configs_yml is set, set it to the default --configs_yml=configs/image.yml  yaml_timestamp  /home/gpu/content/ROMP/active_configs/active_context_2022-07-03_15_21_17.yaml  Loading the configurations from configs/image.yml  Traceback (most recent call last):    File ""/usr/local/anaconda3/lib/python3.8/runpy.py"", line 194, in _run_module_as_main      return _run_code(code, main_globals, None,    File ""/usr/local/anaconda3/lib/python3.8/runpy.py"", line 87, in _run_code      exec(code, run_globals)    File ""/home/gpu/content/ROMP/romp/predict/image.py"", line 79, in        main()    File ""/home/gpu/content/ROMP/romp/predict/image.py"", line 71, in main      processor = Image_processor(args_set=args_set)    File ""/home/gpu/content/ROMP/romp/predict/image.py"", line 14, in __init__      super(Image_processor, self).__init__(**kwargs)    File ""/home/gpu/content/ROMP/romp/predict/base_predictor.py"", line 12, in __init__      super(Predictor, self).__init__(**kwargs)  TypeError: __init__() got an unexpected keyword argument args_set'`    then I print the args_set, it looks like this:    `{'args_set': Namespace(FOV=60, GPUS=0, HMloss_type='MSE', PCK_thresh=150, Rot_typ                                                                                      e='6D', Synthetic_occlusion_ratio=0, add_offsetmap=True, adjust_lr_epoch=[], adju                                                                                      st_lr_factor=0.1, baby_threshold=0.8, backbone='hrnet', batch_size=64, beta_dim=1                                                                                      0, bv_with_fv_condition=True, calc_PVE_error=False, calc_mesh_loss=True, calc_pck                                                                                      =False, calc_smpl_mesh=True, cam_dim=3, cam_dist_thresh=0.1, center_def_kp=True,                                                                                       centermap_conf_thresh=0.25, centermap_size=64, character='smpl', collect_subdirs=                                                                                      False, collision_aware_centermap=False, collision_factor=0.2, color_jittering_rat                                                                                      io=0.2, configs_yml='configs/image.yml', dataset='h36m,mpii,coco,aich,up,ochuman,                                                                                      lsp,movi', dataset_rootdir='/home/gpu/content/dataset/', deconv_num=0, depth_loss                                                                                      _type='Piecewise', distillation_learning=False, distributed_training=False, epoch                                                                                      =120, eval=False, eval_2dpose=False, eval_datasets='pw3d', f=None, fast_eval_iter                                                                                      =-1, fine_tune=True, fix_backbone_training_scratch=False, focal_length=443.4, fps                                                                                      _save=24, fv_conditioned_way='attention', gpu='0', head_block_num=2, homogenize_p                                                                                      ose_space=False, hrnet_pretrain='/home/gpu/content/ROMP/trained_models/pretrain_h                                                                                      rnet.pkl', image_loading_mode='image', input_size=512, inputs='demo/images', inte                                                                                      ractive_vis=False, kernel_sizes=[5], learn_2dpose=False, learn_AE=False, learn_kp                                                                                      2doffset=False, learn_relative=False, learn_relative_age=False, learn_relative_de                                                                                      pth=False, learn_relative_shape=False, local_rank=0, log_path='/home/gpu/content/                                                                                      log/', loss_thresh=1000, lr=0.0003, make_tracking=False, master_batch_size=-1, ma                                                                                      tch_preds_to_gts_for_supervision=False, matching_mode='all', matching_pckh_thresh                                                                                      =0.6, max_person=64, max_supervise_num=-1, merge_smpl_camera_head=False, mesh_clo                                                                                      th='ghostwhite', model_path='trained_models/ROMP_HRNet32_V1.pkl', model_precision                                                                                      ='fp32', model_return_loss=False, model_version=1, multi_person=True, new_trainin                                                                                      g=False, num_depth_level=8, nvxia_model_path='/home/gpu/content/ROMP/model_data/c                                                                                      haracters/nvxia', nw=4, optimizer_type='Adam', output_dir='demo/image_results', p                                                                                      erspective_proj=False, print_freq=50, renderer='pytorch3d', resnet_pretrain='/hom                                                                                      e/gpu/content/ROMP/trained_models/pretrain_resnet.pkl', rot_dim=6, rotate_prob=0.                                                                                      2, sampling_aggregation_way='floor', save_centermap=False, save_dict_results=True                                                                                      , save_mesh=True, save_visualization_on_img=True, scale_anchor=True, show_largest                                                                                      _person_only=False, show_mesh_stand_on_image=False, shuffle_crop_mode=False, shuf                                                                                      fle_crop_ratio_2d=0.1, shuffle_crop_ratio_3d=0.9, smil_model_path='/home/gpu/cont                                                                                      ent/ROMP/model_data/smpl_models/smil_packed_info.pth', smpl_joint_num=22, smpl_me                                                                                      sh_root_align=False, smpl_model_path='/home/gpu/content/ROMP/model_data/smpl_mode                                                                                      ls/smpl_packed_info.pth', smpl_prior_path='/home/gpu/content/ROMP/model_data/para                                                                                      meters/gmm_08.pkl', smpl_uvmap='/home/gpu/content/ROMP/model_data/parameters/smpl                                                                                      _vt_ft.npz', smpla_model_path='/home/gpu/content/ROMP/model_data/smpl_models/SMPL                                                                                      A_NEUTRAL.pth', soi_camera='far', supervise_cam_params=False, supervise_global_ro                                                                                      t=False, surrounding_camera=False, tab='hrnet_cm64_process_images', teacher_model                                                                                      _path='/export/home/suny/CenterMesh/trained_models/3dpw_88_57.8.pkl', temporal_op                                                                                      timization=False, test_interval=2000, top_n_error_vis=6, track_memory_usage=False                                                                                      , use_coordmaps=True, use_eft=True, val_batch_size=4, voc_dir='/home/gpu/content/                                                                                      dataset/VOCdevkit/VOC2012/', wardrobe='/home/gpu/content/ROMP/model_data/wardrobe                                                                                      ', weight_decay=1e-06)}  `    I have totally no idea about why this occurs. Hope for your answer :)"
"Hi, I found another bug during validation. Two tensor shape mismatch  .    Here is my config   "
"Hi, I get an error in validation. I checked and found that the tensor shape of   is  , the tensor shape of outputs['j3d'] is [1, 54, 3].    Could you help me with it?    Here is my config   "
"Hi, thanks for sharing your code about ROMP.  I download the dataset as you said in  . When I run the code, I get the error about loading the lsp dataset. Is there something missing?"
GitHub becomes more and more the wrong platform for FOSS projects:       So currently   switching over from GitHub to  .  How do you think about doing the same with your project? ðŸ™‚ 
Hello first of all thank you for your work. A problem occurs in the process of loading and exporting my Mixamo Avatar file as fbx in convert2fbx.py. The hierarchical structure of the skeleton is attached to the picture. Can you solve this problem? Thank you  !   !     
å¤§ä½¬æ‚¨å¥½ï¼Œéžå¸¸æ„Ÿè°¢æ‚¨çš„å·¥ä½œ~ æˆ‘åœ¨å¤çŽ°è®ºæ–‡ç»“æžœçš„è¿‡ç¨‹ä¸­é‡åˆ°äº†ä¸€äº›é—®é¢˜    æˆ‘é‡‡ç”¨äº†v1.ymlæ¥è¿›è¡Œè®­ç»ƒï¼Œç”±äºŽæ˜¾å¡åŽŸå› ï¼ˆæˆ‘çš„ç”µè„‘ä¸€å¼ 3070æ˜¾å¡ï¼Œ8Gæ˜¾å­˜ï¼‰æˆ‘çš„batchè°ƒå°äº†ï¼Œå…¶ä»–åŸºæœ¬éƒ½æ˜¯é»˜è®¤ï¼Œä»¥ä¸‹æ˜¯æˆ‘çš„ymlæ–‡ä»¶       åœ¨æ–­æ–­ç»­ç»­è®­ç»ƒäº†å¤§æ¦‚1~2å¤©ä¹‹åŽï¼Œæˆ‘å‘çŽ°è¡¨çŽ°åŸºæœ¬ç¨³å®šï¼Œå¹¶ä¸ä¼šå˜å¾—æ›´å¥½ï¼Œæƒ³é—®ä¸‹å¤§ä½¬è¿™æ˜¯ä»€ä¹ˆåŽŸå› ï¼Ÿ    è¿™æ˜¯æˆ‘è®­ç»ƒçš„å‡ æ¬¡log               
ä½œè€…æ‚¨å¥½ï¼Œæˆ‘æ˜¯æ–°ç ”ç©¶è¿™ä¸ªé¢†åŸŸçš„ï¼Œæœ‰ä¸ªå›°æƒ‘æƒ³è¯·æ•™æ‚¨ä¸€ä¸‹ï¼Œè¯·é—®è®¾ç½® root_align=True åŽï¼Œå¾—åˆ°çš„æ˜¯root-relative meshå§ï¼Œç„¶åŽç»è¿‡SMPLæ˜ å°„çŸ©é˜µå¾—åˆ°çš„3D pose ä¹Ÿæ˜¯root-relative 3d poseå—ï¼Ÿå¼±é€è§†æŠ•å½±åº”è¯¥æ˜¯å°†ç»å¯¹çš„  3D human pose æŠ•å½±åˆ°å›¾ç‰‡ä¸­ä¸Ž2D poseå¯¹é½çš„å§ï¼Œåº”è¯¥æ˜¯è¦åŠ ä¸Šroot positionçš„åæ ‡å§ï¼Ÿ é‚£æ˜¯æ€Žä¹ˆå¾—å‡ºç›¸æœºç©ºé—´ä¸­ç»å¯¹çš„ root positionçš„ï¼Ÿ
"æ‚¨å¥½ï¼Œæ„Ÿè°¢æ‚¨çš„å¼€æºæ¨¡åž‹å’Œè´¡çŒ®ã€‚æˆ‘æ˜¯ä¸€ä¸ªæ–°æ‰‹ï¼Œæƒ³è¯·æ•™ä¸€ä¸‹å¦‚ä½•ä»Žè¾“å‡ºçš„å‚æ•°ä¸­ä¼°è®¡ç›¸æœºçš„å†…å‚å’Œå¤–å‚å‘¢ï¼Ÿ  æˆ‘çœ‹åˆ°è¾“å‡ºçš„ï¼š  `outputs =  dict_keys(['cam', 'global_orient', 'body_pose', 'smpl_betas', 'smpl_thetas', 'center_preds', 'center_confs', 'cam_trans', 'verts', 'joints', 'pj2d_org'])`  å…¶ä¸­camæ˜¯n * 3çš„å¼±é€è§†ç›¸æœºå‚æ•°ï¼Œcam_transæ˜¯n * 3çš„ç›¸æœºåˆ°rootçš„å¹³ç§»å‚æ•°ï¼Œå¦‚ä½•è®¡ç®—å¾—åˆ°æ™®é€šé’ˆå­”ç›¸æœºçš„3 * 3çš„å†…å‚ã€4 * 4çš„å¤–å‚å‘¢ï¼Ÿ  æ„Ÿè°¢ï¼ï¼ï¼"
"Hi,   I have a question regarding smpl model you are using in your code. In the `requirements.txt` I see that `smplx` is installed only. However in the paper only SMPL is referenced. Are you using SMPL to create the 3D reconstruction given pose and shape or are you using SMPL-x?  If you are using SMPL-x the reconstructed hand gestures should be pretty close to the image, but then why is this not the case looking at the results?    I would appreciate your feedback,  Thanks,    "
ä½ å¥½ï¼Œåœ¨è¿è¡Œè¿‡ç¨‹ä¸­æˆ‘å‘çŽ°æ™®é€šç›¸æœºçš„è¿åŠ¨æ¨¡ç³Šå¯¹ç»“æžœå½±å“è¾ƒå¤§ï¼Œè¯·é—®ä½ ä»¬ç”¨çš„ç›¸æœºç¡¬ä»¶æ˜¯ä»€ä¹ˆåž‹å·çš„å‘¢ï¼Ÿ
No such file: Relative_human/age_balanced_sample_dict.npz
è¯·é—®æµ‹è¯•3dpwçš„æ—¶pa mpjpeæ—¶ï¼Œé‡‡ç”¨çš„æ˜¯å¤šå°‘ä¸ªå…³èŠ‚ç‚¹ã€‚  æˆ‘çœ‹ä¹‹å‰å¥½å¤šæ–‡ç« ç”¨çš„14ä¸ªç‚¹ï¼Œæœ‰çš„ä¹Ÿæœ‰ç”¨24ä¸ªç‚¹çš„ï¼Œrompç”¨çš„å¤šå°‘å•Šï¼Ÿ
!   è¯·é—®æ‚¨è¿™ä¸ªä¸¤ä¸ªå›¾ä½¿ç”¨çš„æ˜¯ä»€ä¹ˆç”»å›¾è½¯ä»¶å‘¢ï¼Ÿä»¥åŠæ‚¨å¸¸ç”¨ä»€ä¹ˆè½¯ä»¶ç”»ä¸Žäººä½“ä¼°è®¡ç›¸å…³çš„æµç¨‹æ¡†æž¶ç¤ºæ„å›¾ï¼Ÿæœ‰ä»€ä¹ˆå¼€æºé¡¹ç›®æŽ¨èå—ï¼Ÿ
   è¯·é—®åŽé¢çš„Falseæ˜¯å¦åº”è¯¥æ”¹ä¸ºTrue
"@gngdb @vivi90 @Arthur151 @vltmedia @ayedaemon  I've set smoothening coefficient at 24 but still, it has flickering.         and I'm using mode=""video"".  "
"Hello, thank you very much for your work.    Your paper mentioned the evaluation of occlusion benchmarks, tab 5 and tab 6. Tab 6 are known.. The question is how to generate the results of tab 5    Although you are in eval Tab 5 is also mentioned in eval.md, but it is confirmed that tab 4 is actually mentioned here"
"@gngdb @vivi90 @Arthur151 @vltmedia @ayedaemon     I'm getting error ""AttributeError: module 'blenderpy' has no attribute 'app'"" while importing blenderpy, I'm using convert2fbx.py in both linux and windows.    python=3.7 and have followed your guidelines to install it. please, help me ASAP."
"I wanted to test the live webcam export in Blender.  Exactly this way, how it's demonstrated in the  :  !     My first try was to create the required `*.npz` from webcam:     After that i wanted to use it accourding to the  .  **But:** My first command did not create anything.    My `pip freeze` output:     System:   - Python: v`3.9.12`   - OS: Windows 10 Enterprise (64-bit, Version `21H2`, Build `19044.1706`)  "
"Hi, thank you for your excellent work!    I have several questions regarding the ROMP experiments details on the AGORA dataset, and hope you can helpÂ me.     1. AGORA dataset provides detailed annotations, such as ground truth camera data frames and ground truth fittings, but I found it hard to match all those annotations with the `img_info` dict when loading data. I'm not sure what annotations are matched for several keys, including `kp2ds`, `kp3ds`, `params`, `camMats` and `camDist`. Therefore I want to know how you use the AGORA annotations for model finetuning. If it is possible, could you share the code which you use to load agora dataset?    2. Besides, could you also share the config file of finetuning ROMP on agora? Because the only thing I can find is the training settings for BEV from your your ""Putting People in their Place: Monocular Regression of 3D People in Depth"".  "
"å…³äºŽæ•°æ®é›†çš„æ ‡ç­¾ï¼Œæˆ‘æœ‰ä¸€äº›é—®é¢˜ã€‚  1. cocoæ•°æ®é›†çš„poseå’Œshapeå‚æ•°æ˜¯å¦‚ä½•å¾—åˆ°çš„ï¼Ÿæˆ‘åœ¨å¦ä¸€ç¯‡issueé‡Œçœ‹åˆ°h36mçš„æ•°æ®é›†æ˜¯ä»Ž  and  )çš„å·¥ä½œå¾—åˆ°çš„ï¼Œå…¶ä»–çš„æ•°æ®é›†çš„æ ‡æ³¨ä¹Ÿæ˜¯è¿™æ ·å¾—åˆ°çš„å—ï¼Ÿ  2. æˆ‘çœ‹åˆ°å„ä¸ªæ•°æ®é›†è¿”å›žçš„æ ‡ç­¾ä¸­ï¼Œkp2dsæ˜¯æ ‡æ³¨æ–‡ä»¶ä¸­ç›´æŽ¥è¯»å–çš„2dåæ ‡ï¼Œç„¶åŽæ˜ å°„åˆ°SMPL_ALL_54ä¸Šå¾—åˆ°çš„(54,3)çš„åæ ‡ï¼Œå…¶ä¸­æœ‰çš„å…³èŠ‚åæ ‡å› ä¸ºæ ‡æ³¨æ²¡æœ‰æ‰€ä»¥æ˜¯(-2,-2)ï¼›è€Œkp3dsåˆ™æ˜¯ç”±smplå‚æ•°å›žå½’å¾—åˆ°çš„54ä¸ªå…³èŠ‚ç‚¹çš„3Dåæ ‡ã€‚è¯·é—®è¿™æ ·åšæœ‰ä»€ä¹ˆæ·±æ„å—ï¼Ÿä¸ºä»€ä¹ˆä¸éƒ½è¯»å–æ ‡æ³¨ä¸­çš„åæ ‡ï¼Œæˆ–è€…éƒ½ç”¨SMPLæ¥å›žå½’å‘¢ï¼Ÿ"
"Hello, thank you for sharing your great works.    I am trying to reproduce the results on table 2. with resnet50.  However, when I evaluate on 3DPW testset with downloaded resnet50 (trained_model/ROMP_ResNet50_V1.pkl),  the result was mpjpe: 94.98 pa-mpjpe: 56.56. (the result on table 2 is 89.3, 53.5 respectively).  I followed ""python -m romp.test --configs_yml=configs/eval_3dpw_test_resnet.yml"" and I'm using pytorch==1.11.0, cuda==11.3.    I would be appreciate if you could give any advise to reproduce the performance. thank you :)"
It seems there is an unused set:   
ä½ å¥½å¤§ä½¬ï¼Œè°¢è°¢ä½ çš„ç ”ç©¶ï¼Œè¾“å‡ºçš„ç»“æžœä¸­ï¼Œsmpl_thetas æ˜¯è¡¨ç¤ºæ¯ä¸ªå…³èŠ‚ç‚¹çš„åæ ‡å—ï¼Ÿå¦‚æžœæ˜¯åæ ‡ï¼Œä¸ºä»€ä¹ˆæˆ‘ä¸èƒ½åœ¨blenderä¸­ï¼Œä½¿ç”¨24ä¸ªçƒæ‹¼å‡ºä¸€ä¸ªäººå½¢çš„åŠ¨ä½œã€‚
There should be at least in the   an short summary of the differences betwen `bev` and `romp` according to the described commands.
æˆ‘å‘çŽ°æ‚¨å…³äºŽè®­ç»ƒçš„æ–‡æ¡£é‡Œå†™çš„æ˜¯é’ˆå¯¹ROMPV1.1å’ŒROMPv1.0ç‰ˆæœ¬çš„ï¼Œè¯·é—®çŽ°åœ¨çš„masteråˆ†æ”¯æ”¯æŒè®­ç»ƒå˜›  !   
None
"Hi, @Arthur151 , thanks for your excellent work.    I carried out my ROMP training with the latest code on all the datasets listed in the README and got the training log:          But a massive gap was found between my ""MPJPE"" loss with what you posted in issue #121.  My ""MPJPE"" loss was around 30 while yours was around 110.      Did I miss something?"
"handel video /home/gwj/disk/UCF-101/YoYo now...  Using ROMP v1  /home/gwj/disk/smpl/UCF-101/  /home/gwj/disk/UCF-101/YoYo/v_YoYo_g01_c01.avi  Traceback (most recent call last):    File ""/home/gwj/disk/github/ROMP-master/simple_romp/convert.py"", line 162, in        rompDir('/home/gwj/disk/UCF-101/')    File ""/home/gwj/disk/github/ROMP-master/simple_romp/convert.py"", line 158, in rompDir      rompVideo(video_path)    File ""/home/gwj/disk/github/ROMP-master/simple_romp/convert.py"", line 124, in rompVideo      outputs = romp(image)       # å›¾ç‰‡ç»è¿‡rompåŽï¼Œä¼šå¾—åˆ°å…³èŠ‚å‚æ•°    File ""/home/gwj/anaconda3/envs/romp/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl      result = self.forward(*input, **kwargs)    File ""/disk/gwj/github/ROMP-master/simple_romp/romp/utils.py"", line 728, in wrap_func      result = func(*args, **kwargs)    File ""/disk/gwj/github/ROMP-master/simple_romp/romp/main.py"", line 158, in forward      outputs, image_pad_info = self.single_image_forward(image)    File ""/disk/gwj/github/ROMP-master/simple_romp/romp/main.py"", line 105, in single_image_forward      input_image, image_pad_info = img_preprocess(image)    File ""/disk/gwj/github/ROMP-master/simple_romp/romp/utils.py"", line 27, in img_preprocess      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  cv2.error: OpenCV(4.5.5) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'  æ‚¨å¥½ï¼Œæ¯æ¬¡è·‘å®Œconvert.pyå°±ä¼šæŠ¥è¿™ä¸ªé”™æ˜¯ä¸ºä»€ä¹ˆï¼Ÿ"
"Hey @Arthur151 , Thanks for this amazing work.  I ran` bev` on the given demo videos used in colab, Generated the `video_results.npz`  using the below command.    `!bev -m video -i /content/ROMP/demo/videos/sample_video.mp4 -o /content/drive/MyDrive/3DAI/ROMP/BEV_Results  `  Then I'm trying to export it to an fbx animation file but getting the below error,      Can you point it down what am i missing?"
Hello authorï¼I'm learning rompï¼Œit's a great workï¼  I'm curious about why three heads use the same structureï¼Œcan you give me a hintï¼Ÿ
"Hi~Thank you for your work.    Recently, When I run the trainning code by using only two datasets (dataset: 'mpii,crowdpose'), I met a problem as following:  /ROMP-master/romp/lib/models/../utils/../dataset/mixed_dataset.py"", line 73, in __getitem__      upper_bound = self.partition[dataset_id]  IndexError: index 2 is out of bounds for axis 0 with size 2    And When I print the dataset_id, it shows 1 or 2. Could you please help me with this error?    Thank you very much!"
è¯·æ•™ä½œè€…å¦‚ä½•ç”Ÿæˆpklçš„é¢„æµ‹ç»“æžœï¼Œä½¿ç”¨agoraæ•°æ®é›†çš„è¯„æµ‹apiæµ‹è¯•rompæˆ–bev
"Hi,     Q1. In the code, I can see that there are different configuration modes: (`matching_gts`,`parsing`,`forward`). Wanted to ask what these modes should be during train, validation, and test?  Based on the paper seems like `matching_gts` is used for training however it is not clear what modes are used during validation and test?    Q2. Also in `base.py` you have:     What is the difference between `self.eval_cfg` and `self.val_cfg`? Which one is for test and which one is for validation?      Q3. Also regarding the loss function, looking at your logs  , seems like only training loss is logged. Do you calculate validation loss as well? I can see that some metrics are calculated during validation. However, what do I need to do if I want to get validation loss?    Q4. Also, why is the training loss value large? I was training the code and getting similar loss values but I was thinking that I am doing something wrong until I saw your logs in the Github.        Thanks,  "
"     Hi, I noticed that there is something like this. Why do we need to scale the kp2d like this?  And how did you generate the annotation for CMU?    "
"You said you trained the backbone(HRNet, ResNet) with detection and 2dpose datasets on here(     Then could you tell me exactly which dataset you used?    Thank you"
"Thank you for your perfect work.  I have a question when I try bev command. It said "" Failure in downloading the BEV model, please download it by youself from   But I cannot find this BEV.pth.    Could you please the download link for bev inference model?    Thank you very much."
"Hello author, Excuse me, I would like to ask what is the approximate final loss function. I changed the batchsize to 16, and the loss function is bigger after it runs.  Epoch: [119][1593/1653] Time 0.35 RUN 0.77 Lr 5.000000000000001e-07 Loss 220.56 | Losses {'reg': 167.9, 'det': 52.66, 'CenterMap': 52.66, 'P_KP2D': 42.3, 'MPJPE': 29.3, 'PAMPJPE': 37.73, 'Pose': 56.1, 'Shape': 0.14, 'Prior': 2.32}  Epoch: [119][1643/1653] Time 0.34 RUN 0.77 Lr 5.000000000000001e-07 Loss 219.14 | Losses {'reg': 170.75, 'det': 48.4, 'CenterMap': 48.4, 'P_KP2D': 44.36, 'MPJPE': 29.71, 'PAMPJPE': 37.84, 'Pose': 56.38, 'Shape': 0.14, 'Prior': 2.33}  "
helloï¼ŒI have a question about projection. I want to use verts and cam_trans for projection results. Here's how I did it  !   But the result is wrong. What can I do to get the right result?
"Hi, thanks for your great work.  I want to get a .npz file by a video , when I use command `bev -m video -i a.mp4 -o ../outputs/result.mp4  --GPU=6  --renderer=pyrender` , the result is each frame's .npz file . I want to get one .npz file of a video , how can i do? I have try a lot of params like '--calc_smpl , --render_mesh , --smpl_path' , not the result is not i want."
"è¯·é—®ä¸ºä»€ä¹ˆä½¿ç”¨FoVPerspectiveCamerasè€Œä¸æ˜¯å’Œestimate_translationé‡Œé¢ä¸€æ ·çš„""é€è§†æŠ•å½±ç›¸æœºæ¨¡åž‹"",æ¯”å¦‚PerspectiveCameras.  è®­ç»ƒå’ŒæŽ¨ç†é˜¶æ®µä½¿ç”¨ä¸åŒçš„ç›¸æœºæ¨¡åž‹ä¸ä¼šå½±å“æ¨¡åž‹ç»“æžœçš„æ­£ç¡®æ€§å—?"
"è¯·æ•™ä½œè€… æˆ‘å‘çŽ°æœ¬ä»“åº“æä¾›çš„smpl_packed_infoé‡Œé¢çš„å‚æ•°ä¸Žsmplç½‘ç«™ä¸Š""basicModel_neutral_lbs_10_207_0_v1.0.0.pkl""ä¸å¤ªä¸€è‡´ï¼Œè¿™æ˜¯ä»€ä¹ˆåŽŸå› å¯¼è‡´çš„å‘¢ï¼Ÿè¿™å¯¹ç»“æžœæœ‰å½±å“å—"
"Hi, how to convert bev result to BVH format?"
ä½œè€…æ‚¨å¥½ï¼å¦‚é¢˜ï¼Œæˆ‘çŽ°åœ¨æ‰“å¼€ç½‘é¡µdownloadå›¾ç‰‡éƒ½æ˜¯å¸¦æœ‰åæ ‡çš„å›¾åƒï¼Œåˆ†è¾¨çŽ‡å¾ˆä½Žï¼Œå¤§å°åªæœ‰100kbå·¦å³ï¼Œè¯·é—®å¦‚ä½•ä¸‹è½½é«˜æ¸…å›¾åƒï¼Ÿ
"Hello, I'm reading your romp recently. If I want to generate a male or female 3D human model, how should I set it in the romp code? Thank you!"
"!   æ‚¨å¥½ï¼Œæˆ‘æ³¨æ„åˆ°æœ‰å·²ç»å¼€å‘å¥½çš„smplå·¥å…·åŒ…ï¼Œå«smplpytorchï¼Œä¸çŸ¥æ‚¨æœ‰æ²¡æœ‰æ³¨æ„è¿‡è¿™ä¸ªåŒ…ï¼Œæˆ‘æŠŠæ‚¨ä»£ç ä¸­çš„smpl_wrapperæŠ½å–å‡ºæ¥äº†ï¼Œå°è¯•ç€å¯¹æ¯”è¿™ä¸¤è€…çš„åŒºåˆ«ï¼Œæˆ‘å‘çŽ°æ‰“å‡ºæ¥çš„æ•ˆæžœæ˜¯ä½¿ç”¨æ‚¨çš„wrapperæ‰“å‡ºæ¥çš„joint(ä¸Šå›¾æ ‡è®°â€œsmpl2â€é™„è¿‘çš„æ‰“å°)å’Œsmplpytorchè¿™ä¸ªåŒ…æ‰“å‡ºæ¥çš„joint(æ ‡è®°â€œsmpl3â€é™„è¿‘çš„æ‰“å°)æ˜¯ä¸ä¸€æ ·çš„ï¼Œæˆ‘å¥½å¥‡ï¼Œå¦‚æžœæ‚¨æ³¨æ„åˆ°è¿‡smplpytorchè¿™ä¸ªåŒ…ï¼Œä¸ºä»€ä¹ˆä¸ç›´æŽ¥æ‹¿æ¥ç”¨ï¼Œè¦è‡ªå·±å®žçŽ°ä¸€ä¸ªsmplï¼Ÿæ˜¯å¦åšäº†æŸäº›ä¼˜åŒ–ï¼Ÿå¦‚æžœæ²¡æœ‰æ³¨æ„åˆ°è¿‡çš„è¯é‚£æ‰“æ‰°äº†ï¼Œæˆ‘è‡ªå·±æ…¢æ…¢å¯¹æ¯”ä¸‹ä¸ºä»€ä¹ˆä¸åŒå§â€¦â€¦  (ä¸Šå›¾ä¸­çš„wrapper,smpl_layeråˆå§‹åŒ–å¦‚ä¸‹)  from smplpytorch.pytorch.smpl_layer import SMPL_Layer  smpl_layer = SMPL_Layer(      gender='female',      model_root=smpl_path)  from smpl_wrapper import SMPLWrapper  wrapper = SMPLWrapper()  "
"I tried to convert the NPZ file of a single frame into FBX but it reports the error ""IndexError: too many indices for tensor of dimension 1"".  Also, I couldn't find the NPZ file for the exported video, which used to be exported along with the resulting video.    It will be appreciated if you can provide a method to convert the image NPZ file into FBX."
I tried several methods mentioned in the issues like setting GPU: -1. But it's still using GPU.
"Hi,    I am running the code in training mode and after two epochs I get the following error:     Can you please help me figure out why this might be the case?   I do have the model_data/parameters and it contains the following files:     "
"Here is the windows 10 platform, I see a video link on readme file shows ROMP can be used to achieve multi-person 3D mesh recovery in real time with webcam but I can't(or miss) find this part of tutorial.  Does someone can tell me where is it or where I can find information to do that?"
"Hello, Excuse me, I ran into this problem while testing. I'm running into missing parameters and no data being generated while testing.  !   "
"When viewing the NPZ file after running BEVï¼ŒI have some questions about joints  1. Is the joints relative to the root? If  I  want to get the joints of the camera coordinate system,  only need to add cam_trans to the joints?  2. The shape of joints is [N,71,3], what is the sequence of the joints?"
"Hi, I have a simple question about ROMP. I have been struggling putting people into their **correct relative position**, but is it really possible using the root-aligned SMPL meshes without predicting their transl? (And if we have camera param K, will it be possible? )      1. What is the **coordinate system** of the vertices that are used for rendering? I think we are predicting **camera coordinate system points but root-aligned**, correct?    2. Following Q1, before rendering verts onto image, there is a trans added to verts (`'cam_trans in projection.py'`) What is it? and what is `estimate_translation` actually doing? Is this estimating **root's position**?       3. I tried to replace the **verts +trans in Q2** with GT mesh, so verts=GT_verts, without any other changes to your code, but the results are not correct, I expect it to be fully matched the person on the image but there are always shifts, and I also can't use the same FOV otherwise it would be a very small mesh on the image.    Sorry if I understand anything wrong. I think rendering is the final part I didn't understand in your code. Looking forward you for your answer!    Zhengdi"
é¦–å…ˆæ„Ÿè°¢æ‚¨ä¼˜ç§€çš„å·¥ä½œï¼    æˆ‘åœ¨æ ¹æ®docs/evaluation.mdæ‰€å†™çš„å°è¯•å¤çŽ°è®ºæ–‡ä¸­çš„ç»“æžœæ—¶ï¼Œå‘çŽ°æµ‹è¯•çš„æŒ‡æ ‡å¹¶æ²¡æœ‰è¾¾åˆ°è®ºæ–‡ä¸­çš„æ°´å¹³ï¼Œæƒ³è¯·æ‚¨çœ‹ä¸€ä¸‹æ˜¯ä¸æ˜¯å“ªé‡Œå‡ºäº†é—®é¢˜ã€‚    CUDA_VISIBLE_DEVICES=0 python romp/lib/evaluation/collect_3DPW_results.py --configs_yml=configs/eval_3dpw_challenge.yml  python -m romp.test --configs_yml=configs/eval_3dpw_test.yml  python -m romp.test --configs_yml=configs/eval_3dpw_test_resnet.yml  python -m romp.test --configs_yml=configs/eval_3dpw_test_ft.yml  python -m romp.test --configs_yml=configs/eval_3dpw_test_resnet_ft.yml    è¿™å‡ è¡Œå‘½ä»¤åˆ†åˆ«å¯¹åº”çš„æ˜¯ä¸æ˜¯è®ºæ–‡ä¸­Tab.1çš„æœ€åŽä¸€è¡Œã€Tab.2çš„å€’æ•°ç¬¬ä¸€ã€äºŒè¡Œå’ŒTab.3çš„å€’æ•°ç¬¬ä¸€ã€ä¸‰è¡Œï¼Ÿ    è¿è¡Œè¿™äº›ä»£ç æˆ‘å¾—åˆ°çš„ç»“æžœå¦‚ä¸‹ï¼š    é¦–å…ˆæ˜¯3DPW_challenge:    MPJPE: 182.35274653001787  MPJPE_PA: 166.38965381899567  PCK: 26.78395448300638  AUC: 0.45113494167624285  MPJAE: 63.859989172498125  MPJAE_PA: 62.683403766225    å‰©ä¸‹çš„å››è¡Œå‘½ä»¤ç»“æžœåˆ†åˆ«ä¸ºï¼š    MPJPE                        PAMPJPE                        config  87.25                         53.33                          eval_3dpw_test.yml  94.91                         56.49                          eval_3dpw_test_resnet.yml  76.74                         47.37                          eval_3dpw_test_ft.yml  85.16                         52.10                          eval_3dpw_test_resnet_ft.yml  
æ‚¨å¥½  ä»ŽROMPå¼€å§‹å°±ä¸€ç›´å‚è€ƒæ‚¨çš„å·¥ä½œäº†ï¼Œå—ç›ŠåŒªæµ…ï¼Œä¸‡åˆ†æ„Ÿè°¢ã€‚  æ‚¨çš„æ–°ä½œBEVåœ¨äººç¾¤æ·±åº¦ä¸Šçš„æ•ˆæžœçœ‹èµ·æ¥ä»¤äººæƒŠå¹ï¼Œæˆ‘ä»¬æƒ³è¦ç”¨å®ƒåœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¯·é—®å®Œæ•´çš„BEVä»£ç å¤§è‡´åœ¨å‡ æœˆä»½å¯ä»¥å‘å¸ƒå‘¢ï¼Ÿ  
"ä½ å¥½ï¼æ‚¨åœ¨è®ºæ–‡ä¸­æåˆ°CMU panopticä¸Šé¢çš„MPJPEæ˜¯â€œThe evaluation metric is MPJPE after centering the root jointâ€ æˆ‘æ³¨æ„åˆ°ä¹Ÿæœ‰å…¶ä»–ä»¬çš„å·¥ä½œæåˆ°â€œMPJPE is calculated after aligning the pose by the root joint"" è¿™åº”è¯¥å’Œæ‚¨æ˜¯ä¸€ä¸ªæ„æ€å§ï¼Ÿï¼å¦å¤–ï¼Œè¿™é‡Œçš„æ“ä½œæ˜¯ â€æ‰€æœ‰çš„é¢„æµ‹çš„å…³èŠ‚ç‚¹éƒ½å‡åŽ»   é¢„æµ‹çš„rootå…³èŠ‚ç‚¹ç›¸å¯¹äºŽgtä¸­rootå…³èŠ‚ç‚¹çš„ç©ºé—´ä½ç§»â€œçš„æ„æ€å—ï¼Ÿè¿˜æ˜¯éœ€è¦å…¶å®ƒæ›´å¤æ‚çš„ç©ºé—´å˜åŒ–ï¼Ÿè°¢è°¢ï¼"
None
"Hi Arthur, great congratulations to your wonderful work!     I saw you mentioned that, ""poses"" in the result file come from rotation, which is parent relatived.  But who's pelvis' parent?  Could you please give me any information that  how did you calculate pelvis's rotation( very first 3 elements of 72 )?"
"How to get COCO2014-All-ver01.json, where is this file?"
"Hi, I tried to take your predicted smpl **Î¸** to a smplx model while disabling the face and hand, but it seems that the output is not the same.    Is there a way to convert your pose to smplx?"
å¤§ä½¬ï¼ŒCMUä¸Šçš„æµ‹è¯•é›†æ€Žä¹ˆæ²¡æœ‰æ‰¾åˆ°ï¼Ÿ
"Hi, I notice that the person center is computed from 'visible' keypoints, which I think refers to the body joints that we can literally see in the image, instead of the occluded joints.    But for some datasets, the annotation is made by machine, which means that even the 2 person is heavily occluded, there might be full set of 2d keypoints in the annotation file. In this case, the '`person_center`' of the person in the back will actually be on the first person's body:    !     I think **ROMP has the ability to predict the shape from the center of 'visible body parts'**. And when the two center comes too close to each other, the **CAR mechanism** will be activated to push them away. **But there is no guarantee that the center will be pushed to a pixel that belongs to the occluded person**. In this case, is it really possible to infer 2 different people using `pixels` from the same person?    So, in the end, can we really use the massive machine annotated data for training, concerning the above situation ? Also, can we train ROMP with bounding box center?     I'm not sure if this is technically feasible. But I think the center based representation is actually an alternative of bounding box, these single points might carry equal information as bboxes due to the receptive field, it's no longer a single point, but a point that can represent an area. So to some extend, this is feasible, am I understanding correctly ?    But actually, in my previous works of feature matching / pixel-wise descriptor learning. This is actually difficult problem, we would normally filter those occluded points as it would carry ambiguity. For example, we can't let a pixel of a man to be matched to a car. even if there is an implicit man's pixel behind the car"
"It seems, that the version of `simple-romp` might be wrong:     According to:     It should be:      Because of:     How do you think about this?    I would also love to have an version-tag (`simple-romp-v0.3.1` ?) to be able to create `conda` packages easierly:     Of course i would also publish it for you, if it's okay."
"Can I convert .pkl file to .pth? For further conversion to ptl?    I tried to convert using `torch.jit.script`, but I have error   "
"Feature request: VMC protocol export support       Ready to help, implementing it."
"I tested an clean install of `simple-romp`:     and tested it by:     As you can clearly see, there is no   package installed.  An workaround is to install the `black` package in an manual way.  For example also by `pip`:     But it should be already installed, because:      Is this an bug?  Any ideas to fix it?    Also it seems to me, that the `black` package is not required just for testing.  So how about changing the `import`s accordingly?"
ä½œè€…æ‚¨å¥½ï¼Œæ„Ÿè°¢æ‚¨çš„è´¡çŒ®ï¼  æˆ‘æœ‰ä¸¤ä¸ªæ•°æ®å¤„ç†çš„é—®é¢˜ï¼Œæƒ³è¯·æ•™ï¼š  1ï¼‰human3.6mçš„annotsï¼Œæ€Žä¹ˆå¯¹åº”åˆ°è§†é¢‘ä¸Š  2ï¼‰ç”¨æ‚¨æä¾›çš„h36m smpl GTï¼Œè®­ç»ƒå•äººä»»åŠ¡ï¼Œä¸­å¿ƒæŠ å›¾ä½œä¸ºè¾“å…¥ã€‚GTä¸­é™¤äº†è¦æ”¹2D jointsï¼Œ3D Jointså’Œsmplå‚æ•°æ˜¯å¦éœ€è¦æ”¹åŠ¨  æœŸå¾…æ‚¨çš„å›žå¤ï¼Œè°¢è°¢ã€‚
"     Here, you mean to normalize back bbox_hw_norm by `(bbox_norm+1)/2*args().centermap_size` . But it's not correct, it should be `bbox_hw_oncm = (bbox_norm)/2*args().centermap_size `:    if x_min=a, x_max=b, then W=b-a    In the code you first normalize x_min and x_max by xmin = xmin *2 -1, or, if it's computed from kps, you would normalize kps first, then compute the box based on the normalized kps.     So the correct way to transfer bboxes_HW_norm back is [(x_max+1) /2 * centermap_size - (x_min+1) /2 * centermap_size].  `bbox_hw_oncm = (bbox_norm)/2*args().centermap_size `    This is not equal to [(x_max - x_min) + 1] /2 *centermap_size    This would make it completely wrong. To the current version, it doesn't really matter that much, as it only affects the radius calculation."
"     Hi, I notice that     1.when calculating bbox_hw_normed and person_centers from bbox annotation data (e.g. CrowdHuman). The vbox is actually not normalized to [-1, 1] because the `process_kps()` only process kps[:,:2].  "
è¯·æ•™ä½œè€…anchor map ä¸Šæ¯ä¸ªä½ç½®çš„å€¼æ˜¯å¦‚ä½•è®¾ç½®çš„å‘¢ï¼Œæ–‡ç« é‡Œä¼¼ä¹Žæ²¡æœ‰è¯¦ç»†è¯´æ˜Žï¼Ÿ
"To reproduce: `python -m romp.lib.dataset.lsp`    Stack trace:    root@d6613ad97b7d:/workspace/pose-estimation/ROMP# python -m romp.lib.dataset.lsp                                        yaml_timestamp  /workspace/pose-estimation/ROMP/active_configs/active_context_2022-03-30_14_49_41.yaml                  Confidence: 0.2                                                                                                           Traceback (most recent call last):                                                                                              File ""/opt/conda/lib/python3.7/runpy.py"", line 193, in _run_module_as_main                                                  ""__main__"", mod_spec)                                                                                                       File ""/opt/conda/lib/python3.7/runpy.py"", line 85, in _run_code                                                             exec(code, run_globals)                                                                                                     File ""/workspace/pose-estimation/ROMP/romp/lib/dataset/lsp.py"", line 90, in                                          dataset=LSP(regress_smpl=True)                                                                                             File ""/workspace/pose-estimation/ROMP/romp/lib/dataset/lsp.py"", line 11, in __init__                                        self.load_data()                                                                                                           File ""/workspace/pose-estimation/ROMP/romp/lib/dataset/lsp.py"", line 38, in load_data                                       self.load_eft_annots(os.path.join(config.project_dir, 'data/eft_fit/LSPet_ver01.json'))                                    File ""/workspace/pose-estimation/ROMP/romp/lib/dataset/lsp.py"", line 44, in load_eft_annots                                 annots = json.load(open(annot_file_path,'r'))['data']                                                                 FileNotFoundError: [Errno 2] No such file or directory: '/workspace/pose-estimation/ROMP/data/eft_fit/LSPet_ver01.json'"
"Hi, thanks for your great work!  I am trying to use the simple-romp output .npz file of 'joints' as 3D keypoints positions, but i find that it has no global root position movement, i find that in romp export folder, it uses 'cam_trans'   to trans global position. But the simple-romp output .npz file only has 'cam' param, does the cam has the same meaning with 'cam_trans' ?  Look forward to your reply!"
ä½œè€…æ‚¨å¥½ï¼Œæˆ‘æœ‰ä¸€ä¸ªå°é—®é¢˜ï¼Œè¯·é—®ROMPæ˜¯è”åˆ2D3Dä¸€èµ·è®­ç»ƒçš„è¿˜æ˜¯é‡‡ç”¨ä»¥2déª¨æž¶åºåˆ—ä¸ºè¾“å…¥çš„æ–¹å¼ï¼Ÿ
"!   Recently, I read your other paper ""Synthetic Training for Monocular Human Mesh Recovery."" , which is also an excellent work. I am very interested in D2S projection. I agree that the formula (12),(13) make sense, however, the formula (8)-(11) maybe something wrong.  Firstly, (10) is the \delta s, but why (13) is s_i? Secondly, the second ""="" in both (10) and (11) are wrong. An I wonder why \delta s = f_x/d - f_x*z_i/ d+z_i, I think it should be \delta s = f_x/d - f_x/ d+z_i.  I would very much like to get the correct version of these formulas. Thank you so much!"
Hi! I was wondering what is this for. Why do we need to change the XY order of center? I used to ignore this part before   
"Hi, I was wondering how should we decide focal length when we are in various situation, for example sometimes very close or far away    And here in ROMP, we rescale the image into 512. When we apply to a different camera which from what I see the focal length is [1265, 1265] while the image has different size. If I want to project the 3D mesh onto the image,. How should I change the focal length?"
"ä½œè€…æ‚¨å¥½ï¼Œæˆ‘å°†Basic ResNet blockçš„æ•°é‡æ”¹ä¸º3ï¼Œä¿®æ”¹ä¹‹åŽè®­ç»ƒé€Ÿåº¦ç‰¹åˆ«æ…¢ï¼ŒåŽæ¥æˆ‘åˆå°†å…¶æ”¹å›žæ¥ï¼Œå¯æ˜¯è®­ç»ƒä¾æ—§ä¸å¦‚ä»Žå‰ï¼Œéƒ¨åˆ†logå¦‚ä¸‹æ‰€ç¤ºï¼š  Epoch: [0][100/19445] Time 2.87 RUN 0.87 Lr 5e-05 Loss 617.70 | Losses {'reg': 567.27, 'det': 50.43, 'CenterMap': 50.43, 'P_KP2D': 167.41, 'MPJPE': 149.72, 'PAMPJPE': 88.8, 'Pose': 151.92, 'Shape': 0.76, 'Prior': 8.66}  Epoch: [0][150/19445] Time 3.50 RUN 0.88 Lr 5e-05 Loss 507.33 | Losses {'reg': 462.25, 'det': 45.08, 'CenterMap': 45.08, 'P_KP2D': 136.46, 'MPJPE': 143.47, 'PAMPJPE': 69.17, 'Pose': 109.13, 'Shape': 0.86, 'Prior': 3.17}  Epoch: [0][200/19445] Time 4.52 RUN 0.90 Lr 5e-05 Loss 461.33 | Losses {'reg': 425.56, 'det': 35.76, 'CenterMap': 35.76, 'P_KP2D': 103.63, 'MPJPE': 138.02, 'PAMPJPE': 79.82, 'Pose': 100.42, 'Shape': 0.81, 'Prior': 2.87}  Epoch: [0][250/19445] Time 4.62 RUN 0.90 Lr 5e-05 Loss 432.20 | Losses {'reg': 386.74, 'det': 45.46, 'CenterMap': 45.46, 'P_KP2D': 95.82, 'MPJPE': 124.5, 'PAMPJPE': 67.42, 'Pose': 95.41, 'Shape': 0.83, 'Prior': 2.76}  Epoch: [0][300/19445] Time 4.48 RUN 0.90 Lr 5e-05 Loss 446.01 | Losses {'reg': 402.76, 'det': 43.25, 'CenterMap': 43.25, 'P_KP2D': 91.53, 'MPJPE': 140.57, 'PAMPJPE': 79.84, 'Pose': 87.18, 'Shape': 0.84, 'Prior': 2.8}  Epoch: [0][350/19445] Time 5.96 RUN 0.90 Lr 5e-05 Loss 434.57 | Losses {'reg': 389.19, 'det': 45.38, 'CenterMap': 45.38, 'P_KP2D': 91.37, 'MPJPE': 127.12, 'PAMPJPE': 75.18, 'Pose': 91.93, 'Shape': 0.69, 'Prior': 2.9}  Epoch: [0][400/19445] Time 5.32 RUN 0.90 Lr 5e-05 Loss 415.13 | Losses {'reg': 372.84, 'det': 42.29, 'CenterMap': 42.29, 'P_KP2D': 77.94, 'MPJPE': 138.16, 'PAMPJPE': 71.34, 'Pose': 81.85, 'Shape': 0.68, 'Prior': 2.87}  Epoch: [0][450/19445] Time 6.87 RUN 0.91 Lr 5e-05 Loss 417.61 | Losses {'reg': 371.05, 'det': 46.56, 'CenterMap': 46.56, 'P_KP2D': 77.61, 'MPJPE': 125.83, 'PAMPJPE': 85.55, 'Pose': 78.66, 'Shape': 0.72, 'Prior': 2.69}  Epoch: [0][500/19445] Time 5.74 RUN 0.91 Lr 5e-05 Loss 425.18 | Losses {'reg': 382.45, 'det': 42.73, 'CenterMap': 42.73, 'P_KP2D': 75.84, 'MPJPE': 139.61, 'PAMPJPE': 88.98, 'Pose': 74.82, 'Shape': 0.6, 'Prior': 2.6}    Timeä¹‹å‰éƒ½ä¸º0ï¼ŒRUNè¦æ¯”ä¹‹å‰å¤§å°†è¿‘ä¸€å€ï¼Œè¯·é—®ä½œè€…æˆ‘å¯ä»¥æ£€æŸ¥å“ªé‡Œæ¥è¿›è¡Œè°ƒæ•´ï¼Ÿ"
ä½œè€…æ‚¨å¥½ï¼Œæˆ‘æƒ³æ‹¿ä½ ä»¬çš„æ¨¡åž‹åœ¨è‡ªå»ºæ•°æ®é›†ä¸Šåšè®­ç»ƒï¼Œä½†æ˜¯ç›´æŽ¥è®­æ•ˆæžœä¸å¥½ï¼ŒåšcropåŽåº”è¯¥ä¼šæ”¹å–„ä¸€ç‚¹ã€‚ä½ ä»¬çš„ä»£ç é‡Œæä¾›äº†æ ¹æ®bboxè¿›è¡Œcropçš„æ“ä½œå—ï¼Œæˆ‘æ³¨æ„åˆ°è®­ç»ƒçš„æ—¶å€™æœ‰æ¦‚çŽ‡ä¼šçœ‹åˆ°cropåŽçš„å¯è§†åŒ–ï¼Œä½†æ˜¯æ²¡æœ‰æ‰¾åˆ°åšæ“ä½œçš„ä»£ç ä½ç½®ã€‚
None
è¯·æ•™ä¸ªé—®é¢˜ï¼Œæˆ‘åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­å‘çŽ°è‚˜å…³èŠ‚ç”±äºŽé¢„æµ‹é”™è¯¯å¯¼è‡´æ¥å›žç¿»è½¬ï¼Œæ‚¨çŸ¥é“å¦‚ä½•é€šè¿‡é™åˆ¶è§’åº¦æ¥æ ¡æ­£è§’åº¦é—®é¢˜å—ï¼Ÿ
"I am running the following command: ""python -m romp.lib.dataset.pw3d"" and I am getting the following error:     Where am I supposed to get the 'SMPL_NEUTRAL.pkl' file?"
æ•°æ®å¢žå¹¿è²Œä¼¼å¹¶æ²¡æœ‰ç”¨åˆ°clipï¼Ÿ  ROMPåœ¨ç‰©ä½“é®æŒ¡æˆ–äººä½“åŠèº«çš„æƒ…å†µä¸‹æ³›åŒ–æ€§èƒ½ä¹Ÿå¾ˆæ£’å‘€ï¼Œè¯·é—®ä¸»è¦æ˜¯ä»€ä¹ˆåŽŸå› å‘¢ï¼Ÿå¦‚æžœæƒ³è£å‰ªå›¾ç‰‡å˜æˆåŠèº«çš„ï¼Œç›¸åº”çš„æ ‡ç­¾ä¸å¯è§çš„keypointè®¾ç½®ä¸ºé›¶å—ï¼Ÿ
"<img width=""567"" alt=""image"" src=""     hi, @Arthur151 , why drop last two hand joints"
"Hi there,    I save joint positions 'j3d_smpl24' from npz as an obj file, and then export fbx from the same npz.  I find that the joint positions of same frame in these two files can't match as below:  <img width=""768"" alt=""screenshot"" src=""     Could you please tell me how to fit  'j3d_smpl24' to fbx ?  And I also wonder the meaning of 'cam_trans' in model' s output.  Thank you very much!!!"
"Hi, I notice that your **ROMP_HRNet_32.pkl** was trained on `smpl_mesh_root_align=False`. But in **v1.yml**, smpl_mesh_root_align is not set, so it's default value `True`.    So My questions are:     1. (**Solvedâœ”**) Firstly I found my model perform having the same issue as resnet (mesh shift), then I found the reason: `Image.yml` is initially designed for ROMP_HRNet_32.pkl, which was trained on smpl_mesh_root_align=False. If we want to test on image using our model trained from pre-trained model using hrnet and v1.yml, the smpl_mesh_root_align in image.yml should also be set to `True`, just like resnet #106 . So this was solved.    2. When should smpl_mesh_root_align be True or False? Why did you set it to True for v1.yml and resnet, although it's false for ROMP_HRNet_32.pkl? I think for 3D joints loss, it doesn't matter as long as we would do another alignment before calculating MPJPE/PAMPJPE. And for the 2D part, the **weak camera parameters** will be automatically learnt to project those 3D joints to align with GT_2d as long as it's consistent all the time. ~So the last question is:    3. During **fine-tuning** from your model: **ROMP_HRNet_32.pkl using v1_hrnet_3dpw_ft.yml**. the smpl_mesh_root_align is also default value `True`, However, **ROMP_HRNet_32.pkl** was trained with `smpl_mesh_root_align=True`.     As we know from question1: if we use different setting of smpl_mesh_root_align, the visualization will be shifted, I think this could be a problem for training and fine-tuning.    And I tried to train with smpl_mesh_root_align **from scratch**, but it's ended up with error below:       I'm still debugging anyway.  "
I am new to PyTorch and ROMP works fine with 'pyrender' for me on Windows 10 and Python 3.9.7.  So why should i use ROMP with 'pytorch3d' instead?
when testing the demo video on google colab using the command:         I got the following error.         Any help would be appreciated.
I successfully installed the requirements for this on my macOS M1 Max machine. When I run    `python3 -m romp.predict.image --inputs=demo/images --output_dir=demo/image_results`    I get:    `[1]    59077 segmentation fault  python3 -m romp.predict.image --inputs=demo/images`    The demo images are the default ones from this repo. I did not change any settings or configs.    Any tips would be appreciated.
None
Delete and move to #121   
"ä½œè€…æ‚¨å¥½ï¼Œæ‚¨æ‰€æä¾›çš„7ä¸ªæ•°æ®é›†ä¸­æˆ‘æœ‰ä¸¤ä¸ªè®­ç»ƒæœ‰é—®é¢˜ï¼Œçš†ä¸ºFileNotFoundErrorã€‚    **å…¶ä¸­lspçš„æŠ¥é”™å¦‚ä¸‹ï¼š**  Traceback (most recent call last):    File ""/home/omnisky/anaconda3/lib/python3.7/runpy.py"", line 193, in _run_module_as_main      ""__main__"", mod_spec)    File ""/home/omnisky/anaconda3/lib/python3.7/runpy.py"", line 85, in _run_code      exec(code, run_globals)    File ""/data01/wyjh/ROMP/romp/train.py"", line 149, in        main()    File ""/data01/wyjh/ROMP/romp/train.py"", line 145, in main      trainer = Trainer()    File ""/data01/wyjh/ROMP/romp/train.py"", line 14, in __init__      self.loader = self._create_data_loader(train_flag=True)    File ""/data01/wyjh/ROMP/romp/base.py"", line 133, in _create_data_loader      datasets = MixedDataset(train_flag=train_flag)    File ""/data01/wyjh/ROMP/romp/lib/models/../utils/../dataset/mixed_dataset.py"", line 36, in __init__      self.datasets =   for ds in datasets_used]    File ""/data01/wyjh/ROMP/romp/lib/models/../utils/../dataset/mixed_dataset.py"", line 36, in        self.datasets =   for ds in datasets_used]    File ""/data01/wyjh/ROMP/romp/lib/models/../utils/../dataset/lsp.py"", line 11, in __init__      self.load_data()    File ""/data01/wyjh/ROMP/romp/lib/models/../utils/../dataset/lsp.py"", line 38, in load_data      self.load_eft_annots(os.path.join(config.project_dir, 'data/eft_fit/LSPet_ver01.json'))    File ""/data01/wyjh/ROMP/romp/lib/models/../utils/../dataset/lsp.py"", line 44, in load_eft_annots      annots = json.load(open(annot_file_path,'r'))  for ds in datasets_used]    File ""/data01/wyjh/ROMP/romp/lib/models/../utils/../dataset/mixed_dataset.py"", line 36, in        self.datasets =   for ds in datasets_used]    File ""/data01/wyjh/ROMP/romp/lib/models/../utils/../dataset/mpi_inf_3dhp.py"", line 17, in __init__      self.pack_data(annots_file_path)    File ""/data01/wyjh/ROMP/romp/lib/models/../utils/../dataset/mpi_inf_3dhp.py"", line 111, in pack_data      annot2 = sio.loadmat(annot_file_path)['annot2']    File ""/home/omnisky/wyj/lib/python3.7/site-packages/scipy/io/matlab/mio.py"", line 224, in loadmat      with _open_file_context(file_name, appendmat) as f:    File ""/home/omnisky/anaconda3/lib/python3.7/contextlib.py"", line 112, in __enter__      return next(self.gen)    File ""/home/omnisky/wyj/lib/python3.7/site-packages/scipy/io/matlab/mio.py"", line 17, in _open_file_context      f, opened = _open_file(file_like, appendmat, mode)    File ""/home/omnisky/wyj/lib/python3.7/site-packages/scipy/io/matlab/mio.py"", line 45, in _open_file      return open(file_like, mode), True  FileNotFoundError: [Errno 2] No such file or directory: '/data01/wyjh/ROMP/romp/lib/dataset/mpi_inf_3dhp/S1/Seq1/annot.mat'  ï¼ˆåŒæ ·ï¼Œmpi_inf_3dhpä¸­ä¹Ÿæ²¡æœ‰S1è¿™ä¸ªæ–‡ä»¶å¤¹ï¼‰    æˆ‘è¯•ç€æ‰¾äº†å®˜ç½‘æ‰€æä¾›çš„æ•°æ®é›†å‘çŽ°å¹¶æ²¡æœ‰æ‰€ç¼ºå†…å®¹ï¼Œå¸Œæœ›ä½œè€…å¯ä»¥ç»™æˆ‘æŒ‡ç‚¹ä¸€ä¸‹è§£å†³é—®é¢˜çš„æ–¹å‘ã€‚è°¢è°¢æ‚¨ï¼"
æ‚¨å¥½ï¼Œéžå¸¸æ„Ÿè°¢èƒ½å¤Ÿå¼€æºè¿™ç¯‡paperæ¨¡åž‹ï¼Œæ•ˆæžœçœŸçš„éžå¸¸æ£’ã€‚æˆ‘è¿™è¾¹å¸Œæœ›èƒ½å¤Ÿæ›¿æ¢backboneè¿›è¡Œè’¸é¦ï¼Œè¯·é—®æ›´æ¢backboneè¿›è¡Œè’¸é¦è®­ç»ƒæœ‰ä»€ä¹ˆéœ€è¦æ³¨æ„çš„ä¹ˆï¼Ÿæˆ‘è¿™è¾¹æœ‰äº›ç–‘æƒ‘ä¸çŸ¥é“èƒ½å¦å¸®å¿™è§£ç­”ä¸€ä¸‹ï¼š    1. æ›´æ¢çš„backboneæ¨¡åž‹ä¸€èˆ¬æ˜¯éœ€è¦é€šè¿‡ä»€ä¹ˆé¢„è®­ç»ƒå¾—åˆ°å‘¢ï¼Ÿã€æ£€æµ‹ or åˆ†ç±»ä»»åŠ¡ï¼Ÿã€‘  2. æ›´æ¢backboneä»¥åŽï¼Œbackboneéƒ¨åˆ†è¿˜éœ€è¦ç»§ç»­è®­ç»ƒä¹ˆï¼Ÿ  3. æ›´æ¢backboneä»¥åŽæˆ‘çœ‹resnetæœ€åŽè¾¹è¿˜æœ‰ä¸€ä¸ªdeepconvçš„é€‚é…å±‚ï¼Œå…³äºŽè¿™ä¸€å±‚ï¼Œéœ€è¦å•ç‹¬è’¸é¦è®­ç»ƒä¹ˆï¼Ÿ  4. è’¸é¦çš„æ—¶å€™lossæˆ‘è¿™è¾¹ç”¨çš„æ˜¯huber lossï¼Œåœ¨backboneç›´æŽ¥è¾“å‡ºå±‚åšçš„ã€ç›®å‰è¾“å‡ºç»™headéƒ¨åˆ†ä¿æŒäº†å’Œhrnetä¸€è‡´çš„æ•°æ®ï¼Œä¸çŸ¥æ˜¯å¦åˆç†ï¼Œæˆ–è€…è¯´åº”è¯¥è€ƒè™‘ç”¨resnetä½œä¸ºteacherï¼Ÿã€‘  5. è’¸é¦çš„loss functionå»ºè®®ç›´æŽ¥åœ¨æœ€ç»ˆå±‚åšè¿˜æ˜¯æ›´æ—©ä¸€äº›ï¼Ÿ    ä¸‡åˆ†æ„Ÿè°¢ï¼    Bestï¼
!   å¯¹äºŽè¿™ç§è§†è§’ç›¸æœºå‚æ•°åº”è¯¥æ€Žä¹ˆè°ƒæ•´å‘¢ï¼Ÿ
å¯¹è¿™æ–¹é¢æ˜¯é›¶åŸºç¡€ï¼Œæ„Ÿè°¢æŒ‡å¯¼
"Hello, thanks for your great work!    Actually, I have used your code to train my own dataset and saved the dict.  Now I want to render the predicted meshes.   How can I get the renderings using the saved 'verts', 'pose', 'betas', 'cam', 'trans'?  I've got the SMPL model and read your code visualization.py. However, it seems you don't use the 'cam' as cam_param.  If I do what visualization.py does, I can't get a correct rendered image.    So could you please tell me how to get the correct rendered images using saved dict information?"
!   python 3.8
ä½œè€…æ‚¨å¥½ï¼Œæ‚¨æ‰€æä¾›çš„è®­ç»ƒconfigsä¸­GPUSçš„å‚æ•°è®¾å®šï¼Œç”±äºŽæˆ‘å·¥ä½œå®¤çš„æ˜¾å¡æ¯”è¾ƒç´§å¼ ï¼Œæ‰€ä»¥å¾ˆéš¾ä»Ž0å·å¡å¼€å§‹é€‰æ‹©ï¼Œè¯·é—®ä½œè€…æ€Žä¹ˆæ”¹æ‰å¯ä»¥è°ƒç”¨éž0å·å¡è¿›è¡Œè®­ç»ƒï¼Ÿ
"!     ä½œè€…ä½ å¥½ï¼Œéžå¸¸æ„Ÿè°¢æ‚¨æä¾›çš„ç®—æ³•ã€‚å…³äºŽdetect_occluded_personå‡½æ•°æˆ‘æœ‰äº›ç–‘é—®ï¼Œå¸Œæœ›æ‚¨èƒ½è§£ç­”ã€‚  çº¢æ¡†éƒ¨åˆ†çš„å®žçŽ°æ˜¯ä¸ºäº†è®¡ç®—ä¸¤ä¸ªäººæ˜¯å“ªä¸ªè¢«é®æŒ¡äº†å§ï¼Œä½†æ˜¯çœ‹è¿™éƒ¨åˆ†å®žçŽ°å¹¶ä¸èƒ½å®žçŽ°è¿™ä¸ªåŠŸèƒ½å§ã€‚æ„Ÿè§‰full_kp2ds[inds,:,0]å’Œkp2d[:,0]æ˜¯ä¸€æ ·çš„å•Šã€‚"
!   æŠ¥é”™å¦‚å›¾  è¯·é—®è¿™ä¸ªé—®é¢˜å¦‚ä½•è§£å†³å‘¢ï¼Ÿ
"ä½œè€…æ‚¨å¥½ï¼Œæˆ‘åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­é‡åˆ°äº†å¦‚ä¸‹é”™è¯¯ï¼šcv2.error: OpenCV(4.5.4) /tmp/pip-req-build-21t5esfk/opencv/modules/core/src/matrix.cpp:466: error: (-215:Assertion failed) _step >= minstep in function 'Mat'  å…·ä½“æƒ…å†µæ˜¯ï¼Œæˆ‘ä¹‹å‰è®­ç»ƒmpiiå’Œcrowdposeæ—¶ï¼Œåœ¨ç¬¬äº”æ¬¡è¿­ä»£åŽæŠ¥äº†è¿™ä¸ªé”™è¯¯ï¼ŒåŽæ¥åˆè·‘äº†ä¸€æ¬¡ï¼Œåœ¨ç¬¬24ä»£æŠ¥äº†è¿™ä¸ªé”™è¯¯ï¼Œä»Šå¤©è®­ç»ƒcocoä¹‹åŽçš„ç¬¬äº”æ¬¡è¿­ä»£æŠ¥äº†è¿™ä¸ªé”™è¯¯ã€‚å…·ä½“æŠ¥é”™å¦‚ä¸‹ï¼š  Traceback (most recent call last):    File ""/home/omnisky/anaconda3/lib/python3.7/runpy.py"", line 193, in _run_module_as_main      ""__main__"", mod_spec)    File ""/home/omnisky/anaconda3/lib/python3.7/runpy.py"", line 85, in _run_code      exec(code, run_globals)    File ""/data01/wyjh/ROMP/romp/train.py"", line 149, in        main()    File ""/data01/wyjh/ROMP/romp/train.py"", line 146, in main      trainer.train()    File ""/data01/wyjh/ROMP/romp/train.py"", line 34, in train      self.train_epoch(epoch)    File ""/data01/wyjh/ROMP/romp/train.py"", line 81, in train_epoch      for iter_index, meta_data in enumerate(self.loader):    File ""/home/omnisky/wyj/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 521, in __next__      data = self._next_data()    File ""/home/omnisky/wyj/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1203, in _next_data      return self._process_data(data)    File ""/home/omnisky/wyj/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1229, in _process_data      data.reraise()    File ""/home/omnisky/wyj/lib/python3.7/site-packages/torch/_utils.py"", line 425, in reraise      raise self.exc_type(msg)  cv2.error: Caught error in DataLoader worker process 0.  Original Traceback (most recent call last):    File ""/home/omnisky/wyj/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py"", line 287, in _worker_loop      data = fetcher.fetch(index)    File ""/home/omnisky/wyj/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch      data = [self.dataset[idx] for idx in possibly_batched_index]    File ""/home/omnisky/wyj/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in        data = [self.dataset[idx] for idx in possibly_batched_index]    File ""/data01/wyjh/ROMP/romp/lib/models/../utils/../dataset/mixed_dataset.py"", line 71, in __getitem__      annots = self.datasets[inds][index_sample]    File ""/data01/wyjh/ROMP/romp/lib/models/../utils/../dataset/image_base.py"", line 342, in __getitem__      return self.get_item_single_frame(index)    File ""/data01/wyjh/ROMP/romp/lib/models/../utils/../dataset/image_base.py"", line 98, in get_item_single_frame      image, dst_image, org_image = self.prepare_image(image, image_wbg, augments=(color_jitter, syn_occlusion))    File ""/data01/wyjh/ROMP/romp/lib/models/../utils/../dataset/image_base.py"", line 230, in prepare_image      dst_image = cv2.resize(image, tuple(self.input_shape), interpolation = cv2.INTER_CUBIC)  cv2.error: OpenCV(4.5.4) /tmp/pip-req-build-21t5esfk/opencv/modules/core/src/matrix.cpp:466: error: (-215:Assertion failed) _step >= minstep in function 'Mat'  æˆ‘æƒ³çš„æ˜¯è¦ä¸è¦æ¢ä¸€ä¸‹opencvç‰ˆæœ¬ï¼Œè¯´ä¸å®šèƒ½æœ‰æ•ˆæžœï¼Œä½†è¿˜æ˜¯æƒ³å¬å¬ä½œè€…çš„æŒ‡å¯¼ï¼Œè°¢è°¢æ‚¨ï¼"
None
"Hello,   Thanks for your efforts on this excellent repository. I have tried your pretrained model and got nice results. And I have a small question here. How did you generate the parsed data from Human36M? Do you mind share your method of collecting the pose and shape parameters from the Human36M dataset?"
Why did you set k_range=7 (in the paper it's 5) ? Did you just empirically select the value or is there any reference?
"åœ¨è°ƒè¯•ç¨‹åºçš„æ—¶å€™è¿è¡Œresnetä¸»å¹²è®­ç»ƒlspå’Œæµ‹è¯•çš„è¿‡ç¨‹ä¸­éƒ½æŠ¥äº†åŒæ ·çš„é”™è¯¯ï¼Œå…·ä½“å†…å®¹å¦‚ä¸‹ï¼š  Traceback (most recent call last):    File ""/home/omnisky/anaconda3/lib/python3.7/runpy.py"", line 193, in _run_module_as_main      ""__main__"", mod_spec)    File ""/home/omnisky/anaconda3/lib/python3.7/runpy.py"", line 85, in _run_code      exec(code, run_globals)    File ""/data01/wyjh/ROMP/romp/train.py"", line 149, in        main()    File ""/data01/wyjh/ROMP/romp/train.py"", line 146, in main      trainer.train()    File ""/data01/wyjh/ROMP/romp/train.py"", line 34, in train      self.train_epoch(epoch)    File ""/data01/wyjh/ROMP/romp/train.py"", line 81, in train_epoch      for iter_index, meta_data in enumerate(self.loader):    File ""/home/omnisky/wyj/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 521, in __next__      data = self._next_data()    File ""/home/omnisky/wyj/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1203, in _next_data      return self._process_data(data)    File ""/home/omnisky/wyj/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1229, in _process_data      data.reraise()    File ""/home/omnisky/wyj/lib/python3.7/site-packages/torch/_utils.py"", line 425, in reraise      raise self.exc_type(msg)  TypeError: Caught TypeError in DataLoader worker process 0.  Original Traceback (most recent call last):    File ""/home/omnisky/wyj/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py"", line 287, in _worker_loop      data = fetcher.fetch(index)    File ""/home/omnisky/wyj/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch      data = [self.dataset[idx] for idx in possibly_batched_index]    File ""/home/omnisky/wyj/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in        data = [self.dataset[idx] for idx in possibly_batched_index]    File ""/data01/wyjh/ROMP/romp/lib/models/../utils/../dataset/mixed_dataset.py"", line 71, in __getitem__      annots = self.datasets[inds][index_sample]    File ""/data01/wyjh/ROMP/romp/lib/models/../utils/../dataset/image_base.py"", line 342, in __getitem__      return self.get_item_single_frame(index)    File ""/data01/wyjh/ROMP/romp/lib/models/../utils/../dataset/image_base.py"", line 85, in get_item_single_frame      info = self.get_image_info(index)    File ""/data01/wyjh/ROMP/romp/lib/models/../utils/../dataset/lsp.py"", line 67, in get_image_info      image = cv2.imread(imgpath)[:,:,::-1]  TypeError: 'NoneType' object is not subscriptable  ä»¥ä¸Šæ˜¯è¿è¡Œresnetä¸»å¹²è®­ç»ƒlspæ—¶çš„æŠ¥é”™ï¼Œæµ‹è¯•çš„æ—¶å€™åŸºæœ¬ä¸€æ ·ï¼Œåªä¸è¿‡åœ¨è¿™é‡Œæ˜¯lsp.pyçš„ç¬¬67è¡Œï¼Œè€Œæµ‹è¯•çš„æ—¶å€™æ˜¯pw3d.pyçš„ç¬¬124è¡Œã€‚æˆ‘æƒ³çŸ¥é“æˆ‘åº”è¯¥å¦‚ä½•åŽ»è§£å†³è¿™ä¸ªé”™è¯¯ï¼Œå¸Œæœ›ä½œè€…èƒ½ç»™æˆ‘ä¸€äº›æŒ‡ç‚¹ï¼Œè°¢è°¢æ‚¨ï¼"
"My laptop have 4G mem, but I got OOM when run.    Does there any small model provided? Does it need 4G gpu mem to run?    "
              aa2rotmat(axis_angle) returns rotation matrix(3x3) not euler angle(3x1)ï¼Œmaybe it is a bug.
"ROMP is an impressive work indeed! Thanks for sharing the code. Besides, it's admirable of you to deal with all the issues in time.  I have a little question about preprocessing of MPI_INF_3DHP dataset.  I examined the `img_names` field in `dataset/mpi-inf-3dhp/cluster_results_noumap_mpiinf_kmeans.npz` and found out that there exists `video_     I wondered how to match the original videos with `annots.npz` or `cluster_results_noumap_mpiinf_kmeans.npz` that is officially provided. To put it another way, to which original videos that the frames with filenames containing `video_[9,10]` correspond?  It'd be appreciated if you could give me some advice. Thank you for your time."
"Thank you very much for sharing the code.   I used the pretrained backbone (trained_models/pretrain_hrnet.py) you provided, but found that the result is not ideal, worse than train from scratch, here are my logs:        By the way, the pretrained backbone you provide may need to change the key name (plus 'backbone.') in order to run normally. current_model_statedict.keys():      and pretrained_model_statedict.keys():     So I change base.py in L49 to   `model = load_model(self.model_path, model, prefix='', drop_prefix='backbone.', fix_loaded=False)`    Looking forward to your replyï¼"
"     Hi, How did you get ""cam"" info ? I want to use my own data to training. Look forward to your response. Thx."
"The data of the 7 datasets are randomly selected for training, right? The training code can run successfully for a period of time (not more than one epoch), and then will encounter this error. Can you tell the reason? I don't know much about itï¼ŒTTTThank you!     "
éžå¸¸æ£’çš„å·¥ä½œï¼  è¯·é—®æ‚¨èƒ½æä¾›åˆ©ç”¨æ‚¨é¢„è®­ç»ƒå¥½çš„resnet/hrnetåŸºç¡€ä¸Šè®­ç»ƒROMPçš„configsæ–‡ä»¶å—ï¼Ÿä¾‹å¦‚scripts/pretrain.shä½¿ç”¨çš„pretrain.yml
Hi. Thanks your work. Could you show me your training log? I can't reproduce paper's results.  This is my log file and yaml file. I only change the batch-size to 48 because of my memory. Anything else is default.          
æ‚¨å¥½ï¼Œæ‚¨çš„google colabä¼¼ä¹Žå‡ºäº†ä¸€äº›é—®é¢˜ã€‚æˆ‘å‘¨ä¸€è¿˜å¯ä»¥æˆåŠŸè¿è¡Œï¼Œä½†æ˜¯çŽ°åœ¨å¼€å§‹æŠ¥é”™ï¼Œæ‚¨èƒ½å¸®å¿™æ£€æŸ¥ä¸€ä¸‹å—ï¼Ÿ  !   
ä½ å¥½ï¼Œè¯·é—®ä¸€ä¸‹ï¼Œå®žæ—¶é¢„æµ‹çš„é…ç½®æ–‡ä»¶ç”¨çš„å“ªä¸€ä¸ªï¼Ÿæˆ‘è¿™è¾¹åœ¨3090ä¸Šç”¨webcam.ymlæµ‹è¯•äº†ä¸€ä¸‹ï¼Œå¥½åƒè¾¾ä¸åˆ°å®žæ—¶é¢„æµ‹ã€‚æ¨¡åž‹é¢„æµ‹éƒ¨åˆ†å¾—ï½ž50ms/frame
"Thanks so much for sharing the code!    I am trying to test ROMP on demo images and save the output predicted mesh on the image with ""  CUDA_VISIBLE_DEVICES=0 python -u -m romp.predict.image --configs_yml='configs/image.yml'""    However, it terminates does not process any of the images and terminates. One bug that I found was that in romp/predict/image.py line:49 results_dict['mesh_rendering_orgimgs']['figs'] should be changed to results_dict['org_img']['figs'], but I'm still facing the problem:    CUDA_VISIBLE_DEVICES=0 python -u -m romp.predict.image --configs_yml='configs/image.yml'  yaml_timestamp  /home/mahsa/Downloads/ROMP-master/active_configs/active_context_2021-12-13_13_25_29.yaml  Loading the configurations from configs/image.yml  INFO:root:{'tab': 'hrnet_cm64_process_images', 'configs_yml': 'configs/image.yml', 'inputs': 'demo/images', 'output_dir': 'demo/image_results', 'interactive_vis': False, 'show_largest_person_only': False, 'show_mesh_stand_on_image': False, 'soi_camera': 'far', 'make_tracking': False, 'temporal_optimization': False, 'save_dict_results': True, 'save_visualization_on_img': True, 'fps_save': 24, 'character': 'smpl', 'renderer': 'pyrender', 'f': None, 'model_return_loss': False, 'model_version': 1, 'multi_person': True, 'new_training': False, 'perspective_proj': False, 'FOV': 60, 'focal_length': 443.4, 'lr': 0.0003, 'adjust_lr_factor': 0.1, 'weight_decay': 1e-06, 'epoch': 120, 'fine_tune': True, 'GPUS': 0, 'batch_size': 64, 'input_size': 512, 'master_batch_size': -1, 'nw': 4, 'optimizer_type': 'Adam', 'pretrain': 'simplebaseline', 'fix_backbone_training_scratch': False, 'backbone': 'hrnet', 'model_precision': 'fp32', 'deconv_num': 0, 'head_block_num': 2, 'merge_smpl_camera_head': False, 'use_coordmaps': True, 'hrnet_pretrain': '/home/mahsa/Downloads/ROMP-master/trained_models/pretrain_hrnet.pkl', 'resnet_pretrain': '/home/mahsa/Downloads/ROMP-master/trained_models/pretrain_resnet.pkl', 'loss_thresh': 1000, 'max_supervise_num': -1, 'supervise_cam_params': False, 'match_preds_to_gts_for_supervision': False, 'matching_mode': 'all', 'supervise_global_rot': False, 'HMloss_type': 'MSE', 'eval': False, 'eval_datasets': 'pw3d', 'val_batch_size': 4, 'test_interval': 2000, 'fast_eval_iter': -1, 'top_n_error_vis': 6, 'eval_2dpose': False, 'calc_pck': False, 'PCK_thresh': 150, 'calc_PVE_error': False, 'centermap_size': 64, 'centermap_conf_thresh': 0.25, 'collision_aware_centermap': False, 'collision_factor': 0.2, 'center_def_kp': True, 'local_rank': 0, 'distributed_training': False, 'distillation_learning': False, 'teacher_model_path': '/export/home/suny/CenterMesh/trained_models/3dpw_88_57.8.pkl', 'print_freq': 50, 'model_path': 'trained_models/ROMP_HRNet32_V1.pkl', 'log_path': '/home/mahsa/Downloads/log/', 'learn_2dpose': False, 'learn_AE': False, 'learn_kp2doffset': False, 'shuffle_crop_mode': False, 'shuffle_crop_ratio_3d': 0.9, 'shuffle_crop_ratio_2d': 0.1, 'Synthetic_occlusion_ratio': 0, 'color_jittering_ratio': 0.2, 'rotate_prob': 0.2, 'dataset_rootdir': '/home/mahsa/Downloads/dataset/', 'dataset': 'h36m,mpii,coco,aich,up,ochuman,lsp,movi', 'voc_dir': '/home/mahsa/Downloads/dataset/VOCdevkit/VOC2012/', 'max_person': 64, 'homogenize_pose_space': False, 'use_eft': True, 'smpl_mesh_root_align': False, 'Rot_type': '6D', 'rot_dim': 6, 'cam_dim': 3, 'beta_dim': 10, 'smpl_joint_num': 22, 'smpl_model_path': '/home/mahsa/Downloads/ROMP-master/model_data/parameters', 'smpl_J_reg_h37m_path': '/home/mahsa/Downloads/ROMP-master/model_data/parameters/J_regressor_h36m.npy', 'smpl_J_reg_extra_path': '/home/mahsa/Downloads/ROMP-master/model_data/parameters/J_regressor_extra.npy', 'smpl_uvmap': '/home/mahsa/Downloads/ROMP-master/model_data/parameters/smpl_vt_ft.npz', 'wardrobe': '/home/mahsa/Downloads/ROMP-master/model_data/wardrobe', 'mesh_cloth': 'ghostwhite', 'nvxia_model_path': '/home/mahsa/Downloads/ROMP-master/model_data/characters/nvxia', 'track_memory_usage': False, 'adjust_lr_epoch': [], 'kernel_sizes': [5], 'collect_subdirs': False, 'save_mesh': True, 'save_centermap': False}  INFO:root:------------------------------------------------------------------  INFO:root:start building model.  Using ROMP v1  Confidence: 0.25  INFO:root:using fine_tune model: trained_models/ROMP_HRNet32_V1.pkl  WARNING:root:model trained_models/ROMP_HRNet32_V1.pkl not exist!  INFO:root:Train all layers, except: ['_result_parser.params_map_parser.smpl_model.betas']  Initialization finished!  Processing demo/images, saving to demo/image_results  INFO:root:gathering datasets  Loading 3 images to process  /home/mahsa/anaconda3/envs/rmp/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/c10/core/TensorImpl.h:1156.)    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)  /home/mahsa/anaconda3/envs/rmp/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.  To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/aten/src/ATen/native/BinaryOps.cpp:467.)    return torch.floor_divide(self, other)  Processed 0 / 3 images    I would appreciate your help!    "
None
"Which one of the pretrained models should I use - which model is the 'best' one for 'in the wild' images ?  Are the HRNet models better than the respective ResNet models ?  Should I use the 'V1' variant or the 'V1_ft_3DPW' variant ?  For me, it seems that 'V1_ft_3DPW' is slightly better."
"Hi there, I was just checking out the colab demo, and when I tried running it for the demo video provided it throws this error.....   I tried setting   set DISPLAY=:0  export DISPLAY=:0    and it didn't work"
None
ä½œè€…ä½ å¥½ï¼Œæˆ‘æƒ³è¯·æ•™ä¸€ä¸‹æ‚¨ä½¿ç”¨çš„pytorch3dæ˜¯ä»€ä¹ˆç‰ˆæœ¬ã€‚æœ€è¿‘å®‰è£…è¿‡0.2.0ç‰ˆæœ¬ï¼Œå¯æ˜¯ä¼šç¼ºå¤±å¾ˆå¤šä¸œè¥¿ï¼Œå°è¯•äº†0.3.0ç‰ˆæœ¬ï¼Œä½†æœ‰å¾ˆå¤šæ–‡ä»¶ä¼šæŠ¥é”™ï¼š# pyre-fixme[21]: Could not find name `_C` in `pytorch3d`.  å¯èƒ½æ˜¯æˆ‘å®‰è£…çš„é€”å¾„æœ‰é—®é¢˜ï¼Œä½†åœ¨pytorch3dçš„Issuesé‡ŒæŒ‰ç…§ä½œè€…ä»¥åŠæé—®äººçš„æ–¹æ³•å°è¯•è¿‡åŽéƒ½æ²¡æœ‰æˆåŠŸã€‚  è¯·æ‚¨æŒ‡ç‚¹ã€‚  
"Thanks for your great work!!    I got some error pose when I run ROMP on my custom video. If there is no person in the video, ROMP produce error pose at every first image of the batch. For example, when I run video inference code with batch size 8 and the beginning of the input video is just nothing just black, ROMP produces error pose in 000000.jpg, 000008.jpg, ...  If I run this single no person image to ROMP with your image inference code, ROMP detects nothing. Is it just me got such error results?    This image is the sample of error.com/44392318/144944743-fe1089bd-bb9e-4e71-88dc-262c6a439bba.jpg)  .  "
"Dear Arthur151,     I tried to finetuned the ROMP_HRNet32 model with your provided configuration. The training process seems to converge perfectly and evaluation result is good. However, I notice a bias in all the rendered the result with romp.predict.image. I checked romp.predict.image with your provided model (ROMP_HRNet32_V1.pkl), and it does not have this issue. I am little confused with this result. Though I have added some new loss term, I have already set the loss weight to be zero to rule out the influence.     Below, are one of  rendered result and tensorboard info:  !   !     Thank you in advance for any suggestion to solve this issue.  "
æ‚¨å¥½ï¼Œè¯·æ•™ä¸ªé—®é¢˜ï¼Œå½“å‰ç®—æ³•åœ¨unityä¸­éª¨ç›†ç‚¹æ˜¯åŽŸåœ°æ—‹è½¬çš„ï¼Œæœ‰ä»€ä¹ˆæ–¹æ³•æˆ–è€…å¦‚ä½•è°ƒæ•´ä½¿å¾—ç®—æ³•å¯ä»¥å®žçŽ°éª¨ç›†ç‚¹è·Ÿéšç§»åŠ¨å‘¢ï¼Ÿ
"Dear Arthur151,    I wonder whether u are going to provide any instruction on the pretraining process in the future, for I found the pretraining part have large influence on 3D reconstrution performance, especially in the case of multi-person. Right now, I did not find the pretrain.yml in configs folder. Is it possible to provide the config file for pretrain?     Thank you in advance for your help. "
"Hi, thank you very much for sharing the code. I've tried the code and find a question, when I use the Resnet50 trained model (ROMP_ResNet50_V1.pkl)ï¼Œthere's problem with the results by observing the ""save_visulization_on_image"" images. The question is that **all the predicted person's positions are higher than the ground-truth in vertical direction**, however, when I use the trained HRNet32 models, it seems everything all right. So I wonder if there's problem with the released Resnet50 trained model?"
"Hi, I have difficulty training your code or loading datasets.    Sorry, I don't know because I just started studying.    I'm going to use Google Colab to learn as you said in your docs (dataset.md, train.md) by      python -m romp.lib.dataset.lsp    or    sh scripts/V1_train.sh      But there seems to be a problem reading the data.    yaml_timestamp  /content/ROMP/active_configs/active_context_2021-11-24_11_15_34.yaml  Confidence: 0.2  configs_yml: /content/ROMP/configs/v1.yml  model_version: 1  test_projection_part: False  Initialized dataset  visualize in gpu mode  Traceback (most recent call last):    File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main      ""__main__"", mod_spec)    File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code      exec(code, run_globals)    File ""/content/ROMP/romp/lib/dataset/lsp.py"", line 91, in        test_dataset(dataset,with_smpl=True)    File ""/content/ROMP/romp/lib/dataset/image_base.py"", line 492, in test_dataset      for _,r in enumerate(dataloader):    File ""/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py"", line 521, in __next__      data = self._next_data()    File ""/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py"", line 1203, in _next_data      return self._process_data(data)    File ""/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py"", line 1229, in _process_data      data.reraise()    File ""/usr/local/lib/python3.7/dist-packages/torch/_utils.py"", line 425, in reraise      raise self.exc_type(msg)  TypeError: Caught TypeError in DataLoader worker process 0.  Original Traceback (most recent call last):    File ""/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py"", line 287, in _worker_loop      data = fetcher.fetch(index)    File ""/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch      data =      !           Can you help me with it?  Thanks  "
"Dear Arthur151, I came across this error after running video.py to reproduce result of your paper.    > Processing video:  demo/videos/sample_video.mp4  > Processing 257 frames of video demo/videos/sample_video, saving to demo/sample_video_results  > INFO:root:gathering datasets  > Loading 257 images to process  > list index out of range  > list index out of range  > list index out of range  > list index out of range  > list index out of range  > list index out of range  > list index out of range  > list index out of range  > maximum recursion depth exceeded while calling a Python object  > maximum recursion depth exceeded while calling a Python object  > Fatal Python error: Cannot recover from stack overflow.      > Traceback (most recent call last):  >   File ""/home/zjut/.conda/envs/romp/lib/python3.7/runpy.py"", line 193, in _run_module_as_main  >     ""__main__"", mod_spec)  >   File ""/home/zjut/.conda/envs/romp/lib/python3.7/runpy.py"", line 85, in _run_code  >     exec(code, run_globals)  >   File ""/home/zjut/txy/ROMP/romp/predict/video.py"", line 205, in    >     main()  >   File ""/home/zjut/txy/ROMP/romp/predict/video.py"", line 202, in main  >     processor.process_video(args_set.inputs)  >   File ""/home/zjut/.conda/envs/romp/lib/python3.7/site-packages/torch/autograd/grad_mode.py"", line 28, in decorate_context  >     return func(*args, **kwargs)  >   File ""/home/zjut/txy/ROMP/romp/predict/video.py"", line 71, in process_video  >     for test_iter,meta_data in enumerate(internet_loader):  >   File ""/home/zjut/.conda/envs/romp/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 521, in __next__  >     data = self._next_data()  >   File ""/home/zjut/.conda/envs/romp/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1186, in _next_data  >     idx, data = self._get_data()  >   File ""/home/zjut/.conda/envs/romp/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1142, in _get_data  >     success, data = self._try_get_data()  >   File ""/home/zjut/.conda/envs/romp/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1003, in _try_get_data  >     raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e  > RuntimeError: DataLoader worker (pid(s) 21664) exited unexpectedly    I did not solve the problem by adjusting batchsize and num_works=0, I would like to ask if you have any good suggestions to help me, thanks!        "
!   
"Dear Arthur151, I came across this error after running the **V1_train.sh** to reproduce result of your paper.    > Epoch: [1][4528/9722] Time 0.75 RUN 1.26 Lr 2.5e-05 Loss 250.60 | Losses {'reg': 223.19, 'det': 27.41, 'CenterMap': 27.41, 'P_KP2D': 33.48, 'MPJPE': 106.91, 'PAMPJPE': 33.09, 'Pose': 46.39, 'Shape': $  > Epoch: [1][4578/9722] Time 0.65 RUN 1.26 Lr 2.5e-05 Loss 241.55 | Losses {'reg': 218.88, 'det': 22.67, 'CenterMap': 22.67, 'P_KP2D': 32.87, 'MPJPE': 102.17, 'PAMPJPE': 33.1, 'Pose': 47.4, 'Shape': 0.$  > Traceback (most recent call last):  >   File ""/cluster/apps/nss/gcc-6.3.0/python/3.7.4/x86_64/lib64/python3.7/runpy.py"", line 193, in _run_module_as_main  >     ""__main__"", mod_spec)  >   File ""/cluster/apps/nss/gcc-6.3.0/python/3.7.4/x86_64/lib64/python3.7/runpy.py"", line 85, in _run_code  >     exec(code, run_globals)  >   File ""/cluster/home/chqian/src/ROMP/ROMP/romp/train.py"", line 148, in    >     main()  >   File ""/cluster/home/chqian/src/ROMP/ROMP/romp/train.py"", line 145, in main  >     trainer.train()  >   File ""/cluster/home/chqian/src/ROMP/ROMP/romp/train.py"", line 33, in train  >     self.train_epoch(epoch)  >   File ""/cluster/home/chqian/src/ROMP/ROMP/romp/train.py"", line 80, in train_epoch  >     for iter_index, meta_data in enumerate(self.loader):  >   File ""/cluster/home/chqian/src/ROMP/ROMP/venv/lib64/python3.7/site-packages/torch/utils/data/dataloader.py"", line 521, in __next__  >     data = self._next_data()  >   File ""/cluster/home/chqian/src/ROMP/ROMP/venv/lib64/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1183, in _next_data  >     return self._process_data(data)  >   File ""/cluster/home/chqian/src/ROMP/ROMP/venv/lib64/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1229, in _process_data  >     data.reraise()  >   File ""/cluster/home/chqian/src/ROMP/ROMP/venv/lib64/python3.7/site-packages/torch/_utils.py"", line 425, in reraise  >     raise self.exc_type(msg)  > cv2.error: Caught error in DataLoader worker process 1.  > Original Traceback (most recent call last):  >   File ""/cluster/home/chqian/src/ROMP/ROMP/venv/lib64/python3.7/site-packages/torch/utils/data/_utils/worker.py"", line 287, in _worker_loop  >     data = fetcher.fetch(index)  >   File ""/cluster/home/chqian/src/ROMP/ROMP/venv/lib64/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch  >     data = [self.dataset[idx] for idx in possibly_batched_index]  >   File ""/cluster/home/chqian/src/ROMP/ROMP/venv/lib64/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in    >     data = [self.dataset[idx] for idx in possibly_batched_index]  >   File ""/cluster/home/chqian/src/ROMP/ROMP/romp/lib/models/../utils/../dataset/mixed_dataset.py"", line 71, in __getitem__  >     annots = self.datasets[inds][index_sample]  >   File ""/cluster/home/chqian/src/ROMP/ROMP/romp/lib/models/../utils/../dataset/image_base.py"", line 342, in __getitem__  >     return self.get_item_single_frame(index)  >   File ""/cluster/home/chqian/src/ROMP/ROMP/romp/lib/models/../utils/../dataset/image_base.py"", line 98, in get_item_single_frame  >     image, dst_image, org_image = self.prepare_image(image, image_wbg, augments=(color_jitter, syn_occlusion))  >   File ""/cluster/home/chqian/src/ROMP/ROMP/romp/lib/models/../utils/../dataset/image_base.py"", line 230, in prepare_image  >     dst_image = cv2.resize(image, tuple(self.input_shape), interpolation = cv2.INTER_CUBIC)  > cv2.error: OpenCV(4.5.4-dev) /tmp/pip-req-build-5mtujfce/opencv/modules/core/src/matrix.cpp:466: error: (-215:Assertion failed) _step >= minstep in function 'Mat'  >    I tried to load the dataset and check the images loaded without finding any errors. I am not sure where this error comes from. I hope anyone can give me some advice on solving this. Thank you in advance for your help.    My config is below:    > 2) gcc/4.8.5    >  3) cudnn/7.5    >  4) openblas/0.2.20     > 5) python/3.7.4    >  6) cuda/10.1.243    My pacakge version:    > absl-py==1.0.0  > addict==2.4.0  > aio   > aiosignal==1.2.0  > anyio==3.3.4  > argcomplete==1.12.3  > argon2-cffi==21.1.0  > async-timeout==4.0.1  > asynctest==0.13.0  > attrs==21.2.0  > autobahn==21.3.1  > Automat==20.2.0  > Babel==2.9.1  > backcall==0.2.0  > bleach==4.1.0  > bvhtoolbox==0.1.3  > cached-property==1.5.2  > cachetools==4.2.4  > certifi==2021.10.8  > cffi==1.15.0  > charset-normalizer==2.0.7  > chumpy==0.70  > colorama==0.4.4  > commonmark==0.9.1  > constantly==15.1.0  > cryptography==35.0.0  > cycler==0.11.0  > Cython==0.29.24  > debugpy==1.5.1  > decorator==5.1.0  > defusedxml==0.7.1  > Deprecated==1.2.13  > deprecation==2.1.0  > entrypoints==0.3  > filterpy==1.4.5  > fonttools==4.28.1  > frozenlist==1.2.0  > fvcore==0.1.5.post20211023  > google-auth==2.3.3  > google-auth-oauthlib==0.4.6  > grpcio==1.41.1  > h5py==3.5.0  > hyperlink==21.0.0  > idna==3.3  > imageio==2.10.4  > imgaug==0.4.0  > importlib-metadata==4.8.2  > importlib-resources==5.4.0  > incremental==21.3.0  > iopath==0.1.9  > ipykernel==6.5.0  > ipython==7.29.0  > ipython-genutils==0.2.0  > ipywidgets==7.6.5  > jedi==0.18.0  > Jinja2==3.0.3  > joblib==1.1.0  > json5==0.9.6  > jsonschema==4.2.1  > jupyter-client==7.0.6  > jupyter-core==4.9.1  > jupyter-packaging==0.11.1  > jupyter-server==1.11.2  > jupyterlab==3.2.3  > jupyterlab-pygments==0.1.2  > jupyterlab-server==2.8.2  > jupyterlab-widgets==1.0.2  > keyboard==0.13.5  > kiwisolver==1.3.2  > lap==0.4.0  > llvmlite==0.37.0  > loguru==0.5.3  > Markdown==3.3.5  > MarkupSafe==2.0.1  > matplotlib==3.5.0  > matplotlib-inline==0.1.3  > mistune==0.8.4  > multidict==5.2.0  > munkres==1.1.4  > nbclassic==0.3.4  > nbclient==0.5.8  > nbconvert==6.3.0  > nbformat==5.1.3  > nest-asyncio==1.5.1  > networkx==2.6.3  > norfair==0.3.1  > notebook==6.4.5  > numba==0.54.1  > numpy==1.20.3  > numpy-quaternion==2021.11.4.15.26.3  > oauthlib==3.1.1  > open3d==0.13.0  > opencv-contrib-python==4.5.4.58  > opencv-python==4.5.4.58  > packaging==21.2  > pandas==1.3.4  > pandocfilters==1.5.0  > parso==0.8.2  > pexpect==4.8.0  > pickleshare==0.7.5  > Pillow==8.4.0  > plotly==5.4.0  > plyfile==0.7.4  > portalocker==2.3.2  > prettytable==2.4.0  > prometheus-client==0.12.0  > prompt-toolkit==3.0.22  > protobuf==3.19.1  > ptyprocess==0.7.0  > pyasn1==0.4.8  > pyasn1-modules==0.2.8  > pycocotools==2.0.2  > pycparser==2.21  > pygame==2.1.0  > Pygments==2.10.0  > pyparsing==2.4.7  > pyrsistent==0.18.0  > python-dateutil==2.8.2  > pytorch3d==0.6.0  > pytz==2021.3  > PyWavelets==1.2.0  > PyYAML==6.0  > pyzmq==22.3.0  > requests==2.26.0  > requests-oauthlib==1.3.0  > rich==9.13.0  > rsa==4.7.2  > scikit-image==0.18.3  > scikit-learn==1.0.1  > scipy==1.7.2  > Send2Trash==1.8.0  > setuptools-scm==6.3.2  > Shapely==1.8.0  > shyaml==0.6.2  > six==1.16.0  > smplx==0.1.28  > sniffio==1.2.0  > tabulate==0.8.9  > tenacity==8.0.1  > tensorboard==2.7.0  > tensorboard-data-server==0.6.1  > tensorboard-plugin-wit==1.8.0  > termcolor==1.1.0  > terminado==0.12.1  > testpath==0.5.0  > threadpoolctl==3.0.0  > tifffile==2021.11.2  > tomli==1.2.2  > tomlkit==0.7.2  > torch==1.9.0  > torchaudio==0.9.0  > torchfile==0.1.0  > torchvision==0.10.0  > tornado==6.1  > tqdm==4.62.3  > traitlets==5.1.1  > transforms3d==0.3.1  > trimesh==3.9.35  > Twisted==21.7.0  > txaio==21.2.1  > typing-extensions==3.10.0.2  > urllib3==1.26.7  > vedo==2021.0.7  > vtk==9.0.3  > wcwidth==0.2.5  > webencodings==0.5.1  > websocket-client==1.2.1  > Werkzeug==2.0.2  > widgetsnbextension==3.5.2  > wrapt==1.13.3  > wslink==1.1.0  > yacs==0.1.8  > yarl==1.7.2  > zipp==3.6.0  > zope.interface==5.4.0  > "
I was just testing the Colab Demo without any changes on a V100 / CUDA 11.2 / Python 3.7.12 and ran into following errors.    When installing the CUDA libs and libc:  `/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link`    Later at romp.predict.image:  `ImportError: /usr/local/lib/python3.7/dist-packages/pytorch3d/_C.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZNK2at6Tensor2leERKN3c106ScalarE`  
"I'd like to know which body detected in each frame corresponds to which individual's track. When make_tracking=True and save_dict=True, should the results carry this track information?"
"Hi     Thanks for your wonderful work. I'm trying to save the vertices result inferenced by your model by running   `python -m romp.test --configs_yml=configs/eval_3dpw_test_resnet.yml`    And I add     `for i, name in enumerate(outputs     But I found the shape of vertices is 6890 x 3 instead of num_person x 6890 x 3. Why there is only one person in the inference result of each image? And how to got the multiperson  result for 3DPW dataset? Appreciate for your help in advance."
"Hello, great projects!  I downloaded the human3.6m dataset you supplied in google drive, but I find the image is not enough(original human3.6m website said it has 3milliion data, but your image only have 417774 images). So I guess you extract original video with less framerate ? "
ä½œè€…ä½ å¥½ï¼Œ è¯·æ•™ä¸€ä¸‹ï¼Œæ–‡ç« ä¸­ä½ è¯„ä¼°3DOH50Kçš„PA-MPJPEæ—¶ï¼Œæ˜¯åˆ©ç”¨å®ƒçš„å“ªä¸€ä¸ªæ ‡æ³¨ï¼Ÿ  æ˜¯ç±»ä¼¼SPINçš„3DPWé¢„å¤„ç†æ–¹å¼ï¼Œä»ŽSMPLæ ‡ç­¾é‡Œç”Ÿæˆçš„3D å…³é”®ç‚¹ï¼Œ è¿˜æ˜¯ç›´æŽ¥åˆ©ç”¨å®ƒæä¾›çš„3D å…³é”®ç‚¹ä½œä¸ºgtï¼Ÿ  æˆ‘ä¹‹æ‰€ä»¥æœ‰è¿™ä¸ªç–‘é—®æ˜¯å› ä¸º3DOHæä¾›çš„3Då…³é”®ç‚¹æ˜¯ç›¸æœºåæ ‡ç³»ä¸‹ï¼Œä½†æ˜¯æ¯å¼ å›¾åƒç»™å‡ºäº†ç›¸æœºå†…å‚ï¼Œè¿™æ ·è·Ÿæˆ‘ä»¬è®­ç»ƒæ—¶é¢„è®¾çš„5000ç„¦è·ä¸åŒï¼Œè¿™æ ·çš„è¯æ˜¯ä¸æ˜¯æ²¡æ³•ç›´æŽ¥åˆ©ç”¨å®ƒæä¾›çš„3Då…³é”®ç‚¹æ ‡æ³¨ï¼Œè¿˜æ˜¯è¯´éœ€è¦åšä¸€äº›è½¬æ¢ï¼Ÿ    è°¢è°¢~
"Hi,  I am following this work and really appreciate your efforts on this project. I just want to know if there is a plan of build a pipe for converting the whole ROMP model into a onnx model. And if there is not, do you mind distribute some clues so this job could be completed by followers of thie repository."
!   æ‚¨å¥½ï¼Œæƒ³é—®è®¡ç®—shape lossçš„æ—¶å€™ä¸ºä»€ä¹ˆéœ€è¦smpl_shape_masks.sum()>1è€Œä¸æ˜¯å¤§äºŽ0å‘¢ï¼Œè¿˜æœ‰å°±æ˜¯è¡Œ114ä¸ºä»€ä¹ˆè¦å–åå‘¢
"Hi,     I'm studying about your wonderful method in your paper.    But I got two case of questions about the body center shift.    First, what happen if there is person who have a same position with shifted body center.      Second, how does the model know the body center was shifted or not.    Thank you."
"Hi, thanks for sharing your code.    I tested the data loading of h36m as said in your doc (dataset.md) by  `python -m romp.lib.dataset.h36m`    But the visualization of my mesh results doens't match the original image, like    !   !     Can you help me with it?  Thanks"
"@Arthur151   When I run ""python -m romp.lib.dataset.coco14"", the error is as follows, please check it out:  Exception has occurred: NameError  name 'get_3Dcoord_maps' is not defined    File ""/home/zhaohan/wxl/ROMP/romp/lib/maps_utils/centermap.py"", line 23, in prepare_parsing      self.coordmap_3d = get_3Dcoord_maps(size=self.size)    File ""/home/zhaohan/wxl/ROMP/romp/lib/maps_utils/centermap.py"", line 20, in __init__      self.prepare_parsing()    File ""/home/zhaohan/wxl/ROMP/romp/lib/dataset/image_base.py"", line 78, in __init__      self.CM = CenterMap()    File ""/home/zhaohan/wxl/ROMP/romp/lib/dataset/coco14.py"", line 9, in __init__      super(COCO14,self).__init__(train_flag,regress_smpl)    File ""/home/zhaohan/wxl/ROMP/romp/lib/dataset/coco14.py"", line 195, in        dataset = COCO14(train_flag=True, regress_smpl=True)"
"@Arthur151 thanks for your great job of ROMP, but the google drive of training dataset failed by now, please check it out"
"ä½œè€…æ‚¨å¥½ï¼Œ  æƒ³è¯·æ•™ä¸‹ä¸¤ä¸ªé—®é¢˜ï¼Œ  â‘ æ–‡ç« ä¸­ä¼°è®¡çš„ç›¸æœºå‚æ•°æ˜¯   â‘¡å¯ä»¥ç›´æŽ¥ä¼°è®¡[tx,ty,tz]æ¥è¾¾åˆ°[s,tx,ty]åŒæ ·æ•ˆæžœå—ï¼Ÿ "
"Hello, I am preparing to write my master thesis about unsupervised pre-training/ training networks for human pose estimation. Since I currently only have limited GPU resources available and I want to get some intuitions if my idea could improve the performance, i have the following questions:    - Has ROMP used the imagenet pre-trained weights for the backbone?  - Is it possible to easily train on only a subset of the data, is the their a script to get the parsed annotations for the data subsets ?  - Did you try using other pre-trained weights (MOCO, DINO, ...) for the resent50 ?    thanks in advance!"
"Hi there, when I tried exporting the npz file to fbx a weird error occurred...    ***KeyError: 'bpy_prop_collection[key]: key ""f_avg_Pelvis"" not found'***  ***Error: Python script failed, check the message in the system console***  "
Greatly appreciate all the work you have put into this project.  When i tried to retrain the network. I got this error: quaternion is not defined in coco14.py line 103.  I looked into image_base.py fined there is a commented lineï¼š import quaternion. i installed quaternion by pip install quaternion. I still got an error: no module named quaternion.  Do you konw how to resolve this problemï¼Ÿ
"CUDA_VISIBLE_DEVICES=0,1 python -u -m romp.predict.image --configs_yml='configs/image.yml'  ä½ å¥½ï¼Œè¿è¡Œ**å¤šä¸ª**GPUçš„æ—¶å€™ï¼Œåœ¨æ‰§è¡Œåˆ°outputs = self.net_forward(meta_data, cfg=self.demo_cfg)çš„æ—¶å€™æŠ¥é”™  RuntimeError: NCCL Error 2: unhandled system errorã€‚  ç½‘ä¸Šæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„è§£ç­”ï¼Œä¸çŸ¥é“ä½ ç¢°åˆ°è¿‡æ²¡ã€‚"
ä½ å¥½ï¼Œæ‰§è¡ŒBenchmark Evaluationçš„ä»£ç çš„æ—¶å€™ï¼Œè¾“å‡ºçš„æ•°å€¼å’Œgitä¸Šä¸ä¸€æ ·ï¼Œå·®å¼‚å¾ˆå¤§ï¼Œä¸çŸ¥é“ä½ è¿™è¾¹æœ‰æ²¡æœ‰ç¡®è®¤è¿™ç‰ˆä»£ç ï¼š  cd ROMP  CUDA_VISIBLE_DEVICES=0 python romp/lib/evaluation/collect_3DPW_results.py --configs_yml=configs/eval_3dpw_challenge.yml  æˆ‘è¿™è¾¹æ‰§è¡Œçš„ç»“æžœï¼š  !     æˆ‘è‡ªå·±è¿›è¡Œäº†ä¸€ä¸‹æŽ’æŸ¥ï¼š  ä»£ç æ¨¡åž‹çš„åŠ è½½æ­£ç¡®æ€§éªŒè¯ï¼šè¯•äº†ä¸€ä¸‹è¿™ç‰ˆä»£ç çš„ä¸‹é¢ä¸¤ä¸ªæ‰§è¡Œè„šæœ¬ï¼Œæ•ˆæžœæ˜¯OKçš„ã€‚  sh scripts/image.sh  sh scripts/video.sh    æœªå…¬å¸ƒè®­ç»ƒä»£ç ä¹‹å‰çš„ commit 7400b34d1d69112625a2165c78b5c754943651c0  Benchmark Evaluationï¼Œæ•°å€¼ä¹Ÿæ˜¯okçš„ã€‚ä½†æ˜¯å½“å‰è¿™ä¸ª commit 248660608d2de85d635122a1bb75ef4988b9947bçš„æ‰§è¡Œç»“æžœ  ä¸å¯¹ï¼Œä¸çŸ¥é“å“ªé‡Œæ˜¯ä¸æ˜¯æœ‰é—®é¢˜ã€‚  
"ä½ å¥½ï¼Œè¿™é‡Œæœ‰ä¸¤ä¸ªç–‘æƒ‘  1 ä¸ºå•¥crowdpose é‡Œé¢æ˜¯h36m_image.z03  <img width=""1125"" alt=""image"" src=""     2  æä¾›çš„h36mçš„æ•°æ®       è§£åŽ‹çš„æ—¶å€™æœ‰é—®é¢˜ï¼šunzip h36m_image.zip      æŠ¥é”™  <img width=""672"" alt=""image"" src=""         "
æ‚¨å¥½ï¼Œå¾ˆæ„Ÿè°¢æ‚¨åˆ†äº«æ‚¨çš„å·¥ä½œã€‚  æˆ‘åœ¨ä¸‹è½½æ•°æ®é›†çš„æ—¶å€™å‘çŽ°è°·æ­Œç½‘ç›˜æç¤ºâ€œæ­¤æ–‡ä»¶å·²è¶…å‡ºä¸‹è½½é…é¢ï¼Œæ— æ³•ä¸‹è½½â€ã€‚æ›´æ¢äº†ç½‘ç›˜è´¦å·ä¹Ÿæ˜¯ä¸€æ ·ã€‚ä¸çŸ¥é“æ˜¯ä¸æ˜¯æ•°æ®é›†ä¸‹è½½é‡å¤ªå¤šäº†è¢«è°·æ­Œé™åˆ¶äº†ã€‚  è¯·é—®æ‚¨é‚£è¾¹è¿˜å¯ä»¥é€šè¿‡åˆ«çš„æ¸ é“åˆ†äº«ä¸€ä¸‹æ•°æ®é›†å—ï¼Ÿ
"Thanks for sharing your great job!     The test script can not find the active_config folder. May I ask where can i find the active_config folder?    Traceback (most recent call last):    File ""/data00/liyang.1610/anaconda3/envs/romp/lib/python3.6/runpy.py"", line 193, in _run_module_as_main      ""__main__"", mod_spec)    File ""/data00/liyang.1610/anaconda3/envs/romp/lib/python3.6/runpy.py"", line 85, in _run_code      exec(code, run_globals)    File ""/data00/liyang.1610/projects/ROMP/romp/predict/image.py"", line 1, in        from .base_predictor import *    File ""/data00/liyang.1610/projects/ROMP/romp/predict/base_predictor.py"", line 1, in        from ..base import *    File ""/data00/liyang.1610/projects/ROMP/romp/base.py"", line 17, in        from models import build_model,build_teacher_model    File ""/data00/liyang.1610/projects/ROMP/romp/lib/models/__init__.py"", line 1, in        from .build import build_model,build_teacher_model    File ""/data00/liyang.1610/projects/ROMP/romp/lib/models/build.py"", line 8, in        from models.hrnet_32 import HigherResolutionNet    File ""/data00/liyang.1610/projects/ROMP/romp/lib/models/hrnet_32.py"", line 19, in        from utils import BHWC_to_BCHW, copy_state_dict    File ""/data00/liyang.1610/projects/ROMP/romp/lib/models/../utils/__init__.py"", line 7, in        from .center_utils import process_gt_center    File ""/data00/liyang.1610/projects/ROMP/romp/lib/models/../utils/center_utils.py"", line 6, in        def denormalize_center(center, size=args().centermap_size):    File ""/data00/liyang.1610/projects/ROMP/romp/lib/config.py"", line 244, in args      with open(ConfigContext.yaml_filename, 'w') as f:  FileNotFoundError: [Errno 2] No such file or directory: '/data00/liyang.1610/projects/ROMP/active_configs/active_context_2021-09-13_16_17_33.yaml'"
æ„Ÿè°¢æ‚¨çš„å¼€æº...æœ‰ä¸ªé—®é¢˜æƒ³å‘æ‚¨è¯·æ•™ï¼Œaxis-angleå½¢å¼æ˜¯å¦èƒ½è½¬æ¢æˆæ¬§æ‹‰è§’ä»¥å®žçŽ°åœ¨Unityä¸­é€å¸§é©±åŠ¨äººç‰©å‘¢ï¼Ÿ
"ä½œè€…ï¼Œä½ å¥½ã€‚  åœ¨3dpwæ•°æ®çš„leaderboardä¸Šåˆ·åˆ°æ‚¨çš„ä½œå“ã€‚æ‹œè¯»åŽï¼Œæœ‰ä¸€ä¸ªç–‘æƒ‘ï¼š    è®ºæ–‡é‡Œçš„è¾“å‡ºSMPL parameter mapçš„å§¿åŠ¿å‚æ•°Î˜ç»´åº¦æ˜¯6X22ï¼ŒåŽŸå§‹SMPLæ¨¡åž‹æ˜¯3X24,è½´è§’è¡¨ç¤ºï¼› 24->22æ²¡ä»€ä¹ˆé—®é¢˜ã€‚ä½†æ˜¯è¿™ä¸ªç»´åº¦6ï¼Œéœ€è¦æ€Žä¹ˆè¾“å…¥è¿›åŽ»SMPLæ¨¡åž‹å‘¢ï¼Œéœ€è¦è¿›è¡Œè½¬æ¢æˆè½´è§’è¡¨ç¤ºå—ï¼Ÿ å¦å¤–è¿™ç»´åº¦6çš„å«ä¹‰æ˜¯ï¼Ÿ    å¸Œæœ›ä½œè€…èƒ½æŒ‡å¯¼ä¸‹"
Hi è¯·é—®åŽç»­æœ‰æ‰“ç®—åœ¨è®­ç»ƒæ•°æ®ä¸ŠåŠ å…¥AGORAæ•°æ®å—ï¼Ÿæ„Ÿè§‰å’Œä½ çš„ä»»åŠ¡ä¹Ÿéžå¸¸çš„åŒ¹é…ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥åŽ»åšçš„augmentationæ¥èŽ·å¾—æ›´å¤šçš„å¤šäººæ•°æ®ã€‚
when i run  python core/test.py --gpu=0 --configs_yml=configs/single_image.yml  in windowsï¼Œ  when i runï¼Œgive me No J_regressor_h36m.npy file error ï¼Œso i find this ï¼Œ  I'm not sure if this file is correctï¼ŒCan you give me a link  !     !   
"First of all, thank you for this wonderful task that your team have made :)     I wonder how the 3DPW-PC, OC data can be acquired, as there's no information about them in 3DPW homepage.     Thanks a lot! :) "
"Hello - this is excellent work! I understand that the bbox-free method is one thing that allows ROMP to perform so well - but for my purposes I am needing to generate 2D body bounding boxes to cross-reference with 2D locations of faces in image space. My plan was to use the `batch_orth_proj` function from the `projection` script to generate bboxes in image space from the 2D keypoints, but I am unsure where to insert this function and what it's inputs should be as the latest commit no longer contains the `_calc_smplx_params` function that used to be in `base.py` (as mentioned in #15 ). Could you clarify where I can use this function to ultimately generate 2D keypoint detections in image space as part of the final `.npz` output?"
"Hello. Will the code support exporting the mesh to the ""meshlab"""
AssertionError: Path Desktop/ROMP/models/smpl/SMPL_NEUTRAL.pkl does not exist!
"hey @Arthur151, great framework.  but I have one issue, i tried executing it in the google colab environment on a video of 446 frames, took 153 seconds.  The fps is < 3fps, also tried on AWS got almost same around 3.5 fps there.  Is there something i am missing or that's all we can get ?"
æ‚¨å¥½ï¼  æˆ‘åœ¨  ä¸Šæ‰¾åˆ°çš„ä¸¤ä¸ª trackéƒ½æ˜¯ä¸å…è®¸ç”¨ 2D poseå¯¹å›¾ç‰‡è¿›è¡Œcropçš„ï¼Œä½†æ˜¯çœ‹åˆ°æ‚¨è¿™è¾¹è¯´ followçš„ VIBE  éƒ½æ˜¯ç”¨çš„ å®˜æ–¹æä¾›çš„2D poseå¾—åˆ° bboxçš„ï¼Œè¿™æ ·æ˜¯ä¸æ˜¯å°±å°‘äº†ä¸€ä¸ªtrackingçš„è¯¯å·®? è¿˜æ˜¯åœ¨æ¯”èµ›ä¹‹å¤–å¤§å®¶è¯„æµ‹çš„æ—¶å€™å°±æ˜¯ç»™å®šäº†å¯¹åº”å…³ç³»?   æ„Ÿè°¢!
"Hello! IÂ´ve been using your work recently and its very good! However, IÂ´m stuck trying to do the segmentation of the different body parts in the mesh of the results.    So far, I tried to use the SMPL format and this labeler:   but I do not find some information in the results as, in example, the J regressor.    There is a way with the current code to do this? If I didnÂ´t explain myself well enough, do not hesitate to ask me.    Thank you in advance."
"Hello. I am coming from another details about the implementation.  In 2d pose estimation, we always use heatmap regression method, which generates a third attribute ""confidence"" which can be used for detected non-existent keypoints. As for the keypoint whose maximum value of the whole heatmap is too low, the corresponding key-point can be recognised as ""not exist"", which is maybe out of the image or occluded.  However, in 3d pose estimation, there's neither an attribute called ""confidence"", nor a heatmap. Does it means that every point are assumed to be existent? Are there any tricks for detecting the occluded body-parts? "
ä½ å¥½ï¼Œæˆ‘çœ‹ä½ è®ºæ–‡æåˆ°ä½¿ç”¨äº†Moviæ•°æ®é›†ï¼ŒåŽŸå§‹çš„Moviæ•°æ®é›†åªæä¾›äº†3dä»¥åŠAMASSé€šè¿‡fittingå¾—åˆ°çš„å…³äºŽSMPL-Hçš„meshå‚æ•°ã€‚SMPL-Hå…³äºŽshapeæœ‰16ä¸ªå€¼ï¼Œposeæœ‰52ä¸ªï¼Œæˆ‘æƒ³çŸ¥é“ä½ åœ¨ä½¿ç”¨Moviçš„æ•°æ®é›†æ—¶å€™ï¼Œæ˜¯å¦ä½¿ç”¨äº†AMASSæä¾›çš„poseå’Œbetaã€‚æˆ‘ç›®å‰å°†betaçš„å‰10ä¸ªå€¼å’Œposeå‰22ä¸ªå€¼å¯¼å…¥åˆ°æˆ‘ä»¬å½“å‰çš„SMPLé‡Œé¢ï¼Œç„¶åŽå€ŸåŠ©Moviæä¾›çš„ç›¸æœºå†…å‚å’Œå¤–å‚ä»¥åŠå‚è€ƒMoviæä¾›çš„æ•™ç¨‹ï¼Œå§‹ç»ˆæ— æ³•åœ¨æˆ‘ä»¬è¿™ä¸ªç¨‹åºä¸­å¯è§†åŒ–æˆåŠŸã€‚æ±‚æŒ‡æ•™ã€‚    å¦‚æžœæ²¡æœ‰ä½¿ç”¨ï¼Œæ˜¯ä¸æ˜¯å› ä¸ºAMASSæä¾›çš„betaå’Œposeä¸é€‚ç”¨æˆ‘ä»¬è¿™ä¸ªSMPLæ¨¡åž‹ï¼Œä½†æ˜¯çœ‹AMASSè®ºæ–‡å¥½åƒæ˜¯å¯ä»¥çš„ã€‚  MoVi-Toolboxï¼šMoVi-Toolbox        æŒ‰ç…§Moviçš„æ•™ç¨‹ä»¥åŠå°†å…¶meshç³»æ•°æŒ‰ç…§beta     @Arthur151     
"Hello. I feel a little concerned about your ""camera map"".    As illustrated in your paper, the camera map **_contains the 3-dim camera parameters (s, tx , ty ) that describe the 2D scale s and translation t = (tx , ty ) of the person in the image. The scale s reflects the body size and the depth to some extent. tx and ty , ranging in (âˆ’1, 1), reflect the normalized translation of the human body relative to the image center on the x and y axis, respectively._**    From my personal perspective, the camera map may contain 6 parameters: rotation and translation of 3 axis, which comprises the extrinsic parameters of the camera position. Maybe I may recognise the parameter **s** as the translation of z dimension, but I am wondering that how can we obtain the rotation of the xyz dimension?"
"Hi! Is it possible yet to render the model mesh like in the inference with the  single_image config but with model texture (png)?   I see that you have it working on inference with the webcam config already, that's why I'm asking.    Thanks for this project! :)"
"Hello Im trying to run the colab demo, under the ""Run the demo code "" heading, when i run the first command of   `%cd /content/ROMP/src  !CUDA_VISIBLE_DEVICES=0 python core/test.py --gpu=0 --configs_yml=configs/single_image.yml`    I get the following error :   `/content/ROMP/src  pygame 2.0.1 (SDL 2.0.14, Python 3.7.10)  Hello from the pygame community.    Traceback (most recent call last):    File ""core/test.py"", line 1, in        from base import *    File ""/content/ROMP/src/core/base.py"", line 20, in        from evaluation import compute_error_verts, compute_similarity_transform, compute_similarity_transform_torch, \    File ""/content/ROMP/src/core/../lib/models/../utils/../maps_utils/../evaluation/__init__.py"", line 1, in        from .evaluation_matrix import compute_error_verts, compute_similarity_transform, compute_similarity_transform_torch, \    File ""/content/ROMP/src/core/../lib/models/../utils/../maps_utils/../evaluation/evaluation_matrix.py"", line 6, in        from smplx import SMPL    File ""/usr/local/lib/python3.7/dist-packages/smplx/__init__.py"", line 17, in        from .body_models import (    File ""/usr/local/lib/python3.7/dist-packages/smplx/body_models.py"", line 27, in        from loguru import logger  ModuleNotFoundError: No module named 'loguru'`"
"Hello. Thank you for your work.  I wonder that what's the unit of 3d pose? I found it completely different from the annotations in 2d which are the pixel value. Instead, they are the value at around        "
"Hey, thanks for letting this open source!     I was trying to merge your project with another one and for that other project I needed a 207-dim Pose Blend Shape as input. Checking your code the close I can get, it's a 72-dim pose parameter... Is there any way to convert those parameters to the 207 vector that I need?     Thanks"
"I have to try loading the project on VM machine and this issue occurs,  my machine is UBUNTU 18.04, and 3D is enabled in VMware.  Indeed I tried with multiple pyrender and OpenGL Versions but doesn't solve it.      INFO - 2021-05-02 18:43:37,442 - base - finished build model.  Traceback (most recent call last):    File ""/root/Desktop/final/m3d/src/core/test.py"", line 230, in        main()    File ""/root/Desktop/final/m3d/src/core/test.py"", line 210, in main      demo = Demo()    File ""/root/Desktop/final/m3d/src/core/test.py"", line 7, in __init__      self._prepare_modules_()    File ""/root/Desktop/final/m3d/src/core/test.py"", line 15, in _prepare_modules_      self.visualizer = Visualizer(resolution=self.vis_size, input_size=self.input_size,with_renderer=True)    File ""/root/Desktop/final/m3d/src/core/../lib/models/../utils/../maps_utils/../dataset/../dataset/../dataset/../visualization/visualization.py"", line 23, in __init__      self.renderer = get_renderer(resolution=resolution)    File ""/root/Desktop/final/m3d/src/core/../lib/models/../utils/../maps_utils/../dataset/../dataset/../dataset/../visualization/../visualization/renderer.py"", line 142, in get_renderer      renderer = Renderer(faces,resolution=resolution[:2])    File ""/root/Desktop/final/m3d/src/core/../lib/models/../utils/../maps_utils/../dataset/../dataset/../dataset/../visualization/../visualization/renderer.py"", line 72, in __init__      point_size=1.0)    File ""/usr/local/lib/python3.6/dist-packages/pyrender/offscreen.py"", line 31, in __init__      self._create()    File ""/usr/local/lib/python3.6/dist-packages/pyrender/offscreen.py"", line 149, in _create      self._platform.init_context()    File ""/usr/local/lib/python3.6/dist-packages/pyrender/platforms/egl.py"", line 188, in init_context      EGL_NO_CONTEXT, context_attributes    File ""/usr/local/lib/python3.6/dist-packages/OpenGL/platform/baseplatform.py"", line 409, in __call__      return self( *args, **named )    File ""src/errorchecker.pyx"", line 58, in OpenGL_accelerate.errorchecker._ErrorChecker.glCheckError  OpenGL.error.GLError: GLError(   err = 12297,   baseOperation = eglCreateContext,   cArguments = (     ,     ,     ,     ,   ),   result =    )  "
"Hey bro. I am coming again for the another problem.  I want to run the new feature which can export the mesh to .fbx file, and it seems that ""bpy"" package is needed to download. But when I use _pip install bpy_, some CMake error always occurs.   I wonder that which version of CMake you are using when you install bpy?"
First of all thanks a lot for this great repo    How can I export .obj file of a video ?    by change save_mesh to True nothing generate?    How can I save mesh of all video frames In one .fbx file? (Just like vibe repo         or How can I concat all .obj file by exporting via image config (not video) as one .fbx file?        I've try to use the .npy file of video to ROMP/src/lib/utils/convert_fbx.py and generate .fbx file but the generated file doesn't consist of any animation in belender.    Could you please help to solve this issue.  
"Hello. Thanks for your work. I am currently conducting some researches on 3d human pose estimation, and I want to consult some concepts.     I noticed that there are lots of information containing in the pkl files. I am a newcomer in this field, and the values make me confused. For example, I wonder that what's the difference between ""pose"", ""pose2d"", ""poses_60Hz"" and ""jointPositions""? And also, there are 5 items containing the information of SMPL including ""betas"", ""betas_clothed"" and ""pose"", and what's their exact meaning?    Or to be simplified, are there any references which can explain such items with exact meanings in details? It will be my pleasure if you are willing to help me!  "
"Thanks for your work. I am running your demo with **CUDA_VISIBLE_DEVICES=0 python core/test.py --gpu=0 --configs_yml=configs/single_image.yml**  But an error occurs.   I am running with ubuntu16.04. The python verison is 3.6.9. How should I check what's happening?    (romp) jack@jack-System-Product-Name:~/Documents/ROMP/src$ CUDA_VISIBLE_DEVICES=0 python core/test.py --configs_yml=configs/single_image.yml  pygame 2.0.1 (SDL 2.0.14, Python 3.6.13)  Hello from the pygame community.    INFO:root:{'tab': 'hrnet_cm64_single_image_test', 'configs_yml': 'configs/single_image.yml', 'demo_image_folder': '/path/to/image_folder', 'local_rank': 0, 'model_version': 1, 'multi_person': True, 'collision_aware_centermap': False, 'collision_factor': 0.2, 'kp3d_format': 'smpl24', 'eval': False, 'max_person': 64, 'input_size': 512, 'Rot_type': '6D', 'rot_dim': 6, 'centermap_conf_thresh': 0.25, 'centermap_size': 64, 'deconv_num': 0, 'model_precision': 'fp32', 'backbone': 'hrnet', 'gmodel_path': '../trained_models/ROMP_hrnet32.pkl', 'print_freq': 50, 'fine_tune': True, 'gpu': '0', 'batch_size': 64, 'val_batch_size': 1, 'nw': 4, 'calc_PVE_error': False, 'dataset_rootdir': '/home/jack/Documents/dataset/', 'high_resolution': True, 'save_best_folder': '/home/jack/Documents/checkpoints/', 'log_path': '/home/jack/Documents/log/', 'total_param_count': 85, 'smpl_mean_param_path': '/home/jack/Documents/ROMP/models/satistic_data/neutral_smpl_mean_params.h5', 'smpl_model': '/home/jack/Documents/ROMP/models/statistic_data/neutral_smpl_with_cocoplus_reg.txt', 'smplx_model': True, 'cam_dim': 3, 'beta_dim': 10, 'smpl_joint_num': 22, 'smpl_model_path': '/home/jack/Documents/ROMP/models', 'smpl_J_reg_h37m_path': '/home/jack/Documents/ROMP/models/smpl/J_regressor_h36m.npy', 'smpl_J_reg_extra_path': '/home/jack/Documents/ROMP/models/smpl/J_regressor_extra.npy', 'kernel_sizes': [5], 'GPUS': 0, 'use_coordmaps': True, 'webcam': False, 'video_or_frame': False, 'save_visualization_on_img': True, 'output_dir': '/path/to/outputdir', 'save_mesh': True, 'save_centermap': True, 'save_dict_results': True, 'multiprocess': False}  INFO:root:------------------------------------------------------------------  INFO:root:start building model.  Using ROMP v1  INFO:root:using fine_tune model: ../trained_models/ROMP_hrnet32.pkl  INFO:root:finished build model.  Traceback (most recent call last):    File ""core/test.py"", line 225, in        main()    File ""core/test.py"", line 205, in main      demo = Demo()    File ""core/test.py"", line 7, in __init__      self._prepare_modules_()    File ""core/test.py"", line 14, in _prepare_modules_      self.visualizer = Visualizer(resolution=self.vis_size, input_size=self.input_size,with_renderer=True)    File ""/home/jack/Documents/ROMP/src/core/../lib/models/../utils/../maps_utils/../dataset/../dataset/../dataset/../visualization/visualization.py"", line 23, in __init__      self.renderer = get_renderer(resolution=resolution)    File ""/home/jack/Documents/ROMP/src/core/../lib/models/../utils/../maps_utils/../dataset/../dataset/../dataset/../visualization/../visualization/renderer.py"", line 142, in get_renderer      renderer = Renderer(faces,resolution=resolution[:2])    File ""/home/jack/Documents/ROMP/src/core/../lib/models/../utils/../maps_utils/../dataset/../dataset/../dataset/../visualization/../visualization/renderer.py"", line 72, in __init__      point_size=1.0)    File ""/home/jack/anaconda3/envs/romp/lib/python3.6/site-packages/pyrender/offscreen.py"", line 31, in __init__      self._create()    File ""/home/jack/anaconda3/envs/romp/lib/python3.6/site-packages/pyrender/offscreen.py"", line 149, in _create      self._platform.init_context()    File ""/home/jack/anaconda3/envs/romp/lib/python3.6/site-packages/pyrender/platforms/egl.py"", line 188, in init_context      EGL_NO_CONTEXT, context_attributes    File ""/home/jack/anaconda3/envs/romp/lib/python3.6/site-packages/OpenGL/platform/baseplatform.py"", line 402, in __call__      return self( *args, **named )    File ""/home/jack/anaconda3/envs/romp/lib/python3.6/site-packages/OpenGL/error.py"", line 232, in glCheckError      baseOperation = baseOperation,  OpenGL.error.GLError: GLError(   err = 12297,   baseOperation = eglCreateContext,   cArguments = (     ,     ,     ,     ,   ),   result =    )  "
None
I didn't find the download link of resnet model. Have you ever published it? Thanks.
"File ""core/test.py"", line 4, in        from utils.multiprocess import * #Multiprocess    File ""/root/lj28/project/git-code/CenterHMR/src/utils/multiprocess.py"", line 9, in        from utils.demo_utils import OpenCVCapture, Open3d_visualizer     File ""/root/lj28/project/git-code/CenterHMR/src/utils/demo_utils.py"", line 5, in        import open3d as o3d    File ""/opt/conda/lib/python3.6/site-packages/open3d/__init__.py"", line 56, in        _CDLL(next((_Path(__file__).parent / 'cpu').glob('pybind*')))    File ""/opt/conda/lib/python3.6/ctypes/__init__.py"", line 348, in __init__      self._handle = _dlopen(self._name, mode)  OSError: /opt/conda/lib/python3.6/site-packages/open3d/cpu/pybind.cpython-36m-x86_64-linux-gnu.so: undefined symbol: glPushGroupMarkerEXT  "
"Hi, kudos for the great work!  When are you planning to release the training code?  Thanks"
"Hi,    Can I know if the currently released pretrained model has been trained on 3dpw or not? Thanks!"
"Thank you for sharing your amazing work. I have tried to visualize the projected 2d keypoints (i.e. `pj2d`) on the original image.   Taking the `3dpw_sit_on_street.jpg` sample image as an example. Since `pj2d` are normalized (-1~1) coordinates on the input image size (512x 512), I first compute their corresponding image coordinate in the original image size (1920x1080) by:  `pj2d= (pj2d+      Any thoughts on how to compute and visualize projected 2D keypoints correctly would be really appreciated.   "
"@Arthur151  I have a question. In the final outputs of 25 3D op_joints, I want to revert/reverse the values of Z. Kindly update me that how to do it?    And if I want to revert the values of all axis (X, Y and Z) then how I can change it?      Waiting for response. "
"Impressive project, I want to run on Windows, however windows seems donot have the EGL library as required.  However I have encountered this issue:  ImportError: ('Unable to load EGL library', ""Could not find module 'EGL' (or one of its dependencies). Try using the full path with constructor syntax."", 'EGL', None)  Does the openGL library necessary and do the code can run on windows ?"
"I'm trying to draw body joints in 3D space using `3D SMPL joints` and `3D openpose joints` using CenterHMR. I followed SMPL joints order from   and openpose order from      I'm getting wrong 3D skeleton, could you share the right order of joints that CenterHMR outputs? Joints order is same with original SMPL or you changed joints order in output?"
Can you share the video / ppt for your presentation at eccv 2020 workshop? I am also interested in solutions from other teams. But I can not find related materials on the internet.
"Hi,   thanks for releasing the code.   When I run the code on the google colab, I get this error     ----------------  Configuration:  {'tab': 'test', 'configs_yml': 'configs/basic_test.yml', 'demo_image_folder': 'None', 'multi_person': True, 'use_coordmaps': True, 'head_conv_block_num': 2, 'kp3d_format': 'smpl24', 'eval': False, 'max_person': 16, 'BN_type': 'BN', 'Rot_type': '6D', 'center_idx': 1, 'centermap_size': 64, 'HMloss_type': 'MSE', 'model_precision': 'fp32', 'baseline': 'hrnetv5', 'input_size': 512, 'gmodel_path': '../trained_models/pw3d_81.8_58.6.pkl', 'best_save_path': '', 'print_freq': 50, 'epoch': 300, 'fine_tune': True, 'lr': 0.0003, 'weight_decay': 1e-05, 'gpu': '0', 'batch_size': 64, 'val_batch_size': 2, 'nw': 4, 'dataset_rootdir': '/content/dataset/', 'dataset': 'h36m,mpii,coco,aich,up,ochuman,lsp,movi', 'voc_dir': '/content/dataset/VOCdevkit/VOC2012/', 'high_resolution': True, 'save_best_folder': '/content/checkpoints/', 'log_path': '/content/log/', 'total_param_count': 85, 'smpl_model_path': '/content/CenterHMR/models', 'adjust_lr_epoch': [], 'kernel_sizes': [5], 'GPUS': 0, 'save_mesh': False, 'save_centermap': True, 'save_dict_results': True, 'webcam': False, 'multiprocess': False}  ----------------  pygame 2.0.0 (SDL 2.0.12, python 3.6.9)  Hello from the pygame community.    model_type smpl  start building model.  ********************  using fine_tune model:  ../trained_models/pw3d_81.8_58.6.pkl  missing parameters of layers:  []  ********************  finished build model.  Initialization finished!  gathering datasets  Loading 3 internet data from:/content/CenterHMR/demo/images  Size should be int or sequence. Got    Size should be int or sequence. Got    Traceback (most recent call last):    File ""core/test.py"", line 194, in        main()    File ""core/test.py"", line 190, in main      demo.run(demo_image_folder)    File ""core/test.py"", line 28, in run      for test_iter,data_3d in enumerate(loader_val):    File ""/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py"", line 435, in __next__      data = self._next_data()    File ""/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py"", line 1085, in _next_data      return self._process_data(data)    File ""/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py"", line 1111, in _process_data      data.reraise()    File ""/usr/local/lib/python3.6/dist-packages/torch/_utils.py"", line 428, in reraise      raise self.exc_type(msg)  TypeError: Caught TypeError in DataLoader worker process 0.  Original Traceback (most recent call last):    File ""/content/CenterHMR/src/dataset/internet.py"", line 84, in __getitem__      return self.get_item_single_frame(index)    File ""/content/CenterHMR/src/dataset/internet.py"", line 43, in get_item_single_frame      torchvision.transforms.Resize(resized_image_size, interpolation=3),    File ""/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py"", line 253, in __init__      raise TypeError(""Size should be int or sequence. Got {}"".format(type(size)))  TypeError: Size should be int or sequence. Got      During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/worker.py"", line 198, in _worker_loop      data = fetcher.fetch(index)    File ""/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch      data = [self.dataset[idx] for idx in possibly_batched_index]    File ""/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py"", line 44, in        data = [self.dataset[idx] for idx in possibly_batched_index]    File ""/content/CenterHMR/src/dataset/mixed_dataset.py"", line 19, in __getitem__      return self.dataset[index]    File ""/content/CenterHMR/src/dataset/internet.py"", line 88, in __getitem__      return self.get_item_single_frame(index)    File ""/content/CenterHMR/src/dataset/internet.py"", line 43, in get_item_single_frame      torchvision.transforms.Resize(resized_image_size, interpolation=3),    File ""/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py"", line 253, in __init__      raise TypeError(""Size should be int or sequence. Got {}"".format(type(size)))  TypeError: Size should be int or sequence. Got  "
ä½œè€…ä½ å¥½ï¼Œè¯·é—®åœ¨åšè®­ç»ƒæ•°æ®çš„æ—¶å€™ï¼Œcamera mapï¼ˆ1x3ï¼‰æ˜¯é€šè¿‡3dpwçš„ground truthä¸­çš„å“ªäº›æ•°æ®é€šè¿‡ä»€ä¹ˆæ–¹å¼æ¢ç®—è¿‡æ¥çš„å‘¢ï¼Ÿå¯å¦ç»™ä½ ä¸€ä¸ªå…·ä½“çš„å…¬å¼å‘¢ï¼Ÿ
"Visualization bug: OpenGL.error.GLError: GLError(err = 12289,baseOperation = eglMakeCurrent ....    The bug of pyrender is really a pain in the ass... ``bash    æŒ‰ç…§ä¸‹é¢å…³äºŽ centos çš„è§£å†³åŠžæ³•ï¼Œå¯¹æˆ‘çš„linuxç³»ç»Ÿï¼ˆredHatï¼‰ æ— æ•ˆï¼Œä¾ç„¶æŠ¥ä¸Šé¢çš„é”™  for centos  sudo yum install libXext libSM libXrender freeglut-devel    "
"Hi,  I need not the real depth but the order of the different depths. in your paper you mention ..."" For people of different scales, the people with greater scale are in the front. For people of similar scale, the people with greater center confidence are in the front""...  My doubt is how you classify ""different scales"" & ""similar scales"".  (Maybe you have already coded that issue when selecting the rendering order of the different  masks/meshs; if so could you point where that code is?)  Many Thanks in advance!"
"Cell ""!CUDA_VISIBLE_DEVICES=0 python core/test.py --gpu=0 --configs_yml=configs/basic_test_video.yml""  Outputs:  ----------------  Configuration:  {'tab': 'test', 'configs_yml': 'configs/basic_test_video.yml', 'demo_image_folder': '../demo/videos/Messi_1', 'multi_person': True, 'use_coordmaps': True, 'head_conv_block_num': 2, 'kp3d_format': 'smpl24', 'eval': False, 'max_person': 16, 'BN_type': 'BN', 'Rot_type': '6D', 'center_idx': 1, 'centermap_size': 64, 'HMloss_type': 'MSE', 'model_precision': 'fp32', 'baseline': 'hrnetv5', 'input_size': 512, 'gmodel_path': '../trained_models/pw3d_81.8_58.6.pkl', 'best_save_path': '', 'print_freq': 50, 'epoch': 300, 'fine_tune': True, 'lr': 0.0003, 'weight_decay': 1e-05, 'gpu': '0', 'batch_size': 64, 'val_batch_size': 8, 'nw': 4, 'dataset_rootdir': '/content/dataset/', 'dataset': 'h36m,mpii,coco,aich,up,ochuman,lsp,movi', 'voc_dir': '/content/dataset/VOCdevkit/VOC2012/', 'high_resolution': True, 'save_best_folder': '/content/checkpoints/', 'log_path': '/content/log/', 'total_param_count': 85, 'smpl_model_path': '/content/CenterHMR/models', 'adjust_lr_epoch': [], 'kernel_sizes': [5], 'GPUS': 0, 'save_mesh': False, 'save_centermap': False, 'save_dict_results': False, 'webcam': False}  ----------------  Traceback (most recent call last):    File ""core/test.py"", line 4, in        from utils.multiprocess import * #Multiprocess    File ""/content/CenterHMR/src/utils/multiprocess.py"", line 9, in        from utils.demo_utils import OpenCVCapture, Open3d_visualizer     File ""/content/CenterHMR/src/utils/demo_utils.py"", line 4, in        import open3d as o3d    File ""/usr/local/lib/python3.6/dist-packages/open3d/__init__.py"", line 64, in        _CDLL(next((_Path(__file__).parent / 'cpu').glob('pybind*')))    File ""/usr/lib/python3.6/ctypes/__init__.py"", line 348, in __init__      self._handle = _dlopen(self._name, mode)  OSError: libc++.so.1: cannot open shared object file: No such file or directory""      Regards  "
None
Thank you for your awesome work!  I have one question.  Do you have plans to upload the training code?
Colab notebook:      Used Google Colab GPU runtime.  Also had to edit config.py because the config file path string kept getting corrupted.    Error when running run.sh:     
"Hi, thanks for the great work.    Could you please hint me how to convert from rotation matrix representation (Bx3x3) to axis-angle representation (Bx3) (used in original SMPL)?    Thank you!"
"Hi there,    Really awesome work. Unfortunately, it can't run with the provided instructions. Trying to `sh run.sh` results in multiple errors.     First:      I installed smplx from here:      Next:        I had to install Mesa using the instructions from PyRender:      Next:        This one stumped me, couldn't get it to work. Maybe my Mesa installation is still not correct?"
HI!    Awesome work! When do you plan to release the code?
"Hi Authors,    Your work looks very interesting and I would like to dive more into the details, however, I was not able to refer the appendices mentioned in your paper. Can you please direct me to the version where I can find the Appendices.    Regards,  tomar-s"
"Hi,     I'm trying to include your method as a baseline for a research project. I tried running the code following the `.sh` file, and I'm getting the following error          Is this a file you created ? Or is there a package called utils that you are using ?    Thanks,   Lucas"
"I am very happy to see such an interesting work open source. Does the evaluation in your paper refer to one image or all ImageNet images? How to evaluate the various indicators of ImageNet, can you provide the evaluation code?"
"Hi,   Initially, it's a great work congratulations. Secondly, How much does your training take and what was the data amount for the imageNet's first 100 classes ?    Thanks in advance"
" Traceback (most recent call last):    File ""D:\PyCharmpro\PyCharm 2021.2.3\plugins\python\helpers\pydev\pydevd.py"", line 1483, in _exec      pydev_imports.execfile(file, globals, locals)  # execute the script    File ""D:\PyCharmpro\PyCharm 2021.2.3\plugins\python\helpers\pydev\_pydev_imps\_pydev_execfile.py"", line 18, in execfile      exec(compile(contents+""\n"", file, 'exec'), glob, loc)    File ""E:/github code/scouter/train.py"", line 238, in        param_translation(args)    File ""E:/github code/scouter/train.py"", line 221, in param_translation      main(args)    File ""E:/github code/scouter/train.py"", line 178, in main      train_one_epoch(model, data_loader_train, optimizer, device, record, epoch)    File ""E:\github code\scouter\engine.py"", line 8, in train_one_epoch      calculation(model, ""train"", data_loader, device, record, epoch, optimizer)    File ""E:\github code\scouter\engine.py"", line 30, in calculation      logits, loss_list = model(inputs, labels)    File ""D:\Ruanjian\Anaconda\envs\youda\lib\site-packages\torch\nn\modules\module.py"", line 1102, in _call_impl      return forward_call(*input, **kwargs)   File ""E:\github code\scouter\sloter\slot_model.py"", line 108, in forward      x = self.conv1x1(x.view(x.size(0), self.channel, self.feature_size, self.feature_size))  RuntimeError: shape '[32, 128, 9, 9]' is invalid for input of size 802816    What is the cause of this error?  Can you help me? Thank you very muchï¼"
where is prefetch_generator.py ??
"Given a feature map F, the traditional FC layer first average pooling F and then reduce it to the number of class, while SCOUTER first reduces by Conv & ReLU and then sum pooling. The essential transformations are similar, except that SCOUTER has additional attention like No-local. Therefore, can the same explainable results be obtained using only simple Conv & ReLU without attention?"
"Really interesting paper, congratulation!    It would be much easier to understand the dataset's directory if paths of images will be mentioned.  So to make it simpler, I trained the model based on the MNIST dataset, then I tried to use ""torchcam_vis"" and ""test"" but it requires another data probably, that I do not have it.  path to where the data should be: 'sloter/vis/slot_0.png'    I think I did something wrong in between and a step-by-step demonstration of how to utilize the dataset will be a great help for me.  Another question is that if you use both positive and negative SCOUTER through the test part?  Wish all the best for you and your paper!"
"Hi,        Thx for your resource code. But I could not find the hyper parameters for Table 8 results of different teacher-student model pairs, such as the KD KL loss weight and GT cross entropy loss weight for different models and algorithms. Looking forward to your reply, thx a lot."
A great work. You provide some pretrained models.  I want to use these models. What are the means and variances of these datasets?     Did you use the mean and variance of ImageNet ?
"Hi! First of all thank you for this interesting work!    I'm trying to apply this loss function to my multi-label dataset and to choose a good set of hyper-parameters. I'm wondering how the code dynamically change the hyper-parameter gamma_neg mentioned in Section 2.7.  According to Eq.(11), there's a parameter named lambda, I'm not sure what value should I set to this one. Could you please share the setup of this part?      In addition, my dataset is a highly imbalanced one, and there are a lot of samples with labels which are all negative, which means during each iteration, there may appear a situation where all labels are all 0s. Do you think this technique would work on my dataset?    Thank you a lot!"
None
"Hi, thanks for the work. I see in the code that targets are built by object areas as     and     Why the targets are processed like this please?"
"!   This result is difficult to reproduce. Not only am I having a hard time solving this problem, but others are also having this problem too.  Would you share the configuration file and training log of ResNet101?  The result I report is map=82.9."
"The following error occurred in anti_aliasing.py:40 when I used multiple Gpus for training      RuntimeError: Assertion `THCTensor_(checkGPU)(state, 3, input, output, weight)' failed. Some of weight/gradient/input tensors are located on different GPUs. Please move them to a single one. at /pytorch/aten/src/THCUNN/generic/SpatialDepthwiseConvolution.cu:16"
"In train.py, between line 54 and 69,        It says that  >#normalize, # no need, toTensor does normalization    I have different thought. To Tensor scales images to (0, 1) while normalization to (-1, 1).  This should be different.    I do not test for this. Please Check it.    Thank you. :)"
"Why It always return nan?here is my log   (l1_loss): L1Loss()      (new_loss): AsymmetricLossOptimized()      (bcewithlog_loss): AsymmetricLossOptimized()      (iou_loss): IOUloss()    )  )  2022-04-20 13:03:33 | INFO     | yolox.core.trainer:202 - ---> start train epoch1  2022-04-20 13:03:39 | INFO     | yolox.core.trainer:260 - epoch: 1/45, iter: 10/646, mem: 5053Mb, iter_time: 0.555s, data_time: 0.001s, total_loss: nan, iou_loss: 2.4, l1_loss: 0.0, conf_loss: nan, cls_loss: 0.0, lr: 1.498e-10, size: 480, ETA: 4:28:59  2022-04-20 13:03:45 | INFO     | yolox.core.trainer:260 - epoch: 1/45, iter: 20/646, mem: 5571Mb, iter_time: 0.572s, data_time: 0.001s, total_loss: nan, iou_loss: 3.1, l1_loss: 0.0, conf_loss: nan, cls_loss: 0.0, lr: 5.991e-10, size: 640, ETA: 4:33:02  2022-04-20 13:03:48 | INFO     | yolox.core.trainer:260 - epoch: 1/45, iter: 30/646, mem: 5571Mb, iter_time: 0.324s, data_time: 0.001s, total_loss: nan, iou_loss: 3.1, l1_loss: 0.0, conf_loss: nan, cls_loss: 0.0, lr: 1.348e-09, size: 384, ETA: 3:54:11  2022-04-20 13:03:52 | INFO     | yolox.core.trainer:260 - epoch: 1/45, iter: 40/646, mem: 5571Mb, iter_time: 0.380s, data_time: 0.000s, total_loss: nan, iou_loss: 2.3, l1_loss: 0.0, conf_loss: nan, cls_loss: 0.0, lr: 2.396e-09, size: 448, ETA: 3:41:30  2022-04-20 13:03:56 | INFO     | yolox.core.trainer:260 - epoch: 1/45, iter: 50/646, mem: 5571Mb, iter_time: 0.442s, data_time: 0.000s, total_loss: nan, iou_loss: 2.3, l1_loss: 0.0, conf_loss: nan, cls_loss: 0.0, lr: 3.744e-09, size: 512, ETA: 3:39:53  2022-04-20 13:03:59 | INFO     | yolox.core.trainer:260 - epoch: 1/45, iter: 60/646, mem: 5571Mb, iter_time: 0.283s, data_time: 0.001s, total_loss: nan, iou_loss: 2.7, l1_loss: 0.0, conf_loss: nan, cls_loss: 0.0, lr: 5.392e-09, size: 320, ETA: 3:25:58  2022-04-20 13:04:02 | INFO     | yolox.core.trainer:260 - epoch: 1/45, iter: 70/646, mem: 5571Mb, iter_time: 0.275s, data_time: 0.001s, total_loss: nan, iou_loss: 2.7, l1_loss: 0.0, conf_loss: nan, cls_loss: 0.0, lr: 7.339e-09, size: 448, ETA: 3:15:28  2022-04-20 13:04:05 | INFO     | yolox.core.trainer:260 - epoch: 1/45, iter: 80/646, mem: 5571Mb, iter_time: 0.293s, data_time: 0.001s, total_loss: nan, iou_loss: 2.4, l1_loss: 0.0, conf_loss: nan, cls_loss: 0.0, lr: 9.585e-09, size: 512, ETA: 3:08:40  2022-04-20 13:04:07 | INFO     | yolox.core.trainer:260 - epoch: 1/45, iter: 90/646, mem: 5571Mb, iter_time: 0.228s, data_time: 0.001s, total_loss: nan, iou_loss: 2.5, l1_loss: 0.0, conf_loss: nan, cls_loss: 0.0, lr: 1.213e-08, size: 384, ETA: 2:59:52  "
Thanks for your work! But when i use your special OpenImages datasets i found there are some class from mid_to_classes.pth   not in the idx_to_class in your pretrained model --  Open_ImagesV6_TRresNet_L_448.pth -- . So could you tell me how to finished to reflected between mid to idx ? Thank you for answer this issues!
æ‚¨å¥½ï¼Œæˆ‘çœ‹æ‚¨åœ¨è®ºæ–‡é‡Œæåˆ°äº†ä½¿ç”¨è‡ªé€‚åº”çš„æ–¹æ³•æ¥é€‰å–è¶…å‚æ•°ï¼Œè¯·é—®æœ‰æ²¡æœ‰å…·ä½“çš„æ–¹æ³•å’Œä»£ç å¯ä»¥å­¦ä¹ ï¼Ÿ
"position:    line:        30, 86  code:      xs_neg = (xs_neg + self.clip).clamp(max=1),       self.xs_neg.add_(self.clip).clamp_(max=1)  problemï¼š  In the paper, shifted probability should be "" xs_neg = (xs_neg - self.clip).clamp(min=0, max=1) """
"Nice work! I wonder how to pass weights to this custom ASL loss, because our previous task used 'criterion = nn.BCEWithLogitsLoss(weight=self.y_pos_weight)'.  Can you give some advice on handling the weights parameter?  Thanks a lot~"
What is the best practice to increase the number of tags of an existing model without retraining the whole model again?  Possible solutions like freezing layers or train a seperate model for each batch of new tags?  So if we freeze the layers what approach you suggest to freeze which layers and how?
"Hi    TResNet can be easily converted to onnx. i have converted them in the past.  follow the steps listed below, don't reinvent the wheel, or do other stuff    1) update to latest ASL repo  2) edit the class ""SpaceToDepthModule"" to be:     (you can pass the 'remove_model_jit' as an argument for something more modular)  3)   After you create the model and load the proper weights, do:     This will transfrom inplace bn to equivilent standart bn          If after these steps you encounter problems, let me know    Tal    _Originally posted by @mrT23 in  "
Thank you very much!
"     As far as I know, transforms.ToTensor does not have normalization."
é’ˆå¯¹mutil labelçš„æ•°æ®ä¸å‡è¡¡ç¨‹åº¦ï¼Œå¯¹ASL çš„lossè¶…å‚æ•°æœ‰æ²¡æœ‰ä¸€ä¸ªæŽ¨èé…ç½®äº†
I want to check OpenImagesV6 map value with TResNet_L of onnx.    I  check validate.py is for Coco Dataset. Could you provide  validate script for OpenImagesV6 dataset ? Or can you give me some suggestion? Thanks a lot.
I try to conver onnx to tensorrt of version 8.0.1.6    face this issue        Do you success to convert  onnx to tensorrt ?    Thank you!  
"Thanks for such an interesting paper ðŸ‘    I want to convert pretrain weight of TResNet_L to onnx but pytorch jit and export to onnx not work with inplace-abn.    I follow the solution which is answered in issue      &       Here is the steps how I implement.    1.  I replaced InPlaceABN(num_features=nf, activation=activation, activation_param=activation_param) on def conv2d_ABN( ) function with   nn.BatchNorm2d(num_features=nf),  nn.LeakyReLU(activation_param)  in tresnet.py    2.  I remove  isinstance(m, InPlaceABN) on line 174 of tresnet.py  elif isinstance(m, nn.BatchNorm2d) or isinstance(m, InPlaceABN): into  elif isinstance(m, nn.BatchNorm2d)    After chang here is the tresnet.py code  `import torch    import torch.nn as nn    from torch.nn import Module as Module    from collections import OrderedDict    from src.models.tresnet.layers.anti_aliasing import AntiAliasDownsampleLayer    from .layers.avg_pool import FastAvgPool2d    from .layers.general_layers import SEModule, SpaceToDepthModule    from inplace_abn import InPlaceABN        class bottleneck_head(nn.Module):        def __init__(self, num_features, num_classes, bottleneck_features=200):            super(bottleneck_head, self).__init__()            self.embedding_generator = nn.ModuleList()            self.embedding_generator.append(nn.Linear(num_features, bottleneck_features))            self.embedding_generator = nn.Sequential(*self.embedding_generator)            self.FC = nn.Linear(bottleneck_features, num_classes)            def forward(self, x):            self.embedding = self.embedding_generator(x)            logits = self.FC(self.embedding)            return logits            def conv2d(ni, nf, stride):        return nn.Sequential(            nn.Conv2d(ni, nf, kernel_size=3, stride=stride, padding=1, bias=False),            nn.BatchNorm2d(nf),            nn.ReLU(inplace=True)        )            def conv2d_ABN(ni, nf, stride, activation=""leaky_relu"", kernel_size=3, activation_param=1e-2, groups=1):        return nn.Sequential(            nn.Conv2d(ni, nf, kernel_size=kernel_size, stride=stride, padding=kernel_size // 2, groups=groups,                      bias=False),            # InPlaceABN(num_features=nf, activation=activation, activation_param=activation_param)            nn.BatchNorm2d(num_features=nf),            nn.LeakyReLU(activation_param)        )            class BasicBlock(Module):        expansion = 1            def __init__(self, inplanes, planes, stride=1, downsample=None, use_se=True, anti_alias_layer=None):            super(BasicBlock, self).__init__()            if stride == 1:                self.conv1 = conv2d_ABN(inplanes, planes, stride=1, activation_param=1e-3)            else:                if anti_alias_layer is None:                    self.conv1 = conv2d_ABN(inplanes, planes, stride=2, activation_param=1e-3)                else:                    self.conv1 = nn.Sequential(conv2d_ABN(inplanes, planes, stride=1, activation_param=1e-3),                                               anti_alias_layer(channels=planes, filt_size=3, stride=2))                self.conv2 = conv2d_ABN(planes, planes, stride=1, activation=""identity"")            self.relu = nn.ReLU(inplace=True)            self.downsample = downsample            self.stride = stride            reduce_layer_planes = max(planes * self.expansion // 4, 64)            self.se = SEModule(planes * self.expansion, reduce_layer_planes) if use_se else None            def forward(self, x):            if self.downsample is not None:                residual = self.downsample(x)            else:                residual = x                out = self.conv1(x)            out = self.conv2(out)                if self.se is not None: out = self.se(out)                out += residual                out = self.relu(out)                return out            class Bottleneck(Module):        expansion = 4            def __init__(self, inplanes, planes, stride=1, downsample=None, use_se=True, anti_alias_layer=None):            super(Bottleneck, self).__init__()            self.conv1 = conv2d_ABN(inplanes, planes, kernel_size=1, stride=1, activation=""leaky_relu"",                                    activation_param=1e-3)            if stride == 1:                self.conv2 = conv2d_ABN(planes, planes, kernel_size=3, stride=1, activation=""leaky_relu"",                                        activation_param=1e-3)            else:                if anti_alias_layer is None:                    self.conv2 = conv2d_ABN(planes, planes, kernel_size=3, stride=2, activation=""leaky_relu"",                                            activation_param=1e-3)                else:                    self.conv2 = nn.Sequential(conv2d_ABN(planes, planes, kernel_size=3, stride=1,                                                          activation=""leaky_relu"", activation_param=1e-3),                                               anti_alias_layer(channels=planes, filt_size=3, stride=2))                self.conv3 = conv2d_ABN(planes, planes * self.expansion, kernel_size=1, stride=1,                                    activation=""identity"")                self.relu = nn.ReLU(inplace=True)            self.downsample = downsample            self.stride = stride                reduce_layer_planes = max(planes * self.expansion // 8, 64)            self.se = SEModule(planes, reduce_layer_planes) if use_se else None            def forward(self, x):            if self.downsample is not None:                residual = self.downsample(x)            else:                residual = x                out = self.conv1(x)            out = self.conv2(out)            if self.se is not None: out = self.se(out)                out = self.conv3(out)            out = out + residual  # no inplace            out = self.relu(out)                return out            class TResNet(Module):            def __init__(self, layers, in_chans=3, num_classes=1000, width_factor=1.0,                     do_bottleneck_head=False,bottleneck_features=512):            super(TResNet, self).__init__()                # JIT layers            space_to_depth = SpaceToDepthModule()            anti_alias_layer = AntiAliasDownsampleLayer            global_pool_layer = FastAvgPool2d(flatten=True)                # TResnet stages            self.inplanes = int(64 * width_factor)            self.planes = int(64 * width_factor)            conv1 = conv2d_ABN(in_chans * 16, self.planes, stride=1, kernel_size=3)            layer1 = self._make_layer(BasicBlock, self.planes, layers   and re-inference .It can success to covert to onnx but the result is wrong.    Here is the inference code  `import torch  from src.helper_functions.helper_functions import parse_args  from src.loss_functions.losses import AsymmetricLoss, AsymmetricLossOptimized  from src.models import create_model  import argparse  # import matplotlib    # matplotlib.use('TkAgg')  # import matplotlib.pyplot as plt  from PIL import Image  import numpy as np    ############################  import torch.nn as nn  from torch.nn import Module as Module  from collections import OrderedDict    parser = argparse.ArgumentParser(description='ASL MS-COCO Inference on a single image')    parser.add_argument('--model_path', type=str, default='./models_local/Open_ImagesV6_TRresNet_L_448.pth')  parser.add_argument('--pic_path', type=str, default='pics/000000000885.jpg')  parser.add_argument('--model_name', type=str, default='tresnet_l')  parser.add_argument('--input_size', type=int, default=448)  parser.add_argument('--dataset_type', type=str, default='OpenImages')  parser.add_argument('--th', type=float, default=None)    def fuse_bn_to_conv(bn_layer, conv_layer):      # print('bn fuse')      bn_st_dict = bn_layer.state_dict()      conv_st_dict = conv_layer.state_dict()        # BatchNorm params      eps = bn_layer.eps      mu = bn_st_dict['running_mean']      var = bn_st_dict['running_var']      gamma = bn_st_dict['weight']        if 'bias' in bn_st_dict:          beta = bn_st_dict['bias']      else:          beta = torch.zeros(gamma.size(0)).float().to(gamma.device)        # Conv params      W = conv_st_dict['weight']      if 'bias' in conv_st_dict:          bias = conv_st_dict['bias']      else:          bias = torch.zeros(W.size(0)).float().to(gamma.device)        denom = torch.sqrt(var + eps)      b = beta - gamma.mul(mu).div(denom)      A = gamma.div(denom)      bias *= A      A = A.expand_as(W.transpose(0, -1)).transpose(0, -1)        W.mul_(A)      bias.add_(b)        conv_layer.weight.data.copy_(W)      if conv_layer.bias is None:          conv_layer.bias = torch.nn.Parameter(bias)      else:          conv_layer.bias.data.copy_(bias)      def extract_layers(model):      list_layers = []      for n, p in model.named_modules():          list_layers.append(n)      return list_layers      def compute_next_bn(layer_name, resnet):      list_layer = extract_layers(resnet)      assert layer_name in list_layer      if layer_name == list_layer[-1]:          return None      next_bn = list_layer[list_layer.index(layer_name) + 1]      if extract_layer(resnet, next_bn).__class__.__name__ == 'BatchNorm2d':#InPlaceABN          # print(""\n"",next_bn)          return next_bn      return None    def extract_layer(model, layer):      layer = layer.split('.')      module = model      if hasattr(model, 'module') and layer[0] != 'module':          module = model.module      if not hasattr(model, 'module') and layer[0] == 'module':          layer = layer[1:]      for l in layer:          if hasattr(module, l):              if not l.isdigit():                  module = getattr(module, l)              else:                  module = module[int(l)]          else:              return module      return module      def set_layer(model, layer, val):      layer = layer.split('.')      module = model      if hasattr(model, 'module') and layer[0] != 'module':          module = model.module      lst_index = 0      module2 = module      for l in layer:          if hasattr(module2, l):              if not l.isdigit():                  module2 = getattr(module2, l)              else:                  module2 = module2[int(l)]              lst_index += 1      lst_index -= 1      for l in layer[:lst_index]:          if not l.isdigit():              module = getattr(module, l)          else:              module = module[int(l)]      l = layer[lst_index]      setattr(module, l, val)        def fuse_bn(model):      store_next_bn=""""      for n, m in model.named_modules():          if isinstance(m, nn.Conv2d):              next_bn = compute_next_bn(n, model)              if next_bn is not None:                  store_next_bn=store_next_bn+next_bn+"" ""                  # next_bn_ = extract_layer(model, next_bn)                  # fuse_bn_to_conv(next_bn_, m)                  # set_layer(model, next_bn, nn.Identity())                        return store_next_bn      def main():      print('ASL Example Inference code on a single image')        # parsing args      args = parse_args(parser)        # setup model      print('creating and loading the model...')      state = torch.load(args.model_path)      state_dict = torch.load(args.model_path)['model']      args.num_classes = state['num_classes']      model = create_model(args)      store_next_bn=fuse_bn(model)      model_dict = model.state_dict()      # model_dict = model.body        new_state_dict = OrderedDict()      eps=1e-05      for k, v in state['model'].items():#.keys() .items()                   ss = k.split('.')          if ss[-1].endswith('weight'):              find_bn_weight=k.split('.weight')[0]                          if store_next_bn.find(find_bn_weight)!=-1:              v1 = torch.abs(v) + eps          else:              v1 = v          new_state_dict[k] = v1      # import pdb      # pdb.set_trace()              # model.load_state_dict(state['model'])      model.load_state_dict(new_state_dict)      model.cuda()      model.eval()        classes_list = np.array(list(state['idx_to_class'].values()))           print('done\n')        # doing inference      print('loading image and doing inference...')      im = Image.open(args.pic_path)      import time      tic=time.time()      im_resize = im.resize((args.input_size, args.input_size))      np_img = np.array(im_resize, dtype=np.uint8)      tensor_img = torch.from_numpy(np_img).permute(2, 0, 1).float() / 255.0  # HWC to CHW      tensor_batch = torch.unsqueeze(tensor_img, 0).cuda()            output = torch.squeeze(torch.sigmoid(model(tensor_batch)))            np_output = output.cpu().detach().numpy()            detected_classes = classes_list[np_output > args.th]            toc=time.time()      print(""spend time:"",toc-tic)      print(""detected classes: {}"".format(detected_classes))      print('done\n')                    # doing inference      # print('loading image and doing inference...')      # im = Image.open(args.pic_path)      # im_resize = im.resize((args.input_size, args.input_size))      # np_img = np.array(im_resize, dtype=np.uint8)      # tensor_img = torch.from_numpy(np_img).permute(2, 0, 1).float() / 255.0  # HWC to CHW      # tensor_batch = torch.unsqueeze(tensor_img, 0).cuda()                # x = torch.randn(1, 3, 448,448)      # torch.onnx.export(model,      #                       tensor_batch,      #                       ""asl.onnx"",          #                       opset_version=10,      #                       input_names=['input'],      #                       output_names=['output']                           #                       )                       if __name__ == '__main__':      main()  `    **Could you provide onnx pretrain weight to me or help me to check if my implement steps is wrong.    Thank you very much!**    "
"Hi,@mrT23  First,  thank you for sharing the work. May I ask you some questions?  Question A: I'm not clear about the relationship between ""warmup + cosine decay"" and ""one cycle policy""  Question B: When batch_size is 16, how do we choose ema_decay? I only have one GPU.  "
"In the task of ATSS detection ,  should we select which one of ASL  functions, ASLsinglelabel or AsymmetricLossï¼Ÿï¼Œ there are some problems when I directly use the AsymmetricLoss."
å¯¹äºŽå•æ ‡ç­¾ä»»åŠ¡ï¼š  å½“gamma+ä¸º0ï¼ŒASLSingleLabel å°±æ˜¯ torch.nn. CrossEntropyLossï¼Ÿ  å½“gamma+å¤§äºŽ0ï¼ŒASLSingleLabel å°±æ˜¯ Focal  CrossEntropyLoss ï¼Ÿï¼ˆlosses.pyä¸­loss = - self.targets_classes.mul(log_preds)ï¼Œè¡¨æ˜Žgamma-å…¶å®žä¸èµ·ä½œç”¨ï¼‰
"I find that , when gamma+=0, ASLSingleLabel  is actually Equivalent to torch.nn.CrossEntropyLoss  .  And ,   Due to the existence of the codeâ€œloss = - self.targets_classes.mul(log_preds)â€ in  losses.py ,  the value gamma-  does not count, so would you please tell me the difference between ASLSingleLabel(gamma+>0) and Focal loss in your experiment (Table 12 in your paper) ?"
"I am trying to replicate your findings with the oidv6 dataset. So far I have managed to load the oidv6 dataset (only about 1.7million valid images) with their labels. I am using a efficientnetV2 backbone with a final dense layer (no activation function. 9605 classes lead to 11million Params on the last layer!). To augment the data I am using Autoaugment and randaugment. I shuffle the data before training.  My problems:   For some reason the loss Is extremely high (100k+) and after every epoch the loss ""resets"" to a high level. The accuracy only improves by ~0.15% per epoch starting from around 25% after the first epoch.   Batch loss:  !   Current training run:      I was wondering if you had any tips/ideas as to why this is happening. Thank you in advance!"
"This is nice work by you and your team.    This is not an issue but rather a question.   I am considering combining your ASL loss function with a contrastive loss for the purpose of semi-supervised, multi-labeled classification.  Do you have any experience with this?  If so, would you share your experience?    Thanks,  Leslie"
I use the loss for my model (not TResNet) and sometimes it has a negative value. Is this a problem or normal?
"RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [64, 38]] is at version 3; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!    I use a single label classification dataset.  The version of Pytorch I use is 1.7.1."
æœ€è¿‘æƒ³çŽ©ä¸‹å¤šæ ‡ç­¾åˆ†ç±»ï¼ŒåŒå­¦ç»™æˆ‘æŽ¨èå¤§ç¥žçš„è¿™ä¸ªä»£ç ã€‚æˆ‘æŒ‰ç…§requirements.txtè¦æ±‚ï¼ŒæŠŠçŽ¯å¢ƒè£…èµ·æ¥äº†ï¼Œæ•°æ®ä¹Ÿä¸‹è½½ä¸‹æ¥äº† ã€‚å¼€å§‹è®­ç»ƒæ—¶ï¼Œå‡ºçŽ°äº†è¿™ä¸ªé”™è¯¯ï¼šAttributeError: module 'torch._C' has no attribute 'set_grad_enabled'ã€‚æˆ‘è£…çš„torchç‰ˆæœ¬æ˜¯1.7.1ï¼Œcuda11.3ã€‚è¿™ä¸ªé—®é¢˜å¾ˆå¥‡æ€ªï¼Œéš¾é“æ˜¯è£…çš„torchç‰ˆæœ¬æœ‰é—®é¢˜ï¼Ÿ
"Hello again,    Thank you so much for your quick feedback on my last issue. Appreciate it.    I would like to know if you there is any validation code for PASCAL-VOC dataset in order to reproduce the same results.    Thanks,  Inder"
"Hi, where does ""from inplace_abn import InPlaceABN"" come from ? "
"I am getting very high loss valus. I have sigmoid in my last layer, so i removed it from here       Loss is around 100+, when i used   `loss_function=AsymmetricLoss(gamma_neg=0, gamma_pos=0, clip=0)`  `loss_function=AsymmetricLoss(gamma_neg=2, gamma_pos=2, clip=0)`  `loss_function=AsymmetricLoss(gamma_neg=2, gamma_pos=1, clip=0)`    With torch `BCELoss`, it ranges from `0.4-0.8`.    "
"After reading ASL paper and source code, I am trying to find source code about **asymmetric probability margin**. But I can't see any code with respect to asymmetric probability margin in  . Maybe bellow source code is about **asymmetric probability margin**?     But in ASL paper, **asymmetric probability margin** formula for negative sample is `pm=max(pâˆ’m, 0)`, the source code does not make sense.  I think the source code should be bellow:     I am not certain my understanding is right. Maybe I missed something, Please help me. Thanks!!!  "
"In  , there are two hyperparameter in FocalLoss which are **alpha** and **gamma**. But I can't find alpha hyperparameter in your paper, so I want to know what is the value of alpha. Thank a lot!"
"when i use SGD to optimize the model, should i change loss.sum() to loss.mean()?"
"Hi, thanks for your great works. Would you like to release the training and evaluation code for NUS-WIDE datasets?"
"Hello,    Very nice work. I am highly inspired by it and working on training from the scratch. I have a question regarding the default pre-trained weights which you have used to initialize the training. For example: the one which you have used locally were I guess: 'tresnet_m.pth'. Are they the one which are shared in the repo at the link      When I am starting the training from scratch without any pre-trained weights, my loss is extremely high and the val mAP is never crossing 50 or so even after 80 epochs.    Thanks in advance for your time.    Best,  Inder"
"Hi! I use multi-label AsymmetricLoss with default args in a modified timm's train script. When I turn on native AMP, I get  partly NaNs, partly regular floats from ASL. And fp32 is ok.    from log:   "
"Hi, thanks for your great repo.  Open images dataset V6 contains human-verified labels and machine-generated labels. Do you use the machine-generated labels for training and testing?      "
Could this loss function be used for mutil classificationï¼Ÿ
Thank you very much for your work.  I trained the ASL model using one gpu sucessfully. But when I trained it using multiple GPUs. Error occurred.  So Is it possible to train ASL using multiple GPU?  
å¤šè°¢å¤šè°¢å¤šè°¢ï¼
None
None
"Hi! I'm using ASL+EMA for imbalanced multi-label problem. For ordinary softmax problems avg_checkpoints.py from timm works very good. I've tried checkpoint averaging for ASL+EMA, the results are very different, they highly depend on the selected checkpoints. Are there approaches to choose best checkpoints to average them? And should we use checkpoint averaging with EMA at all?    "
"Sorry, may I ask you a questionï¼Ÿ    In paper, you define the shifted probability, pm, as:           pm = max(p-m, 0)  But in code, you define it as:        xs_neg = (xs_neg + self.clip).clamp(max=1)        I think it is means:        pm = min(1-p+m, 1)    Why ?"
"Hi, thank you very much for sharing the dataset and split of nuswide dataset.    I try to reproduce the mAP 65.2 that reported in paper on nuswide dataset, but I can only get a mAP of 64.0 based on the dataset and checkpoint you provided.  Both the data and checkpoint I used are downloaded from the link you provided. The script is obtained by modifying the `validate.py` in this repo and could be found in   The log could be found in   Could you give me some advice on the results?    Thanks very much again!"
"there occurs an error in the tresnet.py file. result = self.forward(*input, **kwargs)  RuntimeError: Some elements marked as dirty during the forward method were not returned as output. The inputs that are modified inplace must all be outputs of the Function.  I don't know why"
"Hi, thank you for releasing your training code for us to reproduce your models.    I notice the date augmention in your code:      train_dataset = CocoDetection(data_path_train,                                    instances_path_train,                                    transforms.Compose([                                        transforms.Resize((args.image_size, args.image_size)),                                        CutoutPIL(cutout_factor=0.5),                                        RandAugment(),                                        transforms.ToTensor(),                                        # normalize,                                    ]))    Here, RandAugment means the newly proposed augmentation by google? And did you use this augmention except cutout in your models? If so, when reproducing your code, I will use the augmention as you did.    Thanks in advance."
None
"Thanks for the inspiring implementation :)    I'm having trouble to reproduce the results on MSCOCO with tresnet_m as backbone, 224 as input.  I have varied lr (1e-4 and 2e-4), batch_size (128 and 64), epochs (40/80 and 14/25) and only got around 79.+% mAP (which are lower than reported pre-train models).    I understand you can't share the original training code due to commerical reasons, can you provide your hyper-parameters for the reproduction? Seems these hyper-params influence the results."
"Hi,    At first, thank you for sharing the training scripts. I really like your work which inspired me a lot.     1)  There may be a bug in the `train.py` as mentioned in #38, which can be fixed by replacing line 155 `target = target` with `target = target.max(dim=1) . Based on the script, I can achieve the mAP 85.5 on COCO(448*448), which is lower than the result 86.6 mAP in paper. Could you please help me find out the reason of this gap?    Thanks for your help and happy new year!"
"     **Can you confirm the following is a bug and the fix is valid?**         Should be:    `mAP_score_regular = mAP(torch.cat(targets).numpy()[:,-1,:], torch.cat(preds_regular).numpy())`    since:    targets is of shape (num_sample, 3, 80) and preds_regular is of shape (num_sample, 80).    The same issue is also on:         Not so sure which targets to pass in:      `mAP_score_ema = mAP(torch.cat(targets).numpy()[:,i,:], torch.cat(preds_ema).numpy())  # i=0,1,2 ?`    B.t.w, what does ""Ema"" stand for?  "
Is          should be:       ?  
"Hello, I tried to reproduce the result on COCO, I implemented my own framework and most of my files are the same as you, I only write my new train.py.  As is introduced in your paper, I have implemented EMA with 0.999, 1cycle policy with max learing rate 2e-4, Adam optimizer with weigth_decay 1e-4, img_size = 448*448 and batchsize = 16.    But when I train my model, the loss decreases from 120 to around 90 and then it just doesn't decrease and the performance on validation date is very bad, whose mAP is around 10, at first I guess it's because I didn't spend much time training, I only trained it for an hour, but when I try to train it longer, the loss still doesn't decrease, could you please tell me what I have done wrong?    My code is avaliable at   thank you for your help."
"I am curious if you tried to combine ASL with other techniques like Spatial Regularization net (  GCN (  , etc? Does that give you additional boost on AP score?    Thank you!"
"Hi @mrT23     I am looking at the pretrained model you provided. I notice that the pretrained Tresnet-L for COCO and OpenImage have different binary size. Though COCO has fewer categories than OpenImage, its size is ~215MB whereas OpenImage's model is only 120MB. I wonder why this is the case?    Thank you!"
"Hi, thank you for your work.  Can you also release some pretrained resnet101 model on COCO?"
"RuntimeError: Given groups=1, weight of size [76, 48, 3, 3], expected input[1, 64, 112, 112] to have 48 channels, but got 64 channels instead    Actually it is giving error on small PNG files, on the other hand if we convert the same file to JPG (with same size) it is working        Can u explain why it is so?    Thanks."
"Thank you for your work !  when i read the code of  losses.py, i find the variable disable_torch_grad_focal_loss in AsymmetricLoss, but i don't find  it appear in elsewhere . so i wonder if the disable_torch_grad_focal_loss is  just used for  paper's experiment comparing ?  and  in normal use,do we need to modify it ?  "
"Hi,    First, thank you for sharing the exciting work.    I was trying to reproduce the results on MS COCO dataset based on my own training framework. When I used cross entropy loss `loss_function=AsymmetricLoss(gamma_neg=0,** gamma_pos=0, clip=0)` to achieve the baseline. The result (with backbone of ResNet101) of mAP ~82.5% was achieved, which is quite similar to the result reported in Fig. 8 of the paper.     Then, I replaced the loss function with `loss_function=AsymmetricLoss(gamma_neg=4, gamma_pos=1, clip=0.05)` -- all other hyper parameters were kept consistent. However, I only got the mAP result of ~82.1%.     Also, the traditional focal loss `loss_function=AsymmetricLoss(gamma_neg=2, gamma_pos=2, clip=0)` can not outperform the baseline (~82.5%), given the same configurations. I am curious about the issue of my training process.    Could you also please share some training tricks? For example, a snippet of code on adjusting learning rate, training transforms similar to that used for validation  , etc. Or, is there any suggestions?    Thank you."
"Hi, I'm trying to implement your novel loss function in my dataset.   One thing about your ASL paper is confusing me, that is, which of the following pretrained model you used to implement ASL loss in your experiment?     1. tresnet_l.pth  2. tresnet_l_448.pth    Thanks in advance!  Best"
"Hi:  I am reading your paper and your source codes, there are some confusions:    1. In the paper, p_m = max(p-m, 0), however, in your implementation, p_m = p + 0.05. Are there some insights? Thanks~ "
"Hi, I have read the Appendices in your paper, in which you mentioned that ""We found that the common ImageNet statistics normalization [16, 8, 28] does not improve results, and instead used a simpler normalization - scaling all the RGB channels to be between 0 and 1"".      But in your validate.py, it was ""normalize = transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])"" ?    So if I want to train a new model using tresnet as a backbone, which of the above normalization is better?    Thanks."
"Hi, thanks for your nice repo.  I am experimenting with your pretrained network on openimages for my thesis.   But I came across some mismatch between the names of the classes you trained your network on and the official ones from OpenImagesV6.  As I understand it, you saved the class names along with the model. Then, in infer.py we load it into the variable 'classes_list'.  When I looked into that variable, the first 50 labels have a very strange string format (e.g. """"""Pig's organ soup"""""") and in addition the last 10 classes also seem to be damaged (e.g. and melon family' or ""pentathlon"""""" (this is raw text as it is in the list)). I attached a dump of the damaged labels as a zipped csv.   (please look at it in a text editor and not in excel)  I wonder what the implications of this damaged classes are? were these really the ones used during training?         To my understanding, the correct ids to be trained on would be here:   and the corresponding class descriptions could be found here:      Thanks in advance for your clarification and help. "
Thanks for this brilliant piece of work.     Can you provide the .py file you used for the training ? (for reproducing purpose )
"æ‚¨å¥½ã€‚  æˆ‘çš„æ•°æ®é›†ä¸€å…±æœ‰2ä¸ªæ ‡ç­¾ï¼Œæ¯ä¸ªæ ‡ç­¾æœ‰ä¸‰ä¸ªå±žæ€§ï¼ša1ã€a2ã€a3ï¼Œb1ã€b2ã€b3ã€‚  å®ƒä»¬ä¹‹é—´çš„æ¯”ä¾‹æ˜¯1,1.38807, 1.35329, 1.05098, 1.92411, 1.01199    ä½†æ˜¯æˆ‘çš„æ ·æœ¬ä¸å¹³è¡¡ï¼Œæ ¹æ®å±žæ€§æŽ’åˆ—ç»„åˆï¼Œå…±æœ‰9ç±»å›¾ç‰‡:a1b1ã€a1b2ã€a1b3ã€a2b1ã€a2b2ã€a2b3ã€a3b1ã€a3b2ã€a3b3  å®ƒä»¬ä¹‹é—´çš„æ•°é‡æ¯”ä¾‹æ˜¯ï¼š55ï¼Œ2.7ï¼Œ2.5ï¼Œ6.4ï¼Œ27ï¼Œ14.5ï¼Œ1ï¼Œ1ï¼Œ42    æˆ‘é¦–å…ˆä½¿ç”¨BCEWithLogitsLossè¿›è¡Œè®­ç»ƒï¼Œæ­¤æ—¶lossä»Ž1.5(å§‹)------>0.11(ç»ˆ)ï¼Œæ­¤æ—¶å¾®å¹³å‡æ˜¯90%ï¼Œå®å¹³å‡æ˜¯70%  ç„¶åŽåœ¨æ­¤åŸºç¡€ä¸Šï¼Œä½¿ç”¨æ‚¨çš„ASLè¿›è¡Œè®­ç»ƒï¼Œæ­¤æ—¶lossä»Ž0.11-----1---->3------>2------>1(ç»ˆ)ï¼Œæ­¤æ—¶å¾®å¹³å‡å’Œå®å¹³å‡åè€Œéƒ½é™ä½Žäº†ã€‚    è¯·é—®ï¼š  1.å®å¹³å‡æ¯”å¾®å¹³å‡å·®20%ï¼Œæ˜¯æ ·æœ¬ä¸å¹³è¡¡é€ æˆçš„å—  2.ä½¿ç”¨äº†ASLä¹‹åŽï¼Œç²¾åº¦åè€Œé™ä½Žï¼Œæ˜¯å‚æ•°è®¾ç½®çš„ä¸å¯¹å—ï¼Œåº”è¯¥å¦‚ä½•è®¾ç½®å‚æ•°ï¼ˆgamma_negã€gamma_posã€clipï¼‰ã€‚"
"Hello, I'm very interested in your novel work and try to run your validate.py file, but get totally wrong mAP:   Actually, I run   python validate.py  \  --model_name=tresnet_l \  --model_path=./models_local/MS_COCO_TRresNet_L_448_86.6.pth    but finally it printed out mAP score: 3.7.  Is something wrong with what I did? "
"Hello, thank you very much and your team's contribution in this respect, I intend to apply this loss function to my image multi label classification model (only label, no border label),   loss_function=AsymmetricLoss()  logits = net(images.to(device))   loss = loss_function(logits,labels.to(device))  I haven't changed your ALS loss function at all. At first, the loss was 156. Finally, it dropped to 4, ACC = 0. What's the matter? Why did the loss value just start to be more than 100, and still be 4 after training, and the accuracy rate is zeroï¼ŸWhen I use BCEloss, it's perfectly normal  train loss: 100%[**************************************************->]4.9414  [epoch 1] train_loss: 21.409  test_accuracy: 0.000  train loss: 100%[**************************************************->]5.7753"
None
"Hi, I download the NUS WIDE from the url presented in paper's appendice  .     However, it seems there are only images without **annotations and your train/test split list**, could you please upload annotations and split list?    Thanks!"
"In paper, ""For very hard negative samples (with p > pâˆ—, where pâˆ— is defined as the point where d (dp/dL)/dz=0,""  How to calculate pâˆ—?  "
The `mAP` function   and the   parameters are inconsistent.    
"Hi,   Could you upload the model pre-trained on Pascal VOC 2007?   Thanks!"
Why xs_neg = (xs_neg + self.clip).clamp(max=1) instead of xs_neg = (xs_neg - self.clip).clamp(min=0)?
"Hi,    I have a question regarding last layer initialisation as it was done in Focal Loss paper, so that  rare classes will initially have some low probability (Pi) of 0.01.    Do you also use some special init strategy?    Thanks,  Ilya"
"I am getting the following error when I use the ASL loss function with the TabNet classifier: TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'. Any idea what I am doing wrong?"
"Thanks for such an interesting paper ðŸ‘     In the paper's equation (4), asymmetric probability shifting is `p_m = max(p-m, 0)`, but in the implementation, it's called asymmetric clipping and there is `xs_neg = (xs_neg + self.clip).clamp(max=1)` which is probably `p_m = min(p+m, 1)`.    Is there a reason for this difference? "
"  File ""/content/gdrive/My Drive/project/ASL/inplace_abn/inplace_abn/functions.py"", line 8, in          from . import _backend  ImportError: cannot import name '_backend'    _backend.pyi file is in the same directory as functions.py"
"Hi, this repo is great!    Do you have any plan for releasing evaluation codes to see the reproduced results from the pretrained weights on MS-COCO and openimages?  I have tried to evaluate your models using this code (  but I failed to reproduce the scores and only got very low scores.       Thanks!"
æ‚¨å¥½ï¼Œéžå¸¸æ„Ÿè°¢æ‚¨ä»¥åŠæ‚¨çš„å›¢é˜Ÿåœ¨è¿™æ–¹é¢çš„è´¡çŒ®ï¼Œæˆ‘å‡†å¤‡å°†æ­¤æŸå¤±å‡½æ•°è¿ç”¨äºŽç»†ç²’åº¦åˆ†ç±»ï¼Œè¯·é—®å¦‚ä½•å°†æ­¤æŸå¤±å‡½æ•°æ­¤ä»»åŠ¡ï¼Œæ˜¯å¦å¯ä»¥å…¬å¼€train.pyç¤ºä¾‹ã€‚
"Hi,    I've tried running both the optimized and the non-optimized version of your loss and both caused an exception.  It seems that there is an issue with the self.target (is == None) in the optimized version and a shape issue with the non-optimized version.  What is the proper shape of the target in your loss?    Thanks"
"When I click it, is says:     > You have no right to access this object because of bucket acl.    Could you please fix it?    Thanks. "
None
"Thank you for your impressive job!  I am working on few shot learning scenario. After reading your paper and code, I have some questions:  1. Could you please upload the few shot data split?  2. The few shot trainning set up is different from CoverTree? It seems that CoverTree do both the pretranning and down-stream task training in few shot scenario? but your work runs pretraining using the whole  ModelNet40 dataset, then train the down-stream task in few show scenario?"
"Thank you for organizing a good code. I have a question about 'dataset render'. The code of processing the dataset in the 'Render' folder looks about the code of ModelNet40. 'PC_Normalisation.py' file should be to transforme to '.ply' to '.obj'. However, the data in the original ModelNet40 was not '.ply' format. Can you provide an unpreprocessing ModelNet40 dataset? Do I need to transform the files in the Model40 to '.ply' format? "
"Hi @hansen7 ,  Thanks for sharing this great work with us.   I have a question regarding the output log during pretraining. The detail is  .  The question is, it seems that the loss did not drop too much or its range is varied from 0.03 to 0.04. Did this happen to your pretraining as well?  But visually, I could see some improvements in the `plots` folder.  Best regards"
!   !   There is no mode containing partseg  Please have a look. Thank you!
None
" hello, when I reproduce the pre-training experiment on your shapenet car data set, I found that the training does not converge very well. I think it is related to the parameter of piece-wise_constant, wish  you can provide the  hyperparameter  setting?"
Thanks for your awesome work. Do you use the code in the render directory to generate complete point cloud and partial point cloud from 3D models for the training of point cloud completion?
Basically in consistent with the updated iccv submission version:    1. Few-shot learning  2. Network dissection and adjusted mutual information  3. Object-Level Contrastive Learning  4. Readme and website
"**General Interpretability:**  -  , specifically sections on `learned features`, `Shapley values`, `Influential Instance`  - ""Network dissection: Quantifying interpretability of deep visual representations"", CVPR 2017  - ""Feature Visualisation"", Olah, et al., Distill 2017.  -  's Portfolio  -  's Portfolio (also, transfer learning)    **General Pre-Training:**   - ""Rethinking ImageNet Pre-training"", ICCV 2019  - ""Rethinking Pre-training and Self-training"", NeurIPS 2020  - ""What is being transferred in transfer learning?"", NeurIPS 2020  - ""What Makes Instance Discrimination Good for Transfer Learning?"", ICLR 2021 Sub    **Ideas from Contrastive Learning:**  -      **Point Cloud Specific:**  - ""Rotation Invariant Convolutions for 3D Point Clouds Deep Learning"", 3DV 2019  - ""Quaternion Equivariant Capsule Networks for 3D Point Clouds"", ECCV 2020  - ""Label-Efficient Learning on Point Clouds using Approximate Convex Decompositions"", ECCV 2020  - ""On the Universality of Rotation Equivariant Point Cloud Networks"", ICLR 2021 Sub    **Extensions**:  - ""Neural Similarity Learning"", NeurIPS 2019"
"**Point Cloud Completion**  - ""Topnet: Structural point cloud decoder"", CVPR 2019  - ""3D Shape Completion with Multi-view Consistent Inference"", AAAI 2020  - ""Morphing and Sampling Network for Dense Point Cloud Completion"", AAAI 2020  - ""Cascaded Refinement Network for Point Cloud Completion"", CVPR 2020  - ""PF-Net: Point Fractal Network for 3D Point Cloud Completion"", CVPR 2020  - ""Point Cloud Completion by Skip-Attention Network With Hierarchical Folding"", CVPR 2020  - ""GRNet: Gridding Residual Network for Dense Point Cloud Completion"", ECCV 2020  - ""SoftPoolNet: Shape Descriptor for Point Cloud Completion and Classification"", ECCV 2020  - ""Weakly-supervised 3D Shape Completion in the Wild"", ECCV 2020  - ""Variational Relational Point Completion Network"", CVPR 2021"
Hi @hansen7   I did not see SensatUrban in your implementation of data setting      Best regards
Hi @hansen7   I saw you have released two types of weights. They are *_cls.pth and *_seg.pth. My guess is that they have the same encoder but just the seg one has a randomly initialized segmentation network appended to the pre-trained encoder.   Not sure if my guess is correct or not so need your information.  Best regards.
"Hi Hansen,    Thanks for your great work. I have one quick question here.    May I ask what's the detailed experiment setting of fine-tuning on ModelNet40? Could you please provide the command of this task?    Now I'm using:       If I made any mistakes, please let me know! Thank you!  "
"Hi Hansen,    Thanks for your work!    I'm trying to reproduce your result in Table 10. It said with the DGCNN backbone, OcCo has 89.2% accuracy. But I loaded your pre-trained model: dgcnn_occo_cls.pth and train the svm with the following command:         But it only gets 88.4% accuracy. May I ask did I do anything wrong?  "
"Hi!  Thanks for the great work!  I have a question regarding the baseline results of few-shot classification. How do you get the results of ""PointNet rand"" in Table 2?  Did you train PointNet from scratch using cross entropy loss on the sampled K-way N-shot training data?  Look forward to your reply! Thanks! "
"Hi Hanchen,    Thanks for sharing your work!    I have two questions and it would be great if you can help me figure this out!    1. Why do you generate incomplete point clouds by masking points occluded by a camera view? What's the difference if you just use a usual incomplete point cloud?    2. What's the difference between this work and PCN? I noticed you directly use PCN network architecture as your model and in my understanding, you just apply PCN to some downstream tasks. If you directly used PCN pre-trained weights, can they also get the improvements on these downstream tasks? "
"Hi @hansen7   Firstly, thank you for your code.  Secondly, I have a question about your code pre-training OcCo. I read your code in the completion task (OcCo), these encoders have the same architecture as the encoder of DGCNN or Pointnet classification task. These pre-trained weights are suitable to initialize for the downstream task classification (because they have the same architecture). However, in other downstream tasks such as part segmentation or semantic segmentation, the encoder of these tasks is not the same as the pre-trained weight. Thus, there are some components of the encoder that will be randomly initialized?    Thank you!"
"Hello and thanks for the terrific code.    I have one question, I am trying to use OcCo pretrained network on the semantic segmentation task to extract features and perform dense matching of 3d points (either Pointnet or pcn).    I have saved and I am loading the same h5 file to avoid alternations due to different sampling. However, if I run the encoder twice (with the same tensor as points), I do not get determenistic results and the returned feature tensors are different, without changing anything. I am in mode model.eval() to have a defined dropout. Could you elaborate please?"
"Hi,    I'm trying to load the lmdb data you shared using the lmdb_dataflow function, but I got an error:    _pickle.UnpicklingError: invalid load key, '\xdc'.    Do you know how to fix this? Thanks"
"### Pre-Training  - ""Don't Stop Pretraining: Adapt Language Models to Domains and Tasks"", ACL 2020  - ""Train No Evil: Selective Masking for Task-Guided Pre-Training"", EMNLP 2020  - ""Co-Tuning for Transfer Learning"", NeurIPS 2020  - ""Learning to Adapt to Evolving Domains"", NeurIPS 2020    ### Neural Representation Learning    Some of Vincent  's work:  - Implicit Neural Representations with Periodic Activation Functions, NeurIPS 2020,    - Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations, NeurIPS 2019  - and his thesis on ""Self-supervised Scene Representation Learning"""
"Hi,    On ModelNet40, I was trying to reproduce the experiment results in Table.9 in the supplementary, but got lower numbers:  PointNet with Jigsaw task: 82% vs.  87.5% in the paper  PointNet with  OcCo task : 85.49% vs. 88.7% in the paper    Similar case for the DGCNN backbone    The scripts to learn the embeddings are copied from provided bash template, e.g:   `python train_completion.py    --gpu 0 --dataset modelnet  --model pointnet_occo   --log_dir modelnet_pointnet_vanilla  `  for PointNet with OcCo task.  For linear SVM training and testing, the scripts are also carried from the bash template, e.g:  `python train_svm.py  --gpu 0  --model pointnet_util  --dataset modelnet40 --restore_path log/completion/modelnet_pointnet_vanilla/checkpoints/best_model.pth`      So I'm wondering if there is some more hyper-parameter tuning needed but not provided in the templates.    Thanks!"
"Hi,    As the PartNormalDataset dataloader for the part-seg task is not provided in the repository, I'm assuming that it is similar, if not the same, as the one  . But according to the training script (train_partseg.py),  it seems that both the input and output of the dataloader have some slight differences from the original one.  So could you please clarify the difference or provide the modified PartNormalDataset dataloader?    Thanks"
"Hi,     Would you please provide the occluded point cloud data used in the training for the completion task (train_completion.py)? I was looking for it based on the documentation   but in no vain.     Thanks!  Chao  "
Hi!    Would you share the weights of DGCNN model? Thank so much!    I am also wondering that why can't I pre-train the model on scanobjectnn dataset?
None
"If I want to use my own sequences, what are the input files, and what is the input data format?  "
I am looking for a pre-trained FCN model on COCO data. I noticed in your article that FCN is trained on COCO data. Could you provide weights? Thank you very much!
"I combin CRNasRNN  to FCNï¼Œbut this error that i can not deal with it,  the title is information of error  "
"Dear Authors,    This is a very great work! Thanks very much.   How to change the parameters of Cropping2D in order to detect image with size about 640 * 480?    Thanks very much."
"Dear All,    Initially an error ""**xrange is not defined""** came when I tried to run the demo.py, then I changed the ""xrange"" to ""range"" and the py script started to run,however , anothe error came:  I0414 00:13:46.749788 14861 upgrade_proto.cpp:628] Successfully upgraded file specified using deprecated V1LayerParameter  Traceback (most recent call last):    File ""crfasrnn_demo.py"", line 205, in        main(sys.argv[1:])    File ""crfasrnn_demo.py"", line 201, in main      run_crfrnn(input_file, output_file, gpu_device)    File ""crfasrnn_demo.py"", line 138, in run_crfrnn      segmentation = crfrnn_segmenter(_MODEL_DEF_FILE, _MODEL_FILE, gpu_device, [im])    File ""crfasrnn_demo.py"", line 97, in crfrnn_segmenter      out = net.forward_all(**{net.inputs[0]: caffe_in})    File ""../caffe/python/caffe/pycaffe.py"", line 176, in _Net_forward_all      for batch in self._batch(kwargs):    **File ""../caffe/python/caffe/pycaffe.py"", line 260**, in _Net_batch      num = len(blobs.itervalues().next())  **AttributeError: 'dict' object has no attribute 'itervalues'**    It seems that it is an compatible issue,     thank you in advance!     "
"In CRFasRNN, the `theta_alpha` and the `theta_gamma` are for spatial terms, and the `theta_beta` is for the term in feature space. In   the `theta_alpha` is `160`, but the `theta_gamma` is `3`. Why did these two parameters set for so much difference? In addition, the `theta_alpha` is `160` means that the image has a big spatial blur, right? What does this mean? Does any know the reason? Thanks."
@sadeepj @bittnt   Is it possible to process multiple images simultaneously via Multithreading.  **Can you please suggest - the required changes to be made for this.**    Thanks  Akash    
"Hi Everyone    I am trying to replicate the training of this model on Pascal VOC augmented dataset which has ~11500 training images and 346 validation images.    I have following queries regarding training:    1. For me, a whole pass through the dataset of 11500 images takes approximately 15-16 hours where i use a batch size of 1. Am I missing something or is it similarly slow for everyone?  2. How many complete passes/epochs through complete datasets are required to train model to desired accuracy roughly 70 Miu?  3. How is the trend - Will initial validation Miu accuracy decrease starting from fcn8 and then decrease. Any trend in this whosoever has trained successfully will really be of help.       Regards  Ankit "
"Hello  I am trying to train cfrasrnn with my own data using    I had to slightly modify some pieces of the code to make it run on my Mac (OSX.12) but it now runs the training.  I have one first problem when trying to parse the log file in order to vizualise loss (using loss_from_log.py   :   _Traceback (most recent call last):    File ""loss_from_log.py"", line 123, in        main()    File ""loss_from_log.py"", line 43, in main      train_loss.append(float(matched.group(1)))  AttributeError: 'NoneType' object has no attribute 'group'_    Indeed, in the log file, there's no ""loss-ft"" output. Why ?    The second is that when I try to use the caffemodel output (e.g. iteration 5000 or 7000) to test if it recognizes object, it now fails to do so (I tried with various objects).  What am I doing wrong please ?  Many thanks if any one can help !"
Hi  I tried to run the crfasrnn_demo.py on aws g3.16xlarge gpu instance.  but it took about 4 seconds.  how to can make it more fast?  cheers.
"Hi,  I am trying to build caffe but get the following errors after make:    [  2%] Built target proto  Scanning dependencies of target caffe  [  2%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/upgrade_proto.cpp.o  /home/rishabh/Downloads/crfasrnn/caffe/src/caffe/util/upgrade_proto.cpp: In function â€˜bool caffe::UpgradeV0LayerParameter(const caffe::V1LayerParameter&, caffe::V1LayerParameter*)â€™:  /home/rishabh/Downloads/crfasrnn/caffe/src/caffe/util/upgrade_proto.cpp:196:79: error: no matching function for call to â€˜caffe::ConvolutionParameter::set_pad(google::protobuf::uint32)â€™           layer_param->mutable_convolution_param()->set_pad(v0_layer_param.pad());                                                                                 ^  In file included from /home/rishabh/Downloads/crfasrnn/caffe/src/caffe/util/upgrade_proto.cpp:9:0:  /usr/local/include/caffe/proto/caffe.pb.h:16543:13: note: candidate: void caffe::ConvolutionParameter::set_pad(int, google::protobuf::uint32)   inline void ConvolutionParameter::set_pad(int index, ::google::protobuf::uint32 value) {               ^  /usr/local/include/caffe/proto/caffe.pb.h:16543:13: note:   candidate expects 2 arguments, 1 provided  /home/rishabh/Downloads/crfasrnn/caffe/src/caffe/util/upgrade_proto.cpp:207:40: error: no matching function for call to â€˜caffe::ConvolutionParameter::set_kernel_size(google::protobuf::uint32)â€™               v0_layer_param.kernelsize());                                          ^  In file included from /home/rishabh/Downloads/crfasrnn/caffe/src/caffe/util/upgrade_proto.cpp:9:0:  /usr/local/include/caffe/proto/caffe.pb.h:16573:13: note: candidate: void caffe::ConvolutionParameter::set_kernel_size(int, google::protobuf::uint32)   inline void ConvolutionParameter::set_kernel_size(int index, ::google::protobuf::uint32 value) {               ^  /usr/local/include/caffe/proto/caffe.pb.h:16573:13: note:   candidate expects 2 arguments, 1 provided  /home/rishabh/Downloads/crfasrnn/caffe/src/caffe/util/upgrade_proto.cpp:228:36: error: no matching function for call to â€˜caffe::ConvolutionParameter::set_stride(google::protobuf::uint32)â€™               v0_layer_param.stride());                                      ^  In file included from /home/rishabh/Downloads/crfasrnn/caffe/src/caffe/util/upgrade_proto.cpp:9:0:  /usr/local/include/caffe/proto/caffe.pb.h:16603:13: note: candidate: void caffe::ConvolutionParameter::set_stride(int, google::protobuf::uint32)   inline void ConvolutionParameter::set_stride(int index, ::google::protobuf::uint32 value) {               ^  /usr/local/include/caffe/proto/caffe.pb.h:16603:13: note:   candidate expects 2 arguments, 1 provided  /home/rishabh/Downloads/crfasrnn/caffe/src/caffe/util/upgrade_proto.cpp: In function â€˜caffe::V1LayerParameter_LayerType caffe::UpgradeV0LayerType(const string&)â€™:  /home/rishabh/Downloads/crfasrnn/caffe/src/caffe/util/upgrade_proto.cpp:480:12: error: â€˜V1LayerParameter_LayerType_CROPâ€™ was not declared in this scope       return V1LayerParameter_LayerType_CROP;              ^  /home/rishabh/Downloads/crfasrnn/caffe/src/caffe/util/upgrade_proto.cpp: In function â€˜const char* caffe::UpgradeV1LayerType(caffe::V1LayerParameter_LayerType)â€™:  /home/rishabh/Downloads/crfasrnn/caffe/src/caffe/util/upgrade_proto.cpp:858:8: error: â€˜V1LayerParameter_LayerType_CROPâ€™ was not declared in this scope     case V1LayerParameter_LayerType_CROP:          ^  src/caffe/CMakeFiles/caffe.dir/build.make:20997: recipe for target 'src/caffe/CMakeFiles/caffe.dir/util/upgrade_proto.cpp.o' failed  make[2]: *** [src/caffe/CMakeFiles/caffe.dir/util/upgrade_proto.cpp.o] Error 1  CMakeFiles/Makefile2:272: recipe for target 'src/caffe/CMakeFiles/caffe.dir/all' failed  make[1]: *** [src/caffe/CMakeFiles/caffe.dir/all] Error 2  Makefile:127: recipe for target 'all' failed  make: *** [all] Error 2    I have successfully built the BVLC caffe repo in the past without any errors.  Please help me out.    Thanks."
"Hi @bittnt   I am trying to train with my own data following Martin Kresner's     lmdb creation is OK nut when I make 'python2.7 solve.py 2>&1 | tee train.log', it gives me the error above. As you can see, batch_size =1 !  WHat should I do ?       mentionned."
"In the code          I think it's a typo, should read `mean_vec = [B,G,R]`  becauase `caffe.io.load_image` loads image as RGB, then you transform it to BGR and then subtract the means. "
"Hello,      Thanks for providing such an awesome project as Open Source.       I am looking for a way to identify only partial list of classes Say only identify cats and dogs while ignoring rest of the classes.  Is there a way to do this? Would identifying only partial list of classes provide better performance?    Thanks In Advance.  Kumar."
"I'm training CRFasRNN layer on top of upsampled score_final layer similar to that in FCN8s on my data, and I quickly noticed that the output from CRF-RNN is not very different to score_final. What could be the reason for that? Just the learning rate or something more interesting? "
"I've been extracting features from CRF-RNN and FCN8s, and I noticed one weird thing: When I extract features:     Features from FCN8s are twice the size of CRF-RNN. Why would that be the case? "
"i am using  cpu with;  32G RAM & 32 CORE CPU & WINDOWS OS     when the code start this line ; net = caffe.Net(model_def_file, model_file, caffe.TEST)  the  python has stopped working .  the code is ;  import sys  import time  import getopt  import os  import numpy as np  from PIL import Image as PILImage    # Path of the Caffe installation.  _CAFFE_ROOT = ""C:\Program Files\python3.5\python-3.5.3.amd64\Lib\site-packages\caffe""    # Model definition and model file paths  _MODEL_DEF_FILE = 'C:/cnn/crfasrnn_caffe/python-scripts/TVG_CRFRNN_new_deploy.prototxt'  # Contains the network definition  _MODEL_FILE = 'C:/cnn/crfasrnn_caffe/python-scripts/TVG_CRFRNN_COCO_VOC.caffemodel'  # Contains the trained weights. Download from      sys.path.insert(0, _CAFFE_ROOT + ""python"")  import caffe    _MAX_DIM = 100      def get_palette(num_cls):      """""" Returns the color map for visualizing the segmentation mask.        Args:          num_cls: Number of classes        Returns:          The color map      """"""        n = num_cls      palette = [0] * (n * 3)      for j in range(0, n):          lab = j          palette[j * 3 + 0] = 0          palette[j * 3 + 1] = 0          palette[j * 3 + 2] = 0          i = 0          while lab:              palette[j * 3 + 0] |= (((lab >> 0) & 1)  > 1) & 1)  > 2) & 1)  >= 3      return palette      def crfrnn_segmenter(model_def_file, model_file, gpu_device, inputs):           """""" Returns the segmentation of the given image.        Args:          model_def_file: File path of the Caffe model definition prototxt file          model_file: File path of the trained model file (contains trained weights)          gpu_device: ID of the GPU device. If using the CPU, set this to -1          inputs: List of images to be segmented         Returns:          The segmented image      """"""            assert os.path.isfile(model_def_file), ""File {} is missing"".format(model_def_file)      assert os.path.isfile(model_file), (""File {} is missing. Please download it using ""                                          ""./download_trained_model.sh"").format(model_file)        if gpu_device >= 0:          caffe.set_device(gpu_device)          caffe.set_mode_gpu()      else:          caffe.set_mode_cpu()                print (_CAFFE_ROOT)      net = caffe.Net(model_def_file, model_file, caffe.TEST)              num_images = len(inputs)      num_channels = inputs[0].shape[2]      assert num_channels == 3, ""Unexpected channel count. A 3-channel RGB image is exptected.""            caffe_in = np.zeros((num_images, num_channels, _MAX_DIM, _MAX_DIM), dtype=np.float32)      for ix, in_ in enumerate(inputs):          caffe_in[ix] = in_.transpose((2, 0, 1))        start_time = time.time()      out = net.forward_all(**{net.inputs[0]: caffe_in})      end_time = time.time()        print(""Time taken to run the network: {:.4f} seconds"".format(end_time - start_time))      predictions = out[net.outputs[0]]        return predictions[0].argmax(axis=0).astype(np.uint8)      def run_crfrnn(input_file, output_file, gpu_device = -1):      """""" Runs the CRF-RNN segmentation on the given RGB image and saves the segmentation mask.        Args:          input_file: Input RGB image file (e.g. in JPEG format)          output_file: Path to save the resulting segmentation in PNG format          gpu_device: ID of the GPU device. If using the CPU, set this to -1      """"""        input_image = 255 * caffe.io.load_image(input_file)      input_image = resize_image(input_image)        image = PILImage.fromarray(np.uint8(input_image))      image = np.array(image)        palette = get_palette(256)      #PIL reads image in the form of RGB, while cv2 reads image in the form of BGR, mean_vec = [R,G,B]       mean_vec = np.array([123.68, 116.779, 103.939], dtype=np.float32)      mean_vec = mean_vec.reshape(1, 1, 3)        # Rearrange channels to form BGR      im = image[:, :, ::-1]      # Subtract mean      im = im - mean_vec        # Pad as necessary      cur_h, cur_w, cur_c = im.shape      pad_h = _MAX_DIM - cur_h      pad_w = _MAX_DIM - cur_w      im = np.pad(im, pad_width=((0, pad_h), (0, pad_w), (0, 0)), mode='constant', constant_values=0)        # Get predictions      segmentation = crfrnn_segmenter(_MODEL_DEF_FILE, _MODEL_FILE, gpu_device, [im])      segmentation = segmentation[0:cur_h, 0:cur_w]        output_im = PILImage.fromarray(segmentation)      output_im.putpalette(palette)      output_im.save(output_file)      def resize_image(image):      """""" Resizes the image so that the largest dimension is not larger than 500 pixels.          If the image's largest dimension is already less than 500, no changes are made.        Args:          Input image        Returns:          Resized image where the largest dimension is less than 500 pixels      """"""        width, height = image.shape[0], image.shape[1]      max_dim = max(width, height)        if max_dim > _MAX_DIM:          if height > width:              ratio = float(_MAX_DIM) / height          else:              ratio = float(_MAX_DIM) / width          image = PILImage.fromarray(np.uint8(image))          image = image.resize((int(height * ratio), int(width * ratio)), resample=PILImage.BILINEAR)          image = np.array(image)        return image      def main(argv):      """""" Main entry point to the program. """"""        input_file = ""C:/cnn/crfasrnn_caffe/python-scripts/input.jpg""      output_file = 'C:\cnn\crfasrnn_caffe\python-scripts'      gpu_device = -1  # Use -1 to run only on the CPU, use 0-3[7] to run on the GPU      try:          opts, args = getopt.getopt(argv, 'hi:o:g:', [""ifile="", ""ofile="", ""gpu=""])      except getopt.GetoptError:          print(""crfasrnn_demo.py -i   -o   -g  "")          sys.exit(2)        for opt, arg in opts:          if opt == '-h':              print(""crfasrnn_demo.py -i   -o   -g  "")              sys.exit()          elif opt in (""-i"", ""ifile""):              input_file = arg          elif opt in (""-o"", ""ofile""):              output_file = arg          elif opt in (""-g"", ""gpudevice""):              gpu_device = int(arg)        print(""Input file: {}"".format(input_file))      print(""Output file: {}"".format(output_file))      if gpu_device >= 0:          print(""GPU device ID: {}"".format(gpu_device))      else:          print(""Using the CPU (set parameters appropriately to use the GPU)"")      run_crfrnn(input_file, output_file, gpu_device= -1)      if __name__ == ""__main__"":      main(sys.argv[1:])"
"Dear @bittnt ,    During training my CRFasRNN model, when I entered the commandï¼š   python ../fcn.berkeleyvision.org/voc-fcn8s-atonce/solve.py    This problem has occurred           this is my train.prototxt,  it's TVG_CRFRNN_new_traintest.prototxt that replace data layer with fcn8s-atonce's  data layer           my solver.prototxt       this my solve.py        "
"Hello, while running crfasrnn_demo.py, i'm not able to go below 4s on a K80 or P100.  It's running on GPU I'm sure (I checked memory allocations).    Seems pretty slow to me, does anyone got any idea on how I could speed up the classification ? Thanks"
"@bittnt   I tried to repeat the training process on PASCAL VOC 2012 (20classes plus background).  Here's my documents:      And here's the   I used for training, which is exact same as what you posted in  .    I trained with   and this caffe   and after around 200k iterations. The result was pretty bad (even much worse than fcn-8s) as attached.    Then I thought that I may miss something, so I used this   to extract info about layers on your pre-trained   and the result kinda surprised me that I cannot find the `MultiStageMeanfield ` as well as `multi_stage_meanfield_param ` on 57th layer. Were u training with a different architecture or ?    I am quite confused now and it will be great for you to give me some hint and potentially share the actual training prototext?    Note:  At beginning I realized that I used a newer caffe than this repo and therefore I need to add crop_param in  ,   and  . After adding those, with your pre-trained model, the demo script output expected images. Therefore, I think the training pipeline and test scripts are ok.    Attached Code, 57th layer of the Caffe -> Json on your pre-trained model:         Where as, here's my 57th layer:     "
None
I trained my own data using TVG_new_traintest.prototxt. I changed the number of output and   the name of the deconvolution layer as well as  initialize the weights of all layers.But the output is still all black.Then I visualized  feature of  every layer.I found that the feature become all black after the first deconvolution layer.Do  you know where the problem lie???
"I use the crfrnn net to make a segmentation of an organ,so the number of output is 2.Then I  modify the example/segmentationcrfrnn/solve.protoxt.Then I run the .sh file.But  it check failed at softmax_loss_layer and said  number of labels don't match number of predictions.Please help to figure out what's the problem??    Could you tell me how to make 'label' lmdb file?The name of label's size should be 121500500,how to make it?The number of my output is 2,so i need to make it be 12500500,but i don't know how to make it.Thank you very much"
None
"math_functions.cu:123] Check failed: status == CUBLAS_STATUS_SUCCESS (11 vs. 0)  CUBLAS_STATUS_MAPPING_ERROR    could you provide the script for generating lmdb dataset, I encountered this problem after a sleep, and I don't know why!"
"I just uncommented the Makefile.config line No.5 so I can use cuDNN, but when running make , this error comes:  NVCC src/caffe/layers/sigmoid_layer.cu  NVCC src/caffe/layers/dropout_layer.cu  NVCC src/caffe/layers/multi_stage_meanfield.cu  NVCC src/caffe/layers/deconv_layer.cu  NVCC src/caffe/layers/hdf5_data_layer.cu  NVCC src/caffe/layers/relu_layer.cu  NVCC src/caffe/layers/lrn_layer.cu  NVCC src/caffe/layers/power_layer.cu  NVCC src/caffe/layers/hdf5_output_layer.cu  NVCC src/caffe/layers/bnll_layer.cu  NVCC src/caffe/layers/cudnn_conv_layer.cu  src/caffe/layers/cudnn_conv_layer.cu(67): error: argument of type ""cudnnAddMode_t"" is incompatible with parameter of type ""const void *""            detected during instantiation of ""void caffe::CuDNNConvolutionLayer ::Forward_gpu(const std::vector  *, std::allocator  *>> &, const std::vector  *, std::allocator  *>> &) [with Dtype=float]""   (145): here    src/caffe/layers/cudnn_conv_layer.cu(67): error: argument of type ""const void *"" is incompatible with parameter of type ""cudnnTensorDescriptor_t""            detected during instantiation of ""void caffe::CuDNNConvolutionLayer ::Forward_gpu(const std::vector  *, std::allocator  *>> &, const std::vector  *, std::allocator  *>> &) [with Dtype=float]""   (145): here    src/caffe/layers/cudnn_conv_layer.cu(67): error: argument of type ""const void *"" is incompatible with parameter of type ""cudnnTensorDescriptor_t""            detected during instantiation of ""void caffe::CuDNNConvolutionLayer ::Forward_gpu(const std::vector  *, std::allocator  *>> &, const std::vector  *, std::allocator  *>> &) [with Dtype=float]""   (145): here    src/caffe/layers/cudnn_conv_layer.cu(67): error: too many arguments in function call            detected during instantiation of ""void caffe::CuDNNConvolutionLayer ::Forward_gpu(const std::vector  *, std::allocator  *>> &, const std::vector  *, std::allocator  *>> &) [with Dtype=float]""   (145): here    src/caffe/layers/cudnn_conv_layer.cu(113): error: argument of type ""const void *"" is incompatible with parameter of type ""cudnnConvolutionBwdFilterAlgo_t""            detected during instantiation of ""void caffe::CuDNNConvolutionLayer ::Backward_gpu(const std::vector  *, std::allocator  *>> &, const std::vector > &, const std::vector  *, std::allocator  *>> &) [with Dtype=float]""   (145): here    src/caffe/layers/cudnn_conv_layer.cu(113): error: argument of type ""float *"" is incompatible with parameter of type ""size_t""            detected during instantiation of ""void caffe::CuDNNConvolutionLayer ::Backward_gpu(const std::vector  *, std::allocator  *>> &, const std::vector > &, const std::vector  *, std::allocator  *>> &) [with Dtype=float]""   (145): here    src/caffe/layers/cudnn_conv_layer.cu(113): error: too few arguments in function call            detected during instantiation of ""void caffe::CuDNNConvolutionLayer ::Backward_gpu(const std::vector  *, std::allocator  *>> &, const std::vector > &, const std::vector  *, std::allocator  *>> &) [with Dtype=float]""   (145): here    src/caffe/layers/cudnn_conv_layer.cu(128): error: argument of type ""const void *"" is incompatible with parameter of type ""cudnnConvolutionBwdDataAlgo_t""            detected during instantiation of ""void caffe::CuDNNConvolutionLayer ::Backward_gpu(const std::vector  *, std::allocator  *>> &, const std::vector > &, const std::vector  *, std::allocator  *>> &) [with Dtype=float]""   (145): here    src/caffe/layers/cudnn_conv_layer.cu(128): error: argument of type ""float *"" is incompatible with parameter of type ""size_t""            detected during instantiation of ""void caffe::CuDNNConvolutionLayer ::Backward_gpu(const std::vector  *, std::allocator  *>> &, const std::vector > &, const std::vector  *, std::allocator  *>> &) [with Dtype=float]""   (145): here    src/caffe/layers/cudnn_conv_layer.cu(128): error: too few arguments in function call            detected during instantiation of ""void caffe::CuDNNConvolutionLayer ::Backward_gpu(const std::vector  *, std::allocator  *>> &, const std::vector > &, const std::vector  *, std::allocator  *>> &) [with Dtype=float]""   (145): here    src/caffe/layers/cudnn_conv_layer.cu(67): error: argument of type ""cudnnAddMode_t"" is incompatible with parameter of type ""const void *""            detected during instantiation of ""void caffe::CuDNNConvolutionLayer ::Forward_gpu(const std::vector  *, std::allocator  *>> &, const std::vector  *, std::allocator  *>> &) [with Dtype=double]""   (145): here    src/caffe/layers/cudnn_conv_layer.cu(67): error: argument of type ""const void *"" is incompatible with parameter of type ""cudnnTensorDescriptor_t""            detected during instantiation of ""void caffe::CuDNNConvolutionLayer ::Forward_gpu(const std::vector  *, std::allocator  *>> &, const std::vector  *, std::allocator  *>> &) [with Dtype=double]""   (145): here    src/caffe/layers/cudnn_conv_layer.cu(67): error: argument of type ""const void *"" is incompatible with parameter of type ""cudnnTensorDescriptor_t""            detected during instantiation of ""void caffe::CuDNNConvolutionLayer ::Forward_gpu(const std::vector  *, std::allocator  *>> &, const std::vector  *, std::allocator  *>> &) [with Dtype=double]""   (145): here    src/caffe/layers/cudnn_conv_layer.cu(67): error: too many arguments in function call            detected during instantiation of ""void caffe::CuDNNConvolutionLayer ::Forward_gpu(const std::vector  *, std::allocator  *>> &, const std::vector  *, std::allocator  *>> &) [with Dtype=double]""   (145): here    src/caffe/layers/cudnn_conv_layer.cu(113): error: argument of type ""const void *"" is incompatible with parameter of type ""cudnnConvolutionBwdFilterAlgo_t""            detected during instantiation of ""void caffe::CuDNNConvolutionLayer ::Backward_gpu(const std::vector  *, std::allocator  *>> &, const std::vector > &, const std::vector  *, std::allocator  *>> &) [with Dtype=double]""   (145): here    src/caffe/layers/cudnn_conv_layer.cu(113): error: argument of type ""double *"" is incompatible with parameter of type ""size_t""            detected during instantiation of ""void caffe::CuDNNConvolutionLayer ::Backward_gpu(const std::vector  *, std::allocator  *>> &, const std::vector > &, const std::vector  *, std::allocator  *>> &) [with Dtype=double]""   (145): here    src/caffe/layers/cudnn_conv_layer.cu(113): error: too few arguments in function call            detected during instantiation of ""void caffe::CuDNNConvolutionLayer ::Backward_gpu(const std::vector  *, std::allocator  *>> &, const std::vector > &, const std::vector  *, std::allocator  *>> &) [with Dtype=double]""   (145): here    src/caffe/layers/cudnn_conv_layer.cu(128): error: argument of type ""const void *"" is incompatible with parameter of type ""cudnnConvolutionBwdDataAlgo_t""            detected during instantiation of ""void caffe::CuDNNConvolutionLayer ::Backward_gpu(const std::vector  *, std::allocator  *>> &, const std::vector > &, const std::vector  *, std::allocator  *>> &) [with Dtype=double]""   (145): here    src/caffe/layers/cudnn_conv_layer.cu(128): error: argument of type ""double *"" is incompatible with parameter of type ""size_t""            detected during instantiation of ""void caffe::CuDNNConvolutionLayer ::Backward_gpu(const std::vector  *, std::allocator  *>> &, const std::vector > &, const std::vector  *, std::allocator  *>> &) [with Dtype=double]""   (145): here    src/caffe/layers/cudnn_conv_layer.cu(128): error: too few arguments in function call            detected during instantiation of ""void caffe::CuDNNConvolutionLayer ::Backward_gpu(const std::vector  *, std::allocator  *>> &, const std::vector > &, const std::vector  *, std::allocator  *>> &) [with Dtype=double]""   (145): here    20 errors detected in the compilation of ""/tmp/tmpxft_000057bc_00000000-13_cudnn_conv_layer.compute_35.cpp1.ii"".  make: *** [.build_release/cuda/src/caffe/layers/cudnn_conv_layer.o] Error 1  "
"when i train my own model, i found all layers do not need backward computation,but there is nothing error information,why?"
"Ubuntu in VirtualBox, I followed the official tutorial:       works great with the official caffe repo. checkout your repo to test crf as rnn. Compiling caffe seems good (no compilation error), but when I want to test it with    `python python/classify.py --print_results examples/images/cat.jpg foo`    I get this error message:  `[libprotobuf ERROR google/protobuf/text_format.cc:274] Error parsing text-format caffe.NetParameter: 6:15: Message type ""caffe.LayerParameter"" has no field named ""input_param"".  WARNING: Logging before InitGoogleLogging() is written to STDERR  F0306 08:43:12.641402 25140 upgrade_proto.cpp:932] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: python/../models/bvlc_reference_caffenet/deploy.prototxt  *** Check failure stack trace: ***  Aborted (core dumped)  `    whereas, with the official caffe repo I get the successful result:  `[('tabby', '0.27933'), ('tiger cat', '0.21915'), ('Egyptian cat', '0.16064'), ('lynx', '0.12844'), ('kit fox', '0.05155')]`    I was really thrilled about this, it is a shame it is not cleanly built (not entirely your fault, caffe is already a nightmare to run on different configurations).    Thanks in advance for your help."
"**Hi,  when I compile the caffe, there is a warning for the function 'multi_stage_meanfield.cpp' as below:**    ""src/caffe/layers/multi_stage_meanfield.cpp: In instantiation of â€˜void caffe::MultiStageMeanfieldLayer ::LayerSetUp(const std::vector *>&, const std::vector *>&) [with Dtype = float]â€™:  src/caffe/layers/multi_stage_meanfield.cpp:258:1:   required from here  src/caffe/layers/multi_stage_meanfield.cpp:72:83: warning: format â€˜%lfâ€™ expects argument of type â€˜double*â€™, but argument 3 has type â€˜float*â€™ [-Wformat=]         fscanf(pFile, ""%lf"", &this->blobs_[0]->mutable_cpu_data()[i * channels_ + i]);                                                                                     ^  src/caffe/layers/multi_stage_meanfield.cpp:79:83: warning: format â€˜%lfâ€™ expects argument of type â€˜double*â€™, but argument 3 has type â€˜float*â€™ [-Wformat=]         fscanf(pFile, ""%lf"", &this->blobs_[1]->mutable_cpu_data()[i * channels_ + i]);                                                                                     ^  src/caffe/layers/multi_stage_meanfield.cpp: In member function â€˜void caffe::MultiStageMeanfieldLayer ::LayerSetUp(const std::vector *>&, const std::vector *>&) [with Dtype = float]â€™:  src/caffe/layers/multi_stage_meanfield.cpp:72:7: warning: ignoring return value of â€˜int fscanf(FILE*, const char*, ...)â€™, declared with attribute warn_unused_result [-Wunused-result]         fscanf(pFile, ""%lf"", &this->blobs_[0]->mutable_cpu_data()[i * channels_ + i]);         ^  src/caffe/layers/multi_stage_meanfield.cpp:79:7: warning: ignoring return value of â€˜int fscanf(FILE*, const char*, ...)â€™, declared with attribute warn_unused_result [-Wunused-result]         fscanf(pFile, ""%lf"", &this->blobs_[1]->mutable_cpu_data()[i * channels_ + i]);         ^  src/caffe/layers/multi_stage_meanfield.cpp: In member function â€˜void caffe::MultiStageMeanfieldLayer ::LayerSetUp(const std::vector *>&, const std::vector *>&) [with Dtype = double]â€™:  src/caffe/layers/multi_stage_meanfield.cpp:72:7: warning: ignoring return value of â€˜int fscanf(FILE*, const char*, ...)â€™, declared with attribute warn_unused_result [-Wunused-result]         fscanf(pFile, ""%lf"", &this->blobs_[0]->mutable_cpu_data()[i * channels_ + i]);         ^  src/caffe/layers/multi_stage_meanfield.cpp:79:7: warning: ignoring return value of â€˜int fscanf(FILE*, const char*, ...)â€™, declared with attribute warn_unused_result [-Wunused-result]         fscanf(pFile, ""%lf"", &this->blobs_[1]->mutable_cpu_data()[i * channels_ + i]);""         ^    **And when I run the demo 'crfasrnn_demo.py', there is an error like below:**    Input file is "" input.jpg  Output file is "" output.png  GPU_DEVICE is "" 0  WARNING: Logging before InitGoogleLogging() is written to STDERR  W0221 16:34:13.797194 11631 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface  W0221 16:34:13.797235 11631 _caffe.cpp:123] Use this instead (with the named ""weights"" parameter):  W0221 16:34:13.797240 11631 _caffe.cpp:125] Net('TVG_CRFRNN_new_deploy.prototxt', 1, weights='TVG_CRFRNN_COCO_VOC.caffemodel')  [libprotobuf ERROR google/protobuf/text_format.cc:245] Error parsing text-format caffe.NetParameter: 619:31: Message type ""caffe.LayerParameter"" has no field named ""multi_stage_meanfield_param"".  F0221 16:34:13.799583 11631 upgrade_proto.cpp:79] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: TVG_CRFRNN_new_deploy.prototxt  *** Check failure stack trace: ***  Aborted (core dumped)    Anyone know the reason? Thanks very much.  "
"I started working with the initial version of CRFasRNN which didn't have GPU. And I just switched to the version with GPU. I compared the 2 (old version, and GPU version) by loading a pretrained model and evaluating it on my dataset. For the same model, the mean IOU with GPU is lower than with the old CPU version by 0.3. Is there any reason why that would be the case?"
"Hi,    I'm using the GPU version (  to train on 2 classes with examples/segmentationcrfasrnn    Getting weird results before out of memory.  (Using K80 on AWS p2 instance).    When the train starts:  <img width=""772"" alt=""screen shot 2017-02-15 at 3 13 42 pm"" src=""     And then I get many (MANY) test iterations (5499999 iterations) before out of memory:  <img width=""711"" alt=""screen shot 2017-02-15 at 3 17 42 pm"" src=""     <img width=""872"" alt=""screen shot 2017-02-15 at 3 31 55 pm"" src=""     The LMDB was made with the script from    on my own images and labels (only 74 images, 64 train and 7 for test).    Please advice.    Thanks     "
"Is CuDNN 5 supported? I installed vanilla caffe w/out any problem, but crf-rnn caffe keeps telling me     ./include/caffe/util/cudnn.hpp: In function â€˜void caffe::cudnn::createPoolingDesc(cudnnPoolingStruct**, caffe::PoolingParameter_PoolMethod, cudnnPoolingMode_t*, int, int, int, int, int, int)â€™:  ./include/caffe/util/cudnn.hpp:124:41: error: too few arguments to function â€˜cudnnStatus_t cudnnSetPooling2dDescriptor(cudnnPoolingDescriptor_t, cudnnPoolingMode_t, cudnnNanPropagation_t, int, int, int, int, int, int)â€™           pad_h, pad_w, stride_h, stride_w));    "
"How did you train CRFasRNN, specifially what did the last layer look like?     What I mean is that it couldn't be just a vector of 21 neurons, because you have to segment the object in addition to classification. Did you use a mask size the input image with 1's in the location of the object and 0 elsewhere? I see that the output is of shape (batch_size, num_class, width, height), but not sure how you used this for training. "
"Hello @bittnt   I train crfasrnn with GPU using the latest caffe as  . I exactly follow the instruction of  .But when I run python solve.py 2>&1 | tee train.log,there have a problem which is as follows:  F0107 19:16:32.735841 25776 multi_stage_meanfield.cu:68] You initialised your network on CPU, please initialize it on GPU.  *** Check failure stack trace: ***  1  The multi_stage_meanfield.cu is as follows.       Is there something wrong about it?    But I have set the model to gpu which is in Makefile.config, solve.py and solve.prototxt. I really don't know what's wrong about it. Can you give me some adviceï¼ŸThank you very much.  "
"I use the TVG_CRFRNN.sh which at caffe/examples/segmentationcrfasrnn/ to train crfasrnn.But  the training iterate 0 and then test infinitely as follows. What should I do?Thanks!    I0102 21:24:32.579792  5267 upgrade_proto.cpp:620] Attempting to upgrade input file specified using deprecated V1LayerParameter: TVG_CRFRNN_COCO_VOC.caffemodel  I0102 21:24:33.525709  5267 upgrade_proto.cpp:628] Successfully upgraded file specified using deprecated V1LayerParameter  [libprotobuf WARNING google/protobuf/io/coded_stream.cc:537] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.  [libprotobuf WARNING google/protobuf/io/coded_stream.cc:78] The total number of bytes read was 537968303  I0102 21:24:34.088544  5267 upgrade_proto.cpp:620] Attempting to upgrade input file specified using deprecated V1LayerParameter: TVG_CRFRNN_COCO_VOC.caffemodel  I0102 21:24:34.936173  5267 upgrade_proto.cpp:628] Successfully upgraded file specified using deprecated V1LayerParameter  I0102 21:24:35.053526  5267 caffe.cpp:211] Starting Optimization  I0102 21:24:35.053546  5267 solver.cpp:293] Solving CRFRNN-VOC  I0102 21:24:35.053550  5267 solver.cpp:294] Learning Rate Policy: fixed  I0102 21:24:35.057199  5267 solver.cpp:346] Iteration 0, Testing net (#0)  I0102 21:35:39.176551  5267 solver.cpp:414]     Test net output #0: label = 0.32  I0102 21:35:39.180371  5267 solver.cpp:414]     Test net output #1: label = 0.32  I0102 21:35:39.180466  5267 solver.cpp:414]     Test net output #2: label = 0.32  I0102 21:35:39.180511  5267 solver.cpp:414]     Test net output #3: label = 0.32  I0102 21:35:39.180546  5267 solver.cpp:414]     Test net output #4: label = 0.32  I0102 21:35:39.180575  5267 solver.cpp:414]     Test net output #5: label = 0.32  I0102 21:35:39.180606  5267 solver.cpp:414]     Test net output #6: label = 0.32  I0102 21:35:39.180635  5267 solver.cpp:414]     Test net output #7: label = 0.32  I0102 21:35:39.180665  5267 solver.cpp:414]     Test net output #8: label = 0.32  I0102 21:35:39.180696  5267 solver.cpp:414]     Test net output #9: label = 0.32  "
"Hi all  I am a new beginner of CRF-RNN.  I had trained a FCN8s model using a public material dataset MINC. Then I planned to train a CRF-RNN model based on this FCN8s model. A net architecture similar as TVG_CRFRNN_COCO_VOC_TRAIN_3_CLASSES.prototxt is employed for training. Below is my solver.prototxt:    net: ""CRFRNN_train_test.prototxt""  test_iter: 1798  test_interval: 999999999  display: 100    average_loss: 1798    lr_policy: ""fixed""  base_lr: 1e-13    momentum: 0.99  iter_size: 1  max_iter: 50000  weight_decay: 0.0005    snapshot: 1000  snapshot_prefix: ""./models/""  test_initialization: false  solver_mode: GPU    In my training process, the train net output loss and iteration loss are always same. I feel very confused about this because I had set average_loss: 1798 in solver.prototxt. And the network also seems not converge. Below is the output of training:    I1204 21:45:52.125540 23363 solver.cpp:242] Iteration 0, loss = 4215.64  I1204 21:45:52.125576 23363 solver.cpp:258]     Train net output #0: loss = 4215.64 (* 1 = 4215.64 loss)  I1204 21:45:52.125587 23363 solver.cpp:571] Iteration 0, lr = 1e-14  I1204 21:50:57.908535 23363 solver.cpp:242] Iteration 100, loss = 3425.36  I1204 21:50:57.908574 23363 solver.cpp:258]     Train net output #0: loss = 3425.36 (* 1 = 3425.36 loss)  I1204 21:50:57.908587 23363 solver.cpp:571] Iteration 100, lr = 1e-14  I1204 21:55:49.283299 23363 solver.cpp:242] Iteration 200, loss = 3252.26  I1204 21:55:49.283339 23363 solver.cpp:258]     Train net output #0: loss = 3252.26 (* 1 = 3252.26 loss)  I1204 21:55:49.283347 23363 solver.cpp:571] Iteration 200, lr = 1e-14  I1204 22:00:58.413457 23363 solver.cpp:242] Iteration 300, loss = 842.061  I1204 22:00:58.413498 23363 solver.cpp:258]     Train net output #0: loss = 842.061 (* 1 = 842.061 loss)  I1204 22:00:58.413512 23363 solver.cpp:571] Iteration 300, lr = 1e-14  I1204 22:05:52.236604 23363 solver.cpp:242] Iteration 400, loss = 5119.25  I1204 22:05:52.236644 23363 solver.cpp:258]     Train net output #0: loss = 5119.25 (* 1 = 5119.25 loss)  I1204 22:05:52.236654 23363 solver.cpp:571] Iteration 400, lr = 1e-14  I1204 22:10:45.105113 23363 solver.cpp:242] Iteration 500, loss = 6157.21  I1204 22:10:45.105150 23363 solver.cpp:258]     Train net output #0: loss = 6157.21 (* 1 = 6157.21 loss)  I1204 22:10:45.105160 23363 solver.cpp:571] Iteration 500, lr = 1e-14  I1204 22:15:35.751629 23363 solver.cpp:242] Iteration 600, loss = 3508  I1204 22:15:35.751672 23363 solver.cpp:258]     Train net output #0: loss = 3508 (* 1 = 3508 loss)  I1204 22:15:35.751685 23363 solver.cpp:571] Iteration 600, lr = 1e-14  I1204 22:20:26.621906 23363 solver.cpp:242] Iteration 700, loss = 10248.3  I1204 22:20:26.621947 23363 solver.cpp:258]     Train net output #0: loss = 10248.3 (* 1 = 10248.3 loss)  I1204 22:20:26.621960 23363 solver.cpp:571] Iteration 700, lr = 1e-14  I1204 22:25:15.441035 23363 solver.cpp:242] Iteration 800, loss = 88.0998  I1204 22:25:15.441072 23363 solver.cpp:258]     Train net output #0: loss = 88.0998 (* 1 = 88.0998 loss)  I1204 22:25:15.441082 23363 solver.cpp:571] Iteration 800, lr = 1e-14  I1204 22:29:59.302548 23363 solver.cpp:242] Iteration 900, loss = 2721.73  I1204 22:29:59.302583 23363 solver.cpp:258]     Train net output #0: loss = 2721.73 (* 1 = 2721.73 loss)  I1204 22:29:59.302593 23363 solver.cpp:571] Iteration 900, lr = 1e-14       Any ideas about my problem? Many thanks for your help!    Best"
I use the latest caffe with cudnn from   to run crfasrnn_demo.py for test.Using the image:  !   And change spatial_filter_weight and bilateral_filter_weight as spatial_filter_weights_str and bilateral_filter_weights_str in TVG_CRFRNN_new_deploy.prototxt  The output is:  !   If I use old caffe without cudnn from   output is:  !   Why use the latest caffe but effect worse?      
"I'm running the test script to segment objects in an 800x800 image. I use Tesla K40 graphics card as a GPU processor. Why do I keep getting message that the process is Killed? Sometimes it works though, but it takes a long time (10-20 seconds) to process it. Here's the output for CUDA deviceQuery, I also install CuDNN 3.0.8 (the only one that worked). Any suggestions?       Device 0: ""Tesla K40m""    CUDA Driver Version / Runtime Version          7.5 / 7.5    CUDA Capability Major/Minor version number:    3.5    Total amount of global memory:                 11520 MBytes (12079136768 bytes)    (15) Multiprocessors, (192) CUDA Cores/MP:     2880 CUDA Cores    GPU Max Clock rate:                            745 MHz (0.75 GHz)    Memory Clock rate:                             3004 Mhz    Memory Bus Width:                              384-bit    L2 Cache Size:                                 1572864 bytes    Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)    Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers    Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers    Total amount of constant memory:               65536 bytes    Total amount of shared memory per block:       49152 bytes    Total number of registers available per block: 65536    Warp size:                                     32    Maximum number of threads per multiprocessor:  2048    Maximum number of threads per block:           1024    Max dimension size of a thread block (x,y,z): (1024, 1024, 64)    Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)    Maximum memory pitch:                          2147483647 bytes    Texture alignment:                             512 bytes    Concurrent copy and kernel execution:          Yes with 2 copy engine(s)    Run time limit on kernels:                     No    Integrated GPU sharing Host Memory:            No    Support host page-locked memory mapping:       Yes    Alignment requirement for Surfaces:            Yes    Device has ECC support:                        Enabled    Device supports Unified Addressing (UVA):      Yes    Device PCI Domain ID / Bus ID / location ID:   0 / 4 / 0    Compute Mode:            Device 1: ""NVS 315""    CUDA Driver Version / Runtime Version          7.5 / 7.5    CUDA Capability Major/Minor version number:    2.1    Total amount of global memory:                 1023 MBytes (1072889856 bytes)    ( 1) Multiprocessors, ( 48) CUDA Cores/MP:     48 CUDA Cores    GPU Max Clock rate:                            1046 MHz (1.05 GHz)    Memory Clock rate:                             875 Mhz    Memory Bus Width:                              64-bit    L2 Cache Size:                                 65536 bytes    Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65535), 3D=(2048, 2048, 2048)    Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers    Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers    Total amount of constant memory:               65536 bytes    Total amount of shared memory per block:       49152 bytes    Total number of registers available per block: 32768    Warp size:                                     32    Maximum number of threads per multiprocessor:  1536    Maximum number of threads per block:           1024    Max dimension size of a thread block (x,y,z): (1024, 1024, 64)    Max dimension size of a grid size    (x,y,z): (65535, 65535, 65535)    Maximum memory pitch:                          2147483647 bytes    Texture alignment:                             512 bytes    Concurrent copy and kernel execution:          Yes with 1 copy engine(s)    Run time limit on kernels:                     No    Integrated GPU sharing Host Memory:            No    Support host page-locked memory mapping:       Yes    Alignment requirement for Surfaces:            Yes    Device has ECC support:                        Disabled    Device supports Unified Addressing (UVA):      Yes    Device PCI Domain ID / Bus ID / location ID:   0 / 3 / 0    Compute Mode:          > Peer access from Tesla K40m (GPU0) -> NVS 315 (GPU1) : No  > Peer access from NVS 315 (GPU1) -> Tesla K40m (GPU0) : No  deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 7.5, CUDA Runtime Version = 7.5, NumDevs = 2, Device0 = Tesla K40m, Device1 = NVS 315"
"Hi,    Technically I can compile with Cuda8.0-Support (USE_CUDNN := 1), but the GPU seems not to be used.    details:  I got a Pascal-Chipset based cuda-8.0 GPU (GTX 1080) and can compile     (e.g. ab4118d)  Then I use this caffe version for training the examples given in     or       (I made the adaption necessary to the prototxts)    The training seems to be using CPU only and takes forever. There are a few MB used on the GPU, but no computational load done.    Also the MultiStageMeanfieldLayerTest/x.TestGradient (make runtest for bittnt/caffe) is not using computing capabilities of the GPUs, other caffe-tests do.    Am I completely overlooking something? How can I make crfasrnn use a cuda-8.0 GPU?  many thanks in advance!   "
"Hello. I'm training the model with fcn8s model (   I want to check whether my training goes well, so could you show your training graph? especially, my initial loss value is about 3000~4000 but in some iteration the loss is 200000 and like that. and I checked inference1 layer filter's weight,  when I see as imagesc in matlab, your model shows diagonal but my initialized model shows diagonal but moved one pixel vertically for all the diagonal pixels. is there anything I have to consider to train? "
Has anyone tried to run the matlab version on windows?   I came across the error below:  > crfrnn_demo > Undefined function or variable 'caffe_'. >  >  Error in caffe.reset_all (line 5) > caffe_('reset'); >  > Error in crfrnn_demo (line 28) > caffe.reset_all();  appreciate if anyone can share how to solve the problem above. 
hello! Thank you very much for sharing the source code for us to learnï¼ I can train my data by using your source code on CPU.But it is too slow. So I want to train my data on GPU. I referred to   there are a few problems that I can't solve.  So I would like to ask you if there is a demo that i can train data on the GPU ? Thank you very much! 
"Hi, thanks for your great work that embeds CRF to DL. I tried your code, actually, the GPU version (  and finetuned it on VOC12 train set. I changed the layer ""inference1"" to ""inference1-ft"" because I want to relearn the CRF layer. The FCN layers are all fixed.  However, during the training, the mIoU performance on the reduced validation set you give here   gets worse and worse.   Initialization: 69.259%, even worse than the input of CRF layer (Q0 69.85%) after 1000 iterations: 69.075% after 30000 iterations: 67.569%  I also tried to unfix the FCN layers but get the similar results.  I am wondering what is wrong here, can you give me some suggestions? Thank you. "
"I want to use the crfasrnn model to train on my data, but there is no network for training. Should I just add a data layer to provide label, and add a softmax layer to get loss ? If not, please give some  suggestions about it. Thank you. "
"I've processed the same image through the demo (  and the installed trained classifier (TVG_CRFRNN_COCO_VOC.prototxt), and I get very different results (the ones on he website are way better). An suggestion why this is the case?  "
"When attempting to run the python demo script I am getting the following error, I am having trouble identifying what is left out, I am thinking it could be an EVN variable. Any suggestion on what could be causing this?   File ""/var/www/crfasrnn_original/python-scripts/demo_test.py"",   line 29, in import caffe File ""../caffe-crfrnn/python/caffe/**init**.py"",  line 1, in from .pycaffe import Net, SGDSolver File ""../caffe-crfrnn/python/caffe/pycaffe.py"",   line 10, in from ._caffe import Net, SGDSolver ImportError: libcudart.so.6.5: cannot open shared object file: No such file or directory "
"Hii, when I run the crfasrnn_demo.py, I tackle the following error:  File ""crfasrnn-master/python-scripts/crfasrnn_demo.py"", line 22, in       import loggingnet ImportError: No module named loggingnet  I am new with caffe but I tried the default caffe demo (come with the caffe environment) and it worked correctly, and all the requirements have been installed (I use Anaconda 2 for Python).  Maybe this error occur due to wrong environment settings? My Environment Variables are: 1. PYTHONPATH=/home/limor/anaconda2/lib/python2.7:/home/limor/caffe-master/python: 2. LD_LIBRARY_PATH=/usr/local/cuda-7.5/lib64: 3. PATH=/usr/local/cuda-7.5/bin:/home/limor/anaconda2/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games 4. CAFFE_ROOT=/home/limor/caffe-master "
"Hi,    I have used the model to do organ segmentation. so, in this case, I have just two class of background and organ. I have re-traind the model again but all my predictions are in black.   since the number of classes is changed to two,  the name for the deconv layer is changed not to load from the pre-trained model. but all my predictions are black !!   I read that its the problem with weight initialization of deconv layer, but I don't know what the proper initialization?   can anybody post the training prototext which has the proper initialization? Or let me know how should it be done?  I have attached my training prototext here:          "
"Running on Virtualbox, CPU only enabled, 4GB RAM, when running the python test script:            "
"@sadeepj @bittnt   Thanks for sharing this awesome piece of work.    I have run the caffe version on a **Tesla K80 GPU**. And the timing is ~4second. Now I am trying to use **cuDNN** to see what speedup it does to the code.  I have tried - cudnn-8.0-linux-x64-**v5.1**, **v6.0**, v**7**.  And all of them are taking me to the same **error** -      `:~/cudnn_version/crfasrnn/caffe$ make  PROTOC src/caffe/proto/caffe.proto  CXX .build_release/src/caffe/proto/caffe.pb.cc  CXX src/caffe/util/im2col.cpp  In file included from ./include/caffe/util/device_alternate.hpp:40:0,                   from ./include/caffe/common.hpp:19,                   from ./include/caffe/util/math_functions.hpp:9,                   from src/caffe/util/im2col.cpp:6:  ./include/caffe/util/cudnn.hpp: In function â€˜const char* cudnnGetErrorString(cudnnStatus_t)â€™:  ./include/caffe/util/cudnn.hpp:18:10: warning: enumeration value â€˜CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSINGâ€™ not handled in switch [-Wswitch]     switch (status) {            ^  ./include/caffe/util/cudnn.hpp:18:10: warning: enumeration value â€˜CUDNN_STATUS_RUNTIME_IN_PROGRESSâ€™ not handled in switch [-Wswitch]  ./include/caffe/util/cudnn.hpp:18:10: warning: enumeration value â€˜CUDNN_STATUS_RUNTIME_FP_OVERFLOWâ€™ not handled in switch [-Wswitch]  ./include/caffe/util/cudnn.hpp: In function â€˜void caffe::cudnn::setConvolutionDesc(cudnnConvolutionStruct**, cudnnTensorDescriptor_t, cudnnFilterDescriptor_t, int, int, int, int)â€™:  ./include/caffe/util/cudnn.hpp:105:70:` **error: too few arguments to function â€˜cudnnStatus_t cudnnSetConvolution2dDescriptor(cudnnConvolutionDescriptor_t, int, int, int, int, int, int, cudnnConvolutionMode_t, cudnnDataType_t)â€™**  `       pad_h, pad_w, stride_h, stride_w, 1, 1, CUDNN_CROSS_CORRELATION));                                                                        ^  ./include/caffe/util/cudnn.hpp:12:28: note: in definition of macro â€˜CUDNN_CHECKâ€™       cudnnStatus_t status = condition; \                              ^  In file included from ./include/caffe/util/cudnn.hpp:5:0,                   from ./include/caffe/util/device_alternate.hpp:40,                   from ./include/caffe/common.hpp:19,                   from ./include/caffe/util/math_functions.hpp:9,                   from src/caffe/util/im2col.cpp:6:  /usr/local/cuda-8.0/include/cudnn.h:537:27: note: declared here   cudnnStatus_t CUDNNWINAPI cudnnSetConvolution2dDescriptor( cudnnConvolutionDescriptor_t convDesc,                             ^  In file included from ./include/caffe/util/device_alternate.hpp:40:0,                   from ./include/caffe/common.hpp:19,                   from ./include/caffe/util/math_functions.hpp:9,                   from src/caffe/util/im2col.cpp:6:  ./include/caffe/util/cudnn.hpp: In function â€˜void caffe::cudnn::createPoolingDesc(cudnnPoolingStruct**, caffe::PoolingParameter_PoolMethod, cudnnPoolingMode_t*, int, int, int, int, int, int)â€™:  ./include/caffe/util/cudnn.hpp:124:41: error: too few arguments to function â€˜cudnnStatus_t cudnnSetPooling2dDescriptor(cudnnPoolingDescriptor_t, cudnnPoolingMode_t, cudnnNanPropagation_t, int, int, int, int, int, int)â€™           pad_h, pad_w, stride_h, stride_w));                                           ^  ./include/caffe/util/cudnn.hpp:12:28: note: in definition of macro â€˜CUDNN_CHECKâ€™       cudnnStatus_t status = condition; \                              ^  In file included from ./include/caffe/util/cudnn.hpp:5:0,                   from ./include/caffe/util/device_alternate.hpp:40,                   from ./include/caffe/common.hpp:19,                   from ./include/caffe/util/math_functions.hpp:9,                   from src/caffe/util/im2col.cpp:6:  /usr/local/cuda-8.0/include/cudnn.h:1031:27: note: declared here   cudnnStatus_t CUDNNWINAPI cudnnSetPooling2dDescriptor(                             ^  Makefile:533: recipe for target '.build_release/src/caffe/util/im2col.o' failed  make: *** [.build_release/src/caffe/util/im2col.o] Error 1  `    **Any suggestion on what I could be missing here ?**"
Does crf-rnn include the pixelwise InfogainLoss function recently added to caffe?      
"My data is 16x4096x8x8 (fc7 layer from FCN), labels 16x1x250x250 (the actual image size is 250x250x3), the crop layer (the one that outputs Q0 and coarse afte splitting) is size 16x2x250x250 (2 is the number of classes) aligned to the size of the label. When I use crf-rnn caffe implementation, I get an    `Check failed: this->net_->bottom_vecs()[layer_ind].size() > 0 (0 vs. 0) Cannot align apparently disconnected blobs.`    error. As I understand, this comes from the fact that label layer isn't used before crop.     Not sure how to fix this error. It doesn't come up in the vanilla caffe implementation.  "
"I'm not sure if this is a Caffe bug or my implementation, but doing the net surgery I found out that     `net.blobs['upscore'].diff  `  is all zeros, and the same is true for all deconv layers. Other layers are fine. At the same time     `net.layers['upscore'].blobs[0].diff   `  are mostly non-zero and seem to be fine.     Any explanation? "
"The error is     `Message type ""caffe.LayerParameter"" has no field named ""crop_param""  `      Is there any way to implement crop_param in the crf-rnn's version of caffe? The layer in my network is       When I remove the crop_param detail, the deploy seems to just simply freeze, i.e it tries to          and nothing happens/freezes. Any suggestions? "
"Hello, I'm new in docker, I would like to know how to run a container with this caffe version ( with crf as rnn layers,etc). The main caffe docker version doesn't support your model. Anyone has an idea how to do it ?    Thank you for any suggestion or indication,    Cordially,    Mateo "
"Hello, I would like to ask 2 questions concerning the input layer :    CanI use a Python Layer( the Python layer provided in the FCN project) instead of the LMDB layer for the Input Images and Labels ? It doesn't give troubles when loading the .caffemodel?  (obviously, modifying the net in the .prototxt file).    Can I use image of arbitrary size (max 512x512 pixels) , without padding ?    Thanks,    Mateo"
"Hi,  I was trying to train the crfasrnn model using the TVG_CRFRNN_new_traintest.prototxt and TVG_CRFRNN_new_solver.prototxt and i got an error(Note i have provided the lmdb files for both the labels and images for testing and training). I am using the pascal VOC 2012 dataset and for converting to lmdb i used data2lmdb.py from  . This was the error:    ../../build/tools/caffe: symbol lookup error: /home/swamy/crfasrnn/caffe/.build_release/tools/../lib/libcaffe.so: undefined symbol: _ZTIN5boost6python15instance_holderE    Please provide any insight you have. Thank you    "
"**1. where is weighting filter outputs step in source code?**  !   I notice that the paper said there is weighting filter outputs step , but the source code meanfield_iteration.cpp in line 155 directly omit this step?    **2. the paper and the code written about 1x1 conv don't match?**  in the paper said that the compatibility transform there is 1x1 conv to sovle this weight linear combination:  !   meanfield_iteration.cpp in line 155~162 directly omit this 1x1 conv?             2. in meanfield_iteration.cpp line : 175:  `vector  eltwise_propagate_down(2, true);`  why is 2 set here, because sum_top_vec_ only have 2 element in it?    **3. in meanfield_iteration.cpp line : 182:**  `caffe_cpu_gemm (CblasNoTrans, CblasTrans, channels_, channels_,                            num_pixels_, (Dtype) 1., pairwise_.cpu_diff() + pairwise_.offset(n),                            message_passing_.cpu_data() + message_passing_.offset(n), (Dtype) 1.,  this->blobs_[2]->mutable_cpu_diff());`  why transpose message_passing_ matrix here?    **4. again in line 190**  `caffe_cpu_gemm (CblasTrans, CblasNoTrans, channels_, num_pixels_,                             channels_, (Dtype) 1., this->blobs_[2]->cpu_data(),                            pairwise_.cpu_diff() + pairwise_.offset(n), (Dtype) 0.,                            message_passing_.mutable_cpu_diff() + message_passing_.offset(n));`  why transpose this->blobs_[2] hereï¼Ÿ    "
"I notice that   this->blobs_[0].reset(new Blob (1, 1, channels_, channels_));  this->blobs_[1].reset(new Blob (1, 1, channels_, channels_));  because   blobs_[0] - spatial kernel weights, blobs_[1] - bilateral kernel weights, blobs_[2] - compatability matrix  so **why the blobs[0] is batch_number=1, channel=1, channels_ X channels_ matrix??**"
"So i was doing a net surgery from VGG19.caffemodel and vgg19_deploy.prototxt with the given prototxt file in the repo, but the below error is showing everytime.   F0514 16:58:40.370375 29026 eltwise_layer.cpp:34] Check failed: bottom[0]->shape() == bottom[i]->shape() bottom[0]: 1 21 349 500 (3664500), bottom[1]: 1 21 334 500 (3507000)  *** Check failure stack trace: ***  Aborted (core dumped)"
"Hi  Instead of downloading TVG_CRFRNN_COCO_VOC.caffemodel, i would like to generate it by retraining.  1. Do I need to write any new code?  2. Also please provide me the datasets used &   3. training steps in detail (python command line details preferred)    Regards  Gopi. J"
"It's done. I've done it. Anyone who happens upon this please learn from my mistakes and read all the issues before posting your own. And just use git's own tools instead of ever merging a codebase by hand.     // old  I've merged the necessary .cpp, .hpp and .cu files into my own caffe, that has other layers depending on cuDNN v5.1.     My errors are different than those in #97 . I did see that issue before, but the rest of the layers support cuDNN 5.1, and besides I can `make all` without problems.     I solved one redefinition error (below) by adding ""_cpu"" to the offending lines in modified_permutohedral.cpp and everything compiles. But I get a neverending stream of errors in `make runtest`... all to do with CUDA.    #77 was one issue, but fixing it has exposed some more problems. One concerns a call to `cudaMemset()`. Another is in `cudaFree()`:    The third common error is `cudaSuccess (77 vs. 0)` inside of `hash_table.hpp` (line 45)    Other tests, e.g. CuDNNConvolutionLayerTest run and pass no problem. It's always in the cuda-related files copied from this repo.           I use cuda 8 and cuDNN 5.1 on ubuntu 16.04, but the errors are the same on OS X with              // Original post  Hi, I am trying to transfer the layers from this repo into another modified caffe version.   I run `make all` and it completes with some warnings. But I get the attached error when running `make test`.    By examining       and      I see that init and compute are defined in both. I am new to C++ and am not sure what to modify to make the test build.    My OS is OS X El Capitan, and I'm building with cuDNN v5 support ON. Any advice appreciated :)  (Will also try on Ubuntu 16.04 machine at a later date)   "
"Pasted the error below.  gopi@gp:~/crfasrnn/python-scripts$ python crfasrnn_demo.py  Traceback (most recent call last):    File ""crfasrnn_demo.py"", line 31, in        import caffe    File ""../caffe/python/caffe/__init__.py"", line 1, in        from .pycaffe import Net, SGDSolver    File ""../caffe/python/caffe/pycaffe.py"", line 13, in        from ._caffe import Net, SGDSolver  ImportError: /home/gopi/crfasrnn/python-scripts/../caffe/python/caffe/../../build/lib/libcaffe.so: undefined symbol: _ZN2cv8imencodeERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKNS_11_InputArrayERSt6vectorIhSaIhEERKSB_IiSaIiEE    My system details:  Ubuntu 16.04 with nvidia gtx 1070 GPU.    FYI, also tried Uncommenting the line:  WITH_PYTHON_LAYER := 1 as per   & did make again, but no improvement"
"when I make  the caffe,the  multi_stage_meanfield.cpp is warnned  src/caffe/layers/multi_stage_meanfield.cpp:72:7: warnning â€˜%lfâ€™ expects argument of type â€˜double*â€™, but argument 3 has type â€˜float*â€™ [-Wformat]  "
"Dear @bittnt ,    We initialize the layer `MultiStageMeanfield ` by filling the diagonal with specified values (*e.g.*, 3 and 5 for spatial and bilateral filters, respectively and -1 for compatilibity transform matrix). And ideally, only these weights (*i.e.* the values on diagonal) will be updated during the training. However, I just find the values off diagonal are also somehow modified (*e.g.* 0.0 -> 0.1).    I was wondering if this is reasonable. What does it mean if so? Should we clean the off-diagonal values before each forward pass?    Sorry I might be missing something.    Thanks!  Kun"
"I'm tring to download the trained caffe model from the link :    but i can't get access to it, could someone share the model with me ? "
"Hi, I am searching for the meanfield implementation in Caffe layers but I can't seem to find it.   Is it in this repository? "
"Dear @bittnt and @bernard24 ,    I was wondering if it is necessary to set `decay_mult: 0` in `MultiStageMeanfield` layer? What I found is that with `decay_mult: 1`, the learned kernel (*i.e.* spatial and bilateral filters) weights are almost 0, even when they were initialized with 3.0 and 5.0, respectively. In your  , you are using the default value (*i.e.*, `decay_mult: 1`).    Would you please share your experience and thoughts on this?    Thanks!  Kun    (p.s. In solver.prototxt, `weight_decay: 0.0005` is used.)"
How many parameters are there in CRF-RNN layer? I'm getting a total of 134489759 for FCN-8s and 134491103 for CRF-RNN. Does it mean the CRF-RNN layer has just 1344 parameters? 
"How did you convert Pascal data set to lmdb format? Specifically, labels for each image. I made something like an array with `datum.data` set to four values, (x,y,h,w) of he bounding box. "
I run the crfasrnn. the input and output as following:  !     !     !     !           why the semantic image has a right and botton shift?  Many thanks.    
"Not sure if anybody is looking a this, just encountered an error when trying to make `src/caffe/layers/multi_stage_meanfield.cpp`, namely         Now I need  this because the last layer in the   `TVG_CRFRNN_new_deploy.prototxt` uses it. In newer versions of caffe, this layer cannot be found in `/src/caffe/layers`...    Thanks"
"I modified the TVG_CRFRNN_new_deploy file according to   .  i input the example image, but i got a different result as following  input  !      output.  !       Could you please tell me how to fix out ?  Many thanks"
"Thanks for this very useful paper. I'm looking to use another segmentation architecture (which requires a completely different build of caffe), but then append the crf-rnn to the last layer of the other network before the softmax loss. This will require combining both caffe versions, and since the other version has a lot of changes and custom layers from the main caffe branch, I was thinking it might be easier to merge multi_stage_meanfield.cpp (and whatever other files are needed) to their build.    I'm a bit unclear as to which files I need to copy over that you have changed in caffe to enable crf-rnn to work, or if that's even how to incorporate your caffe contributions, so any direction on this would be extremely helpful. Thanks!"
"Hi , i can not install the crfasrnn into my computer , i just want to segment the object by using project website    i got a semantic image by using the  website  project. and  i  want to segment them.  For example, i get a semantic image which has Chair, Bicycle ,and TV by the  website  project  how can i change the chair's color to red(255,0,0), the bicycle's color to green(0,255,0), the TV's color to blue(0,0,255).  Then i can segment them easily.    Thanks...  "
"Dear @bittnt ,    I met strange issues when setting the batch_size greater than 3. Sometimes it complains `illegal memory access was encountered`, while sometimes the training loss becomes `nan` immediately.    I was wondering if you have implemented a new multi_stage_meanfield_layer that supports arbitrary batch_size?    Thanks!  Kun"
"Dear @bittnt ,    Usually the output score map should have the same spatial size as the input image during mean field approximation. But in my case the score map is up/down-scaled from the original spatial size. In multi_stage_meanfield_layer.cpp, will it handle such situation by sampling values from bilateral kernel and spatial kernel accordingly?     Thanks!  Kun"
"Dear @bittnt ,    Did you `make runtest` on latest Caffe with CPU/GPU  ? I failed the test_multi_stage_meanfield_layer, while the old version seems okay.    Thanks.  Kun"
"When running a 1280x720 image through crf-rnn I get the annoying message    `syncedmem.cpp:58] Check failed: error == cudaSuccess (2 vs. 0)  out of memory`    This line does some check: `CUDA_CHECK(cudaMalloc(&gpu_ptr_, size_));`    I have 64Gb RAM and run Tesla K40 with 12 Gb memory, while `Memory required for data: 3703865856`    Do I need to change some parameters (`size_`) somewhere to allow use of more memory? "
"Hi,    I was wondering how the bandwidth parameters theta were chosen?    Thanks,    Lyne"
"I get this message when the input is 1000x600 pixels. Sepcifically,    I0208 12:00:25.485682  1955 net.cpp:299] Memory required for data: 2580988744  [libprotobuf WARNING google/protobuf/io/coded_stream.cc:505] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.    So I decided in increase the limit of the allocated memory, just to realize that 5 files in my system have this path:  google/protobuf/io/coded_stream.h.    So I'm baffled: which one to change:     /home/ICTDOMAIN/453615/Downloads/opencv-3.2.0/build/modules/dnn/3rdparty/protobuf/sources/protobuf-3.1.0/src/google/protobuf/io/coded_stream.h    /usr/include/google/protobuf/io/coded_stream.h    /usr/local/lib/python2.7/dist-packages/external/protobuf/src/google/protobuf/io/coded_stream.h    /usr/local/lib/python2.7/dist-packages/tensorflow/include/google/protobuf/io/coded_stream.h    /usr/local/lib/python3.4/dist-packages/external/protobuf/src/google/protobuf/io/coded_stream.h    /usr/local/lib/python3.4/dist-packages/tensorflow/include/google/protobuf/io/coded_stream.h  "
"I am using this version of crfasrnn   .   Error parsing text-format caffe.NetParameter: 758:31: Message type ""caffe.LayerParameter"" has no field named ""multi_stage_meanfield_param"".    My solve.py file:  ---------------------  import caffe  import surgery, score    import numpy as np  import os    import setproctitle  setproctitle.setproctitle(os.path.basename(os.getcwd()))  weights = 'voc-experiment8CRF/snapshot/vocSBDall_iter_400000.caffemodel'  caffe.set_device(0)  caffe.set_mode_gpu()    solver = caffe.SGDSolver('voc-experiment8CRF/solver.prototxt')  solver.net.copy_from(weights)      interp_layers = [k for k in solver.net.params.keys() if 'up' in k]   surgery.interp(solver.net, interp_layers)        val = np.loadtxt('../seg12val.txt', dtype=str)    for _ in range(1):      solver.step(1)       score.seg_tests(solver, False, val, layer='score')    MutliStageMeanField Layer in train.prototxt  -----------------------------------------------------------    layer {    name: ""inference1""#if you set name ""inference1"", code will load parameters from caffemodel.    type: ""MultiStageMeanfield""    bottom: ""unary""    bottom: ""Q0""    bottom: ""data""    top: ""pred""    param {      lr_mult: 10000#learning rate for W_G    }    param {    lr_mult: 10000#learning rate for W_B    }    param {    lr_mult: 1000 #learning rate for compatiblity transform matrix   }    multi_stage_meanfield_param {     num_iterations: 10 #Number of iterations for CRF-RNN     compatibility_mode: POTTS#Initialize the compatilibity transform matrix with a matrix whose diagonal is -1.     threshold: 2     theta_alpha: 160     theta_beta: 3     theta_gamma: 3     spatial_filter_weight: 3     bilateral_filter_weight: 5    }  }"
Is there any model trained on MSCOCO(with 80 classes) available?
I have input images with the size of 640*480.  How can it output 640*480 results?
"@bittnt When I train the model followed by martinkersner/train-CRF-RNN, I  meet the problem as follows:  xupt@xupt:~/WL_project/train-CRF-RNN$ python solve.py 2>&1 | tee train.log  WARNING: Logging before InitGoogleLogging() is written to STDERR  I0102 10:04:24.963816 21120 solver.cpp:48] Initializing solver from parameters:   test_iter: 261  test_interval: 1333  base_lr: 1e-13  display: 50  max_iter: 100000  lr_policy: ""fixed""  momentum: 0.99  weight_decay: 0.0005  snapshot: 1000  snapshot_prefix: ""/home/xupt/WL_project/train-CRF-RNN/train""  solver_mode: GPU  net: ""TVG_CRFRNN_COCO_VOC_TRAIN_3_CLASSES.prototxt""  test_initialization: false  I0102 10:04:24.963953 21120 solver.cpp:91] Creating training net from net file: TVG_CRFRNN_COCO_VOC_TRAIN_3_CLASSES.prototxt  [libprotobuf ERROR google/protobuf/text_format.cc:274] Error parsing text-format caffe.NetParameter: 743:31: Message type ""caffe.LayerParameter"" has no field named ""multi_stage_meanfield_param"".  F0102 10:04:24.964298 21120 upgrade_proto.cpp:88] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: TVG_CRFRNN_COCO_VOC_TRAIN_3_CLASSES.prototxt  *** Check failure stack trace: ***  What's wrong with this?Thank you very much!  "
"`/usr/bin/python2.7 /home/simon/Desktop/crfasrnn-master/python-scripts/crfasrnn_demo.py  Input file is "" input.jpg  Output file is "" output.png  GPU_DEVICE is "" 0  WARNING: Logging before InitGoogleLogging() is written to STDERR  W1207 23:30:13.552337 16866 _caffe.cpp:122] DEPRECATION WARNING - deprecated use of Python interface  W1207 23:30:13.552371 16866 _caffe.cpp:123] Use this instead (with the named ""weights"" parameter):  W1207 23:30:13.552376 16866 _caffe.cpp:125] Net('TVG_CRFRNN_new_deploy.prototxt', 1, weights='TVG_CRFRNN_COCO_VOC.caffemodel')  [libprotobuf ERROR google/protobuf/text_format.cc:274] Error parsing text-format caffe.NetParameter: 626:25: Message type ""caffe.MultiStageMeanfieldParameter"" has no field named ""spatial_filter_weight"".  F1207 23:30:13.554874 16866 upgrade_proto.cpp:88] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: TVG_CRFRNN_new_deploy.prototxt  *** Check failure stack trace: ***`    my caffe version is    who can help me, thanks!"
"Hi, I compiled everything correctly and didn't have any error before this, but as I run the python demo, I received the following error. I didn't find anything like it on Google also. May I get some suggestion? Thanks!  <img width=""986"" alt=""2016-11-26 9 00 01"" src=""   "
"I resize the input image to (500,500), and for some reason it takes ~9s to process it on Tesla K40 with 12Gb memory. Any suggestions on how to speed it up? I know it shouldn't be more that 1-2 seconds for this size."
"The demo is not working for me  gives runtime error on the following line of code  net = tvg_matcaffe_init(use_gpu, gpu_id, model_def_file, model_file);  **the logs on terminal are**    [libprotobuf ERROR google/protobuf/text_format.cc:245] Error parsing text-format caffe.NetParameter: 626:25: Message type ""caffe.MultiStageMeanfieldParameter"" has no field named ""spatial_filter_weight"".  WARNING: Logging before InitGoogleLogging() is written to STDERR  F1123 12:23:38.740509  6596 upgrade_proto.cpp:88] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: TVG_CRFRNN_new_deploy.prototxt      **Stack Trace**      ------------------------------------------------------------------------                abort() detected at Tue Nov 22 15:36:43 2016  ------------------------------------------------------------------------    Configuration:    Crash Decoding     : Disabled    Current Visual     : 0x21 (class 4, depth 24)    Default Encoding   : UTF-8    GNU C Library      : 2.19 stable    MATLAB Architecture: glnxa64    MATLAB Root        : /usr/local/MATLAB/R2014    MATLAB Version     : 8.3.0.532 (R2014a)    Operating System   : Linux 3.19.0-51-generic #58~14.04.1-Ubuntu SMP Fri Feb 26 22:02:58 UTC 2016 x86_64    Processor ID       : x86 Family 6 Model 63 Stepping 2, GenuineIntel    Virtual Machine    : Java 1.7.0_11-b21 with Oracle Corporation Java HotSpot(TM) 64-Bit Server VM mixed mode    Window System      : The X.Org Foundation (11701000), display :0    Fault Count: 1      Abnormal termination:  abort()    Register State (from fault):    RAX = 0000000000000000  RBX = 00007f7604262620    RCX = ffffffffffffffff  RDX = 0000000000000006    RSP = 00007f76e0fc5df8  RBP = 00007f76e0fc5f30    RSI = 00000000000052e0  RDI = 00000000000052b9       R8 = 000000000000ff08   R9 = ffffffffffff1150    R10 = 0000000000000008  R11 = 0000000000000202    R12 = 0000000000000001  R13 = 00007f76e0fc6170    R14 = 00007f76e0fc6b90  R15 = 0000000000000001      RIP = 00007f76f339acc9  EFL = 0000000000000202       CS = 0033   FS = 0000   GS = 0000    Stack Trace (from fault):  [  0] 0x00007f76f339acc9                    /lib/x86_64-linux-gnu/libc.so.6+00224457 gsignal+00000057  [  1] 0x00007f76f339e0d8                    /lib/x86_64-linux-gnu/libc.so.6+00237784 abort+00000328  [  2] 0x00007f760403cd81             /usr/lib/x86_64-linux-gnu/libglog.so.0+00068993 _ZN6google22InstallFailureFunctionEPFvvE+00000000  [  3] 0x00007f760403cdaa             /usr/lib/x86_64-linux-gnu/libglog.so.0+00069034 _ZN6google10LogMessage10SendToSinkEv+00000000  [  4] 0x00007f760403cce4             /usr/lib/x86_64-linux-gnu/libglog.so.0+00068836 _ZN6google10LogMessage9SendToLogEv+00001224  [  5] 0x00007f760403c6e6             /usr/lib/x86_64-linux-gnu/libglog.so.0+00067302 _ZN6google10LogMessage5FlushEv+00000414  [  6] 0x00007f760403f687             /usr/lib/x86_64-linux-gnu/libglog.so.0+00079495 _ZN6google15LogMessageFatalD1Ev+00000025  [  7] 0x00007f76045786de /home/tabazim/Downloads/caffe-master/matlab/+caffe/private/caffe_.mexa64+00747230  [  8] 0x00007f760466e693 /home/tabazim/Downloads/caffe-master/matlab/+caffe/private/caffe_.mexa64+01754771  [  9] 0x00007f76044fd8d3 /home/tabazim/Downloads/caffe-master/matlab/+caffe/private/caffe_.mexa64+00243923  [ 10] 0x00007f76044fe397 /home/tabazim/Downloads/caffe-master/matlab/+caffe/private/caffe_.mexa64+00246679 mexFunction+00000154  [ 11] 0x00007f76eb2c572a      /usr/local/MATLAB/R2014/bin/glnxa64/libmex.so+00120618 mexRunMexFile+00000090  [ 12] 0x00007f76eb2c1a94      /usr/local/MATLAB/R2014/bin/glnxa64/libmex.so+00105108  [ 13] 0x00007f76eb2c2fb4      /usr/local/MATLAB/R2014/bin/glnxa64/libmex.so+00110516  [ 14] 0x00007f76ea6bcad9 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_dispatcher.so+00670425 _ZN8Mfh_file11dispatch_fhEiPP11mxArray_tagiS2_+00000697  [ 15] 0x00007f76e99592b4 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+04461236  [ 16] 0x00007f76e995abc9 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+04467657  [ 17] 0x00007f76e995b3fc /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+04469756  [ 18] 0x00007f76e97d56e3 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02873059  [ 19] 0x00007f76e97e509e /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02936990  [ 20] 0x00007f76e97e5183 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02937219  [ 21] 0x00007f76e991b172 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+04206962  [ 22] 0x00007f76e9751df8 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02334200  [ 23] 0x00007f76e97af30b /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02716427  [ 24] 0x00007f76ea6bcad9 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_dispatcher.so+00670425 _ZN8Mfh_file11dispatch_fhEiPP11mxArray_tagiS2_+00000697  [ 25] 0x00007f76ea2f46e8   /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcos.so+01566440  [ 26] 0x00007f76ea29e482   /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcos.so+01213570  [ 27] 0x00007f76ea2a0465   /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcos.so+01221733  [ 28] 0x00007f76ea2a2e50   /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcos.so+01232464  [ 29] 0x00007f76ea2a073d   /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcos.so+01222461  [ 30] 0x00007f76ea2f7126   /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcos.so+01577254  [ 31] 0x00007f76ea36355b   /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcos.so+02020699  [ 32] 0x00007f76ea66b874 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_dispatcher.so+00338036 _ZN13Mfh_MATLAB_fn11dispatch_fhEiPP11mxArray_tagiS2_+00000244  [ 33] 0x00007f76ea363031   /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcos.so+02019377  [ 34] 0x00007f76e979320e /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02601486  [ 35] 0x00007f76e974e1d0 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02318800  [ 36] 0x00007f76e97501ea /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02327018  [ 37] 0x00007f76e9753167 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02339175  [ 38] 0x00007f76e975126f /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02331247  [ 39] 0x00007f76e9751ec4 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02334404  [ 40] 0x00007f76e97af30b /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02716427  [ 41] 0x00007f76ea6bcad9 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_dispatcher.so+00670425 _ZN8Mfh_file11dispatch_fhEiPP11mxArray_tagiS2_+00000697  [ 42] 0x00007f76ea2f46e8   /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcos.so+01566440  [ 43] 0x00007f76ea2f4995   /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcos.so+01567125  [ 44] 0x00007f76ea29e236   /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcos.so+01212982  [ 45] 0x00007f76ea29e98c   /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcos.so+01214860  [ 46] 0x00007f76ea2a038e   /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcos.so+01221518  [ 47] 0x00007f76ea2a2e50   /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcos.so+01232464  [ 48] 0x00007f76ea2a0a9c   /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcos.so+01223324  [ 49] 0x00007f76ea2a0be9   /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcos.so+01223657  [ 50] 0x00007f76ea2a0dcf   /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcos.so+01224143  [ 51] 0x00007f76ea2a0ff1   /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcos.so+01224689  [ 52] 0x00007f76ea2fc717   /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcos.so+01599255  [ 53] 0x00007f76ea3631f8   /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcos.so+02019832  [ 54] 0x00007f76ea66b874 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_dispatcher.so+00338036 _ZN13Mfh_MATLAB_fn11dispatch_fhEiPP11mxArray_tagiS2_+00000244  [ 55] 0x00007f76ea363031   /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcos.so+02019377  [ 56] 0x00007f76e979320e /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02601486  [ 57] 0x00007f76e974e1d0 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02318800  [ 58] 0x00007f76e97501ea /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02327018  [ 59] 0x00007f76e9753167 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02339175  [ 60] 0x00007f76e975126f /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02331247  [ 61] 0x00007f76e9751ec4 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02334404  [ 62] 0x00007f76e97af30b /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02716427  [ 63] 0x00007f76ea6bcad9 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_dispatcher.so+00670425 _ZN8Mfh_file11dispatch_fhEiPP11mxArray_tagiS2_+00000697  [ 64] 0x00007f76e979320e /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02601486  [ 65] 0x00007f76e97341b0 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02212272  [ 66] 0x00007f76e974f25f /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02323039  [ 67] 0x00007f76e9753167 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02339175  [ 68] 0x00007f76e975126f /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02331247  [ 69] 0x00007f76e9751ec4 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02334404  [ 70] 0x00007f76e97af30b /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02716427  [ 71] 0x00007f76ea6bcc5f /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_dispatcher.so+00670815 _ZN8Mfh_file11dispatch_fhEiPP11mxArray_tagiS2_+00001087  [ 72] 0x00007f76e979320e /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02601486  [ 73] 0x00007f76e97341b0 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02212272  [ 74] 0x00007f76e974f25f /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02323039  [ 75] 0x00007f76e9753167 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02339175  [ 76] 0x00007f76e975126f /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02331247  [ 77] 0x00007f76e9751ec4 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02334404  [ 78] 0x00007f76e97af30b /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02716427  [ 79] 0x00007f76ea6bcc5f /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_dispatcher.so+00670815 _ZN8Mfh_file11dispatch_fhEiPP11mxArray_tagiS2_+00001087  [ 80] 0x00007f76e9782135 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02531637  [ 81] 0x00007f76e97490d9 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02298073  [ 82] 0x00007f76e9745dc7 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02284999  [ 83] 0x00007f76e9746193 /usr/local/MATLAB/R2014/bin/glnxa64/libmwm_interpreter.so+02285971  [ 84] 0x00007f76eb4efafc /usr/local/MATLAB/R2014/bin/glnxa64/libmwbridge.so+00142076  [ 85] 0x00007f76eb4f0791 /usr/local/MATLAB/R2014/bin/glnxa64/libmwbridge.so+00145297 _Z8mnParserv+00000721  [ 86] 0x00007f76f449f92f    /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcr.so+00489775 _ZN11mcrInstance30mnParser_on_interpreter_threadEv+00000031  [ 87] 0x00007f76f4480b6d    /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcr.so+00363373  [ 88] 0x00007f76f4480be9    /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcr.so+00363497  [ 89] 0x00007f76e8e7bd46    /usr/local/MATLAB/R2014/bin/glnxa64/libmwuix.so+00343366  [ 90] 0x00007f76e8e5e382    /usr/local/MATLAB/R2014/bin/glnxa64/libmwuix.so+00222082  [ 91] 0x00007f76f4bf550f /usr/local/MATLAB/R2014/bin/glnxa64/libmwservices.so+02323727  [ 92] 0x00007f76f4bf567c /usr/local/MATLAB/R2014/bin/glnxa64/libmwservices.so+02324092  [ 93] 0x00007f76f4bf157f /usr/local/MATLAB/R2014/bin/glnxa64/libmwservices.so+02307455  [ 94] 0x00007f76f4bf69b5 /usr/local/MATLAB/R2014/bin/glnxa64/libmwservices.so+02329013  [ 95] 0x00007f76f4bf6de7 /usr/local/MATLAB/R2014/bin/glnxa64/libmwservices.so+02330087  [ 96] 0x00007f76f4bf74c0 /usr/local/MATLAB/R2014/bin/glnxa64/libmwservices.so+02331840 _Z25svWS_ProcessPendingEventsiib+00000080  [ 97] 0x00007f76f4481098    /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcr.so+00364696  [ 98] 0x00007f76f44813bf    /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcr.so+00365503  [ 99] 0x00007f76f447c28f    /usr/local/MATLAB/R2014/bin/glnxa64/libmwmcr.so+00344719  [100] 0x00007f76f3731182              /lib/x86_64-linux-gnu/libpthread.so.0+00033154  [101] 0x00007f76f345e47d                    /lib/x86_64-linux-gnu/libc.so.6+01025149 clone+00000109      This error was detected while a MEX-file was running. If the MEX-file  is not an official MathWorks function, please examine its source code  for errors. Please consult the External Interfaces Guide for information  on debugging MEX-files.  "
None
"Hello, I am using TitanX with Cuda 8.0, openCV 3.1, Ubuntu 14.04. I try to build your caffe with `make`, but I got error as bellow:         I opened the Makefile and it has the bellow line, but the error is still happen. I installed openCV 3.1 from source file. Thank you in advance     "
"I'm not sure how this is possible. I'm loading a 1280x720 image, here are the specs:    ------------------------------------------------------+                         | NVIDIA-SMI 352.63     Driver Version: 352.63         |                         |-------------------------------+----------------------+----------------------+  | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |  | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |  |===============================+======================+======================|  |   0  NVS 315             Off  | 0000:03:00.0     N/A |                  N/A |  | 30%   44C    P0    N/A /  N/A |      3MiB /  1023MiB |     N/A      Default |  +-------------------------------+----------------------+----------------------+  |   1  Tesla K40m          Off  | 0000:04:00.0     Off |                    0 |  | N/A   57C    P0    69W / 235W |     55MiB / 11519MiB |     98%      Default |  +-------------------------------+----------------------+----------------------+                                                                                   +-----------------------------------------------------------------------------+  | Processes:                                                       GPU Memory |  |  GPU       PID  Type  Process name                               Usage      |  |=============================================================================|  |    0                  Not Supported                                         |  +-----------------------------------------------------------------------------+    CUDA devuceQuery:    Device 0: ""Tesla K40m""    CUDA Driver Version / Runtime Version          7.5 / 7.5    CUDA Capability Major/Minor version number:    3.5    Total amount of global memory:                 11520 MBytes (12079136768 bytes)    (15) Multiprocessors, (192) CUDA Cores/MP:     2880 CUDA Cores    GPU Max Clock rate:                            745 MHz (0.75 GHz)    Memory Clock rate:                             3004 Mhz    Memory Bus Width:                              384-bit    L2 Cache Size:                                 1572864 bytes    Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)    Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers    Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers    Total amount of constant memory:               65536 bytes    Total amount of shared memory per block:       49152 bytes    Total number of registers available per block: 65536    Warp size:                                     32    Maximum number of threads per multiprocessor:  2048    Maximum number of threads per block:           1024    Max dimension size of a thread block (x,y,z): (1024, 1024, 64)    Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)    Maximum memory pitch:                          2147483647 bytes    Texture alignment:                             512 bytes    Concurrent copy and kernel execution:          Yes with 2 copy engine(s)    Run time limit on kernels:                     No    Integrated GPU sharing Host Memory:            No    Support host page-locked memory mapping:       Yes    Alignment requirement for Surfaces:            Yes    Device has ECC support:                        Enabled    Device supports Unified Addressing (UVA):      Yes    Device PCI Domain ID / Bus ID / location ID:   0 / 4 / 0    Compute Mode:      The required memory is 3.7Gb. Why do I get this error message? Also, when I resized the image to 500x500, it still takes ~7 s/image. Any suggestions on how to make it faster?     "
"Hey, I don't know whether you have corrected this mistake or not but there is an error in the caffe build of your modified caffe revision.    caffe/src/caffe/util/modified_permutohedral.cpp in this file the line number 329:22 has the passed parameter ""feature"" and the parameter passed is ""features"". So this is creating trouble. I changed it to ""features"" and it is working fine."
"Hello,    In the test phase, the mean vector [103.939, 116.779, 123.68] is subtracted from the image RGB values prior to the segmentation. Why in the train phase no mean values are computed or subtracted - at least not in `TVG_CRFRNN_new_traintest.prototxt`?    Thank you,  Anastasia"
"Hello. In the paper section 5.2,  you said  ""The complete architecture of our network, including the FCN-8s part can be found in the supplementary material."" But I can't find the supplementary material. Could you let me know the link for that? thank you. "
"Hi,  Is it possible to release the fcn8s model that is trained on COCO and produces 69.85% on the reduced pascal voc 2012 validation set ? I would like to use that model to train the RNN layer. "
"Hi, is there a GPU/CUDA implementation for the multi_stage_meanfield layer? I can't find a multi_stage_meanfield.cu file or associated forward_gpu() methods in the caffe code. "
"I executed your demo code in CPU mode, but I got errors.  I modified your source code as below and it works now.  72: if gpudevice >=0:  > if gpudevice > 0: also  104: palette = getpallete(256) > pallete = getpallete(256) "
We are installing a new server at the uni. Are there any restrictions/limitations/shortcomings of crf-rnn on any of these:     
"hi .  i train my data on gpu Iteration over 100 thousand times. But the result is very bad. Even a very result is just a full black image.  i can't solve this problem,i need you help , thank you! "
Does crf-rnn support cuda8? This is the only cuda compatible with Ubuntu 16.04. What version of CuDNN is supported on this platform?  
"Hi,  When I run: python crfasrnn_demo.py The problem is as:  ---  Input file is "" input.jpg Output file is "" output.png GPU_DEVICE is "" 0 WARNING: Logging before InitGoogleLogging() is written to STDERR F0829 22:22:03.615937 22095 common.cpp:66] Cannot use GPU in CPU-only Caffe: check mode. **\* Check failure stack trace: *** Aborted (core dumped)  ---  I have uncommented the CPU_ONLY = 1 in the 'Makefile.config'           Changed CMakeLists.txt: caffe_option(CPU_ONLY  ""Build Caffe without CUDA support"" ON)           changed solver_mode: CPU in .prototxt files  ---  However, the problem is still there. Would anyone like to help please? Thanks very much "
"The command ""sh download_trained_model.sh"" says ""failed: No route to host."" And I tried to get the model by directly clicking the link in python-scripts/README.md,but I get Error 404 which says ""The requested URL /~szheng/Res_CRFRNN/TVG_CRFRNN_COCO_VOC.caffemodel was not found on this server. ""  Is the url not used anymore? And where can I get the model? "
"I'm running another unrelated library built on top of caffe (holistic edge detector), any suggestions on how I can import caffe from 2 different sources with the same name (caffe)? "
"Everything seemed to work fine, but the output image for input.jpg from crfasrnn_demo.py is all white. "
"I trained FCN32 (16,8)   with new crop layer where we can specify axis and offset.  For the code here, it seems old crop layer is used. Does the old crop layer code do center cropping?  Or is there a branch of crfasrnn that works with new crop layer.  Thanks "
"Hi, Thanks for your great work on DL.I use the latest FCN code plus the CRF layer, and get the following error at run timeï¼š  I0722 23:26:08.296314 27585 net.cpp:274] Network initialization done. I0722 23:26:08.296448 27585 solver.cpp:60] Solver scaffolding done. libprotobuf WARNING google/protobuf/io/coded_stream.cc:478] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h. I0722 23:26:50.454507 27585 net.cpp:752] Ignoring source layer loss I0722 23:26:50.658272 27585 eltwise_layer.cpp:39] 1  4 16 14 ï¼›1  4 16 14 I0722 23:26:50.659008 27585 eltwise_layer.cpp:39] 1  4 34 30 ï¼›1  4 34 30 I0722 23:26:50.681608 27585 eltwise_layer.cpp:39] 1  4 209 180 ï¼›1  4 209 180 I0722 23:26:50.681612 27585 eltwise_layer.cpp:39] 1  4 209 180 ï¼›1  4 209 180 I0722 23:26:50.681617 27585 eltwise_layer.cpp:39] 1  4 209 180 ï¼›1  4 209 180 I0722 23:26:50.694358 27585 eltwise_layer.cpp:33] 1  4 209 180 ï¼›1  4 209 180 I0722 23:26:50.694373 27585 eltwise_layer.cpp:39] 1  4 209 180 ï¼›1  4 209 180 I0722 23:26:50.809535 27585 solver.cpp:228] Iteration 0, loss = 1994.94 I0722 23:26:50.809564 27585 solver.cpp:244]     Train net output #0: loss = 1994.94 (\* 1 = 1994.94 loss) I0722 23:26:50.809572 27585 sgd_solver.cpp:106] Iteration 0, lr = 1e-10 I0722 23:26:51.226410 27585 eltwise_layer.cpp:39]1  4 22 28 ï¼›1  4 22 28 I0722 23:26:51.228551 27585 eltwise_layer.cpp:39] 1  4 46 58 ï¼›1  4 46 58 I0722 23:26:51.265045 27585 eltwise_layer.cpp:39] 1  4 209 180 (bottom[1]->shape()) ï¼› 1 4 314 407 (bottom[0]->shape()) **F0722 23:26:51.265060 27585 eltwise_layer.cpp:40] ## Check failed: bottom[i]->shape() == bottom[0]->shape()**  I hope you can answer the following questionsï¼š Is it because I didn't resize the image to 500ï¼Š500ï¼Ÿ How to train arbitary image,just like FCN?  Thanks "
"I am wondering how much RAM memory is required to segment 500x500 image?   I am running the crfasrnn on EC2 instance CPU and seems like it is consuming around 6GB of RAM memory. Is that correct number or I am doing something wrong.  If that is true then is there a way to reduce the memory usage.  Also I found one of the Issues which is related to GPU on AWS EC2. Graphics EC2 instance on AWS has maximum of 4GB of video memory, and crfasrnn requires around 8GB of RAM memory. Is there any progress in reducing the memory usage.  Thank you in advance. "
"This is really silly, but I couldn't solve it myself.   Everything works fine when I run the demo script from the directory created when installing crfasrnn, but when I do it from elsewhere (with no other errors reported), I get this:  Check failed: pFile The file 'spatial.par' is not found. Please create it with initial spatial kernel weights  I followed the suggestion in   but it didn't work. I looked at  multi_stage_meanfield.cpp, but I'm not too sure what to do. Once again, all is fine when I run it from crfasrnn-master/python-scripts/  Any suggesitons?  "
"Hi,   I am pretty new to caffe in general and I try to train your CRFasRNN network using the PASCAL VOC data of 2012 in order to get an insight how this works and how the data needs to be prepared. Later I would like to use only some parts of your work and combine it with other layers.   However, I used the solver and train prototxt file in the examples folder under segmentationcrfasrnn but the accuracy is pretty jumpy and low. The training has reached the 19,800th iteration and the output for the test accuracy at each 1000th iteration is the following:  Test net output #0: accuracy = 0.0778186 Test net output #0: accuracy = 0.028924 Test net output #0: accuracy = 0.0566633 Test net output #0: accuracy = 0.0345619 Test net output #0: accuracy = 0.0419963 Test net output #0: accuracy = 0.0290244 Test net output #0: accuracy = 0.0300839 Test net output #0: accuracy = 0.0460825 Test net output #0: accuracy = 0.0236691 Test net output #0: accuracy = 0.0320318 Test net output #0: accuracy = 0.0268505 Test net output #0: accuracy = 0.029318 Test net output #0: accuracy = 0.042781 Test net output #0: accuracy = 0.0517754 Test net output #0: accuracy = 0.0374599 Test net output #0: accuracy = 0.0413573 Test net output #0: accuracy = 0.0380903 Test net output #0: accuracy = 0.0351139 Test net output #0: accuracy = 0.0313849 Test net output #0: accuracy = 0.0325979  As far as I understand the train accuracy and loss are not conclusive here because of batch size = 1.   Could anybody help me figure out how to fix this?   Thanks a lot! Annika  ---  The steps I did so far:  1. Prepare my data 2. download PASCAL data from   3. convert label images into gray scale images having values between 0 and 21 as pixel values (depending on label) and 22 if its the background 4. due to gpu size (max 4GB) I had to scale and pad the images to a size of 150x150 5. Convert into lmdb database 6. one db for the images (Channel: 3) and one for the labels (Channel: 1) 7. test size: 10; train size: 500; 8. Train the network  9. took the prototxt of segmentationcrfasrnn example and changed the lmdb database and ignore label value of the loss layer 10. additionally added an accuracy layer in the end     'layer {             name: ""accuracy""     type: ""Accuracy""     bottom: ""pred""     bottom: ""label""     top: ""accuracy""     accuracy_param {       ignore_label: 21     }' 11. in the solver file I only changed the test_interval to 1000 and display 200 in order to get more output 12. executed command: caffe train -solver TVG_CRFRNN_new_solver.prototxt -weights TVG_CRFRNN_COCO_VOC.caffemodel  "
"I used the crfasrnn to train a new model, and when ""lr_mult"" of Deconvolution layer is assigned as non-zero, for example, 1 or 10, then the backward of all layers are zeros. And the loss will quickly go down to 0.00138931. Why can not the lr_mult  be non-zero ? Thank you. "
"Hi, everyone! Thank you for your reading my issue at first. I'm trying to retrian CRFasRNN using other data instead of VOC. I modifiy `TVG_CRFRNN_new_traintest.prototxt` , and it works well when I train the model with all parameters randomly initialized. However, when I try to fine tune with `fcn-8s-pascal.caffemodel` , it fails.  Here are some log info: **I0618 10:43:15.192327   567 caffe.cpp:128] Finetuning from ./models/fcn-8s-pascal.caffemodel [libprotobuf WARNING google/protobuf/io/coded_stream.cc:505] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h. [libprotobuf WARNING google/protobuf/io/coded_stream.cc:78] The total number of bytes read was 537962613 I0618 10:43:16.506494   567 upgrade_proto.cpp:620] Attempting to upgrade input file specified using deprecated V1LayerParameter: ./models/fcn-8s-pascal.caffemodel I0618 10:43:17.094655   567 upgrade_proto.cpp:628] Successfully upgraded file specified using deprecated V1LayerParameter [libprotobuf WARNING google/protobuf/io/coded_stream.cc:505] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h. [libprotobuf WARNING google/protobuf/io/coded_stream.cc:78] The total number of bytes read was 537962613 I0618 10:43:18.547567   567 upgrade_proto.cpp:620] Attempting to upgrade input file specified using deprecated V1LayerParameter: ./models/fcn-8s-pascal.caffemodel I0618 10:43:19.128402   567 upgrade_proto.cpp:628] Successfully upgraded file specified using deprecated V1LayerParameter I0618 10:43:19.280107   567 caffe.cpp:211] Starting Optimization I0618 10:43:19.280191   567 solver.cpp:293] Solving New_DATA_TRAIN I0618 10:43:19.280205   567 solver.cpp:294] Learning Rate Policy: fixed **\* Aborted at 1466217832 (unix time) try ""date -d @1466217832"" if you are using GNU date *** PC: @     0x7f614c91f528 caffe::SoftmaxWithLossLayer ::Backward_cpu() **\* SIGSEGV (@0x250db0ac) received by PID 567 (TID 0x7f614d0ba780) from PID 621654188; stack trace: ***     @     0x7f614b3a9cb0 (unknown)     @     0x7f614c91f528 caffe::SoftmaxWithLossLayer ::Backward_cpu()     @     0x7f614c991dc9 caffe::Net ::BackwardFromTo()     @     0x7f614c991ea1 caffe::Net ::Backward()     @     0x7f614c8a89e1 caffe::Solver ::Step()     @     0x7f614c8a9225 caffe::Solver ::Solve()     @           0x408edb train()     @           0x4068d1 main     @     0x7f614b394f45 (unknown)     @           0x406fbd (unknown)     @                0x0 (unknown)** Dose anyone know how to solve this problem? Thanks a lot! "
"Dear sir, if i want to use your code, except MKL, CUDA and so on, other tools like protobuf  need i to install? Thank you! "
"I'v recently installed new version of cv2, 3.1.0. Now I've got problems install CRFasRNN:   In window_data_layer.cpp I commented       which solved the problem of â€˜const int CV_LOAD_IMAGE_COLORâ€™ redeclared as different kind of symbol.   Now when processing  compute_image_mean.o I keep getting   `undefined reference to`cv::imread(cv::String const&, int)'`  Can I use opencv v3 with CRF at all, or should I roll back to 2.4? It seems there's some format incompatibility.   cheers "
"Hello,  In crfasrnn paper, section 6  > The compatibility transform parameters of the CRF-RNN were initialized using the Potts model, and kernel width and weight parameters were obtained from a cross-validation process  Can you please shed some more light into the cross-validation process?  Thanks "
"I download the caffe and put it in the folder of crfasrnn. After that, I go to the caffe folder and run as  mkdir build cd build cmake ..  I got the error as follows:     Could you help me to solve it. I set path for pythoncaffe before. Note that, If I used original caffe in the   it worked ` export PYTHONPATH=${HOME}/crfasrnn/caffe/python:$PYTHONPATH` "
"Hi I`m receive this error telling me that my memory is not enough. I already reduce the image size to 228 304 and 10 label only, and batchsize =1. The network says that Memory required for data: 528391700 I`m using 680 and i have 2GB video ram. Still it gives out this erro. anyidea why it happening?  I0429 19:27:37.009716  3199 net.cpp:298] Network initialization done. I0429 19:27:37.009724  3199 net.cpp:299] Memory required for data: 528391700 I0429 19:27:37.009837  3199 solver.cpp:65] Solver scaffolding done. I0429 19:27:37.009905  3199 caffe.cpp:128] Finetuning from /home/snake/caffe_crfrnn/crfasrnn-master/python-scripts/TVG_CRFRNN_COCO_VOC.caffemodel [libprotobuf WARNING google/protobuf/io/coded_stream.cc:505] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h. [libprotobuf WARNING google/protobuf/io/coded_stream.cc:78] The total number of bytes read was 537968303 I0429 19:27:37.648407  3199 upgrade_proto.cpp:620] Attempting to upgrade input file specified using deprecated V1LayerParameter: /home/snake/caffe_crfrnn/crfasrnn-master/python-scripts/TVG_CRFRNN_COCO_VOC.caffemodel I0429 19:27:37.919209  3199 upgrade_proto.cpp:628] Successfully upgraded file specified using deprecated V1LayerParameter [libprotobuf WARNING google/protobuf/io/coded_stream.cc:505] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h. [libprotobuf WARNING google/protobuf/io/coded_stream.cc:78] The total number of bytes read was 537968303 I0429 19:27:38.654881  3199 upgrade_proto.cpp:620] Attempting to upgrade input file specified using deprecated V1LayerParameter: /home/snake/caffe_crfrnn/crfasrnn-master/python-scripts/TVG_CRFRNN_COCO_VOC.caffemodel I0429 19:27:38.930379  3199 upgrade_proto.cpp:628] Successfully upgraded file specified using deprecated V1LayerParameter I0429 19:27:39.024951  3199 caffe.cpp:211] Starting Optimization I0429 19:27:39.025022  3199 solver.cpp:293] Solving TVG_CRF_RNN_COCO_VOC_TRAIN_3_CLASSES I0429 19:27:39.025032  3199 solver.cpp:294] Learning Rate Policy: fixed F0429 19:27:39.100859  3199 syncedmem.cpp:58] Check failed: error == cudaSuccess (2 vs. 0)  out of memory **\* Check failure stack trace: ***     @     0x7f0134bd7daa  (unknown)     @     0x7f0134bd7ce4  (unknown)     @     0x7f0134bd76e6  (unknown)     @     0x7f0134bda687  (unknown)     @     0x7f01352ef9e1  caffe::SyncedMemory::to_gpu()     @     0x7f01352eed69  caffe::SyncedMemory::mutable_gpu_data()     @     0x7f01351e2472  caffe::Blob ::mutable_gpu_data()     @     0x7f013523a070  caffe::BaseConvolutionLayer ::forward_gpu_gemm()     @     0x7f01352f9f21  caffe::ConvolutionLayer ::Forward_gpu()     @     0x7f01351f4751  caffe::Net ::ForwardFromTo()     @     0x7f01351f4ac7  caffe::Net ::ForwardPrefilled()     @     0x7f013521b279  caffe::Solver ::Step()     @     0x7f013521bac5  caffe::Solver ::Solve()     @           0x408f3b  train()     @           0x406931  main     @     0x7f01340e9ec5  (unknown)     @           0x40701d  (unknown)     @              (nil)  (unknown) "
"After running /solve.py  from   the following error pop up  I0428 23:40:04.491811 27105 caffe.cpp:183] Using GPUs 0 I0428 23:40:04.587026 27105 solver.cpp:54] Initializing solver from parameters:  test_iter: 261 test_interval: 1333 base_lr: 1e-13 display: 50 max_iter: 100000 lr_policy: ""fixed"" momentum: 0.99 weight_decay: 0.0005 snapshot: 1000 snapshot_prefix: ""/home/snake/caffe_crfrnn/train-CRF-RNN-master/snapshot/train"" solver_mode: GPU device_id: 0 net: ""/home/snake/caffe_crfrnn/train-CRF-RNN-master/TVG_CRFRNN_COCO_VOC_TRAIN_3_CLASSES.prototxt"" test_initialization: false I0428 23:40:04.587134 27105 solver.cpp:96] Creating training net from net file: /home/snake/caffe_crfrnn/train-CRF-RNN-master/TVG_CRFRNN_COCO_VOC_TRAIN_3_CLASSES.prototxt [libprotobuf ERROR google/protobuf/text_format.cc:245] Error parsing text-format caffe.NetParameter: 213:3: Unknown enumeration value of ""MULTI_STAGE_MEANFIELD"" for field ""type"". F0428 23:40:04.587803 27105 upgrade_proto.cpp:932] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: /home/snake/caffe_crfrnn/train-CRF-RNN-master/TVG_CRFRNN_COCO_VOC_TRAIN_3_CLASSES.prototxt  The MULTI_STAGE_MEANFIELD is build but there was some warning CXX src/caffe/layers/eltwise_layer.cpp src/caffe/layers/multi_stage_meanfield.cpp: In instantiation of â€˜void caffe::MultiStageMeanfieldLayer::LayerSetUp(const std::vectorcaffe::Blob &, const std::vectorcaffe::Blob &) [with Dtype = float]â€™: src/caffe/layers/multi_stage_meanfield.cpp:258:1: required from here src/caffe/layers/multi_stage_meanfield.cpp:72:83: warning: format â€˜%lfâ€™ expects argument of type â€˜doubleâ€™, but argument 3 has type â€˜floatâ€™ [-Wformat=] fscanf(pFile, ""%lf"", &this->blobs_[0]->mutable_cpu_data()[i \* channels_ + i]); ^ src/caffe/layers/multi_stage_meanfield.cpp:79:83: warning: format â€˜%lfâ€™ expects argument of type â€˜doubleâ€™, but argument 3 has type â€˜floatâ€™ [-Wformat=] fscanf(pFile, ""%lf"", &this->blobs_[1]->mutable_cpu_data()[i \* channels_ + i]); ^ src/caffe/layers/multi_stage_meanfield.cpp: In member function â€˜void caffe::MultiStageMeanfieldLayer::LayerSetUp(const std::vectorcaffe::Blob &, const std::vectorcaffe::Blob &) [with Dtype = float]â€™: src/caffe/layers/multi_stage_meanfield.cpp:72:7: warning: ignoring return value of â€˜int fscanf(FILE, const char, ...)â€™, declared with attribute warn_unused_result [-Wunused-result] fscanf(pFile, ""%lf"", &this->blobs_[0]->mutable_cpu_data()[i \* channels_ + i]); ^ src/caffe/layers/multi_stage_meanfield.cpp:79:7: warning: ignoring return value of â€˜int fscanf(FILE, const char, ...)â€™, declared with attribute warn_unused_result [-Wunused-result] fscanf(pFile, ""%lf"", &this->blobs_[1]->mutable_cpu_data()[i \* channels_ + i]); ^ src/caffe/layers/multi_stage_meanfield.cpp: In member function â€˜void caffe::MultiStageMeanfieldLayer::LayerSetUp(const std::vectorcaffe::Blob &, const std::vectorcaffe::Blob &) [with Dtype = double]â€™: src/caffe/layers/multi_stage_meanfield.cpp:72:7: warning: ignoring return value of â€˜int fscanf(FILE, const char, ...)â€™, declared with attribute warn_unused_result [-Wunused-result] fscanf(pFile, ""%lf"", &this->blobs_[0]->mutable_cpu_data()[i \* channels_ + i]); ^ src/caffe/layers/multi_stage_meanfield.cpp:79:7: warning: ignoring return value of â€˜int fscanf(FILE, const char, ...)â€™, declared with attribute warn_unused_result [-Wunused-result] fscanf(pFile, ""%lf"", &this->blobs_[1]->mutable_cpu_data()[i \* channels_ + i]); ^  anyidea how to solve it? "
"I get this error when I run make   CXX src/caffe/net.cpp In file included from src/caffe/net.cpp:13:0: ./include/caffe/util/io.hpp:12:18: fatal error: hdf5.h: No such file or directory  #include ""hdf5.h""                   ^ compilation terminated. Makefile:469: recipe for target '.build_release/src/caffe/net.o' failed make: **\* [.build_release/src/caffe/net.o] Error 1  I did check for the hdf5.h file and found it in this location:  /usr/include/hdf5/serial/hdf5.h /usr/include/opencv2/flann/hdf5.h  I get the same error when I run 'sudo make' what could be the issue? ... has it got something to do with the makefile.config file?  Any help would be appreciated. Thanks "
"Ubuntu 14.04 is in my Parellels(virtual machine).My mac's GNU is NVIDIA,so I have to run the codes in my mac,but I don't know how to install dependencies.Does anyone could help me? "
"After looking up the compile error  `src/caffe/layers/cudnn_conv_layer.cu(67): error: argument of type ""cudnnAddMode_t"" is incompatible with parameter of type ""const void *""` it seems to result from using cudnnv4 instead of v2. Is it the case that this code requires cudnnv2? "
"Hi all, I want to train a classification on the clothing, such as dresses, How can I do it? Thank you, Hang "
"I was able to extract some objects from a challenging image (  lots of shadows, object/background colors are indistinguishable and so on. Some cows on the  image are occluded: there's one cow behind another, although both a well identified.   Here's my question: how do I delineate these 2 objects. If crf-rnn recognizes that they are separate, this should be somehow reflected. I believe, if these were objects of different classes, pixels would have different labels. But what if the occluded object is of the same class?   thanks  "
"mean_vec = np.array([103.939, 116.779, 123.68], dtype=np.float32), this is from the demo. Each value in the array, is it the mean of each array for the specific image?  "
"If I understand correctly, the algorithm distinguishes between two objects of the same class (e.g. 2 cows are of the same colour).  How do I get the number of objects and the edges, i.e. the border where one object ends and the other begins?  "
"Hi,  Thank you sharing the code, the idea of the CRF as RNN is great. I have a question regarding training the model from scratch and on a different type of images.  I ran into a problem when saving parameters into .caffemodel file. The training log shows that the network learns judging by the softmax loss going down, but when I load the snapshot, the parameters are always at the default values.  Have you or anyone experienced anything similar? I have tried to train the basic lenet example on the MNIST and there was no problem.    Thanks in advance.  Adrian Lisko "
"I can go through the ""make"" step, with model downloaded. When running the demo by:     error encountered. Please tell me how I can fix this.  Traceback (most recent call last):   File ""crfasrnn_demo.py"", line 38, in       net = caffe.Segmenter(MODEL_FILE, PRETRAINED)   File ""../caffe-crfrnn/python/caffe/segmenter.py"", line 19, in **init**     caffe.Net.**init**(self, model_file, pretrained_file) Boost.Python.ArgumentError: Python argument types in     Net.**init**(Segmenter, str, str) did not match C++ signature:     **init**(_object*, std::__cxx11::basic_string , std::allocator  >)     __init__(_object*, std::__cxx11::basic_string , std::allocator  >, std::__cxx11::basic_string , std::allocator  >) "
"The filter expects an input image of size Nx3xHxW. When a grayscale/monochrome image is provided, the GPU implementation seg faults (CPU does not). I guess that there is an automatic alloc with zero filling when using the CPU, which does not exist in the CUDA code.   Workaround: concatenate the grayscale image to have three channels. See config below:  layer {   name: ""rgb""   bottom: ""data""   bottom: ""data""   bottom: ""data""   top: ""rgb""   type: ""Concat""   concat_param {     axis: 1   } } "
"I am getting runtime error when I use higher resolution images. *_self.blobs[in_].data[...] = blob ValueError: could not broadcast input array from shape (1,3,700,700) into shape (1,3,500,500)_ *_ I am having trouble figuring out where the self.blobs[in_].data[...] numpy shape is initialized. Need suggestions for debugging. "
I try to run the demo on video file using cv2.VideoCapture of opencv. I mean that I want the demo to work and analyse each frame separately? How to do that and run the demo for each frame? 
"Hi I am having trouble compiling the custom caffe code.  I keep on getting this errors  ../lib/libcaffe.so: undefined reference to `testing::internal::MakeAndRegisterTestInfo(char const*, char const*, char const*, char const*, void const*, void (*)(), void (*)(), testing::internal::TestFactoryBase*)' ../lib/libcaffe.so: undefined reference to`testing::AssertionSuccess()' ../lib/libcaffe.so: undefined reference to `testing::Test::~Test()' ../lib/libcaffe.so: undefined reference to`testing::internal::IsTrue(bool)' ../lib/libcaffe.so: undefined reference to `typeinfo for testing::Test' ../lib/libcaffe.so: undefined reference to`testing::Test::SetUp()' ../lib/libcaffe.so: undefined reference to `testing::Test::TearDown()' ../lib/libcaffe.so: undefined reference to`testing::Test::Test()' ../lib/libcaffe.so: undefined reference to `testing::internal::String::Format(char const*, ...)' ../lib/libcaffe.so: undefined reference to`testing::internal::AssertHelper::~AssertHelper()' ../lib/libcaffe.so: undefined reference to `testing::internal::AssertHelper::operator=(testing::Message const&) const' ../lib/libcaffe.so: undefined reference to`testing::internal::AssertHelper::AssertHelper(testing::TestPartResult::Type, char const_, int, char const_)' ../lib/libcaffe.so: undefined reference to `testing::internal::EqFailure(char const_, char const_, testing::internal::String const&, testing::internal::String const&, bool)' collect2: error: ld returned 1 exit status tools/CMakeFiles/extract_features.dir/build.make:120: recipe for target 'tools/extract_features' failed make[2]: **\* [tools/extract_features] Error 1 CMakeFiles/Makefile2:586: recipe for target 'tools/CMakeFiles/extract_features.dir/all' failed make[1]: **\* [tools/CMakeFiles/extract_features.dir/all] Error 2 Makefile:116: recipe for target 'all' failed make: **\* [all] Error 2 "
"Hi Zhengshuai,  I am a first year phd student in stanford called Jingwei Huang. I am very interested in your work and want to use your architecture for further research. Can you tell me how I can train the data? "
"As I understand according to the paper and the video-lecture:   : the demo-segmentation can separate multiple object close to each other (e.g separate 3 close boats instead of recognize the as a single big bout), and also it can add Ground Truth layer which gives more accurate result and draws the contour of each object.  when I run the demo I cant see these abilities, how can I add them?  thanks "
Hii  I had installed the crfasrnn demo and the code worked well.  What is the image database that have been used in order to train the model? Do you use a popular image database like image net or your own data source? Can I have access to this database?  Thanks 
I looked through the paper but I could not find any speed benchmark that I can compare with. I tried crfasrnn_demo.py on a single instance of g2.2xlarge Amazon AWS and got unexpectedly slow classification speed and wondering if something is wrong or not.  The code below in crfasrnn_demo.py that I modified to time :     The result is 29.7011039257 seconds. What is the average time required for this demo? 
"Hi!  I tried to train CRF-RNN network with 3 classes: bird (801 images), bottle (747 images) and chair (1071 images). 90 % of data were employed for training and the rest 10 % for testing. After 90 thousand iterations, loss of a model decreased approximately to 82 thousand (seemed stagnating for relatively long time) during test phase, and output from trained network is still very poor.   **Questions** - How low should test loss become in order to receive good segmentation results? Is 80 thousand still too much? - Is it possible that performance of network is low because selected class labels occupy often small part of image? Therefore, background class is preferred? - In a case, that loss is still not low enough to provide reasonable segmentation results, is it possible to train segmentation successfully with employed training images (number mentioned above)?  Thank you!  Martin "
In the code section or in the paper I can't find any information of what feature is missing in the normal Caffe so you guys needed to build a custom version. Could you explain a bit here?   Thank you! 
"Hi all, I am trying to train my own images with crfasrnn. My classes are background and people. But my training does not converge, the loss values do not decrease. And the prediction output of my network is only background (class 0) for all image. I think that I have a mistake in my training. Have you ever met the similar case?  Could you give me any suggestion?   Thank you, Thuan "
"Hi!  I am trying to train _CRF-RNN_ using part of VOC 2012 data but when I set _batch_size_ higher than 3, I receive `error == cudaSuccess (2 vs. 0) out of memory`. It is quite surprising to me because I am using nVidia Tesla K40, so there should be enough memory. Moreover, in a log there is not required more memory than I possess. I still can train with only 3 images per batch but I expect that the loss is going to drop very slowly.   **Questions** - Did somebody encounter similar issue, or is it just problem on my side? - How large batch size was used to create publicly avaliable model? (I did not find any mention about it in paper)  Thank you!  Martin "
"Hello ,  I am trying to build caffe with gpu.Initially while compiling an error was displayed saying ""could not find spatial.par ...."",so I copy pasted ""spatial.par"" to the path it compiles and again added ""bilateral.par"" when it threw error on that as well.These I added from python-scripts available for python users.  When I tried to compile using ""make all"",I get the following warning,but it compiles all other files neatly.  src/caffe/layers/multi_stage_meanfield.cpp: In instantiation of â€˜void caffe::MultiStageMeanfieldLayer ::LayerSetUp(const std::vectorcaffe::Blob &, const std::vectorcaffe::Blob &) [with Dtype = float]â€™: src/caffe/layers/multi_stage_meanfield.cpp:254:1:   required from here src/caffe/layers/multi_stage_meanfield.cpp:68:83: warning: format â€˜%lfâ€™ expects argument of type â€˜double_â€™, but argument 3 has type â€˜float_â€™ [-Wformat=]        fscanf(pFile, ""%lf"", &this->blobs_[0]->mutable_cpu_data()[i \* channels_ + i]);                                                                                    ^ src/caffe/layers/multi_stage_meanfield.cpp:75:83: warning: format â€˜%lfâ€™ expects argument of type â€˜double_â€™, but argument 3 has type â€˜float_â€™ [-Wformat=]        fscanf(pFile, ""%lf"", &this->blobs_[1]->mutable_cpu_data()[i \* channels_ + i]);                                                                                    ^ src/caffe/layers/multi_stage_meanfield.cpp: In member function â€˜void caffe::MultiStageMeanfieldLayer ::LayerSetUp(const std::vectorcaffe::Blob &, const std::vectorcaffe::Blob &) [with Dtype = float]â€™: src/caffe/layers/multi_stage_meanfield.cpp:68:7: warning: ignoring return value of â€˜int fscanf(FILE_, const char_, ...)â€™, declared with attribute warn_unused_result [-Wunused-result]        fscanf(pFile, ""%lf"", &this->blobs_[0]->mutable_cpu_data()[i \* channels_ + i]);        ^ src/caffe/layers/multi_stage_meanfield.cpp:75:7: warning: ignoring return value of â€˜int fscanf(FILE_, const char_, ...)â€™, declared with attribute warn_unused_result [-Wunused-result]        fscanf(pFile, ""%lf"", &this->blobs_[1]->mutable_cpu_data()[i \* channels_ + i]);        ^ src/caffe/layers/multi_stage_meanfield.cpp: In member function â€˜void caffe::MultiStageMeanfieldLayer ::LayerSetUp(const std::vectorcaffe::Blob &, const std::vectorcaffe::Blob &) [with Dtype = double]â€™: src/caffe/layers/multi_stage_meanfield.cpp:68:7: warning: ignoring return value of â€˜int fscanf(FILE_, const char_, ...)â€™, declared with attribute warn_unused_result [-Wunused-result]        fscanf(pFile, ""%lf"", &this->blobs_[0]->mutable_cpu_data()[i \* channels_ + i]);        ^ src/caffe/layers/multi_stage_meanfield.cpp:75:7: warning: ignoring return value of â€˜int fscanf(FILE_, const char_, ...)â€™, declared with attribute warn_unused_result [-Wunused-result]        fscanf(pFile, ""%lf"", &this->blobs_[1]->mutable_cpu_data()[i \* channels_ + i]);  Also during ""make runtest"" it takes long time to test the file at ,   1 test from MultiStageMeanfieldLayerTest/2, where TypeParam = caffe::FloatGPU [ RUN      ] MultiStageMeanfieldLayerTest/2.TestGradient  Hence I stopped the test.  Once I build this, I will be trying to use the model in torch 7 using torch-caffe-binding.  Sorry for the long post  Regards srikanth "
@bittnt how to load arbitary image whose longer side is more than 500 pixels?  
"I use the NYU V2 dataset as my traing set. The labels are created as follows. I first create a picture each pixel corresponding to its label and create labels_lmdb using these pictures by convert_imageset in the dictory caffe-root/tools. images_lmdb is created using images in the dataset by that tool convert_imageset directly.  I have trained my model using Tesla K20m, but the loss of the nueral network seems oscillating all the time after 400 iterations. The whole process of 400 iterations takes about 10 hours. I think that there is something wrong with my configurations. I am wondering whether you could help me to find the problems.  I modified the file ""TVG_CRFRNN_COCO_VOC.prototxt"" provided by the project of CRFasRNN in the dicrectory crfasrnn/python-scripts. Details as follows.  # I added the data layer at the begining of that prototxt file to replace the original input part (line 3-8):  layers {   name: ""data""   type: DATA   top: ""data""   include {     phase: TRAIN   }   transform_param {     mean_value: 130.4265     mean_value: 111.4584     mean_value: 103.3727   }   data_param {     source: ""./indoor_images_lmdb""     batch_size: 1     backend: LMDB   } } layers {   name: ""label""   type: DATA   top: ""label""   include {     phase: TRAIN   }   data_param {     source: ""./indoor_labels_lmdb""     batch_size: 1     backend: LMDB   } } # At the end of that file, I added a SOFTMAX_LOSS layer:  layers { type: SOFTMAX_LOSS name: 'loss' top: 'loss'   bottom: 'pred' bottom: 'label'   loss_param { normalize: false }   } # The file ""solver.prototxt"" was created as follows:  net: ""CRFasRNN_train.prototxt"" test_iter: 500 test_interval: 100000 display: 250 lr_policy: ""fixed"" base_lr: 1e-13 momentum: 0.99 weight_decay:0.0016 max_iter: 10000 snapshot: 1000 snapshot_prefix: ""train"" test_initialization: false solver_mode: GPU # The file ""solve.py"" was created as follows:  caffe_root = '../caffe-crfrnn/' import sys sys.path.insert(0, caffe_root + 'python') import caffe import numpy as np base_weights = 'TVG_CRFRNN_COCO_VOC.caffemodel' solver = caffe.SGDSolver('solver.prototxt') solver.net.copy_from(base_weights) solver.step(10000)   The file ""TVG_CRFRNN_COCO_VOC.caffemodel"" is also provided in the directory crfasrnn/python-script. "
"Hi,  I am trying to compile the code, but getting following error:  ./lib/libcaffe.so: undefined reference to `testing::internal::MakeAndRegisterTestInfo(char const*, char const*, char const*, char const*, void const*, void (*)(), void (*)(), testing::internal::TestFactoryBase*)' ../lib/libcaffe.so: undefined reference to`testing::AssertionSuccess()' ../lib/libcaffe.so: undefined reference to `testing::Test::~Test()' ../lib/libcaffe.so: undefined reference to`testing::internal::IsTrue(bool)' ../lib/libcaffe.so: undefined reference to `typeinfo for testing::Test' ../lib/libcaffe.so: undefined reference to`testing::Test::SetUp()' ../lib/libcaffe.so: undefined reference to `testing::Test::TearDown()' ../lib/libcaffe.so: undefined reference to`testing::Test::Test()' ../lib/libcaffe.so: undefined reference to `testing::internal::String::Format(char const*, ...)' ../lib/libcaffe.so: undefined reference to`testing::internal::AssertHelper::~AssertHelper()' ../lib/libcaffe.so: undefined reference to `testing::internal::AssertHelper::operator=(testing::Message const&) const' ../lib/libcaffe.so: undefined reference to`testing::internal::AssertHelper::AssertHelper(testing::TestPartResult::Type, char const_, int, char const_)' ../lib/libcaffe.so: undefined reference to `testing::internal::EqFailure(char const_, char const_, testing::internal::String const&, testing::internal::String const&, bool)' collect2: error: ld returned 1 exit status make[2]: **\* [tools/caffe] Error 1 make[1]: **\* [tools/CMakeFiles/caffe.bin.dir/all] Error 2 make: **\* [all] Error 2  I am using Ubuntu 14.04 and followed the steps mentioned in the web page while installation.  It would be great if you can help me in resolving the error  Thanks.  P.S.: I tried to compile caffe and it worked successfully. "
"Hello,  Thanks for the source code and the demo, I've succesfully downloaded, compiled and run (also the test passed). And now I want to start doing updates of the current weights that are into the blobs of the provided caffemodel, but when I load the model prototxt with the caffe.solver: solver = caffe.SGDSolver(MODEL_FILE) instead of loading the network for use it: net = caffe.Segmenter(MODEL_FILE, PRETRAINED) I have a libprotobuf error: [libprotobuf ERROR google/protobuf/text_format.cc:290] Error parsing text-format caffe.SolverParameter: 1:6: Message type ""caffe.SolverParameter"" has no field named ""input"". WARNING: Logging before InitGoogleLogging() is written to STDERR F1122 00:58:02.739593 28227 io.hpp:57] Check failed: ReadProtoFromTextFile(filename, proto)  **\* Check failure stack trace: *** Aborted (core dumped)  Can you give me some advice of what to do, or what should I do in order to be able to load the solver for the model and update the weights?  Thank you very much,  Sebastian "
"I am trying to run the python demo, but got the following error:    "
"After uncommenting 'caffe.set_mode_gpu()' in the python demo script, it complains caffe has no member called set_mode_gpu  and I try to included this function in **init**.py of caffe-crfasrnn/python, still failed.  So is there any way to use gpu for this project, it just takes too long for one pic (~70s), not like the web demo.  Thx in advance "
"Hey, I get this error:   $ sudo make NVCC src/caffe/util/math_functions.cu src/caffe/util/math_functions.cu(159): error: kernel launches from templates are not allowed in system files  how can I fix this ? Im running Ubuntu 14.04 with Atlas and Python "
"What version do crfasrnn? I am trying to launch code under CUDA Toolkit 7.5, but i got error Check failed: error == cudaSuccess (8 vs. 0)  invalid device function.  Is this because of new CUDA version? "
"Hello. I compiled this code and successfully ran demo. But I have question. For arbitary images, how can I get tags of segment? "
Trying to run the python demo with GPU mode enabled. Seem to be running of out memory (Have 2GB GPU + cuda 6.5)  What is the minimum memory requirement for this demo. Is there any (simple) way of lowing the requirements?  What is the expected run-time on GPU (on my cpu it takes 7-8 seconds)?  Thanks.  
"Mostly trying to compile this on a mac, with CPU_ONLY. So I thought it would be easier to add the differences on top of the upstream caffe  Here is my fork   where caffe is a submodule that points here :    (this makes it a lot easier to see what diverges if caffe-crfrnn stems off from the actual caffe git).  Everything builds and the tests pass (including meanfield layer one), but import caffe in python will segfault.  Would you have any ideas so I can further this ? Thank you "
Are there cuda versions of this code? I can only see the forward and backward cpu versions and no .cu files. The runtime is very slow. 
"Have you or anyone else tested this code with AWS G2 instances?   I tried testing and even after compiling your version of caffe and running make runtest, the code did not work with GPU (Failed with core dump).   I also replaced the commented out line in python code to enable GPU mode     The CPU code seemed to work, but threw an error when trying to show the image, due to lack of display. Is this happening due to lack of memory since AWS GPUs have only 4Gb memory?   Thanks, "
Thanks for the paper! Any chance you could add a summary of the changes made to the Caffe code? That and/or the hash where you started the fork would be great to figure out where the relevant changes are. 
"Hi bittnt,  I had already installed crfasrnn and the code worked well. But the code that you provided just contained the part using the trained model (TVG_CRFRNN_COCO_VOC.caffemodel) to get the result. How could I train a new model using my own data. Could you provide a pipeline about how to train a new model and the prototxt file used in training. Any response will be appreciated.  Best wishes, Huayong "
"Hi, Sadeep Jayasumana,  I had already installed crfasrnn. As given the trained model (TVG_CRFRNN_COCO_VOC.caffemodel) , how could I perform pixel-wise labeling prediction on test image? Could you list the brief steps?  Thanks in advance! Milton "
"Hi @gkioxari , I'm trying to rewrite the code to Pytorch however I'm having difficulties understanding the  .    What is the intuition behind it?    As for my understanding, get the class scores from context_fc7 output (4096->11) then from n number of rois, get the maximum on each context e.g.    [1,2,3,4,5,6,7,8,9,10]  [10, 9,8,7,6,5,4,3,2,1]  '=  [10,9,8,7,6,6,7,8,9,10]"
"Helloï¼ŒI'm sorry if I distubed you! A new problem I met is when training a model according to myself dataset, 'Floating point exception' is occured !! Why?!  Thank you!"
"hi!  The issue  I met is that when loaded the layer conv3_1, the terminal captured the error as fallows:  *** Error in `python': corrupted size vs. prev_size: 0x00007f28395179e0 ***  Wishing your answer!  Thank you very much!  ......  I0606 15:25:45.454169 15772 net.cpp:477] pool2   pool2  I0606 15:25:45.454303 15772 net.cpp:155] Setting up pool2  I0606 15:25:45.454313 15772 net.cpp:163] Top shape: 1 128 56 56 (401408)  I0606 15:25:45.454316 15772 layer_factory.hpp:76] Creating layer conv3_1  I0606 15:25:45.454322 15772 net.cpp:110] Creating Layer conv3_1  I0606 15:25:45.454326 15772 net.cpp:477] conv3_1   conv3_1  I0606 15:25:45.454519 15772 net.cpp:155] Setting up conv3_1  I0606 15:25:45.454526 15772 net.cpp:163] Top shape: 1 256 56 56 (802816)  *** Error in `python': corrupted size vs. prev_size: 0x00007f28395179e0 ***    "
"I would like to ask how  Data Augmentation is performed in the case of the Baseline RCNN that only uses groundtruth ROIs as Primary Regions.    More specifically within the paper, you mention that:    > Rather than limiting training to the ground-truth person  > locations, we use all regions that overlap more than 0.5 with  > a ground-truth box. This condition serves as a form of data  > augmentation. For every primary region, we randomly select  > N regions from the set of candidate secondary regions.  > N is a function of the GPU memory limit (we use a Nvidia  > K40 GPU) and the batch size.  > We fine-tune our network starting with a model trained  > on ImageNet-1K for the image classification task. We tie  > the weights of the fully connected primary and secondary  > layers (fc6, fc7), but not for the final scoring models. We set  > the learning rate to 0.0001, the batch size to 30 and consider  > 2 images per batch. We pick N = 10 and train for 10K  > iterations. Larger learning rates prevented fine-tuning from  > converging.    Thus for the case of the simple RCNN baseline that uses only primary regions and no secondary regions, this means that each batch contains 2 images and 30 ROIs for the ROI-Pooling layer.    Assuming the aforementioned assumption holds, in case the two images contain only 1 primary region each, with what do you fill the rest of the batch (as there should be 28 positions left empty) ?    Since the number of primary regions is not fixed per image,  do you enforce the number of data augmentation samples to be balanced per class somehow?    Would it be possible to share the results you achieve without using data augmentation?"
"user@BLTSP03119:~/Documents/rrc_detection$ make all  PROTOC src/caffe/proto/caffe.proto  CXX .build_release/src/caffe/proto/caffe.pb.cc  CXX src/caffe/syncedmem.cpp  CXX src/caffe/data_reader.cpp  CXX src/caffe/layer_factory.cpp  CXX src/caffe/blob.cpp  CXX src/caffe/solver.cpp  CXX src/caffe/internal_thread.cpp  CXX src/caffe/net.cpp  CXX src/caffe/util/blocking_queue.cpp  CXX src/caffe/util/upgrade_proto.cpp  CXX src/caffe/util/hdf5.cpp  CXX src/caffe/util/db_leveldb.cpp  CXX src/caffe/util/cudnn.cpp  CXX src/caffe/util/insert_splits.cpp  CXX src/caffe/util/im_transforms.cpp  CXX src/caffe/util/io.cpp  CXX src/caffe/util/db_lmdb.cpp  CXX src/caffe/util/im2col.cpp  CXX src/caffe/util/bbox_util.cpp  CXX src/caffe/util/db.cpp  CXX src/caffe/util/math_functions.cpp  CXX src/caffe/util/sampler.cpp  CXX src/caffe/util/benchmark.cpp  CXX src/caffe/util/signal_handler.cpp  CXX src/caffe/common.cpp  CXX src/caffe/layers/prelu_layer.cpp  CXX src/caffe/layers/cudnn_conv_layer.cpp  CXX src/caffe/layers/accuracy_layer.cpp  CXX src/caffe/layers/hdf5_data_layer.cpp  CXX src/caffe/layers/power_layer.cpp  CXX src/caffe/layers/reduction_layer.cpp  CXX src/caffe/layers/softmax_loss_layer.cpp  CXX src/caffe/layers/multibox_loss_layer.cpp  In file included from ./include/caffe/common.hpp:19:0,                   from ./include/caffe/blob.hpp:8,                   from ./include/caffe/layers/multibox_loss_layer.hpp:8,                   from src/caffe/layers/multibox_loss_layer.cpp:6:  ./include/caffe/util/device_alternate.hpp:15:36: error: no â€˜void caffe::MultiBoxLossLayer ::Forward_gpu(const std::vector *>&, const std::vector *>&)â€™ member function declared in class â€˜caffe::MultiBoxLossLayer â€™       const vector *>& top) { NO_GPU; } \                                      ^  src/caffe/layers/multibox_loss_layer.cpp:574:1: note: in expansion of macro â€˜STUB_GPUâ€™   STUB_GPU(MultiBoxLossLayer);   ^  ./include/caffe/util/device_alternate.hpp:19:39: error: no â€˜void caffe::MultiBoxLossLayer ::Backward_gpu(const std::vector *>&, const std::vector &, const std::vector *>&)â€™ member function declared in class â€˜caffe::MultiBoxLossLayer â€™       const vector *>& bottom) { NO_GPU; } \                                         ^  src/caffe/layers/multibox_loss_layer.cpp:574:1: note: in expansion of macro â€˜STUB_GPUâ€™   STUB_GPU(MultiBoxLossLayer);   ^  make: *** [.build_release/src/caffe/layers/multibox_loss_layer.o] Error 1      ________________________________________________________________________________          am using in cpu mode .             i made cpu=1             and use_opencv=1               in makefile.config  "
"Hi, I am having some trouble with making -j8. The error is â€˜std::isinfâ€™ has not been declared   using std::isinf  Thanks!  !   "
"Recently I 've been trying to compile and run your code.    I encountered several problems by experimenting with multiple Ubuntu (14.04 and 16.04) and CUDA versions  (7,  7.5 and 8). I also tried to run this code with two different graphics cards ( Geforce GTX 960 with 4 GB of memory and 1080 GTX with 12 GB of memory).     Although I managed to compile the project successfully at some point, I encountered the following runtime error  [ F1103 12:03:05.402940 10457 syncedmem.hpp:19] Check failed: error == cudaSuccess (30 vs. 0)  unknown error ]  using the 1080 GTX on Ubuntu 14.04 with CUDA 7 .    Since I suspect that the main issue here is compatibility with other library versions, could you please share the details about the specific system setup you used to develop this code.  "
"@gkioxari    in this  ,  it ignore the 0-index.  i think if use the pascal_voc_2012 for training,  the class list shoud be  ('jumping', 'phoning', 'playinginstrument', 'reading', 'ridingbike', 'ridinghorse', 'running', 'takingphoto',   'usingcomputer', 'walking', 'other').   So num_classes in this   is 11. if it ignores the 0-index, which means ignore 'jumping' class when applying  mean subtraction and stds division.  I think the  column 0 in  variable    means the class_id: **0 for jumping**, 1 for phoning ... 10 means other. **Does 0 also means backgroud**  ?  I find both background and jumping will use 0 for class-index in the column 0 of  variable  .   Is that right?    I find this    creates an zero array, the colomn 0 of the variable targets means either groundtruth label is 0, or  IOU is below threshold. so after this methon returns, we can not tell what does 0 means. should I change it from   targets = np.zeros((rois.shape[0], 5), dtype=np.float32)      to   targets = np.zeros((rois.shape[0], 5), dtype=np.float32)   targets[:,0] = -1  Do I miss something important?     Would you give some explanations in details ï¼Ÿ  Thank you very much~ "
"@gkioxari ,  I got a question about the  processing of 'other' class in pascal voc  1. in this  , 'other' class is the last element in the class list.  2. while in this  ,  i find that it ignore the 0-index class.    Should it ignore the 'other' class? but acording to 1,  index of 'other' class is not 0 ?  Any comments ?   "
After the test what should I do to make the results(like comp10_jumping.txt) change into pictures with box? 
"Dear gkioxari,  I'm sorry to bother you.but I meet some trouble.When I try , it occurs some error:    Writing jumping VOC results file  Traceback (most recent call last):    File ""./tools/test_net.py"", line 75, in        test_net(net, imdb)    File ""/home/customer/xubing/RstarCNN/tools/../lib/fast_rcnn/test.py"", line 285, in test_net      imdb._write_voc_results_file(all_boxes)    File ""/home/customer/xubing/RstarCNN/tools/../lib/datasets/pascal_voc.py"", line 247, in _write_voc_results_file      with open(filename, 'wt') as f:  IOError: [Errno 2] No such file or directory: '/home/customer/xubing/RstarCNN/tools/../lib/datasets/../../data/VOCdevkit2012/results/VOC2012/Action/comp10_action_val_jumping.txt'    but there is no sub file 'results' in the file 'VOCdevkit2012',and could you tell me how to get this file or some steps I did wrong before?  could you help me ? Thanks for your help."
"Dear gkioxari,  Thanks for your great work.  However when I try , I found that 'Iteration' in the file ' solver.prototxt' doesn't work  neither in 'train.py'  another problem:  ./tools/test_net.py --gpu 0 --def models/VGG16_RstarCNN/test.prototxt --net output/default/voc_2012_trainval/vgg16_fast_rstarcnn_joint_iter_40000.caffemodel  IOError: [Errno 2] No such file or directory: '/home/user1/RstarCNN/tools/../lib/datasets/../../data/VOCdevkit2012/results/VOC2012/Action/comp10_action_val_jumping.txt'  actually there is no sub file 'results' in the file 'VOCdevkit2012'.  could you help me ? Thank you very much  "
"Dear gkioxari    I am sorry to bother you since I want to rewrite R Star CNN on the MatConvNet. I do not know What is the Back Propagation formula when fuse the pri and sec stream at following two points.  First:  `layer {   name: ""sum_scores""   type: ""Sum""   bottom: ""cls_score""   bottom: ""mil_context_cls_score""   top: ""sum_cls_score"" }`  Second:  how to fuse the following two branches `layer {   name: ""context_roi_pool5""   type: ""ROIPooling""   bottom: ""conv5_3""   bottom: ""secondary_rois""   top: ""context_pool5""   roi_pooling_param {     pooled_w: 7     pooled_h: 7     spatial_scale: 0.0625 # 1/16   } }`  `layer {   name: ""roi_pool5""   type: ""ROIPooling""   bottom: ""conv5_3""   bottom: ""rois""   top: ""pool5""   roi_pooling_param {     pooled_w: 7     pooled_h: 7     spatial_scale: 0.0625 # 1/16   } }`  Thanks!               Francis "
"Hi I want to know in train.prototxt (under models/VGG16_RstarCNN/) ,  What is the meaning of â€œbbox_targetsâ€ã€â€œbbox_loss_weightsâ€  in first data layer    Thanks  "
I'm learning the theory and code of the rstar.   But I find the number of iterations is hard to determine. Is the code support testing on validation set after training several epochs?   
"I am trying to run the code on the Berkeley Attributes of People Dataset (BAPD) . After downloading the BAPD dataset and pre-trained models from the provided links, I modified the `tools/test_net.py` by replacing `imdb = datasets.pascal_voc('val','2012')` by `imdb = datasets.attr_bpad('BAPD')`. Then I run the following command:  `./tools/test_net.py  --gpu 0 --def models/VGG16_RstarCNN_attributes/test.prototxt --net models/VGG16_RstarCNN_attributes/bpad_rstarcnn_train.caffemodel` and got the following error:  `line 216, in im_detect     box_deltas = blobs_out['bbox_pred'] KeyError: 'bbox_pred' `  The reason is that the `VGG16_RstarCNN_attributes/test.prototxt` does not have the layer `bbox_pred`, this is a little weird because `bbox_pred` layers exists in other .prototxt files, such as `VGG16_RstarCNN/test.prototxt` and `VGG16_RstarCNN_stanford40/test.prototxt`.   Can anyone give me a hint on this issue please? Thanks in advance.   "
"Hello, I found the primary regions you use in the paper Contextual Action Recognition with R*CNN is annotated data. Is there some method that can automatically generate the primary regions in an input image?   if not, how can we recognize the actions in an input image without annotation information?   "
I read the paper Contextual Action Recognition with R*CNN  and run the test code on PASCAL VOC and Stanford40 dataset.  but i can not find the test code to run and test MPII . Is the test and train code on the MPII dataset available?    : ) 
While I was trying to run   test.py I got following errror      Is this only my problem.   Also it will be really helpful if there is an description that how we can try to see out put of images.  
None
"Hi, Because I couldn't train RstarCNN with VGG16 model (since I have a GPU sufficiency problem: I use GeForce GT630 ), I'm trying doing the train phase with ZF model (which seems need less GPU memory). But, I still have some problem in changing the parameters of the layers in the ""ZF.caffemodel"" !! Is there any one who has tried this before? can any one help me please ;;; "
"Hi, I am trying to run the code to test the RCNN classifier on the VGG16 reference model, but I get the error:  F0408 15:16:28.126595 23712 common.cpp:141] Check failed: error == cudaSuccess (35 vs. 0)  CUDA driver version is insufficient for CUDA runtime version **\* Check failure stack trace: *** Aborted (core dumped)  Could you help me with the issue? Is it a problem with the CUDA version? If so, which version is needed to make the code run? "
I'm getting make errors when I run 'make all'. 
"can you tell me how to solve this explicit, Thanks"
"Hi, everyone. I tested the pascal-voc 2012 datasets, RstarCNN works great! But there's a little problem when testing on Stanford40.   * In the `./tools/test_net.py` :  (line 13)`from fast_rcnn.test import test_net`  **-->**  `from fast_rcnn.test_stanford40 import test_net`   (line 16)`import datasets.stanford40` was added.  (line 78)`imdb = datasets.pascal_voc('val','2012')`**-->** `imdb = datasets.stanford40('test')`  * In the `./lib/datasets/stanford40.py` :  (line 295)`datasets.pascal_voc('trainval', '2012')`**-->** `d = datasets.stanford40('test')`    After that, I typed   `./tools/test_net.py --gpu 0 --def models/VGG16_RstarCNN_stanford40/test.prototxt --net ./trained_models/stanford40_rstarcnn_train.caffemodel` into the terminal, and finally:   `Network initialization done` and   `Memory required for data: 117424480` appeared.   *(This is satisfying output)*    **Then comes the problem, the terminal says:**   `File ""./tools/test_net.py"", line 78, in  `    `imdb = datasets.stanford40('test')`    `TypeError: 'module' object is not callable`     I tried to figure out the problem myself, so I add the following code to `./tools/test_net.py`:  `print(type(datasets.pascal_voc))`, the print result turn out to be: ** **  `print(type(datasets.stanford40))`, the print result turn out to be: ** **    In `./lib/datasets/`*""pascal_voc.py""* & *""stanford40.py""* are written in almost the same manner, I don't know what cause the problem. **If I missed something please correct me.**  "
"Thanks for your share.   Could you tell me after train_net and test_net, how to draw the figure like"" Top predictions on the PASCAL VOC Action test set""?  thank you"
"It seems both trained and reference model (500M and 3.6G) are not available in the homepage of Berkeley EECS, anyone else has the same problems? Where can I download these models. Any advise is appreciated. "
"Hi all,      When I run the code on PASCAL VOC 2012 Action dataset with the trained models, the output is similar to that in the paper. But, the results, which are generated with the method of training with the reference model, are lower than that in the paper. For example, the AP of phoning is 0.138 which is much lower than that in the paper. The scripts for training are as follows.     "
"Hi Georgia  and all, I train RstarCNN on my dataset, and get loss up and down(see below). I investigated on this for a quite long time, but cannot figure it out. Any Suggestions?  **Running parameters:** 1. System info: K80, Ubuntu 14.04, Cuda 7.5 2. using default cofiguration in `lib/fast-rcnn/config.py` 3. batch size: 256  4. I have shuffled training and testing sample list files to ensure that samples having the same true label distribute randomly.  **Solver prototxt file:**     **Output logs(a small part of snapshot since output is similar):**    "
"Hi,    I train a classifier with : `./tools/train_net.py --gpu 0 --solver models/VGG16_RstarCNN/solver.prototxt  --weights reference_models/VGG16.v2.caffemodel` and get an error. `F1007 19:40:40.235829 26752 syncedmem.cpp:66] Check failed: error == cudaSuccess (2 vs. 0)  out of memory *** Check failure stack trace: *** Aborted (core dumped) ` Where can I modify the 'batch_size'? "
"I'm learning the theory and code of the rstar. But I find the training times takes a little longer.  Is is possible to train the action recognition rstar model using multiple GPUs?  Is the code support this parallelization?  If so, how can i modify the code?  "
The trained model in your homepage is not unzippable. I used archive utility on Mac and tar command on Ubuntu but failed to unzip this file. Please check it.  
"Hi, dear gkioxari, your network setting is based on alexnet, however you used vgg16 as your reference_model. Have you tried fine-tuning some caffemodel of alexnet? "
"Hi gkioxari, I reproduced your code on UCF101 dataset, and finally I wrote all results to text file (use function adapted from `_write_voc_results_file` in lib/fast-rcnn/test.py). The line in those files looks like this:     My question is how to calculate the accuracy on whole dataset  using `score` in those files? I viewed all issues in this project but not found answer yet, so could you please  give me some hints? Thanks.  P.Sï¼š Maybe you can create a   in this project so we can discuss  more efficiently. "
"Recently, I run the test code using your caffe model on two datasets:  pascal voc 2012 validation set and stanford40 test set for action recongnition. I find that the APs on the two dataset i got are both lower than the AP reported in your paper: Contextual Action Recognition with R*CNN.  I use same images, same selective search rois, groundtruth roi and caffe model you provide. Are the models you provide in the github the same as the models you use in the paper? what's the AP you get using the model you provided in public?       : )  "
"Recently, I found the first boxes got from selective search is the image itself. So have you deleted the first box from selective search, or just use all the boxes got from selective search? "
"Hi, thanks for your wonderful work.  I've reproduced your work and I found that you have changed the snapshot writing part.  Then how can I plot the two loss?  From my deep heart, I know I just need to change the default tools/extra/plot_training_log.py.example file.  Can you share your script? "
I had following error. Is this because of I don't have openCV ?     
"Hi Georgia, I reviewed your code and find that the selective search region proposals are load from cached matlab files directly (155 line, in `RStarCNN_ROOT/lib/dataset/pascal_voc.py`). And as far as I know, the code of selective search (like in here:   doesn't give a matlab file output. So could you give me the code that generated matlab files of ss region because I want to use RStarCNN in another dataset,  thanks! "
"I am Sameh NEILI, a PhD student researcher in the field of artificial intelligence. I am very interested with your results with deep-learning. For this, I would reproduce with my own data, but  when I looked at this link code (  I found the 'action RCNN' part only. Is it possible to give me the code for the part of the pose estimation? I will be very grateful for your help. "
The size mentioned in   is misleading as the download size is more than 3.5 GB. 
"when I test a R_CNN classifier,I meet  with a problem :  [libprotobuf ERROR google/protobuf/text_format.cc:245] Error parsing text-format caffe.NetParameter: 401:21: Message type ""caffe.LayerParameter"" has no field named ""roi_pooling_param"". WARNING: Logging before InitGoogleLogging() is written to STDERR F1113 15:08:06.049188 13991 upgrade_proto.cpp:928] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: models/VGG16_RstarCNN/test.prototxt *_\* Check failure stack trace: *** Aborted (core dumped)  I had test with mnist dataset and it does not occur any error. I haven't any clue to solve this problem. "
"If anyone has this error when building the _caffe-fast-rcnn_  it can be fixed by adding to the Makefile.config at the line with _LIBRARIES += glog gflags protobuf leveldb snappy \ lmdb boost_system hdf5_hl hdf5 m \ opencv_core opencv_highgui opencv_imgproc_ add ""opencv_imgcodecs"" so it becomes: _LIBRARIES += glog gflags protobuf leveldb snappy \ lmdb boost_system hdf5_hl hdf5 m \ opencv_core opencv_highgui opencv_imgproc opencv_imgcodecs_  _make clean_ _make -j8 && make pycaffe_  reference:   "
"Thanks for your excellent work.  I want to test the model on a new image and I encounter an error when running the demo in SelectiveSearchCode.  <img width=""999"" alt=""Screen Shot 2021-09-22 at 11 09 32 AM"" src=""   "
">> compile_mex('/Developer/NVIDIA/CUDA-9.1')    root =        '/Users/macos/Desktop/CNNN/utils/cropRectanglesMex'    Hi    while trying to run compile_mex, I've been omit some semicolon to show the output just before the error  get this error    compileCmd =        '""/Developer/NVIDIA/CUDA-9.1/bin/nvcc"" -c cropRectanglesMex.cu -DNDEBUG -DENABLE_GPU -I""/Applications/MATLAB_R2017b.app/extern/include"" -I""/Applications/MATLAB_R2017b.app/toolbox/distcomp/gpu/extern/include"" -I""/Developer/NVIDIA/CUDA-9.1/include"" -I""/Developer/NVIDIA/CUDA-9.1/samples/7_CUDALibraries/common/UtilNPP"" -I""/Developer/NVIDIA/CUDA-9.1/samples/common/inc"" -Xcompiler -fPIC -o ""/Users/macos/Desktop/CNNN/utils/cropRectanglesMex/cropRectanglesMex.o""'    nvcc fatal   : The version ('90100') of the host compiler ('Apple clang') is not supported    ans =         1    Building with 'Xcode with Clang'.  Error using mex  clang: error: no such file or directory:  '/Users/macos/Desktop/CNNN/utils/cropRectanglesMex/cropRectanglesMex.o'      Error in build_cropRectanglesMex (line 38)  mex(mopts{:}) ;    Error in compile_mex (line 9)   build_cropRectanglesMex( cudaRoot );"
"hello, when I check the global model code, I find that the 22 layer of global model has 4096 nodes, but in the oquab model, there is only 2048 nodes, why double numbers of nodes? and how it come out is not find in the code also."
I got this error on Windows and solved it like this:  (file demo.m)   
"Hi  while trying to run ""demo_new_images"", ive got this error    Running Selective Search on 0 images  Generating imdb file results\new_data\imdb.mat  Reading annotation for 1 images  **Error using load  Unable to read file 'models\local.mat'. No such file or directory.**    Error in demo_new_images (line 83)  net = load( netFile, '-mat');    what should I do?     is it necessary to run demo file before if so, i've got this error for ""demo""  Error using readLines (line 14)  File E: can not be opened!    Error in demo (line 45)  test_set = readLines(sprintf(VOCopts.imgsetpath, 'test'));    thank you in advance"
"Hi, any chance to publish in pytorch or other ML framework. Is there any implementation for python wrappers."
"after some few change, I have compiled and get the cropRectanglesMex.mexw64 in Win7 + R2016a + CUDA7.5.  # the full code after changed is:  function build_cropRectanglesMex( cudaRoot,cudaSampleRoot)  %build_cropRectanglesMex builds package cropResizeMex  %  % INPUT:  %   cudaRoot - path to the CUDA installation  %   cudaSampleRoot-path to the CUDA sample    % Anton Osokin, firstname.lastname@gmail.com, March 2015    if ~exist('cudaRoot', 'var')      cudaRoot = '/usr/cuda-7.0' ;  end  nvccPath = fullfile(cudaRoot, 'bin', 'nvcc.exe');  if ~exist(nvccPath, 'file')      error('NVCC compiler was not found!');  end    root = fileparts( mfilename('fullpath') );    % compiling  compileCmd = [ '""', nvccPath, '""', ...          ' -c cropRectanglesMex.cu', ...           ' -DNDEBUG -DENABLE_GPU', ...          ' -I""', fullfile( matlabroot, 'extern', 'include'), '""', ...          ' -I""', fullfile( matlabroot, 'toolbox', 'distcomp', 'gpu', 'extern', 'include'), '""', ...          ' -I""', fullfile( cudaRoot, 'include'), '""', ...          ' -I""', fullfile( cudaSampleRoot, '7_CUDALibraries', 'common', 'UtilNPP'), '""', ...          ' -I""', fullfile( cudaSampleRoot, 'common', 'inc'), '""', ...          ' -Xcompiler', ' -fPIC', ...          ' -o ""', fullfile(root,'cropRectanglesMex.o'), '""'];  system( compileCmd );    % linking  mopts = {'-outdir', root, ...           '-output', 'cropRectanglesMex', ...           ['-L', fullfile(cudaRoot, 'lib','x64')], ...           '-lcudart', '-lnppi', '-lnppc', '-lgpu',... % '-lmwgpu',           fullfile(root,'cropRectanglesMex.o') };  mex(mopts{:}) ;  delete( fullfile(root,'cropRectanglesMex.o') );    # usage:  >>build_cropRectanglesMex('C:\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v7.5','C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v7.5')    "
None
"cnn_head_detection/utils/cropRectanglesMex/build_cropRectanglesMex.m,line 36.  windows7.0 or windows10,cuda7.0 or cuda8.0,can not find mwgpu.lib  Is the lib in linux cuda7.0 ?   "
"error: Image2OrientedGradients  error: BlobStructTextureHist  error: Image2HierarchicalGrouping (line 45)  textureHist = BlobStructTextureHist(blobIndIm, colourIm);   i can't open BlobStructTextureHist.p"
thank you for sharing your code. but i am confused about wheather you train the local and global model separately or not ~~ 
$ python ./src/train.py  cnn/ccnn/src/ccnn.py:19: RuntimeWarning: to-Python converter for std::vector  > already registered; second conversion method ignored.    from python.ccnn import *  F0725 15:37:23.709703  4820 db.cpp:18] Check failed: status.ok() Failed to open leveldb /mnt/a/pathak/fcn_mil_cache/VOC2012/images_train_lmdb  IO error: /mnt/a/pathak/fcn_mil_cache/VOC2012/images_train_lmdb/LOCK: No such file or directory  *** Check failure stack trace: ***      @     0x7f0e98f9e04d  google::LogMessage::Fail()      @     0x7f0e98fa0603  google::LogMessage::SendToLog()      @     0x7f0e98f9dbdb  google::LogMessage::Flush()      @     0x7f0e98f9f54e  google::LogMessageFatal::~LogMessageFatal()      @     0x7f0e99320e23  caffe::db::LevelDB::Open()      @     0x7f0e9937dfb6  caffe::DataLayer ::DataLayerSetUp()      @     0x7f0e9938e882  caffe::BaseDataLayer ::LayerSetUp()      @     0x7f0e9938e919  caffe::BasePrefetchingDataLayer ::LayerSetUp()      @     0x7f0e99367e99  caffe::Net ::Init()      @     0x7f0e9936a05e  caffe::Net ::Net()      @     0x7f0e993000a5  caffe::Solver ::InitTrainNet()      @     0x7f0e9930138e  caffe::Solver ::Init()      @     0x7f0e99301556  caffe::Solver ::Solver()      @     0x7f0e9992edf0  caffe::GetSolver ()      @     0x7f0e998fc7d3  caffe::GetSolverFromString()      @     0x7f0e999086b8  boost::python::objects::caller_py_function_impl ::operator()()      @     0x7f0e987175cd  boost::python::objects::function::call()      @     0x7f0e987177c8  (unknown)      @     0x7f0e9871f823  boost::python::detail::exception_handler::operator()()      @     0x7f0e3a8a8d20  boost::python::detail::translate_exception ::operator()()      @     0x7f0e3a884f3d  boost::_bi::list3 ::operator() ()      @     0x7f0e3a871ec7  boost::_bi::bind_t ::operator() ()      @     0x7f0e3a86b268  boost::detail::function::function_obj_invoker2 ::invoke()      @     0x7f0e9871f5dd  boost::python::handle_exception_impl()      @     0x7f0e98714999  (unknown)      @     0x7f0e9b4567a3  PyObject_Call      @     0x7f0e9b4ecb69  PyEval_EvalFrameEx      @     0x7f0e9b4f24e9  PyEval_EvalCodeEx      @     0x7f0e9b4f270a  PyEval_EvalCode      @     0x7f0e9b50b9cd  run_mod      @     0x7f0e9b50cb48  PyRun_FileExFlags      @     0x7f0e9b50dd68  PyRun_SimpleFileExFlags  å·²æ”¾å¼ƒ (æ ¸å¿ƒå·²è½¬å‚¨)  è¯·é—®è¿™ä¸ªé—®é¢˜ä»€ä¹ˆæ„æ€ï¼Ÿè¯¥æ€Žä¹ˆè§£å†³å•Šï¼Ÿç€æ€¥ï¼Œåœ¨çº¿ç­‰ï¼ï¼ï¼
"Hi,     I am trying to reproduce your work. During this period, sadly, I failed to load python.caffe in ccnn.py. It seems that this lib cannot be found. Besides, I am confused about how you implement the latent distribution optimisation.    Looking forward to your reply. Thank you~"
"Hi,  I am trying to re-generate the numbers reported in your ICCV paper from scratch. Everything works fine when I use the pre-trained models. But when I train the models myself they seem to classify all pixels to the background. The only files I generated myself are LMDBs using the provided code (/extras/generate_lmdb.py).   Can you please also tell your loss value at 35,000 iterations?   Here is the snapshot of last 100 iterations in my case:  I0601 04:28:17.000000 34992 solver.cpp:489] Iteration 34700, lr = 1e-06 I0601 04:28:33.000000 34992 solver.cpp:214] Iteration 34720, loss = 0.0137566 I0601 04:28:33.000000 34992 solver.cpp:229]     Train net output #0: loss = 0.0137567 (\* 1 = 0.0137567 loss) I0601 04:28:33.000000 34992 solver.cpp:489] Iteration 34720, lr = 1e-06 I0601 04:28:53.000000 34992 solver.cpp:214] Iteration 34740, loss = 0.0162213 I0601 04:28:53.000000 34992 solver.cpp:229]     Train net output #0: loss = 0.0162215 (\* 1 = 0.0162215 loss) I0601 04:28:53.000000 34992 solver.cpp:489] Iteration 34740, lr = 1e-06 I0601 04:29:11.000000 34992 solver.cpp:214] Iteration 34760, loss = 0.0857187 I0601 04:29:11.000000 34992 solver.cpp:229]     Train net output #0: loss = 0.0857188 (\* 1 = 0.0857188 loss) I0601 04:29:11.000000 34992 solver.cpp:489] Iteration 34760, lr = 1e-06 I0601 04:29:29.000000 34992 solver.cpp:214] Iteration 34780, loss = 0.365484 I0601 04:29:29.000000 34992 solver.cpp:229]     Train net output #0: loss = 0.365484 (\* 1 = 0.365484 loss) I0601 04:29:29.000000 34992 solver.cpp:489] Iteration 34780, lr = 1e-06 I0601 04:29:47.000000 34992 solver.cpp:214] Iteration 34800, loss = 0.427323 I0601 04:29:47.000000 34992 solver.cpp:229]     Train net output #0: loss = 0.427323 (\* 1 = 0.427323 loss) I0601 04:29:47.000000 34992 solver.cpp:489] Iteration 34800, lr = 1e-06 I0601 04:30:05.000000 34992 solver.cpp:214] Iteration 34820, loss = 0.272257 I0601 04:30:05.000000 34992 solver.cpp:229]     Train net output #0: loss = 0.272258 (\* 1 = 0.272258 loss) I0601 04:30:05.000000 34992 solver.cpp:489] Iteration 34820, lr = 1e-06 I0601 04:30:23.000000 34992 solver.cpp:214] Iteration 34840, loss = 0.0660284 I0601 04:30:23.000000 34992 solver.cpp:229]     Train net output #0: loss = 0.0660286 (\* 1 = 0.0660286 loss) I0601 04:30:23.000000 34992 solver.cpp:489] Iteration 34840, lr = 1e-06 I0601 04:30:40.000000 34992 solver.cpp:214] Iteration 34860, loss = 0.696421 I0601 04:30:40.000000 34992 solver.cpp:229]     Train net output #0: loss = 0.696421 (\* 1 = 0.696421 loss) I0601 04:30:40.000000 34992 solver.cpp:489] Iteration 34860, lr = 1e-06 I0601 04:30:58.000000 34992 solver.cpp:214] Iteration 34880, loss = 0.096779 I0601 04:30:58.000000 34992 solver.cpp:229]     Train net output #0: loss = 0.0967792 (\* 1 = 0.0967792 loss) I0601 04:30:58.000000 34992 solver.cpp:489] Iteration 34880, lr = 1e-06 I0601 04:31:16.000000 34992 solver.cpp:214] Iteration 34900, loss = 0.216992 I0601 04:31:16.000000 34992 solver.cpp:229]     Train net output #0: loss = 0.216992 (\* 1 = 0.216992 loss) I0601 04:31:16.000000 34992 solver.cpp:489] Iteration 34900, lr = 1e-06 I0601 04:31:33.000000 34992 solver.cpp:214] Iteration 34920, loss = 0.047195 I0601 04:31:33.000000 34992 solver.cpp:229]     Train net output #0: loss = 0.0471952 (\* 1 = 0.0471952 loss) I0601 04:31:33.000000 34992 solver.cpp:489] Iteration 34920, lr = 1e-06 I0601 04:31:51.000000 34992 solver.cpp:214] Iteration 34940, loss = 0.32279 I0601 04:31:51.000000 34992 solver.cpp:229]     Train net output #0: loss = 0.32279 (\* 1 = 0.32279 loss) I0601 04:31:51.000000 34992 solver.cpp:489] Iteration 34940, lr = 1e-06 I0601 04:32:08.000000 34992 solver.cpp:214] Iteration 34960, loss = 0.0153671 I0601 04:32:08.000000 34992 solver.cpp:229]     Train net output #0: loss = 0.0153672 (\* 1 = 0.0153672 loss) I0601 04:32:08.000000 34992 solver.cpp:489] Iteration 34960, lr = 1e-06 I0601 04:32:26.000000 34992 solver.cpp:214] Iteration 34980, loss = 0.0963567 I0601 04:32:26.000000 34992 solver.cpp:229]     Train net output #0: loss = 0.0963568 (\* 1 = 0.0963568 loss) I0601 04:32:26.000000 34992 solver.cpp:489] Iteration 34980, lr = 1e-06 35000 iterations t = 892.211614132  Any help is welcome.  Thanks!  "
"Hi @YangZhang4065 ,  Im using create_vgg16_FCN to create FCN model , but on calling the function i'm facing following error :     Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d_21/MaxPool}} = MaxPool ' with input shapes: [?,1,160,128].    I have used padding ='same' on maxpooling in create_vgg16_FCN function , then above error does not comes , but while loading the pretrained weights from Synthia_FCA , i facing incompatible dimensions issue .    Kindly let me know If m missing something ."
"Hi , Im not able to find SYNTHIA_FCN.h5 file you provided in the url . Kindly check."
Can you share the codeï¼Œpleaseï¼Ÿ
Do you have pretrained model for your final version of model ((CC+I+SP) of TPAMI paper) or for any of them?  Thank you. 
"Hello @YangZhang4065 ,  Thanks for the nice work. I'm trying to train model to estimate the global label distribution. Can you give more details of the architecture and training scheme?  Thanks! "
"Thanks for your great help first, I wonder how to obtain the gamut-based color  constancy method in the Paper. I have searched for a long time, but I can not find it. Can you share me the code about color constancy."
Is the code for source-only training available?
"Hi,    I tried to run the code, but the training does not proceed even for one iteration. Actually, it does nothing after printing:      And starts consuming memory.  Any idea on that?    Thanks."
"I'm sorry, but I just can't find where to download the datasheet.Please give me some instruction."
"train_val_FCN_DA.py:155: RuntimeWarning: divide by zero encountered in divide    SP_weight=avg_pixel_number/SP_pixelperSP_num     1/4543 [..............................] - ETA: 7:04:32 - loss: 1.4686 - output_loss: 1.249   2/4543 [..............................] - ETA: 5:56:23 - loss: 1.2655 - output_loss: 1.076   3/4543 [..............................] - ETA: 6:04:32 - loss: 1.2219 - output_loss: 1.064   4/4543 [..............................] - ETA: 7:24:51 - loss: 1.0931 - output_loss: 0.937   5/4543 [..............................] - ETA: 8:40:20 - loss: 1.1262 - output_loss: 0.974   6/4543 [..............................] - ETA: 10:07:33 - loss: 1.0875 - output_loss: 0.94   7/4543 [..............................] - ETA: 11:10:59 - loss: 1.0446 - output_loss: 0.90   8/4543 [..............................] - ETA: 11:22:12 - loss: 0.9914 - output_loss: 0.85   9/4543 [..............................] - ETA: 11:30:05 - loss: 0.9571 - output_loss: 0.82  10/4543 [..............................] - ETA: 11:22:53 - loss: 0.9300 - output_loss: 0.80  11/4543 [..............................] - ETA: 11:24:29 - loss: nan - output_loss: nan - o  12/4543 [..............................] - ETA: 11:17:30 - loss: nan - output_loss: nan - o    After I manually modify the output_shape, otherwise it will lead to the dimension dismatch problem.    out=Lambda(lambda x:x+0., name='output', output_shape=(class_num + 1,nb_rows,nb_cols))(output)      out_2=Lambda(lambda x:x+0., name='output_2', output_shape=(class_num ,1,1))(output)  "
"  I manually check the size of using ""print loaded_im.shape, loaded_label.shape, loaded_target_obj_pre.shape"", it looks good. Have no idea why it crashed. Any idea?  File ""train_val_FCN_DA.py"", line 207, in        seg_model.fit_generator(myGenerator(),callbacks=[Validate_on_CityScape()], steps_per_epoch=steps_per_epoch, epochs=60)    File ""build/bdist.linux-x86_64/egg/keras/legacy/interfaces.py"", line 87, in wrapper    File ""build/bdist.linux-x86_64/egg/keras/engine/training.py"", line 2110, in fit_generator    File ""build/bdist.linux-x86_64/egg/keras/callbacks.py"", line 85, in on_batch_begin    File ""train_val_FCN_DA.py"", line 193, in on_batch_begin      current_predicted_val=self.model.predict(loaded_val_im,batch_size=batch_size)    File ""build/bdist.linux-x86_64/egg/keras/engine/training.py"", line 1765, in predict    File ""build/bdist.linux-x86_64/egg/keras/engine/training.py"", line 153, in _standardize_input_data  ValueError: Error when checking : expected input_1 to have shape (None, 3, 320, 640) but got array with shape (1, 3, 1, 0)  "
"Thanks for your remarkable work! But I have a question:   What's the superpixel loss? Does it mean the segmentation loss on the target image, or label distributions over local landmark superpixels? But how to produce label distributions over local landmark superpixels?  Looking forward to your answer!Thanks!"
"I did set data path folder and .h5 file.  When I ran train_val_FCN_DA.py code , I got this error.      Using Theano backend.  /home/tf/anaconda2/lib/python2.7/site-packages/keras/layers/core.py:633: UserWarning: `output_shape` argument not specified for layer output and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 22, 320, 640)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.    .format(self.name, input_shape))  /home/tf/anaconda2/lib/python2.7/site-packages/keras/layers/core.py:633: UserWarning: `output_shape` argument not specified for layer output_2 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 22, 320, 640)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.    .format(self.name, input_shape))  Start loading files  Start training  Epoch 1/60  Exception in thread Thread-1:  Traceback (most recent call last):    File ""/home/tf/anaconda2/lib/python2.7/threading.py"", line 801, in __bootstrap_inner      self.run()    File ""/home/tf/anaconda2/lib/python2.7/threading.py"", line 754, in run      self.__target(*self.__args, **self.__kwargs)    File ""/home/tf/anaconda2/lib/python2.7/site-packages/keras/engine/training.py"", line 612, in data_generator_task      generator_output = next(self._generator)    File ""train_val_FCN_DA.py"", line 144, in myGenerator      tar_idx=sample(range(len(cityscape_im_generator)),target_batch_size)    File ""/home/tf/anaconda2/lib/python2.7/random.py"", line 323, in sample      raise ValueError(""sample larger than population"")  ValueError: sample larger than population    Traceback (most recent call last):    File ""train_val_FCN_DA.py"", line 203, in        seg_model.fit_generator(myGenerator(),callbacks=[Validate_on_CityScape()], steps_per_epoch=steps_per_epoch, epochs=60)    File ""/home/tf/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.py"", line 88, in wrapper      return func(*args, **kwargs)    File ""/home/tf/anaconda2/lib/python2.7/site-packages/keras/engine/training.py"", line 1877, in fit_generator      str(generator_output))  ValueError: output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: None  "
"Hi, Nice work! I have a query regarding your evaluation.     In your paper you have mentioned that: "" Since we have to resize the images before feeding them to the segmentation network, we resize the output segmentation mask back to the original image size before running the evaluation against the groundtruth annotations. ""    However, in warp_data.py which is called by your eval. code, it seems like the label is also resized to (320,640).     Can you please clarify this inconsistency for me ? Thanks!      "
"Dear Sir/Madam,    I was executing your test_FCN_DA.py . In line no:30, i found an exception @ seg_model.load_weights('SYNTHIA_FCN.h5') because i don't from where i had to get SYNTHIA_FCN.h5.   **Kindly suggest**"
"Hi @YangZhang4065 ,  Thank you so much for your study. I could train and evaluate your code but I couldn't achieve class wise evaluation IoUs with `test_FCN_DA.py`. How could I get these metrics ? Thank you."
"hi thanks for your work and effort, but i could not hit the link for superpixel landmark and label distribution. could you check and update the link ?"
"Hi, thanks for your great work! It's really impressive that you do the segmentation translation from the virtual world to the real world scene. But when I run your code, I have a little problem with your `image_mean` type. It seems that you loaded images in RGB (  and subtracted image mean value `[103.939, 116.779, 123.68]`  from them (      However, in the VGG network, they used image mean value `[103.939, 116.779, 123.68]` in BGR, not RGB (  I just wondered that why you do the operation of RGB-images with BGR-image_mean. (Maybe your SYNTHIA images are saved in BGR type before loading into the model?)  I would be very grateful if you could resolve my doubt.   Thank you so much. :)"
"Hi Yang,    The code of your paper is elegant and easy to reproduce. But where can I find the code to train the multi-class SVM that yields the superpixel annotations of the target domain? Thanks!"
"Hi Yang Zhang,    How are you doing?    I am wondering if the link that in the following instruction is supposed to point so other web page? Current it is pointing to the Pillow installation page.     1, Download leftImg8bit_trainvaltest.zip and leftImg8bit_trainextra.zip in CityScape dataset  . (Require registration)    I am new to Pillow, so I don't know if it is possible to down load dataset with it.     Thank you,  Heng     "
"hello YangZhang  When I ran train_val_FCN_DA.py code , I got this error.  gnss@gnss:/media/gnss/æ–‡æ¡£/mask/AdaptationSeg-master$ python train_val_FCN_DA.pyUsing Theano backend.  Traceback (most recent call last):    File ""train_val_FCN_DA.py"", line 14, in        from city_meanIU import city_meanIU  ImportError: No module named city_meanIU  "
"Hi everyone,   How long does DSOD take when I have 20 000 images for 1 class  My GPU is Quadro P4000, Computational Capacity = 6.1"
Could you provide a Dockerfile? Does anyone have a Dockerfile?    
None
"Hello,    First of all I want to say thank you for releasing the code.  Can you please tell me if I can train with my own custom dataset?  Because it is not clear to me.    Thank you   "
"Thx for your sharing code of grp-dsod.I read the code and I find that result after relu function isn't used in this part.          def global_level(net, from_layer, relu_name):          fc = L.InnerProduct(net[relu_name], num_output=1)          sigmoid = L.Sigmoid(fc, in_place=True)          att_name = ""{}_att"".format(from_layer)          sigmoid = L.Reshape(sigmoid, reshape_param=dict(shape=dict(dim=[-1])))          scale = L.Scale(net[att_name], sigmoid, axis=0, bias_term=False, bias_filler=dict(value=0))          relu = L.ReLU(scale, in_place=True)          residual = L.Eltwise(net[from_layer], scale)          gatt_name = ""{}_gate"".format(from_layer)          net[gatt_name] = residual          return net  **relu = L.ReLU(scale, in_place=True)**  Is it a mistake?Or,is it discarded?"
"Hi, @szq0214. Sorry for bothering you again. Can you tell me what I should change to test on VOC2012, the default is 2007."
"Hi, @szq0214:  I only have two GTX 1080 GPUs. I want to reproduce you GRP-DSOD. When I change the batch_size and accum_batch_size to 6 and 30, the mAP is just 63%. What I should do to get the results as you paper?  Thanks."
"Hi,  I would like to learn from my own dataset composed of only gray level images. Could you tell me how I could adapt DSOD to work using only 1 channel. Thanks !!!"
"I want to use TensorFlow to implement this DSOS, I try to write, but some mistakes. So are there any TensorFlow versionsï¼Ÿ  Thanks"
"Hey guys!     is it also possible to do Video Detection with this model, like in the SSD which is implemented by Wei Liu?     Best Wishes "
"Do some people implement other version, eg. mxnet?  Thanks"
"@szq0214   Hi!  I trained the DSOD by using my datasets,but time is 10 times the time of the SSD.  Why DSOD is so slow?"
"When Iâ€˜m training a DSOD model on VOC 07+12 by `python examples/dsod/DSOD300_pascal.py`ï¼ŒI encounter     > Traceback (most recent call last):  File â€œexamples/dsod/DSOD300_pascal.pyâ€, line 380, in   DSOD300_V3_Body(net, from_layer=â€˜dataâ€™)  NameError: name â€˜DSOD300_V3_Bodyâ€™ is not defined    What should I do to deal with it?  Thank you~"
"Hi,     I tried to train DSOD300 using   on 8 TITAN Xp GPU.    But the result is 76.94% which is lower than the reported result(77.7%).    I couldn't find out why this problem occurs.     Could anyone face this problem?    !    "
May be you can supported grp-dsod pretrained mdoles like dsod-300
"Hi, with your changes to the SSD model, the last layer has 2x2 spatial size, not 1x1 anymore. This stems from the fact that the last 3Ã—3Ã—128 conv layer has padding 1 and also the parallel pooling branch, having kernel size 2, will output a 2x2 feature, instead of 1x1. You can double check this by reading Caffe's code of conv_layer.cpp:    `const int output_dim = (input_dim + 2 * pad_data[i] - kernel_extent) / stride_data[i] + 1;`    output_dim = (3 + 2 * 1 - 3) / 2 + 1 = 2 / 2 + 1 = 1 + 1 = 2    Also, Caffe's output reflects this:       Given this, I think the step size in the Sixth_norm_mbox_priorbox should be 150 (= 300/2) instead of 300 (=300/1).    EDIT: I should also point out that I have made NO modification whatsoever to the source code."
I am getting the following message when running train command  python examples/dsod/DSOD300_pascal.py    32554 detection_output_layer.cu:113] Couldn't find any detections  32554 detection_output_layer.cu:113] Couldn't find any detections  32554 detection_output_layer.cu:113] Couldn't find any detections  32554 detection_output_layer.cu:113] Couldn't find any detections  32554 detection_output_layer.cu:113] Couldn't find any detections  
I am getting the following message when running train command    python examples/dsod/DSOD300_pascal.py    I0312 18:13:06.186707 31109 layer_factory.hpp:77] Creating layer data  I0312 18:13:06.186813 31109 net.cpp:100] Creating Layer data  I0312 18:13:06.186830 31109 net.cpp:408] data -> data  I0312 18:13:06.186846 31109 net.cpp:408] data -> label  F0312 18:13:06.189031 31210 db_lmdb.hpp:15] Check failed: mdb_status == 0 (2 vs. 0) No such file or directory  *** Check failure stack trace: ***      @     0x7f8d26f785cd  google::LogMessage::Fail()      @     0x7f8d26f7a433  google::LogMessage::SendToLog()      @     0x7f8d26f7815b  google::LogMessage::Flush()      @     0x7f8d26f7ae1e  google::LogMessageFatal::~LogMessageFatal()      @     0x7f8d2784b770  caffe::db::LMDB::Open()      @     0x7f8d2768a396  caffe::DataReader ::Body::InternalThreadEntry()      @     0x7f8d2767c465  caffe::InternalThread::entry()      @     0x7f8d1cc4f5d5  (unknown)      @     0x7f8d159ee6ba  start_thread      @     0x7f8d25fcf3dd  clone      @              (nil)  (unknown)  Aborted (core dumped)    Any help would be appreciated. All steps before running the training command were successful.
"Thx for your sharing code.And I want to make a re-implementation of this net with other framwork.But the definition of pooling layer is different from which in caffe.    In caffe,I think the funtion of size of output is a ceil function as shown in most of your code.But in the final,I don't know why it become a floor function.    I mean that the process should be     > 300x300â†’150x150â†’75x75â†’38x38â†’19x19â†’10x10â†’5x5â†’3x3â†’2x2    But in your code,            model2 = add_bl_layer2(model1, 256, dropout, 1) # pooling4: 10x10      net.Third = model2      model3 = add_bl_layer2(model2, 128, dropout, 1) # pooling5: 5x5      net.Fourth = model3      model4 = add_bl_layer2(model3, 128, dropout, 1) # pooling6: 3x3      net.Fifth = model4      model5 = add_bl_layer2(model4, 128, dropout, 1) # pooling7: 1x1    I don't know why 3x3â†’1x1.Could you give me some suggestion?"
"First of all, the idea of training from scratch is awesome. I have a question. Have you tried large input images (600 * 1000)?"
"I copy to my all test files to a path , and i want to batch test this image files,and to get the annotations of test images files ?Could you tell me a methodï¼Ÿ"
I want to know how to prepare voc12 test lmdb to run training on the voc07++12 dataset. Anyone can help me? thanks a lot.
"the last pooling size is not inconsistent with paper.  in the paper:  the size of the pooling7 feature map = 1\*1  in model_libs.py:  the size of the pooling7 feature map = 2\*2    Also, DSOD prediction layers differ from the figure 1 in paper."
How long is your training time based on one TitanX GPU or 8 GPUs?
"I have download the DSOD_voc+coco model and modify the corresponding prototxt according to the video test in SSD project. While it works well in SSD project, the test failed when setting up the DSOD network, throwing the following error:     And here is the modified part in DSOD prototxt: (I mainly modify the input layer and detection output layer according to the SSD settings)  The input layer is:     And the detection layer is:     interestingly, when I close the visualize process by setting visualize: false, the network could work well but I can't tell if the result is right without visualize video. I wonder if anyone met the same problem like this and how do you deal with it?"
"Hi,    Recently, I tried to train DSOD512 version which follows origin-SSD512 settings except for backbone dsod.    But, the accuracy was not good as dsod300.     Have you tried to train dsod512?     Thanks :)"
"It seems your model graph is inconsistent with the paper (Table1 Output Size) for the Transition w/o Pooling Layer (1+2)  In the paper:  Transition w/o Pooling Layer (1) channel = 1120  Transition w/o Pooling Layer (2) channel = 1568  In the model graph:  Convolution49 num output = 1184  Convolution66 num output = 256     Also, I don't quite understand of the purpose of Transition w/o Pooling Layer (1), you don't actually compress nor expand its filter number (num input = num output), and you don't branch it out for prediction. By removing it (Convolution49 + BN 50 + ReLU50) you would have a compact Dense Block (3+4) with 8 x 2 = 16 dense layers. So what's the reason to explicitly inject such extra (BN+ReLU+1x1Conv) block in between?  "
"Hi,    I want to know how to measure the inference time?     Did you use caffe time operator ? or Did you measure full time when VOC 4952 test images are tested ?    Thanks in advance :)"
@szq0214 @liuzhuang13  When I'm trying test the videoï¼Œthe DSOD300 Occupy GPU memory is 1500M ï¼Œjust like SSD_300_ResNet101 . I have tried the official optimized version of the ï¼ŒGPU memory footprint is not particularly seriousï¼ŒIs this a problem with version optimization?
@szq0214 @liuzhuang13 
"Hi, I was trying to fine tune a pre-trained model with my dataset and I need to change number of classes from 21 to 2. So I planed to modify the python script instead of making it in prototxt files. But I found the model python script created is about ""28.6M"", which is different from anyone offered in this repository. If I want to train a model with 2 classes, I should train it without pre-trained model?    Many thanks!"
@szq0214   what is the difference between DSOD300_pascal and DSOD300_pascal++?
"I use my own data,but it reports that check failed :mean_values_.size() == 1 || mean_values_.size() == img_channels Specify either 1 mean_value or as many as channels: 1  Could you help me ?"
None
"SSD300Sâ€  07+12 âœ— VGGNet Plain 46 26.3M 300 Ã—300 69.6  SSD300Sâ€  07+12 âœ— VGGNet Dense 37 26.0M 300 Ã—300 70.4    in the table 4 of your paper,  Dense-ssd seems to be no advantage with VGG-ssd.   similar precision but slower  "
"I can train with only 6 batch size on my single TITAN X (Pascal) without ""out of memory"". So what is your trick to overcome the GPU memory constraints in the paper?   Thank you~~"
"hi @szq0214,  will you release the training code?    thanks."
"When I run` python DSOD300_pascal.py`, I get many information like: I0809 19:00:06.018213  8332 detection_output_layer.cu:113] Couldn't find any detections.  What should I do?"
"Hi,     thank you for your job.It is great.    Some question:   1,the pretrained model can  not detect the small object .   2. is it great than the RON?    thank you"
"Any python script to run detection test with one input image using one of your pretrained models?    Thanks,"
"When I run DSOD300_pascal.py, there is an error: No module named model_libs? So, what can I do?"
"Or do you mean when training from scratch , performance is much worse than fine-tuning ?    And tks for sharing the code."
Hi     Just wonder how did you produce  VOC0712Plus_test_lmdb  We could download images for official website but not VOC12 annotations.  How did you compute the VOC12 mAP outside the evaluation platform without annotation?
None
"Hi, Jiangtao.    I was trying to run your example code of training on imagenet from scratch on Ubuntu 14.04, MATLAB R2017a, CUDA8.0 and cudnn v4. But I have encountered the following error when I run `MPN_COV_main` in MATLAB R2017a:     Could you tell me whether you used `cudnn` library when you trained your model, and if you used, which version you used?"
     So will this error influence the model and why it occured? Thank u!
"Hi Seilna    I've being trying to implement your RWMN. However, when I tried to use video_sub features, the required compact_bilinear_pooling module is missing. So may I ask if it is possible to provide the compact_biliear_pooling as well in the repository?    Thanks very much!"
None
"mldl@mldlUB1604:~/ub16_prj/RWMN$ python train.py   2018-01-16 22:21:50.274948: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.  2018-01-16 22:21:50.274975: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.  2018-01-16 22:21:50.274983: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.  2018-01-16 22:21:50.274989: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.  2018-01-16 22:21:50.274995: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.  2018-01-16 22:21:50.351769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2018-01-16 22:21:50.352033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties:   name: GeForce GTX 950M  major: 5 minor: 0 memoryClockRate (GHz) 1.124  pciBusID 0000:01:00.0  Total memory: 3.95GiB  Free memory: 2.76GiB  2018-01-16 22:21:50.352051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0   2018-01-16 22:21:50.352057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y   2018-01-16 22:21:50.352064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 950M, pci bus id: 0000:01:00.0)  ---------  Variables: name (type shape) [size]  ---------  Total size of variables: 0  Total bytes of variables: 0  memory_write/query_w:0  memory_write/query_b:0  Exception in thread Thread-7:  Traceback (most recent call last):    File ""/usr/lib/python2.7/threading.py"", line 801, in __bootstrap_inner      self.run()    File ""/usr/lib/python2.7/threading.py"", line 754, in run      self.__target(*self.__args, **self.__kwargs)    File ""/home/mldl/ub16_prj/RWMN/custom_input_ops.py"", line 80, in thread_main      for mini_batch in self.iterator():    File ""/home/mldl/ub16_prj/RWMN/custom_input_ops.py"", line 50, in iterator      movie_index = np.random.choice(self.num_movie, 1)    File ""mtrand.pyx"", line 1104, in mtrand.RandomState.choice (numpy/random/mtrand/mtrand.c:17062)  ValueError: a must be greater than 0    "
Could you release your features & training config to reproduce results?  Thanks!
None
"Luo, Hao, Wenxuan Xie, Xinggang Wang, andWenjun Zeng. ""Detect or Track: Towards Cost-Effective Video Object Detection/Tracking"". AAAI, 2019, 8.    Is this also your work. I will be appreciate if you can public the code for this article. I am looking for an efficient multiple object tracking algorithm.   "
"I can't open the link to download the trained model .I am in China ,angbody has the same problem?    "
"Hello,     how can I test this on my own video to see the performance? Thanks  "
"when deploy the model, no groundtruth, so no trackids, so how to compute   in the test network, use all of the rois?  "
Hello~ How can I get teh meta_vid.mat which used by imdb_from_ilsvrc15vid.m?
"Hello,  is there any one who is succeeded in testing the model using pre-trained model on the CPU? If yes can you please share the process to do it??"
"F0719 13:18:41.276878  6182 correlation_layer.cpp:89] Not Implemented Yet  *** Check failure stack trace: ***    --------------------------------------------------------------------------------           Illegal instruction detected at Thu Jul 19 13:18:43 2018 +0530  --------------------------------------------------------------------------------    Configuration:    Crash Decoding           : Disabled - No sandbox or build area path    Crash Mode               : continue (default)    Default Encoding         : UTF-8    Deployed                 : false    Desktop Environment      : Unity    GNU C Library            : 2.23 stable    Graphics Driver          : Unknown hardware     Java Version             : Java 1.8.0_144-b01 with Oracle Corporation Java HotSpot(TM) 64-Bit Server VM mixed mode    MATLAB Architecture      : glnxa64    MATLAB Entitlement ID    : 5221413    MATLAB Root              : /usr/local/MATLAB/R2018a    MATLAB Version           : 9.4.0.813654 (R2018a)    OpenGL                   : hardware    Operating System         : Ubuntu 16.04.4 LTS    Process ID               : 6122    Processor ID             : x86 Family 6 Model 142 Stepping 9, GenuineIntel    Session Key              : 6d05ce2d-fa08-443f-9dd7-94ebfc528a13    Static TLS mitigation    : Enabled: Full    Window System            : The X.Org Foundation (11906000), display :0    Fault Count: 1      Abnormal termination    Register State (from fault):    RAX = 0000000000000001  RBX = 0000000022e43660    RCX = 00007f7edd85d788  RDX = 00007f7e891fe6a0    RSP = 00007f7edd85d770  RBP = 0000000014161c00    RSI = 0000000000000000  RDI = 0000000000000000       R8 = 0000000000000081   R9 = 0000000000000000    R10 = 00007f7f0035d650  R11 = 00007f7e88fdc15b    R12 = 000000000000017b  R13 = 0000000000002388    R14 = 00007f7edd85d800  R15 = 0000000009198b88      RIP = 00007f7effb6dedc  EFL = 0000000000010246       CS = 0033   FS = 0000   GS = 0000    Stack Trace (from fault):  [  0] 0x00007f7effb6dedc              /lib/x86_64-linux-gnu/libpthread.so.0+00052956 pthread_rwlock_unlock+00000044  [  1] 0x00007f7e88fe3789             /usr/lib/x86_64-linux-gnu/libglog.so.0+00075657 _ZN24glog_internal_namespace_5Mutex12ReaderUnlockEv+00000025  [  2] 0x00007f7e88fdc360             /usr/lib/x86_64-linux-gnu/libglog.so.0+00045920 _ZN6google10LogMessage5FlushEv+00000704  [  3] 0x00007f7e88fdee1e             /usr/lib/x86_64-linux-gnu/libglog.so.0+00056862 _ZN6google15LogMessageFatalD2Ev+00000014  [  4] 0x00007f7e893bd400 /home/narendrachintala/git/caffe-rfcn/matlab/+caffe/private/caffe_.mexa64+01823744  [  5] 0x00007f7e893da2b2 /home/narendrachintala/git/caffe-rfcn/matlab/+caffe/private/caffe_.mexa64+01942194  [  6] 0x00007f7e893da4f6 /home/narendrachintala/git/caffe-rfcn/matlab/+caffe/private/caffe_.mexa64+01942774  [  7] 0x00007f7e89241c5f /home/narendrachintala/git/caffe-rfcn/matlab/+caffe/private/caffe_.mexa64+00269407  [  8] 0x00007f7e8924293f /home/narendrachintala/git/caffe-rfcn/matlab/+caffe/private/caffe_.mexa64+00272703 mexFunction+00000163  [  9] 0x00007f7eeb090080                              bin/glnxa64/libmex.so+00413824  [ 10] 0x00007f7eeb090447                              bin/glnxa64/libmex.so+00414791  [ 11] 0x00007f7eeb090f2b                              bin/glnxa64/libmex.so+00417579  [ 12] 0x00007f7eeb07b30c                              bin/glnxa64/libmex.so+00328460  [ 13] 0x00007f7eece842ad                   bin/glnxa64/libmwm_dispatcher.so+00979629 _ZN8Mfh_file16dispatch_fh_implEMS_FviPP11mxArray_tagiS2_EiS2_iS2_+00000829  [ 14] 0x00007f7eece84bae                   bin/glnxa64/libmwm_dispatcher.so+00981934 _ZN8Mfh_file11dispatch_fhEiPP11mxArray_tagiS2_+00000030  [ 15] 0x00007f7ee922cda1                          bin/glnxa64/libmwm_lxe.so+12619169  [ 16] 0x00007f7ee922d982                          bin/glnxa64/libmwm_lxe.so+12622210  [ 17] 0x00007f7ee9315fc9                          bin/glnxa64/libmwm_lxe.so+13574089  [ 18] 0x00007f7ee92b7431                          bin/glnxa64/libmwm_lxe.so+13186097  [ 19] 0x00007f7ee8abd5a8                          bin/glnxa64/libmwm_lxe.so+04822440  [ 20] 0x00007f7ee8abfcbc                          bin/glnxa64/libmwm_lxe.so+04832444  [ 21] 0x00007f7ee8abc01d                          bin/glnxa64/libmwm_lxe.so+04816925  [ 22] 0x00007f7ee8ab5ba1                          bin/glnxa64/libmwm_lxe.so+04791201  [ 23] 0x00007f7ee8ab5dd9                          bin/glnxa64/libmwm_lxe.so+04791769  [ 24] 0x00007f7ee8abb846                          bin/glnxa64/libmwm_lxe.so+04814918  [ 25] 0x00007f7ee8abb92f                          bin/glnxa64/libmwm_lxe.so+04815151  [ 26] 0x00007f7ee8bea503                          bin/glnxa64/libmwm_lxe.so+06055171  [ 27] 0x00007f7ee8bedcf3                          bin/glnxa64/libmwm_lxe.so+06069491  [ 28] 0x00007f7ee90fdf6d                          bin/glnxa64/libmwm_lxe.so+11378541  [ 29] 0x00007f7ee9219fa1                          bin/glnxa64/libmwm_lxe.so+12541857  [ 30] 0x00007f7eece842ad                   bin/glnxa64/libmwm_dispatcher.so+00979629 _ZN8Mfh_file16dispatch_fh_implEMS_FviPP11mxArray_tagiS2_EiS2_iS2_+00000829  [ 31] 0x00007f7eece84bae                   bin/glnxa64/libmwm_dispatcher.so+00981934 _ZN8Mfh_file11dispatch_fhEiPP11mxArray_tagiS2_+00000030  [ 32] 0x00007f7ee922cda1                          bin/glnxa64/libmwm_lxe.so+12619169  [ 33] 0x00007f7ee922d982                          bin/glnxa64/libmwm_lxe.so+12622210  [ 34] 0x00007f7ee9315fc9                          bin/glnxa64/libmwm_lxe.so+13574089  [ 35] 0x00007f7ee92b7431                          bin/glnxa64/libmwm_lxe.so+13186097  [ 36] 0x00007f7ee8abd5a8                          bin/glnxa64/libmwm_lxe.so+04822440  [ 37] 0x00007f7ee8abfcbc                          bin/glnxa64/libmwm_lxe.so+04832444  [ 38] 0x00007f7ee8abc01d                          bin/glnxa64/libmwm_lxe.so+04816925  [ 39] 0x00007f7ee8ab5ba1                          bin/glnxa64/libmwm_lxe.so+04791201  [ 40] 0x00007f7ee8ab5dd9                          bin/glnxa64/libmwm_lxe.so+04791769  [ 41] 0x00007f7ee8abb846                          bin/glnxa64/libmwm_lxe.so+04814918  [ 42] 0x00007f7ee8abb92f                          bin/glnxa64/libmwm_lxe.so+04815151  [ 43] 0x00007f7ee8bea503                          bin/glnxa64/libmwm_lxe.so+06055171  [ 44] 0x00007f7ee8bedcf3                          bin/glnxa64/libmwm_lxe.so+06069491  [ 45] 0x00007f7ee90fdf6d                          bin/glnxa64/libmwm_lxe.so+11378541  [ 46] 0x00007f7ee90ab60c                          bin/glnxa64/libmwm_lxe.so+11040268  [ 47] 0x00007f7ee90b2448                          bin/glnxa64/libmwm_lxe.so+11068488  [ 48] 0x00007f7ee90b3e22                          bin/glnxa64/libmwm_lxe.so+11075106  [ 49] 0x00007f7ee9141807                          bin/glnxa64/libmwm_lxe.so+11655175  [ 50] 0x00007f7ee9141aea                          bin/glnxa64/libmwm_lxe.so+11655914  [ 51] 0x00007f7eeb2f591a                         bin/glnxa64/libmwbridge.so+00207130 _Z8mnParserv+00000874  [ 52] 0x00007f7eed36ebb8                            bin/glnxa64/libmwmcr.so+00641976  [ 53] 0x00007f7efd570e9f                         bin/glnxa64/libmwmlutil.so+06524575 _ZNSt13__future_base13_State_baseV29_M_do_setEPSt8functionIFSt10unique_ptrINS_12_Result_baseENS3_8_DeleterEEvEEPb+00000031  [ 54] 0x00007f7effb6fa99              /lib/x86_64-linux-gnu/libpthread.so.0+00060057  [ 55] 0x00007f7efd571126                         bin/glnxa64/libmwmlutil.so+06525222 _ZSt9call_onceIMNSt13__future_base13_State_baseV2EFvPSt8functionIFSt10unique_ptrINS0_12_Result_baseENS4_8_DeleterEEvEEPbEJPS1_S9_SA_EEvRSt9once_flagOT_DpOT0_+00000102  [ 56] 0x00007f7eed36e9d3                            bin/glnxa64/libmwmcr.so+00641491  [ 57] 0x00007f7f01cec1a2                            bin/glnxa64/libmwmvm.so+03367330 _ZN14cmddistributor15PackagedTaskIIP10invokeFuncIN7mwboost8functionIFvvEEEEENS2_10shared_ptrINS2_13unique_futureIDTclfp_EEEEEERKT_+00000082  [ 58] 0x00007f7f01cec4e8                            bin/glnxa64/libmwmvm.so+03368168 _ZNSt17_Function_handlerIFN7mwboost3anyEvEZN14cmddistributor15PackagedTaskIIP10createFuncINS0_8functionIFvvEEEEESt8functionIS2_ET_EUlvE_E9_M_invokeERKSt9_Any_data+00000024  [ 59] 0x00007f7eed978e6c                            bin/glnxa64/libmwiqm.so+00867948 _ZN7mwboost6detail8function21function_obj_invoker0ISt8functionIFNS_3anyEvEES4_E6invokeERNS1_15function_bufferE+00000028  [ 60] 0x00007f7eed97897f                            bin/glnxa64/libmwiqm.so+00866687 _ZN3iqm18PackagedTaskPlugin7executeEP15inWorkSpace_tagRN7mwboost10shared_ptrIN14cmddistributor17IIPCompletedEventEEE+00000447  [ 61] 0x00007f7eed956ab1                            bin/glnxa64/libmwiqm.so+00727729  [ 62] 0x00007f7eed939ac8                            bin/glnxa64/libmwiqm.so+00608968  [ 63] 0x00007f7eed9348bf                            bin/glnxa64/libmwiqm.so+00587967  [ 64] 0x00007f7f00e1ea05                       bin/glnxa64/libmwservices.so+03262981  [ 65] 0x00007f7f00e1fff2                       bin/glnxa64/libmwservices.so+03268594  [ 66] 0x00007f7f00e208fb                       bin/glnxa64/libmwservices.so+03270907 _Z25svWS_ProcessPendingEventsiib+00000187  [ 67] 0x00007f7eed36ffc3                            bin/glnxa64/libmwmcr.so+00647107  [ 68] 0x00007f7eed3706a4                            bin/glnxa64/libmwmcr.so+00648868  [ 69] 0x00007f7eed3693f1                            bin/glnxa64/libmwmcr.so+00619505  [ 70] 0x00007f7effb686ba              /lib/x86_64-linux-gnu/libpthread.so.0+00030394  [ 71] 0x00007f7effe8541d                    /lib/x86_64-linux-gnu/libc.so.6+01078301 clone+00000109  [ 72] 0x0000000000000000                                    +00000000      This error was detected while a MEX-file was running. If the MEX-file  is not an official MathWorks function, please examine its source code  for errors. Please consult the External Interfaces Guide for information  on debugging MEX-files.  ** This crash report has been saved to disk as /home/narendrachintala/matlab_crash_dump.6122-1 **      Caught MathWorks::System::FatalException      **I am getting this exception that the correlation layer.cpp is not implemented yet..Is anyone aware of this issue?**"
"Hi, I was trying to test this model on ImageNet VID (no modification to the code) and I used the trained models linked in the repo homepage (like   or  ). The problem is that using those models I get completely random predictions: for each one of the 30 classes I get the same ~0.03 score, so the model doesn't detect any proposal and seems to act randomly, as it's been randomly initialized (I quadruple-checked that Caffe gets as input the right .caffemodel file).    For this reason I tried to train the model, but I constantly get missing proposal file errors. I checked and it seems that - for example - for the DET train dataset there are only ~53k proposal files while the ImageNet DET train dataset has got ~456k images. What am I missing here?"
I need help. I don't know how to get the input parameters of rfcn_test() and so on.  What should I do set the parameters?
"@feichtenhofer  Hi,I want to know how can I train a end2end model? Look forward to your reply."
"Hi,   when we used your code to train, we encountered two problems as follows:  1. is it possible for you to share the file structure of the root folder(/data/ILSVRC/), including Annotations, Data, devkit, ImageSets and imdb. For example, since we trained on both VID and DET, in root_path/Data, should we put VID_val and VID_train into VID folder, or just leave DET, VID, VID_train and VID_val in parallel?   2. We have succeed constructing ""imdb_ilsvrc15_train_unflip.mat"", but every time when it was about to construct the corresponding roidb, we always got the warning saying ""GT(xml) file empty/broken: ILSVRC2013_train_extra0/ILSVRC2013_train_00000001"".  But there's no corresponding annotations for the 2013 extra. We cannot find this directory from any ILSVRC dataset, so could you please share where we can get it?  Thanks so much in advance!!    "
can anybody explain some contents about the correlation_layer.cu. I can not get through how the correlation works .  thx for your kindly help~
"Hi~recently I run your code, there are some issues about results. I get the result 79.8% when tau = 1, but I change the tau = 10, the result is so bad, only 8.1%. It confused me, thank you for your reply~~"
"Hi,guys,I'm new here. I've downloaded this code, but do not know how to run this code for test as the author said. Could anyone explain explicitly about the pip line of testing . Thank u very much~ "
"Has anyone had any luck training this model end-to-end *without* external object proposals or pretraining the RFCN network on Imagenet DET? I've been trying to train the D(&T loss) model in pytorch and have only reached a frame mean AP of ~64% on the full imagnet VID validation set. Some implementation notes:    - I'm only training/testing on Imagenet VID (I am not using anything from Imagenet DET).  - As in the paper, I'm sampling 10 frames from each video snippet in the training set. These frames are sampled at regular intervals across the duration of the snippet.   - I'm using resnet-101 with pretrained imagenet weights and am randomly initializing the RPN and RCNN.  - I'm using correlation features on conv3, conv4, and conv5 and am regressing on the ground truth boxes in frame t --> t+tau.  - I am using an L1 smooth loss for the tracking loss.  - I am not linking detections across frames at the moment.    - I am using a batch size of 2 (2 images per video, 2 videos = 4 frames total)   - My initial lr is 5e-4"
"I have run the test code successfully, but I still have some doubts  !     1.To get the ""Detect"" results  by ""ImageNet CLS models"" or Detect models""?      2.What is the difference between D and D&T models? I find their prototxt is the same. does the D models mean D(& T loss)?    Thank you ~      "
"Can you please provide a demo code which given video frames, gives output boxes. ?   There is no need to run the test script on imagenetvid dataset if the code has to be used off the shelf for tracking purposes. "
"When I do evaluation, there is an error like '  This error was detected while a MEX-file was running. If the MEX-file  is not an official MathWorks function, please examine its source code  for errors. Please consult the External Interfaces Guide for information  on debugging MEX-files.'  Someone said it was caused by using different numbers of GPU with the setting in the code. I only have 2 GPU. Is there anyone know where I can change the number of GPU in the code? Or hoe can I set it for only using CPU? Thanks"
"Hi, I'm reading the paper and I feel confused about the tracking regress output.  The paper says that the tracking regression output is the offset between objects in two frames.  But the objects has been detected by rfcn network in two frames actually,  what can the tracking regression output do?    I'm very appreciate if there is any answer."
None
In the code it is looking for .mat file of the region proposals of the data but in the repository you have included the directory how to use them in the code if I want to do validation only?
"I try ftp and onedrive, wget from ftp always be time out, and onedrive has nothing"
"Hi, I am wondering how to set conf, imdb, roidb, varargin in rfcn_test.m? Thank you."
"thanks for your excellent job,but i got confused in some details in your paper.  In your paper,the tracking ROI pooling layer is operate on the stack of {Xcorr,Xreg-t,Xreg-t+1}  as far as i can see:      both Xreg-t and Xreg-t+1 layer has s shape of k*k*4      Xcorr consist of correlation output of  conv3,4,5 respectively,and the correlation output should have shape like H*W*(2d+1)*(2d+1)  so:  1:how to concat different layer together like:Xcorr and Xreg-t  2:how to pool on the stacked feature map  thank you."
"Could you provide an example command to get the results on VID validation set? How to reproduce the 79.8 mAP as in the paper using rfcn_test_vid()? I'm not sure what the inputs: conf, imdb, roidb, etc., correspond to. Thank you!"
Nice work so far! Thanks for your contribution
"There are so many train prototxts in the 'models/rfcn_prototxts/ResNet-101L_ILSVRCvid_corr' directory, what is the difference and which one did you use? Thanks!  !   "
Hi!  I think the title is self-explanatory. If you could add some demo to run it directly after compiling it would be very nice to have.  Nice work so far!
"@feichtenhofer Great work! Thanks for sharing. I'm attempting to run inference on a single VID video snippet and am running into errors when trying to call functions within the original Imagenet devkit ( taken from  ). Are you using a custom/modified devkit?    The first error is an IO error from `devkit/evaluation/eval_vid_detection` on l117. I noticed the number of columns had changed in the input file `predict_file`, and I could fix this by changing:       to        After modifying the line above, I ran into another error in the evaluation:       This function is called within `imdb/imdb_eval_ilsvrc14.m`  . I noticed the file `eval_vid_tracking.m` doesn't exist and it doesn't seem to be in this repo. If you are using a modified devkit, can you please push it? "
"In the imdb funcition, there is Â´      meta_det             = load(fullfile(devkit_path, 'data', 'meta_vid.mat')); Â´, where can I find the Â´meta_vid.matÂ´? Thanks."
"Hi, I didnÂ´t find the file Â´mean_imageÂ´ in the folder models/pre_trained_models/ResNet-101L. Where can I find it? Thanks."
"Hi, I found there was no the file Â´mean_imageÂ´ in the director models/pre_trained_models/ResNet-101L. Where can I find it? Thanks"
"When I finetuned the RoI tracking part on the trained RFCN detector, the model training would be broken down. The RoI I used belongs to the T frame for the correlation features and the pair of frames' feature map. The initiate learning rate I set is 0.0001. How can I solve it?"
I feel confused about the ROI tracking network. Can you first open the `deploy.prototxt`?
"In the track-regression, RoI pooling is operated on the concatenation of the bounding box regression features and the correlation feature. I want to know the details about RoI pooling such as which frame's roi should be used. "
"In Sec. 3.3 of the paper, it's mentioned that the tracking loss is active for   ground truth RoIs which have a track correspondence across the two frames  . I'm interpreting this as meaning that only predicted RoIs assigned to ground-truth RoIs (using an IoU>0.5) with a correspondence between the two frames are used in the tracking loss. Is this the idea?    For example, if the RoI batch size is 256, and each of these 256 RoIs are assigned to a ground truth box in frame   having a correspondence in the next frame  , would I use   and all of their RoI-tracking deltas with the ground-truth delta in the regression of Equation (1) of the paper?"
None
which DL framework does your code use?
None
"@sciencefans :   Dear sir,   in your readme you have written that   train/: Training code for modules scale-forecast network and RSA    but the train folder itself is missing on your repository, either you forgot to upload OR its not public."
@sciencefans @liyuanyaun  Could you please send me the train code? I want use RSA in my own DataSet.  My email is gaojing994919@163.com
"Hi @sciencefans   thank you for your work and paper  Do you know any approximate date to release your code because I want to look into and try it.    Regards,  Anand"
1. è®ºæ–‡ä¸­æåˆ°ä½¿ç”¨half-channel resnet18ä½œä¸ºlrn netï¼Œåœ¨ä»£ç ä¸­å¹¶æ²¡æœ‰çœ‹åˆ°half-channelï¼Œä¸¤è€…åœ¨ç²¾åº¦ä¸Šç›¸å·®å¤šå°‘  2. å¸¦æœ‰å…³é”®ç‚¹çš„widerfaceè®­ç»ƒé›†æ˜¯ä½ ä»¬å†…éƒ¨æ²¡å¼€æºçš„æ•°æ®é›†ä¹ˆ  3. æˆ‘ä½¿ç”¨å¤šscale[2048~64]çš„åŽŸå›¾åŽ»è®­ç»ƒlrnï¼Œå†åŸºäºŽè®­ç»ƒå¥½çš„lrnç¬¬ä¸€éƒ¨åˆ†ç½‘ç»œè®­ç»ƒrsaï¼Œè¿™ç§æ€è·¯å¯¹ä¹ˆï¼Œå¯å¦å‘ŠçŸ¥è®ºæ–‡ä¸­æåˆ°çš„end-to-endè®­ç»ƒæ–¹å¼  4. å¦‚æžœä½¿ç”¨boxä½œä¸ºgroudtruthä»£æ›¿keypointï¼Œå½±å“å¤§ä¹ˆ    æœ›å¤§ç¥žè§£ç­”ï¼å¤šè°¢ï¼  
My speed:     Speed in comments:     My speed is 100x slower than the comments. My graphic card is GTX 1080. I think there should not be so much difference. Could anyone help? 
None
"(1)in RSA section of your paper, i do not understand This paragraphï¼š""During inference, we first have the possible scales of the input from the scale-forecast network. The image is then resized accordingly to the extent that the smallest scale (corresponding to the largest feature map) is resized to the range of [64; 128].""  is  ""the smallest scale"" of faces or image? and do you mean that resize operation is to ensure ""the smallest scale"" in the range of [64; 128].?    (2)for Scale forecast Network in figure 3 of you paper, when it output 28 in [20,30] which means the size of face is in the range of [128,256], corresponding m is equal to 2. this means after RSA, the size of the face is in range of [32,64]. and when it output 35 in [30,40] which means the size of is in the range of [256,512], corresponding m is equal to 3. this means after RSA, the size of the face is in range of [32,64],too. so RSA is to ensure the size of face  in the range of  [32,64]?    look forward your answers, thanks!  "
"TypeError                                 Traceback (most recent call last)    in          6             learning_rate=config.LEARNING_RATE,        7             epochs=5,  ----> 8             layers='heads')    ~/project/try2/Mask_RCNN/mrcnn/model.py in train(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)     2381             max_queue_size=100,     2382             workers=workers,  -> 2383             use_multiprocessing=True,     2384         )     2385         self.epoch = max(self.epoch, epochs)    ~/anaconda3/envs/try2/lib/python3.7/site-packages/keras/legacy/interfaces.py in wrapper(*args, **kwargs)       89                 warnings.warn('Update your `' + object_name + '` call to the ' +       90                               'Keras 2 API: ' + signature, stacklevel=2)  ---> 91             return func(*args, **kwargs)       92         wrapper._original_function = func       93         return wrapper    ~/anaconda3/envs/try2/lib/python3.7/site-packages/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)     1416             use_multiprocessing=use_multiprocessing,     1417             shuffle=shuffle,  -> 1418             initial_epoch=initial_epoch)     1419      1420     @interfaces.legacy_generator_methods_support    ~/anaconda3/envs/try2/lib/python3.7/site-packages/keras/engine/training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)       92     else:       93         callback_model = model  ---> 94     callbacks.set_model(callback_model)       95     callbacks.set_params({       96         'epochs': epochs,    ~/anaconda3/envs/try2/lib/python3.7/site-packages/keras/callbacks.py in set_model(self, model)       52     def set_model(self, model):       53         for callback in self.callbacks:  ---> 54             callback.set_model(model)       55        56     def on_epoch_begin(self, epoch, logs=None):    ~/anaconda3/envs/try2/lib/python3.7/site-packages/keras/callbacks.py in set_model(self, model)      847                     else:      848                         tf.compat.v1.summary.histogram('{}_out'.format(layer.name),  --> 849                                              layer.output)      850         self.merged = tf.summary.merge_all()      851     ~/anaconda3/envs/try2/lib/python3.7/site-packages/tensorflow/python/summary/summary.py in histogram(name, values, collections, family)      177       default_name='HistogramSummary') as (tag, scope):      178     val = _gen_logging_ops.histogram_summary(  --> 179         tag=tag, values=values, name=scope)      180     _summary_op_util.collect(val, collections, [_ops.GraphKeys.SUMMARIES])      181   return val    ~/anaconda3/envs/try2/lib/python3.7/site-packages/tensorflow/python/ops/gen_logging_ops.py in histogram_summary(tag, values, name)      327   # Add nodes to the TensorFlow graph.      328   _, _, _op = _op_def_lib._apply_op_helper(  --> 329         ""HistogramSummary"", tag=tag, values=values, name=name)      330   _result = _op.outputs[:]      331   _inputs_flat = _op.inputs    ~/anaconda3/envs/try2/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)      624               _SatisfiesTypeConstraint(base_type,      625                                        _Attr(op_def, input_arg.type_attr),  --> 626                                        param_name=input_name)      627             attrs[input_arg.type_attr] = attr_value      628             inferred_from[input_arg.type_attr] = input_name    ~/anaconda3/envs/try2/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py in _SatisfiesTypeConstraint(dtype, attr_def, param_name)       58           ""allowed values: %s"" %       59           (param_name, dtypes.as_dtype(dtype).name,  ---> 60            "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))       61        62     TypeError: Value passed to parameter 'values' has DataType bool not in list of allowed values: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, float16, uint32, uint64  "
Dear @cclauss and @waleedka I was trying run the above code snippet. I am attaching the error  below.    !       Can anyone let me know how to resolve this issue. It's been days since I am stuck on this
  !           Why do I get like that when I run the demo file?
"What is the folder structure and files that are required in: DATASET_DIR = os.path.join(ROOT_DIR, ""datasets/nucleus"")    I received an error when running the commands from the notebook 'inspect_nucleus_model.ipynb':     config = nucleus.NucleusInferenceConfig()    Traceback (most recent call last):    File "" "", line 1, in    AttributeError: module 'nucleus' has no attribute 'NucleusInferenceConfig'  "
"499/500 [============================>.] - ETA: 10s - loss: 12.0817 - rpn_class_loss: 0.0832 - rpn_bbox_loss: 1.8053 - mrcnn_class_loss: 0.0936 - mrcnn_bbox_loss: 0.6976 - mrcnn_mask_loss: 0.4940 - roi_alignment_loss: 7.6621 - row_adj_loss: 0.6728 - col_adj_loss: 0.5730/home/jinac/anaconda3/envs/struct/lib/python3.6/site-packages/imgaug/augmenters/base.py:59: SuspiciousSingleImageShapeWarning: You provided a numpy array of shape (1024, 1024, 220) as a single-image augmentation input, which was interpreted as (H, W, C). The last dimension however has a size of >=32, which indicates that you provided a multi-image array with shape (N, H, W) instead. If that is the case, you should use e.g. augmenter(imageS= ) or augment_imageS( ). Otherwise your multi-image input will be interpreted as a single image during augmentation.    category=SuspiciousSingleImageShapeWarning)  Traceback (most recent call last):    File ""/home/jinac/anaconda3/envs/struct/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 2836, in data_generator  ZeroDivisionError: integer division or modulo by zero    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""samples/tabnet/tabnet.py"", line 575, in        augmentation=augmentation)    File ""/home/jinac/anaconda3/envs/struct/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 4133, in train    File ""/home/jinac/anaconda3/envs/struct/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""/home/jinac/anaconda3/envs/struct/lib/python3.6/site-packages/keras/engine/training.py"", line 2250, in fit_generator      max_queue_size=max_queue_size)    File ""/home/jinac/anaconda3/envs/struct/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""/home/jinac/anaconda3/envs/struct/lib/python3.6/site-packages/keras/engine/training.py"", line 2383, in eva"
"python==3.7.15  tensorflow==2.10.0  keras==2.10.0     "".\Mask_RCNN\mrcnn\model.py""       Error occurred as below       Please anyone helps me"
None
"Hi, I am trying to run Maskrcnn on my custom dataset. I have the following issue in the training step. My images are 6000 X 4000 pixels. I am wondering whether it is due to the shape being incompatible with the default configuration     Here is the config     Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.7  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 1  IMAGE_CHANNEL_COUNT            3  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                15  IMAGE_MIN_DIM                  800  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           braceroot_cfg_coco  NUM_CLASSES                    3  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  PRE_NMS_LIMIT                  6000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                100  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  False  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001      Here is my error:     ERROR:root: Error processing image {'id': 18, 'source': 'coco_like', 'path': '/scratch/hl46161/brace_root/CoCo_test_train/data/1302_plant_8A.jpg', 'width': 6000, 'height': 4000, ......}  Traceback (most recent call last):    File ""/home/hl46161/.conda/envs/Maskrcnn-TF2/lib/python3.7/site-packages/mask_rcnn_tf2-1.0-py3.7.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/hl46161/.conda/envs/Maskrcnn-TF2/lib/python3.7/site-packages/mask_rcnn_tf2-1.0-py3.7.egg/mrcnn/model.py"", line 1211, in load_image_gt      image = dataset.load_image(image_id)    File ""/home/hl46161/.conda/envs/Maskrcnn-TF2/lib/python3.7/site-packages/mask_rcnn_tf2-1.0-py3.7.egg/mrcnn/utils.py"", line 359, in load_image      image = skimage.io.imread(self.image_info[image_id]['path'])    File ""/home/hl46161/.conda/envs/Maskrcnn-TF2/lib/python3.7/site-packages/skimage/io/_io.py"", line 48, in imread      img = call_plugin('imread', fname, plugin=plugin, **plugin_args)    File ""/home/hl46161/.conda/envs/Maskrcnn-TF2/lib/python3.7/site-packages/skimage/io/manage_plugins.py"", line 210, in call_plugin      return func(*args, **kwargs)    File ""/home/hl46161/.conda/envs/Maskrcnn-TF2/lib/python3.7/site-packages/skimage/io/_plugins/imageio_plugin.py"", line 10, in imread      return np.asarray(imageio_imread(*args, **kwargs))    File ""/home/hl46161/.conda/envs/Maskrcnn-TF2/lib/python3.7/site-packages/imageio/__init__.py"", line 97, in imread      return imread_v2(uri, format=format, **kwargs)    File ""/home/hl46161/.conda/envs/Maskrcnn-TF2/lib/python3.7/site-packages/imageio/v2.py"", line 226, in imread      with imopen(uri, ""ri"", **imopen_args) as file:    File ""/home/hl46161/.conda/envs/Maskrcnn-TF2/lib/python3.7/site-packages/imageio/core/imopen.py"", line 213, in imopen      plugin_instance = candidate_plugin(request, **kwargs)    File ""/home/hl46161/.conda/envs/Maskrcnn-TF2/lib/python3.7/site-packages/imageio/config/plugins.py"", line 107, in partial_legacy_plugin      return LegacyPlugin(request, legacy_plugin)    File ""/home/hl46161/.conda/envs/Maskrcnn-TF2/lib/python3.7/site-packages/imageio/core/legacy_plugin_wrapper.py"", line 80, in __init__      if not self._format.can_read(request):    File ""/home/hl46161/.conda/envs/Maskrcnn-TF2/lib/python3.7/site-packages/imageio/core/format.py"", line 242, in can_read      return self._can_read(request)    File ""/home/hl46161/.conda/envs/Maskrcnn-TF2/lib/python3.7/site-packages/imageio/plugins/pillow_legacy.py"", line 264, in _can_read      Image = self._init_pillow()    File ""/home/hl46161/.conda/envs/Maskrcnn-TF2/lib/python3.7/site-packages/imageio/plugins/pillow_legacy.py"", line 258, in _init_pillow      Image.preinit()    File ""/home/hl46161/.local/lib/python3.7/site-packages/PIL/Image.py"", line 361, in preinit      from . import JpegImagePlugin    File ""/home/hl46161/.local/lib/python3.7/site-packages/PIL/JpegImagePlugin.py"", line 44, in        from . import Image, ImageFile, TiffImagePlugin    File "" "", line 983, in _find_and_load    File "" "", line 967, in _find_and_load_unlocked    File "" "", line 677, in _load_unlocked    File "" "", line 724, in exec_module    File "" "", line 857, in get_code    File "" "", line 525, in _compile_bytecode  EOFError: EOF read where object expected  "
I have been working on this model for couple of weeks but still facing the issues that tensorflow library that works with mrcnn is not supported by google colab due to which I am unable to access the free gpu and it takes a complete day to train 10 epochs of 10 steps.   how will I activate the gpu runtime with tensorflow 1.13.1 version(supported by mrcnn model)  `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.    UserWarning('Using a generator with `use_multiprocessing=True`'  Epoch 1/10   9/10 [==========================>...] - ETA: 89s - loss: 5.1812 - rpn_class_loss: 0.0661 - rpn_bbox_loss: 2.0932 - mrcnn_class_loss: 1.0029 - mrcnn_bbox_loss: 0.9374 - mrcnn_mask_loss: 1.0817 /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2142: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.    UserWarning('Using a generator with `use_multiprocessing=True`'  10/10 [==============================] - 2200s - loss: 4.9398 - rpn_class_loss: 0.0856 - rpn_bbox_loss: 2.1344 - mrcnn_class_loss: 0.9026 - mrcnn_bbox_loss: 0.8437 - mrcnn_mask_loss: 0.9735 - val_loss: 2.9885 - val_rpn_class_loss: 0.0627 - val_rpn_bbox_loss: 1.0605 - val_mrcnn_class_loss: 0.1267 - val_mrcnn_bbox_loss: 0.8837 - val_mrcnn_mask_loss: 0.8549  Epoch 2/10  10/10 [==============================] - 2080s - loss: 2.8650 - rpn_class_loss: 0.0653 - rpn_bbox_loss: 0.8709 - mrcnn_class_loss: 0.2062 - mrcnn_bbox_loss: 0.9295 - mrcnn_mask_loss: 0.7931 - val_loss: 2.5669 - val_rpn_class_loss: 0.0251 - val_rpn_bbox_loss: 0.7778 - val_mrcnn_class_loss: 0.1389 - val_mrcnn_bbox_loss: 0.8665 - val_mrcnn_mask_loss: 0.7586  Epoch 3/10  10/10 [==============================] - 2071s - loss: 2.5079 - rpn_class_loss: 0.0178 - rpn_bbox_loss: 0.6116 - mrcnn_class_loss: 0.1541 - mrcnn_bbox_loss: 1.0084 - mrcnn_mask_loss: 0.7159 - val_loss: 2.3405 - val_rpn_class_loss: 0.0179 - val_rpn_bbox_loss: 0.7233 - val_mrcnn_class_loss: 0.1208 - val_mrcnn_bbox_loss: 0.7990 - val_mrcnn_mask_loss: 0.6794  Epoch 4/10  10/10 [==============================] - 2076s - loss: 3.5875 - rpn_class_loss: 0.0330 - rpn_bbox_loss: 2.2295 - mrcnn_class_loss: 0.0901 - mrcnn_bbox_loss: 0.6266 - mrcnn_mask_loss: 0.6082 - val_loss: 2.1113 - val_rpn_class_loss: 0.0173 - val_rpn_bbox_loss: 0.6490 - val_mrcnn_class_loss: 0.1017 - val_mrcnn_bbox_loss: 0.7573 - val_mrcnn_mask_loss: 0.5859  Epoch 5/10  10/10 [==============================] - 2081s - loss: 2.0695 - rpn_class_loss: 0.0174 - rpn_bbox_loss: 0.6024 - mrcnn_class_loss: 0.1108 - mrcnn_bbox_loss: 0.7344 - mrcnn_mask_loss: 0.6044 - val_loss: 2.2599 - val_rpn_class_loss: 0.0158 - val_rpn_bbox_loss: 0.6386 - val_mrcnn_class_loss: 0.1036 - val_mrcnn_bbox_loss: 0.8452 - val_mrcnn_mask_loss: 0.6566  Epoch 6/10  10/10 [==============================] - 2073s - loss: 2.1327 - rpn_class_loss: 0.0219 - rpn_bbox_loss: 0.5814 - mrcnn_class_loss: 0.1610 - mrcnn_bbox_loss: 0.6752 - mrcnn_mask_loss: 0.6932 - val_loss: 2.2272 - val_rpn_class_loss: 0.0170 - val_rpn_bbox_loss: 0.6450 - val_mrcnn_class_loss: 0.1082 - val_mrcnn_bbox_loss: 0.8118 - val_mrcnn_mask_loss: 0.6452  Epoch 7/10  10/10 [==============================] - 2075s - loss: 3.2746 - rpn_class_loss: 0.0522 - rpn_bbox_loss: 1.5602 - mrcnn_class_loss: 0.1547 - mrcnn_bbox_loss: 0.8337 - mrcnn_mask_loss: 0.6737 - val_loss: 2.0564 - val_rpn_class_loss: 0.0172 - val_rpn_bbox_loss: 0.6224 - val_mrcnn_class_loss: 0.1280 - val_mrcnn_bbox_loss: 0.6824 - val_mrcnn_mask_loss: 0.6063  Epoch 8/10
None
None
"  Log outputï¼š    loading annotations into memory...  Done (t=0.00s)  creating index...  index created!  ----------------------loss: Tensor(""mrcnn_mask_loss/Mean:0"", shape=(), dtype=float32)  ------------------------------------output_rois: Tensor(""output_rois/mul:0"", shape=(1, ?, 4), dtype=float32)  ------------------------------------rpn_class_loss: Tensor(""rpn_class_loss/cond/Merge:0"", shape=(), dtype=float32)  ------------------------------------rpn_bbox_loss: Tensor(""rpn_bbox_loss/cond/Merge:0"", shape=(), dtype=float32)  ------------------------------------class_loss: Tensor(""mrcnn_class_loss/truediv:0"", shape=(), dtype=float32)  ------------------------------------bbox_loss: Tensor(""mrcnn_bbox_loss/Mean:0"", shape=(), dtype=float32)  ------------------------------------mask_loss: Tensor(""mrcnn_mask_loss/Mean:0"", shape=(), dtype=float32)  ----------------------------inputs: [ ,  ,  ,  ,  ,  ,  ]  ----------------------loss: Tensor(""tower_0/mask_rcnn/mrcnn_mask_loss/Mean:0"", shape=(), dtype=float32, device=/device:GPU:0)  first stage epoch num:121,second stage epoch num:365,third stage epoch num:486  per epoch steps:50  Training network heads  ([array([[[[-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9],           ...,           [-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9]],            [[-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9],           ...,           [-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9]],            [[-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9],           ...,           [-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9]],            ...,            [[-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9],           ...,           [-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9]],            [[-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9],           ...,           [-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9]],            [[-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9],           ...,           [-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9],           [-123.7, -116.8, -103.9]]]], dtype=float32), array([[8.300e+01, 1.280e+02, 1.280e+02, 3.000e+00, 1.024e+03, 1.024e+03,          3.000e+00, 1.120e+02, 1.120e+02, 9.120e+02, 9.120e+02, 6.250e+00,          1.000e+00, 1.000e+00, 1.000e+00, 1.000e+00]]), array([[[0],          [0],          [0],          ...,          [0],          [0],          [0]]], dtype=int32), array([[[ 0.72265625,  0.625     ,  0.60624622,  0.62351739],          [ 0.72265625, -0.625     ,  0.60624622,  0.62351739],          [-0.52734375,  0.625     ,  0.60624622,  0.62351739],          ...,          [ 0.        ,  0.        ,  0.        ,  0.        ],          [ 0.        ,  0.        ,  0.        ,  0.        ],          [ 0.        ,  0.        ,  0.        ,  0.        ]]]), array([[3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32), array([[[112, 112, 443, 550],          [418, 543, 707, 833],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0],          [  0,   0,   0,   0]]], dtype=int32), array([[[[False, False, False, ..., False, False, False],           [False, False, False, ..., False, False, False],           [False, False, False, ..., False, False, False],           ...,           [False, False, False, ..., False, False, False],           [False, False, False, ..., False, False, False],           [False, False, False, ..., False, False, False]],            [[False, False, False, ..., False, False, False],           [False, False, False, ..., False, False, False],           [False, False, False, ..., False, False, False],           ...,           [False, False, False, ..., False, False, False],           [False, False, False, ..., False, False, False],           [False, False, False, ..., False, False, False]],            [[False, False, False, ..., False, False, False],           [False, False, False, ..., False, False, False],           [False, False, False, ..., False, False, False],           ...,           [False, False, False, ..., False, False, False],           [False, False, False, ..., False, False, False],           [False, False, False, ..., False, False, False]],            ...,            [[ True, False, False, ..., False, False, False],           [ True, False, False, ..., False, False, False],           [ True, False, False, ..., False, False, False],           ...,           [ True, False, False, ..., False, False, False],           [ True, False, False, ..., False, False, False],           [False, False, False, ..., False, False, False]],            [[ True, False, False, ..., False, False, False],           [ True, False, False, ..., False, False, False],           [ True, False, False, ..., False, False, False],           ...,           [ True, False, False, ..., False, False, False],           [ True, False, False, ..., False, False, False],           [False, False, False, ..., False, False, False]],            [[ True, False, False, ..., False, False, False],           [ True, False, False, ..., False, False, False],           [ True, False, False, ..., False, False, False],           ...,           [ True, False, False, ..., False, False, False],           [ True, False, False, ..., False, False, False],           [ True, False, False, ..., False, False, False]]]])], [])    Starting at epoch 0. LR=0.001    Checkpoint Path: /home/hpcadmin/hys/hys_1017_dir/train_dir_instance_segmentation  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  rpn_model              (Functional)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)  -----------------------------------layers: (mrcnn\_.*)|(rpn\_.*)|(fpn\_.*)  -----------------------------------model : {'mode': 'training', 'config':  , 'model_dir': '/home/hpcadmin/hys/hys_1017_dir/train_dir_instance_segmentation', 'epoch': 0, 'log_dir': '/home/hpcadmin/hys/hys_1017_dir/train_dir_instance_segmentation/task_020221104T0957', 'checkpoint_path': '/home/hpcadmin/hys/hys_1017_dir/train_dir_instance_segmentation/task_020221104T0957/mask_rcnn_task_0_{epoch:04d}.h5', '_anchor_cache': {(1024, 1024, 3): array([[-0.02211869, -0.01105934,  0.02114117,  0.01008183],         [-0.01564027, -0.01564027,  0.01466276,  0.01466276],         [-0.01105934, -0.02211869,  0.01008183,  0.02114117],         ...,         [ 0.5845174 ,  0.7614669 ,  1.2913378 ,  1.1143883 ],         [ 0.68817204,  0.68817204,  1.1876833 ,  1.1876833 ],         [ 0.7614669 ,  0.5845174 ,  1.1143883 ,  1.2913378 ]],        dtype=float32)}, 'anchors': array([[ -22.627417  ,  -11.3137085 ,   22.627417  ,   11.3137085 ],         [ -16.        ,  -16.        ,   16.        ,   16.        ],         [ -11.3137085 ,  -22.627417  ,   11.3137085 ,   22.627417  ],         ...,         [ 597.96132803,  778.98066402, 1322.03867197, 1141.01933598],         [ 704.        ,  704.        , 1216.        , 1216.        ],         [ 778.98066402,  597.96132803, 1141.01933598, 1322.03867197]]), 'keras_model':  }  ----------------------config.LEARNING_MOMENTUM: 0.9  2022-11-04 09:57:57.260568: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled  ======optimizer_device: /device:CPU:0  the learning_rate_decay_type is: fixed  the optimizer is: sgd  ----------------------------self.total_loss: Tensor(""loss_1/AddN:0"", shape=(), dtype=float32)  ----------------------------params: [ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ]  /home/hpcadmin/hys/conda_env_tf_2/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(""gradients/tower_0/mask_rcnn/roi_align_classifier/concat_grad/sub:0"", shape=(?,), dtype=int32), values=Tensor(""gradients/tower_0/mask_rcnn/roi_align_classifier/concat_grad/GatherV2_2:0"", shape=(?, 7, 7, 256), dtype=float32), dense_shape=Tensor(""gradients/tower_0/mask_rcnn/roi_align_classifier/concat_grad/Shape:0"", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.    warnings.warn(  /home/hpcadmin/hys/conda_env_tf_2/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(""gradients/tower_0/mask_rcnn/roi_align_classifier/concat_grad/sub_1:0"", shape=(?,), dtype=int32), values=Tensor(""gradients/tower_0/mask_rcnn/roi_align_classifier/concat_grad/GatherV2_5:0"", shape=(?, 7, 7, 256), dtype=float32), dense_shape=Tensor(""gradients/tower_0/mask_rcnn/roi_align_classifier/concat_grad/Shape_1:0"", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.    warnings.warn(  /home/hpcadmin/hys/conda_env_tf_2/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(""gradients/tower_0/mask_rcnn/roi_align_classifier/concat_grad/sub_2:0"", shape=(?,), dtype=int32), values=Tensor(""gradients/tower_0/mask_rcnn/roi_align_classifier/concat_grad/GatherV2_8:0"", shape=(?, 7, 7, 256), dtype=float32), dense_shape=Tensor(""gradients/tower_0/mask_rcnn/roi_align_classifier/concat_grad/Shape_2:0"", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.    warnings.warn(  /home/hpcadmin/hys/conda_env_tf_2/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(""gradients/tower_0/mask_rcnn/roi_align_classifier/concat_grad/sub_3:0"", shape=(?,), dtype=int32), values=Tensor(""gradients/tower_0/mask_rcnn/roi_align_classifier/concat_grad/GatherV2_11:0"", shape=(?, 7, 7, 256), dtype=float32), dense_shape=Tensor(""gradients/tower_0/mask_rcnn/roi_align_classifier/concat_grad/Shape_3:0"", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.    warnings.warn(  /home/hpcadmin/hys/conda_env_tf_2/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(""gradients/tower_0/mask_rcnn/roi_align_mask/concat_grad/sub:0"", shape=(?,), dtype=int32), values=Tensor(""gradients/tower_0/mask_rcnn/roi_align_mask/concat_grad/GatherV2_2:0"", shape=(?, 14, 14, 256), dtype=float32), dense_shape=Tensor(""gradients/tower_0/mask_rcnn/roi_align_mask/concat_grad/Shape:0"", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.    warnings.warn(  /home/hpcadmin/hys/conda_env_tf_2/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(""gradients/tower_0/mask_rcnn/roi_align_mask/concat_grad/sub_1:0"", shape=(?,), dtype=int32), values=Tensor(""gradients/tower_0/mask_rcnn/roi_align_mask/concat_grad/GatherV2_5:0"", shape=(?, 14, 14, 256), dtype=float32), dense_shape=Tensor(""gradients/tower_0/mask_rcnn/roi_align_mask/concat_grad/Shape_1:0"", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.    warnings.warn(  /home/hpcadmin/hys/conda_env_tf_2/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(""gradients/tower_0/mask_rcnn/roi_align_mask/concat_grad/sub_2:0"", shape=(?,), dtype=int32), values=Tensor(""gradients/tower_0/mask_rcnn/roi_align_mask/concat_grad/GatherV2_8:0"", shape=(?, 14, 14, 256), dtype=float32), dense_shape=Tensor(""gradients/tower_0/mask_rcnn/roi_align_mask/concat_grad/Shape_2:0"", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.    warnings.warn(  /home/hpcadmin/hys/conda_env_tf_2/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(""gradients/tower_0/mask_rcnn/roi_align_mask/concat_grad/sub_3:0"", shape=(?,), dtype=int32), values=Tensor(""gradients/tower_0/mask_rcnn/roi_align_mask/concat_grad/GatherV2_11:0"", shape=(?, 14, 14, 256), dtype=float32), dense_shape=Tensor(""gradients/tower_0/mask_rcnn/roi_align_mask/concat_grad/Shape_3:0"", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.    warnings.warn(  /home/hpcadmin/hys/conda_env_tf_2/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(""gradients/tower_0/mask_rcnn/ROI/GatherV2_1_grad/Reshape_1:0"", shape=(6000,), dtype=int32), values=Tensor(""gradients/tower_0/mask_rcnn/ROI/GatherV2_1_grad/Reshape:0"", shape=(6000, 4), dtype=float32), dense_shape=Tensor(""gradients/tower_0/mask_rcnn/ROI/GatherV2_1_grad/Cast:0"", shape=(2,), dtype=int32, device=/device:GPU:0))) to a dense Tensor of unknown shape. This may consume a large amount of memory.    warnings.warn(  -----------------------------grads: [ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ]  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/fpn_c5p5/Conv2D_grad/Conv2DBackpropFilter:0"", shape=(1, 1, 2048, 256), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/fpn_c5p5/BiasAdd_grad/BiasAddGrad:0"", shape=(256,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/fpn_c4p4/Conv2D_grad/Conv2DBackpropFilter:0"", shape=(1, 1, 1024, 256), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/fpn_c4p4/BiasAdd_grad/BiasAddGrad:0"", shape=(256,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/fpn_c3p3/Conv2D_grad/Conv2DBackpropFilter:0"", shape=(1, 1, 512, 256), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/fpn_c3p3/BiasAdd_grad/BiasAddGrad:0"", shape=(256,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/fpn_c2p2/Conv2D_grad/Conv2DBackpropFilter:0"", shape=(1, 1, 256, 256), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/fpn_c2p2/BiasAdd_grad/BiasAddGrad:0"", shape=(256,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/fpn_p5/Conv2D_grad/Conv2DBackpropFilter:0"", shape=(3, 3, 256, 256), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/fpn_p5/BiasAdd_grad/BiasAddGrad:0"", shape=(256,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/fpn_p2/Conv2D_grad/Conv2DBackpropFilter:0"", shape=(3, 3, 256, 256), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/fpn_p2/BiasAdd_grad/BiasAddGrad:0"", shape=(256,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/fpn_p3/Conv2D_grad/Conv2DBackpropFilter:0"", shape=(3, 3, 256, 256), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/fpn_p3/BiasAdd_grad/BiasAddGrad:0"", shape=(256,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/fpn_p4/Conv2D_grad/Conv2DBackpropFilter:0"", shape=(3, 3, 256, 256), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/fpn_p4/BiasAdd_grad/BiasAddGrad:0"", shape=(256,), dtype=float32)  --------------------------g: Tensor(""gradients/AddN_28:0"", shape=(3, 3, 256, 512), dtype=float32)  --------------------------g: Tensor(""gradients/AddN_26:0"", shape=(512,), dtype=float32)  --------------------------g: Tensor(""gradients/AddN_11:0"", shape=(1, 1, 512, 6), dtype=float32)  --------------------------g: Tensor(""gradients/AddN_10:0"", shape=(6,), dtype=float32)  --------------------------g: Tensor(""gradients/AddN_22:0"", shape=(1, 1, 512, 12), dtype=float32)  --------------------------g: Tensor(""gradients/AddN_21:0"", shape=(12,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_mask_conv1/conv2d_2/Conv2D_grad/Conv2DBackpropFilter:0"", shape=(3, 3, 256, 256), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_mask_conv1/conv2d_2/BiasAdd_grad/BiasAddGrad:0"", shape=(256,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_mask_bn1/batch_norm_2/FusedBatchNormV3_grad/FusedBatchNormGradV3:1"", shape=(256,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_mask_bn1/batch_norm_2/FusedBatchNormV3_grad/FusedBatchNormGradV3:2"", shape=(256,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_mask_conv2/conv2d_3/Conv2D_grad/Conv2DBackpropFilter:0"", shape=(3, 3, 256, 256), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_mask_conv2/conv2d_3/BiasAdd_grad/BiasAddGrad:0"", shape=(256,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_mask_bn2/batch_norm_3/FusedBatchNormV3_grad/FusedBatchNormGradV3:1"", shape=(256,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_mask_bn2/batch_norm_3/FusedBatchNormV3_grad/FusedBatchNormGradV3:2"", shape=(256,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_class_conv1/conv2d/Conv2D_grad/Conv2DBackpropFilter:0"", shape=(7, 7, 256, 1024), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_class_conv1/conv2d/BiasAdd_grad/BiasAddGrad:0"", shape=(1024,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_class_bn1/batch_norm/FusedBatchNormV3_grad/FusedBatchNormGradV3:1"", shape=(1024,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_class_bn1/batch_norm/FusedBatchNormV3_grad/FusedBatchNormGradV3:2"", shape=(1024,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_mask_conv3/conv2d_4/Conv2D_grad/Conv2DBackpropFilter:0"", shape=(3, 3, 256, 256), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_mask_conv3/conv2d_4/BiasAdd_grad/BiasAddGrad:0"", shape=(256,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_mask_bn3/batch_norm_4/FusedBatchNormV3_grad/FusedBatchNormGradV3:1"", shape=(256,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_mask_bn3/batch_norm_4/FusedBatchNormV3_grad/FusedBatchNormGradV3:2"", shape=(256,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_class_conv2/conv2d_1/Conv2D_grad/Conv2DBackpropFilter:0"", shape=(1, 1, 1024, 1024), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_class_conv2/conv2d_1/BiasAdd_grad/BiasAddGrad:0"", shape=(1024,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_class_bn2/batch_norm_1/FusedBatchNormV3_grad/FusedBatchNormGradV3:1"", shape=(1024,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_class_bn2/batch_norm_1/FusedBatchNormV3_grad/FusedBatchNormGradV3:2"", shape=(1024,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_mask_conv4/conv2d_5/Conv2D_grad/Conv2DBackpropFilter:0"", shape=(3, 3, 256, 256), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_mask_conv4/conv2d_5/BiasAdd_grad/BiasAddGrad:0"", shape=(256,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_mask_bn4/batch_norm_5/FusedBatchNormV3_grad/FusedBatchNormGradV3:1"", shape=(256,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_mask_bn4/batch_norm_5/FusedBatchNormV3_grad/FusedBatchNormGradV3:2"", shape=(256,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_bbox_fc/dense_1/MatMul_grad/MatMul_1:0"", shape=(1024, 16), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_bbox_fc/dense_1/BiasAdd_grad/BiasAddGrad:0"", shape=(16,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_mask_deconv/conv2d_transpose/conv2d_transpose_grad/Conv2DBackpropFilter:0"", shape=(2, 2, 256, 256), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_mask_deconv/conv2d_transpose/BiasAdd_grad/BiasAddGrad:0"", shape=(256,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_class_logits/dense/MatMul_grad/MatMul_1:0"", shape=(1024, 4), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_class_logits/dense/BiasAdd_grad/BiasAddGrad:0"", shape=(4,), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_mask/conv2d_6/Conv2D_grad/Conv2DBackpropFilter:0"", shape=(1, 1, 256, 4), dtype=float32)  --------------------------g: Tensor(""gradients/tower_0/mask_rcnn/mrcnn_mask/conv2d_6/BiasAdd_grad/BiasAddGrad:0"", shape=(4,), dtype=float32)  ------------------------------layer output: Tensor(""rpn_class_loss_1/rpn_class_loss/Identity:0"", shape=(), dtype=float32)  Traceback (most recent call last):    File ""./applications/letrain.py"", line 639, in        tf.app.run()    File ""/home/hpcadmin/hys/conda_env_tf_2/lib/python3.8/site-packages/tensorflow/python/platform/app.py"", line 36, in run      _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)    File ""/home/hpcadmin/hys/conda_env_tf_2/lib/python3.8/site-packages/absl/app.py"", line 308, in run      _run_main(main, args)    File ""/home/hpcadmin/hys/conda_env_tf_2/lib/python3.8/site-packages/absl/app.py"", line 254, in _run_main      sys.exit(main(argv))    File ""./applications/letrain.py"", line 632, in main      LeTrain().keras_model_train(user_mode)    File ""/home/hpcadmin/hys/letrain_dir/letrain2/engine/base_train.py"", line 1473, in keras_model_train      self.run_train_callback(model, dataset_train, data_val, config=config)    File ""./applications/letrain.py"", line 361, in run_train_callback      return train_callback(model=model, dataset_train=dataset_train,    File ""/home/hpcadmin/hys/letrain_dir/letrain2/applications/maskrcnn/get_maskrcnn_loss.py"", line 281, in train_callback      _train(model, config, dataset_train, dataset_val,    File ""/home/hpcadmin/hys/letrain_dir/letrain2/applications/maskrcnn/get_maskrcnn_loss.py"", line 404, in _train      model.compile(learning_rate, config.LEARNING_MOMENTUM)    File ""/home/hpcadmin/hys/letrain_dir/letrain2/applications/maskrcnn/mrcnn/model.py"", line 2525, in compile      self.keras_model.add_metric(loss, name=name, aggregation='mean')    File ""/home/hpcadmin/hys/conda_env_tf_2/lib/python3.8/site-packages/keras/engine/base_layer_v1.py"", line 1242, in add_metric      self._graph_network_add_metric(value, aggregation, name)    File ""/home/hpcadmin/hys/conda_env_tf_2/lib/python3.8/site-packages/keras/engine/functional.py"", line 1009, in _graph_network_add_metric      self._insert_layers(new_layers, new_nodes)    File ""/home/hpcadmin/hys/conda_env_tf_2/lib/python3.8/site-packages/keras/engine/functional.py"", line 936, in _insert_layers      layer_set = set(self._self_tracked_trackables)    File ""/home/hpcadmin/hys/conda_env_tf_2/lib/python3.8/site-packages/tensorflow/python/trackable/data_structures.py"", line 677, in __hash__      raise TypeError(""unhashable type: 'ListWrapper'"")  TypeError: unhashable type: 'ListWrapper'  ERROR:tensorflow:unhashable type: 'ListWrapper'  E1104 09:58:17.444831 140297968976640 letrain.py:642] unhashable type: 'ListWrapper'        Has anyone solved this problem? I have tried to modify it:    self.keras_model.add_loss(loss)  self.keras_ model. add_ metric(loss, name=name, aggregation='mean')  "
"Could not build a TypeSpec for KerasTensor(type_spec=TensorSpec(shape=(None, None, 4), dtype=tf.float32, name=None), name='tf.math.truediv/truediv:0', description=""created by layer 'tf.math.truediv'"") of unsupported type  ."
"Hi, I found some issue on multi-input model problem when I add keras.callbacks.ModelCheckpoint as callback in fit. I use tensorflow 2.6.0 and python 3.9. I don't use tf-nightly because of laptop environment(1660ti).    When I add        keras.callbacks.ModelCheckpoint(              filepath='best_model.keras',              monitor='val_loss',              save_best_only=True          )    in my callback method in model fit, it raise error below;    ValueError: The target structure is of type ` `    KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name='input_1'), name='input_1'...  However the input structure is a sequence ( ) of length 0.    []  nest cannot guarantee that it is safe to map one to the other.    But when I get rid of this option, it works!    I'm really sorry that I can't open my code since its secure. But I was really struggled with handling this error.    Roughly to say my model, there are two inputs and my pre-trained functional model, and concatenate two outputs of pre-trained functional model. Those two functional model is non-trainable(== freezed) things. And there is output with some layers.    Is there any solution about this problem?"
"First of all, I wish all developers good health and all the best ï¼    My training is divided into ""head"" and ""all""  phase, but at the beginning of training ""all"" phase, there was a GPU memory overflow error,  I was very surprised why four RTXA6000 (48G) would get GPU memory overflow error with GPU COUNT = 4 IMAGES_PER_GPU = 4 settings,  because mrcnn/config.py describes the IMAGES_PER_GPU parameter as follows:  # Number of images to train with on each GPU. A 12GB GPU can typically handle 2 images of 1024x1024px.  # Adjust based on your GPU memory and image sizes. Use the highest number that your GPU can handle for best performance.    What causes such a problem?             My model building environment is:  cuda11.3 + cudnn8.2.1 + tensorflow-gpu2.6 + python3.8            My config are as follows ï¼š  Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     16  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.7  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      4  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 4  IMAGE_CHANNEL_COUNT            3  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                22  IMAGE_MIN_DIM                  1024  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           medtest_2464_  NUM_CLASSES                    10  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  PRE_NMS_LIMIT                  6000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                308  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001    "
hello dear ? how can we draw the curve for precision and recall curve for entire dataset not only a single image ????
"Hi, this is my first time asking a question here, and I'm not an English native speaker. So please forgive my poor English.  I used MaskRCNN to train a model that can detect activities on the beach, like swim, surfing, and others.  But I got a really bad training result, it can be said that the model didn't learn anything.    After checking, I found that there is a problem when these codes generate the bbox from the mask.    !   !     The image includes two masks of swimming people. But after generating the bbox process, it only shows one bbox that includes two people.   The bbox should be drawn respectivily one by one people.     I found the code named utils.extract_bboxes(mask), I understand it is the code that can generate bbox coordinate.  But I have no idea why the problem appears when I use my dataset.    Is there any solution or code that can figure out this problem? or is there any explanation for why it will be a problem here?    The bbox generating code from utils.py (line 34, def extract_bboxes(mask))       Thanks a lot.  "
"Hi all,    I've been very excited to get to apply this slightly intimidating project to some new data, but despite all of the great examples of impressive results I've seen out there, I'm really struggling to get results that are at all promising, and so I'm suspecting that there's something fundamental I'm overlooking in my setup.    My dataset consists of aerial RGB shots of a city, with two classes: tree and background.    Images: Aerial RGB photos, all 512x512, training: 324 validation: 36, using random crops of 128x128.  ~46 trees per image on average.    Each training session ends up with something looking pretty similar to this:    !     With the following rough stats when testing on the validation set with no image cropping using the `inspect_model.ipynb` as a guide:     I keep getting the same results (seemingly high confidence with zero or very close to zero IoU, generally clustered at the tops of the images), even after implementing advice I've found elsewhere in this repo (for small datasets) such as only training on heads, initializing with coco weights but not for too long, adjusting my anchor scales to match the general sizes and aspect ratios of the annotations, etc.    So far I'm questioning:  - Is my dataset simply too small for the complexity a Resnet101 backbone?  - Maybe something is up with my annotations?  - I'm screwing up a fundamental aspect of my config  - Unknown unknowns    Checking out the losses, what obviously stands out is the high overall loss (epoch_loss) which increases with each training iteration (just heads -> resnet +4 -> all layers):  <img width=""1242"" alt=""image"" src=""     My config:       So, any initial thoughts on where I'm going wrong?"
"I was trying to run the model on whatever sample was included but I'm facing many problems ranging from dependencies versions at the setup stage to outdated libs at later stages. And I noticed that the latest release was 4-5 years ago, issues are never closed, and if so, they are closed by the authors themself.    I just want to know if there are any forks that are better and actively maintained."
"<img width=""441"" alt=""image"" src=""   <img width=""389"" alt=""image"" src=""   "
"I use Mask-RCNN with weights mask_rcnn_bubble.h5, and it do this:  !   Maybe someone can help me to fix it"
"I input 9 different images as a batch, but it seems the output is 9 repeated results of the first image. Any idea why that happens? Thanks"
"Epoch 1/30  2022-09-23 14:15:21.546178: W .\tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2022-09-23 14:15:21.546334: W .\tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice_36. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2022-09-23 14:15:23.116501: W .\tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2022-09-23 14:15:23.116648: W .\tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice_36. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2022-09-23 14:15:23.694937: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library cublas64_100.dll locally  D:\tf_gpu\tensorflow_gpu\lib\site-packages\scipy\ndimage\interpolation.py:605: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.    ""the returned array has changed."", UserWarning)  2022-09-23 14:34:04.548611: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.53GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.  2022-09-23 14:34:04.548779: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.53GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.  2022-09-23 14:34:06.592608: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.14GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.  2022-09-23 14:34:06.592740: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.14GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.  2022-09-23 14:34:06.629517: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.13GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.  2022-09-23 14:34:06.629639: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.13GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.  2022-09-23 14:34:06.747891: E tensorflow/stream_executor/cuda/cuda_blas.cc:698] failed to run cuBLAS routine cublasSgemm_v2: CUBLAS_STATUS_EXECUTION_FAILED  2022-09-23 14:34:06.749091: I tensorflow/stream_executor/stream.cc:2079] [stream=0000027B44784900,impl=0000027B4F22A8C0] did not wait for [stream=0000027B44785780,impl=0000027B4F22A6E0]  2022-09-23 14:34:06.749181: I tensorflow/stream_executor/stream.cc:5014] [stream=0000027B44784900,impl=0000027B4F22A8C0] did not memcpy device-to-host; source: 000000072A659F00  2022-09-23 14:34:06.749312: F tensorflow/core/common_runtime/gpu/gpu_util.cc:292] GPU->CPU Memcpy failed  Please help me"
"Hi everyone;    would it be possible to use the model to do material detection (between wood, plastic, metal,etc) instead of object detection?    If yes, how can it be done?    Best regards,    Santi"
"my dataset was annotated by labelme image annotator, but i tried convert vgg images annotator json format. anyone can help me to fix an correct json format"
"---------------------------------------------------------------------------  OSError                                   Traceback (most recent call last)  /var/folders/85/ct6fl3cd7jb4xqryxdmdp24m0000gp/T/ipykernel_41965/1575341098.py in         18 model.load_weights(COCO_WEIGHTS_PATH, by_name=True, exclude=[""mrcnn_class_logits"", ""mrcnn_bbox_fc"",  ""mrcnn_bbox"", ""mrcnn_mask""])       19 # train weights (output layers or 'heads')  ---> 20 model.train(dataset_train, dataset_train, learning_rate=config.LEARNING_RATE, epochs=1, layers='heads')    ~/Public/opitblast/venvs/mask-rcnn/lib/python3.7/site-packages/mask_rcnn_tf2-1.0-py3.7.egg/mrcnn/model.py in train(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)     2345         # Create log_dir if it does not exist     2346         if not os.path.exists(self.log_dir):  -> 2347             os.makedirs(self.log_dir)     2348      2349         # Callbacks    /usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/os.py in makedirs(name, mode, exist_ok)      211     if head and tail and not path.exists(head):      212         try:  --> 213             makedirs(head, exist_ok=exist_ok)      214         except FileExistsError:      215             # Defeats race condition when another thread created the path    /usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/os.py in makedirs(name, mode, exist_ok)      221             return      222     try:  --> 223         mkdir(name, mode)      224     except OSError:      225         # Cannot rely on checking for EEXIST, since the operating system    OSError: [Errno 30] Read-only file system: '//logdir'"
Occurs with the new DataGenerator with Sequence          
"  in load_weights(self, filepath, by_name, exclude)     2128      2129         if by_name:  -> 2130             saving.load_weights_from_hdf5_group_by_name(f, layers)     2131         else:     2132             saving.load_weights_from_hdf5_group(f, layers)      in load_weights_from_hdf5_group_by_name(f, layers)     3159     """"""     3160     if 'keras_version' in f.attrs:  -> 3161         original_keras_version = f.attrs['keras_version'].decode('utf8')     3162     else:     3163         original_keras_version = '1'"
"Code:  `model = MaskRCNN(mode='training', model_dir=""logs"", config=config)`  Error:   "
"MackRCNN in google colab .tensorflow=2.4.1,keras=2.4.3    Train the head branches  Passing layers=""heads"" freezes all layers except the head layers. You can also pass a regular expression to select  which layers to train by name pattern.    model.train(dataset_train, dataset_val,              learning_rate=config.LEARNING_RATE,              epochs=10,  --->            layers='heads')     I have this problem:      in __hash__(self)      609     # List wrappers need to compare like regular lists, and so like regular      610     # lists they don't belong in hash tables.  --> 611     raise TypeError(""unhashable type: 'ListWrapper'"")      612       613   def insert(self, index, obj):    TypeError: unhashable type: 'ListWrapper'      Starting at epoch 0. LR=0.001    Checkpoint Path: /content/drive/MyDrive/Mask_RCNN-master/logs/shapes20220901T0127/mask_rcnn_shapes_{epoch:04d}.h5  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  rpn_model              (Functional)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)"
"I had this problem in google colab   tensorflow==2.4.1  kera==2.4.3      if init_with == ""imagenet"":  model.load_weights(model.get_imagenet_weights(), by_name=True)  elif init_with == ""coco"":  # Load weights trained on MS COCO, but skip layers that  # are different due to the different number of classes  # See README for instructions to download the COCO weights  model.load_weights(COCO_MODEL_PATH, by_name=True,  exclude=  in         17     #åŠ è½½é¢„è®­ç»ƒæ¨¡åž‹çš„æƒé‡       18     model.load_weights(COCO_MODEL_PATH, by_name=True,   ---> 19                        exclude=[""mrcnn_class_logits"",""mrcnn_bbox_fc"",""mrcnn_bbox"", ""mrcnn_mask""])       20 elif init_with == ""last"":       21     # Load the last model you trained and continue training          786                                symbolic_weights[i])) +      787                            ', but the saved weight has shape ' +  --> 788                            str(weight_values[i].shape) + '.')      789       790         else:    ValueError: Layer #389 (named ""mrcnn_bbox_fc""), weight   has shape (1024, 40), but the saved weight has shape (1024, 324).    Please anybody can help me."
!   
"use MaskRCNN in google colab:    position:  model = modellib.MaskRCNN(mode=""training"", config=config,model_dir=MODEL_DIR)    mistake tip:    ValueError                                Traceback (most recent call last)    in _apply_op_helper(op_type_name, name, **keywords)      413               preferred_dtype=default_dtype,  --> 414               as_ref=input_arg.is_ref)      415           if input_arg.number_attr and len(    12 frames    in internal_convert_n_to_tensor(values, dtype, name, as_ref, preferred_dtype, ctx)     1566             preferred_dtype=preferred_dtype,  -> 1567             ctx=ctx))     1568   return ret      in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)     1474           ""Tensor conversion requested dtype %s for Tensor with dtype %s: %r"" %  -> 1475           (dtype.name, value.dtype.name, value))     1476     return value    ValueError: Tensor conversion requested dtype int32 for Tensor with dtype int64:      During handling of the above exception, another exception occurred:    TypeError                                 Traceback (most recent call last)    in         19 #4ã€åˆ›å»ºMask RCNNæ¨¡åž‹,æ¨¡åž‹ä¿å­˜åˆ°logsä¸­ã€‚ Create model in training mode       20 print(""MODEL_DIR="",MODEL_DIR)  ---> 21 model = modellib.MaskRCNN(mode=""training"", config=config,model_dir=MODEL_DIR)       22        23       in __init__(self, mode, config, model_dir)     1852         self.model_dir = model_dir     1853         self.set_log_dir()  -> 1854         self.keras_model = self.build(mode=mode, config=config)     1855      1856     def build(self, mode, config):      in build(self, mode, config)     2034                   in __call__(self, *args, **kwargs)      774             try:      775               with ops.enable_auto_cast_variables(self._compute_dtype_object):  --> 776                 outputs = call_fn(cast_inputs, *args, **kwargs)      777       778             except errors.OperatorNotAllowedInGraphError as e:      in call(self, inputs, mask, training)      901     with backprop.GradientTape(watch_accessed_variables=True) as tape,\      902         variable_scope.variable_creator_scope(_variable_creator):  --> 903       result = self.function(inputs, **kwargs)      904     self._check_variables(created_variables, tape.watched_variables())      905     return result      in  (x)     2033             class_loss = KL.Lambda(lambda x: mrcnn_class_loss_graph(*x), name=""mrcnn_class_loss"")(     2034                   in mrcnn_bbox_loss_graph(target_bbox, target_class_ids, pred_bbox)     1132         tf.gather(target_class_ids, positive_roi_ix), tf.int64)     1133     # indices = tf.stack(  in wrapper(*args, **kwargs)      199     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""      200     try:  --> 201       return target(*args, **kwargs)      202     except (TypeError, ValueError):      203       # Note: convert_to_eager_tensor currently raises a ValueError, not a      in stack(values, axis, name)     1387                        (axis, -expanded_num_dims, expanded_num_dims))     1388   -> 1389   return gen_array_ops.pack(values, axis=axis, name=name)     1390      1391       in pack(values, axis, name)     6475   axis = _execute.make_int(axis, ""axis"")     6476   _, _, _op, _outputs = _op_def_library._apply_op_helper(  -> 6477         ""Pack"", values=values, axis=axis, name=name)     6478   _result = _outputs  in _apply_op_helper(op_type_name, name, **keywords)      440                               (prefix, dtype.name))      441             else:  --> 442               raise TypeError(""%s that don't all match."" % prefix)      443           else:      444             raise TypeError(    TypeError: Tensors in list passed to 'values' of 'Pack' Op have types [int32, int64] that don't all match."
"If I train the model in a sequence with different layers, there is a huge jump of loss at the transition from heads to more layers.     However, if I train head first and then start a completely new training for more layers, there is no significant change of lose.      For example:     def train(model):         # Training - Stage 1      epoch_count += 50      print(""Training network heads"")      model.train(dataset_train, dataset_val,                  learning_rate=config.LEARNING_RATE*100,                  epochs=epoch_count,                  layers='heads',       augmentation=None)        epoch_count += 100      print(""Fine tune Resnet stage 3 and up"")      model.train(dataset_train, dataset_val,                  learning_rate=config.LEARNING_RATE*50,                  epochs=epoch_count,                  layers='3+',                  augmentation=augmentation)        epoch_count += 150      print(""Fine tune all layers"")      model.train(dataset_train, dataset_val,                  learning_rate=config.LEARNING_RATE*10,                  epochs=epoch_count,                  layers='all',augmentation=augmentation)    Or:   Train     def train(model):         # Training - Stage 1      epoch_count += 50      print(""Training network heads"")      model.train(dataset_train, dataset_val,                  learning_rate=config.LEARNING_RATE*100,                  epochs=epoch_count,                  layers='heads',       augmentation=None)    Finish and then start a new training:     def train(model):      epoch_count += 100      print(""Fine tune Resnet stage 3 and up"")      model.train(dataset_train, dataset_val,                  learning_rate=config.LEARNING_RATE*50,                  epochs=epoch_count,                  layers='3+',                  augmentation=augmentation)    The grey line is the one with multiple stages in one def train(model) code and the blue line is the manual start of a new training of the second stage. (The blue line was not finished just to use as an example)    !     What could be the problem?     "
"Hi guys,    I am new to Python. I have encountered an issue when trying to run ""inspect_balloon_model.py"".  When running the line ""config = balloon.BalloonConfig()"". It shows: ""AttributeError: module 'balloon' has no attribute 'BalloonConfig'""  Could you please show me how to solve the error? Thanks a lot in advance for your support!    Python version: 3.7.13  Tensorflow 1.15.3; Keras 2.2.4  CUDA v11.7    **This is the full message of AttributeError:**  """"""  config = balloon.BalloonConfig()  Traceback (most recent call last):      File ""C:\Users\Henry Kha\AppData\Local\Temp\ipykernel_26924\409531267.py"", line 1, in        config = balloon.BalloonConfig()    AttributeError: module 'balloon' has no attribute 'BalloonConfig' """""""
"Hello! I'm training my custom dataset for object detection but when I've been training my model, I can see several problems (loss nan and without layers losses):             Config of train and val dataset is below:         "
"When I try to train on a new set of images I get this error:    TypeError: Could not build a TypeSpec for KerasTensor(type_spec=TensorSpec(shape=(None, None, 4), dtype=tf.float32, name=None), name='tf.math.truediv/truediv:0', description=""created by layer 'tf.math.truediv'"") of unsupported type  .    "
"I know the compute_ap function gives you the overlaps of the masks for a image, but I would like to know how could I use this to calculate the mIOU over the whole test dataset.  Also I would be grateful if someone could explain what the overlaps array actually means     I have trouble understanding this, why is returning a 3x3 matrix, I though it would be something like the mean of the overlaps for the image or something like that.  "
"My enviroment is windows NVIDIA GeForce RTX3050LaptopGPU  â€¢ Step 1: I  create Mask_RCNN  environment  python version is 3.6  ï‚§ conda create -n Mask_RCNN python=3.6  â€¢  Step 2:Clone the repository. First, we will clone the mask rcnn repository which has the architecture for Mask R-CNN  Download mrcnn folder from this repository:    â€¢ Step 3: Install the dependencies  ï‚§ pip install tensorflow-gpu==1.15.0  ï‚§ pip install keras==2.2.5  ï‚§ pip install numpy  ï‚§ pip install scipy  ï‚§ pip install Pillow  ï‚§ pip install cython  ï‚§ pip install matplotlib  ï‚§ pip install scikit-image  ï‚§ pip install opencv-python  ï‚§ pip install h5py==2.10.0  ï‚§ pip install  IPython[all]  ï‚§ python setup.py install  â€¢ Step 4: Download the pre-trained weights (trained on MS COCO) ...  ï‚§ Download coco weights : [   â€¢ Step 5: Create folder : Dataset  ï‚§ 3- Create dataset folder. Under dataset folder -> Create 2 folders ""train"" and ""val"".  ï‚§ 4- Put training images in train folder.  ï‚§ 5- Put validation images in val folder  ï‚§ 6- Use VGGAnnotator tool for Annotations."
"hey! :)  I have updated my code to work with grayscale images using the coco pre-trained weights, and it works fine when I exclude the conv1 layer, but it leads to very bad results. I read that a good way to use this layer is to sum the weights of all 3 channels into 1 channel, but I have no idea how to do it (have tried some stuff, and nothing worked).  does anybody have any idea how to do it?   I'm adding a code example I saw for doing this with pytorch from  :  `def _load_pretrained(model, url, inchans=3):      state_dict = model_zoo.load_url(url)      if inchans == 1:          conv1_weight = state_dict['conv1.weight']          state_dict['conv1.weight'] = conv1_weight.sum(dim=1, keepdim=True)      elif inchans != 3:          assert False, ""Invalid number of inchans for pretrained weights""      model.load_state_dict(state_dict)    def resnet50(pretrained=False, inchans=3):      """"""Constructs a ResNet-50 model.      Args:          pretrained (bool): If True, returns a model pre-trained on ImageNet      """"""      model = ResNet(Bottleneck, [3, 4, 6, 3], inchans=inchans)      if pretrained:          _load_pretrained(model, model_urls['resnet50'], inchans=inchans)      return model`    thanks!!!"
What's the reason that ` utils.batch_slice` method's input argument `batch_size` is `config.IMAGES_PER_GPU` instead of `config.BATCH_SIZE` in Mask_RCNN/mrcnn/model.py 290 -303 lines:         Many thanks!    
"It seems like an internal image ID is passed to the load_mask() function at line 1212 in model.py. But how does this make sense? The add_image() function takes the source ID as input. And when implementing load_mask(), how am I supposed to use the internal ID?"
"I trained a mrcnn model using my own dataset. Then I tested this model, no mask found. I printed some variables, and found that the predicted mask values is much smaller than 0.5 as follows:       A full_mask  matrix with all ""False"" value based on this mask. (threshold valuesï¼š0.5)  So what's wrong with my training process? Maybe somebody can help me with this ?"
"Hi, In **Mask_RCNN**, inappropriate dependency versioning constraints can cause risks.    Below are the dependencies and version constraints that the project is using       The version constraint **==** will introduce the risk of dependency conflicts because the scope of dependencies is too strict.  The version constraint **No Upper Bound** and **\*** will introduce the risk of the missing API Error because the latest version of the dependencies may remove some APIs.    After further analysis, in this project,  The version constraint of dependency **keras** can be changed to *>=2.2.0,    The calling methods from the keras      mnist.load_data  saving.load_weights_from_hdf5_group_by_name  saving.load_weights_from_hdf5_group  keras.regularizers.l2                      The calling methods from the  all methods      gt_box.astype  info.np.array.reshape  len  tf.device  tf.reduce_sum  self.random_shape  overlaps_graph  InferenceConfig  config.DetectionTargetLayer  tf.divide  imgaug.augmenters.Fliplr  model.compile  tf.expand_dims  time.time  np.argsort  x.mrcnn_mask_loss_graph.KL.Lambda  tf.nn.top_k  list  self.auto_download  np.around  model_in.append  masks2.shape.masks2.np.reshape.astype  merged.append  utils.resize_mask  K.shape  maskUtils.encode  self.make_parallel  h5py.File  self.build  dataset.load_image  build_fpn_mask_graph  color_splash  gray.image.mask.np.where.astype  layer_outputs.append  tf.add_n  pip.req.parse_requirements  box_centers_x.box_centers_y.np.stack.reshape  tf.where  config.BACKBONE  m.np.around.astype  min  x.parse_image_meta_graph.KL.Lambda  self.compile  chr  set  x_train.np.expand_dims.astype  logging.exception  iaa.Affine  np.random.randint  plt.title  hasattr  self.set_log_dir  dataset.prepare  next  filter  self.DetectionLayer.super.__init__  self.self.__class__.super.image_reference  f.write  model.get_imagenet_weights  masked_image.astype  self.checkpoint_path.replace  skimage.io.imread  n.KL.Concatenate  bg_color.astype  dataset_val.load_balloon  tf.minimum  plt.subplots  ImportError  compute_overlaps  round  K.int_shape  scipy.ndimage.zoom  KL.MaxPooling2D  tf.identity  num_classes.KL.Dense.KL.TimeDistributed  KL.Input  ratios.flatten  random.choice  coco.loadAnns  tf.constant  N.detections.astype  image_id.astype  self.inner_model.summary  np.delete  np.random.choice  self.ParallelModel.super.summary  DetectionTargetLayer  plt.figure  image.astype  tf.cast  saving.load_weights_from_hdf5_group  array.max  modellib.MaskRCNN  config.COMPUTE_BACKBONE_SHAPE  saving.load_weights_from_hdf5_group_by_name  utils.compute_overlaps  self.keras_model.metrics_tensors.append  i.rois.astype  annotations.values  anchors.tf.Variable.KL.Lambda  utils.box_refinement  dataset_val.load_coco  open  self.DetectionTargetLayer.super.__init__  np.abs  plt.ylabel  x.mrcnn_bbox_loss_graph.KL.Lambda  np.asfortranarray  ImageDataGenerator  np.stack  self.config.NAME.lower  rpn  plt.xlabel  np.fliplr  os.path.dirname  FileNotFoundError  tf.reset_default_graph  rle.flatten  i.mask.astype  tf.reduce_mean  np.int32.rle.np.array.reshape  cocoEval.accumulate  coco.loadCats  keras.callbacks.ModelCheckpoint  CocoDataset  name.split  model.train  m.sum  zip_ref.extractall  print  self.random_image  m.max  keras.regularizers.l2  clean_name  build_rpn_model  build_detection_targets  self.map_source_class_id  outputs.keys  np.argmax  os.walk  np.minimum  num_classes.KL.Conv2D.KL.TimeDistributed  images.astype  model.summary  tf.boolean_mask  utils.unmold_mask  tf.sets.set_intersection  COCOeval  plt.text  KL.TimeDistributed  KL.Add  np.split  K.not_equal  mask.T.flatten  re.fullmatch  tf.pad  utils.denorm_boxes  self.keras_model.metrics_names.append  iaa.OneOf  rpn_class_loss_graph  tf.log  iaa.Flipud  i.i.mask.copy  parse_image_meta_graph  tf.concat  colorsys.hsv_to_rgb  datagen.flow  trim_zeros  dataset_val.load_nucleus  self.find_trainable_layer  t.tf.shape.t.tf.reshape.KL.Lambda  model.get_trainable_layers  self.config.LOSS_WEIGHTS.get  np.broadcast_to  checked.append  pred_match.np.cumsum.astype  visualize.display_instances  np.cumsum  iaa.SomeOf  np.zeros  build_model  K.switch  cv2.VideoWriter  box_widths.box_heights.np.stack.reshape  K.mean  getattr  cv2.VideoCapture  np.sqrt  np.expand_dims  vcapture.get  boxes.astype  image_info.update  KL.Conv2DTranspose.KL.TimeDistributed  KL.Lambda  detect_and_color_splash  original_image_shape.astype  tf.Variable  iaa.Multiply  KL.Conv2DTranspose  self.CocoDataset.super.load_mask  np.where  r.astype  COCO  KL.UpSampling2D  setup  KL.Conv2D.KL.TimeDistributed  image_metas.append  apply_mask  tf.reshape  i.overlaps.max  tf.image.non_max_suppression  compute_overlaps_masks  map  tf.split  f.startswith  urllib.request.urlopen  np.logical_not  Exception  datetime.datetime  tf.random_shuffle  ax.set_xlim  generate_random_rois  self.config.WEIGHT_DECAY.keras.regularizers.l2  tf.abs  skimage.color.gray2rgb  smooth_l1_loss  box_to_level.append  scale.shift.boxes.np.divide.astype  iaa.Fliplr  os.path.abspath  ax.set_ylim  plt.axis  num_classes.s.KL.Reshape  self.set_trainable  LooseVersion  build_coco_results  self.inner_model  json.load  clip_boxes_graph  np.pad  np.divide  w.std  trim_zeros_graph  compute_ap  np.multiply  IPython.display.display  matplotlib.use  g.np.where.reshape  display_images  super  np.any  l.get_weights  x.rpn_class_loss_graph.KL.Lambda  OrderedDict  self.__class__.super.image_reference  block.stage.str.KL.Activation  random.shuffle  AP.append  self.ParallelModel.super.__init__  ax.imshow  array.min  mask.astype  use_bias.conv_name_base.kernel_size.kernel_size.nb_filter2.KL.Conv2D  re.match  args.model.lower  compute_backbone_shapes  tf.map_fn  datetime.datetime.now  image_shape.astype  np.logical_and  evaluate_coco  f.endswith  os.makedirs  math.sin  np.all  itertools.product  keras.callbacks.TensorBoard  imgaug.HooksImages  coco.loadRes  s.i.name.input_slices.KL.Lambda  name.KL.Concatenate  clipped.set_shape  tf.squeeze  tf.nn.sparse_softmax_cross_entropy_with_logits  class_keep.set_shape  K.learning_phase  mrcnn_mask_loss_graph  Polygon  table.append  argparse.ArgumentParser  logging.warning  tf.size  KM.Model  np.dot  ax.set_title  bool  outputs_all.append  log2_graph  dataset_train.load_balloon  np.arange  text.ljust  self.get_anchors  np.argwhere  self.keras_model.fit_generator  fpn_classifier_graph  plt.savefig  type  utils.generate_pyramid_anchors  log  BalloonDataset  dataset.get_source_class_id  random_colors  dataset_train.prepare  masks1.shape.masks1.np.reshape.astype  data_generator  DetectionLayer  resnet_graph  dataset.load_mask  model.fit_generator  id.rois.astype  KL.Activation  build_rpn_targets  K.function  ax.text  dataset.load_nucleus  dataset_train.load_nucleus  coco.imgs.keys  np.log  config.config.RPN_NMS_THRESHOLD.proposal_count.ProposalLayer  np.reshape  self.mold_inputs  to_display.append  plt.imshow  BalloonConfig  lines.append  tf.shape  math.radians  self.keras_model.get_layer  x.config.rpn_bbox_loss_graph.KL.Lambda  instance_masks.append  zip  coco.getCatIds  pick.append  tf.range  utils.compute_matches  rle.split  image.copy  skimage.color.rgb2gray  np.sort  kf  self.ancestor  skimage.io.imsave  plt.yticks  mold_image  cv2.VideoWriter_fourcc  boxes.append  model.load_weights  utils.non_max_suppression  pip.download.PipSession  plt.xticks  tf.greater  max  BatchNorm  image_ids.extend  identity_block  self.PyramidROIAlign.super.__init__  CocoConfig  use_bias.conv_name_base.nb_filter1.KL.Conv2D  sys.path.append  dataset_train.load_coco  np.amax  titles.append  vwriter.write  K.binary_crossentropy  model.detect  ax.add_line  source.self.source_class_ids.append  l.outputs_all.append  pool_size.pool_size.fc_layers_size.KL.Conv2D.KL.TimeDistributed  self.class_info.append  scales.flatten  np.copy  plt.subplot  mrcnn_class_loss_graph  tf.sparse_tensor_to_dense  windows.append  self.annToRLE  resize  tf.cond  inputs.extend  compose_image_meta  skimage.draw.polygon  apply_box_deltas_graph  tf.unique  tf.stack  ax.plot  multiprocessing.cpu_count  full_masks.append  compute_iou  coco.getImgIds  dir  overlaps.max  K.cast  np.unique  np.round  mask_to_rle  results.extend  num_classes.KL.Dense  bn_name_base.BatchNorm  config.display  KL.Flatten  str  molded_images.append  config.MEAN_PIXEL.normalized_images.astype  self.self.__class__.super.load_mask  K.abs  maskUtils.decode  x_test.np.expand_dims.astype  callable  conv_block  vcapture.read  tf.gather  sorted  mrcnn_bbox_loss_graph  cv2.rectangle  self.image_info.append  norm_boxes_graph  load_image_gt  rpn_graph  mask.reshape  class_ids.append  train  np.random.shuffle  K.reshape  layer_regex.keys  x.mrcnn_class_loss_graph.KL.Lambda  tf.sqrt  x.KL.Lambda  self.add_class  keras.optimizers.SGD  parser.add_argument  model.find_last  join  os.path.join  maskUtils.frPyObjects  tf.exp  i.refined_boxes.astype  class_ids.astype  random.randint  pip_ver.split  AP.np.array.mean  m.group  np.uint32.image.astype.copy  rle_encode  threshold.mask.np.where.astype  refine_detections_graph  int  outputs.append  mnist.load_data  tf.round  tf.maximum  outputs_np.items  self.add_image  box.astype  utils.norm_boxes  cv2.fillPoly  KL.ZeroPadding2D  display_table  _parse_requirements  tf.multiply  PyramidROIAlign  np.maximum  plt.tight_layout  tuple  np.ones  enumerate  fc_layers_size.KL.Conv2D.KL.TimeDistributed  config.TOP_DOWN_PYRAMID_SIZE.KL.Conv2D  self.self.__class__.super.call  shapes.append  batch_pack_graph  mask.np.logical_not.astype  np.meshgrid  re.compile  KL.Activation.KL.TimeDistributed  self.ProposalLayer.super.__init__  np.exp  pool_size.pool_size.PyramidROIAlign  BatchNorm.KL.TimeDistributed  KL.Concatenate  K.sparse_categorical_crossentropy  utils.box_refinement_graph  submission.append  utils.extract_bboxes  vwriter.release  np.random.rand  use_bias.conv_name_base.nb_filter3.KL.Conv2D  mask.append  range  K.less  tf.reduce_max  anchor_stride.KL.Conv2D  warnings.catch_warnings  lines.Line2D  ax.axis  ParallelModel  coco.getAnnIds  find_contours  NucleusInferenceConfig  detect  compute_matches  self.CocoDataset.super.image_reference  config.DetectionLayer  scores.argsort  det.augment_image  self.keras_model.add_loss  self.draw_shape  tf.image.crop_and_resize  anchors_per_location.KL.Conv2D  results.append  tf.equal  warnings.simplefilter  outputs.values  tf.to_float  window.astype  math.ceil  NucleusDataset  dataset_val.prepare  anchors.append  utils.resize_image  zipfile.ZipFile  tf.logical_and  get_file  skimage.transform.resize  KL.Conv2D  utils.batch_slice  layers.append  maskUtils.merge  np.concatenate  rpn_bbox_loss_graph  self.class_names.index  f.close  np.sum  self.annToMask  utils.compute_iou  tf.argmax  detection_targets_graph  w.max  use_bias.conv_name_base.strides.nb_filter1.KL.Conv2D  display_instances  gray.astype  pkg_resources.get_distribution  os.path.exists  self.ParallelModel.super.__getattribute__  self.keras_model.compile  NucleusConfig  a.values  generate_anchors  tf.transpose  parser.parse_args  args.weights.lower  cocoEval.evaluate  f.mask_dir.os.path.join.skimage.io.imread.astype  np.max  IPython.display.HTML  K.sum  ax.add_patch  iaa.GaussianBlur  np.empty  tf.Assert  self.unmold_detections  plt.show  inputs.append  name.replace  cocoEval.summarize  graph_fn  K.squeeze  scale.astype  isinstance  KL.Reshape  pooled.append  utils.minimize_mask  x.K.squeeze.K.squeeze.KL.Lambda  tf.stop_gradient  cv2.circle  active_class_ids.astype  shutil.copyfileobj  gt_w.gt_h.class_mask.utils.resize.np.round.astype  w.min  K.equal  shift.scale.boxes.np.multiply.np.around.astype  np.array  tf.gather_nd  KL.Dense  tf.tile  a.startswith  utils.resize  np.diff  format  tf.control_dependencies  np.hstack  patches.Rectangle  tf.name_scope  ProposalLayer  outputs.extend  name.outputs.len.o.tf.add_n.KL.Lambda  utils.download_trained_weights  instance_masks.np.stack.astype  augmentation.to_deterministic  input_image.K.shape.x.norm_boxes_graph.KL.Lambda  use_bias.conv_name_base.strides.nb_filter3.KL.Conv2D  self.keras_model.predict                  @moorage   Could please help me check this issue?  May I pull a request to fix it?  Thank you very much.  "
It tooks 50s to detect a picture with i5-8700 CPU and GTX 1060.    tensorflow-gpu         1.5.0  Keras                         2.2.0    You see many of the same problems in `Issues` as follow:   Detecion time toooooo long #2378  It takes 55 seconds to detect a dog in doge.jpg #1895  ...  
"Hi everyone,    is the parameter train_rois_per_image meant per class or in general per image?    As an example: I have 3 classes and I know that each image has exactly one instance of each class. Do I use 1 or 3 then as train_rois_per_image?    Thanks ðŸ˜ƒ "
None
!        
"Hello,  I came across a given problem:  I trained Mask RCNN model and it works fine in an interference mode on images that have annotations, however when I want to work with an image without annotations I get *** No instances to display ***  messange and the image displays as a black window. I use the following code, that was recommended to work for not labeled image:    image1 = mpimg.imread('xxxx/valIMG_1.png')    print(len([image1]))  results1 = model.detect([image1], verbose=1)    ax = get_ax(1)  r1 = results1[0]  visualize.display_instances(image1, r1['rois'], r1['masks'], r1['class_ids'],                              VALset.class_names, r1['scores'], ax=ax,                              title=""Predictions1"")    Thx in advance for all feedback!"
"Hi all,    I am currently using the   for a object tracking project. I have completed the training process successfully, with the loss functions displayed below (read from the event file).   !   Note that I set epochs for head layer as 80 (first stage), and epochs for fine tuning all layers as 160 (second stage).    **My questions are:**     1. Why the event file only shows 80 epochs, and the tensorboard can read as far as 160 epochs?   2. How to evaluate my training results from this graph? (loss stabilizes at about1.0, and mrcnn_class_loss keeps fluctuating) Note that I have only one class in my project.   3. Based on Q2, how can I improve my model (from dataset or hyperparameter setup)?    I proposed a similar issue  . I have checked   to understand the learning curve, and   to learn about hyperparameters. Hope my info can help you deal with the problem.    If you have any ideas, please don't hesitate to comment below.  Many thanks in advance for your kind support.    Best,    Erin"
"Hi I am doing a custom training of Mask RCNN with my dataset of different sizes and resolution of images. I have already done annotating using VGG annotating tool. I started training but Index errors appear while training:     Traceback (most recent call last):    File ""/content/drive/MyDrive/Train_Crack_June19/mrcnn/model.py"", line 1870, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/content/drive/MyDrive/Train_Crack_June19/mrcnn/model.py"", line 1385, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""coco.py"", line 316, in load_mask      mask[rr, cc, i] = 1  IndexError: index 402 is out of bounds for axis 0 with size 402  ERROR:root:Error processing image {'id': '00401.jpg', 'source': 'object', 'path': 'Dataset/val/00401.jpg', 'width': 224, 'height': 224, 'polygons': [{'name': 'polygon', 'all_points_x': [5, 29, 43, 57, 63, 71, 71, 63, 61, 8, 6], 'all_points_y': [246, 250, 250, 249, 243, 243, 246, 253, 255, 255, 253]}, {'name': 'polygon', 'all_points_x': [97, 90, 105, 106, 116, 113, 106, 104, 108, 94, 86, 77, 72, 62, 57, 51, 38, 31, 12, 24, 41, 54, 73, 67, 88, 99, 84, 71, 83], 'all_points_y': [3, 18, 39, 45, 57, 78, 94, 127, 134, 142, 155, 169, 173, 195, 211, 218, 222, 223, 223, 212, 206, 174, 156, 148, 128, 63, 45, 18, 0]}], 'num_ids': [1, 1]}  Traceback (most recent call last):    File ""/content/drive/MyDrive/Train_Crack_June19/mrcnn/model.py"", line 1870, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/content/drive/MyDrive/Train_Crack_June19/mrcnn/model.py"", line 1385, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""coco.py"", line 316, in load_mask      mask[rr, cc, i] = 1  IndexError: index 243 is out of bounds for axis 0 with size 224      and so on..    This is my code    `                   def load_coco(self, dataset_dir, subset):                      #, year=DEFAULT_DATASET_YEAR, class_ids=None                    #class_map=None, return_coco=False, auto_download=False):          """"""Load a subset of the COCO dataset.          dataset_dir: The root directory of the COCO dataset.          subset: What to load (train, val, minival, valminusminival)          year: What dataset year to load (2014, 2017) as a string, not an integer          class_ids: If provided, only loads images that have the given classes.          class_map: TODO: Not implemented yet. Supports maping classes from              different datasets to the same class ID.          return_coco: If True, returns the COCO object.          auto_download: Automatically download and unzip MS-COCO images and annotations          """"""          # if auto_download is True:          #     self.auto_download(dataset_dir, subset, year)                    self.add_class(""object"", 1, ""crack"")          assert subset in [""train"", ""val""]          dataset_dir = os.path.join(dataset_dir, subset)            if subset == ""train"":            annotations = json.load(open(os.path.join(""/content/drive/MyDrive/Train_Crack_June19/Dataset/train"", ""train_json.json"")))          elif subset == ""val"":            annotations = json.load(open(os.path.join(""/content/drive/MyDrive/Train_Crack_June19/Dataset/val"", ""val_json.json"")))                    annotations = list(annotations.values())          annotations = [a for a in annotations if a['regions']]            for a in annotations:               # print(a)              # Get the x, y coordinaets of points of the polygons that make up              # the outline of each object instance. There are stores in the              # shape_attributes (see json format above)              polygons = [r['shape_attributes'] for r in a['regions']]               objects = [s['region_attributes']['crack'] for s in a['regions']]              print(""objects:"",objects)              name_dict = {""crack"": 1}                # key = tuple(name_dict)              num_ids = [name_dict[a] for a in objects]                     # num_ids = [int(n['Event']) for n in objects]              # load_mask() needs the image size to convert polygons to masks.              # Unfortunately, VIA doesn't include it in JSON, so we must read              # the image. This is only managable since the dataset is tiny.              print(""numids"",num_ids)              image_path = os.path.join(dataset_dir, a['filename'])              image = skimage.io.imread(image_path)              height, width = image.shape[:2]                self.add_image(                  ""object"",  ## for a single class just add the name here                  image_id=a['filename'],  # use file name as a unique image id                  path=image_path,                  width=width, height=height,                  polygons=polygons,                  num_ids=num_ids                  )`    `                   def load_mask(self, image_id):          """"""Generate instance masks for an image.          Returns:          masks: A bool array of shape [height, width, instance count] with              one mask per instance.          class_ids: a 1D array of class IDs of the instance masks.          """"""          # If not a Dog-Cat dataset image, delegate to parent class.          image_info = self.image_info[image_id]          if image_info[""source""] != ""object"":              return super(self.__class__, self).load_mask(image_id)            # Convert polygons to a bitmap mask of shape          # [height, width, instance_count]          info = self.image_info[image_id]          if info[""source""] != ""object"":              return super(self.__class__, self).load_mask(image_id)          num_ids = info['num_ids']          mask = np.zeros([info[""height""], info[""width""], len(info[""polygons""])],                          dtype=np.uint8)          for i, p in enumerate(info[""polygons""]):              # Get indexes of pixels inside the polygon and set them to 1            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])            mask[rr, cc, i] = 1                       # Return mask, and array of class IDs of each instance. Since we have          # one class ID only, we return an array of 1s          # Map class names to class IDs.          num_ids = np.array(num_ids, dtype=np.int32)          return mask, num_ids #np.ones([mask.shape[-1]], dtype=np.int32)`    does training with **different sized** images causes it?     I saw this answer   but all it did was modify the ""mask[rr,cc,i] = 1"". I reviewed my annotations but it doesn't have any overlapping vertex.     If I will modify the ""mask[rr,cc,i] = 1"" will this affect my training accuracy? Hoping for answers. Thanks.  "
The model basically is trained with image dataset of JPG and PNG formats and now in inference for the same image file with PNG it's giving a set of outputs which is different from JPG inferencing  
I encountered a problem. My program stuck in this step during training and didn't continue to execute all night. Does anyone know what the problem is?    !   
"hey!  my project also involves cell detection, so I thought I'd try training my CNN using  .  but when I try I get the following error:  `ValueError: Layer #362 (named ""anchors"") expects 1 weight(s), but the saved weights have 0 element(s).`  The training works fine for pretrained coco weigths for example.  this is the code i use to load the weights:    `model = MaskRCNN(mode='training', model_dir='./', config=config) model.load_weights('Usiigaci_3.h5', by_name=True, exclude=[""mrcnn_class_logits"", ""mrcnn_bbox_fc"", ""mrcnn_bbox"", ""mrcnn_mask""])`    I also get a similar problem when trying to load the weights that were generated by training my model over my own photos, to continue the training where I have stopped the last time.  the error received is:  `ValueError: Layer #362 (named ""anchors""), weight   has shape (4, 261888, 4), but the saved weight has shape (2, 261888, 4).  `  loading the weights:  `model.load_weights('new_weigths/40_epochs/mask_rcnn_cell_cfg_0040.h5', by_name=True,                     exclude=[""mrcnn_class_logits"", ""mrcnn_bbox_fc"", ""mrcnn_bbox"", ""mrcnn_mask""])  `    please let me know if you understand why is this happening.  thanks!!"
"Hi guys, I'm facing this error when I do the training and the number of images in the val folder is 8, all the images are in JPG format, I don't understand what's happening, could you please help me?    C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\Scripts\python.exe C:/Users/Abderrahmen/OneDrive/Bureau/Try/custom.py  C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])  C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])  C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])  C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])  C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])  C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    np_resource = np.dtype([(""resource"", np.ubyte, 1)])  C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])  C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])  C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])  C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])  C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])  C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    np_resource = np.dtype([(""resource"", np.ubyte, 1)])  Using TensorFlow backend.  WARNING:tensorflow:From C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\keras\backend\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.    WARNING:tensorflow:From C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\keras\backend\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.    WARNING:tensorflow:From C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\keras\backend\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.    WARNING:tensorflow:From C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\keras\backend\tensorflow_backend.py:2139: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.    WARNING:tensorflow:From C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\keras\backend\tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.    WARNING:tensorflow:From C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\keras\backend\tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.    WARNING:tensorflow:From C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\tensorflow\python\ops\array_ops.py:1354: add_dispatch_support. .wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.  Instructions for updating:  Use tf.where in 2.0, which has the same broadcast rule as np.where  WARNING:tensorflow:From C:\Users\Abderrahmen\OneDrive\Bureau\Try\mrcnn\model.py:553: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.    WARNING:tensorflow:From C:\Users\Abderrahmen\OneDrive\Bureau\Try\mrcnn\utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.    WARNING:tensorflow:From C:\Users\Abderrahmen\OneDrive\Bureau\Try\mrcnn\model.py:600: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.  Instructions for updating:  box_ind is deprecated, use box_indices instead  2022-06-07 16:21:03.602896: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2  objects: ['pallet']  numids [1]  objects: ['pallet']  numids [1]  objects: ['pallet']  numids [1]  objects: ['pallet']  numids [1]  objects: ['pallet']  numids [1]  objects: ['pallet']  numids [1]  objects: ['pallet']  numids [1]  objects: ['pallet']  numids [1]  objects: ['pallet']  numids [1]  objects: ['pallet']  numids [1]  objects: ['pallet']  numids [1]  objects: ['pallet']  numids [1]  objects: ['pallet']  numids [1]  objects: ['pallet']  numids [1]  objects: ['pallet']  numids [1]  objects: ['pallet']  numids [1]  objects: ['pallet']  numids [1]  objects: ['pallet']  numids [1]  objects: ['pallet']  numids [1]  objects: ['pallet']  numids [1]  objects: ['pallet']  numids [1]  objects: ['pallet']  numids [1]  Traceback (most recent call last):    File ""C:/Users/Abderrahmen/OneDrive/Bureau/Try/custom.py"", line 186, in        train(model)    File ""C:/Users/Abderrahmen/OneDrive/Bureau/Try/custom.py"", line 159, in train      dataset_val.load_custom(""C:\\Users\\Abderrahmen\\OneDrive\\Bureau\\Try\\dataset"", ""val"")    File ""C:/Users/Abderrahmen/OneDrive/Bureau/Try/custom.py"", line 97, in load_custom      image = skimage.io.imread(image_path)    File ""C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\skimage\io\_io.py"", line 48, in imread      img = call_plugin('imread', fname, plugin=plugin, **plugin_args)    File ""C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\skimage\io\manage_plugins.py"", line 210, in call_plugin      return func(*args, **kwargs)    File ""C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\skimage\io\_plugins\imageio_plugin.py"", line 10, in imread      return np.asarray(imageio_imread(*args, **kwargs))    File ""C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\imageio\__init__.py"", line 97, in imread      return imread_v2(uri, format=format, **kwargs)    File ""C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\imageio\v2.py"", line 200, in imread      with imopen(uri, ""ri"", **imopen_args) as file:    File ""C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\imageio\core\imopen.py"", line 118, in imopen      request = Request(uri, io_mode, format_hint=format_hint, extension=extension)    File ""C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\imageio\core\request.py"", line 248, in __init__      self._parse_uri(uri)    File ""C:\Users\Abderrahmen\PycharmProjects\pythonProject1\venv\lib\site-packages\imageio\core\request.py"", line 407, in _parse_uri      raise FileNotFoundError(""No such file: '%s'"" % fn)  FileNotFoundError: No such file: 'C:\Users\Abderrahmen\OneDrive\Bureau\Try\dataset\val\9.jpg'    Process finished with exit code 1  "
I cloned the TF 2 branch and adapted the configuration because i have a GPU card with 2GB of memory only.     This is the test image in inference mode:    !     And this is the result:    !     Is there something i missed? 
"I wanted to train Mask RCNN on google Colab on GPU. And this Error/Warning comes in. The same code runs on CPU without any issues. Tensorflow 2.2.0. Keras 2.8.0. Does it have influence on training performance? Any suggestions?    Configurations:  BACKBONE                       resnet50  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.9  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 1  IMAGE_CHANNEL_COUNT            3  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                14  IMAGE_MIN_DIM                  800  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (84, 84)  NAME                           gland  NUM_CLASSES                    2  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  PRE_NMS_LIMIT                  6000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                100  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001      Downloading pretrained model to /content/projektBadawczy/mask_rcnn_coco.h5 ...  ... done downloading pretrained model!  Loading weights  /content/projektBadawczy/mask_rcnn_coco.h5  2022-05-29 06:40:47.457746: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.  Training network heads    Starting at epoch 0. LR=0.001    Checkpoint Path: /content/projektBadawczy/logs/gland20220529T0640/mask_rcnn_gland_{epoch:04d}.h5  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  rpn_model              (Functional)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)  /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.    super(SGD, self).__init__(name, **kwargs)  Epoch 1/30  /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(""training/SGD/gradients/gradients/roi_align_classifier/concat_grad/sub:0"", shape=(None,), dtype=int32), values=Tensor(""training/SGD/gradients/gradients/roi_align_classifier/concat_grad/GatherV2_2:0"", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(""training/SGD/gradients/gradients/roi_align_classifier/concat_grad/Shape:0"", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""shape. This may consume a large amount of memory."" % value  2022-05-29 06:44:01.503850: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_FLOAT } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_FLOAT shape { dim { size: -438 } dim { size: 84 } dim { size: 84 } dim { size: 1 } } } inputs { dtype: DT_FLOAT shape { dim { size: -25 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -25 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 28 } } device { type: ""GPU"" vendor: ""NVIDIA"" model: ""Tesla T4"" frequency: 1590 num_cores: 40 environment { key: ""architecture"" value: ""7.5"" } environment { key: ""cuda"" value: ""11010"" } environment { key: ""cudnn"" value: ""8005"" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14465892352 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { dim { size: -25 } dim { size: 28 } dim { size: 28 } dim { size: 1 } } }  100/100 [==============================] - 168s 1s/step - batch: 49.5000 - size: 1.0000 - loss: 5.7086 - rpn_class_loss: 0.4854 - rpn_bbox_loss: 2.6237 - mrcnn_class_loss: 0.4647 - mrcnn_bbox_loss: 1.4705 - mrcnn_mask_loss: 0.6643 - val_loss: 5.5708 - val_rpn_class_loss: 0.3285 - val_rpn_bbox_loss: 2.2625 - val_mrcnn_class_loss: 0.8341 - val_mrcnn_bbox_loss: 1.4832 - val_mrcnn_mask_loss: 0.6625      "
"I am getting very poor training and validation results. I am trying to train on a custom dataset using coco_weights as a starting weight. There is only one instance of the object in every image.  Is it possible my config parameters setting is not good?        IMAGES_PER_GPU = 1      TRAIN_ROIS_PER_IMAGE=300        NUM_CLASSES = 1 + 1         STEPS_PER_EPOCH =100        DETECTION_MIN_CONFIDENCE = 0.95      VALIDATION_STEPS = 50      MAX_GT_INSTANCES = 1      DETECTION_MAX_INSTANCES = 1            USE_MINI_MASK = True    I set DETECTION_MAX_INSTANCES to 1 because there is only 1 object in all images. i have tried other configurations though with no result  I also set MAX_GT_INSTANCES to 1 because all training images also have only one image, even though I have also tried large values with no result.  I have also tried varying values for TRAIN_ROIS_PER_IMAGE  I have set USE_MINI_MASK to both true and false.  Also, I need to know what values would work best for IMAGE_MIN_DIM,RPN_ANCHOR_SCALES    Also, I suspected the TensorFlow version.  Was obtaining poor results using Tensorflow 2.6.0.  I then tested writing a basic program predicting a simple coco image using coco weights and was obtaining a result predicting 4+ bbs when there was only 1 object. This changes after downgrading to 2.5.0 and I was able to get an accurate prediction on the basic image.  However, using Tensorflow 2.5.0, I am still getting poor training and validation results on my custom dataset.  Please any advice would be appreciated  !   "
"I have a question about the RPN training step. We will use five feature maps to include in the RPN (P2~P6), as shown below. But as everyone knows, feature maps have different sizes (width x height).    !     But in theory, only one RPN is trained, which means only one feature map size is used. So is there any step to resize feature maps and then put them into RPN training? I don't understand this part very well. Thanks!"
"Hello,    ### Understanding of my mind on the code.  I am trying to understand, how the mrcnn's `model.py` works. I have some troubles as an absolute beginner, with the following:    1.  How does the method `build()` in `class MaskRCNN():`  receives the ground truth data:    -  for example, `input_gt_boxes = KL.Input(shape=[None, 4], name=""input_gt_boxes"", dtype=tf.int32)` declares the shape of the input layer `input_gt_boxes` (I understand that this is an symbolic input in keras).   -  I see that the two functions `def load_image_gt()` and `def data_generator()` are the storage blocks of the ground truth data. The variable `bbox` in `def load_image_gt()` contains the ground truth values for each instances per image.  -  Now, when `def load_image_gt()` is called in `def data_generator()` the new variable `gt_boxes` have the ground truth.  >  The question is how and where does the magic happen, how does the input layer `input_gt_boxes` gets the knowledge of the ground truth data `gt_boxes` in  `def data_generator()`. Later, I see that the `gt_boxes` are defined in `def norm_boxes_graph()` with `input_gt_boxes` as input to train the model, but what does the `input_gt_boxes` contains in the first place."
"  in train(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)     2372             max_queue_size=100,     2373             workers=workers,  -> 2374             use_multiprocessing=True,     2375         )     2376         self.epoch = max(self.epoch, epochs)      in wrapper(*args, **kwargs)       85                 warnings.warn('Update your `' + object_name +       86                               '` call to the Keras 2 API: ' + signature, stacklevel=2)  ---> 87             return func(*args, **kwargs)       88         wrapper._original_function = func       89         return wrapper      in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)     2009                 batch_index = 0     2010                 while steps_done   2011                     generator_output = next(output_generator)     2012      2013                     if not hasattr(generator_output, '__len__'):    StopIteration:"
"  in train(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)     2372             max_queue_size=100,     2373             workers=workers,  -> 2374             use_multiprocessing=True,     2375         )     2376         self.epoch = max(self.epoch, epochs)      in wrapper(*args, **kwargs)       85                 warnings.warn('Update your `' + object_name +       86                               '` call to the Keras 2 API: ' + signature, stacklevel=2)  ---> 87             return func(*args, **kwargs)       88         wrapper._original_function = func       89         return wrapper      in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)     2009                 batch_index = 0     2010                 while steps_done   2011                     generator_output = next(output_generator)     2012      2013                     if not hasattr(generator_output, '__len__'):    StopIteration:"
"def make_parallel(self):          """"""Creates a new wrapper model that consists of multiple replicas of          the original model placed on different GPUs.          """"""          # Slice inputs. Slice inputs on the CPU to avoid sending a copy          # of the full inputs to all GPUs. Saves on bandwidth and memory.          input_slices = {name: tf.split(x, self.gpu_count)                          **for name, x in zip(self.inner_model.input_names,**                                             self.inner_model.inputs)}        error in the bold line "
"When using mobile Mask RCNN, I am getting this weird error when adding model weights, can anyone help me? thanks  ValueError: Layer #100 (named ""anchors"") expects 1 weight(s), but the saved weights have 0 element(s)."
"I am just using **maskrcnn** model for my project. But i am unable to find out the accuracy  of my model and plot the learning curve( train and validation Vs epoc) and loss graph (i.e train loss vs test  loss). I have read previous discussions abt accuracy saying mAP is used in object detection, but my advisor is requesting me to show him the accuracy(he is familiar only with the normal accuracy metrics) , learning curve and loss graph.    Even i am troubled whether i have to add data for my model cuz i didn't find the top accuracy and learning curve in mask rcnn.  So please help me for the solution , i have only 3 weeks to submit with those parameters."
"I used the code from this <a href=""  and the results are so random even in the demo.ipynb file. What might be the issue and what can I do to fix it. Thank you.  <img src="" "
capture_event.py: error: the following arguments are required: -i/--image  
None
None
"Hi,    I ran this model on a custom dataset TF2.0 CPU and TF2.7GPU. Got good results on test data for object detection on TF2.0 but TF2.7 GPU results  are totally bad. Not a single object was identified after same number of epochs. Is it because MRCNN model is not ported to TF2.7 as yet."
"i have installed tensorflow with cpu version and keras and other environments successfully,   pip install tensorflow==1.15.0   pip install keras==2.2.4    **but how to set a config when i want to inference with cpu?**  For example in **visualize.py**,  should i set GPU_COUNT = 0 ? but here are a comment that Batch size = GPU_COUNT * IMAGES_PER_GPU,  then Batch size will be 0 ????  i am newbie please help me"
I am using this forked project   created by @ahmedfgad the problem is that I am not getting all the losses displayed   `39/100 [==========>...................] - ETA: 1:30:57 - loss: 3.6783`    it's only the total loss and validation loss that are being shown
"CUDA 11.4   tensorflow2.5-gpu  train succeedï¼Œbut test error:  Traceback (most recent call last):    File ""E:/Mask_RCNN/samples/linemod/test.py"", line 98, in        file_names = next(os.walk(IMAGE_DIR))[2]  StopIteration"
"Hey,    I absolutely don't get how the mAP in the utils code is being calculated. is there a paper etc. that this is based on?"
Is it possible to save the coordinates of the predicted Masks and Bounding Box after detection ?
"Hello everyone,    I am using Maskrcnn with TF 2.5 from this   and training with my own dataset. I have 1200 images for train and 100 for validation. I have only one class, which is corrosion and the objects might be large or small. On my first training I saw that while the bboxes are good enough, the mask is overfitting and all the values are < 0.001 so there is no mask shown in the images. Btw I am using coco pretrained model and training only the heads.    The loss for the mask quickly goes to 0.00x which I guess means that it learns to predict everything black, thus overfitting    I experimented using resnet50 since the images are not many and maybe resnet101 is too complex for the data but no luck yet    Any suggestions on how to get good masks ??    Appreciate the help    "
"Hi, i am a new user to Matterport. I'm not sure if this is the best place to ask. If not, please redirect me. I am trying to work with MatterTag's URL feature to link to local machines. The local machines can be assessed via browser by direct IP, example   However, MatterTag does not allow me to key in this.  It says ""Invalid URL"". Any idea how I could do this? "
"I have been trying to run the shapes.py code on various platforms. I use an M1 Pro mac, and I have tried using the Mac terminal, Colab GPU, and even the university cluster. But the code is stuck at the following:    Starting at epoch 0. LR=0.002    Checkpoint Path: /Users/anishphule/Desktop/M_RCNN/logs/shapes20220411T0355/mask_rcnn_shapes_{epoch:04d}.h5  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  rpn_model              (Functional)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)  WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.    It stays like this for hours and the log file doesn't change, niether does the epoch 1 appear. Someone please help!! I'm trying to use this for my term project!"
"  In the predict value of mask im gettin ""False value"" and not the coodinates,why?    This is the code:  !     print(""Images: {}\nClasses: {}"".format(len(dataset.image_ids), dataset.class_names))    path_to_new_image = '/content/drive/MyDrive/MaskRCNN/Stomata/train/ 73-ab1.jpg'  image1 = mpimg.imread(path_to_new_image)      results1 = model.detect([image1], verbose=1)  r1 = results1[0]  mask = r1['masks']  print(mask)    ax = get_ax(1)  r1 = results1[0]  visualize.display_instances(image1, r1['rois'], r1['masks'], r1['class_ids'],  dataset.class_names, r1['scores'], ax=ax, title=""Predictions1"")  "
"I trained a model, now i want from the detect image the x,y of predicted mask, how can i do ?  "
"!     When I try to run my own raster tif file, during the first round of training, I get this error, what is the errorï¼Ÿ      "
"So currently to modify anything in the model ,  I go to   in the cloned repo files  modify what I want   restart the python kernel  reinstall the repo through    restart the python kernel    is there a better way ?  PS: I work on Google colab"
"Hi! After several tries I finally obtained a useful conda enviorment to run at least the demo from the samples folder. I had to set the enviorment to a python 3.6 version and install tensorflow and keras correctly. I hope this information is useful to you, i wasted a lot of time setting up my environment. Cheers!    P.D.: Change the document suffix from .txt to .yml       "
This warning appears when I evaluate the mode   I use the same code as this repo  
"I have a very similar question as asked here:    I would like to extract the color of the background class, how can this be done?  "
"I want to use Mask-RCNN for training an instance segmentation model. I have 20kX20k TIF images along with corresponding masks (masks are also present i XML format).   I want to ask how should I prepare my dataset for this training. Since the annotations are already provided in XML format, how should I preapre the dataset for this training?"
"Hey,    is it possible to use predefined ROI's for a mask detection, coming from an object detection algorithm like YOLO etc.?"
"Hi    I have the weights of a model which uses semantic segmentation in floor plan images to detect things like: walls, windows, type of rooms.    I have a new dataset of floor plan images and I want to train a model,  using Mask-RCNN and the weights of the previous mentioned model.    Is that possible? Or do the annotations have to be equal (same number of instances)?      Thank you      Some details about the previous mentioned model:  *semantic segmentation  *classes for walls windows, types of rooms (kitchen,bathroom)    New dataset:  *annotation for each instance  *classes for walls, windows, doors, columns, types of rooms(kitchen, bedroom, bathroom, office, etc), office seat, sink, toilet, bike parking, etc.      "
"ValueError: Layer #2 (named ""conv1""), weight   has shape (7, 7, 3, 32), but the saved weight has shape (64, 3, 7, 7).  "
"Hi,    For my thesis I'm trying to use Mask-RCNN to detect defects on a metal 3D-printer. I'm getting stuck at the training part... We could already start from weights that have been implemented on another machine that has somewhat resemblance with this one.    A huge dataset isn't really available and I don't have the time to annotate and search a lot of images. I now have a training set of 110  images and a validation set of around 25.     Defects aren't always very clear and often show up by some different tone and I don't know what the effect would be of this.    I would really appreciate some help in getting some feedback and tips on training the model. If you would like some additional information, please let me know!    !     Some extra info:  - The images have initially a size of 1800x1800.  - 7 different classes    What have I already tried?  - Best training so far: train only heads for 20 epochs lr=0.001, 4+ for 30 epochs lr=0.0001  - This gives some results, but some errors are never detected. (like the large horizontal stripes)  - Some parameters: TRAIN_ROIS_PER_IMAGE = 256, MAX_GT_INSTANCES = 20, DETECTION_MAX_INSTANCES = 25, IMAGE_MIN_DIM = 512, IMAGE_MAX_DIM = 512    Some questions I'm wondering about:  - What should the steps_per_epoch size be: len(training_set)/batch_size ?  - Because of the large horizontal shape of some defects and on the other hand sometimes very small defects, should I play with the anchor boxes scales/ratios?  - Images are always taken at the same angle and some faults only appear in one way. Should I use image augmentation (or only play with parameters like brightness, ... and not shearing, rotation, ...)  - Additional parameters I should really play with?"
"File ""custom.py"", line 83, in load_custom      objects = [s['region_attributes']['names'] for s in a['regions']]    File ""custom.py"", line 83, in        objects = [s['region_attributes']['names'] for s in a['regions']]  KeyError: 'names'     I keep getting this error, I keep getting the above error any help, please"
"hey,    I add images to the train/val-dataset add_image-method the you supposed to, giving the 'image_id' the image_id of each image.  After that I use the prepare()-method.      If I type in   `print(dataset_train.image_ids)`    It shows me a list of integers from 0 to the number of images. Should the values be the image_id's I gave to the add_image-function?    Next problem with the load_mask-method: I need to give it the according index of the image in the dataset_train.image_ids-list, not the real image_id in order to receive the right mask. Am I understanding something wrong?"
"Hi Everybody     I've faced this error during Creating the training model,      TypeError: Could not build a TypeSpec for KerasTensor(type_spec=TensorSpec(shape=(None, None, 4), dtype=tf.float32, name=None), name='tf.math.truediv_1/truediv:0', description=""created by layer 'tf.math.truediv_1'"") of unsupported type  .    Any help please ????"
"Hey,  is there a way to get the mAP's for everey class in a picture seperatly? To determine which classes are getting detected better than others?    Thanks so much!"
"Hello,  I'm using the Evaluation method of the shapes.py file.  There is the specific line:  ` molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)`    What is that for? I absolutely don't understand the meaning of that. Could someone help me please?  Thanks a lot!         "
"Hi All,     I'm new here and trying to implement this code and faced the following error, any help please ??  !   "
!   AttributeError: module 'keras.engine' has no attribute 'Layer'  this error happened when i run the demo.ipynb 
I am trying to apply the MaskRCNN architecture to my specific problem which deals with smaller 64x64 images and only a few (2-4) classes.    I really like the   but am completely unfamiliar with TensorFlow and have experience using PyTorch. Also the remainder of my project is in PyTorch so even if I do familiarize with TF I would have to translate anyway ...    Can someone point me to a custom MaskRCNN example like the synthetic shapes one written in PyTorch?  
"Hi, i was doing a project where i need to predict the severity level of osteoarthritis using this (      But instead of giving the knee joint as input, i have a full x-ray and i want to crop the knee joints then give that knee joints as input to my model. so my query was if i train my mrcnn model on the above mentioned dataset with annotation, will it be able to extract the knee joints from full leg xray ?    Thanks     This is a sample input i have :    "
"**Current behavior**  After having set `TRAIN_BN = False` in  , and having called   with  , `BatchNormalization` layers are trainable again.    **Expected behavior**  `BatchNormalization` layers should persist non-trainable if this was set at program startup.    Am I missing something here?    Am I expected to simply exclude `BatchNormalization` layers from the `layer_regex`?    Thanks ðŸ‘ðŸ½ "
"From my understanding:  1) Ground-truth masks all have the size of the input image HxW and are given in a training dataset.  2) Predicted masks have fixed sizes mxm (e.g. 28x28 as in the paper).  3) Target masks are computed as intersections of predicted RoIs and their associated ground-truth mask. So target masks have the size of the predicted RoIs (different sizes in general).    In order to compute the mask loss, the target masks and the predicted masks need to have the same size. To achieve this, do we resize a predicted mxm mask to the corresponding target-mask size or vice versa?"
"I have tried to custom train my own dataset which has it own COCO JSON format file.     When I try to run ""python3 customTrain.py train --dataset=path/to/dir --weights=coco"" I get the following error:      Traceback (most recent call last):    File ""customTrain.py"", line 279, in        train(model)    File ""customTrain.py"", line 179, in train      dataset_train.load_custom(args.dataset, ""train"")    File ""customTrain.py"", line 87, in load_custom      annotations = [a for a in annotations if a['regions']]    File ""customTrain.py"", line 87, in        annotations = [a for a in annotations if a['regions']]  TypeError: list indices must be integers or slices, not str        My **customtrain.py** looks like the following:    `import os  import sys  import json  import datetime  import numpy as np  import skimage.draw    # Root directory of the project  ROOT_DIR = ""/home/hiwi/Auto-Annotate""    # Import Mask RCNN  sys.path.append(ROOT_DIR)  # To find local version of the library  from mrcnn.config import Config  from mrcnn import model as modellib, utils    # Path to trained weights file  COCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, ""mask_rcnn_coco.h5"")    # Directory to save logs and model checkpoints, if not provided  # through the command line argument --logs  DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, ""logs"")    ############################################################  #  Configurations  ############################################################      class CustomConfig(Config):      """"""Configuration for training on the toy  dataset.      Derives from the base Config class and overrides some values.      """"""      # Give the configuration a recognizable name      NAME = ""custom""        IMAGES_PER_GPU = 1        # Number of classes (including background)      NUM_CLASSES = 1 + 2  # Background + 2 classes        # Number of training steps per epoch      STEPS_PER_EPOCH = 100        # Skip detections with   mask.shape[0]-1] = mask.shape[0]-1              cc[cc > mask.shape[1]-1] = mask.shape[1]-1              mask[rr, cc, i] = 1            # Return mask, and array of class IDs of each instance. Since we have          # one class ID only, we return an array of 1s          num_ids = np.array(num_ids, dtype=np.int32)          return mask.astype(np.bool), num_ids.astype(np.bool), np.ones([mask.shape[-1]], dtype=np.int32)          #return mask.astype(np.bool), np.ones([mask.shape[-1]], dtype=np.int32)        def image_reference(self, image_id):          """"""Return the path of the image.""""""          info = self.image_info[image_id]          if info[""source""] == ""Railtrack"":              return info[""path""]          else:              super(self.__class__, self).image_reference(image_id)      def train(model):      """"""Train the model.""""""      # Training dataset.      dataset_train = CustomDataset()      dataset_train.load_custom(args.dataset, ""train"")      dataset_train.prepare()        # Validation dataset      dataset_val = CustomDataset()      dataset_val.load_custom(args.dataset, ""val"")      dataset_val.prepare()        # *** This training schedule is an example. Update to your needs ***      # Since we're using a very small dataset, and starting from      # COCO trained weights, we don't need to train too long. Also,      # no need to train all layers, just the heads should do it.      print(""Training network heads"")      model.train(dataset_train, dataset_val,                  learning_rate=config.LEARNING_RATE,                  epochs=30,                  layers='heads')        ############################################################  #  Training  ############################################################    if __name__ == '__main__':      import argparse        # Parse command line arguments      parser = argparse.ArgumentParser(          description='Train Mask R-CNN to detect custom objects.')      parser.add_argument(""command"",                          metavar="" "",                          help=""'train' or 'splash'"")      parser.add_argument('--dataset', required=False,                          metavar=""/path/to/custom/dataset/"",                          help='Directory of the Custom dataset')      parser.add_argument('--weights', required=True,                          metavar=""/path/to/weights.h5"",                          help=""Path to weights .h5 file or 'coco'"")      parser.add_argument('--logs', required=False,                          default=DEFAULT_LOGS_DIR,                          metavar=""/path/to/logs/"",                          help='Logs and checkpoints directory (default=logs/)')      parser.add_argument('--image', required=False,                          metavar=""path or URL to image"",                          help='Image to apply the color splash effect on')      parser.add_argument('--video', required=False,                          metavar=""path or URL to video"",                          help='Video to apply the color splash effect on')      args = parser.parse_args()        # Validate arguments      if args.command == ""train"":          assert args.dataset, ""Argument --dataset is required for training""      elif args.command == ""splash"":          assert args.image or args.video,\                 ""Provide --image or --video to apply color splash""        print(""Weights: "", args.weights)      print(""Dataset: "", args.dataset)      print(""Logs: "", args.logs)        # Configurations      if args.command == ""train"":          config = CustomConfig()        # Create model      if args.command == ""train"":          model = modellib.MaskRCNN(mode=""training"", config=config,                                    model_dir=args.logs)          # Select weights file to load      if args.weights.lower() == ""coco"":          weights_path = COCO_WEIGHTS_PATH          # Download weights file          if not os.path.exists(weights_path):              utils.download_trained_weights(weights_path)      elif args.weights.lower() == ""last"":          # Find last trained weights          weights_path = model.find_last()      elif args.weights.lower() == ""imagenet"":          # Start from ImageNet trained weights          weights_path = model.get_imagenet_weights()      else:          weights_path = args.weights        # Load weights      print(""Loading weights "", weights_path)      if args.weights.lower() == ""coco"":          # Exclude the last layers because they require a matching          # number of classes          model.load_weights(weights_path, by_name=True, exclude=[              ""mrcnn_class_logits"", ""mrcnn_bbox_fc"",              ""mrcnn_bbox"", ""mrcnn_mask""])      else:          model.load_weights(weights_path, by_name=True)        # Train or evaluate      if args.command == ""train"":          train(model)      else:          print(""'{}' is not recognized. ""                ""Use 'train' or 'splash'"".format(args.command))`      Kindly help."
"So after detecting multiple ROIs in my image, I want to check for the similarity between them, by using an algorithm similar to ArcFace which measures the cosine distance between vector embeddings. Not between the ROIs themselves but between the actual pixels on the image they correspond to. Any ideas on how to go about this? I'm thinking of 2 ways:  1. Find the region in the image corresponding to the ROI, pass it through another simple classification network, take the weights of the last layer of that classification network and use that against the activations generated by passing different ROIs to find the cosine distance.  2. Instead of feeding to another network, take the weights of the layer that outputs different ROIs and use that itself to find the cosine distance. But, I wonder if this is even possible since the same layer outputs multiple ROIs in the shape of [N, 4].  Any help or advice would be appreciated, thanks."
"Hi everybody! I am using TF 2.5, CUDA 11.2 CUDNN 8.1.0 and the code Mask_RCNN provided by      My use case requires continuous stopping and starting of the training process. This does work, however the code hangs at this call to add_loss for about 5 minutes every time I try to restart training.                 # Add L2 Regularization          # Skip gamma and beta weights of batch normalization layers.          reg_losses = [              keras.regularizers.l2(self.config.WEIGHT_DECAY)(w) / tf.cast(tf.size(input=w), tf.float32)              for w in self.keras_model.trainable_weights              if 'gamma' not in w.name and 'beta' not in w.name]          self.keras_model.add_loss(tf.add_n(reg_losses))      The full stack trace from this hanging part is as follows    ^CTraceback (most recent call last):    File ""train.py"", line 110, in        main(args)    File ""train.py"", line 75, in main      trainer.train()    File ""/app/src/trainer.py"", line 315, in train      self.train_epoch()    File ""/app/src/trainer.py"", line 395, in train_epoch      results = self.model.train()    File ""/app/src/models/instance_segmentation/matterport/models.py"", line 237, in train      layers='all'    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 2354, in train    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 2185, in compile    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py"", line 1071, in add_loss      self._graph_network_add_loss(symbolic_loss)    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py"", line 849, in _graph_network_add_loss      new_nodes, new_layers = _map_subgraph_network(self.inputs, [symbolic_loss])    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py"", line 1079, in _map_subgraph_network      base_layer_utils.create_keras_history(outputs)    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py"", line 185, in create_keras_history      _, created_layers = _create_keras_history_helper(tensors, set(), [])    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py"", line 261, in _create_keras_history_helper      constants[i] = backend.function([], op_input)([])    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py"", line 4052, in __call__      self._make_callable(feed_arrays, feed_symbols, symbol_vals, session)    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py"", line 3988, in _make_callable      callable_fn = session._make_callable_from_options(callable_opts)    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1514, in _make_callable_from_options      self._extend_graph()    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1398, in _extend_graph      tf_session.ExtendSession(self._session)  KeyboardInterrupt          Any advice on why this is happening and how to prevent this long hang from occurring would be greatly appreciated! Thank you!"
"I'm running mask rcnn on TF 2.5.0, CUDA 11.1, CUDNN 8.1.1 using   implementation for tf 2.x.       The first epoch is completely fine, and the sum of the 5 main losses (rpn_class_loss, rpn_bbox_loss, mrcnn_class_loss, mrcnn_bbox_loss, and mrcnn_mask_loss) equals the overall loss both for training and validation.     The second epoch, however, is problematic and the 5 losses are 1/2 of the overall losses both in training and validation. I'm using the ""model.train"" again to train for the second epoch.     I also added another model.train after this to train for 2 additional epochs (total epochs=4). This time the 5 losses were 1/3 of the overall loss!!     Epoch1/1:   100/100 [==============================] - 66s 594ms/step - batch: 49.5000 - size: 8.0000 - loss: 1.3710 - rpn_class_loss: 0.0236 - rpn_bbox_loss: 0.5378 - mrcnn_class_loss: 0.2624 - mrcnn_bbox_loss: 0.3398 - mrcnn_mask_loss: 0.2073 - val_loss: 0.7835 - val_rpn_class_loss: 0.0143 - val_rpn_bbox_loss: 0.4560 - val_mrcnn_class_loss: 0.1051 - val_mrcnn_bbox_loss: 0.1279 - val_mrcnn_mask_loss: 0.0802    Epoch2/2:  100/100 [==============================] - 31s 184ms/step - batch: 49.5000 - size: 8.0000 - loss: 1.3859 - rpn_class_loss: 0.0139 - rpn_bbox_loss: 0.3914 - mrcnn_class_loss: 0.0920 - mrcnn_bbox_loss: 0.0965 - mrcnn_mask_loss: 0.0991 - val_loss: 1.2546 - val_rpn_class_loss: 0.0115 - val_rpn_bbox_loss: 0.4192 - val_mrcnn_class_loss: 0.0752 - val_mrcnn_bbox_loss: 0.0570 - val_mrcnn_mask_loss: 0.0645    Epoch3/4:  100/100 [==============================] - 32s 187ms/step - batch: 49.5000 - size: 8.0000 - loss: 1.8418 - rpn_class_loss: 0.0133 - rpn_bbox_loss: 0.3781 - mrcnn_class_loss: 0.0772 - mrcnn_bbox_loss: 0.0624 - mrcnn_mask_loss: 0.0830 - val_loss: 2.0209 - val_rpn_class_loss: 0.0127 - val_rpn_bbox_loss: 0.4384 - val_mrcnn_class_loss: 0.0951 - val_mrcnn_bbox_loss: 0.0625 - val_mrcnn_mask_loss: 0.0649    Epoch4/4:  100/100 [==============================] - 15s 148ms/step - batch: 49.5000 - size: 8.0000 - loss: 1.5852 - rpn_class_loss: 0.0124 - rpn_bbox_loss: 0.3374 - mrcnn_class_loss: 0.0592 - mrcnn_bbox_loss: 0.0508 - mrcnn_mask_loss: 0.0686 - val_loss: 1.7954 - val_rpn_class_loss: 0.0114 - val_rpn_bbox_loss: 0.3634 - val_mrcnn_class_loss: 0.0833 - val_mrcnn_bbox_loss: 0.0660 - val_mrcnn_mask_loss: 0.0743    Does anyone have an idea why this is happening? "
"Hello,    I've seen a number of projects made for TF2.x compatibility. Despite the fact that I have made necessary changes to the project, none of them work properly.    I've been working on a project that has a number of deep learning components that must work together. Also, as I strive to add new features to my project that include detection and segmentation, I've opted to use this framework. I was thinking that if I only changed a little portion of the MaskRCNN, it would still function with the rest of the project. My work, however, has been entangled due to version differences.    So, I was wondering how I could run MaskRCNN without utilizing `tf.compat.v1.disable eager execution()` in TF 2.4.1. If I utilize this line block, it impacts the entire working process of my project and causes numerous issues.      1.  The first error that I get is:         `TypeError: Could not build a TypeSpec for   with type Operation`    Despite I made it a remark line, the project generates similar issues for the rest of the lines.        2. If I change that line block to a comment line, I get the following error:         `TypeError: To be compatible with tf.eager.defun, Python functions must return zero or more Tensors; in compilation of   at 0x0000015B27E0EEE8>, found return value of type  , which is not a Tensor.`    3. Other issues I encountered when working with `tf.compat.v1.disable eager execution()` are listed below.        3.1- I attempted to utilize that block of code to work for only one file, however once you place it in your project, it impacts the entire project. The rest of the project then produces a variety of mistakes.            3.2- After that, I put `tf.compat.v1.enable eager execution()` exactly before the'return' part of the file. In that scenario, the project states that it is not permitted to enable or disable easger execution while the project is in progress.  "
a Monte Carlo Simulation that allows the user to estimate the area under any given   function between any two points over a positive area. Compare your simulation results with the   actual area under the standard normal density   
"Hello,    I try to train this model on ADE20k dataset, but i don't understand how can i use the method add_class for this particular dataset. The dataset comes with a json file like this one:   but im not sure what should i use from here to add as an argument for my add_class method, if anyone can help me a bit.    Thanks a lot"
"Hi,     I have a question about the retraining model.    I've trained the model with 2 classes using Imagenet pre-trained weights it's working perfectly fine. but, now I need to add a new class to that trained model without using the previous 2 class dataset. can someone please guide me through this?  "
do annotations in coco need to be in polygon (for iscrowd: 0) and uncompressed RLE format (for iscrowd: 1) to work with matterport's MaskRCNN implementation? I can't find a way to encode masks to uncompressed RLE. Are crowded images used at all in training?    _Originally posted by @waspinator in  
"I am trying to customize NMS.  If I interpreted correctly, there is a code in   that outputs randomly generated ROIs.  Is there any way to convert this to an ROI output by RPN?  Also, is there any way to output the number of ROIs generated for a particular image and their bbox coordinates?  I have tried adding tf.Print() in the ProposalLayer class, but they are ignored.    Thanks."
"2022-01-21 14:27:24.934343: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2  To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.  2022-01-21 14:27:26.240526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2776 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1  Traceback (most recent call last):    File "" "", line 3, in      File ""C:\Program Files\JetBrains\PyCharm 2019.3.3\plugins\python\helpers\pydev\_pydev_bundle\pydev_umd.py"", line 197, in runfile      pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script    File ""C:\Program Files\JetBrains\PyCharm 2019.3.3\plugins\python\helpers\pydev\_pydev_imps\_pydev_execfile.py"", line 18, in execfile      exec(compile(contents+""\n"", file, 'exec'), glob, loc)    File ""D:/data/PycharmProjects/Mask RCNN/Mask_RCNN-2.1/samples/balloon/balloon.py"", line 334, in        model = modellib.MaskRCNN(mode=""training"", config=config,    File ""D:\Mask_RCNN-master\Mask_RCNN-2.1\model.py"", line 1768, in __init__      self.keras_model = self.build(mode=mode, config=config)    File ""D:\Mask_RCNN-master\Mask_RCNN-2.1\model.py"", line 1807, in build      gt_boxes = KL.Lambda(lambda x: x / image_scale)(input_gt_boxes)    File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler      raise e.with_traceback(filtered_tb) from None    File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\type_spec.py"", line 867, in type_spec_from_value      raise TypeError(f""Could not build a TypeSpec for {value} of ""    TypeError: Could not build a TypeSpec for KerasTensor(type_spec=TensorSpec(shape=(None, None, 4), dtype=tf.float32, name=None), name='tf.math.truediv/truediv:0', description=""created by layer 'tf.math.truediv'"") of unsupported type  .    I have a stubborn problem with this type error handling for a long time, please help me, thanks a lot.  "
"Hello, I am trying to detect only person class from maskrcnn model, but person reflection is also considered as person. Is there specifically any way to reduce this wrong predictions? One thing which clicked to me is to remove or suppress those pixels whose intensity is pretty low or I can say more uncertain to be predicted as person. But at the output of mask rcnn, we get an array format mask value in terms ( True and False ). Any suggestion? Thanks. "
"moving from     `Ubuntu 18.04 | CUDA 10.1 | tensorflow 2.1.0 | keras 2.3`       to    `Ubuntu 20.04 | CUDA 11.1 | tensorflow 2.4.0 | keras 2.4.0`    I'm getting a different output in        x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (pool_size, pool_size), padding='valid'), name='mrcnn_class_conv1')(x)    namely, in the first setup:        x = Tensor(""mrcnn_class_conv1/Reshape_1:0"", shape=(None, 1000, 1, 1, 1024), dtype=float32)    and in the second setup:        x = KerasTensor(type_spec=TensorSpec(shape=(1, None, 1, 1, 1024), dtype=tf.float32, name=None), name='conv2d/squeeze_batch_dims/Reshape_1:0', description=""created by layer 'conv2d'"")    tensor shapes are different somehow: `(1, None, 1, 1, 1024)` instead of `(None, 1000, 1, 1, 1024)`.    Does anybody have an idea about reasons and possible fixes? Does this migration make sense?    Thanks in advance for any advice!"
I'm trying to perform an ablation study on my model.   Please can anyone help me on how to do ablation on mask RCNN ?
"Is there a way to separate the mask generation part from the BB part with the code in the repo? I haven't looked in detail into it, but I just wanted to know if there's a possibility.    I want to use the faster rcnn part alone and maybe includethe mask generation if required.    This is not an issue but a query. Thanks a lot for your suggestions in advance."
"Hi,     I am currently trying to apply mrcnn twice in my model, the second one on top of the first detected region. However, I am not sure how can I extract the first detected (masked) region.     I have tried setting everywhere else besides the contour region to zero but that doesn't work as the image has a different dimension with the contour points. "
"Training result, there is only one category, but there are multiple boxes.How do you use non-maximum suppression to filter out unwanted boxesï¼Ÿ"
I am working with one class (crack) only in mask rcnn. I want to assign IDs for each crack detected in the image. Can anyone help me? I can't find any sample code with regards to this. Thank you
"I am trying to use this repo in a project without having to copy the whole repo everytime I want to do inference.  I followed the steps and everything is working fine as long as I do      But I would like to just install the module, call `import mrcnn` and use it from wherever. Is this even possible?  I installed it via the setup.py, pip does list it as ""mask-rcnn"" (conda does not), but I am not able to import it, no matter if I try to import ""mask-rcnn"", ""mask_rcnn"" or use importlib.import_module()."
"<img width=""769"" alt=""Screen Shot 2022-01-06 at 1 16 25 AM"" src=""     I tested my model but no masks is showing.    Any help?"
"Hi, I am new to Mask_RCNN. I would like to know how can I find the coordinate of the bounding boxes (masks) in an image (I have more than one class thus more than one bounding box)?"
"Hello, When training MasKRCNN, I modified the model input size to (512, 1024)[h,w] by modifying the ""resize_image"" method. And  I found that after training for several steps, the ""rpn_bbox_loss"" became nan, the ""mrcnn_bbox_loss"" and ""mrcnn_mask_loss"" became 0.0000e+00.  However, when I change the input size back to (1024,1024), the convergence of loss is normal.  Can you help me with what might be the cause ?"
"I have trained a model from pre-trained coco weights, with approximately 960 training images and 320 validation images, applying data augmentation with imgaug and different training stages ('heads', '4 +', 'all' ). I did the training both on a pc with a P4000 GPU  and in google colaboratory with a P100 GPU. The results showed overfitting in the Google Colab environment.  The results are not much better with the GPU P4000 but I get better training and validation loss graphs, but my question is why are not better the results with a GPU with greater graphics memory capacity? I have tried changing the parameters of images per GPU, steps per epoch, decreasing the learning rate, but I still get similar results where the value of loss of validation begins to increase in the last training epochs with the P100. Are there any parameters that Mask R-CNN adjusts automatically according to computational capacity?    Google Colab (Tesla P100):  !   !   Nvidia P4000:  !   !           I appreciate your responses."
"Hi,  I want to train with my own dataset for instance aware semantic segmentation. I've already trained my dataset with UNET model, I want to train with Mask RCNN using the same dataset but here is my question. My dataset mask images are binary images, only 2 class (including bg), how can I use the code with binary images. All the tutorials out there mentioning json files which my dataset does not include. Apart from the binary images my dataset also includes gray images, where each instance corresponds to a different gray level.  Thanks! "
None
need help with instructions regarding cloning this project and best suitable software s  for making it run on my system
"How should I get rid of this error?     # Code       # Error  !     I have checked to include the background into the number of classes and tried adding image_meta = image_meta[0:13], but it's not helping. "
"Hi there,    I am using   `Ubuntu 18.04`  `Tensorflow 1.15`  `Cuda 10.0`  `cuDNN 7.6.5`    The `tf.test.is_gpu_available()` returns True.    The whole program just gets stuck when I try to train the model no matter how small the model is.     Sometime after waiting for minutes I got the exception   `InternalError: (0) Internal: Blass GEMM launch failed`  I know this error occurs when there are multiple sessions running at the same time but I just have exact one session.     I have been tortured for days, can someone give me some advice on it?"
"I was reading   were  the use Mask R-CNN with only one class. One of the evaluated tasks was object detection (were they have 251 **object detection** annotated images) and they report the following results:  !    Then I was reading this   , and there is something that I don't understand    Question:  1.  How can mAP increase by reducing the mini-mask for object detection?  I thought that mini maks were used in Mask-RCNN as a way to improve training speed, in the segmentation task not in object detection  "
"I followed up the guide, train the model using custom DataSet.  However, I worked at learning R&D to develop the custom Product,  So I want to train the optimized network(User-defined)    To do this, How can i do this?    "
"Hi, I am trying to segment the different walls of an image of a room. The model fails miserably. It does not even separate the walls and the masks it gets are very bad.  I labelled the images using VIA. Used around 4000 images for training and trained for 160 epochs    !     As you can see, the only wall it detected was the one in front and that too the mask spreads onto the table and chair and the floor too. Please let me know if anyone has any ideas how to go about this problem "
"I have a problem when convert model from TF1.x to TF2.x.   ValueError: Variable   has `None` for gradient. Please make sure that all of your ops have   a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.  Help me, Thanks"
"Hi,    For some reason, the inspect_balloon_model.ipynb notebook gives the following error:    2021-11-17 21:54:58.731179: W tensorflow/core/common_runtime/colocation_graph.cc:983] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [    /job:localhost/replica:0/task:0/device:CPU:0].  See below for details of this colocation group:  Colocation Debug Info:  Colocation group had the following types and supported devices:   Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]  IsVariableInitialized: CPU   Identity: CPU XLA_CPU XLA_GPU   VariableV2: CPU   Assign: CPU     Colocation members, user-requested devices, and framework assigned devices, if any:    res5b_branch2a_5/kernel (VariableV2) /device:GPU:0    res5b_branch2a_5/kernel/Assign (Assign) /device:GPU:0    res5b_branch2a_5/kernel/read (Identity) /device:GPU:0    Assign_1968 (Assign) /device:GPU:0    IsVariableInitialized_4038 (IsVariableInitialized) /device:GPU:0    Also, it doesn't seem like the GPU is being used at all (when running balloon.py, GPU util shows 0%)    Has anyone encountered this?    Setup is:    numpy==1.18.5  scipy  Pillow  scikit-image==0.16.2  cython  matplotlib  scikit-image  tensorflow==1.15.0  tensorflow-gpu==1.15.0  keras==2.1.6  opencv-python  h5py==2.10.0  imgaug  IPython[all]  pycocotools    K80 GPU,   CUDA 11,  Debian 10    (Does anyone have it working with cuda 11 and tf1.15 combo?)      Thanks"
Hi    I trained successfully on RSNA pneumonia dataset(  This are grayscale images. But when trying to infer from them I am facing issues. I have also removed use_mini_mask argument from load_image_gt method.    Please take a look at screen-shot.  !     How to resolve this?
"I saw the sample/coco/coco.py will select the JSON file by the year, and I don't understand what is its meaning.        parser.add_argument('--year', required=False,                          default=DEFAULT_DATASET_YEAR,                          metavar="" "",                          help='Year of the MS-COCO dataset (2014 or 2017) (default=2014)')"
"i am getting this error when creating a model          this is my code         i am running on google collab, i have tried changing runtime type and the error changes from lamda_4 to lambda_16.    i have tried to change the batchsize nothing changes, tried different versions of libraries tf , keras, h5py but nothing changes aswell    am i missing something ?    "
ModuleNotFoundError: No module named 'tensorflow'  !   !     but my keras is 2.6.0 tensorflow-gpu is 2.6.0  thanks a lot  
"Why do all the tutorials of model training will use trained models?   Theoretically, it will be affected by previous training data to cause it to be less accurate.     1: How could I train a model without using a trained model?  2: Why not many people ask to train a pure new model for more accuracy?  3: Is that better?"
"This project seems to be missing logs file, can someone help me find it?  !   "
"Hi, currently my model to detect and classify defects on a product is not as accurate as I want it to be. Aside from the images of the product, I also have a tabular data that could give more information about the product. So I plan to have another model for the tabular data and concatenate the output somewhere in the Mask_RCNN structure. So is there any suggestion on where I should concatenate this output? Right now, I am thinking of concatenating it somewhere in fpn_classifer_graph and fpn_mask_graph"
"my dataset in evalute and test is the same ,and got a result on evalute when trainningï¼Œbut I reload the model after training and test on the same datasetï¼Œthe result is lower(seven points less than training)ã€‚what can i do to find the reason? thank you for your answer."
I got this problem but I can't remove it. Did anybody can help m  !   e?
"I'm training the model with the parameters:  Config file consists of:     Masks dimensions are 1024x1024 with overlapping 512 px.     Training/valid dataset size = 936/234 -> 80% - 20%    This is my augmentation list:       This is how I train the layers, heads, and all:(I also load pretrained COCO weights at the beginning)       This is the loss graph:(still processing, I got a sample between the training session)  !     However, the val loss graph is increasing instead of decreasing which seems overfitting.  (still processing, I got sample between training session)    !     Does anyone know why validation loss increasing? Is there any way to decrease it?    Here is my output when I try to predict. It seems while bboxes are perfectly aligned, masks are barely filled. What I need is masks instead of bbox areas? Does anyone one how to solve this problem? I really appreciate it.    ! .png?raw=true)"
"Hello everyone,    How can I train the model on large images? The problem is some of the objects are too small compared to the image size, thus I cannot resize the whole image. On average, I have 3-4 objects per image. Even though I trained for a large number of epochs and used many samples, Mask RCNN is just ignoring small objects....    Can anyone help me with this? any insights would be helpful    Thanks."
"Hello,    Does anyone know how to load the model on Flask loadup? Currently I have the below but it loads the model each time a HTTP request comes in:     "
"Can someone guide how to use tensorboard to look at learning curves, I really tried few things available but no graphs coming up.  "
None
"if __name__ == '__main__':      import argparse        # Parse command line arguments      parser = argparse.ArgumentParser(          description='Train Mask R-CNN to detect custom class.')      parser.add_argument(""command"",                          metavar="" "",                          help=""'train' or 'splash'"")      parser.add_argument('--dataset', required=False,                          metavar=""/path/to/custom/dataset/"",                          help='Directory of the custom dataset')      parser.add_argument('--weights', required=True,                          metavar=""/path/to/weights.h5"",                          help=""Path to weights .h5 file or 'coco'"")      parser.add_argument('--logs', required=False,                          default=DEFAULT_LOGS_DIR,                          metavar=""/path/to/logs/"",                          help='Logs and checkpoints directory (default=logs/)')      parser.add_argument('--image', required=False,                          metavar=""path or URL to image"",                          help='Image to apply the color splash effect on')      parser.add_argument('--video', required=False,                          metavar=""path or URL to video"",                          help='Video to apply the color splash effect on')      args = parser.parse_args()"
"Is there a method to save the resultant masks in a shapefile for each of the tested images?     If not, then is there a way/method to save them in jpg/png format? But not each mask in each image. All the masks of the result in one single image.    Asking questions from the pov of detecting building footprints and then calculating their area based on the masks generated. Can anyone help me with the same?    TIA"
"Hi, I am trying to track an object through a set of frames and whilst the model is mostly performing well, on some frames it is predicting that the object is in two different places. I only want 1 mask on each frame.    In every frame there is only one object, so a logical solution to this seems to be to limit the maximum number of masks that can be shown to 1 (the mask with the highest confidence).     Alternatively, set some parameter so that only masks with a confidence of X or higher are shown. I have tried changing some of the parameters of config.py but with no success.    Can anyone point me in the right direction?"
"  I've got following problem:    I am training a TensorFlow **2.6** Matterport MaskRCNN-Port on my **RTX3090**. I've installed CUDA 11.4 and CUDNN. The GPU is shown in Tensorboard.    When I'm training the GPU is idle on 0% GPU-util and suddenly spikes after a time, processing a batch of images. The CPU (64 core) is going 100% on ~2 cores beforehand.    Why is there as much idle time? Tensorboard says there is no input-pipeline ""lag"".    Any sugesstions?     **Specs:**    Linux, RTX3090, 64-core CPU (EPYC), CUDNN 8.0.2, CUDA 11.4.1  "
"I want to detect two classes of objects in my dataset. The size of the images is 320x256. Class 1 objects are of normal size. The size of most class 2 objects is 2x2 pixels. I wonder which parameter can I change to improve the identification result of class 2 objects? I changed RPN_ANCHOR_SCALES to (2, 4, 8, 64, 128). The loss of the trained model is less than 0.7. But the trained model still can't detect class 2 objects in the test dataset. Thank you in advance."
"I trained Mask_RCNN on my dataset. However, the detection results contain rectangles. The area of the object is important to me, so I want to know how to get rid of the rectangle. Thank you in advance.  !   "
None
None
"I am a python novice, but followed your blog to train my own custom model on my dataset. I am comfortable using the custom.py script, but am lost once I get to the predict.py portion. I am not using your balloons, but my own fish dataset. Do I need to change your references to balloons to my fish in the predict.py script? Also, I get lost as to where I should paste the code for the validation set and then the random images. Can these be placed sequentially in the predict.py script? Thanks!"
"Hello I am trying to train Mask_RCNN on my own dataset here is the code I am running for training the model:           The dataset has 5 classes the name of each class in the annotations is 1, 2, 3, 4, 5 and the object that they represent is car, truck, pedestrian, bicyclist, light.    When I run the code it starts fine but when it get's to epochs 1 it keeps prints the following WARNING for ever:    `WARNING:root:You are using the default load_mask(), maybe you need to define your own one.`    Please I need some help"
"Hello I am trying to train Mask R-CNN on a custom dataset I use the weights from coco but on model.train I use head='all' in order to train the model and overwrite the coco weights according to my data. My images are 256x256 and I also changed the max_dim in config.py to be 256 so it doesn't pad or resize my images and when I run the code I get the following error:    `ValueError: Dimension 3 in both shapes must be equal, but are 256 and 1024. Shapes are [7,7,256,256] and [7,7,256,1024]. for 'Assign_658' (op: 'Assign') with input shapes: [7,7,256,256], [7,7,256,1024].  `"
"During training the Mask RCNN on custom dataset, I am getting following error -       /content/drive/MyDrive/Colab/Mask_RCNN/mrcnn/model.py in train(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)     2372             max_queue_size=100,     2373             workers=workers,  -> 2374             use_multiprocessing=workers > 1,     2375         )     2376         self.epoch = max(self.epoch, epochs)    /usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)      794         max_queue_size=max_queue_size,      795         workers=workers,  --> 796         use_multiprocessing=use_multiprocessing)      797       798   def evaluate(self,    /usr/local/lib/python3.7/dist-packages/keras/engine/training_generator_v1.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)      584         shuffle=shuffle,      585         initial_epoch=initial_epoch,  --> 586         steps_name='steps_per_epoch')      587       588   def evaluate(self,    /usr/local/lib/python3.7/dist-packages/keras/engine/training_generator_v1.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)      210     step = 0      211     while step   212       batch_data = _get_next_batch(generator)      213       if batch_data is None:      214         if is_dataset:    /usr/local/lib/python3.7/dist-packages/keras/engine/training_generator_v1.py in _get_next_batch(generator)      344   """"""Retrieves the next batch of input data.""""""      345   try:  --> 346     generator_output = next(generator)      347   except (StopIteration, tf.errors.OutOfRangeError):      348     return None    /usr/local/lib/python3.7/dist-packages/keras/utils/data_utils.py in get(self)      771     while self.is_running():      772       try:  --> 773         inputs = self.queue.get(block=True, timeout=5).get()      774         if self.is_running():      775           self.queue.task_done()    /usr/lib/python3.7/queue.py in get(self, block, timeout)      177                     if remaining   179                     self.not_empty.wait(remaining)      180             item = self._get()      181             self.not_full.notify()    /usr/lib/python3.7/threading.py in wait(self, timeout)      298             else:      299                 if timeout > 0:  --> 300                     gotit = waiter.acquire(True, timeout)      301                 else:      302                     gotit = waiter.acquire(False)  "
"---------------------------------------------------------------------------  AttributeError                            Traceback (most recent call last)    in  ()       15 print(IMAGE_DIR)       16 # Create model object in inference mode.  ---> 17 model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)       18        19 # Load weights trained on MS-COCO    1 frames  /usr/local/lib/python3.7/dist-packages/mrcnn/model.py in set_log_dir(self, model_path)     2254         # Directory for training logs     2255         self.log_dir = os.path.join(self.model_dir, ""{}{:%Y%m%dT%H%M}"".format(  -> 2256             self.config.NAME.lower(), now))     2257      2258         # Create log_dir if not exists    AttributeError: 'NoneType' object has no attribute 'lower'"
None
None
"     Using the fork at   but there are no issues there, so I figured I might ask here.    Hitting the `/detect` endpoint gives the following.         If we uncomment `model.keras_model._make_predict_function()` I get the following.         Now, I've seen a lot of solutions involving a session, but there are no more sessions in this version. As well, a lot say to use `tf.get_default_graph()` but that call also does not exist. There is something I am not understanding here about TensorFlow 2 and how async functions interact.    Any guidance is appreciated!"
"Hello everyone! I am making a model that recognizes fruits. While annotations data, I have new labels from time to time. For example, in the last iteration, I trained the model to recognize bananas and oranges, and in the next iteration, I added apples as a new label(and sometimes many labels are added in a new iteration). I want to make a function that, before training the model, collects a list of all the classes that I have and gives them for training, for example like this:    `def get_class_list(direct):`      `classlist = set()`  `    for cla in os.listdir(direct):`  `       tree = ElementTree.parse(direct + '/' + cla)`  `        root = tree.getroot()`  `         for box in root.findall('.//object'):`  `              name = box.find('name').text`  `               classlist.add(name)`  `  return classlist`          and then, I want to put this list into my Dataset. Now it looks like:        `class KangarooDataset(mrcnn.utils.Dataset):`        def load_dataset(self, dataset_dir, is_train=True):          self.add_class(""dataset"", 1, ""banana"")          self.add_class(""dataset"", 2, ""orange"")          images_dir = dataset_dir + '/images/'          annotations_dir = dataset_dir + '/annots/'            for filename in os.listdir(images_dir):              image_id = filename[:-4]                if is_train and int(image_id) >= 150:                  continue                if not is_train and int(image_id) < 150:                  continue                img_path = images_dir + filename              ann_path = annotations_dir + image_id + '.xml'                self.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path)        def load_mask(self, image_id):          info = self.image_info[image_id]          path = info['annotation']          boxes, w, h = self.extract_boxes(path)          masks = zeros([h, w, len(boxes)], dtype='uint8')            class_ids = list()          for i in range(len(boxes)):              box = boxes[i]              row_s, row_e = box[1], box[3]              col_s, col_e = box[0], box[2]              masks[row_s:row_e, col_s:col_e, i] = 1              class_ids.append(self.class_names.index('kangaroo'))          return masks, asarray(class_ids, dtype='int32')        # A helper method to extract the bounding boxes from the annotation file      def extract_boxes(self, filename):          tree = xml.etree.ElementTree.parse(filename)            root = tree.getroot()            boxes = list()          for box in root.findall('.//bndbox'):              xmin = int(box.find('xmin').text)              ymin = int(box.find('ymin').text)              xmax = int(box.find('xmax').text)              ymax = int(box.find('ymax').text)              coors = [xmin, ymin, xmax, ymax]              boxes.append(coors)            width = int(root.find('.//size/width').text)          height = int(root.find('.//size/height').text)          return boxes, width, height`          `                    And my questions is: how to replace first rows in load_dataset function to dynamic adding classes?                    I've tried                     `self.classli = list_of_classes`          `for c in self.classli:`           `   self.add_class(""dataset"", self.classli.index(c)+1, c)`                      but it added and detect only one class instead of whole list"
The `load_image_gt` resized/augments the image using        I am testing my model on some images and the resultant masks and boxes have been modified according to the resized images.  Is there a way I get revert the results to the original image i.e. remove the effect of the scaling and resizing from the mask/boxes?  
"I am using:    Mask-RCNN cloned from    Tensorflow 2.6.0  Keras 2.6.0  scikit-image 0.18.3  python 3.7.11  CUDA 11.1  cuDNN 8201    GPU:  NVIDIA GeForce RTX 3080    I tried to reproduce the balloon and the shapes detection by running balloon.py and train_shapes.ipynb.  If I just run the train_shapes.ipynb and do not change any parameters, so meaning I train for 1 or 2 epochs, I get this outcome:  !   or this:  !     If I disable the mini_mask (`USE_MINI_MASK = False`), I still get an awful result:  !       If I run the ballon.py code as it came (just tweaking one line in the data loader, see #2592), for 30 epochs, I get predictions like this:  !     Losses look good in general (blue is from train_shapes.py, orange is from balloon.py):  !       If I just display the data with its ground truth mask, it looks fine. To me it seems that there is something going on with the anchors and boxes.    Anyone has any idea what is going wrong? Or an idea on how to debug?    Any help is highly appreciated!      "
"I built a custom MaskRCNN model in .h5 using this code. I managed to save the full model and not the weights alone using `model.keras_model.save()`, and assume it worked correctly.    I need to convert this model to ONNX to inference in Unity Barracuda, and I have been hitting several errors along the way.  I tried:    1. .h5 to ONNX using   and the keras2onnx package, and I hit an error at:           2. I tried defining custom layers using  :             3. .h5 to .pb (frozen graph) and .pbtxt, and then from .pb to ONNX using tf2onnx after finding input and output nodes (seems to be only one of each?):             It seems that `keras.models.load_model()` throws the first two errors - wondering if there is a way I can work with the .pb/.pbtxt model, or a way around without using `load_model()`, or a way to solve the `load_model()` issue?      **Is there a way to convert my custom .h5 model to ONNX through any direct/indirect means?** I have been stuck on this for days! I find no issue here seems to have a solution that works.    Thanks in advance.  "
"The environment I configured is as follows:  CUDA10.0  CUDNN7.4.1  Tensorflow - gpu 1.13.2  python3.7  keras2.2.4  numpy1.17.4  My CUDA version is powered 11.3 (GeForce RTX 3050).  The jupyter Notebook runs very slowly the first time I open it. Every time you run it, you just get some random box or nothing.  Can someone help me analyze why this is"
"Hi, just wondering how the val_loss is calculated. My assumption is val_loss is the sum of the other losses rpn_class_losses, rpn_box_losses etc but this doesn't seem the case in the train_shapes.ipynb.     In particular, the train_loss, val_loss for the train-all-layer phase seems to skyrocket while the other losses remain roughly stable compared to the train-head-only phase    Train head: val loss is 1.854  !     Train all layers: val_loss is 11.4 while the other losses actually decrease  !   "
"Try using custom_callbacks. Counting accuracy takes a lot of time, so don't use it every epoch.    mean_average_precision_callback = modellib.MeanAveragePrecisionCallback(model,\            model_inference, dataset_val, calculate_map_at_every_X_epoch=5, verbose=1)    model.train(dataset_train, dataset_val,               learning_rate=config.LEARNING_RATE,               epochs=100,              layers='heads',              custom_callbacks=[mean_average_precision_callback])    _Originally posted by @VtlNmnk in  "
"My input image for inference module is 256x256 and within the detection function, it's being resized to 1024x1024.  I am trying to get the bounding boxes for the original images and scaling them as well, but I am unable to get the correct coordinates.    Can someone help?"
"Hello everybody,   I am trying two run this code on two input model (RGB image and Edge map of same image) so that I could get better result comparatively.  To feed two inputs I am trying like this:  .     After feeding two input I am using resnet50 backbone model for feature extraction and then concatenate the output feature vector for both images and feed forward to the MRCNN model.     !       I made changes in code model.py.  `   if callable(config.BACKBONE):              _, C12, C13, C14, C15 = config.BACKBONE(input_image1, stage5=True,                                                  train_bn=config.TRAIN_BN)                _, C22, C23, C24, C25 = config.BACKBONE(input_image2, stage5=True,                                                  train_bn=config.TRAIN_BN)                C2 = KL.Concatenate()(   !       I tried fixing this error by creating two different function(resnet_graph1 and resnet_graph2) for backbone model, after concatenation whole model structure is same.         If anyone could tell me what causing this error and some intuitions on how to fix my issue, I would appreciate suggestions.  Thank You           "
"Hey,    iÂ´m trying to train on my own dataset. This works fine but a little issue. The masks are displayed with a offset like you see in the Picture below. Has anybody an Idea, why is displayed like this?   !   "
"Hi Everyone,    I cloned the repo and ran the demo.ipynb ( ) in the samples folder. I didn't experience any issues until the last cell, where it stalled for an oddly long amount of time like ten minutes and spit out this image:  !     I can't conceive what's wrong here as I didn't touch the code, I just cloned the repository and ran the demo to check if everything was working. Tensorflow is definitely using my GPU (3070). My only guess is that the source of the problem is incompatible package versions.    My most relevant package versions are as follows:    tensorflow 1.15  keras 2.1.6  cudatoolkit 10.0.130  cudnn 7.6.5   cuda proprietary driver 460    Here is a list of all my packages if you want to see more:      If anyone has any ideas of what could be causing this, or notices a package incompatibility which could be the cause of my issue, I would appreciate suggestions.    Thanks!"
"I have been trying to train mask rcnn on custom dataset but I am not getting good results on my val or test dataset.  I also tried applying augmentations but that didn't work either. (see  )    This time, I tried to slightly increase the learning rate, and did some modifications to my data. Both train and val images are 256x256 whereas earlier the train data images were 512x512 in size.     This was my learning curve:  !     I am not sure what to do now. The model just doesn't seem to perform well on val data under any circumstances. Any suggestions on what I might be doing wrong or what I could try?  "
"I'm trying to train MRCNN on a custom dataset of 1 object class plus background. Training from COCO weights .    For 60 steps during the first epoch my losses look like this: mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00    After 60 steps, the loss values for both gradually increase to around 4.0 by the second epoch then begin to reduce in further epochs.    They're fine for the other loss values but since my output is very poor I'm wondering if this is a symptom of an issue with my code that I haven't noticed.    Has anyone seen this before/ know of any solution for this?    Configurations: BACKBONE resnet101    BACKBONE_STRIDES [4, 8, 16, 32, 64]    BATCH_SIZE 1    BBOX_STD_DEV [0.1 0.1 0.2 0.2]    COMPUTE_BACKBONE_SHAPE None    DETECTION_MAX_INSTANCES 10    DETECTION_MIN_CONFIDENCE 0.9    DETECTION_NMS_THRESHOLD 0.3    FPN_CLASSIF_FC_LAYERS_SIZE 1024    GPU_COUNT 1    GRADIENT_CLIP_NORM 5.0    IMAGES_PER_GPU 1    IMAGE_CHANNEL_COUNT 3    IMAGE_MAX_DIM 1280    IMAGE_META_SIZE 14    IMAGE_MIN_DIM 720    IMAGE_MIN_SCALE 0    IMAGE_RESIZE_MODE square    IMAGE_SHAPE [1280 1280 3]    LEARNING_MOMENTUM 0.9    LEARNING_RATE 0.0001    LOSS_WEIGHTS {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 3.0, 'mrcnn_class_loss': 2.0, 'mrcnn_bbox_loss': 2.0, 'mrcnn_mask_loss': 3.0}    MASK_POOL_SIZE 14    MASK_SHAPE [28, 28]    MAX_GT_INSTANCES 20    MEAN_PIXEL [110.2 104.8 144.8]    MINI_MASK_SHAPE (56, 56)    NAME DP3D    NUM_CLASSES 2    POOL_SIZE 7    POST_NMS_ROIS_INFERENCE 1000    POST_NMS_ROIS_TRAINING 2000    PRE_NMS_LIMIT 6000    ROI_POSITIVE_RATIO 0.33    RPN_ANCHOR_RATIOS [0.5, 1, 2]    RPN_ANCHOR_SCALES (16, 32, 64, 128, 256)    RPN_ANCHOR_STRIDE 1    RPN_BBOX_STD_DEV [0.1 0.1 0.2 0.2]    RPN_NMS_THRESHOLD 0.7    RPN_TRAIN_ANCHORS_PER_IMAGE 200    STEPS_PER_EPOCH 150    TOP_DOWN_PYRAMID_SIZE 256    TRAIN_BN False    TRAIN_ROIS_PER_IMAGE 100    USE_MINI_MASK False    USE_RPN_ROIS True    VALIDATION_STEPS 10    WEIGHT_DECAY 0.0001"
"Dataset contains 2951 CXR images with masks corresponding to lesion areas (1 positive class). I use 1000 as training split, the rest is testing, and I keep getting a very low mAP, around 0.2925, with 0% precision for IoU=0.95     Previously I used CNCB-NCOV dataset for lesion segmentation (3 positive classes: clean lungs, GGO, C) in CTs, 650 for training, the rest testing, and I kept getting very good results  (0.4245 mean average precision, main MS COCO criterion).     I decided to evaluate the model on the training data, and I found out that there's abnormal overfitting, something like 0.99 mAP.     Any suggestions on what could be done about this? I reduced lrate to 1e-6, resampled that training/test splits several times, ran the model for a different number of epochs? changed the anchors, and the number of output/FPN  layers, nothing's working.      "
"I am using Mask RCNN on custom dataset. I trained using `resnet101` backbone, with 150 training images and 50 validation images. 70 epochs and 50 steps per epoch. But I also saved the best model which was at epoch 40. The results on val dataset were average. But at least the masks created were passable.    I applied augmentations to improve results. All other parameters were kept untouched.       But now, there are masks on hardly 10% of the validation images which don't make sense at all. The masks include big rectangle patch covering vague parts of images (30-40% of the image sometimes), whereas my image includes multiple circular objects. I am not sure what is going wrong, the training loss was 1.7 and dropped to 1.3-1.2 over epochs but the validation loss hovered over ~2 but didn't improve much over time. Can someone help with what I can do to improve results?"
"When I ran 'saving.load_weights_from_hdf5_group_by_name(f, layers)',which is the line 2135 of mrcnn/model.py,the AttributeError raised as: 'str' object has no attribute 'decode'"
"To get mrcnn working, we modified the model.py file.  At the top of file, add the following class (you'll have to fix the layout).  `class AnchorsLayer(KL.Layer):           def __init__(self, anchors, name=""anchors"", **kwargs):          super(AnchorsLayer, self).__init__(name=name, **kwargs)          self.anchors = tf.Variable(anchors)     def call(self, dummy):          return self.anchors      def get_config(self):          config = super(AnchorsLayer, self).get_config()          return config  `    Next, instead of having `self.keras_model._losses = []`, replace it with `self.keras_model._losses.clear()`.    Lastly, instead of having `self.keras_model.add_metric(loss, name)`, replace it with `self.keras_model.add_metric(value=loss, aggregation='mean', name=name)`    These modifications were required for mrcnn to work for us. We hope these changes help fix your issues as well!"
"I am using 2 RTX 3080 Ti for training the model on custom data. I am using this   repo which updated the Mask RCNN to tensorflow 2.x, the training is stuck at 1st epoch and doesn't produce any output."
"I have a question. Does mask rcnn not adjust its weights and learning on the basis of validation dataset after each epoch. Like I have a a dataset divided into train, val and test. Train and val are supplied for training. And if I run the the model on validation dataset, the results are quite poor itself, let alone test dataset. This means validation dataset is not used for training? Just for us to check our val score while training is going on?"
"2021-07-24 20:37:14.184543: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll  Using TensorFlow backend.    Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.7  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 1  IMAGE_CHANNEL_COUNT            3  IMAGE_MAX_DIM                  512  IMAGE_META_SIZE                14  IMAGE_MIN_DIM                  512  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [512 512   3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           shapes  NUM_CLASSES                    2  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  PRE_NMS_LIMIT                  6000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (48, 96, 192, 384, 768)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                100  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           100  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001      0  F:\Mask_RCNN-master\Mask_RCNN-master\date\labelme_json/000001_json/img.png  1  F:\Mask_RCNN-master\Mask_RCNN-master\date\labelme_json/000002_json/img.png  2  F:\Mask_RCNN-master\Mask_RCNN-master\date\labelme_json/000003_json/img.png  0  F:\Mask_RCNN-master\Mask_RCNN-master\date\labelme_json/000001_json/img.png  1  F:\Mask_RCNN-master\Mask_RCNN-master\date\labelme_json/000002_json/img.png  2  F:\Mask_RCNN-master\Mask_RCNN-master\date\labelme_json/000003_json/img.png  WARNING:tensorflow:From D:\Anaconda\envs\mask\lib\site-packages\keras\backend\tensorflow_backend.py:508: The name   tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.    WARNING:tensorflow:From D:\Anaconda\envs\mask\lib\site-packages\keras\backend\tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.    WARNING:tensorflow:From D:\Anaconda\envs\mask\lib\site-packages\keras\backend\tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.    WARNING:tensorflow:From D:\Anaconda\envs\mask\lib\site-packages\keras\backend\tensorflow_backend.py:3661: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.    WARNING:tensorflow:From D:\Anaconda\envs\mask\lib\site-packages\keras\backend\tensorflow_backend.py:1944: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.       WARNING:tensorflow:From D:\Anaconda\envs\mask\lib\site-packages\tensorflow_core\python\ops\array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.  Instructions for updating:  Use tf.where in 2.0, which has the same broadcast rule as np.where  WARNING:tensorflow:From f:\Mask_RCNN-master\Mask_RCNN-master\mrcnn\model.py:553: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.    WARNING:tensorflow:From f:\Mask_RCNN-master\Mask_RCNN-master\mrcnn\utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.    WARNING:tensorflow:From f:\Mask_RCNN-master\Mask_RCNN-master\mrcnn\model.py:600: calling crop_and_resize_v1 (from   tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.  Instructions for updating:  box_ind is deprecated, use box_indices instead  WARNING:tensorflow:From D:\Anaconda\envs\mask\lib\site-packages\keras\backend\tensorflow_backend.py:168: The name   tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.    WARNING:tensorflow:From D:\Anaconda\envs\mask\lib\site-packages\keras\backend\tensorflow_backend.py:175: The name   tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.    WARNING:tensorflow:From D:\Anaconda\envs\mask\lib\site-packages\keras\backend\tensorflow_backend.py:180: The name   tf.Session is deprecated. Please use tf.compat.v1.Session instead.    2021-07-24 20:37:18.571564: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2  2021-07-24 20:37:18.574690: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll  2021-07-24 20:37:18.606738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:  name: GeForce RTX 3060 Laptop GPU major: 8 minor: 6 memoryClockRate(GHz): 1.702  pciBusID: 0000:01:00.0  2021-07-24 20:37:18.607219: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll  2021-07-24 20:37:18.609993: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll  2021-07-24 20:37:18.612701: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll  2021-07-24 20:37:18.613942: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll  2021-07-24 20:37:18.617330: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll  2021-07-24 20:37:18.619851: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll  2021-07-24 20:37:18.627847: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll  2021-07-24 20:37:18.628302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 02021-07-24 20:40:30.549373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:  2021-07-24 20:40:30.549480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0  2021-07-24 20:40:30.549597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N  2021-07-24 20:40:30.549793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4753 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6)  WARNING:tensorflow:From D:\Anaconda\envs\mask\lib\site-packages\keras\backend\tensorflow_backend.py:184: The name   tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.    WARNING:tensorflow:From D:\Anaconda\envs\mask\lib\site-packages\keras\backend\tensorflow_backend.py:193: The name   tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.    WARNING:tensorflow:From D:\Anaconda\envs\mask\lib\site-packages\keras\backend\tensorflow_backend.py:200: The name   tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.      Starting at epoch 0. LR=0.001    Checkpoint Path: F:\Mask_RCNN-master\Mask_RCNN-master\logs\shapes20210724T2040\mask_rcnn_shapes_{epoch:04d}.h5  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  In model:  rpn_model      rpn_conv_shared        (Conv2D)      rpn_class_raw          (Conv2D)      rpn_bbox_pred          (Conv2D)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)  WARNING:tensorflow:From D:\Anaconda\envs\mask\lib\site-packages\keras\optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.    D:\Anaconda\envs\mask\lib\site-packages\tensorflow_core\python\framework\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  D:\Anaconda\envs\mask\lib\site-packages\tensorflow_core\python\framework\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  D:\Anaconda\envs\mask\lib\site-packages\tensorflow_core\python\framework\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  WARNING:tensorflow:From D:\Anaconda\envs\mask\lib\site-packages\keras\backend\tensorflow_backend.py:977: The name   tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.    WARNING:tensorflow:From D:\Anaconda\envs\mask\lib\site-packages\keras\backend\tensorflow_backend.py:964: The name   tf.assign is deprecated. Please use tf.compat.v1.assign instead.    WARNING:tensorflow:From D:\Anaconda\envs\mask\lib\site-packages\keras\callbacks.py:783: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.    WARNING:tensorflow:From D:\Anaconda\envs\mask\lib\site-packages\keras\callbacks.py:786: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.    Epoch 1/50  image_id 1  f:/Mask_RCNN-master/Mask_RCNN-master/train.py:88: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read   for full details.    temp = yaml.load(f.read())  [1]  D:\Anaconda\envs\mask\lib\site-packages\skimage\transform\_warps.py:830: FutureWarning: Input image dtype is bool. Interpolation is not defined with bool data type. Please set order to 0 or explicitely cast input image to another data type. Starting from version 0.19 a ValueError will be raised instead of this warning.    order = _validate_interpolation_order(image.dtype, order)  2021-07-24 20:40:39.773231: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll  2021-07-24 20:41:38.927370: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll"
I;ve found that you can do it with mAP but im yet to find a way to do it with mAR per class.
hi i tried lot but not able to solve this error using following packages   tensorflow 2.3.0  keras 2.4.0    anyone can post some information ?  
I have been digging through the issues and could not find the answer. Does this code work for multiple GPUs when testing? specifically the video code in the sample folder. I am trying to see how to increase the speed of that.
"Hi,    thanks for developing Mask_RCNN, everything works very stable! I have a question or maybe someone has a similar problem. I am training cells on small images (like 350x150) which works very well if I predict on the same image size. I need to predict the same object size on very large images (30.000x50.000) but this fails. Usually Mask_RCNN predicts on huge sized object within this image. Is there any possibility to adjust the settings or training to predict small objects on large images?    Thanks  S"
solved
"Upon deploying a Mask_RCNN based model in production (MLOps), model drift will be inevitable.  How do you deal with model drift over time?"
"I succeeded in running the sample on the CPU. However, when I tried to run it on the GPU, I got the following error. It worked on the CPU, so I think there is probably a problem with the GPU settings. But I couldn't find the problem.    My environment is     ubuntu18.04  tensorflow-gpu==1.15.0  keras==2.1.6  h5py==2.10.0    GPUï¼ˆGTX1030)  CUDA10.0    Someone please help me.  Thank you!    `W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: Unable to open shared object file: no such file or directory ; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64  2021-07-18 01:29:16.535474: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.  Using TensorFlow backend.  `    `File ""demo_img_test.py"", line 17, in  `  `import mrcnn.model as modellib`  `  File "" "", line 971, in _find_and_load`  `  File "" "", line 955, in _find_and_load_unlocked`  `  File "" "", line 656, in _load_unlocked`  `  File "" "", line 626, in _load_backward_compatible`  `  File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 255, in  `  `AttributeError: module 'keras.engine' has no attribute 'Layer'`      "
"I'm trying to learn about mask R CNN, so I follow this tutorial -      I'm running python on windows using vscode.    I get this error message:    PS C:\Users\Utilizador\Desktop\Test_rig_rigid_model\Train1\Ok> & C:/Users/Utilizador/anaconda3/python.exe c:/Users/Utilizador/Downloads/dsdfsdf.py  2021-07-14 20:21:40.779515: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll    Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.7  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 1  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                93  IMAGE_MIN_DIM                  800  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           coco  NUM_CLASSES                    81  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                1000  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001      2021-07-14 20:21:43.692039: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library nvcuda.dll  2021-07-14 20:21:44.179105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:   pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1050 computeCapability: 6.1  coreClock: 1.493GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s  2021-07-14 20:21:44.179781: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll  2021-07-14 20:21:44.187782: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll  2021-07-14 20:21:44.188123: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll  2021-07-14 20:21:44.195564: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cufft64_10.dll  2021-07-14 20:21:44.197512: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library curand64_10.dll  2021-07-14 20:21:44.201059: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusolver64_11.dll  2021-07-14 20:21:44.205431: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusparse64_11.dll  2021-07-14 20:21:44.206861: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found  2021-07-14 20:21:44.207314: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at   for how to download and setup the required libraries for your platform.  Skipping registering GPU devices...  2021-07-14 20:21:44.208371: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2  To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.  2021-07-14 20:21:44.209671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:  2021-07-14 20:21:44.209914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]  WARNING:tensorflow:From C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.  Instructions for updating:  The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.  Traceback (most recent call last):    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 498, in _apply_op_helper      values, name=input_arg.name, as_ref=input_arg.is_ref)    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\profiler\trace.py"", line 163, in wrapped      return func(*args, **kwargs)    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1566, in convert_to_tensor      ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 1536, in _autopacking_conversion_function      return _autopacking_helper(v, dtype, name or ""packed"")    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 1471, in _autopacking_helper      constant_op.constant(elem, dtype=dtype, name=str(i)))    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 265, in constant      allow_broadcast=True)    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 283, in _constant_impl      allow_broadcast=allow_broadcast))    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\framework\tensor_util.py"", line 445, in make_tensor_proto      raise ValueError(""None values not supported."")  ValueError: None values not supported.    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 536, in _apply_op_helper      values, as_ref=input_arg.is_ref).dtype.name    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\profiler\trace.py"", line 163, in wrapped      return func(*args, **kwargs)    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1566, in convert_to_tensor      ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 1536, in _autopacking_conversion_function      return _autopacking_helper(v, dtype, name or ""packed"")    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 1471, in _autopacking_helper      constant_op.constant(elem, dtype=dtype, name=str(i)))    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 265, in constant      allow_broadcast=True)    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 283, in _constant_impl      allow_broadcast=allow_broadcast))    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\framework\tensor_util.py"", line 445, in make_tensor_proto      raise ValueError(""None values not supported."")  ValueError: None values not supported.    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""c:/Users/Utilizador/Downloads/dsdfsdf.py"", line 126, in        model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\mrcnn\model.py"", line 1832, in __init__      self.keras_model = self.build(mode=mode, config=config)    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\mrcnn\model.py"", line 2033, in build      fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE)    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\mrcnn\model.py"", line 954, in fpn_classifier_graph      mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name=""mrcnn_bbox"")(x)    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 970, in __call__      input_list)    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 1108, in _functional_construction_call      inputs, input_masks, args, kwargs)    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 840, in _keras_tensor_symbolic_call      return self._infer_output_signature(inputs, args, kwargs, input_masks)    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 880, in _infer_output_signature      outputs = call_fn(inputs, *args, **kwargs)    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\keras\layers\core.py"", line 553, in call      inputs, (array_ops.shape(inputs)[0],) + self.target_shape)    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\util\dispatch.py"", line 206, in wrapper      return target(*args, **kwargs)    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 195, in reshape      result = gen_array_ops.reshape(tensor, shape, name)    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 8397, in reshape      ""Reshape"", tensor=tensor, shape=shape, name=name)    File ""C:\Users\Utilizador\anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 540, in _apply_op_helper      (input_name, err))  ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.  PS C:\Users\Utilizador\Desktop\Test_rig_rigid_model\Train1\Ok>   "
"Hello everyone, I'm trying to start the agar server(   But when I enter the ""npm start"" command, I get an error    npm install did not solve the problem    !   "
"Agario I start my server, I can't log in through my client. But through the official web client, I can connect, what is the problem?  Writes  ""Connecting  If you cannot connect to the servers, check if you have some anti virus or firewall blocking the connection."""
"I am trying to train a custom dataset with Mask RCNN.  Doing this on TF2 by using the mrcnn folder kindly open sourced by the owner of this repository:      But after the above change and also resolving a good number of bugs, I am getting this error:         Has anyone faced this error? Can anyone please guide me as to how can I solve this issue?        "
"TypeError                                 Traceback (most recent call last)    in         37             ""mrcnn_bbox"", ""mrcnn_mask""])       38   ---> 39 train(model)      in train(model)       20                 learning_rate=config.LEARNING_RATE,       21                 epochs=10,  ---> 22                 layers='heads')       23        24     ~\Mask_RCNN_foun\mrcnn\model.py in train(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)     2372             max_queue_size=100,     2373             workers=workers,  -> 2374             use_multiprocessing=True,     2375         )     2376         self.epoch = max(self.epoch, epochs)    c:\users\owner\anaconda3\envs\maskrcnn_foun\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)       89                 warnings.warn('Update your `' + object_name +       90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)  ---> 91             return func(*args, **kwargs)       92         wrapper._original_function = func       93         return wrapper    c:\users\owner\anaconda3\envs\maskrcnn_foun\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)     2192                 batch_index = 0     2193                 while steps_done   2194                     generator_output = next(output_generator)     2195      2196                     if not hasattr(generator_output, '__len__'):    ~\Mask_RCNN_foun\mrcnn\model.py in data_generator(dataset, config, shuffle, augment, augmentation, random_rois, batch_size, detection_targets, no_augmentation_sources)     1707                     load_image_gt(dataset, config, image_id, augment=augment,     1708                                 augmentation=augmentation,  -> 1709                                 use_mini_mask=config.USE_MINI_MASK)     1710      1711             # Skip images that have no instances. This can happen in cases    ~\Mask_RCNN_foun\mrcnn\model.py in load_image_gt(dataset, config, image_id, augment, augmentation, use_mini_mask)     1210     # Load image and mask     1211     image = dataset.load_image(image_id)  -> 1212     mask, class_ids = dataset.load_mask(image_id)     1213     original_shape = image.shape     1214     image, window, scale, padding, crop = utils.resize_image(      in load_mask(self, image_id)       95         for i, p in enumerate(info[""polygons""]):       96             # Get indexes of pixels inside the polygon and set them to 1  ---> 97             rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])       98        99             mask[rr, cc, i] = 1    TypeError: string indices must be integers      Has anyone ever had this issue?"
I'm new to use MaskRCNN i tried to train the model on my dataset and got 1000 epochs should be finished .. Will train phase give me checkpoints.pth file ? Should i use this file for getting .pkl ? or is there another way i can use ?
"WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  Epoch 1/10  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  WARNING:root:You are using the default load_mask(), maybe you need to define your own one.      I have this error when I am trying to costum MaskRCNN on my own dataset.  Is there anyone has faced the same issue and got the solution"
"   ---------------------------------------------------------------------------  TypeError                                 Traceback (most recent call last)    in         37             ""mrcnn_bbox"", ""mrcnn_mask""])       38   ---> 39 train(model)      in train(model)        4     dataset_train = CustomDataset()        5     print(dataset_train)  ----> 6     dataset_train.load_custom(""C:/Users/Owner/Mask_RCNN_foun/dataset"", ""train"")        7     dataset_train.prepare()        8       in load_custom(self, dataset_dir, subset)       43             # the outline of each object instance. There are stores in the       44             # shape_attributes (see json format above)  ---> 45             polygons = [r['shape_attributes'] for r in a['regions']]       46             objects = [s['region_attributes']['name'] for s in a['regions']]       47             print(""objects:"",objects)      in  (.0)       43             # the outline of each object instance. There are stores in the       44             # shape_attributes (see json format above)  ---> 45             polygons = [r['shape_attributes'] for r in a['regions']]       46             objects = [s['region_attributes']['name'] for s in a['regions']]       47             print(""objects:"",objects)"
"I am actually trying to run one experiment with grayscale input and for that I have already made required changes. The problem I am getting is that the code is able to run training for head epochs but it stopped for all layers training like after printing the layers' names there's no output related to anything, there's no error and gpu memory getting freed which means experiment is getting stopped when all layers training starts.  I am not able to understand what is causing that? And how can I fix that?  Let me know if I need to share anything to make my doubts more clear. Any help would be really appreciated. Thanks!"
"---------------------------------------------------------------------------  OSError                                   Traceback (most recent call last)    in         37             ""mrcnn_bbox"", ""mrcnn_mask""])       38   ---> 39 train(model)      in train(model)        4     dataset_train = CustomDataset()        5     print(dataset_train)  ----> 6     dataset_train.load_custom(""C:Mask_RCNN_foun\dataset"", ""train"")        7     dataset_train.prepare()        8       in load_custom(self, dataset_dir, subset)       29         # }       30         # We mostly care about the x and y coordinates of each region  ---> 31         annotations1 = json.load(open('C:\MaskRCNN_foun\dataset\train\tackcoat_json.json'))       32         # print(annotations1)       33         annotations = list(annotations1.values())  # don't need the dict keys    OSError: [Errno 22] Invalid argument: 'C:\\MaskRCNN_foun\\dataset\train\tackcoat_json.json'  "
"i want to use balloon dataset in my paper.   if this came from this repo, i have to cite this paper[1], right?    [1] Abdulla, W. (2017). Mask r-cnn for object detection and instance segmentation on keras and tensorflow.  "
"TypeError                                 Traceback (most recent call last)    in         26 config = CustomConfig()       27 model = modellib.MaskRCNN(mode=""training"", config=config,  ---> 28                                   model_dir=DEFAULT_LOGS_DIR)       29        30 weights_path = COCO_WEIGHTS_PATH    ~\Mask_RCNN_foun\mrcnn\model.py in __init__(self, mode, config, model_dir)     1868         self.model_dir = model_dir     1869         self.set_log_dir()  -> 1870         self.keras_model = self.build(mode=mode, config=config)     1871      1872     def build(self, mode, config):    ~\Mask_RCNN_foun\mrcnn\model.py in build(self, mode, config)     1907             # Normalize coordinates     1908             gt_boxes = KL.Lambda(lambda x: norm_boxes_graph(  -> 1909                 x, K.shape(input_image)[1:3]))(input_gt_boxes)     1910             # 3. GT Masks (zero padded)     1911             # [batch, height, width, MAX_GT_INSTANCES]    c:\users\owner\anaconda3\envs\maskrcnn\lib\site-packages\keras\engine\base_layer.py in __call__(self, *args, **kwargs)      944     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):      945       return self._functional_construction_call(inputs, args, kwargs,  --> 946                                                 input_list)      947       948     # Maintains info about the `Layer.call` stack.    c:\users\owner\anaconda3\envs\maskrcnn\lib\site-packages\keras\engine\base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)     1082       # Check input assumptions set after layer building, e.g. input shape.     1083       outputs = self._keras_tensor_symbolic_call(  -> 1084           inputs, input_masks, args, kwargs)     1085      1086       if outputs is None:    c:\users\owner\anaconda3\envs\maskrcnn\lib\site-packages\keras\engine\base_layer.py in _keras_tensor_symbolic_call(self, inputs, input_masks, args, kwargs)      814       return tf.nest.map_structure(keras_tensor.KerasTensor, output_signature)      815     else:  --> 816       return self._infer_output_signature(inputs, args, kwargs, input_masks)      817       818   def _infer_output_signature(self, inputs, args, kwargs, input_masks):    c:\users\owner\anaconda3\envs\maskrcnn\lib\site-packages\keras\engine\base_layer.py in _infer_output_signature(self, inputs, args, kwargs, input_masks)      860                               build_graph=False)      861       outputs = tf.nest.map_structure(  --> 862           keras_tensor.keras_tensor_from_tensor, outputs)      863       864     if hasattr(self, '_set_inputs') and not self.inputs:    c:\users\owner\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\util\nest.py in map_structure(func, *structure, **kwargs)      865       866   return pack_sequence_as(  --> 867       structure[0], [func(*x) for x in entries],      868       expand_composites=expand_composites)      869     c:\users\owner\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\util\nest.py in  (.0)      865       866   return pack_sequence_as(  --> 867       structure[0], [func(*x) for x in entries],      868       expand_composites=expand_composites)      869     c:\users\owner\anaconda3\envs\maskrcnn\lib\site-packages\keras\engine\keras_tensor.py in keras_tensor_from_tensor(tensor)      578       break      579   --> 580   out = keras_tensor_cls.from_tensor(tensor)      581       582   if hasattr(tensor, '_keras_mask'):    c:\users\owner\anaconda3\envs\maskrcnn\lib\site-packages\keras\engine\keras_tensor.py in from_tensor(cls, tensor)      170       # Fallback to the generic arbitrary-typespec KerasTensor      171       name = getattr(tensor, 'name', None)  --> 172       type_spec = tf.type_spec_from_value(tensor)      173       return cls(type_spec, name=name)      174     c:\users\owner\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\framework\type_spec.py in type_spec_from_value(value)      578       579   raise TypeError(""Could not build a TypeSpec for %r with type %s"" %  --> 580                   (value, type(value).__name__))      581       582     TypeError: Could not build a TypeSpec for   with type KerasTensor  "
"I want to perform segmentation using coco dataset. I want to perform it on frames obtained from realsense connected to a jetson xavier. I later want to post process the mask data obtained from the segmentation. Which is the best source fo rthis? Also, I want to use python fo the entire process."
"Hello, I'm wondering if you guys plan to upgrade Mask R-CNN to Python 3.9? I get issues with TensorFlow with anything newer than python 3.5."
"Hey guys,    I'm using mask r CNN to segment the lenses of eyeglasses from selfies.     I was wondering:  Should I use 3 classes when training (full framed, semi-framed, rimless), since they differ around the lenses, or should I just keep it as one class?     It would probably work both ways, but what is the best way? Does it matter?  Is there any way to measure which is the best way to do it?   Should I look at any other metrics apart from IoU?  Is there any training data I could look at while training the model to determine the best way?    Kind regards    "
"I got error ValueError: cannot reshape array of size 0 into shape (0) .How can I fix it?  The problem come from   r['masks']) , but i don't know why? thx!        AP, precisions, recalls, overlaps = \          utils.compute_ap(gt_bbox, gt_class_id, gt_mask,                           r[""rois""],                            r[""class_ids""],                            r[""scores""],                           r['masks'])                                 visualize.plot_precision_recall(AP, precisions, recalls)      APs.append(AP)    print(""AP: "", APs)  print(""mAP: "", np.mean(APs))  `    And the result    `Traceback (most recent call last):    File ""test.py"", line 122, in        r['masks'])    File ""C:\Users\USER\anaconda3\envs\tensorflow\lib\site-packages\mrcnn\utils.py"", line 739, in compute_ap      iou_threshold)    File ""C:\Users\USER\anaconda3\envs\tensorflow\lib\site-packages\mrcnn\utils.py"", line 691, in compute_matches      overlaps = compute_overlaps_masks(pred_masks, gt_masks)    File ""C:\Users\USER\anaconda3\envs\tensorflow\lib\site-packages\mrcnn\utils.py"", line 108, in compute_overlaps_masks      masks2 = np.reshape(masks2 > .5, (-1, masks2.shape[-1])).astype(np.float32)    File "" "", line 6, in reshape    File ""C:\Users\USER\anaconda3\envs\tensorflow\lib\site-packages\numpy\core\fromnumeric.py"", line 299, in reshape      return _wrapfunc(a, 'reshape', newshape, order=order)    File ""C:\Users\USER\anaconda3\envs\tensorflow\lib\site-packages\numpy\core\fromnumeric.py"", line 58, in _wrapfunc      return bound(*args, **kwds)  ValueError: cannot reshape array of size 0 into shape (0)`"
"Hello all, First of all thank you @Waleedka for the maskrcnn implementation that was provided, and maintained very well  throughout these years.    My issue stated here, is regarding overfitting,finetuning and prediction offsets on the custom dataset.    The dataset used is real-estate memorandum images and only one target object which is 'valid'.  'valid' in our use-case was the useful text portion in the real-estate memorandum image. you can get an idea by below shown  three images.    !   !   !     Dataset size- 1000 images for training, 22 images for validation.  Annotations used- viatool rectangular region annotations.    Number of Epochs trained- 100  Layers- Only Heads  training loss- 0.37  validation loss- 1.41  learning rate - 0.0002    So, the resulting inferences on the validation set are shown below    !   !   !     you can observe the masks are not properly occupying the bounding boxes and the bounding boxes are some times offset to  some portion of the predictions.  Also after 100th Epoch, the training loss and validation loss are not at all decreasing. i checked upto 40 Epochs.  Increasing the learning rate lead to huge increase in training loss and validation loss  Decreasing the learning rate lead to no improvement in training loss,validation loss and predictions.    It would be great if anyone who faced the similar issue one way or the other could kindly  share their suggestions and references.     Best Regards,    AnishZz"
"There are many issues open that talk about conversion of Model from tensorflow to TFLite but none has straight answer to it. If anyone has able to do it by custom ops, please post it here. It would be really great if you can share the code. Any kind of knowledge sharing would be great. I tried converting the model but I got error(Segmentation Fault) while inference.    If nobody has done it, I would like to contribute to addition of TF Lite feature. It would be great help to our community. "
"I've read that there is a training strategy that separate training into 3 stages,   1. Train heads for few epoch until overfitting, then  2. Train layer 5+ or 4+ for few epoch until overfitting, then  3. Train all layers until overfitting.    But I don't understand, when do you decide overfitting.  Is it the epoch that validation loss increase?    Since I'm not sure when to decide overfitting, I've train each training parameter (all, s3+, s4+, s5+, heads) to see how they are difference.  !   My dataset contains  400 training image and 80 validation image of 6 classes. I use the default config parameter with mask_rcnn_coco.h5 pre-trained weight, no learning rate reducing.  In the image 1_train_all got the lowest validation loss, is this mean that for my dataset training all layer is better than train heads first?  Or the training loss and validation loss is not matter, it depended on the prediction result?  "
"First off, I'm really, _really_ new to (practical data science). I've taken a course in ML before, but this is my first time doing any DL, and I don't even remember half of what I did in that course.  In any case, I'm trying to **train a model on the pklot dataset**, obtained from   The problem is, I have no idea how to train the model on a custom dataset. I wrote this code, modified slightly from   I uploaded it as a notepad, trial1.txt:           I also tried (emphasis on tried) to modify the actual coco.py file, thinking that would make it easier. It's uploaded as trial2.txt:         Any help is appreciated, thank you! I don't even know what I'm looking at half the time, haha. Note that both obviously didn't work :/"
"I am training my own model with custom images(250 images) and weights from Neptun AI project. When I open task manager, CPU usage raises to 100% in the beginning of each epoch step for some seconds; in overall, it uses a lot of CPU.   I have never encountered with this kind of issue while training another model; not sure if it is from Mask RCNN codebase or something else. Can anyone enlighten me about the issue? Is this normal?    Edit: I have set `workers = multiprocessing.cpu_count()` to `workers = multiprocessing.cpu_count() - 4` but it still uses 100%, how is this possible?"
Thanks for the amazing work! I'm still confused with the way how do we approach if we want to run it on test video?
!   Need a solution for this error. Please help!
100/100 [==============================] - 136s 1s/step - loss: 2.0010 - rpn_class_loss: 0.4009 - val_loss: 1.7868 - val_rpn_class_loss: 0.3553  Epoch 2/100  100/100 [==============================] - 117s 1s/step - loss: 1.1283 - rpn_class_loss: 0.2530 - val_loss: 1.5249 - val_rpn_class_loss: 0.2836  Epoch 3/100  100/100 [==============================] - 116s 1s/step - loss: 0.8029 - rpn_class_loss: 0.2133 - val_loss: 1.3147 - val_rpn_class_loss: 0.2652  Epoch 4/100  100/100 [==============================] - 117s 1s/step - loss: 0.6996 - rpn_class_loss: 0.2019 - val_loss: 1.5031 - val_rpn_class_loss: 0.2805    My problem is there was no mrcnn_bbox_losssã€mrcnn_mask_loss and rpn_bbox_loss show in my terminal. And the same at tensorboard in browser.  Please help me. Thanks!
"I am training on my own dataset, the loss is decreasing, but I have some problems, which are because I do not know how to use some parameters correctly. This is my training Configuration:  `IMAGES_PER_GPU = 8        # Number of classes (including background)      NUM_CLASSES = 1 + 2  # Background + 2 Object classes        # Number of training steps per epoch      STEPS_PER_EPOCH = 50        # Skip detections with < 90% confidence      DETECTION_MIN_CONFIDENCE = 0.9        EPOCHS = 50        VALIDATION_STEPS = 10        MAX_GT_INSTANCES = 70      1. The training performance is not as good as I'd hoped. I trained for 50 Epochs now, loss is at ~0.8, but that my be because of the size of my data set, it is only ~350 images yet. I also only trained the heads yet. Still, the predicted images do not look like the model is working, but the other losses are low.     !     2. As you see, I did not change IMAGE_SHAPE, IMAGE_Min_DIM or IMAGE_MAX_DIM in the config. When loading the model in inference mode and not setting those parameters, it throws  `Exception: Image size must be dividable by 2 at least 6 times to avoid fractions when downscaling and upscaling.For example, use 256, 320, 384, 448, 512, ... etc.`  (I also set `GPU_COUNT = 1` and `IMAGES_PER_GPU = 1` for inference.)  The training data is 250x420x4 Pixel (TIF, last one is depth to be ignored here), as is the data I am trying to predict on.   When I set  `config.IMAGE_SHAPE =      How do I set the shape parameters so they are right? This prediction is not just a case of bad performance, right?"
"Hi, I am running the default coco dataset for my own segmentation use case. I am using a jetson Xavier nx with jetpack 4.5 and TensorFlow 1.15.4. It is taking forever to load the model and TensorFlow is taking too long to process even a single image. I am not sure if it is even using the inbuilt GPU. The jetson has only TensorFlow 1.15.5 or TensorFlow 2. So I'm not sure if it's the TensorFlow version that is causing the lag. How do I go about this?   Its taking forever to load the coco model file (h5py file). How do I check the memory usage and if the GPU is being used at all or not?"
"Dear All,  Will a gaming laptop with i7 (10th generation), 16GB, RTX 2070, 1TB HDD be enough to do Mask_RCNN training and testing?  Thank you.  Best regards,  zeyarag  "
Does anyone know how to write the predicted mask co-ordinates to a text file? I want to import my prediction results to GIS.
"hi,  i have a problem when i use this command to try a coco test  `python Mask_RCNN-master/samples/coco/coco.py evaluate --dataset=/path/to/coco/ --model=last`    error :    `File ""C:\Users\User1\Docs\IA\Mask_RCNN-master\venv\lib\site-packages\mrcnn\model.py"", line 2086, in find_last        errno.ENOENT, ""Could not find weight files in {}"".format(dir_name))  FileNotFoundError: [Errno 2] Could not find weight files in C:\Users\User1\Docs\logs\coco20210614T2106`    can u help me please?  "
"hi, i am a university student who is approaching this world for the first time and i am not very familiar with python. I am doing an internship at a company and they assigned me a project that involves training a data set on two classes (silos and chimneys).  Instead of making a copy of the code and modifying this, I preferred to directly replace the existing dataset and json file (I used CVAT to do the image segmentation). I wanted to modify BalloonDataset class in balloon.py to make it efficient for my dataset, but am having difficulty modifying. Would it be possible to get help?    I uploaded three images where I can illustrate the hierarchy of my json file.    !   !   !   "
"i AM USING JETSON XAVIER nx. I usually use tensorflow 1.8 and keras 2.1.5 for geting this repository to work on aws inside jupyter notebook. No other version of tensorflow seems to wrok. Now I have had to switch to the jetson xavire, I was wondering if tensorflow 1.15 would be compatible or not. The jetson doesnt have a version of tensorflow older than 1.15. What can i do?"
I am training the Mask RCNN model on google collaboratory. I have been using the TensorFlow 2.x compatible version by   and facing this issue:     `WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... can't pickle _thread.RLock objects`    The error caused is affecting the saving of trained weights. Model training continues but there is no file saved.
"While running demo.ipynb i get the below error:    Tensorflow version = 1.15.0  ---------------------------------------------------------------------------  AttributeError                            Traceback (most recent call last)    in          3         4 # Load weights trained on MS-COCO  ----> 5 model.load_weights(COCO_MODEL_PATH, by_name=True)    ~\Documents\Mask_RCNN-master\mrcnn\model.py in load_weights(self, filepath, by_name, exclude)     2128      2129         if by_name:  -> 2130             saving.load_weights_from_hdf5_group_by_name(f, layers)     2131         else:     2132             saving.load_weights_from_hdf5_group(f, layers)    c:\users\chethan\anaconda3\envs\rcnn\lib\site-packages\keras\engine\topology.py in load_weights_from_hdf5_group_by_name(f, layers, skip_mismatch, reshape)     3416     """"""     3417     if 'keras_version' in f.attrs:  -> 3418         original_keras_version = f.attrs['keras_version'].decode('utf8')     3419     else:     3420         original_keras_version = '1'    AttributeError: 'str' object has no attribute 'decode'    How can I fix this issue ?"
"Followed the steps outlined in #1121, but my output is looking very different than it should.     Using the code:       I get this image:    !     When the base image should be:    !     ----    I'm totally befuddled since doing simple testing with:         or         which are called in load_image() produce the correct output.    ---    Literally any help would be great.  "
"Hi everybody.    i tried to work with the Balloon Sample.  For some reason, i get an error likle these: ""index 1024 is out of bounds for axis 1 with size 1024"" Its accurant at load_mask():    `# Convert polygons to a bitmap mask of shape          # [height, width, instance_count]          info = self.image_info[image_id]          mask = np.zeros([info[""height""], info[""width""], len(info[""polygons""])],                          dtype=np.uint8)          for i, p in enumerate(info[""polygons""]):              # Get indexes of pixels inside the polygon and set them to 1              rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])              mask[rr, cc, i] = 1`    in the last row.     Somebody has any idea why there is an error?"
"Hi every, a week ago, I trained my custom model as usually, but now it pop up this problem.   I have tried change my tensorflow-gpu, keras, h5py,.. version as follow but nothing seem to work:  tensorflow-gpu==1.15.0  keras==2.2.5/2.1.6  h5py==2.10.0    If you have encountered this problem, please help me. Thanks for advanced.    Traceback (most recent call last):    File ""train.py"", line 520, in        train(model)    File ""train.py"", line 293, in train      layers='heads')    File ""/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 2424, in train    File ""/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 2251, in compile    File ""/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 2252, in      File ""/usr/local/lib/python3.7/dist-packages/keras/regularizers.py"", line 41, in __call__      if self.l2:  ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
"ERROR SHOWS LIKE THIS: PLEASE HELP ME IN THIS I GOT STUCK IN IT:-    AttributeError                            Traceback (most recent call last)    in    ----> 1 model = modellib.MaskRCNN(        2     mode='training',        3     config=config,        4     model_dir=DEFAULT_LOGS_DIR        5 )    C:\ANACONDA\mrcnn\model.py in __init__(self, mode, config, model_dir)     1835         self.model_dir = model_dir     1836         self.set_log_dir()  -> 1837         self.keras_model = self.build(mode=mode, config=config)     1838      1839     def build(self, mode, config):    C:\ANACONDA\mrcnn\model.py in build(self, mode, config)     1853      1854         # Inputs  -> 1855         input_image = KL.Input(     1856             shape=[None, None, config.IMAGE_SHAPE[2]], name=""input_image"")     1857         input_image_meta = KL.Input(shape=[config.IMAGE_META_SIZE],    C:\ANACONDA\lib\site-packages\keras\engine\topology.py in Input(shape, batch_shape, name, dtype, sparse, tensor)     1452     if not dtype:     1453         dtype = K.floatx()  -> 1454     input_layer = InputLayer(batch_input_shape=batch_shape,     1455                              name=name, dtype=dtype,     1456                              sparse=sparse,    C:\ANACONDA\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)       89                 warnings.warn('Update your `' + object_name +       90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)  ---> 91             return func(*args, **kwargs)       92         wrapper._original_function = func       93         return wrapper    C:\ANACONDA\lib\site-packages\keras\engine\topology.py in __init__(self, input_shape, batch_size, batch_input_shape, dtype, input_tensor, sparse, name)     1361         if input_tensor is None:     1362             self.is_placeholder = True  -> 1363             input_tensor = K.placeholder(shape=batch_input_shape,     1364                                          dtype=dtype,     1365                                          sparse=self.sparse,    C:\ANACONDA\lib\site-packages\keras\backend\tensorflow_backend.py in placeholder(shape, ndim, dtype, sparse, name)      505         x = tf.sparse_placeholder(dtype, shape=shape, name=name)      506     else:  --> 507         x = tf.placeholder(dtype, shape=shape, name=name)      508     x._keras_shape = shape      509     x._uses_learning_phase = False    AttributeError: module 'tensorflow' has no attribute 'placeholder'    AND MY CODE WAS:-    model = modellib.MaskRCNN(      mode='training',       config=config,       model_dir=DEFAULT_LOGS_DIR  )    model.load_weights(      COCO_MODEL_PATH,       by_name=True,       exclude=[""mrcnn_class_logits"", ""mrcnn_bbox_fc"", ""mrcnn_bbox"", ""mrcnn_mask""]  ) "
1) I have changed all the steps to train gray images. And it ran perfectly.  Number 1 and 2 are vice versa if visualize the image then shows training error. if train perfectly the shows visualize error.  2) But when I try to visualize the single image it shows me this error.  !      3) When I segment the images it generates continuous empty images there was no segmented mask. Why this happens can anyone tell me?    !     
"Hi ,         I am in the process of implementing this library . I am using the following(**recommended by matterport**) versions of keras & Tensor flow .Code is being  executed on Google Colab.    Keras: 2.5.0  Tensor Flow :2.50    I am getting the following error  ---------------------------------------------------------------------------  AttributeError                            Traceback (most recent call last)    in  ()        1 # define the model  ----> 2 from mrcnn.model import MaskRCNN        3         4 # model_dir is where want intermediate models to be saved , lest use ""workingdirectory"" path that we created        5 model = MaskRCNN(mode='training', model_dir=filepath, config=config)    /content/Mask_RCNN/mrcnn/model.py in  ()      253       254   --> 255 class ProposalLayer(KE.Layer):      256     """"""Receives anchor scores and selects a subset to pass as proposals      257     to the second stage. Filtering is done based on anchor scores and    **AttributeError: module 'keras.engine' has no attribute 'Layer'**      Please advice why this issue would be cropping up and what we can do resolve it ?         "
"I am trying to run the demo of Mask_RCNN and the pre-trained custom model in my data-set.   I always faced this error massage:   %%%%%the error massage %%%%%%%%%%%%  AttributeError                            Traceback (most recent call last)    in  ()       14 sys.path.append(ROOT_DIR)  # To find local version of the library       15 from mrcnn import utils  ---> 16 import mrcnn.model as modellib       17 from mrcnn import visualize       18 # Import COCO config    /content/Mask_RCNN/mrcnn/model.py in  ()      253       254   --> 255 class ProposalLayer(KE.Layer):      256     """"""Receives anchor scores and selects a subset to pass as proposals      257     to the second stage. Filtering is done based on anchor scores and    AttributeError: module 'keras.engine' has no attribute 'Layer'    %%%%the end of error massage%%%%%%%%%%%  which occurred while running the command:  import mrcnn.model as modellib    can any one explain to me what is exactly the problem ,and how can i solve it ? i will be so thankful  "
"Hey   I want to know, how I can print the IoU metric??What function should I call? "
Archive this repository if so
"I am using the given coco.py file to train on COCO 2017 dataset, with some other small changes.    Directory tree looks like this.    Dataset directory       annotations       test2017       train2017       val2017  !     Something similar to this issue here,   "
"I am running this repo on a jupyter notebook online connected to an EC2 instance. However, I getting the following error. I am using tensorflow 1.3.0 and keras 2.0.8 as mentioned in the requirements.txt file. What is the problem? I am running the demo.ipynb file and its not running    "
I have completed training the Mask_RCNN on a very small dataset of 22 training images and 6 validation images which I took with my RealSense Depth camera. I trained on 3 different modes written below and compared the log losses graphs on TensorBoard to see if RGB-Depth has a better performance. However I have problem interpreting the graphs shown below. Can somebody help me. Thank you.     1) RGB mode 3 channel  2) Depth mode 1 channel (16bit)  3) RGB-Depth mode 4 channel    !                    
"---------------------------------------------------------------------------  ValueError                                Traceback (most recent call last)    in        103 config.display()      104 # define the model  --> 105 model = MaskRCNN(mode='training',model_dir='./',config=config)      106 # load weights(mscoco) and exclude the output layers      107 model.load_weights('mask_rcnn_coco.h5',by_name=True,exclude=['mrcnn_class_logits',""mrcnn_bbox_fc"",""mrcnn_bbox"",""mrcnn_mask""])    ~/anaconda3/lib/python3.8/site-packages/mask_rcnn-2.1-py3.8.egg/mrcnn/model.py in __init__(self, mode, config, model_dir)     1835         self.model_dir = model_dir     1836         self.set_log_dir()  -> 1837         self.keras_model = self.build(mode=mode, config=config)     1838      1839     def build(self, mode, config):    ~/anaconda3/lib/python3.8/site-packages/mask_rcnn-2.1-py3.8.egg/mrcnn/model.py in build(self, mode, config)     1932             anchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape)     1933             # A hack to get around Keras's bad support for constants  -> 1934             anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=""anchors"")(input_image)     1935         else:     1936             anchors = input_anchors    ~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)      920                     not base_layer_utils.is_in_eager_or_tf_function()):      921                   with auto_control_deps.AutomaticControlDependencies() as acd:  --> 922                     outputs = call_fn(cast_inputs, *args, **kwargs)      923                     # Wrap Tensors in `outputs` in `tf.identity` to avoid      924                     # circular dependencies.    ~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py in call(self, inputs, mask, training)      887         variable_scope.variable_creator_scope(_variable_creator):      888       result = self.function(inputs, **kwargs)  --> 889     self._check_variables(created_variables, tape.watched_variables())      890     return result      891     ~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py in _check_variables(self, created_variables, accessed_variables)      914           Variables.'''      915       ).format(name=self.name, variable_str=variable_str)  --> 916       raise ValueError(error_str)      917       918     untracked_used_vars = [    ValueError:   The following Variables were created within a Lambda layer (anchors)  but are not tracked by said layer:       The layer cannot safely ensure proper Variable reuse across multiple  calls, and consquently this behavior is disallowed for safety. Lambda  layers are not well suited to stateful computation; instead, writing a  subclassed Layer is the recommend way to define layers with  Variables.    â€‹  "
"Hello,    I am trying to use your code to train my own images (  But I had some issues loading the image with mask by the path. Can you tell me how to achieve it, please?    Thanks!  Thiem"
"I have running a MASK RCNN Model with tensorflow 1.15 and keras 2.1.6 every thing worked correctly but today when I want to run it again I get this error ""module 'tensorflow._api.v1.compat.v2' has no attribute 'internal'""    !   "
"Hi ,  I am new to this model and I found this model really interesting.  I have a question.  I want to put black mask to the background of images.  How can I do it by programming ?   Thanks"
"I was able to do fine tuning using the balloon code. Thank you.    However, only balloons can be detected in models created by fine tuning.    I want to be able to detect balloon classes by adding to the 80 classes that can be detected by ""mask_rcnn.h5"". I mean I want to do ""additional learning"".    If anyone knows how to do it, please let me know.  Thanks!!!"
"AttributeError                            Traceback (most recent call last)    in  ()        1 # Create model object in inference mode.  ----> 2 model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)        3 # Load weights trained on MS-COCO        4 model.load_weights(COCO_MODEL_PATH, by_name=True)    7 frames  /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)      668       except Exception as e:  # pylint:disable=broad-except      669         if hasattr(e, 'ag_error_metadata'):  --> 670           raise e.ag_error_metadata.to_exception(e)      671         else:      672           raise    AttributeError: in user code:        /content/Mask_RCNN/mrcnn/model.py:390 call  *          roi_level = log2_graph(tf.sqrt(h * w) / (224.0 / tf.sqrt(image_area)))      /content/Mask_RCNN/mrcnn/model.py:341 log2_graph  *          return tf.log(x) / tf.log(2.0)        AttributeError: module 'tensorflow' has no attribute 'log'"
"AttributeError                            Traceback (most recent call last)    in  ()        1 # Create model object in inference mode.  ----> 2 model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)        3 # Load weights trained on MS-COCO        4 model.load_weights(COCO_MODEL_PATH, by_name=True)    7 frames  /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)      668       except Exception as e:  # pylint:disable=broad-except      669         if hasattr(e, 'ag_error_metadata'):  --> 670           raise e.ag_error_metadata.to_exception(e)      671         else:      672           raise    AttributeError: in user code:        /content/Mask_RCNN/mrcnn/model.py:390 call  *          roi_level = log2_graph(tf.sqrt(h * w) / (224.0 / tf.sqrt(image_area)))      /content/Mask_RCNN/mrcnn/model.py:341 log2_graph  *          return tf.log(x) / tf.log(2.0)        AttributeError: module 'tensorflow' has no attribute 'log'  "
"Hi everyone,     i wanted to know how i can visualize the Ground Truth Mask of my training dataset. I have a dataset with 8 classes and in every image are multiple different classes. Is it possible to generate a ground truth mask, where i can see all instances?    Thank you very much. "
"Hello everybody,   I have a dataset of images of 1240x1280 which contain cells (not too much crowded). If I want to decrease the training time and calculation cost, should I resize them to 512x512 using the square mode, like this there will be these top and bottom   !   and if i do the cropping, there will be few cells and even images without cells  !   I would be grateful if can someone advice me based on your knowledge and experiences using mask rcnn.  N.B: for prediction my images are also 1024x1280.   "
"When I train the network, rpn_bbox_loss is too high, up to 0.4+. and others all ok less than 0.05 , my env is tf2.4.1 + python3.6,   Anyone has the idea?"
"I use the current setup:    ### Hardware:  - i7 based Acer system  - NVIDIA GEFORCE RTX  - NVIDIA Jetson Xavier (target)    ### Software:  - Ubuntu 18.04  - TensorRT v7.2.2  - CUDA Version 10.2.89   - cuDNN v8.1.1   - GNU make >= v4.1  - cmake >= v3.13  - Python 3.6.5  - Uff 0.6.9  - graphsurgeon    ---    ## Attempt 1     I followed the steps given in   and went on modifying the `conv2d_transpose` function in `/usr/lib/python3.6/dist-packages/uff/converters/tensorflow/converter_functions.py`. I also applied the `0001-Update-the-Mask_RCNN-model-from-NHWC-to-NCHW.patch` patch.     For test, I used the same sample model provided in the instructions. In short, I replicated all the steps.     The `config.py` and `model.py` I used, have been attached below        I get the error ending with:        How is it possible that the README leads to such common error. I found other people experiencing the same troubles  ."
"Hi,    How can i get the pipeline config for a custom dataset training to use with cv::dnn::readNetFromTensorflow?    Has anyone managed to use a custom model like the example balloon in OpenCV cv::dnn::readNetFromTensorflow?    Thanks  Luis Silva"
"I'm training a mask r cnn model to detect 13 class. When I'm testing the model, I get the following error: *** No instances to display *** .    I tried this changes:         Sometimes them work, other times they don't.    I have another question: I have 21 classes but i want to clasify only 13 of them. My goals are:    >  detect this 13 classes inside a specif area (like a truck), how can I do this?    > i would like to predict all items in a single image in maximum 1 second, how can I do this?  "
I got this error while trying to execute this command:  `python3 balloon.py train --dataset=/path/to/balloon/dataset --weights=last`                Help me  
"After successfully training the model (Mask RCNN) I am unable to get the segmented mask image as model output. Instead, I am getting an image as output with small white dots on black background. Unable to understand why I am getting white dots instead of proper mask segmented image as output.   Please suggest me needful to overcome the issue. "
"Goodmorning,  first of all I wanted to thank you for having made available these codes that allow me to create a mask-rcnn model with the associated weights.  However when I want to use the model, the loading of the packages and the loading of the model itself take about 30 seconds. How can I decrease this time? Does anyone have a solution?    Thank you for the attention and availability    Benedetta"
"Hello, I am working on Mask RCNN and encountering some issues. Actually, When objects are 100% overlapping with each other, the model isn't detecting the object on top. While if top object is little out from the region of bottom objects, results are fine. My training dataset contains enough examples like that but no effect. Kindly, help me out, how can I manage the case of 100% overlapping.     !   "
"I found a Github implementation to compute confusion matrix and tp/fp/fn values from here :   I implemented the same as follows-    def confusion_matrix(dataset, model, cfg):  gt_tot = np.array([])  pred_tot = np.array([])  mAP_ = []  for image_id in dataset.image_ids:      # load image, bounding boxes and masks for the image id      image, image_meta, gt_class_id, gt_bbox, gt_mask = modellib.load_image_gt(dataset, cfg, image_id)      # convert pixel values (e.g. center)      scaled_image = modellib.mold_image(image, cfg)      # convert image into one sample      sample = np.expand_dims(scaled_image, 0)      # make prediction      yhat = model.detect([image], verbose=0, mask_threshold=0.5)      # extract results for first sample      r = yhat[0]        # compute gt_tot and pred_tot      gt, pred = utils_confusion.gt_pred_lists(gt_class_id, gt_bbox, r['class_ids'], r['rois'])      gt_tot = np.append(gt_tot, gt)      pred_tot = np.append(pred_tot, pred)        AP_, precision_, recall_, overlap_ = mrcnn.utils.compute_ap(gt_bbox, gt_class_id, gt_mask, r[""rois""], r[""class_ids""],                                                          r[""scores""], r['masks'])      mAP_.append(AP_)        tp, fp, fn = utils_confusion.plot_confusion_matrix_from_data(gt_tot, pred_tot, fz=18, figsize=(20, 20), lw=0.5)      del tp[0]      del fp[0]      del fn[0]      print(""tp for each class :"", tp)      print(""fp for each class :"", fp)      print(""fn for each class :"", fn)  confusion_matrix(val_set,model_inference,Icfg)    This gives the individual matrix+tp/fp/fn values for each image. How can I modify it so that I get the values/matrix of the combined dataset i.e. total number of fp,fn,tp.    Also is there a way to compute precision/recall from the entire dataset from this?"
"I am trying to train the model with a training schedule where the lr is reduced /10 every few epochs. For example, heads trained for 10 epochs at lr, 4+ for next 10 epochs at lr/10, all layers for another 10 epochs at lr/100 (total 30 epochs). However the last increases after every training stage rather than going down i.e. if it finishes the first 10 at a loss value = 1.6, the next stage starts at a loss of 3.4. Does anyone know the reason behind this.     config.LEARNING_RATE=0.001  Training schedule:    model.train(train_set, val_set,    learning_rate=config.LEARNING_RATE/10,    epochs=10,     augmentation=augmentation,     layers='heads')  model.train(train_set, val_set,     learning_rate=config.LEARNING_RATE/100,    epochs=20,    augmentation=augmentation,      layers='4+')  model.train(train_set, val_set,     learning_rate=config.LEARNING_RATE/1000,    epochs=30,    augmentation=augmentation,      layers='all')    Configurations:     # Configuration  class LIDARConfig(Config):      """"""Configuration for training on LIDAR dataset.      Derives from the base Config class and overrides values specific      to the LIDAR dataset.      """"""      # Give the configuration a recognizable name      NAME = ""LIDAR_Celtic""        # Train on 1 GPU and 8 images per GPU. We can put multiple images on each      # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).      GPU_COUNT = 1      IMAGES_PER_GPU = 1      # Number of classes (including background)      NUM_CLASSES = 1 + 1  # background + 1 shape (Celtic)        # Use small images for faster training. Set the limits of the small side      # the large side, and that determines the image shape.      #IMAGE_MIN_DIM = 128      #IMAGE_MAX_DIM = 128      IMAGE_MAX_DIM = 512      IMAGE_MIN_DIM = 320        #MASK_SHAPE = [56,56]        # Use smaller anchors because our image and objects are small      #RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels      #RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)      RPN_ANCHOR_SCALES = (16, 32, 64, 128, 256)      RPN_TRAIN_ANCHORS_PER_IMAGE = 128        # Reduce training ROIs per image because the images are small and have      # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.      TRAIN_ROIS_PER_IMAGE = 100        # Use a small epoch since the data is simple      STEPS_PER_EPOCH = 500        # use small validation steps since the epoch is small      VALIDATION_STEPS = 50        # Maximum number of ground truth instances to use in one image      MAX_GT_INSTANCES = 40      # Max number of final detections      DETECTION_MAX_INSTANCES = 40      # Minimum probability value to accept a detected instance      # ROIs below this threshold are skipped      DETECTION_MIN_CONFIDENCE = 0.8        USE_MINI_MASK = False        LOSS_WEIGHTS = {          ""rpn_class_loss"": 1.,          ""rpn_bbox_loss"": 1.,          ""mrcnn_class_loss"": 1.,          ""mrcnn_bbox_loss"": 1.,          ""mrcnn_mask_loss"": 1.      }  "
"Hi all,    I am new to Mask RCNN. I trained a small dataset with 7 classes using 210 images for training and 28 for validation. This is my configuration. Any suggestion on how to improve it ?    >Configurations:  BACKBONE                       resnet50  BACKBONE_STRIDES                "
Is there any explanation to that?
"Hi all,     I am using this maskrcnn library to do detection and segmentation. I have this class distribution:   **Class_Occurrences = { 0:189 , 1:22,  2:1,  3:40,  4:28,  5:85,  6:40,  7:63,  8:42,  9:5 }**  key: class_id, value: number of occurrences.   First class with key 0 is the background.    Data set contains **189** training images and **53**  validation images.     1.  **Training process 1** : 100 epoch, pre trained coco weights, without augmentation. the result mAP : 0.17  2. **Training process 2** : 100 epoch, pre trained coco weights, with online augmentation. the result mAP : 0.29  Augmentation Config:   ` augmentation = iaa.SomeOf((0, 3),  |!   ! |!   ! |!   ! |!   ! |!   ! |!         my question is, why the mAP is so low ? what I can do to increase the performance ? and why the training loss decreasing while validation loss in not (fluctuating) ?   I tried to add class_weight to work around the data imbalanced but I always get this error :  **Unknown entries in class_weight dictionary: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]. Only expected following keys: []**    **Model Configuration:**    Name          |  Value  :-------------------------:|:-------------------------:   BACKBONE|           resnet101  BACKBONE_STRIDES|                [4, 8, 16, 32, 64]  BATCH_SIZE |                     1  BBOX_STD_DEV |                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE|          None  DETECTION_MAX_INSTANCES|         100  DETECTION_MIN_CONFIDENCE|       0.9  DETECTION_NMS_THRESHOLD|         0.3  FPN_CLASSIF_FC_LAYERS_SIZE|      1024  GPU_COUNT|                       1  GRADIENT_CLIP_NORM|              5.0  IMAGES_PER_GPU|                  1  IMAGE_CHANNEL_COUNT|             3  IMAGE_MAX_DIM|                   1024  IMAGE_META_SIZE|                 22  IMAGE_MIN_DIM|                   800  IMAGE_MIN_SCALE|                 0  IMAGE_RESIZE_MODE|               square  IMAGE_SHAPE|                     [1024 1024    3]  LEARNING_MOMENTUM|               0.9  LEARNING_RATE|                   0.001  LOSS_WEIGHTS|                    {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE|                  14  MASK_SHAPE|                      [28, 28]  MAX_GT_INSTANCES|                100  MEAN_PIXEL|                      [123.7 116.8 103.9]  MINI_MASK_SHAPE|                 (56, 56)  NAM|E|                            object  NUM_CLASSES|                     10  POOL_SIZE|                       7  POST_NMS_ROIS_INFERENCE|         1000  POST_NMS_ROIS_TRAINING|          2000  PRE_NMS_LIMIT|                   6000  ROI_POSITIVE_RATIO|              0.33  RPN_ANCHOR_RATIOS|               [0.5, 1, 2]  RPN_ANCHOR_SCALES|               (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE|               1  RPN_BBOX_STD_DEV|                [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD|               0.7  RPN_TRAIN_ANCHORS_PER_IMAGE|     256  STEPS_PER_EPOCH|                 100  TOP_DOWN_PYRAMID_SIZE|           256  TRAIN_BN|                        False  TRAIN_ROIS_PER_IMAGE|            200  USE_MINI_MASK|                   True  USE_RPN_ROIS|                    True  VALIDATION_STEPS|                100  WEIGHT_DECAY|                    0.0001"
"I am training the model with a pretty small training set of 142 images. To mitigate this problem, I used augmentation, however the model seems to be overfitting in just the first few epochs(training with only heads)   !   , and the val loss doesn't seem to be reducing below 1.6. Does anyone have any suggestions? I have used the learning rates 0.001 and 0.0001. I am attaching the loss plot and the augmentations I have applied.     augmentation = iaa.Sequential([      iaa.Fliplr(0.5),      iaa.Flipud(0.5),      iaa.OneOf([iaa.Affine(rotate=90),                 iaa.Affine(rotate=180),                 iaa.Affine(rotate=270)])  ])  "
"Hi, I am training with custom data.    I resizing the original image to `1024x1024` and put it in. But, GT_mask is `(1127, 1596)`    Why does the size of GT_mask different?    Plz help me ðŸ˜¢            "
"Hi guys, I'm a data science student and I'm trying to build a mask-r-cnn model.     The annotations have been provided to me with this excel structure:  Name,  Class,  Polygon coordinates,  Region_count (nth polygon in the same image).    I've converted this xlsx to csv to json. This is the json output:  `[      {          ""Name"": ""foto_1jpg.jpg"",          ""Class"": ""tennis"",          ""Region_count"": ""1"",          ""Polygon Cordinates"": ""\""all_points_x\"":[154,157,230,275,278,218,160,112,113,154],\""all_points_y\"":[461,461,455,495,576,625,625,563,505,463]""      },      {          ""Name"": ""foto_1jpg.jpg"",          ""Class"": ""soccer"",          ""Region_count"": ""2"",          ""Polygon Cordinates"": ""\""all_points_x\"":[446,557,685,795,826,815,738,628,505,422,346,331,354,443],\""all_points_y\"":[230,186,212,321,411,538,641,687,684,632,525,426,331,224]""      },      {          ""Name"": ""foto_2jpg.jpg"",          ""Class"": ""soccer"",          ""Region_count"": ""1"",          ""Polygon Cordinates"": ""all_points_x:[331,403,518,626,688,734,758,681,594,484,369,314,282,274,329],\""all_points_y\"":[399,340,316,342,380,463,607,736,787,796,745,683,592,503,405]""      }]`    After this I've tried to train mask-r-cnn with this json, but my tries are failed.    How can I adapt my json file to a VIA json file structure ?Or it is better to adapt the python code to my json format ? Thanks for the attention."
"I trained a model to detect one class of grapes.    I want to be able to check whether 2 predication masks are overlapping (after NMS) with the same GT mask.  I added an image describing the problem.   in this case, I want to be able to count both masks (marked in orange) as hit, and be able to calculate the IOU value of each one of them with the same big GT mask (also marked in orange)- in order to calculate correctly (according to what I described) when calculating accuracy.    Any suggestions for metrics that suits for this case or how to solve it?    !         "
"I'm trying to do transfer learning from last weights I have already train,but after execution ,it's only output this without error.  !     I have seen #1546 are similar like my issues.    my code file is here,please give some advice.tks     "
None
"These are the steps that I have followed so far for my grey scale dataset.    **Step 1**         **Step 2**          >     def load_image(self, image_id):  >         """"""Load the specified image and return a [H,W,3] Numpy array.  >         """"""  >         # Load image  >         image = skimage.io.imread(self.image_info[image_id]['path'])           >         image = image[..., np.newaxis] # Extending the size of the image to be (h,w,1)  >         return image     > model.load_weights(COCO_MODEL_PATH, by_name=True,  >                         exclude=[""mrcnn_class_logits"", ""mrcnn_bbox_fc"",   >                                  ""mrcnn_bbox"", ""mrcnn_mask"", ""conv1""])     >  layer_regex = {  >             # all layers but the backbone  >             ""heads"": r""(conv1\_.*)|(mrcnn\_.*)|(rpn\_.*)|(fpn\_.*)"",  >     > def load_image(self, image_id):  >      image = image[..., np.newaxis]       > def resize_image(image, min_dim=None, max_dim=None, min_scale=None, mode=""square""):  > padding = [(top_pad, bottom_pad), (left_pad, right_pad)]  > image = np.pad(image, padding, mode='constant', constant_values=0)  >      > > if len(image.shape) != 3 or image.shape[2] != 3:  >         image = np.squeeze(image, axis = -1)  >         image = np.stack((image,) * 3, -1)  ```    When I run this code,  I faced this issue from train(model)    > `ValueError: len(output_shape) cannot be smaller than the image dimensions`    The issue is different if I used **""rgb2gray(image)""** instead at **Step 2**    > `ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (2,2) and requested shape (3,2)`    **Alternate step 2**    > `ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (2,2) and requested shape (3,2)`    Please provide some assistance."
"Hi, thank for great work  my data is a json file in COCO format but it does not include any segmentation annotation :     {""area"": 10712, ""iscrowd"": 0, ""image_id"": 379, ""bbox"": [557, 747, 103, 104], ""category_id"": 4, ""id"": 3286, ""ignore"": 0, ""segmentation"": []}    I got this error when using my data :    `  WARNING:root:Fail load requirements file, so using default ones.  /usr/local/lib/python3.7/dist-packages/setuptools/dist.py:645: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead    % (opt, underscore_opt))  /usr/local/lib/python3.7/dist-packages/setuptools/dist.py:645: UserWarning: Usage of dash-separated 'license-file' will not be supported in future versions. Please use the underscore name 'license_file' instead    % (opt, underscore_opt))  /usr/local/lib/python3.7/dist-packages/setuptools/dist.py:645: UserWarning: Usage of dash-separated 'requirements-file' will not be supported in future versions. Please use the underscore name 'requirements_file' instead    % (opt, underscore_opt))  running install  running bdist_egg  running egg_info  writing mask_rcnn.egg-info/PKG-INFO  writing dependency_links to mask_rcnn.egg-info/dependency_links.txt  writing top-level names to mask_rcnn.egg-info/top_level.txt  warning: Failed to find the configured license file 'LICENSE'  reading manifest file 'mask_rcnn.egg-info/SOURCES.txt'  reading manifest template 'MANIFEST.in'  warning: no files found matching 'README.md'  warning: no files found matching 'LICENSE'  writing manifest file 'mask_rcnn.egg-info/SOURCES.txt'  installing library code to build/bdist.linux-x86_64/egg  running install_lib  running build_py  creating build/bdist.linux-x86_64/egg  creating build/bdist.linux-x86_64/egg/mrcnn  copying build/lib/mrcnn/visualize.py -> build/bdist.linux-x86_64/egg/mrcnn  copying build/lib/mrcnn/config.py -> build/bdist.linux-x86_64/egg/mrcnn  copying build/lib/mrcnn/model.py -> build/bdist.linux-x86_64/egg/mrcnn  copying build/lib/mrcnn/save_result.py -> build/bdist.linux-x86_64/egg/mrcnn  copying build/lib/mrcnn/__init__.py -> build/bdist.linux-x86_64/egg/mrcnn  copying build/lib/mrcnn/utils.py -> build/bdist.linux-x86_64/egg/mrcnn  copying build/lib/mrcnn/parallel_model.py -> build/bdist.linux-x86_64/egg/mrcnn  byte-compiling build/bdist.linux-x86_64/egg/mrcnn/visualize.py to visualize.cpython-37.pyc  byte-compiling build/bdist.linux-x86_64/egg/mrcnn/config.py to config.cpython-37.pyc  byte-compiling build/bdist.linux-x86_64/egg/mrcnn/model.py to model.cpython-37.pyc  byte-compiling build/bdist.linux-x86_64/egg/mrcnn/save_result.py to save_result.cpython-37.pyc  byte-compiling build/bdist.linux-x86_64/egg/mrcnn/__init__.py to __init__.cpython-37.pyc  byte-compiling build/bdist.linux-x86_64/egg/mrcnn/utils.py to utils.cpython-37.pyc  byte-compiling build/bdist.linux-x86_64/egg/mrcnn/parallel_model.py to parallel_model.cpython-37.pyc  creating build/bdist.linux-x86_64/egg/EGG-INFO  copying mask_rcnn.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO  copying mask_rcnn.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO  copying mask_rcnn.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO  copying mask_rcnn.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO  zip_safe flag not set; analyzing archive contents...  creating 'dist/mask_rcnn-2.1-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it  removing 'build/bdist.linux-x86_64/egg' (and everything under it)  Processing mask_rcnn-2.1-py3.7.egg  Removing /usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg  Copying mask_rcnn-2.1-py3.7.egg to /usr/local/lib/python3.7/dist-packages  mask-rcnn 2.1 is already the active version in easy-install.pth    Installed /usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg  Processing dependencies for mask-rcnn==2.1  Finished processing dependencies for mask-rcnn==2.1  [ ]  !python3 samples/coco/coco.py train --dataset=/content/Mask_RCNN/dataset/coco/ --model=coco  /root/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])  /root/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])  /root/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])  /root/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])  /root/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])  /root/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    np_resource = np.dtype([(""resource"", np.ubyte, 1)])  /root/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])  /root/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])  /root/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])  /root/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])  /root/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])  /root/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    np_resource = np.dtype([(""resource"", np.ubyte, 1)])  Using TensorFlow backend.  WARNING:tensorflow:From samples/coco/coco.py:75: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.    WARNING:tensorflow:From samples/coco/coco.py:77: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.    2021-04-12 18:34:25.759206: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA  2021-04-12 18:34:25.763629: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz  2021-04-12 18:34:25.763909: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bb3a2b9340 executing computations on platform Host. Devices:  2021-04-12 18:34:25.763948: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0):  ,    2021-04-12 18:34:25.766009: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1  2021-04-12 18:34:25.841117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2021-04-12 18:34:25.842035: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bb3a2b9500 executing computations on platform CUDA. Devices:  2021-04-12 18:34:25.842072: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7  2021-04-12 18:34:25.842262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2021-04-12 18:34:25.843002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:   name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235  pciBusID: 0000:00:04.0  2021-04-12 18:34:25.843364: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0  2021-04-12 18:34:25.844951: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0  2021-04-12 18:34:25.846810: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0  2021-04-12 18:34:25.847213: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0  2021-04-12 18:34:25.850217: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0  2021-04-12 18:34:25.851868: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0  2021-04-12 18:34:25.856475: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7  2021-04-12 18:34:25.856617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2021-04-12 18:34:25.857449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2021-04-12 18:34:25.858136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0  2021-04-12 18:34:25.858221: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0  2021-04-12 18:34:25.859743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:  2021-04-12 18:34:25.859776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0   2021-04-12 18:34:25.859792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N   2021-04-12 18:34:25.859929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2021-04-12 18:34:25.860759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2021-04-12 18:34:25.861659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10813 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)  Command:  train  Model:  coco  Dataset:  /content/Mask_RCNN/dataset/coco/  Year:  2017  Logs:  Mask_RCNNr/dataset/coco/logs  Auto Download:  False    Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.7  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 1  IMAGE_CHANNEL_COUNT            3  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                93  IMAGE_MIN_DIM                  800  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           coco  NUM_CLASSES                    81  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  PRE_NMS_LIMIT                  6000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.5  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                1000  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001      WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.    WARNING:tensorflow:From /root/.local/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support. .wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.  Instructions for updating:  Use tf.where in 2.0, which has the same broadcast rule as np.where  WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py:553: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.    WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.    WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py:600: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.  Instructions for updating:  box_ind is deprecated, use box_indices instead  Loading weights  /content/Mask_RCNN/pretrained_model/coco/mask_rcnn_coco.h5  2021-04-12 18:34:34.295531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2021-04-12 18:34:34.296325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:   name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235  pciBusID: 0000:00:04.0  2021-04-12 18:34:34.296445: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0  2021-04-12 18:34:34.296496: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0  2021-04-12 18:34:34.296546: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0  2021-04-12 18:34:34.296595: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0  2021-04-12 18:34:34.296640: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0  2021-04-12 18:34:34.296688: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0  2021-04-12 18:34:34.296732: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7  2021-04-12 18:34:34.296833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2021-04-12 18:34:34.297626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2021-04-12 18:34:34.298317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0  2021-04-12 18:34:34.298383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:  2021-04-12 18:34:34.298404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0   2021-04-12 18:34:34.298419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N   2021-04-12 18:34:34.298540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2021-04-12 18:34:34.299287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2021-04-12 18:34:34.299997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10813 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)  loading annotations into memory...  Done (t=0.06s)  creating index...  index created!  loading annotations into memory...  Done (t=0.01s)  creating index...  index created!  Training network heads    Starting at epoch 0. LR=0.001    Checkpoint Path: Mask_RCNNr/dataset/coco/logs/coco20210412T1834/mask_rcnn_coco_{epoch:04d}.h5  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  In model:  rpn_model      rpn_conv_shared        (Conv2D)      rpn_class_raw          (Conv2D)      rpn_bbox_pred          (Conv2D)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)  /root/.local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  /root/.local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  /root/.local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.    WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.    WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.    Epoch 1/20  sdfsdfsdfsdf  isinstance(segm, list)  ERROR:root:Error processing image {'id': 3772, 'source': 'coco', 'path': '/content/Mask_RCNN/dataset/coco//train2017/3772.jpg', 'width': 827, 'height': 1169, 'annotations': [{'area': 83082, 'iscrowd': 0, 'image_id': 3772, 'bbox': [40, 548, 366, 227], 'category_id': 2, 'id': 12755, 'ignore': 0, 'segmentation': []}, {'area': 77827, 'iscrowd': 0, 'image_id': 3772, 'bbox': [406, 548, 349, 223], 'category_id': 2, 'id': 12756, 'ignore': 0, 'segmentation': []}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""samples/coco/coco.py"", line 273, in load_mask      image_info[""width""])    File ""samples/coco/coco.py"", line 343, in annToMask      rle = self.annToRLE(ann, height, width)    File ""samples/coco/coco.py"", line 321, in annToRLE      rles = maskUtils.frPyObjects(segm, height, width)    File ""pycocotools/_mask.pyx"", line 292, in pycocotools._mask.frPyObjects  IndexError: list index out of range  sdfsdfsdfsdf  isinstance(segm, list)  ERROR:root:Error processing image {'id': 5819, 'source': 'coco', 'path': '/content/Mask_RCNN/dataset/coco//train2017/5819.jpg', 'width': 825, 'height': 1075, 'annotations': [{'area': 141255, 'iscrowd': 0, 'image_id': 5819, 'bbox': [84, 259, 657, 215], 'category_id': 1, 'id': 10957, 'ignore': 0, 'segmentation': []}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""samples/coco/coco.py"", line 273, in load_mask      image_info[""width""])    File ""samples/coco/coco.py"", line 343, in annToMask      rle = self.annToRLE(ann, height, width)    File ""samples/coco/coco.py"", line 321, in annToRLE      rles = maskUtils.frPyObjects(segm, height, width)    File ""pycocotools/_mask.pyx"", line 292, in pycocotools._mask.frPyObjects  IndexError: list index out of range  sdfsdfsdfsdf  isinstance(segm, list)  ERROR:root:Error processing image {'id': 7571, 'source': 'coco', 'path': '/content/Mask_RCNN/dataset/coco//train2017/7571.jpg', 'width': 827, 'height': 1063, 'annotations': [{'area': 54496, 'iscrowd': 0, 'image_id': 7571, 'bbox': [235, 189, 208, 262], 'category_id': 3, 'id': 10756, 'ignore': 0, 'segmentation': []}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""samples/coco/coco.py"", line 273, in load_mask      image_info[""width""])    File ""samples/coco/coco.py"", line 343, in annToMask      rle = self.annToRLE(ann, height, width)    File ""samples/coco/coco.py"", line 321, in annToRLE      rles = maskUtils.frPyObjects(segm, height, width)    File ""pycocotools/_mask.pyx"", line 292, in pycocotools._mask.frPyObjects  IndexError: list index out of range  sdfsdfsdfsdf  isinstance(segm, list)  ERROR:root:Error processing image {'id': 8432, 'source': 'coco', 'path': '/content/Mask_RCNN/dataset/coco//train2017/8432.jpg', 'width': 906, 'height': 1170, 'annotations': [{'area': 13824, 'iscrowd': 0, 'image_id': 8432, 'bbox': [127, 857, 216, 64], 'category_id': 4, 'id': 13861, 'ignore': 0, 'segmentation': []}, {'area': 168399, 'iscrowd': 0, 'image_id': 8432, 'bbox': [445, 236, 297, 567], 'category_id': 1, 'id': 13862, 'ignore': 0, 'segmentation': []}, {'area': 14307, 'iscrowd': 0, 'image_id': 8432, 'bbox': [449, 865, 251, 57], 'category_id': 4, 'id': 13863, 'ignore': 0, 'segmentation': []}, {'area': 6660, 'iscrowd': 0, 'image_id': 8432, 'bbox': [127, 1002, 74, 90], 'category_id': 4, 'id': 13864, 'ignore': 0, 'segmentation': []}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""samples/coco/coco.py"", line 273, in load_mask      image_info[""width""])    File ""samples/coco/coco.py"", line 343, in annToMask      rle = self.annToRLE(ann, height, width)    File ""samples/coco/coco.py"", line 321, in annToRLE      rles = maskUtils.frPyObjects(segm, height, width)    File ""pycocotools/_mask.pyx"", line 292, in pycocotools._mask.frPyObjects  IndexError: list index out of range  sdfsdfsdfsdf  isinstance(segm, list)  ERROR:root:Error processing image {'id': 4528, 'source': 'coco', 'path': '/content/Mask_RCNN/dataset/coco//train2017/4528.jpg', 'width': 825, 'height': 1075, 'annotations': [{'area': 422676, 'iscrowd': 0, 'image_id': 4528, 'bbox': [58, 60, 708, 597], 'category_id': 1, 'id': 6253, 'ignore': 0, 'segmentation': []}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""samples/coco/coco.py"", line 273, in load_mask      image_info[""width""])    File ""samples/coco/coco.py"", line 343, in annToMask      rle = self.annToRLE(ann, height, width)    File ""samples/coco/coco.py"", line 321, in annToRLE      rles = maskUtils.frPyObjects(segm, height, width)    File ""pycocotools/_mask.pyx"", line 292, in pycocotools._mask.frPyObjects  IndexError: list index out of range  sdfsdfsdfsdf  isinstance(segm, list)  ERROR:root:Error processing image {'id': 1084, 'source': 'coco', 'path': '/content/Mask_RCNN/dataset/coco//train2017/1084.jpg', 'width': 850, 'height': 1100, 'annotations': [{'area': 194208, 'iscrowd': 0, 'image_id': 1084, 'bbox': [18, 196, 816, 238], 'category_id': 1, 'id': 4828, 'ignore': 0, 'segmentation': []}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""samples/coco/coco.py"", line 273, in load_mask      image_info[""width""])    File ""samples/coco/coco.py"", line 343, in annToMask      rle = self.annToRLE(ann, height, width)    File ""samples/coco/coco.py"", line 321, in annToRLE      rles = maskUtils.frPyObjects(segm, height, width)    File ""pycocotools/_mask.pyx"", line 292, in pycocotools._mask.frPyObjects  IndexError: list index out of range  Traceback (most recent call last):    File ""samples/coco/coco.py"", line 576, in        augmentation=augmentation)    File ""/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 2375, in train    File ""/usr/local/lib/python3.7/dist-packages/keras/legacy/interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""/usr/local/lib/python3.7/dist-packages/keras/engine/training.py"", line 1732, in fit_generator      initial_epoch=initial_epoch)    File ""/usr/local/lib/python3.7/dist-packages/keras/engine/training_generator.py"", line 185, in fit_generator      generator_output = next(output_generator)    File ""/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 1709, in data_generator    File ""/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 1212, in load_image_gt    File ""samples/coco/coco.py"", line 273, in load_mask      image_info[""width""])    File ""samples/coco/coco.py"", line 343, in annToMask      rle = self.annToRLE(ann, height, width)    File ""samples/coco/coco.py"", line 321, in annToRLE      rles = maskUtils.frPyObjects(segm, height, width)    File ""pycocotools/_mask.pyx"", line 292, in pycocotools._mask.frPyObjects  IndexError: list index out of range    `    I think this caused by the lack of segmentation annotation, please anyone help me solve this problem !  Are there any tool that can create segmentation annotation from bbox? my dataset is large so that I dont want to re-annotate it ðŸ˜­ðŸ˜­    thank you all !    "
"Hello    I am learning TensorFlow and use your code (thank you for the very nice implementation).   However, I don't understand the model. It returns the losses as one type of output besides other outputs (lines 2026 - 2030).   However, in the training dataset that is generated no respective y_pred are (lines 1789 - 1801) provided to compare these outputs to.     I have looked throughout the code but cannot identify any region where this might be added. May I ask to point me towards some information about how this is handled by tensorflow    Thank you in advance  Best regards,  christoph    model.py     "
is the network able to perform accurate segmentation if it is provided with ONLY coloured depth map images (as shown below) instead of regular rgb images as input? I will obviously train the network with annotated images before carrying out the detections.    Has anyone ever tried this?     In the image below i want it to detect the piperail.  !   
"Hi, I a training Mask RCNN for a custom dataset. The training goes well but during inference, I notice that an object sometimes has 2 masks overlapped. Actually, there should be only one mask per object but sometimes I get 2 maks (1 smaller and 1 correctly sized mask as per the object size). Should I have to change any hyperparameters. I am using the default hyperparameters(config.py) as per the balloon example. Help is much appreciated. Thank you very much."
"I am trying to plot the following cluster but keep getting the error below:  please help....  IndexError: boolean index did not match indexed array along dimension 0; dimension is 4334 but corresponding boolean dimension is 4363    **Code**  from sklearn.cluster import KMeans # K-means algorithm  plt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 100, c = 'Orange', label = 'Cluster 1')   plt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')   plt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')   plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 200, c = 'yellow', label = 'Centroids')   plt.title('Clusters of Customers ')   plt.xlabel('Quantity')   plt.ylabel('Total Revenue')   plt.legend()   plt.show()"
"As seen in the images below, he can hardly detect the edges of the pictures. Since I turned all sides of the image while I was training, the accuracy is pretty low. how can i fix this.  Do you have any advice for improving accuracy?    Very important for my thesis please help  thanks in advance    <img width=""353"" alt=""Ekran AlÄ±ntÄ±saaaÄ±"" src=""   <img width=""347"" alt=""Ekran AlÄ±ntÄ±sÄ±8"" src=""   "
Hello everyone! I'm trying to classify satellite images. I have 5 classes in total. My mAP value is like 0.5. My loss value is not going below 0.7.  Do you have any suggestions to better them and get more accurate results?    please help   thanks. 
"Im new to Deep Learning and Mask R-CNN, trying to train and test my own dataset with matterport's code but i have the following result. Instead of getting one mask, i get many small. Also my result isnt accurate. Train's dataset size is 560 crack images.  Any thoughts? Thank you!  !   !   "
None
"Hi everyone,    In model, it is called     for the anchor generation. In utils.generate_pyramid_anchors, we can find this line     meaning that we apply the scales[i] size to generate anchor on the feature map of size feature_shapes[i]. We have on one hand  `config.RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)`  and   `backbone_shapes = compute_backbone_shapes(self.config, image_shape)`  on the other hand, that is in fact the array containing the shapes of the feature maps, the biggest being first. To be clear, we have for an input image of 1024x1024x1  `backbone_shapes = np.array( [256,256], [128,128], [64,64], [32,32], [16,16] )`    It means that we apply the smallest anchor size to the biggest feature maps (may be ok) and the biggest anchor sizes on the smallest feature maps. My question is why? I would have considered to apply the smallest anchor sizes to the smallest features maps? Am I missing something here?    Thank you in advance."
"So the ground truth annotations that I have are in the form of bounding boxes and not polygon masks. As a result, the predictions mostly just mimic the shape of the bounding box. I am trying to detect traces of agricultural fields from LIDAR data, and instead of ""masking"" each discrete instance, there are many overlaps. Also the shape is incorrect since it just forms a rectangular box like the bounding box Does anyone have a solution to this, aside from re-annotating the entire dataset with masks?"
"We are working on a project, vision controlled robot for pick and place partial occluded objects. We are using mask RCNN architecture. Until now, we are successful in producing masks, bounding boxes and getting the location of bboxes. Now to handle occlusion effectively we need the coordinates of the masks, so that we can identify overlapped area. But it is not working. Here is a code snap and result it producing.        import skimage      import matplotlib.pyplot as plt            real_test_dir = '/content/drive/MyDrive/MaskRCNN_occ/dataset/updated data 2/predict'      image_paths = []      for filename in os.listdir(real_test_dir):          if os.path.splitext(filename)[1].lower() in ['.png', '.jpg', '.jpeg']:              image_paths.append(os.path.join(real_test_dir, filename))            for image_path in image_paths:          img = skimage.io.imread(image_path)                    if img.ndim != 3:            image = skimage.color.gray2rgb(img)          if img.shape[-1] == 4:            image = img[..., :3]                print(image_path)          img_arr = np.array(image)          results = model.detect([img_arr], verbose=1)          r = results[0]          print(r['masks'])    And it is producing these results,        /content/drive/MyDrive/MaskRCNN_occ/dataset/updated data 2/predict/Image 3.png      Processing 1 images      image                    shape: (256, 256, 3)         min:    5.00000  max:  255.00000  uint8      molded_images            shape: (1, 512, 512, 3)      min: -117.70000  max:  151.10000  float64      image_metas              shape: (1, 18)               min:    0.00000  max:  512.00000  float64      anchors                  shape: (1, 65472, 4)         min:   -0.17712  max:    1.05188  float32      [[[False False False False False]        [False False False False False]        [False False False False False]        ...        [False False False False False]        [False False False False False]        [False False False False False]]]"
"Hi,    I've managed to train a model with tensorflow 2.x using this wonderful repo:       However I've noticed that when doing multistep training, the accumulated loss is not equal to the sum of the losses when changing the learning rate (i.e. loading the model again for a smaller learning rate). It is actually equal to the sum of the losses by a factor of two.     It seems that in most of tf2 repos the lines in model.py are removed:     This is because these are private variables. However they are supposed to clear the previous losses (set during previous training step). Hence in tf2 implementations the losses are accumulated twice so summed loss is x2    I'm trying to find a way to reset the losses when performing the second training step  Any help would be appreciated     Thanks !"
"Hi guys!    I've been looking for a long time to find the correct way to calculate the F1-Score using the lib Mask-RCNN. I created several issues  ,  ,  ,  , studied for a long time and I believe I found the right form. Before presenting the code used, let's go to the settings I used.    > **mAP = mean Average Precision**    > **mAR = mean Average Recall**    > **f1-score = 2 * (((mAP * mAR) / (mAP + mAR))**    # Calculating mean Average Precision (mAP)    To calculate the mAP, I used the **_compute_ap_** function available in the   module. For each image I call the **_compute_ap_** function, which returns the **Average Recall (AR)** and adds it to a list. After going through all the images, I average the Average Recalls.         Where the parameters:  - **dataset**, is an object of a class that inherits from the Dataset class in  ;  - **model** is an object resulting from the MaskRCNN class available in the module  ;   - **cfg** is an object of a class that inherits the super class        # Calculating mean Average Recall (mAR)    To calculate the mAR I used the post   as a mathematical basis.    The calculation of the mAR is similar to the mAP, except that instead of analyzing precision vs recall, we analyze the recall behavior using different iou thresholds. In the post Average Recall it is defined as:      > AR is the recall averaged over all IoU âˆˆ      In the code what we need to do is create a function that calculates the Average Recall, and then we follow with the approach similar to mAP, we will go through each of the images, calculate their Average Recall, add it to a list and at the end we make an average and we find the mAR.         Basically, we are calling the **_compute_recall_** function of the   module for each of the thresholds that we define in the formula.    Where,     **pred_boxes**: Are the coordinates of the expected bounding box;     **gt_boxes**: Are the coordinates of the actual bounding box;     **list_iou_thresholds**: List of thresholds that will be used.    Now let's add **mAR** to our **evaluate_model** function.           # Calculating F1-Score    Now that we know our **mAP** and **mAR**, just apply the **f1-score** formula. Let's add the f1-score formula to our **evaluate_model** function.         This was the way I found to calculate mAP, mAR and f1-score, what did you think? I believe that I am on the right path, I am not an expert in the area and I had a lot of difficulty in reaching this result, I accept any type of feedback. I hope to contribute in some way!"
"<img width=""337"" alt=""WechatIMG36634"" src=""     Why is that? What should I do? The source is ballon.py.  Thanks!!!!!"
None
I have a training dataset of only around 147 greyscale images. Are there any suggestions for model training or configurations I should use?
"So I am using Mask RCNN for a thesis project, and the dataset was provided to me with annotation as it had already been used as part of a different project. However the original project used Faster RCNN and so the annotations were done accordingly. Would it affect the Mask RCNN training to use those annotations?    @waleedka "
"I train the model of my own dataset, I tried training with single GPUs first, and then, I wanted to accelerate the process, I trained it with 4 GPUs, however, it seems the training time didn't reduce, and sometimes even slower than training with single GPUs, so why did this happen?   My Environment:  Tensorflow-gpu 1.12.0  Keras 2.1.3  Cuda 9.0    I added the following code in the front of my script:  os.environ[""CUDA_VISIBLE_DEVICES""] = ""4,5,6,7""  and change the GPU_COUNT=4  "
i cannot change the parameters of the config such as  'step per epoch' 
"Hi.  I use TF 2.4, Cuda 11, keras 2.4.3  In my experiments with MRCNN with multi-step training, I found something interesting.  When you change the number of training layers (3+, 4+, heads, etc.), you are using more or less gpu memory.  In the classification task, I start training the last layers with a larger batch size, then more layers - a smaller batch size.  But the MRCNN  captures the batch size for one training process using the batch size in the first element of shape in layer ""anchors"".  How can I get around batch size limitation in weights? "
"Hello, I've recently switched computers to be able to run the Resnet101 part of the Mask RCNN code. The problem I've been facing is that the program keeps using the CPU instead of the GPU.     **Current Computer Setup**:   - NVIDIA RTX 3090 24GB GPU  - AMD Ryzen Threadripper 3960X Processor  - Windows Pro operating system  - CUDA 11.1/ 8.1 cudnn    **What I have tried**:  - I have tried installing CUDA 11.0, cudnn 8.0.4 and upgrading to tensorflow-gpu 1.15. The code runs but since it canâ€™t open files from CUDA 10 it runs on CPU instead. This takes hours to finish running  - There was supposedly an updated version of the code ( leekunhee/Mask_RCNN: Mask R-CNN modified to run on TensorFlow 2 (github.com)). Another github issue tab mentioned using tensorflow 2.3.0, keras 2.4.3, cuda 11.1 cudnn 8.1. I installed everything as instructed but the same thing happens where its asking for CUDA 10 files. I took the older dll CUDA 10 files and put them into the CUDA 11.1 and CUDA 11 bin folder but the program keeps asking for files and eventually asks for files that donâ€™t exist in the CUDA 10 folder    I've looked at plenty of help pages and issues others have posted in this github but haven't been able to find a solution that works. All I been trying to do is find a solution that works with the RTX3090 so I can use the GPU. Any advice would be greatly appreciated.     Thank you for your time."
"In the hyperparameters, the standard settings for input image resizing are as below:      `IMAGE_RESIZE_MODE = ""square""`    `IMAGE_MIN_DIM = 800`    `IMAGE_MAX_DIM = 1024`    I have input images that are 6080x3420, so to my understanding, these are resized to 1024x1024 and padded with zeroes to make a square image. Does this happen both in training and when predicting with the trained model?    I ask because I have a model trained on the 6080x3420 images with the above standard settings, but I have noticed that downscaling the test images before predicting has an influence on prediction accuracy. Effectively, the prediction accuracy is highest when downscaling the test images to 12.5% of the original size before running the model on them.  "
"I tried to use these code to perform it on my own custom datasets. However, I could not get the python file to read the JSON file which was annotated usng VGG image annotator v2.0. Are there any solutions to help on this?    I keep facing ""region"" or dictionary/list issues. "
"here is the error i got it .         Traceback (most recent call last):    File ""C:\Users\åˆ˜\Desktop\yolov3-coco-tower\yolo_custom_detection\yolo_object_detection.py"", line 68, in        label = str(classes[class_ids[i]])  IndexError: list index out of range    Process finished with exit code 1    Here is the code.          import cv2  import numpy as np  import glob  import random      # Load Yolo  net = cv2.dnn.readNet(""custom-yolov4-detector_best.weights"", ""yolov4.cfg"")    # Name custom object  classes = [""tower""]    # Images path  images_path = glob.glob(r""mix\*.jpg"")        layer_names = net.getLayerNames()  output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]  colors = np.random.uniform(0, 255, size=(len(classes), 3))    # Insert here the path of your images  random.shuffle(images_path)  # loop through all the images  for img_path in images_path:      # Loading image      img = cv2.imread(img_path)      img = cv2.resize(img, None, fx=0.4, fy=0.4)      height, width, channels = img.shape        # Detecting objects      blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)        net.setInput(blob)      outs = net.forward(output_layers)        # Showing informations on the screen      class_ids = []      confidences = []      boxes = []      for out in outs:          for detection in out:              scores = detection[5:]              class_id = np.argmax(scores)              confidence = scores[class_id]              if confidence > 0.3:                  # Object detected                  print(class_id)                  center_x = int(detection[0] * width)                  center_y = int(detection[1] * height)                  w = int(detection[2] * width)                  h = int(detection[3] * height)                    # Rectangle coordinates                  x = int(center_x - w / 2)                  y = int(center_y - h / 2)                    boxes.append([x, y, w, h])                  confidences.append(float(confidence))                  class_ids.append(class_id)        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)      print(indexes)      font = cv2.FONT_HERSHEY_PLAIN      for i in range(len(boxes)):          if i in indexes:              x, y, w, h = boxes[i]              label = str(classes[class_ids[i]])              color = colors[class_ids[i]]              cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)              cv2.putText(img, label, (x, y + 30), font, 3, color, 2)          cv2.imshow(""Image"", img)      key = cv2.waitKey(0)    cv2.destroyAllWindows()"
"In the model.py if config.USE_MINI_MASK:  y1 = (y1 - gt_y1) / gt_h... then masks = tf.image.crop_and_resize(),  But how can we make sure the predict_box coordinate must be greater than gt_box coordinate ?  if the boxes < 0 in args of  tf.image.crop_and_resize , is it valid ?"
"Greetings,  Can anyone  recommend a tool to generate synthetic microscopy images."
"I want to do changes in DNN architecture in order to increase the FPS    I am using resnet50 as a backbone, I read this paper and tried to understand what are the changes that they did in their architecture.         Someone knows about way to increase the FPS of the net from 5 .    My GPU is Titan RTX    Thank you"
"I've downloaded this repo in order to make a few experiments, and I ran into a ton of trouble just to get the example notebook up and running. Not being exactly an expert in the technologies surrounding machine learning and libs such as Tensorflow, it took a long time and a lot of google fu to figure out I had to downgrade some dependencies in order for this to work.    I can make a PR freezing the versions for the dependencies I listed in the title, I just want to know if that's something that might be of interest before I do it."
"After using the script ""tf_upgrade_v2"" that converts the code to the new version of the tensorflow and after applying the corrections from the   and  , the program runs ok. But after save model and load it with   I get the error:       Maybe the reason is that the MaskRCNN often uses KL.Lambda that should not be used when we want to save a model to file:       @leekunhee  @tomgross    More info:     "
Thanks! 
"Hi,    did some already implement resnext-101 backbone and could share his/her code?    if yes , I want to ask you implement resnext-101 . can you share it or email me( ji3cp4su3ok@icloud.com ), if you are convient. thank you!    Thanks in advance!"
I have a dataset with negative examples/zero instances which I want to keep as part of the traiining process. How should the masks be loaded for these cases with the load_mask() function.
"1. How should the load_image() function be overwritten? I seem to be generating errors when doing the same  2. Training the weights - model.load_weights('C:\\Users\\User\\PycharmProjects\\Test2\\mask_rcnn_coco.h5',                     by_name=True,                     exclude=[""conv1""])  Is this how the pretrained weights should be loaded for training for grayscale"
"I annotated my own dataset using VGG annotator and exported the annotations as json.  When training, I am getting this error:  !   while this image is not even in the training directory, but rather in the val directory:  !   Any insights are appreciated. Thanks!    "
"I want to train on my own datasets and I have about 2200 training image (512X512) and 900 validation image (512X512).  My GPU is GeForce GTX 1070.     batch_size=1    When I set the `STEP_PER_EPOCH = 200`, one epoch takes about 13 minutes.   But when I set `STEP_PER_EPOCH= 500`, one epoch takes almost an hour to complete.  I suppose that when doubling **STEP_PER_EPOCH** the training time should also be double however it doesn't.  Does anyone meet the same problem?    Also, I want to ask that when I set `STEP_PER_EPOCH = 200`, does it mean that it will take about 11 epochs to go through all the image in my training data?"
"i want to show output's background to black or white or gray. (only one color not grayscale)  it is seemed to fix ""color_splash()"" in ""balloon.py"".  how to do it?     "
"# Code   image_id = random.choice(dataset_val.image_ids)  original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\      modellib.load_image_gt(dataset_val, inference_config,                              image_id)    log(""original_image"", original_image)  log(""image_meta"", image_meta)  log(""gt_class_id"", gt_class_id)  log(""gt_bbox"", gt_bbox)  log(""gt_mask"", gt_mask)    visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id,                               dataset_train.class_names, figsize=(8, 8))"
"Hello,  I try to use Mask_RCNN and i have this error.  Someone can help me ?    `ValueError                                Traceback (most recent call last)    in  ()        6             learning_rate=config.LEARNING_RATE,        7             epochs=1,  ----> 8             layers='heads')    17 frames  /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py in train(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)     2372             max_queue_size=100,     2373             workers=workers,  -> 2374             use_multiprocessing=True,     2375         )     2376         self.epoch = max(self.epoch, epochs)    /usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py in wrapper(*args, **kwargs)       85                 warnings.warn('Update your `' + object_name +       86                               '` call to the Keras 2 API: ' + signature, stacklevel=2)  ---> 87             return func(*args, **kwargs)       88         wrapper._original_function = func       89         return wrapper    /usr/local/lib/python3.6/dist-packages/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)     1976      1977         do_validation = bool(validation_data)  -> 1978         self._make_train_function()     1979         if do_validation:     1980             self._make_test_function()    /usr/local/lib/python3.6/dist-packages/keras/engine/training.py in _make_train_function(self)      988                     training_updates = self.optimizer.get_updates(      989                         params=self._collected_trainable_weights,  --> 990                         loss=self.total_loss)      991                 updates = self.updates + training_updates      992                 # Gets loss and metrics. Updates weights at each call.    /tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in get_updates(self, loss, params)      502         if g is not None and v.dtype != dtypes.resource      503     ])  --> 504     return [self.apply_gradients(grads_and_vars)]      505       506   def _set_hyper(self, name, value):    /tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in apply_gradients(self, grads_and_vars, name)      431         _ = self.iterations      432         self._create_hypers()  --> 433         self._create_slots(var_list)      434       435       apply_state = self._prepare(var_list)    /tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/optimizer_v2/gradient_descent.py in _create_slots(self, var_list)       98     if self._momentum:       99       for var in var_list:  --> 100         self.add_slot(var, ""momentum"")      101       102   def _prepare_local(self, var_device, var_dtype, apply_state):    /tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in add_slot(self, var, slot_name, initializer)      583             dtype=var.dtype,      584             trainable=False,  --> 585             initial_value=initial_value)      586       backend.track_variable(weight)      587       slot_dict[slot_name] = weight    /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/variables.py in __call__(cls, *args, **kwargs)      258       return cls._variable_v1_call(*args, **kwargs)      259     elif cls is Variable:  --> 260       return cls._variable_v2_call(*args, **kwargs)      261     else:      262       return super(VariableMetaclass, cls).__call__(*args, **kwargs)    /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/variables.py in _variable_v2_call(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)      252         synchronization=synchronization,      253         aggregation=aggregation,  --> 254         shape=shape)      255       256   def __call__(cls, *args, **kwargs):    /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/variables.py in  (**kws)      233                         shape=None):      234     """"""Call on Variable class. Useful to force the signature.""""""  --> 235     previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)      236     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access      237       previous_getter = _make_getter(getter, previous_getter)    /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/variable_scope.py in default_variable_creator_v2(next_creator, **kwargs)     2550       synchronization=synchronization,     2551       aggregation=aggregation,  -> 2552       shape=shape)     2553      2554     /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/variables.py in __call__(cls, *args, **kwargs)      260       return cls._variable_v2_call(*args, **kwargs)      261     else:  --> 262       return super(VariableMetaclass, cls).__call__(*args, **kwargs)      263       264     /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)     1404           aggregation=aggregation,     1405           shape=shape,  -> 1406           distribute_strategy=distribute_strategy)     1407      1408   def _init_from_args(self,    /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)     1536             initial_value = ops.convert_to_tensor(     1537                 initial_value() if init_from_fn else initial_value,  -> 1538                 name=""initial_value"", dtype=dtype)     1539           if shape is not None:     1540             if not initial_value.shape.is_compatible_with(shape):    /tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype, dtype_hint)     1182   preferred_dtype = deprecation.deprecated_argument_lookup(     1183       ""dtype_hint"", dtype_hint, ""preferred_dtype"", preferred_dtype)  -> 1184   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)     1185      1186     /tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)     1240       name=name,     1241       preferred_dtype=dtype_hint,  -> 1242       as_ref=False)     1243      1244     /tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accepted_result_types)     1271       raise ValueError(     1272           ""Tensor conversion requested dtype %s for Tensor with dtype %s: %r"" %  -> 1273           (dtype.name, value.dtype.name, value))     1274     return value     1275   `"
None
!   
"I am trying to use class weights for my mrcnn project. I tried the following steps but still no luck.    1) I calculated class_weights and stored them as a dictionary.    2) I added ""class_weight""=None parameter in the ""train"" function under model.py. I also added class_weight= class_weights under ""keras.fit"" function in the model.py itself.    3) I passed the ""class_weight"" argument while training through model.train(.......) in training script.    4)It looks like model is accepting the parameter but throws an error saying that my output_name dimension doesn't match with provided elements dimension in class weight.    I checked the source code and I found that the error is generated if len(class_weight_!= len(output_names).  Also, it looks like my output_names is returned null. I don't why though. I have successfully run the model without class weights; never had the issue with output_names.    Appreciate if someone could suggest or direct me towards a solution.  "
"Hello , I need to detect lines or curves in an image . I have annotated polylines using polyline tool in labelbox annotation tool. I trained model using mask rcnn and it was able to predict bounding box and class labels but not the line .can someone help me on this??"
None
I am training a dataset of 580 images and validating the model on 50 images.  Earlier i ran  the training i found all losses were decreasing but dont know if i have changed any config setting or something else. Now I am seeing a weird behaviour in Val_rpn_class_loss and Val_rpn_bbox_loss. its continuosly increasing causing total Val_loss to increase. Have anybody came across similar situation? Please help with your suggestion    !     
"Hello community from Mask_RCNN I'm trying to get this indicators ( mAP, mAR and F1_score) usign utils.compute_ap(..)  here is my code:     Then I use the function:     I get this code from here:   but when I run this code I'm getting some errors when execute this line of code:  `mAP, mAR, F1_score = evaluate_model(dataset_val, model, inference_config)  `     I'm loading my validation dataset of this way:     My validation images has a file called via_region_data.json which is the ground truth of these images..   I have trained my model without problems but now when I'm trying to get the mAP, mAR, F1_score with the code of above  I'm getting this error:    `ValueError: shapes (5,262144) and (3136,5) not aligned: 262144 (dim 1) != 3136 (dim 0)`    The same happen when I try to run this code to get the mAP    why can happens it? what should I do? any recommendation? thanks in advance."
"Dear All,    Any advice with regards to this? I have been trying to get the demo up on jupyter notebook using my macbook.    TypeError                                 Traceback (most recent call last)    in         14 sys.path.append(ROOT_DIR)  # To find local version of the library       15 from mrcnn import utils  ---> 16 import mrcnn.model as modellib       17 from mrcnn import visualize       18 # Import COCO config    ~/Documents/GitHub/matterport_Mask_RCNN/mrcnn/model.py in         28 # Requires TensorFlow 1.3+ and Keras 2.0.8+.       29 from distutils.version import LooseVersion  ---> 30 assert LooseVersion(tf.version) >= LooseVersion(""1.3"")       31 assert LooseVersion(keras.version) >= LooseVersion('2.0.8')       32     ~/opt/miniconda3/envs/tensorflow/lib/python3.8/distutils/version.py in __init__(self, vstring)      302     def __init__ (self, vstring=None):      303         if vstring:  --> 304             self.parse(vstring)      305       306     ~/opt/miniconda3/envs/tensorflow/lib/python3.8/distutils/version.py in parse(self, vstring)      310         # use by __str__      311         self.vstring = vstring  --> 312         components = [x for x in self.component_re.split(vstring)      313                               if x and x != '.']      314         for i, obj in enumerate(components):    TypeError: expected string or bytes-like object"
None
"The bounding box and class prediction are okay but the masks are not. However, masks for the small objects are okay compared to large objects. The story is similar for other images as well. Here's my configurations:    RPN_ANCHOR_SCALES = (16, 32, 64, 128, 256)    TRAIN_ROIS_PER_IMAGE = 64    MAX_GT_INSTANCES = 50    POST_NMS_ROIS_INFERENCE = 500    POST_NMS_ROIS_TRAINING = 1000    USE_MINI_MASK True    MASK_SHAPE    "
"Who can tell me,what is happed and how to sloved it."
"HI all,     I hope you are all coping well with these difficult times.    I have my data and their masks to feed maskrcnn.  I tried the nucleus code , their masks are separated in png files, but mine are not , they are all together in one png file for each image they are all the same class.   I really help modifying their code to suit my needs   please help me please      def load_mask(self, image_id):          """"""Generate instance masks for an image.         Returns:          masks: A bool array of shape [height, width, instance count] with              one mask per instance.          class_ids: a 1D array of class IDs of the instance masks.          """"""          info = self.image_info[image_id]          # Get mask directory from image path          mask_dir = os.path.join(os.path.dirname(os.path.dirname(info['path'])), ""masks"")            # Read mask files from .png image          mask = []          for f in next(os.walk(mask_dir))[2]:              if f.endswith("".png""):                  m = skimage.io.imread(os.path.join(mask_dir, f),as_gray=True).astype(np.bool)          mask = np.stack(mask, axis=-1)          # Return mask, and array of class IDs of each instance. Since we have          # one class ID, we return an array of ones          return mask, np.ones([mask.shape[-1]], dtype=np.int32)              Thank you     "
"I am using mask rcnn model to detect the Foods Samples. I was able to retrieve the bounding boxes and masks, but how to get the coordinates of the two ends and the center of the masks? I want to grasp food samples, so the coordinates of the center and both ends are necessary.     "
"Is there any modifications we could do to only allow the person class to be detected in an image (not chair, car, bike etc.)?  "
"we have trained weight(.h5) file. training works well :)    I checked that it works well in tf1.5 and cuda9.0.   But we want to test without cuda9.0. Is this possible?  If possible, which part of the balloon.py file do I have to modify?"
"Hello Everyone,   I would like to run my code on GPU instead of CPU anyone can help me please?"
"Hi    I am new to ML and Mask RCNN    I am using the coco weights to train the model on segmenting different types of handbag. I have seen   using which I can train only subset of COCO (eg. Mask RCNN would be trained to further segment only handbag) however, I want to not only train on the subset weight of handbag but also classify it as leather or others.     **_Hence, is it possible to train only on the subset of handbag of COCO weight on my training set [leather or other] ?_**  As training on the whole COCO weights are causing issues. If not, any other way/technique that can help?    Thank You"
"Hi all,    I am suffering a problem, in order to acccelate the inference time, I expect to use one gpu card to predict multi images simultaneously?    kind regards"
I would like to create a flood level estimator that predicts the level each instance is submerged in. I have annotated the levels along with the other required annotations required in json file. The model predicts the classes and masks accurately but I'm stuck in where to modify to help it train and learn the level parameters also.  I would like to understand what is returned by the keras_model.predict function in detect function defined in model.py  !   
"What is the input size of the backbone? In this implementation, resnet is used which takes the input of size 224Ã—224Ã—3, while in the article ""   it is mentioned that backbone converts input image of size 1024Ã—1024Ã—3 into a feature map. Kindly help me with that."
"I have install tensorflow2.2.0, it has already include keras, so I don't install keras separately. And how can I use this repo under tensorflow>2.0. Thanks a lot."
I got this error when I try to run this command:            This happens in `mrcnn/model.py:1875`
"I have a dataset ranging in various dimensions 800x450 to 1920x1080 and I was wondering given that the default parameter sets IMAGE_MIN_DIM=800 and IMAGE_MAX_DIM=1024, what happens if I input an image with dimensions larger than that? Would the results and training mess up since the labelled coordinates and boundary boxes will be outside? (For example, it would clip the original image?) This is because I'm trying to all the layers but with IMAGE_MAX_DIM=1920, I run out of GPU memory even though I'm running on three RTX2080TI."
!     I was using the latest version of python and pip but I still had this error. So I downgraded to   Python Version : 3.8.7 and pip version 20.3.3    It still has the same problem.    Should I be changing the code? I'm very new to this so I dont know what to do. At the very least I'm just trying to run this to see what it can do.
"So I am trying to train MaskRCNN on my custom dataset of floorplans. I have used the MaskRCNN-TF2 repository (  and have used the following blog as a guide:       I am trying to train on my custom dataset, but I get the following error regarding the file _model.py_;       I am trying to run the code in an anaconda environment (called _mask_) with the following packages installed:       Does anybody know why this error appears? As it is clear I have installed tensorflow in my environment. "
"Hi everyone,    I have trained a custom model which includes about 12 classes. Input image sizes were 1400x1400 JPG format. In inference mode on my windows pc with GTX 1050ti (i am using tensorflow-gpu 1.13.1 with CUDA 10.0) it takes 1.5-2 seconds to return results (without using the visualize to draw masks; returns coordinates and thats it). So far its all good for me. Couple days ago i have tested my model on NVIDIA JETSON AGX XAVIER (tensorflow 2.3.1 and CUDA 10.2) and it took about 1.4-1.5 seconds to return results (no visualize again). I have tested 1 image at a time on both devices. Isn't it weird XAVIER takes about the same time as my weak 1050ti. Does anyone have an idea about what is the issue or is it normal?"
"I'm trying to visualize and save my results but somehow I'm just saving blank png files to my drive. I tried several things of other issues but couldn't make it work. Does anyone has an idea why this could happen?    Saving the visualized results as png        # Load over images      submission = []      for image_id in dataset.image_ids:          # Load image and run detection          image = dataset.load_image(image_id)          # Detect objects          r = model.detect([image], verbose=0)[0]          # Encode image to RLE. Returns a string of multiple lines          source_id = dataset.image_info[image_id][""id""]          rle = mask_to_rle(source_id, r[""masks""], r[""scores""])          submission.append(rle)          # Save image with masks          visualize.display_instances(              image, r['rois'], r['masks'], r['class_ids'],              dataset.class_names, r['scores'],              show_bbox=True, show_mask=True,              title=""Predictions"")                    plt.savefig(""{}/{}.png"".format(submit_dir, dataset.image_info[image_id][""id""]),                      bbox_inches='tight', pad_inches=-0.5,orientation= 'landscape')          plt.close()"
"Hello, I want to apply data augmentation techniques on my training set such as rotation, flipping during training. So, how to do it? where should I do modifications in the code? "
"I am trying to train a simple Mask R-CNN on a 2 image training set (and 1 image validation set). However, I have over 300 objects per image, so in actuality, there's are approximately 1000 objects in the dataset that I am working with. When I begin training, I encounter the following issue:    Epoch 1/1  tcmalloc: large alloc...  tcmalloc: large alloc...  tcmalloc: large alloc...  tcmalloc: large alloc...  ...    I left for approx. 30 minutes and these messages of ""tcmalloc: large alloc"" continued. Any suggestions as to how to fix this? I'm running the Mask R-CNN on Google Colab and the RAM usage just shoots up to its maximum when training begins. My classification model currently consists of 3 classes (4 classees including the background). Thanks in advance!  "
I want to access the quality of convergence after each epoch or after every 5 epoch.   It helps me to correct/debug the Architecture immediately after finding out convergence quality is not optimal.  I don't want to wait for 5 days then find out validation is not satisfactory.  It will waste the time.    How should i run validation after every 5 epoch?  Could you suggest me?
"I trying to use new Backbone with Mask_rcnn code base instead of Resnet 101.  I am defining ""layer_regex"" for new Backbone for training.    I have included only Conv2D, BatchNormalization and Activation in ""layer_regex"" for training. I have not included Pooling layer, Lambda layer and Concatenate layers in ""layer_regex"" for training.     Should i include Pooling layer, Lambda layer and Concatenate layers in the layer_regex ? if not included, what will be the impact in training?"
"I have used          2 GPUs - Tesla K80      2 IMAGES PER GPU     COCO dataset     STEPS PER EPOCH : 1000       During ""heads"" layer training,  I have got mrcnn_class_loss below zero.  Also mrcnn_bbox_loss and mrcnn_mask_loss is zero at some point      **Epoch - 14/40**     `loss: 1.1273 - rpn_class_loss: 0.3505 - rpn_bbox_loss: 0.7767 - mrcnn_class_loss: 1.0353e-06 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 `       While checking to debug the negative 'mrcnn_class_loss', I have noticied following TODO statement in mrcnn_class_loss_graph method            # TODO: Update this line to work with batch > 1. Right now it assumes all       #       images in a batch have the same active_class_ids        According to Config, self.BATCH_SIZE = self.IMAGES_PER_GPU * self.GPU_COUNT,  so BATCH_SIZE is 4.       1. How should i change the following line of code in  mrcnn_class_loss_graph method?  Could you please suggest me?           `pred_active = tf.gather(active_class_ids[0], pred_class_ids)`       2. rpn_bbox_loss is always above 0.7, not able to come down.  How should i debug to reduce rpn_bbox_loss?        "
"@tomgross, I pulled all your changes for Tensorflow 2.0 compatibility, now I am able to train my custom dataset (based on balloon.py) - code is exactly the same but I am getting this error at the tail end      File ""custom.py"", line 414, in        train(model)    File ""custom.py"", line 243, in train      layers='heads')    File ""/mnt/resource/batch/tasks/shared/LS_root/mounts/clusters/pankaja-gpu1/code/users/pankaja_us/Custom_MaskRCNN/mrcnn/model.py"", line 2414, in train      use_multiprocessing=workers > 1,    File ""/anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 819, in fit      use_multiprocessing=use_multiprocessing)    File ""/anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py"", line 604, in fit      steps_name='steps_per_epoch')    File ""/anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py"", line 221, in model_iteration      batch_data = _get_next_batch(generator)    File ""/anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py"", line 364, in _get_next_batch      generator_output = next(generator)    Please advice."
"Hi    I execute model.py in my project that is base on   But when I train the model with mask_rcnn_pneumonia_0020.h5 with **inference_config_2.NUM_CLASSES == 2** it worked fine but when I change **Stage 12 config** in line 148( inference_config_2.NUM_CLASSES == 3) it gives me the following error.    is this issue in NUM_CLASSES or any other?  thanks in advance       ValueError: Dimension 1 in both shapes must be equal, but are 12 and 8. Shapes are [1024,12] and [1024,8]. for 'Assign_760' (op: 'Assign') with input shapes: [1024,12], [1024,8]."
"I am facing the following error while training, I am facing this error after differnt epochs in each time.  InvalidArgumentError                      Traceback (most recent call last)    in  ()        6             learning_rate=config.LEARNING_RATE,        7             epochs=1,  ----> 8             layers='heads')    7 frames  /tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py in __call__(self, *args, **kwargs)     1470         ret = tf_session.TF_SessionRunCallable(self._session._session,     1471                                                self._handle, args,  -> 1472                                                run_metadata_ptr)     1473         if run_metadata:     1474           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)    InvalidArgumentError: indices[68] = [201, 12] does not index into param shape [256,12,4]    [[{{node mrcnn_bbox_loss/GatherNd}}]]      "
"I trained mask_rcnn on segmenting boxes, however, boxes that closely sit next to each other are considered a single instance.    I know the size of the box in pixel space, is there a way I can utilize this information to achieve the correct segmentation? I tried modifying the size and aspect ratio of the anchor boxes but didn't see positive changes."
"Hello everyone,  I am working on project which segment custom data using Mask RCNN and I want to compute the IOU for the output prediction.      can anyone help??"
Plz help me to solve this issue
"Hi,   any methods to get Precision and Recall for each class, like this pictures below.    and if there a way to generate a confusion matrix.  !   "
"Hi everyone !    Hi, when I train a model on a custom dataset ( 7 classes ) with the same configs and using layers='heads' ,i got a very high loss started with ~3.900.  After 100 epochs, loss became ~1.300 , now am in epochs 180, the loss doesn't decrease around 0.8460.    Epoch 180/200  100/100 [==============================] - 257s - loss: 0.8460 - rpn_class_loss: 0.0319 - rpn_bbox_loss: 0.2793 - mrcnn_class_loss: 0.1216 - mrcnn_bbox_loss: 0.1504 - mrcnn_mask_loss: 0.2628 - val_loss: 1.7520 - val_rpn_class_loss: 0.1110 - val_rpn_bbox_loss: 0.6783 - val_mrcnn_class_loss: 0.3193 - val_mrcnn_bbox_loss: 0.3090 - val_mrcnn_mask_loss: 0.3344  Is that normal loss 0.8 ?  Any good advice?   thanks."
"I don't want to save an h5 model for each epoch, I hope to rewrite it to save an h5 model  after a given epoch.How should i codeï¼Ÿ"
"When I run demo.py, Spyder promptsï¼š ModuleNotFoundError: No module named 'astunparse'"
None
"I am having trouble importing libraries such as rasaterio and GeoPandas into the demo.ipynb file. They can be imported in the first line as such:         This line will run and `geopandas` will be imported. The second line will also run (Configurations), however the third line will not run and will give back an error:          If I restart the kernel, remove `import geopandas` and move it below the third code block, the third code block will run, but when I run the import geopandas block I get this error:          It seems that the model will not allow some libraries to be imported. I have no clue how to resolve this issue. Does anyone have any suggestions?"
"Hi, when I train the model on a custom dataset using the parameter 'head' everything goes fine: i can succesfully train and use the model in inference.  When instead I change the 'head' to everything else, i.e. '3+' or '5+', the model ends the training but when i try to load the weights to use it in inference this error arise:    **Layer 391 (named ""mrcnn_bbox_fc""), weight   has shape (1024, 56), but the saved weight has shape (1024, 40)**    The 'head', '3+', '5+' are referring to the train 'layer' parameter in function `train()`, row 2276 in model.py,  defined as :  layers: Allows selecting wich layers to train. It can be:    -     -A regular expression to match layer names to train  -     -One of these predefined values:  -     -heads: The RPN, classifier and mask heads of the network  -     -all: All the layers  -     -3+: Train Resnet stage 3 and up  -     -4+: Train Resnet stage 4 and up  -     -5+: Train Resnet stage 5 and up    The model architecture is the following:  Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.85  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 1  IMAGE_CHANNEL_COUNT            3  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                26  IMAGE_MIN_DIM                  800  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           digimet  NUM_CLASSES                    14  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  PRE_NMS_LIMIT                  6000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                300  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True (in training), False (in inference)  USE_RPN_ROIS                   True  VALIDATION_STEPS               20  WEIGHT_DECAY                   0.0001            "
The mask mAPs are -  Using Eval functions of Matterport repo - mAP@0.5IOU - 0.15  Using eval.py of TF OD API MRCNN - mAP@0.5IOU - 0.21 (using pascal_voc_instance_segmentation_metrics)    One model is being evaluated using the eval scripts/functions of the above two methods. Was wondering what is different between Keras and Tensorflow implementations?
The repository is using 1.1x version of tensorflow and tf2 is released and is more widely used version of tf.Also tf2 seems to be becoming default for newer implementation with keras. Does matterport have any plans to upgrade the repo to tf2 from the current tf1.x based implementation.    Thanks
"I know how to save the whole image along with mask visualization. But if I want to save each masked region of an image as separate png file, how to do it?"
"Hello!  First, I got an error with anchors in buil functio in model.py. I fixed that with the help of Anchors Layer as was adviced in issue  #1930 .      And now I faced with another error :         My code :       Thank you!"
In #1112 it is mentioned that the overall loss is simply the sum of all minor losses. How can I change this to something like loss = alpha * loss1 + beta * loss2 + ...?
"I could not find the comment section in your website :      The following message appears during training:  2020-11-12 21:39:57.729227: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA  Traceback (most recent call last):    File ""custom.py"", line 346, in        train(model)    File ""custom.py"", line 167, in train      dataset_train.load_custom(args.dataset, ""train"")    File ""custom.py"", line 100, in load_custom      polygons = [r['shape_attributes'] for r in a['regions']]    File ""custom.py"", line 100, in        polygons = [r['shape_attributes'] for r in a['regions']]  TypeError: string indices must be integers"
"Hello! I'm trying to retrain the Mask-RCNN using  only 3 classes from the MS COCO dataset. I've been following the step in #247 in order to do this.  Firstly, I changed `NUM_CLASSES=3` and then I changed the Â´class_ids = [1, 67]` and finally I added `exclude=['mrcnn_class_logits', 'mrcnn_bbox_fc', 'mrcnn_bbox', `'mrcnn_mask']` to the load_weights function on line 469 as in #247. For reference I'm using the 2014 training dataset from MS COCO      However I get an error message saying that something with the image dimensions are off(the error message is much longer than this but it keeps repeating being unable to fit one shape into another):        "
"Can anyone help with the following error? I have been able to install all the requirements but it fails on the setup.     azureuser@user-aml-demo:~/cloudfiles/code/Users/xweng/Mask_RCNN$ python3 setup.py install  WARNING:root:Fail load requirements file, so using default ones.  running install  running bdist_egg  running egg_info  writing mask_rcnn.egg-info/PKG-INFO  writing dependency_links to mask_rcnn.egg-info/dependency_links.txt  writing top-level names to mask_rcnn.egg-info/top_level.txt  reading manifest template 'MANIFEST.in'  writing manifest file 'mask_rcnn.egg-info/SOURCES.txt'  installing library code to build/bdist.linux-x86_64/egg  running install_lib  running build_py  creating build/bdist.linux-x86_64/egg  creating build/bdist.linux-x86_64/egg/mrcnn  copying build/lib/mrcnn/config.py -> build/bdist.linux-x86_64/egg/mrcnn  copying build/lib/mrcnn/model.py -> build/bdist.linux-x86_64/egg/mrcnn  copying build/lib/mrcnn/parallel_model.py -> build/bdist.linux-x86_64/egg/mrcnn  copying build/lib/mrcnn/utils.py -> build/bdist.linux-x86_64/egg/mrcnn  copying build/lib/mrcnn/visualize.py -> build/bdist.linux-x86_64/egg/mrcnn  copying build/lib/mrcnn/__init__.py -> build/bdist.linux-x86_64/egg/mrcnn  byte-compiling build/bdist.linux-x86_64/egg/mrcnn/config.py to config.cpython-36.pyc  byte-compiling build/bdist.linux-x86_64/egg/mrcnn/model.py to model.cpython-36.pyc  byte-compiling build/bdist.linux-x86_64/egg/mrcnn/parallel_model.py to parallel_model.cpython-36.pyc  byte-compiling build/bdist.linux-x86_64/egg/mrcnn/utils.py to utils.cpython-36.pyc  byte-compiling build/bdist.linux-x86_64/egg/mrcnn/visualize.py to visualize.cpython-36.pyc  byte-compiling build/bdist.linux-x86_64/egg/mrcnn/__init__.py to __init__.cpython-36.pyc  creating build/bdist.linux-x86_64/egg/EGG-INFO  copying mask_rcnn.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO  copying mask_rcnn.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO  copying mask_rcnn.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO  copying mask_rcnn.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO  zip_safe flag not set; analyzing archive contents...  creating 'dist/mask_rcnn-2.1-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it  removing 'build/bdist.linux-x86_64/egg' (and everything under it)  Processing mask_rcnn-2.1-py3.6.egg  Removing /anaconda/envs/azureml_py36/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg  Copying mask_rcnn-2.1-py3.6.egg to /anaconda/envs/azureml_py36/lib/python3.6/site-packages  error: [Errno 38] Function not implemented: 'dist/mask_rcnn-2.1-py3.6.egg'"
"I am installing Mask_RCNN using python 3.6, while running command `sudo python3 setup.py install` I encounter the following error:  ` running install  running bdist_egg  running egg_info  writing mask_rcnn.egg-info/PKG-INFO  writing dependency_links to mask_rcnn.egg-info/dependency_links.txt  writing requirements to mask_rcnn.egg-info/requires.txt  writing top-level names to mask_rcnn.egg-info/top_level.txt  reading manifest file 'mask_rcnn.egg-info/SOURCES.txt'  reading manifest template 'MANIFEST.in'  writing manifest file 'mask_rcnn.egg-info/SOURCES.txt'  installing library code to build/bdist.linux-x86_64/egg  running install_lib  running build_py  creating build/bdist.linux-x86_64/egg  creating build/bdist.linux-x86_64/egg/mrcnn  copying build/lib/mrcnn/__init__.py -> build/bdist.linux-x86_64/egg/mrcnn  copying build/lib/mrcnn/visualize.py -> build/bdist.linux-x86_64/egg/mrcnn  copying build/lib/mrcnn/parallel_model.py -> build/bdist.linux-x86_64/egg/mrcnn  copying build/lib/mrcnn/config.py -> build/bdist.linux-x86_64/egg/mrcnn  copying build/lib/mrcnn/utils.py -> build/bdist.linux-x86_64/egg/mrcnn  copying build/lib/mrcnn/model.py -> build/bdist.linux-x86_64/egg/mrcnn  byte-compiling build/bdist.linux-x86_64/egg/mrcnn/__init__.py to __init__.cpython-36.pyc  byte-compiling build/bdist.linux-x86_64/egg/mrcnn/visualize.py to visualize.cpython-36.pyc  byte-compiling build/bdist.linux-x86_64/egg/mrcnn/parallel_model.py to parallel_model.cpython-36.pyc  byte-compiling build/bdist.linux-x86_64/egg/mrcnn/config.py to config.cpython-36.pyc  byte-compiling build/bdist.linux-x86_64/egg/mrcnn/utils.py to utils.cpython-36.pyc  byte-compiling build/bdist.linux-x86_64/egg/mrcnn/model.py to model.cpython-36.pyc  creating build/bdist.linux-x86_64/egg/EGG-INFO  copying mask_rcnn.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO  copying mask_rcnn.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO  copying mask_rcnn.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO  copying mask_rcnn.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO  copying mask_rcnn.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO  zip_safe flag not set; analyzing archive contents...  creating 'dist/mask_rcnn-2.1-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it  removing 'build/bdist.linux-x86_64/egg' (and everything under it)  Processing mask_rcnn-2.1-py3.6.egg  Removing /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg  Copying mask_rcnn-2.1-py3.6.egg to /usr/local/lib/python3.6/dist-packages  mask-rcnn 2.1 is already the active version in easy-install.pth    Installed /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg  Processing dependencies for mask-rcnn==2.1  Searching for grpcio>=1.8.6  Reading    Downloading    Best match: grpcio 1.33.2  Processing grpcio-1.33.2.tar.gz  Writing /tmp/easy_install-xzdp9apl/grpcio-1.33.2/setup.cfg  Running grpcio-1.33.2/setup.py -q bdist_egg --dist-dir /tmp/easy_install-xzdp9apl/grpcio-1.33.2/egg-dist-tmp-duxvlmvj  Found cython-generated files...  warning: no files found matching '*.c' under directory 'src/python/grpcio/grpc'  warning: no files found matching '*.h' under directory 'src/python/grpcio/grpc'  warning: no files found matching '*.inc' under directory 'src/python/grpcio/grpc'  warning: no files found matching '*.python' under directory 'src/python/grpcio/grpc'  warning: no previously-included files matching '*.so' found under directory 'src/python/grpcio/grpc/_cython'  warning: no previously-included files matching '*.pyd' found under directory 'src/python/grpcio/grpc/_cython'  warning: no files found matching 'src/python/grpcio/precompiled.py'  Found cython-generated files...  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory    Could not find  . This could mean the following:    * You're on Ubuntu and haven't run `apt-get install python3-dev`.    * You're on RHEL/Fedora and haven't run `yum install python3-devel` or      `dnf install python3-devel` (make sure you also have redhat-rpm-config      installed)    * You're on Mac OS X and the usual Python framework was somehow corrupted      (check your environment variables or try re-installing?)    * You're on Windows and your Python installation was somehow corrupted      (check your environment variables or try re-installing?)  Traceback (most recent call last):    File ""/usr/lib/python3.6/distutils/unixccompiler.py"", line 118, in _compile      extra_postargs)    File ""/tmp/easy_install-xzdp9apl/grpcio-1.33.2/src/python/grpcio/_spawn_patch.py"", line 54, in _commandfile_spawn    File ""/usr/lib/python3.6/distutils/ccompiler.py"", line 909, in spawn      spawn(cmd, dry_run=self.dry_run)    File ""/usr/lib/python3.6/distutils/spawn.py"", line 36, in spawn      _spawn_posix(cmd, search_path, dry_run=dry_run)    File ""/usr/lib/python3.6/distutils/spawn.py"", line 159, in _spawn_posix      % (cmd, exit_status))  distutils.errors.DistutilsExecError: command 'x86_64-linux-gnu-gcc' failed with exit status 1    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""/tmp/easy_install-xzdp9apl/grpcio-1.33.2/src/python/grpcio/commands.py"", line 264, in build_extensions    File ""/usr/local/lib/python3.6/dist-packages/Cython/Distutils/old_build_ext.py"", line 195, in build_extensions      _build_ext.build_ext.build_extensions(self)    File ""/usr/lib/python3.6/distutils/command/build_ext.py"", line 448, in build_extensions      self._build_extensions_serial()    File ""/usr/lib/python3.6/distutils/command/build_ext.py"", line 473, in _build_extensions_serial      self.build_extension(ext)    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/command/build_ext.py"", line 199, in build_extension    File ""/usr/lib/python3.6/distutils/command/build_ext.py"", line 533, in build_extension      depends=ext.depends)    File ""/tmp/easy_install-xzdp9apl/grpcio-1.33.2/src/python/grpcio/_parallel_compile_patch.py"", line 59, in _parallel_compile    File ""/usr/lib/python3.6/multiprocessing/pool.py"", line 266, in map      return self._map_async(func, iterable, mapstar, chunksize).get()    File ""/usr/lib/python3.6/multiprocessing/pool.py"", line 644, in get      raise self._value    File ""/usr/lib/python3.6/multiprocessing/pool.py"", line 119, in worker      result = (True, func(*args, **kwds))    File ""/usr/lib/python3.6/multiprocessing/pool.py"", line 44, in mapstar      return list(map(*args))    File ""/tmp/easy_install-xzdp9apl/grpcio-1.33.2/src/python/grpcio/_parallel_compile_patch.py"", line 54, in _compile_single_file    File ""/tmp/easy_install-xzdp9apl/grpcio-1.33.2/src/python/grpcio/commands.py"", line 248, in new_compile    File ""/usr/lib/python3.6/distutils/unixccompiler.py"", line 120, in _compile      raise CompileError(msg)  distutils.errors.CompileError: command 'x86_64-linux-gnu-gcc' failed with exit status 1    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/sandbox.py"", line 154, in save_modules    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/sandbox.py"", line 195, in setup_context    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/sandbox.py"", line 250, in run_setup    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/sandbox.py"", line 45, in _execfile    File ""/tmp/easy_install-xzdp9apl/grpcio-1.33.2/setup.py"", line 477, in      File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/__init__.py"", line 129, in setup    File ""/usr/lib/python3.6/distutils/core.py"", line 148, in setup      dist.run_commands()    File ""/usr/lib/python3.6/distutils/dist.py"", line 955, in run_commands      self.run_command(cmd)    File ""/usr/lib/python3.6/distutils/dist.py"", line 974, in run_command      cmd_obj.run()    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/command/bdist_egg.py"", line 172, in run    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/command/bdist_egg.py"", line 158, in call_command    File ""/usr/lib/python3.6/distutils/cmd.py"", line 313, in run_command      self.distribution.run_command(command)    File ""/usr/lib/python3.6/distutils/dist.py"", line 974, in run_command      cmd_obj.run()    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/command/install_lib.py"", line 24, in run    File ""/usr/lib/python3.6/distutils/command/install_lib.py"", line 109, in build      self.run_command('build_ext')    File ""/usr/lib/python3.6/distutils/cmd.py"", line 313, in run_command      self.distribution.run_command(command)    File ""/usr/lib/python3.6/distutils/dist.py"", line 974, in run_command      cmd_obj.run()    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/command/build_ext.py"", line 78, in run    File ""/usr/local/lib/python3.6/dist-packages/Cython/Distutils/old_build_ext.py"", line 186, in run      _build_ext.build_ext.run(self)    File ""/usr/lib/python3.6/distutils/command/build_ext.py"", line 339, in run      self.build_extensions()    File ""/tmp/easy_install-xzdp9apl/grpcio-1.33.2/src/python/grpcio/commands.py"", line 267, in build_extensions    File ""/tmp/easy_install-xzdp9apl/grpcio-1.33.2/src/python/grpcio/support.py"", line 118, in diagnose_build_ext_error    File ""/tmp/easy_install-xzdp9apl/grpcio-1.33.2/src/python/grpcio/support.py"", line 77, in diagnose_compile_error    File ""/tmp/easy_install-xzdp9apl/grpcio-1.33.2/src/python/grpcio/support.py"", line 71, in _expect_compile  commands.CommandError: Diagnostics found a compilation environment issue:    Could not find  . This could mean the following:    * You're on Ubuntu and haven't run `apt-get install python3-dev`.    * You're on RHEL/Fedora and haven't run `yum install python3-devel` or      `dnf install python3-devel` (make sure you also have redhat-rpm-config      installed)    * You're on Mac OS X and the usual Python framework was somehow corrupted      (check your environment variables or try re-installing?)    * You're on Windows and your Python installation was somehow corrupted      (check your environment variables or try re-installing?)      During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""setup.py"", line 67, in        keywords=""image instance segmentation object detection mask rcnn r-cnn tensorflow keras"",    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/__init__.py"", line 129, in setup    File ""/usr/lib/python3.6/distutils/core.py"", line 148, in setup      dist.run_commands()    File ""/usr/lib/python3.6/distutils/dist.py"", line 955, in run_commands      self.run_command(cmd)    File ""/usr/lib/python3.6/distutils/dist.py"", line 974, in run_command      cmd_obj.run()    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/command/install.py"", line 67, in run    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/command/install.py"", line 117, in do_egg_install    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/command/easy_install.py"", line 437, in run    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/command/easy_install.py"", line 679, in easy_install    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/command/easy_install.py"", line 726, in install_item    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/command/easy_install.py"", line 771, in process_distribution    File ""/usr/share/python-wheels/pkg_resources-0.0.0-py2.py3-none-any.whl/pkg_resources/__init__.py"", line 774, in resolve    File ""/usr/share/python-wheels/pkg_resources-0.0.0-py2.py3-none-any.whl/pkg_resources/__init__.py"", line 1057, in best_match    File ""/usr/share/python-wheels/pkg_resources-0.0.0-py2.py3-none-any.whl/pkg_resources/__init__.py"", line 1069, in obtain    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/command/easy_install.py"", line 698, in easy_install    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/command/easy_install.py"", line 724, in install_item    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/command/easy_install.py"", line 909, in install_eggs    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/command/easy_install.py"", line 1177, in build_and_install    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/command/easy_install.py"", line 1163, in run_setup    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/sandbox.py"", line 253, in run_setup    File ""/usr/lib/python3.6/contextlib.py"", line 99, in __exit__      self.gen.throw(type, value, traceback)    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/sandbox.py"", line 195, in setup_context    File ""/usr/lib/python3.6/contextlib.py"", line 99, in __exit__      self.gen.throw(type, value, traceback)    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/sandbox.py"", line 166, in save_modules    File ""/usr/share/python-wheels/setuptools-39.0.1-py2.py3-none-any.whl/setuptools/sandbox.py"", line 140, in resume  ModuleNotFoundError: No module named 'commands'  `"
ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.
"Hi guys,    do you face with this problem during training? My training process get slower with time. I do not know why it is happening and I get this error (like in title) all the time:  !   Also I do not use this repo. I use mask rcnn from this repo:  "
"I want to use compute_ap() function to evaluate my model, but I don't understand the meaning of AP value returned. I don't know how the function works to get AP values of each image. I understand the first step, in which the matches between ground truth and predictions are computed (compute_matches()), but I don't understand the rest of compute_ap() function (how precisions and recalls are compute to get AP). Can anybody help me? Thank you."
"Hi,    did some already implement VGG 16 as backbone and could share his/her code?    @ltrottier you mentioned in another post that you already experimented with VGG 16.    Thanks in advance!"
"""   I followed your suggestions and made some changes, but I donâ€™t know how to implement the fourth and fifth points.can you help me?"
"Hi guys,    I would like know how to calculate tp,tn,fp,tn on prediction mask rcnn?    "
I want to know if this program support multiple GPUs?Thank you!
I have a dataset of PowerPoint slides where some images contain logos. I have built a model that accurately detects these logos.     I now want my model to output a probability score between 0 and 1 for the presence of a logo in the inputted image.   If the model believes no logos are present then calling `model.detect(...)` returns an empty list for the scores as there are no suggested ROIs.  
"Traceback (most recent call last):    File ""D:/tf_gpu/Mask_RCNN-master/samples/coco/coco.py"", line 481, in        dataset_train.load_coco(args.dataset, ""train"", year=args.year, auto_download=args.download)    File ""D:/tf_gpu/Mask_RCNN-master/samples/coco/coco.py"", line 111, in load_coco      coco = COCO(""{}/annotations/instances_{}{}.json"".format(dataset_dir, subset, year))    File ""D:\anaconda1\envs\tensorflow2\lib\site-packages\pycocotools\coco.py"", line 84, in __init__      with open(annotation_file, 'r') as f:  FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\GuoJiaHao\\coco/annotations/instances_train2014.json'    Just started to undertands thisï¼Œhow should I correct this error"
"Hi, I have been working with maskrcnn to detect and segmentation leaves on trees. With several test we decide to use only de detection (box) and discard the segmentation. To increase performance, I would like to know if the segmentation process can be ignore to speed up the detection or I will have to retrain another network. The problem is that the training and validate database have been prepared to train this network and I doubt that I can use it to train an object detection network.  Thenx!"
"I'm a beginner of deep-learning, recently, i try to use the mask-rcnn to make a project about rust detection. Firstly i watch the video how to use this code, then i modify the code of 'balloon', and have a try to train my own data, but i meet the error called keyerror: all_points_y, my dataset are many binary images which contain the rust areas(white) and the background(black). To approprate the requirements of code, i use the edge detection to found the outline of rust areas, then i save the class name(only 'rust' class) and the coordinate of the points which on the outlines as .json file according to the requirement format(firstly i save these data in a dict, then i use json.dump to transform the dict to json and save it):      #{ 'filename': '28503151_5b5b7ec140_b.jpg',          #   'regions': {          #       '0': {          #           'region_attributes': {},          #           'shape_attributes': {          #               'all_points_x': [...],          #               'all_points_y': [...],          #               'name': 'polygon'}},          #       ... more regions ...          #   },          #   'size': 100202          # }  During the training processing, i saw some error messages repeatly about keyerror: all_point_y, the message repeat about 20 times when my train dataset has 120 pictures and the val dataset has 30 pictures. Easily see that not all picture will bring error, but i don't know how the error happened.  I try to search the problem and only find this kind of answer: the points in dataset should be in an polygon instead of circle or rectangle, but the obviously the answer isn't fit for my question.  Futher more, when load the json file, the order of data usually have change,for example, when i save the data, the format is:   'shape_attributes': {       'all_points_x': [...],       'all_points_y': [...],       'name': 'rust'}  but when i load the data, it change to the format as:       'all_points_y': [...],       'name': 'rust',       'all_points_y': [...]}   whether this issue have an affect to the training processing?    ps:   1.In fact , i'm also a beginner of english.  2.Help me, every scholars(dalao), thanksÂ·"
"Create Model and Load Trained WeightsÂ¶  # Create model object in inference mode.  #model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)  model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)  â€‹  # Load weights trained on MS-COCO  model.load_weights(COCO_MODEL_PATH, by_name=True)  ---------------------------------------------------------------------------  NameError                                 Traceback (most recent call last)    in          1 # Create model object in inference mode.        2 #model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)  ----> 3 model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)        4         5 # Load weights trained on MS-COCO    NameError: name 'config' is not defined    Please how we solve this config not defind"
Could someone help me with this error that is giving?    **Dependency version:**  **TensorFlow: 1.13.1;**  **Keras: 2.1.6;**     `
None
"problemï¼š  tensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[400,256,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc    [[node mrcnn_mask_deconv/conv2d_transpose (defined at \anaconda1\envs\tensorflow2\lib\site-packages\keras\backend\tensorflow_backend.py:3009) ]]  Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.   [Op:__inference_keras_scratch_graph_19043]    Function call stack:  keras_scratch_graph    I think it's the GPU running memory problemï¼Œbut I don't know how to reduce the parameters in the config or other parameters to runï¼ŒI hope it can be answered"
"Hey,   I have issue with exporting inference graph i dont know what is the issue i have been trying to export but its not working. Thank you in advance. Help will be appreciated.    Command used:  python ../../models/research/object_detection/export_inference_graph.py  \    --input_type image_tensor   \      --pipeline_config_path /home/bigthinx/TensorFlow/workspace/training_demo/pre-trained-models/mask_rcnn_inception_resnet_v2_atrous_coco_2018_01_28/pipeline.config  \        --trained_checkpoint_prefix /home/bigthinx/TensorFlow/workspace/training_demo/pre-trained-models/mask_rcnn_inception_resnet_v2_atrous_coco_2018_01_28/model.ckpt-340000 \          --output_directory /home/bigthinx/TensorFlow/workspace/training_demo/inference_graph_5k      Log  :/home/bigthinx/.conda/envs/cloth_detection/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])  /home/bigthinx/.conda/envs/cloth_detection/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])  /home/bigthinx/.conda/envs/cloth_detection/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])  /home/bigthinx/.conda/envs/cloth_detection/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])  /home/bigthinx/.conda/envs/cloth_detection/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])  /home/bigthinx/.conda/envs/cloth_detection/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    np_resource = np.dtype([(""resource"", np.ubyte, 1)])  /home/bigthinx/.conda/envs/cloth_detection/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])  /home/bigthinx/.conda/envs/cloth_detection/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])  /home/bigthinx/.conda/envs/cloth_detection/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])  /home/bigthinx/.conda/envs/cloth_detection/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])  /home/bigthinx/.conda/envs/cloth_detection/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])  /home/bigthinx/.conda/envs/cloth_detection/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    np_resource = np.dtype([(""resource"", np.ubyte, 1)])  Traceback (most recent call last):    File ""../../models/research/object_detection/export_inference_graph.py"", line 206, in        tf.app.run()    File ""/home/bigthinx/.conda/envs/cloth_detection/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 40, in run      _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)    File ""/home/bigthinx/.conda/envs/cloth_detection/lib/python3.6/site-packages/absl/app.py"", line 299, in run      _run_main(main, args)    File ""/home/bigthinx/.conda/envs/cloth_detection/lib/python3.6/site-packages/absl/app.py"", line 250, in _run_main      sys.exit(main(argv))    File ""../../models/research/object_detection/export_inference_graph.py"", line 202, in main      side_input_types=side_input_types)  TypeError: export_inference_graph() got an unexpected keyword argument 'use_side_inputs'"
Does anyone have a .yml file for creating the environment using anaconda 3.4 on linux ubuntu 16.04?
"Could anyone help create the environment in anaconda? I've tried it in different ways and it shows error. I'm trying to create an environment using python 3.4, tensorflow 1.15.3 and keras 2.2.4"
"@waleedka @moorage   I want to detect multiple images. I tried but it gives the following error:  `""len(images) must be equal to BATCH_SIZE""`    How can I modify the code for multiples images, where the number of images is greater than the specified batch_size. And the List of images can be used as a input the detection module:         Thanks in Advanced."
"Hi. I am trying to make a program using the already pre-trained model mask_rcnn_balloons.h5, but get a problem when loadning the file.     how I load the model:  path = ""E:/Dokumenter/Ikt450/Assignments/7/models/mask_rcnn_balloon.h5""  model = keras.models.load_model(path)    error:  Traceback (most recent call last):    File ""E:/Dokumenter/Ikt450/Assignments/7/main.py"", line 9, in        model = keras.models.load_model(path)    File ""C:\Users\suvat\.virtualenvs\7-p3jOesU2\lib\site-packages\tensorflow\python\keras\saving\save.py"", line 182, in load_model      return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)    File ""C:\Users\suvat\.virtualenvs\7-p3jOesU2\lib\site-packages\tensorflow\python\keras\saving\hdf5_format.py"", line 175, in load_model_from_hdf5      raise ValueError('No model found in config file.')  ValueError: No model found in config file."
"Hello everyone,    I am currently trying to predict images with my RCNN. Pictures are being processed as usual but I get an error when it comes to displaying the results. It went without a problem the previous times.  670 pictures are being processed. I'm using following code:                    I have no clue what the error even mean nor do I know how to fix it. I' willl be more than happy to provide further information  if it is needed. Thanks in advance"
Hi    I'm a newbie in AI and i'm using labelbox to create my own dataset (instance segmentation) and the annotation output is a single json file.    The issue that i have is that the model that i'm using (Mask RCNN) need to be feed with images with an annotation file in VOC xml for each file.    I need a script that could use the single JSON from labelbox and convert it to multiple images and voc xml annotation file.    Thanks for your help.
Hi to everyone. I have jpg of my images and masks (I don't have annotations). How I can load this in datasets? What changes should I make in the Class Dataset?    Thank you for your time
"      Even though I tried read the train_shapes.ipynb, I still don't know how to use it.Like the guidance tell me to override some functions(load_shapes(), load_image(), load_mask(),load_reference()) . I really don't know how can I do it.        And my dataset can be transofmed like the datasets of nucleus(  you can help me, I will appreciate it.    "
hi am trying this implementation on my own dataset and while visualize the loss function curve a realize that there's an overfitting problem. also when i get a different mAP every time i run the model on the test set images( the test images are used only for test)! can anyone tell me what happen.  here's the loss curve    !     
"Hello everyone,  I get this error whenever I delete the use_mini_mask from the argument list considering that I am using mrcnn2 (the one that works with tensorflow 2).  And I get this error TypeError: load_image_gt() got an unexpected keyword argument 'use_mini_mask' when I leave 'use_mini_mask'. .  Anyone can help???"
"**Started by doing the right dataset training:**    `    **However, an error occurred after that:**    - COULD SOMEONE HELP ME WITH THIS ERROR OR AUTHOR HIMSELF @Wahaha1314 PLEASE !!!   `  "
"Hi,    I have read many issues about data augmentation and I am a little bit confused. Could tell me if I can add all augmentation techniques from imgaug library or I cannot use some of these like flip, rotate (geometric) beacuse of masks?     Maybe you can advise if use Sequential, OneOf or something different method and particular techniques like as blur, contrast, add, multiply?    I would be grateful for any help."
"Hey,   I run sample/demo.py( I copy all code from juypter ) and the detection time is 32s!!!!   I cannot figure out why it is so long.  By the way, I trained  a model with my own data, train time is fine. But detect time is too long.  I am using tensorflo-gpu       and cuda 9.0       and i got this       Who can help me ?   Thanks in advance.  "
"Hi all,    hope you are well and safe..    I am using MaskRCNN for my project, my data are actually multidimensional arrays of ( signals with maximums ).   Any idea how to modify the model code to allow this type of data , what lines should I add/remove in the configuration script ?  I am new to almost everything about this , I do very appreciate any help.. Thank you     "
I calculated mAP for 10 images in validation set using compute_ap @IOU=0.5. And my result is **0.91**. Does this mean my model prediction are good? 
"@waleedka   In your documentation, you said about the inspection of activation. That means I want to inspect the activations at different layers to look for signs of trouble.  Is there any suggestion?  Thanks  "
"Am hosting a mask rcnn model on flask app, the predictions work ok for a couple of inference but then after sometime I get CUDA OOM error -  `(0) Resource exhausted: OOM when allocating tensor with shape[19,512,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc`    To reproduce the error -        Am using -  tensorflow-gpu==1.15.2  Keras==2.2.4"
"I want to implement CBIR, hence need features database to be created from my Mask R-CNN model. So from which layer is one supposed to extract features for the same? I learned that just after FC4096 is recommended, but what's the reason for the same?"
"While i was trying to create flask API for MASK-RCNN model, at the time of detection i am getting following error:--  AttributeError: 'Tensor' object has no attribute 'size'  and error is in model.py line 25    if i am ruunig the demo file normally its working fine but with flask getting problem.. Please help"
"I am trying to implement transfer learning with Mask_RCNN on a new dataset. The dataset is labeled with a class that already exist in coco (boat). After following a few guides, I was able to use a new dataset for the transfer learning. However, when I try to use my new weights for inference, I am pretty much given garbage.     I ran the training ran for 30 epochs with 100 steps per epoch. I set the minimum confidence to 0.9 and enabled the image padding. However, for the number of classes, I used 2 - the guides I used had it set to 2 (should this be 1 for just the background? since the boat class is already a class in coco?). I also had to remove certain layers in my network, specifically mrcnn_class_logits, mrcnn_bbox_fc, mrcnn_bbox, and mrcnn_mask. I had to remove these layers because the shapes did not match and I would be thrown an error.    Is there anything that is glaringly wrong? Can you guys point me to a direction where the problem might lie? If any of you have a comprehensive guide for transfer learning for a class that already exists, I would greatly appreciate a link to that.    I appreciate any help I can receive and thank you for your time as well.    Thanks!"
"Hello, everyone! I'm a student and now I'm trying to use this code.  I already have labeling file with PNG, so I convert that file to json to use pycococreator.  But matterport Mask R-CNN wants to know about all_points x and y coordinates, but my json files which made by pycococreator didn't have that information.  Then how can I do? I mean  1. How can I get(or convert) all_points x and y information to my json files(convert PNG to json).  2. Or any other ways to make matterport Mask R-CNN json(include all_points x and y coordinates) with PNG files(I annotated by using photo shop)    Can you guys help me??  T.T"
"Hello fellow mask_rcnn users,    I am using Mask_RCNN to perform instance segmentation. An I am using labelme as a labelling tool.     From the implementations, I can see that the bounding boxes are all straight. Is there a way to change the class  to accommodate the rotation of the bounding boxes? (Please see the figure below)    !     This is important because the objects I want to detect should have the rotated anchor boxes, so that I can later determine the orientation, whether it is left or right.     I would appreciate any help I can get.    Regards,  Yash Runwal.   "
"Hi,  I wanted to train on my own dataset (about 80 images for training and 20 for validation) using this training schedule (just a toy example to check what's wrong):       The first part of the training run smoothly, but then for the second part it loads the layers to be trained on and then stops without anything:    > root@336d474036f0:/home/jupyter/tailfinMask# python3 tailfin.py train --dataset=/home/jupyter/tailfinMask/datasets --weights=coco  > Using TensorFlow backend.  > Weights:  coco  > Dataset:  /home/jupyter/tailfinMask/datasets  > Logs:  /home/logs  >   > Configurations:  > BACKBONE                       resnet101  > BACKBONE_STRIDES               [4, 8, 16, 32, 64]  > BATCH_SIZE                     1  > BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  > COMPUTE_BACKBONE_SHAPE         None  > DETECTION_MAX_INSTANCES        100  > DETECTION_MIN_CONFIDENCE       0.7  > DETECTION_NMS_THRESHOLD        0.3  > FPN_CLASSIF_FC_LAYERS_SIZE     1024  > GPU_COUNT                      1  > GRADIENT_CLIP_NORM             5.0  > IMAGES_PER_GPU                 1  > IMAGE_CHANNEL_COUNT            3  > IMAGE_MAX_DIM                  512  > IMAGE_META_SIZE                15  > IMAGE_MIN_DIM                  128  > IMAGE_MIN_SCALE                0  > IMAGE_RESIZE_MODE              square  > IMAGE_SHAPE                    [512 512   3]  > LEARNING_MOMENTUM              0.9  > LEARNING_RATE                  0.0005  > LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  > MASK_POOL_SIZE                 14  > MASK_SHAPE                     [28, 28]  > MAX_GT_INSTANCES               100  > MEAN_PIXEL                     [123.7 116.8 103.9]  > MINI_MASK_SHAPE                (56, 56)  > NAME                           tailfin  > NUM_CLASSES                    3  > POOL_SIZE                      7  > POST_NMS_ROIS_INFERENCE        1000  > POST_NMS_ROIS_TRAINING         2000  > PRE_NMS_LIMIT                  6000  > ROI_POSITIVE_RATIO             0.33  > RPN_ANCHOR_RATIOS              [0.5, 1, 2]  > RPN_ANCHOR_SCALES              (16, 32, 64, 128, 256)  > RPN_ANCHOR_STRIDE              1  > RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  > RPN_NMS_THRESHOLD              0.9  > RPN_TRAIN_ANCHORS_PER_IMAGE    128  > STEPS_PER_EPOCH                100  > TOP_DOWN_PYRAMID_SIZE          256  > TRAIN_BN                       False  > TRAIN_ROIS_PER_IMAGE           200  > USE_MINI_MASK                  True  > USE_RPN_ROIS                   True  > VALIDATION_STEPS               50  > WEIGHT_DECAY                   0.005  >   >   > WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.  > Instructions for updating:  > If using Keras pass *_constraint arguments to layers.  > WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.  >   > WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.  > Instructions for updating:  > Use tf.where in 2.0, which has the same broadcast rule as np.where  > WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py:553: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.  >   > WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.  >   > WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py:600: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.  > Instructions for updating:  > box_ind is deprecated, use box_indices instead  > Loading weights  /home/jupyter/Mask_RCNN-master/mask_rcnn_coco.h5  > 2020-09-08 12:13:34.430309: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1  > 2020-09-08 12:13:34.432312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  > 2020-09-08 12:13:34.432553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:   > name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085  > pciBusID: 0000:01:00.0  > 2020-09-08 12:13:34.432712: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0  > 2020-09-08 12:13:34.433655: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0  > 2020-09-08 12:13:34.434435: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0  > 2020-09-08 12:13:34.434668: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0  > 2020-09-08 12:13:34.435815: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0  > 2020-09-08 12:13:34.436673: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0  > 2020-09-08 12:13:34.439150: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7  > 2020-09-08 12:13:34.439272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  > 2020-09-08 12:13:34.439643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  > 2020-09-08 12:13:34.439913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0  > 2020-09-08 12:13:34.440151: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA  > 2020-09-08 12:13:34.468091: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3699850000 Hz  > 2020-09-08 12:13:34.468377: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56171326d790 initialized for platform Host (this does not guarantee that XLA will be used). Devices:  > 2020-09-08 12:13:34.468401: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version  > 2020-09-08 12:13:34.547239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  > 2020-09-08 12:13:34.547560: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561713f235b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:  > 2020-09-08 12:13:34.547582: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1060 6GB, Compute Capability 6.1  > 2020-09-08 12:13:34.547803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  > 2020-09-08 12:13:34.548127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:   > name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085  > pciBusID: 0000:01:00.0  > 2020-09-08 12:13:34.548172: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0  > 2020-09-08 12:13:34.548190: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0  > 2020-09-08 12:13:34.548206: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0  > 2020-09-08 12:13:34.548222: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0  > 2020-09-08 12:13:34.548238: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0  > 2020-09-08 12:13:34.548271: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0  > 2020-09-08 12:13:34.548319: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7  > 2020-09-08 12:13:34.548371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  > 2020-09-08 12:13:34.548603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  > 2020-09-08 12:13:34.548892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0  > 2020-09-08 12:13:34.548964: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0  > 2020-09-08 12:13:34.549883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:  > 2020-09-08 12:13:34.549897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0   > 2020-09-08 12:13:34.549903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N   > 2020-09-08 12:13:34.549984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  > 2020-09-08 12:13:34.550285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  > 2020-09-08 12:13:34.550556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5154 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)  > Train network heads  >   > Starting at epoch 0. LR=0.0005  >   > Checkpoint Path: /home/logs/tailfin20200908T1213/mask_rcnn_tailfin_{epoch:04d}.h5    > Selecting layers to train  > fpn_c5p5               (Conv2D)  > fpn_c4p4               (Conv2D)  > fpn_c3p3               (Conv2D)  > fpn_c2p2               (Conv2D)  > fpn_p5                 (Conv2D)  > fpn_p2                 (Conv2D)  > fpn_p3                 (Conv2D)  > fpn_p4                 (Conv2D)  > In model:  rpn_model  >     rpn_conv_shared        (Conv2D)  >     rpn_class_raw          (Conv2D)  >     rpn_bbox_pred          (Conv2D)  > mrcnn_mask_conv1       (TimeDistributed)  > mrcnn_mask_bn1         (TimeDistributed)  > mrcnn_mask_conv2       (TimeDistributed)  > mrcnn_mask_bn2         (TimeDistributed)  > mrcnn_class_conv1      (TimeDistributed)  > mrcnn_class_bn1        (TimeDistributed)  > mrcnn_mask_conv3       (TimeDistributed)  > mrcnn_mask_bn3         (TimeDistributed)  > mrcnn_class_conv2      (TimeDistributed)  > mrcnn_class_bn2        (TimeDistributed)  > mrcnn_mask_conv4       (TimeDistributed)  > mrcnn_mask_bn4         (TimeDistributed)  > mrcnn_bbox_fc          (TimeDistributed)  > mrcnn_mask_deconv      (TimeDistributed)  > mrcnn_class_logits     (TimeDistributed)  > mrcnn_mask             (TimeDistributed)  > WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.  >   > WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.  >   > WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.  >   > WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.  >   > WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.  >   > Epoch 1/1  > 2020-09-08 12:14:43.906742: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0  > 2020-09-08 12:14:44.052006: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7  > 2020-09-08 12:14:46.667745: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.71GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.  > 2020-09-08 12:14:46.705874: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.  > 2020-09-08 12:14:47.042854: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.12GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.  > 100/100 [==============================] - 91s 910ms/step - loss: 1.8980 - rpn_class_loss: 0.0830 - rpn_bbox_loss: 0.4817 - mrcnn_class_loss: 0.2722 - mrcnn_bbox_loss: 0.4328 - mrcnn_mask_loss: 0.6279 - val_loss: 1.0747 - val_rpn_class_loss: 0.0457 - val_rpn_bbox_loss: 0.4271 - val_mrcnn_class_loss: 0.1380 - val_mrcnn_bbox_loss: 0.3765 - val_mrcnn_mask_loss: 0.4270  > WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.  >   > Train all layers  >   > Starting at epoch 1. LR=5e-05  >   > Checkpoint Path: /home/logs/tailfin20200908T1213/mask_rcnn_tailfin_{epoch:04d}.h5  > Selecting layers to train  > conv1                  (Conv2D)  > bn_conv1               (BatchNorm)  > res2a_branch2a         (Conv2D)  >...  > ...  > mrcnn_class_logits     (TimeDistributed)  > mrcnn_mask             (TimeDistributed)  > root@336d474036f0:/home/jupyter/tailfinMask#  >     I've also tried with resnet50, with similar results expect that it just get stuck at end without it ending it.    Does anyone have an idea how to solve this? Am I missing something in the configuration to train with multistage?  Thanks in advance  "
Hi  i was wondering that can we get the coordinates of the polygon mask generated of the each detected object using Mask_RCNN.? if so how can we get that.?
"How to label a single category with labelme and instance segmentation? There are cat, cat, cat, Cat0, CAT1, cat2 or cat-0, cat-1, cat-2, etc. which annotation method is used?thank you!    I use cat-0, cat-1, cat-2 for annotation, and use labelme2coco to convert annotations.json, where the annotation is [], what's the matter?"
"anybody has formula or codes how to calculate fps in mask-rcnn? i was succesfull detect it in real-time but ,dont know how to calculate the FPS "
"Hello, I have trained the model on custom dataset for 5 epochs, but while doing inference I am getting below error     ](     I tried the following modifications in the code but error is not resolved    1. Lowered `DETECTION_MIN_CONFIDENCE` in `mrcnn/config.py`      2. Fixed Division by Zero error in `mrcnn/utils.py` with these modifications `from __future__ import division`  , `shift = np.array(      ,  "
"Hi All,      I am currently developing a instance segmentation pipeline, for which I need some guidance.  1) First, what among MaskRCNN and ShapeMask would give better result, if at all someone has implemented them?  2) Also, I want instance segmentation for each of my 7 classes, so I definitely would use pretrained model. However, my concern is whether I would still need to annotate the images pixel-wise? If yes, would I need to annotate more images as much as I can, to get better trained model?  3) Please suggest me a pipeline I can follow for getting the instance segmentation to work."
"Hello guys,    I am training a custom model.     I have finished the training part and  I have plotted all the losses against the epochs.    But I want to plot the total loss against the learning rate. Any Idea how to do that?    This is implemented in the fastai library.     I would really appreciate your help.    Thanks,  Yash"
"Hi,  I am training on my own data set and I found out that my loss-values are changing depending on using multiprocessing.  My training results are much better, if I implement use_multiprocessing= False    I am confused why and I dont understand it.    I am training on a platform solution of a company, not Amazon/Google Cloud    Could someone give me any suggestions why this is happening?    I am very glad to hear from you :)  Thanks in advance :)"
@waleedka   I am training MaskRCNN with my own dataset. The loss values are look like this:  Epoch 1895/2000  200/200 [==============================] - 161s 804ms/step - loss: 0.5184 - rpn_class_loss: 0.0224 - rpn_bbox_loss: 0.1595 - mrcnn_class_loss: 0.0464 - mrcnn_bbox_loss: 0.0663 - mrcnn_mask_loss: 0.2238 - val_loss: 2.2940 - val_rpn_class_loss: 0.0479 - val_rpn_bbox_loss: 0.8811 - val_mrcnn_class_loss: 0.4146 - val_mrcnn_bbox_loss: 0.4171 - val_mrcnn_mask_loss: 0.5333  Epoch 1896/2000  113/200 [===============>..............] - ETA: 1:05 - loss: 0.4443 - rpn_class_loss: 0.0166 - rpn_bbox_loss: 0.1182 - mrcnn_class_loss: 0.0413 - mrcnn_bbox_loss: 0.0552 - mrcnn_mask_loss: 0.2129    My question is How can I improve  val_mrcnn_mask_loss and  mrcnn_mask_loss?    Thanks
"When training on a custom dataset, the training process seems to load every image once for every worker working."
"Anyone able to do multi-class instance segmentation on custom dataset using transfer learning ? I am facing the following error :     ValueError: Error when checking input: expected input_image_meta to have shape (None, 19) but got array with shape (1, 18)      i have modified balloon.py to make it multiclass :       class BalloonConfig(Config):      """"""Configuration for training on the toy  dataset.      Derives from the base Config class and overrides some values.      """"""      # Give the configuration a recognizable name      NAME = ""onion""        # We use a GPU with 12GB memory, which can fit two images.      # Adjust down if you use a smaller GPU.      IMAGES_PER_GPU = 1        # Number of classes (including background)      NUM_CLASSES = 7  # Background + balloon        # Number of training steps per epoch      STEPS_PER_EPOCH = 100        # IMAGE_RESIZE_MODE = ""crop""        # Skip detections with < 90% confidence      DETECTION_MIN_CONFIDENCE = 0.9          ############################################################  #  Dataset  ############################################################    class BalloonDataset(utils.Dataset):        def load_balloon(self, dataset_dir, subset):          """"""Load a subset of the Balloon dataset.          dataset_dir: Root directory of the dataset.          subset: Subset to load: train or val          """"""          # Add classes. We have only one class to add.          # self.add_class(""balloon"", 1, ""balloon"")            self.add_class(""onion"", 1, ""smut effected"")          self.add_class(""onion"", 2, ""rotten"")          self.add_class(""onion"", 3, ""half cut"")          self.add_class(""onion"", 4, ""double onion"")          self.add_class(""onion"", 4, ""sprouting"")          self.add_class(""onion"", 6, ""onion"")            # Train or validation dataset?          assert subset in [""train"", ""val""]          dataset_dir = os.path.join(dataset_dir, subset)            # print ('dataset_dir is ', dataset_dir)            # Load annotations          # VGG Image Annotator (up to version 1.6) saves each image in the form:          # { 'filename': '28503151_5b5b7ec140_b.jpg',          #   'regions': {          #       '0': {          #           'region_attributes': {},          #           'shape_attributes': {          #               'all_points_x': [...],          #               'all_points_y': [...],          #               'name': 'polygon'}},          #       ... more regions ...          #   },          #   'size': 100202          # }          # We mostly care about the x and y coordinates of each region          # Note: In VIA 2.0, regions was changed from a dict to a list.          annotations = json.load(open(os.path.join(dataset_dir, ""via_region_data.json"")))          annotations = list(annotations.values())  # don't need the dict keys            # print (annotations)            # The VIA tool saves images in the JSON even if they don't have any          # annotations. Skip unannotated images.          annotations = [a for a in annotations if a['regions']]            # Add images          for a in annotations:              # Get the x, y coordinaets of points of the polygons that make up              # the outline of each object instance. These are stores in the              # shape_attributes (see json format above)              # The if condition is needed to support VIA versions 1.x and 2.x.              if type(a['regions']) is dict:                  polygons = [r['shape_attributes'] for r in a['regions'].values()]                  objects = [s['region_attributes']['onion'] for s in a['regions'].values()]                  # print ('dictionary')              else:                  polygons = [r['shape_attributes'] for r in a['regions']]                   objects = [s['region_attributes']['onion'] for s in a['regions']]                  # print ('list')                # print ('objects : ', objects)              name_dict = {""smut effected"": 1,""rotten"": 2, ""half cut"": 3, ""double onion"": 4, ""sprouting"": 5, ""onion"": 6}              num_ids = [name_dict[a] for a in objects]              # print (""num_ids : "", num_ids)              # objects = [s['region_attributes']['onion'] for s in a['regions']]              # print (type(polygons))              # print (polygons)                # load_mask() needs the image size to convert polygons to masks.              # Unfortunately, VIA doesn't include it in JSON, so we must read              # the image. This is only managable since the dataset is tiny.              image_path = os.path.join(dataset_dir, a['filename'])              # print (image_path)              image = skimage.io.imread(image_path)              height, width = image.shape[:2]              # print (image.shape)              # cv2.imshow(""image"",image)              # cv2.waitKey(0)                self.add_image(                  ""onion"",                  image_id=a['filename'],  # use file name as a unique image id                  path=image_path,                  width=width, height=height,                  polygons=polygons,                  num_ids=num_ids)"
How to check the tensorflow versions compatibility with keras and run the code couldn t find any relevant information regarding this I'm using windows 10. and cuda 10.1 cudnn-7.6.5 @moorage @waleedka @PavlosMelissinos @haeric @rymalia 
"Hello guys,    I am training a custom dataset on a CPU. The training is completed successfully but when I try to visualize the results I get the following error message (check image):    !     My config class is as follows:    `class myMaskRCNNConfig(Config):      NAME = ""MaskRCNN_config""         # set the number of GPUs to use along with the number of images      # per GPU      GPU_COUNT = 1      IMAGES_PER_GPU = 1         # number of classes (we would normally add +1 for the background)      # anchor + BG      NUM_CLASSES = 1+1           # Number of training steps per epoch      STEPS_PER_EPOCH = 10            # Skip detections with < 70% confidence      # This should be changed       DETECTION_MIN_CONFIDENCE = 0.6`      I tried to change the following and retrain:    `IMAGE_RESIZE_MODE == 'square`    but then I got some other error.     Do I have to change something in the config class? Or should I resize all my images to a certain value? "
"I'm working with a large dataset, so when I generate the Dataset objects for training and validation, they take up a lot of memory.  This appears to be limiting my batch size, and I run out of memory when I try to train.  With other Keras models, I normally use an ImageDataGenerator() that loads in batches of images a bit at a time to save memory.    Is there a way to implement this in MRCNN?  Or is it included natively, and I just haven't noticed?"
does anyone know a command that can extract the perimeter of an image in the mask rcnn or opencv?
"I have been train model on ModaNet dataset and save weights on h5 file. I can load weights on python.  After training I save model (with loaded weights) to pb format, use tf.freeze_graph.   When I load saved model in C++, I can take all prediction, but this prediction not equal to prediction which I receive use python."
"Hello guys,    I am doing custom object detection and instance segmentation using mask rcnn.    Hardware:  1. Nvidia Geforce 1050  2. Dell G3 15    Software:  I have installed all the packages listed in requirements.txt with following changes:-  1. tensorflow-gpu == 1.5.0  2. keras == 2.0.8    I have also installed Cuda Toolkit 9.0 and cudnn.    But the training is stuck on epoch 1/5    !     I have checked the tensorflow-gpu usage by using  tf.Session() and I can see that it can find my nvidia GPU: device 0.   But it is not using the said gpu.    Can someone help me out?    Edit 1:   I have read a bunch of issues related to this topic but I couldn't find any solution.     I can see in my Task Manager that the GPU is not being utilised.     I am not even sure if the training is started.     I would really appreciate someone's help.      "
"I would like to find out whether it is better to use instance segmentation or object detection to classify vehicles and count them, in the case of traffic congestion.     From my experience, traffic congestion has a lot of occlusion for bounding box to be accurate, it may classify a car as a truck, and a truck as a car.  I have a relatively large datasets, approx 7000 - 10000 images, it may be better to just use object detection as it will be easier to manage as the dataset gets larger    Image example:   !       If anyone can give some input, that would be greatly appreciated.    Thanks  "
"Please, I'm trying to make an API, I want to put the loss value in a variable and send it to the client at each step"
"Hi,     I have an error when I try to run the demo.ipynb file. I guess it's due to modellib.MasrRCNN function (see error below).   My first guess is that it's due to tensorflow version that i'm using which is 2.2.0. But when I changed that to an older version 1.3.0, there's an incompatibility between keras and tensorflow as keras requires at least tensorflow version 2.2.  Can anyone help please?  Thanks,  ---------------------------------------------------------------------------  TypeError                                 Traceback (most recent call last)    in          1 # Create model object in inference mode.  ----> 2 model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)        3 # Load weights trained on MS-COCO        4 model.load_weights(COCO_MODEL_PATH, by_name=True)    ~\Documents\1_USMBA\sw\Mask_RCNN\mrcnn\model.py in __init__(self, mode, config, model_dir)     1835         self.model_dir = model_dir     1836         self.set_log_dir()  -> 1837         self.keras_model = self.build(mode=mode, config=config)     1838      1839     def build(self, mode, config):    ~\Documents\1_USMBA\sw\Mask_RCNN\mrcnn\model.py in build(self, mode, config)     1899         else:     1900             _, C2, C3, C4, C5 = resnet_graph(input_image, config.BACKBONE,  -> 1901                                              stage5=True, train_bn=config.TRAIN_BN)     1902         # Top-down Layers     1903         # TODO: add assert to varify feature map sizes match what's in config    ~\Documents\1_USMBA\sw\Mask_RCNN\mrcnn\model.py in resnet_graph(input_image, architecture, stage5, train_bn)      178     # Stage 1      179     x = KL.ZeroPadding2D((3, 3))(input_image)  --> 180     x = KL.Conv2D(64, (7, 7), strides=(2, 2), name='conv1', use_bias=True)(x)      181     x = BatchNorm(name='bn_conv1')(x, training=train_bn)      182     x = KL.Activation('relu')(x)    c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\keras\engine\base_layer.py in __call__(self, *args, **kwargs)      895           # Build layer if applicable (if the `build` method has been      896           # overridden).  --> 897           self._maybe_build(inputs)      898           cast_inputs = self._maybe_cast_inputs(inputs)      899     c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\keras\engine\base_layer.py in _maybe_build(self, inputs)     2414         # operations.     2415         with tf_utils.maybe_init_scope(self):  -> 2416           self.build(input_shapes)  # pylint:disable=not-callable     2417       # We must set also ensure that the layer is marked as built, and the build     2418       # shape is stored since user defined build functions may not be calling    c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\keras\layers\convolutional.py in build(self, input_shape)      161         constraint=self.kernel_constraint,      162         trainable=True,  --> 163         dtype=self.dtype)      164     if self.use_bias:      165       self.bias = self.add_weight(    c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\keras\engine\base_layer.py in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)      575         synchronization=synchronization,      576         aggregation=aggregation,  --> 577         caching_device=caching_device)      578     if regularizer is not None:      579       # TODO(fchollet): in the future, this should be handled at the    c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\training\tracking\base.py in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)      741         dtype=dtype,      742         initializer=initializer,  --> 743         **kwargs_for_getter)      744       745     # If we set an initializer and the variable processed it, tracking will not    c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\keras\engine\base_layer_utils.py in make_variable(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)      139       synchronization=synchronization,      140       aggregation=aggregation,  --> 141       shape=variable_shape if variable_shape else None)      142       143     c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\ops\variables.py in __call__(cls, *args, **kwargs)      257   def __call__(cls, *args, **kwargs):      258     if cls is VariableV1:  --> 259       return cls._variable_v1_call(*args, **kwargs)      260     elif cls is Variable:      261       return cls._variable_v2_call(*args, **kwargs)    c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\ops\variables.py in _variable_v1_call(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)      218         synchronization=synchronization,      219         aggregation=aggregation,  --> 220         shape=shape)      221       222   def _variable_v2_call(cls,    c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\ops\variables.py in  (**kwargs)      196                         shape=None):      197     """"""Call on Variable class. Useful to force the signature.""""""  --> 198     previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)      199     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access      200       previous_getter = _make_getter(getter, previous_getter)    c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\ops\variable_scope.py in default_variable_creator(next_creator, **kwargs)     2596         synchronization=synchronization,     2597         aggregation=aggregation,  -> 2598         shape=shape)     2599   else:     2600     return variables.RefVariable(    c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\ops\variables.py in __call__(cls, *args, **kwargs)      261       return cls._variable_v2_call(*args, **kwargs)      262     else:  --> 263       return super(VariableMetaclass, cls).__call__(*args, **kwargs)      264       265     c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)     1432           aggregation=aggregation,     1433           shape=shape,  -> 1434           distribute_strategy=distribute_strategy)     1435      1436   def _init_from_args(self,    c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)     1565           with ops.name_scope(""Initializer""), device_context_manager(None):     1566             initial_value = ops.convert_to_tensor(  -> 1567                 initial_value() if init_from_fn else initial_value,     1568                 name=""initial_value"", dtype=dtype)     1569           if shape is not None:    c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\keras\engine\base_layer_utils.py in  ()      119         (type(init_ops.Initializer), type(init_ops_v2.Initializer))):      120       initializer = initializer()  --> 121     init_val = lambda: initializer(shape, dtype=dtype)      122     variable_dtype = dtype.base_dtype      123   if use_resource is None:    c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\ops\init_ops_v2.py in __call__(self, shape, dtype)      556     else:      557       limit = math.sqrt(3.0 * scale)  --> 558       return self._random_generator.random_uniform(shape, -limit, limit, dtype)      559       560   def get_config(self):    c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\ops\init_ops_v2.py in random_uniform(self, shape, minval, maxval, dtype)     1066       op = random_ops.random_uniform     1067     return op(  -> 1068         shape=shape, minval=minval, maxval=maxval, dtype=dtype, seed=self.seed)     1069      1070   def truncated_normal(self, shape, mean, stddev, dtype):    c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\ops\random_ops.py in random_uniform(shape, minval, maxval, dtype, seed, name)      280     maxval = 1      281   with ops.name_scope(name, ""random_uniform"", [shape, minval, maxval]) as name:  --> 282     shape = tensor_util.shape_tensor(shape)      283     # In case of [0,1) floating results, minval and maxval is unused. We do an      284     # `is` comparison here since this is cheaper than isinstance or  __eq__.    c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\framework\tensor_util.py in shape_tensor(shape)     1013       # not convertible to Tensors because of mixed content.     1014       shape = tuple(map(tensor_shape.dimension_value, shape))  -> 1015   return ops.convert_to_tensor(shape, dtype=dtype, name=""shape"")     1016      1017     c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)     1339      1340     if ret is None:  -> 1341       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)     1342      1343     if ret is NotImplemented:    c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)      319                                          as_ref=False):      320   _ = as_ref  --> 321   return constant(v, dtype=dtype, name=name)      322       323     c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\framework\constant_op.py in constant(value, dtype, shape, name)      260   """"""      261   return _constant_impl(value, dtype, shape, name, verify_shape=False,  --> 262                         allow_broadcast=True)      263       264     c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)      268   ctx = context.context()      269   if ctx.executing_eagerly():  --> 270     t = convert_to_eager_tensor(value, ctx, dtype)      271     if shape is None:      272       return t    c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\framework\constant_op.py in convert_to_eager_tensor(value, ctx, dtype)       93     except AttributeError:       94       dtype = dtypes.as_dtype(dtype).as_datatype_enum  ---> 95   ctx.ensure_initialized()       96   return ops.EagerTensor(value, ctx.device_name, dtype)       97     c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\eager\context.py in ensure_initialized(self)      500       opts = pywrap_tfe.TFE_NewContextOptions()      501       try:  --> 502         config_str = self.config.SerializeToString()      503         pywrap_tfe.TFE_ContextOptionsSetConfig(opts, config_str)      504         if self._device_policy is not None:    c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\eager\context.py in config(self)      878     """"""Return the ConfigProto with all runtime deltas applied.""""""      879     # Ensure physical devices have been discovered and config has been imported  --> 880     self._initialize_physical_devices()      881       882     config = config_pb2.ConfigProto()    c:\users\youssef\anaconda3\envs\maskrcnn\lib\site-packages\tensorflow\python\eager\context.py in _initialize_physical_devices(self)     1167       self._physical_devices = [     1168           PhysicalDevice(name=d.decode(),  -> 1169                          device_type=d.decode().split("":"")[1]) for d in devs]     1170       # Construct the visible device list from all physical devices but ignore     1171       # XLA devices    TypeError: 'NoneType' object is not iterable  "
"Hi i am trying to detect paintings on walls. The paintings have frames my objective is to extract the painting in the frame and replace it with another painting.  When paintings are detected on walls they almost always have prospective distortions and they frequently have octagonal, pentagonal shapes or oval shapes.  Will Mask_RCNN work to find the outlines of the actual painting inside the frame if i create a dataset with paintings labels inside the frames?  Example of paintings i may want to extract from frames:                Obviously any suggestion is welcome!"
"Hello, here is the error message I am getting. I followed the instructions and installed COCO with Visual build tools 2015, not sure what the problems is as I did exactly what was required in the installation read me. Help is greatly appreciated, thanks!   !   !     "
"Hello guys,    I am currently using Matterport Mask RCNN to train on custom dataset.  I have annotated my images using labelme and then made respective changes to the Dataset Class.   As far as I can tell, the changes are working.     When I try to run the following command, I get an error:       Error:         I am using following versions:  Tensorflow: 2.2.0  Keras: 2.4.3    I would really appreciate your help with this.    Regards,  Yash"
"I have windows -10,cuda-10.1,cudnn,tensorflow==2.2.0,tensorflow_gpu==2.2.0,keras==2.3.1 All are installed but still the program runs on cpu   !   Can anyone help me out inthis? @moorage @waleedka @PavlosMelissinos @haeric @rymalia "
"I'm trying to run demo.py but got this error:  ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.    my configurations are like below, but I can't figure it out how to solve this. can anyone help me?    thanks        2020-08-21 01:10:32.698751: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/melodic/lib  2020-08-21 01:10:32.698776: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.    Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.7  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 1  IMAGE_CHANNEL_COUNT            3  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                93  IMAGE_MIN_DIM                  800  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           coco  NUM_CLASSES                    81  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  PRE_NMS_LIMIT                  6000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                1000  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001      2020-08-21 01:10:33.890387: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1  2020-08-21 01:10:33.916158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2020-08-21 01:10:33.916712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:   pciBusID: 0000:01:00.0 name: Quadro M1200 computeCapability: 5.0  coreClock: 1.148GHz coreCount: 5 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 74.65GiB/s  2020-08-21 01:10:33.916865: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/melodic/lib  2020-08-21 01:10:33.916981: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/melodic/lib  2020-08-21 01:10:33.917107: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/melodic/lib  2020-08-21 01:10:33.917230: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/melodic/lib  2020-08-21 01:10:33.917354: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/melodic/lib  2020-08-21 01:10:33.917483: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/melodic/lib  2020-08-21 01:10:33.917593: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/melodic/lib  2020-08-21 01:10:33.917603: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at   for how to download and setup the required libraries for your platform.  Skipping registering GPU devices...  2020-08-21 01:10:33.917811: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA  To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.  2020-08-21 01:10:33.923356: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2799925000 Hz  2020-08-21 01:10:33.923655: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5572b1fff640 initialized for platform Host (this does not guarantee that XLA will be used). Devices:  2020-08-21 01:10:33.923674: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version  2020-08-21 01:10:33.924852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:  2020-08-21 01:10:33.924864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]        Traceback (most recent call last):    File ""/home/rahim/anaconda3/envs/rahim/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py"", line 465, in _apply_op_helper      values = ops.convert_to_tensor(    File ""/home/rahim/anaconda3/envs/rahim/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 1499, in convert_to_tensor      ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)    File ""/home/rahim/anaconda3/envs/rahim/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py"", line 1502, in _autopacking_conversion_function      return _autopacking_helper(v, dtype, name or ""packed"")    File ""/home/rahim/anaconda3/envs/rahim/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py"", line 1437, in _autopacking_helper      constant_op.constant(elem, dtype=dtype, name=str(i)))    File ""/home/rahim/anaconda3/envs/rahim/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 263, in constant      return _constant_impl(value, dtype, shape, name, verify_shape=False,    File ""/home/rahim/anaconda3/envs/rahim/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 280, in _constant_impl      tensor_util.make_tensor_proto(    File ""/home/rahim/anaconda3/envs/rahim/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py"", line 444, in make_tensor_proto      raise ValueError(""None values not supported."")  ValueError: None values not supported.    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""/home/rahim/anaconda3/envs/rahim/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py"", line 483, in _apply_op_helper      observed = ops.convert_to_tensor(    File ""/home/rahim/anaconda3/envs/rahim/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 1499, in convert_to_tensor      ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)    File ""/home/rahim/anaconda3/envs/rahim/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py"", line 1502, in _autopacking_conversion_function      return _autopacking_helper(v, dtype, name or ""packed"")    File ""/home/rahim/anaconda3/envs/rahim/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py"", line 1437, in _autopacking_helper      constant_op.constant(elem, dtype=dtype, name=str(i)))    File ""/home/rahim/anaconda3/envs/rahim/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 263, in constant      return _constant_impl(value, dtype, shape, name, verify_shape=False,    File ""/home/rahim/anaconda3/envs/rahim/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 280, in _constant_impl      tensor_util.make_tensor_proto(    File ""/home/rahim/anaconda3/envs/rahim/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py"", line 444, in make_tensor_proto      raise ValueError(""None values not supported."")  ValueError: None values not supported.    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""demo.py"", line 74, in        model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)    File ""/home/rahim/Desktop/mask-rcnn/Mask_RCNN-master/mrcnn/model.py"", line 1837, in __init__      self.keras_model = self.build(mode=mode, config=config)    File ""/home/rahim/Desktop/mask-rcnn/Mask_RCNN-master/mrcnn/model.py"", line 2035, in build      fpn_classifier_graph(rpn_rois, mrcnn_feature_maps, input_image_meta,    File ""/home/rahim/Desktop/mask-rcnn/Mask_RCNN-master/mrcnn/model.py"", line 951, in fpn_classifier_graph      mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name=""mrcnn_bbox"")(x)    File ""/home/rahim/anaconda3/envs/rahim/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 925, in __call__      return self._functional_construction_call(inputs, args, kwargs,    File ""/home/rahim/anaconda3/envs/rahim/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1117, in _functional_construction_call      outputs = call_fn(cast_inputs, *args, **kwargs)    File ""/home/rahim/anaconda3/envs/rahim/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py"", line 539, in call      result = array_ops.reshape(    File ""/home/rahim/anaconda3/envs/rahim/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper      return target(*args, **kwargs)    File ""/home/rahim/anaconda3/envs/rahim/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py"", line 195, in reshape      result = gen_array_ops.reshape(tensor, shape, name)    File ""/home/rahim/anaconda3/envs/rahim/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 8233, in reshape      _, _, _op, _outputs = _op_def_library._apply_op_helper(    File ""/home/rahim/anaconda3/envs/rahim/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py"", line 486, in _apply_op_helper      raise ValueError(  ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.  "
"Hello,    I was going through the balloon.py script and In the BalloonDataset() class, in the load_balloon function, I printed out polygons and image_path just to check the output and I got some strange results.    For polygon, it printed out the x and y coordinates of the last image. Only the last image.  For image_path, it printed out the image path of the first image.     Does anyone know why this is happening ?     Regards,  Yash."
"I want to use the mask RCNN evaluator to benchmark on another model output. Therefore, I decided to extract the mask RCNN evaluator from [here][1]. The calculation of the mask comes from the earlier [function][2] and takes the following parameters        gt_boxes, gt_class_ids, gt_masks, pred_boxes, pred_class_ids, pred_scores, pred_masks    I want to remove all lines of codes related to `gt_masks` and `pred_masks` because I only want to compare bounding boxes and not masks.      Therefore, I replaced the original code from        def compute_matches(gt_boxes, gt_class_ids, gt_masks,                          pred_boxes, pred_class_ids, pred_scores, pred_masks,                          iou_threshold=0.5, score_threshold=0.0):          """"""Finds matches between prediction and ground truth instances.          Returns:              gt_match: 1-D array. For each GT box it has the index of the matched                        predicted box.              pred_match: 1-D array. For each predicted box, it has the index of                          the matched ground truth box.              overlaps: [pred_boxes, gt_boxes] IoU overlaps.          """"""          # Trim zero padding          # TODO: cleaner to do zero unpadding upstream          gt_boxes = trim_zeros(gt_boxes)          gt_masks = gt_masks[..., :gt_boxes.shape[0]]          pred_boxes = trim_zeros(pred_boxes)          pred_scores = pred_scores[:pred_boxes.shape[0]]          # Sort predictions by score from high to low          indices = np.argsort(pred_scores)[::-1]          pred_boxes = pred_boxes[indices]          pred_class_ids = pred_class_ids[indices]          pred_scores = pred_scores[indices]          pred_masks = pred_masks[..., indices]                # Compute IoU overlaps [pred_masks, gt_masks]          overlaps = compute_overlaps_masks(pred_masks, gt_masks)                # Loop through predictions and find matching ground truth boxes          match_count = 0          pred_match = -1 * np.ones([pred_boxes.shape[0]])          gt_match = -1 * np.ones([gt_boxes.shape[0]])          for i in range(len(pred_boxes)):              # Find best matching ground truth box              # 1. Sort matches by score              sorted_ixs = np.argsort(overlaps[i])[::-1]              # 2. Remove low scores              low_score_idx = np.where(overlaps[i, sorted_ixs]   0:                  sorted_ixs = sorted_ixs[:low_score_idx[0]]              # 3. Find the match              for j in sorted_ixs:                  # If ground truth box is already matched, go to next one                  if gt_match[j] > -1:                      continue                  # If we reach IoU smaller than the threshold, end the loop                  iou = overlaps[i, j]                  if iou   -1:                      continue                  # If we reach IoU smaller than the threshold, end the loop                        # Do we have a match?                  if pred_class_ids[i] == gt_class_ids[j]:                      match_count += 1                      gt_match[j] = i                      pred_match[i] = j                      break                return gt_match, pred_match      Is the modified code equivalent to the original code when it comes to computing the bounding box accuracy?      [1]:      [2]:  "
"hi am trying to test this implementation and the data that i have is not annotated, so i was wondering if i should annotate the test set, and please someone can explain to me when the test set should be annotated (is this has a relation with evaluating the model using Intersection over union)"
"I found this project    which is an extension of this one (@waleedka is one of the contributors) where they try and optimize the model with the help of knowledge distillation but it seems to have been abandoned so I decided to shoot my shot here, maybe someone could be of help    I'm having an issue with their mimic loss, they modified it to an l2-distance but it doesn't work. Could anyone take a look at it maybe??"
"Hi,     I have been running Mask RCNN so far on Colab but now as I have to do cross validation, I decided to run my experiments on a cluster due to usage restrictions on Colab. In the cluster, I first pip installed tensorflow-gpu in my conda environment but due to issues with cuda dependencies it wasn't running on gpu. So I ended up installing tf from conda (conda install tensorflow-gpu==1.15). For all other packages, I am still installing them with pip. And I am using exactly the same code and configs. Now I did a couple of runs on the cluster, but the results are way worse than the results I was getting in Colab (mAP@.50: 0.45 vs 0.68) and the loss seems quite different. Has anyone else had this issue? Can there be such a huge difference in results?    conda create -n maskrcnnenv python=3.6 anaconda  source activate maskrcnnenv  pip3 install -r requirements.txt  # installs all libraries except tf  conda install tensorflow-gpu==1.15 -y  python3 setup.py install    "
"TensorFlow 2.2  cuda 10.1  keras 2.3.1    I am going to multi-class using a balloon file. For 1 class, there is no problem.   However, there is a problem when trying to detect more than two multi class.  If you have any code with multi-class or any part that needs to be modified, please let me know.  The modified part is as follows.      class levelDataset(utils.Dataset):        def load_level(self, dataset_dir, subset):          """"""Load a subset of the level dataset.          dataset_dir: Root directory of the dataset.          subset: Subset to load: train or val          """"""          # Add classes according to the numbe of classes required to detect          self.add_class(""level"", 1, ""water"")          self.add_class(""level"", 2, ""slope"")          self.add_class(""level"", 3, ""bridge"")          self.add_class(""level"", 4, ""water_side"")            # Train or validation dataset?          assert subset in [""train"", ""val""]          dataset_dir = os.path.join(dataset_dir, subset)            # Load annotations          # VGG Image Annotator (up to version 1.6) saves each image in the form:          # { 'filename': '28503151_5b5b7ec140_b.jpg',          #   'regions': {          #       '0': {          #           'region_attributes': {},          #           'shape_attributes': {          #               'all_points_x': [...],          #               'all_points_y': [...],          #               'name': 'polygon'}},          #       ... more regions ...          #   },          #   'size': 100202          # }          # We mostly care about the x and y coordinates of each region          # Note: In VIA 2.0, regions was changed from a dict to a list.          annotations = json.load(open(os.path.join(dataset_dir, ""via_region_data.json"")))          annotations = list(annotations.values())  # don't need the dict keys            # The VIA tool saves images in the JSON even if they don't have any          # annotations. Skip unannotated images.          annotations = [a for a in annotations if a['regions']]            # Add images          for a in annotations:              # Get the x, y coordinaets of points of the polygons that make up              # the outline of each object instance. These are stores in the              # shape_attributes (see json format above)              # The if condition is needed to support VIA versions 1.x and 2.x.              polygons = [r['shape_attributes'] for r in a['regions']]              #labelling each class in the given image to a number                level = [s['region_attributes'] for s in a['regions']]                            num_ids=[]              #Add the classes according to the requirement              for n in level:                  try:                      if n['label']=='water':                          num_ids.append(1)                      elif n['label']=='slope':                          num_ids.append(2)                      elif n['label']=='bridge':                          num_ids.append(3)                      elif n['label']=='water_side':                          num_ids.append(4)                      except:                      pass"
"I have test one image(1000 x1200x3),but the speed is really much slow! It unbelievably takes about 3 mins(160 seconds) to inference one image. My inference device is GTX 1660super. I would be very grateful if someone could help me. "
None
"""**train.py**"" fistly goes well, and then:  well, well, well,UnboundLocalError!  Something like this:  ""**UnboundLocalError: local variable 'image_id' referenced before assignment**""  I'm a student in China who use Mask_RCNN for our lab project. This is my first time to use github, I don't know how to use it but I need some help. (I could not even open the 'Read the guide' webpage so I don't know what to do...) I wish somebody could help me about my probelm. According to what Anaconda Prompt shows, I guess something went wrong in ""train.py"" or ""model.py"". I'm just a student who have just studied computer and science for one year without learning Python. However, I must accomplish the task that my teacher assigned (training the Mask_RCNN with the data we've got) . And now I failed at the last step. I had successfully trained it with one kind of dataset ---- bridge. Now I have to train it with not only bridge, but also rivers, lakes, residential areas, roads and towns. This time I fail... Sincerely hope someone could help me about this problem and teach me how to use github.(like uploading my documents) Thanks!  (base) D:\Mask_RCNN\samples\All>python train.py  C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])  C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])  C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])  C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])  C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])  C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    np_resource = np.dtype([(""resource"", np.ubyte, 1)])  C:\ProgramData\Anaconda3\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])  C:\ProgramData\Anaconda3\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])  C:\ProgramData\Anaconda3\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])  C:\ProgramData\Anaconda3\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])  C:\ProgramData\Anaconda3\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])  C:\ProgramData\Anaconda3\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    np_resource = np.dtype([(""resource"", np.ubyte, 1)])  Using TensorFlow backend.    Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     10  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.7  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 10  IMAGE_CHANNEL_COUNT            3  IMAGE_MAX_DIM                  384  IMAGE_META_SIZE                19  IMAGE_MIN_DIM                  320  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [384 384   3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           shapes  NUM_CLASSES                    7  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  PRE_NMS_LIMIT                  6000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (48, 96, 192, 384, 768)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                10  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           100  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               10  WEIGHT_DECAY                   0.0001      train/label_val/bridge_1_json/img.png  train/label_val/bridge_10_json/img.png  train/label_val/bridge_100_json/img.png  train/label_val/bridge_101_json/img.png  train/label_val/bridge_102_json/img.png  train/label_val/bridge_103_json/img.png  train/label_val/bridge_104_json/img.png  train/label_val/bridge_105_json/img.png  train/label_val/bridge_106_json/img.png  train/label_val/bridge_107_json/img.png  train/label_val/bridge_108_json/img.png  train/label_val/bridge_109_json/img.png  train/label_val/bridge_11_json/img.png  ......  train/label_val/bridge_1_json/img.png  WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.    WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py:1354: add_dispatch_support. .wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.  Instructions for updating:  Use tf.where in 2.0, which has the same broadcast rule as np.where  WARNING:tensorflow:From D:\Mask_RCNN\mrcnn\model.py:554: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.    WARNING:tensorflow:From D:\Mask_RCNN\mrcnn\utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.    WARNING:tensorflow:From D:\Mask_RCNN\mrcnn\model.py:601: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.  Instructions for updating:  box_ind is deprecated, use box_indices instead  2020-08-11 00:01:34.701892: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2  2020-08-11 00:01:34.713004: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll  2020-08-11 00:01:34.749692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:  name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.77  pciBusID: 0000:0b:00.0  2020-08-11 00:01:34.756510: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.  2020-08-11 00:01:34.763634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0  2020-08-11 00:01:35.252071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:  2020-08-11 00:01:35.256724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0  2020-08-11 00:01:35.259855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N  2020-08-11 00:01:35.264368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8693 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:0b:00.0, compute capability: 7.5)    Starting at epoch 0. LR=0.001    Checkpoint Path: D:\Mask_RCNN\samples\All\logs\shapes20200811T0001\mask_rcnn_shapes_{epoch:04d}.h5  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  In model:  rpn_model      rpn_conv_shared        (Conv2D)      rpn_class_raw          (Conv2D)      rpn_bbox_pred          (Conv2D)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)  C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.    WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.    WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.    WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\keras\callbacks\tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.    WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\keras\callbacks\tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.    Epoch 1/30  Traceback (most recent call last):    File ""D:\Mask_RCNN\mrcnn\model.py"", line 1694, in data_generator      image_index = (image_index + 1) % len(image_ids)  ZeroDivisionError: integer division or modulo by zero    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""train.py"", line 257, in        layers='heads')    File ""D:\Mask_RCNN\mrcnn\model.py"", line 2377, in train      use_multiprocessing=True,    File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\legacy\interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\training.py"", line 1732, in fit_generator      initial_epoch=initial_epoch)    File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\training_generator.py"", line 185, in fit_generator      generator_output = next(output_generator)    File ""D:\Mask_RCNN\mrcnn\model.py"", line 1812, in data_generator      ### dataset.image_info[image_id]))  UnboundLocalError: local variable 'image_id' referenced `before` assignment"
"I have tried many times in many different Anaconda environments with many different combinations of Python, Tensorflow, Keras, and CUDA to try to get this to run with my GPU (GTX 1060), but I have never been successful.  Always some sort of build error, or import error.    I know how to install CUDA and the appropriate cuDNN packages.    I am on Windows 10 and I want to use an Anaconda environment for setup.    Does anybody have a combination of all of these that they know works today, with the current Mask RCNN?"
"Hello,    I am trying to implement the inference pipeline of Mask RCNN model.   Environment:  GPU  Cuda : 10.1   tensorflow: 2.2.0  python: 3.7.8    I have ported the matterplot's Mask RCNN implementation from keras to tf.keras and I get the following error:    ``tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.``  ``  (0) Invalid argument: Shapes of all inputs must match: values[0].shape = [81] != values[1].shape = [1000]``         ``  [[{{node mrcnn_detection/stack}}]]``          ` [[mrcnn_detection/map/while/body/_1/GatherV2/_4213]]`  ``  (1) Invalid argument: Shapes of all inputs must match: values[0].shape = [81] != values[1].shape = [1000]`           `[[{{node mrcnn_detection/stack}}]]``    This error trace backs to this, in ``fpn_classifier_graph()``:  ``x = KL.TimeDistributed(KL.Conv2D(1024, (7, 7), padding=""valid""), name=""mrcnn_class_conv1"")(x)``  Here x: `Tensor(""mrcnn_class_bn1/Reshape_1:0"", shape=(None, None, 1, 1, 1024), dtype=float32)`     When implemented the same using keras.layers, I get the following output,  ``x = KL.TimeDistributed(KL.Conv2D(1024, (7, 7), padding=""valid""), name=""mrcnn_class_conv1"")(x)  Here x: Tensor(""mrcnn_class_bn1/Reshape_1:0"", shape=(None, 1000, 1, 1, 1024), dtype=float32)``    The output shape differs from the Keras to tf.keras implementation. Is there any solution to this?    Thank you."
I've been getting this error and I cannot seem to fix it. Been going around in circles. Can someone help?    !   
"Open Images V6 has more category and more segmented images than COCO, so, native support for Open ImagesV6 could be helpful."
"Hello everyone,     I trained my model then when I was about to evaluate/inference it I get this error.    ValueError: You are trying to load a weight file containing 132 layers into a model with 131 layers.    I trained the model well with no problems. However, when I tried to use the inference it is giving me this error.    "
"Hi everybody.     I'm using Mask R-CNN on videos that contain a single object moving. I'm first dividing the video into frames and then treating each frame independently. In every frame, I have a single object belonging to a single class. Primarily, I want to detect the object in each frame and, secondarily, I want to retrieve the mask around that object as accurately as possible in every frame. This is already working more or less but I'd like to know how to tune the model's parameters in order to improve my results.     In particular, I have questions about the following parameters:  - **DETECTION_MAX_INSTANCES**: should this be set to 1 ?  - **DETECTION_MIN_CONFIDENCE**: originally 0.7, should this be set to a lower value ? (to increase the chance of detecting the object)  - **RPN_NMS_THRESHOLD**: originally 0.7, should this be increased ? (to increase the chance of detecting the object)  - **LOSS_WEIGHTS = {          ""rpn_class_loss"": 1.,          ""rpn_bbox_loss"": 1.,          ""mrcnn_class_loss"": 1.,          ""mrcnn_bbox_loss"": 1.,          ""mrcnn_mask_loss"": 1.      }**: should I change something here ? For example first training with a higher rpn_bbox_loss and a higher mrcnn_bbox_loss and then training subsequently with a higher mrcnn_mask_loss.    Additionally, is there any pre-processing or post-processing step that could be useful in this context ?     Any other suggestion on how to improve the accuracy of the process is of course welcome.    Thanks !"
I have implemented matterport mask rcnn. Now i want to improve the mask. How can i do that?
"  1. Since  , why RoI Align is **discrete** while PrRoI Pooling is **continuous** ?  2. Could anyone explain the intuition behind the derivative of PrPool()?  3. Why ""The process (FC layers) after ROI pooling does not share among ROI, and takes time, which makes RPN approaches slow"" ?  4. How does   solve this FC layer problem in question 3 above ?      !     !     "
I have trained my MaskRCNN model. Now I want to check the accuracy of my model. How can I check the accuracy of my model? Thank you.
"Hello guys,    I am wokring with Mask R CNN for object detection and segmentation.     I would like to connect the centers of the two bounding boxes in an image with a line segment and then measure the pixel distance of this line. But I am not sure how I can locate the centers of the bounding boxes as shown in the image below.     !     Can someone please help me out?    Thank You."
"Hello, recently I am building a network that can produce both masks and bounding box level captions.  I refer to the   and   which are all built based on Faster rcnn. I get rid of the bounding box and class ids parts for single object and add bounding box and captions for bounding box level captioning.    For training the model, I used two rpn model with exactly the same architecture and one for masking, another for captioning. **So for the two rpn models I got 4 losses: ""mask_rpn_score_loss, mask_rpn_bbox_loss, caption_rpn_score_loss, caption_rpn_bbox_loss"", all of the operations (data generator, rpn model architecture, loss function) are designed the same as that in mask rcnn.**    When I train the whole model with Adam optimizer with an initial learning rate as 0.001, I found that after training for several steps, **the caption_rpn_bbox_loss became nan suddenly, while other losses still decrease normally.** I cannot figure out what leads to this problem.     There may be no mistakes in computing rpn_gt_bbox, rpn_gt_scores, pred_rpn_bbox and pred_rpn_scores. So I guess maybe there are some dirty data in my dataset like zero size bounding box, but after checking I still cannot find any clues that prove there are dirty data in the dataset.    Below is the ipynb used to combine coco together with visual genome dataset and filter out invalid data.         Could anyone give any advice about how this nan loss occurs or how to solve it?"
I am getting very high scores for all the anchors
"An error occurred when I was running the demo as aboveã€‚  tryï¼šuse lower kerasï¼Œcheck the memory ã€‚but these didn't workã€‚  I hope you can help me solve this problemã€‚THxã€‚        Using TensorFlow backend.    Configurations:  BACKBONE_SHAPES                [[256 256]   [128 128]   [ 64  64]   [ 32  32]   [ 16  16]]  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.7  DETECTION_NMS_THRESHOLD        0.3  GPU_COUNT                      1  IMAGES_PER_GPU                 1  IMAGE_MAX_DIM                  1024  IMAGE_MIN_DIM                  800  IMAGE_PADDING                  True  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           coco  NUM_CLASSES                    81  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                1000  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001      2020-07-27 09:59:23.110946: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA  2020-07-27 09:59:26.243898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties:   name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62  pciBusID: 0000:83:00.0  totalMemory: 10.91GiB freeMemory: 10.75GiB  2020-07-27 09:59:26.243935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0  2020-07-27 09:59:26.650770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10409 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:83:00.0, compute capability: 6.1)  Processing 1 images  image                    shape: (375, 500, 3)         min:    0.00000  max:  255.00000  molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  151.10000  image_metas              shape: (1, 89)               min:    0.00000  max: 1024.00000  2020-07-27 09:59:31.962338: E tensorflow/stream_executor/cuda/cuda_dnn.cc:378] Loaded runtime CuDNN library: 7300 (compatibility version 7300) but source was compiled with 7004 (compatibility version 7000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.  2020-07-27 09:59:31.963367: F tensorflow/core/kernels/conv_ops.cc:717] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo (), &algorithms)     Process finished with exit code 134 (interrupted by signal 6: SIGABRT)  `"
"I'm running on images with very high-res (30k*40k) and using IMAGE_MAX_DIM = 1024 and IMAGE_RESIZE_MODE = ""square"". While inferencing, I'm getting correct ROI coordinates but no masks. All the mask instances are returning false. Does it have anything with resizing?"
I am a beginner in Mask-RCNN. The model predicts a 28x28 segmentation mask then converted to a binary mask of coordinates of the original image to superimpose on it. I cannot understand what code does that job? Kindly guide.
"I'm running into memory issues when I try to train on images with a lot of annotations(about 450 on a 3024x4032 image). The model function I'm getting the error is load_image_gt in model.py when it gets to line 1213:  mask, class_ids = dataset.load_mask(image_id)  Since it's loading the whole mask even with a ridiculous amount of RAM (416GB) I still run into the error around epoch 500. Are there any parameters I can change to not run into this issue, and not use up this much memory? Because I would like this to run on a GPU with 16GB of memory. The memory error is happening right before the resize function and I don't know how to get around not loading the mask."
"I am upgrading to TensorFlow 2.2 CUDA 10.1 and changing the code. But there is one problem.   There is no problem with the existing mask detection. However, there is a problem in training.   The h5 model is created but not recognized. Is it a version problem?   For reference, I did a training test in TensorFlow v1.14 and it worked.  If you have the same version, please advise."
"I tried to run the code on different GPUs and noticed that the network is extremely slow on RTX2080Ti. The GPU usage is close to 0% although memory has been allocated at the beginning. It seems that computation occurs on CPU.    For reference, I am using:   Ubuntu 18.04  Tensorflow 1.14  Cuda 10  CuDNN 7.5  Keras 2.2.5    This configuration works properly on GTX1080Ti cards. Can you please provide a way to reach descent performance on RTX cards as well?"
"Can we attach mask head portion of this network to any other bounding box algorithm let's say YOLO? It might need retraining, but is it possible?"
None
"I followed the   in the wiki in order to make Mask RCNN work with 1 channel images (instead of 3 channel images).  I uploaded the code in this  .  While the program works about 4 out 5 times, for some reason I occasionally get errors like these:         "
"I've been using matterport's Mask R-CNN to train on a custom dataset. However, there seem to be some parameters that i failed to correctly define because on practically all of the images, the bottom or top of the object's mask is cut off:  !   As you can see, the bounding box is fine since it covers the whole blade, but the mask seems to suddenly stop in a horizontal line on the bottom.    - The original images are downscaled to `IMAGE_MIN_DIM = IMAGE_MAX_DIM = 1024` using the `square` mode.   - `USE_MINI_MASK` is set to true with `MINI_MASK_SHAPE = (512, 512)` (somehow if i set it off, RAM gets filled and training chrashes).  - `RPN_ANCHOR_SCALES = (64, 128, 256, 512, 1024)` since the objects occupy a large space of the image.    Other than that the configuration parameters are the default ones. I have tried modifying others such as `DETECTION_NMS_THRESHOLD` or `RPN_NMS_THRESHOLD` but with no improvements.    On another hand, there is a stair-like effect on masks of larger and curvier objects such as this one (in addition to the bottom and top cut-offs):  !     It doesn't feel like the problem comes from the amount of training. These two predictions come from 6 epochs of 7000 steps per epoch (took around 17 hours). And the problem appears from early stage and persists along all the epochs.    Any idea on what changes to make ? "
"Hiï¼ŒI hope to get your adviceï¼Œ  There are 22 classes in my dataset. When I train, I use the pre trained coco weights as follows:  model.load_ weights(model_ path,  by_ name = True,  exclude=[""mrcnn_ class_ logits"", ""mrcnn_ bbox_ fc"", ""mrcnn_ bbox"", ""mrcnn_ mask""])ï¼Œ  But when I predict with weights trained on my own dataset, I got error like below:  ValueError: Layer #391 (named ""mrcnn_bbox_fc""), weight   has shape (1024, 324), but the saved weight has shape (1024, 88).    I am a beginner of CV, hope to get your help, thanks"
Hi All!  I'm a totally newbie to object detection  how can i do feature extraction in size of 4096 to images detected?  is it determined by the number of classes i have in the h5 file? 
"Hi all,    I am training a small dataset and suffering from overfitting. I already tried to tune hyperparameters, data augmentation, adding more data...but I still have issues with overfitting.    Consequently, I would like to use dropout for my training. I just want to add dropout to fc-layers but I am not sure how to do it.    1. To which method of model.py should I add dropout?  2. Can I add the following line to implement dropout? ""x = KL.Dropout(0.5)""  3. How can I proove that it is working properly?    I would be very grateful for your answers and thank you so much in advance :)  "
"I have trained Mask-RCCN on custom dataset where I had only 1 class. Firstly I took coco model weight and trained that on my datasets. So far so good, my custom model can detect that one custom class. But I could not figure a way to detect rest 80 classes(excluding 'BG' calss) from Coco. So my question is how can I train a model that can detect 1 class(Background) + 80 classes(from coco dataset) + 1 class(custom defined by me) ?    I tried to add all Coco classes in self.add_class method with their respective index and then added my own custom class assigned id = 82. But when I tried to train that, I found that loss is naN. So how can I achieve this to get a model which can detect my class + COCO classes? "
**Please explain in breif**
"I created a custom model in the mask R CNN and imported the images from the folder.  But there are some problems, so I ask a question.    1. I want to bring the contour in the form of a shape from the detected mask.  In other words, I want to get only the mask line of the detected data, is there a way? I am trying to do it using opencv or scikit, but it doesn't work. Any help would be appreciated.    real_test_dir = '../test'  image_paths =        2. If you have changed the image, you want to know if it is possible to change the outline. The images were extracted and detected as follows. I would like to extract the contour from here and export it as an image, but I would appreciate it if you let me know how.    for image_path in image_paths:      img = skimage.io.imread(image_path)      img_arr = np.array(img)      results = model.detect(   "
"I want to use Mask R-CNN to make instance segmentation on spectrogram images. As you may know, signals comming from the same source can make patterns in different part of the spectrogram simply because the source can emmit at different time.    My question is: do you think that Mask R-CNN be able to treat such an instance-segmentation problem, that is to say to make detection and be able to set the same instance number for several region in the image ?    "
"Multi-scale scaling of the input data results in more sized data, similar to the image pyramid. For example resize my image from [512*512]  to  [324*324],  [1024*1024], more date is created. And fpn fuse multi-level features with faster speed, due to calculating on the feature map. So whether multi-scale data enhancement method is replaced by fpn, and  there is no need  using the    data augmentation method for maskrcnn?     In addition,  dose maskrcnn require the input size of image fixed?"
"I trained my mask-rcnn model with two classes and now I would like to calculate the average Mean Precision (mAP) of all classes, so I know that I need to calculate the mAP of each class and then make an average to obtain the general mAP . For that, I modified compute_matches as suggested in: #2136 and my code in utils.py was as follows:         To use it I did it as follows:         However I am getting the following error, on the line :         **Could someone please help me with this? I would be very, very grateful, this is an important job for my academic life, and unfortunately I can't understand where I'm going wrong.  Thank you in advance for your attention.**"
"I have a dataset folder which contains three folders named images, tags, masks. Each of these folders contains images. For example:  In images:     In tags:     In masks:       So, I have the bounding box region cropped and a mask from that cropped region. All these are in image format.    How do I prepare this dataset to use for training on a pretrained Mask-RCNN model?"
"I have updated to tensorflow 2.2. I trained with single GPU successfully, but when I run with multi-GPU using parallel-model.py. It showed error like bellow. Have anyone success with tensorflow 2.2 for multi-GPU?    Additionally, in order to use tensorflow 2.2. I need to change all pure keras function to tensorflow.keras function.            "
"Hi,    I know that multi-class segmentation is possible with this repository. My question is regarding multi-label, where same area can be tagged with 2 labels. I am breaking down the code and in the process of modifying it for this use case, but I wanted to know if at all this will work in the end? Technically it should, but what are the challenges while inferencing or training that might not be intuitive?  @waleedka - It would be great to hear what you think.    Thanks,  Aishwarya"
I have an annotated dataset which consists of 1k+ json files. I want to train using those datasets. Each file contains some code as stated below:      How do I proceed to start training using pre-trained model? I have looked into the demo.ipynb and train_shapes.ipynb. I got a rough idea about how it works. I am trying to learn more about it.
"The server has 48 CPUs, but when I run the code, I find that only one CPU is used. How can I use all 48 CPUs?"
"Please, I'm looking for The line that displays:     "
"  # Which weights to start with?  init_with = ""coco""  # imagenet, coco, or last    if init_with == ""imagenet"":      model.load_weights(model.get_imagenet_weights(), by_name=True)  elif init_with == ""coco"":      # Load weights trained on MS COCO, but skip layers that      # are different due to the different number of classes      # See README for instructions to download the COCO weights      model.load_weights(COCO_MODEL_PATH, by_name=True,                         exclude=[""mrcnn_class_logits"", ""mrcnn_bbox_fc"",                                   ""mrcnn_bbox"", ""mrcnn_mask""])  elif init_with == ""last"":      # Load the last model you trained and continue training      model.load_weights(model.find_last(), by_name=True)  ---------------------------------------------------------------------------    AttributeError                            Traceback (most recent call last)      in         10     model.load_weights(COCO_MODEL_PATH, by_name=True,       11                        exclude=[""mrcnn_class_logits"", ""mrcnn_bbox_fc"",   ---> 12                                 ""mrcnn_bbox"", ""mrcnn_mask""])       13 elif init_with == ""last"":       14     # Load the last model you trained and continue training    ~/Desktop/Mask_RCNN-master/mrcnn/model.py in load_weights(self, filepath, by_name, exclude)     2141      2142         if by_name:  -> 2143             saving.load_weights_from_hdf5_group_by_name(f, layers)     2144         else:     2145             saving.load_weights_from_hdf5_group(f, layers)    AttributeError: module 'keras.engine.saving' has no attribute 'load_weights_from_hdf5_group_by_name'"
"    **Issue**   Has anyone faced anything similar to this?  While training, system memory i.e ram and swap memory consumption increases with each epoch.  It keeps increasing until it is out of memory and then gives out memory error and exits.  for eg     Is this to be expected or do we need to change something here?    GPU - Quadro RTX 8000 (48 Gb)  System memory - 128 Gb  Swap memory - 70Gb  `  Config :        GPU_COUNT = 1        IMAGES_PER_GPU = 18        STEPS_PER_EPOCH = 1000        NUM_CLASSES = 1 + 100  # Override in sub-classes        LEARNING_RATE = 0.005      LEARNING_MOMENTUM = 0.9        VALIDATION_STEPS = 50        IMAGE_MIN_DIM = 512      IMAGE_MAX_DIM = 512        WEIGHT_DECAY = 0.01        GRADIENT_CLIP_NORM = 5.0        BACKBONE = ""resnet101""        COMPUTE_BACKBONE_SHAPE = None        BACKBONE_STRIDES = [4, 8, 16, 32, 64]        FPN_CLASSIF_FC_LAYERS_SIZE = 1024        TOP_DOWN_PYRAMID_SIZE = 256        RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)        RPN_ANCHOR_RATIOS = [0.5, 1, 2]        RPN_ANCHOR_STRIDE = 1        RPN_NMS_THRESHOLD = 0.7        RPN_TRAIN_ANCHORS_PER_IMAGE = 256            PRE_NMS_LIMIT = 6000        POST_NMS_ROIS_TRAINING = 2000      POST_NMS_ROIS_INFERENCE = 1000        USE_MINI_MASK = True        MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask        IMAGE_MIN_SCALE = 0        IMAGE_CHANNEL_COUNT = 3        MEAN_PIXEL = np.array([123.7, 116.8, 103.9])        TRAIN_ROIS_PER_IMAGE = 200        ROI_POSITIVE_RATIO = 0.33        POOL_SIZE = 7      MASK_POOL_SIZE = 14        MASK_SHAPE = [28, 28]        MAX_GT_INSTANCES = 40\        DETECTION_MAX_INSTANCES = 100        RPN_BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2])        BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2])        USE_RPN_ROIS = True        TRAIN_BN = False         MULTI_PROCESSING = ""True""        WEIGHT = ""coco""        LAYERS = ""3+""  `    "
None
"I have trained the MaskRCNN for my own dataset containing a single class. Even though I reduce the DETECTION_MIN_CONFIDENCE = 0.7, my results have a confidence score of more than 90%. How to resolve this issue?? @waleedka "
"Hi, Developer community,  I hope you all are in good health. I am using Mask RCNN to build an application and having problems with a version mismatch. because some other things I found online are in TensorFlow 2.x while Mask RCNN is TensorFlow 1.x. It would be great if you'll consider it to provide it in TensorFlow 2.x also.  Thanks"
"Main folder  !     Meta  {      ""classes"":        train1.jpeg.json  {      ""description"": """",      ""tags"": [],      ""size"": {          ""height"": 150,          ""width"": 150      },      ""objects"": [          {              ""id"": 458095953,              ""classId"": 1528218,              ""description"": """",              ""geometryType"": ""polygon"",              ""labelerLogin"": ""shepherd25"",              ""createdAt"": ""2020-06-06T13:58:24.814Z"",              ""updatedAt"": ""2020-06-06T13:58:54.280Z"",              ""tags"": [],              ""classTitle"": ""Handgun"",              ""points"": {                  ""exterior"": [                      [                          36,                          1                      ],                      [                          34,                          6                      ],                      [                          31,                          8                      ],                      [                          34,                          14                      ],                      [                          25,                          91                      ],                      [                          28,                          99                      ],                      [                          28,                          105                      ],                      [                          43,                          110                      ],                      [                          68,                          110                      ],                      [                          70,                          99                      ],                      [                          70,                          99                      ],                      [                          76,                          98                      ],                      [                          94,                          19                      ],                      [                          102,                          12                      ],                      [                          99,                          0                      ]                  ],                  ""interior"": []              }          }      ]  }    I have been working on color splash file which uses VIA annotator  # Load annotations          # VGG Image Annotator (up to version 1.6) saves each image in the form:          # { 'filename': '28503151_5b5b7ec140_b.jpg',          #   'regions': {          #       '0': {          #           'region_attributes': {},          #           'shape_attributes': {          #               'all_points_x': [...],          #               'all_points_y': [...],          #               'name': 'polygon'}},          #       ... more regions ...          #   },          #   'size': 100202          # }          # We mostly care about the x and y coordinates of each region          # Note: In VIA 2.0, regions was changed from a dict to a list.          annotations = json.load(open(os.path.join(dataset_dir, ""via_region_data.json"")))          annotations = list(annotations.values())  # don't need the dict keys            # The VIA tool saves images in the JSON even if they don't have any          # annotations. Skip unannotated images.          annotations = [a for a in annotations if a['regions']]            # Add images          for a in annotations:              # Get the x, y coordinaets of points of the polygons that make up              # the outline of each object instance. These are stores in the              # shape_attributes (see json format above)              # The if condition is needed to support VIA versions 1.x and 2.x.              if type(a['regions']) is dict:                  polygons = [r['shape_attributes'] for r in a['regions'].values()]              else:                  polygons = [r['shape_attributes'] for r in a['regions']]                # load_mask() needs the image size to convert polygons to masks.              # Unfortunately, VIA doesn't include it in JSON, so we must read              # the image. This is only managable since the dataset is tiny.              image_path = os.path.join(dataset_dir, a['filename'])              image = skimage.io.imread(image_path)              height, width = image.shape[:2]                self.add_image(                  ""handgun"",                  image_id=a['filename'],  # use file name as a unique image id                  path=image_path,                  width=width, height=height,                  polygons=polygons)    "
"I try to use keypoint detection to detect the vertexes of shapes,but the performance is not good.Can anybody give me som idea? "
"I have used ssh to connect the cloud server and use screen tool to open a new terminal for training model. But It stops almost every hour, hence I have to manually resume training.    I also want to have a try that when I woke up the next morning, I could see the results of the model's overnight training    Is there anyone who can explain why? and how to solve this problem?"
"when I run the command coco.pu for evaluating my dataset i find this error      Using TensorFlow backend.  Command:  evaluate  Model:  last  Dataset:  /content/Mask_RCNN/assets/  Year:  2014  Logs:  /content/logs  Auto Download:  False    Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 1  IMAGE_CHANNEL_COUNT            3  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                93  IMAGE_MIN_DIM                  800  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           coco  NUM_CLASSES                    81  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  PRE_NMS_LIMIT                  6000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                1000  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001      WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.  Instructions for updating:  If using Keras pass *_constraint arguments to layers.  WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py:341: The name tf.log is deprecated. Please use tf.math.log instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py:399: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.  Instructions for updating:  Use tf.where in 2.0, which has the same broadcast rule as np.where  WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py:423: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.  Instructions for updating:  box_ind is deprecated, use box_indices instead  WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py:758: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py:760: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.  Instructions for updating:  Use `tf.cast` instead.  Traceback (most recent call last):    File ""samples/coco/coco.py"", line 465, in        model_path = model.find_last()    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 2073, in find_last  StopIteration"
Is there any configuration to be done so NMS is performed between classes as well?
"Processing 1 images  image                    shape: (600, 300, 4)         min:   23.00000  max:  255.00000  uint8  Traceback (most recent call last):    File ""blur.py"", line 371, in        video_path=args.video)    File ""blur.py"", line 236, in detect_and_color_splash      r = model.detect([image], verbose=1)[0]    File ""D:\Projects\Aidc\Mask_RCNN\mrcnn\model.py"", line 2503, in detect      molded_images, image_metas, windows = self.mold_inputs(images)    File ""D:\Projects\Aidc\Mask_RCNN\mrcnn\model.py"", line 2402, in mold_inputs      molded_image = mold_image(molded_image, self.config)    File ""D:\Projects\Aidc\Mask_RCNN\mrcnn\model.py"", line 2803, in mold_image      return images.astype(np.float32) - config.MEAN_PIXEL  ValueError: operands could not be broadcast together with shapes (1024,1024,4) (3,)"
"Mask RCNN hasn't been updated in recent months, and only works with TF versions up to 1.15.  I notice that lots of warnings show up every time I train a model, due to deprecation of TensorFlow functions (see below).       Are there any plans to change the calls to thesed deprecated functions, and to allow the use of Tensorflow 2 versions?"
I am trying to do a project on mask rcnn. I have two different data set for the same object. After training on those separate dataset I have found two .h5 file. is it possible to combine these two .h5 file into one to detect object?
It's useless now    old versions tf is in dependence with cython or cpython   (getting error `is not a supported wheel on this platform.`)    `pip install --ignore-installed --upgrade      doesn't work in virtual environment
"         that's pretty annoying, especially on Arch Linux, bleeding edge, remember?    "
None
None
"For my specific task, I would like to exclude those detections that are touching the edges of my picture. Since my pictures are rectangular, padding is being added to them and this makes it difficult for me to find the coordinates of those detections  touching the padding on the top and the bottom. Any ideas how to tackle this?  On the picture, I would like to exclude the detection with yellow mask and bounding box.   !   "
"My layer losses are not appending to metrics and I wonder why. This happens when compiling the model:  !     left: output  right:      Really weird because   is a list and appending should work.     This also causes that I am not seeing these losses in my tensorboard or at the progress bar.    EDIT: found out what the problem is, but not sure why it is in the source code.   is not a list, but a property that outputs a list. But why would it work for other people?"
"I am using the Mask RCNN network to detect a single object class in time-lapse images. I am running the detection on a single RTX2080 GPU, on which I have also done the training. The backbone is ResNet101. I have tried ResNet50 but the detections produced are not very good.    I am experiencing the issue that the inference is quite slow. The inference for the first image usually takes more than 4 minutes, and the following images probably take around 15 seconds per image. I need to run the detections on a very large number of images (100k's), so I would really like to speed this up.    The images I am processing are 6080x3420 and around 5 mb.    Is there any way to speed up the inference time? I have formulated some more specific questions below.    1. Is the inference time dependent on the size of the image?  Meaning, will the inference be faster if I run it on downscaled images. I am asking because, to my knowledge, the images are downscaled in the script itself, right? So I am not sure there would be any gain in downscaling images beforehand, as this also takes time?     2. If the inference time is faster on downscaled images, will I need to train on images of the same downscaled dimensions as well? Since the images are downscaled during training anyway?    3. The inference time is dependent on the number of objects that are detected in an image. Is there any way to speed this up? I have some images that contain 40-60 objects, and the inference can take around a minute for these.    4. I read somewhere that the detections are done on the CPU. Is that right? I have monitored the CPU usage while running the detections and I have no increase here. When monitoring the GPU I do see an increase, but it is just short peaks of 15-20% (I assume every time an image is processed) and then back to zero.    5. For this project, I am actually not using the masks, just the bounding boxes. Is there any way of exploiting this to speed up inference? I would like to still use the Mask RCNN network and not the Faster RCNN, but maybe there is something I could comment out in the code?    The code I use for the detection is given below:       "
"Hi,  I am experimenting the MaskRCNN model for quite sometime. I would like to alter some layers and check the performance.    So is it fine If I use the pre-trained weights, which will be weights developed on different layers since I am changing the layers.  or can I Train without pre-trained from the scratch? Does this repository support that?"
"so my mask rcnn model has successfully trained on my custom dataset, in google colab but the problem occurs, that when I run the command    !python3 /content/car-damage-detector/custom.py splash --weights=/content/logs/damage20200603T0536/mask_rcnn_damage_0010.h5 --image=/content/car-damage-detector/dataset/test/11.jpg    on the test image, the splash image is being generated but without the mask on it. i dont get why because the model has trained successfully here is the link to my colab notebook     I would really appreciate any help."
"I have hunch that, there is some thread or tutorial for this but i need specific help from you.     So i started on Balloon sample and i modify it in Balloon def.     I have rewrite the section with getting polygons and num_ids because i got issue with Interation in list and Dictionary and i have multiple object in img as you can see in JSON.     So i got separated in 2D [for img][for objects] list all_points_x and y with num_ids in 1D. Of course it did not work in the load_mask def. So i modify it too as you can see down below. And my problem is when i try to look at the mask .. it show only white ..no spots nothing. And even when i try to show my mask after i fill it with 0s i should be black but it is not. It is still white.     So my question is how do i ""draw"" on the image. I'm i doing it right and it is problem in my showing ? Is my way of solving this issue wrong ? Another problem is that i have 1920x1080 resolution (desktop) and it will be fun to get it working but that is another thing to solve and i don't want to mix it with this.         --------Balloon def---------  `    def load_balloon(self, dataset_dir, subset):          """"""Load a subset of the Balloon dataset.          dataset_dir: Root directory of the dataset.          subset: Subset to load: train or val          """"""          # Add classes. We have only one class to add.          self.add_class(""RS3"", 1, ""dig_site"")          self.add_class(""RS3"", 2, ""person"")          self.add_class(""RS3"", 3, ""uncovered_dig_site"")          self.add_class(""RS3"", 4, ""deposit"")          self.add_class(""RS3"", 5, ""dig_site_a"")            # Train or validation dataset?          assert subset in [""test"",""train""]          dataset_dir = os.path.join(dataset_dir, subset)          # Load annotations          # VGG Image Annotator saves each image in the form:          # { 'filename': '28503151_5b5b7ec140_b.jpg',          #   'regions': {          #       '0': {          #           'region_attributes': {},          #           'shape_attributes': {          #               'all_points_x': [...],          #               'all_points_y': [...],          #               'name': 'polygon'}},          #       ... more regions ...          #   },          #   'size': 100202          # }          # We mostly care about the x and y coordinates of each region          if(subset=='train'):              annotations = json.load(open('E:/Datasets/RS3/Archeology/annots/via_region_data_train.json'))          else:              annotations = json.load(open('E:/Datasets/RS3/Archeology/annots/via_region_data_test.json'))          annotations = list(annotations.values())  # don't need the dict keys            # The VIA tool saves images in the JSON even if they don't have any          # annotations. Skip unannotated images.            annotations = [a for a in annotations if a['regions']]            for i1 in range(len(annotations)):              polygons = []              num_ids = []              filename =  annotations[i1]['filename']              i3 = annotations[i1]['regions']              for i4 in range (len(i3)):                  polygons_row = []                  polygons_row.append(annotations[i1]['regions'][list(annotations[i1][""regions""])[i4]]['shape_attributes'])                  polygons.append(polygons_row)                  n=annotations[i1]['regions'][list(annotations[i1][""regions""])[i4]]['region_attributes']['names']                  try:                      if n=='dig_site':                          num_ids.append(1)                      elif n=='person':                          num_ids.append(2)                      elif n=='uncovered_dig_site':                          num_ids.append(3)                      elif n =='deposit':                          num_ids.append(4)                      elif n =='dig_site_a':                          num_ids.append(5)                  except:                      pass              image_path = os.path.join(dataset_dir, filename)              image = skimage.io.imread(image_path)              (height, width) = image.shape[:2]              self.add_image(                  'object',                  image_id=filename,                  path=image_path,                  width=width,                  height=height,                  polygons=polygons,                  num_ids=num_ids)          # also change the return value of def load_mask()          num_ids = np.array(num_ids, dtype=np.int32)`    ------------load_mask----------    `    def load_mask(self, image_id):          """"""Generate instance masks for an image.         Returns:          masks: A bool array of shape [height, width, instance count] with              one mask per instance.          class_ids: a 1D array of class IDs of the instance masks.          """"""          # If not a bottle dataset image, delegate to parent class.          image_info = self.image_info[image_id]          if image_info[""source""] != ""object"":              return super(self.__class__, self).load_mask(image_id)            # Convert polygons to a bitmap mask of shape          # [height, width, instance_count]          info = self.image_info[image_id]          if info[""source""] != ""object"":              return super(self.__class__, self).load_mask(image_id)          num_ids = info['num_ids']          mask = np.zeros([info[""height""], info[""width""], len(info[""polygons""])],                          dtype=np.uint8)          print(mask)          print(mask.shape)          io.imshow(mask)          plt.show()          for i in range(len(info[""polygons""])):                rr, cc = skimage.draw.polygon(info[""polygons""][i][0]['all_points_y'], info[""polygons""][i][0]['all_points_x'])                mask[rr, cc, i] = 1            io.imshow(mask)          plt.show()          # Return mask, and array of class IDs of each instance. Since we have          # one class ID only, we return an array of 1s          # Map class names to class IDs.          num_ids = np.array(num_ids, dtype=np.int32)          return mask, info['class_ids']`    ---------JSON file only for 1 img--------    `{    ""1.jpg366644"": {      ""fileref"": """",      ""size"": 366644,      ""filename"": ""1.jpg"",      ""base64_img_data"": """",      ""file_attributes"": {},      ""regions"": {        ""0"": {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [              786,              821,              856,              872,              863,              863,              799,              778,              786            ],            ""all_points_y"": [              474,              498,              486,              480,              437,              412,              426,              438,              474            ]          },          ""region_attributes"": {            ""names"": ""dig_site""          }        },        ""1"": {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [              965,              1022,              1049,              1063,              1074,              1060,              1047,              1021,              1013,              1004,              965            ],            ""all_points_y"": [              470,              513,              493,              469,              454,              445,              429,              418,              422,              418,              470            ]          },          ""region_attributes"": {            ""names"": ""dig_site""          }        },        ""2"": {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [              942,              928,              931,              927,              941,              978,              981,              1005,              1022,              1017,              1032,              1020,              1001,              984,              971,              957,              957,              942            ],            ""all_points_y"": [              594,              617,              631,              642,              654,              677,              688,              661,              630,              619,              618,              603,              591,              580,              568,              572,              590,              594            ]          },          ""region_attributes"": {            ""names"": ""dig_site""          }        },        ""3"": {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [              1473,              1516,              1522,              1502,              1478,              1469,              1465,              1473            ],            ""all_points_y"": [              490,              496,              470,              445,              438,              450,              469,              490            ]          },          ""region_attributes"": {            ""names"": ""uncovered_dig_site""          }        },        ""4"": {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [              1698,              1698,              1709,              1724,              1735,              1745,              1732,              1698            ],            ""all_points_y"": [              400,              419,              433,              438,              430,              412,              404,              400            ]          },          ""region_attributes"": {            ""names"": ""uncovered_dig_site""          }        },        ""5"": {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [              950,              950,              951,              977,              986,              975,              959,              950            ],            ""all_points_y"": [              523,              528,              552,              558,              543,              517,              512,              523            ]          },          ""region_attributes"": {            ""names"": ""person""          }        }      }    }  `    "
None
"I'd like to test on other trained datasets such as cityscapes, is there any .h5 file available?  Thank you very much!"
"I have recently updated my TensorFlow to 2.2 for working with GPU support but after upgrading my maskRCNN program start showing many kinds of errors.    As per my little experience with deep learning, I solved most issues bu is stuck with this one error which I'm getting and is not able to solve it.   123     I have been such with the problem and I really need help. Someone please help me with the issue  "
"I have followed steps on web to creating my own dataset with 4 classes: car, street, person, sign  But when I applyied model  in video, I found some problem with the segmentation. It's not a constant segmentation like every video about Mask RCNN I saw on youtube. Am I doing something wrong?   I'm sorry for my poor English. Thanks for your help  !   !   "
" I changed DETECTION_MIN_CONFIDENCE = 0.but when the model detect ,the all score>0.5.  I want to know what should i do that the score between 0.1 and 0.5 can display?"
"When I train my own model and export it to serving. I get the following signature def         Heres the script that I use to generate tf-serving model:      When i tried to deploy MASKRCNN model   to google ai platform, I get this error:         The cause of this error is: The inputs to the model are defined as unknown batch size (-1). So the same needs to be true for the outputs. Example: mrcnn_detection:/Reshape_1 should be (-1, 100, 6)     How can I change the output batch size ?"
"Hello!           When i try to train Mask_RCNN with balloon dataset i have the following error:    Caused by op 'fpn_p3/convolution', defined at:    File ""balloon.py"", line 331, in        model_dir=args.logs)    File ""C:\Program Files\Python36\lib\site-packages\mrcnn\model.py"", line 1832, in __init__      self.keras_model = self.build(mode=mode, config=config)    File ""C:\Program Files\Python36\lib\site-packages\mrcnn\model.py"", line 1919, in build      P3 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=""SAME"", name=""fpn_p3"")(P3)    File ""C:\Program Files\Python36\lib\site-packages\keras\engine\topology.py"", line 617, in __call__      output = self.call(inputs, **kwargs)    File ""C:\Program Files\Python36\lib\site-packages\keras\layers\convolutional.py"", line 168, in call      dilation_rate=self.dilation_rate)    File ""C:\Program Files\Python36\lib\site-packages\keras\backend\tensorflow_backend.py"", line 3332, in conv2d      data_format=tf_data_format)    File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 751, in convolution      return op(input, filter)    File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 835, in __call__      return self.conv_op(inp, filter)    File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 499, in __call__      return self.call(inp, filter)    File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 187, in __call__      name=self.name)    File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py"", line 630, in conv2d      data_format=data_format, name=name)    File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper      op_def=op_def)    File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 2956, in create_op      op_def=op_def)    File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 1470, in __init__      self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access    InternalError (see above for traceback): cuDNN launch failure : input shape( ]]            ]]      I have NVidia GRID K1 GPU. Do you have any solution? tf is 1.4 for this CUDA and CUDNN  "
None
"Getting this error while running inspect_model     ---------------------------------------------------------------------------  IndexError                                Traceback (most recent call last)    in          1 # Get detection class IDs. Trim zero padding.        2 det_class_ids = mrcnn['detections'][0, :, 4].astype(np.int32)  ----> 3 det_count = np.where(det_class_ids == 0)[0][0]        4 det_class_ids = det_class_ids[:det_count]        5 detections = mrcnn['detections'][0, :det_count]    IndexError: index 0 is out of bounds for axis 0 with size 0  ---------------------------------------------------------------------------      "
"Hi, I'm using augmentation to avoid overfitting can someone tell how to find the size of the dataset   after applying the augmentation technique?"
- Can anybody help me with how to deploy the Mask-RCNN model in a web application?    - I want to upload an image and display the segmented image back on the web page. But I'm not sure of how to deploy the neural network in the web application. Please help.
"Hi, I am trying to run detection on multiple images on multiple GPUs in parallel. Currently there are two issues. Could anyone please help me look into this? I am very confused about these situations.    1. Runtime on single image is not reduced     This would return 0.60s when IMAGES_PER_GPU = 4  This would return 0.26s when IMAGES_PER_GPU = 2  This would return 0.15s when IMAGES_PER_GPU = 1    2. It would fails when GPU_COUNT > 1     This would throw exception   "
I can't find any file which has all the steps to install the requirements and proceed. I am new to using google colab. Please help in how to setup and run this project on colab 
"Hi everyone,     I use augmentation as follows:         and the errors happen with the input array, mask:            Hope you guys can help me,"
I tried the visualize.py solution and also some other solutions as well but notihng is working. Can somebody guide me in this
"Hi, I am trying to calculate the Dice loss for a test set but I don't know how, and I am also new to this library. Can anyone help me ? Thank you very much. "
"Hi,  I have this error when using GPU:    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in _make_op(self, inputs)     2841   def _make_op(self, inputs):     2842     inputs = nest.flatten(inputs)  -> 2843     graph = inputs[0].graph     2844     node_def = self._make_node_def(graph)     2845     with graph.as_default():    IndexError: list index out of range    Although the error doesnÂ´t mention it, I donÂ´t get the error when simulating on CPU, but only when using parallel_model.py. Does somebody know how to solve it or what to change in parallel_model.py to make it work?      "
"I try to convert to tf2.2 version of Mask_rcnn, but following issue occured:  !   when I modify the add_loss according to  ï¼Œthis error occured  ï¼Œ but using `tf.compat.v1.disable_eager_execution()`  is not a good choice    does anyone know how to fix it?"
"I first want to say thanks to @waleedka for sharing this work!    I wonder if anyone has succesfully extened MaskRCNN to Affordance Labelling? More specifically, I am now looking for an easy way to generate data for an object like a hammer which has a label of 1 for the handle (grasp) and 2 for the head (pound). Similar to  .    !     I have used UE4 and ndds to train MaskRCNN on a custom object and generated json files using   My dataset included 15k/3.75k synthetic images for training/validation (80/20 split). I also used image augmentation (e.g. flip left/right, up/down & roate 90deg) and included an ouput of my training below.    This authors of   used 30k images and manually labelling 30k with tools such as labelme would be a drag! One idea I had was to try using a 3D model with multiple meshes which correspond to different labels (e.g. grasp or pound).    I am specifically looking for a dataset with similar camera instrinics to a ZED camera as my aim is to extend this to pose detection using the ROI ouputted from MaskRCNN, see   and  . Any tips would be much appreciated.    !  "
"I am using CPU for detection, when I run model.detect([image], verbose=1), it takes more than 25 seconds to detect for single image.   Is there any way to reduce the detection time?"
"Sir, I used 1536 X 2048 high resolution images and I changed Image resize mode as none and use min mask as false and assigned max and min sizes to overwrite config.py. I am getting output image as well as masks with size 288 X 432. I simply used visualize.py file only and in that I customized display_images to save images. What changes I need to made so that the output image and masks are of the same size as input image. In the attached I highlighted two functions where I make changes. I am simply using demo file in sample folders. Please help me.       "
"Hello everyone,   I would like to use MASK-RCNN with my own model and dataset.I already prepared my dataset and json annotation.  However, I could not import my own model and training with this model  Could you help me, please?  Here is a code example of my own model code    import keras  from keras.models import Sequential  from keras.layers import Dense, Dropout, Flatten  from keras.layers import Conv2D, MaxPooling2D    cnn1 = Sequential()  cnn1.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))  cnn1.add(MaxPooling2D(pool_size=(2, 2)))  cnn1.add(Dropout(0.2))    cnn1.add(Flatten())    cnn1.add(Dense(128, activation='relu'))  cnn1.add(Dense(10, activation='softmax'))"
"Hello,  Is there a function that can measure the generalization error (the error rate using the test set)? With tensorboard I can visualize the validation and train error, but what about the test error? "
None
"Hello everyone, anybody know how to I display the detection using OpenCV?"
"sorry for spamming. it was as tf version issue.  **Thank you. Can be deleted**, @waleedka "
"Hi! Is it possible to run the Mask R-CNN on a GPU: AMD Radeon Pro 5300M. As far as I know with PlaidML you get a backend for keras, but in the code there are several tensorflow operations. Therefore I tried ngraph-bridge, which leads to this error:          I would really appreciate if someone has got recommendations or instructions for this problem. "
None
None
"I have trained the existing ballon splash model on my pc. now I want to visualize the loss, accuracy, etc on tensorboard. how should i do that? thank you."
"My model just doesnt start training, there's no error so I am unable to understand why!   I downloaded json file in COCO format, it just has 10 images (was just trying to see if everything works well, going to add more) training it for 10 epochs, workders = 1, use_multiprocessing=False, Images per gpu =2, steps per epoch =10,validation steps = 10, TRAIN_ROIS_PER_IMAGE=50.... meaning I tried almost all suggestions from a similar issue, my keras version is 2.1.0, TF-GPU=1.15, GPU = Tesla T4, CUDA 10, CuDnn = 7.6    I would really appreciate if somebody can help me with this...  Thanks in advance."
None
"I managed to setup everything for training on 8 GPUs, Then I switched to machine with 16 GPUs and I got following error:             Does anybody had similar issue ? "
"I am training maskrcnn-matterport with my custom dataset and while training I can only visualize loss and val_loss.   !   But I also want to visualize class loss, mask loss etc. like   !     I am simply using ""tensorboard --logdir= "" command to visualize.   hope someone can help me.. Thanks   "
"Hi there!  I'm using the annotation tool Supervisely, but the json format differs from the VIA . I am trying to train my own data following the balloon sample (I successfully run the example), but I'm stuck on modifying the configuration (as it modifies the coco.py to the balloon.py). Can someone give me some advice on how to change the json format or modifying the configuration? Additionally, I did the annotation with bitmaps instead of drawing polygons, how do I have to change the configuration?  Thanks in advance!"
"Hi, I am doing a project with a custom dataset of fruits and I wanted to use colab to train but I am a bit lost on what to do. What are the files that I should upload to colab to train once I have the images an annotations ready?"
None
"Hello everyone,     I have just finished training a custom model, I have been trying to run the detection without using Google Colab/Jupyter notebook by saving all the codes into a python file. But I am not able to see any result/output by running it. Does anyone know what is the correct way to do it? "
"I have been trying to save the whole model in training mode so that after I stop it at some point, I can resume where it left. I Do not want to save only the weights, I want to save the whole model. To do that:  - I tried to save it by changing ModelCheckpoint function in model.py to ""save_weights_only=False"" but it gave me the error:   TypeError: can't pickle _thread.RLock objects while saving the keras model using model.save()  (This error has been encountered by many others and the reason is the deepcopy of get_config() function. When I search for it online, some people say that it is because of the Lambda functions but I've read a couple of comments that it also does not solve the issue)    - I tried to use model.save() function as a custom callback to save the model to an h5 file but since it uses the same principle, it gave me the same error.     - I tried model.to_json() and model.to_yaml() functions but it is the same error.    - I tried to use tf.keras.experimental.export_saved_model() function to save the model as a SavedModel format but I got the error:   AttributeError: 'Model' object has no attribute '_is_graph_network'    - I tried to save only the optimizer weights using   method which are more important and I already save the normal weights. This time I got no error and I saved something as pkl file. But it did not work as expected when I load these optimizer weights, there was not a smooth loss transition. I may place it wrong in my code as well. I load the optimizer weights after I compile the model in the train() function.    I appreciate any help and suggestion, thank you in advance"
"The code was braking when I wanted to train mask rcnn on my own dataset based on baloon example.     I have discovered that in my annotations ellipses and polylines were used, which are not supported with original code.     Just in case somebody has similar issue just change load_mask function in following lines:         into:                 "
"Hi, there! Anybody knows why height(or named 'column' or 'y') is put in the first, and the 'row' is set in after?     Below is Mask_RCNN source code:     And in Skimage shows that is as below:       "
None
"I appologize for the length of the stack but I really dont know what has happend.  Ubuntu 18.04 on ec2  keras 2.1.3  TF-gpu 1.3.1  cuda 10.    I managed to start the training but it runs horribly slow. And that weird error related to the annotations.         `(car) ubuntu@ip-172-31-10-253:~/car$ python damage.py train --dataset=/home/ubuntu/car/dataset --weights=coco  /home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])  /home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])  /home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])  /home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])  /home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])  /home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.    np_resource = np.dtype([(""resource"", np.ubyte, 1)])  Using TensorFlow backend.  Weights:  coco  Dataset:  /home/ubuntu/car/dataset  Logs:  /home/ubuntu/car/logs    Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     32  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.9  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      8  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 4  IMAGE_CHANNEL_COUNT            3  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                14  IMAGE_MIN_DIM                  800  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           damage  NUM_CLASSES                    2  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  PRE_NMS_LIMIT                  6000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                100  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001      WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.  Instructions for updating:  Colocations handled automatically by placer.  Loading weights  /home/ubuntu/car/mask_rcnn_coco.h5  2020-04-27 14:20:33.708378: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA  2020-04-27 14:20:36.086087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2020-04-27 14:20:36.133504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2020-04-27 14:20:36.151731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2020-04-27 14:20:36.160409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2020-04-27 14:20:36.170869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2020-04-27 14:20:36.183475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2020-04-27 14:20:36.197935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2020-04-27 14:20:36.204013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2020-04-27 14:20:36.206043: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5574080e06b0 executing computations on platform CUDA. Devices:  2020-04-27 14:20:36.206074: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7  2020-04-27 14:20:36.206091: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (1): Tesla K80, Compute Capability 3.7  2020-04-27 14:20:36.206100: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (2): Tesla K80, Compute Capability 3.7  2020-04-27 14:20:36.206112: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (3): Tesla K80, Compute Capability 3.7  2020-04-27 14:20:36.206119: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (4): Tesla K80, Compute Capability 3.7  2020-04-27 14:20:36.206130: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (5): Tesla K80, Compute Capability 3.7  2020-04-27 14:20:36.206137: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (6): Tesla K80, Compute Capability 3.7  2020-04-27 14:20:36.206148: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (7): Tesla K80, Compute Capability 3.7  2020-04-27 14:20:36.230853: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300065000 Hz  2020-04-27 14:20:36.232827: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5574087a50c0 executing computations on platform Host. Devices:  2020-04-27 14:20:36.232856: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0):  ,    2020-04-27 14:20:36.233013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:   name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235  pciBusID: 0000:00:17.0  totalMemory: 11.17GiB freeMemory: 11.10GiB  2020-04-27 14:20:36.233079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 1 with properties:   name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235  pciBusID: 0000:00:18.0  totalMemory: 11.17GiB freeMemory: 11.10GiB  2020-04-27 14:20:36.233136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 2 with properties:   name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235  pciBusID: 0000:00:19.0  totalMemory: 11.17GiB freeMemory: 11.10GiB  2020-04-27 14:20:36.233190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 3 with properties:   name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235  pciBusID: 0000:00:1a.0  totalMemory: 11.17GiB freeMemory: 11.10GiB  2020-04-27 14:20:36.233245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 4 with properties:   name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235  pciBusID: 0000:00:1b.0  totalMemory: 11.17GiB freeMemory: 11.10GiB  2020-04-27 14:20:36.233299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 5 with properties:   name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235  pciBusID: 0000:00:1c.0  totalMemory: 11.17GiB freeMemory: 11.10GiB  2020-04-27 14:20:36.233354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 6 with properties:   name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235  pciBusID: 0000:00:1d.0  totalMemory: 11.17GiB freeMemory: 11.10GiB  2020-04-27 14:20:36.233408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 7 with properties:   name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235  pciBusID: 0000:00:1e.0  totalMemory: 11.17GiB freeMemory: 11.10GiB  2020-04-27 14:20:36.243481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7  2020-04-27 14:20:36.258120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:  2020-04-27 14:20:36.258150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 1 2 3 4 5 6 7   2020-04-27 14:20:36.258163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N Y Y Y Y Y Y Y   2020-04-27 14:20:36.258176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 1:   Y N Y Y Y Y Y Y   2020-04-27 14:20:36.258189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 2:   Y Y N Y Y Y Y Y   2020-04-27 14:20:36.258202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 3:   Y Y Y N Y Y Y Y   2020-04-27 14:20:36.258211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 4:   Y Y Y Y N Y Y Y   2020-04-27 14:20:36.258220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 5:   Y Y Y Y Y N Y Y   2020-04-27 14:20:36.258231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 6:   Y Y Y Y Y Y N Y   2020-04-27 14:20:36.258244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 7:   Y Y Y Y Y Y Y N   2020-04-27 14:20:36.258530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10802 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:17.0, compute capability: 3.7)  2020-04-27 14:20:36.258888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10800 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:00:18.0, compute capability: 3.7)  2020-04-27 14:20:36.259154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10800 MB memory) -> physical GPU (device: 2, name: Tesla K80, pci bus id: 0000:00:19.0, compute capability: 3.7)  2020-04-27 14:20:36.259473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10800 MB memory) -> physical GPU (device: 3, name: Tesla K80, pci bus id: 0000:00:1a.0, compute capability: 3.7)  2020-04-27 14:20:36.259826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 10802 MB memory) -> physical GPU (device: 4, name: Tesla K80, pci bus id: 0000:00:1b.0, compute capability: 3.7)  2020-04-27 14:20:36.260138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 10802 MB memory) -> physical GPU (device: 5, name: Tesla K80, pci bus id: 0000:00:1c.0, compute capability: 3.7)  2020-04-27 14:20:36.260410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 10802 MB memory) -> physical GPU (device: 6, name: Tesla K80, pci bus id: 0000:00:1d.0, compute capability: 3.7)  2020-04-27 14:20:36.260689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 10800 MB memory) -> physical GPU (device: 7, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)  Training network heads    Starting at epoch 0. LR=0.001    Checkpoint Path: /home/ubuntu/car/logs/damage20200427T1421/mask_rcnn_damage_{epoch:04d}.h5  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  In model:  rpn_model      rpn_conv_shared        (Conv2D)      rpn_class_raw          (Conv2D)      rpn_bbox_pred          (Conv2D)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)  WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.  Instructions for updating:  Use tf.cast instead.  /home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  /home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/keras/engine/training.py:2033: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.    UserWarning('Using a generator with `use_multiprocessing=True`'  Epoch 1/30  ERROR:root:Error processing image {'id': 'Case_1500_1_83_2_WIN_20160711_13_20_33_Pro.jpg', 'source': 'damage', 'path': '/home/ubuntu/car/dataset/val/Case_1500_1_83_2_WIN_20160711_13_20_33_Pro.jpg', 'width': 1280, 'height': 720, 'polygons': [{'name': 'ellipse', 'cx': 684, 'cy': 265, 'rx': 95, 'ry': 56}, {'name': 'polygon', 'all_points_x': [152, 253, 293, 359, 954, 1079, 1114, 1168, 1215, 1225, 1223, 1189, 699, 303], 'all_points_y': [338, 604, 687, 702, 711, 686, 616, 389, 367, 345, 331, 325, 332, 332]}]}  Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""damage.py"", line 163, in load_mask      rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])  KeyError: 'all_points_y'  ERROR:root:Error processing image {'id': 'Case_1013_3_51_0_RK74357.001.17.06.2016 08_39_07.jpg', 'source': 'damage', 'path': '/home/ubuntu/car/dataset/train/Case_1013_3_51_0_RK74357.001.17.06.2016 08_39_07.jpg', 'width': 1280, 'height': 960, 'polygons': [{'name': 'polygon', 'all_points_x': [761, 768, 770, 776, 772], 'all_points_y': [582, 597, 607, 610, 577]}, {'name': 'ellipse', 'cx': 740, 'cy': 517, 'rx': 53, 'ry': 36}, {'name': 'polygon', 'all_points_x': [358, 353, 364, 369, 381, 381, 371, 364], 'all_points_y': [531, 547, 561, 561, 543, 529, 521, 524]}, {'name': 'polygon', 'all_points_x': [443, 508, 655, 741, 838, 985, 1039, 1024, 996, 966, 869, 711, 584, 517, 503, 450, 441], 'all_points_y': [724, 730, 737, 739, 737, 722, 715, 687, 635, 612, 618, 623, 625, 618, 625, 692, 722]}, {'name': 'polygon', 'all_points_x': [462, 523, 536, 674, 663, 644], 'all_points_y': [437, 538, 514, 560, 530, 484]}]}  Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""damage.py"", line 163, in load_mask      rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])  KeyError: 'all_points_y'  ERROR:root:Error processing image {'id': 'Case_1500_1_83_2_WIN_20160711_13_20_33_Pro.jpg', 'source': 'damage', 'path': '/home/ubuntu/car/dataset/val/Case_1500_1_83_2_WIN_20160711_13_20_33_Pro.jpg', 'width': 1280, 'height': 720, 'polygons': [{'name': 'ellipse', 'cx': 684, 'cy': 265, 'rx': 95, 'ry': 56}, {'name': 'polygon', 'all_points_x': [152, 253, 293, 359, 954, 1079, 1114, 1168, 1215, 1225, 1223, 1189, 699, 303], 'all_points_y': [338, 604, 687, 702, 711, 686, 616, 389, 367, 345, 331, 325, 332, 332]}]}  Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""damage.py"", line 163, in load_mask      rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])  KeyError: 'all_points_y'  2020-04-27 14:26:50.459912: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_0/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.466286: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_0/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.467159: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_0/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.467591: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_0/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.467920: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_1/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.468210: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_1/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.468448: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_1/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.468708: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_1/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.468991: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_2/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.469265: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_2/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.469493: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_2/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.469732: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_2/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.470011: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_3/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.470272: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_3/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.470497: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_3/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.470731: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_3/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.470998: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_4/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.471242: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_4/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.471514: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_4/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.471788: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_4/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.472093: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_5/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.472401: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_5/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.472677: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_5/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.472939: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_5/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.473256: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_6/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.473567: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_6/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.473846: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_6/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.474125: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_6/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.474445: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_7/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.474779: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_7/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.475052: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_7/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:26:50.475270: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_7/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  ERROR:root:Error processing image {'id': 'Case_10709_3_507_6_Bilde_17541781_6.jpg', 'source': 'damage', 'path': '/home/ubuntu/car/dataset/val/Case_10709_3_507_6_Bilde_17541781_6.jpg', 'width': 960, 'height': 1280, 'polygons': [{'name': 'polygon', 'all_points_x': [341, 373, 309, 587, 647, 677, 615, 558, 417, 341], 'all_points_y': [319, 824, 1281, 1277, 837, 724, 528, 421, 313, 319]}]}  Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""damage.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 1280 is out of bounds for axis 0 with size 1280  ERROR:root:Error processing image {'id': 'Third_Set_Case_9966_Bilde_17129032_5.jpg', 'source': 'damage', 'path': '/home/ubuntu/car/dataset/train/Third_Set_Case_9966_Bilde_17129032_5.jpg', 'width': 1280, 'height': 720, 'polygons': [{'name': 'polygon', 'all_points_x': [967, 1034, 1260, 1277, 1268, 1187, 1034], 'all_points_y': [374, 400, 394, 388, 597, 515, 481]}, {'name': 'polygon', 'all_points_x': [0, 736, 906, 1155, 1037, 935, 793, 672, 319, 226, 151, 0], 'all_points_y': [38, 70, 342, 721, 718, 553, 518, 466, 423, 229, 133, 104]}]}  Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""damage.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 720 is out of bounds for axis 0 with size 720  ERROR:root:Error processing image {'id': 'Case_10709_3_507_6_Bilde_17541781_6.jpg', 'source': 'damage', 'path': '/home/ubuntu/car/dataset/val/Case_10709_3_507_6_Bilde_17541781_6.jpg', 'width': 960, 'height': 1280, 'polygons': [{'name': 'polygon', 'all_points_x': [341, 373, 309, 587, 647, 677, 615, 558, 417, 341], 'all_points_y': [319, 824, 1281, 1277, 837, 724, 528, 421, 313, 319]}]}  Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""damage.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 1280 is out of bounds for axis 0 with size 1280  ERROR:root:Error processing image {'id': 'Case_10709_3_507_6_Bilde_17541781_6.jpg', 'source': 'damage', 'path': '/home/ubuntu/car/dataset/val/Case_10709_3_507_6_Bilde_17541781_6.jpg', 'width': 960, 'height': 1280, 'polygons': [{'name': 'polygon', 'all_points_x': [341, 373, 309, 587, 647, 677, 615, 558, 417, 341], 'all_points_y': [319, 824, 1281, 1277, 837, 724, 528, 421, 313, 319]}]}  Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""damage.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 1280 is out of bounds for axis 0 with size 1280  2020-04-27 14:28:12.303575: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_0/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.303758: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_0/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.303838: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_0/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.303913: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_0/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.304024: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_1/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.304097: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_1/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.304168: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_1/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.304258: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_1/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.304408: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_2/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.304499: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_2/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.304588: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_2/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.304674: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_2/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.304821: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_3/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.304908: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_3/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.304995: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_3/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.305069: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_3/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.305200: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_4/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.305277: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_4/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.305356: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_4/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.305436: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_4/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.305573: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_5/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.305654: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_5/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.305739: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_5/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.305825: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_5/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.305970: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_6/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.306056: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_6/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.306140: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_6/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.306229: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_6/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.306388: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_7/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.306473: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_7/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.306559: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_7/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:28:12.306696: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_7/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  ERROR:root:Error processing image {'id': 'Third_Set_Case_9966_Bilde_17129032_5.jpg', 'source': 'damage', 'path': '/home/ubuntu/car/dataset/train/Third_Set_Case_9966_Bilde_17129032_5.jpg', 'width': 1280, 'height': 720, 'polygons': [{'name': 'polygon', 'all_points_x': [967, 1034, 1260, 1277, 1268, 1187, 1034], 'all_points_y': [374, 400, 394, 388, 597, 515, 481]}, {'name': 'polygon', 'all_points_x': [0, 736, 906, 1155, 1037, 935, 793, 672, 319, 226, 151, 0], 'all_points_y': [38, 70, 342, 721, 718, 553, 518, 466, 423, 229, 133, 104]}]}  Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""damage.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 720 is out of bounds for axis 0 with size 720  ERROR:root:Error processing image {'id': 'Third_Set_Case_9966_Bilde_17129032_5.jpg', 'source': 'damage', 'path': '/home/ubuntu/car/dataset/train/Third_Set_Case_9966_Bilde_17129032_5.jpg', 'width': 1280, 'height': 720, 'polygons': [{'name': 'polygon', 'all_points_x': [967, 1034, 1260, 1277, 1268, 1187, 1034], 'all_points_y': [374, 400, 394, 388, 597, 515, 481]}, {'name': 'polygon', 'all_points_x': [0, 736, 906, 1155, 1037, 935, 793, 672, 319, 226, 151, 0], 'all_points_y': [38, 70, 342, 721, 718, 553, 518, 466, 423, 229, 133, 104]}]}  Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""damage.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 720 is out of bounds for axis 0 with size 720  ERROR:root:Error processing image {'id': 'Third_Set_Case_9592_Bilde_17071182_2.jpg', 'source': 'damage', 'path': '/home/ubuntu/car/dataset/train/Third_Set_Case_9592_Bilde_17071182_2.jpg', 'width': 1280, 'height': 720, 'polygons': [{'name': 'polygon', 'all_points_x': [6, 553, 504, 3], 'all_points_y': [235, 278, 721, 721]}, {'name': 'polygon', 'all_points_x': [591, 756, 857, 999, 1210, 1216, 654, 585], 'all_points_y': [261, 255, 295, 397, 492, 573, 611, 298]}]}  Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""damage.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 720 is out of bounds for axis 0 with size 720  2020-04-27 14:28:40.430824: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally  2020-04-27 14:29:04.736345: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.52GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.  2020-04-27 14:29:05.970395: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_7_bfc) ran out of memory trying to allocate 3.52GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.  2020-04-27 14:29:11.037157: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.37GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.  2020-04-27 14:29:13.425438: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.04GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.  2020-04-27 14:29:13.497310: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.05GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.   21/100 [=====>........................] - ETA: 28:54 - loss: 4.6119 - rpn_class_loss: 0.2270 - rpn_bbox_loss: 2.2982 - mrcnn_class_loss: 0.1744 - mrcnn_bbox_loss: 1.0160 - mrcnn_mask_loss: 0.8963ERROR:root:Error processing image {'id': 'Third_Set_Case_9966_Bilde_17129032_5.jpg', 'source': 'damage', 'path': '/home/ubuntu/car/dataset/train/Third_Set_Case_9966_Bilde_17129032_5.jpg', 'width': 1280, 'height': 720, 'polygons': [{'name': 'polygon', 'all_points_x': [967, 1034, 1260, 1277, 1268, 1187, 1034], 'all_points_y': [374, 400, 394, 388, 597, 515, 481]}, {'name': 'polygon', 'all_points_x': [0, 736, 906, 1155, 1037, 935, 793, 672, 319, 226, 151, 0], 'all_points_y': [38, 70, 342, 721, 718, 553, 518, 466, 423, 229, 133, 104]}]}  Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""damage.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 720 is out of bounds for axis 0 with size 720   24/100 [======>.......................] - ETA: 25:37 - loss: 4.3485 - rpn_class_loss: 0.2141 - rpn_bbox_loss: 2.1251 - mrcnn_class_loss: 0.1695 - mrcnn_bbox_loss: 0.9801 - mrcnn_mask_loss: 0.8598ERROR:root:Error processing image {'id': 'Case_7569_1_316_0_Bilde_17239422_1.jpg', 'source': 'damage', 'path': '/home/ubuntu/car/dataset/train/Case_7569_1_316_0_Bilde_17239422_1.jpg', 'width': 1280, 'height': 960, 'polygons': [{'name': 'ellipse', 'cx': 625, 'cy': 381, 'rx': 78, 'ry': 55}, {'name': 'polygon', 'all_points_x': [1016, 1035, 1045, 1051, 1063, 1060, 1047, 1016], 'all_points_y': [316, 321, 327, 341, 336, 324, 312, 309]}, {'name': 'polygon', 'all_points_x': [437, 471, 510, 514, 473, 448], 'all_points_y': [246, 243, 239, 254, 255, 252]}]}  Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""damage.py"", line 163, in load_mask      rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])  KeyError: 'all_points_y'   37/100 [==========>...................] - ETA: 16:49 - loss: 3.6506 - rpn_class_loss: 0.1722 - rpn_bbox_loss: 1.6805 - mrcnn_class_loss: 0.1547 - mrcnn_bbox_loss: 0.8812 - mrcnn_mask_loss: 0.7620ERROR:root:Error processing image {'id': 'Case_5781_4_191_11_WIN_20170303_11_19_00_Pro.jpg', 'source': 'damage', 'path': '/home/ubuntu/car/dataset/train/Case_5781_4_191_11_WIN_20170303_11_19_00_Pro.jpg', 'width': 1280, 'height': 720, 'polygons': [{'name': 'polygon', 'all_points_x': [17, 1277, 1268, 90, 6, 9], 'all_points_y': [539, 498, 681, 718, 721, 539]}]}  Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""damage.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 720 is out of bounds for axis 0 with size 720   52/100 [==============>...............] - ETA: 11:02 - loss: 3.2222 - rpn_class_loss: 0.1454 - rpn_bbox_loss: 1.4223 - mrcnn_class_loss: 0.1434 - mrcnn_bbox_loss: 0.8049 - mrcnn_mask_loss: 0.7062ERROR:root:Error processing image {'id': 'Case_1500_1_83_6_RK64083.003.11.07.2016 14_28_31.jpg', 'source': 'damage', 'path': '/home/ubuntu/car/dataset/train/Case_1500_1_83_6_RK64083.003.11.07.2016 14_28_31.jpg', 'width': 1280, 'height': 960, 'polygons': [{'name': 'ellipse', 'cx': 580, 'cy': 7, 'rx': 159, 'ry': 61}, {'name': 'polygon', 'all_points_x': [4, 439, 1137, 1272, 1263, 1206, 612, 116, 43, 0], 'all_points_y': [162, 112, 84, 73, 555, 665, 702, 666, 577, 473]}]}  Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""damage.py"", line 163, in load_mask      rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])  KeyError: 'all_points_y'   87/100 [=========================>....] - ETA: 2:29 - loss: 2.7434 - rpn_class_loss: 0.1180 - rpn_bbox_loss: 1.1368 - mrcnn_class_loss: 0.1315 - mrcnn_bbox_loss: 0.7144 - mrcnn_mask_loss: 0.6427ERROR:root:Error processing image {'id': 'Case_1500_1_83_6_RK64083.003.11.07.2016 14_28_31.jpg', 'source': 'damage', 'path': '/home/ubuntu/car/dataset/train/Case_1500_1_83_6_RK64083.003.11.07.2016 14_28_31.jpg', 'width': 1280, 'height': 960, 'polygons': [{'name': 'ellipse', 'cx': 580, 'cy': 7, 'rx': 159, 'ry': 61}, {'name': 'polygon', 'all_points_x': [4, 439, 1137, 1272, 1263, 1206, 612, 116, 43, 0], 'all_points_y': [162, 112, 84, 73, 555, 665, 702, 666, 577, 473]}]}  Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""damage.py"", line 163, in load_mask      rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])  KeyError: 'all_points_y'   94/100 [===========================>..] - ETA: 1:07 - loss: 2.6895 - rpn_class_loss: 0.1151 - rpn_bbox_loss: 1.1073 - mrcnn_class_loss: 0.1297 - mrcnn_bbox_loss: 0.7028 - mrcnn_mask_loss: 0.6346ERROR:root:Error processing image {'id': 'Third_Set_Case_9592_Bilde_17071182_2.jpg', 'source': 'damage', 'path': '/home/ubuntu/car/dataset/train/Third_Set_Case_9592_Bilde_17071182_2.jpg', 'width': 1280, 'height': 720, 'polygons': [{'name': 'polygon', 'all_points_x': [6, 553, 504, 3], 'all_points_y': [235, 278, 721, 721]}, {'name': 'polygon', 'all_points_x': [591, 756, 857, 999, 1210, 1216, 654, 585], 'all_points_y': [261, 255, 295, 397, 492, 573, 611, 298]}]}  Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""damage.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 720 is out of bounds for axis 0 with size 720   99/100 [============================>.] - ETA: 11s - loss: 2.6550 - rpn_class_loss: 0.1130 - rpn_bbox_loss: 1.0861 - mrcnn_class_loss: 0.1287 - mrcnn_bbox_loss: 0.6974 - mrcnn_mask_loss: 0.62972020-04-27 14:43:13.970996: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_0/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.971084: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_0/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.971147: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_0/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.971210: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_0/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.971274: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_1/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.971340: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_1/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.971403: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_1/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.971467: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_1/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.971532: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_2/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.971595: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_2/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.971659: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_2/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.971723: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_2/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.971787: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_3/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.971846: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_3/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.971907: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_3/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.971965: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_3/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.972025: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_4/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.972083: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_4/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.972143: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_4/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.972202: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_4/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.972261: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_5/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.972319: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_5/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.972378: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_5/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.972437: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_5/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.972496: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_6/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.972554: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_6/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.972611: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_6/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.972670: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_6/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.972734: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_7/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.972794: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_7/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.972852: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_7/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:13.972911: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_7/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.715467: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_0/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.715547: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_0/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.715589: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_0/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.715628: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_0/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.715667: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_1/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.715705: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_1/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.715742: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_1/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.715780: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_1/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.715818: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_2/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.715855: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_2/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.715892: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_2/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.715929: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_2/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.715968: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_3/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.716005: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_3/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.716042: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_3/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.716079: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_3/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.716117: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_4/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.716154: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_4/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.716191: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_4/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.716227: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_4/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.716266: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_5/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.716304: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_5/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.716341: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_5/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.716377: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_5/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.716415: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_6/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.716452: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_6/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.716489: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_6/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.716527: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_6/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.716565: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_7/mask_rcnn/proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.716603: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_7/mask_rcnn/proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.716640: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_7/mask_rcnn/proposal_targets/strided_slice_74. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2020-04-27 14:43:24.716677: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node tower_7/mask_rcnn/proposal_targets/strided_slice_111. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  ERROR:root:Error processing image {'id': 'Case_1500_1_83_2_WIN_20160711_13_20_33_Pro.jpg', 'source': 'damage', 'path': '/home/ubuntu/car/dataset/val/Case_1500_1_83_2_WIN_20160711_13_20_33_Pro.jpg', 'width': 1280, 'height': 720, 'polygons': [{'name': 'ellipse', 'cx': 684, 'cy': 265, 'rx': 95, 'ry': 56}, {'name': 'polygon', 'all_points_x': [152, 253, 293, 359, 954, 1079, 1114, 1168, 1215, 1225, 1223, 1189, 699, 303], 'all_points_y': [338, 604, 687, 702, 711, 686, 616, 389, 367, 345, 331, 325, 332, 332]}]}  Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""damage.py"", line 163, in load_mask      rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])  KeyError: 'all_points_y'  ERROR:root:Error processing image {'id': 'Case_1500_1_83_2_WIN_20160711_13_20_33_Pro.jpg', 'source': 'damage', 'path': '/home/ubuntu/car/dataset/val/Case_1500_1_83_2_WIN_20160711_13_20_33_Pro.jpg', 'width': 1280, 'height': 720, 'polygons': [{'name': 'ellipse', 'cx': 684, 'cy': 265, 'rx': 95, 'ry': 56}, {'name': 'polygon', 'all_points_x': [152, 253, 293, 359, 954, 1079, 1114, 1168, 1215, 1225, 1223, 1189, 699, 303], 'all_points_y': [338, 604, 687, 702, 711, 686, 616, 389, 367, 345, 331, 325, 332, 332]}]}  Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""damage.py"", line 163, in load_mask      rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])  KeyError: 'all_points_y'  ERROR:root:Error processing image {'id': 'Case_10709_3_507_6_Bilde_17541781_6.jpg', 'source': 'damage', 'path': '/home/ubuntu/car/dataset/val/Case_10709_3_507_6_Bilde_17541781_6.jpg', 'width': 960, 'height': 1280, 'polygons': [{'name': 'polygon', 'all_points_x': [341, 373, 309, 587, 647, 677, 615, 558, 417, 341], 'all_points_y': [319, 824, 1281, 1277, 837, 724, 528, 421, 313, 319]}]}  Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""damage.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 1280 is out of bounds for axis 0 with size 1280  ERROR:root:Error processing image {'id': 'Case_10709_3_507_6_Bilde_17541781_6.jpg', 'source': 'damage', 'path': '/home/ubuntu/car/dataset/val/Case_10709_3_507_6_Bilde_17541781_6.jpg', 'width': 960, 'height': 1280, 'polygons': [{'name': 'polygon', 'all_points_x': [341, 373, 309, 587, 647, 677, 615, 558, 417, 341], 'all_points_y': [319, 824, 1281, 1277, 837, 724, 528, 421, 313, 319]}]}  Traceback (most recent call last):   File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""damage.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 1280 is out of bounds for axis 0 with size 1280  ERROR:root:Error processing image {'id': 'Case_1500_1_83_2_WIN_20160711_13_20_33_Pro.jpg', 'source': 'damage', 'path': '/home/ubuntu/car/dataset/val/Case_1500_1_83_2_WIN_20160711_13_20_33_Pro.jpg', 'width': 1280, 'height': 720, 'polygons': [{'name': 'ellipse', 'cx': 684, 'cy': 265, 'rx': 95, 'ry': 56}, {'name': 'polygon', 'all_points_x': [152, 253, 293, 359, 954, 1079, 1114, 1168, 1215, 1225, 1223, 1189, 699, 303], 'all_points_y': [338, 604, 687, 702, 711, 686, 616, 389, 367, 345, 331, 325, 332, 332]}]}  Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""damage.py"", line 163, in load_mask      rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])  KeyError: 'all_points_y'  100/100 [==============================] - 1663s 17s/step - loss: 2.6473 - rpn_class_loss: 0.1124 - rpn_bbox_loss: 1.0815 - mrcnn_class_loss: 0.1283 - mrcnn_bbox_loss: 0.6963 - mrcnn_mask_loss: 0.6289 - val_loss: 2.0451 - val_rpn_class_loss: 0.0801 - val_rpn_bbox_loss: 0.7030 - val_mrcnn_class_loss: 0.1039 - val_mrcnn_bbox_loss: 0.5978 - val_mrcnn_mask_loss: 0.5602  Epoch 2/30   22/100 [=====>........................] - ETA: 10:43 - loss: 1.9957 - rpn_class_loss: 0.0646 - rpn_bbox_loss: 0.7207 - mrcnn_class_loss: 0.1120 - mrcnn_bbox_loss: 0.5625 - mrcnn_mask_loss: 0.5359ERROR:root:Error processing image {'id': 'Case_1164_3_57_3_WIN_20160627_11_54_27_Pro.jpg', 'source': 'damage', 'path': '/home/ubuntu/car/dataset/train/Case_1164_3_57_3_WIN_20160627_11_54_27_Pro.jpg', 'width': 1280, 'height': 720, 'polygons': [{'name': 'polygon', 'all_points_x': [872, 1028, 1173, 1277, 1274, 973, 921, 889], 'all_points_y': [539, 492, 484, 417, 721, 721, 672, 634]}]}  Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/envs/car/lib/python3.6/site-packages/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""damage.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 720 is out of bounds for axis 0 with size 720   42/100 [===========>..................] - ETA: 8:01 - loss: 1.9938 - rpn_class_loss: 0.0646 - rpn_bbox_loss: 0.7289 - mrcnn_class_loss: 0.1119 - mrcnn_bbox_loss: 0.5548 - mrcnn_mask_loss: 0.5336^Z  [1]+  Stopped                 python damage.py train --dataset=/home/ubuntu/car/dataset --weights=coco    `      Could anybody help ?"
"Hi, what is good configuration to make efficient training ?I am using p2.8xlarge.   My dateset contains train 7500 images, test 1500 of resolution 1600x1600.    I set:         and I get:    ` Resource exhausted: OOM when allocating tensor with shape[1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc  `    For some reason, when i open `nvidia-smi` I can see that it wants to put everything on 1 card GPU_0.    What can I do now ?    Even tho I have set the GPU_COUNT to 8,     this is what i get:    `BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     4  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.9  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 4  IMAGE_CHANNEL_COUNT            3  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                14  IMAGE_MIN_DIM                  800  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           damage  NUM_CLASSES                    2  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  PRE_NMS_LIMIT                  6000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                100  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001  `    There still is GPU_COUNT = 1  "
does removing regression branch affect mask mAP
"I am trying to run the nucleus detection model with the original data used in the paper.   I tried to used both resnet50 and coco weights, but they both show AP 0 for every IoU, and the detection is not working.   What can be the problem?    !     !   "
I am trying to inference the frozen model of mask-rcnn but it gives me this error:     is there any help?
"Hi    I am trying to train a model to recognize what color a certain type of object contains.    My problem is that any these objects may contain zero or more colors, and while right now I am limiting the search on only two colors, in the future this number may increase.  And since the number of possible combinations increase exponentially with the number of colors it seems to me that having one class for each of those combinations is not the best approach.    Ideally, I would use one class for each color (or two, for example ""red"" and ""not_red"" and one of those must be selected), but this would mean that:  1. The model must be able to assign more to one class to each pixel, and potentially the number of simultaneous classes is different for each pixel.  2. The variable class_ids is no longer a 1D array (each mask has one class), but a 2D array (each mask can have more than one class).    While I have no idea if [1] would be a problem, I'm pretty sure [2] would be a problem since the code assumes that the variable class_ids returned by Dataset.load_mask() is a 1D array.    I also thought about training multiple models, one for each class, and then somehow merge their outputs, but I am not sure on how I should proceed in this case.    What would be my best option?  Is this even possible, or should I just use one class for each combination of colors while excluding the least likely combinations?"
"I'm trying to train two classes.  The training method was referred to the link below.       The question is:  When I try to train class A and B,    1. First, train with only the A data.  2. Can I find both A and B by additional training with only B data in the weight file trained in '1.'?          `############################################################  #  Configurations  ############################################################      class CustomConfig(Config):      """"""Configuration for training on the toy  dataset.      Derives from the base Config class and overrides some values.      """"""      # Give the configuration a recognizable name      NAME = ""object""        # We use a GPU with 12GB memory, which can fit two images.      # Adjust down if you use a smaller GPU.      IMAGES_PER_GPU = 1        # Number of classes (including background)      NUM_CLASSES = 1 + 2  # Background + toy        # Number of training steps per epoch      STEPS_PER_EPOCH = 100        # Skip detections with   find A, B**  "
I want to crop detected objects from images and save it. to do this I have to get the coordinates of bounding box. how I can Extract the coordinates of bounding box ? 
"Hi guys,    I met a problem which is when I was traning on large images (4000x2000) with plentiful annotations (100~300 annotated masks), the    mrcnn_bbox_loss  mrcnn_mask_loss  val_mrcnn_bbox_loss:  val_mrcnn_mask_loss  are always be 0.0000e+00  And the trained model cannot detect any instance on the testing images.    The training on small images (200x200) with just several annotations (1~5 annotated masks) is working fine with a good result.    Any ideas? I will be very appreciate,  "
Is it possible to produce the map per class on a custom dataset? 
Could anyone tell me whatÂ´s going on during training with the validation loss curve? why itÂ´s so unstable while the training loss curve is slowly converging but quite stable?. Does it has anything to do with pathological curvatures?          !   
"Hi there,    Thanks for this excellent work.     I am a bit new in the field.  My question might be a bit irrelevant. But I would like to know how I can create the 3D semantic map, when I have the camera pose + depth + semantic labels for every frame.     Cheers,  Jacob"
"I've met this error while I was trying to run sample script **balloon.py** with python 3.7.3 on macOS.    !     I thought that this error must have caused by incompatibility between python 3.7.x and Tensorflow2.    So I installed python 3.6.8 with pyenv and virtual environment for python 3.6.8 and this resolved the problem.         BTW, I met another problem after I downgraded python version.    **AttributeError: module 'tensorflow' has no attribute 'log'**  !     The error was raised due to usage of old Tensorflow library functions. (I have tensorflow 2.1.0)    This error was resolved with the following command ( )         **Result of the command:**  !          Now I'm happy with the result of sample script **balloon.py**    ! "
"Hi everyone,    I am using Google colab to train a custom model. However, Google colab keep prompting me that I am not using the GPU in the instance. Can anyone let me know where do I have to configure the codes to make the model training run on a GPU? Any help is appreciated thank you. "
"Hello  when I ran the Training I got this error;  ValueError: Could not find a format to read the specified file in single-image mode   the full error message is :       my dataset include 2 folders. one satellite image chips and another mask images as label , both in jpg format.      my dataset class is:   "
"Hello all,    I have a question regarding changing the output mask shape. Originally it was commented as   in  , but I am not sure where can I change the mask branch thing. Any ideas would be helpful.    Many thanks in advance!  "
"I have training images which are all of the same size and with an aspect ratio of 4:3 (like 1024x768), so I was wondering what would be the best way to handle rectangular images.     I had a look at the   class and found that there are several ways to resize training images, which correspond to different values for `IMAGE_RESIZE_MODE`. For instance, `""square""` corresponds to resizing and padding the image so that it becomes of size [max_dim, max_dim], whereas `""none""` means that the image will be left unchanged.     For my particular use case, can I just set `IMAGE_RESIZE_MODE = ""none""` and feed the training images to the model as they are? Or is there some kind of benefit in using square images for training?   "
"Hi all,     I am currently at the stage where I have a model trained on a custom dataset, and now want to start looking at how my model is making predictions in more depth, specifically looking at where in the images the model is activating to make class predictions similar to something such as Grad-CAM.     Through using the inspect_balloon_model notebook as a guide I am able to visualise some activations using    `display_images(np.transpose(activations[""res5c_out""][0,:,:,:4], [2, 0, 1]), cols=4)`    I am using layer 'res5c_out' here as I believe this to be the layer I need to look at to generate activations for the final class prediction, is this correct or should I be looking at a deeper layer?    Further, I am unsure as to how I can then take these activation mappings and create a heatmap which can then be overlayed onto the input image. Performing `np.transpose(activations[""res5c_out""][0,:,:,:4], [2, 0, 1])` gives me a whole range of feature maps to look at, I'm unsure which one I need to be using for my heatmaps, or even if this is where I should be generating them.    Any help is greatly appreciated.  "
"During VOC data training, the same object in the graph will be cut into two parts. This can lead to poor results      "
"I created my own dataset, which is to identify a particular type of crop in a field. Mask RCNN creates a polygon mask over that object. Let's say there are about 27 different objects in a image, so MaskRCNN creates a mask over the 27 objects.   How can I find area of each masks?    I've implemented a solution based on my understanding. Please check and let me know if it's correct.  r['masks'].shape[-1] --> returns the number of objects/masks in an image    for i in range(r['masks'].shape[-1]):      mask = r['masks'][:, :, i]      image[mask] = 255      image[~mask] = 0      unique, counts = np.unique(image, return_counts=True)      mask_area = counts[1] / (counts[0] + counts[1])      print(counts[1])    Here,   counts[1] --> pixel area of the mask   counts[0] --> remaining area in the image   "
How to turn the segmentation class of voc2012 into a mask tag?
There is no attribute in model.py for data_generator in mrcnn for tensorflow 2.
"I am following the balloon example, but I annotated my dataset using MATLAB Image Labeler which only produces png masks.     is there a way to use only png masks?    thanks in advance"
"Hello Everybody ,    I am trying to use Mask RCNN for my own dataset with only one category ""building"".    I run the evaluate_coco function on my dataset, I have these results :      I use these function :          As you see, i only have one line threshold for for AP and AR.  How can i do to have the all AP threshold like in these exemple  ?:       Thanks for you replies ! "
"Hi there,  I am trying to use Mask RCNN for object detection of wheat spikes in fields,  I have a large data set of more than 200 images, in every images there are about 20 spikes (objects).  I trained the model for 200 epochs and i get very bad results (even when I predict on the training images), such as:     Only the bounding box which i circled in blue is correct.    I appreciate any help, stuck on this for days.  thank you!"
"In Mask RCNN, the instance segmentation model generates the masks for each detected object. The masks are soft masks (with float pixel values) and of size 28x28 during training, then, the predicted masks are rescaled to the bounding box dimensions, and we can overlay them on the original image to visualize the final output.    Please how I can obtain the 28x28 mask before its rescaled ?    I have the following code :        def apply_mask(image, mask, color, alpha=1):         """"""apply mask to image""""""         for n, c in enumerate(color):              image[:, :, n] = np.where(              mask == 1,              image[:, :, n] * (1 - alpha) + alpha * c,              image[:, :, n]              )              return image        for i in range(n_instances):          if not np.any(boxes[i]):              continue        if ids[i] == 1:          color = [255,255,255]          mask = masks[:, :, i]          image_background_substracted = apply_mask(background, mask, color)            return image_background_substracted      else:          return background"
I am tring to traing color splash model of mask rcnn on colab. But unfortunately GPU is not utilizing. What should i do?  Tensorflow-1.5  Tensorflow-gpu-1.5  Keras-2.3.1   Veraion currently using.  
"Receiving the following error when trying to install     `(MaskRCNN) Î» python setup.py build_ext install  Compiling pycocotools/_mask.pyx because it changed.  [1/1] Cythonizing pycocotools/_mask.pyx  C:\Users\mthompson1\AppData\Local\Continuum\anaconda3\envs\MaskRCNN\lib\site-packages\Cython\Compiler\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: D:\GitHub\cocoapi\PythonAPI\pycocotools\_mask.pyx    tree = Parsing.p_module(s, pxd, full_module_name)  running build_ext  building 'pycocotools._mask' extension  error: Unable to find vcvarsall.bat`    Dependencies were installed via `conda install  ' to include the cuda toolkit.  Please advise."
Its been 3 hours it showing epoch 1/30.  My laptop has no gpu. Now what should i do?
"In 'training' mode, why is the bounding box adjustment from `fpn_classifier_graph()` not applied to the ROIs before generating the mask in `build_fpn_mask_graph()`?    In 'inference' mode, the adjustment was applied in `DetectionLayer()`."
Can the training data set of coco reach the map in the author's paper? Would you like to know if there are any data processing and data enhancement methods for your training? Thank you
"I just want to detect only a couple of classes like cars, buses, trucks etc. can anyone tell how can we do this?"
"my TensorFlow-gpu is 15.0      oduleNotFoundError                       Traceback (most recent call last)  /opt/conda/lib/python3.6/site-packages/keras/callbacks.py in __init__(self, log_dir, histogram_freq, batch_size, write_graph, write_grads, write_images, embeddings_freq, embeddings_layer_names, embeddings_metadata, embeddings_data, update_freq)      744             import tensorflow as tf  --> 745             from tensorflow.contrib.tensorboard.plugins import projector      746         except ImportError:    ModuleNotFoundError: No module named 'tensorflow.contrib.tensorboard'; 'tensorflow.contrib' is not a package    During handling of the above exception, another exception occurred:    ImportError                               Traceback (most recent call last)    in          1 # train weights (output layers or 'heads')  ----> 2 model.train(train_set, test_set, learning_rate=config.LEARNING_RATE, epochs=5, layers='heads')    ~/.local/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py in train(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)     2339         callbacks = [     2340             keras.callbacks.TensorBoard(log_dir=self.log_dir,  -> 2341                                         histogram_freq=0, write_graph=True, write_images=False),     2342             keras.callbacks.ModelCheckpoint(self.checkpoint_path,     2343                                             verbose=0, save_weights_only=True),    /opt/conda/lib/python3.6/site-packages/keras/callbacks.py in __init__(self, log_dir, histogram_freq, batch_size, write_graph, write_grads, write_images, embeddings_freq, embeddings_layer_names, embeddings_metadata, embeddings_data, update_freq)      745             from tensorflow.contrib.tensorboard.plugins import projector      746         except ImportError:  --> 747             raise ImportError('You need the TensorFlow module installed to '      748                               'use TensorBoard.')      749     ImportError: You need the TensorFlow module installed to use TensorBoard.    â€‹    "
"Hi,   I keep having 'Kernel die, restart' error on running demo.ipynb.  This line:`results = model.detect([image], verbose=1)` always cause the error    I changed `BACKBONE` in config.py from resnet101 to resnet50       I follow other suggestions to set up tensorflow/keras version  tensorflow=1.13.1            keras= 2.1.0    I am using MacOS Memory: 16 GB 1600 MHz DDR3  Please help! I am new to ML. Don't know how to resolve this issue    "
   Documentation link for your reference  Thank you
"# NUMBER OF GPUs to use. When using only a CPU, this needs to be set to 1.  GPU_COUNT = 1    ????? This is so confusing. And in train_shape.py it says:    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each  # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).  GPU_COUNT = 1"
"Hello All;    Hope your are doing well;    In Mask RCNN I would like change the color of the mask to be White with the Alpha = 1, I change it frome the right place in vizualize.py, but anything change the mask color still red or blue or another color, why the changes on vizualize.py dont have effect ?"
None
"Hello,  I understand that the training loss is the sum of all the network losses. However, that doesn't seem to be the case for the val_loss, but I can't find any information about this    A simple example:  `3600/3600 [==============================] - 1914s 532ms/step - loss: 0.8277 - rpn_class_loss: 0.0072 - rpn_bbox_loss: 0.1687 - mrcnn_class_loss: 0.2845 - mrcnn_bbox_loss: 0.1520 - mrcnn_mask_loss: 0.2153 - val_loss: 0.6929 - val_rpn_class_loss: 0.0070 - val_rpn_bbox_loss: 0.2407 - val_mrcnn_class_loss: 0.3982 - val_mrcnn_bbox_loss: 0.1687 - val_mrcnn_mask_loss: 0.2139`  Sum of all the validation losses:  0.0070 +  0.2407 +  0.3982 + 0.1687 + 0.2139 = 1,0285, but it says that val_loss = 0.6929  Any ideas ?  "
"when I try to follow the step 2 - which is train_shapes.ipynb ..  while training the model after creating, I got this error in line #Train head branch"
"!   !      visualize.display_instances run ok but AP, precisions, recalls, overlaps = utils.compute_ap(gt_bbox, gt_class_id, gt_mask,  r['rois'], r['class_ids'], r['scores'], r['masks']) not running. help me"
"Hey Guys,  im training the Mask R-CNN Model on my own Dataset consisting of 440 Train images and 60 Validation images. The Goal is to detect structurally Damaged Areas of Bridges, Railings and so on.  For That im using 4 classes. Eventough im using this code of Augmentation:    `max_augs = 3      augmentation = imgaug.augmenters.SomeOf((0, max_augs),      This is the code in my main class:   `class BridgesConfig(Config):      NAME=""bridges""      #Because of weaker GPU:      IMAGES_PER_GPU = 2        # Number of classes (including background):      NUM_CLASSES = 1 + 4  # Background + TK + B_fehlend + B_korrodiert + B_Blasen        # Number of training steps per epoch      STEPS_PER_EPOCH = 520      VALIDATION_STEPS = 62        BACKBONE = ""resnet101""        RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)        USE_MINI_MASK = True      # Skip detections with <70% confidence      DETECTION_MIN_CONFIDENCE = 0        GPU_COUNT = 1`"
"I have two classes(1 object class + background). By default, the model predicts objects with confidence scores above 90%. How can I find the predicted results that are less than 90%. Even I tried retraining by setting `DETECTION_MIN_CONFIDENCE = 0.8`, but still I am unable to find results that are less than 90%."
"Hi all,     I cannot find anything in the documentation on this, and I have been digging through the code trying to understand exactly what is happening to my images through the pipeline.     I am passing images that are RGB, and pre-normalized (0-1). I realize, accordingly that the mean pixel values should then reflect this and be 0-1 (per the mold_image function).     However, I am getting issues with my predictions, and trying to deduce if the issue is my input DN value range. Are the input images supposed to be in int format (0-256)? Will having them 0-1 impact anything through the pipeline?     Kind thanks and best wishes through these times,   Alex "
"Dear author, your GDXray dataset link of metal-defect-detection  has expired. Could you please send it againï¼Ÿ"
"> c:\misc\custom_ops.py(136)__call__()      134                 return input_layer.with_tensor(tf.matmul(input_, matrix) + bias, parameters=self.vars)      135         except Exception:  --> 136             import ipdb; ipdb.set_trace()    ipdb>"
"Traceback (most recent call last):    File ""run_exp.py"", line 68, in        algo.train()    File ""C:\stageI\trainer.py"", line 306, in train      counter = self.build_model(sess)    File ""C:\stageI\trainer.py"", line 280, in build_model      self.init_opt()    File ""C:\stageI\trainer.py"", line 97, in init_opt      c, kl_loss = self.sample_encoded_context(self.embeddings)    File ""C:\stageI\trainer.py"", line 76, in sample_encoded_context      c_mean_logsigma = self.model.generate_condition(embeddings)    File ""C:\stageI\model.py"", line 47, in generate_condition      custom_fully_connected(self.ef_dim * 2).    File ""C:\Users\HP\Anaconda3\lib\site-packages\prettytensor\pretty_tensor_class.py"", line 1973, in method      return input_layer._method_complete(result)    File ""C:\Users\HP\Anaconda3\lib\site-packages\prettytensor\pretty_tensor_class.py"", line 691, in _method_complete      return self.with_tensor(result)    File ""C:\Users\HP\Anaconda3\lib\site-packages\prettytensor\pretty_tensor_class.py"", line 978, in with_tensor      layer = Layer(copy=self, tensor=unwrap(tensor), sequence=None)    File ""C:\Users\HP\Anaconda3\lib\site-packages\prettytensor\pretty_tensor_class.py"", line 931, in __init__      self._name = self._sequence[0].op.name  **TypeError: 'NoneType' object is not subscriptable**"
"**Suddenly have this problem. Isit tensorflow problem? How can I solve this? I install tensorflow 1.14 temporarily to run the code, but it is very slow while training. Please Help**         "
"Hi,    I only want to generate mask and know its corresponding class.   Could you please help me in disabling regressor branch."
"i'm training Mask RCNN for detecting flow chart components. but out of 7 components 1 type is detected at inference. I'm using the balloon example. my script is as follow.    ``class FlowchartConfig(Config):      """"""Configuration for training on the toy  dataset.      Derives from the base Config class and overrides some values.      """"""      # Give the configuration a recognizable name      NAME = ""Flowchart_symbols""        # We use a GPU with 12GB memory, which can fit two images.      # Adjust down if you use a smaller GPU.      IMAGES_PER_GPU = 2        # Number of classes (including background)      NUM_CLASSES = 1 + 7  # Background + flowchart        # Number of training steps per epoch      STEPS_PER_EPOCH = 100        # Skip detections with < 90% confidence      DETECTION_MIN_CONFIDENCE = 0.7      ############################################################  #  Dataset  ############################################################    class FlowchartDataset(utils.Dataset):        def load_flowchart(self, dataset_dir, subset):          """"""Load a subset of the flowchart dataset.          dataset_dir: Root directory of the dataset.          subset: Subset to load: train or val          """"""          # Add classes. We have only one class to add.          self.add_class(""Flowchart_symbols"", 1, ""arrow"")          self.add_class(""Flowchart_symbols"", 2, ""data"")          self.add_class(""Flowchart_symbols"", 3, ""process"")          self.add_class(""Flowchart_symbols"", 4, ""decision"")          self.add_class(""Flowchart_symbols"", 5, ""connection"")          self.add_class(""Flowchart_symbols"", 6, ""text"")          self.add_class(""Flowchart_symbols"", 7, ""terminator"")                      # Train or validation dataset?          assert subset in [""train"", ""val""]          dataset_dir = os.path.join(dataset_dir, subset)            # Load annotations          # VGG Image Annotator (up to version 1.6) saves each image in the form:          # { 'filename': '28503151_5b5b7ec140_b.jpg',          #   'regions': {          #       '0': {          #           'region_attributes': {},          #           'shape_attributes': {          #               'all_points_x': [...],          #               'all_points_y': [...],          #               'name': 'polygon'}},          #       ... more regions ...          #   },          #   'size': 100202          # }          # We mostly care about the x and y coordinates of each region          # Note: In VIA 2.0, regions was changed from a dict to a list.          annotations = json.load(open(os.path.join(dataset_dir, ""via_region_data.json"")))          annotations = list(annotations.values())  # don't need the dict keys            # The VIA tool saves images in the JSON even if they don't have any          # annotations. Skip unannotated images.          annotations = [a for a in annotations if a['regions']]            # Add images          for a in annotations:              # Get the x, y coordinaets of points of the polygons that make up              # the outline of each object instance. These are stores in the              # shape_attributes (see json format above)              # The if condition is needed to support VIA versions 1.x and 2.x.              if type(a['regions']) is dict:                  polygons = [r['shape_attributes'] for r in a['regions'].values()]              else:                  polygons = [r['shape_attributes'] for r in a['regions']]                 class_names_str_temp  = [r['region_attributes'] for r in a['regions'] if a['regions']]              # print(class_names_str_temp)              class_name_nums = []              class_names_str=[i for i in  class_names_str_temp if bool(i) != False]                for i in  class_names_str:                  if i['Flowchart_symbols'] == 'arrow':                      class_name_nums.append(1)                  if i['Flowchart_symbols'] == 'data':                      class_name_nums.append(2)                  if i['Flowchart_symbols'] == 'process':                      class_name_nums.append(3)                  if i['Flowchart_symbols'] == 'decision':                      class_name_nums.append(4)                  if i['Flowchart_symbols'] == 'connection':                      class_name_nums.append(5)                  if i['Flowchart_symbols'] == 'text':                      class_name_nums.append(6)                  if i['Flowchart_symbols'] == 'terminator':                      class_name_nums.append(7)                # load_mask() needs the image size to convert polygons to masks.              # Unfortunately, VIA doesn't include it in JSON, so we must read              # the image. This is only managable since the dataset is tiny.              image_path = os.path.join(dataset_dir, a['filename'])              image = skimage.io.imread(image_path)              height, width = image.shape[:2]                self.add_image(                  ""Flowchart_symbols"",                  image_id=a['filename'],  # use file name as a unique image id                  path=image_path,                  width=width, height=height,                  polygons=polygons,                  class_ids = np.array(class_name_nums))``"
"Hello! I'm trying to learn how to implement DeRPN (  into Mask RCNN and currently, I've identified the parts that need to be modified.    I'm trying to replace the ""ProposalLayer"" in MaskRCNN with the ""derpn_proposal_layers"" in DeRPN. However, I'm facing some problems as the Mask RCNN is in the TensorFlow framework whereas DeRPN is in the Caffe framework which I'm not familiar with. Can anybody help me with that?    "
"I get the following error when I try to deploy my trained model using Streamlit. Please help    InvalidArgumentError: Tensor input_image:0, specified in either feed_devices or fetch_devices was not found in the Graph.    Traceback:  File ""c:\users\hp 840\appdata\local\programs\python\python37\lib\site-packages\streamlit\ScriptRunner.py"", line 322, in _run_script      exec(code, module.__dict__)  File ""C:\Users\Hp 840\Documents\Deepika\predict.py"", line 132, in        predict_image(image)  File ""C:\Users\Hp 840\Documents\Deepika\predict.py"", line 119, in predict_image      results = model.detect([image], verbose=1)  File ""c:\users\hp 840\appdata\local\programs\python\python37\lib\site-packages\mrcnn\model.py"", line 2502, in detect      self.keras_model.predict([molded_images, image_metas, anchors], verbose=0)  File ""c:\users\hp 840\appdata\local\programs\python\python37\lib\site-packages\keras\engine\training.py"", line 1169, in predict      steps=steps)  File ""c:\users\hp 840\appdata\local\programs\python\python37\lib\site-packages\keras\engine\training_arrays.py"", line 294, in predict_loop      batch_outs = f(ins_batch)  File ""c:\users\hp 840\appdata\local\programs\python\python37\lib\site-packages\keras\backend\tensorflow_backend.py"", line 2715, in __call__      return self._call(inputs)  File ""c:\users\hp 840\appdata\local\programs\python\python37\lib\site-packages\keras\backend\tensorflow_backend.py"", line 2671, in _call      session)  File ""c:\users\hp 840\appdata\local\programs\python\python37\lib\site-packages\keras\backend\tensorflow_backend.py"", line 2623, in _make_callable      callable_fn = session._make_callable_from_options(callable_opts)  File ""c:\users\hp 840\appdata\local\programs\python\python37\lib\site-packages\tensorflow\python\client\session.py"", line 1489, in _make_callable_from_options      return BaseSession._Callable(self, callable_options)  File ""c:\users\hp 840\appdata\local\programs\python\python37\lib\site-packages\tensorflow\python\client\session.py"", line 1446, in __init__      session._session, options_ptr)"
"I am trying to train a simple model to recognize and segment hand drawn lines. Each image in my repo contains one line, so I generate the mask by just doing a threshold (which seems to work well based on inspection of masks).       (ve I am using for testing has requirements.txt installed)    For some reason the model building fails when I start training at first epoch, the kernel just crashes. I tried using a few different version combinations of tensorflow/keras and the `shapes` dataset works fine for both combos, but my training fails by either throwing a StopIteration exception      or my python kernel continuously crashes  (mac level error warning saying `python3 unexpectedly quit`)    Any ideas? Thanks!"
"Hello!  I'm getting this error when training on multiple GPUs, but I'm unsure whether or no to be concerned.           I'm using matterport's   script.  The comments at line 58 make it sound like the data is generated, sliced, and the pieces passed to the different GPUs.  Does this mean things are working as intended, it's just that Keras can't tell?    Thanks!  "
"I have trained with my own dataset and can do inference the model to get instance segmented masks. The segmented results look good, but now i want to calculate iou for all classes in an output masks.   Any suggestions how can i calculate iou for mask rcnn segmentations?.  Thanks in advance!  "
"Hello, I have 3D annotations and the images, can i use directly samples of mask-rcnn to create a model ?"
"Hi,  As mentioned in this discussion (  , we can use mask rcnn to segment small objects. However, when I tried mask rcnn on cell adhesion images, it didn't detect the very small  objects shown in the figure.  Can you please advise me on this issue?        <img width=""387"" alt=""small-objects"" src=""     "
i meet this problemï¼Œ Who solve itï¼Ÿ Please help me. thanks U
"When I tried to run the demo.ipynb, turns out this error.    #Process  ipython   %run demo.ipython  AttributeError: '_NamespacePath' object has no attribute 'sort'    then I tried to update pip3    pip3 install --upgrade pip  pip3 install --upgrade setuptools  ipython   %run demo.ipython  Intel MKL FATAL ERROR: Cannot load libmkl_avx2.so or libmkl_def.so."
"So, say I implemented augmentation in the manner seen here:      Is augmentation only applied to the training set, or is it also applied to the validation set?  Is there a way to control which dataset it's applied to, or apply a different set of augmentation to the validation set?    Thanks!"
It's my code:     Error:     where is mask_rcnn_nucleus.h5  file ? 
I'm getting this error when I try to deploy my trained Mask RCNN model as a web application using flask. I have my model saved as model.h5 and I have a separate weights file too. How to solve this error?
"Epoch 1/1  ---------------------------------------------------------------------------  RemoteTraceback                           Traceback (most recent call last)  RemoteTraceback:   """"""  Traceback (most recent call last):    File ""/home/sarojini/anaconda3/lib/python3.7/multiprocessing/pool.py"", line 121, in worker      result = (True, func(*args, **kwds))    File ""/home/sarojini/anaconda3/lib/python3.7/site-packages/keras/utils/data_utils.py"", line 650, in next_sample      return six.next(_SHARED_SEQUENCES[uid])    File ""/home/sarojini/ML projects/mask rnn test - cracks-weld/mrcnn/model.py"", line 1683, in data_generator      image_ids = np.copy(dataset.image_ids)  AttributeError: 'NoneType' object has no attribute 'image_ids'  """""""
"@siontist here is a simple implementation.  hope it will be of some help.         _Originally posted by @chohaku84 in      Hello , I noticed this . I'm new to deep learning . Can you please guide me on how to deploy mask rcnn model as a web application using Flask? I couldn't understand how to run the code you have linked to view results. Please help @chohaku84"
"I want to add custom augmentation when training the model. So I put  model.train(dataset_train, dataset_val,               epochs=10,               learning_rate=config.LEARNING_RATE / 10,               layers='all',              augmentation=augmentation)    It works fine for me on CPU (laptop). But when I try to run it on Colab GPU environment, it gives the error train() got an unexpected keyword argument 'augmentation'"
I want to change nms to soft-nms in this code?please help me how to do?
I am using matterport repository to train MASK RCNN on a custom dataset. I have been successful in training. Now I want to save the trained model and use it in a web application to detect objects. How do I save the mask rcnn model after training? Please guide me.
"    def detection(self,contour_path,config2,weights_path):              print(""we are started is thread"")                                #pid = os.fork()                  #print(""we are in  started a fork"",pid)              print(weights_path)              #config2.BATCH_SIZE=4                            thread_graph = Graph()              with thread_graph.as_default():                  thread_session = Session()                  with thread_session.as_default():                      model = modellib.MaskRCNN(mode=""inference"", config=config2, model_dir='')                      model.load_weights(weights_path, by_name=True)                      print("" is genrated from "",id(model))                     # print(id(model))                      graph = tf.get_default_graph()                                                          with graph.as_default():                  with thread_session.as_default():                      i=0                      while i len(contour_path) and i+1 len(contour_path) and i+1 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"")                                          #     with graph.as_default():                  #         with thread_session.as_default():                  #     #                  #             for i in contour_path:                  #                 image =  skimage.io.imread(i)                                            #                 result = model.detect([image,image],verbose=1)[0]                  #                 result['scores'] = np.array(result['scores'])                  #                 result['rois'] = np.array(result['rois'])                  #                 result['class_ids'] = np.array(result['class_ids'])                  #                 result['masks'] = np.array(result['masks'])                  #                 detection_results[i]=result                                                                      #embed()  def get_layer2_mask_object_detection_results_(self,contour_details ,final_result_dict,ratio,contour_images_folder,image_path,image_contours,config2,weights_path):          contour_paths=[]          global detection_results          detection_results={}          mapping={}                 for key in configfile['Classification_Config']['mapping'].keys():              mapping[configfile['Classification_Config']['mapping'][key]]=key            original_image = cv2.imread(original_image_path)          for detail in contour_details:              details = detail.split("";"")              contour_image_name = details[0]              key = ""_"".join(details[1:])              contour_paths.append(contour_image_name)              #ti=[0]*len(contour_paths)          i=0          #global graph          #graph = tf.get_default_graph()                  global timing          timing={}          rit=0;          #embed()          fclist=[]          print(""total no contourr"",len(contour_paths),""-------------------------------------"")          while i 1):                              temp.append(fclist[k][0])                      elif ( i==1):                          if( len(fclist[k])>1):                              temp.append(fclist[k][1])                      # if i==2:                      #     if( len(fclist[k])>2):                      #         temp.append(fclist[k][2])                      # if i==3:                      #     if( len(fclist[k])>3):                      #         temp.append(fclist[k][3])                                                                  print(""length of each window"",len(temp),""-------------------------"")                  th[rt]=threading.Thread(target=self.detection,args=(temp,config2,weights_path))                  th[rt].start()                  rt=rt+1            for i in range(rt):              th[i].join()              #  #embed()              #  ti[idex]=threading.Thread(target=self.detection,args=(contour_paths[i],config2,weights_path))              #  ti[idex].start()                 #ti[idex].run()                #  print(i,""->>>>"",ti[idex])                #  idex=idex+1                             #  i=i+1                             # didex=0              # print(""no of active threading >>>"",threading.enumerate() ,"">>>>>>>>>>>>>>>>>>>>>>>"")              # while (didex<4)and cidex<len(contour_paths):              #     ti[didex].jo       in()                #     didex=didex+1              #     print(""cidex"",cidex)              #     cidex += 1              # timing[rit]=time.time()-st              # rit=rit+1                #didex=0                           #embed()                # images.append(image)                           # result = model.detect([image],verbose=1)[0]              # result['scores'] = np.array(result['scores'])              # result['rois'] = np.array(result['rois'])              # result['class_ids'] = np.array(result['class_ids'])              # result['masks'] = np.array(result['masks'])              # detection_results[contour_path]=result              # threading.Thread()                 # i=0          # while(i<len(contour_paths)):          #     ti[i].join()          #     i=i+1          #embed()          print(model.config.NUM_CLASSES,""NUM_CLASSES"")            # result = model.detect(images, verbose=1)            # batch prediction          # for i in range(len(result)):          #     result[i]['scores'] = np.array(result[i]['scores'])          #     result[i]['rois'] = np.array(result[i]['rois'])          #     result[i]['class_ids'] = np.array(result[i]['class_ids'])          #     result[i]['masks'] = np.array(result[i]['masks'])          #     detection_results[contour_paths[i]]=result[i]            category_index = mapping          debug_image = cv2.imread(contour_images_folder+""/debug_defect.jpg"")            return self.get_layer2_classification(category_index, contour_details, final_result_dict, detection_results, contour_images_folder, image_contours, original_image,debug_image)  "
I dont get how this works. I am new to ML so i will need help with this. I use a windows system 
anyone has the faster-rcnn-resnet152 coco pretained weight?  or to make this pretrianed?>
Hi.    I have been trying to get Mask RCNN package installed into a python environment and I have been really struggling to get the package versions to work.    Does anyone have a package versions list that definitely works with repository which I can replicate?  What version of CUDA does this environment use?    I have managed to get installs of tensorflow-gpu v1.13.1 and v1.14 and v1.15 installed with the other dependencies. I have found:    v1.13.1 will only run on the cpu.    > v1.14 errors at the model fit execution complaining regarding .metrics_tensor doesn't exist.
"Hi,    I've trained Mask RCNN on my own dataset with 10 object classes where I set the NUM_CLASSES = 10 + 1. The problem is that the trained model only detects the first 9 classes. I've tested it on the training data, the 10th object class also cannot be recognized (there is even no ROIs). The classes have been added with add_class with the correct  class_id (from 1 to 10).    Does anyone have the same problem?    Best.    UPDATE:  Now I've tried another dataset with only one object class (NUM_CLASSES = 1 + 1), the training cannot be started, I only got the output Epoch 1/120 and it stucks there."
"Hello, the score of the test picture is close to 1, and the rectangle outside the mask is also displayed, but why is the mask not shown in the picture"
"I'm trying to train on data with rectangular & square annotations similar to bounding boxes.   In the `inspect_data` notebook, I can see the sample data and bounding boxes correctly but I'm not able to generate mini mask for my data.     `load_image_gt` works fine with `use_mini_mask=False` but it produces no output with `use_mini_mask=True`. So I guess the problem is with `minimize_mask` and `expand_mask` in `utils.py`.    Taking inspiration from   and  , I've modified the code like this         But this doesn't seem to work perfectly as the masks are not accurate.    !     Any idea how this can be fixed?"
"I want to use my custom mrcnn model in a jetson nano, while the memory of the nano is not enough,  is there any method to compress the mrcnn model, make it possible to deploy the mrcnn in a nano.    Thanks a lot !!"
When I use tenserflow 1.15.0 to train my model. It throw   `tensorflow.python.framework.errors_impl.NotFoundError: /nix/store/bplgjkgw62w5g78i77xjzn649hwkqc30-python3-3.7.6-env/lib/python3.7/site-packages/tensorflow_core/contrib/tensor_forest/python/ops/_model_ops.so: undefined symbol: _ZNK10tensorflow12tensorforest20DecisionTreeResource12TraverseTreeERKSt10unique_ptrINS0_13TensorDataSetESt14default_deleteIS3_EEiPiPNS0_8TreePathE  `  Does anyone know how to solve this issue? thx!!!!
"As we know, Mask_RCNN have some metric value, such as val_loss, val_rpn_class_loss.   Then, how to judge the quality of training through the loss curves? For example, I want to judge whether it's overfitting.  "
"I am attempting to detect objects that look similar except for the size. The object can range from 5 pixels to 150 pixels in diameter on 1024x1024 image. There can also be between 1-300 of these objects in a given image.    I seem to keep suffering from fitting to a certain size, the model tends to overfit to the smaller objects and does a fantastic job detecting them, but fails pretty obviously on larger objects. I found that dropping the RPN_ANCHOR_SCALES to (8,16,32,64,128) helped pick up smaller objects, but made larger ones more difficult to find. Does this variable strongly affect the dynamic range of objects that can be picked up? Can you extend the scales? for example: (8,16,32,64,128,256,512)? I know that you need to have BACKBONE_STRIDES be as long or longer than RPN_ANCHOR_SCALES, so I not sure what to do with those, and this may be on the wrong track.    Overall, is it possible to have a nicely generalized model with a large variation in size, and is there any advice to achieving it?    Thanks!"
Can anybody help me on how to deploy Mask RCNN model in a web application? I want to upload an image and display the segmented image back on the web page. But I'm not sure of how to deploy the neural network in the web application . Please help.   
"I want to use a Space Transformer Network (STN) to connect Mask-RCNN to get the parameters of each object, and use these parameters to calculate the rotation angle of these objects, and finally use the rotation angle to update the rotation rectangle(bbox) of the object    The current idea is to add STN after ROIAlign and get the rotation angle to modify mrcnn_bbox    Could you please give me some advice"
"All- Im trying to train my own dataset to detect a ball in an image. I have about 90 images in training dataset and similar in validation. Im using 2 images per GPU with 1 class, 30 epochs and 100 training steps per epoch. The training runs successfully and it is applying the colour splash. However its not picking up the ball (See image below). Is this an overfitting issue or has anyone seen this before?      /content/drive/My Drive/project/weights/mask_rcnn_coco.h5  WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2139: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.  Instructions for updating:  Use tf.where in 2.0, which has the same broadcast rule as np.where  WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/mrcnn/model.py:554: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/mrcnn/utils.py:200: The name tf.log is deprecated. Please use tf.math.log instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/mrcnn/model.py:601: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.  Instructions for updating:  box_ind is deprecated, use box_indices instead  WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.    training  Training network heads    Starting at epoch 0. LR=0.001    Checkpoint Path: /content/drive/My Drive/project/logs/nflball20200303T0934/mask_rcnn_nflball_{epoch:04d}.h5  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  In model:  rpn_model      rpn_conv_shared        (Conv2D)      rpn_class_raw          (Conv2D)      rpn_bbox_pred          (Conv2D)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)  WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.    /usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.    UserWarning('Using a generator with `use_multiprocessing=True`'  WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.    Epoch 1/30  100/100    "
None
" I do a project ,about crystal detection, I need  use mask rcnn to detect the crystal in the picture ,and need mask them in the picture .after label it ,and train it , and then, test it .  ----------------------------------------------------------------------------------------------  I label it like this ï¼š    !     and get the labeled picture like this ï¼š       !       thenï¼Œtrian themï¼Œabout 21 picturesã€‚    this is train log:    `nohup: ignoring input  2020-03-01 15:42:12.932951: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.  2020-03-01 15:42:12.933034: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.  2020-03-01 15:42:12.933044: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.  2020-03-01 15:42:12.933052: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.  2020-03-01 15:42:12.933060: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX512F instructions, but these are available on your machine and could speed up CPU computations.  2020-03-01 15:42:12.933068: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.    Configurations:  BACKBONE                       resnet50  BACKBONE_STRIDES                    !       ------------------------------------------------------------------------------------------                 "
"I trained my own data like `coco/coco.py`, but it was so slowly , like 5s/step,  it caused by  data processing? "
"Hi,    i wanna use rcnn with tflite and i have problem with convert saved h5 model to tflite model.  When i try this     i get error      What am I doing wrong?"
"Hello,  I'm getting this error each time I'm trying to use augmentation.    > SuspiciousSingleImageShapeWarning: You provided a numpy array of shape (1024, 1024, 37) as a single-image augmentation input, which was interpreted as (H, W, C). The last dimension however has a size of >=32, which indicates that you provided a multi-image array with shape (N, H, W) instead. If that is the case, you should use e.g. augmenter(imageS= ) or augment_imageS( ). Otherwise your multi-image input will be interpreted as a single image during augmentation.    My config and the code:               What am I doing wrong?  Thank you in advance."
"Hello guys,    I encountered an error while trying to import kashgari library to my codalab notebook. It doesn't seem to read keras function get_file although I installed the latest version of tensorflow. Any ideas would be really helpful. Below you can see the  error.    Thanks   !     "
"if os.name is 'nt':              workers = 0          else:              workers = multiprocessing.cpu_count()            self.keras_model.fit_generator(              train_generator,              initial_epoch=self.epoch,              epochs=epochs,              steps_per_epoch=self.config.STEPS_PER_EPOCH,              callbacks=callbacks,              validation_data=val_generator,              validation_steps=self.config.VALIDATION_STEPS,              max_queue_size=100,              workers=workers,              use_multiprocessing=True,          )  My use_multiprocessing is True"
"while training my dataset I got following error:      Epoch 1/30    Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1265, in load_image_gt      class_ids = class_ids[_idx]  IndexError: boolean index did not match indexed array along dimension 0; dimension is 2426 but corresponding boolean dimension is 127  multiprocessing.pool.RemoteTraceback:   """"""  Traceback (most recent call last):    File ""/usr/lib/python3.6/multiprocessing/pool.py"", line 119, in worker      result = (True, func(*args, **kwds))    File ""/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py"", line 641, in next_sample      return six.next(_SHARED_SEQUENCES[uid])    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1265, in load_image_gt      class_ids = class_ids[_idx]  IndexError: boolean index did not match indexed array along dimension 0; dimension is 2426 but corresponding boolean dimension is 127  """"""    The above exception was the direct cause of the following exception:    Traceback (most recent call last):    File ""ead.py"", line 433, in        train(model, args.dataset, args.subset)    File ""ead.py"", line 237, in train      layers='heads')    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 2374, in train    File ""/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 1658, in fit_generator      initial_epoch=initial_epoch)    File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py"", line 181, in fit_generator      generator_output = next(output_generator)    File ""/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py"", line 733, in get      six.reraise(*sys.exc_info())    File ""/usr/local/lib/python3.6/dist-packages/six.py"", line 693, in reraise      raise value    File ""/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py"", line 702, in get      inputs = future.get(timeout=30)    File ""/usr/lib/python3.6/multiprocessing/pool.py"", line 644, in get      raise self._value    File ""/usr/lib/python3.6/multiprocessing/pool.py"", line 119, in worker      result = (True, func(*args, **kwds))    File ""/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py"", line 641, in next_sample      return six.next(_SHARED_SEQUENCES[uid])    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1265, in load_image_gt  IndexError: boolean index did not match indexed array along dimension 0; dimension is 2426 but corresponding boolean dimension is 127"
"in file model.py  line 1528   '''   Closest gt box (it might have IoU < 0.7)    gt = gt_boxes[anchor_iou_argmax[i]]  '''  When the best match of gt  index(x)  ,is anchor(y), but the biggest  match of anchor(y)  is not necessarily gt (x) ,so the code is  actually  not correct in  the case  # 2. Set an anchor for each GT box (regardless of IoU value)."
"In the paper of FPN and mask, it used Formula which is  roi_level = log2_graph(tf.sqrt(h * w) / (224.0 / tf.sqrt(image_area)))   roi_level = tf.minimum(5, tf.maximum(2, 4 + tf.cast(tf.round(roi_level), tf.int32)))    and it give me a example is 224x224 ROI (in pixels) maps to P4  But i learn the paper of FPN and Mask-r-cnn, the scale of roi is 128 * 128 in P4, i can not understand 224x224 ROI (in pixels) maps to 128*128?  Please help!!"
"Help! I'm begginer with AI. I run the DFG trafic sign dataset with COCO format, run the inpect_model, and i dound when RPN Prediction failed. How can i fix it?  !   !   !   "
Does anyone have recommendation for what size images to use for training? My custom object to segment fills about 20% of the frame. I am considering using Google Colab Pro VM.
"hi! I'm trying to see accuracy while I'm training the model.    For that purpose, I have modified the model.py inside mrcnn folder like this:    !     I have seen in so many forums that this works for so many people in other neural networks, but not for me in MaskRCNN. When I train again the model with this modification, is not showing accuracy and in history.history() is not saving it. Only the different losses defined in MaskRCNN. What I want to know is how to modify the implementation in order to obtain values 'acc' and 'val_acc' like this:    !     Thanks in advance!!"
I have dataset witch is not in annotated format. it have directory like this -  training_images  //(folder name)          1.jpg          1.txt (bounding box information)          2.jpg          2.txt  semantic_segmentayion //(folder name)         original_images(contains all jpg images)         training_data semantic_segmentation(contains .tif images i.e. masks of images)    I need to train for more class and I did try to modify funtions: load_objects() and load_mask() without success.   i did not understand  how i need to modify class_id  in load_mask function .    So please help. I really appreciate your reading and help!                     
"Hi @waleedka,  Can you please share instruction on how to recrate the weights in release 2.0?  Folowing the current regime seems to yeild sub-optimal results (AP~24 when training with a batch size 16 and 4x scaled LR)"
None
"I have a dataset of 3000 images of dimension 1024x1024. I don't want to resize or crop and I want to train all the layers.  I can train only heads layer but my GPU is mostly idle. It doesn't use more than 10% of it. But if I try to train it for all the layers or resnet 3+ layers, then it crushes saying root resources as exhausted and It doesn't train on GPU as usual.   I am using GTX 1060 6GB, Anaconda and Windows 10.  Tensorflow-gpu==1.14  CUDA=9  Keras==2.2  does anyone face any similar problems?"
"I want to segment sports ground(field) images, I have 1000 ground images and I have attached images of all losses below, The issue is all validation losses are decreasing well except Mask Loss. any suggestion why it is not decreasing?  Configurations:  BACKBONE                       resnet50  BACKBONE_STRIDES                  !   !   !   "
"I want to add a loss with   `global_mask_loss = KL.Lambda(lambda x: build_global_mask_loss(*x), name=""global_mask_loss"")([input_gt_global_masks, gts, config])`  but in the summary it is a such name `tf_op_layer_global_mask_loss/Me`  and when I get layer, it occurs error `ValueError: No such layer: global_mask_loss`"
"Hello and first of all, thank's a lot for the work!  I just wanted to ask if it is possible to use a high amount of greyscale images. For excample 20, 30 or 40.  ...ans has anyone changed the code in a way to make use of it right now?    Thanks!     "
"I have successfully trained a model to identity X & Y objects.  As a second stage, I would like to teach the same model how to identify Z objects so that the end result would be a model capable of telling X, Y & Z.    The problem: X,Y are very different from Z (imagine road & walkways vs. car) so I suspect the RPN would be very different and therefore 'confusing' for the model.  It took a 2k+ dataset training all layers to teach X & Y, whilst training on Z individually is easy with a small set (100~) and transfer learning (when using COCO dataset).    Would it make sense to retrain the model adding Z class (I can see some input on how to do this on #828) or will a second model be more useful in such scenario?    Thanks"
"hi, i have modified the  backbone and its layer  of  **mask RCNN**  algorithm.    i am getting theloss to be **loss: nan**    the description of the error is as below    Epoch 1/100  2020-02-13 03:32:27.707341: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10  2020-02-13 03:32:29.183377: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7  2020-02-13 03:32:35.797619: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.  2020-02-13 03:32:35.854928: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.   99/100 [============================>.] - ETA: 1s - loss: nan - rpn_class_loss: 2701735.7973 - rpn_bbox_loss: nan - mrcnn_class_loss: 0.6861 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:2197: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.    UserWarning('Using a generator with `use_multiprocessing=True`'  WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:791: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.    100/100 [==============================] - 337s 3s/step - loss: nan - rpn_class_loss: 2674718.4462 - rpn_bbox_loss: nan - mrcnn_class_loss: 0.6862 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: 0.6931 - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 0.6931 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 2/100  100/100 [==============================] - 126s 1s/step - loss: nan - rpn_class_loss: 0.6931 - rpn_bbox_loss: nan - mrcnn_class_loss: 0.6931 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: 0.6931 - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 0.6931 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 3/100  100/100 [==============================] - 127s 1s/step - loss: nan - rpn_class_loss: 0.6931 - rpn_bbox_loss: nan - mrcnn_class_loss: 0.6931 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: 0.6931 - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 0.6931 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 4/100  100/100 [==============================] - 126s 1s/step - loss: nan - rpn_class_loss: 0.6931 - rpn_bbox_loss: nan - mrcnn_class_loss: 0.6931 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: 0.6931 - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 0.6931 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 5/100  100/100 [==============================] - 126s 1s/step - loss: nan - rpn_class_loss: 0.6931 - rpn_bbox_loss: nan - mrcnn_class_loss: 0.6931 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: 0.6931 - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 0.6931 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 6/100  100/100 [==============================] - 126s 1s/step - loss: nan - rpn_class_loss: 0.6931 - rpn_bbox_loss: nan - mrcnn_class_loss: 0.6931 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: 0.6931 - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 0.6931 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 7/100  100/100 [==============================] - 127s 1s/step - loss: nan - rpn_class_loss: 0.6931 - rpn_bbox_loss: nan - mrcnn_class_loss: 0.6931 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: 0.6931 - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 0.6931 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 8/100  100/100 [==============================] - 126s 1s/step - loss: nan - rpn_class_loss: 0.6931 - rpn_bbox_loss: nan - mrcnn_class_loss: 0.6931 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: 0.6931 - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 0.6931 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 9/100  100/100 [==============================] - 126s 1s/step - loss: nan - rpn_class_loss: 0.6931 - rpn_bbox_loss: nan - mrcnn_class_loss: 0.6931 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: 0.6931 - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 0.6931 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 10/100  100/100 [==============================] - 126s 1s/step - loss: nan - rpn_class_loss: 0.6931 - rpn_bbox_loss: nan - mrcnn_class_loss: 0.6931 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: 0.6931 - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 0.6931 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 11/100  100/100 [==============================] - 126s 1s/step - loss: nan - rpn_class_loss: 0.6931 - rpn_bbox_loss: nan - mrcnn_class_loss: 0.6931 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: 0.6931 - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 0.6931 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 12/100  100/100 [==============================] - 126s 1s/step - loss: nan - rpn_class_loss: 0.6931 - rpn_bbox_loss: nan - mrcnn_class_loss: 0.6931 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: 0.6931 - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 0.6931 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 13/100  100/100 [==============================] - 127s 1s/step - loss: nan - rpn_class_loss: 0.6931 - rpn_bbox_loss: nan - mrcnn_class_loss: 0.6931 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: 0.6931 - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 0.6931 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 14/100  100/100 [==============================] - 126s 1s/step - loss: nan - rpn_class_loss: 0.6931 - rpn_bbox_loss: nan - mrcnn_class_loss: 0.6931 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: 0.6931 - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 0.6931 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 15/100  100/100 [==============================] - 126s 1s/step - loss: nan - rpn_class_loss: 0.6931 - rpn_bbox_loss: nan - mrcnn_class_loss: 0.6931 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: 0.6931 - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 0.6931 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 16/100  100/100 [==============================] - 126s 1s/step - loss: nan - rpn_class_loss: 0.6931 - rpn_bbox_loss: nan - mrcnn_class_loss: 0.6931 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: 0.6931 - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 0.6931 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 17/100    6/100 [>.............................] - ETA: 1:36 - loss: nan - rpn_class_loss: 0.6931 - rpn_bbox_loss: nan - mrcnn_class_loss: 0.6931 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00ERROR:root:Error processing image {'id': '20181221154818131_0003.jpg.jpg', 'source': 'NamePlate', 'path': 'ROI4/train/20181221154818131_0003.jpg.jpg', 'width': 1024, 'height': 1024, 'polygons': [{'name': 'polygon', 'all_points_x': [615, 615, 1019, 1025, 621, 615], 'all_points_y': [902, 902, 898, 1009, 1015, 902]}]}  Traceback (most recent call last):    Can anyone give me some suggestion about how to solve that? Many thanks!    "
"Hello Everyone , so  , i want to run a detection on multiple images / an entier folder of input images , but still trying to figure out how to accomplish that , i'm using the command lines , and here's my code :             ` # USAGE           # python maskrcnn_fashion_predict.py --weights mask_rcnn_fashion_0019.h5 --labels              fashion_labels.txt --image images/prv_image_11.jpg             # import the necessary packages           from mrcnn.config import Config              from mrcnn import model as modellib           from mrcnn import visualize           import numpy as np           import colorsys           import argparse           import imutils           import random           import cv2           import os           import skimage.io             import matplotlib.image as mpimg           import cv2             import matplotlib.pyplot as plt           import numpy as np             # construct the argument parse and parse the arguments           ap = argparse.ArgumentParser()           ap.add_argument(""-w"", ""--weights"", required=True,            help=""path to Mask R-CNN model weights pre-trained on COCO"")           ap.add_argument(""-l"", ""--labels"", required=True,            help=""path to class labels file"")           ap.add_argument(""-c"", ""--confidence"", type=float, default=0.5,            help=""minimum probability to filter weak detections"")           ap.add_argument(""-i"", ""--image"", required=True,            help=""path to input image to apply Mask R-CNN to"")           args = vars(ap.parse_args())            # load the class label names from disk, one label per line          CLASS_NAMES = open(args[""labels""]).read().strip().split(""\n"")            # generate random (but visually distinct) colors for each class label          # (thanks to Matterport Mask R-CNN for the method!)          hsv = [(i / len(CLASS_NAMES), 1, 1.0) for i in range(len(CLASS_NAMES))]          COLORS = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))          random.seed(42)          random.shuffle(COLORS)            class SimpleConfig(Config):           # give the configuration a recognizable name            NAME = ""fashion""             # set the number of GPUs to use along with the number of images           # per GPU           GPU_COUNT = 1            IMAGES_PER_GPU = 1             # number of classes (we would normally add +1 for the background           # but the background class is *already* included in the class           # names)                     NUM_CLASSES = 1 + 3              # Skip detections with < 90% confidence            DETECTION_MIN_CONFIDENCE = args[""confidence""]                            # initialize the inference configuration                   config = SimpleConfig()                     # initialize the Mask R-CNN model for inference and then load the                   # weights                   print(""[INFO] loading Mask R-CNN model..."")                   model = modellib.MaskRCNN(mode=""inference"", config=config,           model_dir=os.getcwd())                  model.load_weights(args[""weights""], by_name=True)                    # load the input image, convert it from BGR to RGB channel                 # ordering, and resize the image                 # default value 512 form the width                  image = cv2.imread(args[""image""])                 image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)                 image = imutils.resize(image, width=1150)                  # perform a forward pass of the network to obtain the results                print(""[INFO] making predictions with Mask R-CNN..."")                r = model.detect([image], verbose=1)[0]                     image = visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'],                                                  ['BG', 'top', 'boots' , 'bag'], r['scores'],                                                 title="""")                      i = 0                   mask = r[""masks""]                  for i in range(mask.shape[2]):                 image = cv2.imread(args[""image""])                # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)                 image = imutils.resize(image, width=1150)                   for j in range(image.shape[2]):                  image[:,:,j] = image[:,:,j] * mask[:,:,i]                    # cv2.figure(figsize=(8,8))                  # cv2.imshow(""Output"", image)                  # cv2.waitKey()                  filename = ""Output/segment_%d.jpg""%i                  cv2.imwrite(filename,image)                   i+=1  ` Any Suggestion on how to accomplish would be great , thank you."
"Hello, can anybody please tell me how long does it take to train the network with ms coco dataset from scratch? It would be great if anybody can give me a reference that shows the training time required by mask rcnn."
"Hi,  I have used the same code with same training parameter in two different GPU. One is GTX 1080ti and the othe is Tesla V100. However the results are completely different in these two different machines. Can anybody tell me why is this happening and how to figure this out?"
"Hi,     I am also having this issue, as I annotated with via-2.0.8. Is there any way to modify the balloon.py code so that a via-2.x formatted JSON file can be read in? I don't know enough about JSON parsing sadly.    Like @zulfiqarbolt I have also noticed that below the problem code (line 117) at lines 120-128 there is code for via-1.x and via-2.x formats. It seems this needs to also be done for line 117.    EDIT:    After playing around a bit, I believe this code will remove unlabelled images when the JSON has been formatted with via-2.x. Replace line 117 with the following:         Note this has not been tested on a VIA-1.x formatted file, this is just using the code from the balloon.py file as a reference for how they should be handled. Happy to make this a pull request if people want.     @flit86 this may help you?    _Originally posted by @Trotts in  "
"Hey Guys,  i recently started using MaskRCNN for Video Instance Segmentation. I trained it on my Dataset and it is working quite good. I found a problem, when i run it on Videos that i want to improve:    The MaskRCNN Code predicts for each image its Bounding boxes, masks and so on, but in some successive frames, it misses predictions, that it finds in the next frame then. I tried to solve this by saving my predictions in a list and looking for the missing predictions. Then i want to add the missing information to the frame where my model missed some predictions that should be there.  I got it working for Bounding Boxes, Scores and class ids.   Now to my Problem:  Since the 'masks' numpy array which you get from the detect() method is n dimensional of a shape of (1080, 1920, 'number of class ids which are predicted on this image) i dont really know how to add the missing mask to this maskshape...    !     Hope you could follow what i am trying to desribe :)    "
"Hello,  when i run mask rcnn get the following error    Epoch 1/100  Traceback (most recent call last):    File ""C:\Users\HP\Anaconda3\envs\tf-gpu\lib\site-packages\keras\utils\data_utils.py"", line 710, in get      raise StopIteration()  StopIteration    The above exception was the direct cause of the following exception:    Traceback (most recent call last):    File "".\NamePlate.py"", line 384, in        train(model)    File "".\NamePlate.py"", line 219, in train      layers='heads')    File ""D:\gpu\projects1\Mask_RCNN-Multi-Class-Detection\mrcnn\model.py"", line 2381, in train      use_multiprocessing=True,    File ""C:\Users\HP\Anaconda3\envs\tf-gpu\lib\site-packages\keras\legacy\interfaces.py"", line 87, in wrapper      return func(*args, **kwargs)    File ""C:\Users\HP\Anaconda3\envs\tf-gpu\lib\site-packages\keras\engine\training.py"", line 2065, in fit_generator      generator_output = next(output_generator)  RuntimeError: generator raised StopIteration     What are the other changes that i would have to bring in the code, and the parameters. Kindly help."
"Hello,    i use the Mask R-CNN for my project. I have to detect defects in images.  However i am not sure if my parameters (epochs, batches, optimizers, learning_rate) are suited for my problem.  I am actually doining this step by hand and try different possibilities and look what changes in accuracy.  A friend of my told me that i can try to use GridSearchCV to automaticaally select the best hyperparameters for my neural network.    I have tried to incorporate GridSearchCV but i don't know where to put exactly the code.  For an easy example like the following it works but I don't know what to do in Mask R-CNN.         Can somebody help me out?"
"Every anchor maps to certain coordinates in the input image, and it also predicts the coordinates of a bbox. When IoU is computed, if for gt bbox and the anchor or gt bbox and predicted bbox?"
"Hi,     During my classification process, I would like to visualize the focus areas of the Mask RCNN neural network.     I am aware of methods like GradCAM that enable the same.  I tried implementing the GradCAM basics to the mrcnn_class_logits layer, but this gave me an incorrect output.     Is there any way to visualize where the neural network is focusing before it assigns the class to a particular instance?     Any help or guidance will be appreciated. "
I have a spine mri dataset that has all of its training and test data in nii.gz format. How can this dataset be used to train the network?
How to get IoU and AP for balloon dataset
"Hi Matterport,    I am using MASK-RCNN for instance segmentation and I ran into a RAM memory usage (not GPU RAM) problem when doing inference. As I am analyzing images continuously, my RAM usage increases. I also have a swap file (64GB) and I can't analyze more than approximately 2300 images (448x448 pixels). I am not sure what causes this increase in RAM memory.     I am using the command ""detect_and_color_splash"" for analyzing.     I came across related questions but I am not sure if they can be applied for my case. Could it be related to memory footprint (  or memory leak (      Here are some details. I can provide more information if needed.   - Backbone : ResNet101   - Images : 448x448 pixels   - The model has been trained with coco weights with 1660 images. Only ""head"" layers have been trained.   - GPU : GeForce RTX 2060 (6GB)   - RAM : 32GB   - swap file : 64GB  "
None
"Part of my code is down below:    r = model.detect([image], verbose=1)[0]      # loop over of the detected object's bounding boxes and    # masks, drawing each as we go along    for i in range(0, r[""rois""].shape[0]):     mask = r[""masks""][:, :, i]     image = visualize.apply_mask(image, mask,      (1.0, 0.0, 0.0), alpha=0.5)     image = visualize.draw_box(image, r[""rois""][i],      (1.0, 0.0, 0.0))    While the mask it returns is clipped by a bounding box, and the mask seems not complete  Could you help me solve this?  Thanks a Lot!"
There's one thing I could never fully understand: does each anchor predict bbox for exactly one object? What is it has the best overlap/IoU with two or more objects? 
"Hi,  Please bare with me if I am misunderstanding something or everything because I have very limited knowledge in ML.    **Requirements:**    Consider a dataset of personnel card images (front & back). I need to classify each image with some labels such as:     1. Front   2. Back   3. Blurry   4. Cropped (Card object in image is cropped in a way some information might be unreadable)   5. Good (Negative for blurry and cropped)   6. Bad-ID (ID number not readable - may be covered by another object)   7. ID (Negative for Bad-ID)   8. Signed (User has signed back of her card)   9. Not-Signed (Negative for 8)    As it's obvious an object would have multiple labels. (e.g. Labels for an acceptable image are:  1,5,7 or 2,5,8)    I'm currently trying to achieve above by using Mask-RCNN. I'd place similar masks around each card in an image and label each of them with appropriate class ids. I used ballon sample as reference.    **My questions:**     1. Is using deep-learning (and more specifically Mask-RCNN) a good practice for detecting blurry, cropped, signed or not signed scenarios? I know there are some algorithms for this that I can apply after a simple classification of Front or Back. But the problem is that I haven't experienced good results with those algorithms so I decided to do everything in Deep-Learning.   2. My current test results are not really promising with Mask-RCNN (In best case it only detects one class-id [front/back]). I just stared off with very small dataset (80 train + 20 val) to see if I can get it to work. Now I'm in doubt if the problem with my tests is for small dataset I used or I'm totally doing it in a wrong way.    Any advice is highly appreciated.  "
"Hello Everyone , i'm trying to run a predicton on a my set of images , this is the code , that i'm using :   `             # import the necessary packages        from mrcnn.config import Config        from mrcnn import model as modellib        from mrcnn import visualize        import numpy as np        import colorsys        import argparse        import imutils         import random        import cv2        import os          # construct the argument parse and parse the arguments        ap = argparse.ArgumentParser()        ap.add_argument(""-w"", ""--weights"", required=True,   help=""path to Mask R-CNN model weights pre-trained on COCO"")        ap.add_argument(""-l"", ""--labels"", required=True,   help=""path to class labels file"")        ap.add_argument(""-i"", ""--image"", required=True,   help=""path to input image to apply Mask R-CNN to"")        args = vars(ap.parse_args())          # load the class label names from disk, one label per line        CLASS_NAMES = open(args[""labels""]).read().strip().split(""\n"")          # generate random (but visually distinct) colors for each class label       # (thanks to Matterport Mask R-CNN for the method!)       hsv = [(i / len(CLASS_NAMES), 1, 1.0) for i in range(len(CLASS_NAMES))]       COLORS = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))       random.seed(42)       random.shuffle(COLORS)         class SimpleConfig(Config):        # give the configuration a recognizable name        NAME = ""fashion""          # set the number of GPUs to use along with the number of images        # per GPU        GPU_COUNT = 1        IMAGES_PER_GPU = 1           # number of classes (we would normally add +1 for the background         # but the background class is *already* included in the class         # names)         NUM_CLASSES = 1 + 3          # initialize the inference configuration        config = SimpleConfig()          # initialize the Mask R-CNN model for inference and then load the        # weights        print(""[INFO] loading Mask R-CNN model..."")         model = modellib.MaskRCNN(mode=""inference"", config=config,   model_dir=os.getcwd())        model.load_weights(args[""weights""], by_name=True)          # load the input image, convert it from BGR to RGB channel        # ordering, and resize the image        # default value 512 form the width        image = cv2.imread(args[""image""])        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)        image = imutils.resize(image, width=1150)          # perform a forward pass of the network to obtain the results        print(""[INFO] making predictions with Mask R-CNN..."")         r = model.detect([image], verbose=1)[0]           # loop over of the detected object's bounding boxes and masks          for i in range(0, r[""rois""].shape[0]):           # extract the class ID and mask for the current detection, then           # grab the color to visualize the mask (in BGR format)           classID = r[""class_ids""][i]           mask = r[""masks""][:, :, i]           # by default : color = COLORS[classID][::-1]           color = COLORS[classID][::2]             # visualize the pixel-wise mask of the object           image = visualize.apply_mask(image, mask, color, alpha=0.5)                 # convert the image back to BGR so we can use OpenCV's drawing          # functions           image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)            # loop over the predicted scores and class labels          for i in range(0, len(r[""scores""])):        # extract the bounding box information, class ID, label, predicted        # probability, and visualization color        (startY, startX, endY, endX) = r[""rois""][i]        classID = r[""class_ids""][i]        label = CLASS_NAMES[classID]        score = r[""scores""][i]        color = [int(c) for c in np.array(COLORS[classID]) * 255]          # draw the bounding box, class label, and score of the object        cv2.rectangle(image, (startX, startY), (endX, endY), color, 2)        text = ""{}: {:.3f}"".format(label, score)        y = startY - 10 if startY - 10 > 10 else startY + 10        cv2.putText(image, text, (startX, y), cv2.FONT_HERSHEY_SIMPLEX,    0.6, color, 2)             # show the output image           cv2.imshow(""Output"", image)           cv2.waitKey()`    Obviously I made some chages , because this code was originally using the **Coco** Dataset , with it's weights and so on , what i want is to run the sccript , using this Dataset , from this repo :        So , i downloaded the weights , made some changes in the code above and the the **txt** file for the new classes , since it is completely different from the **Coco** one , and when i run the script above with this Command :   `     maskrcnn_fashion_predict.py --weights mask_rcnn_fashion_0019.h5 --labels fashion_labels.txt --image images\prv_image_10.jpg  `    I get this Error :     `      Traceback (most recent call last):           File ""maskrcnn_fashion_predict.py"", line 82, in             image = visualize.apply_mask(image, mask, color, alpha=0.5)           File ""C:\Users\zobirim\AppData\Local\Continuum\anaconda3\lib\site-packages\mask_rcnn-2.1-py3.7.egg\mrcnn\visualize.py"", line 78, in apply_mask  IndexError: tuple index out of range   `    Any Help on how to resolve this issue will be warmly welcomed , thank you all."
"Hello Everyone , so i want to take the segmented regions outputed by the model , to then pass them into a cnn for classification.  let me be more explicit , i have this image , ouputed by my model :           !     well , this is just a test to see if the model works fine on my images , so the next step would be to segment the shirts , shorts , shooes and then , what i want to do , is to take the segmented shirts , shorts and shooes and then pass them into a cnn for classification , cause i want to get the brands.  so , according to you guys , is it possible to do it ? , thank you all.  "
"Hi. I successfully implemented the demo versions-(coco, balloons, and shapes). Now i want to train the model on my custom dataset. I have dataset of almost 200 different meals and i have annotated them by using labelme. As i have noticed that those balloons are annotated by using VGG annotated tools and their json files are different. I have converted into coco format by using labelme tool and my json file is different. I  !    tried to use balloon.py file to train but i got error of keyerror: region.  I have json file in the following format.       {""info"": {""description"": null, ""url"": null, ""version"": null, ""year"": 2020, ""contributor"": null, ""date_created"": ""2020-01-23 12:15:25.787447""}, ""licenses"": [{""url"": null, ""id"": 0, ""name"": null}], ""images"": [{""license"": 0, ""url"": null, ""file_name"": ""JPEGImages/40014010215649790351.jpg"", ""height"": 384, ""width"": 480, ""date_captured"": null, ""id"": 0}, {""license"": 0, ""url"": null, ""file_name"": ""JPEGImages/40014010215649785791.jpg"", ""height"": 384, ""width"": 480, ""date_captured"": null, ""id"": 1}, ""license"": 0, ""url"": null, ""file_name"": ""JPEGImages/40014010215649781051.jpg"", ""height"": 384, ""width"": 480, ""date_captured"": null, ""id"": 100}], ""type"": ""instances"", ""annotations"": [{""id"": 0, ""image_id"": 0, ""category_id"": 6, ""segmentation"": [[309.0569105691057, 143.90243902439025, 423.6910569105691, 265.0406504065041]], area"": 17664.0, ""bbox"": [261.0, 113.0, 128.0, 138.0], ""iscrowd"": 0}, {""id"": 410, ""image_id"": 100, ""category_id"": 2, ""segmentation"": [[94.42276422764229, 190.2439024390244, 236.6991869918699, 327.6422764227642]], ""area"": 20825.0, ""bbox"": [147.0, 58.0, 119.0, 175.0], ""iscrowd"": 0}], ""categories"": [{""supercategory"": null, ""id"": 0, ""name"": ""_background_""}, {""supercategory"": null, ""id"": 1, ""name"": ""0""}, {""supercategory"": null, ""id"": 2, ""name"": ""1""}, {""supercategory"": null, ""id"": 3, ""name"": ""3""}, {""supercategory"": null, ""id"": 4, ""name"": ""14""}, {""supercategory"": null, ""id"": 5, ""name"": ""17""}, {""supercategory"": null, ""id"": 6, ""name"": ""26""},    I need to train my model, so could please anyone suggest me to what should i do and how can i modify the ballooon or coco.py file so i can use it to train on my dataset.  Thanks:"
"I want to use mask-rcnn for text detection, but I do not know how to subclass dataset, could you please give me some help?"
"Can this be used for spine segmentation? If yes, can you please help me doing it. I'm a beginner and find it very difficult to follow instructions.I'd be very grateful for any help."
"I have a dataset of hand-drawn flowcharts. After the training of the dataset, only one type of class is predicted. my initial training dataset consists of 40 images. This is my configuration class         !     "
"`Traceback (most recent call last):    File ""/Users/apple/PycharmProjects/design/venvnew/Mask_RCNN-master/mrcnn/mrcnn_train.py"", line 225, in        layers='all')    File ""/Users/apple/PycharmProjects/design/venvnew/Mask_RCNN-master/mrcnn/new_model.py"", line 574, in train      use_multiprocessing=False,    File ""/Users/apple/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/keras/engine/training.py"", line 1297, in fit_generator      steps_name='steps_per_epoch')    File ""/Users/apple/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/keras/engine/training_generator.py"", line 265, in model_iteration      batch_outs = batch_function(*batch_data)    File ""/Users/apple/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/keras/engine/training.py"", line 973, in train_on_batch      class_weight=class_weight, reset_metrics=reset_metrics)    File ""/Users/apple/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 264, in train_on_batch      output_loss_metrics=model._output_loss_metrics)    File ""/Users/apple/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/keras/engine/training_eager.py"", line 311, in train_on_batch      output_loss_metrics=output_loss_metrics))    File ""/Users/apple/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/keras/engine/training_eager.py"", line 252, in _process_single_batch      training=training))    File ""/Users/apple/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/keras/engine/training_eager.py"", line 127, in _model_loss      outs = model(inputs, **kwargs)    File ""/Users/apple/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 891, in __call__      outputs = self.call(cast_inputs, *args, **kwargs)    File ""/Users/apple/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/keras/engine/network.py"", line 708, in call      convert_kwargs_to_constants=base_layer_utils.call_context().saving)    File ""/Users/apple/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/keras/engine/network.py"", line 860, in _run_internal_graph      output_tensors = layer(computed_tensors, **kwargs)    File ""/Users/apple/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 891, in __call__      outputs = self.call(cast_inputs, *args, **kwargs)    File ""/Users/apple/PycharmProjects/design/venvnew/Mask_RCNN-master/mrcnn/detect_target_layer.py"", line 219, in call      self.config.IMAGES_PER_GPU, names=names)    File ""/Users/apple/PycharmProjects/design/venvnew/Mask_RCNN-master/mrcnn/utils.py"", line 822, in batch_slice      output_slice = graph_fn(*inputs_slice)    File ""/Users/apple/PycharmProjects/design/venvnew/Mask_RCNN-master/mrcnn/detect_target_layer.py"", line 218, in        w, x, y, z, self.config),    File ""/Users/apple/PycharmProjects/design/venvnew/Mask_RCNN-master/mrcnn/detect_target_layer.py"", line 92, in detection_targets_graph      crowd_overlaps = overlaps_graph(proposals, crowd_boxes)    File ""/Users/apple/PycharmProjects/design/venvnew/Mask_RCNN-master/mrcnn/detect_target_layer.py"", line 21, in overlaps_graph      [1, 1, tf.shape(boxes2)[0]]), [-1, 4])    File ""/Users/apple/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/ops/gen_array_ops.py"", line 11293, in tile      input, multiples, name=name, ctx=_ctx)    File ""/Users/apple/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/ops/gen_array_ops.py"", line 11340, in tile_eager_fallback      _attr_Tmultiples, (multiples,) = _execute.args_to_matching_eager([multiples], _ctx, _dtypes.int32)    File ""/Users/apple/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/eager/execute.py"", line 257, in args_to_matching_eager      t, dtype, preferred_dtype=default_dtype, ctx=ctx))    File ""/Users/apple/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/ops.py"", line 1296, in internal_convert_to_tensor      ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)    File ""/Users/apple/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/ops/array_ops.py"", line 1278, in _autopacking_conversion_function      return _autopacking_helper(v, dtype, name or ""packed"")    File ""/Users/apple/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/ops/array_ops.py"", line 1214, in _autopacking_helper      return gen_array_ops.pack(elems_as_tensors, name=scope)    File ""/Users/apple/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/ops/gen_array_ops.py"", line 6304, in pack      ""Pack"", values=values, axis=axis, name=name)    File ""/Users/apple/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 793, in _apply_op_helper      op_def=op_def)    File ""/Users/apple/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/func_graph.py"", line 548, in create_op      compute_device)    File ""/Users/apple/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/ops.py"", line 3429, in _create_op_internal      op_def=op_def)    File ""/Users/apple/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/ops.py"", line 1773, in __init__      control_input_ops)    File ""/Users/apple/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/framework/ops.py"", line 1613, in _create_c_op      raise ValueError(str(e))  ValueError: Duplicate node name in graph: 'mask_rcnn/proposal_targets/packed'  `  I am a newer in using tensorflow, my tensorflow version is 2.0.0, why this situation?"
"I have been using the Mask_RCNN algorithm. It works fine.  I have followed the process of training and validation followed by testing. But now, I would like to change the Backbone, ResNet101 to a new one. Is that feasible? What are the other changes that i would have to bring in the code, and the parameters. Kindly help."
I used custom dataset and was able to detect objects. But how can I know the pixel height and weight of each bounding boxes? Is there a command to it especially using multi-class detection? Thanks!
"I have TensorFlow 1.13 and Keras 2.2.5 installed in a conda environment:       The GPU on this EC2 instance is not found and so the code ends up running on CPU:     The same thing happens outside of the context of this Mask R-CNN code so perhaps it has something to do with the overall Python/TF setup?     Apparently, there is an issue with my custom dataset code because for two separate datasets I get the following kind of error from Keras:     The code I am using for my dataset and training script is  .    Can anyone advise as to 1) whether or not I'm using the best versions of TensorFlow (GPU) and Keras for this, and 2) what may be causing the issue of the incompatible shape shown above?    Thanks in advance for any insight or suggestions."
"The normal image size is 512 to 3600. But **satellite image**  is usually  with shape above **10000*10000**. The  general method spliting it into smaller block  is face with the problem how to merge all results  to  the raw images. Because **the edge  region** of image always get bad result,  some researchers crop the large scale image into **small block with overlapping**. So how to solve the overlapping region's **infer result,  nms seems not suitable**? Or other methods provided are also appreciated"
"I don't know the answer. Help me!    After successful training, the results of the tensorboard are shown in the figure below. But when I run detect(), there is no output at all.     My data is medical data for (2017 x 2048) 1 channel. And I have to detect many boundaries like line and segmentation the boundaries there. But nothing is caught in r   !   !   !   !   !       "
"Hi,  The '' inspect_balloon_model '' code does not work on the unknown, non annotated images! So, how can I validate the trained network on a new image?  "
"Need hlep the project I'm working on, I need masks and bounding boxes for my objects. Is it possible to simplify the architecture by removing the part of the classification?    Thank you!"
"I have two images img1 and img2 (np arrays)    If i use model.detect([img1,img2]) , I get the predictions from the model.    If I combine img1 and img2 as follows:  img_list = []  img_list.append(img1)  img_list.append(img2)    and then model.detect(img_list) does not return any prediction. What could be the problem. Am I using append wrong?"
"Hi,   I have a data set containing .mat files for the ground truth of 200 images, can they be considered labels and can i use them to train my model along with coco dataset?"
"I am trying to train balloon.py file on balloon dataset. Both the train and test datasets contain via_region_data.json file. However the error that I am getting is ""via_region_data.json"" file not available.      !python balloon.py train --dataset=Mask_RCNN-master/data/balloon --weights=coco         I tried every possibility by changing the path but it is showing same error as ""via_region_data.json"" file not found.  "
"Thank you very much for well-documented and awesome implementation on MaskRCNN.  Sorry to open such type of issue. I am tryng from several days to modify the output of the ProposalLayer. As per documentation, _""Proposals in normalized coordinates [batch, rois, (y1, x1, y2, x2)]""_  I want to add/append/concatenate more values with the same format to the **batch**, **rois**, and **(y1, x1, y2, x2**).  Is there any solution please?   @moorage @waleedka @haeric @rymalia @PavlosMelissinos   I know this is not an issue, I have no alternative way to solve this problem without this place.  "
"Hi guys right i have project in brain tumor classification, but i'm dealing with this error for about 3 weeks, this is the error message    > ERROR:root:Error processing image {'id': 'tumor-44.png', 'source': 'Meningioma', 'path': 'D:\\Belajar Machine Learning\\Coba Mask Tumor\\dataset\\images\\tumor-44.png'}  Traceback (most recent call last):    File ""D:\Belajar Machine Learning\Coba Mask Tumor\mrcnn\model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""D:\Belajar Machine Learning\Coba Mask Tumor\mrcnn\model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""tumor.py"", line 228, in load_mask      mask = np.zeros([info[""height""], info[""width""], len(info[""polygons""])],  KeyError: 'height'    i'm so confuse with that error message, some people say because something wrong with the json file when i'm mask the image because not using polygon masking, even though i already use polygon for masking.    this is the load_mask code       I've succeeded the training but i achieve that with round masking, but i want it with polygon masking tho.  thank you so much, hopefully your answer can solve my problem"
"Hi,everyone:       In recently, i want to inference by maskrcnn model in the envirment of C++. I  need the problem that using the opencv\sources\samples\dnn\tf_text_graph_mask_rcnn.py to convert .pb to .pbtxt, but i don't known how to set the config file according to Mask_RCNN\mrcnn\config.py.       Change the mask_rcnn_inception_v2_coco_2018_01_28\pipeline.config gain the problem:  Number of classes: 8  Scales:            [0.25, 0.5, 1.0, 2.0]  Aspect ratios:     [0.5, 1.0, 2.0]  Width stride:      16.000000  Height stride:     16.000000  Features stride:   16.000000  Traceback (most recent call last):    File "".\tf_text_graph_mask_rcnn.py"", line 86, in        assert(graph_def.node[0].op == 'Placeholder')  IndexError: list index out of range  Anyone can give me some advice? Thank you very much!!!     "
"I've noticed that for some classifications mask rcnn segments an entire group of objects. For example in this  , all the plants are segmented as one plant group - individual plants are grouped. This also occurs in this   - all the people segmentations are individual classifications, however, for the oranges, it is a group classification. Is there a way to distinguish group classifications from individual classifications?  "
"in 'model.py' file, 'mrcnn_mask_loss_graph' function, i got pred_masks & target_masks as dtype=boolean.  y_true & y_pred are also dtype=boolean. Then, how can we get the value of K.binary_crossentropy(target=y_true, output=y_pred) which should be dtype=float?"
"hi, i want to know the shapes and values of y_pred and y_true in mrcnn_mask_loss_graph  while training  how can i get those things with my image?  i am newbie on this, so it will be very helpful if you use plain words, thank you.    in addition, i want to show out y_pred while training,   so i can get more suitable boundaries of instances by fine-tuning y_pred"
"Hi everyone:       In recently, I want convert the .h5 to .pb, but the result is model.save_weights() only save the parameters. So i changed as follow:                             keras.callbacks.ModelCheckpoint(self.checkpoint_path,                                                                           verbose=0, save_weights_only=False),          but  there are the error after one epoch, what can i do? Thank you very much!!!"
When I apply following data augmentation technique     > iaa.Sequential([iaa.Fliplr(0.5)])    will the augmentation be applied to each training set and both non augmented and augmented images are used as training sample or just the augmented image will be used.
"Thanks for wonderful repository. I inspect model using your provided notebook, such as inspect_model.ipynb.  Now I would like to visualize output from different layers during training time?  Is there any suggestion please"
I want to see the output of the ProposalLayer and DetectionTargetLayer at each epoch during training.  How can I do it?
"Hi, in case that I added regularizer to layer inside the model, should I also add regularization loss to  `model.compile` at   ?"
"Hello,   I try to use Mask_RCNN with tf 2.0 and i have this error.  Someone can help me ?    `lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1636: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.  Instructions for updating:  If using Keras pass *_constraint arguments to layers.  Traceback (most recent call last):    File ""train.py"", line 315, in        model_dir=args.logs)    File ""/home/hulkbulk/Documents/mrcnn/Mask_RCNN/mrcnn/model.py"", line 1869, in __init__      self.keras_model = self.build(mode=mode, config=config)    File ""/home/hulkbulk/Documents/mrcnn/Mask_RCNN/mrcnn/model.py"", line 1967, in build      anchors = KL.Lambda(lambda x: tf.Variable(anchors) , name=""anchors"")(input_image)    File ""/home/hulkbulk/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 785, in __call__      outputs = call_fn(cast_inputs, *args, **kwargs)    File ""/home/hulkbulk/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/core.py"", line 846, in call      self._check_variables(created_variables, tape.watched_variables())    File ""/home/hulkbulk/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/core.py"", line 872, in _check_variables      raise ValueError(error_str)  ValueError:   The following Variables were created within a Lambda layer (anchors)  but are not tracked by said layer:       The layer cannot safely ensure proper Variable reuse across multiple  calls, and consquently this behavior is disallowed for safety. Lambda  layers are not well suited to stateful computation; instead, writing a  subclassed Layer is the recommend way to define layers with  Variables.`"
When running an example from    I get the following error:-     
"@dkurt @moorage @waleedka   I am trying to convert MaskRCNN model into tensorflow model (.pb and .pbtxt) for using in opencvDNN, like this:  `net = cv2.dnn.readNetFromTensorflow()`  I converted the MaskRCNN model into .pb and .pbtxt. but it fails to load with opencvDNN.    I think we need to open this issue, so I opened it. Is there any suggestion or idea please share here.     Thanks"
"I want to do data augmentation on each training sample such that different functions like flip, rotate, noise are performed randomly on the data samples. Is there any way to achieve that? and will that help? or I should implement each augmentation technique on ever data sample? My dataset consists of just 1000 images."
"Hi, I have annotations in the COCO-JSON format generated by the tool VIA. I am not clear on how to use the annotated data to train a model using faster RCNN.    My data is in this format:     Along with this format of data, I have JSON format as well. This annotation data format is:     It would be great if someone could assist me in understanding on how I can use either of the formats to build a DataLoader.    Thanks"
"Do there any method to plot model accuracy, model loss graph and also how to create confusion matrix for mask rcnn?"
None
"i have add mutliple gpu inference support in the prediction code. And i don't see any improvement (4GPU inference time = 1 gpu inference). here is the code please correct me if i'm doing anything wrong. GPUs are barely used max it goes it 7% and gpu Memory is loaded with images.    `for i in tqdm(range(int(frame_count/int(inference_config.GPU_COUNT * inference_config.IMAGES_PER_GPU)))):            images= []      bgr_images= []      for j in range(int(inference_config.GPU_COUNT * inference_config.IMAGES_PER_GPU)):          # Read the current frame          ret, frame = input_video.read()          if not ret:              break            current_frame += 1            # Change color representation from BGR to RGB before running model.detect()          detect_frame = frame[:, :, ::-1]          bgr_images.append(frame)          images.append(detect_frame)        # print('images size : ' + str(len(images)))      # Run inference on the color-adjusted frame      start_time = time.time()      results = model.detect(images, verbose=0)      print(""--- %s seconds ---"" % (time.time() - start_time))        for l, frame in enumerate(bgr_images):          r = results[l]          n_instances = r['rois'].shape[0]            # Make sure we have enough colors          if len(colors) < n_instances:              # not enough colors, generate more              more_colors = random_colors(n_instances - len(colors),False)                # Change color representation from RGB to BGR before displaying instances              more_colors = [(color[2], color[1], color[0]) for color in more_colors]              colors += more_colors            # Display instances on the original frame          display_frame = display_instances(frame, r['rois'], r['masks'], r['class_ids'],                                      dataset_train.class_names, r['scores'], colors[0:n_instances])            # Make sure we got displayed instances          if display_frame is not None:              frame = display_frame            # Create the output_video if it doesn't yet exist          if output_video is None:              if vid_size is None:                  vid_size = frame.shape[1], frame.shape[0]              output_video = cv2.VideoWriter(str(vid_name), fourcc, float(fps), vid_size, True)            # Resize frame if necessary          if vid_size[0] != frame.shape[1] and vid_size[1] != frame.shape[0]:              frame = cv2.resize(frame, vid_size)            # Write the frame to the output_video          output_video.write(frame)    input_video.release()  output_video.release()`  "
"If I just don't fill the segmentation arrays of the object of class X that I don't need masks for - would it be enough to detect all other classes objects with a mask and a bounding box, and class X objects with just a bounding box? - thanks! "
"How to solve the problem as i has followed the changes in Tensorflow 2.0 compatibility #1896 but still come out these errors  ---------------------------------------------------------------------------  OperatorNotAllowedInGraphError            Traceback (most recent call last)    in          7 warnings.filterwarnings(""ignore"")        8 model.train(dataset_train,dataset_val, learning_rate=config.LEARNING_RATE,epochs=5,  ----> 9             layers='heads')    /kaggle/working/maskrcnn/Mask_RCNN-master/mrcnn/model.py in train(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)     2355         log(""Checkpoint Path: {}"".format(self.checkpoint_path))     2356         self.set_trainable(layers)  -> 2357         self.compile(learning_rate, self.config.LEARNING_MOMENTUM)     2358      2359         # Work-around for Windows: Keras fails on Windows when using    /kaggle/working/maskrcnn/Mask_RCNN-master/mrcnn/model.py in compile(self, learning_rate, momentum)     2168         for name in loss_names:     2169             layer = self.keras_model.get_layer(name)  -> 2170             if layer.output in self.keras_model.losses:     2171                 continue     2172             loss = (    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py in __bool__(self)      763       `TypeError`.      764     """"""  --> 765     self._disallow_bool_casting()      766       767   def __nonzero__(self):    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py in _disallow_bool_casting(self)      532     else:      533       # Default: V1-style Graph execution.  --> 534       self._disallow_in_graph_mode(""using a `tf.Tensor` as a Python `bool`"")      535       536   def _disallow_iteration(self):    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py in _disallow_in_graph_mode(self, task)      521     raise errors.OperatorNotAllowedInGraphError(      522         ""{} is not allowed in Graph execution. Use Eager execution or decorate""  --> 523         "" this function with @tf.function."".format(task))      524       525   def _disallow_bool_casting(self):    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function."
"Do anyone also faced this problem? how can i solve it?  ---------------------------------------------------------------------------  OperatorNotAllowedInGraphError            Traceback (most recent call last)    in          7 warnings.filterwarnings(""ignore"")        8 model.train(dataset_train,dataset_val, learning_rate=config.LEARNING_RATE,epochs=5,  ----> 9             layers='heads')    /kaggle/working/maskrcnn/Mask_RCNN-master/mrcnn/model.py in train(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)     2355         log(""Checkpoint Path: {}"".format(self.checkpoint_path))     2356         self.set_trainable(layers)  -> 2357         self.compile(learning_rate, self.config.LEARNING_MOMENTUM)     2358      2359         # Work-around for Windows: Keras fails on Windows when using    /kaggle/working/maskrcnn/Mask_RCNN-master/mrcnn/model.py in compile(self, learning_rate, momentum)     2168         for name in loss_names:     2169             layer = self.keras_model.get_layer(name)  -> 2170             if layer.output in self.keras_model.losses:     2171                 continue     2172             loss = (    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py in __bool__(self)      763       `TypeError`.      764     """"""  --> 765     self._disallow_bool_casting()      766       767   def __nonzero__(self):    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py in _disallow_bool_casting(self)      532     else:      533       # Default: V1-style Graph execution.  --> 534       self._disallow_in_graph_mode(""using a `tf.Tensor` as a Python `bool`"")      535       536   def _disallow_iteration(self):    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py in _disallow_in_graph_mode(self, task)      521     raise errors.OperatorNotAllowedInGraphError(      522         ""{} is not allowed in Graph execution. Use Eager execution or decorate""  --> 523         "" this function with @tf.function."".format(task))      524       525   def _disallow_bool_casting(self):    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function."
"I'm running this snippet:         And every time it runs it prints different results, even thought it's the same model.  If I remove the first two lines from the loop (running only the last three) then it shows the same results.    Is this expected behavior? If yes, why?"
"Hello @waleedka,  I am trying to use this code to build an object detection model with gray level satellite images.  I am training on a data set(~9000) of 512 gray level satellite images.  The global validation loss is hight >2 however the mask, Bbox and class loss are not that hight?  I'm suffering from overfitting, what can I do to make it better?    **Configuration:**  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 8  IMAGE_MAX_DIM                  512  IMAGE_META_SIZE                14  IMAGE_MIN_DIM                  512  IMAGE_MIN_SCALE                0  IMAGE_PADDING                  True  IMAGE_RESIZE_MODE              none  IMAGE_SHAPE                       !   !   !     Can you please help me fix this? Thanks in advance."
"Hi sir or madam,  could sir/madam update the code in line 553, **tf.random_shuffle** to **tf.compat.v1.random_shuffle** as the latest version of tensorflow has been update the code  tq"
"Hello!    I would like to know if it is possible to train the network on a dataset that is annotated only with bounding boxes. I want the network to be able to predict only bounding boxes. Would you please help me understand if that is possible and what would be the way to do so?    Thank you for your time,"
"tried to train on my own classes, following balloon tutorial but I still get balloon label on my object , any suggestion to get it right "
"Hello!    I was trying to reproduce the balloon training exercise:    python balloon.py train --dataset=./datasets/balloon --weights=coco    and got the following run-time error:    ---------------------------------------------------------------------  Training network heads  Starting at epoch 0. LR=0.001    Selecting layers to train  In model:  rpn_model  mrcnn_bbox_fc          (TimeDistributed)    WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... An op outside of the function building code is being passed  a ""Graph"" tensor. It is possible to have Graph tensors  leak out of the function building context by including a  tf.init_scope in your function building code.  For example, the following function will fail:    @tf.function    def has_init_scope():      my_constant = tf.constant(1.)      with tf.init_scope():        added = my_constant * 2  The graph tensor has name: anchors/Variable:0    Epoch 1/30  Traceback (most recent call last):    File ""balloon.py"", line 383, in        train(model)    File ""balloon.py"", line 199, in train      layers='mrcnn_bbox_fc')    tensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable anchors/Variable from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/anchors/Variable/N10tensorflow3VarE does not exist.    [[{{node ROI/ReadVariableOp}}]]    ----------------------------------------------------------------------------    I used:   TensorFlow 2.0.0, Python 3.6.1 on Mac OS X Mojave.    Thanks in advance ,   and best regards,  Boris    "
"I used,  Mask_RCNN/samples/demo.ipynb    # Load a random image from the images folder  #file_names = next(os.walk(IMAGE_DIR))   "
"# Run object detection  results = model.detect([image], verbose=1)  # Display results  ax = get_ax(1)  r = results[0]  visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'],                               dataset.class_names, r['scores'], ax=ax,                              title=""Predictions"")  log(""gt_class_id"", gt_class_id)  log(""gt_bbox"", gt_bbox)  log(""gt_mask"", gt_mask)  How to extract the x,y coordinates of the predicted polygons?"
"Hello, I have two questions               1. In mrcnn / model, I add the OS. ï¹ exit() program after the log (""anchors"") to solve the problem             2. Predict after training the model, the predicted results are inconsistent with the labels, and solve"
"I'm doing a predictions for cotton plant field. I've three classes, (BG, Cotton, Weed). I don't want my model to detect or label anything else so my `dataset` consist of **100** Cotton field images. Which has both cotton plant and weed plant in it, then I annotated them with `VIA` tool. So I modified almost all code files including `coco.py` `model.py` `config.py` I think I also modified `utils.py`, and trained model. After so many bugs, and three weeks of debugging my model started training on google colab.  !     Above Image is copied from `Inspect_plant.ipynb` which is actually modified form of `inspect_ballon_data.ipynb`. But After training, When I started detection, I didn't got expected results. Below Images has splashed from my plant model.  !   !   !     It has gray-scaled whole image and returned original color pixel for detected classes, It  also has significant accuracy issue, It is splashing `cotton` accurately but neglecting `weed` most of times. My major concern is `weed`, I need to get all `weed plant` coordinates from images, because I'll then drive **robotic arm** to pluck weeds from field.    **What I want**  Now I want it to return results as it did in above `inspect_plant.ipynb` image, that I've attached  above. Moreover, After detection I want it to mask my cotton plant and weed plant with colors I've specified e.g. `Yellow` for `Cotton` and `orange` for `weed`    _Originally posted by @Master-HM in  "
"Hey,    so i started implementing Mask RCNN for my own Dataset. I followed the balloon splash tutorial and right now im at the point where i implemented the functions to load my Data and show Masks/Bounding boxes etc. (everything of the first jupyter notebook is working)  I got a Dataset of around 500 pictures with annotations and want to train on the pretrained coco net, but i am not quite sure how to do it.   My GPU is: NVIDIA GTX 1060 with 3GB RAM  The tutorial says, that if you dont have a GPU with 12GB RAM it is not really possible to train it, so he(in the tutorial) used amazons P2 instances.    I wanted to do that too, but searching on the website i found, that this only works for Linux systems?  I dont really know how to train my net for my data, so it would be really nice if someone of you could help me out...    Best regards  WhyWouldYouTilt"
Python 3.6  TensorFlow: 2.1.0rc0  Keras: 2.2.4-tf    After start training:     
"Traceback (most recent call last):    File ""/home/wangkai/miniconda3/envs/MASK/lib/python3.6/site-packages/theano/tensor/type.py"", line 269, in dtype_specs      }[self.dtype]  KeyError: "" ""    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""samples/bubble.py"", line 263, in        train_model()    File ""samples/bubble.py"", line 191, in train_model      model = modellib.MaskRCNN(mode=""training"", config=config, model_dir=MODEL_DIR)    File ""/home/wangkai/Mask_RCNN/mrcnn/model.py"", line 1838, in __init__      self.keras_model = self.build(mode=mode, config=config)    File ""/home/wangkai/Mask_RCNN/mrcnn/model.py"", line 1863, in build      shape=[None, 1], name=""input_rpn_match"", dtype=tf.int32)    File ""/home/wangkai/miniconda3/envs/MASK/lib/python3.6/site-packages/keras/engine/input_layer.py"", line 178, in Input      input_tensor=tensor)    File ""/home/wangkai/miniconda3/envs/MASK/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""/home/wangkai/miniconda3/envs/MASK/lib/python3.6/site-packages/keras/engine/input_layer.py"", line 87, in __init__      name=self.name)    File ""/home/wangkai/miniconda3/envs/MASK/lib/python3.6/site-packages/keras/backend/theano_backend.py"", line 254, in placeholder      x = T.TensorType(dtype, broadcast)(name)    File ""/home/wangkai/miniconda3/envs/MASK/lib/python3.6/site-packages/theano/tensor/type.py"", line 51, in __init__      self.dtype_specs()  # error checking is done there    File ""/home/wangkai/miniconda3/envs/MASK/lib/python3.6/site-packages/theano/tensor/type.py"", line 272, in dtype_specs      % (self.__class__.__name__, self.dtype))  "
"I have following code in my app.py         This code works great on my local machine with GTX 1060 6GB, but dont work on server with following error:           Running model.detect just in any python script (without flask) working great. What should i do to run model detect in flask environment?"
"Hi all, thanks for this great research and publishing it here! And thanks to many others for their great questions and answers. â¤    I'm working on a model that detects metal bars in bundles of bars. I'm using the Mask R-CNN implementation that comes with Detectron2 (the parameter there is called DETECTIONS_PER_IMAGE I think) and face the issue that the number of bars detected peaks at a certain threshold.   First the detections stopped at 100.  Then I found the above mentioned parameter and adjusted it to 1000. Now the number of detections peaks at about 469. I've adjusted the SCORE_THRESH_TEST to 0.5 but the maximum number seems to be fixed. Because the objects to detect can be pretty small I've also adjusted the ANCHOR_GENERATOR sizes to include  "
I want to optimized the trained model.   How can I know which nodes can be removed and which nodes need to keep during inference?     any suggestion please...  
"When visualized in the inference mode, no anchors remain after NMS in the proposal layer. Could someone help?"
  System information  OS Platform: Microsoft windows 10 x64  TensorFlow version: 1.14  CUDA/cuDNN version: 10.0/7.6  GPU model and memory: Leadtek RTX 2080 Ti Classic 11GB    The training just gets stuck. I have tried different versions of tensorflow but it doesn't work.  Does anybody have the same problem? Any suggestions?    ! 
"The TimeDistributed layers appear in the function  **fpn_classifier_graph**    `print(x.shape)`    `x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (pool_size, pool_size), padding=""valid""),                             name=""mrcnn_class_conv1"")(x)`    `print(x.shape)`      the first x.shape is (8,None,7,7,256)    but the second is (None,32,1,1,1024),    why become 32 ?    And in my code ,the second x.shape will become (None,None,1,1,1024)    Would you please help me to answer this question?"
"@waleedka In the rpn_class_loss_graph() function the author writes:        # Get anchor classes. Convert the -1/+1 match to 0/1 values.      anchor_class = K.cast(K.equal(rpn_match, 1), tf.int32)      # Positive and Negative anchors contribute to the loss,      # but neutral anchors (match value = 0) don't.      indices = tf.where(K.not_equal(rpn_match, 0))      # Pick rows that contribute to the loss and filter out the rest.      rpn_class_logits = tf.gather_nd(rpn_class_logits, indices)      anchor_class = tf.gather_nd(anchor_class, indices)    However,  I don't see any code corresponding to `Convert the -1/+1 match to 0/1 values.` So the loss is computed on `rpn_class_logits` between 0 and 1 and `rpn_match` from {1, -1}.  Do I miss something? Please help me."
"Hi,    I am using Mask RCNN to train data on traffic lights.  When i tried to train the model. It is showing     ""Epoch 1/30 /usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:709: UserWarning: An input could not be retrieved. It could be because a worker has died.We do not have any information on the lost sample. UserWarning)""    I am not sure on which input it is waiting for.  I have modified the dataset load functionalities as below to accomodate on my data.    `lass TrafficLightDataset(utils.Dataset):        def load_trafficlight(self, dataset_dir, subset):          """"""Load a subset of the Balloon dataset.          dataset_dir: Root directory of the dataset.          subset: Subset to load: train or val          """"""          # Add classes. We have only one class to add.          self.add_class(""trafficlight"", 1, ""trafficlight"")            # Train or validation dataset?          assert subset in [""train"", ""val""]          dataset_dir = os.path.join(dataset_dir, subset)            # Load annotations          # VGG Image Annotator (up to version 1.6) saves each image in the form:          # { 'filename': '28503151_5b5b7ec140_b.jpg',          #   'regions': {          #       '0': {          #           'region_attributes': {},          #           'shape_attributes': {          #               'all_points_x': [...],          #               'all_points_y': [...],          #               'name': 'polygon'}},          #       ... more regions ...          #   },          #   'size': 100202          # }          # We mostly care about the x and y coordinates of each region          # Note: In VIA 2.0, regions was changed from a dict to a list.          annotations = json.load(open(os.path.join(dataset_dir, ""annotate_region.json"")))                    # Add images          for a in annotations:            rect = a[""boxes""]            image_id = a[""path""].split('/')[4]            path = ""data/""+ image_id            image_path = os.path.join(dataset_dir, path)            image = skimage.io.imread(image_path)            height, width = image.shape[:2]            self.add_image(                  ""trafficlight"",                  image_id=image_id,  # use file name as a unique image id                  path=image_path,                  width=width, height=height,                  rect=rect)        def load_mask(self, image_id):          """"""Generate instance masks for an image.         Returns:          masks: A bool array of shape [height, width, instance count] with              one mask per instance.          class_ids: a 1D array of class IDs of the instance masks.          """"""          # If not a balloon dataset image, delegate to parent class.          image_info = self.image_info[image_id]          if image_info[""source""] != ""trafficlight"":              return super(self.__class__, self).load_mask(image_id)            # Convert polygons to a bitmap mask of shape          # [height, width, instance_count]          info = self.image_info[image_id]          mask = np.zeros([info[""height""], info[""width""], len(info[""rect""])],                          dtype=np.uint8)          for i, p in enumerate(info[""rect""]):              # Get indexes of pixels inside the polygon and set them to 1              start = (p[""x_min""], p[""y_min""])              end = (p[""x_max""], p[""y_max""])              rr, cc = skimage.draw.polygon(start, end)              mask[rr, cc, i] = 1            # Return mask, and array of class IDs of each instance. Since we have          # one class ID only, we return an array of 1s          return mask.astype(np.bool), np.ones([mask.shape[-1]], dtype=np.int32)        def image_reference(self, image_id):          """"""Return the path of the image.""""""          info = self.image_info[image_id]          if info[""source""] == ""trafficlight"":              return info[""path""]          else:              super(self.__class__, self).image_reference(image_id)  `  If someone has faced this issue. can you please help out.  "
"Hey @waleedka,    I am trying to use this code to build an object detection model. My data format is in XML and I am parsing through it to get the required details. I am facing issue in understanding how to load the bounding box parameter for object detection. Since it is a rectangle box, I have two parameters with me which are x_min, y_min, x_max and y_max.     My XML file is in this format:         My code for loading the dataset is this:          Can you please help me fix this? Thanks in advance."
"I am trainiing use my own dataset,but when train start ,it tips like below:    Epoch 1/10  UserWarning: An input could not be retrieved. It could be because a worker has died.We do not have any information on the lost sample.    UserWarning)    I train use balloon is correct. anybody can help?"
"Hi guys,    I would like know how to calculate confusion matrix on prediction mask rcnn?    Someone can help me?"
"Thank you so much for the project repository, here how can i test the model with new dataset and is there any code for testing the model , please if it there please provide here  "
I want to do data augmentation and also use the original image as well for training
!   
"Hi All,     I am trying to train Mask_RCNN to detect traffic light.  I am using Bosch dataset. The data is not annotated using VIA tool.  Please find the sample JSON format of the annotated images,    {      ""path"": ""./rgb/additional/2015-10-05-10-52-01_bag/24594.png"",       ""boxes"": []    },     {      ""path"": ""./rgb/additional/2015-10-05-10-52-01_bag/24664.png"",       ""boxes"": []    },     {      ""path"": ""./rgb/additional/2015-10-05-10-52-01_bag/24734.png"",       ""boxes"": []    },     {      ""path"": ""./rgb/additional/2015-10-05-10-55-33_bag/56988.png"",       ""boxes"": [        {          ""y_min"": 278.7432988131,           ""x_max"": 527.1012443072,           ""occluded"": false,           ""x_min"": 518.7379907959,           ""y_max"": 296.3838188664,           ""label"": ""Yellow""        }      ]    }  Can you please help out in how to accomodate this dataset to load_mask. "
"I am training with my own dataset of multiple classes annotated on VGG editor and encountered with the below error. Please help me if anyone know.    Epoch 1/30  ERROR:root:Error processing image {'id': 'IMG_20191112_104147.jpg', 'source': 'traffic', 'path': '../../datasets/traffic/val/IMG_20191112_104147.jpg', 'width': 4000, 'height': 3000, 'polygons': [{'name': 'polygon', 'all_points_x': [1024, 1019, 1714, 1689, 1575, 1472, 1189, 1081, 911, 1024], 'all_points_y': [2636, 2646, 2456, 2090, 2008, 1822, 1956, 2106, 2564, 2636]}, {'name': 'polygon', 'all_points_x': [2734, 3017, 3161, 3135, 3027, 2785, 2692, 2764, 2734], 'all_points_y': [2095, 2121, 2075, 1915, 1833, 1838, 1987, 2100, 2095]}, {'name': 'polygon', 'all_points_x': [2270, 2394, 2394, 2270, 2250, 2270], 'all_points_y': [1997, 1997, 1874, 1869, 1956, 1997]}], 'class_ids': [4, 3, 4]}  Traceback (most recent call last):    File ""/content/drive/My Drive/Colab Notebooks/mj_pro_mid/mrcnn/model.py"", line 1705, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/content/drive/My Drive/Colab Notebooks/mj_pro_mid/mrcnn/model.py"", line 1261, in load_image_gt      class_ids = class_ids[_idx]  TypeError: only integer scalar arrays can be converted to a scalar index  ERROR:root:Error processing image {'id': 'IMG_20191112_104159.jpg', 'source': 'traffic', 'path': '../../datasets/traffic/train/IMG_20191112_104159.jpg'"
None
"I run this model on win10 ,use gpu.but just trainning 1or2 steps then the gpu memory keep high and cpu usage is very high ,but gpu usage is 0%-2%. anybody can help  !   !     "
"Hello, I'm trying to train the model for 3 classes but I have been with problems in the process. I'm using the Balloon example with modifications according to the #372 . The problem I think are the annotations, I annotated in VIA 2.0.8 with polygons. The code that I used is:     `      ############################################################  #  Dataset  ############################################################    class MineriaDataset(utils.Dataset):        def load_mineria(self, dataset_dir, subset):          """"""Load a subset of the bottle dataset.          dataset_dir: Root directory of the dataset.          subset: Subset to load: train or val          """"""          # Add classes. We have only one class to add.          self.add_class(""mineria"", 1, ""rio"")          self.add_class(""mineria"", 2, ""piscina"")          self.add_class(""mineria"", 3, ""remocion"")            # Train or validation dataset?          assert subset in [""train"", ""val""]          dataset_dir = os.path.join(dataset_dir, subset)            # We mostly care about the x and y coordinates of each region          annotations1 = json.load(open(os.path.join(dataset_dir, ""train_val.json"")))          #print(annotations)          annotations = list(annotations1.values())  # don't need the dict keys                      # The VIA tool saves images in the JSON even if they don't have any          # annotations. Skip unannotated images.          annotations = [a for a in annotations if a['regions']]            # Add images          for a in annotations:              # print(a)              # Get the x, y coordinaets of points of the polygons that make up              # the outline of each object instance. There are stores in the              # shape_attributes (see json format above)              #polygons = [r['shape_attributes'] for r in a['regions'].values()]              polygons = [r['shape_attributes'] for r in a['regions']]              #print(polygons)              #objects = [s['region_attributes'] for s in a['regions'].values()]              objects = [s['region_attributes'] for s in a['regions']]              #print(objects)              # for n in multi_numbers:              #num_ids = [int(n['mineria']) for n in objects]                            num_ids = []              for n in objects:                  try:                      if 'rio' in n.keys():                          num_ids.append(1)                      elif 'piscina' in n.keys():                          num_ids.append(2)                      elif 'remocion' in n.keys():                          num_ids.append(3)                        except:                      pass                                  # load_mask() needs the image size to convert polygons to masks.              # Unfortunately, VIA doesn't include it in JSON, so we must read              # the image. This is only managable since the dataset is tiny.              image_path = os.path.join(dataset_dir, a['filename'])              image = skimage.io.imread(image_path)              height, width = image.shape[:2]                self.add_image(                  ""mineria"",  ## for a single class just add the name here                  image_id=a['filename'],  # use file name as a unique image id                  path=image_path,                  width=width, height=height,                  polygons=polygons,                  num_ids=num_ids)        def load_mask(self, image_id):          """"""Generate instance masks for an image.         Returns:          masks: A bool array of shape [height, width, instance count] with              one mask per instance.          class_ids: a 1D array of class IDs of the instance masks.          """"""          # If not a bottle dataset image, delegate to parent class.          info = self.image_info[image_id]          if info[""source""] != ""mineria"":              return super(self.__class__, self).load_mask(image_id)          num_ids = info['num_ids']            # Convert polygons to a bitmap mask of shape          # [height, width, instance_count]          info = self.image_info[image_id]          mask = np.zeros([info[""height""], info[""width""], len(info[""polygons""])],                          dtype=np.uint8)          for i, p in enumerate(info[""polygons""]):              # Get indexes of pixels inside the polygon and set them to 1              rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])              mask[rr, cc, i] = 1            # Return mask, and array of class IDs of each instance. Since we have          # one class ID only, we return an array of 1s          #return mask.astype(np.bool), np.ones([mask.shape[-1]], dtype=np.int32)          # Map class names to class IDs.          num_ids = np.array(num_ids, dtype=np.int32)          return mask, info['num_ids']        def image_reference(self, image_id):          """"""Return the path of the image.""""""          info = self.image_info[image_id]          if info[""source""] == ""mineria"":              return info[""path""]          else:              super(self.__class__, self).image_reference(image_id)      def train(model):      """"""Train the model.""""""      # Training dataset.      dataset_train = MineriaDataset()      dataset_train.load_mineria(args.dataset, ""train"")      dataset_train.prepare()        # Validation dataset      dataset_val = MineriaDataset()      dataset_val.load_mineria(args.dataset, ""val"")      dataset_val.prepare()        # *** This training schedule is an example. Update to your needs ***      # Since we're using a very small dataset, and starting from      # COCO trained weights, we don't need to train too long. Also,      # no need to train all layers, just the heads should do it.      print(""Training network heads"")      model.train(dataset_train, dataset_val,                  learning_rate=config.LEARNING_RATE,                  epochs=10,                  layers='heads')`    **After run, I received this error:**     `  Epoch 1/10  ERROR:root:Error processing image {'id': '983.jpg', 'source': 'mineria', 'path': '/content/gdrive/My Drive/FACSAT-1/Mask_RCNN/samples/mineria/dataset/val/983.jpg', 'width': 600, 'height': 600, 'polygons': [{'name': 'polyline', 'all_points_x': [364, 352, 349, 346, 345, 343, 343, 343, 344, 347, 349, 349, 348, 341, 334, 332, 333, 337, 344, 348, 350, 364, 365, 368, 369, 370, 370, 368, 367, 367, 367, 366, 366, 366, 373, 376, 371, 363, 358, 349, 345, 345, 346, 349, 351, 351, 349, 334, 329, 323, 315, 315, 316, 317, 320, 320, 324, 332, 332, 332, 326, 323, 320, 314, 311, 311, 315, 316, 318, 320, 320, 320, 321, 324, 326, 327, 327, 329, 329, 329, 331, 331, 321, 317, 301, 298, 290, 289, 286, 283, 281, 276, 275, 272, 270, 263, 261, 259, 257, 255, 251, 249, 248, 246, 244, 237, 236, 232, 229, 225, 222, 218, 216, 200, 199, 187, 186, 177, 174, 169, 161, 172, 174, 177, 183, 187, 189, 193, 195, 198, 202, 207, 223, 223, 224, 225, 226, 226, 228, 229, 238, 255, 259, 269, 274, 270, 276, 289, 291, 299, 314, 330, 331, 338, 338, 329, 320, 317, 315, 320, 331, 335, 336, 331, 327, 318, 320, 323, 329, 338, 339, 351, 355, 355, 351, 349, 349, 354, 360, 365, 377, 379, 379, 380, 376, 374, 373, 371, 373, 372, 364, 359, 349, 341, 337, 339, 345, 350, 352, 352, 348, 345, 348, 360, 367, 366, 365, 364], 'all_points_y': [162, 168, 169, 170, 173, 178, 180, 182, 184, 190, 193, 197, 200, 204, 210, 215, 216, 218, 218, 217, 217, 219, 219, 219, 220, 222, 223, 229, 234, 237, 239, 246, 247, 249, 263, 267, 273, 272, 272, 273, 283, 289, 291, 295, 297, 299, 305, 314, 318, 319, 322, 326, 332, 335, 335, 336, 339, 344, 346, 349, 355, 356, 356, 357, 358, 366, 371, 374, 376, 379, 380, 381, 382, 385, 386, 387, 388, 392, 393, 394, 394, 402, 404, 404, 410, 410, 412, 412, 414, 416, 417, 417, 417, 415, 410, 405, 403, 403, 400, 399, 395, 394, 390, 387, 386, 381, 381, 379, 378, 378, 385, 395, 397, 402, 404, 407, 411, 414, 415, 415, 417, 421, 418, 420, 420, 415, 414, 412, 411, 410, 410, 410, 399, 397, 396, 391, 387, 384, 384, 384, 388, 406, 412, 417, 419, 419, 420, 420, 417, 414, 411, 409, 409, 400, 395, 382, 371, 367, 366, 362, 358, 358, 344, 339, 336, 332, 327, 325, 322, 319, 318, 309, 301, 295, 289, 285, 279, 276, 275, 276, 278, 272, 267, 266, 264, 259, 257, 250, 224, 217, 216, 216, 214, 215, 215, 209, 207, 206, 200, 190, 185, 177, 173, 167, 164, 164, 163, 162]}, {'name': 'polyline', 'all_points_x': [346, 349, 353, 353, 356, 359, 362, 365, 367, 370, 371, 374, 377, 379, 381, 384, 387, 388, 390, 390, 390, 389, 388, 388, 387, 387, 387, 387, 388, 388, 390, 393, 395, 404, 405, 407, 409, 410, 411, 412, 414, 416, 420, 423, 423, 423, 426, 431, 429, 427, 431, 435, 435, 432, 430, 430, 432, 433, 433, 428, 426, 421, 419, 417, 413, 409, 404, 398, 395, 390, 393, 392, 391, 389, 387, 386, 383, 382, 378, 377, 375, 373, 373, 371, 369, 367, 361, 357, 356, 356, 355, 349, 346], 'all_points_y': [314, 314, 315, 319, 320, 325, 323, 322, 321, 320, 321, 323, 325, 326, 328, 332, 333, 334, 335, 337, 339, 340, 342, 342, 343, 343, 344, 345, 347, 348, 349, 351, 351, 350, 349, 349, 350, 350, 351, 353, 353, 354, 354, 356, 356, 357, 359, 361, 369, 372, 376, 377, 376, 374, 372, 369, 368, 367, 363, 358, 357, 353, 353, 351, 351, 349, 347, 348, 348, 347, 338, 336, 334, 332, 330, 328, 325, 324, 322, 322, 319, 317, 317, 317, 317, 318, 320, 318, 316, 316, 313, 311, 313]}, {'name': 'polyline', 'all_points_x': [373, 379, 379, 378, 377, 376, 376, 376, 376, 378, 379, 381, 382, 384, 385, 385, 385, 385, 384, 383, 382, 382, 384, 386, 388, 391, 392, 393, 392, 392, 393, 394, 396, 398, 398, 398, 398, 403, 408, 407, 405, 401, 398, 397, 395, 392, 389, 389, 389, 389, 385, 384, 381, 380, 381, 382, 382, 380, 379, 381, 382, 382, 378, 374, 374, 375, 376, 373, 372, 372], 'all_points_y': [217, 213, 210, 210, 209, 210, 209, 207, 206, 205, 205, 207, 207, 206, 204, 203, 201, 199, 198, 199, 199, 197, 197, 197, 197, 197, 196, 194, 194, 193, 193, 193, 193, 193, 192, 192, 187, 181, 178, 176, 176, 181, 188, 191, 191, 191, 192, 193, 194, 195, 195, 195, 195, 196, 197, 198, 198, 199, 199, 201, 202, 204, 203, 205, 208, 209, 211, 214, 216, 217]}, {'name': 'polyline', 'all_points_x': [144, 146, 147, 149, 149, 149, 149, 151, 153, 157, 158, 160, 161, 162, 163, 166, 167, 167, 168, 171, 171, 172, 172, 173, 175, 175, 177, 178, 179, 180, 181, 184, 185, 186, 187, 188, 190, 192, 192, 194, 194, 193, 192, 193, 192, 191, 191, 189, 183, 183, 183, 182, 182, 182, 182, 182, 179, 178, 178, 177, 176, 174, 173, 172, 169, 167, 165, 163, 162, 160, 159, 158, 156, 155, 153, 153, 152, 151, 150, 150, 150, 149, 146, 146, 145], 'all_points_y': [302, 302, 302, 302, 303, 304, 305, 306, 307, 308, 307, 306, 306, 305, 305, 305, 305, 305, 305, 307, 307, 307, 307, 308, 309, 310, 311, 312, 312, 311, 311, 309, 309, 308, 306, 305, 305, 304, 303, 301, 300, 299, 298, 298, 297, 296, 296, 296, 298, 299, 301, 303, 304, 305, 306, 306, 309, 309, 309, 308, 308, 307, 306, 305, 304, 304, 304, 303, 304, 304, 305, 305, 305, 305, 306, 305, 304, 303, 302, 302, 301, 301, 301, 301, 303]}, {'name': 'polyline', 'all_points_x': [374, 374, 375, 378, 378, 380, 382, 383, 385, 384, 386, 387, 389, 387, 386, 386, 385, 383, 383, 379, 378, 377, 376, 374, 373, 373, 372, 373, 373, 373, 373, 373, 373], 'all_points_y': [256, 256, 256, 256, 258, 258, 258, 259, 256, 256, 253, 253, 252, 250, 249, 248, 247, 246, 246, 245, 246, 247, 248, 247, 247, 249, 250, 251, 252, 254, 255, 255, 256]}], 'num_ids': []}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1265, in load_image_gt      class_ids = class_ids[_idx]  TypeError: only integer scalar arrays can be converted to a scalar index  ERROR:root:Error processing image {'id': '983.jpg', 'source': 'mineria', 'path': '/content/gdrive/My Drive/FACSAT-1/Mask_RCNN/samples/mineria/dataset/val/983.jpg', 'width': 600, 'height': 600, 'polygons': [{'name': 'polyline', 'all_points_x': [364, 352, 349, 346, 345, 343, 343, 343, 344, 347, 349, 349, 348, 341, 334, 332, 333, 337, 344, 348, 350, 364, 365, 368, 369, 370, 370, 368, 367, 367, 367, 366, 366, 366, 373, 376, 371, 363, 358, 349, 345, 345, 346, 349, 351, 351, 349, 334, 329, 323, 315, 315, 316, 317, 320, 320, 324, 332, 332, 332, 326, 323, 320, 314, 311, 311, 315, 316, 318, 320, 320, 320, 321, 324, 326, 327, 327, 329, 329, 329, 331, 331, 321, 317, 301, 298, 290, 289, 286, 283, 281, 276, 275, 272, 270, 263, 261, 259, 257, 255, 251, 249, 248, 246, 244, 237, 236, 232, 229, 225, 222, 218, 216, 200, 199, 187, 186, 177, 174, 169, 161, 172, 174, 177, 183, 187, 189, 193, 195, 198, 202, 207, 223, 223, 224, 225, 226, 226, 228, 229, 238, 255, 259, 269, 274, 270, 276, 289, 291, 299, 314, 330, 331, 338, 338, 329, 320, 317, 315, 320, 331, 335, 336, 331, 327, 318, 320, 323, 329, 338, 339, 351, 355, 355, 351, 349, 349, 354, 360, 365, 377, 379, 379, 380, 376, 374, 373, 371, 373, 372, 364, 359, 349, 341, 337, 339, 345, 350, 352, 352, 348, 345, 348, 360, 367, 366, 365, 364], 'all_points_y': [162, 168, 169, 170, 173, 178, 180, 182, 184, 190, 193, 197, 200, 204, 210, 215, 216, 218, 218, 217, 217, 219, 219, 219, 220, 222, 223, 229, 234, 237, 239, 246, 247, 249, 263, 267, 273, 272, 272, 273, 283, 289, 291, 295, 297, 299, 305, 314, 318, 319, 322, 326, 332, 335, 335, 336, 339, 344, 346, 349, 355, 356, 356, 357, 358, 366, 371, 374, 376, 379, 380, 381, 382, 385, 386, 387, 388, 392, 393, 394, 394, 402, 404, 404, 410, 410, 412, 412, 414, 416, 417, 417, 417, 415, 410, 405, 403, 403, 400, 399, 395, 394, 390, 387, 386, 381, 381, 379, 378, 378, 385, 395, 397, 402, 404, 407, 411, 414, 415, 415, 417, 421, 418, 420, 420, 415, 414, 412, 411, 410, 410, 410, 399, 397, 396, 391, 387, 384, 384, 384, 388, 406, 412, 417, 419, 419, 420, 420, 417, 414, 411, 409, 409, 400, 395, 382, 371, 367, 366, 362, 358, 358, 344, 339, 336, 332, 327, 325, 322, 319, 318, 309, 301, 295, 289, 285, 279, 276, 275, 276, 278, 272, 267, 266, 264, 259, 257, 250, 224, 217, 216, 216, 214, 215, 215, 209, 207, 206, 200, 190, 185, 177, 173, 167, 164, 164, 163, 162]}, {'name': 'polyline', 'all_points_x': [346, 349, 353, 353, 356, 359, 362, 365, 367, 370, 371, 374, 377, 379, 381, 384, 387, 388, 390, 390, 390, 389, 388, 388, 387, 387, 387, 387, 388, 388, 390, 393, 395, 404, 405, 407, 409, 410, 411, 412, 414, 416, 420, 423, 423, 423, 426, 431, 429, 427, 431, 435, 435, 432, 430, 430, 432, 433, 433, 428, 426, 421, 419, 417, 413, 409, 404, 398, 395, 390, 393, 392, 391, 389, 387, 386, 383, 382, 378, 377, 375, 373, 373, 371, 369, 367, 361, 357, 356, 356, 355, 349, 346], 'all_points_y': [314, 314, 315, 319, 320, 325, 323, 322, 321, 320, 321, 323, 325, 326, 328, 332, 333, 334, 335, 337, 339, 340, 342, 342, 343, 343, 344, 345, 347, 348, 349, 351, 351, 350, 349, 349, 350, 350, 351, 353, 353, 354, 354, 356, 356, 357, 359, 361, 369, 372, 376, 377, 376, 374, 372, 369, 368, 367, 363, 358, 357, 353, 353, 351, 351, 349, 347, 348, 348, 347, 338, 336, 334, 332, 330, 328, 325, 324, 322, 322, 319, 317, 317, 317, 317, 318, 320, 318, 316, 316, 313, 311, 313]}, {'name': 'polyline', 'all_points_x': [373, 379, 379, 378, 377, 376, 376, 376, 376, 378, 379, 381, 382, 384, 385, 385, 385, 385, 384, 383, 382, 382, 384, 386, 388, 391, 392, 393, 392, 392, 393, 394, 396, 398, 398, 398, 398, 403, 408, 407, 405, 401, 398, 397, 395, 392, 389, 389, 389, 389, 385, 384, 381, 380, 381, 382, 382, 380, 379, 381, 382, 382, 378, 374, 374, 375, 376, 373, 372, 372], 'all_points_y': [217, 213, 210, 210, 209, 210, 209, 207, 206, 205, 205, 207, 207, 206, 204, 203, 201, 199, 198, 199, 199, 197, 197, 197, 197, 197, 196, 194, 194, 193, 193, 193, 193, 193, 192, 192, 187, 181, 178, 176, 176, 181, 188, 191, 191, 191, 192, 193, 194, 195, 195, 195, 195, 196, 197, 198, 198, 199, 199, 201, 202, 204, 203, 205, 208, 209, 211, 214, 216, 217]}, {'name': 'polyline', 'all_points_x': [144, 146, 147, 149, 149, 149, 149, 151, 153, 157, 158, 160, 161, 162, 163, 166, 167, 167, 168, 171, 171, 172, 172, 173, 175, 175, 177, 178, 179, 180, 181, 184, 185, 186, 187, 188, 190, 192, 192, 194, 194, 193, 192, 193, 192, 191, 191, 189, 183, 183, 183, 182, 182, 182, 182, 182, 179, 178, 178, 177, 176, 174, 173, 172, 169, 167, 165, 163, 162, 160, 159, 158, 156, 155, 153, 153, 152, 151, 150, 150, 150, 149, 146, 146, 145], 'all_points_y': [302, 302, 302, 302, 303, 304, 305, 306, 307, 308, 307, 306, 306, 305, 305, 305, 305, 305, 305, 307, 307, 307, 307, 308, 309, 310, 311, 312, 312, 311, 311, 309, 309, 308, 306, 305, 305, 304, 303, 301, 300, 299, 298, 298, 297, 296, 296, 296, 298, 299, 301, 303, 304, 305, 306, 306, 309, 309, 309, 308, 308, 307, 306, 305, 304, 304, 304, 303, 304, 304, 305, 305, 305, 305, 306, 305, 304, 303, 302, 302, 301, 301, 301, 301, 303]}, {'name': 'polyline', 'all_points_x': [374, 374, 375, 378, 378, 380, 382, 383, 385, 384, 386, 387, 389, 387, 386, 386, 385, 383, 383, 379, 378, 377, 376, 374, 373, 373, 372, 373, 373, 373, 373, 373, 373], 'all_points_y': [256, 256, 256, 256, 258, 258, 258, 259, 256, 256, 253, 253, 252, 250, 249, 248, 247, 246, 246, 245, 246, 247, 248, 247, 247, 249, 250, 251, 252, 254, 255, 255, 256]}], 'num_ids': []}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1265, in load_image_gt      class_ids = class_ids[_idx]  TypeError: only integer scalar arrays can be converted to a scalar index  ERROR:root:Error processing image {'id': '984.jpg', 'source': 'mineria', 'path': '/content/gdrive/My Drive/FACSAT-1/Mask_RCNN/samples/mineria/dataset/val/984.jpg', 'width': 600, 'height': 600, 'polygons': [{'name': 'polyline', 'all_points_x': [158, 160, 162, 165, 175, 181, 185, 187, 189, 197, 199, 203, 202, 205, 209, 213, 219, 216, 212, 211, 209, 209, 208, 208, 203, 202, 200, 199, 197, 196, 196, 195, 195, 199, 207, 211, 215, 218, 219, 220, 221, 223, 223, 223, 223, 226, 227, 229, 232, 232, 233, 234, 234, 233, 231, 229, 224, 222, 222, 221, 221, 221, 221, 221, 221, 221, 220, 216, 214, 215, 212, 207, 204, 202, 194, 194, 193, 191, 191, 193, 194, 199, 200, 200, 202, 203, 205, 207, 212, 215, 215, 212, 210, 209, 204, 203, 201, 198, 195, 191, 187, 185, 185, 185, 184, 180, 171, 171, 169, 167, 163, 160, 158, 157, 156], 'all_points_y': [352, 352, 352, 354, 360, 359, 357, 352, 347, 335, 336, 335, 338, 338, 340, 340, 335, 333, 332, 330, 329, 328, 327, 326, 318, 316, 315, 315, 313, 311, 310, 307, 305, 304, 299, 297, 295, 291, 290, 290, 287, 284, 281, 278, 274, 267, 267, 266, 271, 270, 270, 269, 266, 264, 261, 261, 265, 268, 268, 270, 272, 274, 276, 278, 279, 281, 284, 290, 292, 293, 293, 296, 297, 299, 303, 304, 304, 307, 307, 310, 312, 317, 318, 319, 323, 324, 326, 329, 333, 334, 336, 337, 338, 337, 337, 333, 332, 331, 335, 339, 348, 351, 352, 353, 355, 357, 355, 355, 354, 353, 352, 350, 350, 350, 350]}, {'name': 'polyline', 'all_points_x': [244, 245, 246, 249, 252, 252, 257, 258, 259, 260, 260, 260, 261, 261, 261, 261, 263, 264, 265, 266, 266, 267, 268, 268, 268, 270, 270, 272, 272, 274, 275, 276, 278, 282, 283, 284, 284, 287, 288, 291, 292, 296, 299, 301, 301, 307, 308, 309, 311, 312, 312, 312, 312, 312, 312, 312, 311, 311, 310, 310, 310, 312, 313, 315, 321, 324, 327, 331, 337, 338, 339, 340, 340, 341, 342, 342, 342, 342, 342, 342, 342, 342, 342, 342, 342, 342, 343, 344, 344, 344, 345, 345, 346, 348, 349, 350, 353, 355, 355, 356, 357, 368, 371, 379, 383, 386, 389, 394, 396, 399, 402, 402, 402, 401, 399, 402, 404, 401, 401, 402, 402, 402, 403, 404, 408, 413, 419, 429, 432, 434, 454, 456, 456, 463, 462, 463, 463, 462, 458, 455, 453, 444, 440, 434, 429, 424, 419, 411, 407, 407, 408, 407, 407, 405, 404, 404, 403, 404, 405, 403, 395, 391, 382, 374, 369, 366, 362, 358, 354, 347, 343, 339, 338, 338, 338, 337, 337, 337, 334, 330, 321, 315, 306, 307, 308, 309, 302, 297, 288, 283, 279, 272, 266, 260, 259, 259, 259, 259, 259, 258, 256, 247, 244], 'all_points_y': [281, 284, 285, 285, 285, 285, 283, 282, 281, 280, 278, 278, 277, 276, 275, 274, 268, 265, 263, 262, 262, 262, 261, 261, 261, 261, 261, 261, 261, 262, 262, 262, 262, 264, 264, 264, 264, 265, 265, 266, 266, 269, 271, 271, 271, 271, 270, 270, 269, 268, 266, 265, 263, 261, 259, 257, 254, 253, 251, 249, 247, 244, 244, 243, 243, 245, 248, 250, 249, 248, 244, 242, 239, 238, 235, 233, 231, 230, 229, 226, 224, 224, 222, 221, 220, 218, 216, 215, 212, 210, 209, 207, 206, 204, 204, 204, 203, 204, 205, 206, 207, 217, 220, 224, 227, 228, 229, 231, 232, 233, 236, 238, 239, 242, 247, 261, 271, 282, 284, 285, 285, 289, 291, 293, 297, 296, 292, 292, 291, 292, 302, 303, 304, 304, 301, 301, 299, 298, 299, 298, 297, 293, 291, 289, 287, 287, 289, 294, 287, 275, 269, 263, 258, 254, 248, 245, 243, 238, 235, 233, 227, 226, 222, 218, 213, 210, 206, 204, 201, 202, 204, 217, 221, 225, 229, 233, 240, 243, 245, 244, 241, 235, 245, 249, 257, 264, 269, 266, 262, 261, 260, 258, 259, 263, 267, 268, 270, 272, 273, 277, 280, 282, 281]}, {'name': 'polyline', 'all_points_x': [277, 275, 275, 276, 276, 276, 273, 272, 268, 265, 264, 264, 264, 263, 263, 263, 263, 264, 265, 268, 271, 272, 272, 259, 258, 255, 253, 253, 256, 256, 260, 261, 264, 270, 270, 268, 265, 264, 262, 261, 261, 261, 263, 265, 270, 274, 275, 273, 273, 275, 275, 277, 277], 'all_points_y': [258, 256, 252, 247, 244, 241, 240, 239, 239, 238, 236, 230, 229, 225, 218, 214, 211, 209, 209, 208, 208, 207, 203, 188, 184, 180, 180, 180, 182, 185, 193, 194, 198, 205, 205, 206, 206, 206, 207, 211, 213, 231, 237, 240, 241, 243, 244, 249, 254, 258, 258, 259, 258]}, {'name': 'polyline', 'all_points_x': [137, 137, 140, 142, 144, 147, 149, 151, 152, 153, 154, 159, 159, 162, 163, 163, 163, 163, 164, 165, 165, 166, 166, 167, 167, 167, 169, 169, 170, 170, 171, 171, 172, 174, 173, 173, 175, 176, 175, 175, 174, 174, 175, 176, 178, 178, 178, 180, 182, 183, 184, 186, 184, 183, 184, 188, 191, 191, 192, 192, 191, 188, 187, 185, 184, 183, 182, 180, 179, 178, 178, 177, 176, 175, 173, 171, 170, 169, 169, 169, 167, 167, 166, 165, 165, 164, 163, 162, 162, 160, 159, 158, 156, 154, 153, 151, 148, 147, 146, 145, 144, 141, 139, 139, 138, 137, 136], 'all_points_y': [317, 314, 314, 314, 314, 312, 313, 313, 313, 311, 310, 313, 313, 314, 313, 313, 312, 312, 312, 312, 313, 314, 315, 316, 317, 317, 318, 318, 318, 318, 317, 315, 314, 315, 316, 316, 317, 317, 316, 314, 313, 312, 311, 312, 313, 314, 315, 316, 315, 314, 313, 312, 310, 309, 308, 309, 310, 310, 310, 310, 308, 308, 308, 307, 307, 312, 313, 313, 313, 312, 311, 310, 310, 310, 310, 312, 312, 312, 313, 314, 314, 313, 312, 311, 310, 310, 310, 310, 310, 312, 312, 310, 308, 309, 310, 311, 311, 311, 312, 312, 312, 312, 312, 312, 312, 312, 315]}, {'name': 'polyline', 'all_points_x': [211, 209, 207, 205, 207, 208, 210, 212, 214, 210, 210, 208, 207, 207, 206, 205, 206, 207, 207, 207, 207, 206, 205, 203, 204, 207, 209, 209, 212, 213, 213, 214, 215, 216, 216, 217, 218, 218, 219, 219, 220, 214, 213, 211], 'all_points_y': [290, 291, 293, 292, 288, 288, 289, 288, 287, 285, 284, 284, 283, 282, 283, 282, 280, 280, 279, 277, 276, 276, 276, 275, 274, 274, 272, 270, 271, 273, 274, 274, 276, 278, 278, 280, 280, 281, 282, 283, 283, 292, 291, 290]}, {'name': 'polyline', 'all_points_x': [235, 235, 233, 232, 233, 233, 235, 235, 235, 236, 238, 239, 239, 240, 241, 242, 243, 243, 243, 243, 241, 241, 240, 238, 238, 237, 237, 237, 236, 235, 235], 'all_points_y': [268, 268, 270, 271, 272, 273, 275, 276, 277, 278, 280, 281, 282, 282, 282, 283, 283, 284, 282, 280, 278, 278, 277, 275, 274, 271, 270, 270, 269, 268, 268]}, {'name': 'polyline', 'all_points_x': [370, 370, 371, 370, 370, 368, 368, 368, 368, 367, 365, 363, 361, 361, 360, 358, 360, 363, 364, 369, 371, 371, 373, 373, 374, 375, 377, 378, 378, 377, 376, 375, 375, 374, 373, 372, 372, 371, 371, 370], 'all_points_y': [386, 388, 390, 392, 392, 392, 390, 389, 389, 389, 390, 391, 395, 398, 399, 399, 401, 401, 401, 402, 402, 399, 396, 395, 394, 394, 392, 392, 392, 391, 390, 390, 390, 389, 389, 388, 387, 387, 387, 387]}, {'name': 'polyline', 'all_points_x': [377, 376, 375, 375, 376, 377, 379, 380, 381, 384, 386, 386, 389, 390, 390, 392, 394, 394, 395, 395, 399, 402, 404, 406, 406, 407, 407, 407, 408, 408, 408, 407, 408, 409, 412, 409, 409, 410, 410, 409, 408, 406, 405, 404, 403, 402, 402, 400, 399, 397, 397, 396, 396, 396, 396, 394, 392, 389, 387, 383, 381, 379, 378, 377], 'all_points_y': [375, 376, 378, 380, 382, 383, 383, 382, 382, 384, 384, 383, 381, 380, 379, 380, 380, 383, 385, 386, 391, 392, 394, 396, 397, 397, 399, 401, 401, 404, 405, 406, 407, 409, 408, 408, 405, 404, 401, 399, 397, 394, 393, 392, 391, 391, 389, 388, 387, 386, 385, 384, 382, 380, 379, 377, 377, 375, 374, 376, 375, 375, 375, 375]}], 'num_ids': []}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1265, in load_image_gt      class_ids = class_ids[_idx]  TypeError: only integer scalar arrays can be converted to a scalar index    Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1265, in load_image_gt      class_ids = class_ids[_idx]  TypeError: only integer scalar arrays can be converted to a scalar index  multiprocessing.pool.RemoteTraceback:   """"""  Traceback (most recent call last):    File ""/usr/lib/python3.6/multiprocessing/pool.py"", line 119, in worker      result = (True, func(*args, **kwds))    File ""/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py"", line 641, in next_sample      return six.next(_SHARED_SEQUENCES[uid])    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1265, in load_image_gt      class_ids = class_ids[_idx]  TypeError: only integer scalar arrays can be converted to a scalar index  """"""    The above exception was the direct cause of the following exception:    Traceback (most recent call last):    File ""mineria.py"", line 392, in        train(model)    File ""mineria.py"", line 228, in train      layers='heads')    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 2374, in train    File ""/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 1658, in fit_generator      initial_epoch=initial_epoch)    File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py"", line 181, in fit_generator      generator_output = next(output_generator)    File ""/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py"", line 733, in get      six.reraise(*sys.exc_info())    File ""/usr/local/lib/python3.6/dist-packages/six.py"", line 693, in reraise      raise value    File ""/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py"", line 702, in get      inputs = future.get(timeout=30)    File ""/usr/lib/python3.6/multiprocessing/pool.py"", line 644, in get      raise self._value  TypeError: only integer scalar arrays can be converted to a scalar index`    Someone can help me, I have spent several time to fix this error but I don't know how to solve it. "
"Which version of TensorFlow, TensorFlow-GPU and Keras would be best for the CUDA 10.0 for training Mask RCNN?. I tried multiple combinations looking at the issues but kept getting issues."
"!     I am trying to train mask-rcnn with the coco data set. my TensorFlow version is 1.3 and TensorFlow- GPU version is 1.9.0. I would like to know how to fix this issue.  I have changed the keep_dims variable to keepdims already in the model file, in spite of that I get this."
"Good day    I'm trying to train the MRCNN on my Anaconda environment. I'm using Python 3.6 and on a Windows 8.1. I believe I've managed to install TF (`version 1.15.0`) and KERAS (`version 2.2.4-tf`) correctly (this is my first time playing around with deep learning btw).     I've managed to get down to the line where the error is happening, and the code is:  `model.train(train_set, test_set, learning_rate=config.LEARNING_RATE, epochs=5, layers='heads')`    The error that comes out is **`AttributeError: 'Model' object has no attribute 'metrics_tensors'`**   (details below):       I've seen a solution being suggested here:   and tried implement it. The problem is, I can't seem to get into the **`mask_rcnn-2.1-py3.6.egg`** folder from Windows Explorer in order to access the `model.py` file..  When I go to the Anaconda prompt, I see that **`mask_rcnn-2.1-py3.6.egg`** is listed, however it does not seem to be a directory/folder (please refer to the screen shot below):  !     I did try to do a search for a `model.py` file on my computer, and found the one located at `C:\Users\student\Mask_RCNN\mrcnn\model.py`. Modified it as per    suggestion, but the error still persists. I was wondering if anyone can help? Any assistance is much appreciated."
"**_tensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading  resource variable res4q_branch2c_1/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource:localhost/res4q_branch2c_1/kernel)      [[{{node res4q_branch2c_1/convolution/ReadVariableOp}}]]_**    I'm running 2 tensorflow versions (2 and 1.15)  in different anaconda environments with different flask servers,  but Mask RCNN in tensorflow 1.15 environment "
"I am currently using a NVIDIA Jetson TX2 with Ubuntu 18.04 to train a Mask R-CNN (Matterport implementation   with my custom dataset. When I try to load the `mask_rcnn_coco.h5`, the following error comes up multiple times:        The same error appears in many topics when training the model, but here I am just trying to load the pre-trained weights. I am not understanding why only a small piece of memory is being employed."
I am trying to train Mask RCNN with 37 (1+36) classes and when I am trying to display data I am getting index error. I have tried changing from  NUM_CLASSES = 1 + 80 to NUM_CLASSES = 1 + 36 in coco.py but that didn't work.                   and facing same issue in every dataset.load_mask(image_id) call  Can someone help me out with this issue?
"Hi,   I am trying to use maskRCNN with a resnet50 backbone. The trained model has 5 classes. I am able to generate uff similar to the coco example provided at       While execution sample_uff_maskRCNN it's creating weights mismatch error. Error code            While UFF and pbtxt has been generated successfully. Please help, if UFF and pbtxt generated from same file how can be weights missmatch?    Config Files I have used for generating uff are as attatched below.           "
"Hi,     I've been using Mask-RCNN to generate masks. The images I am detecting on are quite large (w = 4032, h = 3024). During training these were downscaled to 1024x1024.     I am using the following code to visualise the bboxes and masks in an image:         However this displays an image of 1024x1024, as the image is moulded to what the CNN needs. Is there a way to show the bboxes and masks but still have the image it's original size when displayed?    "
I have trained a model to detect some specific classes and I can run predictions on test images and output images with predictions as masks. But how do I evaluate the accuracy of the model predictions with my manual annotations of the test dataset (not train or val). Is there a function where I can input my ground truth annotations of the test dataset?    
I need an idea to apply Class Activation Mapping into the model
"OS: Windows 10  GPU : 2080 Ti  Keras : 2.1.0  Tensorflow : 1.14  conda 4.7.12      StopIteration                             Traceback (most recent call last)    in          6             learning_rate=config.LEARNING_RATE,        7             epochs=1,  ----> 8             layers='heads')  "
"**OS: Ubuntu18.04  GPU: RTX2080(8g)  tensorflow-gpu: 2.0  CUDA:10.1  cuDNN: 7.6**    **Reference:**      After I fix some API error (like change tf.log to tf.math.log, see pull requests #1817, #1841, #1842 ), I got other error:   Traceback (most recent call last):    File &quot;kangaroo_maskrcnn.py&quot;, line 109, in &lt;module&gt;      model.train(train_set, test_set, learning_rate=config.LEARNING_RATE, epochs=5, layers=&apos;heads&apos;)    File &quot;/home/gaoya/Files/python/keras/Mask_RCNN-master/mrcnn/model.py&quot;, line 2361, in train      self.compile(learning_rate, self.config.LEARNING_MOMENTUM)    File &quot;/home/gaoya/Files/python/keras/Mask_RCNN-master/mrcnn/model.py&quot;, line 2177, in compile      if layer.output in self.keras_model.losses:    File &quot;/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py&quot;, line 765, in __bool__      self._disallow_bool_casting()    File &quot;/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py&quot;, line 534, in _disallow_bool_casting      self._disallow_in_graph_mode(&quot;using a `tf.Tensor` as a Python `bool`&quot;)    File &quot;/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py&quot;, line 523, in _disallow_in_graph_mode      &quot; this function with @tf.function.&quot;.format(task))  tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.       Can someone fix this problem? The code is from the reference above."
"Hi,  I'm trying to implement my model with multi gpu processing. It works with single gpu processing, but that takes too much time.    ## My code       ## My Error       Can anybody help me with this error?"
"At first, thank you so much for the framework    Now I'm teaming up in a research project, as part of my master's degree thesis, which consists in categorize and classify an avocado on differents scales of maturity, the previous approach was only taking the mean of RGB and check it in the scale.  Now we have a dataset in COCO annotations in which each class corresponds to a maturity scale level.  But all the images are avocados with different colors and textures just classified by maturity in COCO Annotations.    Ok I'm sorry, the question is, is it possible to train a model in detectron2 using a dataset of only avocados but characterized by maturity?.  in other words, class 1 is gonna be green avocados and class 2 is gonna be matured avocados.  The main difference between those classes is the color.  Then the response that I want, is an object detection with those classes.  I don't know if is that possible, sorry if I'm not clear.    I will appreciate any response."
"Trying to convert to tf.keras and running into the following issue, anyone has an idea?    ``File ""D:\Project\model.py"", line 2191, in __init__      self.keras_model = self.build(mode=mode, config=config)    File ""D:\Project\model.py"", line 2399, in build      [rpn_rois, mrcnn_class, mrcnn_bbox, input_image_meta])    File ""C:\Users\admin\Anaconda3\envs\ModelTest\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 554, in __call__      outputs = self.call(inputs, *args, **kwargs)    File ""D:\Project\model.py"", line 1117, in call      self.config.IMAGES_PER_GPU)    File ""D:\Project\utils.py"", line 854, in batch_slice      output_slice = graph_fn(*inputs_slice)    File ""D:\Project\model.py"", line 1116, in        lambda x, y, w, z: refine_detections_graph(x, y, w, z, self.config),    File ""D:\Project\model.py"", line 1006, in refine_detections_graph      indices = tf.stack([tf.range(probs.shape[0]), class_ids], axis=1)    File ""C:\Users\admin\Anaconda3\envs\ModelTest\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 1182, in range      limit = ops.convert_to_tensor(limit, dtype=dtype, name=""limit"")    File ""C:\Users\admin\Anaconda3\envs\ModelTest\lib\site-packages\tensorflow\python\framework\ops.py"", line 1039, in convert_to_tensor      return convert_to_tensor_v2(value, dtype, preferred_dtype, name)    File ""C:\Users\admin\Anaconda3\envs\ModelTest\lib\site-packages\tensorflow\python\framework\ops.py"", line 1097, in convert_to_tensor_v2      as_ref=False)    File ""C:\Users\admin\Anaconda3\envs\ModelTest\lib\site-packages\tensorflow\python\framework\ops.py"", line 1175, in internal_convert_to_tensor      ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)    File ""C:\Users\admin\Anaconda3\envs\ModelTest\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 357, in _dimension_tensor_conversion_function      raise ValueError(""Cannot convert an unknown Dimension to a Tensor: %s"" % d)  ValueError: Cannot convert an unknown Dimension to a Tensor: ?``"
"Under `model.py`  we see the keras model compile         And then the metrics for the Mask RCNN metrics are added:         But when training the model, neither `train_accuracy` nor `val_accuracy` are reported.  How to add these metrics and report them with each epoch, alongside the mrcnn metrics?    !   "
"Hi!     I need to force the ROIs to be constant over a changing input. But I am not very experienced with Keras or tensorflow.    My plan is to first evaluate an input image, store the ROI layer activation for that image, and use that activation in the following input images.    So far I've created a new mode, in which ROI is defined as an input layer as such:                assert ROI_tensor is not None, ""Provide ROI activations""              ROI_tensor = K.variable(ROI_tensor)              rpn_rois = KL.Input(tensor=ROI_tensor,                                  name=""ROI"")    Is this a good way to do it? Currently I am getting errors in refine_detections_graph(). I'll ellaborate if needed."
"In the below example, I am trying to annotate the 'grass' class.  What would be the best technique in this instance?   Naturally, a simple polygon would have to include the sheep objects which is undesired.    Thanks    !   "
It would be of great help if the code of model can be remodified to the TensorFlow 2.0 version as only minor changes could fix the problem
"I have trained up a model and everything works fine on that computer. However, when I am using the .h5 file to predict on images in a different computer, it doesn't detect any object and a 'No instances to display' message appears.     The strange thing is the exact same code (I have a script which deals only with predicting) and the .h5 file works on the machine which it was trained on, but not on the different one.     Another strange observation is when I add `  exclude=[ ""mrcnn_class_logits"", ""mrcnn_bbox_fc"", ""mrcnn_bbox"", ""mrcnn_mask""]` , it does show me some instances, which are however not correct.     I am using Tensorflow-GPU 1.14.0, Keras 2.2.4 and Python 3.7.4 on both the computers"
"demo.ipynb(results = model.detect([image], verbose=1)) which [image] is list ,but model.py(molded_images, image_metas, windows = self.mold_inputs(images)) --->  molded_image, window, scale, padding, crop = utils.resize_image ---> utils.py(image_dtype = image.dtype)  here image type must be array,I have no idea when and where image has changed ,but demo.ipynb can run without error ã€‚What's the problem, please"
"In `model.py`, line `1711` there is this code:     This does not make sense to me, as the model should be able to tell when an image has no instances of the desired objects. I remember having used the Tensorflow Object Detection API on Google Cloud, and ""empty"" images were used and made a difference on the final resulting model (fewer false positives).    Maybe there is a way to go around this limitation?"
"if appling focal_loss  to rpn_class ,mrcnn_class ,mrcnn_mask , the performance will be better  ?"
None
"Hi,  I am trying to run the model using 2 images at one time to increase performance.  Code is -  /samples/demo.ipynb  `# Load a random image from the images folder  file_names = next(os.walk(IMAGE_DIR)) "
"In keras,   **batch_size = [ train set size / steps_per_epoch ]**     In the configuration,  **batch_size = GPU_count * Images_Per_GPU**    Given the fact that the steps_per epoch is a parameter to change, these two expression give different results.     Any ideas about which one to hold ?"
any explanation about **the chosen value** for the gradient norm clipping ?
"When I try to run the code in 2.a of inspect_model.ipynb, i.e.     , the detection boxes I get are strangely shaped, and it seems like they haven't been rescaled correctly. When running model.predict and visualize.display_instances, however, the bounding boxes are correct. Does anyone know why the detection boxes from run_graph and from model.predict might be different?"
"After update TensorFlow 1.14 to 2.0 and use tf.keras instead of keras, when using fpn_classifier_graph I get:    `ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.`     "
"**I used bottle.py to retrain on my custom dataset here is the exact code:  !python bottle/bottle.py train --dataset=/content/Mask_RCNN/samples/bottle/dataset --weights=coco**    Using TensorFlow backend.  Weights:  coco  Dataset:  /content/Mask_RCNN/samples/bottle/dataset  Logs:  /content/logs    Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     2  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.9  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 2  IMAGE_CHANNEL_COUNT            3  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                14  IMAGE_MIN_DIM                  800  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           human  NUM_CLASSES                    2  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  PRE_NMS_LIMIT                  6000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                100  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001      WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2139: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.  Instructions for updating:  Use tf.where in 2.0, which has the same broadcast rule as np.where  WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py:553: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py:600: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.  Instructions for updating:  box_ind is deprecated, use box_indices instead  Loading weights  /content/mask_rcnn_coco.h5  WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.    2019-10-24 12:43:23.228850: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz  2019-10-24 12:43:23.229737: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x19fa8bc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:  2019-10-24 12:43:23.229779: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version  2019-10-24 12:43:23.231966: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1  2019-10-24 12:43:23.314495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2019-10-24 12:43:23.315434: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x19fa8d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:  2019-10-24 12:43:23.315465: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7  2019-10-24 12:43:23.315691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2019-10-24 12:43:23.316378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:   name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235  pciBusID: 0000:00:04.0  2019-10-24 12:43:23.316758: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0  2019-10-24 12:43:23.318138: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0  2019-10-24 12:43:23.319474: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0  2019-10-24 12:43:23.319873: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0  2019-10-24 12:43:23.321723: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0  2019-10-24 12:43:23.322925: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0  2019-10-24 12:43:23.326860: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7  2019-10-24 12:43:23.327012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2019-10-24 12:43:23.327851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2019-10-24 12:43:23.328504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0  2019-10-24 12:43:23.328601: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0  2019-10-24 12:43:23.330412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:  2019-10-24 12:43:23.330469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0   2019-10-24 12:43:23.330510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N   2019-10-24 12:43:23.330777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2019-10-24 12:43:23.331843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2019-10-24 12:43:23.332564: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.  2019-10-24 12:43:23.332643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)  WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.    Training network heads    Starting at epoch 0. LR=0.001    Checkpoint Path: /content/logs/human20191024T1243/mask_rcnn_human_{epoch:04d}.h5  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  In model:  rpn_model      rpn_conv_shared        (Conv2D)      rpn_class_raw          (Conv2D)      rpn_bbox_pred          (Conv2D)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)  WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.    /usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.    UserWarning('Using a generator with `use_multiprocessing=True`'  WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.    Epoch 1/10  ERROR:root:Error processing image {'id': 'IMG_20190313_185930.jpg', 'source': 'human', 'path': '/content/Mask_RCNN/samples/bottle/dataset/val/IMG_20190313_185930.jpg', 'width': 2000, 'height': 4000, 'polygons': [{'name': 'polygon', 'all_points_x': [3987, 3745, 3517, 3167, 3094, 2430, 2331, 2036, 1792, 1534, 1488, 1428, 1322, 1183, 1071, 902, 681, 489, 397, 476, 664, 774, 1025, 1134, 1260, 1339, 1491, 1531, 1567, 1620, 1752, 1904, 2149, 2172, 2238, 2314, 2354, 2380, 2552, 2628, 2704, 2926, 3098, 3329, 3593, 3874, 3980, 3990], 'all_points_y': [1666, 1517, 1597, 1600, 1537, 1560, 1517, 1683, 1593, 1286, 1121, 1038, 1038, 1180, 1144, 1164, 1174, 1041, 840, 645, 493, 486, 466, 519, 450, 403, 410, 446, 380, 374, 228, 175, 66, 106, 112, 86, 169, 188, 175, 215, 354, 433, 410, 473, 552, 793, 909, 929]}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 2147 is out of bounds for axis 1 with size 2000  ERROR:root:Error processing image {'id': 'IMG_20190313_185930.jpg', 'source': 'human', 'path': '/content/Mask_RCNN/samples/bottle/dataset/val/IMG_20190313_185930.jpg', 'width': 2000, 'height': 4000, 'polygons': [{'name': 'polygon', 'all_points_x': [3987, 3745, 3517, 3167, 3094, 2430, 2331, 2036, 1792, 1534, 1488, 1428, 1322, 1183, 1071, 902, 681, 489, 397, 476, 664, 774, 1025, 1134, 1260, 1339, 1491, 1531, 1567, 1620, 1752, 1904, 2149, 2172, 2238, 2314, 2354, 2380, 2552, 2628, 2704, 2926, 3098, 3329, 3593, 3874, 3980, 3990], 'all_points_y': [1666, 1517, 1597, 1600, 1537, 1560, 1517, 1683, 1593, 1286, 1121, 1038, 1038, 1180, 1144, 1164, 1174, 1041, 840, 645, 493, 486, 466, 519, 450, 403, 410, 446, 380, 374, 228, 175, 66, 106, 112, 86, 169, 188, 175, 215, 354, 433, 410, 473, 552, 793, 909, 929]}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 2147 is out of bounds for axis 1 with size 2000  ERROR:root:Error processing image {'id': 'IMG_20191004_122811.jpg', 'source': 'human', 'path': '/content/Mask_RCNN/samples/bottle/dataset/val/IMG_20191004_122811.jpg', 'width': 2000, 'height': 4000, 'polygons': [{'name': 'polygon', 'all_points_x': [2539, 2418, 2237, 2005, 1904, 1757, 1656, 1538, 1478, 1364, 1370, 1216, 1034, 883, 860, 930, 1024, 1122, 1283, 1495, 1636, 1666, 1830, 2069, 2354, 2529, 2623, 2848, 3023, 3275, 3671, 3983, 3997, 3976], 'all_points_y': [1474, 1585, 1585, 1417, 1474, 1384, 1421, 1364, 1226, 1155, 1065, 1095, 1132, 1045, 863, 715, 641, 608, 608, 551, 514, 386, 289, 232, 259, 302, 309, 215, 312, 275, 198, 178, 490, 1757]}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 3968 is out of bounds for axis 1 with size 2000  ERROR:root:Error processing image {'id': 'IMG_20191004_122811.jpg', 'source': 'human', 'path': '/content/Mask_RCNN/samples/bottle/dataset/val/IMG_20191004_122811.jpg', 'width': 2000, 'height': 4000, 'polygons': [{'name': 'polygon', 'all_points_x': [2539, 2418, 2237, 2005, 1904, 1757, 1656, 1538, 1478, 1364, 1370, 1216, 1034, 883, 860, 930, 1024, 1122, 1283, 1495, 1636, 1666, 1830, 2069, 2354, 2529, 2623, 2848, 3023, 3275, 3671, 3983, 3997, 3976], 'all_points_y': [1474, 1585, 1585, 1417, 1474, 1384, 1421, 1364, 1226, 1155, 1065, 1095, 1132, 1045, 863, 715, 641, 608, 608, 551, 514, 386, 289, 232, 259, 302, 309, 215, 312, 275, 198, 178, 490, 1757]}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 3968 is out of bounds for axis 1 with size 2000  ERROR:root:Error processing image {'id': 'IMG_20190313_185930.jpg', 'source': 'human', 'path': '/content/Mask_RCNN/samples/bottle/dataset/val/IMG_20190313_185930.jpg', 'width': 2000, 'height': 4000, 'polygons': [{'name': 'polygon', 'all_points_x': [3987, 3745, 3517, 3167, 3094, 2430, 2331, 2036, 1792, 1534, 1488, 1428, 1322, 1183, 1071, 902, 681, 489, 397, 476, 664, 774, 1025, 1134, 1260, 1339, 1491, 1531, 1567, 1620, 1752, 1904, 2149, 2172, 2238, 2314, 2354, 2380, 2552, 2628, 2704, 2926, 3098, 3329, 3593, 3874, 3980, 3990], 'all_points_y': [1666, 1517, 1597, 1600, 1537, 1560, 1517, 1683, 1593, 1286, 1121, 1038, 1038, 1180, 1144, 1164, 1174, 1041, 840, 645, 493, 486, 466, 519, 450, 403, 410, 446, 380, 374, 228, 175, 66, 106, 112, 86, 169, 188, 175, 215, 354, 433, 410, 473, 552, 793, 909, 929]}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 2147 is out of bounds for axis 1 with size 2000  ERROR:root:Error processing image {'id': 'IMG_20190313_185930.jpg', 'source': 'human', 'path': '/content/Mask_RCNN/samples/bottle/dataset/val/IMG_20190313_185930.jpg', 'width': 2000, 'height': 4000, 'polygons': [{'name': 'polygon', 'all_points_x': [3987, 3745, 3517, 3167, 3094, 2430, 2331, 2036, 1792, 1534, 1488, 1428, 1322, 1183, 1071, 902, 681, 489, 397, 476, 664, 774, 1025, 1134, 1260, 1339, 1491, 1531, 1567, 1620, 1752, 1904, 2149, 2172, 2238, 2314, 2354, 2380, 2552, 2628, 2704, 2926, 3098, 3329, 3593, 3874, 3980, 3990], 'all_points_y': [1666, 1517, 1597, 1600, 1537, 1560, 1517, 1683, 1593, 1286, 1121, 1038, 1038, 1180, 1144, 1164, 1174, 1041, 840, 645, 493, 486, 466, 519, 450, 403, 410, 446, 380, 374, 228, 175, 66, 106, 112, 86, 169, 188, 175, 215, 354, 433, 410, 473, 552, 793, 909, 929]}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 2147 is out of bounds for axis 1 with size 2000  ERROR:root:Error processing image {'id': 'IMG_20191004_122811.jpg', 'source': 'human', 'path': '/content/Mask_RCNN/samples/bottle/dataset/val/IMG_20191004_122811.jpg', 'width': 2000, 'height': 4000, 'polygons': [{'name': 'polygon', 'all_points_x': [2539, 2418, 2237, 2005, 1904, 1757, 1656, 1538, 1478, 1364, 1370, 1216, 1034, 883, 860, 930, 1024, 1122, 1283, 1495, 1636, 1666, 1830, 2069, 2354, 2529, 2623, 2848, 3023, 3275, 3671, 3983, 3997, 3976], 'all_points_y': [1474, 1585, 1585, 1417, 1474, 1384, 1421, 1364, 1226, 1155, 1065, 1095, 1132, 1045, 863, 715, 641, 608, 608, 551, 514, 386, 289, 232, 259, 302, 309, 215, 312, 275, 198, 178, 490, 1757]}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 3968 is out of bounds for axis 1 with size 2000  ERROR:root:Error processing image {'id': 'IMG_20191004_122811.jpg', 'source': 'human', 'path': '/content/Mask_RCNN/samples/bottle/dataset/val/IMG_20191004_122811.jpg', 'width': 2000, 'height': 4000, 'polygons': [{'name': 'polygon', 'all_points_x': [2539, 2418, 2237, 2005, 1904, 1757, 1656, 1538, 1478, 1364, 1370, 1216, 1034, 883, 860, 930, 1024, 1122, 1283, 1495, 1636, 1666, 1830, 2069, 2354, 2529, 2623, 2848, 3023, 3275, 3671, 3983, 3997, 3976], 'all_points_y': [1474, 1585, 1585, 1417, 1474, 1384, 1421, 1364, 1226, 1155, 1065, 1095, 1132, 1045, 863, 715, 641, 608, 608, 551, 514, 386, 289, 232, 259, 302, 309, 215, 312, 275, 198, 178, 490, 1757]}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 3968 is out of bounds for axis 1 with size 2000  ERROR:root:Error processing image {'id': 'IMG_20191004_122811.jpg', 'source': 'human', 'path': '/content/Mask_RCNN/samples/bottle/dataset/val/IMG_20191004_122811.jpg', 'width': 2000, 'height': 4000, 'polygons': [{'name': 'polygon', 'all_points_x': [2539, 2418, 2237, 2005, 1904, 1757, 1656, 1538, 1478, 1364, 1370, 1216, 1034, 883, 860, 930, 1024, 1122, 1283, 1495, 1636, 1666, 1830, 2069, 2354, 2529, 2623, 2848, 3023, 3275, 3671, 3983, 3997, 3976], 'all_points_y': [1474, 1585, 1585, 1417, 1474, 1384, 1421, 1364, 1226, 1155, 1065, 1095, 1132, 1045, 863, 715, 641, 608, 608, 551, 514, 386, 289, 232, 259, 302, 309, 215, 312, 275, 198, 178, 490, 1757]}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 3968 is out of bounds for axis 1 with size 2000  ERROR:root:Error processing image {'id': 'IMG_20191004_122811.jpg', 'source': 'human', 'path': '/content/Mask_RCNN/samples/bottle/dataset/val/IMG_20191004_122811.jpg', 'width': 2000, 'height': 4000, 'polygons': [{'name': 'polygon', 'all_points_x': [2539, 2418, 2237, 2005, 1904, 1757, 1656, 1538, 1478, 1364, 1370, 1216, 1034, 883, 860, 930, 1024, 1122, 1283, 1495, 1636, 1666, 1830, 2069, 2354, 2529, 2623, 2848, 3023, 3275, 3671, 3983, 3997, 3976], 'all_points_y': [1474, 1585, 1585, 1417, 1474, 1384, 1421, 1364, 1226, 1155, 1065, 1095, 1132, 1045, 863, 715, 641, 608, 608, 551, 514, 386, 289, 232, 259, 302, 309, 215, 312, 275, 198, 178, 490, 1757]}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 3968 is out of bounds for axis 1 with size 2000  ERROR:root:Error processing image {'id': 'IMG_20190313_185930.jpg', 'source': 'human', 'path': '/content/Mask_RCNN/samples/bottle/dataset/val/IMG_20190313_185930.jpg', 'width': 2000, 'height': 4000, 'polygons': [{'name': 'polygon', 'all_points_x': [3987, 3745, 3517, 3167, 3094, 2430, 2331, 2036, 1792, 1534, 1488, 1428, 1322, 1183, 1071, 902, 681, 489, 397, 476, 664, 774, 1025, 1134, 1260, 1339, 1491, 1531, 1567, 1620, 1752, 1904, 2149, 2172, 2238, 2314, 2354, 2380, 2552, 2628, 2704, 2926, 3098, 3329, 3593, 3874, 3980, 3990], 'all_points_y': [1666, 1517, 1597, 1600, 1537, 1560, 1517, 1683, 1593, 1286, 1121, 1038, 1038, 1180, 1144, 1164, 1174, 1041, 840, 645, 493, 486, 466, 519, 450, 403, 410, 446, 380, 374, 228, 175, 66, 106, 112, 86, 169, 188, 175, 215, 354, 433, 410, 473, 552, 793, 909, 929]}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 2147 is out of bounds for axis 1 with size 2000  ERROR:root:Error processing image {'id': 'IMG_20190313_185930.jpg', 'source': 'human', 'path': '/content/Mask_RCNN/samples/bottle/dataset/val/IMG_20190313_185930.jpg', 'width': 2000, 'height': 4000, 'polygons': [{'name': 'polygon', 'all_points_x': [3987, 3745, 3517, 3167, 3094, 2430, 2331, 2036, 1792, 1534, 1488, 1428, 1322, 1183, 1071, 902, 681, 489, 397, 476, 664, 774, 1025, 1134, 1260, 1339, 1491, 1531, 1567, 1620, 1752, 1904, 2149, 2172, 2238, 2314, 2354, 2380, 2552, 2628, 2704, 2926, 3098, 3329, 3593, 3874, 3980, 3990], 'all_points_y': [1666, 1517, 1597, 1600, 1537, 1560, 1517, 1683, 1593, 1286, 1121, 1038, 1038, 1180, 1144, 1164, 1174, 1041, 840, 645, 493, 486, 466, 519, 450, 403, 410, 446, 380, 374, 228, 175, 66, 106, 112, 86, 169, 188, 175, 215, 354, 433, 410, 473, 552, 793, 909, 929]}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 2147 is out of bounds for axis 1 with size 2000  ERROR:root:Error processing image {'id': 'IMG_20181110_234904.jpg', 'source': 'human', 'path': '/content/Mask_RCNN/samples/bottle/dataset/train/IMG_20181110_234904.jpg', 'width': 3880, 'height': 5184, 'polygons': [{'name': 'polygon', 'all_points_x': [5, 208, 444, 3513, 3606, 3825, 3847, 3907, 3929, 4225, 4296, 4636, 4718, 4877, 5014, 5085, 4932, 4647, 4494, 4252, 3984, 3677, 3677, 3567, 3485, 3820, 3847, 4132, 4340, 4389, 4466, 4428, 4302, 4061, 3907, 3661, 3688, 3683, 4066, 4340, 4466, 4510, 4466, 4346, 2948, 2707, 2548, 2439, 5, 16, 449, 345, 329, 11], 'all_points_y': [378, 66, 5, 11, 148, 274, 329, 329, 241, 142, 181, 110, 164, 27, 258, 499, 1014, 1129, 1014, 1096, 1107, 1025, 1184, 1321, 1529, 1381, 1397, 1348, 1436, 1567, 1808, 2006, 2082, 2197, 2170, 2170, 2395, 2450, 2581, 2756, 3096, 3381, 3639, 3858, 3874, 3754, 3754, 3852, 3863, 1638, 1573, 1501, 1343, 1321]}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 4876 is out of bounds for axis 1 with size 3880  ERROR:root:Error processing image {'id': 'IMG_20181110_234904.jpg', 'source': 'human', 'path': '/content/Mask_RCNN/samples/bottle/dataset/train/IMG_20181110_234904.jpg', 'width': 3880, 'height': 5184, 'polygons': [{'name': 'polygon', 'all_points_x': [5, 208, 444, 3513, 3606, 3825, 3847, 3907, 3929, 4225, 4296, 4636, 4718, 4877, 5014, 5085, 4932, 4647, 4494, 4252, 3984, 3677, 3677, 3567, 3485, 3820, 3847, 4132, 4340, 4389, 4466, 4428, 4302, 4061, 3907, 3661, 3688, 3683, 4066, 4340, 4466, 4510, 4466, 4346, 2948, 2707, 2548, 2439, 5, 16, 449, 345, 329, 11], 'all_points_y': [378, 66, 5, 11, 148, 274, 329, 329, 241, 142, 181, 110, 164, 27, 258, 499, 1014, 1129, 1014, 1096, 1107, 1025, 1184, 1321, 1529, 1381, 1397, 1348, 1436, 1567, 1808, 2006, 2082, 2197, 2170, 2170, 2395, 2450, 2581, 2756, 3096, 3381, 3639, 3858, 3874, 3754, 3754, 3852, 3863, 1638, 1573, 1501, 1343, 1321]}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 4876 is out of bounds for axis 1 with size 3880  2019-10-24 12:44:40.032022: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0  2019-10-24 12:44:41.064467: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7  ERROR:root:Error processing image {'id': 'IMG_20181025_192944.jpg', 'source': 'human', 'path': '/content/Mask_RCNN/samples/bottle/dataset/train/IMG_20181025_192944.jpg', 'width': 3000, 'height': 4000, 'polygons': [{'name': 'polygon', 'all_points_x': [3966, 3729, 3458, 3093, 2653, 2542, 1716, 1254, 1169, 742, 449, 360, 428, 597, 864, 1051, 1208, 1271, 1364, 1669, 2288, 2797, 3394, 3788, 3797, 3996], 'all_points_y': [1797, 2064, 2085, 1953, 1924, 2008, 1860, 1716, 1483, 1530, 1428, 1216, 958, 864, 869, 936, 936, 945, 733, 538, 479, 487, 568, 691, 746, 860]}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 3000 is out of bounds for axis 1 with size 3000  ERROR:root:Error processing image {'id': 'IMG_20181025_192944.jpg', 'source': 'human', 'path': '/content/Mask_RCNN/samples/bottle/dataset/train/IMG_20181025_192944.jpg', 'width': 3000, 'height': 4000, 'polygons': [{'name': 'polygon', 'all_points_x': [3966, 3729, 3458, 3093, 2653, 2542, 1716, 1254, 1169, 742, 449, 360, 428, 597, 864, 1051, 1208, 1271, 1364, 1669, 2288, 2797, 3394, 3788, 3797, 3996], 'all_points_y': [1797, 2064, 2085, 1953, 1924, 2008, 1860, 1716, 1483, 1530, 1428, 1216, 958, 864, 869, 936, 936, 945, 733, 538, 479, 487, 568, 691, 746, 860]}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 3000 is out of bounds for axis 1 with size 3000    4/100 [>.............................] - ETA: 34:06 - loss: 3.3600 - rpn_class_loss: 0.0100 - rpn_bbox_loss: 0.7305 - mrcnn_class_loss: 0.6806 - mrcnn_bbox_loss: 1.3180 - mrcnn_mask_loss: 0.6208ERROR:root:Error processing image {'id': 'IMG_20181026_165816.jpg', 'source': 'human', 'path': '/content/Mask_RCNN/samples/bottle/dataset/train/IMG_20181026_165816.jpg', 'width': 3880, 'height': 5184, 'polygons': [{'name': 'polygon', 'all_points_x': [4318, 4110, 4017, 3529, 3315, 3200, 3036, 3030, 2663, 2597, 2055, 1940, 1534, 1019, 696, 674, 784, 1233, 1315, 1545, 1682, 1770, 1984, 2269, 2439, 3156, 3513, 3841, 4165, 4554, 4773, 5102, 5162, 4696, 4439], 'all_points_y': [3266, 3299, 2937, 2976, 3518, 3677, 3759, 3376, 3047, 2724, 2471, 2696, 3036, 3080, 2778, 2417, 2176, 1973, 1825, 1666, 1266, 1156, 800, 570, 515, 570, 570, 1277, 1880, 2099, 2203, 2323, 2833, 2685, 2521]}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 3880 is out of bounds for axis 1 with size 3880  ERROR:root:Error processing image {'id': 'IMG_20181026_165816.jpg', 'source': 'human', 'path': '/content/Mask_RCNN/samples/bottle/dataset/train/IMG_20181026_165816.jpg', 'width': 3880, 'height': 5184, 'polygons': [{'name': 'polygon', 'all_points_x': [4318, 4110, 4017, 3529, 3315, 3200, 3036, 3030, 2663, 2597, 2055, 1940, 1534, 1019, 696, 674, 784, 1233, 1315, 1545, 1682, 1770, 1984, 2269, 2439, 3156, 3513, 3841, 4165, 4554, 4773, 5102, 5162, 4696, 4439], 'all_points_y': [3266, 3299, 2937, 2976, 3518, 3677, 3759, 3376, 3047, 2724, 2471, 2696, 3036, 3080, 2778, 2417, 2176, 1973, 1825, 1666, 1266, 1156, 800, 570, 515, 570, 570, 1277, 1880, 2099, 2203, 2323, 2833, 2685, 2521]}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 3880 is out of bounds for axis 1 with size 3880  ERROR:root:Error processing image {'id': 'IMG_20181106_143013.jpg', 'source': 'human', 'path': '/content/Mask_RCNN/samples/bottle/dataset/train/IMG_20181106_143013.jpg', 'width': 3880, 'height': 5184, 'polygons': [{'name': 'polygon', 'all_points_x': [5157, 4828, 3798, 3184, 2378, 2230, 1523, 1359, 1496, 1797, 2055, 2208, 2252, 2422, 2751, 2893, 2992, 2921, 2904, 3124, 3266, 3365, 3211, 3562, 3748, 3765, 3946, 4461, 4899, 5085, 5179], 'all_points_y': [3365, 3474, 3365, 3589, 3211, 2959, 2861, 2570, 2280, 2115, 2148, 2230, 1885, 1770, 1595, 1611, 1282, 882, 800, 712, 860, 1107, 1775, 1786, 1715, 1715, 1447, 1003, 773, 718, 926]}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 5082 is out of bounds for axis 1 with size 3880  ERROR:root:Error processing image {'id': 'IMG_20181106_143013.jpg', 'source': 'human', 'path': '/content/Mask_RCNN/samples/bottle/dataset/train/IMG_20181106_143013.jpg', 'width': 3880, 'height': 5184, 'polygons': [{'name': 'polygon', 'all_points_x': [5157, 4828, 3798, 3184, 2378, 2230, 1523, 1359, 1496, 1797, 2055, 2208, 2252, 2422, 2751, 2893, 2992, 2921, 2904, 3124, 3266, 3365, 3211, 3562, 3748, 3765, 3946, 4461, 4899, 5085, 5179], 'all_points_y': [3365, 3474, 3365, 3589, 3211, 2959, 2861, 2570, 2280, 2115, 2148, 2230, 1885, 1770, 1595, 1611, 1282, 882, 800, 712, 860, 1107, 1775, 1786, 1715, 1715, 1447, 1003, 773, 718, 926]}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 5082 is out of bounds for axis 1 with size 3880  ERROR:root:Error processing image {'id': 'IMG_20181026_140009.jpg', 'source': 'human', 'path': '/content/Mask_RCNN/samples/bottle/dataset/train/IMG_20181026_140009.jpg', 'width': 3000, 'height': 4000, 'polygons': [{'name': 'polygon', 'all_points_x': [3992, 3123, 3034, 2432, 2250, 1737, 1542, 1301, 1089, 987, 1025, 1254, 1470, 1555, 1623, 2144, 2525, 2860, 2941, 3521, 3801, 3992, 3966], 'all_points_y': [2682, 2695, 2860, 2911, 2970, 2924, 2610, 2678, 2720, 2568, 2326, 2263, 2326, 2309, 2140, 1996, 1975, 1949, 2097, 2148, 2182, 2102, 2411]}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 3000 is out of bounds for axis 1 with size 3000  ERROR:root:Error processing image {'id': 'IMG_20181026_140009.jpg', 'source': 'human', 'path': '/content/Mask_RCNN/samples/bottle/dataset/train/IMG_20181026_140009.jpg', 'width': 3000, 'height': 4000, 'polygons': [{'name': 'polygon', 'all_points_x': [3992, 3123, 3034, 2432, 2250, 1737, 1542, 1301, 1089, 987, 1025, 1254, 1470, 1555, 1623, 2144, 2525, 2860, 2941, 3521, 3801, 3992, 3966], 'all_points_y': [2682, 2695, 2860, 2911, 2970, 2924, 2610, 2678, 2720, 2568, 2326, 2263, 2326, 2309, 2140, 1996, 1975, 1949, 2097, 2148, 2182, 2102, 2411]}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 3000 is out of bounds for axis 1 with size 3000    8/100 [=>............................] - ETA: 24:35 - loss: 2.7762 - rpn_class_loss: 0.0144 - rpn_bbox_loss: 0.7168 - mrcnn_class_loss: 0.3957 - mrcnn_bbox_loss: 1.0654 - mrcnn_mask_loss: 0.5839ERROR:root:Error processing image {'id': 'IMG_20181106_113715.jpg', 'source': 'human', 'path': '/content/Mask_RCNN/samples/bottle/dataset/train/IMG_20181106_113715.jpg', 'width': 3880, 'height': 5184, 'polygons': [{'name': 'polygon', 'all_points_x': [16, 1677, 1786, 1819, 1940, 2197, 2400, 2713, 2866, 3200, 3419, 3600, 3496, 3239, 2828, 2630, 2713, 2850, 3030, 3315, 3606, 3929, 4083, 4033, 0], 'all_points_y': [27, 16, 225, 543, 652, 619, 532, 521, 575, 652, 860, 1228, 1573, 1858, 1995, 1896, 2143, 2313, 2269, 2230, 2186, 2307, 2636, 2904, 2904]}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 3880 is out of bounds for axis 1 with size 3880  multiprocessing.pool.RemoteTraceback:   """"""  Traceback (most recent call last):    File ""/usr/lib/python3.6/multiprocessing/pool.py"", line 119, in worker      result = (True, func(*args, **kwds))    File ""/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py"", line 641, in next_sample      return six.next(_SHARED_SEQUENCES[uid])    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""bottle/bottle.py"", line 164, in load_mask      mask[rr, cc, i] = 1  IndexError: index 3880 is out of bounds for axis 1 with size 3880  """"""    The above exception was the direct cause of the following exception:    Traceback (most recent call last):    File ""bottle/bottle.py"", line 363, in        train(model)    File ""bottle/bottle.py"", line 199, in train      layers='heads')    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 2374, in train    File ""/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 1658, in fit_generator      initial_epoch=initial_epoch)    File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py"", line 181, in fit_generator      generator_output = next(output_generator)    File ""/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py"", line 733, in get      six.reraise(*sys.exc_info())    File ""/usr/local/lib/python3.6/dist-packages/six.py"", line 693, in reraise      raise value    File ""/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py"", line 702, in get      inputs = future.get(timeout=30)    File ""/usr/lib/python3.6/multiprocessing/pool.py"", line 644, in get      raise self._value  IndexError: index 3880 is out of bounds for axis 1 with size 3880"
"I have some training data annotated with polygons tightly around the objects of interest, but I also have access to (much) more training data annotated with bounding boxes (although less tightly around objects). As fas as I can read in the readme, polygon annotations are actually converted to bounding boxes and the network is trained on these and not the polygon masks. Is that correct? If so, does that mean I could just train the network on my bounding box annotations? (E.g. by converting my bounding box annotations to polygons with four corners).    Thanks!  /HM"
"Is there any word on support for TensorFlow 2.0 and its built-in tf.keras?  I thought that this had been asked before, but I have not seen any updates.    It seems that this would be worthwhile in that TF 2.0 is now official, and  many will be moving to its built-in Keras module.    Otherwise, I presume that upgrading to TF 2.0 is likely to break existing  Matterport code...?    Also, are there discussion forums for Matterport?  This is more of an  implementation issue than a bug, so perhaps it is more appropriate for  a discussion forum, if that exists.  "
Hey I face a issue : when I start to training with coco and when facing :    UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.    UserWarning('Using a generator with `use_multiprocessing=True`'    And it is stuck    Anyone know why???  
I would like to change the names of layers on the .h5 file. Please let me know how I can go about doing this
"I'm training a modified version of MaskRCNN on ~1700 images of dimension 512x512.  LR = 0.0003.    I'm also using image augmentation via imgaug with LR flipping 0.5 of the time and random scaling and translation horizontally and vertically from [0.9 1.1], along with some very small rotations of +/- 5 degrees.    val_rpn_bbox_loss decreases but plateaus around 1.1, while the other Mask RCNN losses keep decreasing    Due to the smaller size, I added a few dimensions to the Anchors to capture objects are more scales.  I still kept the standard Anchor Ratio of [1/2, 1, 2], although some of the objects I'm trying to classify are little wider than can fit tightly bounded inside a 1:2 anchor box, so I might try adding [1/3, 3] to the default ratios.    I'm tracking the training with TensorBoard, and the training loss is dropping with each epoch nicely.  Also the other 4 losses are decreasingly nicely with each epoch, up through around 50 or so epochs:    val_rpn_class_loss   val_mrcnn_class_loss   val_mrcnn_bbox_loss   val_mrcnn_mask_loss     but the pesky val_rpn_bbox_loss is remaining stubbornly high and won't decrease.    What could be causing this?    Also, I've run the model with both image augmentation turned on as well as off, and this doesn't appear to affect the val_rpn_bbox_loss.      I have a screenshot of the losses in Tensorboard, but despite several attempts, GIthub won't let me upload an image to this issue."
"Is it possible that the default values in Config's BACKBONE_STRIDES ([4, 8, 16, 32, 64]) don't match the actual strides for the resnet50 and resnet101 backbones?    The first stage has two times (2, 2) strides, so that seems to be correct.  The second stage however has one conv_block (stride explicitly set to (1,1)) and two identity_blocks (they have always stride (1,1)). So, the actual stride would remain 4.    The real BACKBONE_STRIDES following that logic would be [4, 4, 8, 16, 32], right?    Do I miss something?"
"Hi all,     I'm trying to develop a tree detection algorithm from aerial imagery that incorporates RGB and a depth channel that represents tree height/geometry (called a canopy height model). I've tiled my input array to 1024 x 1024 x 4. I've successfully worked through the balloons sample on RGB imagery, and I've been using the WIKI to update the code in the necessary locations to support 4 channels. Item's 1 through 4 on the wiki seem pretty straight forward.    However,  I'm not clear on what I should specify for the weights for training. I've tried using the coco weights:    `python3 trees_rgbz.py train --dataset=/home/smorf/Mask_RCNN/datasets/forest/ --weights=coco  `    and it returns...    `Loading weights  /home/smorf/Mask_RCNN/mask_rcnn_coco.h5  Traceback (most recent call last):    File ""trees_rgbz.py"", line 365, in        model.load_weights(weights_path, by_name=True, exclude=[""conv1""])    File ""/home/smorf/Mask_RCNN/mrcnn/model.py"", line 2328, in load_weights      saving.load_weights_from_hdf5_group_by_name(f, layers)    File ""/home/smorf/.local/lib/python3.6/site-packages/keras/engine/saving.py"", line 1018, in load_weights_from_hdf5_group_by_name      str(weight_values[i].shape) + '.')  ValueError: Layer #389 (named ""mrcnn_bbox_fc""), weight   has shape (1024, 8), but the saved weight has shape (1024, 324).  `    I've tried updating model.load.weights() to exclude the layers it has issues with    ` model.load_weights(weights_path, by_name=True, exclude=[       ""conv1"", ""mrcnn_class_logits"", ""mrcnn_bbox_fc"",        ""mrcnn_bbox"", ""mrcnn_mask""])  `    And I get the error.....      `Traceback (most recent call last):    File ""/home/smorf/Mask_RCNN/mrcnn/model.py"", line 1968, in data_generator      batch_images[b] = mold_image(image.astype(np.float32), self.config)    File ""/home/smorf/Mask_RCNN/mrcnn/model.py"", line 3001, in mold_image      return images.astype(np.float32) - config.MEAN_PIXEL  ValueError: operands could not be broadcast together with shapes (1024,1024,3) (4,)`    I appreciate any help or suggestions.            "
"Hi, new here.  just want to know which version is the best for the project.  because Ive changed so many version from 1.0 to 2,0 and most of the time there is the bug for no module for **** or attribute error for TensorFlow.      thanks."
"Is there anybody who has made it? Please give a hint to me. I have trouble in the first step, where I should    translate this model to tensorflow(.pb) model(?)."
"I have run the demo.ipynb on jupyter notebook using my trained weights. I want to modify the inference output mask but after I modified the visualize.py, there is no changes in the output, why is that, and so it was with ohter .py documents in the mrcnn folder. My system is ubuntu16.04 and I use the firefox."
"I got this error when i ""pip3 install -r requirements.txt""  !   !     "
I train the model on 2 + 1 (BG) classes. I would like to infer one just one of the classes. How can I do it?
"if i convert h5 to pb,   when i use the pb,  i found the error   ""ValueError: Input 0 of node mrcnn_class_conv1/while/Conv2D/ReadVariableOp/Enter_1 was passed float from mrcnn_class_conv1/kernel_1:0 incompatible with expected resource.""    anyone else meet the error?  how can you solve it ? please give me a help    But, when i use the h5 file directly,  i found the infer time is too long,   about 3~4s,   i run the code in gpu P40, and image size is 1024"
"Hi all,   I am using Mask RCNN for skin segmentation in medical images. may dataset is composed of:  - training: 700 images + json annotations  - validation : 150 images + json annotations   My config file is..       the problem is that the mask is not completed on the boundaries as shown on this image:   ](url)  Is there any proposition how can I improve this segmentation?"
"At lines 1076 - 1109 of model.py:     . . .       I'm not sure to get it right. I see that you are using a variation of the standard cross-entropy loss function, but it doesn't seem to be the Focal Loss. Is there a reason for that? Could you explain in a couple of lines which loss function have you used?         .          Thanks for the nice work!"
"Need some help to figure out what this statement means    display_images(np.transpose(activations[""res2c_out""][0, : , : , : 4], [2, 0, 1]), cols=4)    Thank you"
"I just want to make sure I'm not messing anything up.    - Do I need to resize all of my images to the same WxH dimension before doing the annotations(I'm using label box)?   - If not, do I need to resize them before running them on M_RCNN (in the doc it says that it'll pad them w/zeros but I just want to double check as they are thousands of images).  - If I need to, should I turn them all into a square before doing the annotations? (As they are all rectangles).    thank you"
"i downloaded mask_rcnn_coco.h5 weights and installed pycocotools also and when i try to run demo.ipynb.  i am getting this error as AttributeError: module 'tensorflow' has no attribute 'log'  , please help i am still learning. I installed all the requoremnts.txt file also. please help.    ---------------------------------------------------------------------------  **AttributeError                            Traceback (most recent call last)    in          1 # Create model object in inference mode.  ----> 2 model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)        3         4 # Load weights trained on MS-COCO        5 model.load_weights(COCO_MODEL_PATH, by_name=True)**        this is the error im getting"
"Hello, I have been looking to find a way to reduce overfitting for my model. Let me tell you that my training losses are converging but my validation losses are overfitting a big time and I have a good dataset of 1100 images with some augmentations applied. Since my objects are really thin like cracks so I need to remove overfitting.  I know dropout can be helpful in reducing overfitting problem and I have looked at your previous model resources but found dropout only in fpn_classifier_graph as mentioned in above comments.    I am not sure will adding dropout to this will solve the issue or do I have to add it in the conv. block of ROI pooling. Similarly, do I have to add it in the RPN network and Resnet graph also? Any help would be really appreciated.  @waleedka "
"I am trying to run detection in validation dataset, I am getting the following error.  AssertionError                            Traceback (most recent call last)    in          7         8 # Run object detection  ----> 9 results = model.detect([image], verbose=1)       10        11 # Display results    ~\maskrcnn\mask-rcnn\mask_rcnn_damage_detection\mrcnn\model.py in detect(self, images, verbose)     2500         """"""     2501         assert self.mode == ""inference"", ""Create model in inference mode.""  -> 2502         assert len(     2503             images) == self.config.BATCH_SIZE, ""len(images) must be equal to BATCH_SIZE""     2504     AssertionError: len(images) must be equal to BATCH_SIZE    can somebody help me please?? I am a newbie."
"i have maked my own PASCAL VOC dataï¼Œi converted VOC into coco,but I didn't do the segmentation.can i train this Mask rcnnï¼Ÿ"
"Hello!  I'm starting to apply this training application to try and learn some machine learning, but I'm finding myself unable to get through the training process due to GPU memory constraints.  Whren running the Jupyter Notebook ""Mask_RCNN-master\samples\shapestrain_shapes.ipynb"" I'm getting the error message ""Resource exhausted: OOM when allocating tensor"", which, as I read, is related to my GPU running out of memory to process images.   I've already reduced the load on the GPU in the line IMAGES_PER_GPU = 8 to IMAGES_PER_GPU = 1. And, having 3 GB of video memory, it sometimes manages, but most of the time finds itself overloaded.  Is there a way for me to alleviate the memory use of the GPU further?  Thanks in advance!    EDIT: It's 3 GB, not 2."
"When I run setup.py on the Pycharm,  I see wanring and error.     Can you helo me? Thank you very much"
"As my knowledge, ROI Align crop then resize the feature map into a square which change the object aspect ratio. So I expected that the model can not classify between square and rectangle. Yet with doubt, I opened #1735     But after try testing, it turns out the model can classify 2 classes effectively. As you can see below    !     So my question is: How can it do that?"
"So I have successfully done my own version of the balloon sample. I have labeled my own images, rewritten the `BalloonDataset` and written a version of the `inspect_model` notebook. I have also used the library for _color splashing_ my own 20 second video. Everything works great!    Now I would like to take the color splash concept one step further, and render random letters/ text on the color splash /masks. My research indicates using `OpenCV` for this, but I am stuck. Here is a silly attempt to rewrite the `color_splash(image, mask)` function originally found in `balloon.py`:       How can I fill each mask with an opaque color and render letters on top?"
I am trying train the mask-rcnn using coco dataset:       But it gave me the follows error:       My keras version is 2.3.0.
"I have trained my COCO pretrained network on some images. Now i was testing the network. If i input the same image twice, i get different predictions. This should not be possible.  Anyone got an idea what could be the reason behind this problem.    I am looking forward to your answers"
"I need help. Please.  Even after long hours with 4 GPUs v100 does not start epoch ...  Does not display errors.  I tested with color images. Grayscale images ..  I am based on the balloon example.  I currently have 8 class. Plus the background.  I've done some successful training with just one class. Now with I am having this problem.  Any idea?  Thanks.  <img src="" "
"Hi, thank you for reading my issue!    I have a medical dataset consisting  of 1.4k thyroid ultrasound images, there is only 1 class â€” thyroid nodule. I have converted the dataset into coco format by using pycococreator, and set the number of class to 1+1( including background ) in config file and sample/coco/coco.py.  But after training and evaluating, the Average Precisions and Average Recall are 0.000.     I write this issue to ask:   How to modify the code to make the model suitable for detecting a new category instead of the 81 classes of coco."
Can you do fine tuning on mask r-cnn with just bounding boxes and no pixel-level segmentation? Would this produce decent results?
"Traceback (most recent call last):    File ""OOI.py"", line 36, in        import imgaug.augmenters    File ""C:\Users\HP\Anaconda3\envs\tf-gpu\lib\site-packages\imgaug\__init__.py"", line 6, in        from imgaug.imgaug import *    File ""C:\Users\HP\Anaconda3\envs\tf-gpu\lib\site-packages\imgaug\imgaug.py"", line 12, in        import cv2    File ""C:\Users\HP\AppData\Roaming\Python\Python37\site-packages\cv2\__init__.py"", line 3, in        from .cv2 import *  ModuleNotFoundError: No module named 'cv2.cv2'    when adding image augmentation in mask_rcnn it show the above menioned error. is there any possible ways to add  imgaug in mask_rcnn. "
"Tensorflow 2.0 has released yesterday . I try to change some API in Tensorflow 2.0 , but not well . When Mask_RCNN support Tensorflow 2.0 ?    The problem is     <img width=""813"" alt=""Screen Shot 2019-10-02 at 9 48 51 PM"" src=""       "
"Hi.  I'm trying to run demo.ipynb  But, I encounter following problem.    !     I searched documents, #632  and #1355 ,  which handle same issue.  I tried all suggested ways in the documents, but I still encounter the above problem.     !     I think ""imgaug"" has already been installed, but I don't know why the problem still happens.  Please help !!"
Is it possible to use same model for predicting the instances of single class ie person class?    Please help me
"Hi, I trained a Mask-RCNN using Matterport's Mask_RCNN, unfortunately it does not have an implementation of Mask-Scoring RCNN.   When training on Matterport's Mask-RCNN, I labeled my dataset using VIA Image annotator as done  . Can I somehow use the same annotations and convert them so that I can train the mmdetection's Mask-RCNN  ?  I labeled around 100 images and do not want them to annotate them again.     Any help will be highly appreciated. "
"Hi-    I can see the range of pixel values is (0,255). However, when I run MaskRCNN on 16 bit images, it works just fine but I cannot figure out where in the code this conversion takes place. Can you please point me to your conversion code?     Thanks!"
!     what is the solution to this problem
"Why ['masks'] from Mask_RCNN model are trimmed by bounding boxes? I can clearly see the straight lines on the masks, which come from bounding box lines. Is there any way to prevent this?"
"Hey guys!  I'm using Mask R CNN on a custom dataset, very simple: solid background and objects divided into 4 classes, more or less of the same size, in the same place in the image. The dataset is balanced, every class has exactly the same number of images in the train and in the test. However, regardless of any parameters, length of training, anything, it predicts me one class only.  I have tried to increase the weight for the class prediction - and it is dropping with every epoch, but the predictions remain the same - I always get the class 1 , even if in fact there is class 4 or 3.  Here is the configuration:     Maybe there is something wrong here, because in the dataset the number of classes is correct."
"Question is pretty much the same as the title:    a) What happens with model performance if you miss labels on some of the objects in an image?    b) And second, how do you determine if an object is too small? I havn't tested this, but I think model performance is worse if I annotate very small objects.    c) Finally, if missing annotations decrease model performance and annotating small objects also decrease model performance, how do you find a balance between the two?    "
Have anyone added mask IOU branch from the paper mask scoring rcnn?
"Hey Everyone,    So first up great work on this.    I have a perhaps somewhat basic question - I want to use Mask_RCNN to generate masks which I can then relabel, for example COCO detects a can as a bottle I want to output the mask from the can and then retrain with a new label as ""can""    I had a read on   (down the bottom).    in results['masks'] I get that each true/false relates to if an object is detected in that pixel, is the pixel relative to the ROI of the object or the whole image?    Could anyone point me in the direction of some reading or a way of extracting the mask that I can subsequently reuse for additional training please?    Thanks."
"Hello, I'm trying to reduce as much as possible the False Positives of my model. I would like to know if it may be useful to initially train the entire RPN + FPN model.    What ends up happening is that the RPN is a good enough object/non-object detector but instead in the FPN there is a great bias towards the positives, making it classify bad between object/background.    From what I have seen there is a flag called USE_RPN_ROIS, my idea would be to separate the training into two parts:  - RPN + FPN: Train both.  - FPN: Fix RPN (do not use). Only train FPN and pass the original bounding boxes along with others randomly generated to more accurately train the FPN.    My intuition tells me, make the following changes:  - USE_RPN_ROIS = False (config.py).  - random_rois = True, detection_targets = True in data_generator (model.py)    Would it be a good approach? Is it necessary to add to any more changes?    Thank you."
"Hello,    In the issue 281, there is a question about overcoming to overfitting. There was some suggestion to overcome to overfitting, however non of them related to overfitting. Does anybody know how can add dropout to the Mask R-CNN model?       Thank you for your help."
"Hi , i am using matterport mask rcnn to detect number of instances of a particular class in an image. But the result differs when i use different mean pixel values. Currently i have not found a stable set of values.     MEAN_PIXEL = (123.7, 116.8, 103.9)   MEAN_PIXEL = (43.53, 39.56, 48.22)    I have tried these two , but however the result differs with images. Any help would be greatly appreciated. "
"In the line 1943 of mrcnn/model.py,                 `for p in rpn_feature_maps:`  might be corrected as                `for p in mrcnn_feature_maps:`  as p6 should not be used in classifier heads."
"I am trying to train a model with the balloon example. I can get the training running on my local cpu, but when I attempt to run the `balloon.py` on my gpu (and actually also from the Jupyter Notebook) I get the error below. Everything seemed to install fine, bot requirements and the setup.py.     `python balloon.py train dataset=/home/Mask_RCNN/samples/balloon/balloon/ --weights=mask_rcnn_coco.h5`       `Traceback (most recent call last):    File ""/home/bitcue/Dropbox/mrcnn2/Mask_RCNN/samples/balloon/balloon.py"", line 367, in        train(model)    File ""/home/bitcue/Dropbox/mrcnn2/Mask_RCNN/samples/balloon/balloon.py"", line 201, in train      layers='heads')    File ""/home/bitcue/anaconda3/envs/mrcnn2/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 2354, in train    File ""/home/bitcue/anaconda3/envs/mrcnn2/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 2199, in compile  AttributeError: 'Model' object has no attribute 'metrics_tensors  `      I have the following packages installed in an anaconda environment:     Name                    Version                   Build  Channel  _libgcc_mutex             0.1                        main  _tflow_select             2.1.0                       gpu  absl-py                   0.7.1                    py36_0  astor                     0.8.0                    py36_0  backports                 1.0                        py_2  backports.weakref         1.0rc1                   py36_0  blas                      1.0                         mkl  bleach                    1.5.0                    py36_0  c-ares                    1.15.0            h7b6447c_1001  ca-certificates           2019.5.15                     1    anaconda  certifi                   2019.6.16                py36_1    anaconda  cudatoolkit               9.0                  h13b8566_0  cudnn                     7.6.0                 cuda9.0_0  cupti                     9.0.176                       0  expat                     2.2.6                he6710b0_0  gast                      0.2.2                    py36_0  git                       2.20.1          pl526hacde149_0  grpcio                    1.16.1           py36hf8bcb03_1  h5py                      2.9.0            py36h7918eee_0  hdf5                      1.10.4               hb1b8bf9_0  html5lib                  0.9999999                py36_0  intel-openmp              2019.5                      281  keras-applications        1.0.8                      py_0  keras-gpu                 2.0.8            py36h0585f72_0    anaconda  keras-preprocessing       1.1.0                      py_1  krb5                      1.16.1               h173b8e3_7  libcurl                   7.65.3               h20c2e04_0  libedit                   3.1.20181209         hc058e9b_0  libffi                    3.2.1                hd88cf55_4  libgcc                    7.2.0                h69d50b8_2  libgcc-ng                 9.1.0                hdf63c60_0  libgfortran-ng            7.3.0                hdf63c60_0  libprotobuf               3.8.0                hd408876_0  libssh2                   1.8.2                h1ba5d50_0  libstdcxx-ng              9.1.0                hdf63c60_0  markdown                  3.1.1                    py36_0  mkl                       2019.5                      281  mkl-service               2.3.0            py36he904b0f_0  mkl_fft                   1.0.14           py36ha843d7b_0  mkl_random                1.0.2            py36hd81dba3_0  mock                      3.0.5                    py36_0  ncurses                   6.1                  he6710b0_1  numpy                     1.16.5           py36h7e9f1db_0  numpy-base                1.16.5           py36hde5b4d6_0  openssl                   1.1.1                h7b6447c_0    anaconda  perl                      5.26.2               h14c3975_0  pip                       19.2.2                   py36_0  protobuf                  3.8.0            py36he6710b0_0  python                    3.6.9                h265db76_0  pyyaml                    5.1.2            py36h7b6447c_0  readline                  7.0                  h7b6447c_5  scipy                     1.3.1            py36h7c811a0_0  setuptools                41.0.1                   py36_0  six                       1.12.0                   py36_0  sqlite                    3.29.0               h7b6447c_0  tensorboard               1.13.1           py36hf484d3e_0  tensorflow                1.13.1          gpu_py36h26cf82e_0  tensorflow-base           1.13.1          gpu_py36h8f37b9b_0  tensorflow-estimator      1.13.0                     py_0  tensorflow-gpu            1.13.1               h0d30ee6_0  tensorflow-gpu-base       1.7.0            py36hcdda91b_1  tensorflow-tensorboard    1.5.1            py36hf484d3e_1  termcolor                 1.1.0                    py36_1  tk                        8.6.8                hbc83047_0  werkzeug                  0.15.5                     py_0  wheel                     0.33.4                   py36_0  xz                        5.2.4                h14c3975_4  yaml                      0.1.7                had09818_2  zlib                      1.2.11               h7b6447c_3  "
are the pretrained weights used for all 4 channels? or just the RGB channels?   How does it work for 1 channel images?     
"Here coco2017 script is just renamed coco.py script with COCO year changed to 2017.  `Traceback (most recent call last):    File ""train/coco2017.py"", line 503, in        evaluate_coco(model, dataset_val, coco, ""segm"", limit=int(args.limit))    File ""train/coco2017.py"", line 340, in evaluate_coco      r = model.detect([image], verbose=0)[0]    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 2911, in detect    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 2813, in mold_inputs  AttributeError: 'NoneType' object has no attribute 'shape'`"
"NOTE: It works when training is over as the GPU is free.    I want to run the inference on some test images while training is going on. Using the balloon sample here to run my inference. But, it tries to allocate memory on the GPU even though the following config is set.        # Device to load the neural network on.      # Useful if you're training a model on the same      # machine, in which case use CPU and leave the      # GPU for training.      DEVICE = ""/cpu:0""  # /cpu:0 or /gpu:0        2019-09-18 20:06:19.737835: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA  2019-09-18 20:06:19.770330: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2993940000 Hz  2019-09-18 20:06:19.773522: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56194e1fcff0 executing computations on platform Host. Devices:  2019-09-18 20:06:19.773552: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0):  ,    2019-09-18 20:06:19.775562: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1  2019-09-18 20:06:19.915737: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 11523260416  Aborted (core dumped)  "
"I'm working on a classification project. I'd to change the learning rate. I changed the learning rate in the config.py file, and Re ran the   `Python setup.py install`  The issue is that the PC forks many process, and eat up a huge chunk of memory, before training.    Troubleshooting for this step? on how to revert the changes? or to stop the forking so many process, or how to change learning rate (I know it sounds silly )    I'm using tensorflow-gpu v 1.14.0  cuda 10.0  cudnn 7.4    8gb of ram,  nvidia 1050ti  core I7 6700"
"The segmentation map delivered by the algorithm has overlapping segments as you can see in the below image, which is not desired. How to make sure that the segments don't overlap, and that each pixel belongs to only one class? There has to be an option for that, right?        !   "
"Hi, I am training this model with my own data- RGBD. But I am unable to apply image augmentation. Do the image augmentors works only on 3 channel images?"
"I am trying to detect wall from the image but how to label the wall which is cluttered with many objects like TV, wiring, photo frames, cables, showcase, window ....etc.  "
Does anyone know how to calculate a confusion matrix to check the prediction on each class for the whole 'val' dataset?
"I have trained Custom Mask RCNN on deepfashion2 dataset ,so I trained the model but when i am predicting on the testing images it gives me following Error    ValueError: Layer #391 (named ""mrcnn_bbox_fc""), weight   has shape (1024, 324), but the saved weight has shape (1024, 56).    While training i did like model.load_weights(weights_path, by_name=True, exclude=[""mrcnn_class_logits"", ""mrcnn_bbox_fc"", ""mrcnn_bbox"", ""mrcnn_mask""]).  But during testing, while loading the model if I exclude [""mrcnn_class_logits"", ""mrcnn_bbox_fc"", ""mrcnn_bbox"", ""mrcnn_mask""] not producing the correct results. But If I load the model without the exclude layers in testing it showing the same error."
"Hi,  I am unable to train model on tif files. model.train() fails giving error `cannot decompress ccittfax4` .      `image = skimage.io.imread(self.image_info[image_id]['path'])` in `load_image()` is where this fails.    From what I researched skimage uses tifffile.py to read tif files and this does not support ccittfax4 as decompressor. Reading the same file with opencv works fine, but does not solve my problem because mrcnn code uses skimage.io.imread()    Using plugin ='pil' in skimage.io.imread() solves the problem, but the mrcnn code does not use this flag    Any help?  Thanks,  Amit"
"Hi Mr,  when i run your traning source code with only person class. I already set GPU_COUNT to 2 but it did not run on GPU. It Just run on CPU. COuld you please help me fix it?"
@waleedka @moorage @rymalia   I want to augment only validation data. Would give any suggestion data please?    Thanks
"When i use bmp picture to inference,i encountered a problem that showed above.Can someone help me?"
Can the model classify between square and rectangle classes?
"I want to create a model which solve a multiclass classification problem.   The main concept is:   - every picture contain only one object  - the background is very simple   - all object is coming from the same object class (exampleÃ‰ knives, hats, shoes, etc.)  - te model will learn and predict the name of the object. (example: the model learn all type of knives, and when it get an image it will tell us the name of the knife)    To be clear her is an example, I have 50 types of knife, and the output of the model has to recognize the correct name of the knife. Knife name could be: Chef's Knife,Heavy Duty Utility Knife,Boning Knife, etc.    To solve this problem, I have started to use annotated, segmented (masked) images (Coocolike dataset) and MASK RCNN model.     As a first step, I got a prediction, but I really don't know if I'm on the right way.     My questions are:  - For this problem, Mask RCNN could be the solution, or it is impossible to recognize a tiny difference between two objects from the same class (example Chef's Knife, Heavy Duty Utility Knife)?  - Have you ever seen almost the same problem/solution, with github repo?    Thanks a lot  G      "
I tried to cut the license plate from images by this project.  the image size is about 1920x1080  around 8000 for training 1000 for validation 1000 for test  the   my training configuration is like:     the result is like this    the loss function by tensorboard    but the result is not very well since the license plates are not segmented rightly and too many weird false positive situation.  since the dataset can't be wrong and  so does this project   can anyone tell me how to tune the parameters?  
"  explains perfect how train the Mask_RCNN creating a new class to the model that has been trained with the COCO dataset.    Suppose we need to add the balloon class to the 80 (I think) categories that already the COCO contains, how should I start ? Is it something easy to-do ?    Duplicate of the   (which is still open).    Please CLOSE this issue"
       return 'results_dic[key][0]'  it showing error  as i am making dictionary and passing list and dictionary
"My images comes in a lot of different sizes and when I was going through the code I saw that there is a min and max image size.  It looks like the max size is 1000px and if anything is smaller than it, it would simply add a bunch of zeros to image (please correct me if I'm wrong). My images come anything from ~150px to 1000px.     - Is it more likely that I'll get better training out of of from bigger images? If so what would be your recommendation for the smallest image size?  - For the smaller images will it simply add zero to it or should I still resize all of the images to the same size?  - Should I strip colors from the images or will they help?  "
"In code   `keras.regularizers.l2(self.config.WEIGHT_DECAY)(w) / tf.cast(tf.size(w), tf.float32)`, why this should be divided by tf.size(w) ? "
"When I resize the image to (1033, 688) or (1031,688), I will get the correct result as below.  !   But  when I resize the image to (1032, 688), the truck disappeared.   !   Why?"
"Hello I'm trying to calculate the area in pixels of each mask.  I'm using this function:   `np.reshape(r['masks'], (-1, r['masks'].shape[-1])).astype(np.float32).sum()`    This calculates the full area (I guess).     How can I calculate each? "
"Greetings!    I'm working with an altered version of the balloon implementation, with 7 classes, and would like to know how to alter the code to display the same color between classes. Example: class 1 -> always red, class 2: always pink.....and so on.    I suppose I need to modify the visualize.py where the code generates random colors, but I'm kinda lost here.    Thanks in advance.    "
"I tried to re-start the training on my dataset from last epoch 20.  After I run these codes:    init_with = ""last""  # imagenet, coco, or last  if init_with == ""imagenet"":      model.load_weights(model.get_imagenet_weights(), by_name=True)  elif init_with == ""coco"":      # Load weights trained on MS COCO, but skip layers that      # are different due to the different number of classes      # See README for instructions to download the COCO weights      model.load_weights(COCO_MODEL_PATH, by_name=True,                         exclude=[""mrcnn_class_logits"", ""mrcnn_bbox_fc"",                                   ""mrcnn_bbox"", ""mrcnn_mask""])  elif init_with == ""last"":      # Load the last model you trained and continue training      model.load_weights(model.find_last(), by_name=True)  model.train(dataset_train, dataset_val,               learning_rate=config.LEARNING_RATE,               epochs=10,               layers='heads')     I got the following infos.    Starting at epoch 20. LR=0.001    Checkpoint Path: /content/drive/My Drive/bxy/Mask_RCNN-master/logs/regions20190831T1727/mask_rcnn_regions_{epoch:04d}.h5  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  In model:  rpn_model      rpn_conv_shared        (Conv2D)      rpn_class_raw          (Conv2D)      rpn_bbox_pred          (Conv2D)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)  /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  /usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.    UserWarning('Using a generator with `use_multiprocessing=True`'    After these infos, there should be something like ""  Epoch 1/10 ...  ""  if no error occured.    However,nothing appeared in the following.    What should I do for re-starting the training?  "
"When i test more than 200 pictures , it occurs :  File ""D:/Project/PartTimeProject/Work/hudson-duty-inspection-MaskRCNN/test.py"", line 115, in test1      results = model.detect([orig], verbose=1)    File ""D:\Project\PartTimeProject\Work\hudson-duty-inspection-MaskRCNN\mrcnn\model.py"", line 2505, in detect      molded_images, image_metas, windows = self.mold_inputs(images)    File ""D:\Project\PartTimeProject\Work\hudson-duty-inspection-MaskRCNN\mrcnn\model.py"", line 2401, in mold_inputs      mode=self.config.IMAGE_RESIZE_MODE)    File ""D:\Project\PartTimeProject\Work\hudson-duty-inspection-MaskRCNN\mrcnn\utils.py"", line 448, in resize_image      preserve_range=True)    File ""D:\Project\PartTimeProject\Work\hudson-duty-inspection-MaskRCNN\mrcnn\utils.py"", line 903, in resize      anti_aliasing_sigma=anti_aliasing_sigma)    File ""D:\InstallPath\Develop\Anaconda3-5.1.0\envs\partJob\lib\site-packages\skimage\transform\_warps.py"", line 179, in resize      preserve_range=preserve_range)    File ""D:\InstallPath\Develop\Anaconda3-5.1.0\envs\partJob\lib\site-packages\skimage\transform\_warps.py"", line 871, in warp      order=order, mode=mode, cval=cval))    File ""skimage\transform\_warps_cy.pyx"", line 122, in skimage.transform._warps_cy._warp_fast    File ""D:\InstallPath\Develop\Anaconda3-5.1.0\envs\partJob\lib\site-packages\numpy\core\numeric.py"", line 632, in ascontiguousarray      return array(a, dtype, copy=False, order='C', ndmin=1)  MemoryError  "
"in default configï¼šRPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  as I knowï¼š256 is C2&P2â€˜s size        128 is C3&P3â€˜s size        64 is C4&P4â€˜s size       32 is C5&P5â€˜s size  but 512 is belonging to which output?      "
"I am doing a training with multiple brands of bottles that are similar in shape and contour but not in color    Pepsi  Coke  Mountain dew    Those three brands recognize them correctly but if I pass another brand that is also a bottle but another color always assigns it to a wrong class    Fanta -> Mountain Dew  Orange -> Mountain Dew    How can I make the color feature have more weight in training?      **Configurations**    class BottleConfig(Config):      """"""      Configuration for training on the toy  dataset.      Derives from the base Config class and overrides some values.      """"""      # Give the configuration a recognizable name      NAME = ""bottle""        # We use a GPU with 12GB memory, which can fit two images.      # Adjust down if you use a smaller GPU.      IMAGES_PER_GPU = 4        # Number of classes (including background)      NUM_CLASSES = 1 + 2  # Background + bottles        # Number of training steps per epoch      STEPS_PER_EPOCH = 100        # Skip detections with < 90% confidence      DETECTION_MIN_CONFIDENCE = 0.9      **Dataset**    class BottleDataset(utils.Dataset):        def load_bottle(self, dataset_dir, subset):          """"""          Load a subset of the bottle dataset.          dataset_dir: Root directory of the dataset.          subset: Subset to load: train or val          """"""          # Add classes. We have only one class to add.          self.add_class(""bottle"", 1, ""mountain_dew"")          self.add_class(""bottle"", 2, ""pepsi"")            # Train or validation dataset?          assert subset in [""train"", ""val""]          dataset_dir = os.path.join(dataset_dir, subset)          annotations = json.load(open(os.path.join(dataset_dir, ""via_region_data.json"")))          annotations = list(annotations.values())  # don't need the dict keys            # The VIA tool saves images in the JSON even if they don't have any          # annotations. Skip unannotated images.          annotations = [a for a in annotations if a['regions']]                    # Add images          for a in annotations:              polygons = [r['shape_attributes'] for r in a['regions'].values()]              #names = [r['region_attributes'] for r in a['regions'].values()]              names  = [r['region_attributes']['name'] for r in a['regions'].values()]              class_ids = []              for i in names:                  print(i)                  if i == 'mountain_dew':                      class_ids.append(1)                  if i == 'pepsi':                      class_ids.append(2)                image_path = os.path.join(dataset_dir, a['filename'])              image = skimage.io.imread(image_path)              height, width = image.shape[:2]              print(class_ids)              self.add_image(                  ""bottle"",                  image_id=a['filename'],  # use file name as a unique image id                  path=image_path,                  width=width, height=height,                  polygons=polygons,                  class_ids=np.array(class_ids))                  #names=names                          def load_mask(self, image_id):          image_info = self.image_info[image_id]          if image_info[""source""] != ""bottle"":              return super(self.__class__, self).load_mask(image_id)            info = self.image_info[image_id]            mask = np.zeros([info[""height""], info[""width""], len(info[""polygons""])],                          dtype=np.uint8)          for i, p in enumerate(info[""polygons""]):              # Get indexes of pixels inside the polygon and set them to 1              rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])              mask[rr, cc, i] = 1            class_ids = info['class_ids']          return mask, class_ids"
None
"I'm trying to configure the balloon sample to work for overlapping masks. Given that most masks in the dataset are overlapping polygons are there in the dataset, do you think Mask RCNN will work?"
"Dear Mask_RCNN implementers and github members,    I would like to share my problem on using multi-gpu system.    I run the parallel_model.py as stated in the comment: `python3 parallel_model.py`    During its process, I opened the second terminal window and typed: `nvidia-smi`.    I saw none of my GPU's were used.    Here what I mean:  <img width=""1277"" alt=""Screen Shot 2019-08-30 at 04 08 20"" src=""     Do you have any suggestions for me?    Versions:  Tensorflow:  1.14.0  keras:           2.1.6  python:        3.6.4 (Anaconda)   "
"What would be the best way to assign each object an ID and track the objects through a video while maintaining the same IDs for each object? Basically, I want to be able to specify an object to mask out the background for. The resulting video would contain that object against a white background."
"@moorage @waleedka @PavlosMelissinos @rymalia   When I use,    > IMAGE_MIN_DIM=800  IMAGE_MAX_DIM=1024  and the testing image size is 800by800.    The Mask of the detected object was not good (not covered properly).    But when I use,    > MAGE_MIN_DIM=512  IMAGE_MAX_DIM=512  and the testing image size is 544by544    The Mask of the detected objects were better than before.  What is the problem? If I want to use the above configuration, How can I change parameter to solve this issue?    Thanks  "
Sometimes an image has only background without objects. What should  load_mask() return? One empty tensor or zero tensor?
I have a pre-trained model that has been trained on 81 classes. I would like to detect only the 'person' class. Currently to do this I am having to loop through detections and ignore those whose class_id is not 'person'. Is there any other way to do this?
"Hello,     I want to give location of specific box and get ROI output of that box. I don't know how to do that,      Thank you "
"Hello! I apologize for the vagueness of this question- but to put it simply: can anyone point to a source or guide how people upload non VGG annotated JSONs into MRCNN?     I cannot for the life of me deconstruct the examples to figure out how to upload my own jsons, and I would appreciate if anyone had any advice or knew any outlets for help in this matter    Thanks!"
Any one knows how Masl_pool_size and pool_size can help in optimising the model? I am having accuracy of 84% which is still not enough (at least 95%). During training I have overfilling only because of the mrcnn_mask_loss. Can someone help?  
"Hi, Everyone.  I have generated custom mask data using VGG Image Annotator(one object: person), then trained custom maskrcnn network using balloon example.  There is my train script.       Its been taking some long time to complete 30 epochs, so I tried to use middle module(epoch 10 or something).    There are my scripts to load and use middle module.       But I could not get any result via model.detect. Even if I got some results, but they are wrong result.  Someone please help me.  Thank you"
"**Does Nucleus config will work?** Which config will do better?       **My Data:**     !     **Currently My Config is:**        GPU_COUNT = 1      IMAGES_PER_GPU = 1        # Number of classes (including background)      NUM_CLASSES = 1 + 1  # background + 3 shapes        # Use small images for faster training. Set the limits of the small side      # the large side, and that determines the image shape.            IMAGE_MIN_DIM = 1024      IMAGE_MAX_DIM = 1024            POST_NMS_ROIS_TRAINING = 1000      POST_NMS_ROIS_INFERENCE = 2000             # Non-max suppression threshold to filter RPN proposals.      # You can increase this during training to generate more       #propsals.      RPN_NMS_THRESHOLD = 0.9        IMAGE_RESIZE_MODE = ""crop""      IMAGE_MIN_SCALE = 2.0            # How many anchors per image to use for RPN training      RPN_TRAIN_ANCHORS_PER_IMAGE = 320      MEAN_PIXEL = np.array([43.53, 39.56, 48.22])      USE_MINI_MASK = True      MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask      TRAIN_ROIS_PER_IMAGE = 128      MAX_GT_INSTANCES = 600      DETECTION_MAX_INSTANCES = 400        # Use smaller anchors because our image and objects are small      RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels        # Reduce training ROIs per image because the images are small and have      # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.        # Use a small epoch since the data is simple      STEPS_PER_EPOCH = 300        # use small validation steps since the epoch is small      VALIDATION_STEPS = 35      I used coco weights and trained the network for 300 epocs. I am getting mean AP as 0.65     **Please help from your valuable time.**"
"Dear all.  I used Mask-RCNN to train my models. For decreasing the training time, I used four GPU to train.  However, the training speed is very slow.   In addition, I have four GPU, the used memory of GPU is still low. No matter how I changed the  IMAGES_PER_GPU, every GPU only used 100+ MB memory.  !     I dont' know how to increase the training speed, and how to increase used memory of each GPU? Could you please help me?  Thanks a lot in advance.  "
"I've trained a model, with great results, as follows:     It trains the last layer (heads) starting from mask_rcnn_coco.h5.    Now I want to export my model to a .pb-file. The goal is to create a Tensorflow Lite file to be used in a mobile application.    ------------  ### I've tried using 'keras_to_tensorflow.py'       **But get the error**    `ValueError: Unknown layer: BatchNorm`    ### I've tried freeze_model       **But get this error**    `Attempting to use uninitialized value bn4m_branch2a/moving_mean`    ### I've set save_weights_only to False    `save_weights_only=False`    **But got this error:**  `TypeError: can't pickle _thread.RLock objects`    ### I've tried this suggestion:       **But got this error:**  `Attempting to use uninitialized value bn4m_branch2a/moving_mean`    Based on this 152-comments-strong thread   it seems like a lot of people have the same or similar issues without any clear answer.    1. Are there any other paths I could explore in order to save my model as a .pb-file?  2. How are other people using this model in production?  3. Are there other similar models out there where exporting is possible?      "
"Hello,    I am trying to use the pretrained coco weights with coco classes to run inference on one of my data, however I am not getting any output other than 0 from keras_model.predict. I tried inferencing on coco data and got the same result, no output to display.    I am using the same environment as mentioned in the requirements file, rather I am running it on the mentioned docker. Does someone faced the similar channel or knows how to debug it, please post it here. Thanks"
"I am trying to run MaskRCNN on an Nvidia Jetson TX1. It would be interesting to know what speedup could be achieved with TensorRT optimization. So I loaded the MaskRCNN class with the keras model, froze the (vanilla Resnet-) MaskRCNN graph from the session, saved the graph_def as a .pb file and optimized it with it with TensorRT. Now I have an optimized graph_def.pb, but I am unsure on how to use that graph definition within the MaskRCNN/Keras framwork for inference. Any hints?"
"I've been working with this repo for a while and I need to know how to train the images with 300   or more number of classes, FYI I have the image dataset and the number of classes needed.. Can anyone please help me out on this.     "
"Hello everyoneï¼Œ  I have problem when testing models trained on my own data, but many objects only have detection box without mask. Is my training time not enough, or the parameters are incorrect?"
"when I run the demo ï¼ŒI got this, my python is 3.6 version, numpy is 1.12.1  How to deal with it?"
"Hi,  Thank you for your awesome repository, it's helps me so much on my personal project :100: :+1:   I create this issue just want to share my code for serving model with Tensorflow Serving's gRPC. Hope this helps for anyone that get trouble when deploying model, especially Tensorflow Serving :smile:         Thank you!  Any contributors are welcome :star: :100:   For any questions, you can mention me on this github's issue or mail for me at: hoangphan0710@gmail.com :smile: "
!   I used the cocodata2014ï¼Œ2 GPUsï¼Œdo not change anyother parameters  
"Function `compute_matches` in untils.py, when we try to judge if the ground truth box has been matched, we should use  `if gt_match[j] > -1:` instead of 'if gt_match[j] > 0:'   "
"@waleedka @moorage @Cpruce   I trained MaskRCNN on my own dataset. During training, some values are look like this.    > Epoch 32/40  200/200 [==============================] - 132s 660ms/step - loss: 0.1315 - rpn_class_loss: 9.4215e-04 - rpn_bbox_loss: 0.0107 - mrcnn_class_loss: 0.0074 - mrcnn_bbox_loss: 0.0088 - mrcnn_mask_loss: 0.1037 - val_loss: 1.6259 - val_rpn_class_loss: 0.1048 - val_rpn_bbox_loss: 0.5142 - val_mrcnn_class_loss: 0.2798 - val_mrcnn_bbox_loss: 0.1932 - val_mrcnn_mask_loss: 0.5338  Epoch 33/40  200/200 [==============================] - 133s 663ms/step - loss: 0.1308 - rpn_class_loss: 0.0012 - rpn_bbox_loss: 0.0111 - mrcnn_class_loss: 0.0069 - mrcnn_bbox_loss: 0.0090 - mrcnn_mask_loss: 0.1026 - val_loss: 1.3137 - val_rpn_class_loss: 0.0856 - val_rpn_bbox_loss: 0.4055 - val_mrcnn_class_loss: 0.2032 - val_mrcnn_bbox_loss: 0.1625 - val_mrcnn_mask_loss: 0.4570  Epoch 34/40  200/200 [==============================] - 133s 664ms/step - loss: 0.1240 - rpn_class_loss: 7.8069e-04 - rpn_bbox_loss: 0.0093 - mrcnn_class_loss: 0.0079 - mrcnn_bbox_loss: 0.0071 - mrcnn_mask_loss: 0.0989 - val_loss: 1.4614 - val_rpn_class_loss: 0.0831 - val_rpn_bbox_loss: 0.3221 - val_mrcnn_class_loss: 0.2878 - val_mrcnn_bbox_loss: 0.2030 - val_mrcnn_mask_loss: 0.5654        Why val_loss is greater than 1?  Which value will be better? How can I reduce?  Any suggestion please.  Thanks"
"when I use the image size 1024X800 to train, batch_size = 1, after about 3000 steps,program stopped unexpectedly and raise information:""tensorflow/core/common_runtime/bfc_allocator.cc:380] Check failed: h != kInvalidChunkHandle "",but when i observe memory usage on GTX1070TI,it occupies about 5400M of video memory,there is about 2000M free memory.I try to change the image size to 128X64,it occupies more memory,but the program does not stop unexpectedly.This phenomenon is very strangeã€‚"
I wanna replace 'mask_rcnn_coco.h5' I usually use with a better pre-trained model by using the Object365 dataset and hope somebody can give me some suggestions.Thanks!  
"When I use mask rcnn, I get a bad performance on masks. The mask doesn't match well with the edge of bbox. The predicted bbox coverd the object in the image, but the pred_mask can,t cover the object. What should I do? To optimize the code of mask generator? The image in dataset has a size of 1024*1280 with a large object. Thank you very much"
"When I train the network, rpn_bbox_loss is too high. It is difficult to reduce. Is there any good advice? Many thanks"
"Hello everybody,  I get an error when I want to run demo.py.    C:\Users\Tom\Anaconda3\lib\site-packages\mask_rcnn-2.1-py3.7.egg\mrcnn\visualize.py:167: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.    Does anybody know how to solve that issue?      (base) C:\Users\Tom\Downloads\Mask_RCNN-master\samples>python demo.py  Using TensorFlow backend.    Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.7  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 1  IMAGE_CHANNEL_COUNT            3  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                93  IMAGE_MIN_DIM                  800  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           coco  NUM_CLASSES                    81  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  PRE_NMS_LIMIT                  6000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                1000  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001      WARNING: Logging before flag parsing goes to stderr.  W0812 20:14:34.686454 16204 deprecation_wrapper.py:119] From C:\Users\Tom\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.    W0812 20:14:34.703469 16204 deprecation_wrapper.py:119] From C:\Users\Tom\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.    W0812 20:14:34.705471 16204 deprecation_wrapper.py:119] From C:\Users\Tom\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.    W0812 20:14:34.721485 16204 deprecation_wrapper.py:119] From C:\Users\Tom\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:1919: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.    W0812 20:14:34.723488 16204 deprecation_wrapper.py:119] From C:\Users\Tom\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.    W0812 20:14:36.566161 16204 deprecation_wrapper.py:119] From C:\Users\Tom\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.    W0812 20:14:36.868444 16204 deprecation_wrapper.py:119] From C:\Users\Tom\Anaconda3\lib\site-packages\mask_rcnn-2.1-py3.7.egg\mrcnn\model.py:341: The name tf.log is deprecated. Please use tf.math.log instead.    W0812 20:14:36.874443 16204 deprecation.py:323] From C:\Users\Tom\Anaconda3\lib\site-packages\mask_rcnn-2.1-py3.7.egg\mrcnn\model.py:399: add_dispatch_support. .wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.  Instructions for updating:  Use tf.where in 2.0, which has the same broadcast rule as np.where  W0812 20:14:36.878452 16204 deprecation.py:506] From C:\Users\Tom\Anaconda3\lib\site-packages\mask_rcnn-2.1-py3.7.egg\mrcnn\model.py:423: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.  Instructions for updating:  box_ind is deprecated, use box_indices instead  W0812 20:14:37.043595 16204 deprecation_wrapper.py:119] From C:\Users\Tom\Anaconda3\lib\site-packages\mask_rcnn-2.1-py3.7.egg\mrcnn\model.py:720: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.    W0812 20:14:37.046597 16204 deprecation_wrapper.py:119] From C:\Users\Tom\Anaconda3\lib\site-packages\mask_rcnn-2.1-py3.7.egg\mrcnn\model.py:722: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.    W0812 20:14:37.108654 16204 deprecation.py:323] From C:\Users\Tom\Anaconda3\lib\site-packages\mask_rcnn-2.1-py3.7.egg\mrcnn\model.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.  Instructions for updating:  Use `tf.cast` instead.  2019-08-12 20:14:38.324432: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2  2019-08-12 20:14:38.333807: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll  2019-08-12 20:14:38.459684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:  name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7845  pciBusID: 0000:41:00.0  2019-08-12 20:14:38.466616: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.  2019-08-12 20:14:38.473152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0  2019-08-12 20:14:39.144248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:  2019-08-12 20:14:39.148883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0  2019-08-12 20:14:39.151701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N  2019-08-12 20:14:39.156002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4712 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:41:00.0, compute capability: 6.1)  Processing 1 images  image                    shape: (511, 640, 3)         min:    0.00000  max:  255.00000  uint8  molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  150.10000  float64  image_metas              shape: (1, 93)               min:    0.00000  max: 1024.00000  float64  anchors                  shape: (1, 261888, 4)        min:   -0.35390  max:    1.29134  float32  C:\Users\Tom\Anaconda3\lib\site-packages\mask_rcnn-2.1-py3.7.egg\mrcnn\visualize.py:167: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure."
"Hello everyone,    I am training network heads on a subset of the coco dataset (images where `supercategory=""animal""`) but I have always a nan loss for all types of losses (except the `mrcnn_class_loss`, which is constant ..).    **NB:** I changed the `NUM_CLASSES` in my own config class and set it to the number of classes that want to detect.    Here is what I have :   `Epoch 6/50  100/100 [==============================] - 340s 3s/step - loss: nan - rpn_class_loss: nan - rpn_bbox_loss: nan - mrcnn_class_loss: 2.3979 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: nan - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 2.3979 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 7/50  100/100 [==============================] - 275s 3s/step - loss: nan - rpn_class_loss: nan - rpn_bbox_loss: nan - mrcnn_class_loss: 2.3979 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: nan - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 2.3979 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 8/50  100/100 [==============================] - 278s 3s/step - loss: nan - rpn_class_loss: nan - rpn_bbox_loss: nan - mrcnn_class_loss: 2.3979 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: nan - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 2.3979 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 9/50  100/100 [==============================] - 273s 3s/step - loss: nan - rpn_class_loss: nan - rpn_bbox_loss: nan - mrcnn_class_loss: 2.3979 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: nan - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 2.3979 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 10/50  100/100 [==============================] - 272s 3s/step - loss: nan - rpn_class_loss: nan - rpn_bbox_loss: nan - mrcnn_class_loss: 2.3979 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: nan - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 2.3979 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 11/50  100/100 [==============================] - 274s 3s/step - loss: nan - rpn_class_loss: nan - rpn_bbox_loss: nan - mrcnn_class_loss: 2.3979 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: nan - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 2.3979 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 12/50  100/100 [==============================] - 273s 3s/step - loss: nan - rpn_class_loss: nan - rpn_bbox_loss: nan - mrcnn_class_loss: 2.3979 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: nan - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 2.3979 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 13/50  100/100 [==============================] - 273s 3s/step - loss: nan - rpn_class_loss: nan - rpn_bbox_loss: nan - mrcnn_class_loss: 2.3979 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: nan - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 2.3979 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00  Epoch 14/50  100/100 [==============================] - 272s 3s/step - loss: nan - rpn_class_loss: nan - rpn_bbox_loss: nan - mrcnn_class_loss: 2.3979 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: nan - val_rpn_bbox_loss: nan - val_mrcnn_class_loss: 2.3979 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00`"
Are there any ongoing works or plans on releasing ResNet pretrained Facebook LVIS weights? It includes 10 times more classes then COCO with much better segmentation masks.    URL for LVIS:  
I want to train my model with 4 category and all of the category should comes under one super category. Like bike and car comes under vehicles.  
"**Current Version Stacks:**    python 3.5    tensorflow                    1.14.0                  tensorflow-estimator          1.14.0                  tensorflow-gpu                1.14.0                  tensorflow-tensorboard        0.1.8     protobuf                      3.9.1    Keras                         2.2.4                   Keras-Applications            1.0.7                   Keras-Preprocessing           1.0.9    **Drivers & Machine:**    Ubuntu 16.04 , NVIDIA-SMI 396.54 , CUDA - 9.0 , CuDnn 7 , libcudnn - 7.4.1    **Summary-**    Not able to import abs from tensorflow.python.keras._impl.keras.backend on tensorflow-gpu == 1.14.0    And Not able to get CheckpointableBase attribute on tensorflow-gpu == 1.8.0    Which tensorflow-gpu and keras version supports both CheckpointableBase attribute and 'abs' from tensorflow.python.keras._impl.keras.backend ?    **Details -**    I'm currently training matterport mask rcnn with Keras==2.2.4. According to some threads in order to get abs I have tried updating tensorflow-gpu to 1.8.0 , updating protobuf. Although I was then able to import abs, but end up reciveing "" AttributeError: module 'tensorflow.python.training.checkpointable' has no attribute 'CheckpointableBase' "" on tensorflow-gpu to 1.8.0. Then in order to get CheckpointableBase some threads pointed to again update tensorflow-gpu == 1.14.0.    **Traceback :**       "
"Pip version: 19.2.1  Python version: Python 3.7.4  Operating system: windows 10    I'm getting this error message:  Collecting imgaug    Using cached    Requirement already satisfied: scipy in c:\users\nourelhouda\appdata\roaming\python\python37\site-packages (from imgaug) (1.3.0)  Collecting Shapely (from imgaug)    Using cached          ERROR: Command errored out with exit status 1:       command: 'C:\Python\Python37\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\Nourelhouda\\AppData\\Local\\Temp\\pycharm-packaging\\Shapely\\setup.py'""'""'; __file__='""'""'C:\\Users\\Nourelhouda\\AppData\\Local\\Temp\\pycharm-packaging\\Shapely\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base pip-egg-info           cwd: C:\Users\Nourelhouda\AppData\Local\Temp\pycharm-packaging\Shapely\      Complete output (9 lines):      Traceback (most recent call last):        File "" "", line 1, in          File ""C:\Users\Nourelhouda\AppData\Local\Temp\pycharm-packaging\Shapely\setup.py"", line 80, in            from shapely._buildcfg import geos_version_string, geos_version, \        File ""C:\Users\Nourelhouda\AppData\Local\Temp\pycharm-packaging\Shapely\shapely\_buildcfg.py"", line 200, in            lgeos = CDLL(""geos_c.dll"")        File ""C:\Python\Python37\lib\ctypes\__init__.py"", line 364, in __init__          self._handle = _dlopen(self._name, mode)      OSError: [WinError 126] Le module spÃ©cifiÃ© est introuvable      ----------------------------------------  ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.  "
"I have gtx1070 video card and i installed everything using requirements.txt, however it is very slow   when i train the shapes sample. it takes almost a half minute per image.  So i guess it is using CPU. then how it is using CPU when i installed everything using requirements.txt? So i checked the text file but there was no character ""-gpu"" in the text file. then i reinstalled using ""pip install tensorflow-gpu"". But getting errors related to versions that they dont match to each other. So im totally stuck.     1. Why is it so slow?  2. is it really using CPU? (coz mostly cmd shows how much memory we have when train starts but not showing anything)  3. how to reinstall or correct installations back? should i format my pc or any simple way?  help"
"I try to evaluate my model with intersection over union. Therefore I wanted to ask if someone already wrote a working IoU so I don't have to make it by myself.   Otherwise, are there some strategies I can try or some related works to check?"
"when i use multiprocessing.Pool to predict my own images, the process hang up on saving.load_weights_from_hdf5_group_by_name, what is this issue, anyone help me?"
"Hi, I am trying to run the inspect_model.py notebook on a custom dataset, and am running into an AssertionError with block 1b. RPN Predictions. I have seen previous threads related to this issue (#23, #848, #893, #92) however these are all closed and it seems the issue was fixed in a merge previous to me downloading the code.     The specific code causing the issue is:         Which gives me an output of:         Any help is greatly appriciated!"
"!   !     I am using this for sound effects font(sfx) in train japanese comics(manga). i am using this sfx images   but for example, like Images inserted in posts, are they valid for mask rcnn training?   (all sfxs style are not like only that.)  "
"While running through the step-by-stepinspect_balloon_model.ipynb, I got the error: ModuleNotFoundError: No module named 'samples' when I run: from samples.balloon import balloon.  I tried the solution   but it didn't work.  Can anyone tell me how to fix it?   Thank you."
None
"I don't really know where to start. I use VIA annotator where you can add more attributes, however, is it possible to use several attributes instead of just a single label for training and inference?"
If anybody successfully converted this mask-rcnn model to tensorRT model?
"Hello,    I am trying to display on the final output just the bounding box coming out from the RPN, in order to only locate the object and speed up the algorithm. I am not interested in classification so far.    Is the code prepared to do so in a fast and efficient way? I am searching but I do not get how I could do it, even commenting the lines which perform classification. Can you help me with this?     Thank you!"
"Hello everyone,  I have a problem when i use Mask R CNN with many instance in same image.  In fact, I Have two ( or more ) identification ( BBox+Mask+Score) for same instance.  But if I correctly understand, Mask R CNN use softmax function, so the sum of all score for each class is 1, but in my case, all score are superior at 0.7 ( RPN_NMS_THRESHOLD)  You can see the image result prediction :   !   Here you can see score at the last column:  !   Thanks for you'r reply, have a nice day guys ;)"
The backbone used in this implementation is ResNet or ResNet + FPN?    Thank you
"I'm running train_shapes.ipynb in a anaconda environment.   My GPU is Titan RTX  Ubuntu18.04  cuda10.0  cdunn7.6.0  Tensorflow-gpu tf.__version__==1.3.0  keras 2.0.8  Mask R-CNN 2.1       As described in train_shapes.ipynb:  'On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour'.   And the original output shows:   Epoch 1/1  100/100 [==============================] - 73s - loss: 2.2164 - rpn_class_loss: 0.0242 - rpn_bbox_loss: 1.0638 - mrcnn_class_loss: 0.2426 - mrcnn_bbox_loss: 0.3006 - mrcnn_mask_loss: 0.2385 - val_loss: 1.8454 - val_rpn_class_loss: 0.0232 - val_rpn_bbox_loss: 0.9971 - val_mrcnn_class_loss: 0.1398 - val_mrcnn_bbox_loss: 0.1343 - val_mrcnn_mask_loss: 0.2042    But on my machine it takes more than an hour to run this file with defaulted parameters.  Dose this performance relates to the poor FP64 performance of Titan RTX or some reason else?"
">>> import imgaug  Traceback (most recent call last):    File "" "", line 1, in      File ""C:\ProgramFiles\Anaconda2\lib\site-packages\imgaug\__init__.py"", line 6, in        from imgaug.imgaug import *    File ""C:\ProgramFiles\Anaconda2\lib\site-packages\imgaug\imgaug.py"", line 12, in        import cv2    File ""C:\ProgramFiles\Anaconda2\lib\site-packages\cv2\__init__.py"", line 3, in        from .cv2 import *  ImportError: DLL load failed: %1 is not a valid Win32 application."
"According to  , Mask R-CNN need calibration. Any idea how to do that?"
"I found out how to calculate the map, iou part in the utils.py code, but I don't know how to call these functions during training and print them out. Can someone help me? Many thanks."
"Hi, when i run demo.py to predict my own images, 8 Threads was opened, and cpu utilization too high, why  this stuation appeared? oh, i have no gpu"
"My environment is          When I am trying to run the training demo on coco dataset, I met the following problem       I am sure that Tensorflow has utilized the GPU sources and load the dynamic library correctly because I found it failed to do so for the Cuda version.    I searched for the error and it shows that I have to do the   `sudo rm -rf ~/.nv/ `, but it still does not work.    I also found the solution to set  `config.gpu_options.allow_growth = True`, but I do not know how to add it to the code.    Could you please tell me?    Thanks!"
I wonder what will happen when resnet101 is initialized by resnet50 weights. I'm not quite familiar with Keras. So please anyone can help me? I have seen two very similar comments in Isssues but not yet got answerd.         
"three loss : rpn_bbox_loss and mrcnn_bbox_loss and mrcnn_mask_loss   i train the model for my dataset, my train.py was modified from shapes,and i redefined     RPN_ANCHOR_SCALES = (2 * 6, 4 * 6, 8 * 6, 16 * 6, 32 * 6),  RPN_ANCHOR_RATIOS =    !     "
"My task is trying to separate two plants when it's overlapping.     !     However, I have a concern should I use MaskRCNN or not. The reason is because my generated mask is not connected. I attached an instance mask for explaining.   !     The above mask is very slim and is not connected at some place. The mask is very different from what I trained before. Need advice. Thanks.   "
I am trying to get inference on multiple images in parallel on a single GPU - to increase the speed. Can someone please guide me on how to that?
"Hi there,    I am not sure how best set up STEPS_PER_EPOCH for training my own dataset of 24k images.     I have set  GPU_COUNT = 1  IMAGES_PER_GPU = 1    So, if I set STEPS_PER_EPOCH=100, for example, does that mean only the first 100 images of my dataset is used for training? So the whole potential of my 24k images would not really be used?    Thanks for help :-)"
"Hello everyone,  All my data is gray scale image, I trained on them without getting any error. But when I tried to detect on gray scale image, I got the error `ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (3,2) and requested shape (2,2)`    Question 1: is it really possible to train on gray scale images without adapting some parameters, just like training rgb image?  Question 2: the ValueError I got could be solved somehow, if I convert the gray scale image to rgb image? I went through the discussion on the issues, but didn't quite get it....    I also have some rgb images for the same objects, when I tested on rgb images, the outcome was also not that good.  My guesses are: 1. Overfitting  2. it didn't learn the object from gray scale images at all. 3. Just need a larger dataset (so far I've trained on around 350 images, with transfer learning weights=coco)    Thanks for reading such a long question,  I'm looking forward to more discussion below. :)   Thanks in advance."
"I want to make a program that gives true or false if something exists in the photo, ie, if there is an apple in the image return 1. I wasn't sure how to do this given the format of Mask RCNN since there isn't an index of all the objects found in the image. Does anyone have an idea how to do this? "
"@waleedka @moorage   I want to save full model, not only weight.     Would you help me please. I search on the internet, also read some issues in MaskRCNN. But nothing help me properly.  Would you help me please?  "
"I'm following the guide on transfer learning, but I do not intend to input image masks into the network for training. Is there any way to specify this?"
"- keras: 2.2.4  - python: 3.5.2  - tensorflow: 1.12.0    When I train the model with muti-gpu, I meet the `No device assignments were active during op 'anchors/Variable' creation.` problem:       P.S1: I have modified my `parallel.py` by #1412 to overcome the problem `It looks like you are subclassing `Model` and you forgot to call `super(YourClass, self).__init__()`. Always start with this line.`.    P.S2: I have modifed my `modely.py` to solve the problem `TypeError: List of Tensors when single Tensor expected`.  > Fixed it by changing line 1913 in model.py  > form:  > `anchors = KL.Lambda(lambda x: tf.constant(anchors), name=""anchors"")(input_image)`  > to:  > `anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=""anchors"")(input_image)`      "
thanks
"Hello everyone,   I want to know how to train RGB-D images using Mask RCNN? How do I put my own RGB-D images into the network for training? What parameters need to be modified before it can be achieved? I look forward to receiving your reply, thank you."
"I have written a new sample (happy to share it) focused on detection and classification of forest mushrooms using the 2018 FGCVx Fungi Classification Challenge dataset.    I'd like to load the code to Google AI platform, unfortunately the current code is full of references to os and shutil which make this impractical.    Is there any plan to either switch to TF.FileIO or move to TFRecord all together?    "
"I want to use PYQT to show the result, but I have some problem, can anyone tell me how to solve it?? I think it's not wrong with data format, I had run without PYQT.    !   "
"I have converted the .h5 file  to  .pb file   using         and converted .pb file to .pbtxt file using         after then I have a .pb file and .pbtxt file yes, so I used your blog post        I have used above generated .pb and .pbtxt to produce masked result image so I am getting the error like this :    python3 mask_rcnn.py --mask-rcnn mask-rcnn-coco --image /home/deepedge/mask_rcnn-master/mask-rcnn-bottle-training-master/dataset/val/car39.jpg    [INFO] loading Mask R-CNN from disk...  [libprotobuf ERROR /io/opencv/3rdparty/protobuf/src/google/protobuf/text_format.cc:288] Error parsing text-format opencv_tensorflow.GraphDef: 60540:5: Unknown enumeration value of ""DT_RESOURCE"" for field ""type"".  Traceback (most recent call last):  File ""mask_rcnn.py"", line 48, in  net = cv2.dnn.readNetFromTensorflow(weightsPath, configPath)  cv2.error: OpenCV(4.1.0) /io/opencv/modules/dnn/src/tensorflow/tf_io.cpp:54: error: (-2:Unspecified error) FAILED: ReadProtoFromTextFile(param_file, param). Failed to parse GraphDef file: mask-rcnn-coco/dent.pbtxt in function 'ReadTFNetParamsFromTextFileOrDie'      How to solve?     help    @moorage @waleedka @PavlosMelissinos @haeric @rymalia @mowoe @akTwelve @ameetpatel @yanfengliu @dkrut       "
@waleedka @moorage @rymalia     I would like to  convert from .h5 to .pb file and want to apply freeze_graph and optimization. In this case I have to know output_node_names and input_node_names (it maybe also input and output node).  How can I get? Is it possible to get the name from .h5 file?    Thanks
"hi everyone:  I want to train and detect by gray image  I find i change  the IMAGE_CHANNEL_COUNT = 1 and exclude conv1  but i also get this error as following:  ValueError: Error when checking input: expected input_image to have shape (None, None, 1) but got array with shape (384, 384, 3)    and I try to set IMAGE_CHANNEL_COUNT = 3 ,then training by gray image is success,but when I detect the gray image ,I get this error as following:  operands could not be broadcast together with remapped shapes [original->remapped]: (3,2) and requested shape (2,2)    I know this problem is because gray image dim is 2,so resize image is fail  but I don't know how to solve.    who can teach me how to train and detect gray image,thank        "
"python3  mask_rcnn.py --mask-rcnn mask-rcnn-coco --image /home/deepedge/mask_rcnn-master/mask-rcnn-bottle-training-master/dataset/val/car39.jpg      [INFO] loading Mask R-CNN from disk...  [libprotobuf ERROR /io/opencv/3rdparty/protobuf/src/google/protobuf/text_format.cc:288] Error parsing text-format opencv_tensorflow.GraphDef: 60540:5: Unknown enumeration value of ""DT_RESOURCE"" for field ""type"".  Traceback (most recent call last):    File ""mask_rcnn.py"", line 48, in        net = cv2.dnn.readNetFromTensorflow(weightsPath, configPath)  cv2.error: OpenCV(4.1.0) /io/opencv/modules/dnn/src/tensorflow/tf_io.cpp:54: error: (-2:Unspecified error) FAILED: ReadProtoFromTextFile(param_file, param). Failed to parse GraphDef file: mask-rcnn-coco/dent.pbtxt in function 'ReadTFNetParamsFromTextFileOrDie'    tried to implement mask rcnn with my own generated .pb and .pbtxt same below then error ours     How to solve ?     "
"Hello,   I have a task of image segmentation using mask rcnn and I found a problem to train the model using my own images which contains only one class, I have clinical images of the body that contains a part of a person body. I am trying to create the BodyDataset class following the CocoDataset class example. is it the right way to train mask rcnn on my own dataset. do I need annotations of my images?  "
I have a frozen graph .pb file . and I have to show splash result image as a result.    @waleedka @moorage @robaleman @rymalia @kientruccapa
I have successfully generated a .pb model file from #1126. Can someone please show me how I can use this to run object detentions on images/test?     My end goal is to setup an Android/iOS app which can be passed an updated model file and run detections on cloud. Any help would be greatly appreciated!
"In model.py line 563-571     Tensor positive_overlaps could have a shape of [positive_count, MAX_GT_INSTANCES], which makes the tf condition always be true. This might be subtle as RPN might hardly produce proposals matching no ground truth."
"I am using this model to do the license plate recognition by 60000 fake license plates.  all the characters are counted for around 8000, the image size is 200x 80 with shearing and blur to make them look real.  .the training and validation losses converged peacefully after all, but the results on the test dataset are not very good. although most of the time it can recognize the character rightly with overall 0.99 confidence level. but if there are licenses like ""BC-1334""  it will segment ""33"" as an object only even for the confidence level is 0.1 only. does anyone know why? I tried to modify the anchor ratios but it is not working.  the second problem is that it still can't recognize the small size(80 x40) license plate even I   already  use imaug affine function to resize to 0.25"
"Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.  @"
Hello from Russia!!!! I'm trying to make a branching Color Splash Example â€“ I want to add a measurement of the diameter ballons. But I can't handle with the installation of Color Splash Example as written in the   I can't download the file mask_rcnn_ballon.h5 weighing 244 MB. Please tell me how to upload a file mask_cnn_ballon.h5 weighing 244 MB to the root of the repo? Thanks in advance!   
"Hi, first thanks to all contributors for this project ! :)  So I use Mask-R-CNN to detect different species of three in aerial photography.  For each three I want to show all classes prediction scores.  For example I have two classes (A and B), I want to see prediction scores for A AND B for each instance in my pictures.  I have searched if someone have do this, but I found nothing...  If someone known how to do this, I am interested :D  Thanks for you reply ! :)"
"Hi,  Is the JSON format used by mark_rcnn same as coco json.  if not can you please tell me more about the format ...."
"Hi,  What is the best tool we can use for Annotation which cam be used here?  Is the JSON format customized (or) its standard? are there any tools.  I see tools generating only XML...  can you please help me"
"Hi, I have use MaskRCNN to train my own dataset with tensorflow 1.4.0, but when I use multi-gpus to train, the program will be broken down. So I upgrade my tensorflow to version 1.7.0, the problem is solved.   But the model which was trained with my dataset has a very bad performance, the segment mask is smaller than the object region. And when I use tensorflow 1.4.0 to train my dataset, the performance is very good. And the training code is the same. Is anyone met this problem? Why does this situation happen?    Thanks very much!!"
"    i am getting results like this, but how to display the result of image?    [{'rois': array([[244, 287, 590, 721],  [216, 737, 426, 925]], dtype=int32), 'class': array([1, 1], dtype=int32), 'scores': array([0.9834276 , 0.76979005], dtype=float32), 'mask': array([[[False, False],  [False, False],  [False, False],  ...,  [False, False],  [False, False],  [False, False]],       [[False, False],      [False, False],      [False, False],      ...,      [False, False],      [False, False],      [False, False]],       [[False, False],      [False, False],      [False, False],      ...,      [False, False],      [False, False],      [False, False]],       ...,       [[False, False],      [False, False],      [False, False],      ...,      [False, False],      [False, False],      [False, False]],       [[False, False],      [False, False],      [False, False],      ...,      [False, False],      [False, False],      [False, False]],       [[False, False],      [False, False],      [False, False],      ...,      [False, False],      [False, False],      [False, False]]])}]      "
"Hi guys,    If I am not mistaken the mAP that is  reported is only for the validation set.    I want to use a new test set with unseen for the Net images, but I want to report the mAP for it from the ground truth that I already have in a catalogue.    How can I do that?"
"from forward import ForwardModel  import cv2  import numpy as np    frozen_graph_path = ""/home/deepedge/mask_rcnn-master/mask_dent_frozen_graph.pb""  assign your own Config class    my_config = ""/home/deepedge/mask_rcnn-master/mrcnn/config.py""    forward_model = ForwardModel(frozen_graph_path, my_config)    def test_one_image(image):  images = np.expand_dims(image, axis=0)  results = forward_model(images)  return results    if name == ""main"":  test_image_path = '/home/deepedge/mask_rcnn-master/mask-rcnn-bottle-training-master/dataset/val/car39.jpg'  image = cv2.imread(test_image_path)  r = test_one_image(image)  print(r)      error    $ python3 anomaly.py    2019-07-01 11:02:29.101152: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA  2019-07-01 11:02:29.120748: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3600000000 Hz  2019-07-01 11:02:29.121268: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0xe58be0 executing computations on platform Host. Devices:  2019-07-01 11:02:29.121280: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): ,  Traceback (most recent call last):  File ""anomaly.py"", line 20, in  r = test_one_image(image)  File ""anomaly.py"", line 14, in test_one_image  results = forward_model(images)  File ""/home/deepedge/mask_rcnn-master/mrcnn_serving_ready-master/forward.py"", line 218, in call  molded_images, image_metas, windows = self.mold_inputs(images)  File ""/home/deepedge/mask_rcnn-master/mrcnn_serving_ready-master/forward.py"", line 93, in mold_inputs  min_dim=self.config.IMAGE_MIN_DIM,  AttributeError: 'str' object has no attribute 'IMAGE_MIN_DIM'  "
"i have modified the following in model.py, but during training, weights are still saved for every epoch, i have limited disk space.    keras.callbacks.ModelCheckpoint(self.checkpoint_path, verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=5)"
@moorage @waleedka @haeric @StevenHickson @Sekunde @rymalia 
"Hello everyone,   I ran through demo.py, and in the last cell I got the result:     and the cell is like running forever.     I also try to make the 'images' list smaller (only 2 images in the directory), but it still prints only the information for 1 image (like the above), and no more.    Has anyone encountered to the same problem? how did you solve that? Or any idea what might be the problem?    Thanks in advance.    Kelly"
"I am using keras 2.2.4  My error is as below:    --------------------------------------------------------------------------------------  Traceback (most recent call last):    File ""C:\Users\gana\AppData\Local\Programs\Python\Python35\lib\site-packages\keras\engine\network.py"", line 307, in __setattr__      is_graph_network = self._is_graph_network    File ""parallel_model.py"", line 45, in __getattribute__      return super(ParallelModel, self).__getattribute__(attrname)  AttributeError: 'ParallelModel' object has no attribute '_is_graph_network'    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""parallel_model.py"", line 158, in        model = ParallelModel(model, GPU_COUNT)    File ""parallel_model.py"", line 34, in __init__      self.inner_model = keras_model    File ""C:\Users\gana\AppData\Local\Programs\Python\Python35\lib\site-packages\keras\engine\network.py"", line 310, in __setattr__      'It looks like you are subclassing `Model` and you '  RuntimeError: It looks like you are subclassing `Model` and you forgot to call `super(YourClass, self).__init__()`. Always start with this line.  ---------------------------------------------------------------------------------------    I have tried other solutions that mentioned in this community   Now should i downgrade my version as suggested below? i actually dont wan to do so...what is the solution?  ""in order to use multiple GPUs I needed to downgrade keras 2.2.2 to kera 2.1.3"""
"Hi everyone.  I'm dealing with the shutting down problem while training with pre-trained weights.    There was no problem when training with ""coco"" weights.    However, when I tried to train with the model with the log which is the result of the first training, it shuts down.     Is there anyone who has the same problem?     "
Hi there.  i am writing my custom loss function for a detection purpose in keras.  my loss function returns a batch of tensors with size of 32.  each tensor contains 4 scalar number like this :  `[0.59131676 0.558732927 1.2102505 0.500861049]`  now i wanna feed my batches to model.compile as losses.  but i am a bit confused.  my question is that : does keras model.compile first calculate mean on my batch(i.e create a mean tensor) then sum all 4 numbers of mean tensor or first sum each tensor 4 scalars and then calculate mean of batch or there is an other operation??  i cant understand loss operation on a batch of tensors with size of > 1.  thank you.
how to run the demo in python2.7 since i want to run it in ROS
"The results of the new training, written in the same model, are overwritten or appended to the original model."
"# ModuleNotFoundError: No module named 'coco'    While running through the step-by-step inspect_weights.ipynb, I tried to run the very first notebook cell (just by copy and paste). I got the error:   ---------------------------------------------------------------------------  ModuleNotFoundError                       Traceback (most recent call last)    in  ()        1 sys.path.insert(0, '/~/Downloads/mask_rcnn_coco.h5')  ----> 2 import coco        3 config = coco.CocoConfig()    ModuleNotFoundError: No module named 'coco'  ---------------------------------------------------------------------------  I was hoping someone might be able to help me solve this problem because it has taken me forever to research, and I can't seem to figure it out! The link is:       Thank you so much!    "
"I am trying to plot precision recall plot for my validation dataset using `compute_ap` function      The `compute_ap`  function provides array of precision recall values(the length of array varies based on the image). Looks like class score threshold is the factor that is responsible for varying the array size. Where can I find the class score threshold? Can I specify what threshold to use/how to vary?     Furthermore, These result are for single image. How can I combine those varying length arrays of precision and recall to make final plot for my overall dataset?    @patrickcgray , @shriadke , @kesaroid Would greatly appreciate if you guys could share your thoughts."
"Epoch 1/30   999/1000 [============================>.] - ETA: 1s - loss: 0.2676 - rpn_class_loss: 0.0068 - rpn_bbox_loss: 0.1127 - mrcnn_class_loss: 0.0220 - mrcnn_bbox_loss: 0.0510 - mrcnn_mask_loss: 0.0752      I use 1 gpu, 2 images a bach.      It just stucks here, what should i do to continue training the model.  "
I get Nan loss (mrcnn_class_loss) whenever i use multiple GPUs. Has anyone ever faced this issue?
I keep getting nan values for losses and There are some zombie processes that are generated when I kill the training.    Has anyone ever faced this issue?
"As the title said.  For example, my data contain some classes. Objects in those classes have the width/height ratio vary significantly (0.1 to 10 for example). Given that situation, can the model classify those objects as the same class efficiently?"
"Hi, How would I go about changing the backbone to something simpler like resnet18? Do I modify the code in resnet_graph? Has anyone here already done this?    Thanks!"
"MS COCO classifies objects as small, medium and large on the basis of their area. About 41% of objects are small, 34% are medium and 24% are large.     I want to know how does this '41%, 34% and 24%' division percentages came from? Is their any statistical reasoning for that?"
"In the paper, there is an res50-fpn weights for human detection with an AP=53.6.  Where can I find it? I find it difficult to train such an high AP model."
I am using maskrcnn with Celery for asynchronous task processing and while celery worker take tasks and reach the maskrcnn model it got stuck. How to solve this issue? please help me. And also my hardware is not being comfortable with this model. How to achieve good processing speed?
"Since I want to output square bounding box only.    So I tried to do some modifications:    1. Modifying  box_refinement(box, gt_box), box_refinement_graph(box, gt_box) in utils.py  from returning different df and dw into same dh and dw only.  !     2. Modifying apply_box_deltas_graph apply_box_deltas_graph(boxes, deltas) in model.py where applying same height and width only.  !     3. Modifying RPN_ANCHORS_RATIOS in config.py into      However, all of these don't work.   So I wonder that does there have any other ways which can make the model output square bounding box only?    Thanks"
"Hello, I've got a question about PR curve.  What should I do if I want to plot the precision-recall curve for the entire dataset? Actually there is on the notebook only the PR-curve obtained for a single image.   Is it possible by using cocoeval on pycocotools?    Thanks in advance"
"- **OS:** Ubuntu 16.04 LTS  - **GPU:** Nvidia GTX 1070 or 1080 Ti  - **Driver:** 384.130  - **CUDA:** 9.0  - **cuDNN:** 7  - **Tensorflow:** 1.12.0  - **Keras:** 2.2.4    The MaskRCNN model runs incredibly slow for inference. Using the tensorflow profiler I found, that the PropsalLayer is responsible for more than 50% of the calculation time. It seems that all operations called inside utils.batch_slice() are performed on the cpu insted of the gpu. Reading through the other issues in this git, it seems, that this shouldn't be the case.    What could be the cause for this?    You can find my profiler results here:    "
"Hello there,  I'm trying to do semantic segmentation on satellite images to detect vegetation, water, roads ...   but I want full segmentation without blank areas.  is it possible to work without the background as a class?"
"i am running    sudo python3 keras_to_tensorflow.py --input_model=/home/deepedge/mask_rcnn-master/mask_rcnn_damage_0010.h5 --output_model=/home/deepedge/mask_rcnn-master/dent.pb    and getting    Using TensorFlow backend.  E0619 15:22:44.379571 140399717234496 keras_to_tensorflow.py:95] Input file specified only holds the weights, and not the model definition. Save the model using model.save(filename.h5) which will contain the network architecture as well as its weights. If the model is saved using the model.save_weights(filename) function, either input_model_json or input_model_yaml flags should be set to to import the network architecture prior to loading the weights.  Check the keras documentation for more details (   Traceback (most recent call last):  File ""keras_to_tensorflow.py"", line 182, in  app.run(main)  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 300, in run  _run_main(main, args)  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 251, in _run_main  sys.exit(main(argv))  File ""keras_to_tensorflow.py"", line 128, in main  model = load_model(FLAGS.input_model, FLAGS.input_model_json, FLAGS.input_model_yaml)  File ""keras_to_tensorflow.py"", line 106, in load_model  raise wrong_file_err  File ""keras_to_tensorflow.py"", line 63, in load_model  model = keras.models.load_model(input_model_path)  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py"", line 419, in load_model  model = _deserialize_model(f, custom_objects, compile)  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py"", line 221, in _deserialize_model  model_config = f['model_config']  File ""/usr/local/lib/python3.6/dist-packages/keras/utils/io_utils.py"", line 302, in getitem  raise ValueError('Cannot create group in read only mode.')  ValueError: Cannot create group in read only mode."
"when I use the      to create the pbtxt file , but it give me a wrong message:     "
None
@moorage @waleedka @haeric @rymalia @PavlosMelissinos @karanchahal 
Could you please share me the code to evaluate this work?Thank you!!!
"Hello MRCNN-users,    I was wondering if there are any options/thoughts about how to output a circle detection instead of a bounding box detection. In Mask-RCNN (and Faster-RCNN) there is a bounding-box estimation incorporated in the Region Proposal Network (anchors) and the final layers (bounding-box refinements from regression). However for most objects in agriculture a circle would be a more applicable  than a bounding box (most crops tend to be circular and not rectangular).     I was thinking two options, but there might be more:    1. Alter the network in such a way that the network estimates the center point and the radius of a circle instead of the 4 corner points of the bounding box  2. Use the pixels or contour of the final mask to estimate a circular shape (center point and radius) by another neural-net (regression).     I prefer the first option as this can be better â€œend-to-endâ€ learned, however Iâ€™m not sure if there are better options/thoughts. And if so, how can I efficiently develop/alter the code?     Thanks in advance,  pieterbl86  "
"     I would like to ask you some questions. I want to use the migration learning of Mask R-CNN to extract tea garden terraces and agricultural terraces. The data used is the remote sensing image downloaded by Google Earth (resolution is 0.28 meters), which is marked with VGG online version.      1. How much sample size does it take? What is the ratio of training and verification data?      2. When labeling, what is the size of the image? (1024*1024, 512*512, or other sizes)      3. How large is the size of the image during the inspection?      4. What parameters need to be changed so that better results are obtained?"
What happens when I set Train_bn to False while I train on my own dataset from scratch?    
For how many epochs & steps per epoch/ how many images was the model trained on COCO dataset in order to obtain the mask_rcnn_coco.h5 weights?    
"I am trained my own dataset according to the documentation and default values of the RPN_ANCHOR_SCALES and RPN_ANCHOR_RATIOS. But My question is How can I calculate/decide those parameters for my own dataset.  If I increase the number of the parameter's value, is it Ok? if OK, How?    Thanks"
"I want network to search people by its parts like head, body, arms and legs. How should I annotate the training images where one bodypart splits another? For example, an arm cover a body and split it in two on the image, but I want network to recognize the body as a whole thing."
hi  How to upload the file mask_rcnn_ballon.h5 weighing 244mb in the root repo directory?
"When I set USE_MINI_MASK to False, a bug will appear.     After debug, I think the bug appear in the model.py (line 585 to 606).   The correct code should be:          if config.USE_MINI_MASK:          # Transform ROI corrdinates from normalized image space          # to normalized mini-mask space.          y1, x1, y2, x2 = tf.split(positive_rois, 4, axis=1)          gt_y1, gt_x1, gt_y2, gt_x2 = tf.split(roi_gt_boxes, 4, axis=1)          gt_h = gt_y2 - gt_y1          gt_w = gt_x2 - gt_x1          y1 = (y1 - gt_y1) / gt_h          x1 = (x1 - gt_x1) / gt_w          y2 = (y2 - gt_y1) / gt_h          x2 = (x2 - gt_x1) / gt_w          boxes = tf.concat([y1, x1, y2, x2], 1)            box_ids = tf.range(0, tf.shape(roi_masks)[0])          masks = tf.image.crop_and_resize(tf.cast(roi_masks, tf.float32), boxes,                                           box_ids,                                           config.MASK_SHAPE)          # Remove the extra dimension from masks.          masks = tf.squeeze(masks, axis=3)            # Threshold mask pixels at 0.5 to have GT masks be 0 or 1 to use with          # binary cross entropy loss.          masks = tf.round(masks)      else:          masks = roi_masks          masks = tf.squeeze(masks, axis=3)"
"i am using flask so i face this error when i hit 2 or more  predict api using same time  error is   Cannot interpret feed_dict key as Tensor: Tensor Tensor(\""Placeholder:0\"", shape=(7, 7, 3, 64), dtype=float32) is not an element of this graph.""    please help me "
"The article about Mask-RCNN says: ""For convenient ablation, RPN is trained separately and does not share features with Mask R-CNN, unless specified. For every entry in this paper, RPN and Mask R-CNN have the same backbones and so they are shareable"". The article about Faster-RCNN says: ""We discuss three ways for training networks with features shared:  (i) Alternating training.  (ii) Approximate joint training.  (iii) Non-approximate joint training"".  What is the way for training RPN and Mask-RCNN with features shared here? (i), (ii) or (iii)? And why?"
What is the inference command I should use to test the model after finishing the training ?
"I'm trying to improve the masking result by changing the fully convolutional layer in build_fpn_mask_graph into a U-Net like architecture. I notice the input was of size 14 as stated with  MASK_POOL_SIZE and that the resulting mask was of size 28x28 (MASK_SHAPE), resulting from one transpose convolution operation     I saw another issue that said you could further improve by adding another transpose conv and themask would then be of size 56x56.    How ever I was wondering what the input of build_fpn_mask_graph is, is it a cropped subset of the original image resized into a 14x14 input space?     Is it another cropped subset from an already reduced image?     I think a the result masking of an images that comes from an input space of 14x14 could loose  a bunch of information, by downsampling.    I tried to use MASK_POOL_SIZE = 64 instead of 14 and MASK_SHAPE = 64x64, and changed the FCN into a U-Net, it trained slower and the end results were vey similar to those of the original FCN, so I'm just wondering if any of those changes make sense, and if not then what would be the best way to change the masking branch of the model.  "
"I am creating custom mask-rcnn object detection model.  I labeled the data with polygon shape and created csv file.    I am not able to create tf record from csv as, it has polygon set rather then rectangle bounding box.  Can you help in creating tf record..?    Also i am using this tensroflow, and using mask_rcnn_resnet101_atrous_coco from      And configuring config file and giving path for it.         Using this kind of implementation for mask-rcnn, so I can get checkpoint file.  Rather then using a pre trained h5 model of mask-rcnn    Thanks"
According to   continuous soft mask should be used during training but in function detection_targets_graph mask is rounded to int:     
"Hello,  I have in total 30 classes, but for now I only have images for 20 of them. If I am correct, the number of classes must be fixed in order to continue training of a pretrained model.   For this reason, I am currently training a model with the final 30 classes, but given images only for 20 of them. As soon as I have new images of an unseen class I plan to continue the training of the initial model.  I do it this way to:   1) Save training time, as one model training last more than 20 hours.  2) Save time to adapt configuration file and other small things    But, I am wondering if I forgot something: could this have a bad effect on the final results?  "
"As I understood, each ROI proposed by the RPN will be classified, and this by using only the ROI information. Hence, the other object on the image, or the place of the ROI in the image wont influence the classification? Is that correct or am I missing something?    Any help is greatly appreciated"
"Hello,  I'm trying to save all the model (include the optimizer) in order to continue the training at same state that the last checkpoint. I see that in the library there aren't any method to save the whole model.    For now, I have tried to change the CheckpointModel to save_weights_only = False, but I get the following error:         Honestly I do not know how to correct this problem."
"I'm confused by the comment description in the config.py:    `# crop:   Picks random crops from the image. First, scales the image based      #         on IMAGE_MIN_DIM and IMAGE_MIN_SCALE, then picks a random crop of      #         size IMAGE_MIN_DIM x IMAGE_MIN_DIM. Can be used in training only.      #         IMAGE_MAX_DIM is not used in this mode.`    My images are all 1920x1080, so if I use these settings:    `IMAGE_RESIZE_MODE = ""crop""      IMAGE_MIN_DIM = 640      IMAGE_MAX_DIM = 640  IMAGE_MIN_SCALE = 0`    it would first rescale the image to 640 and then randomly crop 640x640 in that? It makes no sense to me.    Do I then have to set IMAGE_MIN_SCALE to 3 in order to keep the original 1920 and then crop 640? I really don't understand it.    Also, in the wiki the example says:  `# Random crops of size 512x512  IMAGE_RESIZE_MODE = ""crop""  IMAGE_MIN_DIM = 512  IMAGE_MAX_DIM = 512`    but what does IMAGE_MAX_DIM have to do with it? Nothin according to the comments in the code?    Hope someone can help clarify things a little for me :) "
"I get quite a lot of false positives on objects that are similar in shape but different in color. I trained the model only on the object of a specific color. The object always look exactly the same with the same color.    Example:  I trained the model to recognize green boxes with specific text that is the same every time, however I get a lot of false positives on boxes with other colors with different text.    It does, however, recognize the object correctly every time it's in the image, so it catches all the right ones but also quite a lot of false ones.    I only trained the heads on the imagenet pretrained weights. Doesn't it learn colors? Should I change to training more than just the heads?     Any advice on what might be going on is appreciated! :)  "
I want to print the summary of Mask RCNN model but using 'print(model.summary())' is giving error 'MASKRCNN have no attribute 'summary''. I want to use my own trained resNet based classifier but in order to use this as backbone I will need the architecture details of Mask RCNN. How can i get the architecture details?
"When I ran mask-r-cnn on the tennis match video as is, it did not even detect the tennis ball at all.  Just the players and crowd mainly which was weird because there is a sports ball class.      Still, I want to see if mask-r-cnn can track a tennis ball if I use transfer learning and set the config to just 2 classes:  background plus this new ""ball"" from the annotated image frames below.        I am annnotating frames of a tennis broadcast per below:        !     !     !     Note; this used Annotator version 2 but I have switched to version 1 as ver.2 is not compatible with the code as is.  "
None
"I want to train and validate MaskRcnn for only 1 class like balloon dataset. Does anybody know how to do that with?    I tried with by placing ""class_id=[1]"" in line 481 in coco.py so that I can load only 1 class and also can test for 1 class. But it ended up with an error at #Multiproceesing=true(line 3783 in model.py)    I'm getting this error message when I tried the above logic. Does anybody know any other logic??    """"""ValueError: Error when checking input: expected input_image_meta to have shape (93,) but got array with shape (14,)"""""""
appear: failed to enqueue async memcpy from host to device: CUDA_ERROR_NOT_INITIALIZED 
"Multiple processes call detection functionsï¼Œappearï¼šFailed to get device properties, error code: 30"
solve this
"i  want use it  output detections as a COCO JSON,then  then i can use labelme to load the json file  "
"When I read model.py in line 486, I found function detection_targets_graph may have some bugs about the number of training examples per image.  positive_count = int(config.TRAIN_ROIS_PER_IMAGE * config.ROI_POSITIVE_RATIO)   i.e positive_count = int(32 * 0.33) = int(10.56) = 10  r = 1.0 / config.ROI_POSITIVE_RATIO  negative_count = tf.cast(r * tf.cast(positive_count, tf.float32), tf.int32) - positive_count  i.e negative_count = int(1.0 / 0.33 * 10.0) - 10 = int(30.303) - 10 = 20    However, negative_count + positive_count why not equal to TRAIN_ROIS_PER_IMAGEï¼Ÿ  "
"I want to train mask-rcnn with pascal voc 2012 datasets and I use the code in #51 .  However, when training I encounter this error:  `ERROR:root:Error processing image {'id': 1122, 'source': 'voc', 'path': '/content/VOCdevkit/VOC2012/JPEGImages/2010_003532.jpg', 'mask_path': '/content/VOCdevkit/VOC2012/SegmentationObject/2010_003532.png', 'objs': [OrderedDict([('name', 'horse'), ('bndbox', OrderedDict([('xmax', '432'), ('xmin', '66'), ('ymax', '364'), ('ymin', '41')])), ('difficult', '0'), ('occluded', '1'), ('pose', 'Unspecified'), ('truncated', '0')]), OrderedDict([('name', 'horse'), ('bndbox', OrderedDict([('xmax', '147'), ('xmin', '10'), ('ymax', '211'), ('ymin', '19')])), ('difficult', '0'), ('occluded', '1'), ('pose', 'Left'), ('truncated', '1')])], 'width': '500', 'height': '375'}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""/content/Mask_RCNN/samples/pascal_voc/pascal_voc.py"", line 180, in load_mask      mask = np.ones(tmp_mask.shape[:2]).astype(np.bool)  AttributeError: 'bool' object has no attribute 'shape'`    Anyone know how to fix this? I really appreciate.  P:s/ I train on Google Colab"
"Hi there !  If anyone could help clarify my understanding of the Mask RCNN parameters applied to a problem with a large number of ground instances, it would be great.  I am working on a detection of houses in a very dense area, where on a 1024*1024 pixels image, there is on average 1 500 ground truth instances.  Supposing computationnal ressources were powerful enough, in this type of problem am I right to think I would need to change these following parameters with values such as those below?  PRE_NMS_LIMIT = 10 000  POST_NMS_ROIS_TRAINING= 3 000  TRAIN_ROIS_PER_IMAGE = 2 000  ROI_POSITIVE_RATIO= 0.6  RPN_NMS_THRESHOLD= 0.8   MAX_GT_INSTANCES= 2 500    Thanks a lot !  "
"I try to adapt the code for cityscapes instance segmentation training according to the MaskRCNN paper:    > All images are 2048Ã—1024 pixels.    > We train with image scale (shorter side) randomly sampled  > from [800, 1024], which reduces overfitting; inference is on  > a single scale of 1024 pixels    How sould apply that in this implementation?"
"I've followed the readme instructions to install the require packages. Coco is not installed as part of requirements.txt.    When I run the demo.ipynb I get the following error message:     I then followed the build instructions for coco from here:       But I modified the makefile to build it for Pyhon3. This appeared to work as I didn't get any build issues.        This produces the following output:       It appears to build and install correctly. If I open up ipython3 and import coco I get the same error.  Python 3.5.2 (default, Nov 12 2018, 13:43:14)   Type 'copyright', 'credits' or 'license' for more information  IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help.       What am I doing wrong?    "
"It took me a long time to figure out why my code does not work with error `ValueError: Layer #389 (named ""mrcnn_bbox_fc""), weight   has shape (1024, 8), but the saved weight has shape (1024, 324). `    The pretrained weights 2.1 have a value of 2 in number of class meanwhile 2.0 have a value of 81.     exclude= ""mrcnn_class_logits"", ""mrcnn_bbox_fc"", ""mrcnn_bbox"", ""mrcnn_mask"" does not help as it's just simply ignore those layers and if you try this on the demo.ipynb, you can easily find there is nothing right about the result.    Please USE 2.0 INSTEAD to avoid above issue.  "
"So I sucessfully made the balloon example work with multiple classes, however, it seems that I have to train from scratch with imagenet weights if I want to add another class after training the first time.     1. When I change num_classes and train from imagenet weights it works with whatever number of classes I want    2. When using my own trained weights I get a shape error    Any suggestions?    "
"Hi, thanks for the great project!    If I am not missing something, using multiple `data_generator`s (number is typically set to ` workers = multiprocessing.cpu_count()`), the data generators produce training samples in exactly the same order. This is also the case for the `shuffle=True` mode.    This seems to be quite problematic, because the same input image would be fed as a training sample to the network multiple times, subsequently. With batch sizes > 1, (e.g. multi-GPU mode) this could mean that we are feeding in multiple identical images at the same time-step quite regularly.    Each data generator will shuffle the data in exactly the same way in   , when using `shuffle=True`, because they use an identical random seed.     One way to fix it could be setting a new, unique seed, within the data generator loop.      "
"I use the VGG(  to label samples, an image has two detection categoriesï¼Œ how do I label it?  thinksï¼"
"My data set is coco-like and should have fit well, but then it generates error with tensor flow. I didn't find much info online. Does anyone have any ideas?    Starting at epoch 0. LR=0.001    Checkpoint Path: /content/drive/My Drive/newlar/logs/larvaes20190517T0434/mask_rcnn_larvaes_{epoch:04d}.h5  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  In model:  rpn_model      rpn_conv_shared        (Conv2D)      rpn_class_raw          (Conv2D)      rpn_bbox_pred          (Conv2D)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)  /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  /usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.    UserWarning('Using a generator with `use_multiprocessing=True`'  ---------------------------------------------------------------------------  KeyError                                  Traceback (most recent call last)    in  ()        4             learning_rate=config.LEARNING_RATE,        5             epochs=4,  ----> 6             layers='heads')        7         8 end_train = time.time()    10 frames  /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _as_graph_def(self, from_version, add_shapes)     3116       if add_shapes:     3117         for node in graph.node:  -> 3118           op = self._nodes_by_name[node.name]     3119           if op.outputs:     3120             node.attr[""_output_shapes""].list.shape.extend(    KeyError: 'training_9/SGD/gradients/zeros_like_16'"
F.\tensorflow/core/framework/tensor.h:663} Check failed: new_num_elements == NumElements()(6vs11)    it was failing on model.detect()    System details:  Processor-Intel Xenon CPU E5 - 2680 v4 @2.40GHz 2.39GHz(2 processor)    RAM-64 GB    System type- 64 bit OS    Windows  __ Windows server 2016 standard
"Hello we are trying to use the mask rcnn model to detect and move objects. The camera we utiliz provides a grayscale imagea at 1.3MP. The objects we are trying to detect are positioned about 1 - 1.5 m's away from the camera.. We see the objects being detected reasonably however, once in every so many images the bounding box and mask are way off. We are using a dataset consiting of over 700 images. The detection confidence is about 0.999 all the time however the boxes and masks don' always cover the object properly.    We tried training with coco and imagenet. With different learning rates and weight decay values. What would be a concrete method the to increase the b-box determination.    kind regards"
"Hi,    I am currently trying to train the model with the COCO dataset. In particular, I am using a modified version of the 2017 splits, which maintains the defined proportion (118k in the train split, 5k in the validation split), but it changes some of the elements inside each split (some of the images of the original COCO train split are moved in the validation and viceversa). Clearly, I have also defined new annotation/labels files, consistent with these new splits. I start the training by initializing the weights with imaginet: python coco.py train --dataset= MY_COCO_DIR --year=2017 --model=imagenet    I have this issue: the training does not seem to be very effective, since the mAP (computed through python coco.py evaluate --dataset=MY_COCO_DIR --year=2017 --model=MY_MASKRCNN_PATH/logs/coco20190502T2056/mask_rcnn_coco_0160.h5 ) is always quite low:       Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.050   Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.127   Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.030   Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.022   Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.058   Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.084   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.073   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.098   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.099   Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.032   Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.092   Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.151    This is obtained when the training is finished. I use a Titan X and the training takes around 4/5 days.    I am using a single GPU, therefore BATCH_SIZE=2 (since IMAGES_PER_GPU=2). I actually left all the configurations as the default ones, thatâ€™s why I having a hard time at figuring out why the model does not seem to properly fit the data. There are few lines that I took from the output, in case they can be useful:    >     Epoch 1/40: 1000/1000 [==============================] - 1582s 2s/step - loss: 2.4615 - rpn_class_loss: 0.1257 - rpn_bbox_loss: 0.5467 - mrcnn_class_loss: 0.5672 - mrcnn_bbox_loss: 0.5331 - mrcnn_mask_loss: 0.6888 - val_loss: 3.8373 - val_rpn_class_loss: 0.2355 - val_rpn_bbox_loss: 1.1863 - val_mrcnn_class_loss: 0.7991 - val_mrcnn_bbox_loss: 0.9284 - val_mrcnn_mask_loss: 0.6879    Epoch 10/40: 1000/1000 [==============================] - 1480s 1s/step - loss: 1.5970 - rpn_class_loss: 0.0473 - rpn_bbox_loss: 0.3047 - mrcnn_class_loss: 0.3969 - mrcnn_bbox_loss: 0.3597 - mrcnn_mask_loss: 0.4885 - val_loss: 3.1093 - val_rpn_class_loss: 0.1345 - val_rpn_bbox_loss: 0.8850 - val_mrcnn_class_loss: 0.8571 - val_mrcnn_bbox_loss: 0.7166 - val_mrcnn_mask_loss: 0.5161    Epoch 61/120: 1000/1000 [==============================] - 1307s 1s/step - loss: 0.9952 - rpn_class_loss: 0.0181 - rpn_bbox_loss: 0.2322 - mrcnn_class_loss: 0.2095 - mrcnn_bbox_loss: 0.2122 - mrcnn_mask_loss: 0.3233 - val_loss: 2.5287 - val_rpn_class_loss: 0.0368 - val_rpn_bbox_loss: 1.2079 - val_mrcnn_class_loss: 0.4660 - val_mrcnn_bbox_loss: 0.4377 - val_mrcnn_mask_loss: 0.3803    Epoh 91/120: 1000/1000 [==============================] - 1285s 1s/step - loss: 0.9300 - rpn_class_loss: 0.0147 - rpn_bbox_loss: 0.2226 - mrcnn_class_loss: 0.2223 - mrcnn_bbox_loss: 0.1847 - mrcnn_mask_loss: 0.2857 - val_loss: 2.5603 - val_rpn_class_loss: 0.0723 - val_rpn_bbox_loss: 1.0890 - val_mrcnn_class_loss: 0.5659 - val_mrcnn_bbox_loss: 0.3655 - val_mrcnn_mask_loss: 0.4676    Epoch 121/160: 1000/1000 [==============================] - 1710s 2s/step - loss: 1.7273 - rpn_class_loss: 0.0469 - rpn_bbox_loss: 0.5513 - mrcnn_class_loss: 0.4245 - mrcnn_bbox_loss: 0.3350 - mrcnn_mask_loss: 0.3695 - val_loss: 2.2322 - val_rpn_class_loss: 0.1335 - val_rpn_bbox_loss: 0.8672 - val_mrcnn_class_loss: 0.3713 - val_mrcnn_bbox_loss: 0.4424 - val_mrcnn_mask_loss: 0.4179    Epoch 160/160: 1000/1000 [==============================] - 2213s 2s/step - loss: 1.3093 - rpn_class_loss: 0.0246 - rpn_bbox_loss: 0.3485 - mrcnn_class_loss: 0.3370 - mrcnn_bbox_loss: 0.2421 - mrcnn_mask_loss: 0.3571 - val_loss: 2.5502 - val_rpn_class_loss: 0.0882 - val_rpn_bbox_loss: 1.0248 - val_mrcnn_class_loss: 0.5397 - val_mrcnn_bbox_loss: 0.4790 - val_mrcnn_mask_loss: 0.4184    >     Any idea about why is this happening and/or how to solve this problem? I have also tried to train the model with default 2014 split (without moving any image from one split to the other: python coco.py train --dataset=MY_COCO_PATH --year=2014 --model=imagenet --download=True) to find eventual bugs in my custom splits, but I have similar issues during the evaluation. I also tried to increase the learning rate but I start having a nan ( I guess that relates to the issue that you point out on the index page of the repository).    Thank you very much in advance, any help will be appreciated."
"During the training phase,   Do mrcnn_mask and mrcnn_bbox have details about the masks and the coordinates of the masks in the training images?    "
Does anyone know how to remove the FPN?
"Hey, the project is great and i really apreciate your work.    But one thing i wanted to do, if you could tell me if possible, is to remove the instance segmentation, to get faster. Is that possible?"
"In model.py : 1878 line , K.shape(input_image)           "
"Hello,  I am working on medical images and have two classes.  I want to train my model on only positive samples.anybody know how can i do it?"
"For some weird reason, if I set the amount of images per gpu > 1 (which in my case translates to batch size, as I have just 1 videocard), model gets stuck on predict call.    I would greatly appreciate any help. All my experiments with different workarounds, including, but not limited to forcing _make_predict_function, finalizing graph, etc. wouldn't help.    Detection would just consume terrible amounts of RAM (30% of p2.xlarge, which is a lot). nvidia-smi wouldn't show any videocard utilization at all on the other hand.    Here are the specs I use:  K80 - 12GB RAM (aws p2.xlarge)    CUDA versions:  cuda-command-line-tools-9-0   cuda-cublas-9-0   cuda-cufft-9-0   cuda-curand-9-0   cuda-cusolver-9-0   cuda-cusparse-9-0   libcudnn7=7.0.4.31-1+cuda9.0  libnccl2=2.2.13-1+cuda9.0  nvinfer-runtime-trt-repo-ubuntu1604-4.0.1-ga-cuda9.0  libnvinfer4=4.1.2-1+cuda9.0    pip versions:  tensorflow==1.5.0  tensorflow-gpu==1.5.0    Any help would be highly appreciated. The issue is reproducible on any of the python notebooks with tweaks for batch_size increase."
"I train my own data set,change the RPN_RATIOS in config.py,my ratios has more than three elements,when I run the following code   `model = modellib.MaskRCNN(mode='training', config=config, model_dir=ROOT_DIR)    # Exclude the last layers because they require a matching  # number of classes  model.load_weights(COCO_MODEL_PATH, by_name=True, exclude=[      ""mrcnn_class_logits"", ""mrcnn_bbox_fc"",      ""mrcnn_bbox"", ""mrcnn_mask""])`    the error as follows:  `ValueError: Layer #359 (named ""rpn_model""), weight   has shape (1, 1, 512, 10), but the saved weight has shape (6, 512, 1, 1).`  does the coco preptrained weigths not match the new anchors in Layers(such as P2,P3,P3...)  I also remove the cache maded by training previous  "
"Hello, I want to learn about the structure of the unet to modify the network that generates the Mask part, but after I modified it, the tensorboard shows that the network has not been modified. Which part of the code do I need to modify? Please help me, thank you very much."
"Set the epoch to 20, but why did it stop once"
None
"I get this error when i tried to training my own dataset :  How can i solve this error :     > Using TensorFlow backend.  Traceback (most recent call last):    File ""iris.py"", line 86, in        class IrisDataset(utils.Dataset):  AttributeError: module 'utils' has no attribute 'Dataset'    "
"How can I fix this problem. When I run my code CTA.py using TensorFlow, it works. But when I use Tensorflow-gpu, it reports such error.    Epoch 1/1  Traceback (most recent call last):    File ""D:/KerasProject/MaskRCNN/samples/CTA/CTA.py"", line 400, in        train(model)    File ""D:/KerasProject/MaskRCNN/samples/CTA/CTA.py"", line 201, in train      layers='heads')    File ""D:\KerasProject\MaskRCNN\mrcnn\model.py"", line 2375, in train      use_multiprocessing=False,    File ""C:\Users\FengZhiheng\Anaconda3\envs\FengZhihengKeras\lib\site-packages\keras\legacy\interfaces.py"", line 87, in wrapper      return func(*args, **kwargs)    File ""C:\Users\FengZhiheng\Anaconda3\envs\FengZhihengKeras\lib\site-packages\keras\engine\training.py"", line 2065, in fit_generator      generator_output = next(output_generator)    File ""C:\Users\FengZhiheng\Anaconda3\envs\FengZhihengKeras\lib\site-packages\keras\utils\data_utils.py"", line 710, in get      raise StopIteration()  StopIteration    thanks very much"
"I have read instruction on github, but i found that description is not clear about how can i start training my own dataset.  PS: I have masked and colors image for my datasets ( iris region )   I want to training this datasets using segmentation tool  Thanks"
"I change the code into FP16 training by using set_floatx and tf.cast for many variables     I was able to train the model and each loss seems correct but the total loss is Nan.    !     You can see that each loss is reasonable, any ideas about this issue?    Thank you!  "
"I trained my own model by annotating with VIA tool, but was getting this error:  IndexError: index 1000 is out of bounds for axis 1 with size 1000  Is there any solution for this?"
"I would like to crop-out my selections and have them save after running the trained model on an image (ideally, make it so multiple selections will each be saved as a different file). Can anyone pinpoint me where to start? I apologize, but I'm rather new at this"
"Hi,    'coco.py evaluate' reports AP and AR for different area and IOU ratios.    Is there any way I can report per-class AP?    For reference: Here is the sample output for darknet's MAP evaluation:    class_id = 0, name = person, ap = 72.04%         (TP = 7505, FP = 3638)  class_id = 1, name = bicycle, ap = 50.77%        (TP = 137, FP = 77)  class_id = 2, name = car, ap = 60.70%            (TP = 966, FP = 549)  class_id = 3, name = motorbike, ap = 67.90%      (TP = 236, FP = 71)  class_id = 4, name = aeroplane, ap = 75.89%      (TP = 89, FP = 9)  class_id = 5, name = bus, ap = 85.33%            (TP = 200, FP = 44)  class_id = 6, name = train, ap = 81.74%          (TP = 164, FP = 27)  class_id = 7, name = truck, ap = 54.50%          (TP = 172, FP = 131)  class_id = 8, name = boat, ap = 45.91%           (TP = 191, FP = 108)  class_id = 9, name = traffic light, ap = 50.19%          (TP = 252, FP = 155)  class_id = 10, name = fire hydrant, ap = 83.36%          (TP = 64, FP = 10)  class_id = 11, name = stop sign, ap = 79.34%     (TP = 61, FP = 10)  class_id = 12, name = parking meter, ap = 55.42%         (TP = 30, FP = 10)  class_id = 13, name = bench, ap = 34.06%         (TP = 137, FP = 117)  class_id = 14, name = bird, ap = 44.96%          (TP = 213, FP = 167)  class_id = 15, name = cat, ap = 79.90%           (TP = 148, FP = 47)  class_id = 16, name = dog, ap = 79.83%           (TP = 175, FP = 78)  class_id = 17, name = horse, ap = 78.20%         (TP = 236, FP = 104)  class_id = 18, name = sheep, ap = 57.45%         (TP = 187, FP = 191)  class_id = 19, name = cow, ap = 55.34%           (TP = 176, FP = 92)  class_id = 20, name = elephant, ap = 85.12%      (TP = 207, FP = 29)  class_id = 21, name = bear, ap = 82.65%          (TP = 42, FP = 14)  class_id = 22, name = zebra, ap = 81.10%         (TP = 202, FP = 26)  class_id = 23, name = giraffe, ap = 87.19%       (TP = 141, FP = 11)  class_id = 24, name = backpack, ap = 32.94%      (TP = 103, FP = 89)  class_id = 25, name = umbrella, ap = 58.62%      (TP = 204, FP = 90)  class_id = 26, name = handbag, ap = 21.56%       (TP = 101, FP = 112)  class_id = 27, name = tie, ap = 51.04%           (TP = 140, FP = 68)  class_id = 28, name = suitcase, ap = 47.96%      (TP = 124, FP = 54)  class_id = 29, name = frisbee, ap = 76.79%       (TP = 75, FP = 12)  class_id = 30, name = skis, ap = 37.68%          (TP = 94, FP = 64)  class_id = 31, name = snowboard, ap = 49.54%     (TP = 47, FP = 25)  class_id = 32, name = sports ball, ap = 60.01%           (TP = 134, FP = 47)  class_id = 33, name = kite, ap = 44.26%          (TP = 171, FP = 112)  class_id = 34, name = baseball bat, ap = 50.49%          (TP = 61, FP = 24)  class_id = 35, name = baseball glove, ap = 50.09%        (TP = 63, FP = 36)  class_id = 36, name = skateboard, ap = 71.30%            (TP = 144, FP = 28)  class_id = 37, name = surfboard, ap = 65.41%     (TP = 162, FP = 51)  class_id = 38, name = tennis racket, ap = 72.71%         (TP = 123, FP = 20)  class_id = 39, name = bottle, ap = 43.24%        (TP = 418, FP = 357)  class_id = 40, name = wine glass, ap = 52.00%            (TP = 166, FP = 65)  class_id = 41, name = cup, ap = 50.34%           (TP = 414, FP = 254)  class_id = 42, name = fork, ap = 39.27%          (TP = 79, FP = 69)  class_id = 43, name = knife, ap = 29.80%         (TP = 89, FP = 72)  class_id = 44, name = spoon, ap = 25.59%         (TP = 66, FP = 89)  class_id = 45, name = bowl, ap = 50.35%          (TP = 298, FP = 237)  class_id = 46, name = banana, ap = 31.52%        (TP = 104, FP = 110)  class_id = 47, name = apple, ap = 16.24%         (TP = 41, FP = 139)  class_id = 48, name = sandwich, ap = 49.46%      (TP = 69, FP = 49)  class_id = 49, name = orange, ap = 32.54%        (TP = 64, FP = 67)  class_id = 50, name = broccoli, ap = 30.59%      (TP = 89, FP = 87)  class_id = 51, name = carrot, ap = 22.18%        (TP = 71, FP = 89)  class_id = 52, name = hot dog, ap = 42.12%       (TP = 59, FP = 28)  class_id = 53, name = pizza, ap = 60.40%         (TP = 118, FP = 36)  class_id = 54, name = donut, ap = 45.49%         (TP = 106, FP = 103)  class_id = 55, name = cake, ap = 50.47%          (TP = 109, FP = 77)  class_id = 56, name = chair, ap = 42.95%         (TP = 646, FP = 477)  class_id = 57, name = sofa, ap = 59.60%          (TP = 138, FP = 111)  class_id = 58, name = pottedplant, ap = 43.34%           (TP = 154, FP = 90)  class_id = 59, name = bed, ap = 69.93%           (TP = 124, FP = 57)  class_id = 60, name = diningtable, ap = 46.21%           (TP = 297, FP = 254)  class_id = 61, name = toilet, ap = 77.94%        (TP = 135, FP = 19)  class_id = 62, name = tvmonitor, ap = 77.46%     (TP = 187, FP = 75)  class_id = 63, name = laptop, ap = 73.47%        (TP = 158, FP = 56)  class_id = 64, name = mouse, ap = 72.39%         (TP = 64, FP = 20)  class_id = 65, name = remote, ap = 48.88%        (TP = 103, FP = 68)  class_id = 66, name = keyboard, ap = 69.94%      (TP = 69, FP = 19)  class_id = 67, name = cell phone, ap = 42.07%            (TP = 115, FP = 79)  class_id = 68, name = microwave, ap = 71.99%     (TP = 56, FP = 19)  class_id = 69, name = oven, ap = 51.23%          (TP = 67, FP = 38)  class_id = 70, name = toaster, ap = 17.41%       (TP = 1, FP = 4)  class_id = 71, name = sink, ap = 60.39%          (TP = 119, FP = 64)  class_id = 72, name = refrigerator, ap = 73.74%          (TP = 71, FP = 25)  class_id = 73, name = book, ap = 15.44%          (TP = 265, FP = 744)  class_id = 74, name = clock, ap = 74.02%         (TP = 193, FP = 34)  class_id = 75, name = vase, ap = 51.12%          (TP = 162, FP = 83)  class_id = 76, name = scissors, ap = 37.90%      (TP = 19, FP = 8)  class_id = 77, name = teddy bear, ap = 59.65%            (TP = 112, FP = 41)  class_id = 78, name = hair drier, ap = 9.47%     (TP = 1, FP = 1)  class_id = 79, name = toothbrush, ap = 34.71%            (TP = 27, FP = 18)  "
"I train the weights and when i use them to find the output, Everytime I use the same input image, I get different outputs for different executions (input image and weights are the same, so i expect the same output for every execution). Do you have any idea why this could happen and which part I should try debugging?"
"my model was saved by one epoch, but I want to train the 5000 epochs,  so  I want to set the times of the model was saved by 10 epoch.please help me !"
"In the `model.py`, by enabling the `random_rois` option in the `data_generator` function, I encounter an error: `ValueError: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 7 array(s), but instead got the following list of 9 arrays` - any idea how to fix this?   "
"I am trying to use Mask_RCNN for transfer learning for my custom dataset. I want mask_rcnn to detect all the 80 classes from the coco dataset and also i want to add new class for my custom dataset. How to achieve this, can anybody give me a pointer for the same.    as in the samples/balloon it shows that they only trying to detect balloon, in my case if i want to detect all the 80 classes of coco images and then also balloon , how to achieve this ?    Thanks and Regards"
"Hello everyone,     I am running the matterport Mask_RCNN on my own dataset created using waspinator/pycococreator (     Everything works fine when I am using polygons, but as soon as I am using RLE because I have crowded images I got an error when trying to display a few images from the training dataset,     I got problems with images with holes using polygons and I assumed that is why the pycococreator uses RLE when the image is crowded. This part works perfectly, but then since MaskRCCN requires polygons I am stuck in this loop.     Is there a way to convert RLE to Polygons? I am not even sure this would solve the problem"
"Hi, I successfully converted Keras model to serving_model using this repository many thanks to @bendangnuksung. Now I am preparing the client api side. Here is the image loading part of api:     `if len(image_path) > 0:            filenames = [(image_path + '/' + f) for f in listdir(image_path) if isfile(join(image_path, f))]            for filename in filenames:                image_tmp = Image.open(image)                image_np = _load_image_into_numpy_array(image_tmp)                imagedata = np.expand_dims(image_np, axis=0)        else:            image_tmp = Image.open(image)            image_np = _load_image_into_numpy_array(image_tmp)            imagedata = np.expand_dims(image_np, axis=0)`    **and here is the loading part:**    `   for data in imagedata:                request = predict_pb2.PredictRequest()                request.model_spec.name = 'NAME'                  request.model_spec.signature_name = 'serving_default'                request.inputs['input_image'].CopyFrom(make_tensor_proto(data, shape=[1] + list(data.shape)))                            result = stub.Predict(request, 60.0)  # 60 secs timeout`    **I know that I have to provide input_image_meta and input_anchors as well but I don't know how since I got the following error:**    debug_error_string = ""{""created"":""@1556096504.523310000"",""description"":""Error received from peer"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""input size does not match signature: 1!=3 len({inputs}) != len({input_anchors,input_image,input_image_meta}). Sent extra: {inputs}. Missing but required: {input_anchors,input_image,input_image_meta}."",""grpc_status"":3}""    Any guide to solve this error?"
"In the current MRCNN implementation, the mrcnn_class_loss is only calculated for the ""active_class_ids"" of the ""source.id"" mapping (from the utils.Dataset parent class) for each image (really each batch as there is a TODO in the model).    Does this not mean if you have multiple sources the classifier head will not learn from negative examples of other classes?  If so, is there any reason to keep this mechanism there?    I have tested this by modifying MRCNN to enable all classes in every source.  I trained the standard MRCNN and my modification with the same hyperparameters with my dataset that has two sources. I used ReduceLROnPlateau(patience=10, cooldown=20) starting at 2e-3.    The results was that every training loss was significantly larger when using class_id masking by source.  These results were also reflected in the validation losses.    Below is an image of the training loss. The light blue line is using the standard MRCNN (with class_id source masking) and the dark blue line is without.    !   "
"I created my own dataset in COCO-style. So i have the annotation-files for my trainset, validationset, testset and for all my images.   I can use the prediction to see how the Ground Truth looks like and what my NN predicts:       But how can i go through all my testfiles to see how good my NN really is?  I already have all the GTs for my images and I trained my NN for a very long time. But the prediction is bad. I want to see if the NN ""thinks"" it is good or if the NN is just bad.    I have nearly 600 train images and 120 validation images. Here is an example for GT and prediction with the config:       !   !     The prediction said: There is no instances to display. But obviously in the GT you see, that they are ropes...  Therfore i want to go through all my testimages and evaluate the results. I can't believe that after that many epochs the NN don't recognize any ropes. Even with my trainset the prediction is bad."
"In general when working with bounding box regression, number of output neurons is fixed (four). How can Mask RCNN handle polygons, where the number of corners is not fixed."
"I want to know the location of objections( in the form of coordinate of the rectangular box), how cloud I get this output."
I want to know if Mask_Rcnn works with cross validation?  Thank you!
" I used the example of the official website to train balloon, and there was a problem.   File ""skimage/draw/_draw.pyx"", line 259, in skimage.draw._draw.polygon (skimage/draw/_draw.c:3714)  AttributeError: 'list' object has no attribute 'shape'"
"I have trained a Mask R-CNN model and run inference on grayscale images (all 512x512), first using this library (the nucleus sample) and then using   (straightforward E2E Mask R-CNN in COCO format).    The average inference time for the 512x512 images using **this** library on different GPUs is stated in the following table    | GPU       | Avg Inference time  |  | ------------- |:-------------:|  | GeForce GTX1050M      | 1.60 s |  | GeForce GTX1070M | 1.55 s      |  | Tesla P100 | 1.20 s      |    The average inference time for the same images using the Detectron library is    | GPU       | Avg Inference time  |  | ------------- |:-------------:|  | GeForce GTX1050M      | 0.75 s |    Note: I made an edit. Previously I reported the avg inference time as 0.42 s. However, this was not using the same config.    Have not tested on the other GPUs but this is sufficient.    However, the results are accurate and similar in both cases.  Could there be a bottleneck in the inference stage?"
"I change the cityscapes to coco annotations file  and load mask like coco.py.    I finetune heads  on coco pretrained model by 30 epochs,   but got bad result of mask.    And I changed the train rules,  the mask was also bad ,  even worse than coco pretrained model.     anyone can help me to train a good model on cityscape?  THX!"
"When running the `demo.py` from the notebook. I'm running within Docker, this is the Dockerfile:      It seems that `requirements` are missing `libsm6` for some reason. Any hint?"
"Hi folk,     When I am testing the performance of my model. I used model.detect() and model.detect_molded(). But the odd thing is that it will get two different mask prediction.     I checked the code of model.detec_molded() and find a weird thing.      so when its calling self.unmold_detections, the image.shape its passed in is the shape of molded images. However, the unmold_detections require image.shape to be original image shape. the code of unmold_detections are following:        I think here is something wrong. But I am not sure this is the reason why I got two different result from two detect functions. Need some help here. Thanks "
"Hello,    i use Mask-R-CNN for my work in my university and therfore i can't use all the CPU Cores. I use a Geforce 2080 TI and the server has 2 CPU's with 18 cores each. My data is on that same server saved on a SSD.  In the model.py i edited the code in line 2369 where you set the number of workers:       For workers i set the number to 2. But in htop my process is still using to much CPU:    !     Before i edited workers to workers = 4, but still the CPU usage was that high.   When i use ""ps -elf|grep my_user_name | wc"", i get 17 back, although workers = 2. I also got 17 back with workers = 4.   Only with ""ps -elfL|my_user_name | wc"" i got 497 back, and with workers = 4 it was over 700.   !     And my GPU Workload is sometimes 0%, sometimes 3%, sometimes 27%, sometimes over 40%. But i wonder why it is sometimes 0% or 3% during training. In that time the NN didn't save the h5 weights, but is was in training.    This is my GPU Workload so far:  !   I restarted the process twice, therfore you can see in the screenshot the low RAM twice.    Does someone has the same issue or know what causes the issue?  "
"Hello,    i use Mask-R-CNN to detect different ropes in images. I created my own dataset, which contains over 700 images. The shape of each image is (540,960).  I started to train all the layers from my NN and use many epoch (460), because i hope to get overfitting. So i can see on TensorBoard how many epochs is a good number before overfitting started.   My config is:         My results are bad. Maybe it is because i didn't choose validation and testset very wisely... But for example this is a picture which the NN was trained by:  !     And this is an example of the testset:  !     So after 460 epochs i saw the prediction and it didn't find any ropes. In the terminal there was written: *** No instances to display ***     This is the Ground Truth, this has to be the result in the best case:  !     One thing is weird: even if i use an image from the testset, i got bad result:  !     This is the result on TensorBoard:  !     You can see that the validation loss is oscillating, but more importantly the train loss is going down... But still the prediction is very bad, even if the image is from the trainset.    Does somebody know why my prediction of the train-image is very bad, although every epoch i went through the whole trainset and i choose 460 epochs? Are the number of epochs still to less? Or maybe some parameters like learning rate are too small or too high?      "
test by using pb file
"high gpu memory usage low volatile utils,  gpu memory fully occupied,but volatile utils almost is 0, I can't fegure out why"
"In my search for better accuracy to segment person(background removal) I''m trying to further train the the person class using a new data-set and the provided mask_rcnn_coco.h5 weights. I follow the example of balloon color splash and write my own code train the network heads.     The training crashes with the following error.  `ERROR:root:Error processing image {'id': '3.jpg', 'source': 'person', 'path': '/home/ankit/foreground/extraction/training/val/3.jpg', 'width': 425, 'height': 640, 'polygons': [{'name': 'polygon', 'all_points_x': [200, 198, 200, 200, 191, 184, 173, 162, 156, 156, 158, 161, 159, 161, 163, 172, 175, 178, 185, 192, 200, 206, 209, 211, 209, 200, 189, 176, 154, 142, 123, 108, 106, 97, 85, 73, 62, 60, 54, 56, 61, 64, 68, 72, 73, 77, 78, 80, 89, 98, 117, 138, 163, 180, 183, 198, 213, 233, 247, 253, 258, 272, 285, 297, 305, 311, 322, 328, 330, 329, 329, 329, 326, 315, 311, 314, 318, 320, 321, 324, 321, 311, 311, 317, 324, 333, 341, 346, 344, 347, 350, 351, 330, 321, 313, 307, 304, 304, 301, 304, 316, 322, 331, 338, 336, 334, 322, 309, 315, 316, 316, 323, 326, 326, 321, 311, 300, 295, 292, 285, 276, 275, 283, 290, 293, 299, 305, 289, 279, 264, 247, 238, 231, 219, 203, 199, 196, 193, 187, 182, 178, 180, 188, 197, 201, 200], 'all_points_y': [423, 431, 439, 444, 447, 444, 439, 424, 415, 402, 394, 385, 366, 348, 328, 311, 301, 287, 269, 254, 246, 221, 211, 200, 197, 198, 199, 199, 198, 199, 206, 212, 213, 217, 217, 207, 197, 190, 177, 175, 175, 179, 172, 169, 176, 176, 179, 189, 194, 196, 190, 179, 175, 173, 165, 165, 166, 161, 158, 156, 146, 138, 138, 139, 138, 135, 124, 119, 106, 88, 74, 65, 60, 56, 45, 40, 40, 43, 43, 41, 36, 25, 20, 19, 22, 28, 36, 46, 57, 73, 96, 128, 155, 166, 174, 187, 213, 230, 251, 263, 274, 289, 308, 329, 341, 350, 372, 391, 394, 403, 409, 419, 432, 442, 444, 445, 437, 431, 418, 414, 400, 390, 384, 376, 365, 349, 338, 322, 316, 295, 289, 290, 301, 310, 319, 318, 328, 343, 357, 376, 388, 400, 407, 414, 419, 423]}]}  Traceback (most recent call last):    File ""/home/ankit/foreground/lib/python3.7/site-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ankit/foreground/lib/python3.7/site-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)  TypeError: cannot unpack non-iterable NoneType object  multiprocessing.pool.RemoteTraceback:   """"""  Traceback (most recent call last):    File ""/usr/lib/python3.7/multiprocessing/pool.py"", line 121, in worker      result = (True, func(*args, **kwds))    File ""/home/ankit/foreground/lib/python3.7/site-packages/keras/utils/data_utils.py"", line 626, in next_sample      return six.next(_SHARED_SEQUENCES[uid])    File ""/home/ankit/foreground/lib/python3.7/site-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ankit/foreground/lib/python3.7/site-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)  TypeError: cannot unpack non-iterable NoneType object  """"""    The above exception was the direct cause of the following exception:    Traceback (most recent call last):    File ""person_trainer.py"", line 200, in        train(model)    File ""person_trainer.py"", line 125, in train      layers='heads')    File ""/home/ankit/foreground/lib/python3.7/site-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 2374, in train    File ""/home/ankit/foreground/lib/python3.7/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""/home/ankit/foreground/lib/python3.7/site-packages/keras/engine/training.py"", line 1418, in fit_generator      initial_epoch=initial_epoch)    File ""/home/ankit/foreground/lib/python3.7/site-packages/keras/engine/training_generator.py"", line 181, in fit_generator      generator_output = next(output_generator)    File ""/home/ankit/foreground/lib/python3.7/site-packages/keras/utils/data_utils.py"", line 709, in get      six.reraise(*sys.exc_info())    File ""/home/ankit/foreground/lib/python3.7/site-packages/six.py"", line 693, in reraise      raise value    File ""/home/ankit/foreground/lib/python3.7/site-packages/keras/utils/data_utils.py"", line 685, in get      inputs = self.queue.get(block=True).get()    File ""/usr/lib/python3.7/multiprocessing/pool.py"", line 683, in get      raise self._value  TypeError: cannot unpack non-iterable NoneType object  `    Any help will be appreciated. Thanks."
"I train the network with my own training and validation data and save the weights after each epoch.  But, everytime i use the saved weights on the same test image, I get different outputs (detections are different), where i would expect to get the same output every single time for the exact same input.    ANyone faced this issue?"
"When I used a small data set (60), 10 epochs took three hours, but when I used a relatively large data set (1500), why did it only take three hours?    `# GPU because the images are small. Batch size is 8 (GPUs * images/GPU).      GPU_COUNT = 1      IMAGES_PER_GPU = 1         # Number of classes (including background)      NUM_CLASSES = 1 + 1  # background + 1 shapes         # Use small images for faster training. Set the limits of the small side      # the large side, and that determines the image shape.      IMAGE_MIN_DIM = 320      IMAGE_MAX_DIM = 384         # Use smaller anchors because our image and objects are small      RPN_ANCHOR_SCALES = (8 * 6, 16 * 6, 32 * 6, 64 * 6, 128 * 6)  # anchor side in pixels         # Reduce training ROIs per image because the images are small and have      # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.      TRAIN_ROIS_PER_IMAGE = 100         # Use a small epoch since the data is simple      STEPS_PER_EPOCH = 200   `  Is there any configuration I forgot about, about adjusting the number of picturesï¼Ÿ"
c:\programdata\anaconda3\envs\mask-rcnn\lib\site-packages\pycocotools\coco.py in createIndex(self)       97             for ann in self.dataset['annotations']:       98                 imgToAnns[ann['image_id']].append(ann)  ---> 99                 anns[ann['id']] = ann      100       101         if 'images' in self.dataset:    KeyError: 'id'  
None
Which layer should I take which has a good representation of the colour of the detected objects?
"My aim is to implement this model in edge computing, and one thing i'm trying out is quantising the model.    I've converted the .h5 file into a tf frozen graph following  , but when I tried to   it, I get the following error    `ValueError: None is only supported in the 1st dimension. Tensor 'input_image' has invalid shape '[None, None, None, 3]'.`    This is likely a limitation of tflite, but was wondering if anyone managed to bypass it? I inputted an arbitrary shape into the input image shapes, but it doesn't seem to work as well."
"Hello, thanks for Mask-RCNN!    I have some questions. Perhaps its basic but I am new at the field.   I have created .png binary masks for every image of my training dataset, and then a .json file  . My images looks like the nucleus ones. Small randomly distributed sources, some times of a few pixels.    So my repository with the training dataset consists of (names and directory names not accurate):     So the format of my json file is:  `  [{""id"": 1, ""category_id"": 1, ""bbox"": [134.0, 85.0, 1.0, 1.0], ""width"": 227, ""area"": 1, ""height"": 227, ""iscrowd"": 0, ""segmentation"": [[134.0, 85.5, 133.5, 85.0, 134.0, 84.5, 134.5, 85.0, 134.0, 85.5]], ""image_id"": 1}, {""id"": 2, ""category_id"": 1, ""bbox"": [155.0, 92.0, 19.0, 32.0], ""width"": 227, ""area"": 434, ""height"": 227, ""iscrowd"": 0, ""segmentation"": [[162.0, 123.5, 158.0, 123.5, 155.5, 121.0, 155.5, 119.0, 154.5, 118.0, 154.5, 110.0, 155.5, 109.0, 155.5, 106.0, 156.5, 105.0, 156.5, 103.0, 157.5, 102.0, 157.5, 101.0, 158.5, 100.0, 158.5, 99.0, 160.5, 97.0, 160.5, 96.0, 164.0, 92.5, 165.0, 92.5, 166.0, 91.5, 170.0, 91.5, 172.5, 94.0, 172.5, 96.0, 173.5, 97.0, 173.5, 104.0, 172.5, 105.0, 172.5, 108.0, 171.5, 109.0, 171.5, 111.0, 170.5, 112.0, 170.5, 114.0, 168.5, 116.0, 168.5, 117.0, 166.5, 119.0, 166.5, 120.0, 165.0, 121.5, 164.0, 121.5, 162.0, 123.5]], ""image_id"": 1}, {""id"": 3, ""category_id"": 1, ""bbox"": [164.0, 136.0, 4.0, 2.0], ""width"": 227, ""area"": 7, ""height"": 227, ""iscrowd"": 0, ""segmentation"": [[167.0, 137.5, 165.0, 137.5, 163.5, 136.0, 164.0, 135.5, 167.0, 135.5, 167.5, 136.0, 167.5, 137.0, 167.0, 137.5]], ""image_id"": 1},....etc......`    I have a few questions:    **1.** So is it a format that this specific implementation of Mask-RCNN, accepts for training???  **2.** Which is the right format? Can please anyone explain?? As I saw from the balloons json format, it doesn't look like the official coco format, right??  **3.** Is there a clever way to transform my binary masks into the right .json training format??    I need to train the model with my exclusively own custom data. So I have to create  my own training dataset. I don't think I could do the segmentation manually with a package, because I have a few hundred thousands of images, with some hundreds of objects in it. So converting my binary masks as training dataset, is the best option    Please some help anybody!    "
"I try to train my own dataset with 2 classes, I labeled the dataset with VIA2.0 which is a bit different from the 1.0, other parts are ok but it gave me a "" KeyError: 'filename' ""when I start to train.  `{    ""2013-01-18_07_40_02.jpg297695"": {      ""filename"": ""2013-01-18_07_40_02.jpg"",      ""size"": 297695,      ""regions"": [        {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [ 433, 471, 418, 391 ],            ""all_points_y"": [ 143, 174, 191, 163 ]          },          ""region_attributes"": { ""status"": { ""free"": true } }        },        {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [ 401, 439, 383, 359 ],            ""all_points_y"": [ 174, 205, 220, 194 ]          },          ""region_attributes"": { ""status"": { ""free"": true } }        },        {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [ 364, 407, 340, 316 ],            ""all_points_y"": [ 195, 230, 250, 224 ]          },          ""region_attributes"": { ""status"": { ""free"": true } }        },        {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [ 323, 366, 293, 264 ],            ""all_points_y"": [ 221, 264, 291, 247 ]          },          ""region_attributes"": { ""status"": { ""free"": true } }        },        {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [ 273, 309, 233, 207 ],            ""all_points_y"": [ 248, 299, 320, 282 ]          },          ""region_attributes"": { ""status"": { ""free"": true } }        },        {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [ 166, 255, 227, 141 ],            ""all_points_y"": [ 363, 342, 284, 313 ]          },          ""region_attributes"": { ""status"": { ""free"": true } }        },        {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [ 89, 189, 156, 75 ],            ""all_points_y"": [ 416, 397, 319, 341 ]          },          ""region_attributes"": { ""status"": { ""free"": true } }        },        {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [ 478, 532, 562, 520 ],            ""all_points_y"": [ 167, 155, 175, 203 ]          },          ""region_attributes"": { ""status"": { ""free"": true } }        },        {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [ 444, 502, 537, 489 ],            ""all_points_y"": [ 192, 178, 204, 230 ]          },          ""region_attributes"": { ""status"": { ""free"": true } }        },        {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [ 405, 467, 498, 450 ],            ""all_points_y"": [ 219, 204, 231, 266 ]          },          ""region_attributes"": { ""status"": { ""free"": true } }        },        {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [ 366, 435, 462, 410 ],            ""all_points_y"": [ 250, 234, 268, 305 ]          },          ""region_attributes"": { ""status"": { ""free"": true } }        },        {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [ 318, 390, 429, 361 ],            ""all_points_y"": [ 288, 269, 310, 349 ]          },          ""region_attributes"": { ""status"": { ""free"": true } }        },        {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [ 261, 342, 375, 298 ],            ""all_points_y"": [ 329, 308, 346, 397 ]          },          ""region_attributes"": { ""status"": { ""free"": true } }        },        {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [ 694, 756, 791, 683 ],            ""all_points_y"": [ 206, 190, 215, 245 ]          },          ""region_attributes"": { ""status"": { ""free"": true } }        },        {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [ 686, 748, 783, 662 ],            ""all_points_y"": [ 236, 220, 245, 278 ]          },          ""region_attributes"": { ""status"": { ""free"": true } }        },        {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [ 665, 727, 762, 639 ],            ""all_points_y"": [ 267, 251, 276, 318 ]          },          ""region_attributes"": { ""status"": { ""free"": true } }        },        {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [ 630, 705, 745, 605 ],            ""all_points_y"": [ 315, 290, 319, 366 ]          },          ""region_attributes"": { ""status"": { ""free"": true } }        }      ],      ""file_attributes"": {}    },    ""2013-01-18_07_45_03.jpg285736"": {      ""filename"": ""2013-01-18_07_45_03.jpg"",      ""size"": 285736,      ""regions"": [        {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [ 433, 471, 418, 389 ],            ""all_points_y"": [ 143, 174, 191, 163 ]          },          ""region_attributes"": { ""status"": { ""free"": true } }        },        {          ""shape_attributes"": {            ""name"": ""polygon"",            ""all_points_x"": [ 401, 439, 383, 359 ],            ""all_points_y"": [ 174, 205, 220, 194 ]          },          ""region_attributes"": { ""status"": { ""free"": true } }        },`  data fromat is shown above, and I tried to read the filename from JSON file directly, no errors, but it doesn't work in the train file."
"Hi I'm training my own dataset on coco pretrained weights. I'm getting this following error when I'm testing with my newly trained weights. Could someone please help me.    ValueError: Layer #389 (named ""mrcnn_bbox_fc""), weight   has shape (1024, 8), but the saved weight has shape (1024, 324).    Thanks."
"I cannot output the masks using vis_util.visualize_boxes_and_labels_on_image_array.  It seems that the size of the masks is different from the size of the image    this is the error :  ValueError: The image has spatial dimensions (480, 640) but the mask has dimensions (15, 15)    here is my code.. please correct it.         "
"On what is the resnet101 training on, when i use coco weights?  "
None
"It also consuming (System's) 4 GB of RAM while running with GPU but workes fine with the CPU.  Can you see what's going wrong!!       OS: windows 7  RAM : 6 GB  processor: i5 2nd generation  GPU: GTX 750 Ti 1GB  CUDA: 9.0  CuDNN: 7.0.5    ***here is the log*** :  Using TensorFlow backend.  number of frames 98 in file :  vid_1.avi  Starting processing in : S MODE  Loading weights  trained-models\model-b\mask_rcnn_sp_0020_17_03.h5  2019-04-10 17:13:53.870175: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructi  ons that this TensorFlow binary was not compiled to use: AVX  2019-04-10 17:13:53.982181: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\gpu\gpu_device.cc:1105] Found device 0 with pro  perties:  name: GeForce GTX 750 Ti major: 5 minor: 0 memoryClockRate(GHz): 1.15  pciBusID: 0000:01:00.0  totalMemory: 1.00GiB freeMemory: 903.19MiB    2019-04-10 17:13:53.985182: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\gpu\gpu_device.cc:1195] Creating TensorFlow dev  ice (/device:GPU:0) -> (device: 0, name: GeForce GTX 750 Ti, pci bus id: 0000:01  :00.0, compute capability: 5.0)  2019-04-10 17:10:54.218899: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 42 Chunks of size 512 tot  alling 21.0KiB  2019-04-10 17:10:54.218899: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 281 Chunks of size 1024 t  otalling 281.0KiB  2019-04-10 17:10:54.219900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 1 Chunks of size 1280 tot  alling 1.3KiB  2019-04-10 17:10:54.219900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 58 Chunks of size 2048 to  talling 116.0KiB  2019-04-10 17:10:54.219900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 132 Chunks of size 4096 t  otalling 528.0KiB  2019-04-10 17:10:54.220900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 1 Chunks of size 6144 tot  alling 6.0KiB  2019-04-10 17:10:54.220900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 23 Chunks of size 8192 to  talling 184.0KiB  2019-04-10 17:10:54.221900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 1 Chunks of size 12288 to  talling 12.0KiB  2019-04-10 17:10:54.221900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 2 Chunks of size 16384 to  talling 32.0KiB  2019-04-10 17:10:54.221900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 1 Chunks of size 24576 to  talling 24.0KiB  2019-04-10 17:10:54.222900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 1 Chunks of size 32768 to  talling 32.0KiB  2019-04-10 17:10:54.222900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 1 Chunks of size 37632 to  talling 36.8KiB  2019-04-10 17:10:54.222900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 1 Chunks of size 48384 to  talling 47.3KiB  2019-04-10 17:10:54.223900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 7 Chunks of size 65536 to  talling 448.0KiB  2019-04-10 17:10:54.223900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 1 Chunks of size 81920 to  talling 80.0KiB  2019-04-10 17:10:54.223900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 3 Chunks of size 131072 t  otalling 384.0KiB  2019-04-10 17:10:54.224900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 3 Chunks of size 147456 t  otalling 432.0KiB  2019-04-10 17:10:54.224900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 9 Chunks of size 262144 t  otalling 2.25MiB  2019-04-10 17:10:54.225900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 1 Chunks of size 393216 t  otalling 384.0KiB  2019-04-10 17:10:54.225900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 3 Chunks of size 524288 t  otalling 1.50MiB  2019-04-10 17:10:54.225900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 4 Chunks of size 589824 t  otalling 2.25MiB  2019-04-10 17:10:54.226900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 48 Chunks of size 1048576   totalling 48.00MiB  2019-04-10 17:10:54.226900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 1 Chunks of size 1228800  totalling 1.17MiB  2019-04-10 17:10:54.226900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 2 Chunks of size 2097152  totalling 4.00MiB  2019-04-10 17:10:54.227900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 29 Chunks of size 2359296   totalling 65.25MiB  2019-04-10 17:10:54.227900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 1 Chunks of size 2383872  totalling 2.27MiB  2019-04-10 17:10:54.227900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 1 Chunks of size 2883584  totalling 2.75MiB  2019-04-10 17:10:54.228900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 1 Chunks of size 3145728  totalling 3.00MiB  2019-04-10 17:10:54.228900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 1 Chunks of size 4190208  totalling 4.00MiB  2019-04-10 17:10:54.229900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 5 Chunks of size 4194304  totalling 20.00MiB  2019-04-10 17:10:54.229900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 2 Chunks of size 4718592  totalling 9.00MiB  2019-04-10 17:10:54.229900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 1 Chunks of size 5255168  totalling 5.01MiB  2019-04-10 17:10:54.230900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 1 Chunks of size 8257536  totalling 7.88MiB  2019-04-10 17:10:54.230900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 3 Chunks of size 9437184  totalling 27.00MiB  2019-04-10 17:10:54.231900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 1 Chunks of size 11534336   totalling 11.00MiB  2019-04-10 17:10:54.231900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 1 Chunks of size 17829888   totalling 17.00MiB  2019-04-10 17:10:54.231900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 1 Chunks of size 51380224   totalling 49.00MiB  2019-04-10 17:10:54.232900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 2 Chunks of size 67108864   totalling 128.00MiB  2019-04-10 17:10:54.232900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:680] 1 Chunks of size 13421772  8 totalling 128.00MiB  2019-04-10 17:10:54.233900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:684] Sum Total of in-use chunk  s: 541.34MiB  2019-04-10 17:10:54.233900: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:686] Stats:  Limit:                   668930048  InUse:                   567633152  MaxInUse:                659720448  NumAllocs:                    2702  MaxAllocSize:            245523456    2019-04-10 17:10:54.234900: W C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\common_runtime\bfc_allocator.cc:277] *************************  *******_********************************************************___________  2019-04-10 17:10:54.234900: W C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\3  6\tensorflow\core\framework\op_kernel.cc:1198] Resource exhausted: OOM when allo  cating tensor with shape ]]  Hint: If you want to see a list of allocated tensors when OOM happens, add repor  t_tensor_allocations_upon_oom to RunOptions for current allocation info.              ]]  Hint: If you want to see a list of allocated tensors when OOM happens, add repor  t_tensor_allocations_upon_oom to RunOptions for current allocation info.      During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""spa.py"", line 1254, in        startSPA()    File ""spa.py"", line 1239, in startSPA      sp_utils.initTracker(frames, sp, file)    File ""spa.py"", line 854, in initTracker      result = sp.predict( ]]  Hint: If you want to see a list of allocated tensors when OOM happens, add repor  t_tensor_allocations_upon_oom to RunOptions for current allocation info.              ]]  Hint: If you want to see a list of allocated tensors when OOM happens, add repor  t_tensor_allocations_upon_oom to RunOptions for current allocation info.      Caused by op 'rpn_model/rpn_conv_shared/convolution', defined at:    File ""spa.py"", line 1254, in        startSPA()    File ""spa.py"", line 1239, in startSPA      sp_utils.initTracker(frames, sp, file)    File ""spa.py"", line 854, in initTracker      result = sp.predict( ]]  Hint: If you want to see a list of allocated tensors when OOM happens, add repor  t_tensor_allocations_upon_oom to RunOptions for current allocation info.              ]]  Hint: If you want to see a list of allocated tensors when OOM happens, add repor  t_tensor_allocations_upon_oom to RunOptions for current allocation info.    !   "
"Hi all,    I have been trying to train the newtrok using COCO dataset and the pre-trained coco weights. As far as I understand it, with the default configuration we should be able to obtain a good AP. However, the results I have obtained so far are not that promising. The AP I obtained are lower than 0.01 and the different losses do not converge. Any idea of what could be causing this problem? Any idea or suggestion is appreciated.      Here you can see some of the graphs of the losses that I am obtaining:  !   !     "
"Hi,    I try to run balloon.py. However, I meet this error:    Has someone meet the similar thing? Thanks!    (Actually I re-run code on python3 and it's solved. Is it possible to run this code and train custom data on python2? Thanks!)      ERROR:root:Error processing image {'source': 'balloon', 'width': 2048, 'path': u'/users/drseh/Mask_RCNN-master/balloon/val/8053085540_a72bd21a64_k.jpg', 'height': 1595, 'id': u'8053085540_a72bd21a64_k.jpg', 'polygons': [{u'all_points_x': [1085, 1095, 1098, 1100, 1100, 1093, 1083, 1073, 1055, 1037, 1019, 1011, 1002, 998, 995, 991, 990, 993, 994, 994, 1003, 1010, 1017, 1023, 1028, 1028, 1025, 1019, 1024, 1035, 1049, 1061, 1073, 1085], u'all_points_y': [197, 212, 221, 233, 249, 266, 281, 290, 299, 304, 302, 300, 297, 296, 295, 295, 293, 292, 292, 289, 283, 274, 261, 250, 229, 215, 198, 189, 185, 182, 181, 183, 188, 197], u'name': u'polygon'}, {u'all_points_x': [981, 998, 1008, 1016, 1023, 1024, 1023, 1019, 1014, 1008, 997, 984, 977, 969, 963, 950, 943, 937, 929, 918, 911, 907, 906, 909, 917, 928, 938, 950, 963, 972, 981], u'all_points_y': [66, 74, 84, 97, 113, 133, 147, 158, 168, 177, 169, 165, 163, 163, 164, 167, 170, 174, 180, 168, 154, 138, 124, 107, 90, 76, 70, 66, 63, 63, 66], u'name': u'polygon'}, {u'all_points_x': [838, 849, 862, 874, 888, 901, 909, 916, 921, 927, 928, 928, 922, 912, 910, 913, 917, 920, 919, 907, 894, 883, 872, 861, 847, 841, 839, 832, 827, 824, 825, 829, 833, 838], u'all_points_y': [175, 167, 159, 157, 157, 161, 163, 168, 172, 178, 180, 182, 189, 205, 223, 242, 258, 266, 267, 256, 245, 241, 239, 239, 245, 252, 254, 245, 234, 218, 205, 191, 183, 175], u'name': u'polygon'}, {u'all_points_x': [917, 914, 911, 910, 913, 919, 926, 934, 946, 958, 973, 987, 999, 1012, 1023, 1029, 1027, 1023, 1015, 1007, 996, 986, 974, 968, 962, 960, 957, 954, 952, 953, 948, 939, 932, 927, 923, 920, 917], u'all_points_y': [259, 249, 234, 220, 205, 194, 185, 176, 169, 165, 163, 166, 170, 182, 194, 215, 234, 248, 264, 277, 287, 293, 297, 299, 299, 302, 305, 304, 301, 298, 294, 288, 283, 278, 272, 266, 259], u'name': u'polygon'}, {u'all_points_x': [900, 879, 857, 842, 821, 806, 797, 796, 799, 803, 808, 815, 825, 827, 829, 833, 838, 847, 861, 870, 885, 895, 909, 919, 925, 933, 941, 945, 946, 946, 944, 948, 949, 951, 951, 950, 949, 946, 944, 937, 929, 921, 908, 900], u'all_points_y': [373, 374, 371, 366, 358, 343, 327, 314, 307, 297, 291, 284, 277, 276, 270, 261, 253, 246, 240, 239, 242, 247, 257, 266, 275, 287, 302, 317, 329, 339, 344, 344, 343, 343, 347, 350, 351, 350, 349, 355, 362, 368, 372, 373], u'name': u'polygon'}, {u'all_points_x': [994, 984, 969, 954, 943, 932, 924, 932, 938, 941, 945, 949, 950, 950, 949, 947, 944, 946, 945, 941, 938, 931, 930, 940, 948, 953, 952, 954, 957, 962, 969, 980, 986, 992, 994, 994, 990, 990, 995, 997, 1006, 1009, 1010, 1008, 1006, 1000, 994], u'all_points_y': [352, 363, 372, 373, 373, 370, 366, 361, 356, 350, 349, 351, 347, 343, 342, 344, 344, 334, 318, 303, 295, 284, 280, 290, 294, 298, 301, 305, 304, 300, 299, 295, 292, 288, 290, 292, 293, 295, 296, 296, 298, 305, 319, 329, 338, 347, 352], u'name': u'polygon'}]}  Traceback (most recent call last):    File ""users/drseh/Mask_RCNN-master/mrcnn/model.py"", line 1715, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/users/drseh/Mask_RCNN-master/mrcnn/model.py"", line 1225, in load_image_gt      mode=config.IMAGE_RESIZE_MODE)    File ""/users/drseh/Mask_RCNN-master/mrcnn/utils.py"", line 449, in resize_image      preserve_range=True)    File ""/users/drseh/Mask_RCNN-master/mrcnn/utils.py"", line 905, in resize      anti_aliasing_sigma=anti_aliasing_sigma)    File ""/usr/lib64/python2.7/site-packages/skimage/transform/_warps.py"", line 165, in resize      tform.estimate(src_corners, dst_corners)    File ""/usr/lib64/python2.7/site-packages/skimage/transform/_geometric.py"", line 679, in estimate      _, _, V = np.linalg.svd(A)    File ""/usr/lib64/python2.7/site-packages/numpy/linalg/linalg.py"", line 1612, in svd      u, s, vh = gufunc(a, signature=signature, extobj=extobj)    File ""/usr/lib64/python2.7/site-packages/numpy/linalg/linalg.py"", line 106, in _raise_linalgerror_svd_nonconvergence      raise LinAlgError(""SVD did not converge"")  LinAlgError: SVD did not converge      "
"GTX1050:    Epoch 1/1  200/200 [==============================] - 490s 2s/step - loss: 1.8045 - rpn_class_loss: 0.0629 - rpn_bbox_loss: 0.4044 - mrcnn_class_loss: 0.3132 - mrcnn_bbox_loss: 0.5639 - mrcnn_mask_loss: 0.4601 - val_loss: 2.6684 - val_rpn_class_loss: 0.0712 - val_rpn_bbox_loss: 1.0801 - val_mrcnn_class_loss: 0.3276 - val_mrcnn_bbox_loss: 0.6830 - val_mrcnn_mask_loss: 0.5066  CPU times: user 12min 22s, sys: 29.9 s, total: 12min 51s  Wall time: 8min 40s    GTX TITAN X:    Epoch 1/1  200/200 [==============================] - 461s 2s/step - loss: 1.7415 - rpn_class_loss: 0.0592 - rpn_bbox_loss: 0.4563 - mrcnn_class_loss: 0.2490 - mrcnn_bbox_loss: 0.5253 - mrcnn_mask_loss: 0.4517 - val_loss: 1.9419 - val_rpn_class_loss: 0.0657 - val_rpn_bbox_loss: 0.6273 - val_mrcnn_class_loss: 0.2827 - val_mrcnn_bbox_loss: 0.5468 - val_mrcnn_mask_loss: 0.4194  CPU times: user 4min 34s, sys: 7.73 s, total: 4min 42s  Wall time: 10min 32s"
"Hi,    I have a question/problem regarding the compute_matches Function in utils.py (line 656)  Why do we sort the predictions from high to low?         This is already done in the DetectionLayer. In most cases, this is unnecessary but does not lead to an error.    But if there are two or more detections with the same score,    gives a different sorting.    For example, if  , then:      The order of   is changed and the output   computes matches for the **changed array**(!), and not for the original array. Since we do not return the changed error, the matching is wrong    I suppose, this will affect the AP calculation as well."
I'm using Mask_Rcnn to extract person(remove background). The problem is that the masking is not accurate enough to clearly isolate the person completely it generally includes the background or remove hair.   Is there a way to further train the training weights? or increase the accuracy of the mask?  I read on other articles and repositories where they make a custom model to isolate an object(e.g. balloon splash) but there's nothing out there to do this kind of thing.    Any help will be appreciated.  Thanks in advance.
"Hey folks,     I trained and tested a bunch of models with this implementation. Everything works smooth, I'm just wondering if I'm doing something wrong since the results I'm getting are not in line with the original paper. In particular, I'm talking about inference time:  - I'm getting about 5 FPS when performing batch inference, regardless of batch size (nVidia P100)  - Performance is even lower (~2 FPS) when performing inference from a live camera stream.     I should also mention that I changed the architecture backbone to resnet50.     Am I missing something obvious here? Did anyone experience the same issues?"
"Could anyone tell me what happens when:    1. I give --weights (which network is trained using these weights?)  2.  -- weights are not given   3. How is resnet trained?  4. How to find mean_pixel for my dataset, what does it actually mean?    Thanks in advance  "
"Hi, is it possible to give higher weight to specific training images during training? For example, I have 10 images for training. I want to give a higher weight to 3 images than the rest of the 7 images during training. So, that the training will be biased to the 3 images. Is it possible to do that?   If possible, how can I do that?"
"In the train shape demo, I set GPU_COUNT = 4 in the class ShapesConfig, and run the code on my 4-GPU machine. According to previous issues, I also set my Keras version to 2.1.3    After a while, the code was stuck at  `I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally`  and it didn't give any output.    Meanwhile, the process occupies 4 GPUs with almost full memory usage but with 0% GPU compute.  "
"Dimension 1 in both shapes must be equal, but are 12 and 8. Shapes are [1024,12] and [1024,8]. for 'Assign_2268' (op: 'Assign') with input shapes: [1024,12], [1024,8].  "
"Hi Guys,     I tested the implementation on the COCO dataset with the pre-trained weights, while I cannot get the state of art performance shown at   At this point, I am trying to figure out what is going wrong.. Is there any one who can achieve the expected performance with the implementation?"
"I need to significantly increase inference speed. Currently using a good GPU it takes around 470 ms per image, where images are of size 850 X 1200 or larger. Any way I can reduce this time?"
"I'm just a little confused, as far as I know, all the input images need initially to be resized at the same dimension (1024 * 1024, referred from `utils.py`) before processed by the framework. So why the time required to process different images with various size differs significantly? For example, I input three different dimensions of the same image (1920 * 1080, 960 * 540, and 480 * 270), the time usage is 1.42s, 0.95s, and 0.57s, respectively. Why is the time usage differs significantly? Don't all images need to be resized at the same dimension initially?"
"from the official coco website    Images  2014 Train images [83K/13GB]  2014 Val images [41K/6GB]  2014 Test images [41K/6GB]  2015 Test images [81K/12GB]  2017 Train images [118K/18GB]  2017 Val images [5K/1GB]  2017 Test images [41K/6GB]  2017 Unlabeled images [123K/19GB]  or   2014 Train/ValDetection 2015, Captioning 2015, Detection 2016, Keypoints 2016 2014 Testing Captioning 2015 2015 Testing Detection 2015, Detection 2016, Keypoints 2016 2017 Train/Val/Test   Detection 2017, Keypoints 2017, Stuff 2017,  Detection 2018, Keypoints 2018, Stuff 2018, Panoptic 2018 2017 Unlabeled[optional data for any competition]  which dataset should i download?    "
"I'm running Mask R-CNN in a Docker container with 6 Gbs of RAM (I can increase it if needed). I'm training on COCO with          and I get the following error in Epoch 1         There seems to be an issue with a Tensorflow optimizer. My configuration:         Can you help? Thanks    PS before the fatal error, I also get some warnings     "
"Certain use cases need to use python threads in order to effectively process large amounts of I/O (Network and Disk) without stalling. Several instances of the inference model seemingly can fit into the (single) GPU memory as per tests. (GPU has 11GB memory).  (After tweaking keras/tensorflow to not allocate 100% of free memory for each model).    However, when more than a single thread starts to use it's model instance, exceptions happen. See errors. My original approach of trying to pass a single model instance to the threads for inference also result in errors.    It's fairly clear that there is a design limitation here, I was hoping someone could shed light on how feasible it would be to resolve this. Since there is already parallel model support for the multiple GPU case, I was hoping it would be fairly quick to adapt to this multiple models on same GPU case..    The alternatives of using a single thread with a queue would introduce otherwise avoidable bottlenecking due to GIL..      Error Messages for the model per thread on a single GPU approach.      File ""/home/castleguard/.local/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3575, in _GroupControlDeps      return no_op(name=name)    File ""/usr/lib/python3.6/contextlib.py"", line 88, in __exit__      next(self.gen)    File ""/home/castleguard/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 4442, in device      new_top_of_stack = self._device_function_stack.peek_objs()[0]  IndexError: list index out of range         File ""/home/castleguard/.local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 548, in __exit__      c_api.TF_GetCode(self.status.status))  tensorflow.python.framework.errors_impl.NotFoundError: PruneForTargets: Some target nodes not found: group_deps     Exception ignored in:  >  Traceback (most recent call last):    File ""/home/castleguard/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1456, in __del__      self._session._session, self._handle, status)    File ""/home/castleguard/.local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 548, in __exit__      c_api.TF_GetCode(self.status.status))  tensorflow.python.framework.errors_impl.InvalidArgumentError: No such callable handle: 140264961352416"
"I used the following project to train on my own dataset and got a trained weight successfully.      However, when I used the weight to inference on my dataset, the results only contained the bounding box of detection but no mask of segmentation. Does anyone have any idea why it happened? Thanks a lot!"
"The implementation of adding losses in the repo is so strange. Does anyone explain some a bit of the following code, and help me with how to add a custom metrics in it?   "
"Hi everyone !   After many research without found answer for my question, I decide to post here :)  I want to use mask R-CNN to recognize trees in aerial pictures of 5000px\*5000px.  So I have polygons coordinates for all trees but I was constrained to split images in 1024px\*1024px (after annotations...).  Now some of polygons encroach in several images.  I want to know if I need to close polygons before image boundary, or if mask R-CNN is not sensible to that ? :)  Thank for your answers  I hope my english is understandable :D"
"from .../mrcnn/model.py       class MaskRCNN()          def build               about line 1900       _Hi, I am new to Keras and tensorflow.  Your tutorial is fantastic so I try to learn through your code. Does the upper code equals to :_             **if the code is not equal** then the feature map of C5 is upsampling stride 2 in P4, stride 4 in P3 and 8 in P2 because  P5 = conv1(C5)  P4 = up2(P5)+conv1(C4)  P3 = up2(P4)+conv1(C3)  = up2[ up2(P5) +conv1(C4) ]+conv1(C3)   = up4(P5)+up2[ conv1(C4) ]+conv1(c3)    and so on    then P4 = conv3(P4)  P3 = conv3(P3)    cause you dont apply conv  on P4 so the high semantic feature map is down through upsample.       **if the code is equal** then it will be   P5 = conv1(C5)  P4 = up2(P5)+conv1(C4)  P4 = conv3(P4) = conv3(up2(P5)+conv1(C4))  P3 = up2(P4)+conv1(C3)        = up2{ conv3[ up2(P5)+conv1(C4) ] }+conv1(C3)      `"
"when I use the model to train on my  dataset, the model cann't  output 'accuracy' and only 'loss'.And I add 'metrics=['accuracy'] to the 'self.keras_model.compile' that locates in model.py , it also failed.Wish you answerï¼Ÿ"
"Hi, has anyone tried to train the network from scratch? (Without using any of the pre-trained weights provided)    I am trying to do in and, in order to do so, I commented the lines where the weights are loaded. However, I am always getting AP = 0 and AR = 0 for both bounding boxes and masks. Is there something I am doing wrong? and which training schedule would be suitable to do it?    Here are the results I have obtained so far    Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000   Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000   Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000   Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000   Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000   Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000   Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000   Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000   Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000  Prediction time: 110.37733197212219. Average 0.2207546639442444/image  Total time:  118.04929542541504  Running COCO evaluation on 500 images for annotation type *segm*...  Loading and preparing results...  DONE (t=0.00s)  creating index...  index created!  Running per image evaluation...  Evaluate annotation type *segm*  DONE (t=0.96s).  Accumulating evaluation results...  DONE (t=0.34s).   Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000   Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000   Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000   Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000   Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000   Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000   Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000   Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000   Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000  Prediction time: 103.53374075889587. Average 0.20706748151779175/image  Total time:  110.01924180984497"
"I would like to obtain a N x F, where N is the number of detected objects and F are the features after the ROIAlign.  Any suggestions?"
"Hi,     I'm training coco.py with my own dataset and I always get this error around 5-6 epoch. I am just using 1 image as a training data and 1 image as a validation data to check if this works.     Thanks in advance.    Epoch 5/40  50/50 [==============================] - 238s 5s/step - loss: 0.3808 - rpn_class_loss: 0.0215 - rpn_bbox_loss: 0.1410 - mrcnn_class_loss: 0.0560 - mrcnn_bbox_loss: 0.0264 - mrcnn_mask_loss: 0.1360 - val_loss: 7.0040 - val_rpn_class_loss: 1.1675 - val_rpn_bbox_loss: 3.6190 - val_mrcnn_class_loss: 1.0810 - val_mrcnn_bbox_loss: 0.5159 - val_mrcnn_mask_loss: 0.6206  Epoch 6/40  50/50 [==============================] - 239s 5s/step - loss: 0.3499 - rpn_class_loss: 0.0152 - rpn_bbox_loss: 0.1267 - mrcnn_class_loss: 0.0593 - mrcnn_bbox_loss: 0.0266 - mrcnn_mask_loss: 0.1221 - val_loss: 7.1096 - val_rpn_class_loss: 1.3373 - val_rpn_bbox_loss: 3.4969 - val_mrcnn_class_loss: 1.0811 - val_mrcnn_bbox_loss: 0.5628 - val_mrcnn_mask_loss: 0.6316  ERROR:root:Error processing image {'id': 1, 'source': 'coco', 'path': '/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/tio/train2017/_0P04ZWQtMtPMwx3lgLdWA.jpg', 'width': 2592, 'height': 1936, 'annotations': [{'id': 1, 'image_id': 1, 'category_id': 9, 'iscrowd': 0, 'area': 2153, 'bbox': [1198.0, 397.0, 38.0, 78.0], 'segmentation': [[1208.0, 474.5, 1197.5, 403.0, 1235.0, 396.5, 1229.5, 423.0, 1235.5, 467.0, 1208.0, 474.5]], 'width': 2592, 'height': 1936}, {'id': 2, 'image_id': 1, 'category_id': 12, 'iscrowd': 0, 'area': 1767, 'bbox': [1240.0, 862.0, 39.0, 56.0], 'segmentation': [[1244.0, 917.5, 1239.5, 889.0, 1240.5, 868.0, 1272.0, 861.5, 1278.5, 914.0, 1244.0, 917.5]], 'width': 2592, 'height': 1936}, {'id': 3, 'image_id': 1, 'category_id': 30, 'iscrowd': 0, 'area': 23079, 'bbox': [0.0, 1012.0, 217.0, 190.0], 'segmentation': [[0.0, 1201.5, 0.0, 1011.5, 13.0, 1041.5, 137.0, 1038.5, 204.5, 1055.0, 216.0, 1136.5, 134.0, 1136.5, 55.0, 1155.5, 0.0, 1201.5]], 'width': 2592, 'height': 1936}, {'id': 4, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 13454, 'bbox': [285.0, 1117.0, 137.0, 129.0], 'segmentation': [[289.0, 1245.5, 293.5, 1208.0, 290.5, 1177.0, 284.5, 1167.0, 293.5, 1161.0, 316.5, 1129.0, 366.0, 1117.5, 412.0, 1116.5, 421.5, 1216.0, 404.0, 1219.5, 400.5, 1233.0, 391.0, 1236.5, 385.0, 1235.5, 375.0, 1224.5, 371.0, 1224.5, 362.0, 1226.5, 352.0, 1238.5, 345.0, 1239.5, 334.0, 1229.5, 327.0, 1229.5, 306.0, 1232.5, 299.0, 1244.5, 289.0, 1245.5]], 'width': 2592, 'height': 1936}, {'id': 5, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 34185, 'bbox': [0.0, 1136.0, 294.0, 164.0], 'segmentation': [[9.0, 1299.5, 0, 1299.0, 0, 1202.0, 55.0, 1155.5, 134.0, 1136.5, 218.0, 1135.5, 284.0, 1166.5, 290.5, 1177.0, 293.5, 1216.0, 289.5, 1240.0, 283.5, 1251.0, 274.0, 1257.5, 250.0, 1257.5, 243.0, 1246.5, 238.0, 1246.5, 167.0, 1257.5, 168.5, 1266.0, 163.0, 1275.5, 135.0, 1283.5, 125.0, 1278.5, 121.0, 1272.5, 15.0, 1281.5, 15.5, 1293.0, 9.0, 1299.5]], 'width': 2592, 'height': 1936}, {'id': 6, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 14170, 'bbox': [2474.0, 902.0, 118.0, 181.0], 'segmentation': [[2538.0, 1082.5, 2507.0, 1079.5, 2486.0, 1061.5, 2481.5, 1055.0, 2473.5, 1021.0, 2541.5, 934.0, 2591.0, 901.5, 2591.5, 1067.0, 2538.0, 1082.5]], 'width': 2592, 'height': 1936}, {'id': 7, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 49052, 'bbox': [840.0, 1024.0, 296.0, 219.0], 'segmentation': [[956.0, 1242.5, 943.0, 1242.5, 927.5, 1238.0, 906.5, 1039.0, 946.0, 1030.5, 1038.0, 1023.5, 1115.0, 1025.5, 1121.5, 1042.0, 1133.5, 1137.0, 1135.5, 1193.0, 1124.5, 1199.0, 1114.0, 1216.5, 1102.0, 1220.5, 1077.0, 1218.5, 1066.0, 1203.5, 986.0, 1211.5, 980.5, 1234.0, 956.0, 1242.5], [874.0, 1230.5, 863.0, 1230.5, 851.5, 1220.0, 845.5, 1171.0, 854.5, 1117.0, 840.5, 1106.0, 839.5, 1099.0, 845.0, 1088.5, 851.0, 1088.5, 859.5, 1094.0, 857.5, 1081.0, 865.5, 1056.0, 881.0, 1042.5, 888.0, 1041.5, 905.5, 1209.0, 887.0, 1209.5, 882.5, 1227.0, 874.0, 1230.5]], 'width': 2592, 'height': 1936}, {'id': 8, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 38963, 'bbox': [1049.0, 1201.0, 725.0, 90.0], 'segmentation': [[1669.0, 1269.5, 1665.5, 1264.0, 1661.0, 1207.5, 1758.5, 1201.0, 1770.5, 1227.0, 1773.0, 1263.5, 1669.0, 1269.5], [1103.0, 1289.5, 1050.5, 1289.0, 1159.0, 1231.5, 1636.0, 1208.5, 1639.5, 1212.0, 1641.0, 1270.5, 1376.0, 1284.5, 1103.0, 1289.5]], 'width': 2592, 'height': 1936}, {'id': 9, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 383152, 'bbox': [984.0, 1373.0, 1608.0, 453.0], 'segmentation': [[2186.0, 1825.5, 2111.5, 1807.0, 2105.5, 1706.0, 2109.5, 1681.0, 2104.5, 1643.0, 2096.5, 1636.0, 2104.5, 1611.0, 2082.0, 1580.5, 2052.0, 1571.5, 2027.0, 1580.5, 2016.5, 1600.0, 2023.5, 1619.0, 2027.5, 1687.0, 2034.5, 1706.0, 2039.0, 1790.5, 1629.0, 1693.5, 1214.0, 1606.5, 983.5, 1572.0, 1954.0, 1372.5, 2335.0, 1399.5, 2327.5, 1414.0, 2327.5, 1429.0, 2336.5, 1440.0, 2329.5, 1443.0, 2329.5, 1472.0, 2337.0, 1472.5, 2338.5, 1479.0, 2361.5, 1751.0, 2186.0, 1825.5], [2409.0, 1731.5, 2385.5, 1481.0, 2386.0, 1475.5, 2394.5, 1473.0, 2393.5, 1442.0, 2385.5, 1440.0, 2388.5, 1438.0, 2390.5, 1422.0, 2388.5, 1412.0, 2380.5, 1403.0, 2437.0, 1405.5, 2591.0, 1427.5, 2591.5, 1650.0, 2506.0, 1690.5, 2409.0, 1731.5]], 'width': 2592, 'height': 1936}, {'id': 10, 'image_id': 1, 'category_id': 8, 'iscrowd': 0, 'area': 52969, 'bbox': [0.0, 1370.0, 566.0, 193.0], 'segmentation': [[476.0, 1407.5, 472.5, 1405.0, 472.0, 1377.5, 513.0, 1372.5, 563.5, 1371.0, 476.0, 1407.5], [24.0, 1562.5, 0, 1562.0, 0.0, 1399.5, 436.5, 1381.0, 440.0, 1422.5, 110.0, 1556.5, 24.0, 1562.5]], 'width': 2592, 'height': 1936}, {'id': 11, 'image_id': 1, 'category_id': 4, 'iscrowd': 0, 'area': 4182, 'bbox': [1694.0, 917.0, 41.0, 173.0], 'segmentation': [[1712.0, 1089.5, 1711.5, 1027.0, 1707.0, 1019.5, 1698.0, 1022.5, 1693.5, 1010.0, 1693.5, 960.0, 1705.0, 944.5, 1712.0, 953.5, 1728.5, 953.0, 1724.5, 921.0, 1728.0, 916.5, 1734.5, 1088.0, 1712.0, 1089.5]], 'width': 2592, 'height': 1936}, {'id': 12, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8979, 'bbox': [866.0, 253.0, 209.0, 772.0], 'segmentation': [[1052.0, 889.5, 1047.5, 886.0, 997.5, 376.0, 983.5, 272.0, 875.0, 269.5, 866.0, 262.5, 990.0, 252.5, 991.5, 255.0, 1063.5, 882.0, 1063.0, 887.5, 1052.0, 889.5], [1068.0, 971.5, 1068.0, 966.5, 1068.0, 971.5], [1057.0, 976.5, 1057.0, 967.5, 1057.0, 976.5], [1069.0, 981.5, 1069.0, 974.5, 1069.0, 981.5], [1074.0, 1024.5, 1061.5, 1024.0, 1058.0, 976.5, 1060.5, 997.0, 1070.0, 997.5, 1071.5, 994.0, 1074.0, 1024.5], [1070.0, 991.5, 1070.0, 984.5, 1070.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 13, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 527, 'bbox': [1067.0, 951.0, 15.0, 75.0], 'segmentation': [[1081.0, 1025.5, 1075.0, 1024.5, 1073.5, 1021.0, 1067.0, 950.5, 1075.0, 950.5, 1076.5, 956.0, 1073.5, 957.0, 1073.5, 961.0, 1081.0, 1025.5]], 'width': 2592, 'height': 1936}, {'id': 14, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 27156, 'bbox': [2328.0, 1395.0, 100.0, 541.0], 'segmentation': [[2427.0, 1935.5, 2378.0, 1935.5, 2376.5, 1929.0, 2338.5, 1479.0, 2337.0, 1472.5, 2329.5, 1472.0, 2329.5, 1443.0, 2336.5, 1440.0, 2327.5, 1429.0, 2327.5, 1414.0, 2339.0, 1395.5, 2360.0, 1394.5, 2378.0, 1400.5, 2388.5, 1412.0, 2390.5, 1422.0, 2388.5, 1438.0, 2385.5, 1440.0, 2393.5, 1442.0, 2394.5, 1473.0, 2386.0, 1475.5, 2385.5, 1481.0, 2427.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 15, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 6705, 'bbox': [877.0, 954.0, 59.0, 356.0], 'segmentation': [[920.0, 1309.5, 910.5, 1309.0, 913.5, 1288.0, 894.5, 1086.0, 876.5, 960.0, 877.0, 956.5, 899.0, 953.5, 914.5, 1131.0, 935.5, 1301.0, 935.0, 1305.5, 920.0, 1309.5]], 'width': 2592, 'height': 1936}, {'id': 16, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 1177, 'bbox': [617.0, 1014.0, 23.0, 146.0], 'segmentation': [[639.0, 1159.5, 630.0, 1159.5, 628.5, 1153.0, 616.5, 1022.0, 616.5, 1017.0, 622.0, 1013.5, 639.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 17, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 379, 'bbox': [869.0, 882.0, 36.0, 47.0], 'segmentation': [[878.0, 928.5, 872.5, 928.0, 868.5, 882.0, 904.0, 881.5, 875.5, 887.0, 878.0, 928.5]], 'width': 2592, 'height': 1936}, {'id': 18, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 3889, 'bbox': [1728.0, 893.0, 31.0, 267.0], 'segmentation': [[1746.0, 1159.5, 1736.5, 1159.0, 1728.0, 892.5, 1740.5, 896.0, 1740.5, 911.0, 1732.5, 930.0, 1731.5, 957.0, 1738.5, 978.0, 1747.5, 991.0, 1758.5, 1131.0, 1758.5, 1153.0, 1746.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 19, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 9692, 'bbox': [1745.0, 882.0, 61.0, 412.0], 'segmentation': [[1792.0, 1293.5, 1771.0, 1290.5, 1768.5, 1282.0, 1769.0, 1279.5, 1774.5, 1279.0, 1774.5, 1274.0, 1744.5, 890.0, 1745.0, 883.5, 1761.0, 881.5, 1767.5, 884.0, 1796.5, 1270.0, 1797.5, 1277.0, 1805.5, 1282.0, 1805.0, 1290.5, 1792.0, 1293.5]], 'width': 2592, 'height': 1936}, {'id': 20, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8559, 'bbox': [1616.0, 949.0, 64.0, 376.0], 'segmentation': [[1660.0, 1324.5, 1637.5, 1324.0, 1637.5, 1313.0, 1643.5, 1308.0, 1643.5, 1298.0, 1638.5, 1196.0, 1627.5, 1085.0, 1636.0, 1089.5, 1636.5, 1087.0, 1629.5, 1056.0, 1624.5, 1055.0, 1615.5, 959.0, 1616.0, 952.5, 1638.0, 948.5, 1668.5, 1297.0, 1669.5, 1303.0, 1677.5, 1308.0, 1679.5, 1319.0, 1679.0, 1322.5, 1660.0, 1324.5]], 'width': 2592, 'height': 1936}, {'id': 21, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 26362, 'bbox': [2017.0, 1572.0, 103.0, 364.0], 'segmentation': [[2119.0, 1935.5, 2048.5, 1935.0, 2034.5, 1706.0, 2027.5, 1687.0, 2023.5, 1619.0, 2016.5, 1600.0, 2027.0, 1580.5, 2052.0, 1571.5, 2082.0, 1580.5, 2104.5, 1611.0, 2096.5, 1636.0, 2104.5, 1643.0, 2109.5, 1681.0, 2105.5, 1706.0, 2119.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 22, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 18515, 'bbox': [402.0, 982.0, 92.0, 520.0], 'segmentation': [[434.0, 1501.5, 433.5, 1489.0, 445.5, 1480.0, 445.5, 1475.0, 401.5, 993.0, 402.0, 985.5, 431.0, 981.5, 436.5, 982.0, 473.5, 1417.0, 479.5, 1431.0, 483.5, 1475.0, 493.5, 1480.0, 493.0, 1496.5, 434.0, 1501.5]], 'width': 2592, 'height': 1936}, {'id': 23, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 421, 'bbox': [1007.0, 926.0, 12.0, 65.0], 'segmentation': [[1018.0, 990.5, 1011.5, 990.0, 1007.0, 925.5, 1012.5, 926.0, 1018.0, 990.5]], 'width': 2592, 'height': 1936}, {'id': 24, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 1107, 'bbox': [798.0, 259.0, 77.0, 20.0], 'segmentation': [[798.0, 278.5, 797.5, 263.0, 804.0, 258.5, 859.0, 258.5, 874.5, 269.0, 798.0, 278.5]], 'width': 2592, 'height': 1936}, {'id': 25, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 73, 'bbox': [904.0, 877.0, 17.0, 8.0], 'segmentation': [[904.0, 884.5, 907.0, 878.5, 917.0, 876.5, 920.5, 882.0, 904.0, 884.5]], 'width': 2592, 'height': 1936}, {'id': 26, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2731, 'bbox': [868.0, 928.0, 53.0, 90.0], 'segmentation': [[876.0, 1017.5, 867.5, 952.0, 867.5, 929.0, 918.0, 927.5, 914.5, 978.0, 920.5, 1012.0, 919.0, 1014.5, 904.5, 1015.0, 899.0, 953.5, 876.5, 957.0, 884.5, 1016.0, 876.0, 1017.5]], 'width': 2592, 'height': 1936}, {'id': 27, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 12971, 'bbox': [331.0, 822.0, 116.0, 166.0], 'segmentation': [[390.0, 987.5, 381.5, 985.0, 378.5, 964.0, 355.5, 950.0, 334.5, 911.0, 373.0, 906.5, 373.5, 901.0, 372.5, 895.0, 358.5, 876.0, 332.5, 856.0, 330.5, 837.0, 363.0, 836.5, 372.0, 829.5, 394.0, 830.5, 396.0, 824.5, 419.0, 821.5, 421.5, 831.0, 431.0, 831.5, 432.5, 837.0, 446.5, 980.0, 390.0, 987.5]], 'width': 2592, 'height': 1936}, {'id': 28, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 6689, 'bbox': [1599.0, 835.0, 73.0, 120.0], 'segmentation': [[1609.0, 954.5, 1598.5, 846.0, 1608.0, 837.5, 1618.0, 834.5, 1625.0, 834.5, 1628.0, 841.5, 1658.0, 836.5, 1666.0, 838.5, 1667.5, 846.0, 1655.5, 890.0, 1667.0, 890.5, 1671.5, 896.0, 1659.5, 940.0, 1634.0, 950.5, 1609.0, 954.5]], 'width': 2592, 'height': 1936}, {'id': 29, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2285, 'bbox': [1682.0, 880.0, 47.0, 74.0], 'segmentation': [[1728.0, 953.5, 1712.0, 953.5, 1704.0, 942.5, 1695.5, 942.0, 1684.5, 917.0, 1691.5, 912.0, 1682.0, 888.5, 1717.0, 883.5, 1720.5, 880.0, 1728.0, 953.5]], 'width': 2592, 'height': 1936}, {'id': 30, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 16074, 'bbox': [1709.0, 676.0, 104.0, 211.0], 'segmentation': [[1725.0, 886.5, 1708.5, 689.0, 1776.0, 675.5, 1782.5, 680.0, 1786.5, 717.0, 1802.5, 723.0, 1803.5, 739.0, 1790.0, 743.5, 1789.5, 753.0, 1790.5, 762.0, 1806.5, 773.0, 1807.5, 787.0, 1792.5, 788.0, 1792.5, 808.0, 1812.5, 819.0, 1805.5, 840.0, 1796.5, 842.0, 1795.0, 878.5, 1725.0, 886.5]], 'width': 2592, 'height': 1936}, {'id': 31, 'image_id': 1, 'category_id': 25, 'iscrowd': 0, 'area': 4717, 'bbox': [1732.0, 882.0, 83.0, 115.0], 'segmentation': [[1780.0, 996.5, 1775.5, 996.0, 1774.5, 986.0, 1767.5, 896.0, 1767.5, 883.0, 1770.0, 881.5, 1783.0, 888.5, 1800.5, 904.0, 1809.5, 926.0, 1814.5, 949.0, 1808.5, 974.0, 1790.0, 994.5, 1780.0, 996.5], [1751.0, 991.5, 1739.5, 980.0, 1731.5, 957.0, 1732.5, 930.0, 1745.0, 902.5, 1751.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 32, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 4552, 'bbox': [1023.0, 885.0, 69.0, 85.0], 'segmentation': [[1031.0, 969.5, 1022.5, 893.0, 1085.0, 884.5, 1091.5, 947.0, 1067.0, 950.5, 1067.0, 965.5, 1031.0, 969.5], [1076.0, 966.5, 1074.0, 956.5, 1076.5, 957.0, 1076.0, 966.5]], 'width': 2592, 'height': 1936}, {'id': 33, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 3513, 'bbox': [1076.0, 948.0, 63.0, 60.0], 'segmentation': [[1080.0, 1007.5, 1075.5, 948.0, 1135.0, 947.5, 1138.5, 1006.0, 1080.0, 1007.5]], 'width': 2592, 'height': 1936}, {'id': 34, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 329, 'bbox': [1058.0, 966.0, 13.0, 32.0], 'segmentation': [[1070.0, 997.5, 1060.5, 997.0, 1058.0, 965.5, 1067.5, 966.0, 1070.0, 997.5]], 'width': 2592, 'height': 1936}]}  Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_plugins/pil_plugin.py"", line 53, in pil_to_ndarray      im.getdata()[0]    File ""/usr/local/lib/python3.6/dist-packages/PIL/Image.py"", line 1165, in getdata      self.load()    File ""/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py"", line 212, in load      s = read(self.decodermaxblock)  OSError: [Errno 5] Input/output error    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1709, in data_generator    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1211, in load_image_gt    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/utils.py"", line 358, in load_image    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py"", line 61, in imread      img = call_plugin('imread', fname, plugin=plugin, **plugin_args)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/manage_plugins.py"", line 211, in call_plugin      return func(*args, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_plugins/pil_plugin.py"", line 37, in imread      return pil_to_ndarray(im, dtype=dtype, img_num=img_num)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_plugins/pil_plugin.py"", line 61, in pil_to_ndarray      raise ValueError(error_message)  ValueError: Could not load """"   Reason: ""[Errno 5] Input/output error""  Please see documentation at:    ERROR:root:Error processing image {'id': 1, 'source': 'coco', 'path': '/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/tio/train2017/_0P04ZWQtMtPMwx3lgLdWA.jpg', 'width': 2592, 'height': 1936, 'annotations': [{'id': 1, 'image_id': 1, 'category_id': 9, 'iscrowd': 0, 'area': 2153, 'bbox': [1198.0, 397.0, 38.0, 78.0], 'segmentation': [[1208.0, 474.5, 1197.5, 403.0, 1235.0, 396.5, 1229.5, 423.0, 1235.5, 467.0, 1208.0, 474.5]], 'width': 2592, 'height': 1936}, {'id': 2, 'image_id': 1, 'category_id': 12, 'iscrowd': 0, 'area': 1767, 'bbox': [1240.0, 862.0, 39.0, 56.0], 'segmentation': [[1244.0, 917.5, 1239.5, 889.0, 1240.5, 868.0, 1272.0, 861.5, 1278.5, 914.0, 1244.0, 917.5]], 'width': 2592, 'height': 1936}, {'id': 3, 'image_id': 1, 'category_id': 30, 'iscrowd': 0, 'area': 23079, 'bbox': [0.0, 1012.0, 217.0, 190.0], 'segmentation': [[0.0, 1201.5, 0.0, 1011.5, 13.0, 1041.5, 137.0, 1038.5, 204.5, 1055.0, 216.0, 1136.5, 134.0, 1136.5, 55.0, 1155.5, 0.0, 1201.5]], 'width': 2592, 'height': 1936}, {'id': 4, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 13454, 'bbox': [285.0, 1117.0, 137.0, 129.0], 'segmentation': [[289.0, 1245.5, 293.5, 1208.0, 290.5, 1177.0, 284.5, 1167.0, 293.5, 1161.0, 316.5, 1129.0, 366.0, 1117.5, 412.0, 1116.5, 421.5, 1216.0, 404.0, 1219.5, 400.5, 1233.0, 391.0, 1236.5, 385.0, 1235.5, 375.0, 1224.5, 371.0, 1224.5, 362.0, 1226.5, 352.0, 1238.5, 345.0, 1239.5, 334.0, 1229.5, 327.0, 1229.5, 306.0, 1232.5, 299.0, 1244.5, 289.0, 1245.5]], 'width': 2592, 'height': 1936}, {'id': 5, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 34185, 'bbox': [0.0, 1136.0, 294.0, 164.0], 'segmentation': [[9.0, 1299.5, 0, 1299.0, 0, 1202.0, 55.0, 1155.5, 134.0, 1136.5, 218.0, 1135.5, 284.0, 1166.5, 290.5, 1177.0, 293.5, 1216.0, 289.5, 1240.0, 283.5, 1251.0, 274.0, 1257.5, 250.0, 1257.5, 243.0, 1246.5, 238.0, 1246.5, 167.0, 1257.5, 168.5, 1266.0, 163.0, 1275.5, 135.0, 1283.5, 125.0, 1278.5, 121.0, 1272.5, 15.0, 1281.5, 15.5, 1293.0, 9.0, 1299.5]], 'width': 2592, 'height': 1936}, {'id': 6, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 14170, 'bbox': [2474.0, 902.0, 118.0, 181.0], 'segmentation': [[2538.0, 1082.5, 2507.0, 1079.5, 2486.0, 1061.5, 2481.5, 1055.0, 2473.5, 1021.0, 2541.5, 934.0, 2591.0, 901.5, 2591.5, 1067.0, 2538.0, 1082.5]], 'width': 2592, 'height': 1936}, {'id': 7, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 49052, 'bbox': [840.0, 1024.0, 296.0, 219.0], 'segmentation': [[956.0, 1242.5, 943.0, 1242.5, 927.5, 1238.0, 906.5, 1039.0, 946.0, 1030.5, 1038.0, 1023.5, 1115.0, 1025.5, 1121.5, 1042.0, 1133.5, 1137.0, 1135.5, 1193.0, 1124.5, 1199.0, 1114.0, 1216.5, 1102.0, 1220.5, 1077.0, 1218.5, 1066.0, 1203.5, 986.0, 1211.5, 980.5, 1234.0, 956.0, 1242.5], [874.0, 1230.5, 863.0, 1230.5, 851.5, 1220.0, 845.5, 1171.0, 854.5, 1117.0, 840.5, 1106.0, 839.5, 1099.0, 845.0, 1088.5, 851.0, 1088.5, 859.5, 1094.0, 857.5, 1081.0, 865.5, 1056.0, 881.0, 1042.5, 888.0, 1041.5, 905.5, 1209.0, 887.0, 1209.5, 882.5, 1227.0, 874.0, 1230.5]], 'width': 2592, 'height': 1936}, {'id': 8, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 38963, 'bbox': [1049.0, 1201.0, 725.0, 90.0], 'segmentation': [[1669.0, 1269.5, 1665.5, 1264.0, 1661.0, 1207.5, 1758.5, 1201.0, 1770.5, 1227.0, 1773.0, 1263.5, 1669.0, 1269.5], [1103.0, 1289.5, 1050.5, 1289.0, 1159.0, 1231.5, 1636.0, 1208.5, 1639.5, 1212.0, 1641.0, 1270.5, 1376.0, 1284.5, 1103.0, 1289.5]], 'width': 2592, 'height': 1936}, {'id': 9, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 383152, 'bbox': [984.0, 1373.0, 1608.0, 453.0], 'segmentation': [[2186.0, 1825.5, 2111.5, 1807.0, 2105.5, 1706.0, 2109.5, 1681.0, 2104.5, 1643.0, 2096.5, 1636.0, 2104.5, 1611.0, 2082.0, 1580.5, 2052.0, 1571.5, 2027.0, 1580.5, 2016.5, 1600.0, 2023.5, 1619.0, 2027.5, 1687.0, 2034.5, 1706.0, 2039.0, 1790.5, 1629.0, 1693.5, 1214.0, 1606.5, 983.5, 1572.0, 1954.0, 1372.5, 2335.0, 1399.5, 2327.5, 1414.0, 2327.5, 1429.0, 2336.5, 1440.0, 2329.5, 1443.0, 2329.5, 1472.0, 2337.0, 1472.5, 2338.5, 1479.0, 2361.5, 1751.0, 2186.0, 1825.5], [2409.0, 1731.5, 2385.5, 1481.0, 2386.0, 1475.5, 2394.5, 1473.0, 2393.5, 1442.0, 2385.5, 1440.0, 2388.5, 1438.0, 2390.5, 1422.0, 2388.5, 1412.0, 2380.5, 1403.0, 2437.0, 1405.5, 2591.0, 1427.5, 2591.5, 1650.0, 2506.0, 1690.5, 2409.0, 1731.5]], 'width': 2592, 'height': 1936}, {'id': 10, 'image_id': 1, 'category_id': 8, 'iscrowd': 0, 'area': 52969, 'bbox': [0.0, 1370.0, 566.0, 193.0], 'segmentation': [[476.0, 1407.5, 472.5, 1405.0, 472.0, 1377.5, 513.0, 1372.5, 563.5, 1371.0, 476.0, 1407.5], [24.0, 1562.5, 0, 1562.0, 0.0, 1399.5, 436.5, 1381.0, 440.0, 1422.5, 110.0, 1556.5, 24.0, 1562.5]], 'width': 2592, 'height': 1936}, {'id': 11, 'image_id': 1, 'category_id': 4, 'iscrowd': 0, 'area': 4182, 'bbox': [1694.0, 917.0, 41.0, 173.0], 'segmentation': [[1712.0, 1089.5, 1711.5, 1027.0, 1707.0, 1019.5, 1698.0, 1022.5, 1693.5, 1010.0, 1693.5, 960.0, 1705.0, 944.5, 1712.0, 953.5, 1728.5, 953.0, 1724.5, 921.0, 1728.0, 916.5, 1734.5, 1088.0, 1712.0, 1089.5]], 'width': 2592, 'height': 1936}, {'id': 12, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8979, 'bbox': [866.0, 253.0, 209.0, 772.0], 'segmentation': [[1052.0, 889.5, 1047.5, 886.0, 997.5, 376.0, 983.5, 272.0, 875.0, 269.5, 866.0, 262.5, 990.0, 252.5, 991.5, 255.0, 1063.5, 882.0, 1063.0, 887.5, 1052.0, 889.5], [1068.0, 971.5, 1068.0, 966.5, 1068.0, 971.5], [1057.0, 976.5, 1057.0, 967.5, 1057.0, 976.5], [1069.0, 981.5, 1069.0, 974.5, 1069.0, 981.5], [1074.0, 1024.5, 1061.5, 1024.0, 1058.0, 976.5, 1060.5, 997.0, 1070.0, 997.5, 1071.5, 994.0, 1074.0, 1024.5], [1070.0, 991.5, 1070.0, 984.5, 1070.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 13, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 527, 'bbox': [1067.0, 951.0, 15.0, 75.0], 'segmentation': [[1081.0, 1025.5, 1075.0, 1024.5, 1073.5, 1021.0, 1067.0, 950.5, 1075.0, 950.5, 1076.5, 956.0, 1073.5, 957.0, 1073.5, 961.0, 1081.0, 1025.5]], 'width': 2592, 'height': 1936}, {'id': 14, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 27156, 'bbox': [2328.0, 1395.0, 100.0, 541.0], 'segmentation': [[2427.0, 1935.5, 2378.0, 1935.5, 2376.5, 1929.0, 2338.5, 1479.0, 2337.0, 1472.5, 2329.5, 1472.0, 2329.5, 1443.0, 2336.5, 1440.0, 2327.5, 1429.0, 2327.5, 1414.0, 2339.0, 1395.5, 2360.0, 1394.5, 2378.0, 1400.5, 2388.5, 1412.0, 2390.5, 1422.0, 2388.5, 1438.0, 2385.5, 1440.0, 2393.5, 1442.0, 2394.5, 1473.0, 2386.0, 1475.5, 2385.5, 1481.0, 2427.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 15, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 6705, 'bbox': [877.0, 954.0, 59.0, 356.0], 'segmentation': [[920.0, 1309.5, 910.5, 1309.0, 913.5, 1288.0, 894.5, 1086.0, 876.5, 960.0, 877.0, 956.5, 899.0, 953.5, 914.5, 1131.0, 935.5, 1301.0, 935.0, 1305.5, 920.0, 1309.5]], 'width': 2592, 'height': 1936}, {'id': 16, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 1177, 'bbox': [617.0, 1014.0, 23.0, 146.0], 'segmentation': [[639.0, 1159.5, 630.0, 1159.5, 628.5, 1153.0, 616.5, 1022.0, 616.5, 1017.0, 622.0, 1013.5, 639.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 17, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 379, 'bbox': [869.0, 882.0, 36.0, 47.0], 'segmentation': [[878.0, 928.5, 872.5, 928.0, 868.5, 882.0, 904.0, 881.5, 875.5, 887.0, 878.0, 928.5]], 'width': 2592, 'height': 1936}, {'id': 18, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 3889, 'bbox': [1728.0, 893.0, 31.0, 267.0], 'segmentation': [[1746.0, 1159.5, 1736.5, 1159.0, 1728.0, 892.5, 1740.5, 896.0, 1740.5, 911.0, 1732.5, 930.0, 1731.5, 957.0, 1738.5, 978.0, 1747.5, 991.0, 1758.5, 1131.0, 1758.5, 1153.0, 1746.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 19, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 9692, 'bbox': [1745.0, 882.0, 61.0, 412.0], 'segmentation': [[1792.0, 1293.5, 1771.0, 1290.5, 1768.5, 1282.0, 1769.0, 1279.5, 1774.5, 1279.0, 1774.5, 1274.0, 1744.5, 890.0, 1745.0, 883.5, 1761.0, 881.5, 1767.5, 884.0, 1796.5, 1270.0, 1797.5, 1277.0, 1805.5, 1282.0, 1805.0, 1290.5, 1792.0, 1293.5]], 'width': 2592, 'height': 1936}, {'id': 20, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8559, 'bbox': [1616.0, 949.0, 64.0, 376.0], 'segmentation': [[1660.0, 1324.5, 1637.5, 1324.0, 1637.5, 1313.0, 1643.5, 1308.0, 1643.5, 1298.0, 1638.5, 1196.0, 1627.5, 1085.0, 1636.0, 1089.5, 1636.5, 1087.0, 1629.5, 1056.0, 1624.5, 1055.0, 1615.5, 959.0, 1616.0, 952.5, 1638.0, 948.5, 1668.5, 1297.0, 1669.5, 1303.0, 1677.5, 1308.0, 1679.5, 1319.0, 1679.0, 1322.5, 1660.0, 1324.5]], 'width': 2592, 'height': 1936}, {'id': 21, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 26362, 'bbox': [2017.0, 1572.0, 103.0, 364.0], 'segmentation': [[2119.0, 1935.5, 2048.5, 1935.0, 2034.5, 1706.0, 2027.5, 1687.0, 2023.5, 1619.0, 2016.5, 1600.0, 2027.0, 1580.5, 2052.0, 1571.5, 2082.0, 1580.5, 2104.5, 1611.0, 2096.5, 1636.0, 2104.5, 1643.0, 2109.5, 1681.0, 2105.5, 1706.0, 2119.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 22, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 18515, 'bbox': [402.0, 982.0, 92.0, 520.0], 'segmentation': [[434.0, 1501.5, 433.5, 1489.0, 445.5, 1480.0, 445.5, 1475.0, 401.5, 993.0, 402.0, 985.5, 431.0, 981.5, 436.5, 982.0, 473.5, 1417.0, 479.5, 1431.0, 483.5, 1475.0, 493.5, 1480.0, 493.0, 1496.5, 434.0, 1501.5]], 'width': 2592, 'height': 1936}, {'id': 23, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 421, 'bbox': [1007.0, 926.0, 12.0, 65.0], 'segmentation': [[1018.0, 990.5, 1011.5, 990.0, 1007.0, 925.5, 1012.5, 926.0, 1018.0, 990.5]], 'width': 2592, 'height': 1936}, {'id': 24, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 1107, 'bbox': [798.0, 259.0, 77.0, 20.0], 'segmentation': [[798.0, 278.5, 797.5, 263.0, 804.0, 258.5, 859.0, 258.5, 874.5, 269.0, 798.0, 278.5]], 'width': 2592, 'height': 1936}, {'id': 25, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 73, 'bbox': [904.0, 877.0, 17.0, 8.0], 'segmentation': [[904.0, 884.5, 907.0, 878.5, 917.0, 876.5, 920.5, 882.0, 904.0, 884.5]], 'width': 2592, 'height': 1936}, {'id': 26, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2731, 'bbox': [868.0, 928.0, 53.0, 90.0], 'segmentation': [[876.0, 1017.5, 867.5, 952.0, 867.5, 929.0, 918.0, 927.5, 914.5, 978.0, 920.5, 1012.0, 919.0, 1014.5, 904.5, 1015.0, 899.0, 953.5, 876.5, 957.0, 884.5, 1016.0, 876.0, 1017.5]], 'width': 2592, 'height': 1936}, {'id': 27, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 12971, 'bbox': [331.0, 822.0, 116.0, 166.0], 'segmentation': [[390.0, 987.5, 381.5, 985.0, 378.5, 964.0, 355.5, 950.0, 334.5, 911.0, 373.0, 906.5, 373.5, 901.0, 372.5, 895.0, 358.5, 876.0, 332.5, 856.0, 330.5, 837.0, 363.0, 836.5, 372.0, 829.5, 394.0, 830.5, 396.0, 824.5, 419.0, 821.5, 421.5, 831.0, 431.0, 831.5, 432.5, 837.0, 446.5, 980.0, 390.0, 987.5]], 'width': 2592, 'height': 1936}, {'id': 28, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 6689, 'bbox': [1599.0, 835.0, 73.0, 120.0], 'segmentation': [[1609.0, 954.5, 1598.5, 846.0, 1608.0, 837.5, 1618.0, 834.5, 1625.0, 834.5, 1628.0, 841.5, 1658.0, 836.5, 1666.0, 838.5, 1667.5, 846.0, 1655.5, 890.0, 1667.0, 890.5, 1671.5, 896.0, 1659.5, 940.0, 1634.0, 950.5, 1609.0, 954.5]], 'width': 2592, 'height': 1936}, {'id': 29, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2285, 'bbox': [1682.0, 880.0, 47.0, 74.0], 'segmentation': [[1728.0, 953.5, 1712.0, 953.5, 1704.0, 942.5, 1695.5, 942.0, 1684.5, 917.0, 1691.5, 912.0, 1682.0, 888.5, 1717.0, 883.5, 1720.5, 880.0, 1728.0, 953.5]], 'width': 2592, 'height': 1936}, {'id': 30, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 16074, 'bbox': [1709.0, 676.0, 104.0, 211.0], 'segmentation': [[1725.0, 886.5, 1708.5, 689.0, 1776.0, 675.5, 1782.5, 680.0, 1786.5, 717.0, 1802.5, 723.0, 1803.5, 739.0, 1790.0, 743.5, 1789.5, 753.0, 1790.5, 762.0, 1806.5, 773.0, 1807.5, 787.0, 1792.5, 788.0, 1792.5, 808.0, 1812.5, 819.0, 1805.5, 840.0, 1796.5, 842.0, 1795.0, 878.5, 1725.0, 886.5]], 'width': 2592, 'height': 1936}, {'id': 31, 'image_id': 1, 'category_id': 25, 'iscrowd': 0, 'area': 4717, 'bbox': [1732.0, 882.0, 83.0, 115.0], 'segmentation': [[1780.0, 996.5, 1775.5, 996.0, 1774.5, 986.0, 1767.5, 896.0, 1767.5, 883.0, 1770.0, 881.5, 1783.0, 888.5, 1800.5, 904.0, 1809.5, 926.0, 1814.5, 949.0, 1808.5, 974.0, 1790.0, 994.5, 1780.0, 996.5], [1751.0, 991.5, 1739.5, 980.0, 1731.5, 957.0, 1732.5, 930.0, 1745.0, 902.5, 1751.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 32, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 4552, 'bbox': [1023.0, 885.0, 69.0, 85.0], 'segmentation': [[1031.0, 969.5, 1022.5, 893.0, 1085.0, 884.5, 1091.5, 947.0, 1067.0, 950.5, 1067.0, 965.5, 1031.0, 969.5], [1076.0, 966.5, 1074.0, 956.5, 1076.5, 957.0, 1076.0, 966.5]], 'width': 2592, 'height': 1936}, {'id': 33, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 3513, 'bbox': [1076.0, 948.0, 63.0, 60.0], 'segmentation': [[1080.0, 1007.5, 1075.5, 948.0, 1135.0, 947.5, 1138.5, 1006.0, 1080.0, 1007.5]], 'width': 2592, 'height': 1936}, {'id': 34, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 329, 'bbox': [1058.0, 966.0, 13.0, 32.0], 'segmentation': [[1070.0, 997.5, 1060.5, 997.0, 1058.0, 965.5, 1067.5, 966.0, 1070.0, 997.5]], 'width': 2592, 'height': 1936}]}  Traceback (most recent call last):    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1709, in data_generator    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1211, in load_image_gt    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/utils.py"", line 358, in load_image    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py"", line 61, in imread      img = call_plugin('imread', fname, plugin=plugin, **plugin_args)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/manage_plugins.py"", line 211, in call_plugin      return func(*args, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_plugins/pil_plugin.py"", line 36, in imread      im = Image.open(f)    File ""/usr/local/lib/python3.6/dist-packages/PIL/Image.py"", line 2419, in open      prefix = fp.read(16)  OSError: [Errno 5] Input/output error  ERROR:root:Error processing image {'id': 1, 'source': 'coco', 'path': '/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/tio/train2017/_0P04ZWQtMtPMwx3lgLdWA.jpg', 'width': 2592, 'height': 1936, 'annotations': [{'id': 1, 'image_id': 1, 'category_id': 9, 'iscrowd': 0, 'area': 2153, 'bbox': [1198.0, 397.0, 38.0, 78.0], 'segmentation': [[1208.0, 474.5, 1197.5, 403.0, 1235.0, 396.5, 1229.5, 423.0, 1235.5, 467.0, 1208.0, 474.5]], 'width': 2592, 'height': 1936}, {'id': 2, 'image_id': 1, 'category_id': 12, 'iscrowd': 0, 'area': 1767, 'bbox': [1240.0, 862.0, 39.0, 56.0], 'segmentation': [[1244.0, 917.5, 1239.5, 889.0, 1240.5, 868.0, 1272.0, 861.5, 1278.5, 914.0, 1244.0, 917.5]], 'width': 2592, 'height': 1936}, {'id': 3, 'image_id': 1, 'category_id': 30, 'iscrowd': 0, 'area': 23079, 'bbox': [0.0, 1012.0, 217.0, 190.0], 'segmentation': [[0.0, 1201.5, 0.0, 1011.5, 13.0, 1041.5, 137.0, 1038.5, 204.5, 1055.0, 216.0, 1136.5, 134.0, 1136.5, 55.0, 1155.5, 0.0, 1201.5]], 'width': 2592, 'height': 1936}, {'id': 4, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 13454, 'bbox': [285.0, 1117.0, 137.0, 129.0], 'segmentation': [[289.0, 1245.5, 293.5, 1208.0, 290.5, 1177.0, 284.5, 1167.0, 293.5, 1161.0, 316.5, 1129.0, 366.0, 1117.5, 412.0, 1116.5, 421.5, 1216.0, 404.0, 1219.5, 400.5, 1233.0, 391.0, 1236.5, 385.0, 1235.5, 375.0, 1224.5, 371.0, 1224.5, 362.0, 1226.5, 352.0, 1238.5, 345.0, 1239.5, 334.0, 1229.5, 327.0, 1229.5, 306.0, 1232.5, 299.0, 1244.5, 289.0, 1245.5]], 'width': 2592, 'height': 1936}, {'id': 5, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 34185, 'bbox': [0.0, 1136.0, 294.0, 164.0], 'segmentation': [[9.0, 1299.5, 0, 1299.0, 0, 1202.0, 55.0, 1155.5, 134.0, 1136.5, 218.0, 1135.5, 284.0, 1166.5, 290.5, 1177.0, 293.5, 1216.0, 289.5, 1240.0, 283.5, 1251.0, 274.0, 1257.5, 250.0, 1257.5, 243.0, 1246.5, 238.0, 1246.5, 167.0, 1257.5, 168.5, 1266.0, 163.0, 1275.5, 135.0, 1283.5, 125.0, 1278.5, 121.0, 1272.5, 15.0, 1281.5, 15.5, 1293.0, 9.0, 1299.5]], 'width': 2592, 'height': 1936}, {'id': 6, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 14170, 'bbox': [2474.0, 902.0, 118.0, 181.0], 'segmentation': [[2538.0, 1082.5, 2507.0, 1079.5, 2486.0, 1061.5, 2481.5, 1055.0, 2473.5, 1021.0, 2541.5, 934.0, 2591.0, 901.5, 2591.5, 1067.0, 2538.0, 1082.5]], 'width': 2592, 'height': 1936}, {'id': 7, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 49052, 'bbox': [840.0, 1024.0, 296.0, 219.0], 'segmentation': [[956.0, 1242.5, 943.0, 1242.5, 927.5, 1238.0, 906.5, 1039.0, 946.0, 1030.5, 1038.0, 1023.5, 1115.0, 1025.5, 1121.5, 1042.0, 1133.5, 1137.0, 1135.5, 1193.0, 1124.5, 1199.0, 1114.0, 1216.5, 1102.0, 1220.5, 1077.0, 1218.5, 1066.0, 1203.5, 986.0, 1211.5, 980.5, 1234.0, 956.0, 1242.5], [874.0, 1230.5, 863.0, 1230.5, 851.5, 1220.0, 845.5, 1171.0, 854.5, 1117.0, 840.5, 1106.0, 839.5, 1099.0, 845.0, 1088.5, 851.0, 1088.5, 859.5, 1094.0, 857.5, 1081.0, 865.5, 1056.0, 881.0, 1042.5, 888.0, 1041.5, 905.5, 1209.0, 887.0, 1209.5, 882.5, 1227.0, 874.0, 1230.5]], 'width': 2592, 'height': 1936}, {'id': 8, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 38963, 'bbox': [1049.0, 1201.0, 725.0, 90.0], 'segmentation': [[1669.0, 1269.5, 1665.5, 1264.0, 1661.0, 1207.5, 1758.5, 1201.0, 1770.5, 1227.0, 1773.0, 1263.5, 1669.0, 1269.5], [1103.0, 1289.5, 1050.5, 1289.0, 1159.0, 1231.5, 1636.0, 1208.5, 1639.5, 1212.0, 1641.0, 1270.5, 1376.0, 1284.5, 1103.0, 1289.5]], 'width': 2592, 'height': 1936}, {'id': 9, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 383152, 'bbox': [984.0, 1373.0, 1608.0, 453.0], 'segmentation': [[2186.0, 1825.5, 2111.5, 1807.0, 2105.5, 1706.0, 2109.5, 1681.0, 2104.5, 1643.0, 2096.5, 1636.0, 2104.5, 1611.0, 2082.0, 1580.5, 2052.0, 1571.5, 2027.0, 1580.5, 2016.5, 1600.0, 2023.5, 1619.0, 2027.5, 1687.0, 2034.5, 1706.0, 2039.0, 1790.5, 1629.0, 1693.5, 1214.0, 1606.5, 983.5, 1572.0, 1954.0, 1372.5, 2335.0, 1399.5, 2327.5, 1414.0, 2327.5, 1429.0, 2336.5, 1440.0, 2329.5, 1443.0, 2329.5, 1472.0, 2337.0, 1472.5, 2338.5, 1479.0, 2361.5, 1751.0, 2186.0, 1825.5], [2409.0, 1731.5, 2385.5, 1481.0, 2386.0, 1475.5, 2394.5, 1473.0, 2393.5, 1442.0, 2385.5, 1440.0, 2388.5, 1438.0, 2390.5, 1422.0, 2388.5, 1412.0, 2380.5, 1403.0, 2437.0, 1405.5, 2591.0, 1427.5, 2591.5, 1650.0, 2506.0, 1690.5, 2409.0, 1731.5]], 'width': 2592, 'height': 1936}, {'id': 10, 'image_id': 1, 'category_id': 8, 'iscrowd': 0, 'area': 52969, 'bbox': [0.0, 1370.0, 566.0, 193.0], 'segmentation': [[476.0, 1407.5, 472.5, 1405.0, 472.0, 1377.5, 513.0, 1372.5, 563.5, 1371.0, 476.0, 1407.5], [24.0, 1562.5, 0, 1562.0, 0.0, 1399.5, 436.5, 1381.0, 440.0, 1422.5, 110.0, 1556.5, 24.0, 1562.5]], 'width': 2592, 'height': 1936}, {'id': 11, 'image_id': 1, 'category_id': 4, 'iscrowd': 0, 'area': 4182, 'bbox': [1694.0, 917.0, 41.0, 173.0], 'segmentation': [[1712.0, 1089.5, 1711.5, 1027.0, 1707.0, 1019.5, 1698.0, 1022.5, 1693.5, 1010.0, 1693.5, 960.0, 1705.0, 944.5, 1712.0, 953.5, 1728.5, 953.0, 1724.5, 921.0, 1728.0, 916.5, 1734.5, 1088.0, 1712.0, 1089.5]], 'width': 2592, 'height': 1936}, {'id': 12, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8979, 'bbox': [866.0, 253.0, 209.0, 772.0], 'segmentation': [[1052.0, 889.5, 1047.5, 886.0, 997.5, 376.0, 983.5, 272.0, 875.0, 269.5, 866.0, 262.5, 990.0, 252.5, 991.5, 255.0, 1063.5, 882.0, 1063.0, 887.5, 1052.0, 889.5], [1068.0, 971.5, 1068.0, 966.5, 1068.0, 971.5], [1057.0, 976.5, 1057.0, 967.5, 1057.0, 976.5], [1069.0, 981.5, 1069.0, 974.5, 1069.0, 981.5], [1074.0, 1024.5, 1061.5, 1024.0, 1058.0, 976.5, 1060.5, 997.0, 1070.0, 997.5, 1071.5, 994.0, 1074.0, 1024.5], [1070.0, 991.5, 1070.0, 984.5, 1070.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 13, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 527, 'bbox': [1067.0, 951.0, 15.0, 75.0], 'segmentation': [[1081.0, 1025.5, 1075.0, 1024.5, 1073.5, 1021.0, 1067.0, 950.5, 1075.0, 950.5, 1076.5, 956.0, 1073.5, 957.0, 1073.5, 961.0, 1081.0, 1025.5]], 'width': 2592, 'height': 1936}, {'id': 14, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 27156, 'bbox': [2328.0, 1395.0, 100.0, 541.0], 'segmentation': [[2427.0, 1935.5, 2378.0, 1935.5, 2376.5, 1929.0, 2338.5, 1479.0, 2337.0, 1472.5, 2329.5, 1472.0, 2329.5, 1443.0, 2336.5, 1440.0, 2327.5, 1429.0, 2327.5, 1414.0, 2339.0, 1395.5, 2360.0, 1394.5, 2378.0, 1400.5, 2388.5, 1412.0, 2390.5, 1422.0, 2388.5, 1438.0, 2385.5, 1440.0, 2393.5, 1442.0, 2394.5, 1473.0, 2386.0, 1475.5, 2385.5, 1481.0, 2427.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 15, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 6705, 'bbox': [877.0, 954.0, 59.0, 356.0], 'segmentation': [[920.0, 1309.5, 910.5, 1309.0, 913.5, 1288.0, 894.5, 1086.0, 876.5, 960.0, 877.0, 956.5, 899.0, 953.5, 914.5, 1131.0, 935.5, 1301.0, 935.0, 1305.5, 920.0, 1309.5]], 'width': 2592, 'height': 1936}, {'id': 16, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 1177, 'bbox': [617.0, 1014.0, 23.0, 146.0], 'segmentation': [[639.0, 1159.5, 630.0, 1159.5, 628.5, 1153.0, 616.5, 1022.0, 616.5, 1017.0, 622.0, 1013.5, 639.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 17, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 379, 'bbox': [869.0, 882.0, 36.0, 47.0], 'segmentation': [[878.0, 928.5, 872.5, 928.0, 868.5, 882.0, 904.0, 881.5, 875.5, 887.0, 878.0, 928.5]], 'width': 2592, 'height': 1936}, {'id': 18, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 3889, 'bbox': [1728.0, 893.0, 31.0, 267.0], 'segmentation': [[1746.0, 1159.5, 1736.5, 1159.0, 1728.0, 892.5, 1740.5, 896.0, 1740.5, 911.0, 1732.5, 930.0, 1731.5, 957.0, 1738.5, 978.0, 1747.5, 991.0, 1758.5, 1131.0, 1758.5, 1153.0, 1746.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 19, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 9692, 'bbox': [1745.0, 882.0, 61.0, 412.0], 'segmentation': [[1792.0, 1293.5, 1771.0, 1290.5, 1768.5, 1282.0, 1769.0, 1279.5, 1774.5, 1279.0, 1774.5, 1274.0, 1744.5, 890.0, 1745.0, 883.5, 1761.0, 881.5, 1767.5, 884.0, 1796.5, 1270.0, 1797.5, 1277.0, 1805.5, 1282.0, 1805.0, 1290.5, 1792.0, 1293.5]], 'width': 2592, 'height': 1936}, {'id': 20, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8559, 'bbox': [1616.0, 949.0, 64.0, 376.0], 'segmentation': [[1660.0, 1324.5, 1637.5, 1324.0, 1637.5, 1313.0, 1643.5, 1308.0, 1643.5, 1298.0, 1638.5, 1196.0, 1627.5, 1085.0, 1636.0, 1089.5, 1636.5, 1087.0, 1629.5, 1056.0, 1624.5, 1055.0, 1615.5, 959.0, 1616.0, 952.5, 1638.0, 948.5, 1668.5, 1297.0, 1669.5, 1303.0, 1677.5, 1308.0, 1679.5, 1319.0, 1679.0, 1322.5, 1660.0, 1324.5]], 'width': 2592, 'height': 1936}, {'id': 21, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 26362, 'bbox': [2017.0, 1572.0, 103.0, 364.0], 'segmentation': [[2119.0, 1935.5, 2048.5, 1935.0, 2034.5, 1706.0, 2027.5, 1687.0, 2023.5, 1619.0, 2016.5, 1600.0, 2027.0, 1580.5, 2052.0, 1571.5, 2082.0, 1580.5, 2104.5, 1611.0, 2096.5, 1636.0, 2104.5, 1643.0, 2109.5, 1681.0, 2105.5, 1706.0, 2119.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 22, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 18515, 'bbox': [402.0, 982.0, 92.0, 520.0], 'segmentation': [[434.0, 1501.5, 433.5, 1489.0, 445.5, 1480.0, 445.5, 1475.0, 401.5, 993.0, 402.0, 985.5, 431.0, 981.5, 436.5, 982.0, 473.5, 1417.0, 479.5, 1431.0, 483.5, 1475.0, 493.5, 1480.0, 493.0, 1496.5, 434.0, 1501.5]], 'width': 2592, 'height': 1936}, {'id': 23, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 421, 'bbox': [1007.0, 926.0, 12.0, 65.0], 'segmentation': [[1018.0, 990.5, 1011.5, 990.0, 1007.0, 925.5, 1012.5, 926.0, 1018.0, 990.5]], 'width': 2592, 'height': 1936}, {'id': 24, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 1107, 'bbox': [798.0, 259.0, 77.0, 20.0], 'segmentation': [[798.0, 278.5, 797.5, 263.0, 804.0, 258.5, 859.0, 258.5, 874.5, 269.0, 798.0, 278.5]], 'width': 2592, 'height': 1936}, {'id': 25, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 73, 'bbox': [904.0, 877.0, 17.0, 8.0], 'segmentation': [[904.0, 884.5, 907.0, 878.5, 917.0, 876.5, 920.5, 882.0, 904.0, 884.5]], 'width': 2592, 'height': 1936}, {'id': 26, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2731, 'bbox': [868.0, 928.0, 53.0, 90.0], 'segmentation': [[876.0, 1017.5, 867.5, 952.0, 867.5, 929.0, 918.0, 927.5, 914.5, 978.0, 920.5, 1012.0, 919.0, 1014.5, 904.5, 1015.0, 899.0, 953.5, 876.5, 957.0, 884.5, 1016.0, 876.0, 1017.5]], 'width': 2592, 'height': 1936}, {'id': 27, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 12971, 'bbox': [331.0, 822.0, 116.0, 166.0], 'segmentation': [[390.0, 987.5, 381.5, 985.0, 378.5, 964.0, 355.5, 950.0, 334.5, 911.0, 373.0, 906.5, 373.5, 901.0, 372.5, 895.0, 358.5, 876.0, 332.5, 856.0, 330.5, 837.0, 363.0, 836.5, 372.0, 829.5, 394.0, 830.5, 396.0, 824.5, 419.0, 821.5, 421.5, 831.0, 431.0, 831.5, 432.5, 837.0, 446.5, 980.0, 390.0, 987.5]], 'width': 2592, 'height': 1936}, {'id': 28, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 6689, 'bbox': [1599.0, 835.0, 73.0, 120.0], 'segmentation': [[1609.0, 954.5, 1598.5, 846.0, 1608.0, 837.5, 1618.0, 834.5, 1625.0, 834.5, 1628.0, 841.5, 1658.0, 836.5, 1666.0, 838.5, 1667.5, 846.0, 1655.5, 890.0, 1667.0, 890.5, 1671.5, 896.0, 1659.5, 940.0, 1634.0, 950.5, 1609.0, 954.5]], 'width': 2592, 'height': 1936}, {'id': 29, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2285, 'bbox': [1682.0, 880.0, 47.0, 74.0], 'segmentation': [[1728.0, 953.5, 1712.0, 953.5, 1704.0, 942.5, 1695.5, 942.0, 1684.5, 917.0, 1691.5, 912.0, 1682.0, 888.5, 1717.0, 883.5, 1720.5, 880.0, 1728.0, 953.5]], 'width': 2592, 'height': 1936}, {'id': 30, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 16074, 'bbox': [1709.0, 676.0, 104.0, 211.0], 'segmentation': [[1725.0, 886.5, 1708.5, 689.0, 1776.0, 675.5, 1782.5, 680.0, 1786.5, 717.0, 1802.5, 723.0, 1803.5, 739.0, 1790.0, 743.5, 1789.5, 753.0, 1790.5, 762.0, 1806.5, 773.0, 1807.5, 787.0, 1792.5, 788.0, 1792.5, 808.0, 1812.5, 819.0, 1805.5, 840.0, 1796.5, 842.0, 1795.0, 878.5, 1725.0, 886.5]], 'width': 2592, 'height': 1936}, {'id': 31, 'image_id': 1, 'category_id': 25, 'iscrowd': 0, 'area': 4717, 'bbox': [1732.0, 882.0, 83.0, 115.0], 'segmentation': [[1780.0, 996.5, 1775.5, 996.0, 1774.5, 986.0, 1767.5, 896.0, 1767.5, 883.0, 1770.0, 881.5, 1783.0, 888.5, 1800.5, 904.0, 1809.5, 926.0, 1814.5, 949.0, 1808.5, 974.0, 1790.0, 994.5, 1780.0, 996.5], [1751.0, 991.5, 1739.5, 980.0, 1731.5, 957.0, 1732.5, 930.0, 1745.0, 902.5, 1751.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 32, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 4552, 'bbox': [1023.0, 885.0, 69.0, 85.0], 'segmentation': [[1031.0, 969.5, 1022.5, 893.0, 1085.0, 884.5, 1091.5, 947.0, 1067.0, 950.5, 1067.0, 965.5, 1031.0, 969.5], [1076.0, 966.5, 1074.0, 956.5, 1076.5, 957.0, 1076.0, 966.5]], 'width': 2592, 'height': 1936}, {'id': 33, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 3513, 'bbox': [1076.0, 948.0, 63.0, 60.0], 'segmentation': [[1080.0, 1007.5, 1075.5, 948.0, 1135.0, 947.5, 1138.5, 1006.0, 1080.0, 1007.5]], 'width': 2592, 'height': 1936}, {'id': 34, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 329, 'bbox': [1058.0, 966.0, 13.0, 32.0], 'segmentation': [[1070.0, 997.5, 1060.5, 997.0, 1058.0, 965.5, 1067.5, 966.0, 1070.0, 997.5]], 'width': 2592, 'height': 1936}]}  Traceback (most recent call last):    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1709, in data_generator    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1211, in load_image_gt    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/utils.py"", line 358, in load_image    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py"", line 61, in imread      img = call_plugin('imread', fname, plugin=plugin, **plugin_args)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/manage_plugins.py"", line 211, in call_plugin      return func(*args, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_plugins/pil_plugin.py"", line 36, in imread      im = Image.open(f)    File ""/usr/local/lib/python3.6/dist-packages/PIL/Image.py"", line 2419, in open      prefix = fp.read(16)  OSError: [Errno 5] Input/output error  ERROR:root:Error processing image {'id': 1, 'source': 'coco', 'path': '/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/tio/train2017/_0P04ZWQtMtPMwx3lgLdWA.jpg', 'width': 2592, 'height': 1936, 'annotations': [{'id': 1, 'image_id': 1, 'category_id': 9, 'iscrowd': 0, 'area': 2153, 'bbox': [1198.0, 397.0, 38.0, 78.0], 'segmentation': [[1208.0, 474.5, 1197.5, 403.0, 1235.0, 396.5, 1229.5, 423.0, 1235.5, 467.0, 1208.0, 474.5]], 'width': 2592, 'height': 1936}, {'id': 2, 'image_id': 1, 'category_id': 12, 'iscrowd': 0, 'area': 1767, 'bbox': [1240.0, 862.0, 39.0, 56.0], 'segmentation': [[1244.0, 917.5, 1239.5, 889.0, 1240.5, 868.0, 1272.0, 861.5, 1278.5, 914.0, 1244.0, 917.5]], 'width': 2592, 'height': 1936}, {'id': 3, 'image_id': 1, 'category_id': 30, 'iscrowd': 0, 'area': 23079, 'bbox': [0.0, 1012.0, 217.0, 190.0], 'segmentation': [[0.0, 1201.5, 0.0, 1011.5, 13.0, 1041.5, 137.0, 1038.5, 204.5, 1055.0, 216.0, 1136.5, 134.0, 1136.5, 55.0, 1155.5, 0.0, 1201.5]], 'width': 2592, 'height': 1936}, {'id': 4, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 13454, 'bbox': [285.0, 1117.0, 137.0, 129.0], 'segmentation': [[289.0, 1245.5, 293.5, 1208.0, 290.5, 1177.0, 284.5, 1167.0, 293.5, 1161.0, 316.5, 1129.0, 366.0, 1117.5, 412.0, 1116.5, 421.5, 1216.0, 404.0, 1219.5, 400.5, 1233.0, 391.0, 1236.5, 385.0, 1235.5, 375.0, 1224.5, 371.0, 1224.5, 362.0, 1226.5, 352.0, 1238.5, 345.0, 1239.5, 334.0, 1229.5, 327.0, 1229.5, 306.0, 1232.5, 299.0, 1244.5, 289.0, 1245.5]], 'width': 2592, 'height': 1936}, {'id': 5, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 34185, 'bbox': [0.0, 1136.0, 294.0, 164.0], 'segmentation': [[9.0, 1299.5, 0, 1299.0, 0, 1202.0, 55.0, 1155.5, 134.0, 1136.5, 218.0, 1135.5, 284.0, 1166.5, 290.5, 1177.0, 293.5, 1216.0, 289.5, 1240.0, 283.5, 1251.0, 274.0, 1257.5, 250.0, 1257.5, 243.0, 1246.5, 238.0, 1246.5, 167.0, 1257.5, 168.5, 1266.0, 163.0, 1275.5, 135.0, 1283.5, 125.0, 1278.5, 121.0, 1272.5, 15.0, 1281.5, 15.5, 1293.0, 9.0, 1299.5]], 'width': 2592, 'height': 1936}, {'id': 6, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 14170, 'bbox': [2474.0, 902.0, 118.0, 181.0], 'segmentation': [[2538.0, 1082.5, 2507.0, 1079.5, 2486.0, 1061.5, 2481.5, 1055.0, 2473.5, 1021.0, 2541.5, 934.0, 2591.0, 901.5, 2591.5, 1067.0, 2538.0, 1082.5]], 'width': 2592, 'height': 1936}, {'id': 7, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 49052, 'bbox': [840.0, 1024.0, 296.0, 219.0], 'segmentation': [[956.0, 1242.5, 943.0, 1242.5, 927.5, 1238.0, 906.5, 1039.0, 946.0, 1030.5, 1038.0, 1023.5, 1115.0, 1025.5, 1121.5, 1042.0, 1133.5, 1137.0, 1135.5, 1193.0, 1124.5, 1199.0, 1114.0, 1216.5, 1102.0, 1220.5, 1077.0, 1218.5, 1066.0, 1203.5, 986.0, 1211.5, 980.5, 1234.0, 956.0, 1242.5], [874.0, 1230.5, 863.0, 1230.5, 851.5, 1220.0, 845.5, 1171.0, 854.5, 1117.0, 840.5, 1106.0, 839.5, 1099.0, 845.0, 1088.5, 851.0, 1088.5, 859.5, 1094.0, 857.5, 1081.0, 865.5, 1056.0, 881.0, 1042.5, 888.0, 1041.5, 905.5, 1209.0, 887.0, 1209.5, 882.5, 1227.0, 874.0, 1230.5]], 'width': 2592, 'height': 1936}, {'id': 8, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 38963, 'bbox': [1049.0, 1201.0, 725.0, 90.0], 'segmentation': [[1669.0, 1269.5, 1665.5, 1264.0, 1661.0, 1207.5, 1758.5, 1201.0, 1770.5, 1227.0, 1773.0, 1263.5, 1669.0, 1269.5], [1103.0, 1289.5, 1050.5, 1289.0, 1159.0, 1231.5, 1636.0, 1208.5, 1639.5, 1212.0, 1641.0, 1270.5, 1376.0, 1284.5, 1103.0, 1289.5]], 'width': 2592, 'height': 1936}, {'id': 9, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 383152, 'bbox': [984.0, 1373.0, 1608.0, 453.0], 'segmentation': [[2186.0, 1825.5, 2111.5, 1807.0, 2105.5, 1706.0, 2109.5, 1681.0, 2104.5, 1643.0, 2096.5, 1636.0, 2104.5, 1611.0, 2082.0, 1580.5, 2052.0, 1571.5, 2027.0, 1580.5, 2016.5, 1600.0, 2023.5, 1619.0, 2027.5, 1687.0, 2034.5, 1706.0, 2039.0, 1790.5, 1629.0, 1693.5, 1214.0, 1606.5, 983.5, 1572.0, 1954.0, 1372.5, 2335.0, 1399.5, 2327.5, 1414.0, 2327.5, 1429.0, 2336.5, 1440.0, 2329.5, 1443.0, 2329.5, 1472.0, 2337.0, 1472.5, 2338.5, 1479.0, 2361.5, 1751.0, 2186.0, 1825.5], [2409.0, 1731.5, 2385.5, 1481.0, 2386.0, 1475.5, 2394.5, 1473.0, 2393.5, 1442.0, 2385.5, 1440.0, 2388.5, 1438.0, 2390.5, 1422.0, 2388.5, 1412.0, 2380.5, 1403.0, 2437.0, 1405.5, 2591.0, 1427.5, 2591.5, 1650.0, 2506.0, 1690.5, 2409.0, 1731.5]], 'width': 2592, 'height': 1936}, {'id': 10, 'image_id': 1, 'category_id': 8, 'iscrowd': 0, 'area': 52969, 'bbox': [0.0, 1370.0, 566.0, 193.0], 'segmentation': [[476.0, 1407.5, 472.5, 1405.0, 472.0, 1377.5, 513.0, 1372.5, 563.5, 1371.0, 476.0, 1407.5], [24.0, 1562.5, 0, 1562.0, 0.0, 1399.5, 436.5, 1381.0, 440.0, 1422.5, 110.0, 1556.5, 24.0, 1562.5]], 'width': 2592, 'height': 1936}, {'id': 11, 'image_id': 1, 'category_id': 4, 'iscrowd': 0, 'area': 4182, 'bbox': [1694.0, 917.0, 41.0, 173.0], 'segmentation': [[1712.0, 1089.5, 1711.5, 1027.0, 1707.0, 1019.5, 1698.0, 1022.5, 1693.5, 1010.0, 1693.5, 960.0, 1705.0, 944.5, 1712.0, 953.5, 1728.5, 953.0, 1724.5, 921.0, 1728.0, 916.5, 1734.5, 1088.0, 1712.0, 1089.5]], 'width': 2592, 'height': 1936}, {'id': 12, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8979, 'bbox': [866.0, 253.0, 209.0, 772.0], 'segmentation': [[1052.0, 889.5, 1047.5, 886.0, 997.5, 376.0, 983.5, 272.0, 875.0, 269.5, 866.0, 262.5, 990.0, 252.5, 991.5, 255.0, 1063.5, 882.0, 1063.0, 887.5, 1052.0, 889.5], [1068.0, 971.5, 1068.0, 966.5, 1068.0, 971.5], [1057.0, 976.5, 1057.0, 967.5, 1057.0, 976.5], [1069.0, 981.5, 1069.0, 974.5, 1069.0, 981.5], [1074.0, 1024.5, 1061.5, 1024.0, 1058.0, 976.5, 1060.5, 997.0, 1070.0, 997.5, 1071.5, 994.0, 1074.0, 1024.5], [1070.0, 991.5, 1070.0, 984.5, 1070.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 13, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 527, 'bbox': [1067.0, 951.0, 15.0, 75.0], 'segmentation': [[1081.0, 1025.5, 1075.0, 1024.5, 1073.5, 1021.0, 1067.0, 950.5, 1075.0, 950.5, 1076.5, 956.0, 1073.5, 957.0, 1073.5, 961.0, 1081.0, 1025.5]], 'width': 2592, 'height': 1936}, {'id': 14, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 27156, 'bbox': [2328.0, 1395.0, 100.0, 541.0], 'segmentation': [[2427.0, 1935.5, 2378.0, 1935.5, 2376.5, 1929.0, 2338.5, 1479.0, 2337.0, 1472.5, 2329.5, 1472.0, 2329.5, 1443.0, 2336.5, 1440.0, 2327.5, 1429.0, 2327.5, 1414.0, 2339.0, 1395.5, 2360.0, 1394.5, 2378.0, 1400.5, 2388.5, 1412.0, 2390.5, 1422.0, 2388.5, 1438.0, 2385.5, 1440.0, 2393.5, 1442.0, 2394.5, 1473.0, 2386.0, 1475.5, 2385.5, 1481.0, 2427.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 15, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 6705, 'bbox': [877.0, 954.0, 59.0, 356.0], 'segmentation': [[920.0, 1309.5, 910.5, 1309.0, 913.5, 1288.0, 894.5, 1086.0, 876.5, 960.0, 877.0, 956.5, 899.0, 953.5, 914.5, 1131.0, 935.5, 1301.0, 935.0, 1305.5, 920.0, 1309.5]], 'width': 2592, 'height': 1936}, {'id': 16, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 1177, 'bbox': [617.0, 1014.0, 23.0, 146.0], 'segmentation': [[639.0, 1159.5, 630.0, 1159.5, 628.5, 1153.0, 616.5, 1022.0, 616.5, 1017.0, 622.0, 1013.5, 639.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 17, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 379, 'bbox': [869.0, 882.0, 36.0, 47.0], 'segmentation': [[878.0, 928.5, 872.5, 928.0, 868.5, 882.0, 904.0, 881.5, 875.5, 887.0, 878.0, 928.5]], 'width': 2592, 'height': 1936}, {'id': 18, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 3889, 'bbox': [1728.0, 893.0, 31.0, 267.0], 'segmentation': [[1746.0, 1159.5, 1736.5, 1159.0, 1728.0, 892.5, 1740.5, 896.0, 1740.5, 911.0, 1732.5, 930.0, 1731.5, 957.0, 1738.5, 978.0, 1747.5, 991.0, 1758.5, 1131.0, 1758.5, 1153.0, 1746.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 19, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 9692, 'bbox': [1745.0, 882.0, 61.0, 412.0], 'segmentation': [[1792.0, 1293.5, 1771.0, 1290.5, 1768.5, 1282.0, 1769.0, 1279.5, 1774.5, 1279.0, 1774.5, 1274.0, 1744.5, 890.0, 1745.0, 883.5, 1761.0, 881.5, 1767.5, 884.0, 1796.5, 1270.0, 1797.5, 1277.0, 1805.5, 1282.0, 1805.0, 1290.5, 1792.0, 1293.5]], 'width': 2592, 'height': 1936}, {'id': 20, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8559, 'bbox': [1616.0, 949.0, 64.0, 376.0], 'segmentation': [[1660.0, 1324.5, 1637.5, 1324.0, 1637.5, 1313.0, 1643.5, 1308.0, 1643.5, 1298.0, 1638.5, 1196.0, 1627.5, 1085.0, 1636.0, 1089.5, 1636.5, 1087.0, 1629.5, 1056.0, 1624.5, 1055.0, 1615.5, 959.0, 1616.0, 952.5, 1638.0, 948.5, 1668.5, 1297.0, 1669.5, 1303.0, 1677.5, 1308.0, 1679.5, 1319.0, 1679.0, 1322.5, 1660.0, 1324.5]], 'width': 2592, 'height': 1936}, {'id': 21, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 26362, 'bbox': [2017.0, 1572.0, 103.0, 364.0], 'segmentation': [[2119.0, 1935.5, 2048.5, 1935.0, 2034.5, 1706.0, 2027.5, 1687.0, 2023.5, 1619.0, 2016.5, 1600.0, 2027.0, 1580.5, 2052.0, 1571.5, 2082.0, 1580.5, 2104.5, 1611.0, 2096.5, 1636.0, 2104.5, 1643.0, 2109.5, 1681.0, 2105.5, 1706.0, 2119.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 22, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 18515, 'bbox': [402.0, 982.0, 92.0, 520.0], 'segmentation': [[434.0, 1501.5, 433.5, 1489.0, 445.5, 1480.0, 445.5, 1475.0, 401.5, 993.0, 402.0, 985.5, 431.0, 981.5, 436.5, 982.0, 473.5, 1417.0, 479.5, 1431.0, 483.5, 1475.0, 493.5, 1480.0, 493.0, 1496.5, 434.0, 1501.5]], 'width': 2592, 'height': 1936}, {'id': 23, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 421, 'bbox': [1007.0, 926.0, 12.0, 65.0], 'segmentation': [[1018.0, 990.5, 1011.5, 990.0, 1007.0, 925.5, 1012.5, 926.0, 1018.0, 990.5]], 'width': 2592, 'height': 1936}, {'id': 24, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 1107, 'bbox': [798.0, 259.0, 77.0, 20.0], 'segmentation': [[798.0, 278.5, 797.5, 263.0, 804.0, 258.5, 859.0, 258.5, 874.5, 269.0, 798.0, 278.5]], 'width': 2592, 'height': 1936}, {'id': 25, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 73, 'bbox': [904.0, 877.0, 17.0, 8.0], 'segmentation': [[904.0, 884.5, 907.0, 878.5, 917.0, 876.5, 920.5, 882.0, 904.0, 884.5]], 'width': 2592, 'height': 1936}, {'id': 26, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2731, 'bbox': [868.0, 928.0, 53.0, 90.0], 'segmentation': [[876.0, 1017.5, 867.5, 952.0, 867.5, 929.0, 918.0, 927.5, 914.5, 978.0, 920.5, 1012.0, 919.0, 1014.5, 904.5, 1015.0, 899.0, 953.5, 876.5, 957.0, 884.5, 1016.0, 876.0, 1017.5]], 'width': 2592, 'height': 1936}, {'id': 27, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 12971, 'bbox': [331.0, 822.0, 116.0, 166.0], 'segmentation': [[390.0, 987.5, 381.5, 985.0, 378.5, 964.0, 355.5, 950.0, 334.5, 911.0, 373.0, 906.5, 373.5, 901.0, 372.5, 895.0, 358.5, 876.0, 332.5, 856.0, 330.5, 837.0, 363.0, 836.5, 372.0, 829.5, 394.0, 830.5, 396.0, 824.5, 419.0, 821.5, 421.5, 831.0, 431.0, 831.5, 432.5, 837.0, 446.5, 980.0, 390.0, 987.5]], 'width': 2592, 'height': 1936}, {'id': 28, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 6689, 'bbox': [1599.0, 835.0, 73.0, 120.0], 'segmentation': [[1609.0, 954.5, 1598.5, 846.0, 1608.0, 837.5, 1618.0, 834.5, 1625.0, 834.5, 1628.0, 841.5, 1658.0, 836.5, 1666.0, 838.5, 1667.5, 846.0, 1655.5, 890.0, 1667.0, 890.5, 1671.5, 896.0, 1659.5, 940.0, 1634.0, 950.5, 1609.0, 954.5]], 'width': 2592, 'height': 1936}, {'id': 29, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2285, 'bbox': [1682.0, 880.0, 47.0, 74.0], 'segmentation': [[1728.0, 953.5, 1712.0, 953.5, 1704.0, 942.5, 1695.5, 942.0, 1684.5, 917.0, 1691.5, 912.0, 1682.0, 888.5, 1717.0, 883.5, 1720.5, 880.0, 1728.0, 953.5]], 'width': 2592, 'height': 1936}, {'id': 30, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 16074, 'bbox': [1709.0, 676.0, 104.0, 211.0], 'segmentation': [[1725.0, 886.5, 1708.5, 689.0, 1776.0, 675.5, 1782.5, 680.0, 1786.5, 717.0, 1802.5, 723.0, 1803.5, 739.0, 1790.0, 743.5, 1789.5, 753.0, 1790.5, 762.0, 1806.5, 773.0, 1807.5, 787.0, 1792.5, 788.0, 1792.5, 808.0, 1812.5, 819.0, 1805.5, 840.0, 1796.5, 842.0, 1795.0, 878.5, 1725.0, 886.5]], 'width': 2592, 'height': 1936}, {'id': 31, 'image_id': 1, 'category_id': 25, 'iscrowd': 0, 'area': 4717, 'bbox': [1732.0, 882.0, 83.0, 115.0], 'segmentation': [[1780.0, 996.5, 1775.5, 996.0, 1774.5, 986.0, 1767.5, 896.0, 1767.5, 883.0, 1770.0, 881.5, 1783.0, 888.5, 1800.5, 904.0, 1809.5, 926.0, 1814.5, 949.0, 1808.5, 974.0, 1790.0, 994.5, 1780.0, 996.5], [1751.0, 991.5, 1739.5, 980.0, 1731.5, 957.0, 1732.5, 930.0, 1745.0, 902.5, 1751.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 32, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 4552, 'bbox': [1023.0, 885.0, 69.0, 85.0], 'segmentation': [[1031.0, 969.5, 1022.5, 893.0, 1085.0, 884.5, 1091.5, 947.0, 1067.0, 950.5, 1067.0, 965.5, 1031.0, 969.5], [1076.0, 966.5, 1074.0, 956.5, 1076.5, 957.0, 1076.0, 966.5]], 'width': 2592, 'height': 1936}, {'id': 33, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 3513, 'bbox': [1076.0, 948.0, 63.0, 60.0], 'segmentation': [[1080.0, 1007.5, 1075.5, 948.0, 1135.0, 947.5, 1138.5, 1006.0, 1080.0, 1007.5]], 'width': 2592, 'height': 1936}, {'id': 34, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 329, 'bbox': [1058.0, 966.0, 13.0, 32.0], 'segmentation': [[1070.0, 997.5, 1060.5, 997.0, 1058.0, 965.5, 1067.5, 966.0, 1070.0, 997.5]], 'width': 2592, 'height': 1936}]}  Traceback (most recent call last):    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1709, in data_generator    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1211, in load_image_gt    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/utils.py"", line 358, in load_image    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py"", line 61, in imread      img = call_plugin('imread', fname, plugin=plugin, **plugin_args)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/manage_plugins.py"", line 211, in call_plugin      return func(*args, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_plugins/pil_plugin.py"", line 36, in imread      im = Image.open(f)    File ""/usr/local/lib/python3.6/dist-packages/PIL/Image.py"", line 2419, in open      prefix = fp.read(16)  OSError: [Errno 5] Input/output error  ERROR:root:Error processing image {'id': 1, 'source': 'coco', 'path': '/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/tio/train2017/_0P04ZWQtMtPMwx3lgLdWA.jpg', 'width': 2592, 'height': 1936, 'annotations': [{'id': 1, 'image_id': 1, 'category_id': 9, 'iscrowd': 0, 'area': 2153, 'bbox': [1198.0, 397.0, 38.0, 78.0], 'segmentation': [[1208.0, 474.5, 1197.5, 403.0, 1235.0, 396.5, 1229.5, 423.0, 1235.5, 467.0, 1208.0, 474.5]], 'width': 2592, 'height': 1936}, {'id': 2, 'image_id': 1, 'category_id': 12, 'iscrowd': 0, 'area': 1767, 'bbox': [1240.0, 862.0, 39.0, 56.0], 'segmentation': [[1244.0, 917.5, 1239.5, 889.0, 1240.5, 868.0, 1272.0, 861.5, 1278.5, 914.0, 1244.0, 917.5]], 'width': 2592, 'height': 1936}, {'id': 3, 'image_id': 1, 'category_id': 30, 'iscrowd': 0, 'area': 23079, 'bbox': [0.0, 1012.0, 217.0, 190.0], 'segmentation': [[0.0, 1201.5, 0.0, 1011.5, 13.0, 1041.5, 137.0, 1038.5, 204.5, 1055.0, 216.0, 1136.5, 134.0, 1136.5, 55.0, 1155.5, 0.0, 1201.5]], 'width': 2592, 'height': 1936}, {'id': 4, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 13454, 'bbox': [285.0, 1117.0, 137.0, 129.0], 'segmentation': [[289.0, 1245.5, 293.5, 1208.0, 290.5, 1177.0, 284.5, 1167.0, 293.5, 1161.0, 316.5, 1129.0, 366.0, 1117.5, 412.0, 1116.5, 421.5, 1216.0, 404.0, 1219.5, 400.5, 1233.0, 391.0, 1236.5, 385.0, 1235.5, 375.0, 1224.5, 371.0, 1224.5, 362.0, 1226.5, 352.0, 1238.5, 345.0, 1239.5, 334.0, 1229.5, 327.0, 1229.5, 306.0, 1232.5, 299.0, 1244.5, 289.0, 1245.5]], 'width': 2592, 'height': 1936}, {'id': 5, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 34185, 'bbox': [0.0, 1136.0, 294.0, 164.0], 'segmentation': [[9.0, 1299.5, 0, 1299.0, 0, 1202.0, 55.0, 1155.5, 134.0, 1136.5, 218.0, 1135.5, 284.0, 1166.5, 290.5, 1177.0, 293.5, 1216.0, 289.5, 1240.0, 283.5, 1251.0, 274.0, 1257.5, 250.0, 1257.5, 243.0, 1246.5, 238.0, 1246.5, 167.0, 1257.5, 168.5, 1266.0, 163.0, 1275.5, 135.0, 1283.5, 125.0, 1278.5, 121.0, 1272.5, 15.0, 1281.5, 15.5, 1293.0, 9.0, 1299.5]], 'width': 2592, 'height': 1936}, {'id': 6, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 14170, 'bbox': [2474.0, 902.0, 118.0, 181.0], 'segmentation': [[2538.0, 1082.5, 2507.0, 1079.5, 2486.0, 1061.5, 2481.5, 1055.0, 2473.5, 1021.0, 2541.5, 934.0, 2591.0, 901.5, 2591.5, 1067.0, 2538.0, 1082.5]], 'width': 2592, 'height': 1936}, {'id': 7, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 49052, 'bbox': [840.0, 1024.0, 296.0, 219.0], 'segmentation': [[956.0, 1242.5, 943.0, 1242.5, 927.5, 1238.0, 906.5, 1039.0, 946.0, 1030.5, 1038.0, 1023.5, 1115.0, 1025.5, 1121.5, 1042.0, 1133.5, 1137.0, 1135.5, 1193.0, 1124.5, 1199.0, 1114.0, 1216.5, 1102.0, 1220.5, 1077.0, 1218.5, 1066.0, 1203.5, 986.0, 1211.5, 980.5, 1234.0, 956.0, 1242.5], [874.0, 1230.5, 863.0, 1230.5, 851.5, 1220.0, 845.5, 1171.0, 854.5, 1117.0, 840.5, 1106.0, 839.5, 1099.0, 845.0, 1088.5, 851.0, 1088.5, 859.5, 1094.0, 857.5, 1081.0, 865.5, 1056.0, 881.0, 1042.5, 888.0, 1041.5, 905.5, 1209.0, 887.0, 1209.5, 882.5, 1227.0, 874.0, 1230.5]], 'width': 2592, 'height': 1936}, {'id': 8, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 38963, 'bbox': [1049.0, 1201.0, 725.0, 90.0], 'segmentation': [[1669.0, 1269.5, 1665.5, 1264.0, 1661.0, 1207.5, 1758.5, 1201.0, 1770.5, 1227.0, 1773.0, 1263.5, 1669.0, 1269.5], [1103.0, 1289.5, 1050.5, 1289.0, 1159.0, 1231.5, 1636.0, 1208.5, 1639.5, 1212.0, 1641.0, 1270.5, 1376.0, 1284.5, 1103.0, 1289.5]], 'width': 2592, 'height': 1936}, {'id': 9, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 383152, 'bbox': [984.0, 1373.0, 1608.0, 453.0], 'segmentation': [[2186.0, 1825.5, 2111.5, 1807.0, 2105.5, 1706.0, 2109.5, 1681.0, 2104.5, 1643.0, 2096.5, 1636.0, 2104.5, 1611.0, 2082.0, 1580.5, 2052.0, 1571.5, 2027.0, 1580.5, 2016.5, 1600.0, 2023.5, 1619.0, 2027.5, 1687.0, 2034.5, 1706.0, 2039.0, 1790.5, 1629.0, 1693.5, 1214.0, 1606.5, 983.5, 1572.0, 1954.0, 1372.5, 2335.0, 1399.5, 2327.5, 1414.0, 2327.5, 1429.0, 2336.5, 1440.0, 2329.5, 1443.0, 2329.5, 1472.0, 2337.0, 1472.5, 2338.5, 1479.0, 2361.5, 1751.0, 2186.0, 1825.5], [2409.0, 1731.5, 2385.5, 1481.0, 2386.0, 1475.5, 2394.5, 1473.0, 2393.5, 1442.0, 2385.5, 1440.0, 2388.5, 1438.0, 2390.5, 1422.0, 2388.5, 1412.0, 2380.5, 1403.0, 2437.0, 1405.5, 2591.0, 1427.5, 2591.5, 1650.0, 2506.0, 1690.5, 2409.0, 1731.5]], 'width': 2592, 'height': 1936}, {'id': 10, 'image_id': 1, 'category_id': 8, 'iscrowd': 0, 'area': 52969, 'bbox': [0.0, 1370.0, 566.0, 193.0], 'segmentation': [[476.0, 1407.5, 472.5, 1405.0, 472.0, 1377.5, 513.0, 1372.5, 563.5, 1371.0, 476.0, 1407.5], [24.0, 1562.5, 0, 1562.0, 0.0, 1399.5, 436.5, 1381.0, 440.0, 1422.5, 110.0, 1556.5, 24.0, 1562.5]], 'width': 2592, 'height': 1936}, {'id': 11, 'image_id': 1, 'category_id': 4, 'iscrowd': 0, 'area': 4182, 'bbox': [1694.0, 917.0, 41.0, 173.0], 'segmentation': [[1712.0, 1089.5, 1711.5, 1027.0, 1707.0, 1019.5, 1698.0, 1022.5, 1693.5, 1010.0, 1693.5, 960.0, 1705.0, 944.5, 1712.0, 953.5, 1728.5, 953.0, 1724.5, 921.0, 1728.0, 916.5, 1734.5, 1088.0, 1712.0, 1089.5]], 'width': 2592, 'height': 1936}, {'id': 12, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8979, 'bbox': [866.0, 253.0, 209.0, 772.0], 'segmentation': [[1052.0, 889.5, 1047.5, 886.0, 997.5, 376.0, 983.5, 272.0, 875.0, 269.5, 866.0, 262.5, 990.0, 252.5, 991.5, 255.0, 1063.5, 882.0, 1063.0, 887.5, 1052.0, 889.5], [1068.0, 971.5, 1068.0, 966.5, 1068.0, 971.5], [1057.0, 976.5, 1057.0, 967.5, 1057.0, 976.5], [1069.0, 981.5, 1069.0, 974.5, 1069.0, 981.5], [1074.0, 1024.5, 1061.5, 1024.0, 1058.0, 976.5, 1060.5, 997.0, 1070.0, 997.5, 1071.5, 994.0, 1074.0, 1024.5], [1070.0, 991.5, 1070.0, 984.5, 1070.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 13, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 527, 'bbox': [1067.0, 951.0, 15.0, 75.0], 'segmentation': [[1081.0, 1025.5, 1075.0, 1024.5, 1073.5, 1021.0, 1067.0, 950.5, 1075.0, 950.5, 1076.5, 956.0, 1073.5, 957.0, 1073.5, 961.0, 1081.0, 1025.5]], 'width': 2592, 'height': 1936}, {'id': 14, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 27156, 'bbox': [2328.0, 1395.0, 100.0, 541.0], 'segmentation': [[2427.0, 1935.5, 2378.0, 1935.5, 2376.5, 1929.0, 2338.5, 1479.0, 2337.0, 1472.5, 2329.5, 1472.0, 2329.5, 1443.0, 2336.5, 1440.0, 2327.5, 1429.0, 2327.5, 1414.0, 2339.0, 1395.5, 2360.0, 1394.5, 2378.0, 1400.5, 2388.5, 1412.0, 2390.5, 1422.0, 2388.5, 1438.0, 2385.5, 1440.0, 2393.5, 1442.0, 2394.5, 1473.0, 2386.0, 1475.5, 2385.5, 1481.0, 2427.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 15, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 6705, 'bbox': [877.0, 954.0, 59.0, 356.0], 'segmentation': [[920.0, 1309.5, 910.5, 1309.0, 913.5, 1288.0, 894.5, 1086.0, 876.5, 960.0, 877.0, 956.5, 899.0, 953.5, 914.5, 1131.0, 935.5, 1301.0, 935.0, 1305.5, 920.0, 1309.5]], 'width': 2592, 'height': 1936}, {'id': 16, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 1177, 'bbox': [617.0, 1014.0, 23.0, 146.0], 'segmentation': [[639.0, 1159.5, 630.0, 1159.5, 628.5, 1153.0, 616.5, 1022.0, 616.5, 1017.0, 622.0, 1013.5, 639.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 17, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 379, 'bbox': [869.0, 882.0, 36.0, 47.0], 'segmentation': [[878.0, 928.5, 872.5, 928.0, 868.5, 882.0, 904.0, 881.5, 875.5, 887.0, 878.0, 928.5]], 'width': 2592, 'height': 1936}, {'id': 18, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 3889, 'bbox': [1728.0, 893.0, 31.0, 267.0], 'segmentation': [[1746.0, 1159.5, 1736.5, 1159.0, 1728.0, 892.5, 1740.5, 896.0, 1740.5, 911.0, 1732.5, 930.0, 1731.5, 957.0, 1738.5, 978.0, 1747.5, 991.0, 1758.5, 1131.0, 1758.5, 1153.0, 1746.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 19, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 9692, 'bbox': [1745.0, 882.0, 61.0, 412.0], 'segmentation': [[1792.0, 1293.5, 1771.0, 1290.5, 1768.5, 1282.0, 1769.0, 1279.5, 1774.5, 1279.0, 1774.5, 1274.0, 1744.5, 890.0, 1745.0, 883.5, 1761.0, 881.5, 1767.5, 884.0, 1796.5, 1270.0, 1797.5, 1277.0, 1805.5, 1282.0, 1805.0, 1290.5, 1792.0, 1293.5]], 'width': 2592, 'height': 1936}, {'id': 20, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8559, 'bbox': [1616.0, 949.0, 64.0, 376.0], 'segmentation': [[1660.0, 1324.5, 1637.5, 1324.0, 1637.5, 1313.0, 1643.5, 1308.0, 1643.5, 1298.0, 1638.5, 1196.0, 1627.5, 1085.0, 1636.0, 1089.5, 1636.5, 1087.0, 1629.5, 1056.0, 1624.5, 1055.0, 1615.5, 959.0, 1616.0, 952.5, 1638.0, 948.5, 1668.5, 1297.0, 1669.5, 1303.0, 1677.5, 1308.0, 1679.5, 1319.0, 1679.0, 1322.5, 1660.0, 1324.5]], 'width': 2592, 'height': 1936}, {'id': 21, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 26362, 'bbox': [2017.0, 1572.0, 103.0, 364.0], 'segmentation': [[2119.0, 1935.5, 2048.5, 1935.0, 2034.5, 1706.0, 2027.5, 1687.0, 2023.5, 1619.0, 2016.5, 1600.0, 2027.0, 1580.5, 2052.0, 1571.5, 2082.0, 1580.5, 2104.5, 1611.0, 2096.5, 1636.0, 2104.5, 1643.0, 2109.5, 1681.0, 2105.5, 1706.0, 2119.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 22, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 18515, 'bbox': [402.0, 982.0, 92.0, 520.0], 'segmentation': [[434.0, 1501.5, 433.5, 1489.0, 445.5, 1480.0, 445.5, 1475.0, 401.5, 993.0, 402.0, 985.5, 431.0, 981.5, 436.5, 982.0, 473.5, 1417.0, 479.5, 1431.0, 483.5, 1475.0, 493.5, 1480.0, 493.0, 1496.5, 434.0, 1501.5]], 'width': 2592, 'height': 1936}, {'id': 23, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 421, 'bbox': [1007.0, 926.0, 12.0, 65.0], 'segmentation': [[1018.0, 990.5, 1011.5, 990.0, 1007.0, 925.5, 1012.5, 926.0, 1018.0, 990.5]], 'width': 2592, 'height': 1936}, {'id': 24, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 1107, 'bbox': [798.0, 259.0, 77.0, 20.0], 'segmentation': [[798.0, 278.5, 797.5, 263.0, 804.0, 258.5, 859.0, 258.5, 874.5, 269.0, 798.0, 278.5]], 'width': 2592, 'height': 1936}, {'id': 25, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 73, 'bbox': [904.0, 877.0, 17.0, 8.0], 'segmentation': [[904.0, 884.5, 907.0, 878.5, 917.0, 876.5, 920.5, 882.0, 904.0, 884.5]], 'width': 2592, 'height': 1936}, {'id': 26, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2731, 'bbox': [868.0, 928.0, 53.0, 90.0], 'segmentation': [[876.0, 1017.5, 867.5, 952.0, 867.5, 929.0, 918.0, 927.5, 914.5, 978.0, 920.5, 1012.0, 919.0, 1014.5, 904.5, 1015.0, 899.0, 953.5, 876.5, 957.0, 884.5, 1016.0, 876.0, 1017.5]], 'width': 2592, 'height': 1936}, {'id': 27, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 12971, 'bbox': [331.0, 822.0, 116.0, 166.0], 'segmentation': [[390.0, 987.5, 381.5, 985.0, 378.5, 964.0, 355.5, 950.0, 334.5, 911.0, 373.0, 906.5, 373.5, 901.0, 372.5, 895.0, 358.5, 876.0, 332.5, 856.0, 330.5, 837.0, 363.0, 836.5, 372.0, 829.5, 394.0, 830.5, 396.0, 824.5, 419.0, 821.5, 421.5, 831.0, 431.0, 831.5, 432.5, 837.0, 446.5, 980.0, 390.0, 987.5]], 'width': 2592, 'height': 1936}, {'id': 28, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 6689, 'bbox': [1599.0, 835.0, 73.0, 120.0], 'segmentation': [[1609.0, 954.5, 1598.5, 846.0, 1608.0, 837.5, 1618.0, 834.5, 1625.0, 834.5, 1628.0, 841.5, 1658.0, 836.5, 1666.0, 838.5, 1667.5, 846.0, 1655.5, 890.0, 1667.0, 890.5, 1671.5, 896.0, 1659.5, 940.0, 1634.0, 950.5, 1609.0, 954.5]], 'width': 2592, 'height': 1936}, {'id': 29, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2285, 'bbox': [1682.0, 880.0, 47.0, 74.0], 'segmentation': [[1728.0, 953.5, 1712.0, 953.5, 1704.0, 942.5, 1695.5, 942.0, 1684.5, 917.0, 1691.5, 912.0, 1682.0, 888.5, 1717.0, 883.5, 1720.5, 880.0, 1728.0, 953.5]], 'width': 2592, 'height': 1936}, {'id': 30, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 16074, 'bbox': [1709.0, 676.0, 104.0, 211.0], 'segmentation': [[1725.0, 886.5, 1708.5, 689.0, 1776.0, 675.5, 1782.5, 680.0, 1786.5, 717.0, 1802.5, 723.0, 1803.5, 739.0, 1790.0, 743.5, 1789.5, 753.0, 1790.5, 762.0, 1806.5, 773.0, 1807.5, 787.0, 1792.5, 788.0, 1792.5, 808.0, 1812.5, 819.0, 1805.5, 840.0, 1796.5, 842.0, 1795.0, 878.5, 1725.0, 886.5]], 'width': 2592, 'height': 1936}, {'id': 31, 'image_id': 1, 'category_id': 25, 'iscrowd': 0, 'area': 4717, 'bbox': [1732.0, 882.0, 83.0, 115.0], 'segmentation': [[1780.0, 996.5, 1775.5, 996.0, 1774.5, 986.0, 1767.5, 896.0, 1767.5, 883.0, 1770.0, 881.5, 1783.0, 888.5, 1800.5, 904.0, 1809.5, 926.0, 1814.5, 949.0, 1808.5, 974.0, 1790.0, 994.5, 1780.0, 996.5], [1751.0, 991.5, 1739.5, 980.0, 1731.5, 957.0, 1732.5, 930.0, 1745.0, 902.5, 1751.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 32, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 4552, 'bbox': [1023.0, 885.0, 69.0, 85.0], 'segmentation': [[1031.0, 969.5, 1022.5, 893.0, 1085.0, 884.5, 1091.5, 947.0, 1067.0, 950.5, 1067.0, 965.5, 1031.0, 969.5], [1076.0, 966.5, 1074.0, 956.5, 1076.5, 957.0, 1076.0, 966.5]], 'width': 2592, 'height': 1936}, {'id': 33, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 3513, 'bbox': [1076.0, 948.0, 63.0, 60.0], 'segmentation': [[1080.0, 1007.5, 1075.5, 948.0, 1135.0, 947.5, 1138.5, 1006.0, 1080.0, 1007.5]], 'width': 2592, 'height': 1936}, {'id': 34, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 329, 'bbox': [1058.0, 966.0, 13.0, 32.0], 'segmentation': [[1070.0, 997.5, 1060.5, 997.0, 1058.0, 965.5, 1067.5, 966.0, 1070.0, 997.5]], 'width': 2592, 'height': 1936}]}  Traceback (most recent call last):    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1709, in data_generator    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1211, in load_image_gt    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/utils.py"", line 358, in load_image    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py"", line 61, in imread      img = call_plugin('imread', fname, plugin=plugin, **plugin_args)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/manage_plugins.py"", line 211, in call_plugin      return func(*args, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_plugins/pil_plugin.py"", line 36, in imread      im = Image.open(f)    File ""/usr/local/lib/python3.6/dist-packages/PIL/Image.py"", line 2419, in open      prefix = fp.read(16)  OSError: [Errno 5] Input/output error  ERROR:root:Error processing image {'id': 1, 'source': 'coco', 'path': '/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/tio/train2017/_0P04ZWQtMtPMwx3lgLdWA.jpg', 'width': 2592, 'height': 1936, 'annotations': [{'id': 1, 'image_id': 1, 'category_id': 9, 'iscrowd': 0, 'area': 2153, 'bbox': [1198.0, 397.0, 38.0, 78.0], 'segmentation': [[1208.0, 474.5, 1197.5, 403.0, 1235.0, 396.5, 1229.5, 423.0, 1235.5, 467.0, 1208.0, 474.5]], 'width': 2592, 'height': 1936}, {'id': 2, 'image_id': 1, 'category_id': 12, 'iscrowd': 0, 'area': 1767, 'bbox': [1240.0, 862.0, 39.0, 56.0], 'segmentation': [[1244.0, 917.5, 1239.5, 889.0, 1240.5, 868.0, 1272.0, 861.5, 1278.5, 914.0, 1244.0, 917.5]], 'width': 2592, 'height': 1936}, {'id': 3, 'image_id': 1, 'category_id': 30, 'iscrowd': 0, 'area': 23079, 'bbox': [0.0, 1012.0, 217.0, 190.0], 'segmentation': [[0.0, 1201.5, 0.0, 1011.5, 13.0, 1041.5, 137.0, 1038.5, 204.5, 1055.0, 216.0, 1136.5, 134.0, 1136.5, 55.0, 1155.5, 0.0, 1201.5]], 'width': 2592, 'height': 1936}, {'id': 4, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 13454, 'bbox': [285.0, 1117.0, 137.0, 129.0], 'segmentation': [[289.0, 1245.5, 293.5, 1208.0, 290.5, 1177.0, 284.5, 1167.0, 293.5, 1161.0, 316.5, 1129.0, 366.0, 1117.5, 412.0, 1116.5, 421.5, 1216.0, 404.0, 1219.5, 400.5, 1233.0, 391.0, 1236.5, 385.0, 1235.5, 375.0, 1224.5, 371.0, 1224.5, 362.0, 1226.5, 352.0, 1238.5, 345.0, 1239.5, 334.0, 1229.5, 327.0, 1229.5, 306.0, 1232.5, 299.0, 1244.5, 289.0, 1245.5]], 'width': 2592, 'height': 1936}, {'id': 5, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 34185, 'bbox': [0.0, 1136.0, 294.0, 164.0], 'segmentation': [[9.0, 1299.5, 0, 1299.0, 0, 1202.0, 55.0, 1155.5, 134.0, 1136.5, 218.0, 1135.5, 284.0, 1166.5, 290.5, 1177.0, 293.5, 1216.0, 289.5, 1240.0, 283.5, 1251.0, 274.0, 1257.5, 250.0, 1257.5, 243.0, 1246.5, 238.0, 1246.5, 167.0, 1257.5, 168.5, 1266.0, 163.0, 1275.5, 135.0, 1283.5, 125.0, 1278.5, 121.0, 1272.5, 15.0, 1281.5, 15.5, 1293.0, 9.0, 1299.5]], 'width': 2592, 'height': 1936}, {'id': 6, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 14170, 'bbox': [2474.0, 902.0, 118.0, 181.0], 'segmentation': [[2538.0, 1082.5, 2507.0, 1079.5, 2486.0, 1061.5, 2481.5, 1055.0, 2473.5, 1021.0, 2541.5, 934.0, 2591.0, 901.5, 2591.5, 1067.0, 2538.0, 1082.5]], 'width': 2592, 'height': 1936}, {'id': 7, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 49052, 'bbox': [840.0, 1024.0, 296.0, 219.0], 'segmentation': [[956.0, 1242.5, 943.0, 1242.5, 927.5, 1238.0, 906.5, 1039.0, 946.0, 1030.5, 1038.0, 1023.5, 1115.0, 1025.5, 1121.5, 1042.0, 1133.5, 1137.0, 1135.5, 1193.0, 1124.5, 1199.0, 1114.0, 1216.5, 1102.0, 1220.5, 1077.0, 1218.5, 1066.0, 1203.5, 986.0, 1211.5, 980.5, 1234.0, 956.0, 1242.5], [874.0, 1230.5, 863.0, 1230.5, 851.5, 1220.0, 845.5, 1171.0, 854.5, 1117.0, 840.5, 1106.0, 839.5, 1099.0, 845.0, 1088.5, 851.0, 1088.5, 859.5, 1094.0, 857.5, 1081.0, 865.5, 1056.0, 881.0, 1042.5, 888.0, 1041.5, 905.5, 1209.0, 887.0, 1209.5, 882.5, 1227.0, 874.0, 1230.5]], 'width': 2592, 'height': 1936}, {'id': 8, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 38963, 'bbox': [1049.0, 1201.0, 725.0, 90.0], 'segmentation': [[1669.0, 1269.5, 1665.5, 1264.0, 1661.0, 1207.5, 1758.5, 1201.0, 1770.5, 1227.0, 1773.0, 1263.5, 1669.0, 1269.5], [1103.0, 1289.5, 1050.5, 1289.0, 1159.0, 1231.5, 1636.0, 1208.5, 1639.5, 1212.0, 1641.0, 1270.5, 1376.0, 1284.5, 1103.0, 1289.5]], 'width': 2592, 'height': 1936}, {'id': 9, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 383152, 'bbox': [984.0, 1373.0, 1608.0, 453.0], 'segmentation': [[2186.0, 1825.5, 2111.5, 1807.0, 2105.5, 1706.0, 2109.5, 1681.0, 2104.5, 1643.0, 2096.5, 1636.0, 2104.5, 1611.0, 2082.0, 1580.5, 2052.0, 1571.5, 2027.0, 1580.5, 2016.5, 1600.0, 2023.5, 1619.0, 2027.5, 1687.0, 2034.5, 1706.0, 2039.0, 1790.5, 1629.0, 1693.5, 1214.0, 1606.5, 983.5, 1572.0, 1954.0, 1372.5, 2335.0, 1399.5, 2327.5, 1414.0, 2327.5, 1429.0, 2336.5, 1440.0, 2329.5, 1443.0, 2329.5, 1472.0, 2337.0, 1472.5, 2338.5, 1479.0, 2361.5, 1751.0, 2186.0, 1825.5], [2409.0, 1731.5, 2385.5, 1481.0, 2386.0, 1475.5, 2394.5, 1473.0, 2393.5, 1442.0, 2385.5, 1440.0, 2388.5, 1438.0, 2390.5, 1422.0, 2388.5, 1412.0, 2380.5, 1403.0, 2437.0, 1405.5, 2591.0, 1427.5, 2591.5, 1650.0, 2506.0, 1690.5, 2409.0, 1731.5]], 'width': 2592, 'height': 1936}, {'id': 10, 'image_id': 1, 'category_id': 8, 'iscrowd': 0, 'area': 52969, 'bbox': [0.0, 1370.0, 566.0, 193.0], 'segmentation': [[476.0, 1407.5, 472.5, 1405.0, 472.0, 1377.5, 513.0, 1372.5, 563.5, 1371.0, 476.0, 1407.5], [24.0, 1562.5, 0, 1562.0, 0.0, 1399.5, 436.5, 1381.0, 440.0, 1422.5, 110.0, 1556.5, 24.0, 1562.5]], 'width': 2592, 'height': 1936}, {'id': 11, 'image_id': 1, 'category_id': 4, 'iscrowd': 0, 'area': 4182, 'bbox': [1694.0, 917.0, 41.0, 173.0], 'segmentation': [[1712.0, 1089.5, 1711.5, 1027.0, 1707.0, 1019.5, 1698.0, 1022.5, 1693.5, 1010.0, 1693.5, 960.0, 1705.0, 944.5, 1712.0, 953.5, 1728.5, 953.0, 1724.5, 921.0, 1728.0, 916.5, 1734.5, 1088.0, 1712.0, 1089.5]], 'width': 2592, 'height': 1936}, {'id': 12, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8979, 'bbox': [866.0, 253.0, 209.0, 772.0], 'segmentation': [[1052.0, 889.5, 1047.5, 886.0, 997.5, 376.0, 983.5, 272.0, 875.0, 269.5, 866.0, 262.5, 990.0, 252.5, 991.5, 255.0, 1063.5, 882.0, 1063.0, 887.5, 1052.0, 889.5], [1068.0, 971.5, 1068.0, 966.5, 1068.0, 971.5], [1057.0, 976.5, 1057.0, 967.5, 1057.0, 976.5], [1069.0, 981.5, 1069.0, 974.5, 1069.0, 981.5], [1074.0, 1024.5, 1061.5, 1024.0, 1058.0, 976.5, 1060.5, 997.0, 1070.0, 997.5, 1071.5, 994.0, 1074.0, 1024.5], [1070.0, 991.5, 1070.0, 984.5, 1070.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 13, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 527, 'bbox': [1067.0, 951.0, 15.0, 75.0], 'segmentation': [[1081.0, 1025.5, 1075.0, 1024.5, 1073.5, 1021.0, 1067.0, 950.5, 1075.0, 950.5, 1076.5, 956.0, 1073.5, 957.0, 1073.5, 961.0, 1081.0, 1025.5]], 'width': 2592, 'height': 1936}, {'id': 14, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 27156, 'bbox': [2328.0, 1395.0, 100.0, 541.0], 'segmentation': [[2427.0, 1935.5, 2378.0, 1935.5, 2376.5, 1929.0, 2338.5, 1479.0, 2337.0, 1472.5, 2329.5, 1472.0, 2329.5, 1443.0, 2336.5, 1440.0, 2327.5, 1429.0, 2327.5, 1414.0, 2339.0, 1395.5, 2360.0, 1394.5, 2378.0, 1400.5, 2388.5, 1412.0, 2390.5, 1422.0, 2388.5, 1438.0, 2385.5, 1440.0, 2393.5, 1442.0, 2394.5, 1473.0, 2386.0, 1475.5, 2385.5, 1481.0, 2427.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 15, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 6705, 'bbox': [877.0, 954.0, 59.0, 356.0], 'segmentation': [[920.0, 1309.5, 910.5, 1309.0, 913.5, 1288.0, 894.5, 1086.0, 876.5, 960.0, 877.0, 956.5, 899.0, 953.5, 914.5, 1131.0, 935.5, 1301.0, 935.0, 1305.5, 920.0, 1309.5]], 'width': 2592, 'height': 1936}, {'id': 16, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 1177, 'bbox': [617.0, 1014.0, 23.0, 146.0], 'segmentation': [[639.0, 1159.5, 630.0, 1159.5, 628.5, 1153.0, 616.5, 1022.0, 616.5, 1017.0, 622.0, 1013.5, 639.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 17, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 379, 'bbox': [869.0, 882.0, 36.0, 47.0], 'segmentation': [[878.0, 928.5, 872.5, 928.0, 868.5, 882.0, 904.0, 881.5, 875.5, 887.0, 878.0, 928.5]], 'width': 2592, 'height': 1936}, {'id': 18, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 3889, 'bbox': [1728.0, 893.0, 31.0, 267.0], 'segmentation': [[1746.0, 1159.5, 1736.5, 1159.0, 1728.0, 892.5, 1740.5, 896.0, 1740.5, 911.0, 1732.5, 930.0, 1731.5, 957.0, 1738.5, 978.0, 1747.5, 991.0, 1758.5, 1131.0, 1758.5, 1153.0, 1746.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 19, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 9692, 'bbox': [1745.0, 882.0, 61.0, 412.0], 'segmentation': [[1792.0, 1293.5, 1771.0, 1290.5, 1768.5, 1282.0, 1769.0, 1279.5, 1774.5, 1279.0, 1774.5, 1274.0, 1744.5, 890.0, 1745.0, 883.5, 1761.0, 881.5, 1767.5, 884.0, 1796.5, 1270.0, 1797.5, 1277.0, 1805.5, 1282.0, 1805.0, 1290.5, 1792.0, 1293.5]], 'width': 2592, 'height': 1936}, {'id': 20, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8559, 'bbox': [1616.0, 949.0, 64.0, 376.0], 'segmentation': [[1660.0, 1324.5, 1637.5, 1324.0, 1637.5, 1313.0, 1643.5, 1308.0, 1643.5, 1298.0, 1638.5, 1196.0, 1627.5, 1085.0, 1636.0, 1089.5, 1636.5, 1087.0, 1629.5, 1056.0, 1624.5, 1055.0, 1615.5, 959.0, 1616.0, 952.5, 1638.0, 948.5, 1668.5, 1297.0, 1669.5, 1303.0, 1677.5, 1308.0, 1679.5, 1319.0, 1679.0, 1322.5, 1660.0, 1324.5]], 'width': 2592, 'height': 1936}, {'id': 21, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 26362, 'bbox': [2017.0, 1572.0, 103.0, 364.0], 'segmentation': [[2119.0, 1935.5, 2048.5, 1935.0, 2034.5, 1706.0, 2027.5, 1687.0, 2023.5, 1619.0, 2016.5, 1600.0, 2027.0, 1580.5, 2052.0, 1571.5, 2082.0, 1580.5, 2104.5, 1611.0, 2096.5, 1636.0, 2104.5, 1643.0, 2109.5, 1681.0, 2105.5, 1706.0, 2119.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 22, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 18515, 'bbox': [402.0, 982.0, 92.0, 520.0], 'segmentation': [[434.0, 1501.5, 433.5, 1489.0, 445.5, 1480.0, 445.5, 1475.0, 401.5, 993.0, 402.0, 985.5, 431.0, 981.5, 436.5, 982.0, 473.5, 1417.0, 479.5, 1431.0, 483.5, 1475.0, 493.5, 1480.0, 493.0, 1496.5, 434.0, 1501.5]], 'width': 2592, 'height': 1936}, {'id': 23, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 421, 'bbox': [1007.0, 926.0, 12.0, 65.0], 'segmentation': [[1018.0, 990.5, 1011.5, 990.0, 1007.0, 925.5, 1012.5, 926.0, 1018.0, 990.5]], 'width': 2592, 'height': 1936}, {'id': 24, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 1107, 'bbox': [798.0, 259.0, 77.0, 20.0], 'segmentation': [[798.0, 278.5, 797.5, 263.0, 804.0, 258.5, 859.0, 258.5, 874.5, 269.0, 798.0, 278.5]], 'width': 2592, 'height': 1936}, {'id': 25, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 73, 'bbox': [904.0, 877.0, 17.0, 8.0], 'segmentation': [[904.0, 884.5, 907.0, 878.5, 917.0, 876.5, 920.5, 882.0, 904.0, 884.5]], 'width': 2592, 'height': 1936}, {'id': 26, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2731, 'bbox': [868.0, 928.0, 53.0, 90.0], 'segmentation': [[876.0, 1017.5, 867.5, 952.0, 867.5, 929.0, 918.0, 927.5, 914.5, 978.0, 920.5, 1012.0, 919.0, 1014.5, 904.5, 1015.0, 899.0, 953.5, 876.5, 957.0, 884.5, 1016.0, 876.0, 1017.5]], 'width': 2592, 'height': 1936}, {'id': 27, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 12971, 'bbox': [331.0, 822.0, 116.0, 166.0], 'segmentation': [[390.0, 987.5, 381.5, 985.0, 378.5, 964.0, 355.5, 950.0, 334.5, 911.0, 373.0, 906.5, 373.5, 901.0, 372.5, 895.0, 358.5, 876.0, 332.5, 856.0, 330.5, 837.0, 363.0, 836.5, 372.0, 829.5, 394.0, 830.5, 396.0, 824.5, 419.0, 821.5, 421.5, 831.0, 431.0, 831.5, 432.5, 837.0, 446.5, 980.0, 390.0, 987.5]], 'width': 2592, 'height': 1936}, {'id': 28, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 6689, 'bbox': [1599.0, 835.0, 73.0, 120.0], 'segmentation': [[1609.0, 954.5, 1598.5, 846.0, 1608.0, 837.5, 1618.0, 834.5, 1625.0, 834.5, 1628.0, 841.5, 1658.0, 836.5, 1666.0, 838.5, 1667.5, 846.0, 1655.5, 890.0, 1667.0, 890.5, 1671.5, 896.0, 1659.5, 940.0, 1634.0, 950.5, 1609.0, 954.5]], 'width': 2592, 'height': 1936}, {'id': 29, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2285, 'bbox': [1682.0, 880.0, 47.0, 74.0], 'segmentation': [[1728.0, 953.5, 1712.0, 953.5, 1704.0, 942.5, 1695.5, 942.0, 1684.5, 917.0, 1691.5, 912.0, 1682.0, 888.5, 1717.0, 883.5, 1720.5, 880.0, 1728.0, 953.5]], 'width': 2592, 'height': 1936}, {'id': 30, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 16074, 'bbox': [1709.0, 676.0, 104.0, 211.0], 'segmentation': [[1725.0, 886.5, 1708.5, 689.0, 1776.0, 675.5, 1782.5, 680.0, 1786.5, 717.0, 1802.5, 723.0, 1803.5, 739.0, 1790.0, 743.5, 1789.5, 753.0, 1790.5, 762.0, 1806.5, 773.0, 1807.5, 787.0, 1792.5, 788.0, 1792.5, 808.0, 1812.5, 819.0, 1805.5, 840.0, 1796.5, 842.0, 1795.0, 878.5, 1725.0, 886.5]], 'width': 2592, 'height': 1936}, {'id': 31, 'image_id': 1, 'category_id': 25, 'iscrowd': 0, 'area': 4717, 'bbox': [1732.0, 882.0, 83.0, 115.0], 'segmentation': [[1780.0, 996.5, 1775.5, 996.0, 1774.5, 986.0, 1767.5, 896.0, 1767.5, 883.0, 1770.0, 881.5, 1783.0, 888.5, 1800.5, 904.0, 1809.5, 926.0, 1814.5, 949.0, 1808.5, 974.0, 1790.0, 994.5, 1780.0, 996.5], [1751.0, 991.5, 1739.5, 980.0, 1731.5, 957.0, 1732.5, 930.0, 1745.0, 902.5, 1751.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 32, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 4552, 'bbox': [1023.0, 885.0, 69.0, 85.0], 'segmentation': [[1031.0, 969.5, 1022.5, 893.0, 1085.0, 884.5, 1091.5, 947.0, 1067.0, 950.5, 1067.0, 965.5, 1031.0, 969.5], [1076.0, 966.5, 1074.0, 956.5, 1076.5, 957.0, 1076.0, 966.5]], 'width': 2592, 'height': 1936}, {'id': 33, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 3513, 'bbox': [1076.0, 948.0, 63.0, 60.0], 'segmentation': [[1080.0, 1007.5, 1075.5, 948.0, 1135.0, 947.5, 1138.5, 1006.0, 1080.0, 1007.5]], 'width': 2592, 'height': 1936}, {'id': 34, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 329, 'bbox': [1058.0, 966.0, 13.0, 32.0], 'segmentation': [[1070.0, 997.5, 1060.5, 997.0, 1058.0, 965.5, 1067.5, 966.0, 1070.0, 997.5]], 'width': 2592, 'height': 1936}]}  Traceback (most recent call last):    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1709, in data_generator    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1211, in load_image_gt      image = dataset.load_image(image_id)    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/utils.py"", line 358, in load_image      image = skimage.io.imread(self.image_info[image_id]['path'])    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py"", line 61, in imread      img = call_plugin('imread', fname, plugin=plugin, **plugin_args)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/manage_plugins.py"", line 211, in call_plugin      return func(*args, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_plugins/pil_plugin.py"", line 36, in imread      im = Image.open(f)    File ""/usr/local/lib/python3.6/dist-packages/PIL/Image.py"", line 2419, in open      prefix = fp.read(16)  OSError: [Errno 5] Input/output error  Epoch 7/40  50/50 [==============================] - 239s 5s/step - loss: 0.3024 - rpn_class_loss: 0.0134 - rpn_bbox_loss: 0.1034 - mrcnn_class_loss: 0.0498 - mrcnn_bbox_loss: 0.0220 - mrcnn_mask_loss: 0.1137 - val_loss: 7.5646 - val_rpn_class_loss: 1.4109 - val_rpn_bbox_loss: 3.5679 - val_mrcnn_class_loss: 1.2294 - val_mrcnn_bbox_loss: 0.5752 - val_mrcnn_mask_loss: 0.7812  ERROR:root:Error processing image {'id': 1, 'source': 'coco', 'path': '/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/tio/train2017/_0P04ZWQtMtPMwx3lgLdWA.jpg', 'width': 2592, 'height': 1936, 'annotations': [{'id': 1, 'image_id': 1, 'category_id': 9, 'iscrowd': 0, 'area': 2153, 'bbox': [1198.0, 397.0, 38.0, 78.0], 'segmentation': [[1208.0, 474.5, 1197.5, 403.0, 1235.0, 396.5, 1229.5, 423.0, 1235.5, 467.0, 1208.0, 474.5]], 'width': 2592, 'height': 1936}, {'id': 2, 'image_id': 1, 'category_id': 12, 'iscrowd': 0, 'area': 1767, 'bbox': [1240.0, 862.0, 39.0, 56.0], 'segmentation': [[1244.0, 917.5, 1239.5, 889.0, 1240.5, 868.0, 1272.0, 861.5, 1278.5, 914.0, 1244.0, 917.5]], 'width': 2592, 'height': 1936}, {'id': 3, 'image_id': 1, 'category_id': 30, 'iscrowd': 0, 'area': 23079, 'bbox': [0.0, 1012.0, 217.0, 190.0], 'segmentation': [[0.0, 1201.5, 0.0, 1011.5, 13.0, 1041.5, 137.0, 1038.5, 204.5, 1055.0, 216.0, 1136.5, 134.0, 1136.5, 55.0, 1155.5, 0.0, 1201.5]], 'width': 2592, 'height': 1936}, {'id': 4, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 13454, 'bbox': [285.0, 1117.0, 137.0, 129.0], 'segmentation': [[289.0, 1245.5, 293.5, 1208.0, 290.5, 1177.0, 284.5, 1167.0, 293.5, 1161.0, 316.5, 1129.0, 366.0, 1117.5, 412.0, 1116.5, 421.5, 1216.0, 404.0, 1219.5, 400.5, 1233.0, 391.0, 1236.5, 385.0, 1235.5, 375.0, 1224.5, 371.0, 1224.5, 362.0, 1226.5, 352.0, 1238.5, 345.0, 1239.5, 334.0, 1229.5, 327.0, 1229.5, 306.0, 1232.5, 299.0, 1244.5, 289.0, 1245.5]], 'width': 2592, 'height': 1936}, {'id': 5, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 34185, 'bbox': [0.0, 1136.0, 294.0, 164.0], 'segmentation': [[9.0, 1299.5, 0, 1299.0, 0, 1202.0, 55.0, 1155.5, 134.0, 1136.5, 218.0, 1135.5, 284.0, 1166.5, 290.5, 1177.0, 293.5, 1216.0, 289.5, 1240.0, 283.5, 1251.0, 274.0, 1257.5, 250.0, 1257.5, 243.0, 1246.5, 238.0, 1246.5, 167.0, 1257.5, 168.5, 1266.0, 163.0, 1275.5, 135.0, 1283.5, 125.0, 1278.5, 121.0, 1272.5, 15.0, 1281.5, 15.5, 1293.0, 9.0, 1299.5]], 'width': 2592, 'height': 1936}, {'id': 6, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 14170, 'bbox': [2474.0, 902.0, 118.0, 181.0], 'segmentation': [[2538.0, 1082.5, 2507.0, 1079.5, 2486.0, 1061.5, 2481.5, 1055.0, 2473.5, 1021.0, 2541.5, 934.0, 2591.0, 901.5, 2591.5, 1067.0, 2538.0, 1082.5]], 'width': 2592, 'height': 1936}, {'id': 7, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 49052, 'bbox': [840.0, 1024.0, 296.0, 219.0], 'segmentation': [[956.0, 1242.5, 943.0, 1242.5, 927.5, 1238.0, 906.5, 1039.0, 946.0, 1030.5, 1038.0, 1023.5, 1115.0, 1025.5, 1121.5, 1042.0, 1133.5, 1137.0, 1135.5, 1193.0, 1124.5, 1199.0, 1114.0, 1216.5, 1102.0, 1220.5, 1077.0, 1218.5, 1066.0, 1203.5, 986.0, 1211.5, 980.5, 1234.0, 956.0, 1242.5], [874.0, 1230.5, 863.0, 1230.5, 851.5, 1220.0, 845.5, 1171.0, 854.5, 1117.0, 840.5, 1106.0, 839.5, 1099.0, 845.0, 1088.5, 851.0, 1088.5, 859.5, 1094.0, 857.5, 1081.0, 865.5, 1056.0, 881.0, 1042.5, 888.0, 1041.5, 905.5, 1209.0, 887.0, 1209.5, 882.5, 1227.0, 874.0, 1230.5]], 'width': 2592, 'height': 1936}, {'id': 8, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 38963, 'bbox': [1049.0, 1201.0, 725.0, 90.0], 'segmentation': [[1669.0, 1269.5, 1665.5, 1264.0, 1661.0, 1207.5, 1758.5, 1201.0, 1770.5, 1227.0, 1773.0, 1263.5, 1669.0, 1269.5], [1103.0, 1289.5, 1050.5, 1289.0, 1159.0, 1231.5, 1636.0, 1208.5, 1639.5, 1212.0, 1641.0, 1270.5, 1376.0, 1284.5, 1103.0, 1289.5]], 'width': 2592, 'height': 1936}, {'id': 9, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 383152, 'bbox': [984.0, 1373.0, 1608.0, 453.0], 'segmentation': [[2186.0, 1825.5, 2111.5, 1807.0, 2105.5, 1706.0, 2109.5, 1681.0, 2104.5, 1643.0, 2096.5, 1636.0, 2104.5, 1611.0, 2082.0, 1580.5, 2052.0, 1571.5, 2027.0, 1580.5, 2016.5, 1600.0, 2023.5, 1619.0, 2027.5, 1687.0, 2034.5, 1706.0, 2039.0, 1790.5, 1629.0, 1693.5, 1214.0, 1606.5, 983.5, 1572.0, 1954.0, 1372.5, 2335.0, 1399.5, 2327.5, 1414.0, 2327.5, 1429.0, 2336.5, 1440.0, 2329.5, 1443.0, 2329.5, 1472.0, 2337.0, 1472.5, 2338.5, 1479.0, 2361.5, 1751.0, 2186.0, 1825.5], [2409.0, 1731.5, 2385.5, 1481.0, 2386.0, 1475.5, 2394.5, 1473.0, 2393.5, 1442.0, 2385.5, 1440.0, 2388.5, 1438.0, 2390.5, 1422.0, 2388.5, 1412.0, 2380.5, 1403.0, 2437.0, 1405.5, 2591.0, 1427.5, 2591.5, 1650.0, 2506.0, 1690.5, 2409.0, 1731.5]], 'width': 2592, 'height': 1936}, {'id': 10, 'image_id': 1, 'category_id': 8, 'iscrowd': 0, 'area': 52969, 'bbox': [0.0, 1370.0, 566.0, 193.0], 'segmentation': [[476.0, 1407.5, 472.5, 1405.0, 472.0, 1377.5, 513.0, 1372.5, 563.5, 1371.0, 476.0, 1407.5], [24.0, 1562.5, 0, 1562.0, 0.0, 1399.5, 436.5, 1381.0, 440.0, 1422.5, 110.0, 1556.5, 24.0, 1562.5]], 'width': 2592, 'height': 1936}, {'id': 11, 'image_id': 1, 'category_id': 4, 'iscrowd': 0, 'area': 4182, 'bbox': [1694.0, 917.0, 41.0, 173.0], 'segmentation': [[1712.0, 1089.5, 1711.5, 1027.0, 1707.0, 1019.5, 1698.0, 1022.5, 1693.5, 1010.0, 1693.5, 960.0, 1705.0, 944.5, 1712.0, 953.5, 1728.5, 953.0, 1724.5, 921.0, 1728.0, 916.5, 1734.5, 1088.0, 1712.0, 1089.5]], 'width': 2592, 'height': 1936}, {'id': 12, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8979, 'bbox': [866.0, 253.0, 209.0, 772.0], 'segmentation': [[1052.0, 889.5, 1047.5, 886.0, 997.5, 376.0, 983.5, 272.0, 875.0, 269.5, 866.0, 262.5, 990.0, 252.5, 991.5, 255.0, 1063.5, 882.0, 1063.0, 887.5, 1052.0, 889.5], [1068.0, 971.5, 1068.0, 966.5, 1068.0, 971.5], [1057.0, 976.5, 1057.0, 967.5, 1057.0, 976.5], [1069.0, 981.5, 1069.0, 974.5, 1069.0, 981.5], [1074.0, 1024.5, 1061.5, 1024.0, 1058.0, 976.5, 1060.5, 997.0, 1070.0, 997.5, 1071.5, 994.0, 1074.0, 1024.5], [1070.0, 991.5, 1070.0, 984.5, 1070.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 13, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 527, 'bbox': [1067.0, 951.0, 15.0, 75.0], 'segmentation': [[1081.0, 1025.5, 1075.0, 1024.5, 1073.5, 1021.0, 1067.0, 950.5, 1075.0, 950.5, 1076.5, 956.0, 1073.5, 957.0, 1073.5, 961.0, 1081.0, 1025.5]], 'width': 2592, 'height': 1936}, {'id': 14, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 27156, 'bbox': [2328.0, 1395.0, 100.0, 541.0], 'segmentation': [[2427.0, 1935.5, 2378.0, 1935.5, 2376.5, 1929.0, 2338.5, 1479.0, 2337.0, 1472.5, 2329.5, 1472.0, 2329.5, 1443.0, 2336.5, 1440.0, 2327.5, 1429.0, 2327.5, 1414.0, 2339.0, 1395.5, 2360.0, 1394.5, 2378.0, 1400.5, 2388.5, 1412.0, 2390.5, 1422.0, 2388.5, 1438.0, 2385.5, 1440.0, 2393.5, 1442.0, 2394.5, 1473.0, 2386.0, 1475.5, 2385.5, 1481.0, 2427.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 15, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 6705, 'bbox': [877.0, 954.0, 59.0, 356.0], 'segmentation': [[920.0, 1309.5, 910.5, 1309.0, 913.5, 1288.0, 894.5, 1086.0, 876.5, 960.0, 877.0, 956.5, 899.0, 953.5, 914.5, 1131.0, 935.5, 1301.0, 935.0, 1305.5, 920.0, 1309.5]], 'width': 2592, 'height': 1936}, {'id': 16, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 1177, 'bbox': [617.0, 1014.0, 23.0, 146.0], 'segmentation': [[639.0, 1159.5, 630.0, 1159.5, 628.5, 1153.0, 616.5, 1022.0, 616.5, 1017.0, 622.0, 1013.5, 639.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 17, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 379, 'bbox': [869.0, 882.0, 36.0, 47.0], 'segmentation': [[878.0, 928.5, 872.5, 928.0, 868.5, 882.0, 904.0, 881.5, 875.5, 887.0, 878.0, 928.5]], 'width': 2592, 'height': 1936}, {'id': 18, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 3889, 'bbox': [1728.0, 893.0, 31.0, 267.0], 'segmentation': [[1746.0, 1159.5, 1736.5, 1159.0, 1728.0, 892.5, 1740.5, 896.0, 1740.5, 911.0, 1732.5, 930.0, 1731.5, 957.0, 1738.5, 978.0, 1747.5, 991.0, 1758.5, 1131.0, 1758.5, 1153.0, 1746.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 19, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 9692, 'bbox': [1745.0, 882.0, 61.0, 412.0], 'segmentation': [[1792.0, 1293.5, 1771.0, 1290.5, 1768.5, 1282.0, 1769.0, 1279.5, 1774.5, 1279.0, 1774.5, 1274.0, 1744.5, 890.0, 1745.0, 883.5, 1761.0, 881.5, 1767.5, 884.0, 1796.5, 1270.0, 1797.5, 1277.0, 1805.5, 1282.0, 1805.0, 1290.5, 1792.0, 1293.5]], 'width': 2592, 'height': 1936}, {'id': 20, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8559, 'bbox': [1616.0, 949.0, 64.0, 376.0], 'segmentation': [[1660.0, 1324.5, 1637.5, 1324.0, 1637.5, 1313.0, 1643.5, 1308.0, 1643.5, 1298.0, 1638.5, 1196.0, 1627.5, 1085.0, 1636.0, 1089.5, 1636.5, 1087.0, 1629.5, 1056.0, 1624.5, 1055.0, 1615.5, 959.0, 1616.0, 952.5, 1638.0, 948.5, 1668.5, 1297.0, 1669.5, 1303.0, 1677.5, 1308.0, 1679.5, 1319.0, 1679.0, 1322.5, 1660.0, 1324.5]], 'width': 2592, 'height': 1936}, {'id': 21, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 26362, 'bbox': [2017.0, 1572.0, 103.0, 364.0], 'segmentation': [[2119.0, 1935.5, 2048.5, 1935.0, 2034.5, 1706.0, 2027.5, 1687.0, 2023.5, 1619.0, 2016.5, 1600.0, 2027.0, 1580.5, 2052.0, 1571.5, 2082.0, 1580.5, 2104.5, 1611.0, 2096.5, 1636.0, 2104.5, 1643.0, 2109.5, 1681.0, 2105.5, 1706.0, 2119.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 22, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 18515, 'bbox': [402.0, 982.0, 92.0, 520.0], 'segmentation': [[434.0, 1501.5, 433.5, 1489.0, 445.5, 1480.0, 445.5, 1475.0, 401.5, 993.0, 402.0, 985.5, 431.0, 981.5, 436.5, 982.0, 473.5, 1417.0, 479.5, 1431.0, 483.5, 1475.0, 493.5, 1480.0, 493.0, 1496.5, 434.0, 1501.5]], 'width': 2592, 'height': 1936}, {'id': 23, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 421, 'bbox': [1007.0, 926.0, 12.0, 65.0], 'segmentation': [[1018.0, 990.5, 1011.5, 990.0, 1007.0, 925.5, 1012.5, 926.0, 1018.0, 990.5]], 'width': 2592, 'height': 1936}, {'id': 24, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 1107, 'bbox': [798.0, 259.0, 77.0, 20.0], 'segmentation': [[798.0, 278.5, 797.5, 263.0, 804.0, 258.5, 859.0, 258.5, 874.5, 269.0, 798.0, 278.5]], 'width': 2592, 'height': 1936}, {'id': 25, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 73, 'bbox': [904.0, 877.0, 17.0, 8.0], 'segmentation': [[904.0, 884.5, 907.0, 878.5, 917.0, 876.5, 920.5, 882.0, 904.0, 884.5]], 'width': 2592, 'height': 1936}, {'id': 26, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2731, 'bbox': [868.0, 928.0, 53.0, 90.0], 'segmentation': [[876.0, 1017.5, 867.5, 952.0, 867.5, 929.0, 918.0, 927.5, 914.5, 978.0, 920.5, 1012.0, 919.0, 1014.5, 904.5, 1015.0, 899.0, 953.5, 876.5, 957.0, 884.5, 1016.0, 876.0, 1017.5]], 'width': 2592, 'height': 1936}, {'id': 27, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 12971, 'bbox': [331.0, 822.0, 116.0, 166.0], 'segmentation': [[390.0, 987.5, 381.5, 985.0, 378.5, 964.0, 355.5, 950.0, 334.5, 911.0, 373.0, 906.5, 373.5, 901.0, 372.5, 895.0, 358.5, 876.0, 332.5, 856.0, 330.5, 837.0, 363.0, 836.5, 372.0, 829.5, 394.0, 830.5, 396.0, 824.5, 419.0, 821.5, 421.5, 831.0, 431.0, 831.5, 432.5, 837.0, 446.5, 980.0, 390.0, 987.5]], 'width': 2592, 'height': 1936}, {'id': 28, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 6689, 'bbox': [1599.0, 835.0, 73.0, 120.0], 'segmentation': [[1609.0, 954.5, 1598.5, 846.0, 1608.0, 837.5, 1618.0, 834.5, 1625.0, 834.5, 1628.0, 841.5, 1658.0, 836.5, 1666.0, 838.5, 1667.5, 846.0, 1655.5, 890.0, 1667.0, 890.5, 1671.5, 896.0, 1659.5, 940.0, 1634.0, 950.5, 1609.0, 954.5]], 'width': 2592, 'height': 1936}, {'id': 29, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2285, 'bbox': [1682.0, 880.0, 47.0, 74.0], 'segmentation': [[1728.0, 953.5, 1712.0, 953.5, 1704.0, 942.5, 1695.5, 942.0, 1684.5, 917.0, 1691.5, 912.0, 1682.0, 888.5, 1717.0, 883.5, 1720.5, 880.0, 1728.0, 953.5]], 'width': 2592, 'height': 1936}, {'id': 30, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 16074, 'bbox': [1709.0, 676.0, 104.0, 211.0], 'segmentation': [[1725.0, 886.5, 1708.5, 689.0, 1776.0, 675.5, 1782.5, 680.0, 1786.5, 717.0, 1802.5, 723.0, 1803.5, 739.0, 1790.0, 743.5, 1789.5, 753.0, 1790.5, 762.0, 1806.5, 773.0, 1807.5, 787.0, 1792.5, 788.0, 1792.5, 808.0, 1812.5, 819.0, 1805.5, 840.0, 1796.5, 842.0, 1795.0, 878.5, 1725.0, 886.5]], 'width': 2592, 'height': 1936}, {'id': 31, 'image_id': 1, 'category_id': 25, 'iscrowd': 0, 'area': 4717, 'bbox': [1732.0, 882.0, 83.0, 115.0], 'segmentation': [[1780.0, 996.5, 1775.5, 996.0, 1774.5, 986.0, 1767.5, 896.0, 1767.5, 883.0, 1770.0, 881.5, 1783.0, 888.5, 1800.5, 904.0, 1809.5, 926.0, 1814.5, 949.0, 1808.5, 974.0, 1790.0, 994.5, 1780.0, 996.5], [1751.0, 991.5, 1739.5, 980.0, 1731.5, 957.0, 1732.5, 930.0, 1745.0, 902.5, 1751.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 32, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 4552, 'bbox': [1023.0, 885.0, 69.0, 85.0], 'segmentation': [[1031.0, 969.5, 1022.5, 893.0, 1085.0, 884.5, 1091.5, 947.0, 1067.0, 950.5, 1067.0, 965.5, 1031.0, 969.5], [1076.0, 966.5, 1074.0, 956.5, 1076.5, 957.0, 1076.0, 966.5]], 'width': 2592, 'height': 1936}, {'id': 33, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 3513, 'bbox': [1076.0, 948.0, 63.0, 60.0], 'segmentation': [[1080.0, 1007.5, 1075.5, 948.0, 1135.0, 947.5, 1138.5, 1006.0, 1080.0, 1007.5]], 'width': 2592, 'height': 1936}, {'id': 34, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 329, 'bbox': [1058.0, 966.0, 13.0, 32.0], 'segmentation': [[1070.0, 997.5, 1060.5, 997.0, 1058.0, 965.5, 1067.5, 966.0, 1070.0, 997.5]], 'width': 2592, 'height': 1936}]}  Traceback (most recent call last):    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1211, in load_image_gt      image = dataset.load_image(image_id)    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/utils.py"", line 358, in load_image      image = skimage.io.imread(self.image_info[image_id]['path'])    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py"", line 61, in imread      img = call_plugin('imread', fname, plugin=plugin, **plugin_args)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/manage_plugins.py"", line 211, in call_plugin      return func(*args, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_plugins/pil_plugin.py"", line 36, in imread      im = Image.open(f)    File ""/usr/local/lib/python3.6/dist-packages/PIL/Image.py"", line 2419, in open      prefix = fp.read(16)  OSError: [Errno 5] Input/output error  ERROR:root:Error processing image {'id': 1, 'source': 'coco', 'path': '/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/tio/train2017/_0P04ZWQtMtPMwx3lgLdWA.jpg', 'width': 2592, 'height': 1936, 'annotations': [{'id': 1, 'image_id': 1, 'category_id': 9, 'iscrowd': 0, 'area': 2153, 'bbox': [1198.0, 397.0, 38.0, 78.0], 'segmentation': [[1208.0, 474.5, 1197.5, 403.0, 1235.0, 396.5, 1229.5, 423.0, 1235.5, 467.0, 1208.0, 474.5]], 'width': 2592, 'height': 1936}, {'id': 2, 'image_id': 1, 'category_id': 12, 'iscrowd': 0, 'area': 1767, 'bbox': [1240.0, 862.0, 39.0, 56.0], 'segmentation': [[1244.0, 917.5, 1239.5, 889.0, 1240.5, 868.0, 1272.0, 861.5, 1278.5, 914.0, 1244.0, 917.5]], 'width': 2592, 'height': 1936}, {'id': 3, 'image_id': 1, 'category_id': 30, 'iscrowd': 0, 'area': 23079, 'bbox': [0.0, 1012.0, 217.0, 190.0], 'segmentation': [[0.0, 1201.5, 0.0, 1011.5, 13.0, 1041.5, 137.0, 1038.5, 204.5, 1055.0, 216.0, 1136.5, 134.0, 1136.5, 55.0, 1155.5, 0.0, 1201.5]], 'width': 2592, 'height': 1936}, {'id': 4, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 13454, 'bbox': [285.0, 1117.0, 137.0, 129.0], 'segmentation': [[289.0, 1245.5, 293.5, 1208.0, 290.5, 1177.0, 284.5, 1167.0, 293.5, 1161.0, 316.5, 1129.0, 366.0, 1117.5, 412.0, 1116.5, 421.5, 1216.0, 404.0, 1219.5, 400.5, 1233.0, 391.0, 1236.5, 385.0, 1235.5, 375.0, 1224.5, 371.0, 1224.5, 362.0, 1226.5, 352.0, 1238.5, 345.0, 1239.5, 334.0, 1229.5, 327.0, 1229.5, 306.0, 1232.5, 299.0, 1244.5, 289.0, 1245.5]], 'width': 2592, 'height': 1936}, {'id': 5, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 34185, 'bbox': [0.0, 1136.0, 294.0, 164.0], 'segmentation': [[9.0, 1299.5, 0, 1299.0, 0, 1202.0, 55.0, 1155.5, 134.0, 1136.5, 218.0, 1135.5, 284.0, 1166.5, 290.5, 1177.0, 293.5, 1216.0, 289.5, 1240.0, 283.5, 1251.0, 274.0, 1257.5, 250.0, 1257.5, 243.0, 1246.5, 238.0, 1246.5, 167.0, 1257.5, 168.5, 1266.0, 163.0, 1275.5, 135.0, 1283.5, 125.0, 1278.5, 121.0, 1272.5, 15.0, 1281.5, 15.5, 1293.0, 9.0, 1299.5]], 'width': 2592, 'height': 1936}, {'id': 6, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 14170, 'bbox': [2474.0, 902.0, 118.0, 181.0], 'segmentation': [[2538.0, 1082.5, 2507.0, 1079.5, 2486.0, 1061.5, 2481.5, 1055.0, 2473.5, 1021.0, 2541.5, 934.0, 2591.0, 901.5, 2591.5, 1067.0, 2538.0, 1082.5]], 'width': 2592, 'height': 1936}, {'id': 7, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 49052, 'bbox': [840.0, 1024.0, 296.0, 219.0], 'segmentation': [[956.0, 1242.5, 943.0, 1242.5, 927.5, 1238.0, 906.5, 1039.0, 946.0, 1030.5, 1038.0, 1023.5, 1115.0, 1025.5, 1121.5, 1042.0, 1133.5, 1137.0, 1135.5, 1193.0, 1124.5, 1199.0, 1114.0, 1216.5, 1102.0, 1220.5, 1077.0, 1218.5, 1066.0, 1203.5, 986.0, 1211.5, 980.5, 1234.0, 956.0, 1242.5], [874.0, 1230.5, 863.0, 1230.5, 851.5, 1220.0, 845.5, 1171.0, 854.5, 1117.0, 840.5, 1106.0, 839.5, 1099.0, 845.0, 1088.5, 851.0, 1088.5, 859.5, 1094.0, 857.5, 1081.0, 865.5, 1056.0, 881.0, 1042.5, 888.0, 1041.5, 905.5, 1209.0, 887.0, 1209.5, 882.5, 1227.0, 874.0, 1230.5]], 'width': 2592, 'height': 1936}, {'id': 8, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 38963, 'bbox': [1049.0, 1201.0, 725.0, 90.0], 'segmentation': [[1669.0, 1269.5, 1665.5, 1264.0, 1661.0, 1207.5, 1758.5, 1201.0, 1770.5, 1227.0, 1773.0, 1263.5, 1669.0, 1269.5], [1103.0, 1289.5, 1050.5, 1289.0, 1159.0, 1231.5, 1636.0, 1208.5, 1639.5, 1212.0, 1641.0, 1270.5, 1376.0, 1284.5, 1103.0, 1289.5]], 'width': 2592, 'height': 1936}, {'id': 9, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 383152, 'bbox': [984.0, 1373.0, 1608.0, 453.0], 'segmentation': [[2186.0, 1825.5, 2111.5, 1807.0, 2105.5, 1706.0, 2109.5, 1681.0, 2104.5, 1643.0, 2096.5, 1636.0, 2104.5, 1611.0, 2082.0, 1580.5, 2052.0, 1571.5, 2027.0, 1580.5, 2016.5, 1600.0, 2023.5, 1619.0, 2027.5, 1687.0, 2034.5, 1706.0, 2039.0, 1790.5, 1629.0, 1693.5, 1214.0, 1606.5, 983.5, 1572.0, 1954.0, 1372.5, 2335.0, 1399.5, 2327.5, 1414.0, 2327.5, 1429.0, 2336.5, 1440.0, 2329.5, 1443.0, 2329.5, 1472.0, 2337.0, 1472.5, 2338.5, 1479.0, 2361.5, 1751.0, 2186.0, 1825.5], [2409.0, 1731.5, 2385.5, 1481.0, 2386.0, 1475.5, 2394.5, 1473.0, 2393.5, 1442.0, 2385.5, 1440.0, 2388.5, 1438.0, 2390.5, 1422.0, 2388.5, 1412.0, 2380.5, 1403.0, 2437.0, 1405.5, 2591.0, 1427.5, 2591.5, 1650.0, 2506.0, 1690.5, 2409.0, 1731.5]], 'width': 2592, 'height': 1936}, {'id': 10, 'image_id': 1, 'category_id': 8, 'iscrowd': 0, 'area': 52969, 'bbox': [0.0, 1370.0, 566.0, 193.0], 'segmentation': [[476.0, 1407.5, 472.5, 1405.0, 472.0, 1377.5, 513.0, 1372.5, 563.5, 1371.0, 476.0, 1407.5], [24.0, 1562.5, 0, 1562.0, 0.0, 1399.5, 436.5, 1381.0, 440.0, 1422.5, 110.0, 1556.5, 24.0, 1562.5]], 'width': 2592, 'height': 1936}, {'id': 11, 'image_id': 1, 'category_id': 4, 'iscrowd': 0, 'area': 4182, 'bbox': [1694.0, 917.0, 41.0, 173.0], 'segmentation': [[1712.0, 1089.5, 1711.5, 1027.0, 1707.0, 1019.5, 1698.0, 1022.5, 1693.5, 1010.0, 1693.5, 960.0, 1705.0, 944.5, 1712.0, 953.5, 1728.5, 953.0, 1724.5, 921.0, 1728.0, 916.5, 1734.5, 1088.0, 1712.0, 1089.5]], 'width': 2592, 'height': 1936}, {'id': 12, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8979, 'bbox': [866.0, 253.0, 209.0, 772.0], 'segmentation': [[1052.0, 889.5, 1047.5, 886.0, 997.5, 376.0, 983.5, 272.0, 875.0, 269.5, 866.0, 262.5, 990.0, 252.5, 991.5, 255.0, 1063.5, 882.0, 1063.0, 887.5, 1052.0, 889.5], [1068.0, 971.5, 1068.0, 966.5, 1068.0, 971.5], [1057.0, 976.5, 1057.0, 967.5, 1057.0, 976.5], [1069.0, 981.5, 1069.0, 974.5, 1069.0, 981.5], [1074.0, 1024.5, 1061.5, 1024.0, 1058.0, 976.5, 1060.5, 997.0, 1070.0, 997.5, 1071.5, 994.0, 1074.0, 1024.5], [1070.0, 991.5, 1070.0, 984.5, 1070.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 13, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 527, 'bbox': [1067.0, 951.0, 15.0, 75.0], 'segmentation': [[1081.0, 1025.5, 1075.0, 1024.5, 1073.5, 1021.0, 1067.0, 950.5, 1075.0, 950.5, 1076.5, 956.0, 1073.5, 957.0, 1073.5, 961.0, 1081.0, 1025.5]], 'width': 2592, 'height': 1936}, {'id': 14, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 27156, 'bbox': [2328.0, 1395.0, 100.0, 541.0], 'segmentation': [[2427.0, 1935.5, 2378.0, 1935.5, 2376.5, 1929.0, 2338.5, 1479.0, 2337.0, 1472.5, 2329.5, 1472.0, 2329.5, 1443.0, 2336.5, 1440.0, 2327.5, 1429.0, 2327.5, 1414.0, 2339.0, 1395.5, 2360.0, 1394.5, 2378.0, 1400.5, 2388.5, 1412.0, 2390.5, 1422.0, 2388.5, 1438.0, 2385.5, 1440.0, 2393.5, 1442.0, 2394.5, 1473.0, 2386.0, 1475.5, 2385.5, 1481.0, 2427.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 15, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 6705, 'bbox': [877.0, 954.0, 59.0, 356.0], 'segmentation': [[920.0, 1309.5, 910.5, 1309.0, 913.5, 1288.0, 894.5, 1086.0, 876.5, 960.0, 877.0, 956.5, 899.0, 953.5, 914.5, 1131.0, 935.5, 1301.0, 935.0, 1305.5, 920.0, 1309.5]], 'width': 2592, 'height': 1936}, {'id': 16, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 1177, 'bbox': [617.0, 1014.0, 23.0, 146.0], 'segmentation': [[639.0, 1159.5, 630.0, 1159.5, 628.5, 1153.0, 616.5, 1022.0, 616.5, 1017.0, 622.0, 1013.5, 639.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 17, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 379, 'bbox': [869.0, 882.0, 36.0, 47.0], 'segmentation': [[878.0, 928.5, 872.5, 928.0, 868.5, 882.0, 904.0, 881.5, 875.5, 887.0, 878.0, 928.5]], 'width': 2592, 'height': 1936}, {'id': 18, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 3889, 'bbox': [1728.0, 893.0, 31.0, 267.0], 'segmentation': [[1746.0, 1159.5, 1736.5, 1159.0, 1728.0, 892.5, 1740.5, 896.0, 1740.5, 911.0, 1732.5, 930.0, 1731.5, 957.0, 1738.5, 978.0, 1747.5, 991.0, 1758.5, 1131.0, 1758.5, 1153.0, 1746.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 19, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 9692, 'bbox': [1745.0, 882.0, 61.0, 412.0], 'segmentation': [[1792.0, 1293.5, 1771.0, 1290.5, 1768.5, 1282.0, 1769.0, 1279.5, 1774.5, 1279.0, 1774.5, 1274.0, 1744.5, 890.0, 1745.0, 883.5, 1761.0, 881.5, 1767.5, 884.0, 1796.5, 1270.0, 1797.5, 1277.0, 1805.5, 1282.0, 1805.0, 1290.5, 1792.0, 1293.5]], 'width': 2592, 'height': 1936}, {'id': 20, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8559, 'bbox': [1616.0, 949.0, 64.0, 376.0], 'segmentation': [[1660.0, 1324.5, 1637.5, 1324.0, 1637.5, 1313.0, 1643.5, 1308.0, 1643.5, 1298.0, 1638.5, 1196.0, 1627.5, 1085.0, 1636.0, 1089.5, 1636.5, 1087.0, 1629.5, 1056.0, 1624.5, 1055.0, 1615.5, 959.0, 1616.0, 952.5, 1638.0, 948.5, 1668.5, 1297.0, 1669.5, 1303.0, 1677.5, 1308.0, 1679.5, 1319.0, 1679.0, 1322.5, 1660.0, 1324.5]], 'width': 2592, 'height': 1936}, {'id': 21, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 26362, 'bbox': [2017.0, 1572.0, 103.0, 364.0], 'segmentation': [[2119.0, 1935.5, 2048.5, 1935.0, 2034.5, 1706.0, 2027.5, 1687.0, 2023.5, 1619.0, 2016.5, 1600.0, 2027.0, 1580.5, 2052.0, 1571.5, 2082.0, 1580.5, 2104.5, 1611.0, 2096.5, 1636.0, 2104.5, 1643.0, 2109.5, 1681.0, 2105.5, 1706.0, 2119.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 22, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 18515, 'bbox': [402.0, 982.0, 92.0, 520.0], 'segmentation': [[434.0, 1501.5, 433.5, 1489.0, 445.5, 1480.0, 445.5, 1475.0, 401.5, 993.0, 402.0, 985.5, 431.0, 981.5, 436.5, 982.0, 473.5, 1417.0, 479.5, 1431.0, 483.5, 1475.0, 493.5, 1480.0, 493.0, 1496.5, 434.0, 1501.5]], 'width': 2592, 'height': 1936}, {'id': 23, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 421, 'bbox': [1007.0, 926.0, 12.0, 65.0], 'segmentation': [[1018.0, 990.5, 1011.5, 990.0, 1007.0, 925.5, 1012.5, 926.0, 1018.0, 990.5]], 'width': 2592, 'height': 1936}, {'id': 24, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 1107, 'bbox': [798.0, 259.0, 77.0, 20.0], 'segmentation': [[798.0, 278.5, 797.5, 263.0, 804.0, 258.5, 859.0, 258.5, 874.5, 269.0, 798.0, 278.5]], 'width': 2592, 'height': 1936}, {'id': 25, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 73, 'bbox': [904.0, 877.0, 17.0, 8.0], 'segmentation': [[904.0, 884.5, 907.0, 878.5, 917.0, 876.5, 920.5, 882.0, 904.0, 884.5]], 'width': 2592, 'height': 1936}, {'id': 26, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2731, 'bbox': [868.0, 928.0, 53.0, 90.0], 'segmentation': [[876.0, 1017.5, 867.5, 952.0, 867.5, 929.0, 918.0, 927.5, 914.5, 978.0, 920.5, 1012.0, 919.0, 1014.5, 904.5, 1015.0, 899.0, 953.5, 876.5, 957.0, 884.5, 1016.0, 876.0, 1017.5]], 'width': 2592, 'height': 1936}, {'id': 27, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 12971, 'bbox': [331.0, 822.0, 116.0, 166.0], 'segmentation': [[390.0, 987.5, 381.5, 985.0, 378.5, 964.0, 355.5, 950.0, 334.5, 911.0, 373.0, 906.5, 373.5, 901.0, 372.5, 895.0, 358.5, 876.0, 332.5, 856.0, 330.5, 837.0, 363.0, 836.5, 372.0, 829.5, 394.0, 830.5, 396.0, 824.5, 419.0, 821.5, 421.5, 831.0, 431.0, 831.5, 432.5, 837.0, 446.5, 980.0, 390.0, 987.5]], 'width': 2592, 'height': 1936}, {'id': 28, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 6689, 'bbox': [1599.0, 835.0, 73.0, 120.0], 'segmentation': [[1609.0, 954.5, 1598.5, 846.0, 1608.0, 837.5, 1618.0, 834.5, 1625.0, 834.5, 1628.0, 841.5, 1658.0, 836.5, 1666.0, 838.5, 1667.5, 846.0, 1655.5, 890.0, 1667.0, 890.5, 1671.5, 896.0, 1659.5, 940.0, 1634.0, 950.5, 1609.0, 954.5]], 'width': 2592, 'height': 1936}, {'id': 29, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2285, 'bbox': [1682.0, 880.0, 47.0, 74.0], 'segmentation': [[1728.0, 953.5, 1712.0, 953.5, 1704.0, 942.5, 1695.5, 942.0, 1684.5, 917.0, 1691.5, 912.0, 1682.0, 888.5, 1717.0, 883.5, 1720.5, 880.0, 1728.0, 953.5]], 'width': 2592, 'height': 1936}, {'id': 30, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 16074, 'bbox': [1709.0, 676.0, 104.0, 211.0], 'segmentation': [[1725.0, 886.5, 1708.5, 689.0, 1776.0, 675.5, 1782.5, 680.0, 1786.5, 717.0, 1802.5, 723.0, 1803.5, 739.0, 1790.0, 743.5, 1789.5, 753.0, 1790.5, 762.0, 1806.5, 773.0, 1807.5, 787.0, 1792.5, 788.0, 1792.5, 808.0, 1812.5, 819.0, 1805.5, 840.0, 1796.5, 842.0, 1795.0, 878.5, 1725.0, 886.5]], 'width': 2592, 'height': 1936}, {'id': 31, 'image_id': 1, 'category_id': 25, 'iscrowd': 0, 'area': 4717, 'bbox': [1732.0, 882.0, 83.0, 115.0], 'segmentation': [[1780.0, 996.5, 1775.5, 996.0, 1774.5, 986.0, 1767.5, 896.0, 1767.5, 883.0, 1770.0, 881.5, 1783.0, 888.5, 1800.5, 904.0, 1809.5, 926.0, 1814.5, 949.0, 1808.5, 974.0, 1790.0, 994.5, 1780.0, 996.5], [1751.0, 991.5, 1739.5, 980.0, 1731.5, 957.0, 1732.5, 930.0, 1745.0, 902.5, 1751.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 32, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 4552, 'bbox': [1023.0, 885.0, 69.0, 85.0], 'segmentation': [[1031.0, 969.5, 1022.5, 893.0, 1085.0, 884.5, 1091.5, 947.0, 1067.0, 950.5, 1067.0, 965.5, 1031.0, 969.5], [1076.0, 966.5, 1074.0, 956.5, 1076.5, 957.0, 1076.0, 966.5]], 'width': 2592, 'height': 1936}, {'id': 33, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 3513, 'bbox': [1076.0, 948.0, 63.0, 60.0], 'segmentation': [[1080.0, 1007.5, 1075.5, 948.0, 1135.0, 947.5, 1138.5, 1006.0, 1080.0, 1007.5]], 'width': 2592, 'height': 1936}, {'id': 34, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 329, 'bbox': [1058.0, 966.0, 13.0, 32.0], 'segmentation': [[1070.0, 997.5, 1060.5, 997.0, 1058.0, 965.5, 1067.5, 966.0, 1070.0, 997.5]], 'width': 2592, 'height': 1936}]}  Traceback (most recent call last):    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1211, in load_image_gt      image = dataset.load_image(image_id)    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/utils.py"", line 358, in load_image      image = skimage.io.imread(self.image_info[image_id]['path'])    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py"", line 61, in imread      img = call_plugin('imread', fname, plugin=plugin, **plugin_args)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/manage_plugins.py"", line 211, in call_plugin      return func(*args, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_plugins/pil_plugin.py"", line 36, in imread      im = Image.open(f)    File ""/usr/local/lib/python3.6/dist-packages/PIL/Image.py"", line 2419, in open      prefix = fp.read(16)  OSError: [Errno 5] Input/output error  ERROR:root:Error processing image {'id': 1, 'source': 'coco', 'path': '/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/tio/train2017/_0P04ZWQtMtPMwx3lgLdWA.jpg', 'width': 2592, 'height': 1936, 'annotations': [{'id': 1, 'image_id': 1, 'category_id': 9, 'iscrowd': 0, 'area': 2153, 'bbox': [1198.0, 397.0, 38.0, 78.0], 'segmentation': [[1208.0, 474.5, 1197.5, 403.0, 1235.0, 396.5, 1229.5, 423.0, 1235.5, 467.0, 1208.0, 474.5]], 'width': 2592, 'height': 1936}, {'id': 2, 'image_id': 1, 'category_id': 12, 'iscrowd': 0, 'area': 1767, 'bbox': [1240.0, 862.0, 39.0, 56.0], 'segmentation': [[1244.0, 917.5, 1239.5, 889.0, 1240.5, 868.0, 1272.0, 861.5, 1278.5, 914.0, 1244.0, 917.5]], 'width': 2592, 'height': 1936}, {'id': 3, 'image_id': 1, 'category_id': 30, 'iscrowd': 0, 'area': 23079, 'bbox': [0.0, 1012.0, 217.0, 190.0], 'segmentation': [[0.0, 1201.5, 0.0, 1011.5, 13.0, 1041.5, 137.0, 1038.5, 204.5, 1055.0, 216.0, 1136.5, 134.0, 1136.5, 55.0, 1155.5, 0.0, 1201.5]], 'width': 2592, 'height': 1936}, {'id': 4, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 13454, 'bbox': [285.0, 1117.0, 137.0, 129.0], 'segmentation': [[289.0, 1245.5, 293.5, 1208.0, 290.5, 1177.0, 284.5, 1167.0, 293.5, 1161.0, 316.5, 1129.0, 366.0, 1117.5, 412.0, 1116.5, 421.5, 1216.0, 404.0, 1219.5, 400.5, 1233.0, 391.0, 1236.5, 385.0, 1235.5, 375.0, 1224.5, 371.0, 1224.5, 362.0, 1226.5, 352.0, 1238.5, 345.0, 1239.5, 334.0, 1229.5, 327.0, 1229.5, 306.0, 1232.5, 299.0, 1244.5, 289.0, 1245.5]], 'width': 2592, 'height': 1936}, {'id': 5, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 34185, 'bbox': [0.0, 1136.0, 294.0, 164.0], 'segmentation': [[9.0, 1299.5, 0, 1299.0, 0, 1202.0, 55.0, 1155.5, 134.0, 1136.5, 218.0, 1135.5, 284.0, 1166.5, 290.5, 1177.0, 293.5, 1216.0, 289.5, 1240.0, 283.5, 1251.0, 274.0, 1257.5, 250.0, 1257.5, 243.0, 1246.5, 238.0, 1246.5, 167.0, 1257.5, 168.5, 1266.0, 163.0, 1275.5, 135.0, 1283.5, 125.0, 1278.5, 121.0, 1272.5, 15.0, 1281.5, 15.5, 1293.0, 9.0, 1299.5]], 'width': 2592, 'height': 1936}, {'id': 6, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 14170, 'bbox': [2474.0, 902.0, 118.0, 181.0], 'segmentation': [[2538.0, 1082.5, 2507.0, 1079.5, 2486.0, 1061.5, 2481.5, 1055.0, 2473.5, 1021.0, 2541.5, 934.0, 2591.0, 901.5, 2591.5, 1067.0, 2538.0, 1082.5]], 'width': 2592, 'height': 1936}, {'id': 7, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 49052, 'bbox': [840.0, 1024.0, 296.0, 219.0], 'segmentation': [[956.0, 1242.5, 943.0, 1242.5, 927.5, 1238.0, 906.5, 1039.0, 946.0, 1030.5, 1038.0, 1023.5, 1115.0, 1025.5, 1121.5, 1042.0, 1133.5, 1137.0, 1135.5, 1193.0, 1124.5, 1199.0, 1114.0, 1216.5, 1102.0, 1220.5, 1077.0, 1218.5, 1066.0, 1203.5, 986.0, 1211.5, 980.5, 1234.0, 956.0, 1242.5], [874.0, 1230.5, 863.0, 1230.5, 851.5, 1220.0, 845.5, 1171.0, 854.5, 1117.0, 840.5, 1106.0, 839.5, 1099.0, 845.0, 1088.5, 851.0, 1088.5, 859.5, 1094.0, 857.5, 1081.0, 865.5, 1056.0, 881.0, 1042.5, 888.0, 1041.5, 905.5, 1209.0, 887.0, 1209.5, 882.5, 1227.0, 874.0, 1230.5]], 'width': 2592, 'height': 1936}, {'id': 8, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 38963, 'bbox': [1049.0, 1201.0, 725.0, 90.0], 'segmentation': [[1669.0, 1269.5, 1665.5, 1264.0, 1661.0, 1207.5, 1758.5, 1201.0, 1770.5, 1227.0, 1773.0, 1263.5, 1669.0, 1269.5], [1103.0, 1289.5, 1050.5, 1289.0, 1159.0, 1231.5, 1636.0, 1208.5, 1639.5, 1212.0, 1641.0, 1270.5, 1376.0, 1284.5, 1103.0, 1289.5]], 'width': 2592, 'height': 1936}, {'id': 9, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 383152, 'bbox': [984.0, 1373.0, 1608.0, 453.0], 'segmentation': [[2186.0, 1825.5, 2111.5, 1807.0, 2105.5, 1706.0, 2109.5, 1681.0, 2104.5, 1643.0, 2096.5, 1636.0, 2104.5, 1611.0, 2082.0, 1580.5, 2052.0, 1571.5, 2027.0, 1580.5, 2016.5, 1600.0, 2023.5, 1619.0, 2027.5, 1687.0, 2034.5, 1706.0, 2039.0, 1790.5, 1629.0, 1693.5, 1214.0, 1606.5, 983.5, 1572.0, 1954.0, 1372.5, 2335.0, 1399.5, 2327.5, 1414.0, 2327.5, 1429.0, 2336.5, 1440.0, 2329.5, 1443.0, 2329.5, 1472.0, 2337.0, 1472.5, 2338.5, 1479.0, 2361.5, 1751.0, 2186.0, 1825.5], [2409.0, 1731.5, 2385.5, 1481.0, 2386.0, 1475.5, 2394.5, 1473.0, 2393.5, 1442.0, 2385.5, 1440.0, 2388.5, 1438.0, 2390.5, 1422.0, 2388.5, 1412.0, 2380.5, 1403.0, 2437.0, 1405.5, 2591.0, 1427.5, 2591.5, 1650.0, 2506.0, 1690.5, 2409.0, 1731.5]], 'width': 2592, 'height': 1936}, {'id': 10, 'image_id': 1, 'category_id': 8, 'iscrowd': 0, 'area': 52969, 'bbox': [0.0, 1370.0, 566.0, 193.0], 'segmentation': [[476.0, 1407.5, 472.5, 1405.0, 472.0, 1377.5, 513.0, 1372.5, 563.5, 1371.0, 476.0, 1407.5], [24.0, 1562.5, 0, 1562.0, 0.0, 1399.5, 436.5, 1381.0, 440.0, 1422.5, 110.0, 1556.5, 24.0, 1562.5]], 'width': 2592, 'height': 1936}, {'id': 11, 'image_id': 1, 'category_id': 4, 'iscrowd': 0, 'area': 4182, 'bbox': [1694.0, 917.0, 41.0, 173.0], 'segmentation': [[1712.0, 1089.5, 1711.5, 1027.0, 1707.0, 1019.5, 1698.0, 1022.5, 1693.5, 1010.0, 1693.5, 960.0, 1705.0, 944.5, 1712.0, 953.5, 1728.5, 953.0, 1724.5, 921.0, 1728.0, 916.5, 1734.5, 1088.0, 1712.0, 1089.5]], 'width': 2592, 'height': 1936}, {'id': 12, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8979, 'bbox': [866.0, 253.0, 209.0, 772.0], 'segmentation': [[1052.0, 889.5, 1047.5, 886.0, 997.5, 376.0, 983.5, 272.0, 875.0, 269.5, 866.0, 262.5, 990.0, 252.5, 991.5, 255.0, 1063.5, 882.0, 1063.0, 887.5, 1052.0, 889.5], [1068.0, 971.5, 1068.0, 966.5, 1068.0, 971.5], [1057.0, 976.5, 1057.0, 967.5, 1057.0, 976.5], [1069.0, 981.5, 1069.0, 974.5, 1069.0, 981.5], [1074.0, 1024.5, 1061.5, 1024.0, 1058.0, 976.5, 1060.5, 997.0, 1070.0, 997.5, 1071.5, 994.0, 1074.0, 1024.5], [1070.0, 991.5, 1070.0, 984.5, 1070.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 13, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 527, 'bbox': [1067.0, 951.0, 15.0, 75.0], 'segmentation': [[1081.0, 1025.5, 1075.0, 1024.5, 1073.5, 1021.0, 1067.0, 950.5, 1075.0, 950.5, 1076.5, 956.0, 1073.5, 957.0, 1073.5, 961.0, 1081.0, 1025.5]], 'width': 2592, 'height': 1936}, {'id': 14, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 27156, 'bbox': [2328.0, 1395.0, 100.0, 541.0], 'segmentation': [[2427.0, 1935.5, 2378.0, 1935.5, 2376.5, 1929.0, 2338.5, 1479.0, 2337.0, 1472.5, 2329.5, 1472.0, 2329.5, 1443.0, 2336.5, 1440.0, 2327.5, 1429.0, 2327.5, 1414.0, 2339.0, 1395.5, 2360.0, 1394.5, 2378.0, 1400.5, 2388.5, 1412.0, 2390.5, 1422.0, 2388.5, 1438.0, 2385.5, 1440.0, 2393.5, 1442.0, 2394.5, 1473.0, 2386.0, 1475.5, 2385.5, 1481.0, 2427.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 15, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 6705, 'bbox': [877.0, 954.0, 59.0, 356.0], 'segmentation': [[920.0, 1309.5, 910.5, 1309.0, 913.5, 1288.0, 894.5, 1086.0, 876.5, 960.0, 877.0, 956.5, 899.0, 953.5, 914.5, 1131.0, 935.5, 1301.0, 935.0, 1305.5, 920.0, 1309.5]], 'width': 2592, 'height': 1936}, {'id': 16, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 1177, 'bbox': [617.0, 1014.0, 23.0, 146.0], 'segmentation': [[639.0, 1159.5, 630.0, 1159.5, 628.5, 1153.0, 616.5, 1022.0, 616.5, 1017.0, 622.0, 1013.5, 639.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 17, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 379, 'bbox': [869.0, 882.0, 36.0, 47.0], 'segmentation': [[878.0, 928.5, 872.5, 928.0, 868.5, 882.0, 904.0, 881.5, 875.5, 887.0, 878.0, 928.5]], 'width': 2592, 'height': 1936}, {'id': 18, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 3889, 'bbox': [1728.0, 893.0, 31.0, 267.0], 'segmentation': [[1746.0, 1159.5, 1736.5, 1159.0, 1728.0, 892.5, 1740.5, 896.0, 1740.5, 911.0, 1732.5, 930.0, 1731.5, 957.0, 1738.5, 978.0, 1747.5, 991.0, 1758.5, 1131.0, 1758.5, 1153.0, 1746.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 19, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 9692, 'bbox': [1745.0, 882.0, 61.0, 412.0], 'segmentation': [[1792.0, 1293.5, 1771.0, 1290.5, 1768.5, 1282.0, 1769.0, 1279.5, 1774.5, 1279.0, 1774.5, 1274.0, 1744.5, 890.0, 1745.0, 883.5, 1761.0, 881.5, 1767.5, 884.0, 1796.5, 1270.0, 1797.5, 1277.0, 1805.5, 1282.0, 1805.0, 1290.5, 1792.0, 1293.5]], 'width': 2592, 'height': 1936}, {'id': 20, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8559, 'bbox': [1616.0, 949.0, 64.0, 376.0], 'segmentation': [[1660.0, 1324.5, 1637.5, 1324.0, 1637.5, 1313.0, 1643.5, 1308.0, 1643.5, 1298.0, 1638.5, 1196.0, 1627.5, 1085.0, 1636.0, 1089.5, 1636.5, 1087.0, 1629.5, 1056.0, 1624.5, 1055.0, 1615.5, 959.0, 1616.0, 952.5, 1638.0, 948.5, 1668.5, 1297.0, 1669.5, 1303.0, 1677.5, 1308.0, 1679.5, 1319.0, 1679.0, 1322.5, 1660.0, 1324.5]], 'width': 2592, 'height': 1936}, {'id': 21, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 26362, 'bbox': [2017.0, 1572.0, 103.0, 364.0], 'segmentation': [[2119.0, 1935.5, 2048.5, 1935.0, 2034.5, 1706.0, 2027.5, 1687.0, 2023.5, 1619.0, 2016.5, 1600.0, 2027.0, 1580.5, 2052.0, 1571.5, 2082.0, 1580.5, 2104.5, 1611.0, 2096.5, 1636.0, 2104.5, 1643.0, 2109.5, 1681.0, 2105.5, 1706.0, 2119.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 22, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 18515, 'bbox': [402.0, 982.0, 92.0, 520.0], 'segmentation': [[434.0, 1501.5, 433.5, 1489.0, 445.5, 1480.0, 445.5, 1475.0, 401.5, 993.0, 402.0, 985.5, 431.0, 981.5, 436.5, 982.0, 473.5, 1417.0, 479.5, 1431.0, 483.5, 1475.0, 493.5, 1480.0, 493.0, 1496.5, 434.0, 1501.5]], 'width': 2592, 'height': 1936}, {'id': 23, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 421, 'bbox': [1007.0, 926.0, 12.0, 65.0], 'segmentation': [[1018.0, 990.5, 1011.5, 990.0, 1007.0, 925.5, 1012.5, 926.0, 1018.0, 990.5]], 'width': 2592, 'height': 1936}, {'id': 24, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 1107, 'bbox': [798.0, 259.0, 77.0, 20.0], 'segmentation': [[798.0, 278.5, 797.5, 263.0, 804.0, 258.5, 859.0, 258.5, 874.5, 269.0, 798.0, 278.5]], 'width': 2592, 'height': 1936}, {'id': 25, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 73, 'bbox': [904.0, 877.0, 17.0, 8.0], 'segmentation': [[904.0, 884.5, 907.0, 878.5, 917.0, 876.5, 920.5, 882.0, 904.0, 884.5]], 'width': 2592, 'height': 1936}, {'id': 26, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2731, 'bbox': [868.0, 928.0, 53.0, 90.0], 'segmentation': [[876.0, 1017.5, 867.5, 952.0, 867.5, 929.0, 918.0, 927.5, 914.5, 978.0, 920.5, 1012.0, 919.0, 1014.5, 904.5, 1015.0, 899.0, 953.5, 876.5, 957.0, 884.5, 1016.0, 876.0, 1017.5]], 'width': 2592, 'height': 1936}, {'id': 27, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 12971, 'bbox': [331.0, 822.0, 116.0, 166.0], 'segmentation': [[390.0, 987.5, 381.5, 985.0, 378.5, 964.0, 355.5, 950.0, 334.5, 911.0, 373.0, 906.5, 373.5, 901.0, 372.5, 895.0, 358.5, 876.0, 332.5, 856.0, 330.5, 837.0, 363.0, 836.5, 372.0, 829.5, 394.0, 830.5, 396.0, 824.5, 419.0, 821.5, 421.5, 831.0, 431.0, 831.5, 432.5, 837.0, 446.5, 980.0, 390.0, 987.5]], 'width': 2592, 'height': 1936}, {'id': 28, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 6689, 'bbox': [1599.0, 835.0, 73.0, 120.0], 'segmentation': [[1609.0, 954.5, 1598.5, 846.0, 1608.0, 837.5, 1618.0, 834.5, 1625.0, 834.5, 1628.0, 841.5, 1658.0, 836.5, 1666.0, 838.5, 1667.5, 846.0, 1655.5, 890.0, 1667.0, 890.5, 1671.5, 896.0, 1659.5, 940.0, 1634.0, 950.5, 1609.0, 954.5]], 'width': 2592, 'height': 1936}, {'id': 29, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2285, 'bbox': [1682.0, 880.0, 47.0, 74.0], 'segmentation': [[1728.0, 953.5, 1712.0, 953.5, 1704.0, 942.5, 1695.5, 942.0, 1684.5, 917.0, 1691.5, 912.0, 1682.0, 888.5, 1717.0, 883.5, 1720.5, 880.0, 1728.0, 953.5]], 'width': 2592, 'height': 1936}, {'id': 30, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 16074, 'bbox': [1709.0, 676.0, 104.0, 211.0], 'segmentation': [[1725.0, 886.5, 1708.5, 689.0, 1776.0, 675.5, 1782.5, 680.0, 1786.5, 717.0, 1802.5, 723.0, 1803.5, 739.0, 1790.0, 743.5, 1789.5, 753.0, 1790.5, 762.0, 1806.5, 773.0, 1807.5, 787.0, 1792.5, 788.0, 1792.5, 808.0, 1812.5, 819.0, 1805.5, 840.0, 1796.5, 842.0, 1795.0, 878.5, 1725.0, 886.5]], 'width': 2592, 'height': 1936}, {'id': 31, 'image_id': 1, 'category_id': 25, 'iscrowd': 0, 'area': 4717, 'bbox': [1732.0, 882.0, 83.0, 115.0], 'segmentation': [[1780.0, 996.5, 1775.5, 996.0, 1774.5, 986.0, 1767.5, 896.0, 1767.5, 883.0, 1770.0, 881.5, 1783.0, 888.5, 1800.5, 904.0, 1809.5, 926.0, 1814.5, 949.0, 1808.5, 974.0, 1790.0, 994.5, 1780.0, 996.5], [1751.0, 991.5, 1739.5, 980.0, 1731.5, 957.0, 1732.5, 930.0, 1745.0, 902.5, 1751.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 32, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 4552, 'bbox': [1023.0, 885.0, 69.0, 85.0], 'segmentation': [[1031.0, 969.5, 1022.5, 893.0, 1085.0, 884.5, 1091.5, 947.0, 1067.0, 950.5, 1067.0, 965.5, 1031.0, 969.5], [1076.0, 966.5, 1074.0, 956.5, 1076.5, 957.0, 1076.0, 966.5]], 'width': 2592, 'height': 1936}, {'id': 33, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 3513, 'bbox': [1076.0, 948.0, 63.0, 60.0], 'segmentation': [[1080.0, 1007.5, 1075.5, 948.0, 1135.0, 947.5, 1138.5, 1006.0, 1080.0, 1007.5]], 'width': 2592, 'height': 1936}, {'id': 34, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 329, 'bbox': [1058.0, 966.0, 13.0, 32.0], 'segmentation': [[1070.0, 997.5, 1060.5, 997.0, 1058.0, 965.5, 1067.5, 966.0, 1070.0, 997.5]], 'width': 2592, 'height': 1936}]}  Traceback (most recent call last):    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1709, in data_generator    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1211, in load_image_gt    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/utils.py"", line 358, in load_image    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py"", line 61, in imread      img = call_plugin('imread', fname, plugin=plugin, **plugin_args)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/manage_plugins.py"", line 211, in call_plugin      return func(*args, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_plugins/pil_plugin.py"", line 36, in imread      im = Image.open(f)    File ""/usr/local/lib/python3.6/dist-packages/PIL/Image.py"", line 2419, in open      prefix = fp.read(16)  OSError: [Errno 5] Input/output error  ERROR:root:Error processing image {'id': 1, 'source': 'coco', 'path': '/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/tio/train2017/_0P04ZWQtMtPMwx3lgLdWA.jpg', 'width': 2592, 'height': 1936, 'annotations': [{'id': 1, 'image_id': 1, 'category_id': 9, 'iscrowd': 0, 'area': 2153, 'bbox': [1198.0, 397.0, 38.0, 78.0], 'segmentation': [[1208.0, 474.5, 1197.5, 403.0, 1235.0, 396.5, 1229.5, 423.0, 1235.5, 467.0, 1208.0, 474.5]], 'width': 2592, 'height': 1936}, {'id': 2, 'image_id': 1, 'category_id': 12, 'iscrowd': 0, 'area': 1767, 'bbox': [1240.0, 862.0, 39.0, 56.0], 'segmentation': [[1244.0, 917.5, 1239.5, 889.0, 1240.5, 868.0, 1272.0, 861.5, 1278.5, 914.0, 1244.0, 917.5]], 'width': 2592, 'height': 1936}, {'id': 3, 'image_id': 1, 'category_id': 30, 'iscrowd': 0, 'area': 23079, 'bbox': [0.0, 1012.0, 217.0, 190.0], 'segmentation': [[0.0, 1201.5, 0.0, 1011.5, 13.0, 1041.5, 137.0, 1038.5, 204.5, 1055.0, 216.0, 1136.5, 134.0, 1136.5, 55.0, 1155.5, 0.0, 1201.5]], 'width': 2592, 'height': 1936}, {'id': 4, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 13454, 'bbox': [285.0, 1117.0, 137.0, 129.0], 'segmentation': [[289.0, 1245.5, 293.5, 1208.0, 290.5, 1177.0, 284.5, 1167.0, 293.5, 1161.0, 316.5, 1129.0, 366.0, 1117.5, 412.0, 1116.5, 421.5, 1216.0, 404.0, 1219.5, 400.5, 1233.0, 391.0, 1236.5, 385.0, 1235.5, 375.0, 1224.5, 371.0, 1224.5, 362.0, 1226.5, 352.0, 1238.5, 345.0, 1239.5, 334.0, 1229.5, 327.0, 1229.5, 306.0, 1232.5, 299.0, 1244.5, 289.0, 1245.5]], 'width': 2592, 'height': 1936}, {'id': 5, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 34185, 'bbox': [0.0, 1136.0, 294.0, 164.0], 'segmentation': [[9.0, 1299.5, 0, 1299.0, 0, 1202.0, 55.0, 1155.5, 134.0, 1136.5, 218.0, 1135.5, 284.0, 1166.5, 290.5, 1177.0, 293.5, 1216.0, 289.5, 1240.0, 283.5, 1251.0, 274.0, 1257.5, 250.0, 1257.5, 243.0, 1246.5, 238.0, 1246.5, 167.0, 1257.5, 168.5, 1266.0, 163.0, 1275.5, 135.0, 1283.5, 125.0, 1278.5, 121.0, 1272.5, 15.0, 1281.5, 15.5, 1293.0, 9.0, 1299.5]], 'width': 2592, 'height': 1936}, {'id': 6, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 14170, 'bbox': [2474.0, 902.0, 118.0, 181.0], 'segmentation': [[2538.0, 1082.5, 2507.0, 1079.5, 2486.0, 1061.5, 2481.5, 1055.0, 2473.5, 1021.0, 2541.5, 934.0, 2591.0, 901.5, 2591.5, 1067.0, 2538.0, 1082.5]], 'width': 2592, 'height': 1936}, {'id': 7, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 49052, 'bbox': [840.0, 1024.0, 296.0, 219.0], 'segmentation': [[956.0, 1242.5, 943.0, 1242.5, 927.5, 1238.0, 906.5, 1039.0, 946.0, 1030.5, 1038.0, 1023.5, 1115.0, 1025.5, 1121.5, 1042.0, 1133.5, 1137.0, 1135.5, 1193.0, 1124.5, 1199.0, 1114.0, 1216.5, 1102.0, 1220.5, 1077.0, 1218.5, 1066.0, 1203.5, 986.0, 1211.5, 980.5, 1234.0, 956.0, 1242.5], [874.0, 1230.5, 863.0, 1230.5, 851.5, 1220.0, 845.5, 1171.0, 854.5, 1117.0, 840.5, 1106.0, 839.5, 1099.0, 845.0, 1088.5, 851.0, 1088.5, 859.5, 1094.0, 857.5, 1081.0, 865.5, 1056.0, 881.0, 1042.5, 888.0, 1041.5, 905.5, 1209.0, 887.0, 1209.5, 882.5, 1227.0, 874.0, 1230.5]], 'width': 2592, 'height': 1936}, {'id': 8, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 38963, 'bbox': [1049.0, 1201.0, 725.0, 90.0], 'segmentation': [[1669.0, 1269.5, 1665.5, 1264.0, 1661.0, 1207.5, 1758.5, 1201.0, 1770.5, 1227.0, 1773.0, 1263.5, 1669.0, 1269.5], [1103.0, 1289.5, 1050.5, 1289.0, 1159.0, 1231.5, 1636.0, 1208.5, 1639.5, 1212.0, 1641.0, 1270.5, 1376.0, 1284.5, 1103.0, 1289.5]], 'width': 2592, 'height': 1936}, {'id': 9, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 383152, 'bbox': [984.0, 1373.0, 1608.0, 453.0], 'segmentation': [[2186.0, 1825.5, 2111.5, 1807.0, 2105.5, 1706.0, 2109.5, 1681.0, 2104.5, 1643.0, 2096.5, 1636.0, 2104.5, 1611.0, 2082.0, 1580.5, 2052.0, 1571.5, 2027.0, 1580.5, 2016.5, 1600.0, 2023.5, 1619.0, 2027.5, 1687.0, 2034.5, 1706.0, 2039.0, 1790.5, 1629.0, 1693.5, 1214.0, 1606.5, 983.5, 1572.0, 1954.0, 1372.5, 2335.0, 1399.5, 2327.5, 1414.0, 2327.5, 1429.0, 2336.5, 1440.0, 2329.5, 1443.0, 2329.5, 1472.0, 2337.0, 1472.5, 2338.5, 1479.0, 2361.5, 1751.0, 2186.0, 1825.5], [2409.0, 1731.5, 2385.5, 1481.0, 2386.0, 1475.5, 2394.5, 1473.0, 2393.5, 1442.0, 2385.5, 1440.0, 2388.5, 1438.0, 2390.5, 1422.0, 2388.5, 1412.0, 2380.5, 1403.0, 2437.0, 1405.5, 2591.0, 1427.5, 2591.5, 1650.0, 2506.0, 1690.5, 2409.0, 1731.5]], 'width': 2592, 'height': 1936}, {'id': 10, 'image_id': 1, 'category_id': 8, 'iscrowd': 0, 'area': 52969, 'bbox': [0.0, 1370.0, 566.0, 193.0], 'segmentation': [[476.0, 1407.5, 472.5, 1405.0, 472.0, 1377.5, 513.0, 1372.5, 563.5, 1371.0, 476.0, 1407.5], [24.0, 1562.5, 0, 1562.0, 0.0, 1399.5, 436.5, 1381.0, 440.0, 1422.5, 110.0, 1556.5, 24.0, 1562.5]], 'width': 2592, 'height': 1936}, {'id': 11, 'image_id': 1, 'category_id': 4, 'iscrowd': 0, 'area': 4182, 'bbox': [1694.0, 917.0, 41.0, 173.0], 'segmentation': [[1712.0, 1089.5, 1711.5, 1027.0, 1707.0, 1019.5, 1698.0, 1022.5, 1693.5, 1010.0, 1693.5, 960.0, 1705.0, 944.5, 1712.0, 953.5, 1728.5, 953.0, 1724.5, 921.0, 1728.0, 916.5, 1734.5, 1088.0, 1712.0, 1089.5]], 'width': 2592, 'height': 1936}, {'id': 12, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8979, 'bbox': [866.0, 253.0, 209.0, 772.0], 'segmentation': [[1052.0, 889.5, 1047.5, 886.0, 997.5, 376.0, 983.5, 272.0, 875.0, 269.5, 866.0, 262.5, 990.0, 252.5, 991.5, 255.0, 1063.5, 882.0, 1063.0, 887.5, 1052.0, 889.5], [1068.0, 971.5, 1068.0, 966.5, 1068.0, 971.5], [1057.0, 976.5, 1057.0, 967.5, 1057.0, 976.5], [1069.0, 981.5, 1069.0, 974.5, 1069.0, 981.5], [1074.0, 1024.5, 1061.5, 1024.0, 1058.0, 976.5, 1060.5, 997.0, 1070.0, 997.5, 1071.5, 994.0, 1074.0, 1024.5], [1070.0, 991.5, 1070.0, 984.5, 1070.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 13, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 527, 'bbox': [1067.0, 951.0, 15.0, 75.0], 'segmentation': [[1081.0, 1025.5, 1075.0, 1024.5, 1073.5, 1021.0, 1067.0, 950.5, 1075.0, 950.5, 1076.5, 956.0, 1073.5, 957.0, 1073.5, 961.0, 1081.0, 1025.5]], 'width': 2592, 'height': 1936}, {'id': 14, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 27156, 'bbox': [2328.0, 1395.0, 100.0, 541.0], 'segmentation': [[2427.0, 1935.5, 2378.0, 1935.5, 2376.5, 1929.0, 2338.5, 1479.0, 2337.0, 1472.5, 2329.5, 1472.0, 2329.5, 1443.0, 2336.5, 1440.0, 2327.5, 1429.0, 2327.5, 1414.0, 2339.0, 1395.5, 2360.0, 1394.5, 2378.0, 1400.5, 2388.5, 1412.0, 2390.5, 1422.0, 2388.5, 1438.0, 2385.5, 1440.0, 2393.5, 1442.0, 2394.5, 1473.0, 2386.0, 1475.5, 2385.5, 1481.0, 2427.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 15, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 6705, 'bbox': [877.0, 954.0, 59.0, 356.0], 'segmentation': [[920.0, 1309.5, 910.5, 1309.0, 913.5, 1288.0, 894.5, 1086.0, 876.5, 960.0, 877.0, 956.5, 899.0, 953.5, 914.5, 1131.0, 935.5, 1301.0, 935.0, 1305.5, 920.0, 1309.5]], 'width': 2592, 'height': 1936}, {'id': 16, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 1177, 'bbox': [617.0, 1014.0, 23.0, 146.0], 'segmentation': [[639.0, 1159.5, 630.0, 1159.5, 628.5, 1153.0, 616.5, 1022.0, 616.5, 1017.0, 622.0, 1013.5, 639.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 17, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 379, 'bbox': [869.0, 882.0, 36.0, 47.0], 'segmentation': [[878.0, 928.5, 872.5, 928.0, 868.5, 882.0, 904.0, 881.5, 875.5, 887.0, 878.0, 928.5]], 'width': 2592, 'height': 1936}, {'id': 18, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 3889, 'bbox': [1728.0, 893.0, 31.0, 267.0], 'segmentation': [[1746.0, 1159.5, 1736.5, 1159.0, 1728.0, 892.5, 1740.5, 896.0, 1740.5, 911.0, 1732.5, 930.0, 1731.5, 957.0, 1738.5, 978.0, 1747.5, 991.0, 1758.5, 1131.0, 1758.5, 1153.0, 1746.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 19, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 9692, 'bbox': [1745.0, 882.0, 61.0, 412.0], 'segmentation': [[1792.0, 1293.5, 1771.0, 1290.5, 1768.5, 1282.0, 1769.0, 1279.5, 1774.5, 1279.0, 1774.5, 1274.0, 1744.5, 890.0, 1745.0, 883.5, 1761.0, 881.5, 1767.5, 884.0, 1796.5, 1270.0, 1797.5, 1277.0, 1805.5, 1282.0, 1805.0, 1290.5, 1792.0, 1293.5]], 'width': 2592, 'height': 1936}, {'id': 20, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8559, 'bbox': [1616.0, 949.0, 64.0, 376.0], 'segmentation': [[1660.0, 1324.5, 1637.5, 1324.0, 1637.5, 1313.0, 1643.5, 1308.0, 1643.5, 1298.0, 1638.5, 1196.0, 1627.5, 1085.0, 1636.0, 1089.5, 1636.5, 1087.0, 1629.5, 1056.0, 1624.5, 1055.0, 1615.5, 959.0, 1616.0, 952.5, 1638.0, 948.5, 1668.5, 1297.0, 1669.5, 1303.0, 1677.5, 1308.0, 1679.5, 1319.0, 1679.0, 1322.5, 1660.0, 1324.5]], 'width': 2592, 'height': 1936}, {'id': 21, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 26362, 'bbox': [2017.0, 1572.0, 103.0, 364.0], 'segmentation': [[2119.0, 1935.5, 2048.5, 1935.0, 2034.5, 1706.0, 2027.5, 1687.0, 2023.5, 1619.0, 2016.5, 1600.0, 2027.0, 1580.5, 2052.0, 1571.5, 2082.0, 1580.5, 2104.5, 1611.0, 2096.5, 1636.0, 2104.5, 1643.0, 2109.5, 1681.0, 2105.5, 1706.0, 2119.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 22, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 18515, 'bbox': [402.0, 982.0, 92.0, 520.0], 'segmentation': [[434.0, 1501.5, 433.5, 1489.0, 445.5, 1480.0, 445.5, 1475.0, 401.5, 993.0, 402.0, 985.5, 431.0, 981.5, 436.5, 982.0, 473.5, 1417.0, 479.5, 1431.0, 483.5, 1475.0, 493.5, 1480.0, 493.0, 1496.5, 434.0, 1501.5]], 'width': 2592, 'height': 1936}, {'id': 23, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 421, 'bbox': [1007.0, 926.0, 12.0, 65.0], 'segmentation': [[1018.0, 990.5, 1011.5, 990.0, 1007.0, 925.5, 1012.5, 926.0, 1018.0, 990.5]], 'width': 2592, 'height': 1936}, {'id': 24, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 1107, 'bbox': [798.0, 259.0, 77.0, 20.0], 'segmentation': [[798.0, 278.5, 797.5, 263.0, 804.0, 258.5, 859.0, 258.5, 874.5, 269.0, 798.0, 278.5]], 'width': 2592, 'height': 1936}, {'id': 25, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 73, 'bbox': [904.0, 877.0, 17.0, 8.0], 'segmentation': [[904.0, 884.5, 907.0, 878.5, 917.0, 876.5, 920.5, 882.0, 904.0, 884.5]], 'width': 2592, 'height': 1936}, {'id': 26, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2731, 'bbox': [868.0, 928.0, 53.0, 90.0], 'segmentation': [[876.0, 1017.5, 867.5, 952.0, 867.5, 929.0, 918.0, 927.5, 914.5, 978.0, 920.5, 1012.0, 919.0, 1014.5, 904.5, 1015.0, 899.0, 953.5, 876.5, 957.0, 884.5, 1016.0, 876.0, 1017.5]], 'width': 2592, 'height': 1936}, {'id': 27, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 12971, 'bbox': [331.0, 822.0, 116.0, 166.0], 'segmentation': [[390.0, 987.5, 381.5, 985.0, 378.5, 964.0, 355.5, 950.0, 334.5, 911.0, 373.0, 906.5, 373.5, 901.0, 372.5, 895.0, 358.5, 876.0, 332.5, 856.0, 330.5, 837.0, 363.0, 836.5, 372.0, 829.5, 394.0, 830.5, 396.0, 824.5, 419.0, 821.5, 421.5, 831.0, 431.0, 831.5, 432.5, 837.0, 446.5, 980.0, 390.0, 987.5]], 'width': 2592, 'height': 1936}, {'id': 28, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 6689, 'bbox': [1599.0, 835.0, 73.0, 120.0], 'segmentation': [[1609.0, 954.5, 1598.5, 846.0, 1608.0, 837.5, 1618.0, 834.5, 1625.0, 834.5, 1628.0, 841.5, 1658.0, 836.5, 1666.0, 838.5, 1667.5, 846.0, 1655.5, 890.0, 1667.0, 890.5, 1671.5, 896.0, 1659.5, 940.0, 1634.0, 950.5, 1609.0, 954.5]], 'width': 2592, 'height': 1936}, {'id': 29, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2285, 'bbox': [1682.0, 880.0, 47.0, 74.0], 'segmentation': [[1728.0, 953.5, 1712.0, 953.5, 1704.0, 942.5, 1695.5, 942.0, 1684.5, 917.0, 1691.5, 912.0, 1682.0, 888.5, 1717.0, 883.5, 1720.5, 880.0, 1728.0, 953.5]], 'width': 2592, 'height': 1936}, {'id': 30, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 16074, 'bbox': [1709.0, 676.0, 104.0, 211.0], 'segmentation': [[1725.0, 886.5, 1708.5, 689.0, 1776.0, 675.5, 1782.5, 680.0, 1786.5, 717.0, 1802.5, 723.0, 1803.5, 739.0, 1790.0, 743.5, 1789.5, 753.0, 1790.5, 762.0, 1806.5, 773.0, 1807.5, 787.0, 1792.5, 788.0, 1792.5, 808.0, 1812.5, 819.0, 1805.5, 840.0, 1796.5, 842.0, 1795.0, 878.5, 1725.0, 886.5]], 'width': 2592, 'height': 1936}, {'id': 31, 'image_id': 1, 'category_id': 25, 'iscrowd': 0, 'area': 4717, 'bbox': [1732.0, 882.0, 83.0, 115.0], 'segmentation': [[1780.0, 996.5, 1775.5, 996.0, 1774.5, 986.0, 1767.5, 896.0, 1767.5, 883.0, 1770.0, 881.5, 1783.0, 888.5, 1800.5, 904.0, 1809.5, 926.0, 1814.5, 949.0, 1808.5, 974.0, 1790.0, 994.5, 1780.0, 996.5], [1751.0, 991.5, 1739.5, 980.0, 1731.5, 957.0, 1732.5, 930.0, 1745.0, 902.5, 1751.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 32, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 4552, 'bbox': [1023.0, 885.0, 69.0, 85.0], 'segmentation': [[1031.0, 969.5, 1022.5, 893.0, 1085.0, 884.5, 1091.5, 947.0, 1067.0, 950.5, 1067.0, 965.5, 1031.0, 969.5], [1076.0, 966.5, 1074.0, 956.5, 1076.5, 957.0, 1076.0, 966.5]], 'width': 2592, 'height': 1936}, {'id': 33, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 3513, 'bbox': [1076.0, 948.0, 63.0, 60.0], 'segmentation': [[1080.0, 1007.5, 1075.5, 948.0, 1135.0, 947.5, 1138.5, 1006.0, 1080.0, 1007.5]], 'width': 2592, 'height': 1936}, {'id': 34, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 329, 'bbox': [1058.0, 966.0, 13.0, 32.0], 'segmentation': [[1070.0, 997.5, 1060.5, 997.0, 1058.0, 965.5, 1067.5, 966.0, 1070.0, 997.5]], 'width': 2592, 'height': 1936}]}  Traceback (most recent call last):    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1709, in data_generator    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1211, in load_image_gt    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/utils.py"", line 358, in load_image    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py"", line 61, in imread      img = call_plugin('imread', fname, plugin=plugin, **plugin_args)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/manage_plugins.py"", line 211, in call_plugin      return func(*args, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_plugins/pil_plugin.py"", line 36, in imread      im = Image.open(f)    File ""/usr/local/lib/python3.6/dist-packages/PIL/Image.py"", line 2419, in open      prefix = fp.read(16)  OSError: [Errno 5] Input/output error  ERROR:root:Error processing image {'id': 1, 'source': 'coco', 'path': '/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/tio/train2017/_0P04ZWQtMtPMwx3lgLdWA.jpg', 'width': 2592, 'height': 1936, 'annotations': [{'id': 1, 'image_id': 1, 'category_id': 9, 'iscrowd': 0, 'area': 2153, 'bbox': [1198.0, 397.0, 38.0, 78.0], 'segmentation': [[1208.0, 474.5, 1197.5, 403.0, 1235.0, 396.5, 1229.5, 423.0, 1235.5, 467.0, 1208.0, 474.5]], 'width': 2592, 'height': 1936}, {'id': 2, 'image_id': 1, 'category_id': 12, 'iscrowd': 0, 'area': 1767, 'bbox': [1240.0, 862.0, 39.0, 56.0], 'segmentation': [[1244.0, 917.5, 1239.5, 889.0, 1240.5, 868.0, 1272.0, 861.5, 1278.5, 914.0, 1244.0, 917.5]], 'width': 2592, 'height': 1936}, {'id': 3, 'image_id': 1, 'category_id': 30, 'iscrowd': 0, 'area': 23079, 'bbox': [0.0, 1012.0, 217.0, 190.0], 'segmentation': [[0.0, 1201.5, 0.0, 1011.5, 13.0, 1041.5, 137.0, 1038.5, 204.5, 1055.0, 216.0, 1136.5, 134.0, 1136.5, 55.0, 1155.5, 0.0, 1201.5]], 'width': 2592, 'height': 1936}, {'id': 4, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 13454, 'bbox': [285.0, 1117.0, 137.0, 129.0], 'segmentation': [[289.0, 1245.5, 293.5, 1208.0, 290.5, 1177.0, 284.5, 1167.0, 293.5, 1161.0, 316.5, 1129.0, 366.0, 1117.5, 412.0, 1116.5, 421.5, 1216.0, 404.0, 1219.5, 400.5, 1233.0, 391.0, 1236.5, 385.0, 1235.5, 375.0, 1224.5, 371.0, 1224.5, 362.0, 1226.5, 352.0, 1238.5, 345.0, 1239.5, 334.0, 1229.5, 327.0, 1229.5, 306.0, 1232.5, 299.0, 1244.5, 289.0, 1245.5]], 'width': 2592, 'height': 1936}, {'id': 5, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 34185, 'bbox': [0.0, 1136.0, 294.0, 164.0], 'segmentation': [[9.0, 1299.5, 0, 1299.0, 0, 1202.0, 55.0, 1155.5, 134.0, 1136.5, 218.0, 1135.5, 284.0, 1166.5, 290.5, 1177.0, 293.5, 1216.0, 289.5, 1240.0, 283.5, 1251.0, 274.0, 1257.5, 250.0, 1257.5, 243.0, 1246.5, 238.0, 1246.5, 167.0, 1257.5, 168.5, 1266.0, 163.0, 1275.5, 135.0, 1283.5, 125.0, 1278.5, 121.0, 1272.5, 15.0, 1281.5, 15.5, 1293.0, 9.0, 1299.5]], 'width': 2592, 'height': 1936}, {'id': 6, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 14170, 'bbox': [2474.0, 902.0, 118.0, 181.0], 'segmentation': [[2538.0, 1082.5, 2507.0, 1079.5, 2486.0, 1061.5, 2481.5, 1055.0, 2473.5, 1021.0, 2541.5, 934.0, 2591.0, 901.5, 2591.5, 1067.0, 2538.0, 1082.5]], 'width': 2592, 'height': 1936}, {'id': 7, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 49052, 'bbox': [840.0, 1024.0, 296.0, 219.0], 'segmentation': [[956.0, 1242.5, 943.0, 1242.5, 927.5, 1238.0, 906.5, 1039.0, 946.0, 1030.5, 1038.0, 1023.5, 1115.0, 1025.5, 1121.5, 1042.0, 1133.5, 1137.0, 1135.5, 1193.0, 1124.5, 1199.0, 1114.0, 1216.5, 1102.0, 1220.5, 1077.0, 1218.5, 1066.0, 1203.5, 986.0, 1211.5, 980.5, 1234.0, 956.0, 1242.5], [874.0, 1230.5, 863.0, 1230.5, 851.5, 1220.0, 845.5, 1171.0, 854.5, 1117.0, 840.5, 1106.0, 839.5, 1099.0, 845.0, 1088.5, 851.0, 1088.5, 859.5, 1094.0, 857.5, 1081.0, 865.5, 1056.0, 881.0, 1042.5, 888.0, 1041.5, 905.5, 1209.0, 887.0, 1209.5, 882.5, 1227.0, 874.0, 1230.5]], 'width': 2592, 'height': 1936}, {'id': 8, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 38963, 'bbox': [1049.0, 1201.0, 725.0, 90.0], 'segmentation': [[1669.0, 1269.5, 1665.5, 1264.0, 1661.0, 1207.5, 1758.5, 1201.0, 1770.5, 1227.0, 1773.0, 1263.5, 1669.0, 1269.5], [1103.0, 1289.5, 1050.5, 1289.0, 1159.0, 1231.5, 1636.0, 1208.5, 1639.5, 1212.0, 1641.0, 1270.5, 1376.0, 1284.5, 1103.0, 1289.5]], 'width': 2592, 'height': 1936}, {'id': 9, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 383152, 'bbox': [984.0, 1373.0, 1608.0, 453.0], 'segmentation': [[2186.0, 1825.5, 2111.5, 1807.0, 2105.5, 1706.0, 2109.5, 1681.0, 2104.5, 1643.0, 2096.5, 1636.0, 2104.5, 1611.0, 2082.0, 1580.5, 2052.0, 1571.5, 2027.0, 1580.5, 2016.5, 1600.0, 2023.5, 1619.0, 2027.5, 1687.0, 2034.5, 1706.0, 2039.0, 1790.5, 1629.0, 1693.5, 1214.0, 1606.5, 983.5, 1572.0, 1954.0, 1372.5, 2335.0, 1399.5, 2327.5, 1414.0, 2327.5, 1429.0, 2336.5, 1440.0, 2329.5, 1443.0, 2329.5, 1472.0, 2337.0, 1472.5, 2338.5, 1479.0, 2361.5, 1751.0, 2186.0, 1825.5], [2409.0, 1731.5, 2385.5, 1481.0, 2386.0, 1475.5, 2394.5, 1473.0, 2393.5, 1442.0, 2385.5, 1440.0, 2388.5, 1438.0, 2390.5, 1422.0, 2388.5, 1412.0, 2380.5, 1403.0, 2437.0, 1405.5, 2591.0, 1427.5, 2591.5, 1650.0, 2506.0, 1690.5, 2409.0, 1731.5]], 'width': 2592, 'height': 1936}, {'id': 10, 'image_id': 1, 'category_id': 8, 'iscrowd': 0, 'area': 52969, 'bbox': [0.0, 1370.0, 566.0, 193.0], 'segmentation': [[476.0, 1407.5, 472.5, 1405.0, 472.0, 1377.5, 513.0, 1372.5, 563.5, 1371.0, 476.0, 1407.5], [24.0, 1562.5, 0, 1562.0, 0.0, 1399.5, 436.5, 1381.0, 440.0, 1422.5, 110.0, 1556.5, 24.0, 1562.5]], 'width': 2592, 'height': 1936}, {'id': 11, 'image_id': 1, 'category_id': 4, 'iscrowd': 0, 'area': 4182, 'bbox': [1694.0, 917.0, 41.0, 173.0], 'segmentation': [[1712.0, 1089.5, 1711.5, 1027.0, 1707.0, 1019.5, 1698.0, 1022.5, 1693.5, 1010.0, 1693.5, 960.0, 1705.0, 944.5, 1712.0, 953.5, 1728.5, 953.0, 1724.5, 921.0, 1728.0, 916.5, 1734.5, 1088.0, 1712.0, 1089.5]], 'width': 2592, 'height': 1936}, {'id': 12, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8979, 'bbox': [866.0, 253.0, 209.0, 772.0], 'segmentation': [[1052.0, 889.5, 1047.5, 886.0, 997.5, 376.0, 983.5, 272.0, 875.0, 269.5, 866.0, 262.5, 990.0, 252.5, 991.5, 255.0, 1063.5, 882.0, 1063.0, 887.5, 1052.0, 889.5], [1068.0, 971.5, 1068.0, 966.5, 1068.0, 971.5], [1057.0, 976.5, 1057.0, 967.5, 1057.0, 976.5], [1069.0, 981.5, 1069.0, 974.5, 1069.0, 981.5], [1074.0, 1024.5, 1061.5, 1024.0, 1058.0, 976.5, 1060.5, 997.0, 1070.0, 997.5, 1071.5, 994.0, 1074.0, 1024.5], [1070.0, 991.5, 1070.0, 984.5, 1070.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 13, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 527, 'bbox': [1067.0, 951.0, 15.0, 75.0], 'segmentation': [[1081.0, 1025.5, 1075.0, 1024.5, 1073.5, 1021.0, 1067.0, 950.5, 1075.0, 950.5, 1076.5, 956.0, 1073.5, 957.0, 1073.5, 961.0, 1081.0, 1025.5]], 'width': 2592, 'height': 1936}, {'id': 14, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 27156, 'bbox': [2328.0, 1395.0, 100.0, 541.0], 'segmentation': [[2427.0, 1935.5, 2378.0, 1935.5, 2376.5, 1929.0, 2338.5, 1479.0, 2337.0, 1472.5, 2329.5, 1472.0, 2329.5, 1443.0, 2336.5, 1440.0, 2327.5, 1429.0, 2327.5, 1414.0, 2339.0, 1395.5, 2360.0, 1394.5, 2378.0, 1400.5, 2388.5, 1412.0, 2390.5, 1422.0, 2388.5, 1438.0, 2385.5, 1440.0, 2393.5, 1442.0, 2394.5, 1473.0, 2386.0, 1475.5, 2385.5, 1481.0, 2427.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 15, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 6705, 'bbox': [877.0, 954.0, 59.0, 356.0], 'segmentation': [[920.0, 1309.5, 910.5, 1309.0, 913.5, 1288.0, 894.5, 1086.0, 876.5, 960.0, 877.0, 956.5, 899.0, 953.5, 914.5, 1131.0, 935.5, 1301.0, 935.0, 1305.5, 920.0, 1309.5]], 'width': 2592, 'height': 1936}, {'id': 16, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 1177, 'bbox': [617.0, 1014.0, 23.0, 146.0], 'segmentation': [[639.0, 1159.5, 630.0, 1159.5, 628.5, 1153.0, 616.5, 1022.0, 616.5, 1017.0, 622.0, 1013.5, 639.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 17, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 379, 'bbox': [869.0, 882.0, 36.0, 47.0], 'segmentation': [[878.0, 928.5, 872.5, 928.0, 868.5, 882.0, 904.0, 881.5, 875.5, 887.0, 878.0, 928.5]], 'width': 2592, 'height': 1936}, {'id': 18, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 3889, 'bbox': [1728.0, 893.0, 31.0, 267.0], 'segmentation': [[1746.0, 1159.5, 1736.5, 1159.0, 1728.0, 892.5, 1740.5, 896.0, 1740.5, 911.0, 1732.5, 930.0, 1731.5, 957.0, 1738.5, 978.0, 1747.5, 991.0, 1758.5, 1131.0, 1758.5, 1153.0, 1746.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 19, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 9692, 'bbox': [1745.0, 882.0, 61.0, 412.0], 'segmentation': [[1792.0, 1293.5, 1771.0, 1290.5, 1768.5, 1282.0, 1769.0, 1279.5, 1774.5, 1279.0, 1774.5, 1274.0, 1744.5, 890.0, 1745.0, 883.5, 1761.0, 881.5, 1767.5, 884.0, 1796.5, 1270.0, 1797.5, 1277.0, 1805.5, 1282.0, 1805.0, 1290.5, 1792.0, 1293.5]], 'width': 2592, 'height': 1936}, {'id': 20, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8559, 'bbox': [1616.0, 949.0, 64.0, 376.0], 'segmentation': [[1660.0, 1324.5, 1637.5, 1324.0, 1637.5, 1313.0, 1643.5, 1308.0, 1643.5, 1298.0, 1638.5, 1196.0, 1627.5, 1085.0, 1636.0, 1089.5, 1636.5, 1087.0, 1629.5, 1056.0, 1624.5, 1055.0, 1615.5, 959.0, 1616.0, 952.5, 1638.0, 948.5, 1668.5, 1297.0, 1669.5, 1303.0, 1677.5, 1308.0, 1679.5, 1319.0, 1679.0, 1322.5, 1660.0, 1324.5]], 'width': 2592, 'height': 1936}, {'id': 21, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 26362, 'bbox': [2017.0, 1572.0, 103.0, 364.0], 'segmentation': [[2119.0, 1935.5, 2048.5, 1935.0, 2034.5, 1706.0, 2027.5, 1687.0, 2023.5, 1619.0, 2016.5, 1600.0, 2027.0, 1580.5, 2052.0, 1571.5, 2082.0, 1580.5, 2104.5, 1611.0, 2096.5, 1636.0, 2104.5, 1643.0, 2109.5, 1681.0, 2105.5, 1706.0, 2119.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 22, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 18515, 'bbox': [402.0, 982.0, 92.0, 520.0], 'segmentation': [[434.0, 1501.5, 433.5, 1489.0, 445.5, 1480.0, 445.5, 1475.0, 401.5, 993.0, 402.0, 985.5, 431.0, 981.5, 436.5, 982.0, 473.5, 1417.0, 479.5, 1431.0, 483.5, 1475.0, 493.5, 1480.0, 493.0, 1496.5, 434.0, 1501.5]], 'width': 2592, 'height': 1936}, {'id': 23, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 421, 'bbox': [1007.0, 926.0, 12.0, 65.0], 'segmentation': [[1018.0, 990.5, 1011.5, 990.0, 1007.0, 925.5, 1012.5, 926.0, 1018.0, 990.5]], 'width': 2592, 'height': 1936}, {'id': 24, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 1107, 'bbox': [798.0, 259.0, 77.0, 20.0], 'segmentation': [[798.0, 278.5, 797.5, 263.0, 804.0, 258.5, 859.0, 258.5, 874.5, 269.0, 798.0, 278.5]], 'width': 2592, 'height': 1936}, {'id': 25, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 73, 'bbox': [904.0, 877.0, 17.0, 8.0], 'segmentation': [[904.0, 884.5, 907.0, 878.5, 917.0, 876.5, 920.5, 882.0, 904.0, 884.5]], 'width': 2592, 'height': 1936}, {'id': 26, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2731, 'bbox': [868.0, 928.0, 53.0, 90.0], 'segmentation': [[876.0, 1017.5, 867.5, 952.0, 867.5, 929.0, 918.0, 927.5, 914.5, 978.0, 920.5, 1012.0, 919.0, 1014.5, 904.5, 1015.0, 899.0, 953.5, 876.5, 957.0, 884.5, 1016.0, 876.0, 1017.5]], 'width': 2592, 'height': 1936}, {'id': 27, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 12971, 'bbox': [331.0, 822.0, 116.0, 166.0], 'segmentation': [[390.0, 987.5, 381.5, 985.0, 378.5, 964.0, 355.5, 950.0, 334.5, 911.0, 373.0, 906.5, 373.5, 901.0, 372.5, 895.0, 358.5, 876.0, 332.5, 856.0, 330.5, 837.0, 363.0, 836.5, 372.0, 829.5, 394.0, 830.5, 396.0, 824.5, 419.0, 821.5, 421.5, 831.0, 431.0, 831.5, 432.5, 837.0, 446.5, 980.0, 390.0, 987.5]], 'width': 2592, 'height': 1936}, {'id': 28, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 6689, 'bbox': [1599.0, 835.0, 73.0, 120.0], 'segmentation': [[1609.0, 954.5, 1598.5, 846.0, 1608.0, 837.5, 1618.0, 834.5, 1625.0, 834.5, 1628.0, 841.5, 1658.0, 836.5, 1666.0, 838.5, 1667.5, 846.0, 1655.5, 890.0, 1667.0, 890.5, 1671.5, 896.0, 1659.5, 940.0, 1634.0, 950.5, 1609.0, 954.5]], 'width': 2592, 'height': 1936}, {'id': 29, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2285, 'bbox': [1682.0, 880.0, 47.0, 74.0], 'segmentation': [[1728.0, 953.5, 1712.0, 953.5, 1704.0, 942.5, 1695.5, 942.0, 1684.5, 917.0, 1691.5, 912.0, 1682.0, 888.5, 1717.0, 883.5, 1720.5, 880.0, 1728.0, 953.5]], 'width': 2592, 'height': 1936}, {'id': 30, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 16074, 'bbox': [1709.0, 676.0, 104.0, 211.0], 'segmentation': [[1725.0, 886.5, 1708.5, 689.0, 1776.0, 675.5, 1782.5, 680.0, 1786.5, 717.0, 1802.5, 723.0, 1803.5, 739.0, 1790.0, 743.5, 1789.5, 753.0, 1790.5, 762.0, 1806.5, 773.0, 1807.5, 787.0, 1792.5, 788.0, 1792.5, 808.0, 1812.5, 819.0, 1805.5, 840.0, 1796.5, 842.0, 1795.0, 878.5, 1725.0, 886.5]], 'width': 2592, 'height': 1936}, {'id': 31, 'image_id': 1, 'category_id': 25, 'iscrowd': 0, 'area': 4717, 'bbox': [1732.0, 882.0, 83.0, 115.0], 'segmentation': [[1780.0, 996.5, 1775.5, 996.0, 1774.5, 986.0, 1767.5, 896.0, 1767.5, 883.0, 1770.0, 881.5, 1783.0, 888.5, 1800.5, 904.0, 1809.5, 926.0, 1814.5, 949.0, 1808.5, 974.0, 1790.0, 994.5, 1780.0, 996.5], [1751.0, 991.5, 1739.5, 980.0, 1731.5, 957.0, 1732.5, 930.0, 1745.0, 902.5, 1751.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 32, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 4552, 'bbox': [1023.0, 885.0, 69.0, 85.0], 'segmentation': [[1031.0, 969.5, 1022.5, 893.0, 1085.0, 884.5, 1091.5, 947.0, 1067.0, 950.5, 1067.0, 965.5, 1031.0, 969.5], [1076.0, 966.5, 1074.0, 956.5, 1076.5, 957.0, 1076.0, 966.5]], 'width': 2592, 'height': 1936}, {'id': 33, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 3513, 'bbox': [1076.0, 948.0, 63.0, 60.0], 'segmentation': [[1080.0, 1007.5, 1075.5, 948.0, 1135.0, 947.5, 1138.5, 1006.0, 1080.0, 1007.5]], 'width': 2592, 'height': 1936}, {'id': 34, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 329, 'bbox': [1058.0, 966.0, 13.0, 32.0], 'segmentation': [[1070.0, 997.5, 1060.5, 997.0, 1058.0, 965.5, 1067.5, 966.0, 1070.0, 997.5]], 'width': 2592, 'height': 1936}]}  Traceback (most recent call last):    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1709, in data_generator    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1211, in load_image_gt    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/utils.py"", line 358, in load_image    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py"", line 61, in imread      img = call_plugin('imread', fname, plugin=plugin, **plugin_args)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/manage_plugins.py"", line 211, in call_plugin      return func(*args, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_plugins/pil_plugin.py"", line 36, in imread      im = Image.open(f)    File ""/usr/local/lib/python3.6/dist-packages/PIL/Image.py"", line 2419, in open      prefix = fp.read(16)  OSError: [Errno 5] Input/output error  ERROR:root:Error processing image {'id': 1, 'source': 'coco', 'path': '/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/tio/train2017/_0P04ZWQtMtPMwx3lgLdWA.jpg', 'width': 2592, 'height': 1936, 'annotations': [{'id': 1, 'image_id': 1, 'category_id': 9, 'iscrowd': 0, 'area': 2153, 'bbox': [1198.0, 397.0, 38.0, 78.0], 'segmentation': [[1208.0, 474.5, 1197.5, 403.0, 1235.0, 396.5, 1229.5, 423.0, 1235.5, 467.0, 1208.0, 474.5]], 'width': 2592, 'height': 1936}, {'id': 2, 'image_id': 1, 'category_id': 12, 'iscrowd': 0, 'area': 1767, 'bbox': [1240.0, 862.0, 39.0, 56.0], 'segmentation': [[1244.0, 917.5, 1239.5, 889.0, 1240.5, 868.0, 1272.0, 861.5, 1278.5, 914.0, 1244.0, 917.5]], 'width': 2592, 'height': 1936}, {'id': 3, 'image_id': 1, 'category_id': 30, 'iscrowd': 0, 'area': 23079, 'bbox': [0.0, 1012.0, 217.0, 190.0], 'segmentation': [[0.0, 1201.5, 0.0, 1011.5, 13.0, 1041.5, 137.0, 1038.5, 204.5, 1055.0, 216.0, 1136.5, 134.0, 1136.5, 55.0, 1155.5, 0.0, 1201.5]], 'width': 2592, 'height': 1936}, {'id': 4, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 13454, 'bbox': [285.0, 1117.0, 137.0, 129.0], 'segmentation': [[289.0, 1245.5, 293.5, 1208.0, 290.5, 1177.0, 284.5, 1167.0, 293.5, 1161.0, 316.5, 1129.0, 366.0, 1117.5, 412.0, 1116.5, 421.5, 1216.0, 404.0, 1219.5, 400.5, 1233.0, 391.0, 1236.5, 385.0, 1235.5, 375.0, 1224.5, 371.0, 1224.5, 362.0, 1226.5, 352.0, 1238.5, 345.0, 1239.5, 334.0, 1229.5, 327.0, 1229.5, 306.0, 1232.5, 299.0, 1244.5, 289.0, 1245.5]], 'width': 2592, 'height': 1936}, {'id': 5, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 34185, 'bbox': [0.0, 1136.0, 294.0, 164.0], 'segmentation': [[9.0, 1299.5, 0, 1299.0, 0, 1202.0, 55.0, 1155.5, 134.0, 1136.5, 218.0, 1135.5, 284.0, 1166.5, 290.5, 1177.0, 293.5, 1216.0, 289.5, 1240.0, 283.5, 1251.0, 274.0, 1257.5, 250.0, 1257.5, 243.0, 1246.5, 238.0, 1246.5, 167.0, 1257.5, 168.5, 1266.0, 163.0, 1275.5, 135.0, 1283.5, 125.0, 1278.5, 121.0, 1272.5, 15.0, 1281.5, 15.5, 1293.0, 9.0, 1299.5]], 'width': 2592, 'height': 1936}, {'id': 6, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 14170, 'bbox': [2474.0, 902.0, 118.0, 181.0], 'segmentation': [[2538.0, 1082.5, 2507.0, 1079.5, 2486.0, 1061.5, 2481.5, 1055.0, 2473.5, 1021.0, 2541.5, 934.0, 2591.0, 901.5, 2591.5, 1067.0, 2538.0, 1082.5]], 'width': 2592, 'height': 1936}, {'id': 7, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 49052, 'bbox': [840.0, 1024.0, 296.0, 219.0], 'segmentation': [[956.0, 1242.5, 943.0, 1242.5, 927.5, 1238.0, 906.5, 1039.0, 946.0, 1030.5, 1038.0, 1023.5, 1115.0, 1025.5, 1121.5, 1042.0, 1133.5, 1137.0, 1135.5, 1193.0, 1124.5, 1199.0, 1114.0, 1216.5, 1102.0, 1220.5, 1077.0, 1218.5, 1066.0, 1203.5, 986.0, 1211.5, 980.5, 1234.0, 956.0, 1242.5], [874.0, 1230.5, 863.0, 1230.5, 851.5, 1220.0, 845.5, 1171.0, 854.5, 1117.0, 840.5, 1106.0, 839.5, 1099.0, 845.0, 1088.5, 851.0, 1088.5, 859.5, 1094.0, 857.5, 1081.0, 865.5, 1056.0, 881.0, 1042.5, 888.0, 1041.5, 905.5, 1209.0, 887.0, 1209.5, 882.5, 1227.0, 874.0, 1230.5]], 'width': 2592, 'height': 1936}, {'id': 8, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 38963, 'bbox': [1049.0, 1201.0, 725.0, 90.0], 'segmentation': [[1669.0, 1269.5, 1665.5, 1264.0, 1661.0, 1207.5, 1758.5, 1201.0, 1770.5, 1227.0, 1773.0, 1263.5, 1669.0, 1269.5], [1103.0, 1289.5, 1050.5, 1289.0, 1159.0, 1231.5, 1636.0, 1208.5, 1639.5, 1212.0, 1641.0, 1270.5, 1376.0, 1284.5, 1103.0, 1289.5]], 'width': 2592, 'height': 1936}, {'id': 9, 'image_id': 1, 'category_id': 31, 'iscrowd': 0, 'area': 383152, 'bbox': [984.0, 1373.0, 1608.0, 453.0], 'segmentation': [[2186.0, 1825.5, 2111.5, 1807.0, 2105.5, 1706.0, 2109.5, 1681.0, 2104.5, 1643.0, 2096.5, 1636.0, 2104.5, 1611.0, 2082.0, 1580.5, 2052.0, 1571.5, 2027.0, 1580.5, 2016.5, 1600.0, 2023.5, 1619.0, 2027.5, 1687.0, 2034.5, 1706.0, 2039.0, 1790.5, 1629.0, 1693.5, 1214.0, 1606.5, 983.5, 1572.0, 1954.0, 1372.5, 2335.0, 1399.5, 2327.5, 1414.0, 2327.5, 1429.0, 2336.5, 1440.0, 2329.5, 1443.0, 2329.5, 1472.0, 2337.0, 1472.5, 2338.5, 1479.0, 2361.5, 1751.0, 2186.0, 1825.5], [2409.0, 1731.5, 2385.5, 1481.0, 2386.0, 1475.5, 2394.5, 1473.0, 2393.5, 1442.0, 2385.5, 1440.0, 2388.5, 1438.0, 2390.5, 1422.0, 2388.5, 1412.0, 2380.5, 1403.0, 2437.0, 1405.5, 2591.0, 1427.5, 2591.5, 1650.0, 2506.0, 1690.5, 2409.0, 1731.5]], 'width': 2592, 'height': 1936}, {'id': 10, 'image_id': 1, 'category_id': 8, 'iscrowd': 0, 'area': 52969, 'bbox': [0.0, 1370.0, 566.0, 193.0], 'segmentation': [[476.0, 1407.5, 472.5, 1405.0, 472.0, 1377.5, 513.0, 1372.5, 563.5, 1371.0, 476.0, 1407.5], [24.0, 1562.5, 0, 1562.0, 0.0, 1399.5, 436.5, 1381.0, 440.0, 1422.5, 110.0, 1556.5, 24.0, 1562.5]], 'width': 2592, 'height': 1936}, {'id': 11, 'image_id': 1, 'category_id': 4, 'iscrowd': 0, 'area': 4182, 'bbox': [1694.0, 917.0, 41.0, 173.0], 'segmentation': [[1712.0, 1089.5, 1711.5, 1027.0, 1707.0, 1019.5, 1698.0, 1022.5, 1693.5, 1010.0, 1693.5, 960.0, 1705.0, 944.5, 1712.0, 953.5, 1728.5, 953.0, 1724.5, 921.0, 1728.0, 916.5, 1734.5, 1088.0, 1712.0, 1089.5]], 'width': 2592, 'height': 1936}, {'id': 12, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8979, 'bbox': [866.0, 253.0, 209.0, 772.0], 'segmentation': [[1052.0, 889.5, 1047.5, 886.0, 997.5, 376.0, 983.5, 272.0, 875.0, 269.5, 866.0, 262.5, 990.0, 252.5, 991.5, 255.0, 1063.5, 882.0, 1063.0, 887.5, 1052.0, 889.5], [1068.0, 971.5, 1068.0, 966.5, 1068.0, 971.5], [1057.0, 976.5, 1057.0, 967.5, 1057.0, 976.5], [1069.0, 981.5, 1069.0, 974.5, 1069.0, 981.5], [1074.0, 1024.5, 1061.5, 1024.0, 1058.0, 976.5, 1060.5, 997.0, 1070.0, 997.5, 1071.5, 994.0, 1074.0, 1024.5], [1070.0, 991.5, 1070.0, 984.5, 1070.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 13, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 527, 'bbox': [1067.0, 951.0, 15.0, 75.0], 'segmentation': [[1081.0, 1025.5, 1075.0, 1024.5, 1073.5, 1021.0, 1067.0, 950.5, 1075.0, 950.5, 1076.5, 956.0, 1073.5, 957.0, 1073.5, 961.0, 1081.0, 1025.5]], 'width': 2592, 'height': 1936}, {'id': 14, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 27156, 'bbox': [2328.0, 1395.0, 100.0, 541.0], 'segmentation': [[2427.0, 1935.5, 2378.0, 1935.5, 2376.5, 1929.0, 2338.5, 1479.0, 2337.0, 1472.5, 2329.5, 1472.0, 2329.5, 1443.0, 2336.5, 1440.0, 2327.5, 1429.0, 2327.5, 1414.0, 2339.0, 1395.5, 2360.0, 1394.5, 2378.0, 1400.5, 2388.5, 1412.0, 2390.5, 1422.0, 2388.5, 1438.0, 2385.5, 1440.0, 2393.5, 1442.0, 2394.5, 1473.0, 2386.0, 1475.5, 2385.5, 1481.0, 2427.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 15, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 6705, 'bbox': [877.0, 954.0, 59.0, 356.0], 'segmentation': [[920.0, 1309.5, 910.5, 1309.0, 913.5, 1288.0, 894.5, 1086.0, 876.5, 960.0, 877.0, 956.5, 899.0, 953.5, 914.5, 1131.0, 935.5, 1301.0, 935.0, 1305.5, 920.0, 1309.5]], 'width': 2592, 'height': 1936}, {'id': 16, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 1177, 'bbox': [617.0, 1014.0, 23.0, 146.0], 'segmentation': [[639.0, 1159.5, 630.0, 1159.5, 628.5, 1153.0, 616.5, 1022.0, 616.5, 1017.0, 622.0, 1013.5, 639.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 17, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 379, 'bbox': [869.0, 882.0, 36.0, 47.0], 'segmentation': [[878.0, 928.5, 872.5, 928.0, 868.5, 882.0, 904.0, 881.5, 875.5, 887.0, 878.0, 928.5]], 'width': 2592, 'height': 1936}, {'id': 18, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 3889, 'bbox': [1728.0, 893.0, 31.0, 267.0], 'segmentation': [[1746.0, 1159.5, 1736.5, 1159.0, 1728.0, 892.5, 1740.5, 896.0, 1740.5, 911.0, 1732.5, 930.0, 1731.5, 957.0, 1738.5, 978.0, 1747.5, 991.0, 1758.5, 1131.0, 1758.5, 1153.0, 1746.0, 1159.5]], 'width': 2592, 'height': 1936}, {'id': 19, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 9692, 'bbox': [1745.0, 882.0, 61.0, 412.0], 'segmentation': [[1792.0, 1293.5, 1771.0, 1290.5, 1768.5, 1282.0, 1769.0, 1279.5, 1774.5, 1279.0, 1774.5, 1274.0, 1744.5, 890.0, 1745.0, 883.5, 1761.0, 881.5, 1767.5, 884.0, 1796.5, 1270.0, 1797.5, 1277.0, 1805.5, 1282.0, 1805.0, 1290.5, 1792.0, 1293.5]], 'width': 2592, 'height': 1936}, {'id': 20, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 8559, 'bbox': [1616.0, 949.0, 64.0, 376.0], 'segmentation': [[1660.0, 1324.5, 1637.5, 1324.0, 1637.5, 1313.0, 1643.5, 1308.0, 1643.5, 1298.0, 1638.5, 1196.0, 1627.5, 1085.0, 1636.0, 1089.5, 1636.5, 1087.0, 1629.5, 1056.0, 1624.5, 1055.0, 1615.5, 959.0, 1616.0, 952.5, 1638.0, 948.5, 1668.5, 1297.0, 1669.5, 1303.0, 1677.5, 1308.0, 1679.5, 1319.0, 1679.0, 1322.5, 1660.0, 1324.5]], 'width': 2592, 'height': 1936}, {'id': 21, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 26362, 'bbox': [2017.0, 1572.0, 103.0, 364.0], 'segmentation': [[2119.0, 1935.5, 2048.5, 1935.0, 2034.5, 1706.0, 2027.5, 1687.0, 2023.5, 1619.0, 2016.5, 1600.0, 2027.0, 1580.5, 2052.0, 1571.5, 2082.0, 1580.5, 2104.5, 1611.0, 2096.5, 1636.0, 2104.5, 1643.0, 2109.5, 1681.0, 2105.5, 1706.0, 2119.0, 1935.5]], 'width': 2592, 'height': 1936}, {'id': 22, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 18515, 'bbox': [402.0, 982.0, 92.0, 520.0], 'segmentation': [[434.0, 1501.5, 433.5, 1489.0, 445.5, 1480.0, 445.5, 1475.0, 401.5, 993.0, 402.0, 985.5, 431.0, 981.5, 436.5, 982.0, 473.5, 1417.0, 479.5, 1431.0, 483.5, 1475.0, 493.5, 1480.0, 493.0, 1496.5, 434.0, 1501.5]], 'width': 2592, 'height': 1936}, {'id': 23, 'image_id': 1, 'category_id': 21, 'iscrowd': 0, 'area': 421, 'bbox': [1007.0, 926.0, 12.0, 65.0], 'segmentation': [[1018.0, 990.5, 1011.5, 990.0, 1007.0, 925.5, 1012.5, 926.0, 1018.0, 990.5]], 'width': 2592, 'height': 1936}, {'id': 24, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 1107, 'bbox': [798.0, 259.0, 77.0, 20.0], 'segmentation': [[798.0, 278.5, 797.5, 263.0, 804.0, 258.5, 859.0, 258.5, 874.5, 269.0, 798.0, 278.5]], 'width': 2592, 'height': 1936}, {'id': 25, 'image_id': 1, 'category_id': 20, 'iscrowd': 0, 'area': 73, 'bbox': [904.0, 877.0, 17.0, 8.0], 'segmentation': [[904.0, 884.5, 907.0, 878.5, 917.0, 876.5, 920.5, 882.0, 904.0, 884.5]], 'width': 2592, 'height': 1936}, {'id': 26, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2731, 'bbox': [868.0, 928.0, 53.0, 90.0], 'segmentation': [[876.0, 1017.5, 867.5, 952.0, 867.5, 929.0, 918.0, 927.5, 914.5, 978.0, 920.5, 1012.0, 919.0, 1014.5, 904.5, 1015.0, 899.0, 953.5, 876.5, 957.0, 884.5, 1016.0, 876.0, 1017.5]], 'width': 2592, 'height': 1936}, {'id': 27, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 12971, 'bbox': [331.0, 822.0, 116.0, 166.0], 'segmentation': [[390.0, 987.5, 381.5, 985.0, 378.5, 964.0, 355.5, 950.0, 334.5, 911.0, 373.0, 906.5, 373.5, 901.0, 372.5, 895.0, 358.5, 876.0, 332.5, 856.0, 330.5, 837.0, 363.0, 836.5, 372.0, 829.5, 394.0, 830.5, 396.0, 824.5, 419.0, 821.5, 421.5, 831.0, 431.0, 831.5, 432.5, 837.0, 446.5, 980.0, 390.0, 987.5]], 'width': 2592, 'height': 1936}, {'id': 28, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 6689, 'bbox': [1599.0, 835.0, 73.0, 120.0], 'segmentation': [[1609.0, 954.5, 1598.5, 846.0, 1608.0, 837.5, 1618.0, 834.5, 1625.0, 834.5, 1628.0, 841.5, 1658.0, 836.5, 1666.0, 838.5, 1667.5, 846.0, 1655.5, 890.0, 1667.0, 890.5, 1671.5, 896.0, 1659.5, 940.0, 1634.0, 950.5, 1609.0, 954.5]], 'width': 2592, 'height': 1936}, {'id': 29, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 2285, 'bbox': [1682.0, 880.0, 47.0, 74.0], 'segmentation': [[1728.0, 953.5, 1712.0, 953.5, 1704.0, 942.5, 1695.5, 942.0, 1684.5, 917.0, 1691.5, 912.0, 1682.0, 888.5, 1717.0, 883.5, 1720.5, 880.0, 1728.0, 953.5]], 'width': 2592, 'height': 1936}, {'id': 30, 'image_id': 1, 'category_id': 24, 'iscrowd': 0, 'area': 16074, 'bbox': [1709.0, 676.0, 104.0, 211.0], 'segmentation': [[1725.0, 886.5, 1708.5, 689.0, 1776.0, 675.5, 1782.5, 680.0, 1786.5, 717.0, 1802.5, 723.0, 1803.5, 739.0, 1790.0, 743.5, 1789.5, 753.0, 1790.5, 762.0, 1806.5, 773.0, 1807.5, 787.0, 1792.5, 788.0, 1792.5, 808.0, 1812.5, 819.0, 1805.5, 840.0, 1796.5, 842.0, 1795.0, 878.5, 1725.0, 886.5]], 'width': 2592, 'height': 1936}, {'id': 31, 'image_id': 1, 'category_id': 25, 'iscrowd': 0, 'area': 4717, 'bbox': [1732.0, 882.0, 83.0, 115.0], 'segmentation': [[1780.0, 996.5, 1775.5, 996.0, 1774.5, 986.0, 1767.5, 896.0, 1767.5, 883.0, 1770.0, 881.5, 1783.0, 888.5, 1800.5, 904.0, 1809.5, 926.0, 1814.5, 949.0, 1808.5, 974.0, 1790.0, 994.5, 1780.0, 996.5], [1751.0, 991.5, 1739.5, 980.0, 1731.5, 957.0, 1732.5, 930.0, 1745.0, 902.5, 1751.0, 991.5]], 'width': 2592, 'height': 1936}, {'id': 32, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 4552, 'bbox': [1023.0, 885.0, 69.0, 85.0], 'segmentation': [[1031.0, 969.5, 1022.5, 893.0, 1085.0, 884.5, 1091.5, 947.0, 1067.0, 950.5, 1067.0, 965.5, 1031.0, 969.5], [1076.0, 966.5, 1074.0, 956.5, 1076.5, 957.0, 1076.0, 966.5]], 'width': 2592, 'height': 1936}, {'id': 33, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 3513, 'bbox': [1076.0, 948.0, 63.0, 60.0], 'segmentation': [[1080.0, 1007.5, 1075.5, 948.0, 1135.0, 947.5, 1138.5, 1006.0, 1080.0, 1007.5]], 'width': 2592, 'height': 1936}, {'id': 34, 'image_id': 1, 'category_id': 26, 'iscrowd': 0, 'area': 329, 'bbox': [1058.0, 966.0, 13.0, 32.0], 'segmentation': [[1070.0, 997.5, 1060.5, 997.0, 1058.0, 965.5, 1067.5, 966.0, 1070.0, 997.5]], 'width': 2592, 'height': 1936}]}  Traceback (most recent call last):    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1709, in data_generator    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/model.py"", line 1211, in load_image_gt    File ""/content/drive/Colab Notebooks/Mask/Mask_RCNN_2/Mask_RCNN/mrcnn/utils.py"", line 358, in load_image      image = skimage.io.imread(self.image_info[image_id]['path'])    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py"", line 61, in imread      img = call_plugin('imread', fname, plugin=plugin, **plugin_args)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/manage_plugins.py"", line 211, in call_plugin      return func(*args, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/skimage/io/_plugins/pil_plugin.py"", line 36, in imread      im = Image.open(f)    File ""/usr/local/lib/python3.6/dist-packages/PIL/Image.py"", line 2419, in open      prefix = fp.read(16)  OSError: [Errno 5] Input/output error    CONFIGURATION:     onfigurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.7  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 1  IMAGE_CHANNEL_COUNT            3  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                50  IMAGE_MIN_DIM                  800  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           coco  NUM_CLASSES                    38  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  PRE_NMS_LIMIT                  6000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                50  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               10  WEIGHT_DECAY                   0.0001    "
I'm running training in my own dataset based on balloon.py example.    In the first epoch I get this error:          What is it?
"Is there any function that automatically outputs AP, AP50, AP75, AP_S, AP_M and AP_L? These are the default metrics used in the paper and I would like to see the function which was used to achieve these. My question is, how could we evaluate the model which is trained on our own dataset. Is it pycocotools? Or does someone have an easier way of doing this?"
As is commented by author here:  !   any information about the paper?
"The objects I am trying to segment substantially vary in size. Some could be 8x8 pixels and some could be 500x500 pixels, in a 700x700 image.    So far Mask-RCNN does a very good job detecting small objects, but it completely misses the large ones. I know that `RPN_ANCHOR_SCALES` and FPNs are supposed to take care of multi-scale issues. But no matter how much I tweak the `RPN_ANCHOR_SCALES` I cannot get larger objects.    Any insights?  Thanks!"
"Hi,    I have had good results using resnet101 and the pre-trained coco weights: _mask_rcnn_coco.h5_.  Since I wanted to check out resnet50 in order to reduce training time, I changed the backbone, and used the same coco weights(101) as before. I was expecting this to fail, but it doesn't, and I was wondering why.    Is this because it is picking up only the relevant weights from coco 101? "
"Hi all, I am training the network using COCO pre-trained weights but the results are way lower than expected. The dataset I am using is the COCO dataset 2017. I have used different configurations and all of them give me this bad results. I would like to know if any of you knows what can I be doing wrong. I am running evaluation on val2017 using 500 images. Thank you!    Here are the results for bboxes:   Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.007   Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.015   Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.007   Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001   Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.005   Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.016   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.006   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.007   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.007   Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001   Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.004   Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.017    And here for segmentation:   Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.007   Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.012   Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.009   Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000   Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.005   Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.015   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.007   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.008   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.008   Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001   Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.005   Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.015    And here is the configuration used for this results (it is the one I found in the code by default):        # Name the configurations. For example, 'COCO', 'Experiment 3', ...etc.      # Useful if your code needs to do things differently depending on which      # experiment is running.      NAME = ""coco""  # Override in sub-classes        # NUMBER OF GPUs to use. When using only a CPU, this needs to be set to 1.      GPU_COUNT = 1        # Number of images to train with on each GPU. A 12GB GPU can typically      # handle 2 images of 1024x1024px.      # Adjust based on your GPU memory and image sizes. Use the highest      # number that your GPU can handle for best performance.      IMAGES_PER_GPU = 1        # Number of training steps per epoch      # This doesn't need to match the size of the training set. Tensorboard      # updates are saved at the end of each epoch, so setting this to a      # smaller number means getting more frequent TensorBoard updates.      # Validation stats are also calculated at each epoch end and they      # might take a while, so don't set this too small to avoid spending      # a lot of time on validation stats.      STEPS_PER_EPOCH = 1000        # Number of validation steps to run at the end of every training epoch.      # A bigger number improves accuracy of validation stats, but slows      # down the training.      VALIDATION_STEPS = 50        # Backbone network architecture      # Supported values are: resnet50, resnet101.      # You can also provide a callable that should have the signature      # of model.resnet_graph. If you do so, you need to supply a callable      # to COMPUTE_BACKBONE_SHAPE as well      BACKBONE = ""resnet50""        # Only useful if you supply a callable to BACKBONE. Should compute      # the shape of each layer of the FPN Pyramid.      # See model.compute_backbone_shapes      COMPUTE_BACKBONE_SHAPE = None        # The strides of each layer of the FPN Pyramid. These values      # are based on a Resnet101 backbone.      BACKBONE_STRIDES = [4, 8, 16, 32, 64]        # Size of the fully-connected layers in the classification graph      FPN_CLASSIF_FC_LAYERS_SIZE = 1024        # Size of the top-down layers used to build the feature pyramid      TOP_DOWN_PYRAMID_SIZE = 256        # Number of classification classes (including background)      NUM_CLASSES = 80 + 1  # Override in sub-classes        # Length of square anchor side in pixels      RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)        # Ratios of anchors at each cell (width/height)      # A value of 1 represents a square anchor, and 0.5 is a wide anchor      RPN_ANCHOR_RATIOS = [0.5, 1, 2]        # Anchor stride      # If 1 then anchors are created for each cell in the backbone feature map.      # If 2, then anchors are created for every other cell, and so on.      RPN_ANCHOR_STRIDE = 1        # Non-max suppression threshold to filter RPN proposals.      # You can increase this during training to generate more propsals.      RPN_NMS_THRESHOLD = 0.7        # How many anchors per image to use for RPN training      RPN_TRAIN_ANCHORS_PER_IMAGE = 256            # ROIs kept after tf.nn.top_k and before non-maximum suppression      PRE_NMS_LIMIT = 6000        # ROIs kept after non-maximum suppression (training and inference)      POST_NMS_ROIS_TRAINING = 2000      POST_NMS_ROIS_INFERENCE = 1000        # If enabled, resizes instance masks to a smaller size to reduce      # memory load. Recommended when using high-resolution images.      USE_MINI_MASK = True      MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask        # Input image resizing      # Generally, use the ""square"" resizing mode for training and predicting      # and it should work well in most cases. In this mode, images are scaled      # up such that the small side is = IMAGE_MIN_DIM, but ensuring that the      # scaling doesn't make the long side > IMAGE_MAX_DIM. Then the image is      # padded with zeros to make it a square so multiple images can be put      # in one batch.      # Available resizing modes:      # none:   No resizing or padding. Return the image unchanged.      # square: Resize and pad with zeros to get a square image      #         of size [max_dim, max_dim].      # pad64:  Pads width and height with zeros to make them multiples of 64.      #         If IMAGE_MIN_DIM or IMAGE_MIN_SCALE are not None, then it scales      #         up before padding. IMAGE_MAX_DIM is ignored in this mode.      #         The multiple of 64 is needed to ensure smooth scaling of feature      #         maps up and down the 6 levels of the FPN pyramid (2**6=64).      # crop:   Picks random crops from the image. First, scales the image based      #         on IMAGE_MIN_DIM and IMAGE_MIN_SCALE, then picks a random crop of      #         size IMAGE_MIN_DIM x IMAGE_MIN_DIM. Can be used in training only.      #         IMAGE_MAX_DIM is not used in this mode.      IMAGE_RESIZE_MODE = ""square""      IMAGE_MIN_DIM = 256      IMAGE_MAX_DIM = 1024      # Minimum scaling ratio. Checked after MIN_IMAGE_DIM and can force further      # up scaling. For example, if set to 2 then images are scaled up to double      # the width and height, or more, even if MIN_IMAGE_DIM doesn't require it.      # However, in 'square' mode, it can be overruled by IMAGE_MAX_DIM.      IMAGE_MIN_SCALE = 0      # Number of color channels per image. RGB = 3, grayscale = 1, RGB-D = 4      # Changing this requires other changes in the code. See the WIKI for more      # details:        IMAGE_CHANNEL_COUNT = 3        # Image mean (RGB)      MEAN_PIXEL = np.array([123.7, 116.8, 103.9])        # Number of ROIs per image to feed to classifier/mask heads      # The Mask RCNN paper uses 512 but often the RPN doesn't generate      # enough positive proposals to fill this and keep a positive:negative      # ratio of 1:3. You can increase the number of proposals by adjusting      # the RPN NMS threshold.      TRAIN_ROIS_PER_IMAGE = 200        # Percent of positive ROIs used to train classifier/mask heads      ROI_POSITIVE_RATIO = 0.33        # Pooled ROIs      POOL_SIZE = 7      MASK_POOL_SIZE = 14        # Shape of output mask      # To change this you also need to change the neural network mask branch      MASK_SHAPE = [28, 28]        # Maximum number of ground truth instances to use in one image      MAX_GT_INSTANCES = 100        # Bounding box refinement standard deviation for RPN and final detections.      RPN_BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2])      BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2])        # Max number of final detections      DETECTION_MAX_INSTANCES = 100        # Minimum probability value to accept a detected instance      # ROIs below this threshold are skipped      DETECTION_MIN_CONFIDENCE = 0.7        # Non-maximum suppression threshold for detection      DETECTION_NMS_THRESHOLD = 0.3        # Learning rate and momentum      # The Mask RCNN paper uses lr=0.02, but on TensorFlow it causes      # weights to explode. Likely due to differences in optimizer      # implementation.      LEARNING_RATE = 0.001      LEARNING_MOMENTUM = 0.9        # Weight decay regularization      WEIGHT_DECAY = 0.0001        # Loss weights for more precise optimization.      # Can be used for R-CNN training setup.      LOSS_WEIGHTS = {          ""rpn_class_loss"": 1.,          ""rpn_bbox_loss"": 1.,          ""mrcnn_class_loss"": 1.,          ""mrcnn_bbox_loss"": 1.,          ""mrcnn_mask_loss"": 1.      }        # Use RPN ROIs or externally generated ROIs for training      # Keep this True for most situations. Set to False if you want to train      # the head branches on ROI generated by code rather than the ROIs from      # the RPN. For example, to debug the classifier head without having to      # train the RPN.      USE_RPN_ROIS = True        # Train or freeze batch normalization layers      #     None: Train BN layers. This is the normal mode      #     False: Freeze BN layers. Good when using a small batch size      #     True: (don't use). Set layer in training mode even when predicting      TRAIN_BN = False  # Defaulting to False since batch size is often small        # Gradient norm clipping      GRADIENT_CLIP_NORM = 5.0  "
"image ID: coco.195918 (4512)    Processing 1 images  image                    shape: (1024, 1024, 3)       min:    0.00000  max:  255.00000  uint8  molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  151.10000  float64  image_metas              shape: (1, 93)               min:    0.00000  max: 1024.00000  int32  anchors                  shape: (1, 261888, 4)        min:   -0.35390  max:    1.29134  float32  gt_class_id              shape: (6,)                  min:   57.00000  max:   67.00000  int32  gt_bbox                  shape: (6, 4)                min:    0.00000  max: 1024.00000  int32  gt_mask                  shape: (1024, 1024, 6)       min:    0.00000  max:    1.00000  bool    end    no image?"
"A deep learning business tools based on mask_rcnn to do some modification: The original implementation uses Resnet101 as a backbone but it's huge and heavy, and impossible to train on a simple PC,  so we changed it to Mobilenet from keras.applications. Moreover, we use only 3 levels of Feature Pyramid Network instead of 4 for performance reasons.  Can anyone understand it and give me any tips how to use only 3 levels of Feature Pyramid Network instead of 4 for performance reasons?"
What should be the threshold for the precision-recall curve? Is it the IoU threshold?
"Im trying to debug the crop_and_replace function in ROI and replace it with another using eager_execution(), i run the `tf.enable_eager_execution()` in the **models.py** and tried to check if it is running by using `tf.executing_eagerly()` but it returns False! any guide on how can i use it to debug?  "
How to convert multilple jsons got from labelme annotation tool to the one that matches with COCO/BALLOON dataset? 
I want to use intel OpenVino SDK for fast computation on CPU.  Does anyone tried this:   
"I have rectangle images of size 256*960. When I use pad64 setting with min dim 256 and max dim 1024, the training gives zero mrcnn losses. The code runs fine but the trained model makes no sense. Using square mode is fine but much slower. Using crop mode also works. But I'd prefer not using it as the code skips training for all background crops. Can anyone help? Thanks."
"Can i use this repository to segment human face parts like eyes, nose, mouth,ears,.."
"VIDEOIO ERROR: V4L: can't open camera by index 0  Traceback (most recent call last):    File ""/home/njust/ZB/Mask_RCNN-master/samples/demo.py"", line 81, in        results = model.detect([frame], verbose=1)    File ""/home/njust/ZB/Mask_RCNN-master/mrcnn/model.py"", line 2503, in detect      molded_images, image_metas, windows = self.mold_inputs(images)    File ""/home/njust/ZB/Mask_RCNN-master/mrcnn/model.py"", line 2401, in mold_inputs      mode=self.config.IMAGE_RESIZE_MODE)    File ""/home/njust/ZB/Mask_RCNN-master/mrcnn/utils.py"", line 420, in resize_image      image_dtype = image.dtype  AttributeError: 'NoneType' object has no attribute 'dtype'  Processing 1 images  image    Process finished with exit code 1  "
"Below is my code:             The function **stats_graph()** is then added in the function **detect**.    when I run inference, there is always _**ops no flops stats due to incomplete shapes**_. It seems that the placeholders are still not filled with inputs which cause the incomplete shapes.  How to deal with it?"
"When I set the architecture to config.BACKBONE to `resnet101`, and set the image shape to `(1024,1024,3)`, I get the shapes of five stages of backbone resnet: `C1:(256,256,64)`,`C2:(256,256,256)`,`C3:(128,128,512)`,`C4:(64,64,1024)`,`C5:(32,32,2048)`.  But the `config.BACKBONE_STRIDES` is `[4, 8, 16, 32, 64]`.Should the `config.BACKBONE_STRIDES` be `[4,4,8,16,32]`?  Thanks for any help and discussion."
None
"i want just finetune bbox, and do not have segmention"
"I wonder if I can get the training parameters to train the mask rcnn model,such as training batchsize,learning rate and so on.Thanks a lot."
how to use this model only in objection detection 
"Hello Everyone,  I have been working recently on concrete crack detection or lets say crack detection but I have no success with it. I have gone through all mentioned examples and demos of Coco, balloon and nucleus dataset and tried them just to know whats happening. I have only one class for my project so I stick to balloon example and made a new model from scratch to training but while validating my model, prediction says No instances found although when I was trying inspect data copy and load my own dataset and model it was doing just fine But when i tried the Inpect_model file for my validation data I got no prediction.     I have changed the following prameters      But most of the parameters like FPN, ROI in the config.py and model.py files were same.   The results gives me nothing at all. Can somebody points out to my problem what is the issue @waleedka ?    Do I have to make some changes because I thought its similar to balloon example with one class so i simply annotate my data of 82 training images and 19 val images and give it a run.   While checking the inspect data results I got following but while validating my model I got this result.  **Inspecting the pre-process training data**  !   !     **Predictions Result of Model**  !   "
"When I created the model for training, I was able to train normally on my computer, but when someone else executed my code I got the following unexpected error:         I understand what this is wrong, the parameters I pass are basically the default parameters, please guide me some, thank you  "
"Hello,    I have a few different datasets that I'd like to keep separated as instances of the Dataset class.  I want to develop a function that returns a dataset that would be the merging of two input datasets.     I've already done something to generate a dataset by adding multiple input paths to my class init but i wondered if it was possible to merge two already existing datasets.    Has someone already done something like this? I'm struggling to understand how to process the image_ids and image_infos in order to merge two datasets.    Thank you!  "
So how do I explore mask_rcnn? Please let me know the possibilities and limitations to work with mask_rcnn using cpu.
"Now I have two types of data, one is benign thyroid data, the other is malignant thyroid data. They are labeled as mask maps. A map only represents one lesion. Do I need to classify my data according to benign and malignant, and then use Mask Rcnn training?  If not, how does the model know which of my data is benign and which is malignant?    "
"Hi,    My detection works great, but it takes ~9 seconds to complete for a single image! I don't have GPU capabilities and my CPU is i5 at 3.2GHz.    The idea I am having is to somehow load the provided h5 file partially, with only the classes that are of my interest. For example if I want to detect only vehicles (cars, trucks and buses) I don't want it analyzing trees, birds and so on.         Question: Can I load weights only for certain class IDs? This should speed things up greatly, I hope.    Thanks"
None
None
"Hello, not sure if this should be a pull request or not. Pretty new to github and coding in general.   I encountered an issue when trying to run the inspect_balloon_model.ipynb  when trying to run the notebook it gave me the error that samples.balloon was not a module that could be imported. I fixed this by adding `__init__.py` files to both directories  I tried installing the setup file with both python2 and 3, but this made no difference.   Just thought I'd share my experiences in case others run into the same problem."
"Hi,     I set up demo.ipynb correctly so now every time I input an image I get as an output the same image with the elements detected highlighted with different colours. The question is: how can I get how many elements of one class have been detected? e.g. If the network has detected 5 people and 3 cars and  I would like to get as an output ""5 people, 3 cars"".      Thanks,   "
"So, I have been working on concrete crack detection and before writing in the files i tried the balloon example but when i switched to balloon example as cracks are also small compared to balloons and nucleus example was good  start to work as reference. But I am getting this error and I have tried installing various packages but all in vein. No success so far so if anyone of you have gone through the same error please, let me know.      Thank you  -Rakeh"
"According to  , APs can be reported based on small, medium and large-scale objects. These AP Scales are strictly predefined as objects smaller than 32x32 to greater than 96x96 pixels.    Now I have three questions:  1) First of all, how can I get such statistics for my own trained model?  I tried running `python coco.py evaluate --dataset=/my/data/set/ --model=/my/trained/weights.h5` but I get a shape-matching error as  `Traceback (most recent call last):    File ""coco.py"", line 474, in        model.load_weights(model_path, by_name=True)    File ""/home/soroushr/projects/Mask_RCNN/mrcnn/model.py"", line 2130, in load_weights      saving.load_weights_from_hdf5_group_by_name(f, layers)    File ""/home/soroushr/projects/envs/Planet/lib/python3.6/site-packages/keras/engine/saving.py"", line 1149, in load_weights_from_hdf5_group_by_name      str(weight_values[i].shape) + '.')  ValueError: Layer #391 (named ""mrcnn_bbox_fc""), weight   has shape (1024, 324), but the saved weight has shape (1024, 8).`  2) Is there a way to change what is 'large' or 'small' or 'medium'?  3) Can I customize the range and number of scales for which I am getting my APs? For example, assume I want to get APs for objects in scales ranging from ( < 8x8), (8x8 to 32x32), (32x32 to 64x64), (64x64 to 128x128), (128x128 to 256x256), and (256x256 < ). I understand that it requires tuning the RPN_ANCHOR_SCALES to the needs, but assume that is properly taken care of.    Thanks!"
"Hello everyone.  I got 22% training loss and 23% validation loss. But, when I ran the demo on my test set I got random predictions. No one of these predictions was a good result. However, when I ran inspect_model I got good predictions. Please anyone can tell me why? How I improve it?     "
"dear every one       forgive my poor  english pls.     i want to run mask rcnn on a gpu cluster on elephas,but the elephas will call kerasmodel.to_json which will lead to      what can i do to work around it?    any suggestions is welcome,thanks alot!"
"Can this be modified for ""interest point"" detection? I have a single point in an image that I would like to detect. Is it an overkill to use a segmentation network like MaskRCNN to do this? Are there any alternates?"
"On Jupyter, the outputs for inference are displayed.    I get the log   > Saved to \     But, the directory contains a set of **blank images**.    Using Python3.6 on Fedora 29    !   "
"Hey,  im running a modified version of the nucleus sample.    My dataset contains 3629 images and for comparison, I need a prediction for every one of those images.    If I splitted my dataset into, say 65% train and 20% validation and 15% testing set, that means I can only get a prediction for 15% of the images.    Should I train new models each time with different training and validation sets to get a prediction for each image?    Or should I just split the images 75/25 train and validation, and then let the model predict all images (even though then some images are of course not 'unseen' or 'new' to the model)?    Is there some kind of best practice or something?    Thanks!"
I would like to know if this is possible to implement in a video segment. Thank you!
"I used resnet152 to train new model, but when I tested it with demo.ipynb, I couldn't detect any target. Please help me, thanks very much! "
None
How to show the evalution map in the training?
"I think it'll be a great feature for us who train on Google Colab. I am happy to start this thread and work on it. However, some guidance would be very useful @waleedka . What do you think needs to be changed to get this to work? And is it possible to do so with minimal effort in the current way of how Matterport Mask RCNN is designed ?"
I have annotated my huge traning datasets using VGG anotator tools to train MaskRCNN model implemented by matterplot but in matterplot implementation only two backbones are use ResNet 50 and ResNet 101(FPN).But now I want to use other backbones also in mask RCNN.So how I will use different backbones like inception v2 in this implementation
"When I run Create Model and Load Trained Weights, I downloaded mask_rcnn_coco.h5 file.      ---------------------------------------------------------------------------  ImportError                               Traceback (most recent call last)    in  ()        3         4 # Load weights trained on MS-COCO  ----> 5 model.load_weights(COCO_MODEL_PATH, by_name=True)    C:\ProgramData\Anaconda3\lib\site-packages\mask_rcnn-2.1-py3.6.egg\mrcnn\model.py in load_weights(self, filepath, by_name, exclude)     2099         exclude: list of layer names to exclude     2100         """"""  -> 2101         import h5py     2102         # Conditional import to support versions of Keras before 2.2     2103         # TODO: remove in about 6 months (end of 2018)    C:\ProgramData\Anaconda3\lib\site-packages\h5py\__init__.py in  ()       24 # We tried working around this by using ""package_dir"" but that breaks Cython.       25 try:  ---> 26     from . import _errors       27 except ImportError:       28     import os.path as _op    ImportError: cannot import name '_errors'"
"hi every body, i made a dataset, but has error when i load data    > Traceback (most recent call last):  >   File ""catvsdog.py"", line 373, in    >     train(model)  >   File ""catvsdog.py"", line 196, in train  >     dataset_val.load_cat_dog(args.dataset, ""val"")  >   File ""catvsdog.py"", line 117, in load_cat_dog  >     annotations =      i try to editor my dataset code  `{""1.jpeg4055"": {""filename"": ""25.jpg"",""size"": 22699,""regions"":[{""shape_attributes"":{""name"":""rect"",""x"":68,""y"":23,""width"":106,""height"":156},""region_attributes"": {""head"": ""head""}}],""file_attributes"": {}}}`    but error still had           "
"I  add a period=20 to the ModelCheckpoint callback.         But the model weight still be saved each epoch,why??"
"Hi,    So I have used the documentation to train my own model and tested it on my validation set of images.  However, is this just not using the supplied annotated masks I made, from the validation image set?    I want to show the model a new image, that has not been annotated to see how well it performs in segmenting it.    How do I go about doing that?    Thanks in advance!    P.S, how do I get the tensorboard training graphs to monitor my loss?"
"I'm trying to train the model on my own dataset with 4 classes using the COCO pretrained weights. But before training, I get the error `index ' ' is out of bounds for axis ' ' with size ' '`. There are different values in different instances.    I think the error occurs because the mask values returned from `skimage.draw.polygon` exceed the indices in the array. I can't seem to figure out a way to correct this problem. Any help would be appreciated. I followed the balloons script and coco script when creating the script for my custom dataset."
I am getting this error while inspecting model...i had trained on own dataset.  module 'utils' has no attribute 'Dataset'  I am working in CPU windows8.1.........  
"What is ""via_region_data.json""?how can i get it?"
"the code runing longtime in 1/30 epoch without traceback, could anyone tell me what problem.  `  2019-03-10 10:20:02.210479: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1030] Found device 0 with properties:   name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582  pciBusID: 0000:01:00.0  totalMemory: 12.00GiB freeMemory: 9.93GiB  2019-03-10 10:20:02.210812: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)  Training network heads    Starting at epoch 0. LR=0.001    Checkpoint Path: D:\deeplearning_cai\objiect_recognize\mask_RCNN\Mask_RCNN-master\data_marcnn\logs\balloon20190310T1020\mask_rcnn_balloon_{epoch:04d}.h5  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  In model:  rpn_model      rpn_conv_shared        (Conv2D)      rpn_class_raw          (Conv2D)      rpn_bbox_pred          (Conv2D)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)  D:\study_software\Anaconda\lib\site-packages\tensorflow\python\ops\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  Epoch 1/30  `"
"I want a mask rcnn .pb file and anchors file to be used in android, can anyone tell me how do I generate them as I only have a .h5 file"
"When I train the model to identify one object ,it looks like ok.       But when I train the model to identify multiple kinds of objects,it looks like so difficult.       How can I adjust the training process to raise good performance?  Anybody can help?Thanks ahead."
"If I understand correctly, the mask segmentation is applied to the ROI region which is obtained from RPN. However, the ROI is not the final object region which is not that accurate. In this case, will there have a lot of errors when the mask segmentation is applied to the un-accurate ROI?  Currently, the class classification, bounding box regression, and mask segmentation run separately. Why not applying the mask segmentation based on the result of bounding box regression, but not the ROI from RPN?"
"Hi,    I get the following error when I run coco.py in the samples folder:    > ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'  > ImportError: numpy.core.multiarray failed to import   >     Does anyone know why this is happening? I am on Python 3.7.1 and numpy 1.15.4. "
"I am now using Mask_RCNN package to deal with my images.   In the package there is a demo.ipynb which can be used as a start to use Mask_RCNN. Now I am trying to use this pre-trained model to recognize the objects and make masks of my own images. However the results are not good, for example there are some humans in the images cannot be recognized, and also some parts of a certain human cannot be recognized completely, etc. Up to now I only have 12 images and cannot implement a new train on my own data set, so I suppose that is it possible to adjust the parameters of the existed pre-trained model to get a more optimal result? And how can I do that? How to change the parameters of the demo model? I am a green-hand here and really need your help! Thank you so much!"
Hi     I was trying to save the full model with weights and deploy it to TF-Servings.    Can someone guide mw in how to save full model with weights and deploy it on Servings
Please help me to train on CityScape dataset.
"I noticed that when I ran the Mask RCNN in samples/shapes, that classification was different if I ran it first thing vs. several epochs later (that is, the model changes over time when you are using it). Is anyone else experiencing that? I made sure I loaded it in ""inference"" mode. "
When I use two gpus to train Mask_Rcnnï¼Œit will appear error.    InvalidArgumentError (see above for traceback): Cannot colocate nodes 'tower_0/mask_rcnn/Variable/anchors/Variable/read_tower_0/mask_rcnn/Variable_0' and 'anchors/Variable: Cannot merge devices with incompatible ids: '/device:GPU:1' and '/device:GPU:0'     ]]  
None
None
"1) What is the approximate configuration should be a computer for learning mask rcnn on images 1024x1024?  2) Questions about dataset:  2.1) is it necessary to mark the object in the image completely, or some of its parts are better not to allocate (if it concerns cars or gasoline canisters)  2.2) if there are several objects of the same type in the picture that need to be detected, but they stand one after another and partially overlap each other (for example, cars stand in the parking lot one after another) - do i have to mask  each car, even if there is a small piece from the car, or do I have to mask with a certain visibility value (50%? 70%?), etc.? Or maybe better to avoid such images altogether and mark only those in which cars are fully visible?  2.3) what else requirements are imposed to, where it would be possible to learn it?"
"Hello Sir,  I have successfully run MaskRCNN. In the last cell, I have seen that it runs fine but not show the detected image. Please see the picture which is given below:  !     where it should be like this:    !     I don't understand where is the problem. Please help me."
"Hi All,    I have around 39k training images of 500x500 size and in validation set is 10% of the entire dataset (~4800 images). My class to predict the mask for ground feature building in the aerial images. Each image has ~100 buildings of different size and shape. The configuration I am using is below -    BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     4  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.5  DETECTION_NMS_THRESHOLD        0.3  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 1  **IMAGE_MAX_DIM                  512**  IMAGE_META_SIZE                14  **IMAGE_MIN_DIM                  512  IMAGE_RESIZE_MODE              square**  IMAGE_SHAPE                    [512 512   3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  **MAX_GT_INSTANCES               100**  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NUM_CLASSES                    2  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  **RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)**  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  **RPN_NMS_THRESHOLD              0.9**  RPN_TRAIN_ANCHORS_PER_IMAGE    256  **STEPS_PER_EPOCH                9800**  TRAIN_BN                       False  **TRAIN_ROIS_PER_IMAGE           200**  **USE_MINI_MASK                  False**  USE_RPN_ROIS                   True  VALIDATION_STEPS               1220  WEIGHT_DECAY                   0.0001    I used following training scheme,   1. heads = epoch=20, aug=true, LR=0.001  2. 4+ = epochs=40,aug=true,LR=0.001/10  3. ALL = epochs=80,aug=true,LR=0.001/100    But after 25 epochs, val loss started increasing while training loss was going down. Means model started getting overfitted.  Accuracy on test set is -   Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.247   Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.546   Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.192   Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.149   Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.355   Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.163   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.019   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.143   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.331   Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.206   Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.459   Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.377    Can someone highlight, what improvements can be done ? Training set is not too small which could cause the issue. Should I not train the heads and train all the layers instead ? or what hyperparams to tweak to avoid overfitting ?"
"Hi,  Did any one use this model to identify some small objects before?  I tried,but the effects were not so good.  Can anyone help to give me some hints?Thanks ahead.  When I want to identify some ports in this picture,  !     I got like this....it really confuse me totally.Try to improve it ,need lot of helps.  !   "
Hello    I was wondering the meaning of each output you get during training for example :    !     
"Hello  I am trying to train the Mask RCNN model for licence plate detection i followed the ballon detection examlpe and strated training without changing any of the hyperparameters (except multiprocessing=false since i am using cpu) by observing the training i noticed that the loss function started high in the first epochs and goes up and down a lot from one epoch step to another, and it only got down to 0.70 by the 23th epoch (out of 30) with an above 1 val_loss value still, so i figured i should increase the batch size (left batch size=1) since my image dataset is pretty diverse in order to reduce the variance of the loss function but i can't seem to do so since everytime i change the batch size on config.py and launch training it shows me BATCH SIZE=1 each time, i currentlly restarted training with the weights generated from the 23th epoch.    !       I am new to neural networks so bare with me please any help will be much appriciated, thank you all very much and have a good day."
Can i use your model in tensorflow.js?
Hello     I have tried to run  the demo notebook     after  following the installation instruction to a new conda environment      and i have received this error :   ModuleNotFoundError: No module named 'tensorflow'    any help ?     
Hi there!    does anyone know how to implement different backbones such as ResNet18?
"after training samples/coco/coco.py, i want to use coco.py evaluate  to inference images and save the inference output, how can I achieve it?"
"Traceback (most recent call last):    File ""./coco.py"", line 498, in        layers='heads')    File ""/data/g/weidaihua/Mask_RCNN-master/model.py"", line 2207, in train      validation_data=next(val_generator),    File ""/data/g/weidaihua/Mask_RCNN-master/model.py"", line 1604, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/data/g/weidaihua/Mask_RCNN-master/model.py"", line 1163, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""./coco.py"", line 249, in load_mask      image_info[""width""])    File ""./coco.py"", line 308, in annToMask      rle = self.annToRLE(ann, height, width)    File ""./coco.py"", line 294, in annToRLE      rle = maskUtils.merge(rles)    File ""pycocotools/_mask.pyx"", line 145, in pycocotools._mask.merge (pycocotools/_mask.c:3173)    File ""pycocotools/_mask.pyx"", line 122, in pycocotools._mask._frString (pycocotools/_mask.c:2605)  TypeError: Expected bytes, got str    who know the reason? thanks"
"Noticed the following in the method load_coco in coco.py:    If class_ids is None, then all the class IDs are first loaded. This then makes class_ids _not_ None and the line ""image_ids = list(coco.imgs.keys()) is never reached.    I think the image_ids should be loaded first, then the class ids."
"Hi everyone!    I am trying to save the full model (not only the weight).   I use the command:       >> _model.keras_model.save(path_file)_  And it does not give any error.    But, when I try to load the full model shows an error, i am using these comands:      >> _from keras.models import load_model_      >> _model_test = load_model(path_file)_    And it shows this error:    _~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\models.py in load_model(filepath, custom_objects, compile)      268             raise ValueError('No model found in config file.')      269         model_config = json.loads(model_config.decode('utf-8'))  --> 270         model = model_from_config(model_config, custom_objects=custom_objects)      271       272         # set weights    ~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\models.py in model_from_config(config, custom_objects)      345                         'Maybe you meant to use '      346                         '`Sequential.from_config(config)`?')  --> 347     return layer_module.deserialize(config, custom_objects=custom_objects)      348       349     ~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\layers\__init__.py in deserialize(config, custom_objects)       53                                     module_objects=globs,       54                                     custom_objects=custom_objects,  ---> 55                                     printable_module_name='layer')    ~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)      142                 return cls.from_config(config['config'],      143                                        custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +  --> 144                                                            list(custom_objects.items())))      145             with CustomObjectScope(custom_objects):      146                 return cls.from_config(config['config'])    ~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\topology.py in from_config(cls, config, custom_objects)     2523         # First, we create all layers and enqueue nodes to be processed     2524         for layer_data in config['layers']:  -> 2525             process_layer(layer_data)     2526         # Then we process nodes in order of layer depth.     2527         # Nodes that cannot yet be processed (if the inbound node    ~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\topology.py in process_layer(layer_data)     2509      2510             layer = deserialize_layer(layer_data,  -> 2511                                       custom_objects=custom_objects)     2512             created_layers[layer_name] = layer     2513     ~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\layers\__init__.py in deserialize(config, custom_objects)       53                                     module_objects=globs,       54                                     custom_objects=custom_objects,  ---> 55                                     printable_module_name='layer')    ~\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\utils\generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)      136             if cls is None:      137                 raise ValueError('Unknown ' + printable_module_name +  --> 138                                  ': ' + class_name)      139         if hasattr(cls, 'from_config'):      140             custom_objects = custom_objects or {}    ValueError: Unknown layer: BatchNorm_      Has anyone tried to save a full model? Is it possible? has anyone a solution?    Thanks,"
"  # This component assembles an analysis recipe for the annual adaptive comfort component  #  # Honeybee: A Plugin for Environmental Analysis (GPL) started by Mostapha Sadeghipour Roudsari  #   # This file is part of Honeybee.  #   # Copyright (c) 2013-2018, Chris Mackey     # Honeybee is free software; you can redistribute it and/or modify   # it under the terms of the GNU General Public License as published   # by the Free Software Foundation; either version 3 of the License,   # or (at your option) any later version.   #   # Honeybee is distributed in the hope that it will be useful,  # but WITHOUT ANY WARRANTY; without even the implied warranty of   # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the   # GNU General Public License for more details.  #   # You should have received a copy of the GNU General Public License  # along with Honeybee; If not, see   0 and len(_viewFactorInfo) > 0 and _epwFile != None and _srfIndoorTemp.BranchCount > 0 and _zoneAirTemp.BranchCount > 0  and _zoneAirFlowVol.BranchCount > 0 and _zoneAirHeatGain.BranchCount > 0 and initCheck == True:      if _viewFactorInfo[0] != None:          recipe = checkTheInputs()          if recipe != -1:              comfRecipe = recipe"
"Hi, I want to know why we can't use images with only background. This line in model.py skip theme.                 if not np.any(gt_class_ids > 0):                  continue    Maybe If we keep these images, the rpn will learn to recognize negatives regions ?    "
Can this be run in Google Colab gpu?   Did someone do it?
"This has a very good explanation!!    Does any of you donne this without mask, I mean a new branch but with the mask remove just bouding boxes? "
"Hi,    I am having around 40000 aerial images of size 500x500 to predict masks for buildings.Can anyone suggest is it necessary to train heads in this case ? is it fine if i only use 'all' layer option to fine tune all the layers. Currently the results I am getting after training 40 epochs of training 'heads' are not good. its leaving few building to generate mask and for most of them shapes are not good. Any other configuration parameter which can be adjusted ?    Thanks in advance,"
"Thanks a lot for your generous share! When I trained the nucleus.py, there was a problem:  Epoch 1/20 1/632    "
I tried to run this model for not overlapping objects it did great .. but when I tried using it for detecting for example rectangles and the rectangles inside it it fails terribly. I mean if it a case that this model can handle how can i debug mine for errors and what to look for? 
"I trained the nuclei images from the Segmenting Nuclei in Microscopy Images project, both network heads and all layers, with 10 epochs for each (20 total), using the mask_rcnn_coco.h5 weight file as the  starting place, and a modified version of nucleus.py.    Now I have 20 weight files, created from each of those epochs.  I want to train Mask R-CNN on a different dataset, one of images of soil bacteria.    Which weight file, of the 20, should I use instead of the mask_rcnn_coco.h5 file to start this training session?    Each weight file's epoch has a different set of ""losses"".  Should I use the very last epoch's weight   file to start training on my own images?  Or should I choose the weight file from my 20, that has the best set of ""losses"".    Should I use one from the ""heads only"" run, or from the ""all layers"" run?    Thank you for your help.        "
"Can I run this code on my own dataset, using Macbook pro 2015? I have 8GB of RAM."
"We have this function in mrcnn:           However, this only visualizes the images, what if we want to save it under some directory? If we try that with OpenCV:        folder_path = '/tmp'      result_save_path = folder_path + ""/result_image.png""      cv2.imwrite(result_save_path, masked_image.astype(np.uint8))    The image is saved but only the masks appear, rest of the information (labels, bboxes) don't.     What's wrong?    Any thoughts?"
"Hello,  # Image mean (RGB)  MEAN_PIXEL = np.array([43.53, 39.56, 48.22])    How is image mean calculated in configuration file ?    If small objects like scratches needs to be detected from large object like metal surface (detect scratches from metal surface what parameters needs to be changed?)"
"In the demo.ipynb, when I tried to run the code:  model.load_weights(COCO_MODEL_PATH, by_name=True)    I got the following error. Please advise how to solve it    Thanks    TypeError                                 Traceback (most recent call last)    in  ()        3         4 # Load weights trained on MS-COCO  ----> 5 model.load_weights(COCO_MODEL_PATH, by_name=True)    C:\Mask_RCNN-2.1\model.py in load_weights(self, filepath, by_name, exclude)     2035      2036         if by_name:  -> 2037             saving.load_weights_from_hdf5_group_by_name(f, layers)     2038         else:     2039             saving.load_weights_from_hdf5_group(f, layers)    C:\Anaconda3\lib\site-packages\keras\engine\saving.py in load_weights_from_hdf5_group_by_name(f, layers, skip_mismatch, reshape)     1152                                                 weight_values[i]))     1153   -> 1154     K.batch_set_value(weight_value_tuples)    C:\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py in batch_set_value(tuples)     2468             assign_ops.append(assign_op)     2469             feed_dict[assign_placeholder] = value  -> 2470         get_session().run(assign_ops, feed_dict=feed_dict)     2471      2472     C:\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)      928     try:      929       result = self._run(None, fetches, feed_dict, options_ptr,  --> 930                          run_metadata_ptr)      931       if run_metadata:      932         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)    C:\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)     1151     if final_fetches or final_targets or (handle and feed_dict_tensor):     1152       results = self._do_run(handle, final_targets, final_fetches,  -> 1153                              feed_dict_tensor, options, run_metadata)     1154     else:     1155       results = []    C:\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)     1327     if handle is None:     1328       return self._do_call(_run_fn, feeds, fetches, targets, options,  -> 1329                            run_metadata)     1330     else:     1331       return self._do_call(_prun_fn, handle, feeds, fetches)    C:\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)     1333   def _do_call(self, fn, *args):     1334     try:  -> 1335       return fn(*args)     1336     except errors.OpError as e:     1337       message = compat.as_text(e.message)    C:\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)     1318       self._extend_graph()     1319       return self._call_tf_sessionrun(  -> 1320           options, feed_dict, fetch_list, target_list, run_metadata)     1321      1322     def _prun_fn(handle, feed_dict, fetch_list):    C:\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)     1406     return tf_session.TF_SessionRun_wrapper(     1407         self._session, options, feed_dict, fetch_list, target_list,  -> 1408         run_metadata)     1409      1410   def _call_tf_sessionprun(self, handle, feed_dict, fetch_list):    TypeError: TF_SessionRun_wrapper: expected all values in input dict to be ndarray    TypeError: TF_SessionRun_wrapper: expected all values in input dict to be ndarray"
None
When I am running demo.py. I am getting one label for each boundary box with the score. I want all the labels with confidence score for a box. How could I achieve this.    Thanks in advance!
"Hi,     I am trying to deploy Mask RCNN to Google's Cloud ML Engine. I have managed to run the model in batch mode and collect the results of the output layers. However, ideally, I'd like to also compute the final mask as part of the job.     In other words, this part:       I wonder whether anyone managed to accomplish this or has any ideas how to achieve it?"
"When doing inference on my Tesla P4 GPU, the inference time is ~5.3s.    Here is the python source that I'm using to do the inference:         And the output is:         I changed mrcnn/model.py so that it prints the inference time:       Other environment details below.    I'm using tensorflow-gpu:       NVIDIA drivers and CUDA 9.0 are correctly installed:            Is there something wrong / missing in these steps?"
Batch size = gpu_count * number of images per gpu. I understand that this is the batch size for the training data but is the same for the validation data as well?    The reason I ask this is so that I can set validation steps parameter appropriately since   Validation steps = Number of validation samples / Validation batch size
"Hi, I'm trying to train the model with dataset coco 2017, but it reports error as following. Does anyone have the same problem? How to fix it? thanks!  I'm using Ubuntu 16.04 64bit, Python 3.6.7, pip 18.1, tensorflow_gpu 1.13.0-rc0, keras 2.2.4, cuda 10.0.130, libcudnn7-dev_7.4.2.24-1, libcudnn7_7.4.2.24-1    2019-01-31 10:11:33.060384: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2019-01-31 10:11:33.060528: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2019-01-31 10:11:43.572615: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2019-01-31 10:11:43.572742: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]   999/1000 [============================>.] - ETA: 0s - loss: 0.3451 - rpn_class_loss: 0.0034 - rpn_bbox_loss: 0.0729 - mrcnn_class_loss: 0.0632 - mrcnn_bbox_loss: 0.0454 - mrcnn_mask_loss: 0.16012019-01-31 10:24:40.241827: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2019-01-31 10:24:40.241952: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  2019-01-31 10:24:41.861193: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]  1000/1000 [==============================] - 880s 880ms/step - loss: 0.3449 - rpn_class_loss: 0.0034 - rpn_bbox_loss: 0.0729 - mrcnn_class_loss: 0.0631 - mrcnn_bbox_loss: 0.0454 - mrcnn_mask_loss: 0.1600 - val_loss: 2.0419 - val_rpn_class_loss: 0.1070 - val_rpn_bbox_loss: 0.9081 - val_mrcnn_class_loss: 0.4638 - val_mrcnn_bbox_loss: 0.2041 - val_mrcnn_mask_loss: 0.3590  Epoch 6/12  1000/1000 [==============================] - 799s 799ms/step - loss: 0.3522 - rpn_class_loss: 0.0037 - rpn_bbox_loss: 0.0779 - mrcnn_class_loss: 0.0551 - mrcnn_bbox_loss: 0.0508 - mrcnn_mask_loss: 0.1647 - val_loss: 1.5071 - val_rpn_class_loss: 0.0384 - val_rpn_bbox_loss: 0.8576 - val_mrcnn_class_loss: 0.1833 - val_mrcnn_bbox_loss: 0.1751 - val_mrcnn_mask_loss: 0.2529  "
"Hi everybody,  I have been using mask rcnn for a while now running on an AWS EC2 instance (p2.xlarge). This instance came with a Tesla k80 with half the memory (12Gb). It could run inference on 320x240 rgb images in 200ms. Now I am looking to have my own inference machine locally and am thinking about buying a GPU to replace the K80. I couldn't find any benchmarking for maskrcnn. This   was helpful but I wanted to ask you guys about your experience. Would an RTX 2070 be able to handle inference in 200ms, or even lower? I can't run inference on more than one image as I am getting data from a camera so my batch size in inference is limited to 1. Also do you have any experience with 16bit floats for maskrcnn?"
"Hi, Is there any logic or algorithm for selecting the best value for setting the minimum detection threshold?"
"This is more of a question rather than an issue. I've noticed that while only training the heads of the network the training and validation loss decreases and plateaus after a certain number of epochs. When I start training parts of the backbone and eventually the entire network the model begins to overfit (validation loss increases while training loss decreases).     My question is, should I stop trying to train the entire network and just train the network heads and stop once the losses stabilise? Or should I keep playing around with the hyperparameters to see if better performance can be obtained?     I tried increasing the regularisation but it didn't seem to help once the model moved on to training parts of the backbone.    Edit: Would altering regularisation in different training phases help? That is:    - Train network heads with regularisation 0.001  - Train parts of the backbone with regularisation 0.005  - Train entire network with regularisation 0.01"
"Hello I tried to train balloon based on the blog post in medium  but I get an error with StopIteration() , the error is shown below :    F:\Anaconda3\envs\MaskRCNN\lib\site-packages\tensorflow\python\ops\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  Epoch 1/30  Traceback (most recent call last):    File ""balloon.py"", line 364, in        train(model)    File ""balloon.py"", line 199, in train      layers='heads')    File ""F:\Anaconda3\envs\MaskRCNN\lib\site-packages\mask_rcnn-2.1-py3.6.egg\mrcnn\model.py"", line 2375, in train    File ""F:\Anaconda3\envs\MaskRCNN\lib\site-packages\keras\legacy\interfaces.py"", line 87, in wrapper      return func(*args, **kwargs)    File ""F:\Anaconda3\envs\MaskRCNN\lib\site-packages\keras\engine\training.py"", line 2065, in fit_generator      generator_output = next(output_generator)    File ""F:\Anaconda3\envs\MaskRCNN\lib\site-packages\keras\utils\data_utils.py"", line 710, in get      raise StopIteration()  StopIteration    Is there anyone have this error and able to solve this error?? In my opinion this because maybe my GPU memory is too small  Thank you    My enviroment is windows 10 x64 ,GPU :GeForce GTX 1050 2GB CUDA8.0 cudnn6  anaconda under venv: python 3.6 tensorflow-gpu 1.4.0"
"Hi, I have the error in the example     image, window, scale, padding = mrcnn.utils.resize_image(      image,       min_dim=config.IMAGE_MIN_DIM,       max_dim=config.IMAGE_MAX_DIM,      padding=config.IMAGE_PADDING)    resize_image() got an unexpected keyword argument 'padding'    It really isn't. It may not be needed at all?     "
"Hi,  Maybe that's a stupid question. But I can't find a solution. How can i find out the number of neurons or weights (which should be identical [except for the final number of classes])?    Many thanks "
"When I test the model mask_rcnn_coco.h5, the error is encountered."
"Dear All,         I revised the backbone architecture, so I need to train a model from scratch (without using the  pretrain .h5 file). I just comment out the 'model.load_weight', however, the result is terrible. the new model can not detect anything almostly.       Could anybody give me some suggestion how to fix the problem? is there anything do I need to changed in the codes? many thanks.    King regards    Wei"
"After training on my own data (balloon.py), where are my weights stored so that I can load them into model and apply it to select an object in an my image? Function load_weights I saw, but save_weights not"
"I build the model for Nucleus detection and generated the result for stage1_test and tried to submit the result in Kaggle. I got the following error ""Evaluation Exception: The submitted ids must contain all solution ids."" can yo please check whether the csv generation code works fine"
"I want to extract the ROI-pooled features corresponding to each final detection.    Initially it seems there are 1000 detections, but after the `refine_detections_graph()` method the 1000 detections have been filtered down to just a few (by throwing away low probability detections, and highly overlapping detections with NMS).      I'm able to pull out 1000 ROI-pooled features, but I can't figure out how to filter them and apply the same NMS mask that is applied to the corresponding `rois`, `probs`, `deltas`, and `window` objects.    Does anybody have any insight as to how that `keep` mask can be used on the ROI-pooled features with shape `(batch, 1000, 7, 7, 256)` ?"
"ValueError: Layer roi_align_classifier was called with an input that isn't a symbolic tensor. Received type:  . Full input: [[],  ,  ,  ,  ,  ]. All inputs to the layer should be tensors.    thanks for your help"
What is the difference between det_mask_specific  and det_masks? 
I have trained my own data and detect images. I want to get the location of pixels of each mask and its label. How to do that?
"I have trained provided model on Balloon dataset (pretrained weights of ImageNet) as well as on COCO dataset (pretrained weights of ImageNet), with provided configurations and training stages. I am observing **high variance** and **high magnitude** of validation loss for RPN bounding box, while the training loss of RPN bounding box is stable and slowly decreases. Is it a normal behaviour? It strongly influence tha validation loss, which as consequence varies a lot also. Did anybody got the same results after training provided Balloon model?    Balloon Training:  !     !     !     !     Thank you in advance for any tips!    Regards,  Lukasz  "
I need to perform testing after training on my own dataset to determine the performance. Are there methods on how to do this testing as part of evaluation and to get the precision and recall values?
"I would like to extract the features from the final layer to feed into another network, so would it be best to take gt_mask, det_masks (I was looking at inspect_model.ipynb) or any other recommended layer? "
"when i try to train samples/coco/coco.py train, it stopped, and raised an error as follows:   Exception in thread Thread-4:  Traceback (most recent call last):    File ""/home/anaconda3/lib/python3.6/threading.py"", line 916, in _bootstrap_inner      self.run()    File ""/home/anaconda3/lib/python3.6/threading.py"", line 864, in run      self._target(*self._args, **self._kwargs)    File ""/home/anaconda3/lib/python3.6/multiprocessing/pool.py"", line 405, in _handle_workers      pool._maintain_pool()    File ""/home/anaconda3/lib/python3.6/multiprocessing/pool.py"", line 246, in _maintain_pool      self._repopulate_pool()    File ""/home/anaconda3/lib/python3.6/multiprocessing/pool.py"", line 239, in _repopulate_pool      w.start()    File ""/home/anaconda3/lib/python3.6/multiprocessing/process.py"", line 105, in start      self._popen = self._Popen(self)    File ""/home/anaconda3/lib/python3.6/multiprocessing/context.py"", line 277, in _Popen      return Popen(process_obj)    File ""/home/anaconda3/lib/python3.6/multiprocessing/popen_fork.py"", line 19, in __init__      self._launch(process_obj)    File ""/home/anaconda3/lib/python3.6/multiprocessing/popen_fork.py"", line 66, in _launch      self.pid = os.fork()  OSError: [Errno 12] Cannot allocate memory  I don't know what exactly the problem is, can you please help me waiting for your reply! thanks  "
i m trying to run the code on my local terminal which does not have a GPU   it only have  CPU both ballon and coco detection codes are  failing   is it necessary to have GPU to get things done?    
"I use this project to train a new dataset,but mrcnn_class_loss is nan all the time,can anyone tell the possible reason."
"Hey,  I make implementation of the balloon sample with few adjustment on my own dataset,  how can I get the number of instances on the image after prediction was made?  I am really new at this section so please try to provide simplest solution"
"The code runs only on CPU. I guess the error arises due to the this message:  "" Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2"".  Can you kindly explain why?    BTW, I followed all instructions and requirements both in Conda and without. I successfully ran the cuda sample with GPU full usage.    OS: Windows 10  GPU: NVIDIA 1080  Cuda: 9.0  cuDNN: 7.4.2  Python: 3.6"
"For the demo notebook to run, please add    pycocotools    in the requirements.txt. Furthermore, newest version of numpy is missing a method used elsewhere, this can be fixed setting    numpy==1.15.4"
I'm really struggling with how to generate a precision recall curve from the outputs of the utils.compute_ap function. Does anyone have an example of outputting this plot for the precision/recall of multiple images?
"Hi All,     I am recently training the Mask-RCNN on my dataset. The result is pretty good. But there is one issue bother me a lot. The object detection some time always missing one item when it really shouldnt.  !   This prediction missed three leaves. And I also find same issue happened in the model trained with COCO dataset.   !   If anyone faced the same issue, please tell me your thought. Really appreciate that     "
"Hi,  As per your description I'm trying to implement a customize model to detect number strip in credit cards which are available in google.  I collected around 200 images from google and after augmentation I have around 700 images.  I trained model as per your instruction but after training I got only one correct prediction out of ten.  could you please help me how I can increase my model accuracy.    Details are :-   machine = 11 GB RAM GPU  Batch_size = 2  epoch = 100  steps per epoch = 700  Training time = 13 hours  Backend model weights -                                 - mask_rcnn_coco.h5                                - resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5    Please help me to increase my model accuracy or help me to create a customize  model which will help me to understand how a mask RCNN model is working.  "
"Hi everyone!  Firstly, thank you for this repo, it is amazing.  I have 134 classes (for about 2800 photos --- 21 photos / classe). I used VGG image annotator tool to do my annotations (2800 segmentations...)  !     I have quite good results bu I would like to augment my database using the augmentation parameter below (in model.train) if the segmentations will be augmented too ?    !     My segmentations are coordinates in a .JSON file and I would like to do like the Imgaug example below:    !     Except if the augmentation parameter already does what I want..  Waiting for your help.. Thanks !        "
"I want to drop the learning rate at specific times during training. To do so, I implemented a callback in Keras using a LearningRateScheduler. I added the following line in the function train in models.py:          Furthermore, I defined a function to call within the class MaskRCNN() in models.py that takes the epoch number as an argument and returns the learning rate to use in stochastic gradient descent.          However, when I run the code, it seems to be stuck just after epoch 1 (starting from epoch 0). Anybody knows how to implement this?"
"Is there a way to save the model in its entirety i.e. weights and architecture via standard means? Currently the weights of the Mask RCNN are saved at every epoch in the logs directory. Now how can I save the model architecture as well and load it when required?     Usually with keras, you can save a model as an h5 file or have a combination (weights-h5, model-json).  Is that applicable here?  "
"Hi guys,     Any thoughts on the best way to label images that have overlapping objects - partly obscured etc ?     Do we just label the parts that are visible?     Many thanks... "
Hey friends...  How can I save each mask alone with black background around the object segmentation?  I am working on aerial vehicle images and my aim is  to make Fine-Grained Classification  (e.g Small car with Sunroof and his color black).    for example:  !     from this image I want to get each 'car' instance in a separate image.    thank you for your time..  
"Hello everyone,   We are trying to implement Mask RCNN on a different data set. Here, as you can all see, I have provided an image of a satellite taken a picture and the output after passing it through mask RCNN is horrible, it can not detect any kind of object rather it is detecting a portion of the image as a road. What should I possibly do to make it correct or is there any known limitation on this sector of Mask RCNN.  Kindly let me know.   Thanks in advance.  !     **I have used this Configurations:**  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     3  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.7  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 3  IMAGE_MAX_DIM                  320  IMAGE_META_SIZE                14  IMAGE_MIN_DIM                  320  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [320 320   3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           road  NUM_CLASSES                    2  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                10  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               1  WEIGHT_DECAY                   0.0001  "
"I'm getting MemoryError trying to train the nucleus sample with 16GB of RAM:     However, it uses less RAM with  IMAGE_MIN_DIM = 256  IMAGE_MAX_DIM = 256  IMAGE_MIN_SCALE = 1.0  Is that expected and I just need more RAM?"
"In the mrcnn_mask_loss_graph function, the y_pred and y_true are used to calculate the binary cross-entropy loss. I am curious how these are obtained. The pred_masks input has for each proposal a 3d matrix (h,w, n_classes) resulting from a sigmoid layer in build_fpn_mask_graph (shouldn't you use softmax for more than 2 classes?).     y_pred is then obtained by gathering the masks with the target_class_ids. So the assumption is made that the class  (/relevant sigmoid output slice) from the i'th proposal is in fact the ground truth class of the i'th target_mask. Does this mean the pred_masks and the target_masks are aligned somehow? How can you, for instance, assume that the first target_mask corresponds to the first proposal? I.o.w. that if the first target_mask has class one, that the first proposal then also must be class one. I need this information for developing   (which I intend to share if it works), where I need to match proposals and targets by means of max IOU.     I probably misunderstand what is happening, but any help would be greatly appreciated."
"i use labelme tool  to  label my dataset, only one class, but ,i  want to train maskrcnn, save raw multi classes, for example person,dog,   ,in the result ,the model can recognize all classes ,and my class else, how can i do?"
None
"Hi there,    I'm currently trying to understand the mask rcnn in depth and the paper doesn't give an answer to question I have regarding the mask prediction. So I understand that Mask RCNN predicts a mask for each class per bounding box and that the mask predictions are done via FCN. Did I get that right that Mask RCNN has one FCN for each class then since the masks are predicted independently?"
I m currently working on HumanParsing ATR Dataset.  Link to the dataset -    Here is the link to the author's repo -      Dataset only contains the Images and the masked Images. I m having difficulty in figuring out as to how I can extract class ids from the mask images as there is no information provided.   I m don't know what changes to be made to load_mask function to extract the class ids.  The author's repo provides the following information about the class labels.  -   background     0  hat            1  hair           2   sunglass       3  upper-clothes  4  skirt          5  pants          6  dress          7  belt           8  left-shoe      9  right-shoe     10  face           11  left-leg       12  right-leg      13  left-arm       14  right-arm      15  bag            16  scarf          17    And this is a small screenshot of their paper -        I think the class labels are coloured in the mask images. How do I extract them?  Thanks in advance    Regards    
"Since I have trained the model and validate it many times, now I need to read the model and detect the object in images I want in C++.    First, I found that it could read Mask_RCNN model by  ,  but it needed weights in pb and model in pbtxt, so I  write something like        then I turned the pb file to pbtxt     then there was an error like   [libprotobuf ERROR C:\build\3_4_winpack-build-win64-vc14\opencv\3rdparty\protobu  f\src\google\protobuf\text_format.cc:288] Error parsing text-format opencv_tenso  rflow.GraphDef: 41208:5: Unknown enumeration value of ""DT_RESOURCE"" for field ""t  ype"".  OpenCV(3.4.4) Error: Unspecified error (FAILED: ReadProtoFromTextFile(param_file  , param). Failed to parse GraphDef file: D:/model_1.pbtxt) in cv::dnn::ReadTFNet  ParamsFromTextFileOrDie, file C:\build\3_4_winpack-build-win64-vc14\opencv\modul  es\dnn\src\tensorflow\tf_io.cpp, line 54    then I found the models are probably different,   so is there anyone can give me some guides to  do  inference mode in C++?"
"/home/w/anaconda3/lib/python3.6/site-packages/tensorflow-1.12.0-py3.6-linux-x86_64.egg/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  /home/w/anaconda3/lib/python3.6/site-packages/Keras-2.2.4-py3.6.egg/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.    UserWarning('Using a generator with `use_multiprocessing=True`'  Epoch 1/10  image_id 54  ERROR:root:Error processing image {'id': 54, 'source': 'shapes', 'path': 'train_data/pic/25.png', 'width': 1920, 'height': 1080, 'mask_path': 'train_data/cv2_mask/25.png', 'yaml_path': 'train_data/labelme_json/25_json/info.yaml'}  Traceback (most recent call last):    File ""/home/w/Mask_RCNN/mrcnn/model.py"", line 1710, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/w/Mask_RCNN/mrcnn/model.py"", line 1213, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train.py"", line 152, in load_mask      occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)  IndexError: index -1 is out of bounds for axis 2 with size 0  image_id 22  ERROR:root:Error processing image {'id': 22, 'source': 'shapes', 'path': 'train_data/pic/75.png', 'width': 1920, 'height': 1080, 'mask_path': 'train_data/cv2_mask/75.png', 'yaml_path': 'train_data/labelme_json/75_json/info.yaml'}  Traceback (most recent call last):    File ""/home/w/Mask_RCNN/mrcnn/model.py"", line 1710, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/w/Mask_RCNN/mrcnn/model.py"", line 1213, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train.py"", line 152, in load_mask      occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)  IndexError: index -1 is out of bounds for axis 2 with size 0  image_id 54  ERROR:root:Error processing image {'id': 54, 'source': 'shapes', 'path': 'train_data/pic/25.png', 'width': 1920, 'height': 1080, 'mask_path': 'train_data/cv2_mask/25.png', 'yaml_path': 'train_data/labelme_json/25_json/info.yaml'}  Traceback (most recent call last):    File ""/home/w/Mask_RCNN/mrcnn/model.py"", line 1710, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/w/Mask_RCNN/mrcnn/model.py"", line 1213, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train.py"", line 152, in load_mask      occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)  IndexError: index -1 is out of bounds for axis 2 with size 0image_id 54  ERROR:root:Error processing image {'id': 54, 'source': 'shapes', 'path': 'train_data/pic/25.png', 'width': 1920, 'height': 1080, 'mask_path': 'train_data/cv2_mask/25.png', 'yaml_path': 'train_data/labelme_json/25_json/info.yaml'}  Traceback (most recent call last):    File ""/home/w/Mask_RCNN/mrcnn/model.py"", line 1710, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/w/Mask_RCNN/mrcnn/model.py"", line 1213, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train.py"", line 152, in load_mask      occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)  IndexError: index -1 is out of bounds for axis 2 with size 0image_id 5    "
     I'm wondering whether integration of Mars supplanting NumPy would create any performance gains?
Can the input to ResNet be an arbitrary feature map?  I have managed to extend Mask R-CNN by adding using 2 CNNs to get feature maps from 2 images and then combine them.  Can this act as input to ResNet?  I am currently getting the error below and I don't know how to continue on this.    tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes:  ]]    Given the name this occurs in the fpn_p4add in the part of the build() method where the resnet feature maps (C1-C5) are upsampled and added together.  Any help would be appreciated.    Thank you.
"  for i, (shape, _, dims) in enumerate(info['shapes']):              mask[:, :, i:i + 1] = self.draw_shape(mask[:, :, i:i + 1].copy(),                                                    shape, dims, 1)          # Handle occlusions          occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)          for i in range(count - 2, -1, -1):              mask[:, :, i] = mask[:, :, i] * occlusion              occlusion = np.logical_and(                  occlusion, np.logical_not(mask[:, :, i]))    What is the meaning of mask[:, :, i:i + 1] ?And what the handle occlusions is meaning?  "
"I only have mask images and original images.  And I could train the model using my own dataset successfully, but only for one class (one category).I need to train for more class.So please help."
"When I run train_shapes.ipynb I get the following error, and have no idea how to deal with it. Can anyone help? Many thanks!      ---------------------------------------------------------------------------  StopIteration                             Traceback (most recent call last)    in          6             learning_rate=config.LEARNING_RATE,        7             epochs=1,  ----> 8             layers='heads')    d:\program files\anaconda3\envs\maskrcnn\lib\site-packages\mask_rcnn-2.1-py3.5.egg\mrcnn\model.py in train(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)     2373             max_queue_size=100,     2374             workers=workers,  -> 2375             use_multiprocessing=True,     2376         )     2377         self.epoch = max(self.epoch, epochs)    d:\program files\anaconda3\envs\maskrcnn\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)       85                 warnings.warn('Update your `' + object_name +       86                               '` call to the Keras 2 API: ' + signature, stacklevel=2)  ---> 87             return func(*args, **kwargs)       88         wrapper._original_function = func       89         return wrapper    d:\program files\anaconda3\envs\maskrcnn\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)     2063                 batch_index = 0     2064                 while steps_done   2065                     generator_output = next(output_generator)     2066      2067                     if not hasattr(generator_output, '__len__'):    d:\program files\anaconda3\envs\maskrcnn\lib\site-packages\keras\utils\data_utils.py in get(self)      708                 all_finished = all([not thread.is_alive() for thread in self._threads])      709                 if all_finished and self.queue.empty():  --> 710                     raise StopIteration()      711                 else:      712                     time.sleep(self.wait_time)    StopIteration:   "
"IndexError: boolean index did not match indexed array along dimension 0; dimension is 0 but corresponding boolean dimension is 1  Traceback (most recent call last):      File "" "", line 1, in        runfile('E:/Wex/TensorFlow/DeepLearning/Mask_RCNN/samples/shapes/train_shapes.py', wdir='E:/Wex/TensorFlow/DeepLearning/Mask_RCNN/samples/shapes')      File ""D:\Anaconda3\envs\python3.5\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 866, in runfile      execfile(filename, namespace)      File ""D:\Anaconda3\envs\python3.5\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile      exec(compile(f.read(), filename, 'exec'), namespace)      File ""E:/Wex/TensorFlow/DeepLearning/Mask_RCNN/samples/shapes/train_shapes.py"", line 279, in        layers='heads')      File ""E:\Wex\TensorFlow\DeepLearning\Mask_RCNN\mrcnn\model.py"", line 2363, in train      use_multiprocessing=True,      File ""D:\Anaconda3\envs\python3.5\lib\site-packages\keras\legacy\interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)      File ""D:\Anaconda3\envs\python3.5\lib\site-packages\keras\engine\training.py"", line 2194, in fit_generator      generator_output = next(output_generator)      File ""E:\Wex\TensorFlow\DeepLearning\Mask_RCNN\mrcnn\model.py"", line 1707, in data_generator      use_mini_mask=config.USE_MINI_MASK)      File ""E:\Wex\TensorFlow\DeepLearning\Mask_RCNN\mrcnn\model.py"", line 1275, in load_image_gt      class_ids = class_ids[_idx]    IndexError: boolean index did not match indexed array along dimension 0; dimension is 0 but corresponding boolean dimension is 1  Does anyone know how to solve this ?  "
I am able to train the model successfully but my problem is my dataset is quite unbalanced and has 30+ classes out of which 1 class is quite dominating and some classes are quite rare.I want to know whether unbalanced dataset will affect model accuracy and predictions.If yes except undersampling or upsampling is there any techniques I can do. Is there a way to change loss function which will penalise classes as per their distribution
Why did the second stage finish but the third stage quit without any errors?
"Hi all,  I have changed a little bit the possible inputs as mask, to be possible to enter polygon (it is already possible), rectangle, ellipses, and polylines. I have encountered that the network can be trained with everything, but with polylines only shows me the bbox and the mask is every pixel false.     I am thinking that, maybe, the network learns that the best way is not to learn anything and leave the mask empty, could it be possible?     Thanks"
"!   When I validate images, the corner ( Upper left corner) is always detected as an object.  How can I solve it?  Any reply is appreciated."
"I work with GeForce GTX 980 , 4GB,  tf-gpu 1.7 , keras 2.2.4.    I took `train_shapes.ipynb` from `samples` folder. in `Config` I set:  `GPU_COUNT = 1,      IMAGES_PER_GPU = 1,  STEPS_PER_EPOCH = 10,      VALIDATION_STEPS = 3  `  I created `dataset_train` and `dataset_val` with 1 loaded shape as well.  **and then, I added a print inside while loop of `data_generator`. and each time an image is generated I saved to a folder**.  ***I ended with 204 prints and same number of generated images. I expected only 13 images to be created***       I tried to debug this -> it seems like arround ~20 threads are created and each one of them generates a picture.  ****can somebody explain this phenomena?****  (because once I try to work with 512\*512 or 1024\*1024 images -> my GPU is crushing on memory exhaustion)"
"I'm trying to change the network to accept grayscale images and have followed the wiki steps to exclude conv1 when loading weights and include it in addition to heads when training. I am running into a shape mismatch when trying to train.           Was there some other modifications I needed to perform to fix this? I've followed all the steps listed on the wiki as follows:    > 1. In your subclass of Config, set IMAGE_CHANNEL_COUNT to N.  > 2. In the same class, change MEAN_PIXEL from 3 values to N values.  > 3. The load_image() method in the Dataset class is designed for RGB. It converts Grayscale images to RGB and removes the 4th channel if present (because typically it's an alpha channel). You'll need to override this method to handle your images.  > 4. Since you're changing the shape of the input, the shape of the first Conv layer (Conv1) will change as well. So you can't use the provided pre-trained weights. To get around that, use the exclude parameter when you load the weights to exclude the first layer. This allows you to load weights of all layers except conv1, which will be initialized to random weights.  > 5. If you train a subset of layers, remember to include conv1 since it's initialized to random weights. This is relevant if you pass layers=""head"" or layers=""4+"", ...etc. when you call train().    My relevant setup code snippets are as follows:         I also modified model.py to include conv1 in ""heads"":         My images are 12-bit, so I will try changing them and seeing if that helps, but this shape mismatch thing with the input layer has me stumped. Did anyone else run into this issue when using grayscale images?"
"I have use the MaskRCNN to segment the object for robot grasping. But when I just use RGB image, the performance is not good enough since the scene is cluster. I have found some reference in wiki. If you have do that, please give me some advice. Thanks a lot"
"Hello! At the moment, my dataset is composed by approximately 200 images and 400 labels for each class (4 classes).  I tried different data augmentation pipelines (fliplr, flipud, gaussian blur, affine transformation, contrast normalization in particular), to reduce backbone architecture from Res101 to Res50 (without obtaining an improvement), to increase weight_decay (my best result was obtained with weight_decay=0.05) and trying to decrease learning rate every n steps.  Could you suggest me some ways to reduce overfitting (expecially  val_mrcnn_class_loss and  val_mrcnn_mask_loss)?  "
"   GPU_COUNT=1     capture = cv2.VideoCapture(0)      capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)      capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)      while True:          ret, frame = capture.read()          results = model.detect([frame], verbose=0)          r = results[0]          frame = display_instances(frame, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'] )          cv2.imshow('frame', frame)          if cv2.waitKey(1) & 0xFF == ord('q'):              break      capture.release()      cv2.destroyAllWindows()      AND      GPU_COUNT=4     capture = cv2.VideoCapture(0)      capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)      capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)      while True:          image=[]              ret, frame = capture.read()              image.append(frame)          results = model.detect(image, verbose=0)          r = results[0]          for j in range(4)              frame = display_instances(iamge[j], r['rois'], r['masks'], r['class_ids'], class_names, r['scores'] )              cv2.imshow('frame', frame)          if cv2.waitKey(1) & 0xFF == ord('q'):              break      capture.release()    Their test speed is the same.    How to test on Multi GPUs?  "
"Hi everyone, I have a question regarding to the operations after enabling USE_MINI_MASK:  In `detection_targets_graph`,        I think both ROI coordinates and gt_box coordinates are normalized (pixel/image_shape). The only difference is that gt_ is the ground truth and ROI is the prediction. So let's say our prediction is precise, then x1 approximate equals to gt_x1 then the denominator here is almost zero?    I don't understand why they calculate y1 x1 y2 x2 like this. Can someone explain this to me? Thanks a lot!    "
I am considering on training with ApolloScape car instance dataset. But as i was browsing through the issues i noticed that Mask R-CNN is not compatible with Cityscapes due to the fact that Mask R-CNN includes instance segmentation and not semantic segmentation.   I was wondering if the same happens with ApolloScape dataset.
"Hello everyone.     I have a use case to train model on small images with shape (80, 140).   Objects, that i want to detect on these images are even more small, it's a characters.     Already implemented a lot of steps data augmentation like blur, rotation, flip and so on.  This is my config params i've redefined         IMAGES_PER_GPU = 2        NUM_CLASSES = 1 + 33        STEPS_PER_EPOCH = 100        DETECTION_MIN_CONFIDENCE = 0.9        RPN_ANCHOR_SCALES = (16, 32, 48, 64, 88)        RPN_ANCHOR_RATIOS = [0.2, 0.5, 1]        IMAGE_MIN_DIM = int(128)      IMAGE_MAX_DIM = int(320)    Other params are default.    The model can detect characters, for now not very good, but there should be space for improvements.     So my question is, what is the best config will be for the FPN, RPN for the small images and small objects detection? I think need to adjust these or maybe other hyper params.    Can you share any best practices for the Mask R-CNN config for such use case?   "
"I want to know, if I do image target location detection, then my training data must be labeled one by one? For example, I want to detect the people and cars in the image, so when I train, do I have to label the people and cars in the training data?"
"I trained a custom dataset with 39 images for train and 5 images for val. The system I use is a windows 10 OS with 16 GB Memory with Nvidia Geforce 1GB GPU. When I run my detection file, the output is displayed thus:   !     but the required mask needs to be thus:   !     Can anyone specify as to where the error or issue occurs? The loss value is well reduced and has a value of 0.035.     "
"I trained my own dataset with                    steps=500,epochs=20,layers='heads'  and epochs=30, layers='all'.  When I validate my dataset, the results are very poor and the mAPs  are always 0.  What should I improve it?  Thanks for the replies."
"My computer has 4 1080 Ti GPU. So in demo.ipynb, is there any way that can make all of them to do single image inference simultaneously to speed up inference time drastically? Thanks."
"If I do the following: mrcnn = model.run_graph([image], [      (""detections"", model.keras_model.get_layer(""mrcnn_detection"").output),  ])  What will the input be for the ""mrcnn_detection"" layer? Will it run the entire graph until this layer?"
When I run inspect_data and in the configurations choose  `config=shapes.ShapesConfig()`  I get an error saying   module 'shapes' has no attribute 'ShapesConfig'  I am importing shapes and there is no error related to that. I also checked that attribute is defined for the class shapes.  I am running this on AWS EC2 instance for deep learning.
"When I was running the project which is based on Mask R-CNN, I am stuck with this problem. The configuration is cuda=9.0 cudnn=7.1.4 tensorflow-gpu=1.9.0 and keras-gpu=2.2.4 However, when this program is running in Spyder of Anaconda3 in Windows10. It has encountered with above problem which is described more closely as this:     I can ensure that I have installed any other things like pycoco and vs2015 building tools.  Could anybody answer my question.  It is my work to use Mask R-CNN!  SOS!  "
"When I ran     I got the error  `ValueError: Layer #344 (named ""fpn_c5p5""), weight   has shape (1, 1, 2048, 64), but the saved weight has shape (256, 2048, 1, 1).`  "
I need to receive the feature maps of every layer of mask RCNN to be used as a deep feature for other classifier. How do I go about it? Any help or code is much appreciated!
None
"I just change ResNet101 to Xception  codes and weight are from here         _,C2,C3,_,C4,C5 = xception(                  weights='pascal_voc',                  inputs=input_image,                   input_shape=(512, 1024, 3),                   OS=16, alpha=1.)  """"""  shape   #(None, 256, 512, 64)  (None, 128, 256, 128)  (None, 64, 128, 256)  #(None, 32, 64, 728)  (None, 32, 64, 1024)  (None, 32, 64, 2048)   """"""  and I already modify the corresponding part of fpn    P4 = KL.Add(name=""fpn_p4add"")([          KL.UpSampling2D(size=(2, 2), name=""fpn_p5upsampled"")(P5) if config.BACKBONE in [""resnet50"", ""resnet101""] else P5,          KL.Conv2D(256, (1, 1), name='fpn_c4p4')(C4)      ])    Although model.summary() successfully print out the graph of network  I still got these error message    Error message    `Traceback (most recent call last):    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py"", line 654, in _call_cpp_shape_fn_impl      input_tensors_as_shapes, status)    File ""/usr/lib/python3.5/contextlib.py"", line 66, in __exit__      next(self.gen)    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status      pywrap_tensorflow.TF_GetCode(status))  tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimension 0 in both shapes must be equal, but are 1 and 256 for 'Assign_4' (op: 'Assign') with input shapes: [1,1,256,256], [256,512,1,1].    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""cityscape_mix.py"", line 418, in        ""mrcnn_mask""    File ""/home/world4jason/TowardAutoDriving/mrcnn/model_group_seg_only.py"", line 3481, in load_weights      topology.load_weights_from_hdf5_group_by_name(f, layers)    File ""/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py"", line 3158, in load_weights_from_hdf5_group_by_name      K.batch_set_value(weight_value_tuples)    File ""/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py"", line 2188, in batch_set_value      assign_op = x.assign(assign_placeholder)    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py"", line 527, in assign      return state_ops.assign(self._variable, value, use_locking=use_locking)    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py"", line 274, in assign      validate_shape=validate_shape)    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_state_ops.py"", line 43, in assign      use_locking=use_locking, name=name)    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op      op_def=op_def)    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 2632, in create_op      set_shapes_for_outputs(ret)    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1911, in set_shapes_for_outputs      shapes = shape_func(op)    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1861, in call_with_requiring      return call_cpp_shape_fn(op, require_shape_fn=True)    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py"", line 595, in call_cpp_shape_fn      require_shape_fn)    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py"", line 659, in _call_cpp_shape_fn_impl      raise ValueError(err.message)  ValueError: Dimension 0 in both shapes must be equal, but are 1 and 256 for 'Assign_4' (op: 'Assign') with input shapes: [1,1,256,256], [256,512,1,1].`      and the weird things is that sometimes I can trained, but this kind of error msg pop up while I am trying to do the inference  why?"
"when trying to add a class. I'm getting the following error and I have attached the code and json from VGG annotator version-1 below for reference       **Error:**  Epoch 1/30  ERROR:root:Error processing image {'id': '2.jpeg', 'source': 'object', 'path': '/root/Code/test/Image_segmented/Dataset/val/2.jpeg', 'width': 225, 'height': 225, 'polygons': [{'name': 'polygon', 'all_points_x': [47, 85, 93, 97, 102, 111, 112, 112, 112, 113, 198, 218, 219, 207, 110, 110, 101, 93, 85, 51, 32, 33, 15, 14, 35, 42, 47], 'all_points_y': [69, 70, 75, 82, 86, 86, 89, 92, 93, 94, 95, 113, 114, 129, 127, 134, 135, 143, 150, 151, 147, 146, 110, 106, 74, 70, 69]}], 'num_ids': []}  Traceback (most recent call last):    File ""/root/Code/test/mrcnn/model.py"", line 1710, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/root/Code/test/mrcnn/model.py"", line 1266, in load_image_gt      class_ids = class_ids[_idx]  IndexError: boolean index did not match indexed array along dimension 0; dimension is 0 but corresponding boolean dimension is 1     99/100 [============================>.] - ETA: 2s - loss: 0.9285 - rpn_class_loss: 0.0016 - rpn_bbox_loss: 0.1541 - mrcnn_class_loss: 0.0748 - mrcnn_bbox_loss: 0.5152 - mrcnn_mask_loss: 0.1827multiprocessing.pool.RemoteTraceback:   """"""  Traceback (most recent call last):    File ""/usr/lib/python3.6/multiprocessing/pool.py"", line 119, in worker      result = (True, func(*args, **kwds))    File ""/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py"", line 626, in next_sample      return six.next(_SHARED_SEQUENCES[uid])    File ""/root/Code/test/mrcnn/model.py"", line 1710, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/root/Code/test/mrcnn/model.py"", line 1266, in load_image_gt      class_ids = class_ids[_idx]  IndexError: boolean index did not match indexed array along dimension 0; dimension is 0 but corresponding boolean dimension is 1  """"""    The above exception was the direct cause of the following exception:    Traceback (most recent call last):    File ""balloon.py"", line 381, in        train(model)    File ""balloon.py"", line 216, in train      layers='heads')    File ""/root/Code/test/mrcnn/model.py"", line 2375, in train      use_multiprocessing=True,    File ""/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 1418, in fit_generator      initial_epoch=initial_epoch)    File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py"", line 234, in fit_generator      workers=0)    File ""/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 1472, in evaluate_generator      verbose=verbose)    File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py"", line 330, in evaluate_generator      generator_output = next(output_generator)    File ""/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py"", line 709, in get      six.reraise(*sys.exc_info())    File ""/usr/local/lib/python3.6/dist-packages/six.py"", line 693, in reraise      raise value    File ""/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py"", line 685, in get      inputs = self.queue.get(block=True).get()    File ""/usr/lib/python3.6/multiprocessing/pool.py"", line 670, in get      raise self._value  IndexError: boolean index did not match indexed array along dimension 0; dimension is 0 but corresponding boolean dimension is 1.        **code:**    ############################################################  #  Configurations  ############################################################      class objectConfig(Config):   """"""Configuration for training on the toy  dataset.   Derives from the base Config class and overrides some values.   """"""   # Give the configuration a recognizable name   NAME = ""object""     # We use a GPU with 12GB memory, which can fit two images.   # Adjust down if you use a smaller GPU.   IMAGES_PER_GPU = 2     # Number of classes (including background)   NUM_CLASSES = 1 + 2  # Background + balloon     # Number of training steps per epoch   STEPS_PER_EPOCH = 100     # Skip detections with < 90% confidence   DETECTION_MIN_CONFIDENCE = 0.9      ############################################################  #  Dataset  ############################################################    class objectDataset(utils.Dataset):     def load_object(self, dataset_dir, subset):    """"""Load a subset of the Balloon dataset.    dataset_dir: Root directory of the dataset.    subset: Subset to load: train or val    """"""    # Add classes. We have only one class to add.    self.add_class(""object"", 1, ""Key"")    self.add_class(""object"", 2, ""Image"")      # Train or validation dataset?    assert subset in [""train"", ""val""]    dataset_dir = os.path.join(dataset_dir, subset)      # Load annotations    # VGG Image Annotator (up to version 1.6) saves each image in the form:    # { 'filename': '28503151_5b5b7ec140_b.jpg',    #   'regions': {    #       '0': {    #           'region_attributes': {},    #           'shape_attributes': {    #               'all_points_x': [...],    #               'all_points_y': [...],    #               'name': 'polygon'}},    #       ... more regions ...    #   },    #   'size': 100202    # }    # We mostly care about the x and y coordinates of each region    # Note: In VIA 2.0, regions was changed from a dict to a list.    annotations = json.load(open(os.path.join(dataset_dir, ""via_region_data.json"")))    annotations = list(annotations.values())  # don't need the dict keys      # The VIA tool saves images in the JSON even if they don't have any    # annotations. Skip unannotated images.    annotations = [a for a in annotations if a['regions']]      # Add images    for a in annotations:     # Get the x, y coordinaets of points of the polygons that make up     # the outline of each object instance. These are stores in the     # shape_attributes (see json format above)     # The if condition is needed to support VIA versions 1.x and 2.x.     polygons = [r['shape_attributes'] for r in a['regions'].values()]     objects = [s['region_attributes'] for s in a['regions'].values()]       #num_ids=[int(n['object']) for n in objects]     num_ids = []     for n in objects:      try:       if 'Key' in n.keys():        num_ids.append(1)         elif 'Image' in n.keys():        num_ids.append(2)        except:       pass         # load_mask() needs the image size to convert polygons to masks.     # Unfortunately, VIA doesn't include it in JSON, so we must read     # the image. This is only managable since the dataset is tiny.     image_path = os.path.join(dataset_dir, a['filename'])     image = skimage.io.imread(image_path)     height, width = image.shape[:2]       self.add_image(      ""object"",      image_id=a['filename'],  # use file name as a unique image id      path=image_path,      width=width, height=height,      polygons=polygons,      num_ids=num_ids)     def load_mask(self, image_id):    """"""Generate instance masks for an image.      Returns:    masks: A bool array of shape [height, width, instance count] with     one mask per instance.    class_ids: a 1D array of class IDs of the instance masks.    """"""    # If not a balloon dataset image, delegate to parent class.    info = self.image_info[image_id]    if info[""source""] != ""object"":     return super(self.__class__, self).load_mask(image_id)    num_ids = info['num_ids']      # Conver polygons to a bitmap mask of shape    # [height, width, instance_count]    #info = self.image_info[image_id]    mask = np.zeros([info[""height""], info[""width""], len(info[""polygons""])],        dtype=np.uint8)    for i, p in enumerate(info[""polygons""]):     # Get indexes of pixels inside the polygon and set them to 1     rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])     mask[rr, cc, i] = 1    print(info['num_ids'])    #num_ids = np.array(num_ids, dtype=np.int32)    # Return mask, and array of class IDs of each Since we have    # one class ID only, we return an array of 1s    #return mask.astype(np.bool), np.ones([mask.shape[-1]], dtype=np.int32)    return mask,num_ids     def image_reference(self, image_id):    """"""Return the path of the image.""""""    info = self.image_info[image_id]    if info[""source""] == ""object"":     return info[""path""]    else:  super(self.__class__, self).image_reference(image_id)          *********************json snippet of Class Image and Key***********************    19.jpeg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[46,57,60,73,76,87,102,118,122,123,125,129,134,150,156,164,188,189,208,223,237,246,264,266,265,220,217,215,213,208,202,190,176,170,165,162,146,141,116,112,105,90,80,74,74,57,48,40,36,38,37,42,44,46],""all_points_y"":[62,55,52,40,27,24,20,18,18,15,15,17,15,18,12,10,15,17,22,26,38,44,60,64,124,129,125,126,140,153,161,164,163,161,151,137,136,134,133,138,142,144,140,131,126,123,124,121,110,109,90,84,78,62]},""region_attributes"":{""Image"":""1""}}}},""20.jpeg6341"":{""fileref"":"""",""size"":6341,""filename"":""20.jpeg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[54,69,81,92,116,146,176,187,195,217,225,230,235,239,239,241,240,238,233,231,230,227,220,211,203,197,195,190,186,171,160,159,144,136,124,126,125,123,114,101,98,95,94,94,73,51,48,45,34,27,22,20,21,23,25,41,45,41,41,43,47,52,54],""all_points_y"":[78,63,57,54,52,51,55,59,63,79,82,87,89,97,104,114,132,140,143,142,149,155,158,158,157,152,144,144,146,147,147,145,145,147,146,146,152,159,163,161,156,145,144,143,139,139,143,145,145,144,136,129,117,109,95,85,84,83,81,77,74,74,78]},""region_attributes"":{""Image"":""1""}}}},""21.jpeg4667"":{""fileref"":"""",""size"":4667,""filename"":""21.jpeg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[50,50,185,206,213,235,256,273,282,284,295,308,318,323,320,308,292,282,281,263,231,216,202,187,179,178,172,166,163,164,142,133,126,118,112,102,92,84,77,70,63,52,38,29,20,3,18,50],""all_points_y"":[52,52,51,36,16,3,2,11,28,41,42,46,55,68,91,105,112,111,129,148,149,137,116,104,106,111,114,115,106,102,100,95,95,96,90,90,97,93,93,99,95,93,84,84,90,67,53,52]},""region_attributes"":{""Key"":""1""}}}},""22.jpeg4603"":{""fileref"":"""",""size"":4603,""filename"":""22.jpeg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[25,192,205,232,292,301,300,294,281,238,209,191,163,158,16,14,0,14,25],""all_points_y"":[77,77,46,39,41,59,122,144,149,153,152,120,121,118,120,107,86,89,77]},""region_attributes"":{""Key"":""1""}}}},""26.jpeg3282"":{""fileref"":"""",""size"":3282,""filename"":""26.jpeg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[58,45,42,40,42,49,58,88,98,100,100,98,90,81,81,83,81,79,71,63,61,58],""all_points_y"":[81,73,62,48,35,25,24,24,32,43,54,66,78,82,87,90,95,154,168,156,83,81]},""region_attributes"":{""Key"":""1""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[171,158,150,149,152,156,165,181,194,202,208,208,203,199,192,187,188,183,179,172,171],""all_points_y"":[83,78,64,46,36,28,24,25,25,28,37,54,70,75,80,82,147,157,157,147,83]},""region_attributes"":{""Key"":""2""}}}},""29.jpeg5023"":{""fileref"":"""",""size"":5023,""filename"":""29.jpeg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[22,36,46,76,94,99,99,87,73,73,76,76,70,71,68,67,65,65,68,68,70,70,60,54,47,48,44,46,32,22,20,22],""all_points_y"":[51,33,28,29,44,54,84,100,107,114,116,125,126,141,143,149,152,155,157,162,167,183,197,196,189,125,123,109,101,89,77,51]},""region_attributes"":{""Key"":""1""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[138,151,179,186,195,203,207,206,200,186,181,181,184,181,178,178,176,171,166,156,156,158,159,160,161,158,157,155,156,148,150,143,133,127,128,138],""all_points_y"":[41,29,29,31,38,48,56,84,96,107,109,118,123,127,127,189,195,197,197,186,164,162,156,155,151,149,143,140,128,122,109,102,93,82,54,41]},""region_attributes"":{""Key"":""2""}}}}    _Originally posted by @amullapu in  "
"When I call the model with multiple graphics cards ,the program prompts: Input to reshape is a tensor with 1200 values,but the requested shape has 2400.Why is that?"
