"I need to get the binary values of each mask. My goal is to segment the image, such that I can display each mask in the picture one by one.     For example:  Image 1 = mask 1 only  Image 2 = mask 2 only..and so on..    Also, I need to save these images to my disk. How can I implement this using matterport's code?  Thank you very much. "
"I have train on my own dataset, which are small pest. After training, when I detected, the prediction result of the same picture is different every time.  How should I solve the problem?  "
"I am using google cloud instance to train the model. and I am getting error while training.  jet/var/python/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlic  es to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  /jet/var/python/lib/python3.6/site-packages/keras/engine/training_generator.py:46: UserWarning: Using a generator with `use_multipr  ocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.    UserWarning('Using a generator with `use_multiprocessing=True`'  Epoch 1/30  Traceback (most recent call last):    File ""/jet/var/python/lib/python3.6/site-packages/keras/utils/data_utils.py"", line 678, in _data_generator_task      self.queue.put((True, generator_output))    File "" "", line 2, in put    File ""/jet/var/python/lib/python3.6/multiprocessing/managers.py"", line 757, in _callmethod      kind, result = conn.recv()    File ""/jet/var/python/lib/python3.6/multiprocessing/connection.py"", line 250, in recv      buf = self._recv_bytes()    File ""/jet/var/python/lib/python3.6/multiprocessing/connection.py"", line 407, in _recv_bytes      buf = self._recv(4)    File ""/jet/var/python/lib/python3.6/multiprocessing/connection.py"", line 383, in _recv      raise EOFError  EOFError"
"Hi, I have a question that how I can retrain the model without pretrained weight? And how many images should I prepare with resnet-101?"
"Hi All,    I can train a model on one GPU with my own datasets, I want to train a model on two GPUs, how to do that?     I just change GPU_COUNT = 2 in 'mrcnn--> config.py' file is that correct?  Are there other places I need to change?    Kind regards    Wei"
"I have a .h5 file of scratched-trained weights from a previous use case that I would like to leverage within the Mask RCNN framework. They were created using the ResNet50 Architecture as downloaded from Keras, so ought to fit into the ResNet50 Backbone. Indeed, the model compiles and looks like it is fine-tuning, but the rpn_bbox_loss remains ""nan"" and tensorboard confirms no real training is happening. I have tried greatly reducing the learning rate to encourage 'warm up', but this does not remedy the problem.     Even having popped the classification dense layer from my scratch-trained weights, they do seem subtly different from the Keras 'no-top' download within model.py . Is it possible to define a custom backbone architecture that fits my weights? Are there really different architectures within ResNet50?     Many thanks for any insight. "
"Hey have anyoune tried to use google colaboratory's GPU to train your model with, if ur computer does not have enough GPU memory.   If anyone has done it please let me know how 👍 "
"I am trying to show the mAP by using utils.compute_ap() function. I am trying to understand what are the graphs that are shown with TensorBoard: rpn_class_loss, rpn_bbox_loss, class_loss, bbox_loss, mask_loss.     I want to show the mAP after each epoch as the previous losses. However, I am stuck:     In models.py:          I am not sure about what is pred_scores. I believe is detections[...,5] (See Below)         However, mrcnn_mask, mrcnn_bbox, etc. are not drawn from Detection DetectionLayer() but from build_fpn_mask_graph(). I am not sure if is possible to show the mAP during training... Any ideas?"
"Hi all,     The mask rcnn model trained on my own dataset always produces results, in which the predicted bounding box is always remarkably bigger than the corrresponding mask. Anyone meets the same situation? And what is the possible reason?   "
"In coco.py, annotation format is coco annotation format.  In balloon.py, the annotation is achieved by VIA which is different to coco format.  In shapes.py, the annotation is not needed coz there is no dataset.  In nucleus.py, i am not sure if there is an annotation.  I have this dataset :    and i think the closest example i can follow to load my dataset is nucleus.py .   The annotation in the above dataset is in the directory train_labels. So my question is how exactly works the nucleus.py annotation(if there is any), how can i load my dataset and where should i base my code on, and lastly what do the function related to RLE-encoding offer to the solution of nucleus sample?    Any help, tips or insight would be greatly appreciated!  Thanks :)"
"Hi,    Just wanted to understand the impact of resolution of images to the mask rcnn model.     1. What if I trained the model on lower resolution (300x300) image and do the inference on 1000x1000 image as test image ?    2. What if I use the weight from the trained network (300x300) images and retrain the network using these weights on 1000x1000 images ? Should I have to resize the images or I can train the network without resizing ? I mean using IMAGE_MAX_DIM and IMAGE_MIN_DIM flags.    Thanks,  Amardeep"
None
"since the annotations are done with VIA, the format is not the same as COCO, so how can we evaluate the model after training? precisely, How to run mAP evaluation?"
"Hi,    I have trained the Mask RCNN model using around 2 lacs 300*300 resolution images using the default training plan like 80 epochs for Heads, 40 epochs for 4+ layers and 20 epochs to fine tune all the layers. Could you please suggest following :    1. Can I retrain the network using the weights produced in the above training, but different resolution images like (1000*1000) images ? and should I retrain all the layers or just fine tune all the layers ?    Thanks in advance,  Amardeep"
"Train my own data and encountered the following problem.     # Train the head branches  # Passing layers=""heads"" freezes all layers except the head  # layers. You can also pass a regular expression to select  # which layers to train by name pattern.  model.train(dataset_train, dataset_val,               learning_rate=config.LEARNING_RATE,               epochs=1,               layers='heads')          Starting at epoch 0. LR=0.001    Checkpoint Path: E:\Mask_RCNN-master-try\logs\shapes20181118T2016\mask_rcnn_shapes_{epoch:04d}.h5  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  In model:  rpn_model      rpn_conv_shared        (Conv2D)      rpn_class_raw          (Conv2D)      rpn_bbox_pred          (Conv2D)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)    C:\Users\Administrator\AppData\Roaming\Python\Python35\site-packages\tensorflow\python\ops\gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""    Epoch 1/1  image_id 14  ERROR:root:Error processing image {'mask_path': 'train_data/cv2_mask/7.png', 'height': 1857, 'width': 1280, 'yaml_path': 'train_data/labelme_json/7_json/info.yaml', 'path': 'train_data/pic/7.jpg', 'source': 'shapes', 'id': 14}  Traceback (most recent call last):    File ""E:\Mask_RCNN-master-try\mrcnn\model.py"", line 1710, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""E:\Mask_RCNN-master-try\mrcnn\model.py"", line 1213, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File "" "", line 80, in load_mask      labels = self.from_yaml_get_class(image_id)    File "" "", line 13, in from_yaml_get_class      del labels[0]  KeyError: 0  ---------------------------------------------------------------------------  KeyError                                  Traceback (most recent call last)    in  ()        6             learning_rate=config.LEARNING_RATE,        7             epochs=1,  ----> 8             layers='heads')        9        10 image=skimage.color.gray2rgb(image)    E:\Mask_RCNN-master-try\mrcnn\model.py in train(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)     2373             max_queue_size=100,     2374             workers=workers,  -> 2375             use_multiprocessing=True,     2376         )     2377         self.epoch = max(self.epoch, epochs)    C:\Users\Administrator\AppData\Roaming\Python\Python35\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)       89                 warnings.warn('Update your `' + object_name +       90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)  ---> 91             return func(*args, **kwargs)       92         wrapper._original_function = func       93         return wrapper    C:\Users\Administrator\AppData\Roaming\Python\Python35\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)     2143                 batch_index = 0     2144                 while steps_done   2145                     generator_output = next(output_generator)     2146      2147                     if not hasattr(generator_output, '__len__'):    E:\Mask_RCNN-master-try\mrcnn\model.py in data_generator(dataset, config, shuffle, augment, augmentation, random_rois, batch_size, detection_targets, no_augmentation_sources)     1708                     load_image_gt(dataset, config, image_id, augment=augment,     1709                                 augmentation=augmentation,  -> 1710                                 use_mini_mask=config.USE_MINI_MASK)     1711      1712             # Skip images that have no instances. This can happen in cases    E:\Mask_RCNN-master-try\mrcnn\model.py in load_image_gt(dataset, config, image_id, augment, augmentation, use_mini_mask)     1211     # Load image and mask     1212     image = dataset.load_image(image_id)  -> 1213     mask, class_ids = dataset.load_mask(image_id)     1214     original_shape = image.shape     1215     image, window, scale, padding, crop = utils.resize_image(      in load_mask(self, image_id)       78             occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))       79         labels = []  ---> 80         labels = self.from_yaml_get_class(image_id)       81         labels_form = []       82         for i in range(len(labels)):      in from_yaml_get_class(self, image_id)       11             temp = yaml.load(f.read())       12             labels = temp['label_names']  ---> 13             del labels[0]       14         return labels       15     KeyError: 0  "
"WARNING:tensorflow:From /home/hp/Mask_RCNN-2.1/model.py:2078: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.  Instructions for updating:  keep_dims is deprecated, use keepdims instead  image_id 6  /home/hp/anaconda3/lib/python3.5/site-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.    if issubdtype(ts, int):  /home/hp/anaconda3/lib/python3.5/site-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.    elif issubdtype(type(size), float):  /home/hp/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  /home/hp/anaconda3/lib/python3.5/site-packages/keras/engine/training_generator.py:46: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.    UserWarning('Using a generator with `use_multiprocessing=True`'  Epoch 1/10  image_id 22  image_id 68  ERROR:root:Error processing image {'height': 500, 'mask_path': 'train_data/cv2_mask/0325.png', 'width': 500, 'id': 22, 'path': 'train_data/pic/0325.jpg', 'yaml_path': 'train_data/labelme_json/0325_json/info.yaml', 'source': 'shapes'}  Traceback (most recent call last):    File ""/home/hp/Mask_RCNN-2.1/model.py"", line 1637, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/hp/Mask_RCNN-2.1/model.py"", line 1210, in load_image_gt      class_ids = class_ids[_idx]  IndexError: boolean index did not match indexed array along dimension 0; dimension is 0 but corresponding boolean dimension is 4    Does anyone know how to solve this ?    "
I trained 1900 images by cpu  tensorflow 1.12   I used this project which uses Mobilenet v1 instead of Resnet     there is no error messages except this in every epoch   10/1700 [..............................] - ETA: 12:31:02 - loss: 1.6230 - rpn_class_loss: 0.1477 - rpn_bbox_loss: 0.1614 - mrcnn_class_loss: 0.2011 - mrcnn_bbox_loss: 0.5121 - mrcnn_mask_loss: 0.6007DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 41 65536  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 41 65536  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'PLTE' 41 768  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 821 6059  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'PLTE' 41 768  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 821 6059  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 41 65536  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 41 65536  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'PLTE' 41 768  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 821 6059  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 41 65536  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'PLTE' 41 768  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 821 6059  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 41 65536  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'PLTE' 41 768  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 821 6059  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 41 65536  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'PLTE' 41 768  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 821 6059  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'PLTE' 41 768  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 821 6059  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 41 65536  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'PLTE' 41 768  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 821 6059  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 41 65536  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'PLTE' 41 768  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 821 6085  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 41 65536  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'PLTE' 41 768  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 821 6085  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 41 65536  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'PLTE' 41 768  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 821 6085  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 41 65536  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 41 65536  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'PLTE' 41 768  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 821 6085  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'PLTE' 41 768  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 821 6085  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13  DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 41 65536  DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13    Does anyone know how to fix it?
"I want to save the network architecture of this model but getting can't pickle error. I tried some solution but not able to resolve.     _# Callbacks          callbacks = [              keras.callbacks.TensorBoard(log_dir=self.log_dir,                                          histogram_freq=0, write_graph=True, write_images=False),              keras.callbacks.ModelCheckpoint(self.checkpoint_path,                                              verbose=0, **save_weights_only=False**),          ]             # Add custom callbacks to the list          if custom_callbacks:              callbacks += custom_callbacks            # Train          log(""\nStarting at epoch {}. LR={}\n"".format(self.epoch, learning_rate))          log(""Checkpoint Path: {}"".format(self.checkpoint_path))          self.set_trainable(layers)          self.compile(learning_rate, self.config.LEARNING_MOMENTUM)            # Work-around for Windows: Keras fails on Windows when using          # multiprocessing workers. See discussion here:          #            if os.name is 'nt':              workers = 0          else:              workers = multiprocessing.cpu_count()            self.keras_model.fit_generator(              train_generator,              initial_epoch=self.epoch,              epochs=epochs,              steps_per_epoch=self.config.STEPS_PER_EPOCH,              callbacks=callbacks,              validation_data=val_generator,              validation_steps=self.config.VALIDATION_STEPS,              max_queue_size=100,              workers=workers,  #old value is workers              use_multiprocessing=True,          )_    1. In network.py i replaced ""return copy.deepcopy(config) ""  to ""return config"" but got new error json is not serializable.  2. I tried to change all lambda with a function but still getting the can't pickle error.  3. I tried to save the model before training, getting same error.    i tried below code as well to save the model  _model.save('mask_rcnn.h5').    with open('model_architecture_subin.json', 'w') as f:          f.write(model.to_json())_  with yaml also i tried but same issue.    **logs for can't pickle _thread.RLock objects**  Traceback (most recent call last):    File ""custom.py"", line 309, in        train(model)    File ""custom.py"", line 220, in train      imgaug.augmenters.Affine(rotate=(-90, 90))    File ""/home/hasmeet/nj/surface_crack_detection/mask_rcnn_solution/Final_Mask_RCNN_Sol/src/mrcnn/model.py"", line 2453, in train      use_multiprocessing=True,    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/site-packages/keras/engine/training.py"", line 1418, in fit_generator      initial_epoch=initial_epoch)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/site-packages/keras/engine/training_generator.py"", line 251, in fit_generator      callbacks.on_epoch_end(epoch, epoch_logs)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/site-packages/keras/callbacks.py"", line 79, in on_epoch_end      callback.on_epoch_end(epoch, logs)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/site-packages/keras/callbacks.py"", line 457, in on_epoch_end      model_json = self.model.to_json()    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/site-packages/keras/engine/network.py"", line 1239, in to_json      model_config = self._updated_config()    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/site-packages/keras/engine/network.py"", line 1187, in _updated_config      config = self.get_config()    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/site-packages/keras/engine/network.py"", line 934, in get_config      return copy.deepcopy(config)   #can't pickable error    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 150, in deepcopy      y = copier(x, memo)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 240, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 150, in deepcopy      y = copier(x, memo)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 215, in _deepcopy_list      append(deepcopy(a, memo))    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 150, in deepcopy      y = copier(x, memo)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 240, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 150, in deepcopy      y = copier(x, memo)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 240, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 150, in deepcopy      y = copier(x, memo)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 220, in _deepcopy_tuple      y = [deepcopy(a, memo) for a in x]    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 220, in        y = [deepcopy(a, memo) for a in x]    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 150, in deepcopy      y = copier(x, memo)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 220, in _deepcopy_tuple      y = [deepcopy(a, memo) for a in x]    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 220, in        y = [deepcopy(a, memo) for a in x]    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 180, in deepcopy      y = _reconstruct(x, memo, *rv)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 280, in _reconstruct      state = deepcopy(state, memo)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 150, in deepcopy      y = copier(x, memo)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 240, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 180, in deepcopy      y = _reconstruct(x, memo, *rv)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 280, in _reconstruct      state = deepcopy(state, memo)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 150, in deepcopy      y = copier(x, memo)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 240, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 180, in deepcopy      y = _reconstruct(x, memo, *rv)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 280, in _reconstruct      state = deepcopy(state, memo)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 150, in deepcopy      y = copier(x, memo)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 240, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 180, in deepcopy      y = _reconstruct(x, memo, *rv)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 280, in _reconstruct      state = deepcopy(state, memo)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 150, in deepcopy      y = copier(x, memo)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 240, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/copy.py"", line 169, in deepcopy      rv = reductor(4)  TypeError: can't pickle _thread.RLock objects    **logs for Not JSON Serializable**    Traceback (most recent call last):    File ""custom.py"", line 309, in        train(model)    File ""custom.py"", line 220, in train      imgaug.augmenters.Affine(rotate=(-90, 90))    File ""/home/hasmeet/nj/surface_crack_detection/mask_rcnn_solution/Final_Mask_RCNN_Sol/src/mrcnn/model.py"", line 2453, in train      use_multiprocessing=True,    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/site-packages/keras/engine/training.py"", line 1418, in fit_generator      initial_epoch=initial_epoch)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/site-packages/keras/engine/training_generator.py"", line 251, in fit_generator      callbacks.on_epoch_end(epoch, epoch_logs)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/site-packages/keras/callbacks.py"", line 79, in on_epoch_end      callback.on_epoch_end(epoch, logs)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/site-packages/keras/callbacks.py"", line 457, in on_epoch_end      model_json = self.model.to_json()    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/site-packages/keras/engine/network.py"", line 1240, in to_json      return json.dumps(model_config, default=get_json_type, **kwargs)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/json/__init__.py"", line 238, in dumps      **kw).encode(obj)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/json/encoder.py"", line 199, in encode      chunks = self.iterencode(o, _one_shot=True)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/json/encoder.py"", line 257, in iterencode      return _iterencode(o, 0)    File ""/home/hasmeet/.conda/envs/ai_maskrcnn/lib/python3.6/site-packages/keras/engine/network.py"", line 1237, in get_json_type      raise TypeError('Not JSON Serializable:', obj)  TypeError: ('Not JSON Serializable:',  )  "
"Hi,    I would like to implement a detector such as balloon detector which is given as an example here. I was wondering how could we even expect this to work without training on negative training images? Images that are not balloons (or whataver our target is) but nevertheless they should be trained on. I do not see this with mask_rcnn, and as a result, this balloon/whatever detector detects the objects that are not balloons/whatevers.     Is there a way around this? Am I missing something?     Thanks in advance."
"Hi.   Is it possible for the masks to be saved as individual images? If it's possible, how can this be done? I need your help     Thanks"
"In my case, I have extra non-image features which are useful for the object classification. So, how can I add these extra features in the nets? In other words, I just want to add a few nodes in the FC layer before softmax for classification. How can I adjust the code and also the training part? Thank you very much."
"like the title, I set step 150  and finally I find the loss value is about 0.6, Is the value suitable?"
"Since Overfitting is an important problem, is there anyone know how to write the code to print the training accuracy and validation accuracy just like the loss function?  So that we can know it is overfitting or not by evaluating the graph of training accuracy and validation accuracy.  Sorry, I am just a beginner.  "
"Use following lines, I could get my output_detections and output_mrcnn_mask         But x1 = x2 = 1.00392 always, no matter which image I have input.         I have tried run the pb file in Python. The result is the same with C++ except for x1 and x2.    Environment:  Windows 10  Visual Studio 2015  Python 3.5  TensorFlow 1.4.0 in Python and C++    PS: I build TensorFlow for Visual Studio using:  [(     Does anyone have any idea to fix this problem? THANKS"
"During custom dataset training I observe below losses:  n_mask_loss, rpn_class_loss,rpn_bbox_loss,mrcnn_class_loss,mrcnn_bbox_loss,mrcnn_mask_loss,val_loss, , val_rpn_class_loss, val_rpn_bbox_loss,val_mrcnn_class_loss,val_mrcnn_bbox_loss,val_mrcnn_mask_loss.    What is significance and meaning of each loss and how is it minimized?"
"What's the backbone of pre-trained COCO weights (mask_rcnn_coco.h5) ?  Is it Resnet50 or Resnet101?    It seems that no matter which backbone(50/101) I configured as COCOCONFIG.BACKBONE, I can get some good training results using this pre-trained coco weights.    Does the COCOCONFIG.BACKBONE setting have relationship with backbone of mask_rcnn_coco.h5?    Thank you for your time."
rpn_class_loss is a loss from RPN classifier logits for FG/BG.   Why this loss use sparse_categorical_crossentropy ?  Should it be binary_crossentropy ? because it only 2 classes. 
"when I train mask rcnn on some data set on my laptop, after sometime the power goes off and it works on the battery  I am using gtx 1070    Is there something hard-coded that could result in this situation?    Did anyone face a problem like this before    Note: I trained many times using different code and didn't face this problem even once"
"My environment:   windows 10 x64   python 3.6.6  cuda 9.0, V9.0.176  cudnn 7.3.1  tensorflow 1.11.0  keras 2.2.4.       I modified train_shapes.py and added cross validations  my dataset :   1896 images 1224x1024 only 1 class and 1 object for each image  and I got stuck on the first epoch 1/5, there is almost 0 % used in GPU nothing changed for hours.  Then I traced the code and found it stuck at model.py 'keras_model.fit_generator()'.  I tried the solutions in   #287   `trying setting workers=1, use_multiprocessing=False`  or update keras  nothing worked out.  is there anyone who encountered this problem and find others solutions?  help me please"
"when I try to train mask-rcnn on my own data,I got the following error.I used the VIA notation tools(2.0).  ---------------------------------------------------------------------------------------------------------  Checkpoint Path: /home/root1/Mask_RCNN/logs/defect20181105T2153/mask_rcnn_defect_{epoch:04d}.h5  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  In model:  rpn_model      rpn_conv_shared        (Conv2D)      rpn_class_raw          (Conv2D)      rpn_bbox_pred          (Conv2D)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)  /home/root1/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  /home/root1/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training_generator.py:46: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.    UserWarning('Using a generator with `use_multiprocessing=True`'  [name id] [1]  [name id] [1]  [name id] [4]  [name id] [1]  [name id] [1]  [name id] [1]  [name id] [3]  [name id] [1]  [name id] [1]  ERROR:root:Error processing image ……  Traceback (most recent call last):    File ""/home/root1/Mask_RCNN/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/root1/Mask_RCNN/mrcnn/model.py"", line 1265, in load_image_gt      class_ids = class_ids[_idx]  IndexError: boolean index did not match indexed array along dimension 0; dimension is 1 but corresponding boolean dimension is 2  ……………………………………………………  1/100 [..............................] - ETA: 28:31 - loss: 9.1306 - rpn_class_loss: 0.0014 - rpn_bbox_loss: 0.1050 - mrcnn_class_loss: 6.4022 - mrcnn_bbox_loss: 1.9416 - mrcnn_mask_loss: 0.6804Traceback (most recent call last):    File ""defect_detection.py"", line 387, in        train(model)    File ""defect_detection.py"", line 222, in train      layers='heads')    File ""/home/root1/Mask_RCNN/mrcnn/model.py"", line 2374, in train      use_multiprocessing=True,    File ""/home/root1/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""/home/root1/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py"", line 1415, in fit_generator      initial_epoch=initial_epoch)    File ""/home/root1/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training_generator.py"", line 177, in fit_generator      generator_output = next(output_generator)    File ""/home/root1/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/utils/data_utils.py"", line 793, in get      six.reraise(value.__class__, value, value.__traceback__)    File ""/home/root1/anaconda3/envs/tensorflow/lib/python3.6/site-packages/six.py"", line 693, in reraise      raise value  IndexError: boolean index did not match indexed array along dimension 0; dimension is 1 but corresponding boolean dimension is 4  -------------------------------------------------------------------------------------------  and I cannot figure out the solution to solve it,is there any one can give a suggestion.THX in advance."
"I want to visulize the distribution in tensorboard . When I modify the callback in train function of  model.py as:  keras.callbacks.TensorBoard(log_dir=self.log_dir,                                          histogram_freq=1, write_graph=True, write_images=False), it has error as follows:  !   It seems that tensorboard can not visulize the bool balue, How should I fix it?"
"Hi, I am not able to run the shapes example due to some keras error. I am running this on Google colab btw. I am highly a newbie in this and therefore would be glad to receive your help. Below is the screenshot of the error.  <img width=""985"" alt=""screenshot 2018-11-04 at 1 29 48 pm"" src=""   "
"First, thanks for this awesome work! I want to detect multiple masks per bounding box similar to how it's done in the DensePose project. There, they not just detect the object mask, but they output a mask for each u coordinates, v coordinates and part segmentation for each detected person. Did anybody already tried this with this repo or does anybody has some first ideas where I had to make these changes in the code? Thanks!"
"Hello, I have encountered a problem installing your program, as follows:  C:\Users\Administrator>python F:\Mask_RCNN-master\setup.py install  WARNING:root:Fail load requirements file, so using default ones.  running install  running bdist_egg  running egg_info  writing mask_rcnn.egg-info\PKG-INFO  writing dependency_links to mask_rcnn.egg-info\dependency_links.txt  writing top-level names to mask_rcnn.egg-info\top_level.txt  error: package directory 'mrcnn' does not exist"
"As I have only one class that I am trying to segment on my image, and because I am constrained to CPU only inference, I am trying to simplify the network in order to run rpn and get the resulting bounding boxes. I am therefore trying to change the network structure in order to retrain it using only the rpn loss.  If I understood the code well, what I need to change is the function build, train and compile in the class MaskRCNN() (model.py starting at line 1802).    I am not sure however if this is sufficient, is there anything else I should consider?"
"  I ran well yesterday. I reported this error after re-running today. I don't know why    ---------------------------------------------------------------------------  AttributeError                            Traceback (most recent call last)    in         20 # Import Mask RCNN       21 sys.path.append(ROOT_DIR)  # To find local version of the library添加要搜索的目录  ---> 22 from mrcnn import utils       23 from mrcnn import visualize       24 from mrcnn.visualize import display_images    ~/anaconda3/lib/python3.5/importlib/_bootstrap.py in _find_and_load(name, import_)    ~/anaconda3/lib/python3.5/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_)    ~/anaconda3/lib/python3.5/importlib/_bootstrap.py in _load_unlocked(spec)    ~/anaconda3/lib/python3.5/importlib/_bootstrap.py in _load_backward_compatible(spec)    ~/anaconda3/lib/python3.5/site-packages/mask_rcnn-2.1-py3.5.egg/mrcnn/utils.py in         15 import tensorflow as tf       16 import scipy  ---> 17 import skimage.color       18 import skimage.io       19 import skimage.transform    ~/.local/lib/python3.5/site-packages/skimage/__init__.py in        175       176     from .util.lookfor import lookfor  --> 177     from .data import data_dir      178       179     ~/.local/lib/python3.5/site-packages/skimage/data/__init__.py in         13 import numpy as _np       14   ---> 15 from ..io import imread, use_plugin       16 from .._shared._warnings import expected_warnings, warn       17 from ..util.dtype import img_as_bool    ~/.local/lib/python3.5/site-packages/skimage/io/__init__.py in         13        14   ---> 15 reset_plugins()       16        17 WRAP_LEN = 73    ~/.local/lib/python3.5/site-packages/skimage/io/manage_plugins.py in reset_plugins()       93 def reset_plugins():       94     _clear_plugins()  ---> 95     _load_preferred_plugins()       96        97     ~/.local/lib/python3.5/site-packages/skimage/io/manage_plugins.py in _load_preferred_plugins()       73                 'imread']       74     for p_type in io_types:  ---> 75         _set_plugin(p_type, preferred_plugins['all'])       76        77     plugin_types = (p for p in preferred_plugins.keys() if p != 'all')    ~/.local/lib/python3.5/site-packages/skimage/io/manage_plugins.py in _set_plugin(plugin_type, plugin_list)       85             continue       86         try:  ---> 87             use_plugin(plugin, kind=plugin_type)       88             break       89         except (ImportError, RuntimeError, OSError):    ~/.local/lib/python3.5/site-packages/skimage/io/manage_plugins.py in use_plugin(name, kind)      256             kind = [kind]      257   --> 258     _load(name)      259       260     for k in kind:    ~/.local/lib/python3.5/site-packages/skimage/io/manage_plugins.py in _load(plugin)      300         modname = plugin_module_name[plugin]      301         plugin_module = __import__('skimage.io._plugins.' + modname,  --> 302                                    fromlist=[modname])      303       304     provides = plugin_provides[plugin]    ~/.local/lib/python3.5/site-packages/skimage/io/_plugins/matplotlib_plugin.py in          2 from collections import namedtuple        3 import numpy as np  ----> 4 from mpl_toolkits.axes_grid1 import make_axes_locatable        5 import matplotlib.image        6 from ...util import dtype as dtypes  AttributeError  ~/anaconda3/lib/python3.5/site-packages/mpl_toolkits/axes_grid1/__init__.py in         10 #from axes_divider import make_axes_locatable       11   ---> 12 from .parasite_axes import host_subplot, host_axes    ~/anaconda3/lib/python3.5/site-packages/mpl_toolkits/axes_grid1/parasite_axes.py in         21        22 import matplotlib.cbook as cbook  ---> 23 is_string_like = cbook.is_string_like       24        25     AttributeError: module 'matplotlib.cbook' has no attribute 'is_string_like"
How can mask rcnn be used to detect small objects like scratch on big metal surface? Image is metal surface say 'drum' (big image)  and I need to detect small scratches. What parameters needs to be tuned for small object (scratches) detection? 
"Hi, all:  when I use the model in production, I want to use the model to predict images in mutiprocess, but it can't work, How should I do?"
"Hi,    I've been experimenting with Mask RCNN and I would like to know with other metrics you use to see how good it performs more than just loss.    when classificating with bounding boxes I could somehow calculate speficicity and sensitivity depending on the distance between the predicted bounding box and the real object to locate, but with masks I'm not sure how to do it.    Any advice?"
"According to keras documents, the value of STEPS_PER_EPOCH  should typically be equal to the number of samples of your dataset divided by the batch size, why it is a constant in the source code?  If we train 4 samples in 2 epoch, and set batch_size to 2, STEPS_PER_EPOCH should be 2, but if we set STEPS_PER_EPOCH as a constant 3,  in epoch 1, after all samples are added to the train function, the first two samples would be trained twice, casuse STEPS_PER_EPOCH = 3.     Is that right?  "
"Hi All,  have few questions regarding usage of mask Rcnn for small applications...I am asking these questions because it seems pretty slow and needs lot of memory to process.    1) Is it possible to use mask RCNN on desktops (no GPUs) for small applications using open CV?  2) If yes, what woudl be the configuration of the target system?    Please share your experiences / thoughts on these..  "
"@waleedka     Hello, thanks for your great contribution.  I'm studying your code, and if I only need one task ( segmentation or detection), how should I modify the code?  Thanks a lot. "
"This is the complete error output:  ` File ""S:\anaconda\envs\research\lib\site-packages\mask_rcnn-2.1-py3.6.egg\mrcnn\model.py"", line 1692, in data_generator  ZeroDivisionError: integer division or modulo by zero    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""ape.py"", line 366, in        train(model)    File ""ape.py"", line 198, in train      layers='heads')    File ""S:\anaconda\envs\research\lib\site-packages\mask_rcnn-2.1-py3.6.egg\mrcnn\model.py"", line 2374, in train    File ""S:\anaconda\envs\research\lib\site-packages\keras\legacy\interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""S:\anaconda\envs\research\lib\site-packages\keras\engine\training.py"", line 1418, in fit_generator      initial_epoch=initial_epoch)    File ""S:\anaconda\envs\research\lib\site-packages\keras\engine\training_generator.py"", line 234, in fit_generator      workers=0)    File ""S:\anaconda\envs\research\lib\site-packages\keras\legacy\interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""S:\anaconda\envs\research\lib\site-packages\keras\engine\training.py"", line 1472, in evaluate_generator      verbose=verbose)    File ""S:\anaconda\envs\research\lib\site-packages\keras\engine\training_generator.py"", line 330, in evaluate_generator      generator_output = next(output_generator)    File ""S:\anaconda\envs\research\lib\site-packages\mask_rcnn-2.1-py3.6.egg\mrcnn\model.py"", line 1810, in data_generator  UnboundLocalError: local variable 'image_id' referenced before assignment`    This is the dataset class that I implemented:  `def load_ape(self, dataset_dir, subset):          """"""Load a subset of the Balloon dataset.          dataset_dir: Root directory of the dataset.          subset: Subset to load: train or val          """"""          # Add classes. We have only one class to add.          self.add_class(""ape"", 1, ""ape"")          count = 0          # Train or validation dataset?          assert subset in [""train"", ""val""]          if subset == ""train"":              count = 1              file = ""train.txt""          else:              print(""entering val"")              count = 3              file = ""test.txt""          file_path = os.path.join(dataset_dir, file)          image_ids = []          with open(file_path, 'r') as f:              for line in f.readlines():                  temp = line.split(""/"")                  image_ids.append(temp[3].strip())          # if subset == ""val"":          #     print(image_ids)          image_dir = ""JPEGImages""          dataset_dir = os.path.join(dataset_dir, image_dir)           # Add images          for image_id in image_ids:              image_path = os.path.join(dataset_dir, image_id)              image = skimage.io.imread(image_path)              height, width = image.shape[:2]                self.add_image(                  ""ape"",                  image_id=count,                  path=image_path)              print(""count is {}"".format(count))              count+=1               def load_mask(self, image_id):          """"""Generate instance masks for an image.         Returns:          masks: A bool array of shape [height, width, instance count] with              one mask per instance.          class_ids: a 1D array of class IDs of the instance masks.          """"""          # If not a balloon dataset image, delegate to parent class.          info = self.image_info[image_id]          # Get mask directory from image path          mask_dir = os.path.join(os.path.dirname(os.path.dirname(info['path'])), ""mask"")          file_to_find = info['path'][-10:]          # print(""image_id is this: {}"".format(file_to_find))          # print(""mask_dir is this: {}"".format(mask_dir))          # Read mask files from .png image          # mask = []          mask = None          for f in next(os.walk(mask_dir))[2]:              if f.endswith("".png""):                  file_name_temp = f.split(""."")                  file_name_temp = file_name_temp[0]                  file_name_temp = file_name_temp+"".jpg""                  # print(""file_name_temp is this: {}"".format(file_name_temp))                  if file_name_temp == file_to_find[2:]:                      m = skimage.io.imread(os.path.join(mask_dir, f)).astype(np.bool)                      # mask.append(m)                      mask = m          # print(""masks has {} items"".format(len(mask)))          # mask = np.stack(mask, axis=0)          # print(""mask's shape is {}"".format(mask.shape))          # print(""mask's shape is {}"".format(mask.shape[-1]))            # Return mask, and array of class IDs of each instance. Since we have          # one class ID, we return an array of ones          return mask, np.ones([mask.shape[-1]], dtype=np.int32)`    Does anyone know how to fix it?"
"Dear All,     I am suffering problem, I try to modify the CNN backbone from default resnet101 to 152 architecture. However, follow the instruction of the resnet paper,  compared to resnet101, I can not get the good or expected result.    in   file, I changed the backbone name  $ BACKBONE = ""resnet152""    in   file, I add the reset152 graph function.  def resnet152_graph(input_image, architecture, stage5=False, train_bn=True):    in   file, I revised these follow part:          if callable(config.BACKBONE):              _, C2, C3, C4, C5 = config.BACKBONE(input_image, stage5=True,                                                  train_bn=config.TRAIN_BN)          else:              # wei changed here, 2018/09/28, for adding a resnet152 network              # _, C2, C3, C4, C5 = resnet_graph(input_image, config.BACKBONE,              #                                  stage5=True, train_bn=config.TRAIN_BN)              _, C2, C3, C4, C5 = resnet152_graph(input_image, config.BACKBONE,                                                  stage5=True, train_bn=config.TRAIN_BN)        However, compare to the resnet101, the deeper network of resnet152 can not perform the expected result, in other words the mAP is very low and predict results are terrible. Could anybody can help me how to fix the problem.    Many thanks.    Kind regards    Wei        "
"Hey all, i am aiming to do keypoints detection on my own dataset which include only two keypoints and not 17 like human pose estimation.     I tried to use the pre trained weights of COCO for human pose estimation, but of course I get the following error  ""ValueError: Dimension 2 in both shapes must be equal, but are 2 and 17. Shapes are [2,2,2,512] and [2,2,17,512]. for 'Assign_724' (op: 'Assign') with input shapes: [2,2,2,512], [2,2,17,512].""    How can i solve this problem?    Thanks"
"Hi ,    I have around 2 lacs png masks images in my dataset for around 10k images. I have converted these masks images to COCO json annotation format (single annotation.json file).    Just want to understand, which way is more optimized, if i train the network using all the 2lacs image in a single training session. Which will take lot of time. Or I can split my dataset in multiple sub datasets and one by one i train the network incrementally using sub datasets one after another. Are these both ways same or will impact the performance ? which one is receommended ?    Thanks in Advance,  Amardeep"
"As in any other DL, we need to train with **a lot of epoch** to have a good result.  One way to evaluate our model is using loss values.     Anyone knows where does loss values are stored?  There are some loss values given by Mask R-CNN. How to plot it?"
"Dear @waleedka, thank you for your code.  I have been trying around object detection with your code.  I was trying to do cells detection which sizes around 12x12 pixel each.  Nevertheless, there are still a lot of undetected cells.    I saw there were a lot of parameters inside.  I found on `config.py`.  I think that was for detecting large objects.  I have tried to configure a lot of them to have a better detection for my tiny cells.  I have tried to reduce the into `MINI_MASK_SHAPE = (14, 14)`, increase RPN_NMS_THRESHOLD = 0.9, reduce `DETECTION_MIN_CONFIDENCE = 0.1`, DETECTION_MAX_INSTANCES = 500, MAX_GT_INSTANCES = 500, etc.    So do you have suggestions what are the parameters to be edited and what are the values?  The image was 640x480 px  The objects are 12x12 size with more than 300 objects/image.    Thanks"
"While training my own model with Mask RCNN, I realized that the number of checkpoints accumulate over time. After one day of training, I had about 1500 checkpoints, resulting in ~350GB of used disk space. Of course, I can delete old checkpoints manually, but I was wondering, if there already is a setting which limits the number of checkpoints?     Thanks,  Bernhard"
What changes in source code needs to be done to implement multiple classes? Default source code is only for single class. Kindly help.  Thanks
"Hello All,  I am new to tensorflow and python.  I would like to use this mask rcnn as a .so or some sort of library that i can call the inference function in c++. For example, the default coco weights would be a good starting point.    I know with pure tensorflow it is possible to freeze graph.  Is this possible for this implementation of Mask_RCNN with keras, numpy, and potential other libraries?  Does anyone have any pointer or tutorial on how to do this for this version of Mask RCNN?    Thanks,  JC"
"Did anyone knows how to train the mrcnn model for just with resnet101 C1 to C3 and skip on C4 and C5 layers? hope to hear your responce as soon as possible~    Best Regards, By Simon~"
"Hi @waleedka       when i'm training on my own dataset i'm facing this issue.    can you please help me out.    InvalidArgumentError                      Traceback (most recent call last)    in  ()        6             learning_rate=config.LEARNING_RATE,        7             epochs=1,  ----> 8             layers='heads')    ~\Downloads\AI PROJECTS\Mask_RCNN-master\mrcnn\model.py in train(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)     2372             max_queue_size=100,     2373             workers=workers,  -> 2374             use_multiprocessing=True,     2375         )     2376         self.epoch = max(self.epoch, epochs)    C:\ProgramData\Anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)       89                 warnings.warn('Update your `' + object_name + '` call to the ' +       90                               'Keras 2 API: ' + signature, stacklevel=2)  ---> 91             return func(*args, **kwargs)       92         wrapper._original_function = func       93         return wrapper    C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)     1416             use_multiprocessing=use_multiprocessing,     1417             shuffle=shuffle,  -> 1418             initial_epoch=initial_epoch)     1419      1420     @interfaces.legacy_generator_methods_support    C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)      215                 outs = model.train_on_batch(x, y,      216                                             sample_weight=sample_weight,  --> 217                                             class_weight=class_weight)      218       219                 outs = to_list(outs)    C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\training.py in train_on_batch(self, x, y, sample_weight, class_weight)     1215             ins = x + y + sample_weights     1216         self._make_train_function()  -> 1217         outputs = self.train_function(ins)     1218         return unpack_singleton(outputs)     1219     C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py in __call__(self, inputs)     2713                 return self._legacy_call(inputs)     2714   -> 2715             return self._call(inputs)     2716         else:     2717             if py_any(is_tensor(x) for x in inputs):    C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py in _call(self, inputs)     2673             fetched = self._callable_fn(*array_vals, run_metadata=self.run_metadata)     2674         else:  -> 2675             fetched = self._callable_fn(*array_vals)     2676         return fetched ]]  "
"I want to train Mask R-CNN with my dataset that have 4 classes.   Class A -> 3,000  Class B -> 26  Class C -> 8  Class D -> 4    Model always predict Class A because my data was imbalance  How can i fix this problem ?."
!   
"Hello everyone, I want to fuse two mask r-cnn network. One of them must be trained with rgb images other one must be thermal image. I searched it but I could'nt find any example like that."
"InvalidArgumentError                      Traceback (most recent call last)  c:\users\dexp\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\framework\ops.py in _create_c_op(graph, node_def, inputs, control_inputs)     1575   try:  -> 1576     c_op = c_api.TF_FinishOperation(op_desc)     1577   except errors.InvalidArgumentError as e:    InvalidArgumentError: Dimension 1 in both shapes must be equal, but are 33 and 359. Shapes are [256,33] and [256,359]. for 'Assign_35' (op: 'Assign') with input shapes: [256,33], [256,359].    During handling of the above exception, another exception occurred:    ValueError                                Traceback (most recent call last)    in  ()      132       133 if __name__ == '__main__':  --> 134     generate()      in generate()       22        23     network_input, normalized_input = prepare_sequences(notes, pitchnames, n_vocab)  ---> 24     model = create_network(normalized_input, n_vocab)       25     prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)       26     create_midi(prediction_output)      in create_network(network_input, n_vocab)       68        69     # Load the weights to each node  ---> 70     model.load_weights('weights.hdf5')       71        72     return model    c:\users\dexp\appdata\local\programs\python\python36\lib\site-packages\keras\engine\network.py in load_weights(self, filepath, by_name, skip_mismatch, reshape)     1159             else:     1160                 saving.load_weights_from_hdf5_group(  -> 1161                     f, self.layers, reshape=reshape)     1162      1163     def _updated_config(self):    c:\users\dexp\appdata\local\programs\python\python36\lib\site-packages\keras\engine\saving.py in load_weights_from_hdf5_group(f, layers, reshape)      926                              ' elements.')      927         weight_value_tuples += zip(symbolic_weights, weight_values)  --> 928     K.batch_set_value(weight_value_tuples)      929       930     c:\users\dexp\appdata\local\programs\python\python36\lib\site-packages\keras\backend\tensorflow_backend.py in batch_set_value(tuples)     2433                 assign_placeholder = tf.placeholder(tf_dtype,     2434                                                     shape=value.shape)  -> 2435                 assign_op = x.assign(assign_placeholder)     2436                 x._assign_placeholder = assign_placeholder     2437                 x._assign_op = assign_op    c:\users\dexp\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\ops\variables.py in assign(self, value, use_locking)      643       the assignment has completed.      644     """"""  --> 645     return state_ops.assign(self._variable, value, use_locking=use_locking)      646       647   def assign_add(self, delta, use_locking=False):    c:\users\dexp\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\ops\state_ops.py in assign(ref, value, validate_shape, use_locking, name)      214     return gen_state_ops.assign(      215         ref, value, use_locking=use_locking, name=name,  --> 216         validate_shape=validate_shape)      217   return ref.assign(value, name=name)      218     c:\users\dexp\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\ops\gen_state_ops.py in assign(ref, value, validate_shape, use_locking, name)       61     _, _, _op = _op_def_lib._apply_op_helper(       62         ""Assign"", ref=ref, value=value, validate_shape=validate_shape,  ---> 63         use_locking=use_locking, name=name)       64     _result = _op.outputs[:]       65     _inputs_flat = _op.inputs    c:\users\dexp\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\framework\op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)      785         op = g.create_op(op_type_name, inputs, output_types, name=scope,      786                          input_types=input_types, attrs=attr_protos,  --> 787                          op_def=op_def)      788       return output_structure, op_def.is_stateful, op      789     c:\users\dexp\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\util\deprecation.py in new_func(*args, **kwargs)      452                 'in a future version' if date is None else ('after %s' % date),      453                 instructions)  --> 454       return func(*args, **kwargs)      455     return tf_decorator.make_decorator(func, new_func, 'deprecated',      456                                        _add_deprecated_arg_notice_to_docstring(    c:\users\dexp\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\framework\ops.py in create_op(***failed resolving arguments***)     3153           input_types=input_types,     3154           original_op=self._default_original_op,  -> 3155           op_def=op_def)     3156       self._create_op_helper(ret, compute_device=compute_device)     3157     return ret    c:\users\dexp\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\framework\ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)     1729           op_def, inputs, node_def.attr)     1730       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,  -> 1731                                 control_input_ops)     1732      1733     # Initialize self._outputs.    c:\users\dexp\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\framework\ops.py in _create_c_op(graph, node_def, inputs, control_inputs)     1577   except errors.InvalidArgumentError as e:     1578     # Convert to ValueError for backwards compatibility.  -> 1579     raise ValueError(str(e))     1580      1581   return c_op    ValueError: Dimension 1 in both shapes must be equal, but are 33 and 359. Shapes are [256,33] and [256,359]. for 'Assign_35' (op: 'Assign') with input shapes: [256,33], [256,359].      ​I was making a music through the neural network.  It was trained, but then - it can make its own music because of this error.    What am I supposed to do?!"
"hi every body i made a dataset but i think I made this dataset incorrect Because when i want to train it i get take this error:  Loading weights  /home/farzad/mask_rcnn_coco.h5  2018-10-21 10:13:52.335212: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA  Traceback (most recent call last):    File ""custom.py"", line 363, in        train(model)    File ""custom.py"", line 183, in train      dataset_train.load_custom(args.dataset, ""train"")    File ""custom.py"", line 120, in load_custom      annotations = [a for a in annotations if a['regions']]    File ""custom.py"", line 120, in        annotations = [a for a in annotations if a['regions']]  KeyError: 'regions'  can every body explain to me how can i made a correct dataset with one feature by use VGG image annotator"
"I'd like to experiment with changing the backbone architecture to a model that has been trained on a dataset more similar to what I am segmenting, theoretically therefore producing more accurate RPN outputs.    I see that in line 1896 of `model.py` the feature layers of the `resnet_graph` are being extracted and passed into the top-down layers which complete the RPN architecture. Am I right in thinking that I can just pass any feature maps I want to the top-down layers and then train the network as normal since during training the network only trains those top-down layers and above (i.e. segmentation and regression layers) and does not touch the underlying backbone layers, or do I have to provide the whole architecture of my alternative backbone and load in the pre-trained weights and then extract the feature maps as done in `model.py`?"
"Original code runs perfect!  replacing (in place) the images in images folder with other jpg images, gives the following dump    Processing 1 images  ---------------------------------------------------------------------------  TypeError                                 Traceback (most recent call last)  TypeError: '  returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)  SystemError:   returned a result with an error set    The above exception was the direct cause of the following exception:    SystemError                               Traceback (most recent call last)    in          7         8 # Run detection  ----> 9 results = model.detect([image], verbose=1)       10        11 # Visualize results    ~\AppData\Roaming\Python\Python36\site-packages\mask_rcnn-2.1-py3.6.egg\mrcnn\model.py in detect(self, images, verbose)     2498             log(""Processing {} images"".format(len(images)))     2499             for image in images:  -> 2500                 log(""image"", image)     2501      2502         # Mold inputs to format expected by the neural network    ~\AppData\Roaming\Python\Python36\site-packages\mask_rcnn-2.1-py3.6.egg\mrcnn\model.py in log(text, array)       44         text += (""shape: {:20}  min: {:10.5f}  max: {:10.5f}  {}"".format(       45             str(array.shape),  ---> 46             array.min() if array.size else """",       47             array.max() if array.size else """",       48             array.dtype))    ~\AppData\Roaming\Python\Python36\site-packages\numpy\core\_methods.py in _amin(a, axis, out, keepdims, initial)       30 def _amin(a, axis=None, out=None, keepdims=False,       31           initial=_NoValue):  ---> 32     return umr_minimum(a, axis, None, out, keepdims, initial)       33        34 def _sum(a, axis=None, dtype=None, out=None, keepdims=False,    SystemError:   returned a result with an error set"
I have modified the balloon sample to train Mask_rcnn on my dataset. Everything still fine until I begin training code    !     It always stands there. After run through training dataset without run on valid.    !     Is anyone having an ideal? please help.
"Hello, is there a way to run mAP evaluation in balloon.py like coco.py?  I transfer/change some of the evaluation code from coco.py but got stuck on this line:        # Load results. This modifies results with additional attributes.     coco_results = coco.loadRes(results)    The error was there is no 'loadRes'.    Any comment is highly appreciated.  "
"This is the code       Which is giving below error    ValueError:   Layer   has a   variable shape in a non-batch dimension.  TPU models must  have constant shapes for all operations.    You may have to specify `input_length` for RNN/TimeDistributed layers.    Layer:    Input shape: (None, None, None, 3)  Output shape: (None, None, None, 3)  "
"when i am trying to execute the train_shapes.ipynb->model.train(.......)    i'm facing the following error    model.train(dataset_train, dataset_val,               learning_rate=config.LEARNING_RATE,               epochs=1,               layers='heads')    InvalidArgumentError                      Traceback (most recent call last)    in  ()        6             learning_rate=config.LEARNING_RATE,        7             epochs=1,  ----> 8             layers='heads')    ~\Downloads\AI PROJECTS\Mask_RCNN-master\mrcnn\model.py in train(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)     2372             max_queue_size=100,     2373             workers=workers,  -> 2374             use_multiprocessing=True,     2375         )     2376         self.epoch = max(self.epoch, epochs)    C:\ProgramData\Anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)       89                 warnings.warn('Update your `' + object_name + '` call to the ' +       90                               'Keras 2 API: ' + signature, stacklevel=2)  ---> 91             return func(*args, **kwargs)       92         wrapper._original_function = func       93         return wrapper    C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)     1416             use_multiprocessing=use_multiprocessing,     1417             shuffle=shuffle,  -> 1418             initial_epoch=initial_epoch)     1419      1420     @interfaces.legacy_generator_methods_support    C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)      215                 outs = model.train_on_batch(x, y,      216                                             sample_weight=sample_weight,  --> 217                                             class_weight=class_weight)      218       219                 outs = to_list(outs)    C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\training.py in train_on_batch(self, x, y, sample_weight, class_weight)     1215             ins = x + y + sample_weights     1216         self._make_train_function()  -> 1217         outputs = self.train_function(ins)     1218         return unpack_singleton(outputs)     1219     C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py in __call__(self, inputs)     2713                 return self._legacy_call(inputs)     2714   -> 2715             return self._call(inputs)     2716         else:     2717             if py_any(is_tensor(x) for x in inputs):    C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py in _call(self, inputs)     2673             fetched = self._callable_fn(*array_vals, run_metadata=self.run_metadata)     2674         else:  -> 2675             fetched = self._callable_fn(*array_vals)     2676         return fetched ]]  "
"I train the coco2017, use coco.py, but get the result mAP=0.203. It is far worse result, could you tell me the reason?  Command: train  Model: /home/lixiaofeng/project/version5/Mask_RCNN-master/logs/coco20181012T1737/mask_rcnn_coco_0020.h5  Dataset: /home/lixiaofeng/dataset/coco  Year: 2017  Logs: /home/lixiaofeng/project/version5/Mask_RCNN-master/logs  Auto Download: False  Configurations:  BACKBONE   resnet101  BACKBONE_STRIDES [4, 8, 16, 32, 64]  BATCH_SIZE   4  BBOX_STD_DEV   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE   None  DETECTION_MAX_INSTANCES   100  DETECTION_MIN_CONFIDENCE   0.7  DETECTION_NMS_THRESHOLD   0.3  FPN_CLASSIF_FC_LAYERS_SIZE   1024  GPU_COUNT   4  GRADIENT_CLIP_NORM   5.0  IMAGES_PER_GPU   1  IMAGE_MAX_DIM   1024  IMAGE_META_SIZE   93  IMAGE_MIN_DIM   800  IMAGE_MIN_SCALE   0  IMAGE_RESIZE_MODE   square  IMAGE_SHAPE   [1024 1024 3]  LEARNING_MOMENTUM   0.9  LEARNING_RATE   0.001  LOSS_WEIGHTS   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE   14  MASK_SHAPE   [28, 28]  MAX_GT_INSTANCES   100  MEAN_PIXEL   [123.7 116.8 103.9]  MINI_MASK_SHAPE   (56, 56)  NAME   coco  NUM_CLASSES   81  POOL_SIZE   7  POST_NMS_ROIS_INFERENCE   1000  POST_NMS_ROIS_TRAINING   2000  ROI_POSITIVE_RATIO   0.33  RPN_ANCHOR_RATIOS   [0.5, 1, 2]  RPN_ANCHOR_SCALES   (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE   1  RPN_BBOX_STD_DEV   [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD   0.7  RPN_TRAIN_ANCHORS_PER_IMAGE   256  STEPS_PER_EPOCH   1000  TOP_DOWN_PYRAMID_SIZE   256  TRAIN_BN   False  TRAIN_ROIS_PER_IMAGE   200  USE_MINI_MASK   True  USE_RPN_ROIS   True  VALIDATION_STEPS   50  WEIGHT_DECAY   0.0001"
"Hi,  using the customized balloon.py trained the model using my custom data.. at the time of training masks are shown on the images...my images are not that big but small. but when try to check on any new image the masks are not shown... i am sharing my config details here.. does this configuration helps to identify the masks on small objects? i have only one class..    Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 1  IMAGE_MAX_DIM                  512  IMAGE_META_SIZE                14  IMAGE_MIN_DIM                  512  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [512 512   3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [42.17746161 38.21568456 46.82167803]  MINI_MASK_SHAPE                (56, 56)  NAME                           Part1  NUM_CLASSES                    2  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    300  STEPS_PER_EPOCH                100  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001"
"An issue I have consistently found is that my bounding box is often slightly smaller than my mask, thus cause a small ""clipping"" at the mask edge. Is there way to force the bbox to encompass the full mask? Or for me to get the mask before it is clipped by the bbox? Or even naively any way to make the RPN box 5% bigger before generating the mask?    An example is this whale being smushed up against the front of the bbox:    !   "
"Hi, I tried different configs with the network and realized for a lot of time the mask prediction would lose the mask on the image boundary. My mask usually would be a rectangular shape lying on several rows, from very left to very right of the image, but the predicted masks sometime would either miss detect the very left or the very right parts, for several pixels.   Any idea about why this would happen and how I could change the config to improve? Thanks!    FYI, all my training and testing data have the same size (1024, 1024, 3) (so I think probably I don't need to pad anymore)  And I have the config as following:  Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.98  DETECTION_NMS_THRESHOLD        0.3  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 1  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                14  IMAGE_MIN_DIM                  1024  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              none  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_mask_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_bbox_loss': 1.0}  MASK_POOL_SIZE                 70  MASK_SHAPE                     [56, 56]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           Mtc  NUM_CLASSES                    2  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    512  STEPS_PER_EPOCH                100  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           512  USE_MINI_MASK                  False  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001    "
why      loss is defined to multiply by length of output?? it's weired.. Could you please explain it? thank you very much!
None
"I'm having trouble achieving viable results with Mask R-CNN and I can't seem to pinpoint why.  I am using a fairly limited dataset (13 images) of large greyscale images (2560 x 2160) where the detection target is very small (mean area of 26 pixels).  I have run inspect_nucleus_data.ipynb across my data and verified that the masks and images are being interpreted correctly.  I've also followed the wiki guide (  to have my images read and dealt with as greyscale images rather than just converting them to RGB.  Here is one of the images with the detection targets labelled.  !   During training, the loss values are pretty unpredictable, bouncing between around 1 and 2 without ever reaching a steady decline where it seems like it's converging at all.  I'm using these config values at the moment;  they're the best I've been able to come up with while fighting off OOM errors:         I'm training on all layers.  The output I'm getting generally looks like this, with grid-like detections found in weird spots without ever seeming to accurately identify a nucleus.  I've added the red square just to highlight a very obvious cluster of nuclei that have been missed:    !     Here is a binary mask of these same detections so you can see their shape:    !     Could anyone shed some light on what might be going wrong here?"
"I set        but when I check `nvidia-smi`, no jobs are reported. Why is this happening?       "
"Hello,  I'm facing this issue when running ""python setup.py install"".  Need help please.    Thanks in advance!!    Alex EBE  "
None
"Hi,    This is a question rather than issue.  ( can't find a better place looking for suggestions).   I'd like to try this network just for one class,  ( e.g. person)     If I simple reduce the classes from 80 to 1,  the model size is reduced by 2MB only.    It seems intuitive that one may be able to reduce the FC=1024 in the classification head.    And if so,  how to decide the right number for the FC layer?   I tried using FC=256 and it works.   However i'd like to know the design rules behind it.   ( As this can also significantly reduce the model size)    many thanks,  Yannuo"
"When I trained my model, I found that `val_rpn_bbox_loss` tend to overfit easily, so I try to add the regularizer to the layer `rpn_bbox_pred` in my train_xxx.ipynb,      But when I take a look at the loss in training, I found that the loss remains at the same scale as before.    (In contrast, I also experimented by adding regularizer into `rpn_bbox_pred` layer in model.py, and this time, the loss becomes more than ten thousands, just as expected.)    I later found that a model should be recompiled so that the changes will take effect. When I checked   it seems that you must get losses before compiling a model, but at the time it calculates losses, the model hasn't been recompiled, so the regularizer I have added outside model.py won't be taken into consideration.    So  is there a workaround that can solve or bypass the problem?   Thanks in advance."
How to know the object's location using mask r cnn ?  I just want to make a photo that contains only objects.   Could you help me?
"when i run the train_shapes.ipynb, I met the problem that is FileNotFoundError: [Errno 2] Could not find weight files in /home/ubuntu66/logs/shapes20181006T1248. How can i solve this problem?"
"@waleedka , is it possible to feed a single binary mask image of original image to this network instead of having different object mask images and converting them into MS COCO format. More specifically i have aerial orthophotos with vector details seperately. How can i use this information to feed it to the network ?    I am sure there should be the way, as in loadmask() function of Dataset class, COCO json annotations are veing converted to mask images and getting feeded to the network."
"I am getting an error while I am trying to load my own dataset.    An example of my json annotations for one of the images looks like this.  ""RectangleCircleCenter16.jpg40433"":{""fileref"":"""",""size"":40433,""filename"":""RectangleCircleCenter16.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[469,444,440,455,483,562,609,639,685,728,766,821,824,803,762,716,671,632,565,489,469],""all_points_y"":[286,349,418,471,519,572,580,583,580,559,531,444,381,305,238,210,199,194,202,251,286]},""region_attributes"":{""class_ids"":""Circle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[345,346,821,817,345],""all_points_y"":[261,628,626,258,261]},""region_attributes"":{""class_ids"":""Rectangle""}}}}}    My load function looks like this.  class BalloonConfig(Config):  """"""Configuration for training on the toy dataset.  Derives from the base Config class and overrides some values.  """"""  # Give the configuration a recognizable name  NAME = ""balloon""    # We use a GPU with 12GB memory, which can fit two images.  # Adjust down if you use a smaller GPU.  IMAGES_PER_GPU = 1    # Number of classes (including background)  NUM_CLASSES = 1 + 2  # Background + balloon    # Number of training steps per epoch  STEPS_PER_EPOCH = 100    # Skip detections with < 90% confidence  DETECTION_MIN_CONFIDENCE = 0.9  ############################################################    Dataset  ############################################################    class BalloonDataset(utils.Dataset):    def load_balloon(self, dataset_dir, subset):      """"""Load a subset of the Balloon dataset.      dataset_dir: Root directory of the dataset.      subset: Subset to load: train or val      """"""      # Add classes. We have only one class to add.      self.add_class(""balloon"", 1, ""Circle"")      self.add_class(""balloon"", 2, ""Rectangle"")        # Train or validation dataset?      assert subset in [""train"", ""val""]      dataset_dir = os.path.join(dataset_dir, subset)        # Load annotations      # VGG Image Annotator saves each image in the form:      # { 'filename': '28503151_5b5b7ec140_b.jpg',      #   'regions': {      #       '0': {      #           'region_attributes': {},      #           'shape_attributes': {      #               'all_points_x': [...],      #               'all_points_y': [...],      #               'name': 'polygon'}},      #       ... more regions ...      #   },      #   'size': 100202      # }      # We mostly care about the x and y coordinates of each region      annotations = json.load(open(os.path.join(dataset_dir, ""via_region_data.json"")))      annotations = list(annotations.values())  # don't need the dict keys        # The VIA tool saves images in the JSON even if they don't have any      # annotations. Skip unannotated images.      annotations = [a for a in annotations if a['regions']]        # Add images      for a in annotations:          # Get the x, y coordinaets of points of the polygons that make up          # the outline of each object instance. There are stores in the          # shape_attributes (see json format above)          polygons = [r['shape_attributes'] for r in a['regions'].values()]          class_ids = [s['region_attributes'] for s in a['regions'].values()]          # load_mask() needs the image size to convert polygons to masks.          # Unfortunately, VIA doesn't include it in JSON, so we must read          # the image. This is only managable since the dataset is tiny.          image_path = os.path.join(dataset_dir, a['filename'])          image = skimage.io.imread(image_path)          height, width = image.shape[:2]            self.add_image(              ""balloon"",              image_id=a['filename'],  # use file name as a unique image id              path=image_path,              width=width, height=height,              polygons=polygons,              class_ids=class_ids)    def load_mask(self, image_id):      """"""Generate instance masks for an image.     Returns:      masks: A bool array of shape [height, width, instance count] with          one mask per instance.      class_ids: a 1D array of class IDs of the instance masks.      """"""      # If not a balloon dataset image, delegate to parent class.      image_info = self.image_info[image_id]      if image_info[""source""] != ""balloon"":          return super(self.__class__, self).load_mask(image_id)        # Convert polygons to a bitmap mask of shape      # [height, width, instance_count]      info = self.image_info[image_id]      mask = np.zeros([info[""height""], info[""width""], len(info[""polygons""])],                      dtype=np.uint8)      for i, p in enumerate(info[""polygons""]):          # Get indexes of pixels inside the polygon and set them to 1          rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])          mask[rr, cc, i] = 1        # Return mask, and array of class IDs of each instance. Since we have      # one class ID only, we return an array of 1s      return mask, info['class_ids']  The error i get is mentioned below.    C:\ProgramData\Anaconda3\envs\MaskRCNN\lib\site-packages\tensorflow\python\ops\gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  Epoch 1/30  C:\ProgramData\Anaconda3\envs\MaskRCNN\lib\site-packages\skimage\transform_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.  warn(""Anti-aliasing will be enabled by default in skimage 0.15 to ""  ERROR:root:Error processing image {'id': 'RectangleCircle6.jpg', 'source': 'balloon', 'path': 'C:\Windows\System32\Mask_RCNN\samples\DrawingsPoli\Object\train\RectangleCircle6.jpg', 'width': 1169, 'height': 827, 'polygons': [{'name': 'polygon', 'all_points_x': [479, 429, 394, 369, 342, 334, 337, 384, 484, 568, 602, 627, 636, 637, 621, 592, 568, 530, 493, 479], 'all_points_y': [303, 314, 331, 358, 398, 443, 514, 578, 615, 586, 557, 514, 478, 452, 412, 374, 344, 321, 306, 303]}, {'name': 'polygon', 'all_points_x': [479, 481, 836, 836, 479], 'all_points_y': [216, 466, 463, 221, 216]}], 'class_ids': [{'class_ids': 'Circle'}, {'class_ids': 'Rectangle'}]}  Traceback (most recent call last):  File ""C:\Windows\System32\Mask_RCNN\mrcnn\model.py"", line 1717, in data_generator  use_mini_mask=config.USE_MINI_MASK)  File ""C:\Windows\System32\Mask_RCNN\mrcnn\model.py"", line 1272, in load_image_gt  class_ids = class_ids[_idx]  TypeError: only integer scalar arrays can be converted to a scalar index  C:\ProgramData\Anaconda3\envs\MaskRCNN\lib\site-packages\skimage\transform_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.  warn(""Anti-aliasing will be enabled by default in skimage 0.15 to ""  ERROR:root:Error processing image {'id': 'Circle7.jpg', 'source': 'balloon', 'path': 'C:\Windows\System32\Mask_RCNN\samples\DrawingsPoli\Object\train\Circle7.jpg', 'width': 1169, 'height': 827, 'polygons': [{'name': 'polygon', 'all_points_x': [404, 463, 536, 642, 711, 761, 798, 755, 692, 622, 550, 479, 419, 383, 381, 395, 404], 'all_points_y': [317, 249, 212, 214, 252, 310, 416, 534, 587, 615, 619, 592, 542, 459, 375, 332, 317]}], 'class_ids': [{'class_ids': 'Circle'}]}  Traceback (most recent call last):  File ""C:\Windows\System32\Mask_RCNN\mrcnn\model.py"", line 1717, in data_generator  use_mini_mask=config.USE_MINI_MASK)  File ""C:\Windows\System32\Mask_RCNN\mrcnn\model.py"", line 1272, in load_image_gt  class_ids = class_ids[_idx]  TypeError: only integer scalar arrays can be converted to a scalar index  C:\ProgramData\Anaconda3\envs\MaskRCNN\lib\site-packages\skimage\transform_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.  warn(""Anti-aliasing will be enabled by default in skimage 0.15 to ""  ERROR:root:Error processing image {'id': 'Circle2.jpg', 'source': 'balloon', 'path': 'C:\Windows\System32\Mask_RCNN\samples\DrawingsPoli\Object\train\Circle2.jpg', 'width': 1169, 'height': 827, 'polygons': [{'name': 'polygon', 'all_points_x': [485, 528, 557, 594, 625, 699, 734, 733, 725, 697, 636, 571, 532, 497, 469, 449, 442, 442, 460, 485], 'all_points_y': [308, 280, 273, 270, 276, 318, 394, 423, 462, 512, 554, 560, 550, 530, 506, 473, 432, 368, 337, 308]}], 'class_ids': [{'class_ids': 'Circle'}]}  Traceback (most recent call last):  File ""C:\Windows\System32\Mask_RCNN\mrcnn\model.py"", line 1717, in data_generator  use_mini_mask=config.USE_MINI_MASK)  File ""C:\Windows\System32\Mask_RCNN\mrcnn\model.py"", line 1272, in load_image_gt  class_ids = class_ids[_idx]  TypeError: only integer scalar arrays can be converted to a scalar index  C:\ProgramData\Anaconda3\envs\MaskRCNN\lib\site-packages\skimage\transform_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.  warn(""Anti-aliasing will be enabled by default in skimage 0.15 to ""  ERROR:root:Error processing image {'id': 'RectangleCircleCenter16.jpg', 'source': 'balloon', 'path': 'C:\Windows\System32\Mask_RCNN\samples\DrawingsPoli\Object\train\RectangleCircleCenter16.jpg', 'width': 1169, 'height': 827, 'polygons': [{'name': 'polygon', 'all_points_x': [469, 444, 440, 455, 483, 562, 609, 639, 685, 728, 766, 821, 824, 803, 762, 716, 671, 632, 565, 489, 469], 'all_points_y': [286, 349, 418, 471, 519, 572, 580, 583, 580, 559, 531, 444, 381, 305, 238, 210, 199, 194, 202, 251, 286]}, {'name': 'polygon', 'all_points_x': [345, 346, 821, 817, 345], 'all_points_y': [261, 628, 626, 258, 261]}], 'class_ids': [{'class_ids': 'Circle'}, {'class_ids': 'Rectangle'}]}  Traceback (most recent call last):  File ""C:\Windows\System32\Mask_RCNN\mrcnn\model.py"", line 1717, in data_generator  use_mini_mask=config.USE_MINI_MASK)  File ""C:\Windows\System32\Mask_RCNN\mrcnn\model.py"", line 1272, in load_image_gt  class_ids = class_ids[_idx]  TypeError: only integer scalar arrays can be converted to a scalar index  C:\ProgramData\Anaconda3\envs\MaskRCNN\lib\site-packages\skimage\transform_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.  warn(""Anti-aliasing will be enabled by default in skimage 0.15 to ""  ERROR:root:Error processing image {'id': 'D2Rectangle4.jpg', 'source': 'balloon', 'path': 'C:\Windows\System32\Mask_RCNN\samples\DrawingsPoli\Object\train\D2Rectangle4.jpg', 'width': 1169, 'height': 827, 'polygons': [{'name': 'polygon', 'all_points_x': [469, 469, 604, 604, 469], 'all_points_y': [371, 582, 582, 366, 371]}, {'name': 'polygon', 'all_points_x': [492, 493, 704, 706, 492], 'all_points_y': [242, 385, 385, 241, 242]}], 'class_ids': [{'class_ids': 'Rectangle'}, {'class_ids': 'Rectangle'}]}  Traceback (most recent call last):  File ""C:\Windows\System32\Mask_RCNN\mrcnn\model.py"", line 1717, in data_generator  use_mini_mask=config.USE_MINI_MASK)  File ""C:\Windows\System32\Mask_RCNN\mrcnn\model.py"", line 1272, in load_image_gt  class_ids = class_ids[_idx]  TypeError: only integer scalar arrays can be converted to a scalar index  C:\ProgramData\Anaconda3\envs\MaskRCNN\lib\site-packages\skimage\transform_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.  warn(""Anti-aliasing will be enabled by default in skimage 0.15 to ""  ERROR:root:Error processing image {'id': 'Rectangle6.jpg', 'source': 'balloon', 'path': 'C:\Windows\System32\Mask_RCNN\samples\DrawingsPoli\Object\train\Rectangle6.jpg', 'width': 1169, 'height': 827, 'polygons': [{'name': 'polygon', 'all_points_x': [459, 461, 688, 687, 459], 'all_points_y': [264, 457, 459, 261, 264]}], 'class_ids': [{'class_ids': 'Rectangle'}]}  Traceback (most recent call last):  File ""C:\Windows\System32\Mask_RCNN\mrcnn\model.py"", line 1717, in data_generator  use_mini_mask=config.USE_MINI_MASK)  File ""C:\Windows\System32\Mask_RCNN\mrcnn\model.py"", line 1272, in load_image_gt  class_ids = class_ids[_idx]  TypeError: only integer scalar arrays can be converted to a scalar index  Traceback (most recent call last):  File ""balloon.py"", line 361, in   train(model)  File ""balloon.py"", line 196, in train  layers='heads')  File ""C:\Windows\System32\Mask_RCNN\mrcnn\model.py"", line 2381, in train  use_multiprocessing=True,  File ""C:\ProgramData\Anaconda3\envs\MaskRCNN\lib\site-packages\keras\legacy\interfaces.py"", line 91, in wrapper  return func(*args, **kwargs)  File ""C:\ProgramData\Anaconda3\envs\MaskRCNN\lib\site-packages\keras\engine\training.py"", line 1415, in fit_generator  initial_epoch=initial_epoch)  File ""C:\ProgramData\Anaconda3\envs\MaskRCNN\lib\site-packages\keras\engine\training_generator.py"", line 177, in fit_generator  generator_output = next(output_generator)  File ""C:\Windows\System32\Mask_RCNN\mrcnn\model.py"", line 1717, in data_generator  use_mini_mask=config.USE_MINI_MASK)  File ""C:\Windows\System32\Mask_RCNN\mrcnn\model.py"", line 1272, in load_image_gt  class_ids = class_ids[_idx]  TypeError: only integer scalar arrays can be converted to a scalar index"
"I am trying to do some predictions using the Mask_RCNN network. The goal is to use it on my own images. I used the `samples/coco/coco.py` script as inpiration. I loop through the image names and do prediction on each image. The problem is that it crashes, sometimes before the first image and sometimes after 4 images. I do not understand why and the error message is not clear to me.     This comes when `batch_outs = f(ins_batch)` is called in `training_arrays.py` line 294.    `tensorflow.python.framework.errors_impl.InvalidArgumentError: indices ]]`    Seems to me it is related to TensorFlow?    Another error message that comes frequently is:  `./tensorflow/core/framework/tensor.h:643] Check failed: new_num_elements == NumElements() (1 vs. 2)`    and     `tensorflow.python.framework.errors_impl.InvalidArgumentError: indices ]]  `    I have tried both with and without  `GPU`, but no difference.    I have also tried the script under `samples/coco/coco.py` and that gives the same error.    My code   "
"I want to train the network with only 3 RPN_ANCHOR_SCALES (instead of the default 5).  I adapt the scales tuple accordingly from (32,64,128,256,512) to (64,128,256) and trained all the layers after the backbone (layers='5+') in the train method.  I successfully trained the network, but could not predict.  when I ran inference i get the following error:    tensorflow.python.framework.errors_impl.InvalidArgumentError: indices ]]      if I changed the RPN_ANCHOR_SCALES back to the original settings it work.    I use the coco pre-trained weights, I don't think that's the problem since the original weights set only for the backbone and I trained the rest of the network.    How can I train and predict with less than 5 scales?    Thanks in advance."
why is the image molded before train in the mask rccn process?
"Hi,    Is RPN_ANCHOR_SCALES parameter related to original image size, or to resized input image?    It was supposed to be related to resized image.    But I'm seeing examples having one of the RPN_ANCHOR_SCALES equal to IMAGE_MIN_DIM==IMAGE_MAX_DIM (when it's not expected for the problem type)    Thanks in advance.    Cheers,    Virilo"
"Hi,  I would like to know how you get the file mask_rcnn_balloon.h5 from your own images, for not to use mask_rcnn_coco.h5 (which includes all the shapes by coco dataset)?  Thanks, and I congratulate you for this work!    A."
"If the label only 20x20, even 10x10 or 5x5 in one image(1000x1000), what should I set in config?"
"This is not an issue but rather a suggestion of improvement.    I saw the publication of a paper in September showing great improvements for Mask R-CNN performance called  . Besides accelerating the training process (29% faster according to the paper), it enables to have better edges and less false positves, and seems not to be complex to implement, starting from the implementation of Mask R-CNN.    Shall we add this as an improvement?        "
"Right now the loss function related to mask contours (mrcnn_mask_loss_graph) seems to be based on binary crossentropy between the ground truth masks and the predicted masks. Would it improve the final IoUs if we incorporate IoU metrics into the loss based on y_true/y_pred in mrcnn_mask_loss_graph? If so, how?"
" `    I just want to simply add a branch for segmentation  but why i always got the error      `    when i dump the layer's shape with ""K.int_shape"". For example, N2, I always get something like [None, None,None,256]. I think this might be the reason? but if I substitute -1 in KL.Reshape with the real number 1048576 or 1024*1024, I still got same error message.  how can I deal with this problem? this almost drive me crazy lol"
"@waleedka   @PaulChongPeng   Hi,    I am not able to change font color in save_image function of visualize.py in mrcnn. Automatically it save the predicted images in white label and red mask. Anyway to change the font color while saving ?"
"Hi.  I did a test with demo.ipynb. And it worked.  But I got a problem with coco.py.  I want to train with coco dataset in 2014.  I followed your procedure like :    # Train a new model starting from pre-trained COCO weights  python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=coco    And I changed a variable, IMAGES_PER_GPU=1.  But, It didn't work.... I don't know Why.... Help....    This is the console.    Using TensorFlow backend.  Command:  train  Model:  coco  Dataset:  ../../train2014  Year:  2014  Logs:  D:\GT\workspace\Mask_RCNN-master\samples\coco\logs  Auto Download:  False  configure printing start-----------------    Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.7  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 1  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                93  IMAGE_MIN_DIM                  800  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'rpn_class_loss': 1.0, 'mrcnn_class_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           coco  NUM_CLASSES                    81  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                1000  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001      Loading weights  D:\GT\workspace\Mask_RCNN-master\samples\coco\mask_rcnn_coco.h5  loading annotations into memory...  Done (t=22.91s)  creating index...  index created!  loading annotations into memory...  Done (t=10.47s)  creating index...  index created!  loading annotations into memory...  Done (t=1.28s)  creating index...  index created!  Training network heads  test0  test1  test2  test3  test4    Starting at epoch 0. LR=0.001    Checkpoint Path: D:\GT\workspace\Mask_RCNN-master\samples\coco\logs\coco20181001T0901\mask_rcnn_coco_{epoch:04d}.h5  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  In model:  rpn_model      rpn_conv_shared        (Conv2D)      rpn_class_raw          (Conv2D)      rpn_bbox_pred          (Conv2D)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)  C:\Users\Ipcl\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  Epoch 1/40    Kernel died, restarting    --------------------------------------------------------------------------------------------------------------    PLEASE Help me...   "
None
"I trained Mask-RCNN over thermal images, but it gives me very bad results with mAP=0.03 and val losses increase continuously.  what should i do to increase mAP?"
"Im currenty working on a project at University, where we are using python + tensorflow and keras to train an image object detector, to detect different kinds of roots that the plant arabidopsis have. Our current ressults are pretty bad but we do only have about 100 images to train the model with at this moment. We are currently working on cultuvating more plants in order to get more images(more data) to train the model.tensorflow.    We are looking to detect three object clases: stem, main root and secondary root. But the model detects main roots incorrectly where the secondary roots located. It whould be able to detect something like this:      What is the usual sample size that is used to train a neural network accurate results?"
Can I retrain the model on custom dataset which has been tagged using labelme tool? If yes how?
"Hey im using balloon.py with the balloon dataset. Is there a way to get the (x,y)-index of a detected objects mask ? Right now im using the `python3 balloon.py splash --weights=/path/to/weights/file.h5 --image= ` command to detect balloons.   Is there a way in tensorflow or keras to get the detected objects masks index as an output?"
"hi， **I initialize backbone (resnet101 or resnext101 or resnet50) using weights obtained from pretraining on the imagenet dataset, but the loss declined slowly.** I choose to train the network for 160k steps on the MS COCO 2014 train dataset with a batch size of 2 on a single GPU machine.The training consists of three stages each lasting for 40k, 80k, 40k steps respectively: in the first stage only the Mask R-CNN branches and not the ResNet backbone are trained. Next, the prediction heads and parts of the backbone (starting at layer 4) are optimized. Finally, in the third stage, the entire model (backbone and heads) is trained together. For the first two training stages we use a learning rate of 0.001 and for the last one a decreased learning rate of 0.001/10. The optimization is done by SGD with momentum set to 0.9 and weight decay set to 0.0001."
"Is there an easy way to measure the contour/perimeter length of a mask using model.detect? Or with something else like visualize? Detect currently gives the area of the mask, but i would also liek to have the contour length. Please and thank you. "
"There are two modes:  training mode and inference mode. Every time, we need to build the model for training and we save ""last"" weights and after that we change the config and we re-build the model for inference and we load weights. I have some questions please,  Why we change the configuration from train to inference ?  What is the difference in the model structure from train to inference ? Why we don't save the model and load it to do the inference task as usual?  It is possible to implement a fit() method for example to train and return the model and after that we get this model and we call a detect or predict function on image test?  Thanks to clarify these points."
"   I noticed that positive ROIs, negative ROIs and zero padding ROIs all contribute to this loss. Should zero padding contributes to the loss? Is it better if we only use positive ROIs, negative ROIs to calculate the loss? Thanks for your suggestion!"
"Hi,    I'm trying to train satellite images on Mask RCNN. I modified and extended the example code of the balloon dataset. The images are grayscale and I also already applied necessary changes as written in the wiki, but I keep getting this error:         You can see the code I wrote here:      What is the problem in my code?"
"Hey when i run the Ballon.py train command"" python balloon.py train --dataset=balloon1 --weights=coco"" in command prompt to train on the balloon dataset i get the following error:     I have tried to change some variables in the ""Mask_RCNN\mrcnn\config.py"" file but i still get the error. Have abyone else encountered this issue?"
"My environment: Geforce 1080TI, 11GB  When I predict an image ,only model.detect spends 250ms-300ms, how I reduce predict time(model.detect) to below 200ms，except use resnet-50 instead of resnet-101?(Input Image is 2048 by 2048, and pre-process will resize it to 1024 by 1024)  FYI, If I use resnet-50 instead of resnet-101, how much is reduced in MAP?"
"Hello, I wanted to draw precision\recall graph, but I have an error.         The notebook lines are:       My config, images are 768x768   "
Hey im trying to train my own dataset with Mask_RCNN but i get the followin framework errors:    I have followed the following github tutorial on Mask R-CNN for object detection:      Can i do something to decrease the memory needed to train the data set? Or how can i solve this problem?
"Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1717, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1219, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""lkk_with_others.py"", line 184, in load_mask      image_info[""width""])    File ""lkk_with_others.py"", line 244, in annToMask      m = maskUtils.decode(rle)    File ""/home/jasonma/.local/lib/python3.6/site-packages/pycocotools-2.0-py3.6-linux-x86_64.egg/pycocotools/mask.py"", line 91, in decode      return _mask.decode([rleObjs])[:,:,0]    File ""pycocotools/_mask.pyx"", line 150, in pycocotools._mask.decode  MemoryError  Traceback (most recent call last):    File ""/home/jasonma/.local/lib/python3.6/site-packages/keras/utils/data_utils.py"", line 678, in _data_generator_task      self.queue.put((True, generator_output))    File "" "", line 2, in put    File ""/usr/lib/python3.6/multiprocessing/managers.py"", line 772, in _callmethod      raise convert_to_error(kind, result)  multiprocessing.managers.RemoteError:   ---------------------------------------------------------------------------  Traceback (most recent call last):    File ""/usr/lib/python3.6/multiprocessing/managers.py"", line 228, in serve_client      request = recv()    File ""/usr/lib/python3.6/multiprocessing/connection.py"", line 251, in recv      return _ForkingPickler.loads(buf.getbuffer())  MemoryError  ---------------------------------------------------------------------------   360/1000 [=========>....................] - ETA: 10:01 - loss: 1.4038 - rpn_class_loss: 0.0164 - rpn_bbox_loss: 0.1168 - mrcnn_class_loss: 0.5269 - mrcnn_bbox_loss: 0.2843 - mrcnn_mask_loss: 0.4594ERROR:root:Error processing image {'id': 20180010804, 'source': 'coco', 'path': '/home/jasonma/dev_sdb/data/mask-rcnn-data/lkk/JPEGImages/5586248_103589_1531625416350_010697E9.jpg', 'width': 3024, 'height': 4032, 'annotations': [{'segmentation': [[600, 1106, 600, 1461, 686, 1461, 686, 1106]], 'area': 30530, 'iscrowd': 0, 'ignore': 0, 'image_id': 20180010804, 'bbox': [600, 1106, 86, 355], 'category_id': 1, 'id': 651486}, {'segmentation': [[2049, 1071, 2049, 1493, 2130, 1493, 2130, 1071]], 'area': 34182, 'iscrowd': 0, 'ignore': 0, 'image_id': 20180010804, 'bbox': [2049, 1071, 81, 422], 'category_id': 1, 'id': 651487}, {'segmentation': [[2363, 541, 2363, 835, 2451, 835, 2451, 541]], 'area': 25872, 'iscrowd': 0, 'ignore': 0, 'image_id': 20180010804, 'bbox': [2363, 541, 88, 294], 'category_id': 1, 'id': 651488}, {'segmentation': [[472, 1014, 472, 1480, 585, 1480, 585, 1014]], 'area': 52658, 'iscrowd': 0, 'ignore': 0, 'image_id': 20180010804, 'bbox': [472, 1014, 113, 466], 'category_id': 1, 'id': 651489}, {'segmentation': [[385, 1018, 385, 1483, 489, 1483, 489, 1018]], 'area': 48360, 'iscrowd': 0, 'ignore': 0, 'image_id': 20180010804, 'bbox': [385, 1018, 104, 465],}    Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1717, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1219, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""lkk_with_others.py"", line 202, in load_mask      mask = np.stack(instance_masks, axis=2).astype(np.bool)    File ""/home/jasonma/.local/lib/python3.6/site-packages/numpy/core/shape_base.py"", line 360, in stack      return _nx.concatenate(expanded_arrays, axis=axis, out=out)  MemoryError  Traceback (most recent call last):    File ""/home/jasonma/.local/lib/python3.6/site-packages/keras/utils/data_utils.py"", line 677, in _data_generator_task      generator_output = next(self._generator)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1717, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1219, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""lkk_with_others.py"", line 202, in load_mask      mask = np.stack(instance_masks, axis=2).astype(np.bool)    File ""/home/jasonma/.local/lib/python3.6/site-packages/numpy/core/shape_base.py"", line 360, in stack      return _nx.concatenate(expanded_arrays, axis=axis, out=out)  MemoryError  Traceback (most recent call last):    File ""lkk_with_others.py"", line 452, in        layers='all')    File ""/usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 2381, in train    File ""/home/jasonma/.local/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""/home/jasonma/.local/lib/python3.6/site-packages/keras/engine/training.py"", line 1415, in fit_generator      initial_epoch=initial_epoch)    File ""/home/jasonma/.local/lib/python3.6/site-packages/keras/engine/training_generator.py"", line 177, in fit_generator      generator_output = next(output_generator)    File ""/home/jasonma/.local/lib/python3.6/site-packages/keras/utils/data_utils.py"", line 793, in get      six.reraise(value.__class__, value, value.__traceback__)    File ""/home/jasonma/.local/lib/python3.6/site-packages/six.py"", line 693, in reraise      raise value  multiprocessing.managers.RemoteError:   ---------------------------------------------------------------------------  Traceback (most recent call last):    File ""/usr/lib/python3.6/multiprocessing/managers.py"", line 228, in serve_client      request = recv()    File ""/usr/lib/python3.6/multiprocessing/connection.py"", line 251, in recv      return _ForkingPickler.loads(buf.getbuffer())  MemoryError  ---------------------------------------------------------------------------    MemoryError comes lot of times, some time it can go on training ,but end it quit       -----------------------  config my triain    class CocoConfig(Config):      """"""Configuration for training on MS COCO.      Derives from the base Config class and overrides values specific      to the COCO dataset.      """"""      # Give the configuration a recognizable name      NAME = ""coco""        # We use a GPU with 12GB memory, which can fit two images.      # Adjust down if you use a smaller GPU.      IMAGES_PER_GPU = 1        # Uncomment to train on 8 GPUs (default is 1)      GPU_COUNT = 1        # Number of classes (including background)      NUM_CLASSES = 1 + 163  # COCO has 80 classes            TRAIN_ROIS_PER_IMAGE = 512      # VALIDATION_STEPS = 75      RPN_ANCHOR_RATIOS = [0.5, 0.75, 2]      DETECTION_MIN_CONFIDENCE = 0.7      DETECTION_MAX_INSTANCES = 400      POST_NMS_ROIS_INFERENCE = 2000    one TiTan X : 12GB   @waleedka @champs @PavlosMelissinos @haeric @rymalia   "
When you run detection using this code:         you face:         Although it does visualize an image (not sure if this is supposed to be the image though) but nevertheless the error is there.     Any thoughts for a fix?
I want to log learning rate in tensorboard. And i am not able to do so by adding lr to metric_names. How to do it.
"Dear all,  For the **Mask-rcnn balloon sample**, now    **My objective:**    Feed the balloon dataset with orientation and  root/handler position annotation info to the modified mask_rcnn and then **it can finally learn how to detect the orientation(in degree) and root position for every balloon in val dataset.**    **Status:**  1. For traning datasets, I used the VIA  to draw a line(from root/handler to the very bottom of each balloon) that can indicate the **orientation** for every balloon instance in an image, because in VIA a line contains 2 points, thus the 1st point in json file will be the root/handler coordinate of each balloon instance.    2. I developed some programs which can :      - get the root coordinate (float value)for all the balloon instance in an image from the json file      - calculate the orientation (angle in degree, float value with 2 digits)by the 2 points of the line for all balloons    **Issues**:  1. I think to send the acquired angle and root position info to the mask rcnn, I need to add them to the      is that correct?    2.  I think i also need to define the loss function for root and orientation in model.py, but the type i gusee i should use the apsame one as the mask loss or box loss,could someone give me some hints?    3. couls someone give me some idea where should i change in the model.py or others?    Many thanks!"
     According to this line it works properly only with batch=1. Or did I misunderstood the comment?
"Hi  I am trying to run the following cell    import warnings  warnings.filterwarnings(""ignore"")  model.train(dataset_train, dataset_val,learning_rate=config.LEARNING_RATE, epochs=NUM_EPOCHS, layers='all',augmentation=augmentation)    And I am getting this error    File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1335, in _do_call  raise type(e)(node_def, op, message)    InvalidArgumentError: indices ]]    Caused by op 'ROI_4/GatherV2_20', defined at:  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\spyder\utils\ipython\start_kernel.py"", line 269, in   main()  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\spyder\utils\ipython\start_kernel.py"", line 265, in main  kernel.start()  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\ipykernel\kernelapp.py"", line 486, in start  self.io_loop.start()  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\tornado\platform\asyncio.py"", line 127, in start  self.asyncio_loop.run_forever()  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\asyncio\base_events.py"", line 422, in run_forever  self._run_once()  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\asyncio\base_events.py"", line 1432, in _run_once  handle._run()  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\asyncio\events.py"", line 145, in _run  self._callback(*self._args)  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\tornado\ioloop.py"", line 759, in _run_callback  ret = callback()  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\tornado\stack_context.py"", line 276, in null_wrapper  return fn(*args, **kwargs)  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 263, in enter_eventloop  self.eventloop(self)  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\ipykernel\eventloops.py"", line 134, in loop_qt5  return loop_qt4(kernel)  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\ipykernel\eventloops.py"", line 122, in loop_qt4  _loop_qt(kernel.app)  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\ipykernel\eventloops.py"", line 106, in loop_qt  app.exec()  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\ipykernel\eventloops.py"", line 39, in process_stream_events  kernel.do_one_iteration()  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 298, in do_one_iteration  stream.flush(zmq.POLLIN, 1)  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\zmq\eventloop\zmqstream.py"", line 357, in flush  self._handle_recv()  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\zmq\eventloop\zmqstream.py"", line 480, in _handle_recv  self._run_callback(callback, msg)  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\zmq\eventloop\zmqstream.py"", line 432, in _run_callback  callback(*args, **kwargs)  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\tornado\stack_context.py"", line 276, in null_wrapper  return fn(*args, **kwargs)  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 283, in dispatcher  return self.dispatch_shell(stream, msg)  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 233, in dispatch_shell  handler(stream, idents, msg)  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 399, in execute_request  user_expressions, allow_stdin)  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\ipykernel\ipkernel.py"", line 208, in do_execute  res = shell.run_cell(code, store_history=store_history, silent=silent)  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\ipykernel\zmqshell.py"", line 537, in run_cell  return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2662, in run_cell  raw_cell, store_history, silent, shell_futures)  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2785, in _run_cell  interactivity=interactivity, compiler=compiler, result=result)  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2909, in run_ast_nodes  if self.run_code(code, result):  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2963, in run_code  exec(code_obj, self.user_global_ns, self.user_ns)  File """", line 1, in   runfile('E:/Spyder/pneumonia22.py', wdir='E:/Spyder')  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 705, in runfile  execfile(filename, namespace)  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile  exec(compile(f.read(), filename, 'exec'), namespace)  File ""E:/Spyder/pneumonia22.py"", line 209, in   model = modellib.MaskRCNN(mode='training', config=config, model_dir=MODEL_DIR)  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\mask_rcnn-2.1-py3.6.egg\mrcnn\model.py"", line 1845, in init  self.keras_model = self.build(mode=mode, config=config)  File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\mask_rcnn-2.1-py3.6.egg\mrcnn\model.py"", line 1973, in build  config=config)( ]]"
"Hi,  I am unable to run the shapes sample, can somebody please provide their weights after training so I can check it out.   Thanks"
"@waleedka Hello, I use tensorflow1.9 + python3.5    When I run the demo, it shows ""No instances to display""         I don't know why. Thanks for help!"
"Dose it make sense to make config value serializable? Not only display in stdout, that would be more convenient to log parameters.    Or is there any suggestion/convention to record parameters?    Thanks."
"!     Now that's indeed the question... is it just resting or is it actually dead? The last commit is over 2 months old and @waleedka's activity has been scarce for quite some time in general. I hope that the reason for this are perhaps a long holiday, a bunch of more interesting things in his life than this repo, that he won the lottery or another merry reason and not that something bad happened.    This repo has been maintained by @waleedka  for quite a long time and he never asked for anything in return. Therefore, I'm very grateful. However, for the very same reason, I think that it would be a lot of wasted potential, if this repo was no longer maintained by anybody. That's why I'd like to discuss the situation - because perhaps some of you might know more than me - and also possible solutions, if this repo was actually dead. After all, there are already quite a number of pull requests which should IMHO be accepted or closed (e.g. #390, #926, #925, #878, #877, #873, #823, #798, #740).     Off the top pf my head I can think of two solutions (given that the repo was actually no longer maintained):  1. @waleedka  assigns somebody as a contributor, so that he/she can accept/close pull requests, etc.. I think somebody who already made some important contributions would be a good choice (e.g. @Borda or @philferriere). Of course that person would also have to have the time to maintain the project.  2. Somebody could mirror this project and maintain the project from there on.    Personally, I would strongly prefer the first solution or even better, that @waleedka is just on holiday and returns to keep up the great work."
how can I find Miss rate for the trained model on my own dataset?   
"hello,  Can Mask RCNN produce a donut-shape mask prediction? I trained my model with donut-shape mask but instead of a full donut, I got 'C' shape mask plus some small pieces and cannot get a full donut.  Thank you,  C"
"When I use model.keras_model.save('test.h5'), some erros prompt.  So how can I save the model structure including weight?"
"Hi everyone,    I am currently using Mask R-CNN with my own dataset and I used the **VIA tool** for my groundtruth data.  I would like to have a bigger dataset using **Imgaug**, but I don't know if it is possible because the algorithm wants annotations with each image. I have never used Imgaug before, so I highly appreciate your advices !    Thanks :-)"
"I have been trying to install Matterport's Mask_RCNN from github in Kaggle kernel for a challenge. It says ""in progress"", then sometimes it goes till ""step 2/2"" and then fails after a while with message ""Failed"". I have tried ""Mask_RCNN"" and ""matterport/Mask_RCNN"" as repo names. Kaggle team suggested to post the issue here. Need help!"
"@waleedka I am getting the TypeError like this.  Can you please give the solution for this error.    dataset_train = DetectorDataset(image_fps_train, image_annotations, ORIG_SIZE, ORIG_SIZE)  Traceback (most recent call last):    File """", line 1, in  dataset_train = DetectorDataset(image_fps_train, image_annotations, ORIG_SIZE, ORIG_SIZE)    File """", line 48, in init  super().init(self)    TypeError: super() takes at least 1 argument (0 given)"
"    File ""C:\Users\SIDDHESHWAR\Anaconda3\lib\site-packages\keras\engine\network.py"", line 317, in __setattr__      'It looks like you are subclassing `Model` and you '    RuntimeError: It looks like you are subclassing `Model` and you forgot to call `super(YourClass, self).__init__()`. Always start with this line."
"Hi, I have encounter one problem. I tried to use 2 gpu to train the network from the beginning. I'm using the coco 2017 datasets which has 117266 training images and 4952 validation images. After having completed one training epoch, it couldn't save the model and outputted    > 100/100 [==============================] - 61s 606ms/step - loss: 3.8918 - rpn_class_loss: 0.0513 - rpn_bbox_loss: 0.6449 - mrcnn_class_loss: 1.3554 - mrcnn_bbox_loss: 0.9415 - mrcnn_mask_loss: 0.8987 - val_loss: 3.2013 - val_rpn_class_loss: 0.0345 - val_rpn_bbox_loss: 0.5427 - val_mrcnn_class_loss: 0.9298 - val_mrcnn_bbox_loss: 0.9336 - val_mrcnn_mask_loss: 0.7607    >Epoch 00001: saving model to /mnt/3d48e0b9-40a9-42ab-95a8-44a0a6d88180/home/terry/Mask-RCNN/Mask_RCNN/logs/coco20180910T1646/mask_rcnn_coco_0001.h5  Traceback (most recent call last):    File ""shibaba.py"", line 70, in        layers=""all"")    File ""/mnt/3d48e0b9-40a9-42ab-95a8-44a0a6d88180/home/terry/Mask-RCNN/Mask_RCNN/mrcnn/model.py"", line 2387, in train      use_multiprocessing=False,    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py"", line 1415, in fit_generator      initial_epoch=initial_epoch)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training_generator.py"", line 247, in fit_generator      callbacks.on_epoch_end(epoch, epoch_logs)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/callbacks.py"", line 77, in on_epoch_end      callback.on_epoch_end(epoch, logs)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/callbacks.py"", line 455, in on_epoch_end      self.model.save(filepath, overwrite=True)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/network.py"", line 1085, in save      save_model(self, filepath, overwrite, include_optimizer)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/saving.py"", line 116, in save_model      'config': model.get_config()    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/network.py"", line 926, in get_config      return copy.deepcopy(config)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 243, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 218, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 243, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 243, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 223, in _deepcopy_tuple      y = [deepcopy(a, memo) for a in x]    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 223, in        y = [deepcopy(a, memo) for a in x]    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 223, in _deepcopy_tuple      y = [deepcopy(a, memo) for a in x]    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 223, in        y = [deepcopy(a, memo) for a in x]    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 297, in _reconstruct      state = deepcopy(state, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 243, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 297, in _reconstruct      state = deepcopy(state, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 243, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 218, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 223, in _deepcopy_tuple      y = [deepcopy(a, memo) for a in x]    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 223, in        y = [deepcopy(a, memo) for a in x]    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 243, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 297, in _reconstruct      state = deepcopy(state, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 243, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/software/anaconda3/envs/carnd-term1/lib/python3.5/copy.py"", line 306, in _reconstruct      y.__dict__.update(state)  AttributeError: 'NoneType' object has no attribute 'update'      Since I have modified the network by adding pooling layer in ROIAlign layer, I can't utilize the transfer learning method. The modified parts are:  1. Pool_size is doubled, for both function fpn_classifier_graph and build_fpn_mask_graph  `x = PyramidROIAlign([2* pool_size, 2*pool_size], name=""roi_align_classifier"")([rois, image_meta] + feature_maps)`   2. I add one pooling layer after bilinear interpolation in ROIAlign. ` for i in range(4):pooled[i] = tf.nn.max_pool(pooled[i], ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1), padding=""VALID"", name=""ROIPool"")`    My question is that WHAT SHOULD I DO TO SAVE MY MODEL AND ITS WEIGHTS?    >versions:  keras 2.2.0  keras-base 2.2.0  python 3.5.2  tensorflow-gpu 1.9.0  CUDA 9.0  cuDNN 7.1.04  "
"Hi~  Thank you for your code.  I have a question in the DetectionTargetLayer code. This layer seems to be used for finding the target classes/boxes/masks for the ground truth. However, I am a little confused that why not just directly use the groundtruth classes/boxes/masks to get the losses? It seems that this layer is not mentioned in the MRCNN paper. Also, since we can only obtain the groundtruth in the training mode, so the detections might not be the same without the DetectionTargetLayer when testing (for example, more detections might be proposed because rpn_rois from the RPN are not selected by the IOU)."
"I get this error whenever I try to execute the following code:  Kindly notice that my coding experience is not big.  ``import cv2      capture = cv2.VideoCapture(0)        # these 2 lines can be removed if you dont have a 1080p camera.      capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)      capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)        while True:          ret, frame = capture.read()          results = model.detect([frame], verbose=0)          r = results[0]          frame = display_instances(              frame, r['rois'], r['masks'], r['class_ids'], class_names, r['scores']          )          cv2.imshow('frame', frame)          if cv2.waitKey(1) & 0xFF == ord('q'):              break        capture.release()      cv2.destroyAllWindows()  ---------------------------------------------------------------------------  AttributeError                            Traceback (most recent call last)    in  ()        8 while True:        9     ret, frame = capture.read()  ---> 10     results = model.detect([frame], verbose=0)       11     r = results[0]       12     frame = display_instances(    ~\Desktop\Mask_RCNN-master\samples\model.py in detect(self, images, verbose)     2508      2509         # Mold inputs to format expected by the neural network  -> 2510         molded_images, image_metas, windows = self.mold_inputs(images)     2511      2512         # Validate image sizes    ~\Desktop\Mask_RCNN-master\samples\model.py in mold_inputs(self, images)     2406                 min_scale=self.config.IMAGE_MIN_SCALE,     2407                 max_dim=self.config.IMAGE_MAX_DIM,  -> 2408                 mode=self.config.IMAGE_RESIZE_MODE)     2409             molded_image = mold_image(molded_image, self.config)     2410             # Build image_meta    ~\Anaconda3\envs\tensorflow\lib\site-packages\mask_rcnn-2.1-py3.6.egg\mrcnn\utils.py in resize_image(image, min_dim, max_dim, min_scale, mode)      427     """"""      428     # Keep track of image dtype and return results in the same dtype  --> 429     image_dtype = image.dtype      430     # Default window (y1, x1, y2, x2) and default scale == 1.      431     h, w = image.shape[:2]    AttributeError: 'NoneType' object has no attribute 'dtype' ``"
I have a large dataset where roughly half the images have masks and half contain no masks (i.e. contain no instances of the target object). What is the correct way to use the large number of negative images? Exclude them from the training? Thank you. 
"When I ran demo.ipynb, I got ""ModuleNotFoundError: No module named 'mrcnn'"" in line 15, ""from mrcnn import utils"" .   I tried the solution in   which is running ""python setup.py install"", but that error still exists.   I checked my packages and I'm sure that I have already installed mrcnn."
"Hello everyone,    First a big thanks for this incredible work!!! I am really impressed by the segmentation part!  I am trying to use this implementation of MASK-RCNN on unbalanced dataset. I would have liked to use 'class_weight' parameter in the fit_generator() function from the mrcnn/model.py file.  But, I think this wont be correct, as It should not be influencing the segmentation network for example. Hence, I am a bit confused, how to use during training only a weight value for each class in the loss function when doing the detection?    I hope to have some help here :) in any case thank you for all!"
Can I use gender recognition based on the whole body?
"when I ran the first cell of demo.py in samples..the downloading started and got an error message:  An existing connection was forcibly closed by the remote host""  Then running the cell again didnt caused any error.  but encountered some problem later.I am now not able to load the model.  The mask_rcnn_coco.h5 file downloaded to my root only had 120 mb.  Any idea why i am not able load this file?"
"i am having some issue due to which my PC turns off during training, and according to tutorial i ama trying to load the last trained model to continue the training but the following error occured.    error: Unrecognized arguments: --model=last    while running both of these commands (i am working on person data set)    python person.py train --dataset=/path/to/dataset --model=last --weights=coco  python person.py train --dataset=/path/to/dataset --model=last   "
"I want to run test code on `mrcnn/parallel_model.py` that has been updated following the PR #710, but still get the error:    >Traceback (most recent call last):  File ""/home/forestriveral/DeepLearning/Keras/Mask_RCNN/mrcnn/parallel_model_modified.py"", line 174, in     model = ParallelModel(model, GPU_COUNT)    File ""/home/forestriveral/DeepLearning/Keras/Mask_RCNN/mrcnn/parallel_model_modified.py"", line 35, in __init__    merged_outputs = self.make_parallel(keras_model, gpu_count)    File ""/home/forestriveral/DeepLearning/Keras/Mask_RCNN/mrcnn/parallel_model_modified.py"", line 72, in make_parallel    output_names = inner_model.output_names  AttributeError: 'Model' object has no attribute 'output_names'    Ubuntu 16.04:  Tensorflow_gpu = 1.9  Keras version = 2.2.2    Any thoughts?   "
There are some errror:         When training on cityscapes dataset.  Any idea why this happen?
"Hello, I met this questions about plot_PR,this is my  !   and this is others:  !   why my PR is so stright and other's circle? "
"If i do random augmentation like padding, do i need to change data label (coordinates)?    `sometimes(iaa.CropAndPad(              percent=(-0.05, 0.1),              pad_mode=ia.ALL,              pad_cval=(0, 255)          )),`    `model.train(dataset_train, dataset_val,                  learning_rate=config.LEARNING_RATE,                  epochs=30,                  augmentation=augmentation,                  layers='heads')`"
"Help me! Beginner here. when I first run the MaskRCNN demo.py - I face at lot of module no found errors; I solved most of them and I am a bit stuck here! I tried every resolution I could find online even reinstalled the pycocotools but it seems that anything I do doesn't fix it. Please help!    import mrcnn.model as modellib  import os  import sys  import random  import math  import numpy as np  import skimage.io  import matplotlib  import matplotlib.pyplot as plt  ​  # Root directory of the project  ROOT_DIR = os.path.abspath(""../"")  ​  # Import Mask RCNN  sys.path.append(ROOT_DIR)  # To find local version of the library  from mrcnn import utils  import mrcnn.model as modellib  from mrcnn import visualize  # Import COCO config  sys.path.append(os.path.join(ROOT_DIR, ""\samples\coco/""))  # To find local version  import coco  ​  %matplotlib inline   ​  # Directory to save logs and trained model  MODEL_DIR = os.path.join(ROOT_DIR, ""logs"")  ​  # Local path to trained weights file  COCO_MODEL_PATH = os.path.join(ROOT_DIR, ""mask_rcnn_coco.h5"")  # Download COCO trained weights from Releases if needed  if not os.path.exists(COCO_MODEL_PATH):      utils.download_trained_weights(COCO_MODEL_PATH)  ​  # Directory of images to run detection on  IMAGE_DIR = os.path.join(ROOT_DIR, ""images"")  ---------------------------------------------------------------------------  ModuleNotFoundError                       Traceback (most recent call last)    in  ()       18 # Import COCO config       19 sys.path.append(os.path.join(ROOT_DIR, ""\samples\coco/""))  # To find local version  ---> 20 import coco       21        22 get_ipython().run_line_magic('matplotlib', 'inline')    f:\Anaconda\Mask_RCNN\samples/coco\coco.py in  ()       40 # If the PR is merged then use the original repo.       41 # Note: Edit PythonAPI/Makefile and replace ""python"" with ""python3"".  ---> 42 from pycocotools.coco import COCO       43 from pycocotools.cocoeval import COCOeval       44 from pycocotools import mask as maskUtils    ModuleNotFoundError: No module named 'pycocotools'"
"Hello ,Now i miss a error when i used Jupyter, but i change it with .py not an error. This is erorr information.I hope anybody solve it for me. Thank you!  My env is :   tensorflow                         1.4.0                 tensorflow-tensorboard             0.4.0       Keras                              2.1.0   ---------------------------------------------------------------------------  AssertionError                            Traceback (most recent call last)    in  ()       16 sys.path.append('/home/yu/Mask_RCNN/mrcnn')  # To find local version of the library       17 from mrcnn import utils  ---> 18 import mrcnn.model as modellib       19 from mrcnn import visualize       20 # Import Maritime config    ~/Mask_RCNN/mrcnn/model.py in  ()       29 # Requires TensorFlow 1.3+ and Keras 2.0.8+.       30 from distutils.version import LooseVersion  ---> 31 assert LooseVersion(tf.__version__) >= LooseVersion(""1.3"")       32 assert LooseVersion(keras.__version__) >= LooseVersion('2.0.8')       33     AssertionError:   "
Will I have to annotated **keypoints** and mask separately if I want to make a **custom dataset**? I am so confused about the annotation. I have found many annotation tools which are used to annotate masks using **polygons** . Will only annotation using polygons work for keypoints detection or I will have to annotate keypoints too?
"Hi,  I used surgery.py file for running mask rcnn on two classes. The file can be found on the link below:       This surgery robot detection method uses two classes ""arm"" and ""ring"". I just replaced the two class name with my class name ""yes"" and ""no"" in the surgery.py file. But i start to train my model. It gives me the following error.    mrcnn_mask (TimeDistributed)  /home/hdfsf16/.conda/envs/mask/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py100 UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  /home/hdfsf16/.conda/envs/mask/lib/python3.5/site-packages/keras/engine/training.py:1987: UserWarning: Using a generator with use_multiprocessing=True and multiple workers may duplicate your data. Please consider using thekeras.utils.Sequence class. UserWarning('Using a generator withuse_multiprocessing=True`'  ERROR:root:Error processing image {'width': 1280, 'id': '15691.png', 'names': [{'type': 'yes'}], 'source': 'type', 'height': 1024, 'polygons': [{'all_points_y': [647, 648, 657, 657, 669, 666, 657, 652, 640, 615, 619, 641, 647], 'name': 'polygon', 'all_points_x': [661, 676, 686, 700, 695, 658, 651, 651, 653, 647, 658, 660, 661]}], 'path': 'data/surgery/train/15691.png'}  Traceback (most recent call last):  File ""/home/hdfsf16/.conda/envs/mask/Surgery-Robot-Detection-Segmentation-master/Surgery-Robot-Detection-Segmentation-master/mrcnn/model.py"", line 1696, in data_generator  use_mini_mask=config.USE_MINI_MASK)  File ""/home/hdfsf16/.conda/envs/mask/Surgery-Robot-Detection-Segmentation-master/Surgery-Robot-Detection-Segmentation-master/mrcnn/model.py"", line 1210, in load_image_gt  mask, class_ids = dataset.load_mask(image_id)  File ""surgery.py"", line 164, in load_mask  class_names = info[""type""]  KeyError: 'type'  ERROR:root:Error processing image {'width': 1280, 'id': '2103.png', 'names': [{'type': 'yes'}], 'source': 'type', 'height': 1024, 'polygons': [{'all_points_y': [661, 655, 651, 645, 639, 648, 657, 661], 'name': 'polygon', 'all_points_x': [657, 669, 685, 669, 650, 646, 646, 657]}], 'path': 'data/surgery/train/2103.png'}  Traceback (most recent call last):  File ""/home/hdfsf16/.conda/envs/mask/Surgery-Robot-Detection-Segmentation-master/Surgery-Robot-Detection-Segmentation-master/mrcnn/model.py"", line 1696, in data_generator  use_mini_mask=config.USE_MINI_MASK)  File ""/home/hdfsf16/.conda/envs/mask/Surgery-Robot-Detection-Segmentation-master/Surgery-Robot-Detection-Segmentation-master/mrcnn/model.py"", line 1210, in load_image_gt  mask, class_ids = dataset.load_mask(image_id)  File ""surgery.py"", line 164, in load_mask  class_names = info[""type""]  KeyError: 'type"
"when i run “inspect_balloon_model.py” with my own trained model (even with the author‘s model “mask_rcnn_balloon.h5”)，the detection boxes couldn't display correctly，in fact, the detection boxes are messy，what is wrong？  "
"I want to detect a line in an image, but I find Mask RCNN can only generate a bounding box, the generated mask is completely zeros. Why?"
"hi everybody  i used demo.ipynb and When I run the program, the image that the output shows each time with a different and random color, for example, indicates the person in the image once in blue and again in violet. I want to display the same image every time in the program, for example, a person with a constant color and the colors are not random.That is, all dogs are shown with a specific color. All cats with a color and ...  Can anyone help me?"
"I haven't run Mask R-CNN for a few months, rewrote some code today. The model initializes successfull, but when the masks start loading, something happens. The load_mask method finds the corresponding pickled file, unpickles into an array and returns it:             The error I'm getting is that `image_id` is NoneType:         I'm very much at loss here: hacking into `model.py` didn't help, and I don't see the loop over which it is iteratred. Any help?   "
I am using 2 classes to train : background(class_id=0) & object class(class_id=1). My code is similar to balloons.py code. The object gets detected well but the problem is background also generates masks and it gets predicted as class_id=1. I trained the model for ~50 epochs. All the background images which don't have any object generate masks. How can I solve this issue? @waleedka Can this be resolved by training for more epochs/what have I done wrong?
"I am currently adapting this code to be able to handle images with 5 classes in them. After training, only a single class is being identified at all. I am not exactly sure why only one class is being identified. If the input_gt_class_ids is a vector of 5 integers representing the classes, only the last one is being identified (ie, if input_gt_class_ids  = [1, 2, 3, 4, 5], only class 1 is being identified). Has anyone come across this problem? It seems to be a pretty fundamental bug in my code, as no matter what I change the outputs are still like it."
"I was wondering if if would be easy, or even possible to apply this for 3D volumes?  Any quick tips, ideas to share?"
None
"Hi !  Thank you very much for a really great tool, I've just implemented training on my dataset and results were surprisingly good.  But only there is a subtle issue: when the training process runs for a long time in Jupyter, results often not dumped where they should ( more precisely they are dumped, but I see no easy law to predict when it happens ).  Hence 'the find_last method' sometimes finds an empty folder, because the last checkpoint results really could be empty. But some weights along the way may be available.    I've written a method to save weights by mirroring the functionality of a 'load_weights' method and it seems to work fine.  I'm pretty certain, that I did this in vain, but I see no simple way to do it when I already have the trained model.  If you indeed know how to do it then could you please reflect this in the documentation?  If not, then shall I do a PR?    Sincerely,  Alexey Miasnikov"
Hi     Could you mention how to perform prediction on a batch of images ?  Thanks. 
"Hi, my dataset format VOC-2007 and i have 6 class. How can i convert my own dataset to COCO format or Can i train Mask RCNN with my own VOC-2007 dataset? Please help me. Thanks..."
"I'm finally done training on my own dataset and i'd like to extract the features of each mask for thousands of images. I thought of re-purposing the code from the CrowdAI mapping challenge notebook and adding a line to it, but it calculates the area of all the masks in one image. Essentially, all class_ids per image show the same area.      here's the line that i'm adding:     _mask[""area""] = int(np.reshape(r['masks'], (-1, r['masks'].shape[-1])).astype(np.float32).sum())    to this code:    for files in tqdm.tqdm(ALL_FILES):      images = [skimage.io.imread(x) for x in files]      predoctions = model.detect(images, verbose=0)      for _idx, r in enumerate(predoctions):          _file = files[_idx]          image_id = os.path.basename(_file)          for _idx, class_id in enumerate(r[""class_ids""]):              if class_id == 1:                  mask = r[""masks""].astype(np.uint8)[:, :, _idx]                  bbox = np.around(r[""rois""][_idx], 1)                  bbox = [float(x) for x in bbox]                  _result = {}                  _result[""image_id""] = image_id                  _result[""category_id""] = 1                  _result[""score""] = float(r[""scores""][_idx])                  _mask = maskUtils.encode(np.asfortranarray(mask))                  _mask[""area""] = int(np.reshape(r['masks'], (-1, r['masks'].shape[-1])).astype(np.float32).sum())                  _mask[""counts""] = _mask[""counts""].decode(""UTF-8"")                  _result[""segmentation""] = _mask                  _result[""bbox""] = [bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0]]                  _final_object.append(_result)              if class_id == 2:                  mask = r[""masks""].astype(np.uint8)[:, :, _idx]                  bbox = np.around(r[""rois""][_idx], 1)                  bbox = [float(x) for x in bbox]                  _result = {}                  _result[""image_id""] = image_id                  _result[""category_id""] = 2                  _result[""score""] = float(r[""scores""][_idx])                  _mask = maskUtils.encode(np.asfortranarray(mask))                  _mask[""area""] = int(np.reshape(r['masks'], (-1, r['masks'].shape[-1])).astype(np.float32).sum())                  _mask[""counts""] = _mask[""counts""].decode(""UTF-8"")                  _result[""segmentation""] = _mask                  _result[""bbox""] = [bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0]]                  _final_object.append(_result)              if class_id == 3:                  mask = r[""masks""].astype(np.uint8)[:, :, _idx]                  bbox = np.around(r[""rois""][_idx], 1)                  bbox = [float(x) for x in bbox]                  _result = {}                  _result[""image_id""] = image_id                  _result[""category_id""] = 3                  _result[""score""] = float(r[""scores""][_idx])                  _mask = maskUtils.encode(np.asfortranarray(mask))                  _mask[""area""] = int(np.reshape(r['masks'], (-1, r['masks'].shape[-1])).astype(np.float32).sum())                  _mask[""counts""] = _mask[""counts""].decode(""UTF-8"")                  _result[""segmentation""] = _mask                  _result[""bbox""] = [bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0]]                  _final_object.append(_result)    fp = open(""predictions.json"", ""w"")  import json  print(""Writing JSON..."")  fp.write(json.dumps(_final_object))  fp.close()      After this, i save it json. and while it looks similar to an annotation.json dataset file, it lacks segmentation_ids. I'm sorry if this is an amateur question. I've been looking for a solution for days and have stopped making progress. "
"I'm facing issues when using this repo with **TF 1.4.1 and Keras 2.0.8.**    In order to make the training phase work I have to change :  - `K.int_shape(x) to K.shape(x)` line 967 in `model.py  - `loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target_class_ids, logits=pred_class_logits)` to   `new = tf.transpose(target_class_ids)  new = tf.reshape(new, [1, -1])  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=new, logits=pred_class_logits)`    In order to make inference phase work I have to change :   `indices = tf.stack([tf.range(probs.shape[0]), class_ids], axis=1)` to   `indices = tf.stack([tf.range(config.POST_NMS_ROIS_INFERENCE), class_ids], axis=1)` line 716.    This modification in refine_detections_graph allows me to avoid this error : `""Cannot convert an unknow Dimension to a Tensor: %s"" % d)`"", but when I use `inspect_ablloon_model `notebook, with `mask_rcnn_ballon.h5 `weights, network doesn't make prediction.  Any idea why the network doesn't not predict bbox/segmentation ?  Is this related to modifications I did cause of my TF version ?    Thanks in advance,"
"Why it take one more seconds to detect an image?  I run a demo, in a for loop:    for i in range(10):          file_names = next(os.walk(IMAGE_DIR))[2]          image = skimage.io.imread(os.path.join(IMAGE_DIR, random.choice(file_names)))          start_time = time.clock()          results = model.detect([image], verbose=1)          end_time = time.clock()          print(""time  :   "", end_time - start_time)    The test images are random chooses from coco dataset.  The first image always take 4-5 seconds, I consider it was woke up GPU,  but the other images also need 1-2 seconds, Why?     My GPU is Geforce GTX 1080 Ti, BACKBONE = ""resnet101"".    Processing 1 images  image                    shape: (394, 640, 3)         min:    0.00000  max:  255.00000  uint8  molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  150.10000  float64  image_metas              shape: (1, 93)               min:    0.00000  max: 1024.00000  float64  anchors                  shape: (1, 261888, 4)        min:   -0.35390  max:    1.29134  float32  time  :    4.908735999999999  Processing 1 images  image                    shape: (425, 640, 3)         min:    0.00000  max:  255.00000  uint8  molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  151.10000  float64  image_metas              shape: (1, 93)               min:    0.00000  max: 1024.00000  float64  anchors                  shape: (1, 261888, 4)        min:   -0.35390  max:    1.29134  float32  time  :    1.8078380000000003  Processing 1 images  image                    shape: (491, 640, 3)         min:    0.00000  max:  255.00000  uint8  molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  151.10000  float64  image_metas              shape: (1, 93)               min:    0.00000  max: 1024.00000  float64  anchors                  shape: (1, 261888, 4)        min:   -0.35390  max:    1.29134  float32  time  :    1.7254550000000002  Processing 1 images  image                    shape: (480, 640, 3)         min:    0.00000  max:  255.00000  uint8  molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  151.10000  float64  image_metas              shape: (1, 93)               min:    0.00000  max: 1024.00000  float64  anchors                  shape: (1, 261888, 4)        min:   -0.35390  max:    1.29134  float32  time  :    1.8081569999999978    The paper mentioned it only take 200ms.     "
"Hi!    I am using Mask RCNN to segment cells and nuclei. I have a dataset consisting of 600 images of nuclei (from the kaggle challenge) and 20 images of nuclei + cells.    I have reorganized the dataset into coco format to support >1 class, and began training. I have expected that the model will be able to detect nuclei as good as the model with ""only nuclei"" training set, but it failed on both cells and nuclei.    I am therefore wondering, if it is possible, that the small amount of cells could impact the accuracy on nuclei?    Thanks in advance!"
"Hi all,  I am very new to python kaggle and github so sorry for any inconvenience. I have posted this there on   too with no luck so trying now here.    I am trying to run inspect_nucleus_model.ipynb I got till Run Detection but there I have the following output:       Thanks  Ugur"
"Hi all  I am going through the code, (that works wonderfully btw, thumbs up to the Author) in an attempt to better understand it.  I am suing the pre-trained model. After reading the relevant documents I expect that the Non Maximal Suppression function model->nms_keep_map, or model->ProposalLayer->call->nms will be used to remove overlapping BB's.  I also expect to see the segmentation- where a BB feature map will be fed to a Network, classifying each pixel as Background or Foreground. This is supposed to happen in model->build_fpn_mask_graph  I have placed breakpoints in both, but they are visited only during model loading, and not during prediction/detection as I would expect.   Can someone please tell me what I am missing here?  Thanks!"
"Hello all,    I am currently using the provided pre-trained resnet 101 backbone. It works very good. However, I am attending visual objects detected in the image with textual phrases in order to localize textual phrases in images. So, I need much more detected objects. At least some important ones like trees, streets...    Otherwise, my attention function doesn't even have the chance to attend most of the image regions, as 90% of the image is classified as background. I currently have enough computing power to train a CNN, but a pre-trained CNN with more than 80 classes would be more helpful, as I do not have much time left for my masters thesis.     Can anyone give me some hints, to solve this problem?    Thank you very much for your help!!"
"Guys thank you for this project, this repo has a very good API!!  I followed ""Splash of Color: Instance Segmentation with Mask R-CNN "" tutorial and implemented for my own dataset.  My trained model runs in 13 seconds on CPU(i7 8 cores) for one image.  I would like it to work 3-4 seconds.  I have some ideas how to make this work:  1.Compile tensorflow for current process architecture.  2.Use different backbone resnet50 or mobilenet2 (   3.Use smaller images size(How to do this?  will pretraining work with different sizes?)  4.NN pruning  Is there other thinks I could do?  Is is achievable tasks?  Thanks in advance.  "
"Hello,  First of all I would like to thank you for this amazing project and I am curios if is possible to remove some classes from the default project trained on coco. For example, I would like to remove dinning table and books from prediction without retraining everything.    Thank you!"
"I am having issues training on my own dataset, I have 4 classes and the images are 640 x 640 which are then padded up to 1024 using ""square"".   the problem is that i have adjusted the Backbone strides and the RPN Anchor scales to suit my data, this is made up of large objects approx 300-500 pixels square.  once the network is trained i cannot evaluate it, when i use inspect model it tells me that i have 9666 anchors and i get this error message.  ---------------------------------------------------------------------------  InvalidArgumentError                      Traceback (most recent call last)    in  ()        5                                        dataset.image_reference(image_id)))        6 # Run object detection  ----> 7 results = model.detect([image], verbose=1)        8         9 # Display results    /host/Mask_RCNN/mrcnn/model.py in detect(self, images, verbose)     2529         # Run object detection     2530         detections, _, _, mrcnn_mask, _, _, _ =\  -> 2531             self.keras_model.predict([molded_images, image_metas, anchors], verbose=0)     2532         # Process detections     2533         results = []    /usr/local/lib/python3.5/dist-packages/keras/engine/training.py in predict(self, x, batch_size, verbose, steps)     1165                                             batch_size=batch_size,     1166                                             verbose=verbose,  -> 1167                                             steps=steps)     1168      1169     def train_on_batch(self, x, y,    /usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py in predict_loop(model, f, ins, batch_size, verbose, steps)      292                 ins_batch[i] = ins_batch[i].toarray()      293   --> 294             batch_outs = f(ins_batch)      295             batch_outs = to_list(batch_outs)      296             if batch_index == 0:    /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py in __call__(self, inputs)     2664                 return self._legacy_call(inputs)     2665   -> 2666             return self._call(inputs)     2667         else:     2668             if py_any(is_tensor(x) for x in inputs):    /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py in _call(self, inputs)     2634                                 symbol_vals,     2635                                 session)  -> 2636         fetched = self._callable_fn(*array_vals)     2637         return fetched[:len(self.outputs)]     2638     /usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in __call__(self, *args)     1452         else:     1453           return tf_session.TF_DeprecatedSessionRunCallable(  -> 1454               self._session._session, self._handle, args, status, None)     1455      1456     def __del__(self):    /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)      517             None, None,      518             compat.as_text(c_api.TF_Message(self.status.status)),  --> 519             c_api.TF_GetCode(self.status.status))      520     # Delete the underlying status object from memory otherwise it stays alive      521     # as there is a reference to status from this from the traceback due to    InvalidArgumentError: indices[0] = 62091 is not in [0, 9666)    my config looks like this:  Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [13, 19, 25, 32, 38]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.7  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 1  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                16  IMAGE_MIN_DIM                  800  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'mrcnn_bbox_loss': 1.0, 'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           raspberry  NUM_CLASSES                    4  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.75, 1, 1.5]  RPN_ANCHOR_SCALES              (208, 304, 400, 512, 608)  RPN_ANCHOR_STRIDE              2  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.9  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                20  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           40  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001    any help greatly appreciated"
"Hello,  During prediction I observe multiple overlapping bounding boxes. But I get non-maximum suppression is applied. How do I eliminate bounding boxes?    Thanks"
"hello, did anyone know that can mask rcnn used on OHEM method to improve accuracy?"
"I noticed that after (modellib.mold_image(image, Config), the  dtype=uint8 disappears from the image array. When I plot the mold_image, it gives very weird color. Does this mean that I should give the model a float image?"
What will I need to edit in visualize.py to get the outputted image w/ masks and text saved to an image file (.jpeg) instead of showing immediately? 
"Training doesn't proceed after warnings below. Program continues to run, but i can't see my network training. Does it generally take >2 hours to construct the graph. I am using a Titan Xp GPU on Ubuntu.         "
how to count objects that detected and print it on image or video  
"When I finished training my own dataset, I can't do anything to compute the accuracy as well as the AP of my running.How can I work it out?  Thanks for your attention."
How to plot the losses result with Tensorboard?
"I've had success training on objects which fill in large portions of their bounding boxes. For long, thin, diagonal objects that only fill in a small portion of their bonding boxes, segmentation doesn't work as well.    Are there any strategies I could use to help segment these types of objects? My entire dataset is already diagonal objects, so I don't think using augmentation to rotate the objects would help in this case.       appears to have a similar issue.     ! "
Tested successuful on Linux but not on Mac. Not sure if it's tensorflow issue because I made the change to async according to   to get tensorflow working but met the following error while running demo notebook.    UnboundLocalError: local variable 'self' referenced before assignment
"Hi, I am fine-tuning Resnet 5+ layers on my own dataset, and set the layers to 120. But when it comes to training, the system output yields epoch numbers, ""mrcnn_box_losses"" and ""losses"" with respect to 1000 epochs, which is not set by me. May I know how these 1000 epochs are correlated with the 120 epochs for the model.train()? Thanks."
"hi, when I loaded all the files down, I can run the balloon demo to detect balloon correctly.  But, when I want to train a new model with the given balloon data set according to the steps given, there was an error:  Layer #389 (named ""mrcnn_bbox_fc""), weight   has shape (1024, 8), but the saved weight has shape (1024, 324)    Can some one help me?  I used the mask_rcnn_coco.h5 as the pretrained model.    python3 balloon.py train --dataset=/opt/projects/samples/balloon/balloonImages/datasets/ --weights=/opt/projects/samples/balloon/mask_rcnn_coco.h5   Using TensorFlow backend.  Weights:  /opt/projects/samples/balloon/mask_rcnn_coco.h5  Dataset:  /opt/projects/samples/balloon/balloonImages/datasets/  Logs:  /opt/projects/logs_balloon       Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     2  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.9  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 2  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                14  IMAGE_MIN_DIM                  800  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           balloon  NUM_CLASSES                    2  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                100  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001      Loading weights  /opt/projects/samples/balloon/mask_rcnn_coco.h5     Traceback (most recent call last):    File ""balloon.py"", line 357, in        model.load_weights(weights_path, by_name=True)    File ""/opt/projects/mrcnn/model.py"", line 2140, in load_weights      reshape=False)    File ""/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py"", line 1017, in load_weights_from_hdf5_group_by_name      str(weight_values[i].shape) + '.')  ValueError: Layer #389 (named ""mrcnn_bbox_fc""), weight   has shape (1024, 8), but the saved weight has shape (1024, 324)."
This error happens when I tried to use the pre-trained model to perform inference mode on my custom dataset.     Anyone knows how to solve this?
Hi @waleedka !    Thanks for the great implementation of mask rcnn.    I have a little question - what is the shortest way to add weighting to different components of the loss? I want to give a bigger weight for class loss.
"Hi,  first of all, thank you for your great work!     I have my own dataset, which is very similar to the nucleus dataset (but i have 2 classes: nuclei, cell). I have converted it to ms coco format (with json file etc) and am using a combination of code for coco with some changes from nucleus.    I would like to train the model on both my data and the kaggle nucleus dataset, but I am not sure how should I fill below lines of config:        IMAGE_RESIZE_MODE = ""square""  IMAGE_MIN_DIM = 800  IMAGE_MAX_DIM = 1024  IMAGE_MIN_SCALE = 0    In the kaggle datasets there are images of size around 300x300, but my datasets contains images 1392x1040. Does that mean that I should use IMAGE_MIN_DIM = 200 and IMAGE_MAX_DIM = 1392? What image_resize_mode should be good for images with that much variation in size?    Also, I don't understand fully the IMAGE_MIN_SCALE, the clarification from config.py did not help:  `Minimum scaling ratio. Checked after MIN_IMAGE_DIM and can force further up scaling. For example, if set to 2 then images are scaled up to double the width and height, or more, even if MIN_IMAGE_DIM doesn't require it. Howver, in 'square' mode, it can be overruled by IMAGE_MAX_DIM.`    I would be very grateful for any tips or links that will help me to understand how these dims work.     Thanks!"
hi..I trained the model on multiple object classes.After running detection on a test image it shows no instances to display..can anyone suggest a solution please. 
When I run a last cell in the demo file in the sample folder I get a long waiting time and dead kernel. I have a mask_rcnn_coco.h5 file in my Mask_RCNN folder and everything seems to be alright except the dead kernel.
"Hello,    I manage to run the package correctly with GPU_COUNT = 1 and any count of IMAGES_PER_GPU.  However, when I try to set GPU_COUNT >=1 and use multiple GPU's it produces the following error (it's long I'm attaching only the last part):    ...  InvalidArgumentError (see above for traceback): Input to reshape is a tensor with 600 values, but the requested shape has 1200     ]]     Can you please assist me ?    Tensorflow version: 1.9.0    Many thanks!  Noam"
is it possible to calculate confusion matrix using coco evaluate?  I would like to know for each detection in the image what is the GT label?
"I train on images with 70 car damages, so some images have very small and thin objects (stains, key scratches, etc) and the others are quite large and wide (bumper scuffs). I only have one class type ('damage' + 'BG').    As I increase the train data, it looks like my model is getting better but mrcnn_class_loss is very unstable all the time and I don't understand why. All other graphs looks good though.   Any ideas what's going on here?  !   "
"Hi, I have a question about 'input_gt_masks'. For mask branch we got 28*28 feature map when using deconvolution. Then we use it to compare with label. But how should we deal with mask target to get 28*28 label？"
Execution of detection or prediction on unseen dataset yields no bounding mask. However if I take sample from train/validation set for some images bounds are detected. What could be reason - is it overfitting or lack of training samples or anchors needs to be adjusted?    Thanks
#What are the simplified steps that a new one can understand the working of Mask RCNN? How can we visualize with a single flowchart/diagram?
"I noticed lately that whenever I load an existing model for inference, a new empty directory is created inside given `model_dir`.  For example, the following simple snippet will create a new folder named with current timestamp:         However, this behaviour is undesirable in inference mode, because what one would normally do next is to call `model.load_weights()` on some existing model weight file contained in its own `log_dir`.    By looking at the code, it appears that `MaskRCNN.set_log_dir()` creates a new `log_dir` if one doesn't exist already. However, since it looks for a file path containing `datetime.datetime.now()`, it will most likely not find it (unless a model was created within the same minute).    Perhaps, this might be solved in `MaskRCNN.set_log_dir()` by conditioning the creation of `self.log_dir` and `self.checkpoint_path` on `self.mode == ""training""`; and then disabling the possibility to train if not in training mode, so that `self.log_dir` and `self.checkpoint_path` would never come into play."
"It would be good to have differential learning rate, that is, assigning different learning rates to different stages of the training process.    Best Regards."
For this model the compilation stage of the code does not seem to consider the mutli-task loss function required. It instead seems to evaluate both separately. Is there any reason why they are done separately and the multi-task objective function is not considered?
"Hi,everyone.   I was wondering how to download 5K minival from .json file, do I need to download the whole coco datasets  and the .json could take use of some of them, or is there a way to just download the mini patch?  Can somebody give me a clue, than you."
"In the DetectionTargetLayer class in the model code, you calculate the deltas based on the randomly sampled positive proposals and the ground truths with the best IoU for those proposals. I was wondering why it was done like this as it mixes the proposed coordinates and the ground truth ones. In the paper the coordinates are not mixed like this, instead the deltas between the anchors and ground truth bounding boxes are calculated [t*_y, t*_x, t*_h, t*_w]. Separately, the deltas to go between the anchors and the proposed bounding boxes are calculated [t_y, t_x, t_h, t_w]. I am curious as to the reasoning to calculate deltas that don't take into account the anchors.    "
"I would like to add custom classes (like the sample adding Ballon) while retain the original 80 classes COCO already has. For example, after re-training with Ballon dataset with pre-trained COCO model, how can I do object detection on classes like Balloon, Person, Cat, Car?        "
"Is it can real-time? If not, can you tell me how fast did test an image? And about image , Is the image size can affect detection time?"
"I have been going through the code, mainly focusing on the RPN part of it. I am a little confused about the deltas. Mainly, what are they? You are calling the outputs of the RPN deltas for bounding boxes, but should it not just output the bounding boxes themselves (ie, output [x,y,w,h] rather than [dx,dy,log(dw), log(dh)]?     Also, in the first step of the ProposalLayer you multiple the deltas with some standard deviation. I am a little confused as to why this is being done. It may relate to my lack of understanding on the purpose of the deltas in the first place. Any clarification would be much appreciated. "
"I made the json file according to the json in balloon dataset, it looks like this:       And when I trained, it would stuck at the last step of the first epoch. Please be aware that when I train on balloon data, that never happened. It stucked so long that I cannot see any progress even in the next day's morning. And if I check the GPU and CPU usage, I found no GPU is used while GPU memory is almost full. And cpu is also almost full.    Here is part of the err message if I pressed Ctrl + C:         I spent so much time on this point that I really wanted to give up. If anyone have any idea about this, I would be appreciate."
I was able to train model based on mrcnn. I need to crop the masked part of image. Any suggestion how can I do that?
"I want to concatenete some data to the matrices after the region proposal is done (to  ). I have a separate code that needs to read data from rois and perform a few matrix operations and return a new matrix that needs to be used for classification and masks.    If I call my function by passing rois as an argument, how do I read data from the tensor?  "
"I trained my model successfully and now I am trying to predict on images. I am specifying ""IMAGES_PER_GPU"" in InferenceConfig class. Till 32 ""IMAGES_PER_GPU"" there is no error but after that, it's giving me the error below. I checked some issues on tensorflow repository and I think it might be related to the input that is being passed. Also, the below error is giving an error in line 828 of utils.py    **inputs_slice =  ]]     ]]    During handling of the above exception, another exception occurred:    InvalidArgumentError                      Traceback (most recent call last)    in  ()        4     for img_id in batch_id :        5         img_batch.append(skimage.io.imread(os.path.join(resizedImageDir, filenames ]]     ]]    Caused by op 'ROI_1/strided_slice_228', defined at:    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py"", line 193, in _run_module_as_main      ""__main__"", mod_spec)    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py"", line 85, in _run_code      exec(code, run_globals)    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py"", line 3, in        app.launch_new_instance()    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py"", line 658, in launch_instance      app.start()    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py"", line 486, in start      self.io_loop.start()    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py"", line 132, in start      self.asyncio_loop.run_forever()    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py"", line 422, in run_forever      self._run_once()    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py"", line 1434, in _run_once      handle._run()    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/events.py"", line 145, in _run      self._callback(*self._args)    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/ioloop.py"", line 758, in _run_callback      ret = callback()    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py"", line 300, in null_wrapper      return fn(*args, **kwargs)    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 536, in        self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 450, in _handle_events      self._handle_recv()    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 480, in _handle_recv      self._run_callback(callback, msg)    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 432, in _run_callback      callback(*args, **kwargs)    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py"", line 300, in null_wrapper      return fn(*args, **kwargs)    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 283, in dispatcher      return self.dispatch_shell(stream, msg)    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 233, in dispatch_shell      handler(stream, idents, msg)    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 399, in execute_request      user_expressions, allow_stdin)    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py"", line 208, in do_execute      res = shell.run_cell(code, store_history=store_history, silent=silent)    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py"", line 537, in run_cell      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2662, in run_cell      raw_cell, store_history, silent, shell_futures)    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2785, in _run_cell      interactivity=interactivity, compiler=compiler, result=result)    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2903, in run_ast_nodes      if self.run_code(code, result):    File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2963, in run_code      exec(code_obj, self.user_global_ns, self.user_ns)    File "" "", line 10, in        model_dir=MODEL_DIR)    File ""/home/ubuntu/crack_prediction/asphalt/bin/Mask_RCNN/mrcnn/model.py"", line 1832, in __init__      self.keras_model = self.build(mode=mode, config=config)    File ""/home/ubuntu/crack_prediction/asphalt/bin/Mask_RCNN/mrcnn/model.py"", line 1960, in build      config=config)( ]]     ]]"
"Hi, I encountered a problem loading masks for the model. I know that there are limits that crop the image to be smaller than IMAGE_MAX_DIM, which is 1024 in config.py, however, there are bounding boxes in my ground truth data that have coordinates larger than 1024 that are not processed to stay in the range of 1024, causing the error of “index 1024 is out of bounds for axis 0 with size 1024” in the log file. Are there any ways that can deal with this kind of situation? Thanks so much."
I think we need such a config variable
"I am doing the programming on Linux    I just changed some configs in the balloon.py and used it to train my own data.    I thought it was just a warning, but it does not continue any longer :(    here is the detailed log         Can saomeone help me?"
"For the image of size [256, 256, 3], the RPN_ANCHOR_SCALES is set as (8, 16, 32, 64, 128).   The feature scale of each level is:   P2: [64, 64]  P3: [32, 32]  P4: [16, 16]  P5: [8, 8]  P6: [4, 4]    a 50×50 ROI (in pixels) maps to:       log2((50/256) / (224/256)) ~= -2.2      min(5, max(2, 4-2)) = 2  so the roi_level is P2.    However, each scale is associated with a level of the pyramid. 32(P4) < 50 < 64(P5). So why does the 50×50 ROI not belong to P4?    Thanks for any help!"
"Hi. I trained the model with my own dataset and it's working fine. During inference, some objects are filtered out because of low confidence. When I lower the DETECTION_MIN_CONFIDENCE parameter, the remaining objects are indeed detected, but sometimes overlapping bounding boxes/masks are located in the same object in the image. These overlapping masks are not filtered by NMS because they're from different classes (labels), i.e., the network is predicting the same object, with almost exactly the same mask, but assigning different labels to those predictions.    I was wondering if there is a way to apply NMS for different classes too (in my case, it is rare to find overlapping objects of different classes in the ground truth images)."
"When I training Mask RCNN use  python3 samples/coco/coco.py train --dataset=mypath --model=coco  after train 7th epoch, I got an error:  floating  point exception(core dumped)"
"Hi, when I am trying to call compute_ap method in utils,   def compute_ap(gt_boxes, gt_class_ids, gt_masks,                 pred_boxes, pred_class_ids, pred_scores, pred_masks,  iou_threshold=0.5):  I am not sure what the pred_scores should be, I've tried r['scores'] and other scores, but those do not seem to be the correct ones. Could you please give some advice on how to find or generate pred_scores? Also, I am trying to calculate the Intersection over Union of predicted masks and ground truth masks, but failed to use the cocoEval.evaluate() because I am using the balloon model, is there any other ways to calculate the IoU of masks?   Thanks so much!"
I am inspecting my model using the  . And the bounding boxes plotted are not aligned with the image.  Here is an screenshot:    !     Can anybody help me how to fix it?  Thank you.  
"I want only outline segmentation without using Keras, it is possible with only tobject detection in research/object detection ? because deeplab is image complete segmentation, in object detection we see the bounding bow and i think mask overlap complete detected object.  but here with nucleus example i can see only outline. it is very important and i have answer nowhere !    thanks !"
I am observing that model tends to return 99% confidence level in most results (not all) even if results are not accurate. The distribution look squeezed to the 0.99 What can be the reason? And how to avoid that?
"When I run the first part of the demo, I get the following error:      Traceback (most recent call last):    File ""demo.py"", line 16, in        import mrcnn.model as modellib    File ""/data/harryyhw/Mask_RCNN/mrcnn/model.py"", line 31, in        assert LooseVersion(tf.__version__) >= LooseVersion(""1.3"")  AttributeError: module 'tensorflow' has no attribute '__version__'    How do I fix this? "
"@waleedka     I just red   mentioning that adding an MLP to the very end of the mask branch gave better results  than FCN left alone. I would like to do so, I guess on `build_fpn_mask_graph()` function yet I am not sure about how to do it.    Do you have any idea?    Thank you !"
"I'm trying to train my own data, and i'm having this message. how can i solve it ?      Traceback (most recent call last):      File "" "", line 1, in        runfile('D:/Users/user_m/Mask/Mask_RCNN/usine.py', args='train --dataset=""D:/Users/user_m/Mask/Mask_RCNN/data/usine/"" --weights=D:/Users/user_m/Mask/Mask_RCNN/mask_rcnn_coco.h5  --image=D:/Users/user_m/Mask/Mask_RCNN/data/usine/predict --subset=D:/Users/user_m/Mask/Mask_RCNN/data/usine/val', wdir='D:/Users/user_m/Mask/Mask_RCNN')      File ""D:\Users\user_m\AppData\Local\Continuum\anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 705, in runfile      execfile(filename, namespace)      File ""D:\Users\user_m\AppData\Local\Continuum\anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile      exec(compile(f.read(), filename, 'exec'), namespace)      File ""D:/Users/user_m/Mask/Mask_RCNN/usine.py"", line 448, in        model_dir=args.logs)      File ""D:\Users\user_m\Mask\Mask_RCNN\mrcnn\model.py"", line 1845, in __init__      self.keras_model = self.build(mode=mode, config=config)      File ""D:\Users\user_m\Mask\Mask_RCNN\mrcnn\model.py"", line 1870, in build      shape=[None, 1], name=""input_rpn_match"", dtype=tensorflow.int32)      File ""D:\Users\user_m\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\input_layer.py"", line 176, in Input      input_tensor=tensor)      File ""D:\Users\user_m\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\legacy\interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)      File ""D:\Users\user_m\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\engine\input_layer.py"", line 85, in __init__      name=self.name)      File ""D:\Users\user_m\AppData\Local\Continuum\anaconda3\lib\site-packages\keras\backend\theano_backend.py"", line 246, in placeholder      x = T.TensorType(dtype, broadcast)(name)      File ""D:\Users\user_m\AppData\Local\Continuum\anaconda3\lib\site-packages\theano\tensor\type.py"", line 51, in __init__      self.dtype_specs()  # error checking is done there      File ""D:\Users\user_m\AppData\Local\Continuum\anaconda3\lib\site-packages\theano\tensor\type.py"", line 272, in dtype_specs      % (self.__class__.__name__, self.dtype))    TypeError: Unsupported dtype for TensorType:  "
"Is normalization (or some similar algorithm) a pre-processing step on images fed into mask_rcnn? If not, do you think this will make my model perform bad on transfer learning due to new images being very matte?"
"HI,   I have two questions   1. how can i get the files 'via_region_data.json', i'm having this error: No such file or directory User\...\via_region_data.json'    2. i'm trying to train my own data but i'm having the following error: TypeError: Unsupported dtype for TensorType:      Can anyone help ?  Thanks"
"Hi,  When I ran the demo.ipynb notebook no GPU memory is utilized. What changes do I need to make to run it in GPU?    Thanks "
"Hi, I am trying to use a bootleg adam optimizer by just calling different runs to model.train using decreasing learning rates in each call to model.train. (I used epochs=1 for testing purposes)       After this call, the checkpoint log prints `Checkpoint Path: /home/logs/cells20180717T1057/mask_rcnn_cells_{epoch:04d}.h5`  and continues to train and logs the model with these lines:      This is all happening within the first call to model.train above and the time print statement says one epoch took about 300 seconds.    The problem arises during the next model.train call. It logs the same `Checkpoint Path: /home/logs/cells20180717T1057/mask_rcnn_cells_{epoch:04d}.h5` line, but then it does not print any statement about the epoch number or loss values. Instead it continues immediately to the third call to model.train which also does not show any signs of training. Both the second and third calls to model.train only took about 30 seconds, which is significantly shorter than the time it took to run the first call so I suspect that the model is not training at all during the second and third calls to model.train.    My script does not crash and I haven't changed the `set_log_dir` function in model.py. I have only modified verbosity modes. I know that I can just add an adam optimizer to the keras model but I would also like to run additional calls to model.train but using `layers='heads'` instead of 'all'    Has anyone else had this issue? Thanks."
"Hello all,    I would like to get the indexes of the bounding box proposals which have been kept after the non max suppression. The network is currently only providing (y1, x1, y2, x2, class_id, class_score) in the final detection layer.    However, I would like to know the keep index of that bounding box proposals (?, 1000, 1024) as well, as I will later process the 1024 dimensional vectors of the detections.    I would be very pleased for any help!    Thank you very much and best regards"
"I trained my own data use Mask RCNN。Now, I want to calculate mask area through pixel. How to do that? "
None
"Following the balloons read me, I downloaded the weightfile and dataset and moved them to the recommended directories. I then tried to run the following command from the root directory:          Initially I get the error ""ModuleNotFoundError: No module named 'mrcnn'"" as mentioned in a previous issue. So I replaced the ""../../"" in balloon.py to the absolute path name for the root directory.     I am now getting the following error:      "
"We’re using Mask RCNN for pixelwise cloud segmentation in whole-sky infrared camera images.  The camera is a ground-based infrared imaging system which points straight up towards the sky and captures images on a regular interval.     Currently we’re only using a single object class (cloud) and all other pixels are considered to be background.  We have training/validation/test ground-truth masks which classify each pixel as either cloud or not-cloud/background.  We’re using COCO pre-trained weights and we’re using the same approach that the Nucleus detection example uses (train the network heads for 20 epochs, then continue training all layers for N epochs).     The ground-truth masks are fairly detailed, in some cases very small groupings of pixels are classified as clouds.  This results in ground-truth clouds with edges which are not always well-defined as the clouds sometimes have thinner edges and they may taper off in some cases.    When we evaluate the model, the detected cloud-masks are very, for lack of a better phrase, rounded off.  The cloud edges aren’t quite as intricate and detailed as the ground-truth masks.     With the overall goal of increasing the fine-detail and accuracy of the detected masks, here are my questions:    (1) If we adjusted our training schedule, could we possibly get more accurate detection results?      (2) Are there any training or detection configuration parameters which may affect the accuracy of the detected masks?  Or, perhaps parts of the model code that could be adjusted to get better accuracy?    (3) Any other advice or things to investigate which may help us?    Thank you all for your help and thanks to Matterport for this wonderful tool."
"I have one memory issue when I trained the shapes model of samples, however, I expanded the memory, reduced the max_queue_size, IMAGE_PER_GPU, numbers of images, everything I can think of might be the problem. Still not work.    Here is the error message:     `    And the packages are strictly follow the requirements(first time I tried the latest versions failed):  numpy  scipy  Pillow  cython  matplotlib  scikit-image  tensorflow>=1.3.0  keras>=2.0.8  opencv-python  h5py  imgaug    Please, anybody, tell me what is going wrong!  Thanks in advance!"
"hi,thansk for your code.I have a question about demo.py,If I have a folder,and there are a lot of pictures under this folder and I want to detect all of them, but One by one picture to detect a bit slow. so can you show me how to do it? thanks so much."
Does anyone try to use groupnormalization with Mask_RCNN
"The detection_targets_graph function description states that the variable ""deltas"" has an output shape (TRAIN_ROIS_PER_IMAGE, NUM_CLASSES, (dy, dx, log(dh), log(dw)).     However, when I check the shape of ""deltas"", it has a shape (TRAIN_ROIS_PER_IMAGE, (dy, dx, log(dh), log(dw)). Is there a reason why this is happening ?    Is this because the program assigns only one ground truth bbox to each proposal ? According to the original paper, every proposal must have a bbox refinement for each class.   "
"I am training on a dataset of 19 images. Also, I am doing validation on 17 images.  Time training 19 images = 30 seconds.  Time validating 17 images = 200 seconds.  Why is this happening? Domains is completly similar.    Each image has about 100 masks. Yes, its alot. Maskrcnn is probably not made to handle so many, but I am trying to fix just that.    I see that during validation, it spend alot of time computing overlaps, its a bottleneck. But I dont understand why during training it does not consider this to be a bottle neck, why? "
"Dear All and @waleedka ,  I've been using this repo to detect and create masking for crack damage on bridge structures. My training dataset has 850 images and overall I got decent result. As you can see in two images below, the model can detect and segment horizontal and vertical crack well. But it fails in detecting diagonal crack. So, I'm thinking that I could solve this problem if I have more images.  !   !   I noticed that we could use image augmentation in the training stage using the keyword option _augmentation_ as follow        model.train(dataset_train, dataset_val,                  learning_rate=config.LEARNING_RATE,                  epochs=30,                  layers='all',                   augmentation = imgaug.augmenters.Sequential([                       imgaug.augmenters.Fliplr(1),                       imgaug.augmenters.Flipud(1),                       imgaug.augmenters.Affine(rotate=(-45, 45)),                       imgaug.augmenters.Affine(rotate=(-90, 90)),                       imgaug.augmenters.Affine(scale=(0.5, 1.5))]))    However, from what I understand, this augmentation is applied consecutively to each image. In other words, for each image, the augmentation apply flip LR, and then followed by flip UD, then followed by rotation of -45 and 45, then followed by another rotation of -90 and 90, and lastly followed by scaling with factor 0.5 and 1.5.     So, my question is, **Is there a way to apply each augmentation separately for each image?** What I meant by this is, I want each augmentation to generate one extra data (and mask) alongside with the original. If this can be achieved, the augmentation will generate 6x total images when I apply 5 image augmentation making the whole dataset contains 5100 images.     Thank you and I really appreciate the helps."
I am able to get instance annonations  of minival2014 dataset from   but i need minival2014 image dataset. 
Here are losses plots with training in blue and validation in red.    !     My validation dataset represents 15% of the entire dataset. I also tried with 25%. I am using the coco dataset to init the parameters of the model and `resnet50` as a backbone.    My dataset only contains one label. Here are two examples of images coming from the dataset:     !     I am training on `heads` with a LR of 0.001. I also tried to train on `4+` with a LR of `0.0001` with the same result.    Did someone also encounter this issue of increasing validation losses? It looks like the network is overfitting. Any suggestions are welcome.
"I'm using 4k images, but Mask RCNN performs better if I reduce the image size. This is surprising because my objects are relatively small.    Does anyone have any configuration recommendations, please? For example, do I need to change anything about the anchors? Do I need more anchors? Do I need to change their size? etc."
I want to detect object like license plate by using parallelogram， how to detect the four keypoints？
"had anyone has the mask rcnn with resnet50 backbone pretrained weight model trained on coco dataset?? if do have, pls tell me the download link or the release model page,tks~      By Simon"
"Hello everyone,    I wonder if someone has trained Mask R-CNN for the CrowdAI mapping challenge, and I am very curious about the results, yet my computer is too weak to make such an end-to-end training.  Did you have good results? Can you make the trained weights available to work on improvement?    Many thanks !    "
"To get more accurate masks, I want to set MASK_SHAPE=[input image size, input_image_size] in config.py. Then  what changes should I do in codes?"
What are the hyperparameters to look at that have the most influence on decreasing inference time without affecting detection performance too drastically?
"Hello, I'm trying to apply mrcnn to satellite imagery. I'm inspecting the data for errors since I was getting poor training results (I've already made adjustments for the fact that my inputs have 8 channels instead of 3.) When I use the inspect data notebook on my dataset, I see that a lot of images have no objects (over 100 out of 469). But in the nucleus example, all training images have at least one object. I think an issue I'm having is that I am not correctly encoding the mask for negative samples with no objects.    Here is my data inspection notebook with example images:      For each image that has no field, I have an empty mask of shape (262, 262, 1) that is all zeros, same shape as the image except for the channels. This would be different from a mask with one field which would have 1s for fields and 0s for no field.   `mrcnn.visualize.display_instances()` works on images with at least 1 field but fails for images with no fields. the reason is that `load_image_gt()` in mrcnn/model.py changes the shape of the masks with no objects from (262, 262, 1) to (262, 262, 0) in this step:      This then makes mask an empty array, with no zeros to indicate the background class, causing display to fail and possibly the model training to fail (still not sure if this is what causes training issues but I think it is likely)    Any suggestions on how to encode sample masks with only the background class, no objects?        "
"Hello,  I wanted to ask where to best provide the ground truth bounding boxes one might have for occluded masks where we have access to the full un-occluded masks such that the classifier can predict the bounding boxes during inference for occluded objects more precisely.    "
"hello everyone,    I am currently trying to change the mask loss for weighted bce dice loss available here :      To make it compatible with this implementation, I took the same gathering of prediction and ground truth masks as follows :            weighted_bce_loss and weighted_dice_loss being exactly the same as on the link above.    And I changed in model.py --> class MaskRCNN -->build :     `mask_loss` = KL.Lambda(lambda x: mrcnn_mask_loss_graph(*x), name=""mrcnn_mask_loss"")([target_mask, target_class_ids, mrcnn_mask])`    To     `mask_loss = KL.Lambda(lambda x: weighted_bce_dice_loss(*x), name=""mrcnn_mask_loss"")([target_mask, target_class_ids, mrcnn_mask])`      Yet I get the following error :     `ValueError: Shape must be rank 4 but is rank 3 for 'mrcnn_mask_loss/AvgPool' (op: 'AvgPool') with input shapes: [?,?,?].`    Does anyone know what to do to figure it out? I feel a bit lost..    Thank you!"
"Hello!    Thank you to all the contributors of this project; I really appreciate having open-source tools as great as this one everyone can use.     After training, I ran through  , and noticed my model never predicted anything.     More specifically, after running `results = model.detect([image], verbose=1)`,   my results variable was `{'rois': array([], shape=(0, 4), dtype=int32),   'class_ids': array([], dtype=int32),   'scores': array([], dtype=float32),   'masks': array([], shape=(1024, 1024, 0), dtype=float64)}`    So I decided to inspect my weights (the output is pasted below. I notice many of the weights are dead, and I have no idea how to even begin debugging that. I trained my model again a second time to make sure it wasn't something random, and got the same dead weights. Does anyone know how to fix/go about debugging this? What more information would be useful to debug?    Thank you!      WEIGHT NAME | SHAPE | MIN | MAX | STD  -- | -- | -- | -- | --  conv1/kernel:0 | (7, 7, 3, 64) | -0.6710 | +0.7043 | +0.1111  conv1/bias:0 | (64,) | -0.0000 | +0.0000 | +0.0000  bn_conv1/gamma:0 | (64,) | +0.5126 | +2.6686 | +0.4648  bn_conv1/beta:0 | (64,) | -2.6540 | +6.3536 | +1.8954  bn_conv1/moving_mean:0 | (64,) | -3.5374 | +3.3343 | +1.0033  bn_conv1/moving_variance:0*** Overflow? | (64,) | +184.1078 | +83614.7344 | +13554.5430  res2a_branch2a/kernel:0 | (1, 1, 64, 64) | -0.7175 | +0.3922 | +0.0714  res2a_branch2a/bias:0 | (64,) | +0.0000 | +0.0000 | +0.0000  bn2a_branch2a/gamma:0 | (64,) | +0.5092 | +2.0661 | +0.3737  bn2a_branch2a/beta:0 | (64,) | -2.4109 | +3.6075 | +1.1673  bn2a_branch2a/moving_mean:0 | (64,) | -4.6250 | +7.6156 | +2.1027  bn2a_branch2a/moving_variance:0 | (64,) | +0.0522 | +8.7720 | +1.4478  res2a_branch2b/kernel:0 | (3, 3, 64, 64) | -0.3900 | +0.3638 | +0.0303  res2a_branch2b/bias:0 | (64,) | +0.0000 | +0.0000 | +0.0000  bn2a_branch2b/gamma:0 | (64,) | +0.4204 | +2.5295 | +0.3371  bn2a_branch2b/beta:0 | (64,) | -2.2855 | +5.9126 | +1.4382  bn2a_branch2b/moving_mean:0 | (64,) | -4.0114 | +2.9984 | +1.3228  bn2a_branch2b/moving_variance:0 | (64,) | +0.1224 | +21.4742 | +3.0398  res2a_branch2c/kernel:0 | (1, 1, 64, 256) | -0.3974 | +0.3476 | +0.0396  res2a_branch2c/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  res2a_branch1/kernel:0 | (1, 1, 64, 256) | -0.7720 | +0.9004 | +0.0555  res2a_branch1/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn2a_branch2c/gamma:0 | (256,) | +0.0110 | +2.8200 | +0.6065  bn2a_branch2c/beta:0 | (256,) | -1.1255 | +1.5218 | +0.4352  bn2a_branch2c/moving_mean:0 | (256,) | -2.0083 | +1.8371 | +0.6858  bn2a_branch2c/moving_variance:0 | (256,) | +0.0000 | +2.1370 | +0.3869  bn2a_branch1/gamma:0 | (256,) | +0.0044 | +3.0644 | +0.6813  bn2a_branch1/beta:0 | (256,) | -1.1255 | +1.5218 | +0.4352  bn2a_branch1/moving_mean:0 | (256,) | -6.6483 | +9.7145 | +1.5846  bn2a_branch1/moving_variance:0 | (256,) | +0.0000 | +7.3274 | +1.2134  res2b_branch2a/kernel:0 | (1, 1, 256, 64) | -0.2969 | +0.2203 | +0.0345  res2b_branch2a/bias:0 | (64,) | +0.0000 | +0.0000 | +0.0000  bn2b_branch2a/gamma:0 | (64,) | +0.7462 | +1.9490 | +0.2805  bn2b_branch2a/beta:0 | (64,) | -1.6881 | +1.5779 | +0.7924  bn2b_branch2a/moving_mean:0 | (64,) | -2.5128 | +1.1116 | +0.6401  bn2b_branch2a/moving_variance:0 | (64,) | +0.2289 | +2.1671 | +0.4634  res2b_branch2b/kernel:0 | (3, 3, 64, 64) | -0.2401 | +0.3183 | +0.0322  res2b_branch2b/bias:0 | (64,) | +0.0000 | +0.0000 | +0.0000  bn2b_branch2b/gamma:0 | (64,) | +0.6205 | +1.6177 | +0.2136  bn2b_branch2b/beta:0 | (64,) | -2.0027 | +2.3983 | +1.0999  bn2b_branch2b/moving_mean:0 | (64,) | -1.3383 | +0.9408 | +0.4840  bn2b_branch2b/moving_variance:0 | (64,) | +0.1663 | +1.1179 | +0.2229  res2b_branch2c/kernel:0 | (1, 1, 64, 256) | -0.2402 | +0.2796 | +0.0373  res2b_branch2c/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn2b_branch2c/gamma:0 | (256,) | -0.0169 | +2.1301 | +0.5133  bn2b_branch2c/beta:0 | (256,) | -1.7113 | +1.2913 | +0.3883  bn2b_branch2c/moving_mean:0 | (256,) | -1.0901 | +0.9931 | +0.3158  bn2b_branch2c/moving_variance:0 | (256,) | +0.0003 | +0.1778 | +0.0336  res2c_branch2a/kernel:0 | (1, 1, 256, 64) | -0.2099 | +0.2637 | +0.0351  res2c_branch2a/bias:0 | (64,) | +0.0000 | +0.0000 | +0.0000  bn2c_branch2a/gamma:0 | (64,) | +0.5741 | +1.6880 | +0.2447  bn2c_branch2a/beta:0 | (64,) | -1.8764 | +1.0904 | +0.8106  bn2c_branch2a/moving_mean:0 | (64,) | -1.9883 | +1.6709 | +0.7214  bn2c_branch2a/moving_variance:0 | (64,) | +0.0000 | +2.5057 | +0.3889  res2c_branch2b/kernel:0 | (3, 3, 64, 64) | -0.2180 | +0.2013 | +0.0317  res2c_branch2b/bias:0 | (64,) | +0.0000 | +0.0000 | +0.0000  bn2c_branch2b/gamma:0 | (64,) | +0.7570 | +1.6493 | +0.2084  bn2c_branch2b/beta:0 | (64,) | -2.2206 | +1.8783 | +0.6046  bn2c_branch2b/moving_mean:0 | (64,) | -1.3461 | +0.2783 | +0.2443  bn2c_branch2b/moving_variance:0 | (64,) | +0.1355 | +0.8499 | +0.1394  res2c_branch2c/kernel:0 | (1, 1, 64, 256) | -0.2752 | +0.3500 | +0.0360  res2c_branch2c/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn2c_branch2c/gamma:0 | (256,) | -0.0585 | +2.1540 | +0.5583  bn2c_branch2c/beta:0 | (256,) | -1.5696 | +1.5349 | +0.4400  bn2c_branch2c/moving_mean:0 | (256,) | -1.0839 | +1.0337 | +0.2750  bn2c_branch2c/moving_variance:0 | (256,) | +0.0005 | +0.3150 | +0.0604  res3a_branch2a/kernel:0 | (1, 1, 256, 128) | -0.3339 | +0.3003 | +0.0302  res3a_branch2a/bias:0 | (128,) | +0.0000 | +0.0000 | +0.0000  bn3a_branch2a/gamma:0 | (128,) | +0.6103 | +1.6424 | +0.2388  bn3a_branch2a/beta:0 | (128,) | -1.5789 | +1.4490 | +0.6198  bn3a_branch2a/moving_mean:0 | (128,) | -2.9416 | +1.6888 | +0.7045  bn3a_branch2a/moving_variance:0 | (128,) | +0.1753 | +2.5666 | +0.3677  res3a_branch2b/kernel:0 | (3, 3, 128, 128) | -0.3837 | +0.3767 | +0.0224  res3a_branch2b/bias:0 | (128,) | +0.0000 | +0.0000 | +0.0000  bn3a_branch2b/gamma:0 | (128,) | +0.6049 | +1.6216 | +0.2346  bn3a_branch2b/beta:0 | (128,) | -2.7685 | +1.7469 | +0.7927  bn3a_branch2b/moving_mean:0 | (128,) | -4.0939 | +1.9894 | +0.9729  bn3a_branch2b/moving_variance:0 | (128,) | +0.2301 | +2.1462 | +0.3635  res3a_branch2c/kernel:0 | (1, 1, 128, 512) | -0.3739 | +0.4344 | +0.0293  res3a_branch2c/bias:0 | (512,) | +0.0000 | +0.0000 | +0.0000  res3a_branch1/kernel:0 | (1, 1, 256, 512) | -0.4660 | +0.6425 | +0.0267  res3a_branch1/bias:0 | (512,) | +0.0000 | +0.0000 | +0.0000  bn3a_branch2c/gamma:0 | (512,) | -0.0070 | +2.7299 | +0.5396  bn3a_branch2c/beta:0 | (512,) | -1.5448 | +1.2565 | +0.3735  bn3a_branch2c/moving_mean:0 | (512,) | -1.0744 | +1.0758 | +0.2540  bn3a_branch2c/moving_variance:0 | (512,) | +0.0007 | +0.4447 | +0.0556  bn3a_branch1/gamma:0 | (512,) | +0.0055 | +2.5519 | +0.4483  bn3a_branch1/beta:0 | (512,) | -1.5448 | +1.2565 | +0.3735  bn3a_branch1/moving_mean:0 | (512,) | -2.3782 | +2.7351 | +0.6031  bn3a_branch1/moving_variance:0 | (512,) | +0.0027 | +4.0631 | +0.5343  res3b_branch2a/kernel:0 | (1, 1, 512, 128) | -0.1618 | +0.1947 | +0.0235  res3b_branch2a/bias:0 | (128,) | +0.0000 | +0.0000 | +0.0000  bn3b_branch2a/gamma:0 | (128,) | +0.5785 | +1.4288 | +0.1442  bn3b_branch2a/beta:0 | (128,) | -4.3484 | +0.5885 | +0.6330  bn3b_branch2a/moving_mean:0 | (128,) | -2.0957 | +1.3789 | +0.5864  bn3b_branch2a/moving_variance:0 | (128,) | +0.0000 | +2.3962 | +0.4263  res3b_branch2b/kernel:0 | (3, 3, 128, 128) | -0.1759 | +0.1772 | +0.0212  res3b_branch2b/bias:0 | (128,) | +0.0000 | +0.0000 | +0.0000  bn3b_branch2b/gamma:0 | (128,) | +0.5108 | +1.7942 | +0.2506  bn3b_branch2b/beta:0 | (128,) | -3.8255 | +1.3434 | +0.5701  bn3b_branch2b/moving_mean:0 | (128,) | -1.4255 | +1.1129 | +0.3043  bn3b_branch2b/moving_variance:0 | (128,) | +0.1766 | +0.7534 | +0.1141  res3b_branch2c/kernel:0 | (1, 1, 128, 512) | -0.3443 | +0.3358 | +0.0253  res3b_branch2c/bias:0 | (512,) | +0.0000 | +0.0000 | +0.0000  bn3b_branch2c/gamma:0 | (512,) | -0.0725 | +2.1215 | +0.4311  bn3b_branch2c/beta:0 | (512,) | -1.5019 | +1.1662 | +0.3798  bn3b_branch2c/moving_mean:0 | (512,) | -0.6253 | +0.4826 | +0.1396  bn3b_branch2c/moving_variance:0 | (512,) | +0.0004 | +0.1537 | +0.0256  res3c_branch2a/kernel:0 | (1, 1, 512, 128) | -0.3298 | +0.3685 | +0.0195  res3c_branch2a/bias:0 | (128,) | +0.0000 | +0.0000 | +0.0000  bn3c_branch2a/gamma:0 | (128,) | +0.4060 | +1.6956 | +0.1883  bn3c_branch2a/beta:0 | (128,) | -2.6959 | +1.9445 | +0.7213  bn3c_branch2a/moving_mean:0 | (128,) | -1.8667 | +1.5206 | +0.5616  bn3c_branch2a/moving_variance:0 | (128,) | +0.1836 | +11.5327 | +1.6813  res3c_branch2b/kernel:0 | (3, 3, 128, 128) | -0.3258 | +0.3736 | +0.0189  res3c_branch2b/bias:0 | (128,) | +0.0000 | +0.0000 | +0.0000  bn3c_branch2b/gamma:0 | (128,) | +0.4601 | +2.1788 | +0.2544  bn3c_branch2b/beta:0 | (128,) | -1.5868 | +0.5887 | +0.5735  bn3c_branch2b/moving_mean:0 | (128,) | -1.4784 | +1.7390 | +0.5389  bn3c_branch2b/moving_variance:0 | (128,) | +0.0802 | +1.8846 | +0.3461  res3c_branch2c/kernel:0 | (1, 1, 128, 512) | -0.2876 | +0.2320 | +0.0210  res3c_branch2c/bias:0 | (512,) | +0.0000 | +0.0000 | +0.0000  bn3c_branch2c/gamma:0 | (512,) | -0.0055 | +3.0426 | +0.5922  bn3c_branch2c/beta:0 | (512,) | -2.3689 | +0.4396 | +0.3694  bn3c_branch2c/moving_mean:0 | (512,) | -0.5233 | +0.5001 | +0.1118  bn3c_branch2c/moving_variance:0 | (512,) | +0.0001 | +0.2193 | +0.0325  res3d_branch2a/kernel:0 | (1, 1, 512, 128) | -0.2975 | +0.3463 | +0.0326  res3d_branch2a/bias:0 | (128,) | +0.0000 | +0.0000 | +0.0000  bn3d_branch2a/gamma:0 | (128,) | +0.7355 | +2.3945 | +0.2799  bn3d_branch2a/beta:0 | (128,) | -2.6433 | +0.7561 | +0.6374  bn3d_branch2a/moving_mean:0 | (128,) | -1.5133 | +1.3112 | +0.5186  bn3d_branch2a/moving_variance:0 | (128,) | +0.3950 | +2.4953 | +0.4500  res3d_branch2b/kernel:0 | (3, 3, 128, 128) | -0.2722 | +0.2084 | +0.0231  res3d_branch2b/bias:0 | (128,) | +0.0000 | +0.0000 | +0.0000  bn3d_branch2b/gamma:0 | (128,) | +0.6817 | +1.6945 | +0.2585  bn3d_branch2b/beta:0 | (128,) | -1.3648 | +1.5992 | +0.7058  bn3d_branch2b/moving_mean:0 | (128,) | -1.3848 | +1.6231 | +0.4467  bn3d_branch2b/moving_variance:0 | (128,) | +0.2501 | +1.4870 | +0.2550  res3d_branch2c/kernel:0 | (1, 1, 128, 512) | -0.2785 | +0.2808 | +0.0264  res3d_branch2c/bias:0 | (512,) | +0.0000 | +0.0000 | +0.0000  bn3d_branch2c/gamma:0 | (512,) | -0.0088 | +1.7209 | +0.4284  bn3d_branch2c/beta:0 | (512,) | -1.8971 | +1.1821 | +0.3293  bn3d_branch2c/moving_mean:0 | (512,) | -0.9004 | +1.0312 | +0.2361  bn3d_branch2c/moving_variance:0 | (512,) | +0.0002 | +0.2382 | +0.0376  res4a_branch2a/kernel:0 | (1, 1, 512, 256) | -0.2300 | +0.3408 | +0.0208  res4a_branch2a/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4a_branch2a/gamma:0 | (256,) | +0.6213 | +1.6364 | +0.1724  bn4a_branch2a/beta:0 | (256,) | -1.4195 | +0.9165 | +0.4065  bn4a_branch2a/moving_mean:0 | (256,) | -1.7994 | +1.3411 | +0.5326  bn4a_branch2a/moving_variance:0 | (256,) | +0.4059 | +2.5504 | +0.3406  res4a_branch2b/kernel:0 | (3, 3, 256, 256) | -0.2673 | +0.1794 | +0.0154  res4a_branch2b/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4a_branch2b/gamma:0 | (256,) | +0.5855 | +1.7492 | +0.2654  bn4a_branch2b/beta:0 | (256,) | -1.8370 | +1.3979 | +0.6066  bn4a_branch2b/moving_mean:0 | (256,) | -4.1237 | +3.8698 | +0.8002  bn4a_branch2b/moving_variance:0 | (256,) | +0.3422 | +5.1990 | +0.6299  res4a_branch2c/kernel:0 | (1, 1, 256, 1024) | -0.3327 | +0.3842 | +0.0215  res4a_branch2c/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  res4a_branch1/kernel:0 | (1, 1, 512, 1024) | -0.3326 | +0.4207 | +0.0194  res4a_branch1/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4a_branch2c/gamma:0 | (1024,) | +0.0711 | +2.3668 | +0.4037  bn4a_branch2c/beta:0 | (1024,) | -0.9383 | +0.8868 | +0.2182  bn4a_branch2c/moving_mean:0 | (1024,) | -0.7338 | +0.5173 | +0.1823  bn4a_branch2c/moving_variance:0 | (1024,) | +0.0052 | +0.3592 | +0.0381  bn4a_branch1/gamma:0 | (1024,) | +0.0340 | +2.7792 | +0.5026  bn4a_branch1/beta:0 | (1024,) | -0.9383 | +0.8868 | +0.2182  bn4a_branch1/moving_mean:0 | (1024,) | -2.0030 | +2.1764 | +0.5562  bn4a_branch1/moving_variance:0 | (1024,) | +0.0150 | +2.8921 | +0.4241  res4b_branch2a/kernel:0 | (1, 1, 1024, 256) | -0.1966 | +0.2359 | +0.0149  res4b_branch2a/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4b_branch2a/gamma:0 | (256,) | +0.5655 | +1.7434 | +0.1850  bn4b_branch2a/beta:0 | (256,) | -2.7034 | +1.0420 | +0.4734  bn4b_branch2a/moving_mean:0 | (256,) | -5.8585 | +2.2085 | +0.8371  bn4b_branch2a/moving_variance:0 | (256,) | +0.3672 | +4.1957 | +0.5199  res4b_branch2b/kernel:0 | (3, 3, 256, 256) | -0.4363 | +0.1963 | +0.0137  res4b_branch2b/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4b_branch2b/gamma:0 | (256,) | +0.5148 | +2.3007 | +0.2138  bn4b_branch2b/beta:0 | (256,) | -2.5484 | +1.8564 | +0.4803  bn4b_branch2b/moving_mean:0 | (256,) | -2.3830 | +1.2051 | +0.4302  bn4b_branch2b/moving_variance:0 | (256,) | +0.1051 | +1.3241 | +0.1624  res4b_branch2c/kernel:0 | (1, 1, 256, 1024) | -0.4376 | +0.2951 | +0.0182  res4b_branch2c/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4b_branch2c/gamma:0 | (1024,) | +0.0550 | +1.9430 | +0.2674  bn4b_branch2c/beta:0 | (1024,) | -1.6468 | +1.0155 | +0.2534  bn4b_branch2c/moving_mean:0 | (1024,) | -0.4318 | +0.3222 | +0.0893  bn4b_branch2c/moving_variance:0 | (1024,) | +0.0014 | +0.1232 | +0.0124  res4c_branch2a/kernel:0 | (1, 1, 1024, 256) | -0.3869 | +0.3373 | +0.0176  res4c_branch2a/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4c_branch2a/gamma:0 | (256,) | +0.4627 | +1.8857 | +0.1667  bn4c_branch2a/beta:0 | (256,) | -2.3987 | +0.4881 | +0.4225  bn4c_branch2a/moving_mean:0 | (256,) | -2.1715 | +1.3109 | +0.5122  bn4c_branch2a/moving_variance:0 | (256,) | +0.3156 | +3.7006 | +0.3449  res4c_branch2b/kernel:0 | (3, 3, 256, 256) | -0.1645 | +0.2576 | +0.0144  res4c_branch2b/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4c_branch2b/gamma:0 | (256,) | +0.5553 | +1.9006 | +0.1967  bn4c_branch2b/beta:0 | (256,) | -1.6549 | +0.7045 | +0.4104  bn4c_branch2b/moving_mean:0 | (256,) | -0.9572 | +0.9092 | +0.2355  bn4c_branch2b/moving_variance:0 | (256,) | +0.0833 | +0.4668 | +0.0613  res4c_branch2c/kernel:0 | (1, 1, 256, 1024) | -0.2897 | +0.2609 | +0.0180  res4c_branch2c/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4c_branch2c/gamma:0 | (1024,) | +0.0488 | +1.4497 | +0.2278  bn4c_branch2c/beta:0 | (1024,) | -1.2008 | +0.5866 | +0.2045  bn4c_branch2c/moving_mean:0 | (1024,) | -0.3011 | +0.3420 | +0.0888  bn4c_branch2c/moving_variance:0 | (1024,) | +0.0017 | +0.0979 | +0.0111  res4d_branch2a/kernel:0 | (1, 1, 1024, 256) | -0.1940 | +0.2954 | +0.0172  res4d_branch2a/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4d_branch2a/gamma:0 | (256,) | +0.4422 | +1.3527 | +0.1540  bn4d_branch2a/beta:0 | (256,) | -2.3216 | +0.5091 | +0.4648  bn4d_branch2a/moving_mean:0 | (256,) | -1.9416 | +0.9021 | +0.4438  bn4d_branch2a/moving_variance:0 | (256,) | +0.3172 | +2.7555 | +0.2899  res4d_branch2b/kernel:0 | (3, 3, 256, 256) | -0.2014 | +0.1756 | +0.0144  res4d_branch2b/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4d_branch2b/gamma:0 | (256,) | +0.5293 | +1.9392 | +0.1907  bn4d_branch2b/beta:0 | (256,) | -1.6099 | +0.7762 | +0.4073  bn4d_branch2b/moving_mean:0 | (256,) | -1.1186 | +1.0655 | +0.2733  bn4d_branch2b/moving_variance:0 | (256,) | +0.0713 | +0.6757 | +0.0735  res4d_branch2c/kernel:0 | (1, 1, 256, 1024) | -0.2050 | +0.2395 | +0.0172  res4d_branch2c/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4d_branch2c/gamma:0 | (1024,) | -0.0369 | +1.6456 | +0.3121  bn4d_branch2c/beta:0 | (1024,) | -1.4837 | +0.3445 | +0.2622  bn4d_branch2c/moving_mean:0 | (1024,) | -0.3652 | +0.3959 | +0.0742  bn4d_branch2c/moving_variance:0 | (1024,) | +0.0001 | +0.1066 | +0.0112  res4e_branch2a/kernel:0 | (1, 1, 1024, 256) | -0.2258 | +0.3061 | +0.0199  res4e_branch2a/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4e_branch2a/gamma:0 | (256,) | +0.4383 | +1.4458 | +0.1636  bn4e_branch2a/beta:0 | (256,) | -2.5108 | +0.5567 | +0.4712  bn4e_branch2a/moving_mean:0 | (256,) | -2.5709 | +0.6034 | +0.4014  bn4e_branch2a/moving_variance:0 | (256,) | +0.4043 | +1.8636 | +0.2398  res4e_branch2b/kernel:0 | (3, 3, 256, 256) | -0.1471 | +0.2231 | +0.0142  res4e_branch2b/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4e_branch2b/gamma:0 | (256,) | +0.6506 | +1.8581 | +0.1985  bn4e_branch2b/beta:0 | (256,) | -1.5885 | +0.6607 | +0.3785  bn4e_branch2b/moving_mean:0 | (256,) | -0.4668 | +0.4923 | +0.1528  bn4e_branch2b/moving_variance:0 | (256,) | +0.0593 | +0.3520 | +0.0370  res4e_branch2c/kernel:0 | (1, 1, 256, 1024) | -0.1782 | +0.2650 | +0.0178  res4e_branch2c/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4e_branch2c/gamma:0 | (1024,) | -0.0013 | +1.5005 | +0.2636  bn4e_branch2c/beta:0 | (1024,) | -1.1083 | +0.6393 | +0.2529  bn4e_branch2c/moving_mean:0 | (1024,) | -0.5799 | +1.0717 | +0.1284  bn4e_branch2c/moving_variance:0 | (1024,) | +0.0003 | +0.1734 | +0.0171  res4f_branch2a/kernel:0 | (1, 1, 1024, 256) | -0.1534 | +0.3297 | +0.0212  res4f_branch2a/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4f_branch2a/gamma:0 | (256,) | +0.4254 | +1.5467 | +0.1698  bn4f_branch2a/beta:0 | (256,) | -1.9724 | +0.8233 | +0.4594  bn4f_branch2a/moving_mean:0 | (256,) | -1.1565 | +0.9281 | +0.3362  bn4f_branch2a/moving_variance:0 | (256,) | +0.4186 | +2.1176 | +0.2681  res4f_branch2b/kernel:0 | (3, 3, 256, 256) | -0.2927 | +0.2759 | +0.0145  res4f_branch2b/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4f_branch2b/gamma:0 | (256,) | +0.6498 | +2.9422 | +0.2627  bn4f_branch2b/beta:0 | (256,) | -1.0929 | +0.7714 | +0.3901  bn4f_branch2b/moving_mean:0 | (256,) | -0.9003 | +0.5086 | +0.1662  bn4f_branch2b/moving_variance:0 | (256,) | +0.0853 | +0.9656 | +0.0859  res4f_branch2c/kernel:0 | (1, 1, 256, 1024) | -0.2321 | +0.2935 | +0.0183  res4f_branch2c/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4f_branch2c/gamma:0 | (1024,) | +0.0042 | +1.9843 | +0.2409  bn4f_branch2c/beta:0 | (1024,) | -1.6360 | +1.2499 | +0.2984  bn4f_branch2c/moving_mean:0 | (1024,) | -0.6203 | +0.5245 | +0.1393  bn4f_branch2c/moving_variance:0 | (1024,) | +0.0006 | +1.1390 | +0.0446  res4g_branch2a/kernel:0 | (1, 1, 1024, 256) | -0.0685 | +0.0685 | +0.0396  res4g_branch2a/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4g_branch2a/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4g_branch2a/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4g_branch2a/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4g_branch2a/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4g_branch2b/kernel:0 | (3, 3, 256, 256) | -0.0361 | +0.0361 | +0.0208  res4g_branch2b/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4g_branch2b/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4g_branch2b/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4g_branch2b/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4g_branch2b/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4g_branch2c/kernel:0 | (1, 1, 256, 1024) | -0.0685 | +0.0685 | +0.0395  res4g_branch2c/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4g_branch2c/gamma:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  bn4g_branch2c/beta:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4g_branch2c/moving_mean:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4g_branch2c/moving_variance:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  res4h_branch2a/kernel:0 | (1, 1, 1024, 256) | -0.0685 | +0.0685 | +0.0395  res4h_branch2a/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4h_branch2a/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4h_branch2a/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4h_branch2a/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4h_branch2a/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4h_branch2b/kernel:0 | (3, 3, 256, 256) | -0.0361 | +0.0361 | +0.0209  res4h_branch2b/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4h_branch2b/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4h_branch2b/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4h_branch2b/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4h_branch2b/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4h_branch2c/kernel:0 | (1, 1, 256, 1024) | -0.0685 | +0.0685 | +0.0395  res4h_branch2c/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4h_branch2c/gamma:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  bn4h_branch2c/beta:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4h_branch2c/moving_mean:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4h_branch2c/moving_variance:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  res4i_branch2a/kernel:0 | (1, 1, 1024, 256) | -0.0685 | +0.0685 | +0.0395  res4i_branch2a/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4i_branch2a/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4i_branch2a/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4i_branch2a/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4i_branch2a/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4i_branch2b/kernel:0 | (3, 3, 256, 256) | -0.0361 | +0.0361 | +0.0208  res4i_branch2b/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4i_branch2b/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4i_branch2b/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4i_branch2b/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4i_branch2b/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4i_branch2c/kernel:0 | (1, 1, 256, 1024) | -0.0685 | +0.0685 | +0.0395  res4i_branch2c/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4i_branch2c/gamma:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  bn4i_branch2c/beta:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4i_branch2c/moving_mean:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4i_branch2c/moving_variance:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  res4j_branch2a/kernel:0 | (1, 1, 1024, 256) | -0.0685 | +0.0685 | +0.0396  res4j_branch2a/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4j_branch2a/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4j_branch2a/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4j_branch2a/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4j_branch2a/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4j_branch2b/kernel:0 | (3, 3, 256, 256) | -0.0361 | +0.0361 | +0.0208  res4j_branch2b/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4j_branch2b/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4j_branch2b/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4j_branch2b/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4j_branch2b/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4j_branch2c/kernel:0 | (1, 1, 256, 1024) | -0.0685 | +0.0685 | +0.0395  res4j_branch2c/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4j_branch2c/gamma:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  bn4j_branch2c/beta:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4j_branch2c/moving_mean:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4j_branch2c/moving_variance:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  res4k_branch2a/kernel:0 | (1, 1, 1024, 256) | -0.0685 | +0.0685 | +0.0396  res4k_branch2a/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4k_branch2a/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4k_branch2a/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4k_branch2a/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4k_branch2a/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4k_branch2b/kernel:0 | (3, 3, 256, 256) | -0.0361 | +0.0361 | +0.0208  res4k_branch2b/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4k_branch2b/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4k_branch2b/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4k_branch2b/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4k_branch2b/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4k_branch2c/kernel:0 | (1, 1, 256, 1024) | -0.0685 | +0.0685 | +0.0396  res4k_branch2c/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4k_branch2c/gamma:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  bn4k_branch2c/beta:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4k_branch2c/moving_mean:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4k_branch2c/moving_variance:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  res4l_branch2a/kernel:0 | (1, 1, 1024, 256) | -0.0685 | +0.0685 | +0.0395  res4l_branch2a/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4l_branch2a/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4l_branch2a/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4l_branch2a/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4l_branch2a/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4l_branch2b/kernel:0 | (3, 3, 256, 256) | -0.0361 | +0.0361 | +0.0208  res4l_branch2b/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4l_branch2b/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4l_branch2b/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4l_branch2b/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4l_branch2b/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4l_branch2c/kernel:0 | (1, 1, 256, 1024) | -0.0685 | +0.0685 | +0.0396  res4l_branch2c/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4l_branch2c/gamma:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  bn4l_branch2c/beta:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4l_branch2c/moving_mean:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4l_branch2c/moving_variance:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  res4m_branch2a/kernel:0 | (1, 1, 1024, 256) | -0.0685 | +0.0685 | +0.0395  res4m_branch2a/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4m_branch2a/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4m_branch2a/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4m_branch2a/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4m_branch2a/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4m_branch2b/kernel:0 | (3, 3, 256, 256) | -0.0361 | +0.0361 | +0.0208  res4m_branch2b/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4m_branch2b/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4m_branch2b/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4m_branch2b/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4m_branch2b/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4m_branch2c/kernel:0 | (1, 1, 256, 1024) | -0.0685 | +0.0685 | +0.0395  res4m_branch2c/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4m_branch2c/gamma:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  bn4m_branch2c/beta:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4m_branch2c/moving_mean:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4m_branch2c/moving_variance:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  res4n_branch2a/kernel:0 | (1, 1, 1024, 256) | -0.0685 | +0.0685 | +0.0395  res4n_branch2a/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4n_branch2a/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4n_branch2a/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4n_branch2a/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4n_branch2a/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4n_branch2b/kernel:0 | (3, 3, 256, 256) | -0.0361 | +0.0361 | +0.0208  res4n_branch2b/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4n_branch2b/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4n_branch2b/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4n_branch2b/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4n_branch2b/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4n_branch2c/kernel:0 | (1, 1, 256, 1024) | -0.0685 | +0.0685 | +0.0395  res4n_branch2c/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4n_branch2c/gamma:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  bn4n_branch2c/beta:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4n_branch2c/moving_mean:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4n_branch2c/moving_variance:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  res4o_branch2a/kernel:0 | (1, 1, 1024, 256) | -0.0685 | +0.0685 | +0.0395  res4o_branch2a/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4o_branch2a/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4o_branch2a/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4o_branch2a/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4o_branch2a/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4o_branch2b/kernel:0 | (3, 3, 256, 256) | -0.0361 | +0.0361 | +0.0208  res4o_branch2b/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4o_branch2b/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4o_branch2b/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4o_branch2b/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4o_branch2b/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4o_branch2c/kernel:0 | (1, 1, 256, 1024) | -0.0685 | +0.0685 | +0.0395  res4o_branch2c/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4o_branch2c/gamma:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  bn4o_branch2c/beta:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4o_branch2c/moving_mean:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4o_branch2c/moving_variance:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  res4p_branch2a/kernel:0 | (1, 1, 1024, 256) | -0.0685 | +0.0685 | +0.0395  res4p_branch2a/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4p_branch2a/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4p_branch2a/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4p_branch2a/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4p_branch2a/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4p_branch2b/kernel:0 | (3, 3, 256, 256) | -0.0361 | +0.0361 | +0.0208  res4p_branch2b/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4p_branch2b/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4p_branch2b/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4p_branch2b/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4p_branch2b/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4p_branch2c/kernel:0 | (1, 1, 256, 1024) | -0.0685 | +0.0685 | +0.0395  res4p_branch2c/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4p_branch2c/gamma:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  bn4p_branch2c/beta:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4p_branch2c/moving_mean:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4p_branch2c/moving_variance:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  res4q_branch2a/kernel:0 | (1, 1, 1024, 256) | -0.0685 | +0.0685 | +0.0395  res4q_branch2a/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4q_branch2a/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4q_branch2a/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4q_branch2a/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4q_branch2a/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4q_branch2b/kernel:0 | (3, 3, 256, 256) | -0.0361 | +0.0361 | +0.0208  res4q_branch2b/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4q_branch2b/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4q_branch2b/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4q_branch2b/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4q_branch2b/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4q_branch2c/kernel:0 | (1, 1, 256, 1024) | -0.0685 | +0.0685 | +0.0395  res4q_branch2c/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4q_branch2c/gamma:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  bn4q_branch2c/beta:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4q_branch2c/moving_mean:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4q_branch2c/moving_variance:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  res4r_branch2a/kernel:0 | (1, 1, 1024, 256) | -0.0685 | +0.0685 | +0.0395  res4r_branch2a/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4r_branch2a/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4r_branch2a/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4r_branch2a/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4r_branch2a/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4r_branch2b/kernel:0 | (3, 3, 256, 256) | -0.0361 | +0.0361 | +0.0208  res4r_branch2b/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4r_branch2b/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4r_branch2b/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4r_branch2b/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4r_branch2b/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4r_branch2c/kernel:0 | (1, 1, 256, 1024) | -0.0685 | +0.0685 | +0.0395  res4r_branch2c/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4r_branch2c/gamma:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  bn4r_branch2c/beta:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4r_branch2c/moving_mean:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4r_branch2c/moving_variance:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  res4s_branch2a/kernel:0 | (1, 1, 1024, 256) | -0.0685 | +0.0685 | +0.0395  res4s_branch2a/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4s_branch2a/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4s_branch2a/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4s_branch2a/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4s_branch2a/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4s_branch2b/kernel:0 | (3, 3, 256, 256) | -0.0361 | +0.0361 | +0.0208  res4s_branch2b/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4s_branch2b/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4s_branch2b/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4s_branch2b/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4s_branch2b/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4s_branch2c/kernel:0 | (1, 1, 256, 1024) | -0.0685 | +0.0685 | +0.0395  res4s_branch2c/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4s_branch2c/gamma:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  bn4s_branch2c/beta:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4s_branch2c/moving_mean:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4s_branch2c/moving_variance:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  res4t_branch2a/kernel:0 | (1, 1, 1024, 256) | -0.0685 | +0.0685 | +0.0396  res4t_branch2a/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4t_branch2a/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4t_branch2a/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4t_branch2a/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4t_branch2a/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4t_branch2b/kernel:0 | (3, 3, 256, 256) | -0.0361 | +0.0361 | +0.0208  res4t_branch2b/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4t_branch2b/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4t_branch2b/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4t_branch2b/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4t_branch2b/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4t_branch2c/kernel:0 | (1, 1, 256, 1024) | -0.0685 | +0.0685 | +0.0396  res4t_branch2c/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4t_branch2c/gamma:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  bn4t_branch2c/beta:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4t_branch2c/moving_mean:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4t_branch2c/moving_variance:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  res4u_branch2a/kernel:0 | (1, 1, 1024, 256) | -0.0685 | +0.0685 | +0.0395  res4u_branch2a/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4u_branch2a/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4u_branch2a/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4u_branch2a/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4u_branch2a/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4u_branch2b/kernel:0 | (3, 3, 256, 256) | -0.0361 | +0.0361 | +0.0208  res4u_branch2b/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4u_branch2b/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4u_branch2b/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4u_branch2b/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4u_branch2b/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4u_branch2c/kernel:0 | (1, 1, 256, 1024) | -0.0685 | +0.0685 | +0.0395  res4u_branch2c/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4u_branch2c/gamma:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  bn4u_branch2c/beta:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4u_branch2c/moving_mean:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4u_branch2c/moving_variance:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  res4v_branch2a/kernel:0 | (1, 1, 1024, 256) | -0.0685 | +0.0685 | +0.0395  res4v_branch2a/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4v_branch2a/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4v_branch2a/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4v_branch2a/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4v_branch2a/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4v_branch2b/kernel:0 | (3, 3, 256, 256) | -0.0361 | +0.0361 | +0.0209  res4v_branch2b/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4v_branch2b/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4v_branch2b/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4v_branch2b/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4v_branch2b/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4v_branch2c/kernel:0 | (1, 1, 256, 1024) | -0.0685 | +0.0685 | +0.0395  res4v_branch2c/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4v_branch2c/gamma:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  bn4v_branch2c/beta:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4v_branch2c/moving_mean:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4v_branch2c/moving_variance:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  res4w_branch2a/kernel:0 | (1, 1, 1024, 256) | -0.0685 | +0.0685 | +0.0396  res4w_branch2a/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4w_branch2a/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4w_branch2a/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4w_branch2a/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4w_branch2a/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4w_branch2b/kernel:0 | (3, 3, 256, 256) | -0.0361 | +0.0361 | +0.0209  res4w_branch2b/bias:0 | (256,) | +0.0000 | +0.0000 | +0.0000  bn4w_branch2b/gamma:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  bn4w_branch2b/beta:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4w_branch2b/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  bn4w_branch2b/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  res4w_branch2c/kernel:0 | (1, 1, 256, 1024) | -0.0685 | +0.0685 | +0.0395  res4w_branch2c/bias:0 | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4w_branch2c/gamma:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  bn4w_branch2c/beta:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4w_branch2c/moving_mean:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  bn4w_branch2c/moving_variance:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  res5a_branch2a/kernel:0 | (1, 1, 1024, 512) | -0.1841 | +0.3310 | +0.0180  res5a_branch2a/bias:0 | (512,) | +0.0000 | +0.0000 | +0.0000  bn5a_branch2a/gamma:0 | (512,) | +0.5353 | +1.5944 | +0.1286  bn5a_branch2a/beta:0 | (512,) | -1.7563 | +0.2876 | +0.2979  bn5a_branch2a/moving_mean:0 | (512,) | -2.4816 | +1.2297 | +0.4716  bn5a_branch2a/moving_variance:0 | (512,) | +0.4805 | +1.8482 | +0.1591  res5a_branch2b/kernel:0 | (3, 3, 512, 512) | -0.1746 | +0.2715 | +0.0120  res5a_branch2b/bias:0 | (512,) | +0.0000 | +0.0000 | +0.0000  bn5a_branch2b/gamma:0 | (512,) | +0.4558 | +1.5420 | +0.1543  bn5a_branch2b/beta:0 | (512,) | -1.8196 | +0.8387 | +0.3520  bn5a_branch2b/moving_mean:0 | (512,) | -2.4021 | +0.9629 | +0.2726  bn5a_branch2b/moving_variance:0 | (512,) | +0.1679 | +1.1064 | +0.1030  res5a_branch2c/kernel:0 | (1, 1, 512, 2048) | -0.3315 | +0.4318 | +0.0163  res5a_branch2c/bias:0 | (2048,) | +0.0000 | +0.0000 | +0.0000  res5a_branch1/kernel:0 | (1, 1, 1024, 2048) | -0.6221 | +0.4647 | +0.0135  res5a_branch1/bias:0 | (2048,) | +0.0000 | +0.0000 | +0.0000  bn5a_branch2c/gamma:0 | (2048,) | +0.8884 | +3.4923 | +0.2918  bn5a_branch2c/beta:0 | (2048,) | -1.8104 | +0.9799 | +0.2466  bn5a_branch2c/moving_mean:0 | (2048,) | -0.5896 | +0.6866 | +0.0685  bn5a_branch2c/moving_variance:0 | (2048,) | +0.0074 | +0.1888 | +0.0087  bn5a_branch1/gamma:0 | (2048,) | +0.2609 | +4.5746 | +0.5600  bn5a_branch1/beta:0 | (2048,) | -1.8104 | +0.9799 | +0.2466  bn5a_branch1/moving_mean:0 | (2048,) | -1.4720 | +2.3808 | +0.2325  bn5a_branch1/moving_variance:0 | (2048,) | +0.0867 | +1.0037 | +0.0806  res5b_branch2a/kernel:0 | (1, 1, 2048, 512) | -0.3159 | +0.5767 | +0.0144  res5b_branch2a/bias:0 | (512,) | +0.0000 | +0.0000 | +0.0000  bn5b_branch2a/gamma:0 | (512,) | +0.3977 | +1.4291 | +0.1259  bn5b_branch2a/beta:0 | (512,) | -1.3801 | +0.4281 | +0.2507  bn5b_branch2a/moving_mean:0 | (512,) | -4.9699 | +4.1305 | +0.6429  bn5b_branch2a/moving_variance:0 | (512,) | +1.6246 | +8.9529 | +0.6646  res5b_branch2b/kernel:0 | (3, 3, 512, 512) | -0.2169 | +0.2839 | +0.0112  res5b_branch2b/bias:0 | (512,) | +0.0000 | +0.0000 | +0.0000  bn5b_branch2b/gamma:0 | (512,) | +0.3494 | +1.5498 | +0.1200  bn5b_branch2b/beta:0 | (512,) | -1.8669 | +0.8800 | +0.3054  bn5b_branch2b/moving_mean:0 | (512,) | -1.2184 | +0.9200 | +0.2024  bn5b_branch2b/moving_variance:0 | (512,) | +0.0939 | +0.3945 | +0.0378  res5b_branch2c/kernel:0 | (1, 1, 512, 2048) | -0.2000 | +0.2767 | +0.0143  res5b_branch2c/bias:0 | (2048,) | +0.0000 | +0.0000 | +0.0000  bn5b_branch2c/gamma:0 | (2048,) | +0.5736 | +2.8466 | +0.2742  bn5b_branch2c/beta:0 | (2048,) | -2.6379 | +0.5442 | +0.2517  bn5b_branch2c/moving_mean:0 | (2048,) | -0.4205 | +0.7181 | +0.0452  bn5b_branch2c/moving_variance:0 | (2048,) | +0.0032 | +0.0582 | +0.0031  res5c_branch2a/kernel:0 | (1, 1, 2048, 512) | -0.2887 | +0.5145 | +0.0176  res5c_branch2a/bias:0 | (512,) | +0.0000 | +0.0000 | +0.0000  bn5c_branch2a/gamma:0 | (512,) | +0.3663 | +1.2492 | +0.1158  bn5c_branch2a/beta:0 | (512,) | -1.6642 | +0.7532 | +0.3176  bn5c_branch2a/moving_mean:0 | (512,) | -2.1299 | +5.3278 | +0.4875  bn5c_branch2a/moving_variance:0 | (512,) | +1.9676 | +20.6179 | +1.0893  res5c_branch2b/kernel:0 | (3, 3, 512, 512) | -0.1417 | +0.1438 | +0.0105  res5c_branch2b/bias:0 | (512,) | +0.0000 | +0.0000 | +0.0000  bn5c_branch2b/gamma:0 | (512,) | +0.5163 | +1.3347 | +0.1349  bn5c_branch2b/beta:0 | (512,) | -1.8711 | +1.1808 | +0.3583  bn5c_branch2b/moving_mean:0 | (512,) | -0.6347 | +0.4114 | +0.0675  bn5c_branch2b/moving_variance:0 | (512,) | +0.0662 | +0.3554 | +0.0192  res5c_branch2c/kernel:0 | (1, 1, 512, 2048) | -0.1346 | +0.3000 | +0.0147  res5c_branch2c/bias:0 | (2048,) | +0.0000 | +0.0000 | +0.0000  bn5c_branch2c/gamma:0 | (2048,) | +0.4348 | +3.0732 | +0.3270  bn5c_branch2c/beta:0 | (2048,) | -3.8847 | -0.2490 | +0.3030  bn5c_branch2c/moving_mean:0 | (2048,) | -0.2456 | +0.0848 | +0.0393  bn5c_branch2c/moving_variance:0 | (2048,) | +0.0013 | +0.0438 | +0.0033  fpn_c5p5/kernel:0 | (1, 1, 2048, 256) | -0.0570 | +0.0579 | +0.0294  fpn_c5p5/bias:0 | (256,) | -0.0006 | +0.0008 | +0.0002  fpn_c4p4/kernel:0 | (1, 1, 1024, 256) | -0.0724 | +0.0727 | +0.0395  fpn_c4p4/bias:0 | (256,) | -0.0005 | +0.0007 | +0.0002  fpn_c3p3/kernel:0 | (1, 1, 512, 256) | -0.0946 | +0.0970 | +0.0511  fpn_c3p3/bias:0 | (256,) | -0.0006 | +0.0007 | +0.0003  fpn_c2p2/kernel:0 | (1, 1, 256, 256) | -0.1134 | +0.1124 | +0.0627  fpn_c2p2/bias:0 | (256,) | -0.0007 | +0.0009 | +0.0003  fpn_p5/kernel:0 | (3, 3, 256, 256) | -0.0364 | +0.0367 | +0.0208  fpn_p5/bias:0 | (256,) | -0.0001 | +0.0001 | +0.0000  fpn_p2/kernel:0 | (3, 3, 256, 256) | -0.0418 | +0.0435 | +0.0208  fpn_p2/bias:0 | (256,) | -0.0007 | +0.0006 | +0.0002  fpn_p3/kernel:0 | (3, 3, 256, 256) | -0.0379 | +0.0378 | +0.0208  fpn_p3/bias:0 | (256,) | -0.0003 | +0.0003 | +0.0001  fpn_p4/kernel:0 | (3, 3, 256, 256) | -0.0369 | +0.0369 | +0.0208  fpn_p4/bias:0 | (256,) | -0.0001 | +0.0001 | +0.0000  rpn_conv_shared/kernel:0 | (3, 3, 256, 512) | -0.0376 | +0.0383 | +0.0170  rpn_conv_shared/bias:0 | (512,) | -0.0011 | +0.0018 | +0.0004  rpn_class_raw/kernel:0 | (1, 1, 512, 6) | -0.1184 | +0.1248 | +0.0631  rpn_class_raw/bias:0 | (6,) | -0.0030 | +0.0030 | +0.0025  rpn_bbox_pred/kernel:0 | (1, 1, 512, 12) | -0.1500 | +0.1247 | +0.0593  rpn_bbox_pred/bias:0 | (12,) | -0.0182 | +0.0079 | +0.0075  mrcnn_class_conv1/kernel:0 | (7, 7, 256, 1024) | -0.0128 | +0.0130 | +0.0057  mrcnn_class_conv1/bias:0 | (1024,) | -0.0002 | +0.0002 | +0.0001  mrcnn_class_bn1/gamma:0 | (1024,) | +0.9956 | +1.0018 | +0.0007  mrcnn_class_bn1/beta:0 | (1024,) | -0.0002 | +0.0002 | +0.0001  mrcnn_class_bn1/moving_mean:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  mrcnn_class_bn1/moving_variance:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  mrcnn_class_conv2/kernel:0 | (1, 1, 1024, 1024) | -0.0579 | +0.0571 | +0.0313  mrcnn_class_conv2/bias:0 | (1024,) | -0.0010 | +0.0008 | +0.0002  mrcnn_class_bn2/gamma:0 | (1024,) | +0.9966 | +1.0018 | +0.0005  mrcnn_class_bn2/beta:0 | (1024,) | -0.0010 | +0.0008 | +0.0002  mrcnn_class_bn2/moving_mean:0*** dead? | (1024,) | +0.0000 | +0.0000 | +0.0000  mrcnn_class_bn2/moving_variance:0*** dead? | (1024,) | +1.0000 | +1.0000 | +0.0000  mrcnn_class_logits/kernel:0 | (1024, 2) | -0.0880 | +0.0838 | +0.0437  mrcnn_class_logits/bias:0 | (2,) | -0.0013 | +0.0013 | +0.0013  mrcnn_bbox_fc/kernel:0 | (1024, 8) | -0.0773 | +0.0808 | +0.0430  mrcnn_bbox_fc/bias:0 | (8,) | -0.0034 | +0.0014 | +0.0017  mrcnn_mask_conv1/kernel:0 | (3, 3, 256, 256) | -0.0390 | +0.0399 | +0.0208  mrcnn_mask_conv1/bias:0 | (256,) | -0.0002 | +0.0008 | +0.0002  mrcnn_mask_bn1/gamma:0 | (256,) | +0.9994 | +1.0050 | +0.0009  mrcnn_mask_bn1/beta:0 | (256,) | -0.0002 | +0.0008 | +0.0002  mrcnn_mask_bn1/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  mrcnn_mask_bn1/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  mrcnn_mask_conv2/kernel:0 | (3, 3, 256, 256) | -0.0392 | +0.0403 | +0.0208  mrcnn_mask_conv2/bias:0 | (256,) | -0.0004 | +0.0008 | +0.0002  mrcnn_mask_bn2/gamma:0 | (256,) | +0.9995 | +1.0048 | +0.0008  mrcnn_mask_bn2/beta:0 | (256,) | -0.0004 | +0.0008 | +0.0002  mrcnn_mask_bn2/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  mrcnn_mask_bn2/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  mrcnn_mask_conv3/kernel:0 | (3, 3, 256, 256) | -0.0393 | +0.0409 | +0.0209  mrcnn_mask_conv3/bias:0 | (256,) | -0.0007 | +0.0014 | +0.0003  mrcnn_mask_bn3/gamma:0 | (256,) | +0.9996 | +1.0059 | +0.0009  mrcnn_mask_bn3/beta:0 | (256,) | -0.0007 | +0.0014 | +0.0003  mrcnn_mask_bn3/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  mrcnn_mask_bn3/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  mrcnn_mask_conv4/kernel:0 | (3, 3, 256, 256) | -0.0414 | +0.0428 | +0.0208  mrcnn_mask_conv4/bias:0 | (256,) | -0.0016 | +0.0039 | +0.0007  mrcnn_mask_bn4/gamma:0 | (256,) | +0.9993 | +1.0087 | +0.0011  mrcnn_mask_bn4/beta:0 | (256,) | -0.0016 | +0.0039 | +0.0007  mrcnn_mask_bn4/moving_mean:0*** dead? | (256,) | +0.0000 | +0.0000 | +0.0000  mrcnn_mask_bn4/moving_variance:0*** dead? | (256,) | +1.0000 | +1.0000 | +0.0000  mrcnn_mask_deconv/kernel:0 | (2, 2, 256, 256) | -0.0580 | +0.0598 | +0.0313  mrcnn_mask_deconv/bias:0 | (256,) | -0.0061 | +0.0096 | +0.0034  mrcnn_mask/kernel:0 | (1, 1, 256, 2) | -0.1809 | +0.1511 | +0.0903  mrcnn_mask/bias:0 | (2,) | -0.0755 | +0.0000 | +0.0378    "
"Hi ,     I would like to customize metrics by adding  mAP for training.  Usually I define custom function (for instance _calculate_mAP_ ) and then pass it to compile method as showed below:       As far as I understand,  in model.py module there is no argument to pass to keras compile method (copied below from the source code)         I would be thankful if you could extend your repo with such an option.    Thank you in advance!  "
"I know that the weights are automatically stored after each epoch in `.h5` file.  But I would like to save the whole network architecture together with the weight. I guess I have to insert some code in `model.py`, but I am not sure how to do it. Thanks"
"In my application, I want to detect large targets in images. Network input image size is 256*256*3. So I set BACKBONE_STRIDES = [8, 16] and RPN_ANCHOR_SCALES = (64,128). Is it OK? Or the two parameters must contain 5 numbers separately? If it's ok, should I change codes in model.py, like _, C2, C3, C4, C5 = resnet_graph(input_image, ""resnet50"", stage5=True)? Thank you very much."
"Hello and thank you for code,    The notebook **train_shapes.ipynb** when run on _xeon E5520 (12Gb RAM) / GTX 960 (4Gb)/tensorflow 1.9 rc_ met an error as I tried to train the model:       The cell returns at the end:     Is there a way to turn around this issue?"
"Hi, I use the following cmd to train:  python3 coco.py train --dataset=~/dataset/COCO2014/ --model=coco  But hang up around the beginning of stage2(after passing stage1).  What might be the reason?  The only modification is changing steps to 50 and epochs to 1, so as to see the result sooner.    Thanks."
"Hello everyone,    I am currently trying to add a CRF to the output of the mask graph to improve segmentation task after I saw this implementation of the referring paper:   However, I am not sure about how to get the ouput masks, and if I can load trained weights once I add this layer.  Does anyone have any idea about it?    Many thanks for this great project !     **Edit :** I tried non trained dense CRF but gets poor results, I am looking for the trainable version of it"
Hi!  How do I export test image with bounding box of mask and class name of her?  My class name is a white sections of image and on result image masks not visible.  Running command:  `python train.py splash --weights=/path/to/weights/file.h5 --image= `
"My image data is known to have 1 instance (not 0 and not above 1). There are parameters to control the max predicted number. But how can I force the model to predict at least one instance? In some case, it does not give any output. This leads to a huge error rate. I have set min_confidence to 0.5.    Many thanks!"
"All the instances in the mask are 1 connected components, however, in the predicted mask, there are a lot of 'make-no-sense' disconnected components for one instance.    !     Any suggestion on how to avoid them?"
"i trained a model on my own dataset,but splash doesn't  work anymore.it did earlier but now just returns a greyscaled version of the test image."
"thanks for your sharing. It is really excellent work, and give me help a lot.  I am doubt that how can i split batch and num_boxes after ROIAlign? In author's code '    box_to_level = tf.concat(box_to_level, axis=0)      box_range = tf.expand_dims(tf.range(tf.shape(box_to_level)[0]), 1)      box_to_level = tf.concat([tf.cast(box_to_level, tf.int32), box_range],                               axis=1)      # Rearrange pooled features to match the order of the original boxes      # Sort box_to_level by batch then box index      # TF doesn't have a way to sort by two columns, so merge them and sort.      sorting_tensor = box_to_level[:, 0] * 100000 + box_to_level[:, 1]      ix = tf.nn.top_k(sorting_tensor, k=tf.shape(box_to_level)[0]).indices[::-1]      ix = tf.gather(box_to_level[:, 2], ix)      pooled = tf.gather(pooled, ix)      # Re-add the batch dimension      pooled = tf.expand_dims(pooled, 0)      return pooled'    i think shape returned should be(1,batchsize*num_boxes,poolsize,poolsize,c) not (batchsize,num_boxes,poolsize,poolsize,c). Am I get something mistake? thanks"
"My video is 4k.  I cropped only target area from original video with 100% resolution.  And I segmented and trained, but AP was not good  So, I captured images from original video with reduced resolution (2k), and I trained again.  This performance was better than the first trial.  Are there someone who knows the reason?    Thank you.       "
"hello, i've run your github repo on train_shape.ipynb,but when training,i receive the error:  ResourceExhaustedError: OOM when allocating tensor with shape ]]  Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.  even i have git your latest update project and run on train_shape.ipynb,it still have the same  error.....,how to solve this problem??  "
"i am trying to train data, and get poor results while adding a lot of ""normals"" without masks.  even though the loss looks pretty good.     i wanted to ask about the loss.   i saw that in mask loss and bbox loss os written: Only positive ROIs contribute to the loss.    what does it mean?    is that mean that only what the computer guessed right is in the loss?   y does it true?    and what will happen if i change it to all, and not onlt positive?"
"Thanks a lot for sharing your Mask RCNN on github and the codes indeed help me a lot on my project in lab. To continue your spirit of sharing, I create a repository based on your work, demonstrating effect on our dataset and also sharing instructions on training the Mask RCNN on new dataset with multiple classes. Two main difference from previous work:  1. Codes to loading annotations from  Vgg Image Annotations(VIA) tool for dataset with multiple classes.  2. Codes for visualization.  3. Some detailed instruction for applyting Mask RCNN on new dataset.    The link to my repository is:     I would be very happy if you could share my project on your Mask RCNN page. I hope it could help more developers to implement your codes more easily."
"Hi All!    I would like extract the encodings of the final detected bounding boxes. My professor told me that it would be better, if we can get them before the softmax. Is that possible in this model? Currently I can get the encodings from the proposals in the shape of [batch_size][boundingbox][1024]. However these boundingboxes are the ones before the non max suppression.    Thank you and best regards  Mehmet"
"First, thanks for well-documented and awesome implementation on Mask-RCNN!    I am working on nucleus data which have much more crowd nucleus compare to DSB 2018 data, but same goal to segment the nucleus part. One image contains lots of object, up to 1,500 masks on one image.    I have read model.py code for understanding how RPN works. For Proposal layer, it take anchors from RPN, sort it by foreground class score and apply NMS. So most of the ROIs will have high probability of  being foreground class.           After Proposal layer, it will send those ROIs into DetectionTargetLayer and sample some positive ROIs and negative ROIs by comparing the IOU of Ground Truth. The sampled ROIs will be used to train the classifier and regressor. Mask-RCNN paper suggests that the ratio of positive ROI and negative ROI should be 1:3.           My question is, what if our RPN is trained super well and it can catch almost every foreground class? if that happened, DetectionTargetLayer will get lots of positive ROIs and sample only few negative ROIs which might affect the training of classifier?    In my cases, many nucleus are pretty small and crowd, most of nucleus are side by side. So it's easy for RPN selecting too many positive anchors on IOU > 0.5, I am wondering this might hurt the result.    If I misunderstand anything please correct me and thanks again for awesome implementation!"
only resNet -50?
@waleedka Setting augmentation to True in `load_image_gt` Gives the error  'bool' object has no attribute 'to_deterministic'  That's caused by   `det = augmentation.to_deterministic()`  Why would you want to convert a boolian ? 
"I'm using a Titan X GPU on a Dell Precision tower with 12 cores. I'm running the training on GPU, the GPU usage reaches to 11.6GBs but at the same time all CPU cores reach to 100% usage and the system then crashes and reboots. But when I used the same card on another tower I was able to run the code. Can anyone suggest what might be the issue?"
Hi.  I have a question: What Mask RCNN actually learns? Does it learn shapes? Does it learn transitions from background to object? Or both? Or something else?   Thank you. 
"training on CPU takes about half an hour per epoch with only 10 steps per epoch.How do i speed up training process.please be elaborate. Thank you  MY SYSTEM CONFIGURATION:  i5 7th gen,8 gigs ddr4 ram, 2gb nvidia gtx 1050  also,the pretrained weight file im using is inception_resnetv2  "
"Thanks for the great repo!  Could you clarify which coco version (2014/2017) you used for training the coco checkpoint that you released? Did you use the entire validation set for computing the metrics shown in the release notes?    Cheers,   Julien"
"Could you explain the differences among RPN_TRAIN_ANCHORS_PER_IMAGE, POST_NMS_ROIS_TRAINING, and TRAIN_ROIS_PER_IMAGE. Are they all for one image? I am confused there are great differences in values of  the three parameters. "
Is it necessary  to use a GPU to run this project?
"hello! thanks for the libraries, i'd be stuffed if i tried doing any of this without them! i am trying to train coco to recognize a new single class, so i made a dataset with polygon annotations and set up a script like balloon.py example. after a bunch of twiddling, i can get it to start actually training, but its really slow and i think its only using the cpu. i know this question comes up alot, so i want to assure people that its not because im using tensorflow instead of tensorflow-gpu. when i run the balloon.py example, i can see the gpu working hard. when i run my own script, i see tensorflow grabs all the memory, as well as logging that its using the gpu, but the gpu never ends up doing work during the training. ive included some of the relevant details below:    the training file:         and some of the output that might be relevant? this is all the output before it starts actually training.       finally, once its training, it takes about a minute per step, and here is what task manager shows for hardware usage:    !     thank you for making it this far! i hope i've included all the relevant info- any advice or even shots in the dark are appreciated!    "
Who has a trained model on coco dataset? 
"  I just try the demo;but there is an error when compiling it.I don't know how to fix it.    Below is the detailed informaiton:  Traceback (most recent call last):    File ""D:/Program/mask_rcnn/my_test1.py"", line 43, in          model = modellib.MaskRCNN(mode=""inference"", config=config, model_dir=""D:/Program/mask_rcnn/logs/"")      File ""D:\Program\mask_rcnn\build\lib\mrcnn\model.py"", line 1831, in __init__      self.set_log_dir()      File ""D:\Program\mask_rcnn\build\lib\mrcnn\model.py"", line 2256, in set_log_dir      self.config.NAME.lower(), now))  AttributeError: 'NoneType' object has no attribute 'lower'"
"I encounter a strange issue; when I increase the GPU count from 1 to 2 to 3 to 4 (my max GPU count) the training time significantly decreases per epoch. Even when I use the CUDA_ACTIVE_DEVICES with the GPU ids. All GPU's are activated and utilized when training (so they are active)    The GPU's have 12GB memory, I tried both 2 images per GPU processing (Batch_size=8) as 1 image per GPU (Batch_size=4). The fastest training time is now (strangely) GPU_COUNT=1 and IMAGES_PER_GPU=1 (?).     Roughly it takes 25 minutes per epoch (COCO dataset training with 1000 cycles p.e.) with 1 activated GPU, 40 minutes with 2 GPU's, 50 minutes with 3 GPU's and 1hr10min with 4 GPU's.    Is it some issue in the parallel_model.py code (maybe in combination with Windows OS)? There is a clear trend visible (>GPU_COUNT = >TRAINING_TIME).    The specifications:  - OS: Windows10  - GPU: Nvidia Geforce GTX TitanX (4x)  - GPU driver: 391.35  - Tensorflow_gpu = 1.5  - Keras version = 2.1.6  - CNN architecture: Resnet50 (resnet50_notop.h5)  - Starting weights=imagenet  - Config settings: similar as Resnet101 COCO (coco.py) example    I tried to search the issue list, however I cannot find a similar issue. I don't understand the problem :-("
"Hardware: NIH Biowulf cluster node with a tesla P100 gpu and 40g of memory.   Software: Tensorflow-gpu 1.8.0, CUDA 7.0, cuDNN 9.0, centOS 7  I am trying to train my own dataset using the coco weights as a starting point.  When I actually begin training the model with these lines,       `model = modellib.MaskRCNN(mode=""training"", config=config,                       model_dir=MODEL_DIR)`    `model.load_weights(COCO_MODEL_PATH, by_name=True,                             exclude=[""mrcnn_class_logits"", ""mrcnn_bbox_fc"",                                      ""mrcnn_bbox"", ""mrcnn_mask""])`  `with tf.device('/device:GPU:0'):           model.train(dataset_train, dataset_test,                       learning_rate=config.LEARNING_RATE,                       epochs=1,                       layers='heads')`      Then I receive this error after running 99/100 steps on the first epoch.     `OpenBLAS blas_thread_init: pthread_create: Resource temporarily unavailable`  `OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 513778 max`    I've tried updating software and checking dependencies but I don't know what else to do. Thanks."
"I've recently downloaded the latest code and it doesn't work when I set ""weights=last"". Instead, it looks like it always starts a new empty folder in 'logs' before checking for the latest folder (which is the newly created empty one).    When I revert back to an older version of the code, it works fine. Does anyone know where this bug is occurring, please?"
"Hello!    I annotated some images for my own dataset. My dataset has a very limited size so I would like to augment my dataset.    Upon searching previously opened issues, I came to know that we can pass an argument for augmentation as shown below:    `model.train(dataset_train, dataset_val,                  learning_rate=config.LEARNING_RATE,                  epochs=30,                  layers='heads', augmentation = imgaug.augmenters.OneOf([                      imgaug.augmenters.Fliplr(0.5),                      imgaug.augmenters.Flipud(0.5),                      imgaug.augmenters.Affine(rotate=(-45, 45)),                      imgaug.augmenters.Affine(rotate=(-90, 90)),                      imgaug.augmenters.Affine(scale=(0.5, 1.5))]))`    However, I cannot visualize how get we get the polygon points after this augmentation. After augmentation the dataset like this, does the code automatically generate new polygon points for various instances?    Regards,  Chris Henry"
"I'm trying to make my own dataset.  I used VIA which is suggested by balloon example.  But I get this error when training.    >   File ""/usr/local/lib/python3.6/json/__init__.py"", line 299, in load  >     parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)  >   File ""/usr/local/lib/python3.6/json/__init__.py"", line 354, in loads  >     return _default_decoder.decode(s)  >   File ""/usr/local/lib/python3.6/json/decoder.py"", line 339, in decode  >     obj, end = self.raw_decode(s, idx=_w(s, 0).end())  >   File ""/usr/local/lib/python3.6/json/decoder.py"", line 355, in raw_decode  >     obj, end = self.scan_once(s, idx)  > json.decoder.JSONDecodeError: Expecting ',' delimiter: line 2 column 1 (char 6747)    I think there is an error in the json file. The json file I annotated is like below.    `{""00000001.jpg650197"":{""filename"":""00000001.jpg"",""size"":650197,""regions"":[{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[109,109,235,247,305,307,185,173],""all_points_y"":[177,160,32,32,88,108,240,243]},""region_attributes"":{""name"":""cellphone"",""categories"":""cellphone""}},{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[225,369,383,436,436,296,277,227],""all_points_y"":[269,152,152,216,232,351,355,289]},""region_attributes"":{""name"":""cellphone"",""categories"":""cellphone""}}],""file_attributes"":{}}`    Is there something wrong when I annotating the dataset?"
"Hi all,  I would like to make an environment for anaconda for this project. With it, it should be easy to install this project with tree commands        git clone        cd Mask_RCNN/      conda env create      conda activate mask_rcnn    Here is my first attemps with this file         But i've this error with opencv          Thanks for help !"
在安装好的TITAN X上可以跑通官方的气球例子，但是换到自己的数据集时 按照看气球例子得到的经验 每张图片的label需要按照顺时针或者逆时针方向对物体的轮廓进行采样，按照这样的做法做后，训练模型，得到上面的那种错误。在调bug的时候还尝试了下面几种方案  1：仅仅保留8张图片来训练模型，每张图片的label都是同一个label，发现有的图片能跑通，但是有的跑不通  2：仅仅保留1张图片来训练模型，同时调节batch_size为1，发现同一个label在有的图片上能跑通，在有的图片上就不能跑通(已保证图片的size可以包含label)  求教！
"Does anybody know what is the data bias in coco and ImageNet? What is their unique advantage?     E.g., which one is better at small objects? which one is better at dense objects? which one is better at faces?    In my data, I feel that coco 1) tends to give a big mask instead of several small ones as imageNet does for a crowded image;  2) COCO gives a smaller training loss value when using the same setting.  Thanks! "
"First thanks for releasing this on Github @waleedka!  I can't seem to output anything larger than 1600 from the pretrained weights (mask_rcnn_coco.h5), using the notebook demo.ipynb. Any idea why?   "
"Because of some reasons , i want to use the tensorflow == 1.2 .  So, I want to ask someone that can you use this version to execute this code ?"
"i have tseted this model，but it runs more slowly than msak-rccn default model  hardware：my notebook 1050ti+i5  code：i just Modify the code of  msak-rccn demo，Replace the msak-rccn folder with the author's folder      #msak-rccn demo#   COCO_MODEL_PATH =  ""mask_rcnn_coco.h5""    #my code#   COCO_MODEL_PATH =  ""mobile_mask_rcnn_coco.h5""  so Why is it like that？Thank you for your help  "
"Hi,  Thank you for your amazing work. good job indeed.  I want to know if it is possible to have a different number of classes in detector head and Mask head. for example, the detector only proposes RIO of a desk with a PC together as a region of interest, but in the mask head, we segment computer and desk as two different classes?"
tensorflow.python.framework.errors_impl.PermissionDeniedError: /logs; Permission denied    I was running the code for the balloons.py in the google compute engine.  everything looked fine until the error message above.  I am not sure what that error means or how to fix it.  
"the size of my training data is 1024x2048  and what i set in the Config class is like this            class DataConfig(Config):              IMAGE_RESIZE_MODE = 'none'              IMAGE_MIN_DIM = 1024              IMAGE_MAX_DIM = 2048    it shows that the image shape is 2048x2048  IMAGE_RESIZE_MODE              none  IMAGE_SHAPE                    [2048 2048    3]    and what in config.py is            if self.IMAGE_RESIZE_MODE == ""crop"":              self.IMAGE_SHAPE = np.array([self.IMAGE_MIN_DIM, self.IMAGE_MIN_DIM, 3])          else:              self.IMAGE_SHAPE = np.array([self.IMAGE_MAX_DIM, self.IMAGE_MAX_DIM, 3])    so this means i always get a square image?    =====================================    BTW is it suggested to use square images for training ?    "
"Hi, the loss of mrcnn_class seems to be different from the original paper.              # Find predictions of classes that are not in the dataset.      pred_class_ids = tf.argmax(pred_class_logits, axis=2)        # TODO: Right now it assumes all images in a batch have the same active_class_ids      pred_active = tf.gather(active_class_ids[0], pred_class_ids)      # Loss      loss = tf.nn.sparse_softmax_cross_entropy_with_logits(          labels=target_class_ids, logits=pred_class_logits)        # Erase losses of predictions of classes that are not in the active classes of the image.      loss = loss * pred_active    If the total dataset has 80 classes which means the classifier will not predict the classes outside the 80, so in this way, the loss only includes the values of pred_classes rather than 80 values. But in the fast-rcnn, the classifier loss does not define as it. And I do not understand the advantages of doing in this way. Thanks!"
"Hi, I'm working on a multi-class detection problem while it's hard to distinguish the items from human vision. I first work on a binary detection (either one class of the item or background) and it works good, though the results couldn't be that accurate (I guess because even from human vision it is hard to tell).   However, when working with joint multi-class detection problem, I find it normally has nothing detected and there is no output even for training data. Since during the train, the loss and val_loss is not high, I suppose it should at least work good on the training data, is it possible that I mess up with the code? Or it is just impossible to do multi-class detection for the problem.    This is the loss I get after training for multiclass, but I couldn't get any output even for training data in the problem.  Epoch 40/40  100/100 [==============================] - 84s 845ms/step - loss: 0.7847 - rpn_class_loss: 0.0044 - rpn_bbox_loss: 0.3186 - mrcnn_class_loss: 0.1197 - mrcnn_bbox_loss: 0.1222 - mrcnn_mask_loss: 0.2197 - val_loss: 5.7929 - val_rpn_class_loss: 0.0367 - val_rpn_bbox_loss: 5.2939 - val_mrcnn_class_loss: 0.0052 - val_mrcnn_bbox_loss: 0.3068 - val_mrcnn_mask_loss: 0.1503  In comparison, this is the loss I get for training binary problem (one of the classes in the multi problem), and I can get output for every pic in this problem.  100/100 [==============================] - 68s 683ms/step - loss: 0.2497 - rpn_class_loss: 0.0013 - rpn_bbox_loss: 0.0878 - mrcnn_class_loss: 0.0090 - mrcnn_bbox_loss: 0.0417 - mrcnn_mask_loss: 0.1101 - val_loss: 1.8302 - val_rpn_class_loss: 0.0020 - val_rpn_bbox_loss: 0.3195 - val_mrcnn_class_loss: 0.0171 - val_mrcnn_bbox_loss: 0.5660 - val_mrcnn_mask_loss: 0.9256      Could anyone elaborate on the problem? I have no clue where should I start working on. Your help is really appreciate, thank you:) "
"When I utilize the compute_batch_ap function given in one of the notebooks, even with ROIs with high confidence, I am still getting all the above values as 0. Does anyone have any idea why? "
"system:ubuntu14.04  GPU:GTX1080  tensorflow version =1.3.0  keras version= 2.1.0  ///////////////////////////////////  excuse me, Could you help me?  probelom below:  Traceback (most recent call last):    File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/common_shapes.py"", line 654, in _call_cpp_shape_fn_impl      input_tensors_as_shapes, status)    File ""/usr/lib/python3.4/contextlib.py"", line 66, in __exit__      next(self.gen)    File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status      pywrap_tensorflow.TF_GetCode(status))  tensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid reduction dimension 1 for input with 0 dimensions. for 'Mean' (op: 'Mean') with input shapes: [], [] and with computed input tensors: input[1] =  .    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""train_shapes.py"", line 334, in        layers=""all"")    File ""/usr/local/lib/python3.4/dist-packages/mask_rcnn-2.1-py3.4.egg/mrcnn/model.py"", line 2332, in train    File ""/usr/local/lib/python3.4/dist-packages/mask_rcnn-2.1-py3.4.egg/mrcnn/model.py"", line 2162, in compile    File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/math_ops.py"", line 1382, in reduce_mean      name=name)    File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 1362, in _mean      keep_dims=keep_dims, name=name)    File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op      op_def=op_def)    File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py"", line 2632, in create_op      set_shapes_for_outputs(ret)    File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py"", line 1911, in set_shapes_for_outputs      shapes = shape_func(op)    File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py"", line 1861, in call_with_requiring      return call_cpp_shape_fn(op, require_shape_fn=True)    File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/common_shapes.py"", line 595, in call_cpp_shape_fn      require_shape_fn)    File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/common_shapes.py"", line 659, in _call_cpp_shape_fn_impl      raise ValueError(err.message)  ValueError: Invalid reduction dimension 1 for input with 0 dimensions. for 'Mean' (op: 'Mean') with input shapes: [], [] and with computed input tensors: input[1] =  .  "
"Hi,    Usually when validating a CNN network, it gives the accuracy per class. In Tensorboard, I only see overall class accuracy, is there a simple way to display this information?    Thanks,  Eugenio"
"Hi, when you change the learning rate on a job, the loss function suddenly goes into the roof.  It seems like all weights are set to init again, or something, and you have to start over. Why? I would like to keep my weights with my good performing model with a low loss, but just change the learning rate without it starting over. How?    Best regards  zungam"
"hello.i wish to implement mask rcnn on my own dataset.it consists of images containing graphite flakes of 5 types,namely a,b,c,d and e.  !   !   !   !   !   how do i proceed starting from scratch.please help asap andbe as elaborate as possible.  sincere thanks          "
"Hello, I trained maskrcnn model with my image datasets.When I set IMAGE_RESIZE_MODE = ""square"", it works well , get mAP(iou=50%)=0.817. BUT when I tried training networks with origion size images by setting **IMAGE_RESIZE_MODE = ""none""**, I got very poor evaluation mAP(iou=50%)=0.15.  I used tensorboard and noticed that the loss behave unnormal(IMAGE_RESIZE_MODE=""none""):  !   !     **detail about my dataset:**  1 All image has **same size 640x480**  2 **Only two classes: ""background"" and ""object""**, but many images contain multi instances     Does anyone know why this happen?"
"Hello Team,    Please reply ASAP because I am unable to get any solution to integrate Mask_RCNN in an iOS application for object detection as well as masking.    Thanks  Kamal Thakur"
"I'm using tensorflow-gpu 1.7.0 and when I run my training with GPU_COUNT=1 in my GeForce GTX 1080, it uses >90% memory on all 4 GPUs but utilises only one GPU.    Here's my configuration:  !     Here's the screenshot of GPU memory usage:  !       Any idea why its hogging memory on all GPUs?"
"When I was looking at the   function in the visualization code, I noticed the images are all cast to type uint8 before being displayed. Why is this the case?    Does the input image need to be of type uint8?     Also does the input need to be rgb (i.e. have 3 channels)?"
"Hi All,     Loving MRCNN and the great community of users.    I poked around the issues but I couldn't find a discussion on the effects on training of the mask boundaries.  Below are a couple of questions hopefully making my query more clear. Insights into any or all of the following are welcomed.    1. How ""tight"" or ""loose"" should training masks be? I.e. should a polygon edge be as close as possible to the edge of the object (""tight""); leave a little space around the object (""loose""); or inside of the object outline (""over tight"")?    2. How should occluded objects be handled? Should I mask occluded objects as though the occluding (blocking) object  is not there (i.e. including part of the blocking object in my target objects mask); or shouId  I include only pixels that are part of that object.    3. Do all objects of a given type need to be masked? e.g. given an image containing two objects of a common category, If I only mask one of them, will that ""confuse"" the model training.     If anyone has considered these issues or studied their effects I'd be greatful to hear from you.    Thank you!"
"In balloon example, the value of DETECTION_MIN_CONFIDENCE is 0.9, whereas in nucleus and coco examples the value of DETECTION_MIN_CONFIDENCE is set as 0. What is the use of it and how should its value be set according to the number of classes?"
"Hi,    I've been trying to make Mask-RCNN work on my coco-style dataset, however whatever I did, it did not produce any good results. I've started to think on variables that I may have set wrong, but I could not find any.     Could it be because my training data is too small? I have 18 training images, 2 test images and 2 validation images. Only 2 classes to be detected/segmented (plus the background). Is this too less of data for the task? Doesn't Mask-RCNN use augmentation anyway? Below I am sharing some of the configurations I have:         And this is the training command I am using:         In short, can you pinpoint to me the most crucial parameters and/or points which I should be careful about? I am trying to recognize screws on HDD surfaces, and I am getting terrible results like these:      !   !     !   !          I would really appreciate any idea.    "
"I use code in model.py to test predict time for an image like below:  t1 = time.time()  detections, mrcnn_class, mrcnn_bbox, mrcnn_mask, \              rois, rpn_class, rpn_bbox =\              self.keras_model.predict([molded_images, image_metas], verbose=0)  print('predict_time:{:.3f}s'.format(time.time()-t1))    The results are time for the first image is 3.3s and time for images from the second is all around 0.065s. I am wondering why time for the first image is much larger? Thank you."
"In `demo.ipynb`, the line `model.load_weights(COCO_MODEL_PATH, by_name=True)` causes the error:         This module isn't compatible with Keras 2.2.0 that was released a few days back. Switching back to Keras 2.1.6 fixed it for me. The code can either be updated to not use the deprecated module or the `requirements.txt` can exclude Keras 2.2 version.    "
The issue regarding I am unable to convert MLModel for iOS.
"mrcnn_class_loss: 0.8616  mrcnn_class_loss: 0.7972  mrcnn_class_loss: nan  small  dataset  of hat, and get nan loss ,help ... i have no idea .. set  lr=0.0001 useless      "
"In the balloon RCNN example, the load_mask function ends with -              # Return mask, and array of class IDs of each instance. Since we have          # one class ID only, we return an array of 1s          return mask.astype(np.bool), np.ones([mask.shape[-1]], dtype=np.int32)    An array of 1s is returned as there is only 1 class id. But what if we have multiple classes like balloon and cake."
"Hello,  I am trying to train the network using custom classes and images. I followed both the shapes and balloon examples, and they train just fine.   However, with my images, I am stuck in Epoch 1/1 and then the kernel dies (as seen in another thread, I changed workers = 1 and multiprocessing = False, but it still crashes).  My images are 1280x720... Should I resize them before training? I see that in utils.py there's a couple functions to resize the mask and the image.. but I do not know how to use them in the Dataset loader.  Thanks!"
I can not change the code in model.py and utils.py. It can't work!!  can you tell me how to change it ?  thank you !  
"First of all, thanks for sharing this great work!    Please let me ask a question on relationship between a ROI box and its GT segmentation.  Let's say `r` is a box coordinates from RPN. To my understanding, RPN itself predicts `r`, and the DETECTION layer takes input (a feature map ROI-pooled with `r`) and further predicts it final coordinates `r'`. I think the segmentation GT is also given from a GT segmentation patch cropped by `r`. Then, wouldn't there be a discrepancy between final `r'` and the segmentation GT (cropped by `r`)? In other words, I am wondering that, while SEGMENTATION layer learns from GT cropped by `r`, DETECTION layer will slightly change its final coordinates.    Questions:  (1) Am I missing something?  (2) It wouldn't be a problem since `r` and `r'` would be finally very similar?  (2) It wouldn't be the case since MaskRCNN actually crop with `r'` (ie, after the DETECTION layer) instead of `r`?    I hope my questions are clear.  "
"Hi,    I am not sure if it's the right place to put this, but I want to use the oppurtunity to thank all the developers that were/are working on this Mask RCNN implementation. At ImageMonkey we are now using Mask RCNN for a few months and its working like are charm - many thanks for the the hard work you're putting into this framework!    # What's ImageMonkey    is an attempt to create a public, open source image dataset, where everybody can contribute to. We are by far not as big as labelme, but hopfully we get there some day ;)     In order to give you some figures. We currently have:    * ~ 13900 images  * ~ 21000 labeled object  * ~ 8000 validated objects  * ~ 20000 annotations      One of the core principles of ImageMonkey is, that we want to make the data available and accessible for everybody. That's why we focus on powerful APIs and tight third party tool integration.     # What are we using Mask RCNN for?    * Assisted annotation    In order to speed up the tedious task of annotating an object, we implemented the possibility to load auto-generated annotations. Those annotations are generated by a dedicated Mask RCNN process which is constantly scanning the dataset for newly added labels and images.     Users can then load those auto-generated annotations to speed up their annotation.  Here's a short demo:     !       * Integration of the ImageMonkey dataset    Since a few days it's now also possible   itself.       There are still a few things missing in our implementation (like better documentation, more configuration parameters, tensorboard integration...), but I am already really happy that we are now at a point where we can apply Mask RCNN to our dataset.     At this point, I would like to give kudos to all the Mask RCNN contributors for the awesome documentation. It was much easier than anticipated to integrate the dataset. Awesome work!    (Please feel free to close this issue - just wanted to let you know that we are using this great framework and are really happy with it!)"
"Everything is running fine when I use the 'square' option for my images, but if i change this to 'crop' I get the following error. Does anyone know why, please?     "
"Hello,    To resume a training session, in addition to reading the last saved weights, do we also have to manually set the learning rate (to a lower value) as well?    Thanks a lot!"
"    (Conv2D)      rpn_class_raw          (Conv2D)      rpn_bbox_pred          (Conv2D)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)  /home/yu/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  /home/yu/.local/lib/python3.5/site-packages/keras/engine/training.py:2039: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.    UserWarning('Using a generator with `use_multiprocessing=True`'  image_id 2  image_id 2  image_id 0  image_id 2  image_id 1  image_id 2  image_id 1  image_id 2  image_id 0  image_id 0  image_id 1  image_id 2  image_id 2  image_id 2  image_id 0  image_id 1  image_id 1  image_id 2  image_id 2  image_id 1  image_id 1  image_id 2  image_id 1  image_id 0  image_id 0  image_id 1  image_id 0  image_id 2  image_id 0  image_id 0  image_id 2  image_id 2  image_id 2  image_id 2  image_id 2  image_id 2  Epoch 1/10  image_id 2  image_id 2  image_id 2  image_id 2  class_ids-----------> []  /home/yu/.local/lib/python3.5/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.    warn(""Anti-aliasing will be enabled by default in skimage 0.15 to ""  class_ids-----------> []  /home/yu/.local/lib/python3.5/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.    warn(""Anti-aliasing will be enabled by default in skimage 0.15 to ""  class_ids-----------> []  /home/yu/.local/lib/python3.5/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.    warn(""Anti-aliasing will be enabled by default in skimage 0.15 to ""  class_ids-----------> []  /home/yu/.local/lib/python3.5/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.    warn(""Anti-aliasing will be enabled by default in skimage 0.15 to ""  class_ids-----------> []  /home/yu/.local/lib/python3.5/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.    warn(""Anti-aliasing will be enabled by default in skimage 0.15 to ""  class_ids-----------> []  /home/yu/.local/lib/python3.5/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.    warn(""Anti-aliasing will be enabled by default in skimage 0.15 to ""  class_ids-----------> []  /home/yu/.local/lib/python3.5/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.    warn(""Anti-aliasing will be enabled by default in skimage 0.15 to ""  class_ids-----------> []  /home/yu/.local/lib/python3.5/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.    warn(""Anti-aliasing will be enabled by default in skimage 0.15 to ""  class_ids-----------> []  /home/yu/.local/lib/python3.5/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.    warn(""Anti-aliasing will be enabled by default in skimage 0.15 to ""  class_ids-----------> []  /home/yu/.local/lib/python3.5/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.    warn(""Anti-aliasing will be enabled by default in skimage 0.15 to ""  class_ids-----------> []  /home/yu/.local/lib/python3.5/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.    warn(""Anti-aliasing will be enabled by default in skimage 0.15 to ""  class_ids-----------> []  /home/yu/.local/lib/python3.5/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.    warn(""Anti-aliasing will be enabled by default in skimage 0.15 to ""  class_ids-----------> []  /home/yu/.local/lib/python3.5/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.    warn(""Anti-aliasing will be enabled by default in skimage 0.15 to ""  ERROR:root:Error processing image {'height': 300, 'yaml_path': '/home/yu/Mask_RCNN/test/total/0_json/info.yaml', 'source': 'shapes', 'id': 0, 'width': 400, 'path': '/home/yu/Mask_RCNN/test/fish/0.jpg', 'mask_path': '/home/yu/Mask_RCNN/test/mask/0.png'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/samples/shapes/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/samples/shapes/model.py"", line 1263, in load_image_gt      class_ids = class_ids[_idx]  IndexError: boolean index did not match indexed array along dimension 0; dimension is 0 but corresponding boolean dimension is 20  "
None
"I want to use Mask-RCNN demo.py in my jupyter notebook but when i ran the the demo code , i faced such a problem  OSError                                   Traceback (most recent call last)    in  ()        3         4 # Load weights trained on MS-COCO  ----> 5 model.load_weights(COCO_MODEL_PATH, by_name=True)    ~/Mask_RCNN/mrcnn/model.py in load_weights(self, filepath, by_name, exclude)     2102         if h5py is None:     2103             raise ImportError('`load_weights` requires h5py.')  -> 2104         f = h5py.File(filepath, mode='r')     2105         if 'layer_names' not in f.attrs and 'model_weights' in f:     2106             f = f['model_weights']    ~/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds)      310             with phil:      311                 fapl = make_fapl(driver, libver, **kwds)  --> 312                 fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)      313       314                 if swmr_support:    ~/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/h5py/_hl/files.py in make_fid(name, mode, userblock_size, fapl, fcpl, swmr)      140         if swmr and swmr_support:      141             flags |= h5f.ACC_SWMR_READ  --> 142         fid = h5f.open(name, flags, fapl=fapl)      143     elif mode == 'r+':      144         fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)    h5py/_objects.pyx in h5py._objects.with_phil.wrapper()    h5py/_objects.pyx in h5py._objects.with_phil.wrapper()    h5py/h5f.pyx in h5py.h5f.open()    OSError: Unable to open file (truncated file: eof = 74436151, sblock->base_addr = 0, stored_eof = 257557808)    i can not solve my problem  can any body help me??  thank's"
I have 4200+ images to train model.  And configuration is   Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES                      !       Thanks for all your response!    
I have tried at least 10 times and the download start then freezes and finally fail with a forbidden error. I am logged to github...
I have noticed that somtimes network generetes two or more prediction with high iou > 0.8 of different classes. And the correct one is with highest confidence all others are wrong. How to force model to filter out this cases? It looks like this is usual non max supression target
"Hi,  maybe it's wrong. I'd like to know whether maskrcnn can handle with the pixel level segmentation task(not the instance level).  Is there any benchmark I can refer?  Can anyone tell me the trades-off between pixel level and instance level segmentation task with maskrcnn?  In my opinion, alough it's super slow to convert instance segmentation result into pixel level segemetation result, the pixels can be really classified with its highest class score.     any reply is helpful, thanks."
Thanks
"I have to use sliding window for predicting, because my own image is too big to directly predict, but I have no idea to merge the sliding window mask."
"Extensions  ================  I want to retrain the Mask RCNN implementation on my own dataset on grayscale images of size 288x288. Currently, I am repeating the grayscale image into the 3 different channels and passing that as the input.     There are 5 total classes (4 foreground classes + 1 background). Following the _shapes_ demo, I extended the `utils.Dataset` class to a new class (called Class A). I override the `load_mask ` function and return a mask with 4 channels (i.e. mask is of size 288x288x4).     I created Class B which extends `utils.config`. I have set my `IMAGE_RESIZE_MODE` parameter to `""none""`. I did this because both the mask and input image are of the same 2D dimension (288 x 288). I am also changing the `IMAGE_MIN_DIM `and `IMAGE_MAX_DIM `parameters to both be 288, because all our images are 288x288.    Questions  ============  1. Are these changes the config necessary?  2. Does the `load_mask ` function have to also return a mask for the background?  3. If all the steps above are valid, I am getting the error below. How can this be addressed?    Error  ================    `tensorflow.python.framework.errors_impl.InvalidArgumentError: flat indices[5, :] = [0, 248102] does not index into param (shape: [1,20730,4]).`"
"Checkpoint Path: /home/yu/Mask_RCNN/logs/shapes20180605T2146/mask_rcnn_shapes_{epoch:04d}.h5  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  In model:  rpn_model      rpn_conv_shared        (Conv2D)      rpn_class_raw          (Conv2D)      rpn_bbox_pred          (Conv2D)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)  /home/yu/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  /home/yu/.local/lib/python3.5/site-packages/keras/engine/training.py:2039: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.    UserWarning('Using a generator with `use_multiprocessing=True`'  ERROR:root:Error processing image {'width': 2048, 'id': 6, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_3.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_3_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_3.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  ERROR:root:Error processing image {'width': 2048, 'id': 4, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_6.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_6_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_6.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  ERROR:root:Error processing image {'width': 2048, 'id': 5, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_5.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_5_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_5.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  ERROR:root:Error processing image {'width': 2048, 'id': 1, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_2.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_2_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_2.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  ERROR:root:Error processing image {'width': 2048, 'id': 1, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_2.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_2_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_2.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  ERROR:root:Error processing image {'width': 2048, 'id': 3, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_7.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_7_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_7.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  ERROR:root:Error processing image {'width': 2048, 'id': 6, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_3.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_3_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_3.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  ERROR:root:Error processing image {'width': 2048, 'id': 2, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_4.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_4_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_4.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  Process Process-1:  Traceback (most recent call last):    File ""/usr/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap      self.run()    File ""/usr/lib/python3.5/multiprocessing/process.py"", line 93, in run      self._target(*self._args, **self._kwargs)    File ""/home/yu/.local/lib/python3.5/site-packages/keras/utils/data_utils.py"", line 630, in data_generator_task      generator_output = next(self._generator)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  ERROR:root:Error processing image {'width': 2048, 'id': 0, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_1.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_1_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_1.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  ERROR:root:Error processing image {'width': 2048, 'id': 2, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_4.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_4_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_4.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  ERROR:root:Error processing image {'width': 2048, 'id': 5, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_5.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_5_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_5.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  ERROR:root:Error processing image {'width': 2048, 'id': 4, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_6.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_6_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_6.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  ERROR:root:Error processing image {'width': 2048, 'id': 4, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_6.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_6_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_6.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  ERROR:root:Error processing image {'width': 2048, 'id': 1, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_2.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_2_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_2.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  ERROR:root:Error processing image {'width': 2048, 'id': 0, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_1.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_1_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_1.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  ERROR:root:Error processing image {'width': 2048, 'id': 3, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_7.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_7_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_7.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  Process Process-2:  Traceback (most recent call last):    File ""/usr/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap      self.run()    File ""/usr/lib/python3.5/multiprocessing/process.py"", line 93, in run      self._target(*self._args, **self._kwargs)    File ""/home/yu/.local/lib/python3.5/site-packages/keras/utils/data_utils.py"", line 630, in data_generator_task      generator_output = next(self._generator)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  ERROR:root:Error processing image {'width': 2048, 'id': 6, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_3.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_3_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_3.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  ERROR:root:Error processing image {'width': 2048, 'id': 1, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_2.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_2_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_2.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  ERROR:root:Error processing image {'width': 2048, 'id': 0, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_1.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_1_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_1.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  ERROR:root:Error processing image {'width': 2048, 'id': 2, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_4.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_4_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_4.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  ERROR:root:Error processing image {'width': 2048, 'id': 5, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_5.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_5_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_5.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  Process Process-3:  Traceback (most recent call last):    File ""/usr/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap      self.run()    File ""/usr/lib/python3.5/multiprocessing/process.py"", line 93, in run      self._target(*self._args, **self._kwargs)    File ""/home/yu/.local/lib/python3.5/site-packages/keras/utils/data_utils.py"", line 630, in data_generator_task      generator_output = next(self._generator)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  ERROR:root:Error processing image {'width': 2048, 'id': 4, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_6.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_6_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_6.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  ERROR:root:Error processing image {'width': 2048, 'id': 3, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_7.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_7_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_7.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  ERROR:root:Error processing image {'width': 2048, 'id': 6, 'mask_path': '/home/yu/Mask_RCNN/test/mask/fish_3.png', 'height': 1536, 'source': 'shapes', 'yaml_path': '/home/yu/Mask_RCNN/test/total/fish_3_json/info.yaml', 'path': '/home/yu/Mask_RCNN/test/fish/fish_3.bmp'}  Traceback (most recent call last):    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  Process Process-4:  Traceback (most recent call last):    File ""/usr/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap      self.run()    File ""/usr/lib/python3.5/multiprocessing/process.py"", line 93, in run      self._target(*self._args, **self._kwargs)    File ""/home/yu/.local/lib/python3.5/site-packages/keras/utils/data_utils.py"", line 630, in data_generator_task      generator_output = next(self._generator)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1695, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 1210, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""train_shapes.py"", line 137, in load_mask      num_obj = self.get_obj_index('fish_'+img)  TypeError: Can't convert 'PngImageFile' object to str implicitly  Epoch 1/1  Traceback (most recent call last):    File ""train_shapes.py"", line 222, in        layers='heads')    File ""/home/yu/Mask_RCNN/mrcnn/model.py"", line 2328, in train      use_multiprocessing=True,    File ""/home/yu/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py"", line 87, in wrapper      return func(*args, **kwargs)    File ""/home/yu/.local/lib/python3.5/site-packages/keras/engine/training.py"", line 2065, in fit_generator      generator_output = next(output_generator)  StopIteration      What can I fix it and go on train?Thanks!"
"I updated Cuda from 8->9, Cudnn 6->7 and TF 1.4->1.5 and now the demo is stuck in:    `    results = model.detect(arr_img, verbose=0)`"
"Hello everyone,    I am currently working on the implementation of Mask R-CNN with is working quite well.  However I still have some issues for sharp objects: when you have for instance sharp angles at the edge of the objects to be detected, segmentation masks are often too smooth for these areas (objects to detect are pretty big).    I assume that it comes from the mask loss which is binary crossentropy if I'm right, and I wonder if other losses could be aware of such a problem, to have a more accurate per-pixel detection, like soft dice or Jaccard losses as presented here:      Does anyone have the same issue? Could it come from an other parameter that I would have missed?    Thank you ! "
Hi   recently i used Mask-RCNN demo.ipynb  file and successfully  ran it on my jupyter notebook.  now i want to use Mask-RCNN on my own dataset but it is recomended that first start with train_shapes.ipynb .  but when i run train_shaped code it apperas this problem.     `  does 'log' refer to log in python math module or it is an other thing??  How can  solve this issue??  thank you .  
"Hi, thanks for sharing your work. I am trying to implement the code for my own dataset. I read the article (  demonstrating by building the balloon dataset, however I don't understand how you make use of the 'region_attributes' information  in the JSON file generated from VGG Image Annotator (VIA). In other words, I dont understand how you add label name into the dataset to tell the model the corresponding label of each bounding box. Is it because you only have balloon and it's a binary classification so you don't need the label name?    How should I modify the balloon.py if I have a dataset with multiple labels? I really appreciate your reading and help!"
"Actually I have run well on keras 2.0.8 befoore , but this time I have to run on keras 2.0.5,  it is embarrassing....  so where should I change in the python code in mrcnn folder? or what should I add on  keras 2.0.5???  thanks"
None
"Is there a way to disable masks training, and train only bounding boxes? In some cases no masks are needed. I can achieve that by setting weight of mask loss to zero, but training and prediction time is not decreasing.  Can it be achieved without mask training/prediction overhead?"
"I want to use Mask-RCNN demo.py in my jupyter notebook but when i ran the the demo code , i faced such a problem      ModuleNotFoundError                       Traceback (most recent call last)    in  ()       19 # Import COCO config       20 sys.path.append(os.path.join(ROOT_DIR, ""samples/coco/""))  # To find local version  ---> 21 import coco       22        23 get_ipython().run_line_magic('matplotlib', 'inline')    ~/Mask_RCNN/samples/coco/coco.py in  ()       32 import time       33 import numpy as np  ---> 34 import imgaug  #   (pip3 install imageaug)       35        36 # Download and install the Python COCO tools from      ModuleNotFoundError: No module named 'imgaug'      i can not solve my problem  can any body help me??  thank's"
"I have written a callback to evaluate the trained model with a custom metric on some validation data. However, my approach is really naive and probably memory-consuming (getting OOM after some epochs??), as I create a new Mask RCNN model and load the model in inference mode with a validation config and the last checkpoint file that is created after each epoch has finished. So I'm basically having 2 models run at the end of each epoch - the one that is training and another to validate the one that is training.  I wonder, if I can use the model, that I'm training, to do validation without setting up a new model. But I'm not sure how to do this. Can anybody give me a hint?  It should work since val_loss, val_rpn_loss and so forth is calculated, for which the model should be loaded somehow in inference mode. Unfortunately, I'm having trouble to get my head around the important code snippets.   "
"Hi folks,    I've just update Tensorflow to version 1.8.  A script that I've been using without errors, now it fails with:    F ./tensorflow/core/util/cuda_launch_config.h:127] Check failed: work_element_count > 0 (0 vs. 0)    I had to downgrade TF back to version 1.6 to get it ok again."
"Hi @waleedka,  Do you know about the Light Head RCNN Project? (Paper:    If yes which parts of your model do you think need to be adjusted to make use of this method to decrease inference time?    Would be nice hearing from you!"
"config: GTX1070ti(8g) + win7(i5 memory:16g) + cuda8.0 + cudnn6 + keras2.0.8 + tensorflow-gpu1.4.0  when I training the network, it will work normal, a few minutes later, the CPU Utilization ratio will suddenly down to 0. And about 1 minute later, the system will auto restart and appear a error (code: 1962 can't find operation system).  thanks for help"
"If I want to detect text content such as English sentences or Chinese sentences, is the Mask_RCNN works well?  Can you give some advice or other solution to work character recognition? thanks very much~!"
"I have two GPUs the type is Tesla p100,  and I prepare two images for training. I set   GPU_COUNT = 2  IMAGES_PER_GPU = 1  but when I train the model,  I use nvidia-smi to check the gpu  !   !   The GPU was not used successfully, I have change  model.py  workers = 1  use_multiprocessing=False  it still not work.  My environment  ubuntu 16.04  tensorflow 1.6.0  keras 2.1.6  cuda 9.0  cudnn 7.0  @waleedka "
"First, thank the authors for the well documented project.  Here, I have some confusion about the anchor as the followings:    According the codes in  , the anchors in all feature maps start from (0, 0).  anchor centers in all feature maps:  scale1(stride=4): (0, 0), (0, 0), (0, 0), (0, 4), ... , (0, 8), .... , (0, 16), ....  scale2(stride=8): (0, 0), (0, 0), (0, 0), (0, 8), ... , (0, 16), ....  scale3(stride=16): (0, 0), (0, 0), (0, 0), (0, 16), ....  ...  It's wonderful that some anchors in different scales are aligned.    However, in another project   ,  the anchors in the single scale feature start from (7.5, 7.5) which depends on the size of the reference (0, 0, 15, 15) window. See the codes In the    anchor centers in the feature map(stride=16):  (7.5, 7.5), ...,  (7.5, 23.5), ... , (7.5, 39.5)    In my opinion, the second definition seems more reasonable.   Taking the perceptual field into account, I think the anchor centers should start from (stride/2, stride/2).     I wonder whether the performance  would change with different anchor setting?    "
"Hello,    Thank you very much for the code!    After installing and running some sample code, I realized that it does not use my GPUs at all. I spent quite some time to figure out that during the installation I executed `pip3 install -r requirements.txt` as recommended, which removed my previously installed `tensorflow-gpu` and installed `tensorflow` instead.    So I think 'tensorflow' should be removed from `requirements.txt`, leaving the user the choice to install the right version for them.    Thanks."
"How can we save segmentation results the same width and height with original images? Thank you.  I use code below to save segmentation results, the results are 416*416. But my original images are all 640*512:  i=0  for image_id in dataset.image_ids:      #print(image_id)      image, image_meta, gt_class_id, gt_bbox, gt_mask = modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)      info = dataset.image_info[image_id]      # Run object detection      results = model.detect([image], verbose=1)      ax = get_ax(1)      r = results[0]      visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], dataset.class_names, i, r['scores'], figsize=(16, 16),ax=ax)      i=i+1"
"I have images of crystals, they look like this:         And I have labelled them:         With my current config, I have trained a resnet50, using coco, and I obtain images like this:         This is quite good, but it is far away from being perfect. I wonder if there's something in my config I can do to obtain better results.    My current config looks like this:    Configurations:  BACKBONE                       resnet50  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  DETECTION_MAX_INSTANCES        400  DETECTION_MIN_CONFIDENCE       0  DETECTION_NMS_THRESHOLD        0.3  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 1  IMAGE_MAX_DIM                  640  IMAGE_META_SIZE                14  IMAGE_MIN_DIM                  400  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [640 640   3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               200  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           crystals  NUM_CLASSES                    2  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (16, 32, 64, 128, 256)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.9  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                300  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           128  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               5  WEIGHT_DECAY                   0.0001    I have tried stuff like smaller RPN_ANCHOR_SCALES, but it crashes. I algo tried with Imagenet instead of Coco, but it also crashes.    Thanks for any input!"
"Hi,  I have some images that are gray scale images. These images' shape are (512, 512). But the model need to input the image which is loaded in RGB. So I use numpy.stack to change my image shape from (512, 512) to (512, 512, 3). The training result with this revision isn't good at all, so I'm wondering that what the reason is for the bad training result.     Is it inappropriate to make this revision or the model is not suitable for training gray scale image?  Please give me some advise.    BTW, does anyone has the gray scale input version of Mask R-CNN?  Please provide me or tell me how to revise the model.    Thanks."
"Hi,    I am trying to make this work for two days with the dataset I have. I am using TF 1.7. CuDNN 7 and CUDA 9.    My dataset consists of devices and I have only 2 classes apart from the background. I've annotated the parts in the devices by naming them. Am I doing something wrong with the annotation, perhaps?        I've downloaded the weights of ImageNet (since coco weights did not work for me, gave an error) and after launching the program I get to see the following:         The full error message is too long to post here, but in short it keeps telling that there is an error with  ""all_points_y"". Any idea about what's going on?"
"Hi.  I have images of size 704x704 and I get a lot of small false positives (network thinks that something like 10x10 is an object, but it's not). True object's side is usually more than 30px and I want to somehow filter them via configuration.  I had anchor sizes 8, 16, 32, 64, 96, 128, 160, then deleted 8 and 16, but it did not help.  How can I configure this?    Thank you for the project. "
"Dear author:   Firstly I would like to thank you very much for publishing your codes.  I have a question about the fpn_classifier_graph function (which is in line 918 in ""model.py""). I found that after running the PyramidROIAlign class, the shape of tensor x is (1,?,7,7,256), however, after running the KL.TimeDistributed command just after is, the shape of tensor x becomes (?, 150, 1, 1, 1024).   It seems to me that the first two dimensions (namely, 1 and ?) of tensor x means the number of figures in each batch and the number of proposals in one figure, respectively. Thus, it seems that after executing the KL.TimeDistributed command, the the number of figures in each batch changes from 1 to ?, while the number of proposals in one figure changes from ? to 150. My question is that how can the KL.Conv2D function achieve this? It seems to me that this function is just a convolutional layer, how can it affect the number of images in a batch (as well as the number of proposals in one figure)?  Thank you in advance!"
 How to replace the BACKBONE “resnet101” with “resnet50”?
"I am training a model on a custom dataset, and I am having difficulty getting the model to identify large features. It is comically bad at it. The length of the bounding box sides of these features can range from ~15% to 100% of the length of the image. Using an altered inspect_model notebook, I have found that the RPN fails to predict candidate regions that are large. In response I've tried changing the RPN anchor size, stride, anchor ratio etc... Nothing seems to yield much of an improvement. What can I do to make my Mask RCNN model better identify large features? Thanks."
"Hi @waleedka, I was wondering if there's any interest in translating the rpoject to pure tf? I've done most of the part but I'm stuck at a particular point regarding shape inference of tensors. Basically what happens is that the output tensors shapes are completely different from the ones that you have to specify when using pure tf. Keras is doing automatic shape inference and changing the output shapes something that doesn't happen in tf. I would love to go more in detail and explain the issue but I wouldn't want to clutter the `issues` if there's any interest in this, and probably u can help me to understand a bit better what inputs go where in terms of their shapes and what's the exact outputs expected after each model/submodel call, just drop me a line and we can further discuss it."
"I want to use Mask-RCNN demo.py in my jupyter notebook but when i ran the the demo code , i faced such a problem      `  I used from this   workround   but it has not any correct code and i can not solve my problem  can any body help me??  thank's  "
"I'm using the package tensorflow-gpu==1.5.0, installed from `conda`. When I run the samples/demo.py, I found although the memory-usage of the GPU is nearly 90%, the GPU-Util is still 0% and the program is running so slow..... How can I do about this? Thanks a lot!    There are some screenshots:  - The logs of the script:  !     - The memory-usage and GPU-Util of my GPU  !     - The usage of CPU  !       "
"Hello everyone, newbie here :)  I can not see a solution to test the balloon dataset or whatever dataset (except the coco one on the demo) on random pictures, it only tests them on the ones that are preset. Thanks in advance and sorry if I bothered you asking dumb question.  Regards"
"hello, I  set IMAGE_RESIZE_MODE = ""pad64"" to train my images data, I just want my image to be rectangle and  not square,  such as 1280x960, but I find the **mrcnn_class_loss** descends quickly to under 0.1 ,  but my learning rate is 0.001, which is small enough.  **Even if I set IMAGE_RESIZE_MODE = ""pad64"", and let IMAGE_MIN_DIM = 960, IMAGE_MAX_DIM = 960,**  mrcnn_class_loss  also  descends quickly to under 0.1 ,  is it overfiting?     how does the input size affect the mrcnn_class_loss? and it seems that input size does not affect the box_loss, why?    And if I changed  to  IMAGE_RESIZE_MODE = ""square"" ,to be 1280x1280, it performs well . but I still want my input image to be rectangle, such as 1280x960 , then what can I do ? "
"Hi, everything is going okey, and it looks like my model is getting better, but mrcnn_class_loss is going up all the time and I dont understand why. Any ideas?  I only have one class type in my images and in my model (+ background).  !   Same results in my validation loss graphs.  "
"I am trying to train the mask rcnn mode with coco weights on my  dataset on a gtx 1080ti 8gb on Ubuntu 16.04. The images' resolutions are all 512x512. Every time I call train(), the kernel runs out of memory and crashed. I can run all the samples but except for my own dataset. The cuda version installed is 9.0, and cudnn is 7.0 now. I have tried cuda9.1 and cudnn7.1, but still cannot solve the problem. BTW, I can run successfully with CPU. Posted below is my config output.     "
"I installed all requarenments and run MASK-RNN demo on CPU.    But i can't run the code on GPU and even more, other projects crash when i run them on GPU. My env:    - OS: Win10 64  - Python 3.5.4  - Tensorflow-gpu 1.7.0  - Nvidia driver  - CUDA Toolkit - 9.0  - cuDNN - 7.1 for CUDA 9.0  - Microsoft Visual C++ Redistributable (2008 - 2015)  - Microsoft Build Tools 2015    What i do wrong?"
why in training set a  I have to set the coordinate of image in mask Rcnn (ballon dataset)  json file
"I have been using this project for like 2 months, and it was fine. Suddenly since last week, I am getting the error:    Traceback (most recent call last):    File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main      ""__main__"", mod_spec)    File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code      exec(code, run_globals)    File ""/usr/lib/python3.6/site-packages/ipykernel_launcher.py"", line 16, in        app.launch_new_instance()    File ""/usr/lib/python3.6/site-packages/traitlets/config/application.py"", line 657, in launch_instance      app.initialize(argv)    File "" "", line 2, in initialize    File ""/usr/lib/python3.6/site-packages/traitlets/config/application.py"", line 87, in catch_config_error      return method(app, *args, **kwargs)    File ""/usr/lib/python3.6/site-packages/ipykernel/kernelapp.py"", line 448, in initialize      self.init_sockets()    File ""/usr/lib/python3.6/site-packages/ipykernel/kernelapp.py"", line 238, in init_sockets      self.shell_port = self._bind_socket(self.shell_socket, self.shell_port)    File ""/usr/lib/python3.6/site-packages/ipykernel/kernelapp.py"", line 180, in _bind_socket      s.bind(""tcp://%s:%i"" % (self.ip, port))    File ""zmq/backend/cython/socket.pyx"", line 495, in zmq.backend.cython.socket.Socket.bind    File ""zmq/backend/cython/checkrc.pxd"", line 25, in zmq.backend.cython.checkrc._check_rc  zmq.error.ZMQError: Address already in use    when trying to train with     model.train(dataset, dataset,               learning_rate=config.LEARNING_RATE,               epochs=10,               layers='heads')    I am doing a very small variation of the shapes project in the demos, but with my own images.    I am using the GPU (Nvidia) and I am using arch-linux. I am running my code through a Jupyter Notebook, and eventually I receive a message saying that the kernel crashed.    As said, this code was working like 2-3 weeks ago, and as far as I can tell I didn't change anything.    Thanks for your help"
"In my application, I need the output to be a matrix of probabilities for each class, such that each value in the matrix denotes the probability of this corresponding pixel to be in this class.  Could anybody elaborate on explaining how I could achieve this idea? Your help is really appreciated.  "
"Hi,    I'm still trying to sort out, how many images are randomly cropped from an image if your config is in the 'cropping' mode? Say you have a 1024x1024 image and you are setting 512x512 crops. If you presume you don't want to have overlapping images, there could be theoretically 4 different images cropped. Is this the case or is only one random 512x512 image cropped that could be located anywhere in the original image? If it's the latter, what is the suggested setting for the number of steps per epoch? As far as I know, the usual setting is #Images/#Images_per_gpu but if it's only cropping one 512x512 image this is obviously too low and potential training data is wasted. Obviously, one solution would be to set the number just higher but how can I assure that I get the most out of my original images (e.g. best case: 4 images that are not overlapping)    Thanks for the help!"
"Hi, this is not a matterport issue, actually its an improvement to Keras so I will post this on Keras soon,  but it actually doubled the training speed of this repo in particular, so I will share it! I analyzed this repo for bottlenecks, and deduced the biggest bottleneck down to the `model.data_generator` which fetch batches ready on a seperate thread to improve speed. However, on my training job it didnt help much, because some batches took 0.1 seconds to load, and others 20 seconds. This means that if loading a batch takes longer than the GPU uses to train on the last batch, the GPU will halt to wait on a batch to load.     To make the GPU allways have an image ready to load into GPU memory without waiting time, I propose having a FIFO queue fetching batches non-stop. This means that while the GPU is busy doing what it does, the CPU will always work in the background to have images ready (forever). This litle trick doubled the training time of my network (which uses 704x704 images as input and between 5 and 2000 masks on each image).    **How to:**    In  , add this class:         Now, add this below the line 145:  `batch_thread = ThreadingBatches(generator=output_generator) `  and overwrite the line 155 from:  `generator_output = next(output_generator)`  to  `generator_output = batch_thread.popNextBatch()`    And voala, superfast training! :)"
"splash = color_splash(image, r['masks'])          #print(splash[:,0].max())          # Save output          file_name = ""data/splash_cup_rgb.png""          #skimage.io.imsave(file_name, splash)          cv2.imwrite(file_name, splash)          srcImg = cv2.imread(""/home/zs/Mask_RCNN/samples/balloon/data/splash_cup_rgb.png"")          cv2.imwrite(""data/test.png"", srcImg)          print(srcImg.shape)          print(srcImg)          print(srcImg[:,0].max())          print(srcImg[:,1].max())          print(srcImg[:,2].max())        the srcImg is a zero array, but I want to obtain the colour of the mask area."
"Hi,    First of all, thank you very much for a well-documented project and an excellent blog post. I want to run Faster-RCNN +FPN on a custom dataset just for object detection. Is it possible to do this? If yes, what steps should I follow?    Thanks and Regards,  Karthik"
"Hi,    I train a resnet50 with the config `TRAIN_BN = False` and, sometimes, there is an overflow on the layer bn_conv1.   In this case, `visualize.display_weight_stats(model)` returns:       How is it possible to hit overflow on batch normalization layers when TRAIN_BN is set to False?"
"Like this , I am training bollon in GPUs occer this issues,in detiles:  Training network heads    Starting at epoch 0. LR=0.001    Checkpoint Path: /home/yu/Mask_RCNN/logs/balloon20180517T2146/mask_rcnn_balloon_{epoch:04d}.h5  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  In model:  rpn_model      rpn_conv_shared        (Conv2D)      rpn_class_raw          (Conv2D)      rpn_bbox_pred          (Conv2D)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)  Traceback (most recent call last):    File ""balloon.py"", line 359, in        train(model)    File ""balloon.py"", line 194, in train      layers='heads')    File ""/home/yu/.virtualenvs/Mask_RCNN/local/lib/python3.5/site-packages/mask_rcnn-2.1-py3.5.egg/mrcnn/model.py"", line 2308, in train    File ""/home/yu/.virtualenvs/Mask_RCNN/local/lib/python3.5/site-packages/mask_rcnn-2.1-py3.5.egg/mrcnn/model.py"", line 2143, in compile  TypeError: reduce_mean() got an unexpected keyword argument 'keepdims'    tf = 1.4 keras =2.0.8 I hope anyone fix it .thank you!"
"Anytime I try to run a Canari command I wind up with an error either telling me that I lack the canari 1.0 framework or I need a downgraded Scapy version, but the most often error that I encounter ImportError: Import by filename is not supported.  I have read through the docs and followed several threads but have not arrived at a solution.   I am operating Debian and using Python3, pip and Canari (without a virtualenv currently).   I have run across a handful of package that I can import to Maltego but for the vast majority I am unable to launch a single canari command.   Any help would be greatly appreciated. "
"Hi, while working with the Mask RCNN I noticed that it cannot detect long thin-ish objects (e.g. a pipe or a wire). It is possible that I do not have enough samples (small and imbalanced dataset), but I was wondering if anyone has had a similar issue and how you solved it (or suggestions to check).     "
"Combining dataset to mask_rcnn_coco.h5     Hello, I finished training my data and it was successful. But I have no idea how to combine my dataset to mask_rcnn_coco.h5    Please get me some help guys."
"With Ubuntu14.04, Python3.6.0, Tensorflow1.4.0, Keras2.0.8, using multi GPU output error as:         More detailed log is as follows (click me)         "
"Hi, I am using the keypoint detection idea presented here in a similar network architecture. In my case I am doing a binary keypoint detection. I am finding that the network is getting stuck guessing that all pixels are of class 0, as class 1 pixels are very sparse (only 40 or so per image). How do I remedy this?"
"Besides data augmentation, using resnet 50 mentioned in   Has anyone tried L1 regularization, Drop-out, Max norm constraints or other type of ways to avoid over-fitting? Does it work well?     "
"Has anyone used `Mask RCNN` on `TensorFlow Lite`? I managed to create a `tflite` file from my network, but there are some `TensorFlow Lite` unsupported operators used in `Mask RCNN`: `ResizeNearestNeighbor`, `Stack`, and `TensorFlowShape`.    `Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: ResizeNearestNeighbor, Stack, TensorFlowShape.`    Has anyone managed to implement such operators? There is some information over here about how to do it, but I wanted to ask in case that someone already implemented them.  .    "
None
"hi, I train coco on m-rcnn with image size as (1024, 1024), the train model just runs around 40 epochs per day on one titanx while the original paper said 8 GPUs could finish training on 44hours with 160k epochs. Does any one know how can I speed up the training exclude resize the image to (224,224)? Thanks so much."
"I have very small dataset but images are too big to be read as a standard image. Hence I have to convert my image into an numpy array. Also, I have seperate annotation files for each image since my images are multilabel.   Would such kind of images work in MAskRCNN i.e which are having multiple labels in one image. If yes, then:    1. How to use numpy arrays as my dataset   2. How to use xml annotation files as my ground truth    "
"I modified a couple lines of the orginal nucleus.py codes to adapt my own dataset (about 2k 1800*1800 images).  I installed the tensorflow-gpu 1.7.0 version. And I also used CUDA_VISIBLE_DEVICES to assign the target GPUs.  However, when the network (ResNet 50) starts to train, I find there are lots of propcess (top -c) in the memory. And the fisrt epcoh (1/20) is still not finished after six hours running.    I am wondering if the code is running on CPU rather than GPU?  Do I need to write some specific codes to assign GPUs? Can anyone give me some hints?"
None
Trying to install the library on Windows 10. I have python 3.6 and all tensorflow object detection dependencies are installed well. Getting errors during install    C:\maskRCNN>python setup.py install   
"According to the mode.py line 1921       you feed       My problem is , why not :        my conception is numbers of anchors from per location need to be equal with len(anchors) , am I missing any details ?    thanks    "
"I want to test the program ,but i can't find the test coding."
"Hi, thanks for this repo, its the best ive ever seen.  To reduce the computational complexity of feed forward and backprop, I would like to have less neurons in the architecure. I would like to train on 512x512 images or 640x360 preferably. However, I see there is resize/cropping present, such that images become 1024x1024. Does this mean that reducing the amount of neurons is impossible? Say I train on 512x512, then I presume they are upscaled to 1024x1024, which woudnt reduce the computational load of the network!?    Say that IT IS possible to reduce the input dimensions, then will weights from coco / imagenet still work? Or do I have to train from scratch?    Best regards  Zungam"
"Hi,    I want to train my own dataset and need to write my own metric for this. I'm planning to this with a custom callback (inherited from keras.callbacks.Callback). However, I couldn't figure out how to access the validation (and train) data that is used in the current epoch when the model is training. Could you give me some hints on where to look in the code? I spotted some data generators to generate the validation data but I'm not sure how to use them...    Also, if there is an easier way to calculate and print own metrics, I'm absolutely open for any ideas. Thank you!"
"i noticed the model.train following the format    model.train(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation=None)    but if there is a way that i can AVOID having val_dataset? and providing training data_set only? Because i just simply need more labeled data for training that's all    also, if this is necessary,  what is the best percentage of train/val that we should have the model trained in an optimised way"
"Hi, thankyou for this repo. Im trying to wrap my head around some basic architecture.     Question 1) If I want to train every weight in the entire stack, what do I need to do?     Question 2) Mask-RCNN is based on RCNN, so will training from scratch in this repo also train the RCNN part? Or is it only the mask part which will be trained?     Question 3) What part in the arcitecture uses ResNet?    Question 4) What is the difference between using the weights from the --model=imagenet and the weights from--model=coco?    Question 5) If I am to train everything (RCNN part and Mask part) from scratch, how long time would it take on a 1080 Ti 11GB?     Question 6) If I only do the Mask from scratch, how long would it take?    Best regards  Zungam"
"Hi,    I'm trying to visualize the differences between the ground truth and predictions of a model of an image. I'm trying to use the `visualize.display_differences` for this. However, I believe this method fails when the implementation has to pad the image for inference that is if you choose the pad64 setting in your inference config.    When I'm trying to load the ground truth data with `r = modellib.load_image_gt`, my r['masks'] are of the shape of the padded version of the image and the number of ground truth segmentations. But then the `visualize.display_differences` when computing the overlap of the predicted and ground truth mask, as the non-padded version is used for the predicted image. See the complete error below:    > ---------------------------------------------------------------------------  > ValueError                                Traceback (most recent call last)  >   in  ()  >                class_names,  >                show_box=False, show_mask=False,  > ---->          iou_threshold=0.5, score_threshold=0.5)  >   > ~/Documents/xxx/Mask_RCNN/mrcnn/visualize.py in display_differences(image, gt_box, gt_class_id, gt_mask, pred_box, pred_class_id, pred_score, pred_mask, class_names, title, ax, show_mask, show_box, iou_threshold, score_threshold)  >              gt_box, gt_class_id, gt_mask,  >              pred_box, pred_class_id, pred_score, pred_mask,  > -->          iou_threshold=iou_threshold, score_threshold=score_threshold)  >          # Ground truth = green. Predictions = red  >          colors = [(0, 1, 0, .8)] * len(gt_match)\  >   > ~/Documents/xxx/Mask_RCNN/mrcnn/utils.py in compute_matches(gt_boxes, gt_class_ids, gt_masks, pred_boxes, pred_class_ids, pred_scores, pred_masks, iou_threshold, score_threshold)  >        >          # Compute IoU overlaps [pred_masks, gt_masks]  > -->      overlaps = compute_overlaps_masks(pred_masks, gt_masks)  >        >          # Loop through predictions and find matching ground truth boxes  >   > ~/Documents/xxx/Mask_RCNN/mrcnn/utils.py in compute_overlaps_masks(masks1, masks2)  >        >          # intersections and union  > -->      intersections = np.dot(masks1.T, masks2)  >          union = area1[:, None] + area2[None, :] - intersections  >          overlaps = intersections / union  >   > ValueError: shapes (30,4343056) and (4460544,6) not aligned: 4343056 (dim 1) != 4460544 (dim 0)    The shape of the original image is (2084, 2084, 3) while the padded (molded) image is (2112, 2112, 3).    Can you tell me how to fix this? Thanks!"
"!   !   !   !   I used mask_RCNN  to detect the cars in the parking lot, I'm not sure why it worked pretty well for one frame and partially good for the other why the cars in the left half of the frame were not detected at al. I tried to cut off the frame and apply the model on the cropped frame separately, but the performance didn't change at all.  Do you have any idea?"
TypeError: Axis must be specified when shapes of a and weights differ.
"Hi,    If you pass one set of gt_masks and a empty set of predicted masks to funtion  `compute_overlaps_masks(masks1, masks2)`, there will be a error:  `ValueError: cannot reshape array of size 0 into shape (0)`    This happens when you are computing batch AP and some images have no instance detected.  "
"I am training this network on my own data. In my training dataset, I have many images without any annotated targets. These images are not unannotated. They are annotated by human but just do not contain any target object. For example, say I want to detect dogs. In addition to annotated images of dogs, I also show the network images of cat which do not have any annotated target as negative samples to teach the network what should not be dogs.    I modified `balloon.py`  to train on my own data, where I commended the following line to include such images that do not have any annotated target.          But when I test the trained model, I found it still detects some objects in my negative samples. So how are training images without any annotated target handled by your implementation? What should be the correct scenario to train the network with negative samples? Much appreciated for your help."
"/home/jgq/anaconda3/envs/python34/bin/python /media/jgq/GXL/project/2018/DDIM-OD/train_ddim.py train --dataset=data_process --weight=coco  Using TensorFlow backend.  Weights:  coco  Dataset:  data_process  Logs:  /media/jgq/GXL/project/2018/DDIM-OD/logs    Configurations:  BACKBONE                       resnet101  BACKBONE_SHAPES                [[256 256]   [128 128]   [ 64  64]   [ 32  32]   [ 16  16]]  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.9  DETECTION_NMS_THRESHOLD        0.3  GPU_COUNT                      1  IMAGES_PER_GPU                 1  IMAGE_MAX_DIM                  1024  IMAGE_MIN_DIM                  800  IMAGE_PADDING                  True  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           bioisland  NUM_CLASSES                    5  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                100  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               5  WEIGHT_DECAY                   0.0001      Loading weights  /media/jgq/GXL/project/2018/DDIM-OD/mask_rcnn_coco.h5  2018-05-08 09:34:54.784447: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.  2018-05-08 09:34:54.784468: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.  2018-05-08 09:34:54.784486: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.  2018-05-08 09:34:54.784489: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.  2018-05-08 09:34:54.784506: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.  2018-05-08 09:34:54.940052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2018-05-08 09:34:54.940281: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:   name: GeForce GTX 1080  major: 6 minor: 1 memoryClockRate (GHz) 1.835  pciBusID 0000:01:00.0  Total memory: 7.92GiB  Free memory: 7.41GiB  2018-05-08 09:34:54.940291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0   2018-05-08 09:34:54.940295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y   2018-05-08 09:34:54.940300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)  starting prepare train data  starting prepare val data  Training network heads    Starting at epoch 0. LR=0.001    Checkpoint Path: /media/jgq/GXL/project/2018/DDIM-OD/logs/bioisland20180508T0934/mask_rcnn_bioisland_{epoch:04d}.h5  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  In model:  rpn_model      rpn_conv_shared        (Conv2D)      rpn_class_raw          (Conv2D)      rpn_bbox_pred          (Conv2D)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)  /home/jgq/anaconda3/envs/python34/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  /home/jgq/anaconda3/envs/python34/lib/python3.4/site-packages/keras/engine/training.py:1987: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.    UserWarning('Using a generator with `use_multiprocessing=True`'  Epoch 1/10  /home/jgq/anaconda3/envs/python34/lib/python3.4/site-packages/scipy/ndimage/interpolation.py:616: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.    ""the returned array has changed."", UserWarning)  /home/jgq/anaconda3/envs/python34/lib/python3.4/site-packages/scipy/ndimage/interpolation.py:616: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.    ""the returned array has changed."", UserWarning)    1/100 [..............................] - ETA: 410s - loss: 4.8978 - rpn_class_loss: 0.0816 - rpn_bbox_loss: 3.0647 - mrcnn_class_loss: 1.7514 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00    2/100 [..............................] - ETA: 227s - loss: 4.6023 - rpn_class_loss: 0.0419 - rpn_bbox_loss: 1.9728 - mrcnn_class_loss: 1.4380 - mrcnn_bbox_loss: 0.6925 - mrcnn_mask_loss: 0.4571            3/100 [..............................] - ETA: 167s - loss: 3.8355 - rpn_class_loss: 0.0296 - rpn_bbox_loss: 1.4527 - mrcnn_class_loss: 1.1266 - mrcnn_bbox_loss: 0.7389 - mrcnn_mask_loss: 0.4877    4/100 [>.............................] - ETA: 136s - loss: 3.3419 - rpn_class_loss: 0.0360 - rpn_bbox_loss: 1.1600 - mrcnn_class_loss: 0.9058 - mrcnn_bbox_loss: 0.7312 - mrcnn_mask_loss: 0.5088    5/100 [>.............................] - ETA: 117s - loss: 3.5668 - rpn_class_loss: 0.0351 - rpn_bbox_loss: 1.8142 - mrcnn_class_loss: 0.7255 - mrcnn_bbox_loss: 0.5849 - mrcnn_mask_loss: 0.4071    6/100 [>.............................] - ETA: 105s - loss: 3.8032 - rpn_class_loss: 0.0336 - rpn_bbox_loss: 2.3381 - mrcnn_class_loss: 0.6048 - mrcnn_bbox_loss: 0.4874 - mrcnn_mask_loss: 0.3392    7/100 [=>............................] - ETA: 95s - loss: 3.6564 - rpn_class_loss: 0.0297 - rpn_bbox_loss: 2.1107 - mrcnn_class_loss: 0.5572 - mrcnn_bbox_loss: 0.5503 - mrcnn_mask_loss: 0.4085     8/100 [=>............................] - ETA: 89s - loss: 3.8016 - rpn_class_loss: 0.0435 - rpn_bbox_loss: 2.4317 - mrcnn_class_loss: 0.4876 - mrcnn_bbox_loss: 0.4815 - mrcnn_mask_loss: 0.3574    9/100 [=>............................] - ETA: 83s - loss: 3.6490 - rpn_class_loss: 0.0400 - rpn_bbox_loss: 2.2238 - mrcnn_class_loss: 0.4707 - mrcnn_bbox_loss: 0.5142 - mrcnn_mask_loss: 0.4003   10/100 [==>...........................] - ETA: 78s - loss: 3.8509 - rpn_class_loss: 0.0417 - rpn_bbox_loss: 2.3887 - mrcnn_class_loss: 0.4257 - mrcnn_bbox_loss: 0.5512 - mrcnn_mask_loss: 0.4436   11/100 [==>...........................] - ETA: 75s - loss: 3.7439 - rpn_class_loss: 0.0435 - rpn_bbox_loss: 2.2545 - mrcnn_class_loss: 0.4008 - mrcnn_bbox_loss: 0.5820 - mrcnn_mask_loss: 0.4631   12/100 [==>...........................] - ETA: 71s - loss: 3.6049 - rpn_class_loss: 0.0412 - rpn_bbox_loss: 2.0746 - mrcnn_class_loss: 0.3779 - mrcnn_bbox_loss: 0.6244 - mrcnn_mask_loss: 0.4869   13/100 [==>...........................] - ETA: 68s - loss: 3.5330 - rpn_class_loss: 0.0392 - rpn_bbox_loss: 1.9521 - mrcnn_class_loss: 0.3714 - mrcnn_bbox_loss: 0.6446 - mrcnn_mask_loss: 0.5257   14/100 [===>..........................] - ETA: 66s - loss: 3.4339 - rpn_class_loss: 0.0376 - rpn_bbox_loss: 1.8234 - mrcnn_class_loss: 0.3529 - mrcnn_bbox_loss: 0.6849 - mrcnn_mask_loss: 0.5351   15/100 [===>..........................] - ETA: 64s - loss: 3.2090 - rpn_class_loss: 0.0370 - rpn_bbox_loss: 1.7039 - mrcnn_class_loss: 0.3294 - mrcnn_bbox_loss: 0.6392 - mrcnn_mask_loss: 0.4994   16/100 [===>..........................] - ETA: 62s - loss: 3.1388 - rpn_class_loss: 0.0368 - rpn_bbox_loss: 1.6199 - mrcnn_class_loss: 0.3278 - mrcnn_bbox_loss: 0.6515 - mrcnn_mask_loss: 0.5028   17/100 [====>.........................] - ETA: 60s - loss: 3.0607 - rpn_class_loss: 0.0352 - rpn_bbox_loss: 1.5324 - mrcnn_class_loss: 0.3310 - mrcnn_bbox_loss: 0.6547 - mrcnn_mask_loss: 0.5075   18/100 [====>.........................] - ETA: 59s - loss: 3.0089 - rpn_class_loss: 0.0356 - rpn_bbox_loss: 1.4572 - mrcnn_class_loss: 0.3307 - mrcnn_bbox_loss: 0.6677 - mrcnn_mask_loss: 0.5178   19/100 [====>.........................] - ETA: 57s - loss: 2.9409 - rpn_class_loss: 0.0343 - rpn_bbox_loss: 1.3839 - mrcnn_class_loss: 0.3221 - mrcnn_bbox_loss: 0.6661 - mrcnn_mask_loss: 0.5346   20/100 [=====>........................] - ETA: 56s - loss: 2.9232 - rpn_class_loss: 0.0366 - rpn_bbox_loss: 1.3771 - mrcnn_class_loss: 0.3072 - mrcnn_bbox_loss: 0.6509 - mrcnn_mask_loss: 0.5513   21/100 [=====>........................] - ETA: 54s - loss: 3.0412 - rpn_class_loss: 0.0366 - rpn_bbox_loss: 1.5670 - mrcnn_class_loss: 0.2925 - mrcnn_bbox_loss: 0.6199 - mrcnn_mask_loss: 0.5251   22/100 [=====>........................] - ETA: 53s - loss: 2.9590 - rpn_class_loss: 0.0355 - rpn_bbox_loss: 1.5090 - mrcnn_class_loss: 0.2831 - mrcnn_bbox_loss: 0.6127 - mrcnn_mask_loss: 0.5186   23/100 [=====>........................] - ETA: 52s - loss: 2.9014 - rpn_class_loss: 0.0344 - rpn_bbox_loss: 1.4518 - mrcnn_class_loss: 0.2767 - mrcnn_bbox_loss: 0.6211 - mrcnn_mask_loss: 0.5174   24/100 [======>.......................] - ETA: 51s - loss: 2.9078 - rpn_class_loss: 0.0335 - rpn_bbox_loss: 1.4113 - mrcnn_class_loss: 0.2771 - mrcnn_bbox_loss: 0.6342 - mrcnn_mask_loss: 0.5518   25/100 [======>.......................] - ETA: 50s - loss: 2.8614 - rpn_class_loss: 0.0322 - rpn_bbox_loss: 1.3599 - mrcnn_class_loss: 0.2834 - mrcnn_bbox_loss: 0.6357 - mrcnn_mask_loss: 0.5503   26/100 [======>.......................] - ETA: 48s - loss: 2.9103 - rpn_class_loss: 0.0318 - rpn_bbox_loss: 1.3994 - mrcnn_class_loss: 0.2798 - mrcnn_bbox_loss: 0.6401 - mrcnn_mask_loss: 0.5592   27/100 [=======>......................] - ETA: 47s - loss: 2.9387 - rpn_class_loss: 0.0311 - rpn_bbox_loss: 1.3674 - mrcnn_class_loss: 0.2707 - mrcnn_bbox_loss: 0.7049 - mrcnn_mask_loss: 0.5647   28/100 [=======>......................] - ETA: 46s - loss: 2.9125 - rpn_class_loss: 0.0338 - rpn_bbox_loss: 1.3246 - mrcnn_class_loss: 0.2724 - mrcnn_bbox_loss: 0.7119 - mrcnn_mask_loss: 0.5698   29/100 [=======>......................] - ETA: 45s - loss: 2.8980 - rpn_class_loss: 0.0332 - rpn_bbox_loss: 1.3070 - mrcnn_class_loss: 0.2673 - mrcnn_bbox_loss: 0.7040 - mrcnn_mask_loss: 0.5865   30/100 [========>.....................] - ETA: 44s - loss: 2.8521 - rpn_class_loss: 0.0332 - rpn_bbox_loss: 1.2662 - mrcnn_class_loss: 0.2593 - mrcnn_bbox_loss: 0.7140 - mrcnn_mask_loss: 0.5794   31/100 [========>.....................] - ETA: 44s - loss: 2.8123 - rpn_class_loss: 0.0322 - rpn_bbox_loss: 1.2309 - mrcnn_class_loss: 0.2539 - mrcnn_bbox_loss: 0.7100 - mrcnn_mask_loss: 0.5854   32/100 [========>.....................] - ETA: 43s - loss: 2.7963 - rpn_class_loss: 0.0315 - rpn_bbox_loss: 1.2160 - mrcnn_class_loss: 0.2501 - mrcnn_bbox_loss: 0.7122 - mrcnn_mask_loss: 0.5864   33/100 [========>.....................] - ETA: 42s - loss: 2.7763 - rpn_class_loss: 0.0307 - rpn_bbox_loss: 1.1974 - mrcnn_class_loss: 0.2475 - mrcnn_bbox_loss: 0.7117 - mrcnn_mask_loss: 0.5891   34/100 [=========>....................] - ETA: 41s - loss: 2.7392 - rpn_class_loss: 0.0298 - rpn_bbox_loss: 1.1679 - mrcnn_class_loss: 0.2477 - mrcnn_bbox_loss: 0.7080 - mrcnn_mask_loss: 0.5859   35/100 [=========>....................] - ETA: 40s - loss: 2.6995 - rpn_class_loss: 0.0290 - rpn_bbox_loss: 1.1398 - mrcnn_class_loss: 0.2453 - mrcnn_bbox_loss: 0.7040 - mrcnn_mask_loss: 0.5814   36/100 [=========>....................] - ETA: 39s - loss: 2.6515 - rpn_class_loss: 0.0285 - rpn_bbox_loss: 1.1112 - mrcnn_class_loss: 0.2396 - mrcnn_bbox_loss: 0.6964 - mrcnn_mask_loss: 0.5758   37/100 [==========>...................] - ETA: 38s - loss: 2.6511 - rpn_class_loss: 0.0279 - rpn_bbox_loss: 1.0910 - mrcnn_class_loss: 0.2358 - mrcnn_bbox_loss: 0.7180 - mrcnn_mask_loss: 0.5784   38/100 [==========>...................] - ETA: 38s - loss: 2.6420 - rpn_class_loss: 0.0288 - rpn_bbox_loss: 1.0662 - mrcnn_class_loss: 0.2348 - mrcnn_bbox_loss: 0.7308 - mrcnn_mask_loss: 0.5814   39/100 [==========>...................] - ETA: 37s - loss: 2.6040 - rpn_class_loss: 0.0281 - rpn_bbox_loss: 1.0419 - mrcnn_class_loss: 0.2337 - mrcnn_bbox_loss: 0.7211 - mrcnn_mask_loss: 0.5791   40/100 [===========>..................] - ETA: 36s - loss: 2.5944 - rpn_class_loss: 0.0285 - rpn_bbox_loss: 1.0267 - mrcnn_class_loss: 0.2283 - mrcnn_bbox_loss: 0.7277 - mrcnn_mask_loss: 0.5831   41/100 [===========>..................] - ETA: 35s - loss: 2.5873 - rpn_class_loss: 0.0282 - rpn_bbox_loss: 1.0149 - mrcnn_class_loss: 0.2246 - mrcnn_bbox_loss: 0.7337 - mrcnn_mask_loss: 0.5858   42/100 [===========>..................] - ETA: 35s - loss: 2.5803 - rpn_class_loss: 0.0277 - rpn_bbox_loss: 1.0036 - mrcnn_class_loss: 0.2217 - mrcnn_bbox_loss: 0.7391 - mrcnn_mask_loss: 0.5882   43/100 [===========>..................] - ETA: 34s - loss: 2.5557 - rpn_class_loss: 0.0283 - rpn_bbox_loss: 0.9826 - mrcnn_class_loss: 0.2192 - mrcnn_bbox_loss: 0.7358 - mrcnn_mask_loss: 0.5897   44/100 [============>.................] - ETA: 33s - loss: 2.5287 - rpn_class_loss: 0.0284 - rpn_bbox_loss: 0.9633 - mrcnn_class_loss: 0.2175 - mrcnn_bbox_loss: 0.7276 - mrcnn_mask_loss: 0.5919   45/100 [============>.................] - ETA: 33s - loss: 2.5293 - rpn_class_loss: 0.0278 - rpn_bbox_loss: 0.9624 - mrcnn_class_loss: 0.2154 - mrcnn_bbox_loss: 0.7320 - mrcnn_mask_loss: 0.5915   46/100 [============>.................] - ETA: 32s - loss: 2.5068 - rpn_class_loss: 0.0273 - rpn_bbox_loss: 0.9450 - mrcnn_class_loss: 0.2142 - mrcnn_bbox_loss: 0.7298 - mrcnn_mask_loss: 0.5906   47/100 [=============>................] - ETA: 31s - loss: 2.5437 - rpn_class_loss: 0.0270 - rpn_bbox_loss: 0.9406 - mrcnn_class_loss: 0.2123 - mrcnn_bbox_loss: 0.7358 - mrcnn_mask_loss: 0.6279   48/100 [=============>................] - ETA: 30s - loss: 2.5540 - rpn_class_loss: 0.0267 - rpn_bbox_loss: 0.9279 - mrcnn_class_loss: 0.2088 - mrcnn_bbox_loss: 0.7574 - mrcnn_mask_loss: 0.6333   49/100 [=============>................] - ETA: 30s - loss: 2.5458 - rpn_class_loss: 0.0266 - rpn_bbox_loss: 0.9095 - mrcnn_class_loss: 0.2067 - mrcnn_bbox_loss: 0.7616 - mrcnn_mask_loss: 0.6414   50/100 [==============>...............] - ETA: 29s - loss: 2.5201 - rpn_class_loss: 0.0265 - rpn_bbox_loss: 0.8928 - mrcnn_class_loss: 0.2037 - mrcnn_bbox_loss: 0.7615 - mrcnn_mask_loss: 0.6357   51/100 [==============>...............] - ETA: 28s - loss: 2.5022 - rpn_class_loss: 0.0260 - rpn_bbox_loss: 0.8822 - mrcnn_class_loss: 0.2015 - mrcnn_bbox_loss: 0.7587 - mrcnn_mask_loss: 0.6337   52/100 [==============>...............] - ETA: 28s - loss: 2.4968 - rpn_class_loss: 0.0275 - rpn_bbox_loss: 0.8696 - mrcnn_class_loss: 0.1997 - mrcnn_bbox_loss: 0.7650 - mrcnn_mask_loss: 0.6350   53/100 [==============>...............] - ETA: 27s - loss: 2.4723 - rpn_class_loss: 0.0271 - rpn_bbox_loss: 0.8541 - mrcnn_class_loss: 0.1970 - mrcnn_bbox_loss: 0.7623 - mrcnn_mask_loss: 0.6317   54/100 [===============>..............] - ETA: 27s - loss: 2.4641 - rpn_class_loss: 0.0266 - rpn_bbox_loss: 0.8463 - mrcnn_class_loss: 0.2004 - mrcnn_bbox_loss: 0.7622 - mrcnn_mask_loss: 0.6286   55/100 [===============>..............] - ETA: 26s - loss: 2.4351 - rpn_class_loss: 0.0262 - rpn_bbox_loss: 0.8310 - mrcnn_class_loss: 0.1980 - mrcnn_bbox_loss: 0.7547 - mrcnn_mask_loss: 0.6253   56/100 [===============>..............] - ETA: 25s - loss: 2.4102 - rpn_class_loss: 0.0260 - rpn_bbox_loss: 0.8179 - mrcnn_class_loss: 0.1966 - mrcnn_bbox_loss: 0.7481 - mrcnn_mask_loss: 0.6217   57/100 [================>.............] - ETA: 25s - loss: 2.4039 - rpn_class_loss: 0.0256 - rpn_bbox_loss: 0.8141 - mrcnn_class_loss: 0.1937 - mrcnn_bbox_loss: 0.7492 - mrcnn_mask_loss: 0.6213   58/100 [================>.............] - ETA: 24s - loss: 2.3975 - rpn_class_loss: 0.0255 - rpn_bbox_loss: 0.8030 - mrcnn_class_loss: 0.1923 - mrcnn_bbox_loss: 0.7560 - mrcnn_mask_loss: 0.6208   59/100 [================>.............] - ETA: 23s - loss: 2.3806 - rpn_class_loss: 0.0258 - rpn_bbox_loss: 0.7909 - mrcnn_class_loss: 0.1910 - mrcnn_bbox_loss: 0.7516 - mrcnn_mask_loss: 0.6213   60/100 [=================>............] - ETA: 23s - loss: 2.3913 - rpn_class_loss: 0.0266 - rpn_bbox_loss: 0.8070 - mrcnn_class_loss: 0.1888 - mrcnn_bbox_loss: 0.7484 - mrcnn_mask_loss: 0.6204   61/100 [=================>............] - ETA: 22s - loss: 2.3715 - rpn_class_loss: 0.0264 - rpn_bbox_loss: 0.7956 - mrcnn_class_loss: 0.1860 - mrcnn_bbox_loss: 0.7439 - mrcnn_mask_loss: 0.6195   62/100 [=================>............] - ETA: 21s - loss: 2.3536 - rpn_class_loss: 0.0260 - rpn_bbox_loss: 0.7846 - mrcnn_class_loss: 0.1848 - mrcnn_bbox_loss: 0.7423 - mrcnn_mask_loss: 0.6159   63/100 [=================>............] - ETA: 21s - loss: 2.3325 - rpn_class_loss: 0.0262 - rpn_bbox_loss: 0.7737 - mrcnn_class_loss: 0.1826 - mrcnn_bbox_loss: 0.7372 - mrcnn_mask_loss: 0.6128   64/100 [==================>...........] - ETA: 20s - loss: 2.3176 - rpn_class_loss: 0.0260 - rpn_bbox_loss: 0.7637 - mrcnn_class_loss: 0.1821 - mrcnn_bbox_loss: 0.7365 - mrcnn_mask_loss: 0.6094   65/100 [==================>...........] - ETA: 20s - loss: 2.3006 - rpn_class_loss: 0.0256 - rpn_bbox_loss: 0.7541 - mrcnn_class_loss: 0.1805 - mrcnn_bbox_loss: 0.7343 - mrcnn_mask_loss: 0.6061   66/100 [==================>...........] - ETA: 19s - loss: 2.2798 - rpn_class_loss: 0.0254 - rpn_bbox_loss: 0.7430 - mrcnn_class_loss: 0.1788 - mrcnn_bbox_loss: 0.7285 - mrcnn_mask_loss: 0.6041   67/100 [===================>..........] - ETA: 18s - loss: 2.2792 - rpn_class_loss: 0.0253 - rpn_bbox_loss: 0.7339 - mrcnn_class_loss: 0.1774 - mrcnn_bbox_loss: 0.7392 - mrcnn_mask_loss: 0.6033   68/100 [===================>..........] - ETA: 18s - loss: 2.2696 - rpn_class_loss: 0.0253 - rpn_bbox_loss: 0.7277 - mrcnn_class_loss: 0.1765 - mrcnn_bbox_loss: 0.7393 - mrcnn_mask_loss: 0.6009   69/100 [===================>..........] - ETA: 17s - loss: 2.2470 - rpn_class_loss: 0.0252 - rpn_bbox_loss: 0.7173 - mrcnn_class_loss: 0.1743 - mrcnn_bbox_loss: 0.7320 - mrcnn_mask_loss: 0.5982   70/100 [====================>.........] - ETA: 17s - loss: 2.2425 - rpn_class_loss: 0.0250 - rpn_bbox_loss: 0.7183 - mrcnn_class_loss: 0.1723 - mrcnn_bbox_loss: 0.7306 - mrcnn_mask_loss: 0.5963   71/100 [====================>.........] - ETA: 16s - loss: 2.2329 - rpn_class_loss: 0.0252 - rpn_bbox_loss: 0.7120 - mrcnn_class_loss: 0.1704 - mrcnn_bbox_loss: 0.7306 - mrcnn_mask_loss: 0.5948   72/100 [====================>.........] - ETA: 15s - loss: 2.2095 - rpn_class_loss: 0.0250 - rpn_bbox_loss: 0.7027 - mrcnn_class_loss: 0.1684 - mrcnn_bbox_loss: 0.7212 - mrcnn_mask_loss: 0.5923   73/100 [====================>.........] - ETA: 15s - loss: 2.1931 - rpn_class_loss: 0.0250 - rpn_bbox_loss: 0.6946 - mrcnn_class_loss: 0.1664 - mrcnn_bbox_loss: 0.7163 - mrcnn_mask_loss: 0.5908   74/100 [=====================>........] - ETA: 14s - loss: 2.1853 - rpn_class_loss: 0.0247 - rpn_bbox_loss: 0.6875 - mrcnn_class_loss: 0.1650 - mrcnn_bbox_loss: 0.7178 - mrcnn_mask_loss: 0.5904   75/100 [=====================>........] - ETA: 14s - loss: 2.1696 - rpn_class_loss: 0.0245 - rpn_bbox_loss: 0.6787 - mrcnn_class_loss: 0.1646 - mrcnn_bbox_loss: 0.7162 - mrcnn_mask_loss: 0.5855   76/100 [=====================>........] - ETA: 13s - loss: 2.1727 - rpn_class_loss: 0.0246 - rpn_bbox_loss: 0.6703 - mrcnn_class_loss: 0.1633 - mrcnn_bbox_loss: 0.7286 - mrcnn_mask_loss: 0.5860   77/100 [======================>.......] - ETA: 13s - loss: 2.1647 - rpn_class_loss: 0.0244 - rpn_bbox_loss: 0.6659 - mrcnn_class_loss: 0.1633 - mrcnn_bbox_loss: 0.7272 - mrcnn_mask_loss: 0.5841   78/100 [======================>.......] - ETA: 12s - loss: 2.1554 - rpn_class_loss: 0.0241 - rpn_bbox_loss: 0.6583 - mrcnn_class_loss: 0.1618 - mrcnn_bbox_loss: 0.7292 - mrcnn_mask_loss: 0.5821   79/100 [======================>.......] - ETA: 11s - loss: 2.1430 - rpn_class_loss: 0.0238 - rpn_bbox_loss: 0.6537 - mrcnn_class_loss: 0.1608 - mrcnn_bbox_loss: 0.7239 - mrcnn_mask_loss: 0.5807   80/100 [=======================>......] - ETA: 11s - loss: 2.1310 - rpn_class_loss: 0.0236 - rpn_bbox_loss: 0.6490 - mrcnn_class_loss: 0.1600 - mrcnn_bbox_loss: 0.7192 - mrcnn_mask_loss: 0.5793   81/100 [=======================>......] - ETA: 10s - loss: 2.1212 - rpn_class_loss: 0.0235 - rpn_bbox_loss: 0.6438 - mrcnn_class_loss: 0.1587 - mrcnn_bbox_loss: 0.7182 - mrcnn_mask_loss: 0.5771   82/100 [=======================>......] - ETA: 10s - loss: 2.1098 - rpn_class_loss: 0.0233 - rpn_bbox_loss: 0.6370 - mrcnn_class_loss: 0.1572 - mrcnn_bbox_loss: 0.7177 - mrcnn_mask_loss: 0.5746   83/100 [=======================>......] - ETA: 9s - loss: 2.1073 - rpn_class_loss: 0.0233 - rpn_bbox_loss: 0.6336 - mrcnn_class_loss: 0.1566 - mrcnn_bbox_loss: 0.7121 - mrcnn_mask_loss: 0.5816    84/100 [========================>.....] - ETA: 9s - loss: 2.0990 - rpn_class_loss: 0.0230 - rpn_bbox_loss: 0.6278 - mrcnn_class_loss: 0.1558 - mrcnn_bbox_loss: 0.7110 - mrcnn_mask_loss: 0.5813   85/100 [========================>.....] - ETA: 8s - loss: 2.0850 - rpn_class_loss: 0.0229 - rpn_bbox_loss: 0.6213 - mrcnn_class_loss: 0.1548 - mrcnn_bbox_loss: 0.7068 - mrcnn_mask_loss: 0.5791   86/100 [========================>.....] - ETA: 7s - loss: 2.0820 - rpn_class_loss: 0.0229 - rpn_bbox_loss: 0.6153 - mrcnn_class_loss: 0.1557 - mrcnn_bbox_loss: 0.7089 - mrcnn_mask_loss: 0.5792   87/100 [=========================>....] - ETA: 7s - loss: 2.0718 - rpn_class_loss: 0.0229 - rpn_bbox_loss: 0.6095 - mrcnn_class_loss: 0.1549 - mrcnn_bbox_loss: 0.7080 - mrcnn_mask_loss: 0.5765   88/100 [=========================>....] - ETA: 6s - loss: 2.0709 - rpn_class_loss: 0.0226 - rpn_bbox_loss: 0.6080 - mrcnn_class_loss: 0.1566 - mrcnn_bbox_loss: 0.7071 - mrcnn_mask_loss: 0.5765   89/100 [=========================>....] - ETA: 6s - loss: 2.0713 - rpn_class_loss: 0.0224 - rpn_bbox_loss: 0.6134 - mrcnn_class_loss: 0.1550 - mrcnn_bbox_loss: 0.7070 - mrcnn_mask_loss: 0.5735   90/100 [==========================>...] - ETA: 5s - loss: 2.0624 - rpn_class_loss: 0.0225 - rpn_bbox_loss: 0.6204 - mrcnn_class_loss: 0.1532 - mrcnn_bbox_loss: 0.6992 - mrcnn_mask_loss: 0.5672   91/100 [==========================>...] - ETA: 5s - loss: 2.0626 - rpn_class_loss: 0.0224 - rpn_bbox_loss: 0.6145 - mrcnn_class_loss: 0.1550 - mrcnn_bbox_loss: 0.7043 - mrcnn_mask_loss: 0.5664   92/100 [==========================>...] - ETA: 4s - loss: 2.0539 - rpn_class_loss: 0.0222 - rpn_bbox_loss: 0.6097 - mrcnn_class_loss: 0.1544 - mrcnn_bbox_loss: 0.7012 - mrcnn_mask_loss: 0.5663   93/100 [==========================>...] - ETA: 3s - loss: 2.0421 - rpn_class_loss: 0.0220 - rpn_bbox_loss: 0.6048 - mrcnn_class_loss: 0.1533 - mrcnn_bbox_loss: 0.6980 - mrcnn_mask_loss: 0.5640   94/100 [===========================>..] - ETA: 3s - loss: 2.0283 - rpn_class_loss: 0.0220 - rpn_bbox_loss: 0.5988 - mrcnn_class_loss: 0.1522 - mrcnn_bbox_loss: 0.6936 - mrcnn_mask_loss: 0.5616   95/100 [===========================>..] - ETA: 2s - loss: 2.0198 - rpn_class_loss: 0.0220 - rpn_bbox_loss: 0.5942 - mrcnn_class_loss: 0.1509 - mrcnn_bbox_loss: 0.6867 - mrcnn_mask_loss: 0.5659   96/100 [===========================>..] - ETA: 2s - loss: 2.0161 - rpn_class_loss: 0.0218 - rpn_bbox_loss: 0.5911 - mrcnn_class_loss: 0.1504 - mrcnn_bbox_loss: 0.6898 - mrcnn_mask_loss: 0.5630   97/100 [============================>.] - ETA: 1s - loss: 2.0079 - rpn_class_loss: 0.0219 - rpn_bbox_loss: 0.5857 - mrcnn_class_loss: 0.1496 - mrcnn_bbox_loss: 0.6871 - mrcnn_mask_loss: 0.5636   98/100 [============================>.] - ETA: 1s - loss: 2.0016 - rpn_class_loss: 0.0218 - rpn_bbox_loss: 0.5823 - mrcnn_class_loss: 0.1491 - mrcnn_bbox_loss: 0.6878 - mrcnn_mask_loss: 0.5607   99/100 [============================>.] - ETA: 0s - loss: 2.0148 - rpn_class_loss: 0.0216 - rpn_bbox_loss: 0.5795 - mrcnn_class_loss: 0.1480 - mrcnn_bbox_loss: 0.6850 - mrcnn_mask_loss: 0.5807Epoch 00000: val_loss improved from inf to 1.17010, saving model to /media/jgq/GXL/project/2018/DDIM-OD/logs/bioisland20180508T0934/mask_rcnn_bioisland_0000.h5  Traceback (most recent call last):    File ""/media/jgq/GXL/project/2018/DDIM-OD/train_ddim.py"", line 329, in        train(model)    File ""/media/jgq/GXL/project/2018/DDIM-OD/train_ddim.py"", line 213, in train      layers='heads')    File ""/media/jgq/GXL/project/2018/DDIM-OD/model.py"", line 2252, in train      use_multiprocessing=True,    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/site-packages/keras/legacy/interfaces.py"", line 87, in wrapper      return func(*args, **kwargs)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/site-packages/keras/engine/training.py"", line 2082, in fit_generator      callbacks.on_epoch_end(epoch, epoch_logs)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/site-packages/keras/callbacks.py"", line 77, in on_epoch_end      callback.on_epoch_end(epoch, logs)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/site-packages/keras/callbacks.py"", line 417, in on_epoch_end      self.model.save(filepath, overwrite=True)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/site-packages/keras/engine/topology.py"", line 2553, in save      save_model(self, filepath, overwrite, include_optimizer)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/site-packages/keras/models.py"", line 107, in save_model      'config': model.get_config()    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/site-packages/keras/engine/topology.py"", line 2394, in get_config      return copy.deepcopy(config)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 226, in _deepcopy_tuple      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 226, in _deepcopy_tuple      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 219, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 226, in _deepcopy_tuple      y.append(deepcopy(a, memo))    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 300, in _reconstruct      state = deepcopy(state, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 155, in deepcopy      y = copier(x, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 246, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 182, in deepcopy      y = _reconstruct(x, rv, 1, memo)    File ""/home/jgq/anaconda3/envs/python34/lib/python3.4/copy.py"", line 309, in _reconstruct      y.__dict__.update(state)  AttributeError: 'NoneType' object has no attribute 'update'    Process finished with exit code 1  "
"Almost 1/3 of the images I have to train are close to 6k resolution, with the average training image size being around 2.7k. The segmentation masks also occasionally take up a large fraction of the total image. I'm getting infrequent errors that I think may be related to these large sizes. Using a Tesla P100 to train.     Any advice on config settings for this large image/mask situation?"
"Hi! I'm trying to implement mask-rcnn on a Movidius neural stick. For that purpose I've exported the graph and weights in a .pb format. However, due to Movidius requirements, I'd need to export the graph with only one output node. Is it feasible? Thanks  "
"Hi,    First, I wanna say thank you for providing this great implementation of Mask_RCNN. I am using this repo for a while but can't figure out a proper training strategy, so I hope someone can give me some suggestions.    Image: CBIS-DDSM (X-ray images)  training:981, validation:250.  Roughly one instance per image( highly class imbalance).    A sample image and correspoing mask:         My Configurations:  BACKBONE                       resnet101  BATCH_SIZE                     2  DETECTION_MIN_CONFIDENCE       0.7  DETECTION_NMS_THRESHOLD        0.3  IMAGE_MAX_DIM                  512  IMAGE_MIN_DIM                  512  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [512 512   3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  MEAN_PIXEL                     [53.129 53.129 53.129]  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (16, 32, 64, 128, 256)  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    512  TRAIN_ROIS_PER_IMAGE           320  WEIGHT_DECAY                   0.0001    Augmentation:  augmentation = iaa.SomeOf((0, 2), [      iaa.Fliplr(0.5),      iaa.Flipud(0.5),])    I read many issues in this repo, and I summarized several strategies that could possibly help me.  1. Use data augmentation.  2. Innit with coco weights.  3. Only train classifier without changing the backbone weights of coco(this is from Andrew Ng's onlie course, that he said if you only have several hundred images, only train the classifier is a good idea)  4. Try to generate more positive proposals.    Based on these suggestions, I did some incomplete research. But I haven't got nice predictions.  "
"I ran upon this error when executing the demo.ipynb: ""Coco has no attribute CocoConfig"" (altough the coco module was imported successfully)  I searched really long for an answer. Finally, I discovered that in the coco.py file (from:   as suggested by the installation guide from this repository) does indeed not contain a class called CocoConfig.   I thought I downloaded the wrong repository, but that was not the case. Then I found a code snippet here:   that contained the class CocoConfig. I ended up just pasting the class definition into the demo.py file.        And it worked! It is really annoying that the demo example already contains mistakes. But maybe someone else will find this helpful.  But on the other hand: I would not think that this is really a mistake, so what did I do wrong? I followed all the instructions (and also additional instructions from here:      So this ""bug"" is already solved but I thought this experience could save some time for others having the same problem."
"I'm getting this error very intermittently during training (approx. 10 times per 1000 iterations). I have variable size images and masks, so I'm thinking this may be an issue with some of the very large images in my dataset (for example, sizes 5456x3632, 2592x1944, etc.). It continues to train without crashing due to the error, but I'm unsure if there will be any negative consequences later on. I have `IMAGE_MAX_DIM=1024`.     "
"I'm stucked in this codes 2 weeks ago, I cannot resolve the problem until now.  In function image_stats, there always be a problem named 'OSError:    !   !             "
"## TypeError: can't pickle _thread.lock objects  I want to save all model not only weights ,so i changed   save_weights_only=True   to     save_weights_only=False. But it occur the error  everytime the code parpares to save checkpoints ."
"Traceback (most recent call last):    File ""/disk1/g201708021059/anaconda3/envs/tensorflow/lib/python3.6/threading.py"", line 916, in _bootstrap_inner      self.run()    File ""/disk1/g201708021059/anaconda3/envs/tensorflow/lib/python3.6/threading.py"", line 864, in run      self._target(*self._args, **self._kwargs)    File ""/disk1/g201708021059/anaconda3/envs/tensorflow/lib/python3.6/multiprocessing/managers.py"", line 177, in accepter      t.start()    File ""/disk1/g201708021059/anaconda3/envs/tensorflow/lib/python3.6/threading.py"", line 846, in start      _start_new_thread(self._bootstrap, ())  RuntimeError: can't start new thread      thanks so much! "
"Hi,  when I train on my own data sets with the sample balloon.py  I set `IMAGES_PER_GPU = 1`  some data sets works well, but others fail with logs  `G:\Python\Python36\lib\site-packages\tensorflow\python\ops\gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  Epoch 1/30  2018-2018-05-05 22:23:50.024744: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.71GiB. The caller indicates that this is n  ot a failure, but may mean that there could be performance gains if more memory were available.  2018-05-05 22:23:51.399874: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allo2018-05-05 22:23:51.436205: W T:\src\github\tensorflow  \tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.76GiB. The caller indicates that this is not a failure, but may mean that there could be performance  gains if more memory were available.  2018-05-2018-05-05 22:23:51.530532: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.51GiB. The caller indicates that this i  s not a fa2018-05-05 22:23:52.481737: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.69GiB. The caller indicates that this   is not a failure, but may mean that there could be performance gains if more memory were available.  2018-05-05 22:23:52.797468: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.69GiB. The caller indicates that this is not a  failure, but may mean that there could be performance gains if more memory were available.  2018-05-05 22:23:52.978723: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.25GiB. The caller indicates that this is not a  failure, but may mean that there could be performance gains if more memory were available.  2018-05-05 22:23:54.121217: W T:\src\github\tensorflow\tensorflow\core\common_runtime\bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.05GiB. The caller indicates that this is not a    3/100 [..............................] - ETA: 15:01 - loss: 2.4849 - rpn_class_loss: 0.0260 - rpn_bbox_loss: 0.4997 - mrcnn_class_loss: 0.3269 - mrcnn_bbox_loss: 0.9418 - mrcnn_mask_loss: 0.69042018-05-05 22:24:12.  091627: F T:/src/github/tensorflow\tensorflow/core/util/cuda_launch_config.h:127] Check failed: work_element_count > 0 (0 vs. 0)  `  My hardware  Nvida Quadro K1200  with 4GB memory  My environment  tensorflow 1.8.0, cuda 9.0   "
"Hello!    I wanted to retrain this network to output bounding boxes + classifications, without the Mask.    Is there a simple way to do it?  Maybe something hacky like setting the loss function weights related to the mask portion of the network to 0 and inputting random masks for training?    Thank you  João Antunes"
"As of today it looks to me that ""mrcnn/__init__.py"" is an empty file, making impossibile to install the repository.  Am I missing something?"
UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.
"Got to 93/1000 then 30/1000 on Epoch 1/40, first and second tries respectively, when this error occurred. Running unedited coco.py with `python samples/coco/coco.py train --dataset=/home/coco_dataset --model=coco --download=True`    system: nvidia-docker on a GCE with 8 CPUs, 30GB RAM, Tesla P100   versions: Python 3.6.4, Cuda compilation tools, release 9.0, V9.0.176, tensorflow-gpu 1.7.0, Keras 2.1.5"
"i think the elements in the list are in reverse order. It should be (128, 64, 32, 16, 8).  Big feature map should generate big region proposal.  "
"Hi there,     When training on multiple GPUs with the latest master branch, fails with Tensorflow 1.4.0. There is a zero division going on when we have parallel model. Has anybody tested this with TF  <= 1.4.0 ?"
"Hi folks,    I'm currently trying to work with some mask with are all background, i.e., no object is present in the image.    How am I supposed to load those masks? In particular, which is the expected output of load_mask method?    Thank you."
"The line at      `low_score_idx = np.where(overlaps[i, sorted_ixs] < score_threshold)[0]`    filters the idx with overlap boxes with lower iou than `score_threshold`. But `overlaps` array contains iou of each box and not the `score` of each box isnt?  "
"I want to extend a new task after the res50 model,how can I do?    extend a softmax classifier."
"(using tensorboard)  and the same, the class loss you posted started at a quite small number?  But when I train the model from scratch, the box loss and class loss start from 0.7~0.9 and I think it might take very long  time to descent to 0.14 or even 0.02.  Could you show the whole complete training log please?"
"hi i want to import skimage.io but it has this error  ImportError                               Traceback (most recent call last)    in  ()  ----> 1 import skimage.io    C:\Python\Anaconda3\lib\site-packages\skimage\io\__init__.py in  ()        5 """"""        6   ----> 7 from .manage_plugins import *        8 from .sift import *        9 from .collection import *    C:\Python\Anaconda3\lib\site-packages\skimage\io\manage_plugins.py in  ()       26 from glob import glob       27   ---> 28 from .collection import imread_collection_wrapper       29        30     C:\Python\Anaconda3\lib\site-packages\skimage\io\collection.py in  ()       10 import numpy as np       11 import six  ---> 12 from PIL import Image       13        14 from ..external.tifffile import TiffFile    C:\Python\Anaconda3\lib\site-packages\PIL\Image.py in  ()       56     # Also note that Image.core is not a publicly documented interface,       57     # and should be considered private and subject to change.  ---> 58     from . import _imaging as core       59     if PILLOW_VERSION != getattr(core, 'PILLOW_VERSION', None):       60         raise ImportError(""The _imaging extension was built for another ""    ImportError: DLL load failed: The specified module could not be found.          please help me to solve it"
None
"I am trying to train Mask RCNN on my own data but I get the errors below when training or loading the mini mask or using the modellib.data_generator.    Anyone else had this problem?    ERROR:root:Error processing image {'id': 'location-494-270.jpg', 'source': 'flowers', 'path': '/home/ubuntu/ssl/notebooks/Mask_RCNN/Mask_RCNN/datasets/flowers/train/file2.jpg', 'width': 640, 'height': 400, 'polygons': [{'name': 'rect', 'x': 301, 'y': 178, 'width': 40, 'height': 93}]}  Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1692, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1207, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""flowers.py"", line 158, in load_mask      rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])  KeyError: 'all_points_y'Epoch 1/30  "
"I try to change num_classes and not use the last layer with  `model.load_weights(COCO_MODEL_PATH, by_name=True,                             exclude=[""mrcnn_class_logits"", ""mrcnn_bbox_fc"",                                      ""mrcnn_bbox"", ""mrcnn_mask""])`    but i got this :   `ValueError: Error when checking input: expected input_image_meta to have shape (16,) but got array with shape (93,)`    however it runs ok with NUM_CLASSES=81  "
"Thank you for this awesome repository! I am new in this field. I try to fine-tune network on my own data (images of person indoor). I realized that with complex objects like person I get not good enough mask. I plan increase MASK_SHAPE during training (for example, [128, 128]). Will I get better result? Should I train with full coco dataset then? Or something else must be changed? I need to get very precise mask of person (close to the contour). May be it is not possible?"
"When I run to evaluate on a coco data-set, I am getting the following results. Can you please help me understand the output.     Evaluate  loading annotations into memory...  Done (t=4.37s)  creating index...  index created!  Running COCO evaluation on 20 images.  Loading and preparing results...  DONE (t=0.00s)  creating index...  index created!  Running per image evaluation...  Evaluate annotation type *bbox*  DONE (t=0.14s).  Accumulating evaluation results...  DONE (t=0.18s).   Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.498   Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.760   Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.539   Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.315   Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.508   Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.712   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.438   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.508   Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.511   Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.325   Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.548   Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.726  Prediction time: 36.726747035980225. Average 1.8363373517990111/image  Total time:  40.04298138618469    I am not able to understand different precision value mentioned,"
"I am trying to train the mask rcnn mode with imagenet weights on a custom dataset on a gtx 1080ti 11gb on Ubuntu 16.04. Regardless of what image resolution that I use, the kernel runs out of memory and crashes whenever I call train(). I am using a batch size of one. Before I restart the kernel, I kill all python processes to clear the GPU memory. What could be causing the memory issue? The demos runs perfectly. I can also run all the inspect_data code on my dataset, and everything looks kosher. Posted below is my config output.        "
I don't know why I'm getting this error. Please help asap.      Loading weights  /output/Mask_RCNN/mask_rcnn_coco.h5  *** Error in `python3': double free or corruption (out): 0x00000000106e4150 ***  ======= Backtrace: =========  /lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f9cf42577e5]  /lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7f9cf426037a]  /lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7f9cf426453c]  /usr/local/nvidia/lib64/libcuda.so.1(+0x2ee19c)[0x7f9c52ea919c]  /usr/local/nvidia/lib64/libcuda.so.1(+0x2ee1e3)[0x7f9c52ea91e3]  /usr/local/nvidia/lib64/libcuda.so.1(+0x2ee484)[0x7f9c52ea9484]  /usr/local/nvidia/lib64/libcuda.so.1(+0x1e0b30)[0x7f9c52d9bb30]  /usr/local/nvidia/lib64/libcuda.so.1(+0x1bacfc)[0x7f9c52d75cfc]  /usr/local/nvidia/lib64/libcuda.so.1(cuInit+0x4e)[0x7f9c52dc480e]  /usr/local/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x910307)[0x7f9c575d9307]  /usr/local/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN9perftools8gputools4cuda10CUDADriver4InitEv+0x62)[0x7f9c575d94f2]  /usr/local/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZNK9perftools8gputools4cuda12CudaPlatform18VisibleDeviceCountEv+0x12)[0x7f9c575ea3b2]  /usr/local/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow20BaseGPUDeviceFactory13CreateDevicesERKNS_14SessionOptionsERKSsPSt6vectorIPNS_6DeviceESaIS8_EE+0xef)[0x7f9c5750790f]  /usr/local/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow13DeviceFactory10AddDevicesERKNS_14SessionOptionsERKSsPSt6vectorIPNS_6DeviceESaIS8_EE+0x166)[0x7f9c57526c86]  /usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow20DirectSessionFactory10NewSessionERKNS_14SessionOptionsE+0x98)[0x7f9c5be1a3b8]  /usr/local/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow10NewSessionERKNS_14SessionOptionsEPPNS_7SessionE+0x11f)[0x7f9c5756a07f]  /usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(TF_NewDeprecatedSession+0x21)[0x7f9c5934bb21]  /usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(+0x14766be)[0x7f9c58fe36be]  /usr/local/lib/libpython3.6m.so.1.0(_PyCFunction_FastCallDict+0x209)[0x7f9cf4896af9]  /usr/local/lib/libpython3.6m.so.1.0(+0x162e91)[0x7f9cf4929e91]  /usr/local/lib/libpython3.6m.so.1.0(_PyEval_EvalFrameDefault+0x3d6b)[0x7f9cf492e3bb]  /usr/local/lib/libpython3.6m.so.1.0(+0x162af4)[0x7f9cf4929af4]  /usr/local/lib/libpython3.6m.so.1.0(+0x162da7)[0x7f9cf4929da7]  /usr/local/lib/libpython3.6m.so.1.0(_PyEval_EvalFrameDefault+0x38c6)[0x7f9cf492df16]  /usr/local/lib/libpython3.6m.so.1.0(+0x162af4)[0x7f9cf4929af4]  /usr/local/lib/libpython3.6m.so.1.0(_PyFunction_FastCallDict+0x136)[0x7f9cf49321c6]  /usr/local/lib/libpython3.6m.so.1.0(_PyObject_FastCallDict+0x166)[0x7f9cf483a806]  /usr/local/lib/libpython3.6m.so.1.0(_PyObject_Call_Prepend+0xcd)[0x7f9cf483a8fd]  /usr/local/lib/libpython3.6m.so.1.0(PyObject_Call+0x6a)[0x7f9cf483a5ca]  /usr/local/lib/libpython3.6m.so.1.0(+0xed1d9)[0x7f9cf48b41d9]  /usr/local/lib/libpython3.6m.so.1.0(+0xe8363)[0x7f9cf48af363]  /usr/local/lib/libpython3.6m.so.1.0(_PyObject_FastCallDict+0x8a)[0x7f9cf483a72a]  /usr/local/lib/libpython3.6m.so.1.0(_PyObject_FastCallKeywords+0x70)[0x7f9cf483abb0]  /usr/local/lib/libpython3.6m.so.1.0(+0x162c58)[0x7f9cf4929c58]  /usr/local/lib/libpython3.6m.so.1.0(_PyEval_EvalFrameDefault+0x38c6)[0x7f9cf492df16]  /usr/local/lib/libpython3.6m.so.1.0(+0x162160)[0x7f9cf4929160]  /usr/local/lib/libpython3.6m.so.1.0(+0x16308d)[0x7f9cf492a08d]  /usr/local/lib/libpython3.6m.so.1.0(_PyEval_EvalFrameDefault+0x3d6b)[0x7f9cf492e3bb]  /usr/local/lib/libpython3.6m.so.1.0(+0x162160)[0x7f9cf4929160]  /usr/local/lib/libpython3.6m.so.1.0(+0x16308d)[0x7f9cf492a08d]  /usr/local/lib/libpython3.6m.so.1.0(_PyEval_EvalFrameDefault+0x3d6b)[0x7f9cf492e3bb]  /usr/local/lib/libpython3.6m.so.1.0(+0x162af4)[0x7f9cf4929af4]  /usr/local/lib/libpython3.6m.so.1.0(+0x162da7)[0x7f9cf4929da7]  /usr/local/lib/libpython3.6m.so.1.0(_PyEval_EvalFrameDefault+0x3d6b)[0x7f9cf492e3bb]  /usr/local/lib/libpython3.6m.so.1.0(+0x162af4)[0x7f9cf4929af4]  /usr/local/lib/libpython3.6m.so.1.0(+0x162da7)[0x7f9cf4929da7]  /usr/local/lib/libpython3.6m.so.1.0(_PyEval_EvalFrameDefault+0x38c6)[0x7f9cf492df16]  /usr/local/lib/libpython3.6m.so.1.0(+0x162af4)[0x7f9cf4929af4]  /usr/local/lib/libpython3.6m.so.1.0(PyEval_EvalCodeEx+0x2f)[0x7f9cf492a0cf]  /usr/local/lib/libpython3.6m.so.1.0(PyEval_EvalCode+0x1b)[0x7f9cf492a0fb]  /usr/local/lib/libpython3.6m.so.1.0(PyRun_FileExFlags+0xb2)[0x7f9cf495e3b2]  /usr/local/lib/libpython3.6m.so.1.0(PyRun_SimpleFileExFlags+0xf6)[0x7f9cf495e536]  /usr/local/lib/libpython3.6m.so.1.0(Py_Main+0xed3)[0x7f9cf497a3d3]  python3(main+0x174)[0x400ac4]  /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7f9cf4200830]  python3(_start+0x29)[0x400b89]  ======= Memory map: ========  00400000-00401000 r-xp 00000000 00:4e 1971                               /usr/local/bin/python3.6  00601000-00602000 r--p 00001000 00:4e 1971                               /usr/local/bin/python3.6  00602000-00603000 rw-p 00002000 00:4e 1971                               /usr/local/bin/python3.6  00858000-10703000 rw-p 00000000 00:00 0                                  [heap]  7f9c1c000000-7f9c1c021000 rw-p 00000000 00:00 0  7f9c1c021000-7f9c20000000 ---p 00000000 00:00 0  7f9c24000000-7f9c24021000 rw-p 00000000 00:00 0  7f9c24021000-7f9c28000000 ---p 00000000 00:00 0  7f9c28000000-7f9c28021000 rw-p 00000000 00:00 0  7f9c28021000-7f9c2c000000 ---p 00000000 00:00 0  7f9c2c000000-7f9c2c021000 rw-p 00000000 00:00 0  7f9c2c021000-7f9c30000000 ---p 00000000 00:00 0  7f9c31ebb000-7f9c38a7f000 rw-p 00000000 00:00 0  7f9c38a80000-7f9c3aa00000 rw-p 00000000 00:00 0  7f9c3aa00000-7f9c3ac00000 rw-p 00000000 00:00 0  7f9c3ac15000-7f9c3ae95000 rw-p 00000000 00:00 0  7f9c3ae95000-7f9c3ae99000 r-xp 00000000 00:4e 2700                       /usr/local/lib/python3.6/lib-dynload/_lsprof.cpython-36m-x86_64-linux-gnu.so  7f9c3ae99000-7f9c3b098000 ---p 00004000 00:4e 2700                       /usr/local/lib/python3.6/lib-dynload/_lsprof.cpython-36m-x86_64-linux-gnu.so  7f9c3b098000-7f9c3b099000 r--p 00003000 00:4e 2700                       /usr/local/lib/python3.6/lib-dynload/_lsprof.cpython-36m-x86_64-linux-gnu.so  7f9c3b099000-7f9c3b09a000 rw-p 00004000 00:4e 2700                       /usr/local/lib/python3.6/lib-dynload/_lsprof.cpython-36m-x86_64-linux-gnu.so  7f9c3b09a000-7f9c3b59a000 rw-p 00000000 00:00 0  7f9c3b59a000-7f9c3b5ab000 r-xp 00000000 00:4e 2693                       /usr/local/lib/python3.6/lib-dynload/_sqlite3.cpython-36m-x86_64-linux-gnu.so  7f9c3b5ab000-7f9c3b7ab000 ---p 00011000 00:4e 2693                       /usr/local/lib/python3.6/lib-dynload/_sqlite3.cpython-36m-x86_64-linux-gnu.so  7f9c3b7ab000-7f9c3b7ac000 r--p 00011000 00:4e 2693                       /usr/local/lib/python3.6/lib-dynload/_sqlite3.cpython-36m-x86_64-linux-gnu.so  7f9c3b7ac000-7f9c3b7af000 rw-p 00012000 00:4e 2693                       /usr/local/lib/python3.6/lib-dynload/_sqlite3.cpython-36m-x86_64-linux-gnu.so  7f9c3b7af000-7f9c3b96f000 rw-p 00000000 00:00 0  7f9c3b96f000-7f9c3b972000 r-xp 00000000 00:4e 2704                       /usr/local/lib/python3.6/lib-dynload/resource.cpython-36m-x86_64-linux-gnu.so  7f9c3b972000-7f9c3bb71000 ---p 00003000 00:4e 2704                       /usr/local/lib/python3.6/lib-dynload/resource.cpython-36m-x86_64-linux-gnu.so  7f9c3bb71000-7f9c3bb72000 r--p 00002000 00:4e 2704                       /usr/local/lib/python3.6/lib-dynload/resource.cpython-36m-x86_64-linux-gnu.so  7f9c3bb72000-7f9c3bb73000 rw-p 00003000 00:4e 2704                       /usr/local/lib/python3.6/lib-dynload/resource.cpython-36m-x86_64-linux-gnu.so  7f9c3bb73000-7f9c3bcf3000 rw-p 00000000 00:00 0  7f9c3bcf3000-7f9c3bcf8000 r-xp 00000000 00:4e 6835                       /usr/local/lib/python3.6/site-packages/skimage/external/tifffile/_tifffile.cpython-36m-x86_64-linux-gnu.so  7f9c3bcf8000-7f9c3bef7000 ---p 00005000 00:4e 6835                       /usr/local/lib/python3.6/site-packages/skimage/external/tifffile/_tifffile.cpython-36m-x86_64-linux-gnu.so  7f9c3bef7000-7f9c3bef8000 rw-p 00004000 00:4e 6835                       /usr/local/lib/python3.6/site-packages/skimage/external/tifffile/_tifffile.cpython-36m-x86_64-linux-gnu.so  7f9c3bef8000-7f9c3bf06000 r-xp 00000000 00:4e 2661                       /usr/local/lib/python3.6/lib-dynload/_elementtree.cpython-36m-x86_64-linux-gnu.so  7f9c3bf06000-7f9c3c105000 ---p 0000e000 00:4e 2661                       /usr/local/lib/python3.6/lib-dynload/_elementtree.cpython-36m-x86_64-linux-gnu.so  7f9c3c105000-7f9c3c106000 r--p 0000d000 00:4e 2661                       /usr/local/lib/python3.6/lib-dynload/_elementtree.cpython-36m-x86_64-linux-gnu.so  7f9c3c106000-7f9c3c108000 rw-p 0000e000 00:4e 2661                       /usr/local/lib/python3.6/lib-dynload/_elementtree.cpython-36m-x86_64-linux-gnu.so  7f9c3c108000-7f9c3c588000 rw-p 00000000 00:00 0  7f9c3c588000-7f9c3c59a000 r-xp 00000000 00:4e 9758                       /usr/local/lib/python3.6/site-packages/pandas/_libs/testing.cpython-36m-x86_64-linux-gnu.so  7f9c3c59a000-7f9c3c79a000 ---p 00012000 00:4e 9758                       /usr/local/lib/python3.6/site-packages/pandas/_libs/testing.cpython-36m-x86_64-linux-gnu.so  7f9c3c79a000-7f9c3c79c000 rw-p 00012000 00:4e 9758                       /usr/local/lib/python3.6/site-packages/pandas/_libs/testing.cpython-36m-x86_64-linux-gnu.so  7f9c3c79c000-7f9c3c7dc000 rw-p 00000000 00:00 0  7f9c3c7dc000-7f9c3c7dd000 r-xp 00000000 00:4e 9896                       /usr/local/lib/python3.6/site-packages/pandas/util/_move.cpython-36m-x86_64-linux-gnu.so  7f9c3c7dd000-7f9c3c9dd000 ---p 00001000 00:4e 9896                       /usr/local/lib/python3.6/site-packages/pandas/util/_move.cpython-36m-x86_64-linux-gnu.so  7f9c3c9dd000-7f9c3c9de000 rw-p 00001000 00:4e 9896                       /usr/local/lib/python3.6/site-packages/pandas/util/_move.cpython-36m-x86_64-linux-gnu.so  7f9c3c9de000-7f9c3c9f3000 r-xp 00000000 00:4e 10100                      /usr/local/lib/python3.6/site-packages/pandas/io/msgpack/_unpacker.cpython-36m-x86_64-linux-gnu.so  7f9c3c9f3000-7f9c3cbf2000 ---p 00015000 00:4e 10100                      /usr/local/lib/python3.6/site-packages/pandas/io/msgpack/_unpacker.cpython-36m-x86_64-linux-gnu.so  7f9c3cbf2000-7f9c3cbf5000 rw-p 00014000 00:4e 10100                      /usr/local/lib/python3.6/site-packages/pandas/io/msgpack/_unpacker.cpython-36m-x86_64-linux-gnu.so  7f9c3cbf5000-7f9c3cc04000 r-xp 00000000 00:4e 10101                      /usr/local/lib/python3.6/site-packages/pandas/io/msgpack/_packer.cpython-36m-x86_64-linux-gnu.so  7f9c3cc04000-7f9c3ce04000 ---p 0000f000 00:4e 10101                      /usr/local/lib/python3.6/site-packages/pandas/io/msgpack/_packer.cpython-36m-x86_64-linux-gnu.so  7f9c3ce04000-7f9c3ce06000 rw-p 0000f000 00:4e 10101                      /usr/local/lib/python3.6/site-packages/pandas/io/msgpack/_packer.cpython-36m-x86_64-linux-gnu.so  7f9c3ce06000-7f9c3cf06000 rw-p 00000000 00:00 0  7f9c3cf06000-7f9c3cf1c000 r-xp 00000000 00:4e 9766                       /usr/local/lib/python3.6/site-packages/pandas/_libs/json.cpython-36m-x86_64-linux-gnu.so  7f9c3cf1c000-7f9c3d11c000 ---p 00016000 00:4e 9766                       /usr/local/lib/python3.6/site-packages/pandas/_libs/json.cpython-36m-x86_64-linux-gnu.so  7f9c3d11c000-7f9c3d11d000 rw-p 00016000 00:4e 9766                       /usr/local/lib/python3.6/site-packages/pandas/_libs/json.cpython-36m-x86_64-linux-gnu.so  7f9c3d11d000-7f9c3d1a3000 r-xp 00000000 00:4e 9768                       /usr/local/lib/python3.6/site-packages/pandas/_libs/parsers.cpython-36m-x86_64-linux-gnu.so  7f9c3d1a3000-7f9c3d3a3000 ---p 00086000 00:4e 9768                       /usr/local/lib/python3.6/site-packages/pandas/_libs/parsers.cpython-36m-x86_64-linux-gnu.so  7f9c3d3a3000-7f9c3d3a9000 rw-p 00086000 00:4e 9768                       /usr/local/lib/python3.6/site-packages/pandas/_libs/parsers.cpython-36m-x86_64-linux-gnu.so  7f9c3d3a9000-7f9c3d42b000 rw-p 00000000 00:00 0  7f9c3d42b000-7f9c3d444000 r-xp 00000000 00:4e 9756                       /usr/local/lib/python3.6/site-packages/pandas/_libs/reshape.cpython-36m-x86_64-linux-gnu.so  7f9c3d444000-7f9c3d644000 ---p 00019000 00:4e 9756                       /usr/local/lib/python3.6/site-packages/pandas/_libs/reshape.cpython-36m-x86_64-linux-gnu.so  7f9c3d644000-7f9c3d647000 rw-p 00019000 00:4e 9756                       /usr/local/lib/python3.6/site-packages/pandas/_libs/reshape.cpython-36m-x86_64-linux-gnu.so  7f9c3d647000-7f9c3d687000 rw-p 00000000 00:00 0  7f9c3d687000-7f9c3d721000 r-xp 00000000 00:4e 9767                       /usr/local/lib/python3.6/site-packages/pandas/_libs/groupby.cpython-36m-x86_64-linux-gnu.so  7f9c3d721000-7f9c3d920000 ---p 0009a000 00:4e 9767                       /usr/local/lib/python3.6/site-packages/pandas/_libs/groupby.cpython-36m-x86_64-linux-gnu.so  7f9c3d920000-7f9c3d926000 rw-p 00099000 00:4e 9767                       /usr/local/lib/python3.6/site-packages/pandas/_libs/groupby.cpython-36m-x86_64-linux-gnu.so  7f9c3d926000-7f9c3da28000 rw-p 00000000 00:00 0  7f9c3da28000-7f9c3dac7000 r-xp 00000000 00:4e 9762                       /usr/local/lib/python3.6/site-packages/pandas/_libs/window.cpython-36m-x86_64-linux-gnu.so  7f9c3dac7000-7f9c3dcc7000 ---p 0009f000 00:4e 9762                       /usr/local/lib/python3.6/site-packages/pandas/_libs/window.cpython-36m-x86_64-linux-gnu.so  7f9c3dcc7000-7f9c3dcce000 rw-p 0009f000 00:4e 9762                       /usr/local/lib/python3.6/site-packages/pandas/_libs/window.cpython-36m-x86_64-linux-gnu.so  7f9c3dcce000-7f9c3dd50000 rw-p 00000000 00:00 0  7f9c3dd50000-7f9c3dd55000 r-xp 00000000 00:4e 2681                       /usr/local/lib/python3.6/lib-dynload/mmap.cpython-36m-x86_64-linux-gnu.so  7f9c3dd55000-7f9c3df54000 ---p 00005000 00:4e 2681                       /usr/local/lib/python3.6/lib-dynload/mmap.cpython-36m-x86_64-linux-gnu.so  7f9c3df54000-7f9c3df55000 r--p 00004000 00:4e 2681                       /usr/local/lib/python3.6/lib-dynload/mmap.cpython-36m-x86_64-linux-gnu.so  7f9c3df55000-7f9c3df56000 rw-p 00005000 00:4e 2681                       /usr/local/lib/python3.6/lib-dynload/mmap.cpython-36m-x86_64-linux-gnu.so  7f9c3df56000-7f9c3df96000 rw-p 00000000 00:00 0  7f9c3df96000-7f9c3e083000 r-xp 00000000 00:4e 9765                       /usr/local/lib/python3.6/site-packages/pandas/_libs/sparse.cpython-36m-x86_64-linux-gnu.so  7f9c3e083000-7f9c3e283000 ---p 000ed000 00:4e 9765                       /usr/local/lib/python3.6/site-packages/pandas/_libs/sparse.cpython-36m-x86_64-linux-gnu.so  7f9c3e283000-7f9c3e288000 rw-p 000ed000 00:4e 9765                       /usr/local/lib/python3.6/site-packages/pandas/_libs/sparse.cpython-36m-x86_64-linux-gnu.so  7f9c3e288000-7f9c3e388000 rw-p 00000000 00:00 0  7f9c3e388000-7f9c3e3f0000 r-xp 00000000 00:4e 9770                       /usr/local/lib/python3.6/site-packages/pandas/_libs/period.cpython-36m-x86_64-linux-gnu.so  7f9c3e3f0000-7f9c3e5ef000 ---p 00068000 00:4e 9770                       /usr/local/lib/python3.6/site-packages/pandas/_libs/period.cpython-36m-x86_64-linux-gnu.so  7f9c3e5ef000-7f9c3e5f7000 rw-p 00067000 00:4e 9770                       /usr/local/lib/python3.6/site-packages/pandas/_libs/period.cpython-36m-x86_64-linux-gnu.so  7f9c3e5f7000-7f9c3e738000 rw-p 00000000 00:00 0  7f9c3e738000-7f9c3e966000 r-xp 00000000 00:4e 9761                       /usr/local/lib/python3.6/site-packages/pandas/_libs/join.cpython-36m-x86_64-linux-gnu.so  7f9c3e966000-7f9c3eb65000 ---p 0022e000 00:4e 9761                       /usr/local/lib/python3.6/site-packages/pandas/_libs/join.cpython-36m-x86_64-linux-gnu.so  7f9c3eb65000-7f9c3eb6e000 rw-p 0022d000 00:4e 9761                       /usr/local/lib/python3.6/site-packages/pandas/_libs/join.cpython-36m-x86_64-linux-gnu.so  7f9c3eb6e000-7f9c3eb70000 rw-p 00000000 00:00 0  7f9c3eb70000-7f9c3ebcc000 r-xp 00000000 00:4e 9763                       /usr/local/lib/python3.6/site-packages/pandas/_libs/index.cpython-36m-x86_64-linux-gnu.so  7f9c3ebcc000-7f9c3edcc000 ---p 0005c000 00:4e 9763                       /usr/local/lib/python3.6/site-packages/pandas/_libs/index.cpython-36m-x86_64-linux-gnu.so  7f9c3edcc000-7f9c3edd1000 rw-p 0005c000 00:4e 9763                       /usr/local/lib/python3.6/site-packages/pandas/_libs/index.cpython-36m-x86_64-linux-gnu.so  7f9c3edd1000-7f9c3ee93000 rw-p 00000000 00:00 0  7f9c3ee93000-7f9c3eea0000 r-xp 00000000 00:4e 9769                       /usr/local/lib/python3.6/site-packages/pandas/_libs/hashing.cpython-36m-x86_64-linux-gnu.so  7f9c3eea0000-7f9c3f09f000 ---p 0000d000 00:4e 9769                       /usr/local/lib/python3.6/site-packages/pandas/_libs/hashing.cpython-36m-x86_64-linux-gnu.so  7f9c3f09f000-7f9c3f0a1000 rw-p 0000c000 00:4e 9769                       /usr/local/lib/python3.6/site-packages/pandas/_libs/hashing.cpython-36m-x86_64-linux-gnu.so  7f9c3f0a1000-7f9c3f121000 rw-p 00000000 00:00 0  7f9c3f121000-7f9c3f281000 r-xp 00000000 00:4e 9760                       /usr/local/lib/python3.6/site-packages/pandas/_libs/algos.cpython-36m-x86_64-linux-gnu.so  7f9c3f281000-7f9c3f481000 ---p 00160000 00:4e 9760                       /usr/local/lib/python3.6/site-packages/pandas/_libs/algos.cpython-36m-x86_64-linux-gnu.so  7f9c3f481000-7f9c3f48d000 rw-p 00160000 00:4e 9760                       /usr/local/lib/python3.6/site-packages/pandas/_libs/algos.cpython-36m-x86_64-linux-gnu.so  7f9c3f48d000-7f9c3f511000 rw-p 00000000 00:00 0  7f9c3f511000-7f9c3f677000 r-xp 00000000 00:4e 9759                       /usr/local/lib/python3.6/site-packages/pandas/_libs/interval.cpython-36m-x86_64-linux-gnu.so  7f9c3f677000-7f9c3f877000 ---p 00166000 00:4e 9759                       /usr/local/lib/python3.6/site-packages/pandas/_libs/interval.cpython-36m-x86_64-linux-gnu.so  7f9c3f877000-7f9c3f883000 rw-p 00166000 00:4e 9759                       /usr/local/lib/python3.6/site-packages/pandas/_libs/interval.cpython-36m-x86_64-linux-gnu.so  7f9c3f883000-7f9c3f885000 rw-p 00000000 00:00 0  7f9c3f885000-7f9c3f98b000 r-xp 00000000 00:4e 9764                       /usr/local/lib/python3.6/site-packages/pandas/_libs/lib.cpython-36m-x86_64-linux-gnu.so  7f9c3f98b000-7f9c3fb8a000 ---p 00106000 00:4e 9764                       /usr/local/lib/python3.6/site-packages/pandas/_libs/lib.cpython-36m-x86_64-linux-gnu.so  7f9c3fb8a000-7f9c3fb98000 rw-p 00105000 00:4e 9764                       /usr/local/lib/python3.6/site-packages/pandas/_libs/lib.cpython-36m-x86_64-linux-gnu.so  7f9c3fb98000-7f9c3fb9c000 rw-p 00000000 00:00 0  7f9c3fb9c000-7f9c3fc22000 r-xp 00000000 00:4e 9755                       /usr/local/lib/python3.6/site-packages/pandas/_libs/hashtable.cpython-36m-x86_64-linux-gnu.so  7f9c3fc22000-7f9c3fe21000 ---p 00086000 00:4e 9755                       /usr/local/lib/python3.6/site-packages/pandas/_libs/hashtable.cpython-36m-x86_64-linux-gnu.so  7f9c3fe21000-7f9c3fe29000 rw-p 00085000 00:4e 9755                       /usr/local/lib/python3.6/site-packages/pandas/_libs/hashtable.cpython-36m-x86_64-linux-gnu.so  7f9c3fe29000-7f9c3fe6a000 rw-p 00000000 00:00 0  7f9c3fe6a000-7f9c40005000 r-xp 00000000 00:4e 9757                       /usr/local/lib/python3.6/site-packages/pandas/_libs/tslib.cpython-36m-x86_64-linux-gnu.so  7f9c40005000-7f9c40204000 ---p 0019b000 00:4e 9757                       /usr/local/lib/python3.6/site-packages/pandas/_libs/tslib.cpython-36m-x86_64-linux-gnu.so  7f9c40204000-7f9c40218000 rw-p 0019a000 00:4e 9757                       /usr/local/lib/python3.6/site-packages/pandas/_libs/tslib.cpython-36m-x86_64-linux-gnu.so  7f9c40218000-7f9c4081e000 rw-p 00000000 00:00 0  7f9c4081e000-7f9c4083b000 r-xp 00000000 00:4e 9015                       /usr/lib/x86_64-linux-gnu/libyaml-0.so.2.0.4  7f9c4083b000-7f9c40a3b000 ---p 0001d000 00:4e 9015                       /usr/lib/x86_64-linux-gnu/libyaml-0.so.2.0.4  7f9c40a3b000-7f9c40a3c000 r--p 0001d000 00:4e 9015                       /usr/lib/x86_64-linux-gnu/libyaml-0.so.2.0.4  7f9c40a3c000-7f9c40a3d000 rw-p 0001e000 00:4e 9015                       /usr/lib/x86_64-linux-gnu/libyaml-0.so.2.0.4  7f9c40a3d000-7f9c40a6d000 r-xp 00000000 00:4e 2519                       /usr/local/lib/python3.6/site-packages/_yaml.cpython-36m-x86_64-linux-gnu.so  7f9c40a6d000-7f9c40c6c000 ---p 00030000 00:4e 2519                       /usr/local/lib/python3.6/site-packages/_yaml.cpython-36m-x86_64-linux-gnu.so  7f9c40c6c000-7f9c40c6d000 r--p 0002f000 00:4e 2519                       /usr/local/lib/python3.6/site-packages/_yaml.cpython-36m-x86_64-linux-gnu.so  7f9c40c6d000-7f9c40c70000 rw-p 00030000 00:4e 2519                       /usr/local/lib/python3.6/site-packages/_yaml.cpython-36m-x86_64-linux-gnu.so  7f9c40c70000-7f9c40df1000 rw-p 00000000 00:00 0  7f9c40df1000-7f9c40e08000 r-xp 00000000 00:4e 8860                       /usr/local/lib/python3.6/site-packages/h5py/h5l.cpython-36m-x86_64-linux-gnu.so  7f9c40e08000-7f9c41008000 ---p 00017000 00:4e 8860                       /usr/local/lib/python3.6/site-packages/h5py/h5l.cpython-36m-x86_64-linux-gnu.so  7f9c41008000-7f9c4100b000 rw-p 00017000 00:4e 8860                       /usr/local/lib/python3.6/site-packages/h5py/h5l.cpython-36m-x86_64-linux-gnu.so  7f9c4100b000-7f9c4100c000 rw-p 00086000 00:4e 8860                       /usr/local/lib/python3.6/site-packages/h5py/h5l.cpython-36m-x86_64-linux-gnu.so  7f9c4100c000-7f9c4100e000 rw-p 00087000 00:4e 8860                       /usr/local/lib/python3.6/site-packages/h5py/h5l.cpython-36m-x86_64-linux-gnu.so  7f9c4100e000-7f9c41027000 r-xp 00000000 00:4e 8855                       /usr/local/lib/python3.6/site-packages/h5py/h5o.cpython-36m-x86_64-linux-gnu.so  7f9c41027000-7f9c41226000 ---p 00019000 00:4e 8855                       /usr/local/lib/python3.6/site-packages/h5py/h5o.cpython-36m-x86_64-linux-gnu.so  7f9c41226000-7f9c4122a000 rw-p 00018000 00:4e 8855                       /usr/local/lib/python3.6/site-packages/h5py/h5o.cpython-36m-x86_64-linux-gnu.so  7f9c4122a000-7f9c4122b000 rw-p 00080000 00:4e 8855                       /usr/local/lib/python3.6/site-packages/h5py/h5o.cpython-36m-x86_64-linux-gnu.so  7f9c4122b000-7f9c4122d000 rw-p 00081000 00:4e 8855                       /usr/local/lib/python3.6/site-packages/h5py/h5o.cpython-36m-x86_64-linux-gnu.so  7f9c4122d000-7f9c4126d000 rw-p 00000000 00:00 0  7f9c4126d000-7f9c41271000 r-xp 00000000 00:4e 8869                       /usr/local/lib/python3.6/site-packages/h5py/h5fd.cpython-36m-x86_64-linux-gnu.so  7f9c41271000-7f9c41471000 ---p 00004000 00:4e 8869                       /usr/local/lib/python3.6/site-packages/h5py/h5fd.cpython-36m-x86_64-linux-gnu.so  7f9c41471000-7f9c41472000 rw-p 00004000 00:4e 8869                       /usr/local/lib/python3.6/site-packages/h5py/h5fd.cpython-36m-x86_64-linux-gnu.so  7f9c41472000-7f9c41473000 rw-p 00016000 00:4e 8869                       /usr/local/lib/python3.6/site-packages/h5py/h5fd.cpython-36m-x86_64-linux-gnu.so  7f9c41473000-7f9c41474000 rw-p 00017000 00:4e 8869                       /usr/local/lib/python3.6/site-packages/h5py/h5fd.cpython-36m-x86_64-linux-gnu.so  7f9c41474000-7f9c41480000 r-xp 00000000 00:4e 8859                       /usr/local/lib/python3.6/site-packages/h5py/h5i.cpython-36m-x86_64-linux-gnu.so  7f9c41480000-7f9c41680000 ---p 0000c000 00:4e 8859                       /usr/local/lib/python3.6/site-packages/h5py/h5i.cpython-36m-x86_64-linux-gnu.so  7f9c41680000-7f9c41682000 rw-p 0000c000 00:4e 8859                       /usr/local/lib/python3.6/site-packages/h5py/h5i.cpython-36m-x86_64-linux-gnu.so  7f9c41682000-7f9c41683000 rw-p 00042000 00:4e 8859                       /usr/local/lib/python3.6/site-packages/h5py/h5i.cpython-36m-x86_64-linux-gnu.so  7f9c41683000-7f9c41685000 rw-p 00043000 00:4e 8859                       /usr/local/lib/python3.6/site-packages/h5py/h5i.cpython-36m-x86_64-linux-gnu.so  7f9c41685000-7f9c416ac000 r-xp 00000000 00:4e 8873                       /usr/local/lib/python3.6/site-packages/h5py/h5g.cpython-36m-x86_64-linux-gnu.so  7f9c416ac000-7f9c418ab000 ---p 00027000 00:4e 8873                       /usr/local/lib/python3.6/site-packages/h5py/h5g.cpython-36m-x86_64-linux-gnu.so  7f9c418ab000-7f9c418af000 rw-p 00026000 00:4e 8873                       /usr/local/lib/python3.6/site-packages/h5py/h5g.cpython-36m-x86_64-linux-gnu.so  7f9c418af000-7f9c418b0000 rw-p 00000000 00:00 0  7f9c418b0000-7f9c418b1000 rw-p 000f7000 00:4e 8873                       /usr/local/lib/python3.6/site-packages/h5py/h5g.cpython-36m-x86_64-linux-gnu.so  7f9c418b1000-7f9c418b2000 rw-p 000f8000 00:4e 8873                       /usr/local/lib/python3.6/site-packages/h5py/h5g.cpython-36m-x86_64-linux-gnu.so  7f9c418b2000-7f9c418ce000 r-xp 00000000 00:4e 8863                       /usr/local/lib/python3.6/site-packages/h5py/h5f.cpython-36m-x86_64-linux-gnu.so  7f9c418ce000-7f9c41ace000 ---p 0001c000 00:4e 8863                       /usr/local/lib/python3.6/site-packages/h5py/h5f.cpython-36m-x86_64-linux-gnu.so  7f9c41ace000-7f9c41ad1000 rw-p 0001c000 00:4e 8863                       /usr/local/lib/python3.6/site-packages/h5py/h5f.cpython-36m-x86_64-linux-gnu.so  7f9c41ad1000-7f9c41ad2000 rw-p 00000000 00:00 0  7f9c41ad2000-7f9c41ad3000 rw-p 00093000 00:4e 8863                       /usr/local/lib/python3.6/site-packages/h5py/h5f.cpython-36m-x86_64-linux-gnu.so  7f9c41ad3000-7f9c41ad5000 rw-p 00094000 00:4e 8863                       /usr/local/lib/python3.6/site-packages/h5py/h5f.cpython-36m-x86_64-linux-gnu.so  7f9c41ad5000-7f9c41ae6000 r-xp 00000000 00:4e 8862                       /usr/local/lib/python3.6/site-packages/h5py/h5ds.cpython-36m-x86_64-linux-gnu.so  7f9c41ae6000-7f9c41ce6000 ---p 00011000 00:4e 8862                       /usr/local/lib/python3.6/site-packages/h5py/h5ds.cpython-36m-x86_64-linux-gnu.so  7f9c41ce6000-7f9c41ce8000 rw-p 00011000 00:4e 8862                       /usr/local/lib/python3.6/site-packages/h5py/h5ds.cpython-36m-x86_64-linux-gnu.so  7f9c41ce8000-7f9c41ce9000 rw-p 00059000 00:4e 8862                       /usr/local/lib/python3.6/site-packages/h5py/h5ds.cpython-36m-x86_64-linux-gnu.so  7f9c41ce9000-7f9c41ceb000 rw-p 0005a000 00:4e 8862                       /usr/local/lib/python3.6/site-packages/h5py/h5ds.cpython-36m-x86_64-linux-gnu.so  7f9c41ceb000-7f9c41d07000 r-xp 00000000 00:4e 8857                       /usr/local/lib/python3.6/site-packages/h5py/h5d.cpython-36m-x86_64-linux-gnu.so  7f9c41d07000-7f9c41f06000 ---p 0001c000 00:4e 8857                       /usr/local/lib/python3.6/site-packages/h5py/h5d.cpython-36m-x86_64-linux-gnu.so  7f9c41f06000-7f9c41f09000 rw-p 0001b000 00:4e 8857                       /usr/local/lib/python3.6/site-packages/h5py/h5d.cpython-36m-x86_64-linux-gnu.so  7f9c41f09000-7f9c41f0a000 rw-p 00000000 00:00 0  7f9c41f0a000-7f9c41f0b000 rw-p 00097000 00:4e 8857                       /usr/local/lib/python3.6/site-packages/h5py/h5d.cpython-36m-x86_64-linux-gnu.so  7f9c41f0b000-7f9c41f0d000 rw-p 00098000 00:4e 8857                       /usr/local/lib/python3.6/site-packages/h5py/h5d.cpython-36m-x86_64-linux-gnu.so  7f9c41f0d000-7f9c41f16000 r-xp 00000000 00:4e 8872                       /usr/local/lib/python3.6/site-packages/h5py/_proxy.cpython-36m-x86_64-linux-gnu.so  7f9c41f16000-7f9c42116000 ---p 00009000 00:4e 8872                       /usr/local/lib/python3.6/site-packages/h5py/_proxy.cpython-36m-x86_64-linux-gnu.so  7f9c42116000-7f9c42117000 rw-p 00009000 00:4e 8872                       /usr/local/lib/python3.6/site-packages/h5py/_proxy.cpython-36m-x86_64-linux-gnu.so  7f9c42117000-7f9c42118000 rw-p 0002e000 00:4e 8872                       /usr/local/lib/python3.6/site-packages/h5py/_proxy.cpython-36m-x86_64-linux-gnu.so  7f9c42118000-7f9c42119000 rw-p 0002f000 00:4e 8872                       /usr/local/lib/python3.6/site-packages/h5py/_proxy.cpython-36m-x86_64-linux-gnu.so  7f9c42119000-7f9c42123000 r-xp 00000000 00:4e 8864                       /usr/local/lib/python3.6/site-packages/h5py/h5ac.cpython-36m-x86_64-linux-gnu.so  7f9c42123000-7f9c42322000 ---p 0000a000 00:4e 8864                       /usr/local/lib/python3.6/site-packages/h5py/h5ac.cpython-36m-x86_64-linux-gnu.so  7f9c42322000-7f9c42324000 rw-p 00009000 00:4e 8864                       /usr/local/lib/python3.6/site-packages/h5py/h5ac.cpython-36m-x86_64-linux-gnu.so  7f9c42324000-7f9c42325000 rw-p 0003a000 00:4e 8864                       /usr/local/lib/python3.6/site-packages/h5py/h5ac.cpython-36m-x86_64-linux-gnu.so  7f9c42325000-7f9c42326000 rw-p 0003b000 00:4e 8864                       /usr/local/lib/python3.6/site-packages/h5py/h5ac.cpython-36m-x86_64-linux-gnu.so  7f9c42326000-7f9c4237b000 r-xp 00000000 00:4e 8871                       /usr/local/lib/python3.6/site-packages/h5py/h5p.cpython-36m-x86_64-linux-gnu.so  7f9c4237b000-7f9c4257a000 ---p 00055000 00:4e 8871                       /usr/local/lib/python3.6/site-packages/h5py/h5p.cpython-36m-x86_64-linux-gnu.so  7f9c4257a000-7f9c42583000 rw-p 00054000 00:4e 8871                       /usr/local/lib/python3.6/site-packages/h5py/h5p.cpython-36m-x86_64-linux-gnu.so  7f9c42583000-7f9c42584000 rw-p 00000000 00:00 0  7f9c42584000-7f9c42585000 rw-p 00332000 00:4e 8871                       /usr/local/lib/python3.6/site-packages/h5py/h5p.cpython-36m-x86_64-linux-gnu.so  7f9c42585000-7f9c42587000 rw-p 00333000 00:4e 8871                       /usr/local/lib/python3.6/site-packages/h5py/h5p.cpython-36m-x86_64-linux-gnu.so  7f9c42587000-7f9c425a0000 r-xp 00000000 00:4e 8847                       /usr/local/lib/python3.6/site-packages/h5py/h5s.cpython-36m-x86_64-linux-gnu.so  7f9c425a0000-7f9c4279f000 ---p 00019000 00:4e 8847                       /usr/local/lib/python3.6/site-packages/h5py/h5s.cpython-36m-x86_64-linux-gnu.so  7f9c4279f000-7f9c427a2000 rw-p 00018000 00:4e 8847                       /usr/local/lib/python3.6/site-packages/h5py/h5s.cpython-36m-x86_64-linux-gnu.so  7f9c427a2000-7f9c427a3000 rw-p 00000000 00:00 0  7f9c427a3000-7f9c427a4000 rw-p 00081000 00:4e 8847                       /usr/local/lib/python3.6/site-packages/h5py/h5s.cpython-36m-x86_64-linux-gnu.so  7f9c427a4000-7f9c427a6000 rw-p 00082000 00:4e 8847                       /usr/local/lib/python3.6/site-packages/h5py/h5s.cpython-36m-x86_64-linux-gnu.so  7f9c427a6000-7f9c427c7000 r-xp 00000000 00:4e 8861                       /usr/local/lib/python3.6/site-packages/h5py/h5a.cpython-36m-x86_64-linux-gnu.so  7f9c427c7000-7f9c429c6000 ---p 00021000 00:4e 8861                       /usr/local/lib/python3.6/site-packages/h5py/h5a.cpython-36m-x86_64-linux-gnu.so  7f9c429c6000-7f9c429ca000 rw-p 00020000 00:4e 8861                       /usr/local/lib/python3.6/site-packages/h5py/h5a.cpython-36m-x86_64-linux-gnu.so  7f9c429ca000-7f9c429cb000 rw-p 000b2000 00:4e 8861                       /usr/local/lib/python3.6/site-packages/h5py/h5a.cpython-36m-x86_64-linux-gnu.so  7f9c429cb000-7f9c429cd000 rw-p 000b3000 00:4e 8861                       /usr/local/lib/python3.6/site-packages/h5py/h5a.cpython-36m-x86_64-linux-gnu.so  7f9c429cd000-7f9c429d6000 r-xp 00000000 00:4e 8867                       /usr/local/lib/python3.6/site-packages/h5py/h5z.cpython-36m-x86_64-linux-gnu.so  7f9c429d6000-7f9c42bd5000 ---p 00009000 00:4e 8867                       /usr/local/lib/python3.6/site-packages/h5py/h5z.cpython-36m-x86_64-linux-gnu.Aborted (core dumped)
"Firstly,I am very happy I can use the open source.But I just met a False that i can't fix it.    Exception in thread Thread-3:  Traceback (most recent call last):    File ""D:\python\lib\threading.py"", line 916, in _bootstrap_inner      self.run()    File ""D:\python\lib\threading.py"", line 864, in run      self._target(*self._args, **self._kwargs)    File ""E:\jw_files\graduate\projects\yx_pro\mtcnn\pig_landmark\venv\lib\site-packages\keras\utils\data_utils.py"", line 568, in data_generator_task      generator_output = next(self._generator)  ValueError: generator already executing    Traceback (most recent call last):    File ""E:/jw_files/graduate/projects/github/maskrcnn/Mask_RCNN-2.0/train.py"", line 248, in        layers='heads')    File ""E:\jw_files\graduate\projects\github\maskrcnn\Mask_RCNN-2.0\model.py"", line 2200, in train      workers=max(self.config.BATCH_SIZE // 2, 2)    File ""E:\jw_files\graduate\projects\yx_pro\mtcnn\pig_landmark\venv\lib\site-packages\keras\legacy\interfaces.py"", line 87, in wrapper      return func(*args, **kwargs)    File ""E:\jw_files\graduate\projects\yx_pro\mtcnn\pig_landmark\venv\lib\site-packages\keras\engine\training.py"", line 2011, in fit_generator      generator_output = next(output_generator)  StopIteration      What should I do for training it? "
"Hello!    I am training the network on my own dataset, and I am running on an issue:    InvalidArgumentError: Reduction axis 1 is empty in shape  ]]     ]]    During handling of the above exception, another exception occurred:    InvalidArgumentError                      Traceback (most recent call last)    in  ()        6             learning_rate=config.LEARNING_RATE,        7             epochs=5,  ----> 8             layers='heads')    ~\TensorFlow\Master_RCNN_1204\mrcnn\model.py in train(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation)     2315             max_queue_size=100,     2316             workers=workers,  -> 2317             use_multiprocessing=True,     2318         )     2319         self.epoch = max(self.epoch, epochs)    ~\Anaconda3\envs\tensorflow\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)       89                 warnings.warn('Update your `' + object_name +       90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)  ---> 91             return func(*args, **kwargs)       92         wrapper._original_function = func       93         return wrapper    ~\Anaconda3\envs\tensorflow\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)     2222                     outs = self.train_on_batch(x, y,     2223                                                sample_weight=sample_weight,  -> 2224                                                class_weight=class_weight)     2225      2226                     if not isinstance(outs, list):    ~\Anaconda3\envs\tensorflow\lib\site-packages\keras\engine\training.py in train_on_batch(self, x, y, sample_weight, class_weight)     1881             ins = x + y + sample_weights     1882         self._make_train_function()  -> 1883         outputs = self.train_function(ins)     1884         if len(outputs) == 1:     1885             return outputs ]]     ]]    Caused by op 'proposal_targets_1/ArgMax', defined at:    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\runpy.py"", line 193, in _run_module_as_main      ""__main__"", mod_spec)    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\runpy.py"", line 85, in _run_code      exec(code, run_globals)    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel_launcher.py"", line 16, in        app.launch_new_instance()    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\traitlets\config\application.py"", line 658, in launch_instance      app.start()    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\kernelapp.py"", line 486, in start      self.io_loop.start()    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\tornado\platform\asyncio.py"", line 112, in start      self.asyncio_loop.run_forever()    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\asyncio\base_events.py"", line 421, in run_forever      self._run_once()    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\asyncio\base_events.py"", line 1425, in _run_once      handle._run()    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\asyncio\events.py"", line 127, in _run      self._callback(*self._args)    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\tornado\platform\asyncio.py"", line 102, in _handle_events      handler_func(fileobj, events)    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\tornado\stack_context.py"", line 276, in null_wrapper      return fn(*args, **kwargs)    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\zmq\eventloop\zmqstream.py"", line 450, in _handle_events      self._handle_recv()    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\zmq\eventloop\zmqstream.py"", line 480, in _handle_recv      self._run_callback(callback, msg)    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\zmq\eventloop\zmqstream.py"", line 432, in _run_callback      callback(*args, **kwargs)    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\tornado\stack_context.py"", line 276, in null_wrapper      return fn(*args, **kwargs)    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\kernelbase.py"", line 283, in dispatcher      return self.dispatch_shell(stream, msg)    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\kernelbase.py"", line 233, in dispatch_shell      handler(stream, idents, msg)    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\kernelbase.py"", line 399, in execute_request      user_expressions, allow_stdin)    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\ipkernel.py"", line 208, in do_execute      res = shell.run_cell(code, store_history=store_history, silent=silent)    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\zmqshell.py"", line 537, in run_cell      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2662, in run_cell      raw_cell, store_history, silent, shell_futures)    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2785, in _run_cell      interactivity=interactivity, compiler=compiler, result=result)    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2903, in run_ast_nodes      if self.run_code(code, result):    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2963, in run_code      exec(code_obj, self.user_global_ns, self.user_ns)    File "" "", line 3, in        model_dir=MODEL_DIR)    File ""C:\Users\edfla\TensorFlow\Master_RCNN_1204\mrcnn\model.py"", line 1820, in __init__      self.keras_model = self.build(mode=mode, config=config)    File ""C:\Users\edfla\TensorFlow\Master_RCNN_1204\mrcnn\model.py"", line 1969, in build      target_rois, input_gt_class_ids, gt_boxes, input_gt_masks])    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\keras\engine\topology.py"", line 619, in __call__      output = self.call(inputs, **kwargs)    File ""C:\Users\edfla\TensorFlow\Master_RCNN_1204\mrcnn\model.py"", line 660, in call      self.config.IMAGES_PER_GPU, names=names)    File ""C:\Users\edfla\TensorFlow\Master_RCNN_1204\mrcnn\utils.py"", line 825, in batch_slice      output_slice = graph_fn(*inputs_slice)    File ""C:\Users\edfla\TensorFlow\Master_RCNN_1204\mrcnn\model.py"", line 659, in        w, x, y, z, self.config),    File ""C:\Users\edfla\TensorFlow\Master_RCNN_1204\mrcnn\model.py"", line 563, in detection_targets_graph      roi_gt_box_assignment = tf.argmax(positive_overlaps, axis=1)    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 432, in new_func      return func(*args, **kwargs)    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 214, in argmax      return gen_math_ops.arg_max(input, axis, name=name, output_type=output_type)    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 818, in arg_max      name=name)    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper      op_def=op_def)    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 3290, in create_op      op_def=op_def)    File ""C:\Users\edfla\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 1654, in __init__      self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access    InvalidArgumentError (see above for traceback): Reduction axis 1 is empty in shape  ]]     ]]  "
"I run inspect_model.ipynb.  but i saw below message.      Loading weights  /home/hong/PycharmProjects/Mask_RCNN/mask_rcnn_coco.h5  ---------------------------------------------------------------------------  InternalError                             Traceback (most recent call last)    in  ()       14 # Load weights       15 print(""Loading weights "", weights_path)  ---> 16 model.load_weights(weights_path, by_name=True)    ~/PycharmProjects/Mask_RCNN/mrcnn/model.py in load_weights(self, filepath, by_name, exclude)     2095      2096         if by_name:  -> 2097             topology.load_weights_from_hdf5_group_by_name(f, layers)     2098         else:     2099             topology.load_weights_from_hdf5_group(f, layers)    ~/venv/lib/python3.5/site-packages/keras/engine/topology.py in load_weights_from_hdf5_group_by_name(f, layers, skip_mismatch, reshape)     3466                                             weight_values[i]))     3467   -> 3468     K.batch_set_value(weight_value_tuples)    ~/venv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py in batch_set_value(tuples)     2371             assign_ops.append(assign_op)     2372             feed_dict[assign_placeholder] = value  -> 2373         get_session().run(assign_ops, feed_dict=feed_dict)     2374      2375     ~/venv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py in get_session()      177                 config = tf.ConfigProto(intra_op_parallelism_threads=num_thread,      178                                         allow_soft_placement=True)  --> 179             _SESSION = tf.Session(config=config)      180         session = _SESSION      181     if not _MANUAL_VAR_INIT:    ~/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py in __init__(self, target, graph, config)     1507      1508     """"""  -> 1509     super(Session, self).__init__(target, graph, config=config)     1510     # NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.     1511     self._default_graph_context_manager = None    ~/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py in __init__(self, target, graph, config)      636           # pylint: enable=protected-access      637         else:  --> 638           self._session = tf_session.TF_NewDeprecatedSession(opts, status)      639     finally:      640       tf_session.TF_DeleteSessionOptions(opts)    ~/venv/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)      514             None, None,      515             compat.as_text(c_api.TF_Message(self.status.status)),  --> 516             c_api.TF_GetCode(self.status.status))      517     # Delete the underlying status object from memory otherwise it stays alive      518     # as there is a reference to status from this from the traceback due to    InternalError: Failed to create session.  "
"I would like to know, how to print the co-ordinates of all the masked objects in an image. How to get the final bounding box co-ordinates?  Also, my dataset of apples has center of apple and the radius, even when just a part of apple is visible. Will the model still train and identify(or mask) the apples well?"
"I encountered the error when change train_shapes for my personal dataset   and i only have one object to detect  Epoch 1/2  Traceback (most recent call last):    File ""train_shapes.py"", line 273, in        layers=""all"")    File ""/home/zhiqi.cheng/Mask_RCNN-master/mrcnn/model.py"", line 2328, in train      use_multiprocessing=False,    File ""/home/zhiqi.cheng/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""/home/zhiqi.cheng/anaconda2/lib/python2.7/site-packages/keras/engine/training.py"", line 2230, in fit_generator      class_weight=class_weight)    File ""/home/zhiqi.cheng/anaconda2/lib/python2.7/site-packages/keras/engine/training.py"", line 1883, in train_on_batch      outputs = self.train_function(ins)    File ""/home/zhiqi.cheng/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py"", line 2482, in __call__      **self.session_kwargs)    File ""/home/zhiqi.cheng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 905, in run      run_metadata_ptr)    File ""/home/zhiqi.cheng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1140, in _run      feed_dict_tensor, options, run_metadata)    File ""/home/zhiqi.cheng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run      run_metadata)    File ""/home/zhiqi.cheng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call      raise type(e)(node_def, op, message)  tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes:  ]]            ]]    And my changed train_shapes file are as followed:    # coding: utf-8    # # Mask R-CNN - Train on Shapes Dataset  #   #   # This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.  #   # The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster.     # In[1]:      import os  import sys  import random  import math  import re  import time  import numpy as np  import cv2  import matplotlib  import matplotlib.pyplot as plt  from PIL import Image  import yaml  # Root directory of the project  ROOT_DIR = os.path.abspath(""../../"")    # Import Mask RCNN  sys.path.append(ROOT_DIR)  # To find local version of the library  from mrcnn.config import Config  from mrcnn import utils  import mrcnn.model as modellib  from mrcnn import visualize  from mrcnn.model import log    #get_ipython().magic(u'matplotlib inline')    # Directory to save logs and trained model  MODEL_DIR = os.path.join(ROOT_DIR, ""logs"")    # Local path to trained weights file  COCO_MODEL_PATH = os.path.join(ROOT_DIR, ""mask_rcnn_coco.h5"")  # Download COCO trained weights from Releases if needed  if not os.path.exists(COCO_MODEL_PATH):      utils.download_trained_weights(COCO_MODEL_PATH)      # ## Configurations    # In[2]:      class ShapesConfig(Config):      """"""Configuration for training on the toy shapes dataset.      Derives from the base Config class and overrides values specific      to the toy shapes dataset.      """"""      # Give the configuration a recognizable name      NAME = ""shapes""        # Train on 1 GPU and 8 images per GPU. We can put multiple images on each      # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).      GPU_COUNT = 1      IMAGES_PER_GPU = 2        # Number of classes (including background)      NUM_CLASSES = 1 + 1  # background + 3 shapes        # Use small images for faster training. Set the limits of the small side      # the large side, and that determines the image shape.      IMAGE_MIN_DIM = 600      IMAGE_MAX_DIM = 600        # Use smaller anchors because our image and objects are small      RPN_ANCHOR_SCALES = (8*3, 16*3, 32*3, 64*3, 128*3)  # anchor side in pixels        # Reduce training ROIs per image because the images are small and have      # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.      TRAIN_ROIS_PER_IMAGE = 32        # Use a small epoch since the data is simple      STEPS_PER_EPOCH = 100        # use small validation steps since the epoch is small      VALIDATION_STEPS = 5        config = ShapesConfig()  config.display()      # ## Notebook Preferences    # In[3]:      def get_ax(rows=1, cols=1, size=8):      """"""Return a Matplotlib Axes array to be used in      all visualizations in the notebook. Provide a      central point to control graph sizes.            Change the default size attribute to control the size      of rendered images      """"""      _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))      return ax      # ## Dataset  #   # Create a synthetic dataset  #   # Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:  #   # * load_image()  # * load_mask()  # * image_reference()    # In[4]:    class BallDataset(utils.Dataset):      def get_obj_index(self, image):          n = np.max(image)          return n      def from_yaml_get_class(self,image_id):          info=self.image_info[image_id]          with open(info['yaml_path']) as f:              temp=yaml.load(f.read())              labels=temp['label_names']              del labels[0]          return labels      def draw_mask(self, num_obj, mask, image):          info = self.image_info[image_id]          for index in range(num_obj):              for i in range(info['width']):                  for j in range(info['height']):                      at_pixel = image.getpixel((i, j))                      if at_pixel == index + 1:                          mask[j, i, index] =1          return mask      def load_shapes(self, count, height, width, img_floder, mask_floder, imglist,dataset_root_path,yaml_floder):          """"""Generate the requested number of synthetic images.          count: number of images to generate.          height, width: the size of the generated images.          """"""          # Add classes          self.add_class(""shapes"", 1, ""ball"")          for i in range(count):              filestr = imglist[i].split(""."")[0]              #filestr = filestr.split(""_"")[1]              mask_path = mask_floder + ""/"" + filestr + "".png""              yaml_path=yaml_floder              self.add_image(""shapes"", image_id=i, path=img_floder + ""/"" + imglist[i],                             width=width, height=height, mask_path=mask_path,yaml_path=yaml_path)      def load_mask(self, image_id):          """"""Generate instance masks for shapes of the given image ID.          """"""          global iter_num          info = self.image_info[image_id]          count = 1  # number of object          img = Image.open(info['mask_path'])          num_obj = self.get_obj_index(img)          mask = np.zeros([info['height'], info['width'], num_obj], dtype=np.uint8)          mask = self.draw_mask(num_obj, mask, img)          occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)          for i in range(count - 2, -1, -1):              mask[:, :, i] = mask[:, :, i] * occlusion              occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))          labels=[]          labels=self.from_yaml_get_class(image_id)          labels_form=[]          for i in range(len(labels)):              if labels[i].find(""ball"")!=-1:                  #print ""box""                  labels_form.append(""ball"")          class_ids = np.array([self.class_names.index(s) for s in labels_form])          return mask, class_ids.astype(np.int32)        # In[5]:    dataset_root_path=""/home/zhiqi.cheng/Mask_RCNN-master/maskdataset/""  img_floder = dataset_root_path+""pic""  mask_floder = dataset_root_path+""mask""  yaml_floder = dataset_root_path+""info.yaml""  imglist = os.listdir(img_floder)  count = len(imglist)  width = 600  height = 600  # Training dataset  dataset_train = BallDataset()  dataset_train.load_shapes(count, 600, 600, img_floder, mask_floder, imglist,dataset_root_path,yaml_floder)  dataset_train.prepare()    # Validation dataset  dataset_val = BallDataset()  dataset_val.load_shapes(count, 600, 600, img_floder, mask_floder, imglist,dataset_root_path,yaml_floder)  dataset_val.prepare()      # In[6]:      # Load and display random samples  image_ids = np.random.choice(dataset_train.image_ids, 4)  for image_id in image_ids:      image = dataset_train.load_image(image_id)      mask, class_ids = dataset_train.load_mask(image_id)      visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)      # ## Ceate Model    # In[ ]:      # Create model in training mode  model = modellib.MaskRCNN(mode=""training"", config=config,                            model_dir=MODEL_DIR)      # In[7]:      # Which weights to start with?  init_with = ""coco""  # imagenet, coco, or last    if init_with == ""imagenet"":      model.load_weights(model.get_imagenet_weights(), by_name=True)  elif init_with == ""coco"":      # Load weights trained on MS COCO, but skip layers that      # are different due to the different number of classes      # See README for instructions to download the COCO weights      model.load_weights(COCO_MODEL_PATH, by_name=True,                         exclude=[""mrcnn_class_logits"", ""mrcnn_bbox_fc"",                                   ""mrcnn_bbox"", ""mrcnn_mask""])  elif init_with == ""last"":      # Load the last model you trained and continue training      model.load_weights(model.find_last()[1], by_name=True)      # ## Training  #   # Train in two stages:  # 1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.  #   # 2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=""all` to train all layers.    # In[8]:      # Train the head branches  # Passing layers=""heads"" freezes all layers except the head  # layers. You can also pass a regular expression to select  # which layers to train by name pattern.  #model.train(dataset_train, dataset_val,               #learning_rate=config.LEARNING_RATE,               #epochs=1,               #layers='heads')      # In[9]:      # Fine tune all layers  # Passing layers=""all"" trains all layers. You can also   # pass a regular expression to select which layers to  # train by name pattern.  model.train(dataset_train, dataset_val,               learning_rate=config.LEARNING_RATE / 10,              epochs=2,               layers=""all"")      # In[10]:      # Save weights  # Typically not needed because callbacks save after every epoch  # Uncomment to save manually  # model_path = os.path.join(MODEL_DIR, ""mask_rcnn_shapes.h5"")  # model.keras_model.save_weights(model_path)      # ## Detection    # In[11]:    "
"I have read the   , and have tried to applied to my own dataset.  But it doesn't work.  Could you explain the detail from scratch?  (like how to add classes at ObjectDatasets)      What I have done at fanDatasets:    (fan is the class I trained)         and after adjusted, i ran ` python3 fan.py train --dataset=/home/superyang/Documents/Mask_RCNN/datasets/fan --weights=last`    did I miss anything?      Thank you!"
"One thing I've noticed from using Mask_RCNN is that it is designed not a package completely. This means that suppose you want to use `Mask_RCNN` as a building block in code, it will break. For example:    Suppose I have a repo, `MyProject/` and contained within it I have a forked version of `Mask_RCNN` as such:  ├── MyProject  ├── some_code.py  ├── Mask_RCNN  │   ├── assets  │   ├── coco.py  │   ├── ...    If I want to use any of the useful functions in `some_code.py` , for example useful functions from `coco.py`, I would need to run     `from Mask_RCNN import coco`    This first off requires `Mask_RCNN/` to contain an `__init__.py`, and second, in importing `coco.py`, the current statement is `import utils` which should then at this point be imported not directly, but from Mask_RCNN.  There are similar examples in `visualize.py` and `model.py`.    I can create a pull request for this, it only requires a couple of minor changes in the headers of    * `coco.py`  * `visualize.py`  * `model.py`"
"Hello, I'm trying to extract the feature vectors of each ROI pooling feature maps, by running the image through model.run_graph literately. However, the consumed memory will keep increasing every iteration and then to 100%, then kills the program. My code is as follow:        class_fc_layer =  model.keras_model.get_layer(""pool_squeeze"").output      proposals_layer =  model.keras_model.get_layer(""ROI"").output      probs_layer =  model.keras_model.get_layer(""mrcnn_class"").output      deltas_layer =  model.keras_model.get_layer(""mrcnn_bbox"").output      for image_id in dataset.image_ids:          image = dataset.load_image(image_id)          mrcnn = model.run_graph([image], [              (""class_fc"", class_fc_layer),              (""proposals"", proposals_layer),              (""probs"", probs_layer),              (""deltas"", deltas_layer)])    I think the the line outputs_np = kf(model_in) in run_graph() is causing the leaks, and I have found people reporting this similar issues with Keras   Any idea on how to fix this, or other recommended approach for extracting the feature vectors?"
"when i predict my data,there alwayls offset to down&right,how can i adjust the pre"
"if an image is not square, then zero padding is added at the top/bottom or right/left   why?  I think keep ratio is enough"
"I use your docker envirionment ,but the tensorflow seems only support cpu.what should I do if I want to continue use the docker environment? thanks!"
"I was looking at the coco sample `inspect_model.py`      Where they run the subgraph:         The example later filters out detections that have a class of 0 indicating background.         The shape sizes of the above outputs indicates that we get 1000 region proposals from RPN/FPN during inference. Later this gets filtered and refined down to 100 region proposals. But the end result is that there are only 8 inferred instances.    So why does the detections layer still output regions that have BG as their strongest classification? Inside the `model.py`, it appears that the output is connected to `build_fpn_mask_graph`. And I can't see where they are dealing with regions with 0 as their classification.    If the BG regions are being  filtered out. Where does this occur in the pipeline?"
None
"Inside PyramidROIAlign, we determine the levels of the feature pyramid network to assign to the ROI in question.     The equation is from section 4.2 equation (1) of the   paper.     In the code comments it says that a  . However, when we feed those params into this equation:       Then we assign the roi_level to P5 because it passed the max value of 5.   Therefore, if our ROI is larger than 224, it is automatically assigned to P5, and the issue is that P5 has a really small spatial resolution (1//64) of the original image shape, and we are giving it the bulk of the ROI's. Or so it seems maybe I am wrong.       Question (1): What are typical ROI sizes for a (1024, 1024, 3) image? Would these regions scale linearly if I reduce the input image dimension?    Question (2). If we are training at a lower resolution (say `(256,256, 3)`) then scaling by 256 won't really work because it is being wrapped in a log function so wouldn't that be a nonlinear scale?  "
"Hi Guys,    Just ran the demo(mask_rcnn_coco.h5 model) i really liked what you have done. This is one of the best repositories on github. I ran the demo on my laptop with the following specs:    Nvidia 940 MX GPU 2GB  RAM 8GB  Intel Core i5 7200U @2.5GHz Processor        while True:          t1=time.time()          ret, frame = capture.read()          results = model.detect([frame], verbose=0)          t2=time.time()          print(""Time 1 : ""+str(t2-t1))          r = results[0]          frame = display_instances(             frame, r['rois'], r['masks'], r['class_ids'], class_names, r['scores']          )          t3=time.time()          print (""Time 2 : ""+str(t3-t2))          cv2.imshow('frame', frame)          if cv2.waitKey(1) & 0xFF == ord('q'):              break    So Time 1: is always over 2 seconds and the video is far away from smooth.   When you guys shot the 4K video did you guys use the same model ?   What was the hardware configuration for object detection in 4K mode ?    Thanks      You guys rock !  "
"i just modified the val data when evaluate,the, i got this problem  here is my code    elif args.command == ""evaluate"":      # Validation dataset      # Validation dataset          dataset_val = BalloonDataset()          QS=dataset_val.load_Ddim(args.dataset, ""val"")          # dataset_val.load_Ddim(dataset_train_path,"""")          dataset_val.prepare()          evaluate_coco(model, dataset_val, QS, ""bbox"", limit=int(args.limit))    the def evaluate_coco change that   def evaluate_coco(model, dataset, coco, eval_type=""bbox"", limit=0):      """"""Runs official COCO evaluation.      dataset: A Dataset object with valiadtion data      eval_type: ""bbox"" or ""segm"" for bounding box or segmentation evaluation      limit: if not 0, it's the number of images to use for evaluation      """"""      # Pick COCO images from the dataset      image_ids = dataset.image_ids        # Limit to a subset      if limit:          image_ids = image_ids[:limit]        # Get corresponding COCO image IDs.      QS_image_ids = [dataset.image_info[id][""id""] for id in image_ids]        t_prediction = 0      t_start = time.time()        results = []      for i, image_id in enumerate(image_ids):          # Load image          image = dataset.load_image(image_id)            # Run detection          t = time.time()          r = model.detect([image], verbose=0)[0]          t_prediction += (time.time() - t)            # Convert results to COCO format          image_results = build_coco_results(dataset, QS_image_ids[i:i + 1],                                             r[""rois""], r[""class_ids""],                                             r[""scores""], r[""masks""])          results.extend(image_results)        # Load results. This modifies results with additional attributes.      coco_results = coco.loadRes(results)        # Evaluate      cocoEval = COCOeval(coco, coco_results, eval_type)      cocoEval.params.imgIds = QS_image_ids      cocoEval.evaluate()      cocoEval.accumulate()      cocoEval.summarize()        print(""Prediction time: {}. Average {}/image"".format(          t_prediction, t_prediction / len(image_ids)))      print(""Total time: "", time.time() - t_start)      when the code running at the line coco_results = coco.loadRes(results),error just appearanced    so what can i do ? my data form is that   {              ""area"": 336600.0,              ""category_id"": 1,              ""segmentation"": [                  [                      2545.333333333333,                      1226.3333333333333,                      2551.333333333333,                      1453.3333333333333,                      2541.333333333333,                      1726.3333333333333,                      3181.333333333333,                      1736.3333333333333,                      3201.333333333333,                      1486.3333333333333,                      3191.333333333333,                      1243.3333333333333,                      2895.333333333333,                      1230.3333333333333                  ]              ],              ""image_id"": ""IMG_20180322_154017"",              ""bbox"": [                  2541.333333333333,                  1226.3333333333333,                  660.0,                  510.0              ],              ""iscrowd"": 0,              ""id"": 1          }, .....  so how can i use my data      "
When training with `coco.py` i see following error:         when training on my own jupyter-notebook i get much further but at some point i see this error:   
"I have dense images, 1024 * 1024 but there are 2000 same objects in every image. There are two strategies I can think of, hope you can give me some suggestion.    1. Train on small patch (128 * 128) and test on big image (1024 * 1024). Small patch training gives perfect results on small testing image. However, the performance on big test image is terrible (many objects are not detected). This is weird. How can I overcome this?    2. Train on big size image and test on big size image. I noticed bad performance when training on 256 * 256 images. where there are 100 objects/image.     The reason that I can not split my testing image is that, the image is too dense, splitting will cause many half objects and hard to stitch. Any suggestion is appreciated! Thanks."
"Hi Team,  Thanks for the great codes. I noticed that there is a recent update. It would be nice that if you can write something about what you update/change in every updates. Thanks!"
"Hi Waleed,    I have made a few changes to model.py, let me know if you want to me to  push them -         - added imgaug augmentations (lines 1234-1267 in the file attached)        - CropAndPad        - translate_percent        - scale        - rotate        - shear        - added 'Adam' Optimizer (lines 2153-2160)     - added a change to 'build_fpn_mask_graph' so mask_shape is according to     set in config.MASK_SHAPE (lines 2001, 2051, 951, 990-1000)     - renamed the 'detections' output (inference only) so it would be easier     to deploy in production (using tf.serving) (lines 2053-2054)      thanks for this repo. It is great.       Best,  Avi Avidan  0523385564    "
Could you advise on this issue (see image attached)?  !   
"Results on the main page (segm/bbox for minival dataset) are far lower than precision reported in the MaskRCNN paper and the leaderboard (68%@0.5 and 43%@mAP). As I understand it's a different subset of the test data, but the drop is very significant. Why is that?"
"@waleedka   First I would like to say thank you for this great implementation of Mask R-CNN!  I am currently working on my own dataset but I noticed a strange behavior when training.  I divided my training stage as follows, using a lr_decay in `model.py`    `model.train(train_dataset, val_dataset,                   learning_rate=config.LEARNING_RATE,                   epochs=50,                   layers='heads')`  `model.train(train_dataset, val_dataset,                   learning_rate=config.LEARNING_RATE/10,                   epochs=70,                   layers='4+')`  `model.train(train_dataset, val_dataset,                   learning_rate=config.LEARNING_RATE/10,                   epochs=90                   layers='3+')  `  My `val_loss` goes up when training lower stages, but the result, when testing on some images, actually improves.   Does it come from the way that are taken into account false positive / false negatives in the loss computation or is it something else? Have you ever met that matter?    Thank you !"
"Hi, I annotated 2 object classes in my training images and I would like to predict one object of each class. How can I change the setting to do this? I tried DETECTION_MAX_INSTANCES = 2  and the prediction shows 2 predictions of one object class."
How do I find mAP for a model trained using balloon?
"Hello. Tell me please, how can I run on the GPU? At the moment I can run on video and pictures, but unfortunately only on the CPU ..  I have a GPU1080 with 8GB, how can I run it on a GPU? Please tell me  Thanks"
"Inside model.py the   inputs and outputs that are sent to the training model through `fit_generator`. An example output of the properties of the data yielded from fit generator is:       However, when we build the actual tensors in the Mask_RCNN.  function, we have the following tensors as inputs:      Notice that the 5th tensor gets converted from `int32` to `float32` after the gt_boxes go through the   process.     This all makes sense and is dandy, but I can't figure out why the `target_class_ids` are being cast from int32 -> int64 inside  "
hello! i cloned this repo.   i do commands:  pip3 install -r requirements.txt  python3 setup.py install    I don't understand how i can run demo on video.. Please.. can someone help me?
"I used this work (just a mock of coco.py)and fed my own medical image data into the model,  the train and validation process looks good with a valid loss,   but the test result is terrible, and the class is always 1 with a high confidence.    Anyone has ideas to how to debug? I don't know how to see the result when training, which only indicates the losses.    "
"in test phase, if input 2 images, PyramidROIAlign also return a tensor whose shape(1, ?, 7, 7, 256), the first dimenstion is 1, how can decide boxes from which image?"
Is there a limit of masks that can be identified for an image?? Because it seems to be 100 for me..????   Or is there a way to change it??
Hi everyone! Thanks for the awesome work and resources. I have been trying to use Matterport's Mask_RCNN for a tiny project. I'm running this on GPU but model.detect is taking upwards of 30 seconds to yield results.   Input images are larger than 500 x 500 x 3 and usually ~ 800 x 800 x 3. Has anyone else experienced this?
"Hello everyone,    did anyone try to train this model on Wider Face dataset?  I'm interested in doing so, but i'm not an experienced user so any thoughts / recommendations would be very helpful for me. As for now i managed to launch the available demos regarding inference, shapes, i did some reading of the coco training example.    I will appreciate any help :)     "
Is there a way to avoid overlapping of masks??
I can run the inspect_balloon_model.ipynb.  But how can I get the single balloon picture from the whole picture?
"If we attempt to train on coco 2014 for just one class `person`. Do we only have to change:  `NUM_CLASSES= 1 + 1`  and inside coco.py pass `class_ids = [1]` to `dataset_train.load_coco()`    When training I noticed that my mrcnn_class_loss starts to rise, and holds very low values `~0.03`. I am also certain that is it loading ~42k images, which would make sense because that is how many annotated images contain humans in the 2014 dataset.     Do I have to change something inside the mrcnn_class_graph or something with the mrcnn_class_logits as well?   "
"Hey everyone,     I managed to put my own dataset in the network and train it, however I would like to know if it's possible to actually see the images or feature maps while they are being created. I know that there's the Keras Callback function that can output that but it's only at the end of the epoch and it doesn't seem to work at times when training (Getting an Xla_Compile error apparently due to scalars).     I also thought of using the Tensorflow summary methods to display them but I just can't figure out where to put the summaries...    Thank you in advance!"
"Hello,    First of all, I'd like to thank the devs for their great work on this project. Thought I'd contribute a tiny bit by suggesting two very simple performance tweaks:    1/ The current model.train function in model.py (purposedly) uses only 1 worker thread on Windows (not tested on Linux) due to a ""bug"" in Keras with use_multiprocessing=True. However, it is possible to call Keras's fit_generator with multiple workers and use_multiprocessing = False. In my use case (training on 30k full HD images, Win 10, Keras 2.1.5), using:  `workers = multiprocessing.cpu_count()  `  resulted in a ~30% speedup on each epoch (single 1080 Ti with 6700k CPU). I suspect performance can still be improved, as average GPU usage is quite low, but it's already a nice step-up.    2/ In visualize.py, the apply_mask function performs a where on the same image-sized mask three times. No big deal when calling it once or twice, but not great when called in a loop over 3k validation images. Instead of the current code, I suggest:       Hoping this will be helpful to someone"
"i just choose 5 class to train ,then it is just still,what can i do ?    /home/jgq/anaconda3/envs/python34/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  /home/jgq/anaconda3/envs/python34/lib/python3.4/site-packages/keras/engine/training.py:1987: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.    UserWarning('Using a generator with `use_multiprocessing=True`'  Epoch 1/40      i debug this ,and find the problem maybe in model.py in linenumber 2308-2318  self.keras_model.fit_generator(                  train_generator,                  initial_epoch=self.epoch,                  epochs=epochs,                  steps_per_epoch=self.config.STEPS_PER_EPOCH,                  callbacks=callbacks,                  validation_data=val_generator,                  validation_steps=self.config.VALIDATION_STEPS,                  max_queue_size=100,                  workers=workers,                  use_multiprocessing=True,  here the  validation_data=val_generator,but,it is do nothing ,can u help me? @ waleedka "
"Do you think using `tf.name_scope`  would make sense in order to group nodes of the graph in Tensorboard ? That would make it easier to read it.    Also, scopes could help grouping scalar values."
"I can run the balloon.py training fine, and get good results. But when I run the balloon_inspector_model.py code I get the error:    class BalloonDataset(utils.Dataset):  AttributeError: module 'utils' has no attribute 'Dataset'    Strange given that balloon.py, which is where the error comes from, runs fine itself. Does anyone know what am I doing wrong here?  I'm running it from the same folder as balloon.py.  "
"Hi, Waleed and other experts,         I am using this implementation for the 2018 data science bowl on Kaggle, and most of time, I am running on colab, which has 2 cpu and 1 K80 gpu.         However, I often encounter this StopIteration error shown in attached image. It didn't show up at the beginning of training process. As a matter of fact, the time it shows up is relatively random, but generally within the first 10 epochs.         The only way I currently can fix it is by setting workers = 0, which disables the multiprocessing ability for data_generator. But it will make the program significantly slower than using multiprocessing in data_generator.         Do you have any idea how to fix this and why this happens? Thanks!         <img width=""862"" alt=""defect"" src=""   "
"Hello,    I am currently trying to use the mask r-cnn for my own dataset, but precision seems to be limited during training by either RPN ROI refinement or Bbox refinement that seem to be not matching all of the objects on the image. I guess that I have to play with parameters of the the RPN or the bbox refinement, but can't find it out.     **EDIT** : I found that this came from the bbox from the rcnn, NOT from the RPN that works perfectly. Yet I don't know what parameter I should change to take more detections into account during training and so to fix that matter plus improving accuracy.    Any clue about how to fix it?    Thanks!"
"I noticed latest code change to use skimage.transform.resize, but seems for model trained using either old code(with scipy.misc.imresize) or latest code.   I always get better nuclei detection map score if change to use  scipy.misc.imresize for detection.   ie, using latest training, with mAP 0.5800 but if I change to use scipy.misc.imresize for detection it can improve to 0.58288."
"If I trained my own dataset by this Mask R-CNN model, how I could evaluate this model?  In this case, what is accuracy?   It seems that this model involves the Detection and Segmentation, I have no idea about the evaluation of this model.....  I'm new here, need help."
"Hi @waleedka ,     This was one of the posts today in the competition (copied below). it is about a new normalization technique called Group Normalization. The paper reports that it helps to speed up Mask RCNN training. One implementation of Group Normalization is also listed in the github link below.   Would you have time to add this feature to the repo ?   Thanks  Mehul    Copy pasted from discussion:      ""There's a useful new normalization technique called Group Norm (GN) that has many of the same benefits as Batch Norm (BN), but works well for small batch sizes. Of particular relevance, the authors try GN on Mask R-CNN and claim an increase of 1.4 in bounding box AP on COCO after replacing frozen BN layers in the ResNet backbone with GN and adding GN to the feature pyramid network.         For anyone using the Matterport code (or any keras model), there are already a couple of implementations on github that can be easily added to your model.              I don't know how it will affect LB results yet, but it does make training noticeably faster and allows you to use larger learning rates with Matterport code (e.g. 1e-2)."""
"I've run an out-of-the-box Mask-RCNN (mask_rcnn_coco.h5 weights) on the Cow class from MS COCO 2017 validation set (instance segmentation), it's a total of 87 images (only masks of Cow instances in the ground truth). I'm getting a suspiciously low precision: 0.36@50% IoU and 0.29 mAP, which is far lower than the other results I've found (e.g. around 0.6 and 0.37 accordingly). Why is this the case? To get AP, I used the `compute_ap` function from MRCNN utils. The config file is use is the same excecpt the following hyperparameters:       I also set DETECTION_MIN_CONFIDENCE to 0.7, but the changes were miniscule."
"I try to use multi gpu -> GPU_COUNT = 2   but in training process , it's show error ""Integer division by zero""  so I clone code from 4 month ago it work. , but now (update code) it not work.    !   "
"Hello everyone,    I am wondering if k-fold cross validation is a good way to get the best results for mAP results. Yet it seems a bit complicated to implement it.    Have you already tried it? Does it improve results a lot? Did you start from a single json for your whole dataset?    Thank you!"
"Dear @waleedka and developers,  After testing some photos with very small and crowded objects, I could see that Mask-RCNN are still effective. I am wondering about objects like: numbers, printed characters/digits/letters, is Mask-RCNN suitable? If not, please advise which CNN would be?"
"Installed with anaconda2: python3.5                                            opencv3.2(menpo)    Here is my command:  `python balloon.py splash --weights=./mask_rcnn_road_0029.h5 --video=./test_video`    OpenCV Error: Assertion failed (fps >= 1) in open, file /home/travis/miniconda/conda-bld/conda_1485299292920/work/opencv-3.2.0/modules/videoio/src/cap_mjpeg_encoder.cpp, line 638    How should I solve this issue?thanks a lot  "
"Hi,    It seems that after the recent updates, train_shapes.ipynb can no longer be run on colab. About a week ago, it works on colab, which means your most recent update may disrupt something.    There are two version of error report and each one happens unregularly.   Here are the error report 1:     Here are the error report 2:       Can you help to solve it? It should be important because more and more users begin to use colab to run their model on free gpu.     Thank you."
"I see that we can get the output of certain layers as shown in inspect_model.ipynb:    activations = model.run_graph([image], [      (""input_image"",        model.keras_model.get_layer(""input_image"").output),      (""res4w_out"",          model.keras_model.get_layer(""res4w_out"").output),  # for resnet100      (""rpn_bbox"",           model.keras_model.get_layer(""rpn_bbox"").output),      (""roi"",                model.keras_model.get_layer(""ROI"").output),  ])    However, when I try to get the output of 'pool5' layer of resnet-101, I get a 'no such layer' error. What name should I use to get the output of 'pool5' layer?  "
"We would like if anyone can guide us on how to initialize weights for training from scratch.  We are planning to replace the Conv2D layers with SeparableConv2D layers so we cant use the previous "".h5"" file for current purpose.    ThankYou "
I am attempting to train on 2017 coco data. There is no valminusminival2017.json file.     The instances_valminusminival json files are for 2014 data only. There isn't one for 2017 data. I am guessing you can rename file `instances_valminusminival2014.json` to `instances_valminusminival2017.json` but I am not sure how this will affect your validation loss.     Does anybody know what the instances_valminusminival dataset consists of ? / How it was computed? I simply commented out   in order to train but I don't know if I am missing something.
"When I execute the code, the following error occured.    Process finished with exit code 136 (interrupted by signal 8: SIGFPE).    This error occurs when the keras predicts detections, mrcnn_class, mrcnn_bbox, mrcnn_mask, rois, rpn_class, rpn_bbox parameters.    My computer has 32 GB RAM with GTX 1080 Ti, so I don't think there is a memory lack. Do you have any ideas?"
"loading annotations into memory...  ---------------------------------------------------------------------------  FileNotFoundError                         Traceback (most recent call last)    in  ()        5 elif config.NAME == ""coco"":        6     dataset = coco.CocoDataset()  ----> 7     dataset.load_coco(COCO_DIR, ""minival"")        8         9 # Must call before using the dataset    ~/Downloads/mrcnn_test/coco.py in load_coco(self, dataset_dir, subset, year, class_ids, class_map, return_coco, auto_download)      106             self.auto_download(dataset_dir, subset, year)      107   --> 108         coco = COCO(""{}/annotations/instances_{}{}.json"".format(dataset_dir, subset, year))      109         if subset == ""minival"" or subset == ""valminusminival"":      110             subset = ""val""    ~/Downloads/mrcnn_test/pycocotools/coco.py in __init__(self, annotation_file)       77             print('loading annotations into memory...')       78             tic = time.time()  ---> 79             dataset = json.load(open(annotation_file, 'r'))       80             assert type(dataset)==dict, 'annotation file format {} not supported'.format(type(dataset))       81             print('Done (t={:0.2f}s)'.format(time.time()- tic))    FileNotFoundError: [Errno 2] No such file or directory: '~/Downloads/mrcnn_test/coco/annotations/instances_minival2014.json'  **# I just begin to test it,so can you help me to solve this problem?**"
None
"Hello every one,     There are multiple val losses defined, combined loss, loss for rpn box/class, mask box/class and mask loss. etc. If my primary interest is mAP, which loss shall i use to pick the best performance epoch?"
"I want to predict multiple objects, and then comparing there are linear or not.  Let say, for instance, specifying one row must be the same orientation. Also how can I group also de same objects to beme one mask.    How can I implement in python?  "
"Hi     I could run `mask_rcnn_coco.h5` with my webcam using   , but I get the following error when I try to run `mask_rcnn_balloon.h5` I replaced it with `mask_rcnn_coco.h5`   I am not sure how to solve it       "
"Hi,    I am new to deep learning. Could you give me some guideline how to:  1. get the pixel coordinate of the mask (4 greatest values, corresponding to 4 directions of the image) and the bounding boxes.  2. I try to locate only the person in image (dataset UCF-101). Do I have to fine tune with my dataset first?    Thank you."
"Hi    Thanks for your effort,     I tested `demo.ipynb` and it works properly. But when I change the file `mask_rcnn_coco.h5` with my own `.h5` that I trained, it gives me the following error.    Any help please ??  `  ValueError: Dimension 1 in both shapes must be equal, but are 324 and 8. Shapes are [1024,324] and [1024,8]. for 'Assign_682' (op: 'Assign') with input shapes: [1024,324], [1024,8].`"
"  Hello,    I hope you can help me.  First I have trained the head layers only.  Now I want to use the last saved .h5 file and train all layers with a samller learning rate.  But when I change  initialize init_with==""last"" the training does not work.    Is there something else I have to change?    Thanks in advance!    Christoph         I always get one of these two errors:    !   ! "
"Recently, I have achieved decent performance with this Mask RCNN implementation when IoU is between 0.5-0.7 (mAP is 0.8+), however when IoU is higher, like 0.9, 0.95, the mAP is almost always 0. I wonder if there is any parameter in config I can tune to improve this? "
"I train a model on the newest code on coco, and when testing, the first picture takes 65s.  My env:  ubuntu 16.04,  py3.6, tf 1.5.0-gpu, titanx 12g  The next pictures only take several hundred mili-seconds.    Do you meet the same problem?     Btw, I also test on an older version (commit 1c51787), it only takes 1.9s."
"Hi,    I'm trying to train my dataset for the Data Science Bowl 2018 competition and I'm having trouble. The loss is always NaN no matter what I try. I know that my data set is structured properly, as it looks like this:      So the problem isn't the data. Once I load it and try to run it, the model compiles, but the loss is consistently NaN and I can't figure out why. Can someone help?       "
"Hi everyone,  I trained MaskRcnn on my own data set and wrote something similar to demo.ipynb to test it.  The only problem is that all objects are of the same class which is the name at pos 1 in my class_names list.  Please note that I hard coded class_ids in training to be [1,2,3] as I only have two classes.    Thanks in advance"
"Processing 1 images  image                    shape: (1224, 1632, 3)       min:    0.00000  max:  255.00000  Traceback (most recent call last):    File ""demo_mine.py"", line 84, in        results = model.detect([image], verbose=1)    File ""/home/fxd/sy3u/Mask_RCNN-master/model.py"", line 2366, in detect      molded_images, image_metas, windows = self.mold_inputs(images)    File ""/home/fxd/sy3u/Mask_RCNN-master/model.py"", line 2269, in mold_inputs      padding=self.config.IMAGE_PADDING)    File ""/home/fxd/sy3u/Mask_RCNN-master/utils.py"", line 421, in resize_image      image, (round(h * scale), round(w * scale)))    File ""/usr/local/lib/python2.7/dist-packages/numpy/lib/utils.py"", line 101, in newfunc      return func(*args, **kwds)    File ""/usr/local/lib/python2.7/dist-packages/scipy/misc/pilutil.py"", line 564, in imresize      imnew = im.resize(size, resample=func[interp])    File ""/usr/local/lib/python2.7/dist-packages/PIL/Image.py"", line 1747, in resize      return self._new(self.im.resize(size, resample, box))  TypeError: integer argument expected, got float    Thanks for you to help me！"
"Hi, could you share your resnet101 imagenet weights for training the network from scratch, couldn't find it anywhere in the code. Thx"
"In the implementation of extract box from mask, the implementation used  >To validate this approach, we compared our computed bounding boxes to those provided by the COCO dataset. We found that ~2% of bounding boxes differed by 1px or more, ~0.05% differed by 5px or more, and only 0.01% differed by 10px or more.     I have changed it to     And I got the error as below. How can I fix it? The reason is that I want to make a larger box from the mask, instead of different 1 pixel.        This is my current solution       But I am not sure it is completed solution because maybe x2 and y2 also need to check and other recovert boxes may need to change. "
"Hi,     when training our model, we are running into memory issues on our cluster. We are only interested in image segmentation and not instance segmentation or object detection. Therefore, we would like to disable those branches.   Would it be possible to train the model without using these branches? If so, which layers would we need to disable to do so?"
"I am currently trying to use the model on a medical segmentation task. However, I am not quite sure if  I understand all the variables in CONFIG correctly.   My confusion arises because of the way POST_NMS_ROIS_TRAINING, TRAIN_ROIS_PER_IMAGE and ROI_POSITIVE_RATIO are described in CONFIG:   As far as I understand (thanks to @waleedka in several issues and comments in the code),  POST_NMS_ROIS_TRAINING limits the amount of ROIs which survive non-maximum supression at the end of the RPN and TRAIN_ROIS_PER_IMAGE of those are fed to the mask/classifier-Head of the network.    What bugs me about ROI_POSITIVE_RATIO is that I understand it as the ratio of positive to negative region-proposals here. Since I have no idea how many of the proposals are positive, how can I tune any of those parameters to fit the 0.33 ratio?  In my case, there is only one instance in each image (and those are relatively small most of the time), do any of you have some kind of heuristics with which I could start? Should I change the  TRAIN_ROIS_PER_IMAGE  to something like 20 and adjust POST_NMS_ROIS_TRAINING to 60?  Or maybe I just dont fully understand the problem, but any kind of help would be much appreciated."
The class_names as defined in demo.ipynb are       where class_names[28] refers to 'tie' while it is actually 'umbrella' according to COCO dataset
"Hi, may I know what is the output for this method?    Because when I print the overlaps returned from ""compute_ap"" method in utils.py for one of my predicted image, the output is as follows:    [[0.         0.25871372 0.         0.849182   0.         0.        ]   [0.         0.9107732  0.         0.2756873  0.         0.        ]   [0.         0.         0.2628291  0.         0.         0.79343367]   [0.         0.         0.87064016 0.         0.         0.33170015]   [0.8500652  0.         0.         0.         0.08712522 0.        ]   [0.10231254 0.         0.         0.         0.6626506  0.        ]]    I do not really understand how to find the IoU from this 2D array.  Any help would be appreciated. Thank you."
"Hi,  I know this issue may have barely relevance to this project, but I'm at my wits' end.  So I'm here asking for help, any help will be really appreciate.    I'm working on some experiment about ""Single Object Tracking"", by using LSTM after Mask RCNN.  Use feature maps and bbox coordinates from Mask RCNN as input of LSTM, and LSTM will output the bbox of target object. (Only one object will be tracked, so output size=4)  Because of ""FPN"", their will be 4 layers of feature maps(P2-P5), so I modified MRCNN model to return the variable ""roi_level"", which represents each bbox of MRCNN results correspond to which layer of feature map.  When input to LSTM, the input will be bbox of MRCNN results and specific feature map according to ""target object's roi_level"".    Here is a problem, the feature maps size are too big for LSTM input (for example, P2: 256x256x256), and I have tried using PCA to reduce and fix all feature maps' dimensions to 16x16x64, but ends up that the LSTM cannot learn well.  I have also tried that re-train MRCNN with modifying feature map depth to 1 instead of 256, then use ""4 LSTM"" for each layer of feature maps, generate 4 bboxes, and output the right one according to roi_level.  It still didn't work.    My LSTM seems like it cannot learn from those information because I found that as the training iteration increase, the accuracy of training dataset isn't improved.    Is there any idea that I can give it a try?"
"Hi,  I'm using UEA computer vision image labeling tool to label some images, and it outputs a json file for each image.   I changed the load_mask function so that it takes into consideration the format of my json file but I reliased that Mask Rcnn only takes one Annotation file ( json file ) for training. Is there anyway I can input multiple json files ( one for each image) in training or convert my json files to one json file.   Thanks in advance"
"Hi I would like to add one more class name with this class names in MSCOCO dataset......  class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',                 'bus', 'train', 'truck', 'boat', 'traffic light',                 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',                 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',                 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',                 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',                 'kite', 'baseball bat', 'baseball glove', 'skateboard',                 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',                 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',                 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',                 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',                 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',                 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',                 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',                 'teddy bear', 'hair drier', 'toothbrush']    How I can do this...?"
"Hi, I used labelme to create .json file for each image of my own, but I want to join them into one file annotation.json file as the one in MS COCO, please show me how, thanks"
"platform: ubuntu16.04  cuda: 9.0  cndnn: 7.0  tensorflow: tf-gpu 1.6  GPU: GTX1080 * 1    I am training resnet50 with my own data on ubuntu.  It consume almost all my resources, include cpu, memory and IO.  At the begining of training, everything is ok. At around 50 steps at the first epoch, suddenly the system appears halted, I even can not move mouse. Load average reached to 12(my cpu have only 4 cores), memory 93%(total 16GB), IO: 100%, but the gpu seems not working, below 10%.    cpu and memory:       IO:       There are some warning about `use_multiprocessing=True`, so I change it to false, which makes little difference. The speed of training is faster. At around 100+ step, the system halted again.    The size of pictures is 512*512 and the code runs fine on win10.  "
"Hi,      I am trying to predict keypoints on COCO subdataset, while training, there is an error:      **InvalidArgumentError: Input to reshape is a tensor with 2515456 values, but the requested shape requires a multiple of 13328**     ]]     ]]    Please help !"
"Good news. Not long ago, Tensorflow added an example of Mask R-CNN:  "
"Hi,  I'm wondering the reason why the MEAN_PIXEL is set to thoes values ""np.array([123.7, 116.8, 103.9])"",  I can't find this in the original paper, anyone could explaine to me please?    Thanks a lot "
"Hello again,    I successfully managed to use my dataset but the problem apparently is with the shapes inbetween layers...     I get the following error:    InvalidArgumentError (see above for traceback): Incompatible shapes:  ]]    I did change the batch size to see if there were any changes, and the shapes switch to    !     I resized the images to be (512,512) as well. The problem seems to come from the ""mul"" gradient... But that's all I can even understand...    Thanks for your help."
"Hi,   I would like to add to my training set negative images, which do not contain any object of interest (No ground truth mask or bbox).   I managed to start training the network with this additional data. However, to my understanding of the implementation of the loss function, these images do not contribute to the loss.   Does anyone know, if it is possible to add images to the training, which do not contain any ground truth bounding box?    mrcnn_bbox_loss_graph:    rpn_bbox_loss_graph:    Thanks  "
How to save weights and models
"When I reading the code of `compute_ap` in  . I have a doubt.    If I understand corectlly, in this code we compute the AP of each pictures by computing precision and recall of this image. And then we average the APs to get mean AP. However it is quite diffrent from the implement in pascal_voc ( ). VOC code compute all overlaps. And then compute the precison and recall of each classes to get AP of the whole test sets.     So, is the result of this two different ways the same?    The code of  `compute_ap` is as following. Thanks for helping.     "
"If we have an image where there is a smaller object polygon completely enclosed by the larger object polygon behind it. For the ground truth masks, is it sufficient for the larger object mask to be representing the entire polygon, or does it need to subtract the ground truth mask of the smaller polygon?"
"Mask R-CNN predicts K binary masks (for K classes) for each RoI in the image separately, so how is it implemented to combine those masks into a single image? How does it deal with situations where a pixel is predicted to be in multiple binary masks (from multiple RoIs) and situations where a pixel is predicted to be in multiple binary masks of the K classes? Thanks."
"When feed large image into the the net , I got ""ValueError: height and width must be > 0"" in resize_image, anyone know how to solve it?"
"Hello all, I have a plane to implement an improvement of mask segmentation _branch. It is described as following:    >We further create a short path from layer conv3 to a fc layer. There are two 3×3 convolutional layers where the second shrinks channels to half to reduce computational overhead. The mask size we use is  28 × 28 so that the fc layer produces a 784 × 1 × 1 vector. This vector is reshaped to the same spatial size as the mask predicted by FCN. To obtain the final mask prediction, mask of each class from FCN and foreground/background prediction from fc are added.   !     The below code shows my implementation that I want to ask two questions:  1. What is the number of feature of `mrcnn_mask_conv4_fc` and `mrcnn_mask_conv5_fc`. The paper said that **""two 3×3 convolutional layers where the second shrinks channels""**. Is it 128?  2. What is the reshape size (after fully connected layer). How can we fill the `?` in the ` x_fc = K.reshape(x_fc, (-1, ?, ?, ?, ?)))`?  3. I have run the code but it shows some error likes below. I guess have some problem with `Add ` function     This is my implementation     "
"Hello!    I recently saw this implementation of the mask RCNN and wanted to try it out on my aerial imagery dataset. It is basically a whole lot of satellite images and the goal is to segment the buildings on it (later on, we want to be able to get the forests as well).    My datasets are composed of these images and binary images representing the masks (basically black background with the shape of the buildings in white) and I wanted to know if it could be used with this implementation.     Thanks in advance!"
"I am trying to optimize the current detection model which takes around 400 ms per frame (1080i). The goal is to reach realtime detection using multiple GPU/ Servers.  I work with specific images in which I know approximate area where the object is. So my thought was to mask or even cut the whole image and feed just an area where I am interested to find the object. Unfortunately this did not improve detection time. My second thought was to tell the model to find for specific class and not all the classes it was trained for so it would speed up detection time. But I could not find such an option.     My second issue is that I was not able to utilize second GPU. In the code there are lines:      if args.command == ""train"":          config = SoccerConfig()      else:          class InferenceConfig(SoccerConfig):              # Set batch size to 1 since we'll be running inference on              # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU              GPU_COUNT = 1              IMAGES_PER_GPU = 1              DETECTION_MIN_CONFIDENCE = 0    which override GPU_COUNT for detection. I tried to set it to 2 and submit two images to .detect function but I receive Tensorflow error:  Invalid argument: Input to reshape is a tensor with 600 values, but the requested shape has 1200    Is it possible to set model configuration to a better performance even if I have to pay with accuracy?    My OS is Windows, CUDA8, two 1080i,  and config is as follows:    Configurations:  BACKBONE_SHAPES                [[256 256]   [128 128]   [ 64  64]   [ 32  32]   [ 16  16]   [  8   8]   [  4   4]]  BACKBONE_STRIDES               [4, 8, 16, 32, 64, 128, 256]  BATCH_SIZE                     1  BBOX_STD_DEV                   [ 0.1  0.1  0.2  0.2]  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0  DETECTION_NMS_THRESHOLD        0.3  GPU_COUNT                      1  IMAGES_PER_GPU                 1  IMAGE_MAX_DIM                  1024  IMAGE_MIN_DIM                  800  IMAGE_PADDING                  True  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [ 123.7  116.8  103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           test  NUM_CLASSES                    30  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (16, 32, 64, 128, 256, 512, 1024)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [ 0.1  0.1  0.2  0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                1000  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001"
"Hi     I have a binary dataset that I m training using resnet50 and MobileNet.  My graphics card is NVIDIA GeForce GTX 970(3.5 + 0.5 Gigs)     When i tried to train with resnet101 , I got the error   > _OOM when allocating tensor with shape[200,28,28,256]_    This is probably because the memory is not sufficient to load the entire model.  When I train with ""all"" layers in training schedule for resnet50 and MobileNet , I get the same error.     So I have experimented with training schedules and the training runs for following   resnet50 - 3+ layers  mobilenet - 1+ layers      I intend to train using a training schedule   step 1-----1-40 epochs - train heads  step 2-----40-120 epochs - train 4+  step 3-----120-160 epochs - train layers 1 to 4   step 4-----160 -200 epochs - train layers 4+     Did anyone experiment with training schedules ? How does replacing fine tuning ""all"" layers as last training step by step 3 and step 4 affect the loss? "
"I extended Mask RCNN for  dataset    I tried to use early stopping having this very bad results.    !     After a training session, I recover the best checkpoint (according to the val_loss score), and start a new training session with a lower learning rate. I’m using a validation set of 133 pictures. And measuring val_loss over all the 133 pictures on each epoch.    The checkpoints contains only the weights. So perhaps, after reseting the momentum, it has lost its way.  Perhaps I have a bug in the code. Or perhaps I'm trusting too much in a val_loss for just a few observations.    What could be happening? How important is to preserve the momentum? Is it usual to start losing model accuracy at the beginning of resuming a training?"
Is there a code for input video image or webcam? and how fast would it be? In paper they said 5fps but is it fast enough to be called real-time?    
"I want to achieve the 4k video demo, would like to ask what kind of hardware configuration needs, I use the CPU speed is very slow, a single image takes 5 to 7 seconds, the computer brand is mac book pro"
"When I change ROIAlign to ROIPooling, I found training speed is double faster that before. Is this normal or not? Thank you very much."
"Error:  In Demo notebook, when using `GPU_COUNT=3` and        it shows    ` InvalidArgumentError: Input to reshape is a tensor with 600 values, but the requested shape has 1800`    Environment:  python3  tensorflow 1.4.0  Hardware:  3x Titan X    Full stack trace:     "
"Hi,  This is a really great project with great possiblities.    I myself am a medical phd student, consequently my programming knowledge is limited.  I have my own dataset of images with cells, which I want to try and train this model with.    I have used the object detection of google tensorflow before and this worked.  However I have not made a model with masks before, neither can I get pycocotools to work.    I did get the training shapes to work.    I was wondering if someone could explain to me the tools they use on how to make their own dataset fit for this model?  I presume the detection works the same as the tensorflow object detection API, but how do you prepare your mask data? What tools do you use and how should it be prepared for the input?     Any help is greatly appreciated,  Kind regards,  Sören  "
"I have changed the Mask RCNN code and annotations to include with every annotation also a  . My project is about flood level estimation. So every annotation has associated   as well as  . The   denotes the level of the flood in the images    I have implemented the   exactly as   which is defined in function  . I have used only some classes of coco dataset like person, bus, car, bicycle and added around 1200 images to the dataset of my own of flood. I have also added 2 classes   and   to the annotation file. All annotations from coco dataset has been assigned ""no level"" which is denoted by level_id 1. So the total level_ids are 12 and 6 class ids.     I have also made required changes in PythonAPI to support level_ids and level_loss.   So after starting training around 4500 iteration the level loss turns to nan which in turn makes the whole loss nan. Sometimes also little earlier. Does have any idea why that might be happening?    I have tried lowering   till 0.00001 but I still get nan in  .  I also tried the numbering of level_ids from 0-11 to 1-12 but still no effect.  I have attached below some part of the training log of loss    @waleedka : Can you give any suggestion where I might be going wrong? or even how to debug to get to the cause? Any help would be really appreciated.    Thanks in advance.     "
"Hi @waleedka , how can I add a new metric like mAP. I usually define a function called mAP(y_true, y_pred) and then pass it to the model.compile like metrics=[mAP] but this does not work this time?"
"Running on an AWS *p2.xlarge* instance (one 12GB Tesla K80, 64GB RAM) along with the `1.5.0-gpu-py3` docker image of tensorflow, I get the following behavior after modifying the demo notebook to take in a variable number of images and adjusting the `IMAGES_PER_GPU` variable in the configuration class:    * With batches of one image, the first call to `detect` takes about 20 seconds, and subsequent calls to `detect` take under a second. The GPU is being used.  * With batches of two or more images, the first call to `detect` never completes. The **python3** process just consumes an increasingly large amount of system memory and 100% of CPU. `nvidia-smi` shows the **python3** process but the GPU is not used."
Hello—fantastic project this! I was wondering how I could go about getting the pixel coordinates of the four corners of each bounding box? I'm not sure where to start looking for this.
"In the Mask R-CNN paper, it mentioned that `learning rate of 0.02 which is decreased by 10 at 120k iteration`. not sure what does `decreased by 10` means, 10%? However, in this implementation, learning rate decay is not used, i wonder if there is any specific reason of not using it? In general, it is always good to use learning rate decay right? Right now, with my small train set (600+ images), the val_loss stuck after ~ 40 epochs which loss is still improving, i wonder if introduction lr decay will help. Thx in advance! "
"In this method, there's a line that resizes crowd masks if the mask returned by annToMask() is smaller than its annotated dimensions. The line is as follows:    if m.shape[0] != image_info[""height""] or m.shape[1] != image_info[""width""]:      m = np.ones([image_info[""height""], image_info[""width""]], dtype = bool)    This is supposed to resize m so that its height and width matches the input image's height and width. However, isn't this line setting m (the mask) to a Boolean array of the same size as the input image and, in the process, writing over the instance masks' values?"
"Hi,    I would like to train Mask RCNN for a single object detection and segmentation. I followed the approach described in the **train_shapes.ipynb** file. Besides, I have verified the prepared data (bounding box and mask) using the **inspect_data.ipynb** file. However, when I run training with my data (using **train_shapes.ipynb** file) it does not progress and only displays:  `Epoch 1/1`     At this point, it is difficult to know what went wrong as it does not show any other error message. Therefore, I would like to ask for help to resolve this situation ...    Thanks."
"Hi,    I'm trying to run demo.ipynb with jupyter but I enconter this issue:    ----> import skimage.io  No module named 'skimage'    But I have installed skimage using conda (I also tried using pip). And I'm able to use skimage directly with python.  The version of skimage I have is 0.13.1    Thanks for help"
"Hello,    I only have a small training set with about 670 labelled images and would like to further improve the accuracy by training entire backbone network instead of only heads. However, after about 30,40 epoch, the network suffer from overfitting already. ResNet already uses batch norm, so i wonder if there is sth else i can do to improve the situation? How about dropout? If i apply dropout, can i still load the pre-trainned resent weight from CoCo or Imagenet? Or some other technique? Thank you!"
The original paper mentions mask rcnn can be used to do keypoint detection. Does this implementation favor it? 
"Hi,  I am running the code coco.py in pycharm . i have downloaded cocoapi form   and placed int the same directory as Mask_RCNN . this wokred for me in Jupyter notebook. But in pycharm i am getting this error as in the screenshot while importing the coco dataset. Please kindly advise . thanks in advance  !     "
"Hello,    I would like to know if mask Rcnn can be used for binary images. In addition, can I generate a data and generate their annotation directly. (For example if I want to generate digits, I can directly know the correct pixels and segment them but is it possible to generate annotate file after this?)"
After training almost 15 hours correctly the training crashes ginving a Segmentation fault (core dumpted) and nothing else. Any Idea what this could be?
"Hi,  I have a previously annotated dataset that has the following format:  1 - input:  RGB images  2 - output: grayscale mask images, 2 classes (1 channel) (no bbox annotated, no instances annotated, simple segmentation)    Is it possible to train using this dataset knowing that instances are not considered nor bboxed? If possible, can you guys give me the image format required for the mask?     Thanks!   "
"Hi I am getting this following error as in the screen shot when i run ## Create Model and Load Trained Weights section  .Please kindly advise what could be the issue and how to resolve  !       OSError                                   Traceback (most recent call last)    in  ()        3         4 # Load weights trained on MS-COCO  ----> 5 model.load_weights(COCO_MODEL_PATH, by_name=True)    ~\MaskRCNN\Mask_RCNN\model.py in load_weights(self, filepath, by_name, exclude)     2015         if h5py is None:     2016             raise ImportError('`load_weights` requires h5py.')  -> 2017         f = h5py.File(filepath, mode='r')     2018         if 'layer_names' not in f.attrs and 'model_weights' in f:     2019             f = f['model_weights']    c:\users\swati\anaconda\envs\maskrcnn\lib\site-packages\h5py\_hl\files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds)      267             with phil:      268                 fapl = make_fapl(driver, libver, **kwds)  --> 269                 fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)      270       271                 if swmr_support:    c:\users\swati\anaconda\envs\maskrcnn\lib\site-packages\h5py\_hl\files.py in make_fid(name, mode, userblock_size, fapl, fcpl, swmr)       97         if swmr and swmr_support:       98             flags |= h5f.ACC_SWMR_READ  ---> 99         fid = h5f.open(name, flags, fapl=fapl)      100     elif mode == 'r+':      101         fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)    h5py\_objects.pyx in h5py._objects.with_phil.wrapper()    h5py\_objects.pyx in h5py._objects.with_phil.wrapper()    h5py\h5f.pyx in h5py.h5f.open()    OSError: Unable to open file (truncated file: eof = 153075712, sblock->base_addr = 0, stored_eof = 257557808)  "
"In this function, there is an if block as follows:    if config.USE_MINI_MASK:          y1, x1, y2, x2              = tf.split(positive_rois, 4, axis = 1)          gt_y1, gt_x1, gt_y2, gt_x2  = tf.split(roi_gt_boxes, 4, axis = 1)          gt_h                        = gt_y2 - gt_y1          gt_w                        = gt_x2 - gt_x1          y1                          = (y1 - gt_y1) / gt_h          x1                          = (x1 - gt_x1) / gt_w          y2                          = (y2 - gt_y1) / gt_h          x2                          = (x2 - gt_x1) / gt_w          boxes                       = tf.concat([y1, x1, y2, x2], 1)    What is the point of normalizing the ROI proposals this way? At this way, both y1 and gt_y1 are normalized to [0, 1] w.r.t. the input image coordinates. Why are we normalizing y1 again (and x1, y2, x2)?"
"Similar to coco.py, I modifies the shape.ipynb to train on my dataset and first stage training on `heads` completed but the EOFError arises half-way during the first epoch training on `4+`. The info is as below, I can train it on single GPU without this error.    There are 2k images in total  and config like below. Is there problem with step_per_epoch or validation_steps?    GPU_COUNT = 2  IMAGES_PER_GPU = 5  STEPS_PER_EPOCH = 200  VALIDATION_STEPS = 200     "
running train_shapes with the following config on a machine with `CPU` only:      and and training with `layers='heads'`:     generates the following `InvalidArgumentError`:     
"Hello Everyone,    Can you explain how do I train this detectron for object detection task only. By object detection only, I mean I have a rectangular bounding box around objects I need to detect. I don't have a mask for object and I don't even want that in my inference.    All I want is to train it like object detection frameworks like Yolo and SSD, just with rectangular bounding box around object.    Any suggestions will be highly appreciated.  Thank you,  Regards,  Dharma KC"
"Hey,   I wanted to ask how the mean average precision is calculated. My model produces many false positives, but this does not seem to be reflected in the mAP.    One example: I predict three bounding boxes, two of them FPs. Below the precision/recall scores.    `precisions = [1.         1.         0.5        0.33333333 0.        ]`  `recalls = [0. 1. 1. 1. 1.]`  `overlaps = [[0.75] [0.  ] [0.  ]]`    For this example, I get mAP=1.0. Shouldn't this be lower? What am I doing wrong here?  "
"   -      Hi!  @waleedka I am trying to train the project from scratch using coco 2014 dataset  to get the same recognition result like the weights (mask_rcnn_coco.h5).         But i am encountered 2 problems mainly:         1.          The trained network can only recognize person correctly, while the other objects are wrongly classified ,for example: cars in the val images are labeled as motorcycles;   many other instances are classified as person.        2.         During the training procedure, the total loss dropped dramatically in the 1 stage. But the loss' descent stopped in the 2 and 3 stage, and the mask_cls_loss is about 0.7 while the total loss is 2.4. I have tried to change learning_rate from 2/1000 to 5/10000 but nothing  happended.    -  The parameters i used  are basically the same as you provided，and i  found that in coco dataset，person instances number are much more than the others instances. So  i am trying to manually decrease the person instances for network's training to keep all instances number balanced.         I really appreciate any suggestions from yours about howto reduce training total losses or mask_cls_losses! Many Thanks!"
"I am using class `DataSet` and function `data_generation` for my own model and I find that the coordinates of bbox generated by `generate_pyramid_anchors()` is [N, (y1, x1, y2, x2)].  The docs in this functions says it is [N, (y1, x1, y2, x2)]."
None
"Hi,    I am trying to train for coco2014 dataset. (I just followed this repo's training example)    I put coco dataset on      home/mson/download/Mask_RCNN/coco/train2014/(image files..)     home/mson/download/Mask_RCNN/coco/val2014/(image files..)     home/mson/download/Mask_RCNN/coco/annotations/(.json files including 'instances_train2014.json')    The command I am using is:  python coco.py train --dataset=/path/to/coco/ --model=coco    The result I can see is that   FileNotFoundError: [Errno 2] No such file or directory: '/path/to/coco/annotations /instances_train2014.json'    Can anyone please help? Please see the logs below.    /home/mson/anaconda3/envs/tf/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.    from ._conv import register_converters as _register_converters  Using TensorFlow backend.  Command:  train  Model:  coco  Dataset:  /path/to/coco/  Year:  2014  Logs:  /home/mson/download/Mask_RCNN/logs  Auto Download:  False    Configurations:  BACKBONE_SHAPES                [[256 256]   [128 128]   [ 64  64]   [ 32  32]   [ 16  16]]  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     2  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.7  DETECTION_NMS_THRESHOLD        0.3  GPU_COUNT                      1  IMAGES_PER_GPU                 2  IMAGE_MAX_DIM                  1024  IMAGE_MIN_DIM                  800  IMAGE_PADDING                  True  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           coco  NUM_CLASSES                    81  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                1000  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001      Loading weights  /home/mson/download/Mask_RCNN/mask_rcnn_coco.h5  2018-02-09 16:07:30.877009: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA  2018-02-09 16:07:31.080556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero  2018-02-09 16:07:31.080894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties:   name: Quadro K5200 major: 3 minor: 5 memoryClockRate(GHz): 0.771  pciBusID: 0000:02:00.0  totalMemory: 7.93GiB freeMemory: 7.51GiB  2018-02-09 16:07:31.080922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Quadro K5200, pci bus id: 0000:02:00.0, compute capability: 3.5)  loading annotations into memory...  Traceback (most recent call last):    File ""coco.py"", line 476, in        dataset_train.load_coco(args.dataset, ""train"", year=args.year, auto_download=args.download)    File ""coco.py"", line 108, in load_coco      coco = COCO(""{}annotations/instances_{}{}.json"".format(dataset_dir, subset, year))    File ""/home/mson/anaconda3/envs/tf/lib/python3.6/site-packages/pycocotools/coco.py"", line 79, in __init__      dataset = json.load(open(annotation_file, 'r'))  FileNotFoundError: [Errno 2] No such file or directory: '/path/to/coco/annotations/instances_train2014.json'"
Is there a way to run the RPN on an image (as in cell  ) and **get the predicted probabilities for the proposals**?    
"TensorFlow Version: 1.5.0  Keras Version: 2.1.2  Python version: 3.5.2  GPU: NVIDIA 1080 GTX    Hello,    I am trying to extract the region of interest output of various datasets with the given model file, however after around a thousand iterations, I receive some errors. I don't think it is an issue caused by the limitations of my GPU, since this is only a forward pass task. I have to restart evaluating afterwards and same thing happens again. There are no problems with receiving the final output with the same datasets.     This was the draft code that I wrote.         First, the program is beginning showing these warnings:      Then the warnings change to:         And finally, I receive this error.         In essence, my purpose is to evaluate multiple series of images more than a single image and to solve the corresponding problem, I have tried defining the allocator type as BFC as a suggestion. I also evaluate with a batch size of 1. However, I think that there might be an issue with the garbage collection. Does anyone of you have a suggestion to solve this problem?    Best regards,"
"Hello,    How can I fine-tune the model on a custom dataset that might have a different number of classes using the pretrained weights?"
"Hi Waleed,    Thank you again for the great resource !!!   I have two quick questions about the flag: MINI_MASK_SHAPE.    1. When you train and test the coco dataset, I believe you always use MINI_MASK_SHAPE=(56, 56) instead of the full mask. Is this the main reason that the coco evaluation result is 5-6% lower than the original paper result ?    2. If I plan to fine tune the coco pretrained Mask RCNN on some dense segmentation datasets, would you recommend me to turn off USE_MINI_MASK or increase MINI_MASK_SHAPE, e.g. (112, 112) ?    Many thanks !   "
Is it possible to live object detection Mask RCNN using single GPU...?
"1. Do the images have to be 128*128? When I try 256*256 I get the ""invalid bounding box error"" which is because of using mini masks and when not using mini masks, I get seg faults.  2. Do the images have to be square in this implementation?"
"I have saved the mask_rcnn model as the .pb file .But there are totally two parts of input in keras code: image and meta. I couldn't find how to feed them into the input in tensorflow c++.  This is my c++ code:                     Status run_status = session->Run(                               {{""input_image"", image_tensor},{""input_image_meta"",meta_tensor}},                               {""output_node0""}, {}, &outputs                   );    And I got an error  ""Running model failed: Not found: FeedInputs :unable to find feed output input_image_meta"". Are there any tricks to solve the problem??  Thanks!  "
"I'm a bit skeptical about computation of the average precision (see `utils.compute_ap`).   For a given prediction bounding box the following conditions have to be met for it to have a  `pred_match` value of 1 (see       - max IOU should be > `iou_threshold`, and  - the ground truth box for which such IOU is reached is not linked to any another prediction bounding box (multiple detection issue), and    - label of the prediction bounding box and this ground truth box should be identical    However, by doing this: `if gt_match[j] == 1: continue` you allow the pred box to be associated to a ground truth box with a lesser IOU and potentially decrease the number of False Positives    Am I missing something ?"
"What should I do in config.py?  like   BACKBONE_STRIDES ([1, 2, 4, 8, 16])?  MASK_SHAPE(14,14)?  RPN_ANCHOR_SCALES = (1, 2, 4, 8, 16)?  THANKS."
"I want to run the mask rcnn on android , but I have not .pb file"
Has anyone implemented the ResNeXt backboned version of Mask R-CNN and tested the results? 
"Hi,    I'm trying to remove the Resnet101 5th stage and other relevant layers.    But I got ""Invalid argument: Input to reshape is a tensor with 408576 values, but the requested shape requires a multiple of 204800"" error."
"Hi I am trying to run the MSCOCO evaluation code but I find the following error:  Traceback (most recent call last):    File ""/home/kunolab/Matiqul/Mask_RCNN-master/coco.py"", line 415, in        evaluate_coco(model, dataset_val, coco, ""bbox"", limit=10)    File ""/home/kunolab/Matiqul/Mask_RCNN-master/coco.py"", line 285, in evaluate_coco      coco_results = coco.loadRes(results)    File ""/usr/local/lib/python3.4/dist-packages/pycocotools/coco.py"", line 308, in loadRes      if type(resFile) == str or type(resFile) == unicode:  NameError: name 'unicode' is not defined    Please how I can solve this....."
hii I have dataset prepared through labelme (they are json files with their respective images) and I know I have to edit Dataset class but I need to know how to load my dataset ?   
"Hi,I just could see the loss during training and want to know how to display  accuracy during  training  !         "
"I have prepared my dataset with images and annotations, but when I get to training step ""Exception: Invalid bounding box with area of zero"" raised. my images are [ 1280 * 720] and I did all the annotation based on this size.   how to fix the problem of large images, I see that the original paper used [800, 1024].   Is there a way to modify and use large images instead? "
"@waleedka     Hi, I'm trying to train mask-rcnn coco model,  But it doesn't go well.  I'm curious about how did you train your own model.  Can you open your hyper parameter settings for 'mask_rcnn_coco.h5'?    if you could open your training strategy in detail, it can be helpful to me!  thank you :+1: "
I run this program with caffe using one gpu. But the gpu memory is not enough. so how to set the gpu_memory_fraction like in tensorflow? 
"I am runing a code 'demo.ipynb'  I cannot solve this simple problem. If I change the code in  the line 403 'utils.py' with below, then it shows nothing. What should I do ?? ..    `  h_scaled = int(round(h*scale))  w_scaled = int(round(w*scale))    if h_scaled > 0 and w_scaled > 0:       image = scipy.misc.imresize(image, (h_scaled, w_scaled))  `       >   ValueError                                Traceback (most recent call last)    in  ()        6         7 # Run detection  ----> 8 results = model.detect([image], verbose=1)        9        10 # Visualize results    /home/Mask_RCNN/model.pyc in detect(self, images, verbose)     2331                 log(""image"", image)     2332         # Mold inputs to format expected by the neural network  -> 2333         molded_images, image_metas, windows = self.mold_inputs(images)     2334         if verbose:     2335             log(""molded_images"", molded_images)    /home/Mask_RCNN/model.pyc in mold_inputs(self, images)     2234                 min_dim=self.config.IMAGE_MIN_DIM,     2235                 max_dim=self.config.IMAGE_MAX_DIM,  -> 2236                 padding=self.config.IMAGE_PADDING)     2237             molded_image = mold_image(molded_image, self.config)     2238             # Build image_meta    /home/Mask_RCNN/utils.py in resize_image(image, min_dim, max_dim, padding)      401         ##origin      402         image = scipy.misc.imresize(  --> 403            image, (int(round(h * scale)), int(round(w * scale))))  # jaesungchoe      404       405         # ##jaesungchoe    /usr/local/lib/python2.7/dist-packages/numpy/lib/utils.pyc in newfunc(*args, **kwds)       99             """"""`arrayrange` is deprecated, use `arange` instead!""""""      100             warnings.warn(depdoc, DeprecationWarning, stacklevel=2)  --> 101             return func(*args, **kwds)      102       103         newfunc = _set_function_name(newfunc, old_name)    /usr/local/lib/python2.7/dist-packages/scipy/misc/pilutil.pyc in imresize(arr, size, interp, mode)      562         size = (size[1], size[0])      563     func = {'nearest': 0, 'lanczos': 1, 'bilinear': 2, 'bicubic': 3, 'cubic': 3}  --> 564     imnew = im.resize(size, resample=func[interp])      565     return fromimage(imnew)      566     /usr/lib/python2.7/dist-packages/PIL/Image.pyc in resize(self, size, resample)     1536             return self.convert('RGBa').resize(size, resample).convert('RGBA')     1537   -> 1538         return self._new(self.im.resize(size, resample))     1539      1540     def rotate(self, angle, resample=NEAREST, expand=0):    ValueError: height and width must be > 0    >     "
"Hello,    I have two separate directories in an ""image"" directory to test the network. Inside these two folders, there are 742 and 661 images sequentially. When I specify the batch size as 1 for testing, my code works without any issues. However, I want to accelerate the testing speed by specifying the batch size as 10.     In model.py file, the following assertion  `assert len(images) == self.config.BATCH_SIZE, ""len(images) must be equal to BATCH_SIZE""  `  is making my code to skip the last few images at the last iteration of testing. I have had a few attempts for solving the issue and I can share them if necessary. How can we do such a change that the code can do the following:  1) will create a list of 10 images, and then clear the list after evaluation.  2) will be able to reduce the batch size in the last iteration.      Thank you for creating and developing this repository again.  "
I've deployed MaskRCNN (finet'd on my data) on images size 250x250. What I've noticed is that mAP very strongly (up to 10 ppts) depends on the pad size: 256x256 is better than 512x512 and so on. I don't quite understand why this is the case. Any suggestions?  
"Hi,guys  When I retrain the model from the mask_rcnn_coco.h5, the start loss is different from the final loss provided in the repository. Such as the mrcnn_mask_loss starts from 0.2627 and ends at 0.2607, but the provided loss likely starts from 0.27 and ends at 0.08. I wonder why my mask_loss don't start from 0.08? I show my loss visualized by tensorboard:  <img width=""712"" alt=""2018-01-12 7 04 21"" src=""   "
"ValueError                                Traceback (most recent call last)    in  ()        1 # Create model object in inference mode.  ----> 2 model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)        3         4 # Load weights trained on MS-COCO        5 model.load_weights(COCO_MODEL_PATH, by_name=True)    E:\root\Mask_RCNN\model.py in __init__(self, mode, config, model_dir)     1730         self.model_dir = model_dir     1731         self.set_log_dir()  -> 1732         self.keras_model = self.build(mode=mode, config=config)     1733      1734     def build(self, mode, config):    E:\root\Mask_RCNN\model.py in build(self, mode, config)     1821         # RPN Model     1822         rpn = build_rpn_model(config.RPN_ANCHOR_STRIDE,  -> 1823                               len(config.RPN_ANCHOR_RATIOS), 256)     1824         # Loop through pyramid layers     1825         layer_outputs = []  # list of lists    E:\root\Mask_RCNN\model.py in build_rpn_model(anchor_stride, anchors_per_location, depth)      843     input_feature_map = KL.Input(shape=[None, None, depth],      844                                  name=""input_rpn_feature_map"")  --> 845     outputs = rpn_graph(input_feature_map, anchors_per_location, anchor_stride)      846     return KM.Model([input_feature_map], outputs, name=""rpn_model"")      847     E:\root\Mask_RCNN\model.py in rpn_graph(feature_map, anchors_per_location, anchor_stride)      800     shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',      801                        strides=anchor_stride,  --> 802                        name='rpn_conv_shared')(feature_map)      803       804     # Anchor Score. [batch, height, width, anchors per location * 2].    D:\Anaconda3\lib\site-packages\keras\engine\topology.py in __call__(self, inputs, **kwargs)      574                                          '`layer.build(batch_input_shape)`')      575                 if len(input_shapes) == 1:  --> 576                     self.build(input_shapes[0])      577                 else:      578                     self.build(input_shapes)    D:\Anaconda3\lib\site-packages\keras\layers\convolutional.py in build(self, input_shape)      123             channel_axis = -1      124         if input_shape[channel_axis] is None:  --> 125             raise ValueError('The channel dimension of the inputs '      126                              'should be defined. Found `None`.')      127         input_dim = input_shape[channel_axis]    ValueError: The channel dimension of the inputs should be defined. Found `None`.  Do you meet this error?"
"I have an application that we are dealing with the task of detecting of small objects that are characterized by a deformable shape and also they are articulated in nature. So, can we use this algorithm? otherwise any other suggestions."
"It takes me two days to running this code on my own data set. I thought there should be more details in the guidance.  1.  When using `add_image()` in the `utils.Dataset` class, the `image_id` must be consecutive integer from 1 to some number, because `image_id` is the index of a list.  2. Class number also should be consecutive integer from 1 to some number, or you will get an `nan` loss."
"I'm work on the source code of Mask_RCNN and I find something interesting.  Code:     output:     We have 5 kinds of feature maps in different size: 32\*32, 16\*16, 8\*8, 4\*4, 2\*2.  In each of the pixel of each feature map, we generate 3 kinds of anchors of different ratios. In other words, the anchors in each feature map should be [32\*32, 16\*16, 8\*8, 4\*4, 2\*2] * 3, but I find that the number of anchors generated by function generate_pyramid_anchors() is three times the number above.  Code:     output:     Code:     output:     Those anchors repeated 3 times. I wonder is this a bug or just for convenient?  "
"Sometimes, a picture gives no positive_roi, and this will raise an error:    W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Reduction axis 1 is empty in shape  ]]  Traceback (most recent call last):    File ""/home/xxx/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1323, in _do_call      return fn(*args)    File ""/home/xxx/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1302, in _run_fn      status, run_metadata)    File ""/home/xxx/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__      c_api.TF_GetCode(self.status.status))  tensorflow.python.framework.errors_impl.InvalidArgumentError: Reduction axis 1 is empty in shape  ]]     ]]    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""portrait_seg.py"", line 206, in        layers='4+')    File ""/home/xxx/Desktop/keras_Mask_RCNN/model.py"", line 2211, in train      use_multiprocessing=True,    File ""/home/xxx/anaconda2/envs/py36/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 87, in wrapper      return func(*args, **kwargs)    File ""/home/xxx/anaconda2/envs/py36/lib/python3.6/site-packages/keras/engine/training.py"", line 2096, in fit_generator      class_weight=class_weight)    File ""/home/xxx/anaconda2/envs/py36/lib/python3.6/site-packages/keras/engine/training.py"", line 1814, in train_on_batch      outputs = self.train_function(ins)    File ""/home/xxx/anaconda2/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 2352, in __call__      **self.session_kwargs)    File ""/home/xxx/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 889, in run      run_metadata_ptr)    File ""/home/xxx/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1120, in _run      feed_dict_tensor, options, run_metadata)    File ""/home/xxx/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run      options, run_metadata)    File ""/home/xxx/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call      raise type(e)(node_def, op, message)  tensorflow.python.framework.errors_impl.InvalidArgumentError: Reduction axis 1 is empty in shape  ]]     ]]    Caused by op 'proposal_targets/ArgMax', defined at:    File ""portrait_seg.py"", line 168, in        model_dir=MODEL_DIR)    File ""/home/xxx/Desktop/keras_Mask_RCNN/model.py"", line 1744, in __init__      self.keras_model = self.build(mode=mode, config=config)    File ""/home/xxx/Desktop/keras_Mask_RCNN/model.py"", line 1885, in build      target_rois, input_gt_class_ids, gt_boxes, input_gt_masks])    File ""/home/xxx/anaconda2/envs/py36/lib/python3.6/site-packages/keras/engine/topology.py"", line 603, in __call__      output = self.call(inputs, **kwargs)    File ""/home/xxx/Desktop/keras_Mask_RCNN/model.py"", line 641, in call      self.config.IMAGES_PER_GPU, names=names)    File ""/home/xxx/Desktop/keras_Mask_RCNN/utils.py"", line 673, in batch_slice      output_slice = graph_fn(*inputs_slice)    File ""/home/xxx/Desktop/keras_Mask_RCNN/model.py"", line 640, in        w, x, y, z, self.config),    File ""/home/xxx/Desktop/keras_Mask_RCNN/model.py"", line 544, in detection_targets_graph      roi_gt_box_assignment = tf.argmax(positive_overlaps, axis=1)    File ""/home/xxx/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 316, in new_func      return func(*args, **kwargs)    File ""/home/xxx/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 205, in argmax      return gen_math_ops.arg_max(input, axis, name=name, output_type=output_type)    File ""/home/xxx/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 441, in arg_max      name=name)    File ""/home/xxx/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper      op_def=op_def)    File ""/home/xxx/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op      op_def=op_def)    File ""/home/xxx/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__      self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access    InvalidArgumentError (see above for traceback): Reduction axis 1 is empty in shape  ]]     ]]        I tried to fix this by add a check if there is a positive_indices, in model.py, line528:        # Subsample ROIs. Aim for 33% positive      # Positive ROIs      positive_count = int(config.TRAIN_ROIS_PER_IMAGE *                           config.ROI_POSITIVE_RATIO)      positive_indices = tf.random_shuffle(positive_indices)[:positive_count]      a = positive_indices.get_shape()[0]      if a == 0:          positive_indices = tf.constant([1])      positive_count = tf.shape(positive_indices)[0]    However, it can avoid some wrong cases, but still raise error in other cases.  How to fix this throghly?    "
"Hi,    Thank you for making public Mask RCNN on github. It is really amazing work.  I tried to replace the ResNet-101 encoder with Inception-ResNet-V2 encoder from keras.  Unfortunately, I didn't get better results.    These are the endpoints I use to build the feature pyramid. They correspond to the different scales.         I'm training `InceptionResNet-V2` on coco dataset train + valvalminusminival with one GPU, one image/gpu 2000 steps / epoch. Initial learning rate is 0.006. And training strategy is first end to end and then fine tune for heads.           Unfortunately, loss doesn't decrease as provided model: train loss is 0.84, and val loss 0.2. bbox results are lower. Provided model has train loss 0.7 when finetuning.         I have some questions:      1. I noticed you use ResNet101 encoder and download ResNet50 weights. How do you apply the weights from ResNet50 to ResNet101?    2. I'm trying to reproduce the results from the repository using ResNet101. I currently have:   - 1 GPU. 2 Images / GPU, STEPS_PER_EPOCH = 1000.    The training strategy is default (as in given example):         Is there anything I need to add to reproduce the results? I'm downloading ImageNet ResNet50 weights for ResNet101 encoder.    3. How can I train using Inception-Resnet-V2 encoder? I also tried to use atrous convolution in RPN when building FPN. I managed to decrease the training loss under 1.00, but it's not enough. Why do you first train the heads (freeze the encoder) and then fine tune the encoder? In my experiements, I first trrained end-to-end, and then fine-tuned the heads. Am I missing something? Do you do any data augmentation, besides the random horizotnal flips from `load_image_gt` method?         I'm pasting here the loss from tensorboard when using InceptionResnetV2. At last stage of training, when only the heads are training with learning rate/100, the loss seems to jump up instead of decreasing. Maybe it's because I used the same learning rate as previous stage. For all other learning stages, when learning rate is decreased 10 times,  loss decreases.    !         Thank you,  Vlad                    "
"When run the coco evaluation, I found I can only set the batch size to 1. How can I set the batch size like train."
"Hi,i am a new to tf and keras.I run Mask_RCNN-2.0 .when i debug the project,i meet a problem.But i dont understand it likeing this:  Traceback (most recent call last):    File ""/home/whao/pycharm-2017.1.2/helpers/pydev/pydevd.py"", line 1585, in        globals = debugger.run(setup['file'], None, None, is_module)    File ""/home/whao/pycharm-2017.1.2/helpers/pydev/pydevd.py"", line 1015, in run      pydev_imports.execfile(file, globals, locals)  # execute the script    File ""/home/whao/pycharm-2017.1.2/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile      exec(compile(contents+""\n"", file, 'exec'), glob, loc)    File ""/home/whao/Mask_RCNN-2.0/coco.py"", line 382, in        model_dir=args.logs)    File ""/home/whao/Mask_RCNN-2.0/model.py"", line 1735, in __init__      self.keras_model = self.build(mode=mode, config=config)    File ""/home/whao/Mask_RCNN-2.0/model.py"", line 1875, in build      target_rois, input_gt_class_ids, gt_boxes, input_gt_masks])    File ""/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py"", line 603, in __call__      output = self.call(inputs, **kwargs)    File ""/home/whao/Mask_RCNN-2.0/model.py"", line 636, in call      self.config.IMAGES_PER_GPU, names=names)    File ""/home/whao/Mask_RCNN-2.0/utils.py"", line 673, in batch_slice      output_slice = graph_fn(*inputs_slice)    File ""/home/whao/Mask_RCNN-2.0/model.py"", line 635, in        w, x, y, z, self.config),    File ""/home/whao/Mask_RCNN-2.0/model.py"", line 531, in detection_targets_graph      negative_count = int((positive_count / config.ROI_POSITIVE_RATIO) - positive_count)  TypeError: unsupported operand type(s) for /: 'Tensor' and 'float'    why the int32 of tensor can operand to the float constant:      positive_count = tf.shape(positive_indices)[0]      # Negative ROIs. Add enough to maintain positive:negative ratio.      negative_count = int((positive_count / config.ROI_POSITIVE_RATIO) - positive_count)    it differs from the first version:      r = 1.0 / config.ROI_POSITIVE_RATIO      negative_count = tf.cast(r * tf.cast(positive_count, tf.float32), tf.int32) - positive_count  What should i do to be able to do that like Mask_RCNN-2.0?"
"I want to prepare my own dataset like mscoco format , any suggestions?    "
"    Traceback (most recent call last):    File ""model.py"", line 1604, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""model.py"", line 1169, in load_image_gt      padding=config.IMAGE_PADDING)    File ""utils.py"", line 401, in resize_image      image, (round(h * scale), round(w * scale)))    File ""/usr/lib/python2.7/dist-packages/scipy/misc/pilutil.py"", line 487, in imresize      imnew = im.resize(size, resample=func[interp])    File ""/usr/lib/python2.7/dist-packages/PIL/Image.py"", line 1538, in resize      return self._new(self.im.resize(size, resample))  TypeError: integer argument expected, got float  Traceback (most recent call last):    File ""/home/letsperf/.local/lib/python2.7/site-packages/keras/utils/data_utils.py"", line 635, in data_generator_task      generator_output = next(self._generator)    File ""model.py"", line 1604, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""model.py"", line 1169, in load_image_gt      padding=config.IMAGE_PADDING)    File ""utils.py"", line 401, in resize_image      image, (round(h * scale), round(w * scale)))    File ""/usr/lib/python2.7/dist-packages/scipy/misc/pilutil.py"", line 487, in imresize      imnew = im.resize(size, resample=func[interp])    File ""/usr/lib/python2.7/dist-packages/PIL/Image.py"", line 1538, in resize      return self._new(self.im.resize(size, resample))  TypeError: integer argument expected, got float  Traceback (most recent call last):      File "" "", line 5, in        layers='heads')      File ""model.py"", line 2215, in train      use_multiprocessing=True,      File ""/home/letsperf/.local/lib/python2.7/site-packages/keras/legacy/interfaces.py"", line 87, in wrapper      return func(*args, **kwargs)      File ""/home/letsperf/.local/lib/python2.7/site-packages/keras/engine/training.py"", line 2115, in fit_generator      generator_output = next(output_generator)      File ""/home/letsperf/.local/lib/python2.7/site-packages/keras/utils/data_utils.py"", line 735, in get      six.reraise(value.__class__, value, value.__traceback__)      File "" "", line 2, in reraise    TypeError: integer argument expected, got float  "
"Hi!! My project is to detect distress on road. Can Mask_RCNN be used for that? If yes, then please guide me how to start?   My dataset consists of 2k images with positive and negative samples. Would that be enough?"
How do you apply the model to the person bone keypoint data? How does the bone point data need to be processed
hi  I wanted to run this on python 2.7 and I am getting this error-(      /home/letsperf/.local/lib/python2.7/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.22) or chardet (2.3.0) doesn't match a supported version!    RequestsDependencyWarning)  usage: coco.py [-h] --dataset /path/to/coco/ [--year  ] --model                 /path/to/weights.h5 [--logs /path/to/logs/]                 [--limit  ] [--download  ]                    coco.py: error: too few arguments  )
hi   I want to convert my dataset into annoted format like mscoco 
"hi,    Did somebody convert images (jpg/png) to coco format?     Thanks"
"I'm trying to train Mask_RCNN on my own dataset. Everything was fine, I created the annotation and starting building on the coco example but when I start visualizing the data and training, suddenly this message stopped me to continue. I used coco.py to train coco dataset, but the same happened and it stopped. I tried to track the issue line by line and this issue starts when I try to create the mask, from here the message pop up:      >         rle = self.annToRLE(ann, height, width)      >         m = maskUtils.decode(rle)    I am running on anaconda 4.3.29 and python3.6"
"First, Thank you for sharing your code with us.  I have already replaced the ResNet-based backbone networks  by using DenseNet (Keras) and no Error reported,but inference  performance has a issue when I run the demo.ipynb.Pretrained weights is ""densenet161_weights_tf.h5""  Predict result likes the following image:  55  Rectangle(xy=(579, 140), width=27, height=5, angle=0)  traffic light  Rectangle(xy=(191, 105), width=23, height=23, angle=0)  traffic light  Rectangle(xy=(31, 160), width=21, height=8, angle=0)  traffic light  Rectangle(xy=(517, 106), width=18, height=5, angle=0)  traffic light  Rectangle(xy=(483, 117), width=21, height=3, angle=0)  traffic light  Rectangle(xy=(53, 130), width=28, height=5, angle=0)  traffic light  Rectangle(xy=(51, 136), width=30, height=7, angle=0)  traffic light  Rectangle(xy=(524, 110), width=17, height=4, angle=0)  traffic light  Rectangle(xy=(63, 153), width=27, height=6, angle=0)  traffic light  Rectangle(xy=(493, 113), width=27, height=5, angle=0)  traffic light  Rectangle(xy=(337, 108), width=21, height=5, angle=0)  traffic light  Rectangle(xy=(486, 111), width=26, height=4, angle=0)  traffic light  Rectangle(xy=(35, 165), width=16, height=5, angle=0)  traffic light  Rectangle(xy=(290, 85), width=18, height=3, angle=0)  traffic light  Rectangle(xy=(428, 138), width=13, height=9, angle=0)  traffic light  Rectangle(xy=(486, 93), width=18, height=5, angle=0)  traffic light  Rectangle(xy=(562, 136), width=19, height=8, angle=0)  traffic light  Rectangle(xy=(570, 136), width=70, height=4, angle=0)  traffic light  Rectangle(xy=(363, 138), width=25, height=4, angle=0)  traffic light  Rectangle(xy=(90, 103), width=30, height=5, angle=0)  traffic light  Rectangle(xy=(163, 109), width=52, height=2, angle=0)  teddy bear  Rectangle(xy=(260, 96), width=15, height=9, angle=0)  teddy bear  Rectangle(xy=(81, 101), width=9, height=9, angle=0)  traffic light  Rectangle(xy=(421, 121), width=19, height=13, angle=0)  traffic light  Rectangle(xy=(24, 155), width=17, height=7, angle=0)  traffic light  Rectangle(xy=(602, 213), width=24, height=9, angle=0)  traffic light  Rectangle(xy=(66, 156), width=27, height=7, angle=0)  traffic light  Rectangle(xy=(429, 131), width=13, height=8, angle=0)  traffic light  Rectangle(xy=(313, 107), width=12, height=3, angle=0)  traffic light  Rectangle(xy=(55, 143), width=21, height=6, angle=0)  traffic light  Rectangle(xy=(33, 167), width=18, height=8, angle=0)  traffic light  Rectangle(xy=(333, 121), width=12, height=9, angle=0)  traffic light  Rectangle(xy=(191, 103), width=16, height=7, angle=0)  traffic light  Rectangle(xy=(494, 52), width=17, height=8, angle=0)  traffic light  Rectangle(xy=(428, 135), width=8, height=7, angle=0)  traffic light  Rectangle(xy=(320, 110), width=9, height=11, angle=0)  traffic light  Rectangle(xy=(186, 99), width=45, height=5, angle=0)  traffic light  Rectangle(xy=(320, 110), width=25, height=4, angle=0)  traffic light  Rectangle(xy=(33, 155), width=27, height=5, angle=0)  traffic light  Rectangle(xy=(197, 110), width=25, height=7, angle=0)  traffic light  Rectangle(xy=(428, 81), width=16, height=10, angle=0)  traffic light  Rectangle(xy=(328, 105), width=13, height=8, angle=0)  traffic light  Rectangle(xy=(225, 108), width=28, height=12, angle=0)  traffic light  Rectangle(xy=(532, 120), width=31, height=7, angle=0)  traffic light  Rectangle(xy=(8, 88), width=30, height=18, angle=0)  traffic light  Rectangle(xy=(496, 57), width=17, height=6, angle=0)  traffic light  Rectangle(xy=(6, 135), width=35, height=16, angle=0)  traffic light  Rectangle(xy=(98, 121), width=25, height=5, angle=0)  traffic light  Rectangle(xy=(489, 88), width=26, height=4, angle=0)  traffic light  Rectangle(xy=(58, 148), width=15, height=5, angle=0)  traffic light  Rectangle(xy=(10, 125), width=30, height=14, angle=0)  traffic light  Rectangle(xy=(414, 115), width=13, height=10, angle=0)  traffic light  Rectangle(xy=(44, 141), width=22, height=7, angle=0)  traffic light  Rectangle(xy=(183, 106), width=17, height=10, angle=0)  traffic light  Rectangle(xy=(575, 56), width=38, height=4, angle=0)  baseball bat  !   I don't know how to solve this problem.What should I do ?"
"/Mask_RCNN-master/model.py:1423: RuntimeWarning: overflow encountered in uint_scalars    gt_w = gt[3] - gt[1]     14/100 [===>..........................] - ETA: 22:48 - loss: 27610388.0000 - rpn_class_loss: 0.0590 - rpn_bbox_loss: 27610386.0000 - mrcnn_class_loss: 0.7650 - mrcnn_bbox_loss: 0.4698 - mrcnn_mask_loss: 0.0930    as you can see, the rpn_bbox_loss is so big ,why are so that?? is anyone have meet the same problems???"
"Hi,   Thanks for providing your code, I have been looking forward to playing around with it since the paper was released.   I have been playing around with the notebooks you provided, testing them both with the 2017 and 2014 versions of the COCO dataset.   I am having a small issue I cannot seem to figure out though. The last step of the inspect_data.ipynb notebook seem to never get any positive ROIs.     Example given in the attached screen shot.    !   "
"Hi, I have trained a model on this dataset:   to segment face and hair.  When testing, sometimes I got only one mask when it should be two(one for hair and one for face).    After debugging, I find in model.py line2332, only the first 'detections' is non-zero, and the first 'mrcnn_mask' is non-zero. Seems that the network only gives one valid mask.    However, I plot the first four masks as below:  !     The first mask has segmented hair and face successfully! The problem is why it only gives one mask?  "
"Hi guys,  Could you describe how to save image after visualize.display_instances () plot it in jupyter notebook.  I'd like to save it automaticaly.     "
"I think save val_loss to weight file,so set checkpoint_path :  self.checkpoint_path = os.path.join(self.model_dir, ""mask_rcnn_{epoch:04d}_{val_loss:.4f}_.h5"")  but it is error: typeError:unsupported format string passed to numpy.ndarray.__format__  any suggest?"
The complete error message goes:  `AttributeError: Can't pickle local object 'GeneratorEnqueuer.start. .data_generator_task'  `
"Hi @waleedka , when I am training the model with 8 GPUs in a single machine,  as the training procedure goes, the speed slows down. Any suggestions to improve my situation? How long does it take you to finish the training process with P100? Many thanks."
"Problems happen when run the demo in train_shapes.ipynb.    When IMAGE_MAX_DIM = 128, the demo is OK.   But the warning happens when I set IMAGE_MAX_DIM = 1024. "
"Hi, I am new to tf, and I am wondering why ""Config.IMAGE_PADDING = False"" can not be deployed in the current version. Can not tf support various input shapes now?   If I want to do some changes to support the Config.IMAGE_PADDING parameter being true, do you have some suggestions on how to achieve it?  "
"Hi, excellent project, I had fun reading your code.    I did some expriments by reconfiguring the codes and the pretrained coco weight to adapt python2.7 and a 2 class problem. I also disabled multiprocessing as in #13  because I don't have root access on shared memory of the machine.     It worked fine when I was training on one GPU, but when I tried to run on multiple GPUs I run into a problem like this:         This exception seems to be raised while the coco weights are being loaded. However, the **parallel_model.py** file runs perfectly on its test code.    I did many searches and I can't solve this one. Is there anyone have a similar problem?    I'm running on Keras(2.0.8) and Tensorflow(1.4.0)  "
"After I begin download mask_rcnn_coco.h5, the download process stopped quickly. Is it normal?"
"Sorry, if this is dumb question!   The resnet tensor flow model that is in tensorflow, expects the input images to be processed by the per_image_standardization function, that subtracts the mean and divides by variance. Mass_RCNN, only subtracts a fixed offset from the image channels, which is from the Keras version of resnet. It seems both projects are using the caffe weights, shouldn't image normalization be the same, since it needs to do whatever was done in caffe?  "
"Hi, I meet a problem when I run the train_shapes.ipynb. In the Training block, when I run model.train(dataset_train, dataset_val, learning_rate=config.LEARNING_RATE, epochs=1, layers='heads')   it reports a error, this is it's detail:  !   !   I have run the demo successfully, but it seems I can not train my own model. please help if you know anything about my error! Thanks a lot!"
"Hi, I'm really impressed by your works.     I want to replace the ResNet-based backbone networks to my   that is rather faster.    Could you advice how to import my networks?    P.S : I alreday had the WR-Inception network models that was trained by using tf-slim.    "
"It would be great to log some useful images to tensorboard, like origin image / results at each step / output image with masks and bounding boxes in training and evaluation process.  Thanks!"
Use OpenCV library？  thx！  
"some question about GPU_COUNT and mask_shape:     1. In the config.py, it says `GPU_COUNT` : **NUMBER of gpus to use, for training on cpu, use 1**.so if I want to use a single gpu from multi-gpu machine, what should i set the `GPU_COUNT`.**1 or 2?**. I found in the class `ParallelModel`, the GPU_COUNT represent the real number of gpu without minus 1(which i think 1 for cpu, and 2 for a single gpu....)    2. Zero volatile GPU-Util but high GPU Memory Usage: most time the volatile GPU-Util is zero but  GPU Memory Usage is very high, and the training is slow after i set the `USE_MINI_MASK = False`, Is this a bug? what should I do? the input size of image is 1024*1024     3. in config.py have two paras: `mask_pool_size` and 'mask_shape', however in  FCN only have one deconv layer which means the `mask_shape` = 2*`mask_pool_size`. so what i should do , if I want a more dense segmentation without resize from 28 * 28 to the Roi size"
"Hi,  Thanks for the great work!    I want to train Mask RCNN on my own dataset (numpy array of images and masks, or two folder for images and masks). Could you help me how can I do that?  I've checked COCO dataset and Shapes dataset codes, but I couldn't understand how Dataset class actually works."
"Why are there two functions 'display_instances' and 'display_detections' ?  They are almost the same, and I don't find the code in where 'display_detections' is used."
"I haven't been able to run `train_shapes.ipynb` to completion perhaps because of threading issues. Training the head branches fails with the following output:         I have also tried to modify fit_generator()'s param as follows:         Unfortunatly, that also results in a crash with the jupyter notebook crashing without any output...    It could well be that the generator is threadsafe. After a quick perusal, however, I haven't found any serializing code anywhere. Threadsafe data generators usually implement some kind of locking mechanism. Here are examples that are threadsafe:   and      Here's a bit of information about my own GPU config:    (from notebook):         (from jupyter notebook log):         Has anyone else observed similar issues?"
"I'm playing around with the shapes data-set; I have increased the settings by 4:         And also generate 100x more samples. However I get errors like this that stop execution:    > ValueError                                Traceback (most recent call last)    in  ()        6             learning_rate=config.LEARNING_RATE,        7             epochs=1,  ----> 8             layers='heads')  /home/iliauk/Mask_RCNN/model.py in train(self, train_dataset, val_dataset, learning_rate, epochs, layers)     2072             ""steps_per_epoch"": self.config.STEPS_PER_EPOCH,     2073             ""callbacks"": callbacks,  -> 2074             ""validation_data"": next(val_generator),     2075             ""validation_steps"": self.config.VALIDATION_STPES,     2076             ""max_queue_size"": 100,  /home/iliauk/Mask_RCNN/model.py in data_generator(dataset, config, shuffle, augment, random_rois, batch_size, detection_targets)     1525             image_id = image_ids[image_index]     1526             image, image_meta, gt_boxes, gt_masks = \  -> 1527                 load_image_gt(dataset, config, image_id, augment=augment, use_mini_mask=config.USE_MINI_MASK)     1528      1529             # Skip images that have no instances. This can happen in cases  /home/iliauk/Mask_RCNN/model.py in load_image_gt(dataset, config, image_id, augment, use_mini_mask)     1150     # Resize masks to smaller size to reduce memory usage     1151     if use_mini_mask:  -> 1152         mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)     1153      1154     # Image meta data  /home/iliauk/Mask_RCNN/utils.py in minimize_mask(bbox, mask, mini_shape)      433         y1, x1, y2, x2 = bbox[i][:4]      434         m = m[y1:y2, x1:x2]  --> 435         m = scipy.misc.imresize(m.astype(float), mini_shape, interp='bilinear')      436         mini_mask[:, :, i] = np.where(m >= 128, 1, 0)      437     return mini_mask  /anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/utils.py in newfunc(*args, **kwds)       99             """"""`arrayrange` is deprecated, use `arange` instead!""""""      100             warnings.warn(depdoc, DeprecationWarning, stacklevel=2)  --> 101             return func(*args, **kwds)      102       103         newfunc = _set_function_name(newfunc, old_name)  /anaconda/envs/py35/lib/python3.5/site-packages/scipy/misc/pilutil.py in imresize(arr, size, interp, mode)      552       553     """"""  --> 554     im = toimage(arr, mode=mode)      555     ts = type(size)      556     if issubdtype(ts, numpy.signedinteger):  /anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/utils.py in newfunc(*args, **kwds)       99             """"""`arrayrange` is deprecated, use `arange` instead!""""""      100             warnings.warn(depdoc, DeprecationWarning, stacklevel=2)  --> 101             return func(*args, **kwds)      102       103         newfunc = _set_function_name(newfunc, old_name)  /anaconda/envs/py35/lib/python3.5/site-packages/scipy/misc/pilutil.py in toimage(arr, high, low, cmin, cmax, pal, mode, channel_axis)      334         if mode in [None, 'L', 'P']:      335             bytedata = bytescale(data, high=high, low=low,  --> 336                                  cmin=cmin, cmax=cmax)      337             image = Image.frombytes('L', shape, bytedata.tostring())      338             if pal is not None:  /anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/utils.py in newfunc(*args, **kwds)       99             """"""`arrayrange` is deprecated, use `arange` instead!""""""      100             warnings.warn(depdoc, DeprecationWarning, stacklevel=2)  --> 101             return func(*args, **kwds)      102       103         newfunc = _set_function_name(newfunc, old_name)  /anaconda/envs/py35/lib/python3.5/site-packages/scipy/misc/pilutil.py in bytescale(data, cmin, cmax, high, low)       91        92     if cmin is None:  ---> 93         cmin = data.min()       94     if cmax is None:       95         cmax = data.max()  /anaconda/envs/py35/lib/python3.5/site-packages/numpy/core/_methods.py in _amin(a, axis, out, keepdims)       27        28 def _amin(a, axis=None, out=None, keepdims=False):  ---> 29     return umr_minimum(a, axis, None, out, keepdims)       30        31 def _sum(a, axis=None, dtype=None, out=None, keepdims=False):  ValueError: zero-size array to reduction operation minimum which has no identity"
"Hi @waleedka  : Thanks for the great work!   Is it possible to train for keypoint detection? Sorry for the wrong title of the issue, I can't correct it. "
None
None
"For my project, I need to label ocluded objects.  An example could be labelling all the pen parts  **as the same instance**  and labelling the hand from the following image:    !     I have tried to label the hand and the pen with masks that intersect each other but the out model was very poor.    So the questions are:    - Does this mrcnn implementation handle this kind of annotations and  is capable of giving such an ""split mask"" output?  - If so, How should I properly label this pen?    **NOTE:** There is a   but the answer is not very clear.    Thanks!  "
"I have been checking out PR   to make things compatible with TF/Keras 2.10.0 on my side. Things seems to work - kind of.     However, I am getting weird output in the `inspect_balloon_model.ipynb` and was wondering whether you got something similar.      seems to give a plausible output   !   ... while the cells underneath are total gibberish in terms of output.  !   ---  Similar with the line      which seems to give a good result   !   ... but the cells below are total gibberish   !     Any ideas what's going on?"
"I am a  Rookie who just start to learn mask R-CNN recently. After installing all the dependencies , I run the 'demo' code in Jupyter. The code proceed successfully without error, but there is no segmentation on the picture as expected. And the output of the result says ""*** No instances to display *** "". Could anyone help me with that? Thanks!  best wishes  LmZheng"
How can I let my model show bboxes that only prob over 60 haves?
"I'm using Tensorflow 1.13.1 and Keras 2.3.1, that's what worked and I got it working. However, I only get 5 fps.  I'm running the code using PyCharm and all dependencies listed. Should I try Conda or something else? How can I optimize performance using my GPU?  It's a GTX 1060 6GB with CUDA 11.7."
"Hello guys, I'm trying to re-train the Mask RCNN to fit into my dataset, but i stuck at the config and really confused about some parameters. **Please anyone that maybe able to help me distinguish the difference between :**    1. EPOCH  2. STEPS_PER_EPOCH  3. BATCH_SIZE                          Explanation with example like the numbers of the dataset and how many iteration that the models do between those parameters will be much appreciated. Thank you :)"
"I am training the model on 3 classes `car, truck and bus`. I extract the categories from the original coco dataset. when training the model all the loss are `nan`.     Here is my configuration class:   "
"hey!  so I'm running the training code on a Linux VM, and after using this version    to update mrcnn to support TensorFlow 2, and run it on GPU, it doesn't save all the weights after each iteration.  when running the code first with a small training set size (4 images) for 5 epochs, all the weights get saved except for epoch 4 for some reason.  when running it for 10 epochs using a larger training set size of 700 images, it only saves the weights for epochs 1 and 2 and then stops (sometimes saving the weights only for epochs 1 and 3).  did anybody experience this or know how to fix it?  thanks!"
"Hi everybody,  i know there are lot's of issues regarding this topic already (cause i read through them all in the last 2 painful days).  I managed to save my custom Mask_RCNN model in Keras (.h5) as seemingly working (.pb) and saved model format. However I failed when trying to load the model in openCV or when trying to convert it to onnx as a workaround.    Trying   led me to a .pb model file but the .pbtxt file seems to be corrupted and is not accepted as this error (also posted by @kaanaykutkabakci) appears:    `""cv2.error: OpenCV(4.1.2) C:\projects\opencv-python\opencv\modules\dnn\src\tensorflow\tf_io.cpp:54: error: (-2:Unspecified error) FAILED: ReadProtoFromTextFile(param_file, param). Failed to parse GraphDef file: ./IG/protobuf.pbtxt in function 'cv::dnn::ReadTFNetParamsFromTextFileOrDie'""`    Does anybody here has clue how to get a working .pbtxt ?  Or is there an actually working guide for getting my custom model running in openCV that i could follow?    Any help is very much appretiated, since I am running out of patience trying to solve this on my own :(          "
"Hello,    When using     for my test, I have some problems in just loading my training and validation dataset.    I provide my code here so you can better figure out what's wrong.    # Basic setup  >       import os      os.environ     **My computer setup:**  GPU: NVIDIA Quadro P620, 2GB  CUDA: 11.2  cuDNN: 8.1  tensorflow 2.8.0  Keras 2.7.0  windows 10    It only takes a few second when I run my code on Google Colab (Ubuntun 18.04, run perfectly), but it takes a long time on local machine with the error message `AttributeError: module 'tensorflow.compat.v2.__internal__.distribute' has no attribute  'strategy_supports_no_merge_call`, and both CPU and GPU are almost out of memory.     I wonder if there is something different between Ubuntun/Linux system and windows 10 that I haven't notice? I have previously tested tensorflow 1.x on windows 10, and similar problem occurs. This really troubles me for a long time. Could anyone please help me?    Thanks in advance for your kindness.    Best,  Erin"
"While training on my own dataset I get the following error. Can't find which point it is thrown. Could you help me?    Full output   `Using TensorFlow backend.  Traceback (most recent call last):    File ""cleavage.py"", line 187, in        if not os.path.exists(weights_path):  NameError: name 'weights_path' is not defined  >> python cleavage.py  Using TensorFlow backend.  Training network heads    Starting at epoch 0. LR=0.001    Checkpoint Path: //logdir//train/mask_rcnn_cleavage_{epoch:04d}.h5  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  In model:  rpn_model      rpn_conv_shared        (Conv2D)      rpn_class_raw          (Conv2D)      rpn_bbox_pred          (Conv2D)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)  /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    warnings.warn(  /usr/local/lib/python3.8/dist-packages/keras/engine/training_generator.py:48: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.    warnings.warn(  WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... cannot pickle '_thread.RLock' object  Epoch 1/30  Killed`    I can also share my code if it helps."
"When I was trying Mask R-CNN Demo, I got valueerror after running kernel below.    ----------------------------------------------------------------------------------------------------------  model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)  model.load_weights(COCO_MODEL_PATH, by_name=True)  ---------------------------------------------------------------------------------------------------------      Error I got is below.    -----------------------------------------------------------------------------------------------------------------    ValueError                                Traceback (most recent call last)  Input In [3], in  ()        1 # Create model object in inference mode.  ----> 2 model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)        4 # Load weights trained on MS-COCO        5 model.load_weights(COCO_MODEL_PATH, by_name=True)    File ~/Mask_RCNN/mrcnn/model.py:1838, in MaskRCNN.__init__(self, mode, config, model_dir)     1836 self.model_dir = model_dir     1837 self.set_log_dir()  -> 1838 self.keras_model = self.build(mode=mode, config=config)    File ~/Mask_RCNN/mrcnn/model.py:2044, in MaskRCNN.build(self, mode, config)     2035 mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\     2036     fpn_classifier_graph(rpn_rois, mrcnn_feature_maps, input_image_meta,     2037                          config.POOL_SIZE, config.NUM_CLASSES,     2038                          train_bn=config.TRAIN_BN,     2039                          fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE)     2041 # Detections     2042 # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in     2043 # normalized coordinates  -> 2044 detections = DetectionLayer(config, name=""mrcnn_detection"")(     2045     [rpn_rois, mrcnn_class, mrcnn_bbox, input_image_meta])     2047 # Create masks for detections     2048 detection_boxes = KL.Lambda(lambda x: x[..., :4])(detections)    File ~/.conda/envs/mask-rcnn/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py:951, in Layer.__call__(self, *args, **kwargs)      945 # Functional Model construction mode is invoked when `Layer`s are called on      946 # symbolic `KerasTensor`s, i.e.:      947 # >> inputs = tf.keras.Input(10)      948 # >> outputs = MyLayer()(inputs)  # Functional construction mode.      949 # >> model = tf.keras.Model(inputs, outputs)      950 if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):  --> 951   return self._functional_construction_call(inputs, args, kwargs,      952                                             input_list)      954 # Maintains info about the `Layer.call` stack.      955 call_context = base_layer_utils.call_context()    File ~/.conda/envs/mask-rcnn/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py:1090, in Layer._functional_construction_call(self, inputs, args, kwargs, input_list)     1086 if keras_tensor.keras_tensors_enabled():     1087   with call_context.enter(     1088       layer=self, inputs=inputs, build_graph=True, training=training_value):     1089     # Check input assumptions set after layer building, e.g. input shape.  -> 1090     outputs = self._keras_tensor_symbolic_call(     1091         inputs, input_masks, args, kwargs)     1093     if outputs is None:     1094       raise ValueError('A layer\'s `call` method should return a '     1095                        'Tensor or a list of Tensors, not None '     1096                        '(layer: ' + self.name + ').')    File ~/.conda/envs/mask-rcnn/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py:822, in Layer._keras_tensor_symbolic_call(self, inputs, input_masks, args, kwargs)      820   return nest.map_structure(keras_tensor.KerasTensor, output_signature)      821 else:  --> 822   return self._infer_output_signature(inputs, args, kwargs, input_masks)    File ~/.conda/envs/mask-rcnn/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py:863, in Layer._infer_output_signature(self, inputs, args, kwargs, input_masks)      857   with autocast_variable.enable_auto_cast_variables(      858       self._compute_dtype_object):      859     # Build layer if applicable (if the `build` method has been      860     # overridden).      861     # TODO(kaftan): do we maybe_build here, or have we already done it?      862     self._maybe_build(inputs)  --> 863     outputs = call_fn(inputs, *args, **kwargs)      865   self._handle_activity_regularization(inputs, outputs)      866 self._set_mask_metadata(inputs, outputs, input_masks,      867                         build_graph=False)    File ~/.conda/envs/mask-rcnn/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:670, in convert. .decorator. .wrapper(*args, **kwargs)      668 except Exception as e:  # pylint:disable=broad-except      669   if hasattr(e, 'ag_error_metadata'):  --> 670     raise e.ag_error_metadata.to_exception(e)      671   else:      672     raise    ValueError: in user code:        /home/nagasaka/Mask_RCNN/mrcnn/model.py:810 call  *          detections_batch = utils.batch_slice(      /home/nagasaka/Mask_RCNN/mrcnn/utils.py:821 batch_slice  *          output_slice = graph_fn(*inputs_slice)      /tmp/tmpi9t6nidi.py:17    **          detections_batch = ag__.converted_call(ag__.ld(utils).batch_slice, ([ag__.ld(rois), ag__.ld(mrcnn_class), ag__.ld(mrcnn_bbox), ag__.ld(window)], ag__.autograph_artifact((lambda x, y, w, z: ag__.converted_call(ag__.ld(refine_detections_graph), (ag__.ld(x), ag__.ld(y), ag__.ld(w), ag__.ld(z), ag__.ld(self).config), None, fscope))), ag__.ld(self).config.IMAGES_PER_GPU), None, fscope)      /home/nagasaka/Mask_RCNN/mrcnn/model.py:702 refine_detections_graph  **          indices = tf.stack([tf.range(probs.shape[0]), class_ids], axis=1)      /home/nagasaka/.conda/envs/mask-rcnn/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:201 wrapper          return target(*args, **kwargs)      /home/nagasaka/.conda/envs/mask-rcnn/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1856 range          limit = ops.convert_to_tensor(limit, dtype=dtype, name=""limit"")      /home/nagasaka/.conda/envs/mask-rcnn/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py:163 wrapped          return func(*args, **kwargs)      /home/nagasaka/.conda/envs/mask-rcnn/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1540 convert_to_tensor          ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)      /home/nagasaka/.conda/envs/mask-rcnn/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:339 _constant_tensor_conversion_function          return constant(v, dtype=dtype, name=name)      /home/nagasaka/.conda/envs/mask-rcnn/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:264 constant          return _constant_impl(value, dtype, shape, name, verify_shape=False,      /home/nagasaka/.conda/envs/mask-rcnn/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:281 _constant_impl          tensor_util.make_tensor_proto(      /home/nagasaka/.conda/envs/mask-rcnn/lib/python3.9/site-packages/tensorflow/python/framework/tensor_util.py:445 make_tensor_proto          raise ValueError(""None values not supported."")        ValueError: None values not supported.    ---------------------------------------------------------------------------------------------------------------------------------------    Since I am new to mask r-cnn, could you please help me to solve the problem?"
My question is when evaluate iou mask for evaluate AP with different  therthold.   The predict mask is 28x28 to evaluate AP or resize to original size of roi  after that compute AP
I am confused in training part using 28x28 for loss function. In the evaluation or testing part to compute AP used the 28x28 after that up sampling with therthold 0.5  or using full image?? Such as 1024x1024 
"I am trying to run the cells in `inspect_baloon_data.ipynb`  for the Mini Masks cells for the following code I noticed that my masks and images does not match after using `load_image_gt` function. I guess this may be the reason I get  `ValueError: operands could not be broadcast together with shapes (56,56) (1024,1024) (1024,1024) ` error when I ran `visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names)` line. My question is how should I resolve this?     <img width=""796"" alt=""Screen Shot 2022-04-27 at 17 00 27"" src=""   `"
"This is a wonderful repository! I am trying to apply Tensorflow model optimization(pruning and quantization), I was able to do that. But stuck at training.  I have really hard time get this network trained on TF versions > 2.0  The training gets stuck in the first epoch as mentioned by a lot of people in many of the issues raised here.    As mentioned by one of the comments, I am using   repository converted for TF2. Everything works until training.    One solution was to ensure if I have enabled GPU, and yes it is enabled.       Below are settings maintained in `requirements.txt`     Please help"
"Hi all,    I have trained a custom instance segmentation model and it is working fine. Now I want my inference results to be in coco JSON format. How can I perform this operation?"
"I'm trying to train a custom COCO-format dataset. My datasets are json files with the aforementioned COCO-format, with each item in the ""annotations"" section looking like this:    [![Annotations][1]][1]    There are 20 classes, with polygon masks for the entire object, and then polygon masks for the parts within the object. The images are of shape 256x448x3.    The code for the dataset preparation is as follows:         This seems to work well at least in loading the images & masks in a presentable format, as testing it out to visualize the images, masks, and class ids for some of the images yields good results where I can see each image, binary mask, and class ids of each mask.     However, when I actually try to train the model on the training dataset created, I get the following error:         I have no clue what this error is actually indicating, nor what the potential solution might possibly be. I have a feeling it might have to do with how the data is being formatted & handled by the CocoLikeDataset class, but I'm not sure.    Any help with this identifying the issue and solving it is appreciated!    Thanks!        [1]:  "
"The dataset is in segmented masks in png format as shown below structure, any suggestion on how to train it. The dataset structure is :    Object_Images           -> img1.png           -> img2.png           . . .   Object_Segmented_Masks           ->img1_seg_mask.png           ->img2_seg_mask.png          . . .     There is no annotations data in usual 'json' format. Any suggestion?"
"Hello people. I have executed my code. Unfortunately I noticed that no images are loaded. So my question to you is, where do I specify which images are loaded? Or where do I load the images.  !     "
None
"I trained certain models for detecting objects of 3 different classes and chose the model with the highest mAP. However, it seems that the masks are not correctly displayed. Is there any solution?  !     "
"Hello there!  I've trained MRCNN on my custom dataset of dental X-rays. I've not made any changes to the model architecture. Just trained the model for 100 epochs and save the weights, in "".h5"" format, after each epoch and using its own ""save_weights"" method. But whenever I try to load the weights again for further training, using ""load_weights"" method, I get the error shown below.   Please help me to solve this issue.    **ValueError: Layer #362 (named ""anchors""), weight   has shape (2, 261888, 4), but the saved weight has shape (1, 261888, 4).**"
"i'm training a model to recognize hands and want to extract the segmentation masks after detection.  Here is an example detection:    <img width=""878"" alt=""Screenshot 2022-01-11 at 14 38 15"" src=""     After detection, I reshape the masks boolean array  so I can access each segmentation mask individually (masks     Whilst this is the shape of the segmentation mask, and the dimensions are the same as the original image, the output image is not the segmentation as it appears in the original image. I am looking for the extracted masks to be output as they are overlayed on the original image, and not 'zoomed-in' as they are currently. I assumed because the masks array held the same dimensions of the original image that the masks would retain their position, but apparently not. How can I output the segmentation masks as they appear in the original image?    cheers"
"Hello,    I am trying to train Mask R-CNN with my own dataset (images .tif, annotations in shapefile) by referring to codes available in this repo. While checking if the masks are correctly loaded or not with the code below I can see only 2 instances for each image, even though there are multiple instances and I can see the properties x, y with train_dataset.image_info       But visualizing the image only 2 instances is seen, even though there are around 12.    !       The code for my load_mask is as follows :          The code I am using to look at random example is          Is there anything I should change in load_mask or visualize.py to visualize /load all the masks for each image?   Any help would be greatly appreciated. Thankyou!  "
"I ma using below code for my project. But I cannot solve this error:    ERROR:root:Error processing image ......  IndexError: boolean index did not match indexed array along dimension 0; dimension is 0 but corresponding boolean dimension is 54    ##################################################  class CustomConfig(Config):      """"""Configuration for training on the custom dataset.      Derives from the base Config class and overrides some values.      """"""      # Give the configuration a recognizable name      NAME = ""custom""        # We use a GPU with 12GB memory, which can fit two images.      # Adjust down if you use a smaller GPU.      IMAGES_PER_GPU = 1        # Number of classes (including background)      NUM_CLASSES = 1 + 4  # Background + number of classes (Here, 4)        # Number of training steps per epoch      STEPS_PER_EPOCH = 100        # Skip detections with   mask.shape[0]-1] = mask.shape[0]-1              cc[cc > mask.shape[1]-1] = mask.shape[1]-1                  mask[rr, cc, i] = 1            # Return mask, and array of class IDs of each instance. Since we have          # one class ID only, we return an array of 1s          num_ids = np.array(num_ids, dtype=np.int32)           return mask.astype(np.bool), num_ids#.astype(np.bool), np.ones([mask.shape[-1]], dtype=np.int32),          #return mask.astype(np.bool), np.ones([mask.shape[-1]], dtype=np.int32)  # --> works but only detect 1 class        whenever I use ""return mask.astype(np.bool), np.ones([mask.shape[-1]], dtype=np.int32)"" the code works fine but it cannot recognise the name of other classes and it only returns the name of class 1 for any objects.  And in case of using this:          I receive an error:        Any help is appreciated.    "
None
"Hi all,     I am encountering this error but wasn't sure how to resolve it. Any help or suggestions would be appreciated.     # CODE       # Error  !     I have tried  1. changing `workers=workers` to` workers = 1` or `workers = 0`  2.  commenting out `use_multiprcoessing = False`  3. changing `use_multiprocessing = True `  4. donwgrading keras to version 2.0.8  but none of it is making any difference to the error.     "
"  TypeError: zip argument #1 must support iteration  ERROR:root:Error processing image {'id': 92429, 'source': 'category', 'path': '/content/drive/MyDrive/data/train/92429.jpg', 'width': 2048, 'height': 2048, 'polygons': [[1314.81, 1331.95, 1351.5, 1355.09, 1274.49], [837.512, 820.695, 821.326, 775.959, 788.965]]}  Traceback (most recent call last):    File ""/content/drive/My Drive/Mask_RCNN/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/content/drive/My Drive/Mask_RCNN/mrcnn/model.py"", line 1212, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File "" "", line 99, in load_mask      all_x, all_y = zip(*p) #unzip the list      This is the error I'm stuck at while training the Mask RCNN model on a custom dataset.  "
None
"I have ran into an error while running this code below from . Any help would be appreciated    # Code  %Create model object in inference mode.  model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)  % Load weights trained on MS-COCO  model.load_weights(COCO_MODEL_PATH, by_name=True)    # Error  ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.    I have tried this method but I'm still getting the error     !   "
"Note: I am running this locally on Jupyter notebook, not on Google Colab.    """""""""""""""""""""""""""" CODE """"""""""""""""""""""""""""""""""""""""  sys.path.append(ROOT_DIR)  # To find local version of the library  from mrcnn.config import Config  from mrcnn import utils  *import mrcnn.model as modellib*  from mrcnn import visualize  from mrcnn.model import log  """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""    """""""""""""""""""" ERROR""""""""""""""""""""""""""""""""""""""""""""  ImportError                               Traceback (most recent call last)  ~\AppData\Local\Temp/ipykernel_17240/1810757526.py in         17 from mrcnn.config import Config       18 from mrcnn import utils  ---> 19 import mrcnn.model as modellib       20 from mrcnn import visualize       21 from mrcnn.model import log    ~\Anaconda3\lib\site-packages\mrcnn\model.py in         22 import keras.backend as K       23 import keras.layers as KL  ---> 24 from keras.layers import Layers as KE       25 #import keras.engine.topology as KE       26 import keras.models as KM    ImportError: cannot import name 'Layers' from 'keras.layers' (C:\Users\a1737542\Anaconda3\lib\site-packages\keras\layers\__init__.py)  """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""  I know this issue has been brought up several time across the internet, but none of the solutions provided so far is working for me.  I have tried  1. Changing 'import keras.engine as KE' to 'import keras.engine.topology as KE'  2. !pip install tensorflow==1.13.1      !pip install keras==2.0.8      !pip install h5py==2.10.0  Getting this error : ERROR: Could not find a version that satisfies the requirement tensorflow==1.13.1 (from versions: 2.5.0rc0, 2.5.0rc1, 2.5.0rc2, 2.5.0rc3, 2.5.0, 2.5.1, 2.5.2, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.7.0rc0, 2.7.0rc1, 2.7.0)  ERROR: No matching distribution found for tensorflow==1.13.1  I am having a tensorflow version of 2.7.0 now. "
"I have a custom dataset and doesn't have Nvidia Graph Card, I want to use Intel Graph Card for training or inferencing."
"After I trained the model for text recognition, it was confusing to recognize characters between 6 and 9.     Could anyone tell me the reason?"
"I'm going to learn the sapcenet and detect the building.  However, learning is no longer progressing in Epoch 1. (GPU is 100% working.)  Print out the phrase below and stop at Epoch 1/1.  'Converting sparse IndexedSlices to a dense Tensor of unknown shape.'    There are only two classes I need: Background and Building.  I wrote layers='head' but it doesn't work out.    I need the help of many people because I am a beginner at detecting objects for the first time.    Please help me.    The code I executed is as follows. (Attach the entire code as .txt.) Please convert to ipynd and take a look.):    model = modellib.MaskRCNN(mode=""training"", config=config,                            model_dir=MODEL_DIR)  init_with = ""imagenet""  # imagenet, coco, or last    if init_with == ""imagenet"":      model.load_weights(model.get_imagenet_weights(), by_name=True)  elif init_with == ""coco"":      # Load weights trained on MS COCO, but skip layers that      # are different due to the different number of classes      # See README for instructions to download the COCO weights      model.load_weights(COCO_MODEL_PATH, by_name=True,                         exclude=     For your information, my working environment is as follows:  window 10  intel i5 10400f  gtx3060ti  32g ram  tensorflow-gpu 1.13.1  keras 2.1.6"
I was trying to remove the background and only include the the segmented object in my image but however only 1 image worked but the rest showing me this error. Any one has faced this issue?   !   
"I had been using the code in Google Colab to train a model using the pretrained one based on my own data. In February the training and detection was working fine. However, the exact same code results in an error when training a new model, and detection fails when trying to use prevoiusly trained models from February.   Instead of finding one or two rois on a test image as desired, the program detects about one hundred. When visualized, the rois are not even remotely close to anything that would make sense.  The last working code used these versions:   python ==3.6.4  tensorflow==2.0   tensorflow-gpu==2.0  Since then my colleague managed to get the model training part working by modifying the keras core.py and the mrcnn model.py files, but the detection still fails using models trained this way.   I suspect the problem is caused due to Colab using newer versions eg. tensorflow 2.6.0, but I am not sure whether this is the case or how it could be managed.   "
"I am getting this error message when loading weights from custom model I trained even though I used the same config where both configs use        IMAGE_MIN_DIM = 256      IMAGE_MAX_DIM = 256    I also already tried using how COCO weights are loaded but still not working        >   model.load_weights(os.path.join(MODEL_DIR,modelh5_path),exclude=[""mrcnn_class_logits"", ""mrcnn_bbox_fc"",   >                                 ""mrcnn_bbox"", ""mrcnn_mask""])    (Pretrained weights using    Detail on error message    ```    >   File ""D:\Anaconda3\envs\mask_rcnn\lib\site-packages\tensorflow\python\keras\engine\training_generator.py"", line 256, in model_iteration  >     batch_outs = batch_function(*batch_data)  >   >   File ""D:\Anaconda3\envs\mask_rcnn\lib\site-packages\tensorflow\python\keras\engine\training_v1.py"", line 1038, in train_on_batch  >     extract_tensors_from_dataset=True)  >   >   File ""D:\Anaconda3\envs\mask_rcnn\lib\site-packages\tensorflow\python\keras\engine\training_v1.py"", line 2308, in _standardize_user_data  >     batch_size=batch_size)  >   >   File ""D:\Anaconda3\envs\mask_rcnn\lib\site-packages\tensorflow\python\keras\engine\training_v1.py"", line 2335, in _standardize_tensors  >     exception_prefix='input')  >   >   File ""D:\Anaconda3\envs\mask_rcnn\lib\site-packages\tensorflow\python\keras\engine\training_utils.py"", line 582, in standardize_input_data  >     str(data_shape))  >   > ValueError: Error when checking input: expected input_image to have shape (512, 512, 3) but got array with shape (256, 256, 3)  >   >     Any suggestions?"
"Hi, quick question: how do you deactivate random crops for prediction?     I am following the ""nucleus"" tutorial      I used the same config for training on my data. But for prediction, I would like to detect on the whole image. I tried editing the config file with `IMAGE_RESIZE_MODE = ""none""` , but I get the error:  `InvalidArgumentError (see above for traceback): Incompatible shapes: [1,256,56,64] vs. [1,256,56,63]`    Any tips ?  "
"I have trained Mask_RCNN on my custom dataset using Tensorflow 2.3 but i'm not able to inference image.    # Test on a random image  image_id = random.choice(dataset_val.image_ids)  original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\      modellib.load_image_gt(dataset_val, inference_config,                              image_id)    log(""original_image"", original_image)  log(""image_meta"", image_meta)  log(""gt_class_id"", gt_class_id)  log(""gt_bbox"", gt_bbox)  log(""gt_mask"", gt_mask)    visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id,                               dataset_train.class_names, figsize=(8, 8))     **ERROR**    original_image           shape: (128, 128, 3)         min:   38.00000  max:  246.00000  uint8  image_meta               shape: (14,)                 min:    0.00000  max: 1590.00000  int32  gt_class_id              shape: (29,)                 min:    1.00000  max:    1.00000  int32  gt_bbox                  shape: (29, 4)               min:    0.00000  max:  128.00000  int32  gt_mask                  shape: (56, 56, 29)          min:    0.00000  max:    1.00000  bool  ---------------------------------------------------------------------------  ValueError                                Traceback (most recent call last)    in         12        13 visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id,   ---> 14                             dataset_train.class_names, figsize=(8, 8))    ~\Documents\boundary detection\Mask_RCNN\mrcnn\visualize.py in display_instances(image, boxes, masks, class_ids, class_names, scores, title, figsize, ax, show_mask, show_bbox, colors, captions)      150         mask = masks[:, :, i]      151         if show_mask:  --> 152             masked_image = apply_mask(masked_image, mask, color)      153       154         # Mask Polygon    ~\Documents\boundary detection\Mask_RCNN\mrcnn\visualize.py in apply_mask(image, mask, color, alpha)       77                                   image[:, :, c] *       78                                   (1 - alpha) + alpha * color[c] * 255,  ---> 79                                   image[:, :, c])       80     return image       81       in where(*args, **kwargs)    ValueError: operands could not be broadcast together with shapes (56,56) (128,128) (128,128)    "
the model isn't compatible with later versions of tensorflow and keras
"   The above code segment is extracted from the utils.py file inside the Mask-RCNN repository and, I am confused about the fact that even though the function description illustrates that it's calculating the AP(Average Precision ) at the end it returns mAP, precisions, recalls, overlaps. But I have noticed that in many source codes authors have implemented additional codes on their own to calculate the mAP by considering this returned mAp value as AP(Average Precision). What exactly does this function return? Is there anyone out there who can give a clear code segment to calculate the mAP, mAR, accuracy, and F1 score?"
     isn't this logically wrong here?  From what I know of the Mask RCNN architecture we should have anchors per pixel as len(scales)*len(ratios). Then why during making the rpn graph do we use only the three ratios and not the product of the number of scales and the number of ratios?
"Hey, I am stuck in a problem. Need to rotate images as well as their annotations for MRCNN Training.  I tried a code but it's not working right will you please help me out. You can just copy-paste the code and get the results no need to edit.          "
"Hello everyone,  the past couple of days I spent trying to get tflite running with select tf ops on my raspberry pi. How I did that you can find here:     Now it works, great.  Then I thought, there is so much logic going on behind this great opensource project, and the instance segmentation masks are not going to appear by themselves as they do when running on regular tensorflow.  I hope it is nothing that cannot be fixed. I counted 3 input tensors and 7 output tensors of which I do not have the slightest clue of what they are doing except for maybe the 512x512 input tensor for the image.    Btw, the conversion went fine, no issue there. I am only concerned how to use the outputs to gain useful information from the model.  Also, this is not the matterport version, but the Leekunhee (  I used that one because at the beginning of learning about mask r cnn I though that the leekunhee was the only one maintained to tensorflow 2.x. By now I think they are very similar, because it is actually forked from here.  Currently I am using tensorflow lite 2.5 on a 64 bit raspberry pi 4 (1GB of ram), with python 3.7.3.    Source code:         And here is the ouput:    > Before loading the interpreter  > INFO: Created TensorFlow Lite XNNPACK delegate for CPU.  > INFO: Created TensorFlow Lite delegate for select TF ops.  > INFO: TfLiteFlexDelegate delegate: 13 nodes delegated out of 480 nodes with 6 partitions.  >   > INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.  >   > INFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 18 nodes with 2 partitions.  >   > Afterloading interpreter  > Beyond input details  > Mask R CNN input shape [  1 512 512   3]  > Mask R CNN input shape [ 1 14]  > Mask R CNN input shape [1 1 4]  > Mask R CNN output shape [  1 100   6]  > Mask R CNN output shape [  1 500   2]  > Mask R CNN output shape [  1 500   2   4]  > Mask R CNN output shape [  1 100  28  28   2]  > Mask R CNN output shape [1 1 1]  > Mask R CNN output shape [1 1 2]  > Mask R CNN output shape [1 1 4]  > Before invoke  > 2021-08-25 17:58:07.981717: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 25088000 exceeds 10% of free system memory.  > After invoke    Yep, no errors. A reason to feel good? Not yet, I really need to figure out how to use this model now. Please help me with ideas on how to format inputs and use outputs so that one actually has masks for each instances in the end."
"Hi I'm unable to run training session. I'm using tensor flow 2.3 and modified model.py, utils.py and other files from some available accordingly. Any help would be highly appreciated    ''  ''  ''  OSError                                   Traceback (most recent call last)    in         10     model.load_weights('Users/20013819/Documents/boundary detection/Mask_RCNN/mask_rcnn_coco.h5', by_name=True,       11                        exclude=[""mrcnn_class_logits"", ""mrcnn_bbox_fc"",   ---> 12                                 ""mrcnn_bbox"", ""mrcnn_mask""])       13 elif init_with == ""last"":       14     # Load the last model you trained and continue training    ~\Documents\boundary detection\Mask_RCNN\mrcnn\model.py in load_weights(self, filepath, by_name, exclude)     2113         if h5py is None:     2114             raise ImportError('`load_weights` requires h5py.')  -> 2115         with h5py.File(filepath, mode='r') as f:     2116             if 'layer_names' not in f.attrs and 'model_weights' in f:     2117                 f = f['model_weights']    c:\users\20013819\anaconda3\envs\tensorflow\lib\site-packages\h5py\_hl\files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)      406                 fid = make_fid(name, mode, userblock_size,      407                                fapl, fcpl=make_fcpl(track_order=track_order),  --> 408                                swmr=swmr)      409       410             if isinstance(libver, tuple):    c:\users\20013819\anaconda3\envs\tensorflow\lib\site-packages\h5py\_hl\files.py in make_fid(name, mode, userblock_size, fapl, fcpl, swmr)      171         if swmr and swmr_support:      172             flags |= h5f.ACC_SWMR_READ  --> 173         fid = h5f.open(name, flags, fapl=fapl)      174     elif mode == 'r+':      175         fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)    h5py\_objects.pyx in h5py._objects.with_phil.wrapper()    h5py\_objects.pyx in h5py._objects.with_phil.wrapper()    h5py\h5f.pyx in h5py.h5f.open()    OSError: Unable to open file (unable to open file: name = 'Users/20013819/Documents/boundary detection/Mask_RCNN/mask_rcnn_coco.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)   '' '' ''     "
Can any one help me with this error??
"Hi All,     I want to train mask_rcnn on my custom dataset for 1 class with coco annotation format so i was trying to edit coco.py config according to my dataset but ended up getting up errors. I have also looked at balloon sample for 1 class but that is not using coco format.    If anyone come across such scenarios please help.  Regards,  Chhigan Sharma    #""""ANNOTTATIONS""    ""categories"": [          {              ""supercategory"": ""AgriculturalFields"",              ""id"": 1,              ""name"": ""agfields_singleclass""          }      ],      ""images"": [          {              ""file_name"": ""COCO_val2016_000000100004.jpg"",              ""id"": 100004,              ""height"": 128,              ""width"": 128          },   ............................................................continue    ""annotations"": [          {              ""id"": 1,              ""image_id"": 100004,              ""category_id"": 1,              ""mycategory_name"": ""agfields_singleclass"",              ""old_multiclass_category_name"": ""springcereal"",              ""old_multiclass_category_id"": 1,              ""bbox"": [                  8.8,                  110.51,                  13.74,                  17.49              ],              ""area"": 89.76729724821337,              ""iscrowd"": 0,              ""segmentation"": [                  [                      8.8,                      128.0,                      20.3,                      110.51,                      22.54,                      111.97,                      19.9,                      116.38,                      22.3,                      117.25,                      15.1,                      128.0,                      8.8,                      128.0                  ]              ]          },    # sample annotations of my dataset"
None
"Hi,    I have an application running with mask rcnn model, due to slow speed of mask rcnn I have switched to object detection model i.e yolov4 model. Now the application flow is setup for masks as well, I am trying to convert the bounding boxes points into mask format i.e exactly same as mask rcnn format. I am using opencv drawcontours but there are some issues not able to create the exact same format of mask as mask rcnn outputs.    Here is the sample code:         Please help me figured out the issue with code snippet above.   "
"Hi all,  I understand that the compute_ap function calculates the AP metric.    My model has 5 classes + background.    When I generate the precision-recall curve several times, the AP values are different.    Can someone explain to me? Thank you for your help.  !   "
"the original repo can run on cpu if we use tensorflo==1.3.0, but the speed is slow  if we want to get about x10 faster training speed, we need to use tensorflow-gpu    the environment i used:   "
"I trained a custom model to detect guitars. I scraped and labelled the training and validation sets myself, ending with *98* training images ad *10* validation images.    Now I'm pretty noobish when it comes to computer vision and ML, but could this just be a case of not enough training images?     The result is literally nothing gets detected.    I am using the following versions:    - Tensorflow: 1.15  - Keras: 2.3.1  - H5py: 2.10.0  - scikit-image: 0.16.2    Here's the code for the training script:     `    And here's the code I'm using to test the model after it's trained:         I call special attention to the call to `load_weights`, where I had to add a bunch of `exclusions` because it kept erroring out with errors about layers being the wrong shape (?).    I'd really appreciate some insight into what might be happening here. Thank you!  "
"Hello!    I've been trying to get the RCNN to detect mosquitoes, which I've been able to do successfully but it also classifies chairs, vases, and tables as a mosquito as well. I think these are from the base 80 objects trained.    I've been trying to remove these other objects with no success.    I've tried to re-add the base classes back and set ""mosquito"" as a new class. But I keep getting errors with the weight shape.    Is there anyway to separate any objects detected in the background from the mosquito?     Any help at all would be greatly appreciated. Thanks!    class TestConfig(Config):   NAME = ""test""   GPU_COUNT = 1   IMAGES_PER_GPU = 1   NUM_CLASSES = 1 + 80 + 1    `ValueError: Layer #391 (named ""mrcnn_bbox_fc""), weight   has shape (1024, 324), but the saved weight has shape (1024, 8).  `    <img width=""551"" alt=""Capture"" src=""   <img width=""775"" alt=""Capture2"" src=""   "
"Hi guys, I have got an error occurred while training the model       "
Hi all  I would like to manipulate function below in a way that returns masked image instead of displaying it     could you please give me a hint?
"I want no layer to be frozen during training, so all weights could be updated by training on my custom data.  I set `TRAIN_BN = True` in the `Config` class in `config.py`.  Is it correct? Will this help me achieve what I want?"
How can I set the confidence threshold for the inference module?  I am using the `detect` function in the `model` module and want to set a lower confidence. Where can I do that?    
"Right now, model checkpoint at each epoch is saved. Can I change this to N epochs, say save model after every 20 epochs and then the last one?"
"I keep getting different error relating to `Keras` and `Tensorflow` most likely due to version compatibility issues.   Since I was getting this error: `TypeError: Could not build a TypeSpec for   out.txt` with versions and dependencies that are needed for proper training, that'd be great!"
Hi all  I have downloaded the .h5 format of model in order to use it in tensorflow.  but when i want to load it the error bellow occurs:    Sample code:       and error:    !   
"So I was trying to save the augmented images and masks on them but I ran through some error when I tried to save masks, As mask gives a boolean array so I tried to apply it on the image like this   !       But then I face type error  `TypeError:can't multiply sequence of non-int type float`  Then I replaced 0.5*color[c]*255 with color[c]*255  But then I faced this error  `operands could not be broadcast together shapes (1024,1024) (765,)`"
Everything works until I reach the training of the head branches and then fine tuning all the layers    This is the output of training the head branches             Below is my class for using my dataset.          Would appreciate any leads! I did make changes with the load_mask() part as I was inspired by Issue #2154    Edit because I copied the class wrong
"Hello, I have a problem loading new images for detection. I have already trained my model with my own dataset. Now I want to use this trained model to make predictions on new images (not annotated), but I can't use the load_image function to load images that I have in another folder different from the Mask-rcnn directory. Could someone please help me?"
"I learned custom data with the ""balloon"" sample. There are five classes. For example     ""AAA""  ""BBB""  ""CCC""  ""DDD""  ""EEE""     As a result of learning, all images will be recognized as class ""AAA"".    Why. I don't know why.    I used the annotation software ""VGG Image Annotator (VIA)"" when learning.  I set ""name"" in ""Attributes"" and entered ""AAA"", ""BBB"", etc. in it.  The saved file is ""AS JSON"" instead of ""COCO FORMAT"".  In the source, it is defined as follows.       # code  self.add_class(""balloon"", 1, ""AAA"")  self.add_class(""balloon"", 2, ""BBB"")  self.add_class(""balloon"", 3, ""CCC"")  self.add_class(""balloon"", 4, ""DDD"")  self.add_class(""balloon"", 5, ""EEE"")    please tell me. Thank you.     "
"I have the annotations in the below format , I am getting some errors . How to define the load mask and my json looks like :      {""_via_settings"":{""ui"":{""annotation_editor_height"":25,""annotation_editor_fontsize"":0.8,""leftsidebar_width"":18,""image_grid"":{""img_height"":80,""rshape_fill"":""none"",""rshape_fill_opacity"":0.3,""rshape_stroke"":""yellow"",""rshape_stroke_width"":2,""show_region_shape"":true,""show_image_policy"":""all""},""image"":{""region_label"":""__via_region_id__"",""region_color"":""__via_default_region_color__"",""region_label_font"":""10px Sans"",""on_image_annotation_editor_placement"":""NEAR_REGION""}},""core"":{""buffer_size"":18,""filepath"":{},""default_filepath"":""""},""project"":{""name"":""Validation_Annotations_New""}},""_via_img_metadata"":{""19Apple.jpg4222"":{""filename"":""19Apple.jpg"",""size"":4222,""regions"":[{""shape_attributes"":{""name"":""rect"",""x"":31,""y"":43,""width"":162,""height"":154},""region_attributes"":{""class"":{""apple"":true}}}],""file_attributes"":{}},""19_banana.jpg4849"":{""filename"":""19_banana.jpg"",""size"":4849,""regions"":[{""shape_attributes"":{""name"":""rect"",""x"":27,""y"":31,""width"":207,""height"":125},""region_attributes"":{""class"":{""banana"":true}}}],""file_attributes"":{}},""20Apple.jpg3368"":{""filename"":""20Apple.jpg"",""size"":3368,""regions"":[{""shape_attributes"":{""name"":""rect"",""x"":91,""y"":38,""width"":116,""height"":130},""region_attributes"":{""class"":{""apple"":true}}}],""file_attributes"":{}},""21Apple.jpg36817"":{""filename"":""21Apple.jpg"",""size"":36817,""regions"":[{""shape_attributes"":{""name"":""rect"",""x"":113,""y"":62,""width"":177,""height"":177},""region_attributes"":{""class"":{""apple"":true}}}],""file_attributes"":{}}},""_via_attributes"":{""region"":{""class"":{""type"":""checkbox"",""description"":"""",""options"":{""apple"":"""",""banana"":"""",""\""\"""":""""},""default_options"":{}}},""file"":{}}}  "
"I have my own dataset with 1 class (+ background) with 100 images in train and 16 in validation dataset. I trained (based on coco weights) 8 epochs (1000 steps) on heads, 3 epochs for 4+ and 4 epochs for all layers. I can see on tensorboard that the validation loss went down like exp(-x) - as expected.     Now I looked at my model using inspect_model.ipynb and unfortunately my results don't look good at all. I did see though that the RPN predictions looked perfect, for N=5 my object was always detected by all bounding boxes.     Do I have to train for more epochs? Any other hints on why it didn't work well?     I wasn't sure how to set my steps, I have 2 images per gpu (=batch size) and 100 training images, does it make sense to set steps to 50 then? I tried it with 50 steps and epochs 10 (stage 1), 5 (stage 2) and 55 (stage 3), but same outcome as above. "
Hello    I'm trying to train the mask RCNN on my own dataset and I would like to use mAP as a metric. Does anyone know how to use it so that it will print the results during training (and print graphs in tensorboard if possible)?    Thank you
"Hello everyone,    I'm trying to train the matterport Mask RCNN on a custom dataset with only 2 classes (3 if you count the background). However, any configuration I'm trying I don't seem to be able to get the rpn_bbox_loss lower than the value 2 (which I suppose has no real significance). In addition to this, the mrcnn_class_loss and mrcnn_mask_loss are always 0 (starting from epoch 1) and the mrcnn_bbox_loss is really low then gets to 0. The rpn_class_loss is the only one behaving normally (starting high then getting low).    My training images have the shapesroughly 200x200x3.  Should I change the IMAGE_MIN_DIM and IMAGE_MAX_DIM to 256 (this is how I've been trying to train with)?  If yes, what should my RPN_ANCHOR_SCALES be? They currently are (8, 16, 32, 64, 128)  Should I change any other config parameters for these shapes to work? I'm fairly new to training the mask rcnn so any help is more than welcome.  This is my full config:  Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES                  !   !   !     When doing inference on the training images (just to prove that it doesn't work at all) the bounding boxes are totally messed up and so are the classes. You can also see that the mask is not consistent along the bbox, as it is somewhat ""interrupted"".  !       Thanks everyone!"
"I run the demo and threw an error  Keras: 2.4.3   Tensorflow: 2.5.0    If Tensorflow  drops down to version 2.1, will encounter #2574 problems                "
"imgaug's ""Rot90"" is a useful conversion, especially for square images. However, in the current code, this conversion seems to be applied only to images, not to masks.    I suppose this can be solved by changing lines 1238-1240 of model.py as follows.  MASK_AUGMENTERS = [""Sequential"", ""SomeOf"", ""OneOf"", ""Sometimes"", ""Fliplr"", ""Flipud"", ""CropAndPad"", ""Affine"", ""PiecewiseAffine"", **""Rot90""**]    Is this correct?"
"--------------------------------------------------------------------------  TypeError                                 Traceback (most recent call last)    in  ()  ----> 1 clf.fit(x = (X_left, X_center, X_right), y = to_categorical(y_train, len(y_values)+1), verbose=1, epochs = 15,)    9 frames  /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)      975           except Exception as e:  # pylint:disable=broad-except      976             if hasattr(e, ""ag_error_metadata""):  --> 977               raise e.ag_error_metadata.to_exception(e)      978             else:      979               raise    TypeError: in user code:        /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *          return step_function(self, iterator)      /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **          outputs = model.distribute_strategy.run(run_step, args=(data,))      /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run          return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)      /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica          return self._call_for_each_replica(fn, args, kwargs)      /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica          return fn(*args, **kwargs)      /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **          outputs = model.train_step(data)      /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:754 train_step          y_pred = self(x, training=True)      /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1012 __call__          outputs = call_fn(inputs, *args, **kwargs)      /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:389 call          outputs = layer(inputs, **kwargs)      /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1008 __call__          self._maybe_build(inputs)      /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:2710 _maybe_build          self.build(input_shapes)  # pylint:disable=not-callable      /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py:272 wrapper          output_shape = fn(instance, input_shape)      /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/merge.py:500 build          del reduced_inputs_shapes[i][self.axis]        TypeError: list indices must be integers or slices, not ListWrapper"
"I meet this problem when training my model on the GPU of HPC(SLURM), but when I train on the CPU, it is ok.   Is there a superior coder who can help me, please?      WARNING:tensorflow:From /home/jsxian2/anaconda3/envs/pytensorflow/lib/python3.7/site-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.    Epoch 1/30  Exception in thread Thread-3:  Traceback (most recent call last):    File ""/home/jsxian2/anaconda3/envs/pytensorflow/lib/python3.7/threading.py"", line 917, in _bootstrap_inner      self.run()    File ""/home/jsxian2/anaconda3/envs/pytensorflow/lib/python3.7/threading.py"", line 865, in run      self._target(*self._args, **self._kwargs)    File ""/home/jsxian2/anaconda3/envs/pytensorflow/lib/python3.7/site-packages/keras/utils/data_utils.py"", line 681, in _run      with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:    File ""/home/jsxian2/anaconda3/envs/pytensorflow/lib/python3.7/site-packages/keras/utils/data_utils.py"", line 676, in        initargs=(seqs, self.random_seed))    File ""/home/jsxian2/anaconda3/envs/pytensorflow/lib/python3.7/multiprocessing/context.py"", line 119, in Pool      context=self.get_context())    File ""/home/jsxian2/anaconda3/envs/pytensorflow/lib/python3.7/multiprocessing/pool.py"", line 176, in __init__      self._repopulate_pool()    File ""/home/jsxian2/anaconda3/envs/pytensorflow/lib/python3.7/multiprocessing/pool.py"", line 241, in _repopulate_pool      w.start()    File ""/home/jsxian2/anaconda3/envs/pytensorflow/lib/python3.7/multiprocessing/process.py"", line 112, in start      self._popen = self._Popen(self)    File ""/home/jsxian2/anaconda3/envs/pytensorflow/lib/python3.7/multiprocessing/context.py"", line 277, in _Popen      return Popen(process_obj)    File ""/home/jsxian2/anaconda3/envs/pytensorflow/lib/python3.7/multiprocessing/popen_fork.py"", line 20, in __init__      self._launch(process_obj)    File ""/home/jsxian2/anaconda3/envs/pytensorflow/lib/python3.7/multiprocessing/popen_fork.py"", line 70, in _launch      self.pid = os.fork()  BlockingIOError: [Errno 11] Resource temporarily unavailable  Exception in thread Thread-2:  Traceback (most recent call last):    File ""/home/jsxian2/anaconda3/envs/pytensorflow/lib/python3.7/threading.py"", line 917, in _bootstrap_inner      self.run()    File ""/home/jsxian2/anaconda3/envs/pytensorflow/lib/python3.7/threading.py"", line 865, in run      self._target(*self._args, **self._kwargs)    File ""/home/jsxian2/anaconda3/envs/pytensorflow/lib/python3.7/site-packages/keras/utils/data_utils.py"", line 681, in _run      with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:    File ""/home/jsxian2/anaconda3/envs/pytensorflow/lib/python3.7/site-packages/keras/utils/data_utils.py"", line 676, in        initargs=(seqs, self.random_seed))    File ""/home/jsxian2/anaconda3/envs/pytensorflow/lib/python3.7/multiprocessing/context.py"", line 119, in Pool      context=self.get_context())    File ""/home/jsxian2/anaconda3/envs/pytensorflow/lib/python3.7/multiprocessing/pool.py"", line 176, in __init__      self._repopulate_pool()    File ""/home/jsxian2/anaconda3/envs/pytensorflow/lib/python3.7/multiprocessing/pool.py"", line 241, in _repopulate_pool      w.start()    File ""/home/jsxian2/anaconda3/envs/pytensorflow/lib/python3.7/multiprocessing/process.py"", line 112, in start      self._popen = self._Popen(self)    File ""/home/jsxian2/anaconda3/envs/pytensorflow/lib/python3.7/multiprocessing/context.py"", line 277, in _Popen      return Popen(process_obj)    File ""/home/jsxian2/anaconda3/envs/pytensorflow/lib/python3.7/multiprocessing/popen_fork.py"", line 20, in __init__      self._launch(process_obj)    File ""/home/jsxian2/anaconda3/envs/pytensorflow/lib/python3.7/multiprocessing/popen_fork.py"", line 70, in _launch      self.pid = os.fork()  BlockingIOError: [Errno 11] Resource temporarily unavailable  "
"Hello everybody,  Can someone clarify the cropping mode for me ? in config.py it says:   crop: Picks random crops from the image. First, scales the image based # on IMAGE_MIN_DIM and IMAGE_MIN_SCALE, then picks a random crop of # size IMAGE_MIN_DIM x IMAGE_MIN_DIM. Can be used in training only. # IMAGE_MAX_DIM is not used in this mode.    My images are all 1024x1280, so if I use these settings:    IMAGE_RESIZE_MODE = ""crop"" IMAGE_MIN_DIM = 512 IMAGE_MAX_DIM = 512 IMAGE_MIN_SCALE = 0    it would first rescale the image to 512 and then randomly crop 640x640 in that? I don't understand it doesn't make sense to me.  or does it crops randomly from the image (1024x1280) images with size 512x512. Please can someone explain this to me. I am really in need to understand this for my project. thanks in advance."
"hello everybody, I am trying to use imagenet pretrained weights but I got this error:  !   as you see by_name=true and for the coco I used the adequate excludes but it shows me the same error with coco and imagenet wieghts.  I really don't understand where is the problem.   here is the configuration I used:  !   I will be grateful if can someone help me with this matter."
"Hello everybody,  I am trying to use display_top_masks but I failed, here is the code and the error.  !   Can someone enlighten me and tell me the expected mask to feed to this function. I don't understand why I got this error.  Thank you in advance.    "
"  got this error when i try to convert              179     model_config = f.attrs.get('model_config')      180     if model_config is None:  --> 181       raise ValueError('No model found in config file.')      182     model_config = json_utils.decode(model_config.decode('utf-8'))      183     model = model_config_lib.model_from_config(model_config,    ValueError: No model found in config file."
"Hey, tried adding pull request this,   and this      anyone please help me to deal with this situation trying to figure it for last 2 days   !   "
"The same code that I set up on windows, and it also gives good mean average precision. If I try to run on almost the same on linux, it does train the model but the mAP for some reason is comparatively very low. val loss also moves around alot on linux.    Did anyone face the same problem? Since everything is almost same, can a different version of a library make any difference.    Does anybody have experience regarding this?    Thanks."
"Getting this error while training my Mask RCNN model. I ran this command:  !python optic_disc.py train --dataset=dataset/ --weights=coco    Traceback (most recent call last):    File ""optic_disc.py"", line 364, in        train(model)    File ""optic_disc.py"", line 183, in train      dataset_train.load_optic_disc(args.dataset, ""train"")    File ""optic_disc.py"", line 128, in load_optic_disc      polygons = [r['shape_attributes'] for r in a['regions']]     File ""optic_disc.py"", line 128, in        polygons = [r['shape_attributes'] for r in a['regions']]   TypeError: 'NoneType' object is not subscriptable    I tried searching the net, but I am unable to find a solution. Can you help me with this?"
"Hello ,  I am new to Mask R-CNN and I am confused and don't know where to start to learn how to use this repos, what kind of annotations does it expect? does a dataset with only images and binary masks enough ?or should I have a json annotation (polygon segmentation with vgg annotator)? I didn't find a documentation or a good tutorial or a guideline that explains the basics. Please if someone have an idea where to start (some tutorials ...) and explains to me the type of expected annotations I'll be very grateful because I really need it for my academic project. Thanks in advance. "
"I have been training Mask R-CNN model with my own custom training data. In the data/train folder I have an image with the annotation (via_region_data.json) and I have exactly the same image and same annotation (via_region_data.json) in the data/val folder as well. I have trained the model with that train and val data (contains exactly same image). And, I processed that image with the model which was trained with that image. I expect to see 100% percentage accuracy for both of detection and masking.     Model has 100% percentage (36 objects I have in the image all of them have bounding boxes) accuracy in detection but it has 86% percentage in masking (36 object I have in the image and just 31 of them have masking). I need to increase the ration of masking in this image to be sure that okay my model is ready to train with mode images to reach super accuracy Do you have any suggestion for this case? Thanks.."
"Hello!    I am working on a model with a high imbalance of data, and since i cannot separate it in small datasets to compensate that imbalance (I'd lose about 80% of the dataset) I thought that using the class_weight option from keras.fit would be the best option.     It seemed as simple as adding the class_weight parameter in the model.py file (in def train ...) having something like this:            self.keras_model.fit(              train_generator,              initial_epoch=self.epoch,              epochs=epochs,              steps_per_epoch=self.config.STEPS_PER_EPOCH,              callbacks=callbacks,              validation_data=val_generator,              validation_steps=self.config.VALIDATION_STEPS,              max_queue_size=100,              class_weight=class_weight,              workers=1,#workers,              use_multiprocessing=False, #workers > 1,          )    I tried creating class_weight as a dict and as a list, but i got the following error when training:      ValueError: Provided `class_weight` was a list of 3 elements, but the model has 0 outputs. You should provide one `class_weight`array per model output.      Did anyone face the same error? Or someone has idea of how to solve it?     Thanks!        "
The dataset that I have only has bounding box annotations in the pascal voc format. Is there a way to just use that to the do the instance segmentation? Perhaps a change in the code somewhere?? 
"In the configuration it says:     # Shape of output mask      # To change this you also need to change the neural network mask branch      MASK_SHAPE = [28, 28]    Does anyone have any suggestions as to where these changes are to be done and how??    Update:     I added the following extra layer to def build_fpn_mask_graph to model.py, and set MASK_SHAPE =[56,56] :        x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding=""same""),                             name=""mrcnn_mask_conv5"")(x)      x = KL.TimeDistributed(BatchNorm(),                             name='mrcnn_mask_bn5')(x, training=train_bn)      x = KL.Activation('relu')(x)    But am still getting an error:    tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.    (0) Invalid argument: Incompatible shapes: [11,56,56] vs. [11,28,28]    [[{{node mrcnn_mask_loss_1/mul}}]]    [[add_metric_7/Identity/_6415]]    (1) Invalid argument: Incompatible shapes: [11,56,56] vs. [11,28,28]    [[{{node mrcnn_mask_loss_1/mul}}]]"
Getting following error while training MRCNN 0.1 on custom dataset. Please help.       
"On the left is my predicted result and on the right is the ground truth. The results is okay (class prediction is correct, mask overlap is correct) but still, the AP for this particular image is **0** for some reason. I am using iou=0.5, like voc.  This kind of thing is affecting the overall result when it happens to other images also.       !       Code to compute the mAP: (very similar to the nucleus example)    def compute_batch_ap(dataset, image_ids, verbose=1):      APs = []      precisions = []      recalls = []      overlaps = []            for image_id in image_ids:          # Load image          image, image_meta, gt_class_id, gt_bbox, gt_mask =\              modellib.load_image_gt(dataset, config,                                     image_id, use_mini_mask=False)          # Run object detection          results = model.detect_molded(image[np.newaxis], image_meta[np.newaxis], verbose=0)          # Compute AP over 0.5          r = results[0]          ap, precisions_out, recalls_out, overlaps_out = utils.compute_ap(              gt_bbox, gt_class_id, gt_mask,              r['rois'], r['class_ids'], r['scores'], r['masks'])                    APs.append(ap)          precisions.append(precisions_out)          recalls.append(recalls_out)          overlaps.append(overlaps_out)                    if verbose:              info = dataset.image_info[image_id]              meta = modellib.parse_image_meta(image_meta[np.newaxis,...])              print(""{:3} {}   AP: {:.2f}"".format(                  meta[""image_id""][0], meta[""original_image_shape""][0], ap))                    return APs, precisions, recalls, overlaps    limit = 513  APs, precisions, recalls, overlaps = compute_batch_ap(dataset, dataset.image_ids[:limit])      print(""Mean AP over {} images: {:.4f}"".format(len(APs), np.mean(APs)))"
"So my implementation detects the instances of object within the image, however the masks/bounding boxes being generated are quite big and around the image, thus overlapping with others and causing incoherency, I have a number of agricultural fields adjacent to one another, therefore it is important for the mask the to mimic the shape and size of the object as closely as possibly. What possible hyperparameters would require hypertuning for this. I have attached an example below  !   "
"`results = model.detect([_image[i]], verbose=1)`  `r = result[0]`  `print(r['masks'])`    I generate coco format json file from Image using COCO weight  but output has too many segmentation position (x, y location)    How to compress mask location??    [original]  `[x1, y1], [x2, y2], [x3, y3], ... [x100, y100]`    [goal]  `[x1, y1], [x2, y2], ... , [x20, y20]`    [present]  `[100, 300], [100.2, 300.2], [100,4, 300,4]...`  It's too many location information...      "
Are there any pre-trained weights available for imagenet with resnet101 backcone??
## Error Logs       dedicated GPU memory occupation on exception: 4.1 GB / 8 GB    ## Tensorflow Configuration        ## Code Section of Exception:     modified model of the example `train_shapes.ipynb`  network : `resnet 101`      ## Hardware Details:     CPU: i7-10750H  ( 6 Cores )  GPU: RTX 3070 Laptop GPU 1299MHZ  CUDA version : 10.0  CUDnn version : 7.6.5  Operating System : Windows 10  Environmental Variables:       ## Library Details (Python3.7):         
"Hi,    In multi-step training configuration (e.g. 10 epochs for the head then 10 epochs for the whole network), the global loss has a strange behavior:   - for the first 10 epochs, the global loss is equal to the sum of the others losses, that's OK  - for epoch 11, when all the layers are trainable, the global loss significantly increases and is not equal to the sum of the other losses anymore. However the other losses are OK; they decrease.   - for the remaining epochs the global loss decreases as expected but still with a high value    I thought that the model checkpoint was not correctly loaded so the training started with random weights at epoch 11 but the losses are good. It seems that the computation of the global loss is wrong    Any idea on what happens ?    Thanks !    "
"i am running on google colab ,keras==2.2.5 ,tensorflow_version 1.x   can anyone help me to fix this error           Starting at epoch 0. LR=0.001    Checkpoint Path: /content/drive/MyDrive/mask/MaskRCNN/logs/object20210223T1300/mask_rcnn_object_{epoch:04d}.h5  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  In model:  rpn_model      rpn_conv_shared        (Conv2D)      rpn_class_raw          (Conv2D)      rpn_bbox_pred          (Conv2D)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)  /tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  /tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  /tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.  Instructions for updating:  If using Keras pass *_constraint arguments to layers.  Traceback (most recent call last):    File ""/content/drive/MyDrive/mask/custom.py"", line 198, in        train(model)       File ""/content/drive/MyDrive/mask/custom.py"", line 181, in train      layers='heads')    File ""/content/drive/MyDrive/mask/mrcnn/model.py"", line 2374, in train      use_multiprocessing=True,    File ""/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 1658, in fit_generator      initial_epoch=initial_epoch)    File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py"", line 42, in fit_generator      model._make_train_function()    File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 512, in _make_train_function      loss=self.total_loss)    File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py"", line 504, in get_updates      return [self.apply_gradients(grads_and_vars)]    File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py"", line 433, in apply_gradients      self._create_slots(var_list)    File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/optimizer_v2/gradient_descent.py"", line 100, in _create_slots      self.add_slot(var, ""momentum"")    File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py"", line 585, in add_slot      initial_value=initial_value)    File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/variables.py"", line 260, in __call__      return cls._variable_v2_call(*args, **kwargs)    File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/variables.py"", line 254, in _variable_v2_call      shape=shape)    File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/variables.py"", line 235, in        previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)    File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/variable_scope.py"", line 2552, in default_variable_creator_v2      shape=shape)    File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/variables.py"", line 262, in __call__      return super(VariableMetaclass, cls).__call__(*args, **kwargs)    File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py"", line 1406, in __init__      distribute_strategy=distribute_strategy)    File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py"", line 1538, in _init_from_args      name=""initial_value"", dtype=dtype)    File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py"", line 1184, in convert_to_tensor      return convert_to_tensor_v2(value, dtype, preferred_dtype, name)    File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py"", line 1242, in convert_to_tensor_v2      as_ref=False)    File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py"", line 1273, in internal_convert_to_tensor      (dtype.name, value.dtype.name, value))  ValueError: Tensor conversion requested dtype float32_ref for Tensor with dtype float32:  "
None
Please help  Problem:  `model.keras_model.metrics_names `  retruns just    
"Hi, I could really do with some help in understanding the outputs of the tflite model of Mask RCNN.    I have converted the model to tflite using Tensorflow 2.3.    The model takes 3 inputs - the resized image, image meta and anchors    I have written the android equivalent code for the resized image - included padding and subtracted pixel means, as well as the image meta and anchors. The model is running but I can't seem to understand which output stands for what.     There are 7 outputs of the tflite model which have dimensions as follows:    [1][1][1]  [1][1000][81][4]  [1][1000][81]  [1][100][6]  [1][100][28][28][81]  [1][1][4]  [1][1][2]    I assume [1][100][28][28][81] is for the masks and I thought [1][100][6] would be the class ID, probability score and bounding boxes. However that doesn't seem to be consistent with the results. I can share the tflite model and the first few results of the outputs if that helps. Any help would be very much appreciated!"
"hi, I'm new to Mask R-CNN.  Now I'm using open hemorrhage data, and when I load ground truth images and targets, the ground truth visualized the mask is larger than actual mask.    I attached the image and code.  !     There's no problem with data, and I didn't change any code.    how can I fix this problem?    Thanx to read and thank you for sparing your precious time for me.    #########code################    dataset = dataset_test  fig = plt.figure(figsize=(20, 200))  for i in range(24):      image_id = random.choice(dataset.image_ids)      image_id = i      original_image, image_meta, gt_class_id, gt_bbox, gt_mask = modellib.load_image_gt(dataset, inference_config,image_id, use_mini_mask=False)      img = np.stack([original_image[:,:,0],original_image[:,:,0],original_image[:,:,0]],axis=2)          plt.subplot(30, 2, 2*i + 1)      visualize.display_instances(img, gt_bbox, gt_mask, gt_class_id,                                   dataset.class_names, ax=fig.axes[-1])  #########code################  "
"Hi, I am getting the following error for training on my custom dataset.         In a reply to this issue, raised before,   by @hengai, he suggested this:    > Modify config.py file: USE_MINI_MASK = True to USE_MINI_MASK = False ,and I solve it.    On that, he was told by @HAMZARaouia:    > @hengai your option doesnt' solve the pbm :/    This config does not solve the issue for me either. And, I cannot find any other solution for it in the Issues section of this repository.    What else could be wrong with my configs, or maybe the code? Here is my set of configs:     "
"Hi. My training data set consists of 295 images and I set STEPS_PER_EPOCH = 295. But when I train my model, it always shows that there are 1000 steps per epoch. How can I change the steps per epoch?"
"I am trying to change the score and class for my detection results as shown below  <img width=""99"" alt=""微信截图_20201206175129"" src=""   You can see that the white fonts is hard to read on my white background.  I searched some solutions and located the code in the display_instance of the visualize.py from     def display_instances(image, boxes, masks, class_ids, class_names,                        scores=None, title="""",                        figsize=(16, 16), ax=None,                        show_mask=True, show_bbox=True,                        colors=None, captions=None):      it seems the color can changed. But when i change it at:           # Label          if not captions:              class_id = class_ids[i]              score = scores[i] if scores is not None else None              label = class_names[class_id]              caption = ""{} {:.3f}"".format(label, score) if score else label          else:              caption = captions[i]          ax.text(x1, y1 + 8, caption,                  color='r', size=11, backgroundcolor=""none"")    nothing happens.    Any suggestions?    Thank you for your time.  "
None
"Hi all,     I am running a  4 class detection. modified from the ballon example. I have 160 for training and 40 pics for val.  Of course this is a small dataset, but i have seen some magical touch in other projects, they also get reasonable results. But mine is pretty terrible.    But my test results only showed 1 class.   Here is my configureation   Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        200  DETECTION_MIN_CONFIDENCE       0.9  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 1  IMAGE_CHANNEL_COUNT            3  IMAGE_MAX_DIM                  512  IMAGE_META_SIZE                17  IMAGE_MIN_DIM                  512  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [512 512   3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  1e-05  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               20  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           microplastic  NUM_CLASSES                    5  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        2000  POST_NMS_ROIS_TRAINING         2000  PRE_NMS_LIMIT                  6000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.3, 1, 7]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.3  RPN_TRAIN_ANCHORS_PER_IMAGE    800  STEPS_PER_EPOCH                300  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       True  TRAIN_ROIS_PER_IMAGE           800  USE_MINI_MASK                  False  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.001        and results:    <img width=""768"" alt=""Untitled"" src=""     If you can read from the pic, all mask are labelled as fragment, which is one of the four class i designed. Is there any place i messed up therefore only one class is showed? I have other pics, in all of them, only fragments are showed.    My loss curve all looks ok.   <img width=""768"" alt=""Untitled1"" src=""     Thank you for your time.    Your every suggestion will be helpful.            "
"system setup :   rtx 3080  python 3.7.4  windows 10  tensorflow : 2.5.0-dev20201118 (nightly build)  keras : 2.4.3  cuda : 11.0  cudnn : 8.0.4.30    **background:**  model was running fine on old GPU (rtx 2060) , after changing to the 3080 , using tensorflow would take a long time for some operations (not related to the model, for example the line : tf.constant([[1.0,2.0,3.0],[4.0,5.0,6.0]]) )  as well as displaying NAN  for the loss values (all except classification loss which displayed one constant value)  ,   the long waiting times was a tensorflow version issue ,which was fixed by upgrading to the nightly build (2.5.0-dev20201118) and the rest of the resulting requirements as specified in system setup above ,however the upgrade resulted in errors when running the model !!!    **issue description:**  on running training came across the following errror      Exception has occurred: TypeError  Could not build a TypeSpec for   with type KerasTensor    File ""C:\Users\ashaf102\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\framework\type_spec.py"", line 554, in type_spec_from_value      (value, type(value).__name__))    File ""C:\Users\ashaf102\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\keras_tensor.py"", line 205, in from_tensor      type_spec = type_spec_module.type_spec_from_value(tensor)    File ""C:\Users\ashaf102\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\keras_tensor.py"", line 606, in keras_tensor_from_tensor      out = keras_tensor_cls.from_tensor(tensor)    File ""C:\Users\ashaf102\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\util\nest.py"", line 672, in        structure[0], [func(*x) for x in entries],    File ""C:\Users\ashaf102\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\util\nest.py"", line 672, in map_structure      structure[0], [func(*x) for x in entries],    File ""C:\Users\ashaf102\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 871, in _infer_output_signature      keras_tensor.keras_tensor_from_tensor, outputs)    File ""C:\Users\ashaf102\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 824, in _keras_tensor_symbolic_call      return self._infer_output_signature(inputs, args, kwargs, input_masks)    File ""C:\Users\ashaf102\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 1093, in _functional_construction_call      inputs, input_masks, args, kwargs)    File ""C:\Users\ashaf102\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 954, in __call__      input_list)    File ""C:\AsafProj\DefectDetector\defect_detection\mask\mrcnn\model.py"", line 1880, in build      x, K.shape(input_image)[1:3]))(input_gt_boxes)    File ""C:\AsafProj\DefectDetector\defect_detection\mask\mrcnn\model.py"", line 1841, in __init__      self.keras_model = self.build(mode=mode, config=config)    File ""C:\AsafProj\DefectDetector\defect_detection\defectTrain.py"", line 41, in __init__      self.model = modellib.MaskRCNN(mode=""training"", config=self.config,model_dir=self.logDir)    File ""C:\AsafProj\DefectDetector\asaf_defect.py"", line 28, in        trainer = Train.defectTrainer()    File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\Lib\runpy.py"", line 85, in _run_code      exec(code, run_globals)    File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\Lib\runpy.py"", line 96, in _run_module_code      mod_name, mod_spec, pkg_name, script_name)    File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\Lib\runpy.py"", line 263, in run_path      pkg_name=pkg_name, script_name=fname)    File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\Lib\runpy.py"", line 85, in _run_code      exec(code, run_globals)    File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\Lib\runpy.py"", line 193, in _run_module_as_main      ""__main__"", mod_spec)    Thanks in advance  "
"Hey  I'm trying to train mask rcnn on my own dataset using tranfer learning. While training, I got stuck here.  `  Starting at epoch 0. LR=0.001    Checkpoint Path: /content/logs/shapes20201107T2123/mask_rcnn_shapes_{epoch:04d}.h5  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  In model:  rpn_model      rpn_conv_shared        (Conv2D)      rpn_class_raw          (Conv2D)      rpn_bbox_pred          (Conv2D)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)    ---------------------------------------------------------------------------    AttributeError                            Traceback (most recent call last)      in  ()        6             learning_rate=config.LEARNING_RATE,        7             epochs=1,  ----> 8             layers='heads')    1 frames    /content/mrcnn/model.py in train(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)     2352         log(""Checkpoint Path: {}"".format(self.checkpoint_path))     2353         self.set_trainable(layers)  -> 2354         self.compile(learning_rate, self.config.LEARNING_MOMENTUM)     2355      2356         # Work-around for Windows: Keras fails on Windows when using    /content/mrcnn/model.py in compile(self, learning_rate, momentum)     2197                 tf.reduce_mean(layer.output, keepdims=True)     2198                 * self.config.LOSS_WEIGHTS.get(name, 1.))  -> 2199             self.keras_model.metrics_tensors.append(loss)     2200      2201     def set_trainable(self, layer_regex, keras_model=None, indent=0, verbose=1):    AttributeError: 'Model' object has no attribute 'metrics_tensors'`"
"This might be a question to @waleedka, Waleed Abdulla. I'm following his post  . This is a great post!    However I fail to run:    `python3 balloon.py train --dataset=/path/to/dataset --model=coco`    I get this message, that I don't understand:    ""  Traceback (most recent call last):    File ""balloon.py"", line 330, in        model = modellib.MaskRCNN(mode=""training"", config=config,    File ""/home/pingo/git/Mask_RCNN/mrcnn/model.py"", line 1837, in __init__      self.keras_model = self.build(mode=mode, config=config)    File ""/home/pingo/git/Mask_RCNN/mrcnn/model.py"", line 1934, in build      anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=""anchors"")(input_image)    File ""/home/pingo/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 925, in __call__      return self._functional_construction_call(inputs, args, kwargs,    File ""/home/pingo/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1117, in _functional_construction_call      outputs = call_fn(cast_inputs, *args, **kwargs)    File ""/home/pingo/.local/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py"", line 904, in call      self._check_variables(created_variables, tape.watched_variables())    File ""/home/pingo/.local/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py"", line 931, in _check_variables      raise ValueError(error_str)  ValueError:   The following Variables were created within a Lambda layer (anchors)  but are not tracked by said layer:       The layer cannot safely ensure proper Variable reuse across multiple  calls, and consquently this behavior is disallowed for safety. Lambda  layers are not well suited to stateful computation; instead, writing a  subclassed Layer is the recommend way to define layers with  Variables.  ""    I'm using Python 3.8.5 and the following packages:  Cython==0.29.21  imgaug==0.4.0  ipyparallel==6.3.0  ipywidgets==7.5.1  Keras==2.4.3  mask-rcnn==2.1  nose==1.3.7  qtconsole==4.7.7  Sphinx==3.3.0  tensorflow==2.3.1  tensorflow-gpu==2.3.1    Please help me to understand what going on. "
I am using maskrcnn for my own dataset of 2 classes.   while training the model i'm getting this attribute error.  I am using google colab.     **error:**     **model.py lines:**     tensorflow & keras combinations i have tried:     1. tf: 1.15.0 & keras: 2.1.0   2. tf: 1.15.0 & keras: 2.2.5   3. tf: 1.9.0 & keras: 2.1.0   4. tf: 1.9.0 & keras: 2.2.0    
"Hi,     I have done annotation using yolo annotator on ~ 300 images. I get txt file for each images with class and coordinates of rectangles.     I wonder if i can easily convert this annotation dataset into via json format? Or at least an understandable format for mask-rcnn...    "
"### System information    -   **Have I written custom code (as opposed to using a stock example script      provided in TensorFlow)**:      - Yes  -   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:      - Linux Ubuntu 18.04.4 LTS  -   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue      happens on a mobile device**:  -   **TensorFlow installed from (source or binary)**:      - `pip install tensorflow`  -   **TensorFlow version (use command below)**:      - v2.3.0-54-gfcc4b966f1 2.3.1  -   **Python version**:      - 3.6.9  -   **Bazel version (if compiling from source)**:  -   **GCC/Compiler version (if compiling from source)**:  -   **CUDA/cuDNN version**:      - 10.1/7.6.5  -   **GPU model and memory**:      - Geforce RTX 2080 SUPER (8GB)  -   **Exact command to reproduce**:      - The minimal python script which can reproduce the issue is in the Source code / logs section below.      ### Describe the problem  I would like to check if inputs to a `Model` is valid or not by using the `tf.debugging.Assert()` function.  But as you can see in the Source code/logs section, the straightforward implementation results in an OperatorNotAllowedInGraphError.  I believe such simple syntax will help many TensorFlow users.    ### Source code / logs     when I ran this script, I got an error:   "
"This is the way I am caluculating map :   AP, precisions, recalls, overlaps = compute_ap(gt_bbox, gt_class_id, gt_mask, r[""rois""], r[""class_ids""], r[""scores""], r['masks'])    APs.append(AP)    My Test dataset contains some of the images where no objects are present. Is that the reason I am getting nan values while calculating mAp ?     [0.42857142857142855, 0.0, 0.6634146341463415, 0.0, 0.6818181818181818, 0.6818181818181818, 0.6634146341463415, 0.32, 0.32, 0.6634146341463415, 0.6634146341463415, 0.6634146341463415, 0.32, 0.0, 0.4551724137931034, 0.0, 0.0, 0.0, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]    "
"Tried using     mean_average_precision_callback = modellib.MeanAveragePrecisionCallback(model,  model_inference, dataset_val, calculate_map_at_every_X_epoch=5, verbose=1)    giving error as :     AttributeError                            Traceback (most recent call last)      in  ()  ----> 1 mean_average_precision_callback = modellib.MeanAveragePrecisionCallback(model,        2 model_inference, dataset_val, calculate_map_at_every_X_epoch=5, verbose=1)    AttributeError: module 'mrcnn.model' has no attribute 'MeanAveragePrecisionCallback'"
"Hi, I want to check how much time do Mask RCNN usually take for training? Right now, I'm using 10 images (7 for training and 3 for validation) and this requires 3-4h per epoch and will take me more than 15h if I do 5 epochs. As I'm using Colab, it will probably get disconnected before training is completed. Also, the training time for such a small data seems ridiculous to me so I'm seeking advice here.    Additional info:  I'm using tensorflow 1.7 and keras 2.1.5 (the only combination I found to have work) since I realised the latest and other ver of TF seems to give me errors and I have spent a lot of time debugging it but to no avail. Image annotation was done in VIA and exported in json file. Initial trial seems to take me 5h per epoch so I tried to cropped and resize (downsize) as much as possible from ~2000x1500 to something like ~300x150. Even so, training time doesn't seem to have improved significantly, became 3h+ per epoch (might even be zero improvements if it's due to the fluctuation). Not all of my training images are of same size (not sure if it affects) and for each training image I have about 20-30 annotations of the object I want to detect. Also, I realised when running in Colab, I keep getting the warning message that I'm allocated a GPU but not actually using it which I don't understand why. Tried using tensorflow-gpu then I will get the same error as mentioned above and the code will not run, so I end up using tensorflow 1.7 again.    I'm very new to this field and would appreciate any advice on my current situation, or tips to speed up my training time. TIA!"
None
"Hi everyone !    I'm trying to fine-tune coco pre-trained model in object detection task (without masks layers) . I used same configs as authors used in their blog. But I got very high rpn bounding box loss and non-decrease after training done with learning rate~1e-6. Loss always around 240. If I set it higher, rpn_bbox_loss would becoming NaN after few epochs, the other loss seems not that bad.     410/1000 [===========>..................] - ETA: 29:53 - loss: 245.9679 - rpn_class_loss: 0.0386 - rpn_bbox_loss: 244.3961 -   411/1000 [===========>..................] - ETA: 29:49 - loss: 245.5678 - rpn_class_loss: 0.0385 - rpn_bbox_loss: 243.9965 - mrcnn_class_loss: 0.3495 - mrcnn_bbox_loss: 0.0633    Is there any good advice? Many thanks"
None
"Hello,    I have successfully trained the model and I can see the results one after another.    `  def visualize_model(image_id):      image, image_meta, gt_class_id, gt_bbox, gt_mask = modellib.load_image_gt(dataset_val, config, image_id,   use_mini_mask=False)      info = dataset_val.image_info[image_id]      print(""image ID: {}.{} ({}) {}"".format(info[""source""], info[""id""], image_id, dataset_val.image_reference(image_id)))        # Run object detection      results = model.detect([image], verbose=1)            # Display results      r = results[0]            return visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], dataset_val.class_names)`          `  for image_id in range(0, len(test_images) - 2):      visualize_model(image_id=image_id)  `    But I want to have a subplot of the results (images).     Any idea how to do this?  I have tried multiple ways but it seemed to fail every single time.   I want to use the function plt.subplot, plt.subplots from matplotlib.     Regards,  Yash Runwal."
"Hi, during testing, with this command  `python tools/test_net.py --config-file ""configs/e2e_mask_rcnn_R_50_FPN_1x.yaml"" `  i get this error     anything wrong?  thanks for helping"
"i have been getting the above error while installing coco. Have alraedy follwed all the steps as mentioned in the link    While ruunig the command $ pip install git+  i am facing following error:-   Defaulting to user installation because normal site-packages is not writeable  Collecting git+     Cloning   to /tmp/pip-req-build-shfdssk5  Building wheels for collected packages: pycocotools    Building wheel for pycocotools (setup.py) ... error    ERROR: Command errored out with exit status 1:     command: /home/renu/anaconda3/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-req-build-shfdssk5/PythonAPI/setup.py'""'""'; __file__='""'""'/tmp/pip-req-build-shfdssk5/PythonAPI/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-joj6fjct         cwd: /tmp/pip-req-build-shfdssk5/PythonAPI    Complete output (18 lines):    running bdist_wheel    running build    running build_py    creating build    creating build/lib.linux-x86_64-3.7    creating build/lib.linux-x86_64-3.7/pycocotools    copying pycocotools/__init__.py -> build/lib.linux-x86_64-3.7/pycocotools    copying pycocotools/coco.py -> build/lib.linux-x86_64-3.7/pycocotools    copying pycocotools/mask.py -> build/lib.linux-x86_64-3.7/pycocotools    copying pycocotools/cocoeval.py -> build/lib.linux-x86_64-3.7/pycocotools    running build_ext    building 'pycocotools._mask' extension    creating build/temp.linux-x86_64-3.7    creating build/temp.linux-x86_64-3.7/pycocotools    creating build/common    gcc -pthread -B /home/renu/anaconda3/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/renu/anaconda3/lib/python3.7/site-packages/numpy/core/include -I../common -I/home/renu/anaconda3/include/python3.7m -c pycocotools/_mask.c -o build/temp.linux-x86_64-3.7/pycocotools/_mask.o    unable to execute 'gcc': No such file or directory    error: command 'gcc' failed with exit status 1    ----------------------------------------    ERROR: Failed building wheel for pycocotools    Running setup.py clean for pycocotools    ERROR: Command errored out with exit status 1:     command: /home/renu/anaconda3/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-req-build-shfdssk5/PythonAPI/setup.py'""'""'; __file__='""'""'/tmp/pip-req-build-shfdssk5/PythonAPI/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all         cwd: /tmp/pip-req-build-shfdssk5    Complete output (11 lines):    Traceback (most recent call last):      File "" "", line 1, in        File ""/tmp/pip-req-build-shfdssk5/PythonAPI/setup.py"", line 25, in          cythonize(ext_modules)      File ""/home/renu/.local/lib/python3.7/site-packages/Cython/Build/Dependencies.py"", line 972, in cythonize        aliases=aliases)      File ""/home/renu/.local/lib/python3.7/site-packages/Cython/Build/Dependencies.py"", line 815, in create_extension_list        for file in nonempty(sorted(extended_iglob(filepattern)), ""'%s' doesn't match any files"" % filepattern):      File ""/home/renu/.local/lib/python3.7/site-packages/Cython/Build/Dependencies.py"", line 114, in nonempty        raise ValueError(error_msg)    ValueError: 'pycocotools/_mask.pyx' doesn't match any files    ----------------------------------------    ERROR: Failed cleaning build dir for pycocotools  Failed to build pycocotools  DEPRECATION: Could not build wheels for pycocotools which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at    Installing collected packages: pycocotools      Running setup.py install for pycocotools ... error      ERROR: Command errored out with exit status 1:       command: /home/renu/anaconda3/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-req-build-shfdssk5/PythonAPI/setup.py'""'""'; __file__='""'""'/tmp/pip-req-build-shfdssk5/PythonAPI/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /tmp/pip-record-p_de5uei/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /home/renu/.local/include/python3.7m/pycocotools           cwd: /tmp/pip-req-build-shfdssk5/PythonAPI      Complete output (8 lines):      running install      running build      running build_py      running build_ext      building 'pycocotools._mask' extension      gcc -pthread -B /home/renu/anaconda3/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/renu/anaconda3/lib/python3.7/site-packages/numpy/core/include -I../common -I/home/renu/anaconda3/include/python3.7m -c pycocotools/_mask.c -o build/temp.linux-x86_64-3.7/pycocotools/_mask.o      unable to execute 'gcc': No such file or directory      error: command 'gcc' failed with exit status 1      ----------------------------------------  ERROR: Command errored out with exit status 1: /home/renu/anaconda3/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-req-build-shfdssk5/PythonAPI/setup.py'""'""'; __file__='""'""'/tmp/pip-req-build-shfdssk5/PythonAPI/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /tmp/pip-record-p_de5uei/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /home/renu/.local/include/python3.7m/pycocotools Check the logs for full command output.    Please suggest me what to do.?  "
"I have two separate labeled datasets say dataset_1 and dataset_2. The dataset_1 was labeled using VIA older version and dataset_2 with a newer VIA annotation tool. The JSON file is really long with +-1k records. Since  VIA has changed JSON formatting in later versions. Now instead of a dictionary, ""regions"" have a list. I want to convert the JSON file from :  **dictionary regions from**    `{""G0010033.JPG1978549"":{""fileref"":"""",""size"":1978549,""filename"":""G0010033.JPG"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[...]},""region_attributes"":{}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[...],""all_points_y"":[...]},""region_attributes"":{}},""2"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[...],""all_points_y"":[...]},""region_attributes"":{}},""3"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[...],""all_points_y"":[...]},""region_attributes"":{}}}},...more images ....}  `    **To have a list**     `{""G0010033.JPG1978549"": {""filename"": ""G0010033.JPG"", ""size"": 1978549, ""regions"": [{""shape_attributes"": {""name"": ""polygon"", ""all_points_x"": [405, 387, 361, 343, 338, 361, 387, 404, 404], ""all_points_y"": [1456, 1461, 1467, 1467, 462, 1456, 1454, 1455, 1454]}, ""region_attributes"": {}}, {""shape_attributes"": {""name"": ""polygon"", ""all_points_x"": [], ""all_points_y"": [...]}, ""region_attributes"": {}}, {""shape_attributes"": {""name"": ""polygon"", ""all_points_x"": [...], ""all_points_y"": [...]}, ""region_attributes"": {}}, {""shape_attributes"": {""name"": ""polygon"", ""all_points_x"": [...], ""all_points_y"": [...]}, ""region_attributes"": {}}, {""shape_attributes"": {""name"": ""polygon"", ""all_points_x"": [....], ""all_points_y"": [...]}, ""region_attributes"": {}}], ""file_attributes"": {}},...more images...}  `    Thank you in advance"
"Hello guys,    I have been training on the custom dataset.   I have changed a few parameters such as:     The training starts well but it is stuck after almost completing the first epoch. You can see below in the image    !       I would also like to add that I have waited for around 2 hrs after first epoch to post this issue.   I am using CPU.    The time taken for the training is not an issue. It's just that it is stuck.    Also,  When I only train for 1 epoch then I do not see the rois or the masks in the test image which I test the model on. Any idea why this is also happening?    Can someone please help me out?    Regards,  Yash.  "
"I have run the model and obtained the mask of the object, I want to acquire the array containing the mask of the object, I have reviewed r and results but they do contain the mask of each of the object detected. I am after one specific object detected, which I could be reverse engineer using to match from the r['scores']. If possible please guide me how to obtain it?"
"Hello guys,    So in the requirements.txt file it is given tensorflow>=1.3.   But when I use tensorflow 2.2.0, I am getting some errors about the lambda layers when I define my model.     So which tensorflow version should I dowload?  Better yet, can someone tell me the necessary changes I should make in the requirements.txt file?     Also, is it necessary to install tf-gpu and Cuda? I have only 128 images and I suspect that the training should not be too intensive so I would like to start training on CPU first.     Please let me know. I would really appreicate your help.     Regards,  Yash."
"Hello everyone,    In Tensorboard as well as in a log csv file I am saving via   `keras.callbacks.CSVLogger('log.csv', append=True, separator=';')`  I only see loss and val_loss. How do I log the other calculated losses like  `loss_names = [""rpn_class_loss"", ""rpn_bbox_loss"",                        ""mrcnn_class_loss"", ""mrcnn_bbox_loss"", ""mrcnn_mask_loss""]`?  "
"Hello guys,    So I have been using Mask-RCNN for work. I have to do custom object detection. For this I have labelled all my images using either **polygon or circle** depending on the geometry of the object in the given image. Now I have an annotations folder with separate annotations file for each image. i.e. I have 128 images and hence I have 128 annotations.json files.     According to the balloon.py script we need only one annotation file. So my question is:    1. How do I merge these several JSON files into one?    2. Or is there a way to change the export_boxes and load_mask functions given in the code to accommodate my problem? If so, how do I do that? Also, consider that that I have 2 shapes in my JSON files. One is Polygon and other is circle.     So a part of the JSON file which contains both polygon and circle is given below:  `{        ""label"": ""anchor"",        ""points"": [          [            35.70270270270271,            18.37837837837838          ],          [            60.56756756756755,            15.675675675675675          ],          [            70.29729729729729,            32.43243243243243          ],          [            59.486486486486484,            49.729729729729726          ],          [            38.40540540540539,            49.729729729729726          ],          [            30.29729729729729,            37.2972972972973          ]        ],        ""group_id"": null,        ""shape_type"": ""polygon"",        ""flags"": {}      },      {        ""label"": ""anchor"",        ""points"": [          [            244.35135135135135,            168.64864864864865          ],          [            250.2972972972973,            183.78378378378378          ]        ],        ""group_id"": null,        ""shape_type"": ""circle"",        ""flags"": {}      }`    I would like to add something else. I have seen other issues which are related to this problem but I couldn't find any decisive answers and hence I am opening a new issue here.     Please help me out.     With regards,  Yash."
"@waleedka     Hi sir,   I'm using Mask R-CNN for detecting objects on technical plans. My dataset is generated from a large PDFs that have been devided in multiple images of size 800x800 (and then split into 80% 10% 10% train, val and test). When I test the trained model on images similar to the ones used for training (test dataset), I get good results. But when i try to test the model in new generated images of different size (lets say 416x416), the performance of the model becomes so bad (I thought about the DPI used when converting from PDFs to images, because the value of DPI used in the images of training and testing wasnt the same as the one used o generate the images of size 416x416). In fact, I tried to use different resizing (pad64, crop, square) in different sizes, but i couldnt have the same results that I got while testing on the test dataset of size 800x800. Could you please help me with this, I couldnt find an explanation. "
"Hi, I am following this tutorial      When I arrive at the training step:     model = MaskRCNN(mode='training', model_dir='./', config=config)    The following error occurs:  The following Variables were created within a Lambda layer (anchors)  but are not tracked by said layer...    I read in other post that this is not compatible with tf 2.2, so I installed 1.5.    With tf 1.5,     from mrcnn.model import MaskRCNN  runs: ImportError: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
"Hello guys. I meet a problem during the training step of Mask RCNN. I tried many times but still not work.    !     The tf version is 1.13.1 and my image size is (128,128).    Config:    BACKBONE                       resnet101  BACKBONE_SHAPES                [[32 32]   [16 16]   [ 8  8]   [ 4  4]   [ 2  2]]  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     3  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.7  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      0  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 1  IMAGE_CHANNEL_COUNT            3  IMAGE_MAX_DIM                  128  IMAGE_META_SIZE                14  IMAGE_MIN_DIM                  128  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              pad64  IMAGE_SHAPE                    [128 128   3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           cell  NUM_CLASSES                    2  OPTIMIZER                      SGD  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  PRE_NMS_LIMIT                  6000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                6  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           20  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               5  WEIGHT_DECAY                   0.0001      Really appreciate your helps!!!"
None
-
We know that the original paper uses different types of ResNet (  but where does the author (@matterport) take/make/train the mask_rcnn_coco.h5 file?  Did he train the model? Where is the source code from that model?
"In Mask RCNN, I got the following result.   <img width=""298"" alt=""mapping_challenge"" src=""     But I want only mask image the same size as the input image     "
"Selecting layers to train  In model:  rpn_model  ---------------------------------------------------------------------------  ValueError                                Traceback (most recent call last)    in  ()  ----> 1 model.train(dataset_train,dataset_val,epochs=50,learning_rate=config.LEARNING_RATE,layers=""head"")    2 frames  /content/Mask_RCNN/mrcnn/model_1.py in train(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)     2351         log(""Checkpoint Path: {}"".format(self.checkpoint_path))     2352         self.set_trainable(layers)  -> 2353         self.compile(learning_rate, self.config.LEARNING_MOMENTUM)     2354      2355         # Work-around for Windows: Keras fails on Windows when using    /content/Mask_RCNN/mrcnn/model_1.py in compile(self, learning_rate, momentum)     2166             ""mrcnn_class_loss"", ""mrcnn_bbox_loss"", ""mrcnn_mask_loss""]     2167         for name in loss_names:  -> 2168             layer = self.keras_model.get_layer(name)     2169             if layer.output in self.keras_model.losses:     2170                 continue    /usr/local/lib/python3.6/dist-packages/keras/engine/topology.py in get_layer(self, name, index)     1866                 return layer     1867   -> 1868         raise ValueError('No such layer: ' + name)     1869      1870     @property    ValueError: No such layer: rpn_class_loss"
I want to get the probability pixel map for plotting the ROC-curve.  But the model returns the score of detected box and masks indicated false and True.    This is an example; the model found 24 boxes and mask.  This is code     This is output  !   !           
"ValueError: Layer #389 (named ""mrcnn_bbox_fc""), weight   has shape (1024, 88), but the saved weight has shape (1024, 324).      please help me!!!          `Instructions for updating:                                                                                                                                                                   box_ind is deprecated, use box_indices instead                                                                                                                                               Loading weights  /home/wy/pig/Mask_RCNN/mask_rcnn_coco.h5                                                                                                                                    Traceback (most recent call last):                                                                                                                                                             File ""coco.py"", line 486, in                                                                                                                                                            model.load_weights(model_path, by_name=True)                                                                                                                                               File ""/home/wy/anaconda3/envs/py36/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 2130, in load_weights                                                           File ""/home/wy/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/saving.py"", line 1328, in load_weights_from_hdf5_group_by_name                                                     str(weight_values[i].shape) + '.')                                                                                                                                                       ValueError: Layer #389 (named ""mrcnn_bbox_fc""), weight   has shape (1024, 88), but the saved weight has shape (1024, 324).                                                                                    `                                                                                                                                                             "
"Hello guys, recently I am trying to update this project from tf1 to tf2. But when I change keras to tf.keras, I got a lot of errors and still cannot solve some of them currently.   There are so many knowledges I don't to fix these errors and one of them is **ValueError: The two structures don't have the same sequence length. Input structure has length 0, while shallow structure has length 14.** which occurs during training the model and I am trapped in it for weeks.    Could anyone help me solve this problem or share your updated code of model.py of changing keras to tf.keras?  Thanks a lot."
I have not been able to find what this variable does in config.py. Please someone explain.
None
"Here is .  **Run samples/test/py or samples/shapes/train_shapes.ipynb can get this problem directly.**  Background: I transfer mask rcnn code from tf1 to tf2. Currently, I'm using tf2.1 and simply change keras to tf.keras  When I run Mask R-CNN shape sample and execute  `model.train(dataset_train, dataset_val,             learning_rate=config.LEARNING_RATE,              epochs=1,             layers='heads')`    I got errors and don't know how to figure it out.    Traceback (most recent call last):    File ""/Users/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code      exec(code_obj, self.user_global_ns, self.user_ns)    File "" "", line 1, in        runfile('/Users/PycharmProjects/Mask-Dence-Cap-R-CNN/Project/samples/test.py', wdir='/Users/PycharmProjects/Mask-Dence-Cap-R-CNN/Project/samples')    File ""/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_bundle/pydev_umd.py"", line 197, in runfile      pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script    File ""/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile      exec(compile(contents+""\n"", file, 'exec'), glob, loc)    File ""/Users/PycharmProjects/Mask-Dence-Cap-R-CNN/Project/samples/test.py"", line 260, in        layers='heads')    File ""/Users/PycharmProjects/Mask-Dence-Cap-R-CNN/Project/model.py"", line 585, in train      use_multiprocessing=True,    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper      return method(self, *args, **kwargs)    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1098, in fit      tmp_logs = train_function(iterator)    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 780, in __call__      result = self._call(*args, **kwds)    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 823, in _call      self._initialize(args, kwds, add_initializers_to=initializers)    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 697, in _initialize      *args, **kwds))    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 2855, in _get_concrete_function_internal_garbage_collected      graph_function, _, _ = self._maybe_define_function(args, kwargs)    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 3213, in _maybe_define_function      graph_function = self._create_graph_function(args, kwargs)    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 3075, in _create_graph_function      capture_by_value=self._capture_by_value),    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py"", line 986, in func_graph_from_py_func      func_outputs = python_func(*func_args, **func_kwargs)    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 600, in wrapped_fn      return weak_wrapped_fn().__wrapped__(*args, **kwds)    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py"", line 973, in wrapper      raise e.ag_error_metadata.to_exception(e)      Traceback (most recent call last):    File ""/Users/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code      exec(code_obj, self.user_global_ns, self.user_ns)    File "" "", line 1, in        runfile('/Users/PycharmProjects/Mask-Dence-Cap-R-CNN/mrcnn/samples/test.py', wdir='/Users/PycharmProjects/Mask-Dence-Cap-R-CNN/mrcnn/samples')    File ""/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_bundle/pydev_umd.py"", line 197, in runfile      pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script    File ""/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile      exec(compile(contents+""\n"", file, 'exec'), glob, loc)    File ""/Users/PycharmProjects/Mask-Dence-Cap-R-CNN/mrcnn/samples/test.py"", line 252, in        layers='heads')    File ""/Users/PycharmProjects/Mask-Dence-Cap-R-CNN/mrcnn/model.py"", line 2320, in train      use_multiprocessing=True,    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 324, in new_func      return func(*args, **kwargs)    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1829, in fit_generator      initial_epoch=initial_epoch)    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper      return method(self, *args, **kwargs)    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1098, in fit      tmp_logs = train_function(iterator)    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 780, in __call__      result = self._call(*args, **kwds)    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 823, in _call      self._initialize(args, kwds, add_initializers_to=initializers)    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 697, in _initialize      *args, **kwds))    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 2855, in _get_concrete_function_internal_garbage_collected      graph_function, _, _ = self._maybe_define_function(args, kwargs)    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 3213, in _maybe_define_function      graph_function = self._create_graph_function(args, kwargs)    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 3075, in _create_graph_function      capture_by_value=self._capture_by_value),    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py"", line 986, in func_graph_from_py_func      func_outputs = python_func(*func_args, **func_kwargs)    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 600, in wrapped_fn      return weak_wrapped_fn().__wrapped__(*args, **kwds)    File ""/Users/.local/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py"", line 973, in wrapper      raise e.ag_error_metadata.to_exception(e)  ValueError: in user code:      /Users/.local/lib/python3.6/site-pckages/tensorflow/python/keras/engine/training.py:806 train_function  *          return step_function(self, iterator)      /Users/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **          outputs = model.distribute_strategy.run(run_step, args=(data,))      /Users/.local/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run          return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)      /Users/.local/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2652 call_for_each_replica          return self._call_for_each_replica(fn, args, kwargs)      /Users/.local/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:3012 _call_for_each_replica          return fn(*args, **kwargs)      /Users/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **          outputs = model.train_step(data)      /Users/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:759 train_step          self.compiled_metrics.update_state(y, y_pred, sample_weight)      /Users/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/compile_utils.py:388 update_state          self.build(y_pred, y_true)      /Users/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/compile_utils.py:319 build          self._metrics, y_true, y_pred)      /Users/.local/lib/python3.6/site-packages/tensorflow/python/util/nest.py:1139 map_structure_up_to          **kwargs)      /Users/.local/lib/python3.6/site-packages/tensorflow/python/util/nest.py:1221 map_structure_with_tuple_paths_up_to          expand_composites=expand_composites)      /Users/.local/lib/python3.6/site-packages/tensorflow/python/util/nest.py:854 assert_shallow_structure          input_length=len(input_tree), shallow_length=len(shallow_tree)))      ValueError: The two structures don't have the same sequence length. Input structure has length 0, while shallow structure has length 14."
None
"I used the Mask_RCNN to train my own dataset which has 3 classes. The initial experiment results seem ok but I want to improve it so I add more epoches. I have 719 training data and use data augmentation during the training.    Config：        GPU_COUNT = 1      IMAGES_PER_GPU = 1         NUM_CLASSES = 1 + 3  # background + 3 shapes         IMAGE_MIN_DIM = 512      IMAGE_MAX_DIM = 512        RPN_ANCHOR_SCALES = (16,32,64,128,256)  # anchor side in pixels            TRAIN_ROIS_PER_IMAGE = 200         STEPS_PER_EPOCH = 240         VALIDATION_STEPS = 30    I tried twice and training strategies are like:    **1st(blue loss graph)**   head layers with lr=0.002 for 2 epoches + all layers with lr=0.001 for 38 epoches    **2nd(green loss graph)**  head layers with lr=0.002 for 2 epoches + all layers with lr=0.001 for 40 epoches + all layers with lr=0.0001 for 15 epoches    Both used pre-trained weights `mask_rcnn_coco.h5` and other parameters remain the same      **This is the training loss**  !     **val loss**  !     I just dont know why the val loss increased that much. And I think it is the `rpn_bbox_loss` that caused the great change and I found that the validation loss goes really high after the **head layers training was done**.  !   !      I tried many ways to solve the problem and I checked the validation data again. I think the masks are loading correctly and i tried more `STEPS_PER_EPOCH=719` to match my training dataset and the problem happened every time.    Does anyone know which part might go wrong and help me solve the problem. I appreciate it a lot    PS **I recreate the anaconda environment** before i started the second training because something is wrong with the IPython    Best Regards"
"Hi guys. I am trying to use your model to detect a small object in a large image in a mammogram (DDSM Dataset).  I trained a model with a cropped image of 1024x1024 with the objects that I want to detect.  For detection in a large image, I use a sliding window.  And I have a problem. The model saw training images containing small objects (mass), but there may be uninteresting objects in a large image, such as text, numbers, etc.   I can't add all objects in one window. See image below.  The model is activated on them. I want this to not happen.  Can I add blank masked images to my training set (Coco like dataset)? How should I do it?  !     "
" mrcnn_bbox = keras.layers.Reshape((s[1], num_classes, 4), name=""mrcnn_bbox"")(x)      957     ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported."
"# Create model in training mode  model = modellib.MaskRCNN(mode=""training"", config=config,                            model_dir=MODEL_DIR)  ---------------------------------------------------------------------------    ValueError                                Traceback (most recent call last)      in          1 # Create model in training mode        2 model = modellib.MaskRCNN(mode=""training"", config=config,  ----> 3                           model_dir=MODEL_DIR)        4     ~/Desktop/Mask_RCNN-master/mrcnn/model.py in __init__(self, mode, config, model_dir)     1835         self.model_dir = model_dir     1836         self.set_log_dir()  -> 1837         self.keras_model = self.build(mode=mode, config=config)     1838      1839     def build(self, mode, config):    ~/Desktop/Mask_RCNN-master/mrcnn/model.py in build(self, mode, config)     1932             anchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape)     1933             # A hack to get around Keras's bad support for constants  -> 1934             anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=""anchors"")(input_image)     1935         else:     1936             anchors = input_anchors    /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)      920                     not base_layer_utils.is_in_eager_or_tf_function()):      921                   with auto_control_deps.AutomaticControlDependencies() as acd:  --> 922                     outputs = call_fn(cast_inputs, *args, **kwargs)      923                     # Wrap Tensors in `outputs` in `tf.identity` to avoid      924                     # circular dependencies.    /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py in call(self, inputs, mask, training)      887         variable_scope.variable_creator_scope(_variable_creator):      888       result = self.function(inputs, **kwargs)  --> 889     self._check_variables(created_variables, tape.watched_variables())      890     return result      891     /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py in _check_variables(self, created_variables, accessed_variables)      914           Variables.'''      915       ).format(name=self.name, variable_str=variable_str)  --> 916       raise ValueError(error_str)      917       918     untracked_used_vars = [    ValueError:   The following Variables were created within a Lambda layer (anchors)  but are not tracked by said layer:       The layer cannot safely ensure proper Variable reuse across multiple  calls, and consquently this behavior is disallowed for safety. Lambda  layers are not well suited to stateful computation; instead, writing a  subclassed Layer is the recommend way to define layers with  Variables."
"hi, there!      I want to ask why I detect a long thin target, the edge of the target often misses part of the border！     Do you have any suggestions for parameter improvement?"
"I am facing this issue while using Mask_RCNN to train on my custom dataset with multiple classes.    This error occurs when I start training. This is what I get:    `/home/parth/anaconda3/envs/compVision/lib/python3.7/site-packages/skimage/transform/_warps.py:830: FutureWarning: Input image dtype is bool. Interpolation is not defined with bool data type. Please set order to 0 or explicitely cast input image to another data type. Starting from version 0.19 a ValueError will be raised instead of this warning.    order = _validate_interpolation_order(image.dtype, order)`    I keep getting this for like a hundred times and then the kernel dies.  Please Help!!"
I would like to compute mAP (mean average precision) on my own dataset. I am only detecting one class of objects. I am using utils.compute_ap() to compute AP per image but I am having an error due to dimension incompatibility between predicted masks and ground truth masks and this is mainly due to the fact that the number of detected instances is not the same.  How to deal with it ?
"Hello, everyone,  I'm thinking about implementing a data augmentation in my project, because my data set is comparatively small and tends to overfitting.   My annotations are saved as .xml files, and I have extracted the bounding box of each image.   For the augmentation I plan to use Fliplr, Flipud and Rotate for the augmentation.     But I am confused whether I need to adjust my annotations to the geometric transformation?    For example, I turn my image upside down: The bounding boxes will be in a different position than the original annotation file. Will the program automatically detect them or do I have to transform them?    I knew that similar discussions had already taken place in this forum, but the answers were different or sometimes confusing    I'd really glad to get your feedback.    "
Have anyone tried on it?
I am training a model to detect guns the model was trained once and I am getting this  as the RPN target  !   but my RPN predictions are all wrong  !   !   How can i fix this?
"In image_id = random.choice(dataset.image_ids)  image, image_meta, gt_class_id, gt_bbox, gt_mask =\      modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)  info = dataset.image_info[image_id]  print(""image ID: {}.{} ({}) {}"".format(info[""source""], info[""id""], image_id,                                          dataset.image_reference(image_id)))    # Run object detection  results = model.detect([image], verbose=1)    # Display results  ax = get_ax(1)  r = results[0]  visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'],                               dataset.class_names, r['scores'], ax=ax,                              title=""Predictions"")  log(""gt_class_id"", gt_class_id)  log(""gt_bbox"", gt_bbox)  log(""gt_mask"", gt_mask)        **I am getting the following error:**    AttributeError                            Traceback (most recent call last)    in          7         8 # Run object detection  ----> 9 results = model.detect([image], verbose=1)       10        11 # Display results    ~/anaconda3/lib/python3.7/site-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py in detect(self, images, verbose)     2499             log(""Processing {} images"".format(len(images)))     2500             for image in images:  -> 2501                 log(""image"", image)     2502      2503         # Mold inputs to format expected by the neural network    ~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)      566         xla_context.Exit()      567     else:  --> 568       result = self._call(*args, **kwds)      569       570     if tracing_count == self._get_tracing_count():    ~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)      604       # In this case we have not created variables on the first call. So we can      605       # run the first trace but we should fail if variables are created.  --> 606       results = self._stateful_fn(*args, **kwds)      607       if self._created_variables:      608         raise ValueError(""Creating variables on a non-first call to a function""    ~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)     2360     """"""Calls a graph function specialized to the inputs.""""""     2361     with self._lock:  -> 2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)     2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access     2364     ~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)     2701      2702       self._function_cache.missed.add(call_context_key)  -> 2703       graph_function = self._create_graph_function(args, kwargs)     2704       self._function_cache.primary[cache_key] = graph_function     2705       return graph_function, args, kwargs    ~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)     2591             arg_names=arg_names,     2592             override_flat_arg_shapes=override_flat_arg_shapes,  -> 2593             capture_by_value=self._capture_by_value),     2594         self._function_attributes,     2595         # Tell the ConcreteFunction to clean up its graph once it goes out of    ~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)      976                                           converted_func)      977   --> 978       func_outputs = python_func(*func_args, **func_kwargs)      979       980       # invariant: `func_outputs` contains only Tensors, CompositeTensors,    ~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)      437         # __wrapped__ allows AutoGraph to swap in a converted function. We give      438         # the function a weak reference to itself to avoid a reference cycle.  --> 439         return weak_wrapped_fn().__wrapped__(*args, **kwds)      440     weak_wrapped_fn = weakref.ref(wrapped_fn)      441     ~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)      966           except Exception as e:  # pylint:disable=broad-except      967             if hasattr(e, ""ag_error_metadata""):  --> 968               raise e.ag_error_metadata.to_exception(e)      969             else:      970               raise    AttributeError: in converted code:        /home/user/anaconda3/lib/python3.7/site-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py:46 log  *          if array.size:        AttributeError: 'Tensor' object has no attribute 'size'  "
"Hello,  I am quite new here and I hope you could help me.  I am training on my own dataset for a university project and I want to run the training process on my GPU.  First of all, I followed the CUDA-installation instructions and verify my installation. Running of deviceQuery.exe was sucessfull.  Afterwards, I create an environment and downloaded all packages of requirement.txt. Furthermore, I installed tensorflow-gpu instead of tensorflow.  By running the following commands in jupyterlab;  tf.test.is_gpu_available() -->true    tf.test.is_built_with_cuda() -->true    hello = tf.constant('Hello, TensorFlow!')  sess = tf.Session()  print(sess.run(hello))   -->'Hello, TensorFlow!'    from tensorflow.python.client import device_lib  device_lib.list_local_devices()  -->[name: ""/device:CPU:0""   device_type: ""CPU""   memory_limit: 268435456   locality {   }   incarnation: 11579283865742518988,   name: ""/device:GPU:0""   device_type: ""GPU""   memory_limit: 3189928755   locality {     bus_id: 1     links {     }   }   incarnation: 9744168715882205447   physical_device_desc: ""device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2""]    Due to these outputs, I expected that tf-gpu would be probaly integrated and training would run on GPU.  BUT, by starting training process, the GPU-usage is 0-1%, while my CPU-usage 50% and my memory >90%  I also changed the tf-versions to 1.12/1.15/1.13...but nothing changed.  Also the other issue-topics(like #2163) does not solve my problem    Sytem Details:  Windows 10  GeForce GTX 970  i7-7700 CPU  CUDA Toolkit 9.0  cudnn 7.6.5.32/7.0.5 (tried both unsucessfull)  Python 3.6.10  TF-GPU 1.8.0  Keras 2.2.4    I have no more ideas how to continue.   I would be very,very grateful, if you could help me     Thanks in advance :) :)   "
"Besides from using `imgaug`, what are the other types we can implement in this implementation? Augmentation is the key thing to get a better-generalized model.     In this implementation, is it possible to use SOTA augmentation approaches like plug and play or we need to experiment on it? Augmentation like: `CutMix`, `GridMask` etc."
"Hi,  I am using your Mask R CNN code for nucleus detection. I am not training the model, just using the code for detection. I would like to plot regions proposed by RPN algorithm. How can I do this? Also I could not get the idea very well. In the test step, is it proposing different regions for each image? Or is it using learned regions from training?"
"I'm looking for a way to visualize only selected ROIs after prediction.     Here's a sample flow:    - Model predicts the results for a given image  - A pseudo-logic is applied to filter out ROIs that meet certain extra criteria  - This selected set of ROIs is then visualized on the image    I have been able to build a logic like so:         But when this logic is executed, while I think I'm getting only the masks that I want in `new_result['masks']` , the masks are appearing distorted on the visualized image.    Any help is appreciated."
"There is an issue of using the latest versions of` `imgaug` while performing augmentation methods in training time.  At every iteration during training, the following log message is to keep generating repeatedly. Is it harmful? I don't know. Is there any way to hind this annoying log message?    `SuspiciousSingleImageShapeWarning: You provided a numpy array of shape (512, 512, 82) as a single-image augmentation input, which was interpreted as (H, W, C). The last dimension however has a size of >=32, which indicates that you provided a multi-image array with shape (N, H, W) instead. If that is the case, you should use e.g. augmenter(imageS= ) or augment_imageS( ). Otherwise your multi-image input will be interpreted as a single image during augmentation. category=SuspiciousSingleImageShapeWarning)`"
"I'm trying to calculate the mean average recall (mAR), for that I need to calculate the Average Recall (AR), does anyone know if the 'compute_recall' function returns the AR of an image, or is it just the common recall calculation for an image. The return of this function is different from the recall returned by the 'compute_ap' function.      Help me please!"
"The ""**compute_ap**"" function uses which standard to calculate Average Precision, Pascal VOC 2012 or COCO? I am confused by this because the way COCO calculates tests several thresholds, and in the function it uses only one, and the way Pascal calculates makes the 11 point interpolation, I cannot see any of these patterns applied to this ""**compute_ap**"" function , does anyone know which mathematical formula this function uses?"
"Hi everyone,    I am trying to detect 6-7 classes but the objects are huge in relation with the image (sometimes it could be the whole image). We are having problems to train the maskrcnn to detect them (despite being very distinct) plus masks never get close to the borders.    Is there any configuration to improve for such objects?       Thanks,  "
"I think this type of issue opened previously, but I have encountered this in different scenarios. I've trained my model in GCP and saved the last best weight. The training was great and ended normally. I could inference the trained model nicely on the server. And now, I wanted to inference the model in kaggle and got this error.    Any catch?"
"I customized the ""  repository to train with my own data set, for object detection, ignoring the segmentation part of the mask. Now, I am evaluating my results, I can calculate the MAP through the function: compute_ap, from ""  which returns the ""precisions, recalls, overlaps ""for each image. I would like to know how can I calculate the mean Average Recall (mAR) using this function: compute_recall from "" "
"I've used balloon.py to train an apple segmentation model,after training I've try it with a 960x1280 image,but the output mask's shape is 1024x1024.What should I do to change the output mask's size from 1024x1024 to 960x1280?(mask means r['masks'] in the end of inspect_balloon_data.ipynb)"
"It mainly occurs in `tensorflow 2x`, I think. And I know they're a few options we can select to get rid of this issue:    - downgrade to compatible version such as `tf 1.x`, OR  - modify code as: `tf.random_shuffle` to `tf.random.shuffle`    Is there any other way we can solve this issue when using this repo? I am mainly trying it on kaggle, so any suggestion regarding this would be really appreciated.     Thanks"
"Hi,   For some experiment purposes, I require the following format open-source data set which is for the **multi-class** problem.         The `XML` annotation should only contain bounding box information (like in PASCAL VOC).  And `n times class` images are in a single folder (like annotation). The data set should neither too big nor too small and even some reputation.     Any suggestions would be appreciated. :) "
"I'm trying to inference on custom dataset that is grayscale (128,128,1) by:    import skimage  image_id = random.choice(val.image_ids)    image, image_meta, gt_class_id, gt_bbox, gt_mask =\      load_image_gt(val, inference_config, image_id, use_mini_mask=False)  info = val.image_info     "
"I customized the ""  repository to train with my own dataset. Now I am evaluating my results, **I can calculate the MAP, but I cannot calculate the F1-Score**.  I have this function: **compute_ap**, from ""  that returns the ""mAP, precisions, recalls, overlaps"" for each image. The point is that I can't apply the F1-score formula because the variables ""precisions"" and ""recalls"" are lists.           "
"  Hi, I've seen others report the same problem but couldn't find a solution.      I can run the training script (balloon.py) and it absolutely can see my physical GPU, and it consumes all of the GPU available memory (and would like more), but on actual training, the GPU is at 0% - 3% load, whereas the CPU looks very active on about 30% load.  Is this correct/normal?    Config:  Windows 10  RTX 2070 Super  Cuda 10.0  Tensorflow-gpu 1.15.0  Keras 2.2.5    Samples of output:  `2020-05-06 09:23:30.107041: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll  2020-05-06 09:23:30.140764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:  name: GeForce RTX 2070 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.785  pciBusID: 0000:01:00.0  2020-05-06 09:23:30.146624: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll`    `2020-05-06 09:23:30.154658: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll`    `2020-05-06 09:23:30.163516: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll`    `2020-05-06 09:23:30.169803: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll`    `2020-05-06 09:23:30.178774: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll`    `2020-05-06 09:23:30.187260: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll`    `2020-05-06 09:23:30.201514: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll`    `2020-05-06 09:23:30.206479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0`    (and later)    `2020-05-06 09:23:52.576259: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.`    Any help would be greatly appreciated.  I feel like I bought a decent graphics card and it's not being used.    "
.
None
I have a dataset with 12 different classes the model detected all the bounding boxes but only generate the  mask for 10 classes i train the all layers and i trained the heads but still get the same results any one have an idea what should i do?  note: i changed config many times and still the same results.  
"I have trained a mask rcnn model before and it seems good. But now I want to add some new data with the same category to finetune it. The new data include 20 images which are 1280*960 size with 300 objects, and the images used before are smaller with about 30 object.    I have modified the config.py as #1884. But after one epoch trained on Head, the model perform worse, and after 20 epoches, the model can only detect few objects on images.   Is it normal that the model perform worse when I add new images?    Here is the code I trained on new images.       In addition, the new images contain many objects some of which are quiet small, and the loss with new images is more than 6 in the first epoch."
"_Hello Everyone._  _So , i'm following this tutorial to train a MaskRCnn on a custom Dataset on colab :_     _After the session ends and i restart the new one  , i try resuming the training from where it stoped using this command : !python roof.py train --dataset=Data/ --weights=last_  _But , it dosen't work because the *Logs* file where the checkpoints are saved is a temporary file that gets created when initializing the training , so my question is :_   _Dose anyone knows how i can save my checkpoints and resume training from the latest training model without losing anything everytime the session restarts ? , thank you_"
"I trained a model, with 20 epochs previously. I wanted to continue training for further 5 epochs. So I used the below code:    `python text_data.py train --dataset=C:/Users/Administrator/Desktop/Infomill/Mask_RCNN/samples/text_data/dataset --weights=C:/Users/Administrator/Desktop/Infomill/Mask_RCNN/logs/text_data20200408T1218/mask_rcnn_text_data_0020.h5`    It stops before running epochs.    `2020-04-21 09:28:43.923959: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll  Using TensorFlow backend.  Weights:  C:/Users/Administrator/Desktop/Infomill/Mask_RCNN/logs/text_data20200408T1218/mask_rcnn_text_data_0020.h5  Dataset:  C:/Users/Administrator/Desktop/Infomill/Mask_RCNN/samples/text_data/dataset  Logs:  C:\Users\Administrator\Desktop\Infomill\Mask_RCNN\logs    Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     2  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.8  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 2  IMAGE_CHANNEL_COUNT            3  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                14  IMAGE_MIN_DIM                  800  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           text_data  NUM_CLASSES                    2  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  PRE_NMS_LIMIT                  6000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                100  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001      WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.  Instructions for updating:  If using Keras pass *_constraint arguments to layers.  WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.    WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\ops\array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.  Instructions for updating:  Use tf.where in 2.0, which has the same broadcast rule as np.where  WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\mask_rcnn-2.1-py3.7.egg\mrcnn\utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.    WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\mask_rcnn-2.1-py3.7.egg\mrcnn\model.py:600: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.  Instructions for updating:  box_ind is deprecated, use box_indices instead  Loading weights  C:/Users/Administrator/Desktop/Infomill/Mask_RCNN/logs/text_data20200408T1218/mask_rcnn_text_data_0020.h5  2020-04-21 09:28:54.157771: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll  2020-04-21 09:28:54.296206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:  name: Tesla M60 major: 5 minor: 2 memoryClockRate(GHz): 1.1775  pciBusID: 0000:00:1e.0  2020-04-21 09:28:54.303298: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll  2020-04-21 09:28:54.309726: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_100.dll'; dlerror: cublas64_100.dll not found  2020-04-21 09:28:54.316531: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cufft64_100.dll'; dlerror: cufft64_100.dll not found  2020-04-21 09:28:54.322606: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'curand64_100.dll'; dlerror: curand64_100.dll not found  2020-04-21 09:28:54.329583: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusolver64_100.dll'; dlerror: cusolver64_100.dll not found  2020-04-21 09:28:54.336719: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_100.dll'; dlerror: cusparse64_100.dll not found  2020-04-21 09:28:54.353264: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll  2020-04-21 09:28:54.358109: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at   for how to download and setup the required libraries for your platform.  Skipping registering GPU devices...  2020-04-21 09:28:54.376245: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2  2020-04-21 09:28:54.386473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:  2020-04-21 09:28:54.392336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]  Re-starting from epoch 20  Training network heads    Starting at epoch 20. LR=0.001    Checkpoint Path: C:\Users\Administrator\Desktop\Infomill\Mask_RCNN\logs\text_data20200408T1218\mask_rcnn_text_data_{epoch:04d}.h5  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  In model:  rpn_model      rpn_conv_shared        (Conv2D)      rpn_class_raw          (Conv2D)      rpn_bbox_pred          (Conv2D)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)  C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\framework\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\framework\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\framework\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.    WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.    WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.    WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\keras\callbacks\tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.    WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\keras\callbacks\tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.`  "
if I have a picture with only one object to detect. I want to print the detected object only   (size of the image = size of the object).  how I can do this ??  
"i have a data of 1000 labelled image using VGG Image Annotator (900 train , 100 val), the problem when the loss of train get down 0.9* i start to get an over fitting problem.  i kept training it until loss = 0.006* then i test the model using images from training set but i get the the out without any bounding box and without any mask  (*** No instances to display *** ) .  i used 'coco' and i used 'imageNet' pre trained weights , also i changed step per epoch and the i changed the learning rate and i used ResNet101 and RestNet50 and i still face the problem .  i think the problem in my data .   any one can tell what should my data type must and what should the size of the images and the best way to label them ?  "
"I've 10 classes in my OD task. If I wanna find the precision score for each class separately, how to do it?"
What are the best evaluation metrics based on different schemes?
I'm trying to use mrcnn today but i had this problem.   do you have an idea for the solution ?      
"Hi,    I did a training with my own data  (136 images, 20 Epochs, 600 steps per Epoch) to detect particles in images, but when I run the detection I obtain too big and too many bounding boxes and no masks. What could be the problem? Could this be an underfitting or overfitting issue? How can I fix it? The results are very bad also for images that I already trained. I attach photo:  !       "
"Hi everyone,    I trained the model with 2 classes, vehicle and bottle. Now the model perfectly detects both of the objects but It says vehicle (class_id=1) to all of them. It perfectly detects but it says all bottles are vehicle. I probably make some mistakes while configuring the code. Does anybody have an idea about this issue?    Thank you for your time."
"In classification problems (especially in keras), we can split the training dataset into the train set and validation set using `validation_split` argument. In this mask-rcnn implement, is it possible to split the data in this manner?    we have two datasets (train, test) and prepare it. To make a validation split we can do this while preparing the data and end up with (train, val, test) OR can we do it after preparing the data, like split the train set into two parts (training part, and validation part)? Thanks. "
"Hello everyone,    Anchors by default are 32, 64, 128, 256, 512, I changed them with 16, 32, 64, 128, 256 and It worked perfectly for me. Now I need to use Mask R CNN for 2 classes, but the size of these classes are a way different. One of them is between 17-150 and another one is between 600-800.     1) To what values should I set the anchors?    2) Can I set the anchors to be more than 5 values like 32, 64, 128, 256, 512, 1024, 1536 instead of just default ones?    Thank you for your time."
"Hello everyone,  I get this error whenever I use modellib.load_image_gt() considering that I am using mrcnn2 (the one that works with tensorflow 2)."
"I'm new in the object detection task. This evaluation metrics give me confusion. I've encountered following the implementation of `mAP`. Is `mAP` and `AP` refer to the same results? How to calculate `mAP50/AP50` and `mAP75/AP75` and `AP@[0.5:0.95]`. I have read this blog:   and   but still, this thing bugging me.         Any suggestions would be appreciated. Thanks."
"Hi, everyone, I meet two errors when I try to use multi gpu for training.  At first, it shows `TypeError: Axis must be specified when shapes of a and weights differ.`, like this issue #534, and I solved this problem by @mosay95's answer,     > modify self.keras_model.fit_generator() in model.py  validation_data=val_generator to validation_data=next(val_generator)    After that, it shows `Integer division by zero`, like this issue #395     At the beginning of my project, I can use multi gpu normally. But now I don't know where I modified  so that it doesn't work.    I use keras==2.2.4, tensorflow-gpu==1.14 and 1.15(at first is 1.14, I try to update to solve problem but not worked), ubuntu18"
"Are the individual losses in percentaje? I have this doubt because my first training achieved a validation loss of 1.2 (sum of all individual validation losses) but my second training (loading the weights with 1.2 val_loss) achieved a val_loss of 0.86, but when i use the 1.2 val_loss weights for prediction, the performance is way better than the 0.86 val_loss weights. In fact, the results of the first training are the ones i was hoping to achieve.    What confuses me the most is the val_mrcnn_class_loss  and  the val_mrcnn_bbox_loss since the first training has higher values than the second one.    In the first training, after the 5th epoch i was disconnected from Colab, and in the second training after the 2nd epoch the model got overfitted    FIRST TRAINING  Epoch 5/10  110/110 [==============================] - 2754s 25s/step -   loss: 0.3204 -  rpn_class_loss: 0.0025 -   rpn_bbox_loss: 0.0899 -   mrcnn_class_loss: 0.0267 -   mrcnn_bbox_loss: 0.0574 -   mrcnn_mask_loss: 0.1439 -     val_loss: 1.1282 -   val_rpn_class_loss: 3.5456e-04 -   val_rpn_bbox_loss: 0.1737 -   val_mrcnn_class_loss: 0.1224 -   val_mrcnn_bbox_loss: 0.3284 -   val_mrcnn_mask_loss: 0.5034    SECOND TRAINING    Epoch 2/10  110/110 [==============================] - 12276s 112s/step -   loss: 0.8186 -   rpn_class_loss: 0.0092 -   rpn_bbox_loss: 0.2425 -   mrcnn_class_loss: 0.0479 -   mrcnn_bbox_loss: 0.2408 -   mrcnn_mask_loss: 0.2781 -     val_loss: 0.8675 -   val_rpn_class_loss: 0.0104 -   val_rpn_bbox_loss: 0.3051 -   val_mrcnn_class_loss: 0.0473 -   val_mrcnn_bbox_loss: 0.2383 -   val_mrcnn_mask_loss: 0.2663"
 Tensorflow-1.5.0 Tensorflow-gpu-1.5.0 Keras-2.1.5
I am new to Deep Learning. I am working on a project that deals with marine image datasets that I collected from a different source. I couldn't find any images related to underwater on COCO. So I would like to what to do when we have the image dataset? I annotated the images using RectLablel. But the code sample provided deals with COCO. Can someone help with this?
"I think STEPS_PER_EPOCH =traning_set / (IMAGES_PER_GPU * GPU_COUNT). But, there is a note about the STEPS_PER_EPOCH in the file of configure.py as follows:  Number of training steps per epoch      # **This doesn't need to match the size of the training set**.   How should I set the value of STEPS_PER_EPOCH？"
"Currently, I am using GPU RTX 2070, CUDA 10. I have tried many combination fo TF and Keras to make it work but it's not working at all. I wish to make it work on GPU. I have referred to many issues from GitHub but my issue is not yet resolved.    I am always getting this error:     I understand there are related issues in the forum but trust me I tried all solutions but no luck.  It will be great if anyone can please help me?****"
"Traceback (most recent call last):    File ""guardrail.py"", line 439, in        train(model, args.dataset, args.subset)    File ""guardrail.py"", line 239, in train      epochs=40,layers='heads')    File ""/data/vap/train-Mobilenet/Maskrcnn/mrcnn/model.py"", line 2379, in train      use_multiprocessing=False,    File ""/data/librarys/anaconda3/envs/Mask_RCNN/lib/python3.7/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""/data/librarys/anaconda3/envs/Mask_RCNN/lib/python3.7/site-packages/keras/engine/training.py"", line 1732, in fit_generator      initial_epoch=initial_epoch)    File ""/data/librarys/anaconda3/envs/Mask_RCNN/lib/python3.7/site-packages/keras/engine/training_generator.py"", line 100, in fit_generator      callbacks.set_model(callback_model)    File ""/data/librarys/anaconda3/envs/Mask_RCNN/lib/python3.7/site-packages/keras/callbacks/callbacks.py"", line 68, in set_model      callback.set_model(model)    File ""/data/librarys/anaconda3/envs/Mask_RCNN/lib/python3.7/site-packages/keras/callbacks/tensorboard_v2.py"", line 116, in set_model      super(TensorBoard, self).set_model(model)    File ""/data/librarys/anaconda3/envs/Mask_RCNN/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py"", line 1532, in set_model      self.log_dir, self.model._get_distribution_strategy())  # pylint: disable=protected-access  AttributeError: 'Model' object has no attribute '_get_distribution_strategy'  "
"Just a quick question, I want to train this implementation of Mask RCNN from **scratch** (i.e. **without using the COCO or Imagenet weights**). How can I do that?"
"<img width=""1178"" alt=""Screen Shot 2020-03-23 at 12 22 30 PM"" src=""     I downgraded my tensorflow version to 2.0 but I am still seeing this error while running train_shapes notebook."
It's my code in google cloud vm:      But It's error about permisson:     Help!!
None
"Hi all,    I am trying to train a model with mask_rcnn_coco.h5 as the base model. However, the output produced by the trained model is complete garbage.      **Case 1**: I annotated 5 or so images and trained a model on it for 67 epochs. And here is the last line of the log:    ` 684/1000 [===================>..........] - ETA: 10:09 - loss: 0.0150 - rpn_class_loss: 5.0984e-05 - rpn_bbox_loss: 5.0718e-05 - mrcnn_class_loss: 9.9723e-04 - mrcnn_bbox_loss: 1.3671e-04 - mrcnn_mask_loss: 0.0137`    Loss is clearly very low, so I expect it to give good results AT LEAST on the training images (overfitting). But it produces on garbage or NONE on training images itself.    **Case 2:**   I thought maybe I am using very small dataset (~5 images). Training on somewhat larger dataset should give good results. So, I started training on COCO VAL dataset which has 5k images. I trained for 40 epochs. I wasn't expecting very accurate model, but at least it should give 40-50% accuracy. But most of the time it generates random masks or detects no instances.    Here is the last line of the log:  `1000/1000 [==============================] - 1995s 2s/step - loss: 2.7258 - rpn_class_loss: 0.2099 - rpn_bbox_loss: 0.7793 - mrcnn_class_loss: 0.6342 - mrcnn_bbox_loss: 0.5556 - mrcnn_mask_loss: 0.5468 - val_loss: 2.7712 - val_rpn_class_loss: 0.1800 - val_rpn_bbox_loss: 0.7944 - val_mrcnn_class_loss: 0.7132 - val_mrcnn_bbox_loss: 0.5553 - val_mrcnn_mask_loss: 0.5282  `    If I look at the losses in tensorboard, all losses are decreasing over time with some of them reaching ~0.  Also, I noticed that even though I am using a pre-trained model, the starting loss is around 75. I am not sure if that's normal.    I am unable to figure out what's going wrong. Any leads/help would be appreciated.    Thanks"
How do I use the functions for computing AP (like compute_batch_ap_range etc from the utils) on multiple GPU's parallel?
"I have to implement object detection for three classes i.e. 'Person', 'Road' and 'Tyre'. The person class is already trained on the COCO dataset.    How do I use the weights of this already trained 'Person' class and my 2 new classes together to detect the objects?     My annotation file is only annotated for 'Tyre' and 'Road', do I have to completely ceate a new dataset with the 'Person' class annotated aswell?    Thanks"
AttributeError: module 'tensorflow' has no attribute 'Summary'
I'm having 2 instruments I want to detect. I trained the model on 1000 images (500 each).  I did predictions with pictures and the model recognises both instrument on different pictures.   When I combine the two instruments on 1 picture only 1 of the two is recognised.     My code:              Prediction:         Can someone help me please?
"I have been training the mask rcnn for a while and can't seem to figure out this behaviour.  As can be seen in the figure below, the masks don't go out to the full boundary of the item.    !     The masks that are being fed in during training have full masks.    Any ideas? I have played around with the parameters a bit (adding one more layer for finer masks helped a bit) but I couldn't resolve this behaviour completely."
"In my application - Detection objects incorrectly is much better than missing an object outright. I.e. If there's 10 instances, getting 10 boxes regardless of accuracy is paramount.    I am looking at the configurations for MaskRCNN, and see the 5 loss function weights.   Am I right to think that the RPN_Objectness loss is the one I should be weighted very high?  I have them set to:  first stage localization - 1.0  first stage objectness - 4.0  second stage localization - 2.0  second stage classification - 3.0  second stage mask - 2.0    Should I ramp up objectness even more - should I make all other 1.0? "
"I'm trying to run the balloon example but the process gets killed with no errors in the log file.    I've tried changing all these variables, but nothing changes the log output, it always exactly stops on the same `training/SGD/Variable_37:0` step, which is never fully flushed to stdout.    I'm using the `waleedka/modern-deep-learning/` docker image.     Variables I've tried changing,     I've tried limiting the image data set to just 3 images as well.    At first I thought it might be an issue with my mac using `Intel Iris 1536 MB` as graphics card, but the moment where the epoch quits seems too consistent for it to be a memory issue?  The provided notebooks both work like a charm  I'm not sure what might be causing this problem during training, any hints/ tips welcome. thanks in advance!    The last lines of stdout       The last lines of the log files       Pip dependencies   "
"My project used to train the model under Windows 10, and the prediction result is also very good, which can display the frame and mask.    Then I moved the source code of the project to Ubuntu 16.04, reconfigured the environment, and the versions of various library files were consistent. There was no error prompt when running the project, but the predicted results were not covered, even the categories and borders were not accurate. Even if I load the same weight file that I've trained.  !   !   "
"Hi,    I have copied and altered the 'balloons: Splash of Colour' example to train my own network. When I try to train I get the following error message. Please can you help me solve this problem?     My 'train' folder consists of 5 images and a corresponding .json file exported from VIA image annotator. The 'val' folder has only a single image and a .json file. I know this is a very small dataset, but I want to get it working before I label more images.    Thank you    (tensorflow 1.14.0, Python 3.7.5, Keras 2.3.1, Ubuntu Linux)    ---------------------------------------------------------------------------------------------------------------------------------    **$ python3 OutField.py train --dataset=/home/xksx14/Documents/WGMH/tensorflow/Mask_RCNN-master/samples/OutField/Dataset --weights=coco**    Using TensorFlow backend.  Weights:  coco  Dataset:  /home/xksx14/Documents/WGMH/tensorflow/Mask_RCNN-master/samples/OutField/Dataset  Logs:  /home/xksx14/Documents/WGMH/tensorflow/Mask_RCNN-master/logs    Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.5  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 1  IMAGE_CHANNEL_COUNT            3  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                14  IMAGE_MIN_DIM                  800  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           Tree  NUM_CLASSES                    2  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  PRE_NMS_LIMIT                  6000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                100  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001      WARNING:tensorflow:From /home/xksx14/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.    WARNING:tensorflow:From /home/xksx14/.local/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support. .wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.  Instructions for updating:  Use tf.where in 2.0, which has the same broadcast rule as np.where  WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py:553: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.    WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.    WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py:600: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.  Instructions for updating:  box_ind is deprecated, use box_indices instead  Loading weights  /home/xksx14/Documents/WGMH/tensorflow/Mask_RCNN-master/mask_rcnn_coco.h5  2020-02-20 11:08:44.639704: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA  2020-02-20 11:08:44.663008: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3600000000 Hz  2020-02-20 11:08:44.663557: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x115f7d00 executing computations on platform Host. Devices:  2020-02-20 11:08:44.663578: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0):  ,    2020-02-20 11:08:44.664563: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1  2020-02-20 11:08:44.686808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:   name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335  pciBusID: 0000:65:00.0  2020-02-20 11:08:44.686999: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0  2020-02-20 11:08:44.688050: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0  2020-02-20 11:08:44.689014: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0  2020-02-20 11:08:44.689266: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0  2020-02-20 11:08:44.690484: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0  2020-02-20 11:08:44.691423: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0  2020-02-20 11:08:44.694329: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7  2020-02-20 11:08:44.695070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0  2020-02-20 11:08:44.695104: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0  2020-02-20 11:08:44.769505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:  2020-02-20 11:08:44.769531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0   2020-02-20 11:08:44.769537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N   2020-02-20 11:08:44.770693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7327 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:65:00.0, compute capability: 6.1)  2020-02-20 11:08:44.772244: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x120755a0 executing computations on platform CUDA. Devices:  2020-02-20 11:08:44.772283: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1  Training network heads    Starting at epoch 0. LR=0.001    Checkpoint Path: /home/xksx14/Documents/WGMH/tensorflow/Mask_RCNN-master/logs/tree20200220T1108/mask_rcnn_tree_{epoch:04d}.h5  Selecting layers to train  fpn_c5p5               (Conv2D)  fpn_c4p4               (Conv2D)  fpn_c3p3               (Conv2D)  fpn_c2p2               (Conv2D)  fpn_p5                 (Conv2D)  fpn_p2                 (Conv2D)  fpn_p3                 (Conv2D)  fpn_p4                 (Conv2D)  In model:  rpn_model      rpn_conv_shared        (Conv2D)      rpn_class_raw          (Conv2D)      rpn_bbox_pred          (Conv2D)  mrcnn_mask_conv1       (TimeDistributed)  mrcnn_mask_bn1         (TimeDistributed)  mrcnn_mask_conv2       (TimeDistributed)  mrcnn_mask_bn2         (TimeDistributed)  mrcnn_class_conv1      (TimeDistributed)  mrcnn_class_bn1        (TimeDistributed)  mrcnn_mask_conv3       (TimeDistributed)  mrcnn_mask_bn3         (TimeDistributed)  mrcnn_class_conv2      (TimeDistributed)  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)  Traceback (most recent call last):    File ""OutField.py"", line 364, in        train(model)    File ""OutField.py"", line 199, in train      layers='heads')    File ""/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 2354, in train    File ""/usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg/mrcnn/model.py"", line 2198, in compile    File ""/home/xksx14/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py"", line 888, in binary_op_wrapper      y, dtype_hint=x.dtype.base_dtype, name=""y"")    File ""/home/xksx14/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1145, in convert_to_tensor_v2      as_ref=False)    File ""/home/xksx14/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1224, in internal_convert_to_tensor      ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)    File ""/home/xksx14/.local/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 305, in _constant_tensor_conversion_function      return constant(v, dtype=dtype, name=name)    File ""/home/xksx14/.local/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 246, in constant      allow_broadcast=True)    File ""/home/xksx14/.local/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 284, in _constant_impl      allow_broadcast=allow_broadcast))    File ""/home/xksx14/.local/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py"", line 454, in make_tensor_proto      raise ValueError(""None values not supported."")  ValueError: None values not supported.  "
None
"Hi there,     I did a K-Fold cross validation to get the best model out of my dataset. So each fold got me some weights saved into an h5 file. I now want to average the weights of my 3 best models (I know it might generate a bad result but still want to try).    So I tried to open the .h5 files with the h5py library to make an average of them but that's a total mess and got lost.    Does anybody tried to do the same or have some tips ?    Thank you very much !"
I Would like to use OIMD_V5 instance masks to train Mask_RCNN. I need to convert OIMD_v5 instance segmentation annotation file (.csv) to Coco json format. Any suggestion?    Thanks!
"Hello Everyone  , i'm trying to do a multi output & multi loss on a set of cropped segments outputed by a mask rcnn , and the results sucks , like this is what i get :                                     !     but when i try it on a ''regular'' image , i do get very good results :     !       So , i was wondering if the black background does have an impact on the classification process ?   Thank you."
I have my dataset in coco format and it has 25 classes (24 + background). I edited coco.py file and changed the configuration to accommodate for 25 classes. I used the same functions used for coco training. But after loading my trained model and testing it it is returning class_ids greater than my number of classes. (e.g. it is returning class_ids 71 and 60 and I only have 25).    training configuration       inference configuration         I am loading the model as follows:        and I am running my detection as follows:       where **class_names** is a list of 24 classes.    
"Hello Everyone , i have this masked objects :     !     And i want to save every masked object separatly as a jpg photo in a different file , i.e , in my output file i want to have for exemple :     !     and all the other objects.    Any suggestion on how to execute that would be super appreciated , thank you."
"Hi, if I want to add custom_callbacks to my training, I got the error:         My Code:     training.py       model.py       My Setup:  tensorflow(-gpu) 1.13.1  keras 2.0.8     The error occurs on CPU and GPU."
"Hi everyone,  If someone can explain what's wrong with this:  ValueError: **operands could not be broadcast together with shapes (56,56) (640,640) (640,640)**   in this function:  def apply_mask(image, mask, color, alpha=0.5):      """"""Apply the given mask to the image.      """"""      for c in range(3):          image[:, :, c] = np.where(mask == 1,                                    image[:, :, c] *                                    (1 - alpha) + alpha * color[c] * 255,                                    image[:, :, c])        return image  They have the same shape , i tried to find a hint , but found nothing.    If someone could help, i would be grateful    "
"I have a problem with training. After one step it stucks and i don't know why.     !     Configurations:  BACKBONE                       resnet50  BACKBONE_SHAPES                [[256 256]   [128 128]   [ 64  64]   [ 32  32]   [ 16  16]]  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.7  DETECTION_NMS_THRESHOLD        0.3  GPU_COUNT                      1  IMAGES_PER_GPU                 1  IMAGE_MAX_DIM                  1024  IMAGE_MIN_DIM                  800  IMAGE_PADDING                  True  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.005  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           train  NUM_CLASSES                    5  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                30  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               20  WEIGHT_DECAY                   0.0001    tensorflow-gpu=1.10.0  Keras=2.3.1"
"Hello all,    I followed the  . However, the training process always stopped at the firs step. I am wondering if anyone can help with this issue.  !     The libraries in my virtual environment is as follows  !   !   !     One thing I do want to highlight is that I am using keras==2.2.5 and tensorflow-gpu==1.13.1, which are higher than the  . I used the NVIDIA GeForce GTX 1080, cuda 10.2, and cudnn 7.4.2.  "
I want to train the model on custom data but the dataset size is small.  Does this implementation provide a method to freeze the initial layers and train the model only on some last layers?
i made a model which predict the area of the pants on image_1 and the area of the legs (avatar legs) i get the two masks but i now i want to cut the area of pants from image_1 and put the pants on the area of the legs on the avatar image .    1- crop the pants from image_1   2- put the cropped image on the mask of the legs.    Any one can help me please ?
I`m having a problem about this:     !     I train my own dataset based on instructions and samples in the balloon.    I already check the paths and everything and still doesn`t work. Any suggestions?
"Hi, I trained my MaskRCNN and everything seems fine, until I changed   `IMAGE_RESIZE_MODE = ""none ""`  in config.py  After that and also for the RESIZE_MODE ""square"" I got the error:     `ERROR:root:Error processing image {'id': 'Picture.jpg_47096', 'source': 'form', 'path': 'Picture.jpg', 'width': 2048, 'height': 1536, 'polygons': [{'name': 'polygon', 'all_points_x': [764, 769, 1480, 1451, 764], 'all_points_y': [857, 952, 1139, 1006, 857]}], '_class': 'firstclass'}  Traceback (most recent call last):    File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1673, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1238, in load_image_gt      class_ids = class_ids[_idx]  IndexError: boolean index did not match indexed array along dimension 0; dimension is 1 but corresponding boolean dimension is 257  `    If I got two masks of the same class in one picture, I got the error with    `IndexError: boolean index did not match indexed array along dimension 0; dimension is 2 but corresponding boolean dimension is 258`    The Parameter class_ids is 1D, _idx has the dimension 257.   The function load_mask is returning the mask with [h,w,no of instances] as said in other issues. For this problem I couldn't find a solution yet, I also checked all related issues here.  Please help, I'm suffering for many days at that error..."
After having managed to make my code work on a Quadro K6000 and a GTX 1060 I am trying to make my code work on RTX 2080 Ti.    Are there any people here who have managed to make it work with an RTX 2080 Ti ?    
"Hi, i trained the model with only 4 classes and BG. The detection is good but the evaluation results are very low. Precision and recall values are between 0.001 and 0.15 and i don't understand why. I used pretrained coco weights to start training on the following classes: person, car, dining table and chair.  Can someone help me? "
None
None
"Training on 2080 ti with Cuda 9.0 is very slow. Program is running but approximately 3-5% gpu utilization. memory is allocated. tensorflow-gpu==1.13.1, keras=2.1.3  Anyone solved this problem?? Thanks in advance  !   "
"As I know, the final detections do not include background class detection.    I tried to change this line (      to     But it results no detections when run predict    Any idea of how to keep background-class detection?"
"i wanted to train multiple classes with json file exported by new VIA annotator, but i am stuck at this error  [{'name': 'bottle', 'type': 'unknown', 'image_quality': {'good': True, 'frontal': True, 'good_illumination': True}}]  Traceback (most recent call last):    File ""final2.py"", line 367, in        train(model)    File ""final2.py"", line 188, in train      dataset_train.load_custom(args.dataset, ""train"")    File ""final2.py"", line 126, in load_custom      num_ids = [int(n['object']) for n in objects]    File ""final2.py"", line 126, in        num_ids = [int(n['object']) for n in objects]  KeyError: 'object'    and here is my equivalent load_mask()    def load_custom(self, dataset_dir, subset):          """"""Load a subset of the bottle dataset.          dataset_dir: Root directory of the dataset.          subset: Subset to load: train or val          """"""          # Add classes. We have only one class to add.          self.add_class(""object"", 1, ""bottle"")          self.add_class(""object"", 2, ""glass"")          self.add_class(""object"", 3, ""paper"")          self.add_class(""object"", 4, ""trash"")            # Train or validation dataset?          assert subset in [""train"", ""val""]          dataset_dir = os.path.join(dataset_dir, subset)            # Load annotations          # VGG Image Annotator saves each image in the form:          # { 'filename': '28503151_5b5b7ec140_b.jpg',          #   'regions': {          #       '0': {          #           'region_attributes': {},          #           'shape_attributes': {          #               'all_points_x': [...],          #               'all_points_y': [...],          #               'name': 'polygon'}},          #       ... more regions ...          #   },          #   'size': 100202          # }          # We mostly care about the x and y coordinates of each region          annotations1 = json.load(open(os.path.join(dataset_dir, ""via_region_data.json"")))          # print(annotations1)          annotations = list(annotations1.values())  # don't need the dict keys            # The VIA tool saves images in the JSON even if they don't have any          # annotations. Skip unannotated images.          annotations = [a for a in annotations if a['regions']]            # Add images          for a in annotations:              # print(a)              # Get the x, y coordinaets of points of the polygons that make up              # the outline of each object instance. There are stores in the              # shape_attributes (see json format above)              polygons = [r['shape_attributes'] for r in a['regions']]               objects = [s['region_attributes'] for s in a['regions']]              print(objects)              num_ids = [int(n['object']) for n in objects]              # load_mask() needs the image size to convert polygons to masks.              # Unfortunately, VIA doesn't include it in JSON, so we must read              # the image. This is only managable since the dataset is tiny.              print(""numids"",num_ids)              image_path = os.path.join(dataset_dir, a['filename'])              image = skimage.io.imread(image_path)              height, width = image.shape[:2]                self.add_image(                  ""object"",  ## for a single class just add the name here                  image_id=a['filename'],  # use file name as a unique image id                  path=image_path,                  width=width, height=height,                  polygons=polygons,                  num_ids=num_ids)        def load_mask(self, image_id):          """"""Generate instance masks for an image.         Returns:          masks: A bool array of shape [height, width, instance count] with              one mask per instance.          class_ids: a 1D array of class IDs of the instance masks.          """"""          # If not a bottle dataset image, delegate to parent class.          image_info = self.image_info[image_id]          if image_info[""source""] != ""object"":              return super(self.__class__, self).load_mask(image_id)            # Convert polygons to a bitmap mask of shape          # [height, width, instance_count]          info = self.image_info[image_id]          if info[""source""] != ""object"":              return super(self.__class__, self).load_mask(image_id)          num_ids = info['num_ids']          mask = np.zeros([info[""height""], info[""width""], len(info[""polygons""])],                          dtype=np.uint8)          for i, p in enumerate(info[""polygons""]):              # Get indexes of pixels inside the polygon and set them to 1           rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])             mask[rr, cc, i] = 1            # Return mask, and array of class IDs of each instance. Since we have          # one class ID only, we return an array of 1s          # Map class names to class IDs.          num_ids = np.array(num_ids, dtype=np.int32)          return mask, num_ids        def image_reference(self, image_id):          """"""Return the path of the image.""""""          info = self.image_info[image_id]          if info[""source""] == ""object"":              return info[""path""]          else:              super(self.__class__, self).image_reference(image_id)    please help me out!"
"I run on jupyter notebook and during the process, the image output didn`t load and the kernel just died. Here is the trace in conda prompt:    !     I am using python3.6.9, cuda9.0, cudNN9.0 for windows 10. Thanks for the advice!"
"EDIT: I figured it out, it was my fault in some other part of the code, my bad! Closing.    Hello,    I have successfully trained a network to make some predictions. My problem is that the inference time seems to steadily increase over time (continuously running inferences over a day, at 2 inferences/s).  I have observed this behavior on several machines, running the same program. On one of them I started a process yesterday, which was running at 100-150 ms /inference, and after a day (specifically, 170 K inferences) it is running at 150-200ms.    My inference architecture is as follows:    My model is kept within a class, which owns a  tensorflow graph `self.graph = tf.Graph()`       My model lives in the same class, and is used when prompted with something like this:           I have been profiling memory, using suggestions found here:   and did not notice any leak.      Did I do something obviously wrong here?  Has this behaviour been observed by anyone before? "
   I get this error message when I train using the Nucleus example on my own greyscale images following these adaptions:      Any suggestions?  
"@moorage @waleedka    I would like to save only losses to further analysis after training. Is there any way to save them into a file like .csv or txt.,    Thanks"
I have 16 bit images - will this be a problem when using your code (with transfer learning from coco)? Should I convert them to 8 bit while loading them or is the architecture capable of handling those as well?
"I am trying to use full size masks and training crashes with following error    ""expected input_gt_masks to have shape"" (I forgot to save the shape size).    Also my IMAGE_MIN_DIM is 256 and IMAGE_MAX_DIM is 384    Thanks."
"Hi    I have a dataset where images are larger than the related mask images. The masks are smaller but scaled down. For example if the image is 4000x3000 then mask is 1000X750.    Do I need need to scale-up the mask image in the ""load_mask"" function of my dataset?    My dataset size is about 210,000 images and I feel if I can avoid this extra burden of CPU, it might be handy.    Thanks in advance."
"Hi!  I have images of size 533x50 that contain radio frecuency interference. They are labeled as Narrow Band (NB) Intermediate Band (IB) and Wide Band (WB).     !     !     All of the objects (RFI) heights are always 50px, so the classification is based on the widths.  The minimum width in the dataset is 5px (NB) and the maximum is 232px (WB)  My question is, what parameters should i have to modify to adapt the model to my images?    My actual configuration setting is this:  NUM_CLASSES = 1 + 3  GPU_COUNT = 1  IMAGES_PER_GPU = 1  IMAGE_CHANNEL_COUNT = 1  MEAN_PIXEL = np.array([128])             <-----------( I use grey scale images)  BACKBONE = ""resnet50""  IMAGE_RESIZE_MODE = ""none""             <-----------(!!!)  IMAGE_MIN_DIM = 50             <-----------(!!!)  IMAGE_MAX_DIM = 553             <-----------(!!!)  DETECTION_MAX_INSTANCES = 50  DETECTION_MIN_CONFIDENCE = 0.7  RPN_ANCHOR_SCALES = (5, 7, 9, 50, 32, 64, 128, 256, 512)             <-----------(!!!)  RPN_ANCHOR_RATIOS = [0.5, 1, 2]             <-----------(!!!)    I don't know how to configure exactly the parameters marked with an arrow    Thank you"
"Good day!  in the paper ，there is one/two fully connected layer    !     but in this code, there is two conv layer ,         I am loogking for a reason ,  have read lots of resources, but get nothing , someone knows?    orz"
"# Speed issue  Sorry for my question and thanks for your great work. my question is how increase MaskRCNN prediction speed, i have already read those speed related previous issue. In those previous  issue you was talking about   but it have already created .  Still now it take 4.5 s or above per image prediction. Here i train image dimension 1024 * 1024 ,use backbone Resnet101 and my orginal image size around 2400 * 3400.    ### Is it possible the prediction time reduce 1s per image.  "
"I downloaded everything as recommended in mask rcnn but this error keep popping up.    !     And also this one,    !     I haven`t touch the code yet and felt quite lost. Thanks for the advice!"
"Is there any I can do to download these weights? Or any alternative links? I tried downloading this for 3 hours already in any devices and in different network. Is this still open? Please help, thanks."
"Hello!! This code is very nice, I have a question, wait for answer, thanks~    RPN_BBOX_STD_DEV  use multiply:           BBOX_STD_DEV use divide:         their values are equal:           one use multiply , another use divide  , why?        "
"Dear all,  My training gets stuck at random epochs/steps when using GPU training. Sometimes the training would go through without problems, and sometimes it can't. Most of the time, when I re-install Tensorflow 1.15 the training can be continued for 16 - 20 epochs before it hangs. I tried to set ""use_multiprocessing=False or True"", ""workers=1 or 0"", ResNet50 and ResNet101. The image size is 1024 by 1024, also tried reducing the ROIs from 200 to 50, none of these methods helped. I have no problem when using CPU to train the model, but it takes too long.  My GPU is Quadra M1000M, which has 4GB VRAM. CUDA 10, Tensorflow 1.15, Keras 2.3.1, Python 3.6.6. My OS is Windows 7 Pro 64 Bits.  I am wondering if this problem is caused by the small VRAM?  Thanks."
I'm trying to train a Mask RCNN network using a custom dataset. I've modified the balloon.py example to load my annotation file (json file) and the load_mask function to create the masks on the fly. It all works well... except that I get this warming:       Does anyone know why my function is not recognised and used? Thanks!    My custom class is this:       And my environment/packages     
"Now ,I have modify the resnet graph, C1.shape is [512,512,3],  added  C1 and  Upsampled P2 ,and adapted the box size to different feature map,but they are all belong to mrcnn branch,  about the rpn branch, i can not modify anything,  who can teach me how to do it?"
"Hello everyone,   I have been using this repo for quite a long time and now I have extended it to multiple classes but facing error. Below is my snippet attached for the reference.    You can see the error prompt, I am confused if the error is related to tensorflow or keras incompatibility or related to ski-mage.     I made it working when working with windows laptop and everything works fine but now i have switched to Linux Ubuntu and getting this issue. Does anyone find this issue and know the fix to the problem. The same thing is working on windows OS but not on Ubuntu.    Windows Dependencies =   Tensorflow-gpu : 1.10.0  Keras = 2.2.4    Ubuntu =   Tensorflow-gpu = 1.14.0 (Since cuda toolkit installed only works with tf.version => 1.13.x)  Keras 2.2.4    I'm really stuck at the moment since my work is stopped because of this problem  Thank you.  !   "
"I runned `samples/balloon/balloon.py` as below.     After 20 hours, `mask_rcnn_balloon_0030.h5` was generated in `Mask_RCNN/logs/balloon20191117T0132`. However, color splash effect with the .h5 file doesn't work as well as that with `mask_rcnn_balloon.h5` downloaded from   .    Here's an example (used   as input):    - result using `mask_rcnn_balloon.h5` (downloaded from        - result using `mask_rcnn_balloon_0030.h5` (which I generated):      Any ideas? Do I need to increase epoch number (30) or step number (100) in order to replicate the `mask_rcnn_balloon.h5` file?    My machine environment:    Ubuntu 16.04.6 LTS  CUDA 9.1  Python 3.6.8  tensorflow 1.14.0  keras 2.2.5"
How can I convert segmentation masks stored as PNG images in order to train it on Mask R-CNN for both multi-class segmentation and instance segmentation? Each of the mask consists of N different kinds of pixel annotations. There are no other annotations such as XML/JSON or polygon formats for this. Below I have attached the instance and class annotations.     Class map:  !     Instance map:    !     Looking for suggestions. Thanks.    
"In  , precisions array should be float type like as done for calculating recalls. Currently, it either gives array of 1 or 0 hence it affects the calculation of mAP.  Original:     Proposed change for the precision calculation     "
I have train a model based on COCO and found that no more that 7 object detected. How to handle it? Did it related to POOL_SIZE setting?
I'm trying to print the confusion matrix in the model.py file via          code taken from      an exception i raised stating that zero dimension numpy array cannot be concatenated.     Thanks.
I am working on vehicle counting. I want to train the model with my own images. I can't understand the training process. Please help me
"Hello everyone,    I've trained MaskRCNN with my own dataset. It runs well, but it doesn't generate masks. The BBOX are really great and accurate, the classification aswell but no mask generated.    I am based on the balloon.py to generate my mask from VIA annotator.  I decided to take a deeper look in ""load_mask"", and when I plot the masks generated, it works well. I have the polygon annotated from VIA in white , and the rest in black. Yet, all the mask returned by model.detect are full of zeros... Once again, the BBOX and the classification are perfect !    If you have any guideline to help me out, it will be really great. Thank you !    PS: While someone reply to this, i'll save all the mask generated to a folder per image like a classical dataset with masks given ...      **UPDATE:**  Little update, it turns out that in the function unmold_mask from utils.py, my mask values are all under the threshold value which 0.5... but I don't know why yet."
"Hi!  Awesome library first of all!    I am having an issue running the training code / a rewritten version of the balloon example for custom data on my server. I am transfer learning around 1000 images of Full HD resolution on a DGX-1 (8x V100 GPUs with 32GB VRAM each).   All drivers are installed correctly, Tensorflow-GPU has version 1.14.0 (non-GPU TF is not installed in the virtualenv), Keras has 2.2.5 due to the issue #1754.  The training works fine, albeit very slow (it takes around 50min per epoch) and the GPUs are not running much at all (all is done by the two 20-core Intel Xeon E5-2698) which is not very efficient. When running `nvidia-smi`, it does report the 8 jobs it is supposed to have, but every GPU has 0% utilization and only uses 418MB out of 32GB VRAM.  During training, it uses 400GB - 500GB of RAM    Is there any way to enable GPU training / any possibilty it is disabled by default? I am running the parameters: `IMAGES_PER_GPU=4` with 100 steps per epoch and GPU_COUNT=8"
I am trying to solve over fitting by changing validation and training data. But always val_loss value is greater than 1.      Is there any specific solution or suggestion please?    @waleedka @moorage 
"> When i try to execute de 3rd part of demo.ipynb:    `  # Create model object in inference mode.  model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)    # Load weights trained on MS-COCO  model.load_weights(COCO_MODEL_PATH, by_name=True)`        > i have the following error and it seems that there is a problem with the line:  model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)    AttributeError                            Traceback (most recent call last)    in          1 # Create model object in inference mode.  ----> 2 model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)        3         4 # Load weights trained on MS-COCO        5 model.load_weights(COCO_MODEL_PATH, by_name=True)    ~/opt/anaconda3/envs/Mask_RCNN/mrcnn/model.py in __init__(self, mode, config, model_dir)     1835         self.model_dir = model_dir     1836         self.set_log_dir()  -> 1837         self.keras_model = self.build(mode=mode, config=config)     1838      1839     def build(self, mode, config):    ~/opt/anaconda3/envs/Mask_RCNN/mrcnn/model.py in build(self, mode, config)     2036                                      config.POOL_SIZE, config.NUM_CLASSES,     2037                                      train_bn=config.TRAIN_BN,  -> 2038                                      fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE)     2039      2040             # Detections    ~/opt/anaconda3/envs/Mask_RCNN/mrcnn/model.py in fpn_classifier_graph(rois, feature_maps, image_meta, pool_size, num_classes, train_bn, fc_layers_size)      923     # Shape: [batch, num_rois, POOL_SIZE, POOL_SIZE, channels]      924     x = PyramidROIAlign([pool_size, pool_size],  --> 925                         name=""roi_align_classifier"")([rois, image_meta] + feature_maps)      926     # Two 1024 FC layers (implemented with Conv2D for consistency)      927     x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (pool_size, pool_size), padding=""valid""),    ~/opt/anaconda3/envs/MaskRCNN/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py in symbolic_fn_wrapper(*args, **kwargs)       73         if _SYMBOLIC_SCOPE.value:       74             with get_graph().as_default():  ---> 75                 return func(*args, **kwargs)       76         else:       77             return func(*args, **kwargs)    ~/opt/anaconda3/envs/MaskRCNN/lib/python3.7/site-packages/keras/engine/base_layer.py in __call__(self, inputs, **kwargs)      487             # Actually call the layer,      488             # collecting output(s), mask(s), and shape(s).  --> 489             output = self.call(inputs, **kwargs)      490             output_mask = self.compute_mask(inputs, previous_mask)      491     ~/opt/anaconda3/envs/Mask_RCNN/mrcnn/model.py in call(self, inputs)      388         # e.g. a 224x224 ROI (in pixels) maps to P4      389         image_area = tf.cast(image_shape[0] * image_shape[1], tf.float32)  --> 390         roi_level = log2_graph(tf.sqrt(h * w) / (224.0 / tf.sqrt(image_area)))      391         roi_level = tf.minimum(5, tf.maximum(      392             2, 4 + tf.cast(tf.round(roi_level), tf.int32)))    ~/opt/anaconda3/envs/Mask_RCNN/mrcnn/model.py in log2_graph(x)      339 def log2_graph(x):      340     """"""Implementation of Log2. TF doesn't have a native implementation.""""""  --> 341     return tf.log(x) / tf.log(2.0)      342       343     AttributeError: module 'tensorflow' has no attribute 'log'    "
"While I have encountered the problem that is mentioned in the title, I tried to run the whole process again to ensure there is nothing else wrong.     1) I uninstall and re-download Anaconda, as well the Mask_RCNN  2) I run requirements.txt and setup.py as is needed.  3) I run the train_shapes.ipynb code.   4) First, it downloads the coco model, and it runs until we get to the training cell.     4i) Error: 'Model' object has no attribute 'metrics_tensors'. Solved by downgrading Keras to 2.1.0.     4ii) Error: module 'tensorflow' has no attribute 'random_shuffle' Solved by updating tensorflow to 1.13.     4iii) RuntimeError: generator raised StopIteration    This error seems to cannot be overcome. I have thoroughly investigated it, though. First of all, it comes from the model.train() function. In the model code, train_generator and val_generator are defined through data_generator() function which loads the images. But, through printing, I have concluded that the code does not go into data_generator() function, thus it does not load images,  and as a result, Keras has nothing to train, leading to StopIteration.    I discovered this issue while I was training my own dataset, but I assumed there was a problem with my code, so I tried it on the sample code by first deleting everything.     Thanks for your time."
"Hello everyone, I'm opening this issue cause I did not see something similar in others yet. Well I saw one which has been solved by modifying the dataset but I checked and it's not my case.    When I'm running the training I get stuck here:  .  .  .mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)  .  .  Epoch 1/5    I used the balloon code and adapted it for my own dataset, as I said I checked labels and masks generated, it's all good. I don't think the issue comes from the dataset.    NUM_CLASSES                    2 (mine + bg)  GPU_COUNT                      1  IMAGES_PER_GPU                 1  (As I'm training on CPU, I also set use_multiprocessing to false)    Tensorflow 1.13  Keras 2.0.8  Windows 10 64Bits  Python 3.6.8  CPU: i7-6700    I do not have any other warning or hint given by the console ... It's just stuck. My CPU isn't charged, so it's probably not training. Let me know if you need any additional information ... And thanks for helping."
"I have trained a MaskRCNN model on three class (Plus BG class). With Tensorflow 1.15.0 model.detect() returns at max one instance of each class. When I downgraded to Tensorflow 1.14.0 , model.detect() returns multiple instances of each class, as expected.  Why could this be happening?"
"Good morning! I would tell you that there's an inconsistency in JSON annotation formatting from various tools. I tried to load your JSON sample from balloon dataset, but it said the file is corrupt, even it is readable by this project, and I found out that your JSON annotation format parsing does not support current VIA custom generated JSON file from anywhere because of different values and formatting.     You need to solve this problem, make sure this project supports JSON annotation from any sources. Your formatting might be outdated.    I will appreciate your concerns that I can't train my own model right now due to image annotation problems."
None
"Same here.  Posted   yesterday, have a look.    _Originally posted by @noamnav in        Excuse me.  Now that I saw your answer.  I found my problem.  The lock is in dataset validation.  In one of the images the mask point has dropped to the edge of the image exactly.  And that created the problem."
None
"Trying to follow the Shapes sample,  I cannot get past Epoch 1/1 - the script seems to be running, however no sign of processing and task is never completed.       When examining the task manager, I have noticed GPU is at ~0% (see below). However all signs are the GPU is tuned and should be utilized by tensorflow. The following is received in the terminal:    _`_2019-10-07 21:24:16.026724: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Quadro K1100M, pci bus id: 0000:02:00.0, compute capability: 3.0)`_    Spec as follows:  Win 10  Python 36  keras 2.08  Tensorflow 1.5  GPU Quadro K1100M    Any ideas please?  Should the GPU appear very active whilst training?  If you have followed this sample, can you please post your terminal script for the training part?    !     !     thanks"
"Any one suggest me please, How can I use drop out layer in MaskRCNN?  @waleedka @moorage "
"I train a new model based on your implementation and the compile method is not even changed.  I am sure that my training labels did not exceed the number of categories. I note that even when the very step that the total loss gives nan, the other loss seems to be normal.     Epoch 2/40     610/1000 [=================>............] - ETA: 9:38 - loss: nan - rpn_class_loss: 0.4507 - rpn_bbox_loss: 0.3960 - mrcnn_class_loss: 0.0864 - mrcnn_bbox_loss: 0.2338 - mrcnn_mask_loss: 0.4258"
"I have been ruuning this technique for my own dataset and its been a while since im using it. Im trying to incorporate the mAP function to have a better understanding than just getting val_loss and other errors which are usually overfitted with my data and one can not have a clear understanding of the results.     I'm following nucleus example to show mAP as it is called with compute_batch_AP but when I tried to run I get the error shown below. Due to some reason, my values are 0 and i have no idea why and then 5 image return me this error.     If someone follows this, I would like to know a better way. I have tried the available source example train_shapes but still I get 0 as a result. For now, my trainig images are 112 and validation are 31. I know the dataset is small but with few augmentation,, I believe it goes above 400~500 after image augmentation.     Any other suggestions or help would be appreciatable. Is there other way to show mAP after every epoch would also be one really good way and again I have tried @gaborvecsei code in the issue #1024 available but I couldn't solve it.    Thanks for any help or pointers to get me in right direction.   !   "
I need exact block diagram to understand the model file.. please help me
"I am using maskrcnn for  the detection of oil slicks on the sea surface using radar images  In fact, I trained Maskrcnn on a dataset of SAR images(tif).  Everything works fine for the training. However, I have some issues to run the prediction on big images. When I give as input small images (2000,2000), it works. However, when I try bigger sized  images, for instance, (8000 ,5000), it generates the error below:   Resource exhausted: OOM when allocating tensor with shape[1,64,7264,7264] .Kind regards  I am trying to solve this by dividing the images into crops and running the prediction on every crop,  then reconstructing the whole result of the image. The problem is that I don't know how to reconstruct the hole result:  for each crop I get the dictionary of the below arrays.  The dict contains -masks: [H, W, N] instance binary masks.  -rois: [N, (y1, x1, y2, x2)] detection bounding boxes  -class_ids: [N] int class IDs  -scores: [N] float probability scores for the class IDs  I manage to reconstruct the mask as I know the position of the crop in the big image.   However, for the rest of the dictionary elements, I can't figure out how to group them properly in one array each with respect to the original large image.   Could you please give me more insight on the above issues? I thank you a lot in advance for your collaboration.     "
None
"when trying to learn from the train_shapes file, the error 'Model' object has no attribute 'metrics_tensors'. How to handle this?    !   "
Is it possible to customize and use Mask-RCNN model on Classification dataset?  Dataset format: Image and corresponding class.
"Hello,  I modified and tried running evaluation from the provided coco.py with multiple images . However, during debugging, I found that the detection results were similar to each other (copies). I am not sure whether it is a problem from the model or pre-processing steps. Is there anyone who experienced similar problem?"
None
"Hi all, I'm trying to re-train the Mask RCNN (starting with the COCO weights). My goal is for the model to recognize two new classes. The instances of one class are rectangular but have rounded edges, while the instances of the other class are also rectangular but have sharp edges. All images of the rounded-edge class have the dimensions 200 x 279, and all images of the sharp-edge class have the dimensions 200 x 292. Per class, my annotations are identical for all of the images containing an instance of that class because the images of that class all have the same dimensions and because an instance always takes up the entire image (with the exception of the edges for instances of the rounded-edge class).      I have 2000 images per class, but I will need to train using less images because I only have one 1080Ti (11GB) and an i7-8700K. I'm on Windows 10 and using `tensorflow-gpu = 1.14.0`, `Keras = 2.2.5`, and `CUDA = 10.0`. I have not been able to make it past epoch 1/30 when training, _even after running for two hours_. I've noticed that my GPU RAM fills up to 9.4GB consistently and my CPU usage spikes, but my GPU usage does not spike and instead stays under 2% or so. Though I've changed the configurations frequently, this is what I currently have:     I'm not attached to these configurations in any way, as I have just been experimenting in hopes of reducing training time.    Does anybody know if I should be witnessing a spike in GPU usage while training, rather than a spike in only GPU RAM? If so, does the problem seem to be my configurations, library issues, or am I just hitting the limits of my system? One final thing to note is that I do have a few `Tensorflow` and `numpy` deprecation warnings, seeing as this repo needs to have a few function calls updated. Any guidance would be deeply appreciated."
None
None
"Hello everybody,    I am working on an instance segmentation task with very few objects that can be placed in a box. I've already trained a model which works fine in most cases. However I get the problem of wrong object detections in an empty box/image.    To adress this problem I want to retrain the model with some additional empty box images to adapt the model to this particular case.    However as far as I dug into the issue, the training procedure does not consider images with no annotation attached to it. The following lines are an extract of an .json annotation file I use. As you can see the first image has an ""annotation"" attached to it whereas the second doesn't. Thus the second image will not be considered during training.    Does anyone know how to use the code of this repo to train image samples that do not contain an object? Can I make it work by changing the annotation file or do I need to change the code? If I have to change the code how deep do I have to go into the details of the training process?  I'd appreciate any solution or usefull comment adressing this problem. Thank you!    {""images"": [{""id"": 1, ""file_name"": ""Train001520.tif"", ""width"": 768, ""height"": 576, ""date_captured"": ""2019-07-12 19:49:23.068417"", ""license"": 1, ""coco_url"": """", ""flickr_url"": """"}, {""id"": 2, ""file_name"": ""Train001521.tif"", ""width"": 768, ""height"": 576, ""date_captured"": ""2019-07-12 19:49:23.068417"", ""license"": 1, ""coco_url"": """", ""flickr_url"": """"}], ""annotations"": [{""id"": 1, ""image_id"": 1, ""category_id"": 1, ""iscrowd"": 0, ""area"": 28630, ""bbox"": [240.0, 115.0, 315.0, 334.0], ""segmentation"": [[338.0, %... AND SO ON%  438.5, 286.0, 448.5]], ""width"": 768, ""height"": 576}], ""categories"": [{""supercategory"": ""banana"", ""id"": 1, ""name"": ""banana""}]}"
"Hi,  When I restart model training after loading last weights using model.find_last() my losses jump up   I use the following to load the weights from the last run  `model.load_weights(model.find_last(),by_name=True) `  `model.train(train, test, learning_rate=train_config.LEARNING_RATE / 10 , epochs=60, layers='all')`  !     In the image the circled point (epoch 30) is where I stopped my last run. When I restarted training the loss jumped up. Any ideas why that might be happening?  Thanks,  Amit  "
"Hello, as far as I know, there are functions `plot_overlaps` and `plot_precision_recall` from `visualize.py` that support us draw precision-recall curve and grid of ground truth objects, but only for each image.  But now I have a dataset just for testing, and I want to draw precision-recall curve and grid of ground truth of this whole testing dataset for evaluated. So, is there anybody could help me with that problem?  Here is my code to calculate mAP for this whole dataset     "
"Traceback (most recent call last):    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1356, in _do_call      return fn(*args)    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1341, in _run_fn      options, feed_dict, fetch_list, target_list, run_metadata)    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1429, in _call_tf_sessionrun      run_metadata)  tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.    (0) Invalid argument: Input to reshape is a tensor with 2048 values, but the requested shape requires a multiple of 204800    [[{{node tower_0/mask_rcnn/mrcnn_class_bn1/Reshape_1}}]]    [[tower_1/mask_rcnn/proposal_targets/strided_slice_54/_2135]]    (1) Invalid argument: Input to reshape is a tensor with 2048 values, but the requested shape requires a multiple of 204800    [[{{node tower_0/mask_rcnn/mrcnn_class_bn1/Reshape_1}}]]  0 successful operations.  1 derived errors ignored.    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""coco.py"", line 507, in        augmentation=augmentation)    File ""/tmp/Mask_RCNN/mrcnn/model.py"", line 2419, in train      use_multiprocessing=True,    File ""/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 2177, in fit_generator      class_weight=class_weight)    File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 1849, in train_on_batch      outputs = self.train_function(ins)    File ""/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py"", line 2475, in __call__      **self.session_kwargs)    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 950, in run      run_metadata_ptr)    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1173, in _run      feed_dict_tensor, options, run_metadata)    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _do_run      run_metadata)    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1370, in _do_call      raise type(e)(node_def, op, message)  tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.    (0) Invalid argument: Input to reshape is a tensor with 2048 values, but the requested shape requires a multiple of 204800    [[node tower_0/mask_rcnn/mrcnn_class_bn1/Reshape_1 (defined at /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1891) ]]    [[tower_1/mask_rcnn/proposal_targets/strided_slice_54/_2135]]    (1) Invalid argument: Input to reshape is a tensor with 2048 values, but the requested shape requires a multiple of 204800    [[node tower_0/mask_rcnn/mrcnn_class_bn1/Reshape_1 (defined at /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1891) ]]  0 successful operations.  1 derived errors ignored.    Errors may have originated from an input operation.  Input Source operations connected to node tower_0/mask_rcnn/mrcnn_class_bn1/Reshape_1:   tower_0/mask_rcnn/mrcnn_class_bn1/batchnorm/add_1 (defined at /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1853)    Input Source operations connected to node tower_0/mask_rcnn/mrcnn_class_bn1/Reshape_1:   tower_0/mask_rcnn/mrcnn_class_bn1/batchnorm/add_1 (defined at /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1853)    Original stack trace for 'tower_0/mask_rcnn/mrcnn_class_bn1/Reshape_1':    File ""coco.py"", line 458, in        model_dir=args.logs)    File ""/tmp/Mask_RCNN/mrcnn/model.py"", line 1877, in __init__      self.keras_model = self.build(mode=mode, config=config)    File ""/tmp/Mask_RCNN/mrcnn/model.py"", line 2105, in build      model = ParallelModel(model, config.GPU_COUNT)    File ""/tmp/Mask_RCNN/mrcnn/parallel_model.py"", line 37, in __init__      merged_outputs = self.make_parallel()    File ""/tmp/Mask_RCNN/mrcnn/parallel_model.py"", line 81, in make_parallel      outputs = self.inner_model(inputs)    File ""/usr/local/lib/python3.6/dist-packages/keras/engine/topology.py"", line 617, in __call__      output = self.call(inputs, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/keras/engine/topology.py"", line 2078, in call      output_tensors, _, _ = self.run_internal_graph(inputs, masks)    File ""/usr/local/lib/python3.6/dist-packages/keras/engine/topology.py"", line 2229, in run_internal_graph      output_tensors = _to_list(layer.call(computed_tensor, **kwargs))    File ""/usr/local/lib/python3.6/dist-packages/keras/layers/wrappers.py"", line 208, in call      y = K.reshape(y, (-1, input_length) + output_shape[2:])    File ""/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py"", line 1891, in reshape      return tf.reshape(x, shape)    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 7715, in reshape      ""Reshape"", tensor=tensor, shape=shape, name=name)    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper      op_def=op_def)    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func      return func(*args, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3616, in create_op      op_def=op_def)    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 2005, in __init__      self._traceback = tf_stack.extract_stack()    Changed this part of code.                                                                 if callable(config.BACKBONE):              _, C2, C3, C4, C5 = config.BACKBONE(input_image, stage5=True,                                                  train_bn=config.TRAIN_BN)          else:              _, C2, C3, C4, C5 = resnet_graph(input_image, config.BACKBONE,                                               stage5=True, train_bn=config.TRAIN_BN)          **# Top-down Layers          # TODO: add assert to varify feature map sizes match what's in config          P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c5p5')(C5)          P4 = KL.Add(name=""fpn_p4add"")([              KL.UpSampling2D(size=(2, 2), name=""fpn_p5upsampled"")(P5),              KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c4p4')(C4)])          P3 = KL.Add(name=""fpn_p3add"")([              KL.UpSampling2D(size=(2, 2), name=""fpn_p4upsampled"")(P4),              KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c3p3')(C3)])          #P2 = KL.Add(name=""fpn_p2add"")([          #    KL.UpSampling2D(size=(2, 2), name=""fpn_p3upsampled"")(P3),          #    KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c2p2')(C2)])          # Attach 3x3 conv to all P layers to get the final feature maps.          #P2 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=""SAME"", name=""fpn_p2"")(P2)          P3 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=""SAME"", name=""fpn_p3"")(P3)          P4 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=""SAME"", name=""fpn_p4"")(P4)          P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=""SAME"", name=""fpn_p5"")(P5)          #P6 is used for the 5th anchor scale in RPN. Generated by          # subsampling from P5 with stride of 2.          #P6 = KL.MaxPooling2D(pool_size=(1, 1), strides=2, name=""fpn_p6"")(P5)**            # Note that P6 is used in RPN, but not in the classifier heads.          **#rpn_feature_maps = [P2, P3, P4, P5, P6]          rpn_feature_maps = [P3, P4, P5]          mrcnn_feature_maps = [P3, P4, P5]          #mrcnn_feature_maps = [P2, P3, P4, P5]**    Inside the PyramidROIAlign Class    for i, level **in enumerate(range(3, 6)):** (from 2 to 3 for fpn layers p3,p4,p5)    Custom anchors are provided in config file as         BACKBONE_STRIDES = [4, 8, 16]        # Size of the fully-connected layers in the classification graph      FPN_CLASSIF_FC_LAYERS_SIZE = 1024        # Size of the top-down layers used to build the feature pyramid      TOP_DOWN_PYRAMID_SIZE = 256        # Number of classification classes (including background)      NUM_CLASSES = 1  # Override in sub-classes        # Length of square anchor side in pixels      # RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)      RPN_ANCHOR_SCALES = (32, 64, 128)        #ANCHORS for 608x608 image -> 608/4 , 608/8 , 608/16 scales      RPN_ANCHOR_SCALES_W = (132,81,128,21,31,15,5,3,1)      RPN_ANCHOR_SCALES_H = (131,126,50,61,33,44,14,7,3)        Input_image_size = (608, 608)      I don't want to use P2 and P6 fpn features.     Can someone please help asap?                    "
"My original image is of shape 892 X 643 and the predicted bounding box is for the resized image 1024 X 1024. How do I get the bounding box for the original image?  If it is not possible, then do I need to train with IMAGE_RESIZE =""none"" ?"
"We are digging into the problem that there is a slight difference between the detection results of the model served on-line and the one we used to detect off-line, which should be the same, because the online model is saved by calling `model.keras_model.save_weights` of the offline model.    It comes out that each time `save_weights` is called, a different .h5 file is gotten, checking with md5sum.    Is it a common issue? And is it related to the difference between detection results?"
"I want to change the backbone from ResNet to ResNeXt, so I can't directly use the pre-trained model, how can I train from scratch? Thank you!"
"How does the below lines in balloon.py and coco.py work?     from mrcnn import **model** as **modellib, utils**    class BalloonDataset(**utils.Dataset**):            ....     **Should it be?**    from mrcnn import **model, utils** as **modellib, utils**    class BalloonDataset(**utils.Dataset**):         "
"Hello,     I'm trying to use this module in Google Colab.  So I successfully installed the module with the following commands.    ` !git clone    `!pip install -r 'Mask_RCNN/requirements.txt' `  `!cd Mask_RCNN ; python setup.py install`    Then `!pip show mask-rcnn` works, but when I tried to import mrcnn it said 'No Module Found'  !     Can someone help me out with this problem? Thank you so much."
"During ""inference"" mode, the PyramidROIAlign is called twice for Mask head. While in ""training"" mode, the PyramidROIAlign is called once.    Does this impact the inference‘s performance？    in def build(self, mode, config):"
"I generated the frozen graph from        But when I load my frozen graph and run it with a picture to inference, the detection result is different than I directly run the model.    For example, I directly run the model I can have detentions as the following.        But when I run my frozen graph I will have the following results, the fifth and sixth are different than the original model.     detections[:,4] [1. 1. 1. 3. 3. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.    0. 0. 0. 0.]    I checked the bounding box for the first 4 detentions are similar, but the frozen graph has 2 different ones at the fifth and sixth, any ideas why frozen graph will generate different detection results, and how to solve it?    Thanks"
"I want to display a subset of the coco classes during inference on an image e.g. if I want to display cars *(class id: 3 as per `demo.ipynb`)* or persons *(class id: 1)* only, what changes should I make in `visualize.py` (  function)?"
"I have a problem with train input image shape (train with  pre-trained model),  First I was saw that height and width should be Multiples of 64.  original image size is (h=1180,w=754), so i resized image to 1216,768. but it failed.    Next thing i did was, to resize image to 1024,1024 because I thought weight shape is suitable for   square(h=w image) .  but  i can't solve it. why did it accured error?    it's error message. thank you.    Traceback (most recent call last):    File ""C:/Users/sanghoon/AssIstoon/sfx_extract/train_test.py"", line 514, in        ""mrcnn_bbox"",    File ""C:\Users\sanghoon\AssIstoon\sfx_extract\mrcnn\model.py"", line 2133, in load_weights      saving.load_weights_from_hdf5_group_by_name(f, layers)    File ""C:\Users\sanghoon\Anaconda3\envs\tf16\lib\site-packages\keras\engine\saving.py"", line 1149, in load_weights_from_hdf5_group_by_name      str(weight_values[i].shape) + '.')  ValueError: Layer #2 (named ""conv1""), weight   has shape (7, 7, 1024, 64), but the saved weight has shape (64, 3, 7, 7)."
"I have trained a MaskRCNN and it's doing well, but it's not able to identify all the objects in my dataset. I have 20K images, but it's not able to overfit. Thus, I am looking for help choosing parameters, as I do have objects of various sizes.    The image size is 100 x 400. Here are my current parameters:       Since the performance isn't perfect, I ran the tutorial in the `inspect_data.ipynb` notebook. I ran the cell to show me the Anchors, and got this result:    !     To me, it seems like there aren't enough anchors here, compared to the demo in the notebook. However, no matter how I change my anchor scales parameters, it doesn't change.    Does anyone have an idea of if my parameters are good or I should change them?    Thanks"
"While finetuning what would be the best layers to select? But usually you would want to train lower layers (if top layer is input): assuming that higher levels learnt some general features, and you want to fine tune lower layers with some additional specific samples.   Which of those is lower: heads, 3+, 4+ maybe something else?"
"@waleedka     while using the demo,ipynb , when i run the inference using 2 GPU's it works fine , but when i scale that to 4GPUS i get        InvalidArgumentError: Input to reshape is a tensor with 600 values, but the requested shape has 4800       [[{{node tower_0_4/mask_rcnn/mrcnn_detection/Reshape_1}} = Reshape[T=DT_FLOAT,       Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]       (tower_0_4/mask_rcnn/mrcnn_detection/packed,       tower_0_4/mask_rcnn/mrcnn_detection/Reshape_1/shape)]]       Any ideas?  Thanks    "
"I am getting an issue with line no 734   `recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)`    What happens if there is actually no object instance on an image, but my model is predicting an object? gt_bbox will be empty , hence len(gt_match) would be zero. This division by zero will fail. I am actually facing this problem, where I get the error message. I have an image in the test folder, but it has no object instance for the class which I am training for.    > \mrcnn\utils.py:734: RuntimeWarning: invalid value encountered in true_divide    Thanks!  Amit  "
"I have converted the .h5 file to .pb file using         and converted .pb file to .pbtxt file using         after then I have a .pb file and .pbtxt file yes, so I used your blogpost         I have used above generated .pb and .pbtxt to produce masked result image so I am getting the error like this :    python3 mask_rcnn.py --mask-rcnn mask-rcnn-coco --image /home/deepedge/mask_rcnn-master/mask-rcnn-bottle-training-master/dataset/val/car39.jpg    [INFO] loading Mask R-CNN from disk...  [libprotobuf ERROR /io/opencv/3rdparty/protobuf/src/google/protobuf/text_format.cc:288] Error parsing text-format opencv_tensorflow.GraphDef: 60540:5: Unknown enumeration value of ""DT_RESOURCE"" for field ""type"".  Traceback (most recent call last):  File ""mask_rcnn.py"", line 48, in  net = cv2.dnn.readNetFromTensorflow(weightsPath, configPath)  cv2.error: OpenCV(4.1.0) /io/opencv/modules/dnn/src/tensorflow/tf_io.cpp:54: error: (-2:Unspecified error) FAILED: ReadProtoFromTextFile(param_file, param). Failed to parse GraphDef file: mask-rcnn-coco/dent.pbtxt in function 'ReadTFNetParamsFromTextFileOrDie'    How to solve?    please help  @bendangnuksung @moorage @waleedka @PavlosMelissinos @haeric @Scitator @akTwelve @gakarak "
"Hi there,  How to visualize instance segmentation from the balloon sample?  thanks!"
"The train.log:     I have tried to reduce LR, but it make no difference.  And I believe 'num_classes = 1 + 1 ' is correct.  Could anyone tell me why?  Thanks a lot."
"My System:   RTX2060   CUDA10.0  tenorflow-gpu 1.12   python 3.7    I have two question as follow:  1.I run demo.py and detection time is 8s, I want to know it's normal or too slow?  Because,I have other gpu(GTX950M) is also 8s, but I don't know which part I don't setting.    2.When I run the demo.py,I find tje detection time is more and more, first time is 8s , 5th is 11s,10th is 18s , so anyone know I need any code or other reason??    ### demo.py code is following:    import os  import sys  import random  import math  import numpy as np  import skimage.io  import matplotlib  import matplotlib.pyplot as plt  import time  from tensorflow.python.client import device_lib  print(device_lib.list_local_devices())  os.environ['CUDA_VISIBLE_DEVICES'] = '0'  starttime=time.time()    ROOT_DIR = os.path.abspath(""../"")  sys.path.append(ROOT_DIR)  # To find local version of the library  from mrcnn import utils  import mrcnn.model as modellib  from mrcnn import visualize  sys.path.append(os.path.join(ROOT_DIR, ""samples/coco/""))  # To find local version  import coco  MODEL_DIR = os.path.join(ROOT_DIR, ""logs"")  COCO_MODEL_PATH = os.path.join(ROOT_DIR, ""mask_rcnn_coco.h5"")    if not os.path.exists(COCO_MODEL_PATH):       utils.download_trained_weights(COCO_MODEL_PATH)  IMAGE_DIR = os.path.join(ROOT_DIR, ""images"")  class InferenceConfig(coco.CocoConfig):      GPU_COUNT = 1      IMAGES_PER_GPU = 1    config = InferenceConfig()  config.display()  model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)  model.load_weights(COCO_MODEL_PATH, by_name=True)  class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',                 'bus', 'train', 'truck', 'boat', 'traffic light',                 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',                 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',                 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',                 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',                 'kite', 'baseball bat', 'baseball glove', 'skateboard',                 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',                 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',                 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',                 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',                 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',                 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',                 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',                 'teddy bear', 'hair drier', 'toothbrush']  file_names = next(os.walk(IMAGE_DIR))[2]  image = skimage.io.imread(os.path.join(IMAGE_DIR, random.choice(file_names)))  results = model.detect([image], verbose=1)  r = results[0]  visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'],                               class_names, r['scores'])  taketimes=time.time()-starttime  print(taketimes)      ### ouput result is following:    [name: ""/device:CPU:0""  device_type: ""CPU""  memory_limit: 268435456  locality {  }  incarnation: 1420536741638677088  , name: ""/device:GPU:0""  device_type: ""GPU""  memory_limit: 4857462784  locality {    bus_id: 1    links {    }  }  incarnation: 15860079568509747610  physical_device_desc: ""device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5""  ]    Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.7  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 1  IMAGE_CHANNEL_COUNT            3  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                93  IMAGE_MIN_DIM                  800  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           coco  NUM_CLASSES                    81  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  PRE_NMS_LIMIT                  6000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                1000  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001      Processing 1 images  image                    shape: (626, 640, 3)         min:    0.00000  max:  255.00000  uint8  molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  151.10000  float64  image_metas              shape: (1, 93)               min:    0.00000  max: 1024.00000  float64  anchors                  shape: (1, 261888, 4)        min:   -0.35390  max:    1.29134  float32  9.030733346939087       ### And,Config is default        please help me  thank you"
"`  Epoch 1/30  E0701 12:48:29.641799 12024 model.py:1810] Error processing image {'id': 'D1DF8BDF-8223-4788-9C47-6720714FD9F2-2.png', 'source': 'receipt', 'path': './data/train\\D1DF8BDF-8223-4788-9C47-6720714FD9F2-2.png', 'width': 2448, 'height': 3264, 'polygons':      This happens for a lot of files in the dataset.  Why is this happening?    "
"The prediction is made according to Mask_RCNN/samples/demo.ipynb. I iterated the part of 'model.detect([image], verbose=1)' using the same picture and used time.time() to test the time cost on single iteration.  I found that it took around 3s to predict single image no matter it is the first or second iteration. Is there any way to reduce the prediction time?   I used 'nvidia-smi -l 1'  to monitor the GPU(Tesla P100), and found no running processes. The memory-usage is also 0MiB/16276MiB. Is there anything wrong with my config?"
When running the source code in demo.ipynb     An error occurs:  `TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type float64 of argument 'x'.`    **My env:**  - Ubuntu 1604 LTS  - CUDA 9.0  - cudnn 7.1  - tensorflow-gpu 1.12.0  - Keras 2.2.4    - GPU 1080Ti
"It looks like my region proposal network is biased to predicting regions along image edges. Any ideas what could be causing this?     !     Eventually non-max suppression and the region classification stage filters these out but they bias my final mask detections so that a lot of objects are missed.  !       Below are my configs, any help is appreciated!         "
"my c++ code  input the x,y=0,  I didn't know what's wrong, can someone help me ?   "
"As the titles says, How can I make it?    Now every new train will create a folder with a name contains timestamp, I don't want it , I just want  to save the model.h5 into a fixed path, so I can load it next time, without changing the directory."
"I train a model and try to  run test on it ,the model seems good because the bounding boxs have high confidence. But I found  the AP is zero.  so I run  ""Predicted Masks"" code to see the mask:  `# Get predictions of mask head  mrcnn = model.run_graph([original_image], [      (""detections"", model.keras_model.get_layer(""mrcnn_detection"").output),      (""masks"", model.keras_model.get_layer(""mrcnn_mask"").output),  ])`  the output is :  detections               shape: (1, 100, 6)           min:    0.00000  max:    6.00000  float32  masks                    shape: (1, 100, 28, 28, 9)   min:    0.00000  max:    0.99173  float32    It seems no problem,But When I do next step:  `# Masks  det_boxes = utils.denorm_boxes(mrcnn[""detections""][0, :, :4], original_image.shape[:2])  det_mask_specific = np.array([mrcnn[""masks""][0, i, :, :, c]                                 for i, c in enumerate(det_class_ids)])  det_masks = np.array([utils.unmold_mask(m, det_boxes[i], original_image.shape)                        for i, m in enumerate(det_mask_specific)])  log(""det_mask_specific"", det_mask_specific)  log(""det_masks"", det_masks)`    the output:    det_mask_specific        shape: (6, 28, 28)           min:    0.00000  max:    0.51804  float32  det_masks                shape: (6, 1024, 1024)       min:    0.00000  max:    0.00000  bool    the det_masks is all 0,I think this is the reason why AP is 0 . Maybe the number in the mask all smaller than the threshold in unmold_mask function calls the problem , but the max number in det_mask_specific  is  0.51804   ,bigger than the     threshold  which is 0.5, so I don't konw what's worng.   I just  train a few epoch ,and II only have few images ,maybe mrcnn_mask_loss is still  high?"
Is there any way to lower processing time for images? Right now it's taking about 6 seconds per image from my webcam. I would like to do object-detection from a video feed at 4-5fps minimum. 
I am working on building footprint detection with the use of Mask_RCNN. I have completed the training process and testing with different images. Now my doubt is how to perform post-processing step to improve the quality of masks.     Something like shown in below image  !     Can anyone help me with this? Thanks in advance😁
"New to this technique and deep learning in general.    After training my dataset and running a prediction on a new image, how can I retrieve pixel positions of the region identified by the mask in the image?    Another question is when training my data, how can I make it such that only 1 line is printed to console per epoch? I know that in keras there is a model.fit() option where you can specify the verbose level but I don't see that function in the utils here."
"Hi,    I want to train a model on 5 classes, I don't exactly know in which shape to return my mask from the `load_mask()` function. Currently my mask shape looks like this:    `mask.shape`  (1080, 1618, 5)    where every pixel/3rd dimension is mapped to a 5 long array that corresponds to one of the 5 classes.     In my `class_ids` I put an array with the classes that are found in that image, like:  `array(     However when I train I get the error:     This for example is an image that contains objects that belong to 3 classes of the 5. So I'm guessing the 3rd dimension of `mask` always needs to be as long as the `class_ids`. But can anyone tell what exactly I need to be putting in these 2 variables?"
I want to get the instance of people.  How can I do to train coco for one class(person)?
"Lets say I have two objects A and B. Both of these objects have a common region. For example, I am detecting railway tracks and the railway tracks intersect as shown in the following image.     !     I don't want to break either of the railway lines which intersect. This will lead to two railway lines have overlapping regions. I wanted to know if it is okay to have annotations like this for MaskRCNN."
"this result is right? I use the coco's net of  maskrcnn_coco.h5   and turn it to .pb file  and use the image in the file of the images ,but the output is  , why it is not print the values of the 'rois','class','scores'  and print the None value ?"
"Problems with visualization in the inspect_balloon_model file. I downloaded the dataset, python files, and the trained balloon model from the matterport release. These are some examples of the error with visualization that I am facing. There are no error messages that halt the program and it runs smoothly, just oddly. If anyone has faced this problem or knows how to solve it, please help! Thank you in advance :)    Splash image on the left and the right is the balloon identification.    <img width=""1388"" alt=""Screen Shot 2019-06-07 at 2 27 04 PM"" src=""     <img width=""2220"" alt=""Screen Shot 2019-06-07 at 2 28 10 PM"" src=""     <img width=""2009"" alt=""Screen Shot 2019-06-10 at 2 20 19 PM"" src=""     <img width=""2168"" alt=""Screen Shot 2019-06-10 at 3 58 58 PM"" src=""     "
"I am trying to train my model from 'last'. I have previously trained my model 'heads', now i wish to train 'all'. My code is similar to     I trained my 'heads' first using the initialization code similar to #362  followed by  `model.train(dataset_train,dataset_val,                  learning_rate=config.LEARNING_RATE,                  epochs=30,                  layers='heads')`    A while later in another file, I trained 'all' layers using the same initial code followed by   `model.train(dataset_train,dataset_val,                  learning_rate=config.LEARNING_RATE,                  epochs=30,                  layers='all')`  I also set `init_with = ""last""`.    OUTPUT     .  .  .  mrcnn_class_bn2        (TimeDistributed)  mrcnn_mask_conv4       (TimeDistributed)  mrcnn_mask_bn4         (TimeDistributed)  mrcnn_bbox_fc          (TimeDistributed)  mrcnn_mask_deconv      (TimeDistributed)  mrcnn_class_logits     (TimeDistributed)  mrcnn_mask             (TimeDistributed)      This model structure is the output. After this execution finishes without any error.  Training doesn't take place. Nor is a new weights file saved.  My code file is linked      "
"TypeError: list indices must be integers or slices, not str  Any ideas on how to fix this ?     !   "
"I'm trying to train MaskRCNN using custom data and want to monitor its performance the same way as it's possible for tensorflow-object-detection api using a command like ""tensorboard --logdir= "". Using the current logs from training I can see the graph of the network but no scalar or loss values. Is there any way that I can get those results in a nice way? "
"I am Trying to implement Nucleus.py    `Loading weights  C:\Users\Family\.keras\models\resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5`  `Traceback (most recent call last):`  `  File ""nucleus.py"", line 512, in  `  `    train(model, args.dataset, args.subset)`  `  File ""nucleus.py"", line 284, in train`  `    dataset_train.load_nucleus(dataset_dir, subset)`  `  File ""nucleus.py"", line 230, in load_nucleus`  `     image_ids = os.listdir(dataset_dir)`  `FileNotFoundError: [WinError 3] The system cannot find the path specified: '/path/to/dataset\\stage1_train'`      I am getting this error again and again while initiating training, even after placing files is specified folder. I have successfully run inspect_nucleus_model.ipynb. Any help would be appreciated.    Thank You."
"Hi, I am training this model with my own data- RGBD. But I am unable to apply image augmentation. Do the image augmentors works only on 3 channel images?    "
!       **Any ideas on how to fix this ?**  @waleedka @moorage @PavlosMelissinos @haeric @rymalia   Thank you
"I'll be using version 1 as version 2 seems incompatible with the `balloon.py` code.      But can I use the circle that gives (x,y) of the center and radius rather than a polygon?    I am trying to track a tennis ball in a TV tennis broadcast.    "
"I am using Mask_RCNN to localize the damaged region in the car. I have annotated my images using the VIA tool as used in the balloons example. Therefore, I used the same load_mask() function used in the balloon.py file but on running the inspect_data.ipynb file, I am being shown the error:    WARNING:root:You are using the default load_mask(), maybe you need to define your own one.  My load_mask() function is unable to override the parent function.  So, please help."
I don't see the `prepare()` method in `class ShapesDataset`.      What am I missing?
"Hi all,  I was wondering from which pixel point of the image does the bounding box coordinate start??    I mean if my r[""rois""] is lets say  [113,  84 , 117 , 93] (x1,y1,x2,y2). Which corner is the one that I start counting 113 pixels on x-axis and 84 on y-axis, in order to find the starting pixel of my bounding box?    How can I reverse the bounding box results in order to reverse the output 90 degrees counter-clockwise??    thanks          --          "
"Hello everyone,  I'm trying to use the Mask RCNN for my dataset, which is composed of pictures of documents with the objective of document segmentation. My dataset has 2 classes (background +1).  I customized the code present in the file `train_shapes.ipynb `for the train with my dataset.    The problem is:  When I try to train with my dataset, into` model.train `function has a `while True: `that produce an infinite loop, preventing my train.    I need your help.    Thank you"
"In config.py,  Uncomment to train on 8 GPUs (default is 1)  GPU_COUNT = 1  but just only one GPU, how to training with GPU?"
"Hi,        For a few images, our polygon annotation points have gone beyond the height and width of the image. We were getting 'index out of bounds' error for these images, for which we clipped the x and y co-ordinate of the polygons. However this has led to the network loss becoming nan. How should we proceed ahead without ignoring the polygons whose points lie outside the image."
None
None
Sorry if I missed in the README but where is the original academic paper to cite?
"Hi there! I've been tuning the hyperparameters and I changed the alpha value (from 0.5 to 0.3) regarding this thread #1185 to make my masks more transparent. However the output mask is now coloring the complement of the object in the bounding box, instead of the object itself. Did anybody face any situation like that? I reverted the alpha value but it is still the same. "
"Hello I am using the code to detect objects and I need to deduct the center of the object. Using the bounding box gives a decent approximation but i have found that from time to time the box leaps away from the masks to side. The mask are more consistent so I am attempting to use the data used to give the mask its border. I have not been able to single out this data, would you be able to tell me if it is at all possible to retrieve the mask in x,y coordinates? How would i go about doing this?"
"Hi all,  I trained the model with own dataset and there is a problem while detecting. No matter how many instances in the test images, there is only one instance being detected, just like this    !       The mask is relatively accurate, however the count of bbox is apparently wrong, it includes all masks in the image.  Has any one ever encountered this problem?  THX!      Here are my configurations:  Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.7  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 1  IMAGE_CHANNEL_COUNT            3  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                14  IMAGE_MIN_DIM                  800  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           shapes  NUM_CLASSES                    2  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  PRE_NMS_LIMIT                  6000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                100  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           100  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001"
"I wanted to extract the features from the mask-RCNN model.Hence i changed the model.py as to return the features from the masked region.  I referred this code to make the changes.  [   Now this model.py file return features along with boxes, class_ids, scores, full_masks   in line no: 2483  Now in the after running the jupyter notebook demop.ipynb file when i try to print the results i the features are not being displayed when i print(results).    Do i need to build things again after changing the model.py file? or is there any mistake im doing?    Please help!."
"I am trying to run demo.ipynb, but I keep getting a resource exhausted error when I try to load the pre-trained weights. This is my config:    Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.7  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 1  IMAGE_CHANNEL_COUNT            3  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                93  IMAGE_MIN_DIM                  800  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           coco  NUM_CLASSES                    81  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  PRE_NMS_LIMIT                  6000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                1000  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001"
"I followed the example from  .   See a gist with my notebook code    I swapped out the class of nucleus with crack (for cracks in infrastructure). I added the my data in the same format:  I.E. (train/image1/images/Image1.png  train/image1/masks/m1.png ...) However when I train now it is unable to load my PNGs:    > ERROR:root:Error processing image {'id': 'image-289', 'source': 'crack', 'path': 'D:\\git_projects\\CrackSegmentation\\train\\image-289\\images/image-289.png'}  Traceback (most recent call last):    File ""c:\users\nick.purcell\appdata\local\continuum\miniconda3\envs\pocmaskrcnn\lib\site-packages\mask_rcnn-2.1-py3.7.egg\mrcnn\model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""c:\users\nick.purcell\appdata\local\continuum\miniconda3\envs\pocmaskrcnn\lib\site-packages\mask_rcnn-2.1-py3.7.egg\mrcnn\model.py"", line 1220, in load_image_gt      mask = utils.resize_mask(mask, scale, padding, crop)    File ""c:\users\nick.purcell\appdata\local\continuum\miniconda3\envs\pocmaskrcnn\lib\site-packages\mask_rcnn-2.1-py3.7.egg\mrcnn\utils.py"", line 508, in resize_mask      mask = scipy.ndimage.zoom(mask, zoom=[scale, scale, 1], order=0)    File ""c:\users\nick.purcell\appdata\local\continuum\miniconda3\envs\pocmaskrcnn\lib\site-packages\scipy\ndimage\interpolation.py"", line 595, in zoom      zoom = _ni_support._normalize_sequence(zoom, input.ndim)    File ""c:\users\nick.purcell\appdata\local\continuum\miniconda3\envs\pocmaskrcnn\lib\site-packages\scipy\ndimage\_ni_support.py"", line 65, in _normalize_sequence      raise RuntimeError(err)  RuntimeError: sequence argument must have length equal to input rank    This happens with multiple images and it ultimately fails.   Is it expecting a different input shape or something?    I am not sure where I went wrong"
None
None
"I'm running prediction using pretrained model, images coming from camera. I'm testing the program and after a minute or so the following error appears:    >   File ""mask_rcnn/main.py"", line 83, in execute_recognition      results = model.detect( ]]     ]]      Any ideas why and how to fix it?"
"can anyone please help me get a feature vector of the masked region? I'm really struggling to get it  I first referred to this Issue: #1249 and then #1190 i did all the changes mentioned there but i get errors like ""positional argument required ""roi_pooled_features"". I'm still stuck here for a month and any help would be really appreciated."
"Good day.  I am training my own dataset.  I use via 2.0.5  When I train, I get the error  File ""samples/bottles/bottle.py"", line 381, in   train(model)  File ""samples/bottles/bottle.py"", line 203, in train  dataset_val.load_bottle(args.dataset, ""val"")  File ""samples/bottles/bottle.py"", line 128, in load_bottle  polygons = [r['shape_attributes'] for r in a['regions']]  File ""samples/bottles/bottle.py"", line 128, in   polygons = [r['shape_attributes'] for r in a['regions']]  TypeError: string indices must be integers    PLEASE HELP!!!!!    My code is :    def load_bottle(self, dataset_dir, subset):  """"""Load a subset of the Balloon dataset.  dataset_dir: Root directory of the dataset.  subset: Subset to load: train or val  """"""  # Add classes. We have only one class to add.  self.add_class(""bottle"", 1, ""PET"")  self.add_class(""bottle"", 2, ""COLOR HDPE"")  self.add_class(""bottle"", 3, ""WHITE HDPE"")        # Train or validation dataset?      assert subset in [""train"", ""val""]      dataset_dir = os.path.join(dataset_dir, subset)        # Load annotations      # VGG Image Annotator (up to version 1.6) saves each image in the form:      # { 'filename': '28503151_5b5b7ec140_b.jpg',      #   'regions': {      #       '0': {      #           'region_attributes': {},      #           'shape_attributes': {      #               'all_points_x': [...],      #               'all_points_y': [...],      #               'name': 'polygon'}},      #       ... more regions ...      #   },      #   'size': 100202      # }      # We mostly care about the x and y coordinates of each region      # Note: In VIA 2.0, regions was changed from a dict to a list.      annotations = json.load(open(os.path.join(dataset_dir, ""via_region_data.json"")))      annotations = list(annotations.values())  # don't need the dict keys        # The VIA tool saves images in the JSON even if they don't have any      # annotations. Skip unannotated images.      annotations = [a for a in annotations if a['regions']]        # Add images      for a in annotations:          # Get the x, y coordinaets of points of the polygons that make up          # the outline of each object instance. These are stores in the          # shape_attributes (see json format above)          # The if condition is needed to support VIA versions 1.x and 2.x.          #if type(a['regions']) is dict:          polygons = [r['shape_attributes'] for r in a['regions']]          names = [r['region_attributes'] for r in a['regions']]                    #else:          #polygons = [r['shape_attributes'] for r in a['regions']]             # load_mask() needs the image size to convert polygons to masks.          # Unfortunately, VIA doesn't include it in JSON, so we must read          # the image. This is only managable since the dataset is tiny.          image_path = os.path.join(dataset_dir, a['filename'])          image = skimage.io.imread(image_path)          height, width = image.shape[:2]            self.add_image(              ""bottle"",              image_id=a['filename'],  # use file name as a unique image id              path=image_path,              width=width, height=height,              polygons=polygons,              names=names)    def load_mask(self, image_id):      """"""Generate instance masks for an image.     Returns:      masks: A bool array of shape [height, width, instance count] with          one mask per instance.      class_ids: a 1D array of class IDs of the instance masks.      """"""      # If not a balloon dataset image, delegate to parent class.      image_info = self.image_info[image_id]      if image_info[""source""] != ""bottle"":          return super(self.__class__, self).load_mask(image_id)            # Convert polygons to a bitmap mask of shape      # [height, width, instance_count]      info = self.image_info[image_id]      class_names = info[""names""]      mask = np.zeros([info[""height""], info[""width""], len(info[""polygons""])],                      dtype=np.uint8)      for i, p in enumerate(info[""polygons""]):          # Get indexes of pixels inside the polygon and set them to 1          rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])          mask[rr, cc, i] = 1      class_ids = np.zeros([len(info[""polygons""])])        for i, p in enumerate(class_names):          if p['class'] == 'PET':              class_ids[i] = 1          elif p['class'] == 'COLOR HDPE':              class_ids[i] = 2          elif p['class'] == 'WHITE HDPE':              class_ids[i] = 3      class_ids = class_ids.astype(int)      # Return mask, and array of class IDs of each instance. Since we have      # one class ID only, we return an array of 1s      return mask.astype(np.bool), class_ids    def image_reference(self, image_id):      """"""Return the path of the image.""""""      info = self.image_info[image_id]      if info[""source""] == ""bottle"":          return info[""path""]      else:          super(self.__class__, self).image_reference(image_id)"
"Getting this error on some images, any ideas?     coming from:   molded_images, image_metas, windows = self.mold_inputs(images)  ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (3,2) and requested shape (2,2)"
None
None
None
"I have installed pycocotools with ""pip install git+  however when I try to implement **python model_main.py** it gives me error of                      **ModuleNotFoundError: No module named 'pycocotools._mask'**    I have also tried    git clone    cd PythonAPI  python setup.py build_ext install    It gives error:    error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit status 2    I have installed C++ tools.     I have installed the pycocotools with cocoapi. Any kinds of help appreciated."
"Hey everyone,    i just trained my Mask R-CNN with grayscale images.   Now i want to visualize my result. When i use my code, which i used for rgb images, i get an error in visualize.py in the apply_mask function, because the dimensions aren't the same.   For visualization i used parts of waspinators code:       The function which i use and which cause the problem is display_instances in visualize.py.    This is the apply_mask function where the error occure:       My images have the shape (w,d,1) because they are grayscale. For this i edited the load_image function in utils.py like i supposed to do according to the wiki here.    Does somebody know how i can visualize my results?    In apply_mask i tried to avoid the problem by first delete the last dimension, so i can convert the image in rgb:         But then i get this error:       "
Hi I am trying to use the ROS node for mask rcnn but there seems to be a problem with the msg definition. When I request the info of the node in run time it returns that the visualisation has the type sensor_msgs/Image. The input should be the same but has an unknown type. How would I go about fixing this?    Thanks in advance.
"I trained a mask-rcnn model on a Linux machine with a 1080TI, everything went smoothly.  For some purpose, I deployed the model on a Windows machine with 32GB RAM and a 1080Ti GPU to do some instance segmentation, it keeps restarting the windows machine.  Anyone knows the reason?"
"Hi I am trying to combine the ROS node for the mask rcnn model with an industrial grayscale camera. I think i altered the code to the point where the data of the camera is being accepted by the model. I followed the steps in the wiki and also changed the encoding that the model expects. Now i am getting the following error:  `File ""/MYPC/cv_bridge/core.py"", line 171, in imgmsg_to_cv2 `  `       dtype=dtype, buffer=img_msg.data)`  `TypeError: buffer is too small for requested array`  I've been looking into the CvBridge documentation for a possibility to increase the buffer size  but can't find one. Would this better be solved at the camera end of the situation or do you possibly have a solution to remedy this problem?    Thanks in advance! "
"It's my first attempt to visualize model by tensorboard , I have fed several inputs to model ,but to failed display the model graph.....  this is  the inference code ,how should I change it?   "
"There are 80+1 classes in coco dataset, while you only get 2 classes. So when loading weights you should exclude some layers such as 'mrcnn_bbox_fc','mrcnn_class_logits’(fill the layer's name in the load_weights method), then start fine-tuning.    _Originally posted by @Alexlastname in      Hi Alexlastname    Maybe this is trivial but how can I applied what you said in the post? , I am new on training networks.     Thanks  "
"Hi, I'm curious how to calculate the mAPs for each class?   I'm checking the function `compute_ap` but it calculated mean average precision of total dataset."
When doing inference I see a CPU spike to 100% but no change for the GPU usage. How can I make sure MaskRCNN uses my GPU instead of CPU?
"After training my model, I used balloon.py's splash for a video, but then encountered an **error:**    OpenCV Error: Assertion failed (fps >= 1) in open, file /io/opencv/modules/videoio/src/cap_mjpeg_encoder.cpp, line 638  Traceback (most recent call last):    File ""balloon.py"", line 367, in        video_path=args.video)    File ""balloon.py"", line 250, in detect_and_color_splash      fps, (width, height))  cv2.error: /io/opencv/modules/videoio/src/cap_mjpeg_encoder.cpp:638: error: (-215) fps >= 1 in function open      Can anybody tell me why this error is occurring? I was able to successfully use the splash effect on my videos before, but this just happened now. Thank you.  "
It's taking around 4 seconds to infer a picture of about 1000X500 using 1080TI.   Are there any obvious ways I'm missing to speed up the inference? Exporting a frozen model?     Another question - would retraining the model for grayscale only improve the inference time?
"Hello,   I would like to know if someone has been able to train using an xml annotation file, or if someone has converted xml to json.  Can someone show me an example how to use the load_() function if I change my xml file to json?  I made some annotations with VIA tool and it was correct, but I would really like to use the xml annotations as they are done and it takes a long time to start over.  I would appreciate it."
"In the file of sample/coco/coco.py, the sample do not modify STEPS_PER_EPOCH, so that STEPS_PER_EPOCH = 1000. Is it sufficent to coco dataset? Shall I set STEPS_PER_EPOCH = num_samples_coco / IMAGES_PER_GPU / GPU_COUNT? "
"Hi,     I'm using Docker to manage the code environment. If you can help me to make it work, I'll be glad to release the image so that anyone can easily run the code without any installation issue. My Dockerfile is         where `startup.sh` is simply     `/bin/bash -c ""jupyter notebook --ip 0.0.0.0 --allow-root --no-browser --NotebookApp.token='foobar'""`    I build my image without major complaints: the only weird messages are     Other than that, the build process seems fine. Then I run the container, which simply starts a Jupyter notebook, and I launch `demo.ipynb`: the first cell gives me this error         I don't understand what's happening, could you please help me? Thanks in advance."
"Hi all,    I am currently trying to plot the precision-recall curve for my validation dataset @ 0.5 iou_threshold.    The utils.compute_ap() function is ideal to get the precision-recall curve for one image along with visualize.plot_precision_recall().    However it is not clear how to plot prediction-recall for multiple images.    The precision and recall arrays are both 4 elements each which is confusing, i thought they would be each a single value.    !     Precision is the first array, Recall is the second as returned by compute_ap()    My inital thought was to make a larger array of precisons and recalls from each image and then plot this array.     ~~However since precison and recall are not singular values this has confused me somewhat.~~  Edit: The middle 2 indexes of the array are the values of 2 different classes.    Hope someone can provide some clarity on this!"
"I trained until the third step appeared tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[200,14,14,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc.  How do I inherit config.py parameters?  "
None
"Hello everyone,     When training the model, I can see the losses going down.  I want to log these losses to a file over time so that I can draw a loss/accuracy graph.  I looked for some solutions on the internet, but they are all for model.fit() function.  Here we have our custom function mode.train().  So, I would like to ask, how can I add a call back function to save the loss over time?  Thanks!!    "
"Good day, I am trying to follow the   via this   where I used my own class, which detects roofs in the image. I just replaced the `balloon` class with `roof` so the configuration has only 2 classes (including the background). I annotated my own images using VIA v.2.0.5. However during training, I am getting the following error:    `Epoch 1/30  ERROR:root:Error processing image {'id': '0030_0040.png', 'source': 'roof', 'path': 'roxas-mask-rcnn/val/0030_0040.png', 'width': 255, 'height': 260, 'polygons': [{'name': 'polygon', 'all_points_x': [7, 42, 90, 14, 1, 1, 19], 'all_points_y': [107, 89, 175, 213, 186, 144, 135]}, {'name': 'polygon', 'all_points_x': [185, 231, 253, 254, 220], 'all_points_y': [142, 117, 157, 183, 202]}, {'name': 'polygon', 'all_points_x': [82, 119, 135, 92], 'all_points_y': [220, 202, 226, 250]}]}  Traceback (most recent call last):    File ""/home/deankarlo/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/deankarlo/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1275, in load_image_gt      source_class_ids = dataset.source_class_ids[dataset.image_info[image_id][""source""]]  KeyError: 'roof'`    Why is it looking for a key named `roof`? Is it supposed to look for `source` instead? Any help is appreciated. Thank you!"
     The same script runs well on the GPU machine.  tensorflow-gpu ==1.12.0  Keras == 2.2.4    using the same versions of tensorflow and keras on a local mac pro(the cpu machine I want to run the inference mode).    Any ideas?   Thanks.
"Hi All,    1 - Could you please help me what is the difference between demo.ipynb(for testing model) file and inspect_model.ipynb file.    2 - How I can test Images without Annotated json file."
GPU_COUNT = 2  IMAGES_PER_GPU = 2  when I ran ballon splash，it crashed ，and raised AssertionError: len(images) must be equal to BATCH_SIZE    len(images) = 1  BATCH_SIZE = 2
"I think the original code published does not account for data augmentation. If so, could anyone share a typical data augmentation (flip, rotate, etc.) function which we can use on mask_rcnn training script?"
Hi everybody.   So I have i.e. 10 different classes to predict with one class for one image. I've already trained network for first class with only images that contain this class and it works good. Now I want to train for second class with another images that contain exactly second class but want to keep model to predict first class.   How can I do it better? Do I really have to combine these two datasets and train model for 2 classes form begin? Give me please some advice.   Thanx.
During training one of the images cannot be processed:              
"When looking at:       we use `ix` as index to assign to rpn_bbox.  Since during the rpn bbox loss computation, we use the indices of the anchors where rpn_match is 1, I believe it should be `rpn_bbox[i]`, not `rpn_bbox[ix]`.    That might explain the issue reported here:       @waleedka @keineahnung2345 you might want to have a look as it would mean the box part of the RPN is not being trained at all ..."
"Hi,    I am trying to deploy a Mask RCNN variant to Google Cloud ML engine. To do this, I need to get the model ready to accept variable batch sizes for prediction.     If I take a look a the output layers:       I was able to set the batch dimension to None for all the output layers except 'ROI/packed_2:0'.        Can you help me figure out where to modify model.py such that the first dimension of ROI/packed_2:0 is None as well?     Thanks.    Best,  Noah"
"I am wondering what the purpose of the source parameter is in the add_image function?    In the balloon.py sample it is simply set to balloon.    This source is appended to self.images_from_source_map in the util prepare function.    `{'balloon.24631331976_defa3bb61f_k.jpg': 0, 'balloon.16335852991_f55de7958d_k.jpg': 1, 'balloon.14898532020_ba6199dd22_k.jpg': 2, 'balloon.8053085540_a72bd21a64_k.jpg': 3, 'balloon.6810773040_3d81036d05_k.jpg': 4, 'balloon.5603212091_2dfe16ea72_b.jpg': 5, 'balloon.5555705118_3390d70abe_b.jpg': 6, 'balloon.4838031651_3e7b5ea5c7_b.jpg': 7, 'balloon.4581425993_72b9b15fc0_b.jpg': 8, 'balloon.3825919971_93fb1ec581_b.jpg': 9, 'balloon.3800636873_ace2c2795f_b.jpg': 10, 'balloon.2917282960_06beee649a_b.jpg': 11, 'balloon.410488422_5f8991f26e_b.jpg': 12}`    For multi class application how would this source behavior change?     Example: Images with cats and dogs (some images just dogs some images just cats and some with both!)"
"I think it's pretty obvious to people but still deserves attention. Since,  1. should be. ""Clone repository"". Then  2. should be ""Install requirements"".     !   "
"After training on my own data, where are my weights stored so that I can apply them to select an object in an my image?"
As we know that the training of this Mask R-CNN implementation is carried out to the head part of the model. How to train the other part(s) of the model?    Thanks
"Hi,  I want to save the coordinates of every objects detected after the inference in a COCO style json file. So, that it can be used for further training.   May be I have to save the contours=find_contours() from visualize.py file. But how can I save in a json file like COCO style? Can anybody help?"
"Hi,  I would like to build a siamese mrcnn network where i copy the mrcnn model as the two branches of my siamese model. I have to wrap the mrcnn model as a sub model into my new main model.   I can do it in my toy model but when i try it with this implementation the sub model Inputs do not get value during the training    What i try:  `input_image = KL.Input(                  shape= ]]      My toy model that works:    `from __future__ import print_function  import keras  from keras.datasets import mnist  from keras.models import Sequential, Model  from keras.layers import *  from keras import backend as K  from keras.callbacks import TensorBoard    batch_size = 128  num_classes = 10  epochs = 12    # input image dimensions  img_rows, img_cols = 28, 28    # the data, split between train and test sets  (x_train, y_train), (x_test, y_test) = mnist.load_data()    if K.image_data_format() == 'channels_first':      x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)      x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)      input_shape = (1, img_rows, img_cols)  else:      x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)      x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)      input_shape = (img_rows, img_cols, 1)    x_train = x_train.astype('float32')  x_test = x_test.astype('float32')  x_train /= 255  x_test /= 255  print('x_train shape:', x_train.shape)  print(x_train.shape[0], 'train samples')  print(x_test.shape[0], 'test samples')    # convert class vectors to binary class matrices  y_train = keras.utils.to_categorical(y_train, num_classes)  y_test = keras.utils.to_categorical(y_test, num_classes)    def build_sub_model(input_shape):        #model = Sequential()      inpa = Input(input_shape, name=""sub_inpa"")      inpb = Input(input_shape, name=""sub_inpb"")            x = Conv2D(32, kernel_size=(3, 3),                      activation='relu')(inpa)      x = Conv2D(64, (3, 3), activation='relu')(x)      x = MaxPooling2D(pool_size=(2, 2))(x)      x = Dropout(0.25)(x)      x = Flatten()(x)      x = Dense(128, activation='relu')(x)      x = Dropout(0.5)(x)      x = Dense(num_classes, activation='softmax')(x)      model = Model([inpa, inpb], [x, inpb], name=""sub_model"")      print(model.summary())      print(model.inputs)      return model    def build_model():          sub_model = build_sub_model(input_shape)        inp2a = Input(input_shape, name=""inp2a"")      inp2b = Input(input_shape, name=""inp2b"")        inputs = [inp2a, inp2b]      sub_out = sub_model(inputs)      #print(len(x_inpb), x_inpb)      model = Model(inputs, sub_out)      print(model.summary())      print(model.get_layer(""sub_model"").inputs)      model.get_layer(""sub_model"").inputs = model.inputs      print(model.summary())      print(model.get_layer(""sub_model"").summary())      print(model.get_layer(""sub_model"").inputs)      for i in model.get_layer(""sub_model"").layers:          if isinstance(i, InputLayer):              print('sub', i, i.name, i.get_output_at(0))      print(model.inputs)      for i in model.layers:          if isinstance(i, InputLayer):              print('main', i, i.name, i.get_output_at(0))            return model    model2 =  build_model()    model2.compile(loss=[keras.losses.categorical_crossentropy, keras.losses.mean_squared_error],                 loss_weights=[1., 1.],                optimizer=keras.optimizers.Adadelta(),                metrics=['accuracy'])  callbacks = [               TensorBoard(log_dir='/home/fothar/cluster_segmentation_redesign/logs/xxx/',                                          histogram_freq=0, write_graph=True, write_images=False)          ]                  model2.fit([x_train, x_train], [y_train, x_train],            batch_size=batch_size,            epochs=epochs,            verbose=1,            callbacks=callbacks,            validation_data=([x_test, x_test], [y_test, x_test]))  score = model2.evaluate([x_test, x_test], [y_test, x_test], verbose=0)  print(len(score), score)`    "
"Similar to #342     I want to add a LSTM to the model for each pyramidal layer of the FPN's outputs. I'm training and predicting on video where the objects are in enclosed spaces and the medium is video so the sequences preceding the current step are related to the previous several.    However, when I add the layers to the P2,P3,P4,P5 in the form of:     I get no change to the graph on tensorboard for the model. It still shows the last conv2d layer fpn_pX as the last layer.    Is the graph dependent on the initial coco trained weights or is this LSTM not getting added to the graph?  <img width=""886"" alt=""screen shot 2019-01-11 at 8 33 51 pm"" src=""     "
Basically I am trying to remove background from my 1 object images and I do not have datasets with ground truth masks to train my model covering all my categories? 
"I recently trained the Mask RCNN on some satellite images, but during inference mode, I'm getting random predictions for the same set of weights for the same image.    That is to say, for a given set of weights and the same image I'm getting different bounding box and mask predictions.    Edit: This is similar to issue #1146.     Does anyone have an idea of why this is the case?    The following are the different predictions that the model produced for the same image.    !     !     !       On some cases, the model produces reasonable predictions like that of the 2nd image but other times it produces completely random results.    I would highly appreciate any insight into this problem."
"I've been trying to utilise the Mask RCNN to read satellite geotiff images from the SpaceNet dataset for building detection. These high res .tif  images have 4 bands namely RGB and Near infrared (NIR). I've tried cutting out the NIR channel and converting it to jpeg files before training the Mask RCNN but the performance was quite bad. Now this is most probably due to information loss from the conversion from tif to jpeg as I have observed the jpeg images are not nearly as clear as the tif images.     So my question is, has anyone used the Mask RCNN for high res tif images and if so how did you go about doing it?"
"Hi,  Let's suppose, I have two ROI in an image. In the first frame both ROI-1 and ROI-2 are correctly detected. In the immediate next frame ROI-1 is detected, however, ROI-2 is not detected. Is it possible to copy the mask of ROI-2 from the first frame and paste it to the next frame?  How can I do that?"
ImportError: No module named 'tensorflow.contrib.nccl.python'  
Hi and I apologise if this is not the correct place to address this.  I am using Mask RCNN to segment images of cell components (nuclei etc).  I am trying to extend Mask RCNN to take multiple input images sort of something like a multiview model.   So instead of a single image I am trying to have two images simultaneously input to the model pass them through a series of separate CNNs to extract separate feature maps and then merge them into a single one.  I would like this to be the input to Mask RCNN instead.    I have implemented the CNNs which are very simple ones to start with however I am not really sure as to how to implement this for the rest of the network or where to make changes.  Can someone help me on how to approach this? 
"   Output:          and thus:    `ValueError: operands could not be broadcast together with shapes (1200,800) (1075,1433) (1075,1433)`     Shouldn't the created mask automatically take the shape of the image? Why isn't this working?"
"train on multiprocessing, use centos7, python3  SyntaxError: broken PNG file (chunk b'\x8d\x03\x98\xa6')"
"I've had good success training Mask RCNN to detect my objects.    Now my goal is to extract the ROI-pooled features from a detection zone so that I can do further analysis and/or training with them. It looks like prior to being classified each ROI on the final feature maps are resized to `7x7x256`.    It seems like this line   is responsible for doing the ROI pooling - i.e. extracting the fixed size `7x7x256` features from various sized boxes around the image. So I made the following modification:       So that `fpn_classifier_graph()` now returns the actual features, as well as the class logits, bboxes and whatever else...    The `pooled_rois` have shape `(batch, 1000, 7, 7, 256)`. However shortly after the method `refine_detections_graph` takes in the roi object scores and box coordinates, in order to prune away (overlapping?, background?) results I think.   After going through this method only `config.DETECTION_MAX_INSTANCES` = 100 detections remain. I can't really follow how they are filtered and possibly re-ordered, however I see that extensive use of NMS and `tf.gather` is made. Also I'm not totally sure what is meant when the docstring explains that the input `rois: [N, (y1, x1, y2, x2)] in normalized coordinates`. What exactly is the normalization? To the unit square? To the shrunken image size (128x128 by default?)?    I think I need to apply the same filtering operations to reduce the number of ROI-pooled features down from 1000 to 100 inside the `refine_detections_graph()` method. Any help with this is much appreciated."
"I have the following piece of code in Python 2.7 (and that my `PYTHONPATH` is set to 2.7 too):         which I want to use to externally invoke mask_rcnn prediction function by using the Python 3 in my `PYTHONPATH`.  This technique isn't anything new, and it works really fine with other programs that I have. However, with Mask_RCNN I face the famous error of:          which is probably happening because of module/package issues of Python.     Can someone tell me how to wrap Mask_RCNN in a way that one can simply call it externally?"
"Since origin ""ParallelModel"" does not work for keras 2.2.4 due to KM.model's change.So I decide to rewrite the ParallelModel to adapt the latest keras version.    However simple adaption is boring, so I combined the ""multi_gpu_model"" from keras official code and the Matterports code to create this version. Since the keras' code claims that the ""split in place tech"" runs faster and more efficient on gpu memery.     Here is the code, hope this helps       "
"HI, I am using the matterport code for an use case where i am generating masks to blur personal details in images like faces, car no plate etc. The masks generated are very transparent and hence nothing gets blurred actually. I tried changing the code in visualize.py for each of the brightness and saturation values.  def random_colors(N, bright=True):  """"""  Generate random colors.  To get visually distinct colors, generate them in HSV space then  convert to RGB.  """"""  brightness = 1.0 if bright else 0.7  hsv = [(i / N, 1, brightness) for i in range(N)]  colors = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))  random.shuffle(colors)  return colors    But if i change the brightness to 0 or 1 , it either becomes dark or white but the density of color donot change. How can i achieve a complete opaque color, kindly advice. Many thanks in advance."
"hi guys,    I tried to train own model on raccoon dataset, and I just have one class.    However, I faced below problem when I using the file inspect_model.ipynb.    `Traceback (most recent call last):      File "" "", line 1, in        runfile('/home/dongs/Documents/Project/mask-rcnn/Mask_RCNN-master/samples/raccoon/inspect_model.py', wdir='/home/dongs/Documents/Project/mask-rcnn/Mask_RCNN-master/samples/raccoon')      File ""/home/dongs/anaconda3/lib/python3.6/site-packages/spyder/utils/site/sitecustomize.py"", line 705, in runfile      execfile(filename, namespace)      File ""/home/dongs/anaconda3/lib/python3.6/site-packages/spyder/utils/site/sitecustomize.py"", line 102, in execfile      exec(compile(f.read(), filename, 'exec'), namespace)      File ""/home/dongs/Documents/Project/mask-rcnn/Mask_RCNN-master/samples/raccoon/inspect_model.py"", line 130, in        r['rois'], r['class_ids'], r['scores'], r['masks'])      File ""/home/dongs/Documents/Project/mask-rcnn/Mask_RCNN-master/mrcnn/utils.py"", line 728, in compute_ap      iou_threshold)      File ""/home/dongs/Documents/Project/mask-rcnn/Mask_RCNN-master/mrcnn/utils.py"", line 680, in compute_matches      overlaps = compute_overlaps_masks(pred_masks, gt_masks)      File ""/home/dongs/Documents/Project/mask-rcnn/Mask_RCNN-master/mrcnn/utils.py"", line 108, in compute_overlaps_masks      masks1 = np.reshape(masks1 > .5, (-1, masks1.shape[-1])).astype(np.float32)      File ""/home/dongs/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py"", line 279, in reshape      return _wrapfunc(a, 'reshape', newshape, order=order)      File ""/home/dongs/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py"", line 51, in _wrapfunc      return getattr(obj, method)(*args, **kwds)    ValueError: cannot reshape array of size 0 into shape (0)`    I find the problem is the r['class_id'] is null, is there anybody know how to resolve it?"
I have a pre-trained model with Mask_RCNN on GPU and I am trying to run this saved model on CPU.  I already load the model in the interference model with GPU_COUNT = 1. But when I want to detect with 'model.detect' the following error occurs      `indices ]]  `  
"Hi guys,    If there anybody know what is wrong this this problem?  My machine environment:    Ubuntu 16.04.5  GPU: Graphics (32502MiB)  NVIDIA-SMI:384.111  CUDA:10.0  Python:3.6.6  Tensorflow:1.10    `7fb539838000-7fb539878000 rw-p 00000000 00:00 0   7fb539878000-7fb539886000 r-xp 00000000 08:02 23082032                   /root/anaconda3/lib/python3.6/site-packages/scipy/_lib/_ccallback_c.cpython-36m-x86_64-linux-gnu.so  7fb539886000-7fb539a85000 ---p 0000e000 08:02 23082032                   /root/anaconda3/lib/python3.6/site-packages/scipy/_lib/_ccallback_c.cpython-36m-x86_64-linux-gnu.so  7fb539a85000-7fb539a87000 rw-p 0000d000 08:02 23082032                   /root/anaconda3/lib/python3.6/site-packages/scipy/_lib/_ccallback_c.cpython-36m-x86_64-linux-gnu.so  7fb539a87000-7fb539b07000 rw-p 00000000 00:00 0   7fb539b07000-7fb539b09000 r-xp 00000000 08:02 14952606                   /usr/lib/x86_64-linux-gnu/libXau.so.6.0.0  7fb539b09000-7fb539d09000 ---p 00002000 08:02 14952606                   /usr/lib/x86_64-linux-gnu/libXau.so.6.0.0  7fb539d09000-7fb539d0a000 r--p 00002000 08:02 14952606                   /usr/lib/x86_64-linux-gnu/libXau.so.6.0.0  7fb539d0a000-7fb539d0b000 rw-p 00003000 08:02 14952606                   /usr/lib/x86_64-linux-gnu/libXau.so.6.0.0  7fb539d0b000-7fb539d32000 r-xp 00000000 08:02 15601156                   /root/anaconda3/lib/libxcb.so.1.1.0  7fb539d32000-7fb539f32000 ---p 00027000 08:02 15601156                   /root/anaconda3/lib/libxcb.so.1.1.0  7fb539f32000-7fb539f33000 r--p 00027000 08:02 15601156                   /root/anaconda3/lib/libxcb.so.1.1.0  7fb539f33000-7fb539f34000 rw-p 00028000 08:02 15601156                   /root/anaconda3/lib/libxcb.so.1.1.0  7fb539f34000-7fb539f37000 r-xp 00000000 08:02 15600950                   /root/anaconda3/lib/libuuid.so.1.0.0  7fb539f37000-7fb53a136000 ---p 00003000 08:02 15600950                   /root/anaconda3/lib/libuuid.so.1.0.0  7fb53a136000-7fb53a137000 r--p 00002000 08:02 15600950                   /root/anaconda3/lib/libuuid.so.1.0.0  7fb53a137000-7fb53a138000 rw-p 00003000 08:02 15600950                   /root/anaconda3/lib/libuuid.so.1.0.0  7fb53a138000-7fb53a17c000 r-xp 00000000 08:02 15609028                   /root/anaconda3/lib/libpcre.so.1.2.10  7fb53a17c000-7fb53a37b000 ---p 00044000 08:02 15609028                   /root/anaconda3/lib/libpcre.so.1.2.10  7fb53a37b000-7fb53a37c000 r--p 00043000 08:02 15609028                   /root/anaconda3/lib/libpcre.so.1.2.10  7fb53a37c000-7fb53a37d000 rw-p 00044000 08:02 15609028                   /root/anaconda3/lib/libpcre.so.1.2.10  7fb53a37d000-7fb53a4b2000 r-xp 00000000 08:02 14952921                   /usr/lib/x86_64-linux-gnu/libX11.so.6.3.0  7fb53a4b2000-7fb53a6b2000 ---p 00135000 08:02 14952921                   /usr/lib/x86_64-linux-gnu/libX11.so.6.3.0  7fb53a6b2000-7fb53a6b3000 r--p 00135000 08:02 14952921                   /usr/lib/x86_64-linux-gnu/libX11.so.6.3.0  7fb53a6b3000-7fb53a6b7000 rw-p 00136000 08:02 14952921                   /usr/lib/x86_64-linux-gnu/libX11.so.6.3.0  7fb53a6b7000-7fb53a6c8000 r-xp 00000000 08:02 14952933                   /usr/lib/x86_64-linux-gnu/libXext.so.6.4.0  7fb53a6c8000-7fb53a8c7000 ---p 00011000 08:02 14952933                   /usr/lib/x86_64-linux-gnu/libXext.so.6.4.0  7fb53a8c7000-7fb53a8c8000 r--p 00010000 08:02 14952933                   /usr/lib/x86_64-linux-gnu/libXext.so.6.4.0  7fb53a8c8000-7fb53a8c9000 rw-p 00011000 08:02 14952933                   /usr/lib/x86_64-linux-gnu/libXext.so.6.4.0  7fb53a8c9000-7fb53a8d2000 r-xp 00000000 08:02 14960047                   /usr/lib/x86_64-linux-gnu/libXrender.so.1.3.0  7fb53a8d2000-7fb53aad1000 ---p 00009000 08:02 14960047                   /usr/lib/x86_64-linux-gnu/libXrender.so.1.3.0  7fb53aad1000-7fb53aad2000 r--p 00008000 08:02 14960047                   /usr/lib/x86_64-linux-gnu/libXrender.so.1.3.0  7fb53aad2000-7fb53aad3000 rw-p 00009000 08:02 14960047                   /usr/lib/x86_64-linux-gnu/libXrender.so.1.3.0  7fb53aad3000-7fb53aae9000 r-xp 00000000 08:02 14952957                   /usr/lib/x86_64-linux-gnu/libICE.so.6.3.0  7fb53aae9000-7fb53ace8000 ---p 00016000 08:02 14952957                   /usr/lib/x86_64-linux-gnu/libICE.so.6.3.0  7fb53ace8000-7fb53ace9000 r--p 00015000 08:02 14952957                   /usr/lib/x86_64-linux-gnu/libICE.so.6.3.0  7fb53ace9000-7fb53acea000 rw-p 00016000 08:02 14952957                   /usr/lib/x86_64-linux-gnu/libICE.so.6.3.0  7fb53acea000-7fb53aced000 rw-p 00000000 00:00 0   7fb53aced000-7fb53acf4000 r-xp 00000000 08:02 14952967                   /usr/lib/x86_64-linux-gnu/libSM.so.6.0.1  7fb53acf4000-7fb53aef3000 ---p 00007000 08:02 14952967                   /usr/lib/x86_64-linux-gnu/libSM.so.6.0.1  7fb53aef3000-7fb53aef4000 r--p 00006000 08:02 14952967                   /usr/lib/x86_64-linux-gnu/libSM.so.6.0.1  7fb53aef4000-7fb53aef5000 rw-p 00007000 08:02 14952967                   /usr/lib/x86_64-linux-gnu/libSM.so.6.0.1  7fb53aef5000-7fb53af13000 r--p 00000000 08:02 15610462                   /root/anaconda3/lib/libglib-2.0.so.0.5600.2  7fb53af13000-7fb53af90000 r-xp 0001e000 08:02 15610462                   /root/anaconda3/lib/libglib-2.0.so.0.5600.2  7fb53af90000-7fb53b00f000 r--p 0009b000 08:02 15610462                   /root/anaconda3/lib/libglib-2.0.so.0.5600.2  7fb53b00f000-7fb53b010000 r--p 00119000 08:02 15610462                   /root/anaconda3/lib/libglib-2.0.so.0.5600.2  7fb53b010000-7fb53b011000 rw-p 0011a000 08:02 15610462                   /root/anaconda3/lib/libglib-2.0.so.0.5600.2  7fb53b011000-7fb53b012000 rw-p 00000000 00:00 0   7fb53b012000-7fb53b2f5000 r-xp 00000000 08:02 23216536                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libvpx-81a43c0a.so.5.0.0  7fb53b2f5000-7fb53b4f5000 ---p 002e3000 08:02 23216536                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libvpx-81a43c0a.so.5.0.0  7fb53b4f5000-7fb53b4f7000 rw-p 002e3000 08:02 23216536                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libvpx-81a43c0a.so.5.0.0  7fb53b4f7000-7fb53b4fe000 rw-p 00000000 00:00 0   7fb53b4fe000-7fb53b4ff000 rw-p 00313000 08:02 23216536                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libvpx-81a43c0a.so.5.0.0  7fb53b4ff000-7fb53b519000 r-xp 00000000 08:02 23216533                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libswresample-b4363bfa.so.3.2.100  7fb53b519000-7fb53b719000 ---p 0001a000 08:02 23216533                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libswresample-b4363bfa.so.3.2.100  7fb53b719000-7fb53b71b000 rw-p 0001a000 08:02 23216533                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libswresample-b4363bfa.so.3.2.100  7fb53b71b000-7fb53b71d000 rw-p 0001d000 08:02 23216533                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libswresample-b4363bfa.so.3.2.100  7fb53b71d000-7fb53b7b1000 r--p 00000000 08:02 15600136                   /root/anaconda3/lib/libstdc++.so.6.0.25  7fb53b7b1000-7fb53b816000 r-xp 00094000 08:02 15600136                   /root/anaconda3/lib/libstdc++.so.6.0.25  7fb53b816000-7fb53b84d000 r--p 000f9000 08:02 15600136                   /root/anaconda3/lib/libstdc++.so.6.0.25  7fb53b84d000-7fb53b857000 r--p 0012f000 08:02 15600136                   /root/anaconda3/lib/libstdc++.so.6.0.25  7fb53b857000-7fb53b85b000 rw-p 00139000 08:02 15600136                   /root/anaconda3/lib/libstdc++.so.6.0.25  7fb53b85b000-7fb53b85e000 rw-p 00000000 00:00 0   7fb53b85e000-7fb53bb5a000 r-xp 00000000 08:02 23216535                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libQtCore-9549151f.so.4.8.7  7fb53bb5a000-7fb53bb6a000 ---p 002fc000 08:02 23216535                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libQtCore-9549151f.so.4.8.7  7fb53bb6a000-7fb53bb96000 rw-p 0030c000 08:02 23216535                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libQtCore-9549151f.so.4.8.7  7fb53bb96000-7fb53bd59000 ---p 00338000 08:02 23216535                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libQtCore-9549151f.so.4.8.7  7fb53bd59000-7fb53bd69000 rw-p 002fb000 08:02 23216535                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libQtCore-9549151f.so.4.8.7  7fb53bd69000-7fb53bd6a000 rw-p 00000000 00:00 0   7fb53bd6a000-7fb53bd8e000 r-xp 00000000 08:02 23216537                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libQtTest-1183da5d.so.4.8.7  7fb53bd8e000-7fb53bf8e000 ---p 00024000 08:02 23216537                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libQtTest-1183da5d.so.4.8.7  7fb53bf8e000-7fb53bf97000 rw-p 00024000 08:02 23216537                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libQtTest-1183da5d.so.4.8.7  7fb53bf97000-7fb53ca9f000 r-xp 00000000 08:02 23216530                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libQtGui-6d0f14dd.so.4.8.7  7fb53ca9f000-7fb53cc9f000 ---p 00b08000 08:02 23216530                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libQtGui-6d0f14dd.so.4.8.7  7fb53cc9f000-7fb53ccf8000 rw-p 00b08000 08:02 23216530                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libQtGui-6d0f14dd.so.4.8.7  7fb53ccf8000-7fb53ccfa000 rw-p 00000000 00:00 0   7fb53ccfa000-7fb53ce28000 rw-p 00b61000 08:02 23216530                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libQtGui-6d0f14dd.so.4.8.7  7fb53ce28000-7fb53cea7000 r-xp 00000000 08:02 23216534                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libswscale-15b3fdc6.so.5.2.100  7fb53cea7000-7fb53d0a6000 ---p 0007f000 08:02 23216534                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libswscale-15b3fdc6.so.5.2.100  7fb53d0a6000-7fb53d0a8000 rw-p 0007e000 08:02 23216534                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libswscale-15b3fdc6.so.5.2.100  7fb53d0a8000-7fb53d0b0000 rw-p 00000000 00:00 0   7fb53d0b0000-7fb53d0b2000 rw-p 00080000 08:02 23216534                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libswscale-15b3fdc6.so.5.2.100  7fb53d0b2000-7fb53d10a000 r-xp 00000000 08:02 23216528                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libavutil-e1b1a17d.so.56.18.102  7fb53d10a000-7fb53d309000 ---p 00058000 08:02 23216528                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libavutil-e1b1a17d.so.56.18.102  7fb53d309000-7fb53d314000 rw-p 00057000 08:02 23216528                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libavutil-e1b1a17d.so.56.18.102  7fb53d314000-7fb53d323000 rw-p 00000000 00:00 0   7fb53d323000-7fb53d327000 rw-p 00063000 08:02 23216528                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libavutil-e1b1a17d.so.56.18.102  7fb53d327000-7fb53d53a000 r-xp 00000000 08:02 23216529                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libavformat-b6bcbe33.so.58.17.101  7fb53d53a000-7fb53d73a000 ---p 00213000 08:02 23216529                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libavformat-b6bcbe33.so.58.17.101  7fb53d73a000-7fb53d779000 rw-p 00213000 08:02 23216529                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libavformat-b6bcbe33.so.58.17.101  7fb53d779000-7fb53e354000 r-xp 00000000 08:02 23216531                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libavcodec-eac15e48.so.58.21.104  7fb53e354000-7fb53e554000 ---p 00bdb000 08:02 23216531                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libavcodec-eac15e48.so.58.21.104  7fb53e554000-7fb53e5b1000 rw-p 00bdb000 08:02 23216531                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libavcodec-eac15e48.so.58.21.104  7fb53e5b1000-7fb53ed85000 rw-p 00000000 00:00 0   7fb53ed85000-7fb53ed95000 rw-p 00c38000 08:02 23216531                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libavcodec-eac15e48.so.58.21.104  7fb53ed95000-7fb53eda9000 r-xp 00000000 08:02 23216532                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libz-a147dcb0.so.1.2.3  7fb53eda9000-7fb53efa8000 ---p 00014000 08:02 23216532                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libz-a147dcb0.so.1.2.3  7fb53efa8000-7fb53efa9000 rw-p 00013000 08:02 23216532                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libz-a147dcb0.so.1.2.3  7fb53efa9000-7fb53efaa000 rw-p 00015000 08:02 23216532                   /root/anaconda3/lib/python3.6/site-packages/cv2/.libs/libz-a147dcb0.so.1.2.3  7fb53efaa000-7fb5406d6000 r-xp 00000000 08:02 23216524                   /root/anaconda3/lib/python3.6/site-packages/cv2/cv2.cpython-36m-x86_64-linux-gnu.so  7fb5406d6000-7fb5408d5000 ---p 0172c000 08:02 23216524                   /root/anaconda3/lib/python3.6/site-packages/cv2/cv2.cpython-36m-x86_64-linux-gnu.so  7fb5408d5000-7fb540950000 rw-p 0172b000 08:02 23216524                   /root/anaconda3/lib/python3.6/site-packages/cv2/cv2.cpython-36m-x86_64-linux-gnu.so  7fb540950000-7fb540a09000 rw-p 00000000 00:00 0   7fb540a09000-7fb540a47000 rw-p 017a7000 08:02 23216524                   /root/anaconda3/lib/python3.6/site-packages/cv2/cv2.cpython-36m-x86_64-linux-gnu.so  7fb540a47000-7fb540ac7000 rw-p 00000000 00:00 0   7fb540ac7000-7fb540b82000 r-xp 00000000 08:02 19929655                   /root/anaconda3/lib/python3.6/site-packages/numpy/random/mtrand.cpython-36m-x86_64-linux-gnu.so  7fb540b82000-7fb540d81000 ---p 000bb000 08:02 19929655                   /root/anaconda3/lib/python3.6/site-packages/numpy/random/mtrand.cpython-36m-x86_64-linux-gnu.so  7fb540d81000-7fb540d82000 r--p 000ba000 08:02 19929655                   /root/anaconda3/lib/python3.6/site-packages/numpy/random/mtrand.cpython-36m-x86_64-linux-gnu.so  7fb540d82000-7fb540da6000 rw-p 000bb000 08:02 19929655                   /root/anaconda3/lib/python3.6/site-packages/numpy/random/mtrand.cpython-36m-x86_64-linux-gnu.so  7fb540da6000-7fb540de7000 rw-p 00000000 00:00 0   7fb540de7000-7fb540df0000 r-xp 00000000 08:02 19929554                   /root/anaconda3/lib/python3.6/site-packages/numpy/fft/fftpack_lite.cpython-36m-x86_64-linux-gnu.so  7fb540df0000-7fb540fef000 ---p 00009000 08:02 19929554                   /root/anaconda3/lib/python3.6/site-packages/numpy/fft/fftpack_lite.cpython-36m-x86_64-linux-gnu.so  7fb540fef000-7fb540ff0000 r--p 00008000 08:02 19929554                   /root/anaconda3/lib/python3.6/site-packages/numpy/fft/fftpack_lite.cpython-36m-x86_64-linux-gnu.so  7fb540ff0000-7fb540ff1000 rw-p 00009000 08:02 19929554                   /root/anaconda3/lib/python3.6/site-packages/numpy/fft/fftpack_lite.cpython-36m-x86_64-linux-gnu.so  7fb540ff1000-7fb541199000 r-xp 00000000 08:02 15599915                   /root/anaconda3/lib/libiomp5.so  7fb541199000-7fb541398000 ---p 001a8000 08:02 15599915                   /root/anaconda3/lib/libiomp5.so  7fb541398000-7fb54139b000 r--p 001a7000 08:02 15599915                   /root/anaconda3/lib/libiomp5.so  7fb54139b000-7fb5413a5000 rw-p 001aa000 08:02 15599915                   /root/anaconda3/lib/libiomp5.so  7fb5413a5000-7fb5413d4000 rw-p 00000000 00:00 0   7fb5413d4000-7fb5413d6000 r-xp 00000000 08:02 19929387                   /root/anaconda3/lib/python3.6/site-packages/numpy/_mklinit.cpython-36m-x86_64-linux-gnu.so  7fb5413d6000-7fb5415d5000 ---p 00002000 08:02 19929387                   /root/anaconda3/lib/python3.6/site-packages/numpy/_mklinit.cpython-36m-x86_64-linux-gnu.so  7fb5415d5000-7fb5415d6000 r--p 00001000 08:02 19929387                   /root/anaconda3/lib/python3.6/site-packages/numpy/_mklinit.cpython-36m-x86_64-linux-gnu.so  7fb5415d6000-7fb5415d7000 rw-p 00002000 08:02 19929387                   /root/anaconda3/lib/python3.6/site-packages/numpy/_mklinit.cpython-36m-x86_64-linux-gnu.so  7fb5415d7000-7fb541617000 rw-p 00000000 00:00 0   7fb541617000-7fb541664000 r-xp 00000000 08:02 21505982                   /root/anaconda3/lib/python3.6/lib-dynload/_decimal.cpython-36m-x86_64-linux-gnu.so  7fb541664000-7fb541863000 ---p 0004d000 08:02 21505982                   /root/anaconda3/lib/python3.6/lib-dynload/_decimal.cpython-36m-x86_64-linux-gnu.so  7fb541863000-7fb54186c000 rw-p 0004c000 08:02 21505982                   /root/anaconda3/lib/python3.6/lib-dynload/_decimal.cpython-36m-x86_64-linux-gnu.so  7fb54186c000-7fb5418ac000 rw-p 00000000 00:00 0   7fb5418ac000-7fb5418d2000 r-xp 00000000 08:02 19929647                   /root/anaconda3/lib/python3.6/site-packages/numpy/linalg/_umath_linalg.cpython-36m-x86_64-linux-gnu.so  7fb5418d2000-7fb541ad1000 ---p 00026000 08:02 19929647                   /root/anaconda3/lib/python3.6/site-packages/numpy/linalg/_umath_linalg.cpython-36m-x86_64-linux-gnu.so  7fb541ad1000-7fb541ad2000 r--p 00025000 08:02 19929647                   /root/anaconda3/lib/python3.6/site-packages/numpy/linalg/_umath_linalg.cpython-36m-x86_64-linux-gnu.so  7fb541ad2000-7fb541ad3000 rw-p 00026000 08:02 19929647                   /root/anaconda3/lib/python3.6/site-packages/numpy/linalg/_umath_linalg.cpython-36m-x86_64-linux-gnu.so  7fb541ad3000-7fb541ad6000 r-xp 00000000 08:02 19929479                   /root/anaconda3/lib/python3.6/site-packages/numpy/linalg/lapack_lite.cpython-36m-x86_64-linux-gnu.so  7fb541ad6000-7fb541cd6000 ---p 00003000 08:02 19929479                   /root/anaconda3/lib/python3.6/site-packages/numpy/linalg/lapack_lite.cpython-36m-x86_64-linux-gnu.so  7fb541cd6000-7fb541cd7000 r--p 00003000 08:02 19929479                   /root/anaconda3/lib/python3.6/site-packages/numpy/linalg/lapack_lite.cpython-36m-x86_64-linux-gnu.so  7fb541cd7000-7fb541cd8000 rw-p 00004000 08:02 19929479                   /root/anaconda3/lib/python3.6/site-packages/numpy/linalg/lapack_lite.cpython-36m-x86_64-linux-gnu.so  7fb541ce9000-7fb541da9000 rw-p 00000000 00:00 0   7fb541da9000-7fb541dac000 r-xp 00000000 08:02 21505911                   /root/anaconda3/lib/python3.6/lib-dynload/_random.cpython-36m-x86_64-linux-gnu.so  7fb541dac000-7fb541fab000 ---p 00003000 08:02 21505911                   /root/anaconda3/lib/python3.6/lib-dynload/_random.cpython-36m-x86_64-linux-gnu.so  7fb541fab000-7fb541fac000 rw-p 00002000 08:02 21505911                   /root/anaconda3/lib/python3.6/lib-dynload/_random.cpython-36m-x86_64-linux-gnu.so  7fb541fac000-7fb541fae000 r-xp 00000000 08:02 21505903                   /root/anaconda3/lib/python3.6/lib-dynload/_bisect.cpython-36m-x86_64-linux-gnu.so  7fb541fae000-7fb5421ae000 ---p 00002000 08:02 21505903                   /root/anaconda3/lib/python3.6/lib-dynload/_bisect.cpython-36m-x86_64-linux-gnu.so  7fb5421ae000-7fb5421af000 rw-p 00002000 08:02 21505903                   /root/anaconda3/lib/python3.6/lib-dynload/_bisect.cpython-36m-x86_64-linux-gnu.so  7fb5421af000-7fb5421ef000 rw-p 00000000 00:00 0   7fb5421ef000-7fb542201000 r-xp 00000000 08:02 21505972                   /root/anaconda3/lib/python3.6/lib-dynload/_sha3.cpython-36m-x86_64-linux-gnu.so  7fb542201000-7fb542400000 ---p 00012000 08:02 21505972                   /root/anaconda3/lib/python3.6/lib-dynload/_sha3.cpython-36m-x86_64-linux-gnu.so  7fb542400000-7fb542402000 rw-p 00011000 08:02 21505972                   /root/anaconda3/lib/python3.6/lib-dynload/_sha3.cpython-36m-x86_64-linux-gnu.so  7fb542402000-7fb54240b000 r-xp 00000000 08:02 21505970                   /root/anaconda3/lib/python3.6/lib-dynload/_blake2.cpython-36m-x86_64-linux-gnu.so  7fb54240b000-7fb54260a000 ---p 00009000 08:02 21505970                   /root/anaconda3/lib/python3.6/lib-dynload/_blake2.cpython-36m-x86_64-linux-gnu.so  7fb54260a000-7fb54260c000 rw-p 00008000 08:02 21505970                   /root/anaconda3/lib/python3.6/lib-dynload/_blake2.cpython-36m-x86_64-linux-gnu.so  7fb54260c000-7fb542833000 r-xp 00000000 08:02 15614064                   /root/anaconda3/lib/libcrypto.so.1.0.0  7fb542833000-7fb542a33000 ---p 00227000 08:02 15614064                   /root/anaconda3/lib/libcrypto.so.1.0.0  7fb542a33000-7fb542a5a000 rw-p 00227000 08:02 15614064                   /root/anaconda3/lib/libcrypto.so.1.0.0  7fb542a5a000-7fb542a5d000 rw-p 00000000 00:00 0   7fb542a5d000-7fb542acc000 r-xp 00000000 08:02 15614065                   /root/anaconda3/lib/libssl.so.1.0.0  7fb542acc000-7fb542ccb000 ---p 0006f000 08:02 15614065                   /root/anaconda3/lib/libssl.so.1.0.0  7fb542ccb000-7fb542cd6000 rw-p 0006e000 08:02 15614065                   /root/anaconda3/lib/libssl.so.1.0.0  7fb542cd6000-7fb542cdc000 r-xp 00000000 08:02 15614088                   /root/anaconda3/lib/python3.6/lib-dynload/_hashlib.cpython-36m-x86_64-linux-gnu.so  7fb542cdc000-7fb542edc000 ---p 00006000 08:02 15614088                   /root/anaconda3/lib/python3.6/lib-dynload/_hashlib.cpython-36m-x86_64-linux-gnu.so  7fb542edc000-7fb542edd000 rw-p 00006000 08:02 15614088                   /root/anaconda3/lib/python3.6/lib-dynload/_hashlib.cpython-36m-x86_64-linux-gnu.so  7fb542edd000-7fb542edf000 r-xp 00000000 08:02 21505905                   /root/anaconda3/lib/python3.6/lib-dynload/grp.cpython-36m-x86_64-linux-gnu.so  7fb542edf000-7fb5430df000 ---p 00002000 08:02 21505905                   /root/anaconda3/lib/python3.6/lib-dynload/grp.cpython-36m-x86_64-linux-gnu.so  7fb5430df000-7fb5430e0000 rw-p 00002000 08:02 21505905                   /root/anaconda3/lib/python3.6/lib-dynload/grp.cpython-36m-x86_64-linux-gnu.so  7fb5430e0000-7fb543105000 r-xp 00000000 08:02 15609225                   /root/anaconda3/lib/liblzma.so.5.2.4  7fb543105000-7fb543304000 ---p 00025000 08:02 15609225                   /root/anaconda3/lib/liblzma.so.5.2.4  7fb543304000-7fb543305000 r--p 00024000 08:02 15609225                   /root/anaconda3/lib/liblzma.so.5.2.4  7fb543305000-7fb543306000 rw-p 00025000 08:02 15609225                   /root/anaconda3/lib/liblzma.so.5.2.4  7fb543306000-7fb54330d000 r-xp 00000000 08:02 15614089                   /root/anaconda3/lib/python3.6/lib-dynload/_lzma.cpython-36m-x86_64-linux-gnu.so  7fb54330d000-7fb54350d000 ---p 00007000 08:02 15614089                   /root/anaconda3/lib/python3.6/lib-dynload/_lzma.cpython-36m-x86_64-linux-gnu.so  7fb54350d000-7fb54350f000 rw-p 00007000 08:02 15614089                   /root/anaconda3/lib/python3.6/lib-dynload/_lzma.cpython-36m-x86_64-linux-gnu.so  7fb54350f000-7fb543521000 r-xp 00000000 08:02 15614084                   /root/anaconda3/lib/python3.6/lib-dynload/_bz2.cpython-36m-x86_64-linux-gnu.so  7fb543521000-7fb543721000 ---p 00012000 08:02 15614084                   /root/anaconda3/lib/python3.6/lib-dynload/_bz2.cpython-36m-x86_64-linux-gnu.so  7fb543721000-7fb543723000 rw-p 00012000 08:02 15614084                   /root/anaconda3/lib/python3.6/lib-dynload/_bz2.cpython-36m-x86_64-linux-gnu.so  7fb543723000-7fb543739000 r-xp 00000000 08:02 15609324                   /root/anaconda3/lib/libz.so.1.2.11  7fb543739000-7fb543938000 ---p 00016000 08:02 15609324                   /root/anaconda3/lib/libz.so.1.2.11  7fb543938000-7fb543939000 r--p 00015000 08:02 15609324                   /root/anaconda3/lib/libz.so.1.2.11  7fb543939000-7fb54393a000 rw-p 00016000 08:02 15609324                   /root/anaconda3/lib/libz.so.1.2.11  7fb54393a000-7fb543940000 r-xp 00000000 08:02 15614095                   /root/anaconda3/lib/python3.6/lib-dynload/zlib.cpython-36m-x86_64-linux-gnu.so  7fb543940000-7fb543b40000 ---p 00006000 08:02 15614095                   /root/anaconda3/lib/python3.6/lib-dynload/zlib.cpython-36m-x86_64-linux-gnu.so  7fb543b40000-7fb543b42000 rw-p 00006000 08:02 15614095                   /root/anaconda3/lib/python3.6/lib-dynload/zlib.cpython-36m-x86_64-linux-gnu.so  7fb543b42000-7fb543d02000 rw-p 00000000 00:00 0   7fb543d02000-7fb543d1a000 r-xp 00000000 08:02 21505974                   /root/anaconda3/lib/python3.6/lib-dynload/_pickle.cpython-36m-x86_64-linux-gnu.so  7fb543d1a000-7fb543f19000 ---p 00018000 08:02 21505974                   /root/anaconda3/lib/python3.6/lib-dynload/_pickle.cpython-36m-x86_64-linux-gnu.so  7fb543f19000-7fb543f1d000 rw-p 00017000 08:02 21505974                   /root/anaconda3/lib/python3.6/lib-dynload/_pickle.cpython-36m-x86_64-linux-gnu.so  7fb543f1d000-7fb543f9d000 rw-p 00000000 00:00 0   7fb543f9d000-7fb543fa6000 r-xp 00000000 08:02 21505955                   /root/anaconda3/lib/python3.6/lib-dynload/_struct.cpython-36m-x86_64-linux-gnu.so  7fb543fa6000-7fb5441a5000 ---p 00009000 08:02 21505955                   /root/anaconda3/lib/python3.6/lib-dynload/_struct.cpython-36m-x86_64-linux-gnu.so  7fb5441a5000-7fb5441a8000 rw-p 00008000 08:02 21505955                   /root/anaconda3/lib/python3.6/lib-dynload/_struct.cpython-36m-x86_64-linux-gnu.so  7fb5441a8000-7fb5441af000 r-xp 00000000 08:02 15600714                   /root/anaconda3/lib/libffi.so.6.0.4  7fb5441af000-7fb5443af000 ---p 00007000 08:02 15600714                   /root/anaconda3/lib/libffi.so.6.0.4  7fb5443af000-7fb5443b0000 r--p 00007000 08:02 15600714                   /root/anaconda3/lib/libffi.so.6.0.4  7fb5443b0000-7fb5443b1000 rw-p 00008000 08:02 15600714                   /root/anaconda3/lib/libffi.so.6.0.4  7fb5443b1000-7fb5443cc000 r-xp 00000000 08:02 15614085                   /root/anaconda3/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so  7fb5443cc000-7fb5445cc000 ---p 0001b000 08:02 15614085                   /root/anaconda3/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so  7fb5445cc000-7fb5445d0000 rw-p 0001b000 08:02 15614085                   /root/anaconda3/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so  7fb5445d0000-7fb544610000 rw-p 00000000 00:00 0   7fb544619000-7fb544659000 rw-p 00000000 00:00 0   7fb544659000-7fb5447a6000 r-xp 00000000 08:02 19929656                   /root/anaconda3/lib/python3.6/site-packages/numpy/core/umath.cpython-36m-x86_64-linux-gnu.so  7fb5447a6000-7fb5449a6000 ---p 0014d000 08:02 19929656                   /root/anaconda3/lib/python3.6/site-packages/numpy/core/umath.cpython-36m-x86_64-linux-gnu.so  7fb5449a6000-7fb5449a7000 r--p 0014d000 08:02 19929656                   /root/anaconda3/lib/python3.6/site-packages/numpy/core/umath.cpython-36m-x86_64-linux-gnu.so  7fb5449a7000-7fb5449ac000 rw-p 0014e000 08:02 19929656                   /root/anaconda3/lib/python3.6/site-packages/numpy/core/umath.cpython-36m-x86_64-linux-gnu.so  7fb5449ac000-7fb5449ae000 rw-p 00000000 00:00 0   7fb5449ae000-7fb5449c4000 r-xp 00000000 08:02 21505975                   /root/anaconda3/lib/python3.6/lib-dynload/_datetime.cpython-36m-x86_64-linux-gnu.so  7fb5449c4000-7fb544bc4000 ---p 00016000 08:02 21505975                   /root/anaconda3/lib/python3.6/lib-dynload/_datetime.cpython-36m-x86_64-linux-gnu.so  7fb544bc4000-7fb544bc7000 rw-p 00016000 08:02 21505975                   /root/anaconda3/lib/python3.6/lib-dynload/_datetime.cpython-36m-x86_64-linux-gnu.so  7fb544bc7000-7fb545078000 r-xp 00000000 08:02 15601337                   /root/anaconda3/lib/libmkl_rt.so  7fb545078000-7fb545278000 ---p 004b1000 08:02 15601337                   /root/anaconda3/lib/libmkl_rt.so  7fb545278000-7fb54527e000 r--p 004b1000 08:02 15601337                   /root/anaconda3/lib/libmkl_rt.so  7fb54527e000-7fb545280000 rw-p 004b7000 08:02 15601337                   /root/anaconda3/lib/libmkl_rt.so  7fb545280000-7fb545292000 rw-p 00000000 00:00 0   7fb545292000-7fb54542d000 r-xp 00000000 08:02 19929657                   /root/anaconda3/lib/python3.6/site-packages/numpy/core/multiarray.cpython-36m-x86_64-linux-gnu.so  7fb54542d000-7fb54562d000 ---p 0019b000 08:02 19929657                   /root/anaconda3/lib/python3.6/site-packages/numpy/core/multiarray.cpython-36m-x86_64-linux-gnu.so  7fb54562d000-7fb545630000 r--p 0019b000 08:02 19929657                   /root/anaconda3/lib/python3.6/site-packages/numpy/core/multiarray.cpython-36m-x86_64-linux-gnu.so  7fb545630000-7fb545647000 rw-p 0019e000 08:02 19929657                   /root/anaconda3/lib/python3.6/site-packages/numpy/core/multiarray.cpython-36m-x86_64-linux-gnu.so  7fb545647000-7fb54565f000 rw-p 00000000 00:00 0   7fb54565f000-7fb54566c000 r-xp 00000000 08:02 21505962                   /root/anaconda3/lib/python3.6/lib-dynload/math.cpython-36m-x86_64-linux-gnu.so  7fb54566c000-7fb54586b000 ---p 0000d000 08:02 21505962                   /root/anaconda3/lib/python3.6/lib-dynload/math.cpython-36m-x86_64-linux-gnu.so  7fb54586b000-7fb54586d000 rw-p 0000c000 08:02 21505962                   /root/anaconda3/lib/python3.6/lib-dynload/math.cpython-36m-x86_64-linux-gnu.so  7fb54586d000-7fb54596d000 rw-p 00000000 00:00 0   7fb54596d000-7fb54596f000 r-xp 00000000 08:02 21505917                   /root/anaconda3/lib/python3.6/lib-dynload/_heapq.cpython-36m-x86_64-linux-gnu.so  7fb54596f000-7fb545b6f000 ---p 00002000 08:02 21505917                   /root/anaconda3/lib/python3.6/lib-dynload/_heapq.cpython-36m-x86_64-linux-gnu.so  7fb545b6f000-7fb545b71000 rw-p 00002000 08:02 21505917                   /root/anaconda3/lib/python3.6/lib-dynload/_heapq.cpython-36m-x86_64-linux-gnu.so  7fb545b71000-7fb545bf1000 rw-p 00000000 00:00 0   7fb545c01000-7fb545c04000 r--p 00000000 08:02 15599960                   /root/anaconda3/lib/libgcc_s.so.1  7fb545c04000-7fb545c11000 r-xp 00003000 08:02 15599960                   /root/anaconda3/lib/libgcc_s.so.1  7fb545c11000-7fb545c14000 r--p 00010000 08:02 15599960                   /root/anaconda3/lib/libgcc_s.so.1  7fb545c14000-7fb545c15000 r--p 00012000 08:02 15599960                   /root/anaconda3/lib/libgcc_s.so.1  7fb545c15000-7fb545c16000 rw-p 00013000 08:02 15599960                   /root/anaconda3/lib/libgcc_s.so.1  7fb545c16000-7fb545d56000 rw-p 00000000 00:00 0   7fb545d56000-7fb545f16000 r-xp 00000000 08:02 28858414                   /lib/x86_64-linux-gnu/libc-2.23.so  7fb545f16000-7fb546116000 ---p 001c0000 08:02 28858414                   /lib/x86_64-linux-gnu/libc-2.23.so  7fb546116000-7fb54611a000 r--p 001c0000 08:02 28858414                   /lib/x86_64-linux-gnu/libc-2.23.so  7fb54611a000-7fb54611c000 rw-p 001c4000 08:02 28858414                   /lib/x86_64-linux-gnu/libc-2.23.so  7fb54611c000-7fb546120000 rw-p 00000000 00:00 0   7fb546120000-7fb546228000 r-xp 00000000 08:02 28858446                   /lib/x86_64-linux-gnu/libm-2.23.so  7fb546228000-7fb546427000 ---p 00108000 08:02 28858446                   /lib/x86_64-linux-gnu/libm-2.23.so  7fb546427000-7fb546428000 r--p 00107000 08:02 28858446                   /lib/x86_64-linux-gnu/libm-2.23.so  7fb546428000-7fb546429000 rw-p 00108000 08:02 28858446                   /lib/x86_64-linux-gnu/libm-2.23.so  7fb546429000-7fb546430000 r-xp 00000000 08:02 28858488                   /lib/x86_64-linux-gnu/librt-2.23.so  7fb546430000-7fb54662f000 ---p 00007000 08:02 28858488                   /lib/x86_64-linux-gnu/librt-2.23.so  7fb54662f000-7fb546630000 r--p 00006000 08:02 28858488                   /lib/x86_64-linux-gnu/librt-2.23.so  7fb546630000-7fb546631000 rw-p 00007000 08:02 28858488                   /lib/x86_64-linux-gnu/librt-2.23.so  7fb546631000-7fb546633000 r-xp 00000000 08:02 28858508                   /lib/x86_64-linux-gnu/libutil-2.23.so  7fb546633000-7fb546832000 ---p 00002000 08:02 28858508                   /lib/x86_64-linux-gnu/libutil-2.23.so  7fb546832000-7fb546833000 r--p 00001000 08:02 28858508                   /lib/x86_64-linux-gnu/libutil-2.23.so  7fb546833000-7fb546834000 rw-p 00002000 08:02 28858508                   /lib/x86_64-linux-gnu/libutil-2.23.so  7fb546834000-7fb546837000 r-xp 00000000 08:02 28858427                   /lib/x86_64-linux-gnu/libdl-2.23.so  7fb546837000-7fb546a36000 ---p 00003000 08:02 28858427                   /lib/x86_64-linux-gnu/libdl-2.23.so  7fb546a36000-7fb546a37000 r--p 00002000 08:02 28858427                   /lib/x86_64-linux-gnu/libdl-2.23.so  7fb546a37000-7fb546a38000 rw-p 00003000 08:02 28858427                   /lib/x86_64-linux-gnu/libdl-2.23.so  7fb546a38000-7fb546a50000 r-xp 00000000 08:02 28858482                   /lib/x86_64-linux-gnu/libpthread-2.23.so  7fb546a50000-7fb546c4f000 ---p 00018000 08:02 28858482                   /lib/x86_64-linux-gnu/libpthread-2.23.so  7fb546c4f000-7fb546c50000 r--p 00017000 08:02 28858482                   /lib/x86_64-linux-gnu/libpthread-2.23.so  7fb546c50000-7fb546c51000 rw-p 00018000 08:02 28858482                   /lib/x86_64-linux-gnu/libpthread-2.23.so  7fb546c51000-7fb546c55000 rw-p 00000000 00:00 0   7fb546c55000-7fb546ef8000 r-xp 00000000 08:02 15614077                   /root/anaconda3/lib/libpython3.6m.so.1.0  7fb546ef8000-7fb5470f8000 ---p 002a3000 08:02 15614077                   /root/anaconda3/lib/libpython3.6m.so.1.0  7fb5470f8000-7fb547161000 rw-p 002a3000 08:02 15614077                   /root/anaconda3/lib/libpython3.6m.so.1.0  7fb547161000-7fb547192000 rw-p 00000000 00:00 0   7fb547192000-7fb5471b8000 r-xp 00000000 08:02 28858394                   /lib/x86_64-linux-gnu/ld-2.23.so  7fb5471c0000-7fb5471c1000 rw-p 00000000 00:00 0   7fb5471c1000-7fb5471c2000 r--p 00000000 08:02 15609898                   /root/anaconda3/lib/libgthread-2.0.so.0.5600.2  7fb5471c2000-7fb5471c3000 r-xp 00001000 08:02 15609898                   /root/anaconda3/lib/libgthread-2.0.so.0.5600.2  7fb5471c3000-7fb5471c4000 r--p 00002000 08:02 15609898                   /root/anaconda3/lib/libgthread-2.0.so.0.5600.2  7fb5471c4000-7fb5471c5000 r--p 00002000 08:02 15609898                   /root/anaconda3/lib/libgthread-2.0.so.0.5600.2  7fb5471c5000-7fb5471c6000 rw-p 00003000 08:02 15609898                   /root/anaconda3/lib/libgthread-2.0.so.0.5600.2  7fb5471c6000-7fb5471c7000 rwxp 00000000 00:00 0   7fb5471c7000-7fb547207000 rw-p 00000000 00:00 0   7fb547207000-7fb54722e000 r--p 00000000 08:02 28858944                   /usr/lib/locale/C.UTF-8/LC_CTYPE  7fb54722e000-7fb54722f000 r--p 00000000 08:02 28858951                   /usr/lib/locale/C.UTF-8/LC_NUMERIC  7fb54722f000-7fb547230000 r--p 00000000 08:02 28858954                   /usr/lib/locale/C.UTF-8/LC_TIME  7fb547230000-7fb5473a2000 r--p 00000000 08:02 28858943                   /usr/lib/locale/C.UTF-8/LC_COLLATE  7fb5473a2000-7fb5473a3000 r--p 00000000 08:02 28858949                   /usr/lib/locale/C.UTF-8/LC_MONETARY  7fb5473a3000-7fb5473a4000 r--p 00000000 08:02 28858948                   /usr/lib/locale/C.UTF-8/LC_MESSAGES/SYS_LC_MESSAGES  7fb5473a4000-7fb5473a9000 rw-p 00000000 00:00 0   7fb5473a9000-7fb5473aa000 r--p 00000000 08:02 28858952                   /usr/lib/locale/C.UTF-8/LC_PAPER  7fb5473aa000-7fb5473ab000 r--p 00000000 08:02 28858950                   /usr/lib/locale/C.UTF-8/LC_NAME  7fb5473ab000-7fb5473ac000 r--p 00000000 08:02 28858942                   /usr/lib/locale/C.UTF-8/LC_ADDRESS  7fb5473ac000-7fb5473ad000 r--p 00000000 08:02 28858953                   /usr/lib/locale/C.UTF-8/LC_TELEPHONE  7fb5473ad000-7fb5473ae000 r--p 00000000 08:02 28858946                   /usr/lib/locale/C.UTF-8/LC_MEASUREMENT  7fb5473ae000-7fb5473b5000 r--s 00000000 08:02 28859270                   /usr/lib/x86_64-linux-gnu/gconv/gconv-modules.cache  7fb5473b5000-7fb5473b6000 r--p 00000000 08:02 28858945                   /usr/lib/locale/C.UTF-8/LC_IDENTIFICATION  7fb5473b6000-7fb5473b7000 rw-p 00000000 00:00 0   7fb5473b7000-7fb5473b8000 r--p 00025000 08:02 28858394                   /lib/x86_64-linux-gnu/ld-2.23.so  7fb5473b8000-7fb5473b9000 rw-p 00026000 08:02 28858394                   /lib/x86_64-linux-gnu/ld-2.23.so  7fb5473b9000-7fb5473ba000 rw-p 00000000 00:00 0   7fff77216000-7fff77242000 rw-p 00000000 00:00 0                          [stack]  7fff77302000-7fff77305000 r--p 00000000 00:00 0                          [vvar]  7fff77305000-7fff77307000 r-xp 00000000 00:00 0                          [vdso]  ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]  Aborted (core dumped)`    "
"I try to change optimizer from sgd(lr=0.001) to adam(lr=0.0001), but get "
"Hi  I have a sample data set on which I am running maskRCNN. I am creating a model model = modellib.MaskRCNN(mode='training', config=config, model_dir=MODEL_DIR) and training model using   model.train(dataset_train, dataset_val,               learning_rate=config.LEARNING_RATE,               epochs=NUM_EPOCHS,               layers='all',              augmentation = augmentation)  I want to somehow plot the accuracy after the training is done. Is there  a way i can get the History object as we get it in Keras fit() method or can someone suggest me how to use Tensorboard for this trained model?    Regards  Suhird"
Just in case you got such an error:    `AttributeError: 'FigureCanvasAgg' object has no attribute 'manager'`    this is because of the new matplotlib version. You can fix it using `pip install --upgrade matplotlib==2.2.3`
"9/500 [..............................] - ETA: 43:03 - loss: nan - rpn_class_loss: 0.4798 - rpn_bbox_loss: 0.5489 - mrcnn_class_loss: 1.1758 - mrcnn_bbox_loss: 0.4309 - mrcnn_mask_loss: 0.2511  As descreibe below, the total loss=nan, however the other 5 loss is not.  I can't figure out why. Can someone help?"
I got  *** stack smashing detected ***: python terminated  Aborted (core dumped)  error when I tried the command  python balloon.py train --dataset=~/Mask_RCNN/datasets/balloon --weights=coco  cudatoolkit==9.0  cudnn==7.1.2  tensorflow-gpu==1.12.0  
"I use pre-trained model for my project. I want to know how can I rescale the mask because I want to cover the edges of the detected objects. If I use the current scale, there are still some edges that are not covered by the mask. Thank you!"
"right after executing this:      # Get activations of a few sample layers      activations = model.run_graph([image], [          (""input_image"",        model.keras_model.get_layer(""input_image"").output),          (""res4w_out"",          model.keras_model.get_layer(""res4w_out"").output),  # for resnet100          (""rpn_bbox"",           model.keras_model.get_layer(""rpn_bbox"").output),          (""roi"",                model.keras_model.get_layer(""ROI"").output),      ])    I'm getting this error-chain:    ---------------------------------------------------------------------------  InvalidArgumentError                      Traceback (most recent call last)    in  ()        4     (""res4w_out"",          model.keras_model.get_layer(""res4w_out"").output),  # for resnet100        5     (""rpn_bbox"",           model.keras_model.get_layer(""rpn_bbox"").output),  ----> 6     (""roi"",                model.keras_model.get_layer(""ROI"").output),        7 ])    ~/Mask_RCNN/mrcnn/model.py in run_graph(self, images, outputs, image_metas)     2710         if model.uses_learning_phase and not isinstance(K.learning_phase(), int):     2711             model_in.append(0.)  -> 2712         outputs_np = kf(model_in)     2713      2714         # Pack the generated Numpy arrays into a a dict and log the results.    /usr/local/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in __call__(self, inputs)     2664                 return self._legacy_call(inputs)     2665   -> 2666             return self._call(inputs)     2667         else:     2668             if py_any(is_tensor(x) for x in inputs):    /usr/local/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in _call(self, inputs)     2633                                 feed_symbols,     2634                                 symbol_vals,  -> 2635                                 session)     2636         fetched = self._callable_fn(*array_vals)     2637         return fetched[:len(self.outputs)]    /usr/local/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in _make_callable(self, feed_arrays, feed_symbols, symbol_vals, session)     2585         callable_opts.target.append(self.updates_op.name)     2586         # Create callable.  -> 2587         callable_fn = session._make_callable_from_options(callable_opts)     2588         # Cache parameters corresponding to the generated callable, so that     2589         # we can detect future mismatches and refresh the callable.    /usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _make_callable_from_options(self, callable_options)     1469     """"""     1470     self._extend_graph()  -> 1471     return BaseSession._Callable(self, callable_options)     1472      1473     /usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in __init__(self, session, callable_options)     1423         with errors.raise_exception_on_not_ok_status() as status:     1424           self._handle = tf_session.TF_SessionMakeCallable(  -> 1425               session._session, options_ptr, status)     1426       finally:     1427         tf_session.TF_DeleteBuffer(options_ptr)    /usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)      526             None, None,      527             compat.as_text(c_api.TF_Message(self.status.status)),  --> 528             c_api.TF_GetCode(self.status.status))      529     # Delete the underlying status object from memory otherwise it stays alive      530     # as there is a reference to status from this from the traceback due to    InvalidArgumentError: input_image:0 is both fed and fetched.    I looked around and found that latest keras-vis version and a tf version higher than 1.8 is needed to solve similar problems.  so I upgraded my tf to 1.12 , and installed keras-vis 0.4.1 - yet it keeps appearing    appreciate some help  \m/"
"hi  i added the classes as noted, and I'm training from zero on all layers, but it doesn't show all classes as noted in the database, instead it pick a random class and classifies all masks with it ?!      the classes are:          self.add_class(""foods"", 1, ""banana"")          self.add_class(""foods"", 2, ""onions"")          self.add_class(""foods"", 3, ""figs"")          self.add_class(""foods"", 4, ""pear"")          self.add_class(""foods"", 5, ""rice"")          self.add_class(""foods"", 8, ""lemon"")          self.add_class(""foods"", 9, ""pringles"")          self.add_class(""foods"", 11, ""tomate"")"
I like to specify that the model should only identify / return non-overlapping instances.    I did not find any configuration setting for this nor an easy way to change the model code.    Maybe someone can give me a hint how this can be achieved?    Thank you very much
"I have a video of 5 minutes length. The video may contain varieties of object. I want to search dogs from that video, which can only be found at 2:30 min to 2:35 min. So, how can I find that specific frame from that video containing dogs?"
"Hello, I have a question about the backpropagation in training. In the original paper, for convenient ablation, RPN is trained separately and does not share features with Mask R-CNN, unless specified. But I see the demo code of training train RPN and Mask R-CNN at same time.  During the training(code version), I do not know if rpn_class_loss, rpn_bbox_loss only affect the layers of RPN and backbone, they have no effect about the part of Mask R-CNN layers? We know the loss value that will be minimized by the model will then be the sum of all individual losses in Keras. Does the 'big loss' work on all layers in the network or just the backbone? I want to know whether the different training methods will affect the model and I really want to know the effective range all kinds of loss. Could you help me to figure it out?   Looking forward to your reply!! "
"Reading the working examples of this algorithm I saw that the VGG Image Annotator is used to rotate the objects, however I have my objects separated by folders (This is the way I use it for CNN).    Example:          Dataset (Folder)           |--- Class-1 (SubFolder)           |        .   |--- Image-1           |        .   |--- Image-N           |--- Class-N (SubFolder)    Is it possible to train my dataset for the CNN Mask in this way?         "
"I followed the instructions on the wiki and modified the code from train_shapes.ipynb  > 1. In your subclass of Config, set IMAGE_CHANNEL_COUNT to N.  > 2. In the same class, change MEAN_PIXEL from 3 values to N values.    > 3. The load_image() method in the Dataset class is designed for RGB. It converts Grayscale images to RGB and removes the 4th channel if present (because typically it's an alpha channel). You'll need to override this method to handle your images.     > 4.Since you're changing the shape of the input, the shape of the first Conv layer (Conv1) will change as well. So you can't use the provided pre-trained weights. To get around that, use the exclude parameter when you load the weights to exclude the first layer. This allows you to load weights of all layers except conv1, which will be initialized to random weights.     > 5.If you train a subset of layers, remember to include conv1 since it's initialized to random weights. This is relevant if you pass layers=""head"" or layers=""4+"", ...etc. when you call train().  in train_shapes.py     in model.py:     it showed the error like this:  ERROR:root:Error processing image {'id': 1132, 'source': 'Newton Ring', 'path':  'C:/imageandmask\\img-1493_json/img.png', 'width': 2448, 'height': 2048, 'mask_p  ath': 'C:/imageandmask\\img-1493_json/label.png', 'yaml_path': 'C:/imageandmask\  \img-1493_json/info.yaml'}  Traceback (most recent call last):    File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv2\lib\sit  e-packages\numpy\lib\arraypad.py"", line 935, in _normalize_shape      shape_arr = np.broadcast_to(shape_arr, (ndims, 2))    File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv2\lib\sit  e-packages\numpy\lib\stride_tricks.py"", line 176, in broadcast_to      return _broadcast_to(array, shape, subok=subok, readonly=True)    File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv2\lib\sit  e-packages\numpy\lib\stride_tricks.py"", line 128, in _broadcast_to      op_flags=[op_flag], itershape=shape, order='C')  ValueError: operands could not be broadcast together with remapped shapes [origi  nal->remapped]: (3,2) and requested shape (2,2)    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""C:\Mask_RCNN\mrcnn\model.py"", line 1710, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""C:\Mask_RCNN\mrcnn\model.py"", line 1220, in load_image_gt      mode=config.IMAGE_RESIZE_MODE)    File ""C:\Mask_RCNN\mrcnn\utils.py"", line 457, in resize_image      image = np.pad(image, padding, mode='constant', constant_values=0)    File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv2\lib\sit  e-packages\numpy\lib\arraypad.py"", line 1200, in pad      pad_width = _validate_lengths(narray, pad_width)    File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv2\lib\sit  e-packages\numpy\lib\arraypad.py"", line 979, in _validate_lengths      normshp = _normalize_shape(narray, number_elements)    File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv2\lib\sit  e-packages\numpy\lib\arraypad.py"", line 938, in _normalize_shape      raise ValueError(fmt % (shape,))  ValueError: Unable to create correctly shaped tuple from [(83, 84), (0, 0), (0,  0)]  Traceback (most recent call last):    File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv2\lib\sit  e-packages\numpy\lib\arraypad.py"", line 935, in _normalize_shape      shape_arr = np.broadcast_to(shape_arr, (ndims, 2))    File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv2\lib\sit  e-packages\numpy\lib\stride_tricks.py"", line 176, in broadcast_to      return _broadcast_to(array, shape, subok=subok, readonly=True)    File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv2\lib\sit  e-packages\numpy\lib\stride_tricks.py"", line 128, in _broadcast_to      op_flags=[op_flag], itershape=shape, order='C')  ValueError: operands could not be broadcast together with remapped shapes [origi  nal->remapped]: (3,2) and requested shape (2,2)    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""train_shapes.py"", line 262, in        layers='heads',augmentation=augmentation)    File ""C:\Mask_RCNN\mrcnn\model.py"", line 2376, in train      use_multiprocessing=True,    File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv2\lib\sit  e-packages\keras\legacy\interfaces.py"", line 91, in wrapper      return func(*args, **kwargs)    File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv2\lib\sit  e-packages\keras\engine\training.py"", line 1418, in fit_generator      initial_epoch=initial_epoch)    File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv2\lib\sit  e-packages\keras\engine\training_generator.py"", line 181, in fit_generator      generator_output = next(output_generator)    File ""C:\Mask_RCNN\mrcnn\model.py"", line 1710, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""C:\Mask_RCNN\mrcnn\model.py"", line 1220, in load_image_gt      mode=config.IMAGE_RESIZE_MODE)    File ""C:\Mask_RCNN\mrcnn\utils.py"", line 457, in resize_image      image = np.pad(image, padding, mode='constant', constant_values=0)    File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv2\lib\sit  e-packages\numpy\lib\arraypad.py"", line 1200, in pad      pad_width = _validate_lengths(narray, pad_width)    File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv2\lib\sit  e-packages\numpy\lib\arraypad.py"", line 979, in _validate_lengths      normshp = _normalize_shape(narray, number_elements)    File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv2\lib\sit  e-packages\numpy\lib\arraypad.py"", line 938, in _normalize_shape      raise ValueError(fmt % (shape,))  ValueError: Unable to create correctly shaped tuple from [(83, 84), (0, 0), (0,  0)]  I traced the error and found it is from      image.shape is [2048, 2448]  I know the problem is still about the number of channels, but I don't know what else I have to modify or change,can someone tell me please?  "
"I find for each layer you build in `fpn_classifier_graph()` in `model.py`, you add a `TimeDistributed` wrapper. I don't understand what the purpose is. From Keras documentation, it said the `TimeDistributed` wrapper has something to do with temporal data, but I don't think the feature maps are temporal data. I also read the original paper and found no clues.    Could you please explain a little bit more about that?    Thanks very much!  "
None
"Hi I am currently attempting to run the demo using the instructions from ""demo.ipynb.""  I named the demo code ""demo.py."" When I run the code (i.e. python3 demo.py) I receive the following error :     **I tensorflow/core/platform/cpu_feature_guard.cc:141].Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA .**     How can one fix this problem?"
"Hi,  After training the results are   `Epoch 30/30  25/25 [==============================] - 1797s 72s/step - loss: 1.0709 - rpn_class_loss: 0.0133 - rpn_bbox_loss: 0.3766 - mrcnn_class_loss: 0.2582 - mrcnn_bbox_loss: 0.1942 - mrcnn_mask_loss: 0.2286 - val_loss: 1.0804 - val_rpn_class_loss: 0.0092 - val_rpn_bbox_loss: 0.2886 - val_mrcnn_class_loss: 0.3349 - val_mrcnn_bbox_loss: 0.2035 - val_mrcnn_mask_loss: 0.2441`  but I don't know what is the meaning of each indicator, for example what is the difference between rpn_class_loss and val_rpn_class_loss or loss and val_loss, etc.    thanks"
"Hi,    The comment of the output variable: target_class_ids in ""class DetectionTargetLayer()"" is as below:             target_class_ids: [batch, TRAIN_ROIS_PER_IMAGE]. Integer class IDs.      While, the sub-function: compute_output_shape(self, input_shape) defines the output variable: target_class_ids as below, which is not consistent with the comment.            (None, 1),  # class_ids      According to my understanding, the comment is right. Do you have any idea?"
My target values are not in the range        Thanks in advance
"My enviroment is windows 10 x64  ,GPU :GeForce GTX 1060 3GB CUDA9.0 cudnn7.1.4  anaconda under venv: python 3.6  tensorflow-gpu      1.11.0    I only modified balloon.py for `ROOT_DIR = os.path.abspath(""C:/HSIR/Mask_RCNN/"")`    when I typed this:  `python balloon.py train --dataset=C:/HSIR/Mask_RCNN/datasets/balloon_dataset/balloon/ --weights=coco`    it showed the error:  Loading weights  C:\HSIR\Mask_RCNN\mask_rcnn_coco.h5  2018-10-30 14:15:41.495794: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2  2018-10-30 14:15:41.812090: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1405] Found device 0 with properties:  name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.759  pciBusID: 0000:01:00.0  totalMemory: 3.00GiB freeMemory: 2.42GiB  2018-10-30 14:15:41.812250: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1484] Adding visible gpu devices: 0  2018-10-30 14:15:42.486622: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:  2018-10-30 14:15:42.486782: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971]      0  2018-10-30 14:15:42.490780: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:984] 0:   N  2018-10-30 14:15:42.491521: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2125 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:01:00.0, compute capability: 6.1)  Training network heads  Traceback (most recent call last):    File ""balloon.py"", line 364, in        train(model)    File ""balloon.py"", line 199, in train      layers='heads')    File ""C:\HSIR\Mask_RCNN\mrcnn\model.py"", line 2341, in train      histogram_freq=0, write_graph=True, write_images=False),    File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\keras\callbacks.py"", line 745, in __init__      from tensorflow.contrib.tensorboard.plugins import projector    File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\contrib\__init__.py"", line 31, in        from tensorflow.contrib import coder    File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\contrib\coder\__init__.py"", line 22, in        from tensorflow.contrib.coder.python.layers.entropybottleneck import *    File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\contrib\coder\python\layers\entropybottleneck.py"", line 24, in        from tensorflow.contrib.coder.python.ops import coder_ops    File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\contrib\coder\python\ops\coder_ops.py"", line 30, in        resource_loader.get_path_to_datafile(""_coder_ops.so""))    File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\contrib\util\loader.py"", line 56, in load_op_library      ret = load_library.load_op_library(path)    File ""C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\python\framework\load_library.py"", line 56, in load_op_library      lib_handle = py_tf.TF_LoadLibrary(library_filename)  tensorflow.python.framework.errors_impl.NotFoundError: C:\Users\willy_sung\AppData\Local\Continuum\anaconda3\envs\venv\lib\site-packages\tensorflow\contrib\coder\python\ops\_coder_ops.so not found    but when I try some basic function of tensorflow ,nothing is wrong .  can someone help me please?"
"Hi,  I am trying to run the below ""Create model in inference mode"" and  Getting below error message (ValueError ). I am not able find out the exact solution. Please help me  ----------------------------------------------------------------------------------------------------------------------------  # Create model in inference mode  with tf.device(DEVICE):      model = modellib.MaskRCNN(mode=""inference"", model_dir=LOGS_DIR,config=config)  -----------------------------------------------------------------------------------------------------------------------------  Error message:  -------------------------------------------------------------------------------------------------------  ---------------------------------------------------------------------------  ValueError                                Traceback (most recent call last)  /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)      509                 as_ref=input_arg.is_ref,  --> 510                 preferred_dtype=default_dtype)      511           except TypeError as err:    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)     1106     if ret is None:  -> 1107       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)     1108     /miniconda/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_conversion_function(v, dtype, name, as_ref)      959     return NotImplemented  --> 960   return _autopacking_helper(v, inferred_dtype, name or ""packed"")      961     /miniconda/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_helper(list_or_tuple, dtype, name)      921           elems_as_tensors.append(  --> 922               constant_op.constant(elem, dtype=dtype, name=str(i)))      923       return gen_array_ops.pack(elems_as_tensors, name=scope)    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)      195       tensor_util.make_tensor_proto(  --> 196           value, dtype=dtype, shape=shape, verify_shape=verify_shape))      197   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)      423     if values is None:  --> 424       raise ValueError(""None values not supported."")      425     # if dtype is provided, forces numpy array to be the type    ValueError: None values not supported.    During handling of the above exception, another exception occurred:    ValueError                                Traceback (most recent call last)  /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)      523               observed = ops.internal_convert_to_tensor(  --> 524                   values, as_ref=input_arg.is_ref).dtype.name      525             except ValueError as err:    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)     1106     if ret is None:  -> 1107       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)     1108     /miniconda/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_conversion_function(v, dtype, name, as_ref)      959     return NotImplemented  --> 960   return _autopacking_helper(v, inferred_dtype, name or ""packed"")      961     /miniconda/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_helper(list_or_tuple, dtype, name)      921           elems_as_tensors.append(  --> 922               constant_op.constant(elem, dtype=dtype, name=str(i)))      923       return gen_array_ops.pack(elems_as_tensors, name=scope)    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)      195       tensor_util.make_tensor_proto(  --> 196           value, dtype=dtype, shape=shape, verify_shape=verify_shape))      197   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)      423     if values is None:  --> 424       raise ValueError(""None values not supported."")      425     # if dtype is provided, forces numpy array to be the type    ValueError: None values not supported.    During handling of the above exception, another exception occurred:    ValueError                                Traceback (most recent call last)    in  ()        1 # Create model in inference mode        2 with tf.device(DEVICE):  ----> 3     model = modellib.MaskRCNN(mode=""inference"", model_dir=LOGS_DIR, config=config)    /project/bioinformatics/Rajaram_lab/s183574/myCopy/Nuclei-Counting-Segmentation/Mask_RCNN-master/mrcnn/model.py in __init__(self, mode, config, model_dir)     1834         self.model_dir = model_dir     1835         self.set_log_dir()  -> 1836         self.keras_model = self.build(mode=mode, config=config)     1837      1838     def build(self, mode, config):    /project/bioinformatics/Rajaram_lab/s183574/myCopy/Nuclei-Counting-Segmentation/Mask_RCNN-master/mrcnn/model.py in build(self, mode, config)     2035                                      config.POOL_SIZE, config.NUM_CLASSES,     2036                                      train_bn=config.TRAIN_BN,  -> 2037                                      fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE)     2038      2039             # Detections    /project/bioinformatics/Rajaram_lab/s183574/myCopy/Nuclei-Counting-Segmentation/Mask_RCNN-master/mrcnn/model.py in fpn_classifier_graph(rois, feature_maps, image_meta, pool_size, num_classes, train_bn, fc_layers_size)      948     # Reshape to [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))]      949     s = K.int_shape(x)  --> 950     mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name=""mrcnn_bbox"")(x)      951       952     return mrcnn_class_logits, mrcnn_probs, mrcnn_bbox    /miniconda/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)      686       687       if not in_deferred_mode:  --> 688         outputs = self.call(inputs, *args, **kwargs)      689         if outputs is None:      690           raise ValueError('A layer\'s `call` method should return a Tensor '    /miniconda/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py in call(self, inputs)      438   def call(self, inputs):      439     return array_ops.reshape(inputs,  --> 440                              (array_ops.shape(inputs)[0],) + self.target_shape)      441       442   def get_config(self):    /miniconda/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py in reshape(tensor, shape, name)     6195   if _ctx is None or not _ctx._eager_context.is_eager:     6196     _, _, _op = _op_def_lib._apply_op_helper(  -> 6197         ""Reshape"", tensor=tensor, shape=shape, name=name)     6198     _result = _op.outputs[:]     6199     _inputs_flat = _op.inputs    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)      526               raise ValueError(      527                   ""Tried to convert '%s' to a tensor and failed. Error: %s"" %  --> 528                   (input_name, err))      529             prefix = (""Input '%s' of '%s' Op has type %s that does not match"" %      530                       (input_name, op_type_name, observed))    ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported."
It's unclear to me how to get the mean pixel. Do I have to iterate over every image in my training/validation set to calculate the mean pixel?
"I run the code in inspect_model to evaluate the model. I found the RPN training part is working well. But in RPN predictions, there are 6000 pre_nms_anchors. Display shows all those anchors boxes are ALWAYS on top half of the image. It seems the grid of anchors is never covering the whole image. What’s wrong with my config? Thanks.     I use COCO weights and no augmentation. And the ground truth boxes should be distributed randomly in the training dataset.            IMAGE_MIN_DIM = 768      IMAGE_MAX_DIM = 768            RPN_ANCHOR_SCALES = (8, 16, 32, 64,128),      RPN_ANCHOR_RATIOS = [0.5, 1, 2]          # How many anchors per image to use for RPN training      RPN_TRAIN_ANCHORS_PER_IMAGE = 128          RPN_NMS_THRESHOLD = 0.9            # Number of ROIs per image to feed to classifier/mask heads      TRAIN_ROIS_PER_IMAGE = 256      ROI_POSITIVE_RATIO = 0.2            # ROIs kept after non-maximum supression (training and inference)      POST_NMS_ROIS_TRAINING = 1024      POST_NMS_ROIS_INFERENCE = 1024"
"I have a dataset that has original images and masks, so I followed the nucleus sample style to write my own train script. Both original images and masks have the shape of [480,640,3]. When I start training, it keeps reporting error:  `ERROR:root:Error processing image {'id': '000895.jpg', 'source': 'ape', 'path': 'S:\\project\\LINEMOD\\ape\\JPEGImages\\000895.jpg', 'width': 640, 'height': 480}  Traceback (most recent call last):    File ""S:\anaconda\envs\research\lib\site-packages\mask_rcnn-2.1-py3.6.egg\mrcnn\model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""S:\anaconda\envs\research\lib\site-packages\mask_rcnn-2.1-py3.6.egg\mrcnn\model.py"", line 1220, in load_image_gt      mask = utils.resize_mask(mask, scale, padding, crop)    File ""S:\anaconda\envs\research\lib\site-packages\mask_rcnn-2.1-py3.6.egg\mrcnn\utils.py"", line 506, in resize_mask      mask = scipy.ndimage.zoom(mask, zoom=[scale, scale, 1], order=0)    File ""S:\anaconda\envs\research\lib\site-packages\scipy\ndimage\interpolation.py"", line 573, in zoom      zoom = _ni_support._normalize_sequence(zoom, input.ndim)    File ""S:\anaconda\envs\research\lib\site-packages\scipy\ndimage\_ni_support.py"", line 65, in _normalize_sequence      raise RuntimeError(err)  RuntimeError: sequence argument must have length equal to input rank  image_id is this: 000598.jpg  mask_dir is this: S:\project\LINEMOD\ape\mask  masks has 1 items  mask's shape is (480, 640, 3, 1)  ERROR:root:Error processing image {'id': '000598.jpg', 'source': 'ape', 'path': 'S:\\project\\LINEMOD\\ape\\JPEGImages\\000598.jpg', 'width': 640, 'height': 480}  Traceback (most recent call last):    File ""S:\anaconda\envs\research\lib\site-packages\mask_rcnn-2.1-py3.6.egg\mrcnn\model.py"", line 1709, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""S:\anaconda\envs\research\lib\site-packages\mask_rcnn-2.1-py3.6.egg\mrcnn\model.py"", line 1220, in load_image_gt      mask = utils.resize_mask(mask, scale, padding, crop)    File ""S:\anaconda\envs\research\lib\site-packages\mask_rcnn-2.1-py3.6.egg\mrcnn\utils.py"", line 506, in resize_mask      mask = scipy.ndimage.zoom(mask, zoom=[scale, scale, 1], order=0)    File ""S:\anaconda\envs\research\lib\site-packages\scipy\ndimage\interpolation.py"", line 573, in zoom      zoom = _ni_support._normalize_sequence(zoom, input.ndim)    File ""S:\anaconda\envs\research\lib\site-packages\scipy\ndimage\_ni_support.py"", line 65, in _normalize_sequence`    And here is my dataset code.   `class ApeDataset(utils.Dataset):        def load_ape(self, dataset_dir, subset):          """"""Load a subset of the Balloon dataset.          dataset_dir: Root directory of the dataset.          subset: Subset to load: train or val          """"""          # Add classes. We have only one class to add.          self.add_class(""ape"", 1, ""ape"")            # Train or validation dataset?          assert subset in [""train"", ""val""]          if subset == ""train"":              file = ""train.txt""          else:              file = ""test.txt""          file_path = os.path.join(dataset_dir, file)          image_ids = []          with open(file_path, 'r') as f:              for line in f.readlines():                  temp = line.split(""/"")                  image_ids.append(temp[3].strip())          image_dir = ""JPEGImages""          dataset_dir = os.path.join(dataset_dir, image_dir)                     # Add images          for image_id in image_ids:              image_path = os.path.join(dataset_dir, image_id)              image = skimage.io.imread(image_path)              height, width = image.shape[:2]              print(""images shape is :{}"".format(image.shape))              self.add_image(                  ""ape"",                  image_id=image_id,                  path=image_path,                  width=width, height=height)               def load_mask(self, image_id):          """"""Generate instance masks for an image.         Returns:          masks: A bool array of shape [height, width, instance count] with              one mask per instance.          class_ids: a 1D array of class IDs of the instance masks.          """"""          # If not a balloon dataset image, delegate to parent class.          info = self.image_info[image_id]          # Get mask directory from image path          mask_dir = os.path.join(os.path.dirname(os.path.dirname(info['path'])), ""mask"")          # print(""image_id is this: {}"".format(info['id']))          # print(""mask_dir is this: {}"".format(mask_dir))          # Read mask files from .png image          mask = []          for f in next(os.walk(mask_dir))[2]:              if f.endswith("".png""):                  file_name_temp = f.split(""."")                  file_name_temp = file_name_temp[0]                  file_name_temp = file_name_temp+"".jpg""                  # print(""file_name_temp is this: {}"".format(file_name_temp))                  if file_name_temp == info['id'][2:]:                      m = skimage.io.imread(os.path.join(mask_dir, f)).astype(np.bool)                      mask.append(m)          # print(""masks has {} items"".format(len(mask)))          mask = np.stack(mask, axis=-1)          # print(""mask's shape is {}"".format(mask.shape))            # Return mask, and array of class IDs of each instance. Since we have          # one class ID, we return an array of ones          return mask, np.ones([mask.shape[-1]], dtype=np.int32)        def image_reference(self, image_id):          """"""Return the path of the image.""""""          info = self.image_info[image_id]          if info[""source""] == ""ape"":              return info[""path""]          else:              super(self.__class__, self).image_reference(image_id)`  Any help is aprreciated."
"I am trying to detect whether an object is in its right location or not?  For example, there is a table in an image and a pen on the table. I want to detect whether the pen is on the table or not?  May be I have to check whether the bounding box of pen is inside the bounding box of table or not.  Can anyone help, how can I do this?  "
"Hi,  Getting below error message and not able find out the exact solution.  ----------------------------------------------------------------------------------------------------------------------------  # Create model in inference mode  with tf.device(DEVICE):      model = modellib.MaskRCNN(mode=""inference"", model_dir=LOGS_DIR,config=config)  -----------------------------------------------------------------------------------------------------------------------------  Error message:  -------------------------------------------------------------------------------------------------------  ---------------------------------------------------------------------------  ValueError                                Traceback (most recent call last)  /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)      509                 as_ref=input_arg.is_ref,  --> 510                 preferred_dtype=default_dtype)      511           except TypeError as err:    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)     1106     if ret is None:  -> 1107       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)     1108     /miniconda/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_conversion_function(v, dtype, name, as_ref)      959     return NotImplemented  --> 960   return _autopacking_helper(v, inferred_dtype, name or ""packed"")      961     /miniconda/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_helper(list_or_tuple, dtype, name)      921           elems_as_tensors.append(  --> 922               constant_op.constant(elem, dtype=dtype, name=str(i)))      923       return gen_array_ops.pack(elems_as_tensors, name=scope)    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)      195       tensor_util.make_tensor_proto(  --> 196           value, dtype=dtype, shape=shape, verify_shape=verify_shape))      197   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)      423     if values is None:  --> 424       raise ValueError(""None values not supported."")      425     # if dtype is provided, forces numpy array to be the type    ValueError: None values not supported.    During handling of the above exception, another exception occurred:"
"Hi everyone,   I would appreciate if you could help me with my problem:    I have a dataset of 1280*800 images. Currently, I trained the model without modifying the IMAGE_MIN_DIM and the IMAGE_MAX_DIM parameters from the basic config.py script, so basically :            - My original images have dimensions:                                    -- MIN_SIZE_ORIGINAL_DIM: 800                                    -- MAX_SIZE_ORIGINAL_DIM: 1280                   - Mask R-CNN used the model parameters:                                    -- IMAGE_MIN_DIM = 800                                   -- IMAGE_MAX_DIM = 1024    After the training, I tested my weights in inference mode and I could not get masks and images in their original size. Instead, I got 1024*1024 images with zero-padding at the highest and lowest sides of the images. I understand this is normal since Mask R-CNN uses square images (from what I understood from the paper).     I have two questions:     1.- In inference mode, is there a way to obtain images, masks and bboxes in its original 1280*800 size ?       and    2.-  Should I retrain the model, but modifying the config.py parameters to the original dimensions of my images?                                                                             -- IMAGE_MIN_DIM = 800                                                                          -- IMAGE_MAX_DIM = 1280    Thank you in advance for your time."
"hello,  When i predicted images with GPU_COUNT = 1 and i found it used CPU, how to use one GPU to infer?"
"I am running the below step - on my work environment to reproduce the code (using the kaggle data set)    I spent some time and not able to figure it out. Please help me.     I am using pyhon 3.6 and keras - 2.1.6-tf version and Linux    ------------------------------------------------------------------------------------------------------------  # Create model in inference modefrom -    file - I am trying to run the below code (with default setting) and getting the error (copied far below). I  with tf.device(DEVICE):      model = modellib.MaskRCNN(mode = ""inference"", model_dir = LOGS_DIR, config = config)  -------------------------------------------------------------------------------------------------------------------  ---------------------------------------------------------------------------  ValueError                                Traceback (most recent call last)  /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)      509                 as_ref=input_arg.is_ref,  --> 510                 preferred_dtype=default_dtype)      511           except TypeError as err:    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)     1106     if ret is None:  -> 1107       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)     1108     /miniconda/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_conversion_function(v, dtype, name, as_ref)      959     return NotImplemented  --> 960   return _autopacking_helper(v, inferred_dtype, name or ""packed"")      961     /miniconda/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_helper(list_or_tuple, dtype, name)      921           elems_as_tensors.append(  --> 922               constant_op.constant(elem, dtype=dtype, name=str(i)))      923       return gen_array_ops.pack(elems_as_tensors, name=scope)    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)      195       tensor_util.make_tensor_proto(  --> 196           value, dtype=dtype, shape=shape, verify_shape=verify_shape))      197   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)      423     if values is None:  --> 424       raise ValueError(""None values not supported."")      425     # if dtype is provided, forces numpy array to be the type    ValueError: None values not supported.    During handling of the above exception, another exception occurred:    ValueError                                Traceback (most recent call last)  /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)      523               observed = ops.internal_convert_to_tensor(  --> 524                   values, as_ref=input_arg.is_ref).dtype.name      525             except ValueError as err:    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)     1106     if ret is None:  -> 1107       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)     1108     /miniconda/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_conversion_function(v, dtype, name, as_ref)      959     return NotImplemented  --> 960   return _autopacking_helper(v, inferred_dtype, name or ""packed"")      961     /miniconda/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_helper(list_or_tuple, dtype, name)      921           elems_as_tensors.append(  --> 922               constant_op.constant(elem, dtype=dtype, name=str(i)))      923       return gen_array_ops.pack(elems_as_tensors, name=scope)    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)      195       tensor_util.make_tensor_proto(  --> 196           value, dtype=dtype, shape=shape, verify_shape=verify_shape))      197   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)      423     if values is None:  --> 424       raise ValueError(""None values not supported."")      425     # if dtype is provided, forces numpy array to be the type    ValueError: None values not supported.    During handling of the above exception, another exception occurred:    ValueError                                Traceback (most recent call last)    in  ()        2 print (LOGS_DIR)        3 with tf.device(DEVICE):  ----> 4     model = modellib.MaskRCNN(mode = ""inference"", model_dir = LOGS_DIR, config = config)    /project/bioinformatics/Rajaram_lab/s183574/myCopy/Nuclei-Counting-Segmentation/mrcnn/model.py in __init__(self, mode, config, model_dir)     1839         self.model_dir = model_dir     1840         self.set_log_dir()  -> 1841         self.keras_model = self.build(mode=mode, config=config)     1842      1843     def build(self, mode, config):    /project/bioinformatics/Rajaram_lab/s183574/myCopy/Nuclei-Counting-Segmentation/mrcnn/model.py in build(self, mode, config)     2040                                      config.POOL_SIZE, config.NUM_CLASSES,     2041                                      train_bn=config.TRAIN_BN,  -> 2042                                      fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE)     2043      2044             # Detections    /project/bioinformatics/Rajaram_lab/s183574/myCopy/Nuclei-Counting-Segmentation/mrcnn/model.py in fpn_classifier_graph(rois, feature_maps, image_meta, pool_size, num_classes, train_bn, fc_layers_size)      954     # Reshape to [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))]      955     s = K.int_shape(x)  --> 956     mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name=""mrcnn_bbox"")(x)      957       958     return mrcnn_class_logits, mrcnn_probs, mrcnn_bbox    /miniconda/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)      686       687       if not in_deferred_mode:  --> 688         outputs = self.call(inputs, *args, **kwargs)      689         if outputs is None:      690           raise ValueError('A layer\'s `call` method should return a Tensor '    /miniconda/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py in call(self, inputs)      438   def call(self, inputs):      439     return array_ops.reshape(inputs,  --> 440                              (array_ops.shape(inputs)[0],) + self.target_shape)      441       442   def get_config(self):    /miniconda/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py in reshape(tensor, shape, name)     6195   if _ctx is None or not _ctx._eager_context.is_eager:     6196     _, _, _op = _op_def_lib._apply_op_helper(  -> 6197         ""Reshape"", tensor=tensor, shape=shape, name=name)     6198     _result = _op.outputs[:]     6199     _inputs_flat = _op.inputs    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)      526               raise ValueError(      527                   ""Tried to convert '%s' to a tensor and failed. Error: %s"" %  --> 528                   (input_name, err))      529             prefix = (""Input '%s' of '%s' Op has type %s that does not match"" %      530                       (input_name, op_type_name, observed))    ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.       as_ref=input_arg.is_ref,  --> 510                 preferred_dtype=default_dtype)      511           except TypeError as err:    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)     1106     if ret is None:  -> 1107       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)     1108     /miniconda/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_conversion_function(v, dtype, name, as_ref)      959     return NotImplemented  --> 960   return _autopacking_helper(v, inferred_dtype, name or ""packed"")      961     /miniconda/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_helper(list_or_tuple, dtype, name)      921           elems_as_tensors.append(  --> 922               constant_op.constant(elem, dtype=dtype, name=str(i)))      923       return gen_array_ops.pack(elems_as_tensors, name=scope)    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)      195       tensor_util.make_tensor_proto(  --> 196           value, dtype=dtype, shape=shape, verify_shape=verify_shape))      197   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)      423     if values is None:  --> 424       raise ValueError(""None values not supported."")      425     # if dtype is provided, forces numpy array to be the type    ValueError: None values not supported.    During handling of the above exception, another exception occurred:    ValueError                                Traceback (most recent call last)  /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)      523               observed = ops.internal_convert_to_tensor(  --> 524                   values, as_ref=input_arg.is_ref).dtype.name      525             except ValueError as err:    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)     1106     if ret is None:  -> 1107       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)     1108     /miniconda/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_conversion_function(v, dtype, name, as_ref)      959     return NotImplemented  --> 960   return _autopacking_helper(v, inferred_dtype, name or ""packed"")      961     /miniconda/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_helper(list_or_tuple, dtype, name)      921           elems_as_tensors.append(  --> 922               constant_op.constant(elem, dtype=dtype, name=str(i)))      923       return gen_array_ops.pack(elems_as_tensors, name=scope)    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)      195       tensor_util.make_tensor_proto(  --> 196           value, dtype=dtype, shape=shape, verify_shape=verify_shape))      197   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)      423     if values is None:  --> 424       raise ValueError(""None values not supported."")      425     # if dtype is provided, forces numpy array to be the type    ValueError: None values not supported.    During handling of the above exception, another exception occurred:    ValueError                                Traceback (most recent call last)    in  ()        2 print (LOGS_DIR)        3 with tf.device(DEVICE):  ----> 4     model = modellib.MaskRCNN(mode = ""inference"", model_dir = LOGS_DIR, config = config)    /project/bioinformatics/Rajaram_lab/s183574/myCopy/Nuclei-Counting-Segmentation/mrcnn/model.py in __init__(self, mode, config, model_dir)     1839         self.model_dir = model_dir     1840         self.set_log_dir()  -> 1841         self.keras_model = self.build(mode=mode, config=config)     1842      1843     def build(self, mode, config):    /project/bioinformatics/Rajaram_lab/s183574/myCopy/Nuclei-Counting-Segmentation/mrcnn/model.py in build(self, mode, config)     2040                                      config.POOL_SIZE, config.NUM_CLASSES,     2041                                      train_bn=config.TRAIN_BN,  -> 2042                                      fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE)     2043      2044             # Detections    /project/bioinformatics/Rajaram_lab/s183574/myCopy/Nuclei-Counting-Segmentation/mrcnn/model.py in fpn_classifier_graph(rois, feature_maps, image_meta, pool_size, num_classes, train_bn, fc_layers_size)      954     # Reshape to [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))]      955     s = K.int_shape(x)  --> 956     mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name=""mrcnn_bbox"")(x)      957       958     return mrcnn_class_logits, mrcnn_probs, mrcnn_bbox    /miniconda/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)      686       687       if not in_deferred_mode:  --> 688         outputs = self.call(inputs, *args, **kwargs)      689         if outputs is None:      690           raise ValueError('A layer\'s `call` method should return a Tensor '    /miniconda/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py in call(self, inputs)      438   def call(self, inputs):      439     return array_ops.reshape(inputs,  --> 440                              (array_ops.shape(inputs)[0],) + self.target_shape)      441       442   def get_config(self):    /miniconda/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py in reshape(tensor, shape, name)     6195   if _ctx is None or not _ctx._eager_context.is_eager:     6196     _, _, _op = _op_def_lib._apply_op_helper(  -> 6197         ""Reshape"", tensor=tensor, shape=shape, name=name)     6198     _result = _op.outputs[:]     6199     _inputs_flat = _op.inputs    /miniconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)      526               raise ValueError(      527                   ""Tried to convert '%s' to a tensor and failed. Error: %s"" %  --> 528                   (input_name, err))      529             prefix = (""Input '%s' of '%s' Op has type %s that does not match"" %      530                       (input_name, op_type_name, observed))    ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.      "
"For the limit of GPU, I can only train the network using a small batch size. And because of it, I get lower performance.     So, can we train the network using a large batch size on single GPU?    For example, if our GPU can only handle only one image, can we accumulate the gradients for 16 steps and then update the trainable parameters, so as to equivalent to the batch size of 16?    I am not sure if it is reasonable, and I do not know how to implement it. Can you give me some helps?"
"hi，matterport.  When i train a model with coco.py, i want to modify STEPS_PER_EPOCH=100, IMAGE_MAX_DIM=800 in config.py. However, these values have not changed.   BTW, print() in model.py can't print the log.    The command is as follows:  python3 coco.py --dataset=/home/xulinquan/data/other_data/coco/ --logs=../../logs train --year=2017 --model=../../mask_rcnn_coco_0149.h5    the logs:  !     how to fix it?"
"OutPut Message:  ---------------------------------------------------------------------------  AttributeError                            Traceback (most recent call last)    in  ()       29 from mrcnn import visualize       30 from mrcnn.visualize import display_images  ---> 31 from mrcnn import model as modellib       32 from mrcnn.model import log       33     /project/bioinformatics/Rajaram_lab/s183574/myCopy/Nuclei-Counting-Segmentation/mrcnn/model.py in  ()       19 import tensorflow as tf       20 import tensorflow.python.keras  ---> 21 import tensorflow.python.keras.backend as K       22 import tensorflow.python.keras.layers as KL       23 import tensorflow.python.keras.engine as KE    AttributeError: module 'tensorflow' has no attribute 'python'    --------------------------------------------------------------------------------------------------  Code:  ---------------------------------------------------------------------------------------------------  import os  import sys  import itertools  import math  import logging  import json  import re  import random  import time  import concurrent.futures  import numpy as np  import matplotlib  import matplotlib.pyplot as plt  import matplotlib.patches as patches  import matplotlib.lines as lines  from matplotlib.patches import Polygon  #import imgaug  #from imgaug import augmenters as iaa    # Root directory of the project  ROOT_DIR = os.getcwd()  if ROOT_DIR.endswith(""samples/nucleus""):      # Go up two levels to the repo root      ROOT_DIR = os.path.dirname(os.path.dirname(ROOT_DIR))        # Import Mask RCNN  sys.path.append(ROOT_DIR)  from mrcnn import utils  from mrcnn import visualize  from mrcnn.visualize import display_images  from mrcnn import model as modellib  from mrcnn.model import log    import nucleus    %matplotlib inline "
"During the training process, validation_data = next(val_generator) in mask_rcnn_2.1, while validation_data = val_generator in mask_rcnn_2.0, what is the difference?    #model.py of version mask_rcnn_2.1    self.keras_model.fit_generator(              train_generator,              initial_epoch=self.epoch,              epochs=epochs,              steps_per_epoch=self.config.STEPS_PER_EPOCH,              callbacks=callbacks,              # validation_data = val_generator in version 2.0              **validation_data=next(val_generator),**              validation_steps=self.config.VALIDATION_STEPS,              max_queue_size=100,              workers=workers,              use_multiprocessing=True,          )"
The Linemod dataset has mask (binary images) images available but no bounding boxes or corresponding JSON file. I want to train the Mask RCNN on it but not sure how to properly modify the dataset. Is there any instruction to do this?    Any help is appreciated.
"I try to  predict with  GPU_COUNT = 2, but got the issue:    <img width=""1120"" alt=""screen shot 2018-10-20 at 8 38 15 pm"" src=""     `keras.__version__ = 2.1.5`  and  `tf.__version__=1.7.0`  and my config was:           And it worked fine with single gpu like that:             "
"I got an error message ""The kernel appears to have died. It will restart automatically"" on Paperspace, when I ran the first cell of ""demo.ipynb"".    This message would appear when ""from mrcnn import utils"" is called.    I will really appreciate it if you could tell me how to address this problem."
"Hello,     >         print(""Training network heads"")  >         model.train(dataset_train, dataset_val,  >                     learning_rate=config.LEARNING_RATE,  >                     epochs=1,  >                     layers='heads',  >                     augmentation=augmentation)    The layers='heads' means that when I train the network, only the part of heads is trained, and other parts will not participate in the training. If I need to train a new model without loading Pre-training weight，should the configuration of model.train be set to ‘layers = 'all' ?"
"@waleedka I used maskrcnn with different datasets and it is working fine. Now, I need to use faster rcnn for some reasons. MaskRCNN is equal to FasterRCNN + mask branch. I am wondering if possible to modify this code to use it as Faster RCNN only during inference mode (I don't need mask). I debug the code and everything about inference is done in the `detect()` function which call the keras Model predict function. I am thinking about redefine keras `predict()` function. Any suggestion?"
"Hello,     I've trained the model multiple times on my own image dataset.    The first time I trained it from scratch. Next, I stopped the model and then I trained it again using the last trained weights (I used the method find_last from the class MaskRCNN).   After that, I have stopped the training and restarted multiple times, always using the last trained weights.     I displayed all event files I obtained in Tensorboard. What called my attention was the fact that all loss curves start from the same point.    Intuitively, I'd expect the curves to start at different points because each curve calculates the initial loss with with the (optimized) weights from the preceding training. So normally I would expect to see that each new curve starts in a lower lever that the previous one.    Clearly this is not happening.    Could anyone explain me why is that?    By the way, I use SGD optimizer and I have a batch size of 1 image.     !   "
"First, i try to modify this     !     then, i commented this line of code    !     so,modify this code   !     last   !     but ,then i run this detect code ,the error occurred     !     the error occurred in this code    !         when i try to use some other method ,also failed, i need your help please ,thanks very mach"
"When training multi-gpu with 2 Rtx 2080 Ti, first i got various Memory Errors.  I solved them putting   use_multiprocessing=False   and  IMAGE_RESIZE_MODE = ""crop""  IMAGE_MIN_DIM = 512  IMAGE_MAX_DIM = 512  on Config.   But then when training i got Segmentation Fault (core dumped).      The following is the DockerFile i used for the installation.    ##### Base Image setup  FROM tensorflow/tensorflow:latest-gpu-py3 #         RUN apt-get update -y    RUN apt-get install -y git python-pip wget    RUN apt install libnccl2=2.3.5-2+cuda9.0 libnccl-dev=2.3.5-2+cuda9.0    ##### Install the Tensorflow Object Detection API from here  #####      RUN mkdir -p /tensorflow/models  RUN git clone   /tensorflow/models    RUN apt-get install -y protobuf-compiler python-pil python-lxml python-tk && \      pip install Cython && \      pip install contextlib2 && \      pip install jupyter && \      pip install matplotlib    RUN git clone --depth 1   && \      cd cocoapi/PythonAPI && \      python3 setup.py build_ext --inplace && \      rm -rf build && \      python3 setup.py build_ext install && \      rm -rf build && \      cp -r pycocotools /tensorflow/models/research && \      cd ../../ && \      rm -rf cocoapi    RUN curl -OL ""  && \      unzip protoc-3.0.0-linux-x86_64.zip -d proto3 && \      mv proto3/bin/* /usr/local/bin && \      mv proto3/include/* /usr/local/include && \      rm -rf proto3 protoc-3.0.0-linux-x86_64.zip    RUN cd /tensorflow/models/research && \      protoc object_detection/protos/*.proto --python_out=.    ENV PYTHONPATH $PYTHONPATH:/tensorflow/models/research:/tensorflow/models/research/slim    ##### Install Mask R-Cnn from here  #####      RUN cd /tensorflow/models/research/object_detection && mkdir -p /mask_rcnn  RUN git clone   /tensorflow/models/research/object_detection/mask_rcnn && \      rm -rf /tensorflow/models/research/object_detection/mask_rcnn/mrcnn    COPY mrcnn /tensorflow/models/research/object_detection/mask_rcnn/mrcnn    RUN mkdir -p /tensorflow/models/research/object_detection/mask_rcnn/coco && \      wget -q -O /tensorflow/models/research/object_detection/mask_rcnn/coco/mask_rcnn_coco.h5      RUN cd /tensorflow/models/research/object_detection/mask_rcnn && \         pip3 install numpy && \         pip3 install scipy && \         pip3 install Pillow && \         pip3 install cython && \         pip3 install matplotlib && \         pip3 install scikit-image && \         pip3 uninstall keras -y && \         pip3 install keras==2.1.3 && \         pip3 install opencv-python && \         pip3 install h5py && \         pip3 install imgaug && \         pip3 install IPython[all] && \         python3 setup.py install    RUN apt update && apt install -y libsm6 libxext6    ##### Front-end Configuration    EXPOSE 6006    EXPOSE 8888    COPY custom /tensorflow/models/research/object_detection/mask_rcnn/custom    WORKDIR /tensorflow/models/research/object_detection/mask_rcnn    CMD [""jupyter"", ""notebook"", ""--allow-root""]        Anyone has experienced this error?  Thanks"
"Dear all, I tried the balloon sample.  When I trained the model with balloon dataset, on windows, it was ok.   But when I tried in a GPU server, it gave the following error:         I have installed all the requirements.  I have done also the setup.py    My environment in the server:  HPCT R 425 gs - 10 GP | Ubuntu Server 16.04 | | 385 GB | Titan V * 10    Suggestions and solutions are very welcomed    "
I've got a nice augmentation setup using the great imgaug library that was chosen for this project but I'm not certain how I should change my training parameters given this augmentation.    I'm training basically using the same example from the imgaug documentation with some minor changes:       So given that this code basically runs these augmentations randomly for 5 out of 6 images should I increase my steps per epoch x6? Should I increase my num of epochs x6? How does the augmentation actually get applied during training?     I see it is applied in load_image_gt() which is called in the data_generator() but I'm not totally clear what should change in my training regimen based on these augmentations.
"Hi,  How can I set different color for different class? For example, I want all the cars to have same color, and all lamposts have same color but different from cars color."
It might be a dependency problem but any idea what could cause an EOFError on multiprocessing fit_generator?  Thanks heaps for any help and for sharing this lib in the first place!    stack:         other people with same error:  
"I've already followed the installation steps  , and have CUDA and cuDNN installed. However, when I try to use one of the sample files for training, it seems like it's using CPU rather than utilizing GPU:       The program gets stuck for about a minute after the last line.    While the training is running, the GPU usage doesn't change at all:         However,  it'll try to devour as much CPU power as possible. Below is a screenshot of htop monitor while it's running:  !      None of the files related to this training has been altered from the current version of the repo.    "
I stumbled across this   blog and was wondering how can I apply the following technique to my training and test data:            Thank you in advance for your assistance.   
"    My json file looks like this.  {""Circle1.jpg36854"":{""fileref"":"""",""size"":36854,""filename"":""Circle1.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_{""Circle1.jpg36854"":{""fileref"":"""",""size"":36854,""filename"":""Circle1.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_poinx"":[539,600,655,700,712,716,710,696,681,630,577,524,486,460,452,453,466,478,492,519,539],""all_points_y"":[287,280,303,345,385,417,450,482,509,539,548,537,507,460,418,395,352,332,315,299,287]},""region_attributes"":{""class_ids"":""Circle""}}}},""Circle2.jpg36075"":{""fileref"":"""",""size"":36075,""filename"":""Circle2.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[485,528,557,594,625,699,734,733,725,697,636,571,532,497,469,449,442,442,460,485],""all_points_y"":[308,280,273,270,276,318,394,423,462,512,554,560,550,530,506,473,432,368,337,308]},""region_attributes"":{""class_ids"":""Circle""}}}},""Circle3.jpg36206"":{""fileref"":"""",""size"":36206,""filename"":""Circle3.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[454,487,527,578,627,665,694,723,736,739,717,682,641,591,550,509,484,443,427,433,454],""all_points_y"":[327,291,267,257,263,278,299,336,382,441,497,536,559,568,567,550,531,490,438,362,327]},""region_attributes"":{""class_ids"":""Circle""}}}},""Circle4.jpg36748"":{""fileref"":"""",""size"":36748,""filename"":""Circle4.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[444,472,508,554,588,617,652,700,733,754,747,727,688,628,562,490,447,417,414,432,444],""all_points_y"":[322,285,266,249,248,247,256,290,328,396,460,506,549,577,582,557,513,457,391,339,322]},""region_attributes"":{""class_ids"":""Circle""}}}},""Circle5.jpg37021"":{""fileref"":"""",""size"":37021,""filename"":""Circle5.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[413,436,460,481,505,533,559,586,633,672,701,749,769,755,720,658,597,521,479,433,414,405,405,413],""all_points_y"":[359,307,283,265,252,242,236,235,239,256,274,330,428,480,538,576,593,584,560,518,469,425,378,359]},""region_attributes"":{""class_ids"":""Circle""}}}},""Circle6.jpg37335"":{""fileref"":"""",""size"":37335,""filename"":""Circle6.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[450,504,567,632,688,736,776,775,756,698,640,585,514,452,418,394,388,415,439,450],""all_points_y"":[277,237,222,229,247,290,374,456,513,571,598,608,596,561,521,458,394,318,288,277]},""region_attributes"":{""class_ids"":""Circle""}}}},""Circle7.jpg37619"":{""fileref"":"""",""size"":37619,""filename"":""Circle7.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[404,463,536,642,711,761,798,755,692,622,550,479,419,383,381,395,404],""all_points_y"":[317,249,212,214,252,310,416,534,587,615,619,592,542,459,375,332,317]},""region_attributes"":{""class_ids"":""Circle""}}}},""Circle8.jpg37791"":{""fileref"":"""",""size"":37791,""filename"":""Circle8.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[389,417,440,470,525,590,648,741,790,800,792,732,686,619,555,475,430,399,375,369,377,389],""all_points_y"":[320,278,254,231,206,199,208,249,348,402,494,571,604,626,628,604,567,525,471,415,347,320]},""region_attributes"":{""class_ids"":""Circle""}}}},""Circle9.jpg37931"":{""fileref"":"""",""size"":37931,""filename"":""Circle9.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[426,475,513,551,592,641,685,721,761,788,808,810,795,729,701,650,611,524,462,427,388,361,358,374,406,426],""all_points_y"":[251,215,198,190,186,195,210,233,272,312,375,436,512,594,610,632,642,637,609,581,530,462,391,314,272,251]},""region_attributes"":{""class_ids"":""Circle""}}}},""Circle10.jpg38455"":{""fileref"":"""",""size"":38455,""filename"":""Circle10.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[378,408,433,464,494,539,608,660,717,791,827,815,775,690,602,522,427,357,344,356,378],""all_points_y"":[295,253,230,210,195,179,177,189,211,290,387,486,566,630,653,650,605,501,408,338,295]},""region_attributes"":{""class_ids"":""Circle""}}}},""D2Rectangle1.jpg33355"":{""fileref"":"""",""size"":33355,""filename"":""D2Rectangle1.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[484,487,589,589,484],""all_points_y"":[371,542,543,367,371]},""region_attributes"":{""class_ids"":""Rectangle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[510,511,685,682,510],""all_points_y"":[274,383,383,276,274]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""D2Rectangle2.jpg33318"":{""fileref"":"""",""size"":33318,""filename"":""D2Rectangle2.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[481,479,593,592,481],""all_points_y"":[369,557,558,369,369]},""region_attributes"":{""class_ids"":""Rectangle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[504,506,690,689,504],""all_points_y"":[267,385,383,267,267]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""D2Rectangle3.jpg35098"":{""fileref"":"""",""size"":35098,""filename"":""D2Rectangle3.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[476,476,599,598,476],""all_points_y"":[374,566,566,369,374]},""region_attributes"":{""class_ids"":""Rectangle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[498,500,697,699,498],""all_points_y"":[255,387,385,256,255]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""D2Rectangle4.jpg34855"":{""fileref"":"""",""size"":34855,""filename"":""D2Rectangle4.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[469,469,604,604,469],""all_points_y"":[371,582,582,366,371]},""region_attributes"":{""class_ids"":""Rectangle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[492,493,704,706,492],""all_points_y"":[242,385,385,241,242]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""D2Rectangle5.jpg35080"":{""fileref"":"""",""size"":35080,""filename"":""D2Rectangle5.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[464,464,610,609,464],""all_points_y"":[369,591,592,371,369]},""region_attributes"":{""class_ids"":""Rectangle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[488,487,709,708,488],""all_points_y"":[232,385,384,229,232]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""D2Rectangle6.jpg35980"":{""fileref"":"""",""size"":35980,""filename"":""D2Rectangle6.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[455,456,619,618,455],""all_points_y"":[369,609,606,367,369]},""region_attributes"":{""class_ids"":""Rectangle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[479,480,716,714,479],""all_points_y"":[221,384,382,218,221]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""D2Rectangle7.jpg36202"":{""fileref"":"""",""size"":36202,""filename"":""D2Rectangle7.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[451,452,625,624,451],""all_points_y"":[370,614,614,371,370]},""region_attributes"":{""class_ids"":""Rectangle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[473,475,720,717,473],""all_points_y"":[207,383,383,207,207]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""D2Rectangle8.jpg36269"":{""fileref"":"""",""size"":36269,""filename"":""D2Rectangle8.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[444,445,638,635,444],""all_points_y"":[374,630,629,366,374]},""region_attributes"":{""class_ids"":""Rectangle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[465,468,730,729,465],""all_points_y"":[194,390,387,195,194]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""D2Rectangle9.jpg36416"":{""fileref"":"""",""size"":36416,""filename"":""D2Rectangle9.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[439,439,634,637,439],""all_points_y"":[372,637,639,368,372]},""region_attributes"":{""class_ids"":""Rectangle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[463,464,732,731,463],""all_points_y"":[187,385,382,184,187]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""DRectangle1.jpg34920"":{""fileref"":"""",""size"":34920,""filename"":""DRectangle1.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[429,429,637,635,429],""all_points_y"":[407,520,517,400,407]},""region_attributes"":{""class_ids"":""Rectangle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[622,626,742,742,622],""all_points_y"":[309,500,501,306,309]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""DRectangle2.jpg35769"":{""fileref"":"""",""size"":35769,""filename"":""DRectangle2.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[415,415,635,636,415],""all_points_y"":[402,526,527,398,402]},""region_attributes"":{""class_ids"":""Rectangle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[624,623,755,756,624],""all_points_y"":[302,509,507,297,302]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""DRectangle3.jpg35946"":{""fileref"":"""",""size"":35946,""filename"":""DRectangle3.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[400,398,637,636,400],""all_points_y"":[401,535,535,395,401]},""region_attributes"":{""class_ids"":""Rectangle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[628,630,771,770,628],""all_points_y"":[292,518,519,286,292]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""DRectangle4.jpg36317"":{""fileref"":"""",""size"":36317,""filename"":""DRectangle4.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[386,386,647,643,386],""all_points_y"":[391,541,540,387,391]},""region_attributes"":{""class_ids"":""Rectangle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[627,629,785,786,627],""all_points_y"":[280,523,522,279,280]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""DRectangle5.jpg36283"":{""fileref"":"""",""size"":36283,""filename"":""DRectangle5.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[374,375,642,641,374],""all_points_y"":[392,549,549,389,392]},""region_attributes"":{""class_ids"":""Rectangle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[629,631,800,797,629],""all_points_y"":[272,535,532,272,272]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""Rectangle1.jpg33807"":{""fileref"":"""",""size"":33807,""filename"":""Rectangle1.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[508,510,639,638,508],""all_points_y"":[315,406,405,312,315]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""Rectangle2.jpg34547"":{""fileref"":"""",""size"":34547,""filename"":""Rectangle2.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[499,499,650,648,499],""all_points_y"":[305,420,418,301,305]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""Rectangle3.jpg34210"":{""fileref"":"""",""size"":34210,""filename"":""Rectangle3.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[487,486,659,658,487],""all_points_y"":[298,431,430,293,298]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""Rectangle4.jpg34496"":{""fileref"":"""",""size"":34496,""filename"":""Rectangle4.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[478,477,676,672,478],""all_points_y"":[288,443,440,283,288]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""Rectangle5.jpg34201"":{""fileref"":"""",""size"":34201,""filename"":""Rectangle5.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[468,468,677,677,468],""all_points_y"":[277,450,449,272,277]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""Rectangle6.jpg35177"":{""fileref"":"""",""size"":35177,""filename"":""Rectangle6.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[459,461,688,687,459],""all_points_y"":[264,457,459,261,264]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""Rectangle30.jpg33247"":{""fileref"":"""",""size"":33247,""filename"":""Rectangle30.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[468,469,675,676,468],""all_points_y"":[209,510,510,212,209]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""Rectangle31.jpg33932"":{""fileref"":"""",""size"":33932,""filename"":""Rectangle31.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[464,464,683,684,464],""all_points_y"":[208,513,514,209,208]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""Rectangle32.jpg33456"":{""fileref"":"""",""size"":33456,""filename"":""Rectangle32.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[458,456,689,687,458],""all_points_y"":[204,519,518,201,204]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""Rectangle33.jpg34054"":{""fileref"":"""",""size"":34054,""filename"":""Rectangle33.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[450,449,694,693,450],""all_points_y"":[198,522,524,197,198]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""Rectangle34.jpg33632"":{""fileref"":"""",""size"":33632,""filename"":""Rectangle34.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[447,446,701,703,447],""all_points_y"":[197,525,526,194,197]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""Rectangle35.jpg34133"":{""fileref"":"""",""size"":34133,""filename"":""Rectangle35.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[439,440,709,706,439],""all_points_y"":[189,529,528,188,189]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""Rectangle36.jpg33902"":{""fileref"":"""",""size"":33902,""filename"":""Rectangle36.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[434,433,713,712,434],""all_points_y"":[184,536,534,182,184]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""RectangleCircle1.jpg36599"":{""fileref"":"""",""size"":36599,""filename"":""RectangleCircle1.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[494,466,441,419,400,387,417,460,496,534,557,581,601,605,593,574,558,525,494],""all_points_y"":[346,353,365,380,415,459,529,555,561,554,539,518,484,460,428,393,364,348,346]},""region_attributes"":{""class_ids"":""Circle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[489,487,782,780,489],""all_points_y"":[267,460,458,263,267]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""RectangleCircle2.jpg36706"":{""fileref"":"""",""size"":36706,""filename"":""RectangleCircle2.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[487,457,428,403,390,377,382,395,411,490,542,571,602,613,614,605,583,569,545,503,487],""all_points_y"":[336,343,355,376,404,432,488,519,543,572,566,543,515,479,457,431,400,374,355,339,336]},""region_attributes"":{""class_ids"":""Circle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[487,491,790,789,487],""all_points_y"":[255,463,459,250,255]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""RectangleCircle3.jpg37551"":{""fileref"":"""",""size"":37551,""filename"":""RectangleCircle3.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[487,442,407,372,368,384,402,422,458,491,544,580,608,619,618,616,604,589,538,516,487],""all_points_y"":[326,337,361,407,478,518,541,559,574,577,574,549,514,481,453,440,411,384,341,328,326]},""region_attributes"":{""class_ids"":""Circle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[484,485,805,804,484],""all_points_y"":[247,464,464,243,247]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""RectangleCircle4.jpg37814"":{""fileref"":"""",""size"":37814,""filename"":""RectangleCircle4.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[482,418,390,369,358,357,393,439,474,549,581,628,627,603,583,556,531,498,477,482],""all_points_y"":[320,342,363,397,440,495,552,577,592,579,556,493,452,397,373,351,334,321,319,320]},""region_attributes"":{""class_ids"":""Circle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[482,487,814,812,482],""all_points_y"":[241,458,459,241,241]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""RectangleCircle5.jpg37836"":{""fileref"":"""",""size"":37836,""filename"":""RectangleCircle5.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[486,453,407,390,358,343,353,389,486,543,582,620,631,632,617,607,585,564,546,501,486],""all_points_y"":[311,318,337,351,391,441,509,568,606,590,563,521,479,447,412,392,369,346,331,313,311]},""region_attributes"":{""class_ids"":""Circle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[483,484,825,826,483],""all_points_y"":[227,458,459,230,227]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""RectangleCircle6.jpg38168"":{""fileref"":"""",""size"":38168,""filename"":""RectangleCircle6.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[479,429,394,369,342,334,337,384,484,568,602,627,636,637,621,592,568,530,493,479],""all_points_y"":[303,314,331,358,398,443,514,578,615,586,557,514,478,452,412,374,344,321,306,303]},""region_attributes"":{""class_ids"":""Circle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[479,481,836,836,479],""all_points_y"":[216,466,463,221,216]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""RectangleCircleCenter1.jpg38298"":{""fileref"":"""",""size"":38298,""filename"":""RectangleCircleCenter1.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[401,402,772,768,401],""all_points_y"":[289,544,544,289,289]},""region_attributes"":{""class_ids"":""Rectangle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[543,514,503,502,520,537,572,624,665,672,627,574,543],""all_points_y"":[337,366,393,435,467,484,498,492,456,378,333,325,337]},""region_attributes"":{""class_ids"":""Circle""}}}},""RectangleCircleCenter2.jpg37762"":{""fileref"":"""",""size"":37762,""filename"":""RectangleCircleCenter2.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[553,515,506,504,524,538,566,593,642,663,670,678,666,628,585,548,553],""all_points_y"":[331,360,391,435,469,481,493,498,483,455,440,410,360,326,317,335,331]},""region_attributes"":{""class_ids"":""Circle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[397,399,772,774,397],""all_points_y"":[282,542,540,281,282]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""RectangleCircleCenter3.jpg38358"":{""fileref"":"""",""size"":38358,""filename"":""RectangleCircleCenter3.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[532,507,503,539,603,655,683,675,621,560,532],""all_points_y"":[336,382,436,484,499,477,427,359,315,320,336]},""region_attributes"":{""class_ids"":""Circle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[395,396,773,773,395],""all_points_y"":[283,544,545,279,283]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""RectangleCircleCenter4.jpg37947"":{""fileref"":"""",""size"":37947,""filename"":""RectangleCircleCenter4.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[560,519,510,505,516,534,557,584,633,656,679,690,668,640,604,560],""all_points_y"":[316,353,378,416,449,470,485,494,486,472,444,390,341,324,310,316]},""region_attributes"":{""class_ids"":""Circle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[391,392,779,779,391],""all_points_y"":[275,550,548,275,275]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""RectangleCircleCenter5.jpg38447"":{""fileref"":"""",""size"":38447,""filename"":""RectangleCircleCenter5.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[590,546,515,504,509,518,556,601,630,673,686,698,678,664,623,590],""all_points_y"":[307,318,356,393,421,455,483,492,492,462,432,391,346,323,310,307]},""region_attributes"":{""class_ids"":""Circle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[386,387,785,784,386],""all_points_y"":[269,555,553,267,269]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""RectangleCircleCenter6.jpg37823"":{""fileref"":"""",""size"":37823,""filename"":""RectangleCircleCenter6.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[562,532,513,506,512,534,569,614,641,664,682,695,696,669,627,597,562],""all_points_y"":[307,329,360,388,436,466,486,491,482,464,446,417,362,319,299,299,307]},""region_attributes"":{""class_ids"":""Circle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[384,385,789,788,384],""all_points_y"":[266,558,556,265,266]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""RectangleCircleCenter14.jpg40658"":{""fileref"":"""",""size"":40658,""filename"":""RectangleCircleCenter14.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[491,464,451,457,475,511,553,633,688,766,809,804,792,763,717,653,607,561,515,491],""all_points_y"":[275,325,373,430,489,530,555,571,560,513,431,363,309,270,231,214,214,225,250,275]},""region_attributes"":{""class_ids"":""Circle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[357,358,813,810,357],""all_points_y"":[263,616,612,260,263]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""RectangleCircleCenter15.jpg40470"":{""fileref"":"""",""size"":40470,""filename"":""RectangleCircleCenter15.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[527,480,456,445,454,486,539,604,655,696,741,795,820,800,770,736,691,628,563,527],""all_points_y"":[237,282,333,378,455,512,555,573,575,564,545,484,398,321,270,239,215,205,216,237]},""region_attributes"":{""class_ids"":""Circle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[353,355,815,811,353],""all_points_y"":[261,624,624,266,261]},""region_attributes"":{""class_ids"":""Rectangle""}}}},""RectangleCircleCenter16.jpg40433"":{""fileref"":"""",""size"":40433,""filename"":""RectangleCircleCenter16.jpg"",""base64_img_data"":"""",""file_attributes"":{},""regions"":{""0"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[469,444,440,455,483,562,609,639,685,728,766,821,824,803,762,716,671,632,565,489,469],""all_points_y"":[286,349,418,471,519,572,580,583,580,559,531,444,381,305,238,210,199,194,202,251,286]},""region_attributes"":{""class_ids"":""Circle""}},""1"":{""shape_attributes"":{""name"":""polygon"",""all_points_x"":[345,346,821,817,345],""all_points_y"":[261,628,626,258,261]},""region_attributes"":{""class_ids"":""Rectangle""}}}}}        My load function and load mask are as follows.    class BalloonDataset(utils.Dataset):        def load_balloon(self, dataset_dir, subset):          """"""Load a subset of the Balloon dataset.          dataset_dir: Root directory of the dataset.          subset: Subset to load: train or val          """"""          # Add classes. We have only one class to add.          self.add_class(""balloon"", 1, ""Circle"")          self.add_class(""balloon"", 2, ""Rectangle"")            # Train or validation dataset?          assert subset in [""train"", ""val""]          dataset_dir = os.path.join(dataset_dir, subset)            # Load annotations          # VGG Image Annotator saves each image in the form:          # { 'filename': '28503151_5b5b7ec140_b.jpg',          #   'regions': {          #       '0': {          #           'region_attributes': {},          #           'shape_attributes': {          #               'all_points_x': [...],          #               'all_points_y': [...],          #               'name': 'polygon'}},          #       ... more regions ...          #   },          #   'size': 100202          # }          # We mostly care about the x and y coordinates of each region          annotations = json.load(open(os.path.join(dataset_dir, ""via_region_data.json"")))          annotations = list(annotations.values())  # don't need the dict keys            # The VIA tool saves images in the JSON even if they don't have any          # annotations. Skip unannotated images.          annotations = [a for a in annotations if a['regions']]            # Add images          for a in annotations:              # Get the x, y coordinaets of points of the polygons that make up              # the outline of each object instance. There are stores in the              # shape_attributes (see json format above)              polygons = [r['shape_attributes'] for r in a['regions'].values()]              objects = [s['region_attributes'] for s in a['regions'].values()]              print(objects)                            num_ids = [int(n['class_ids']) for n in objects]                              # load_mask() needs the image size to convert polygons to masks.              # Unfortunately, VIA doesn't include it in JSON, so we must read              # the image. This is only managable since the dataset is tiny.              image_path = os.path.join(dataset_dir, a['filename'])              image = skimage.io.imread(image_path)              height, width = image.shape[:2]                self.add_image(                  ""balloon"",                  image_id=a['filename'],  # use file name as a unique image id                  path=image_path,                  width=width, height=height,                  polygons=polygons,                  num_ids=num_ids)        def load_mask(self, image_id):          """"""Generate instance masks for an image.         Returns:          masks: A bool array of shape [height, width, instance count] with              one mask per instance.          class_ids: a 1D array of class IDs of the instance masks.          """"""          # If not a balloon dataset image, delegate to parent class.          info = self.image_info[image_id]          if info[""source""] != ""balloon"":              return super(self.__class__, self).load_mask(image_id)          num_ids = info['num_ids']          # Convert polygons to a bitmap mask of shape          # [height, width, instance_count]                    mask = np.zeros([info[""height""], info[""width""], len(info[""polygons""])],                          dtype=np.uint8)          for i, p in enumerate(info[""polygons""]):              # Get indexes of pixels inside the polygon and set them to 1              rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])              mask[rr, cc, i] = 1            # Return mask, and array of class IDs of each instance. Since we have          # one class ID only, we return an array of 1s          num_ids = np.array(num_ids, dtype=np.int32)          return mask, num_ids        def image_reference(self, image_id):          """"""Return the path of the image.""""""          info = self.image_info[image_id]          if info[""source""] == ""balloon"":              return info[""path""]          else:              super(self.__class__, self).image_reference(image_id)    Error message:  Loading weights  C:\Windows\System32\Mask_RCNN\mask_rcnn_coco.h5  2018-10-04 15:59:41.117001: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2  2018-10-04 15:59:41.373850: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1105] Found device 0 with properties:  name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.392  pciBusID: 0000:01:00.0  totalMemory: 4.00GiB freeMemory: 3.29GiB  2018-10-04 15:59:41.381220: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)  [{'class_ids': 'Circle'}]  Traceback (most recent call last):    File ""balloon.py"", line 366, in        train(model)    File ""balloon.py"", line 185, in train      dataset_train.load_balloon(args.dataset, ""train"")    File ""balloon.py"", line 128, in load_balloon      num_ids = [int(n['class_ids']) for n in objects]    File ""balloon.py"", line 128, in        num_ids = [int(n['class_ids']) for n in objects]  ValueError: invalid literal for int() with base 10: 'Circle'      I am new to MaskRCNN. How do i solve this?  "
"Hello,  I am training a model with training dataset (high precision but relatively low recall). As an example, assuming that 10 cars appear in an image but for some reasons (e.g., missing small cars), we only successfully annotated 6 cars as ""car"". The rest of 4 cars were labeled as ""ignore"" or similar, that I want to ignore in training phase.  I think that those 4 cars should not be ""BG"" because a classifier trained with this dataset will confuse between ""car"" and ""BG"" which in turn may lead to decrease the performance.    A similar example can be found from Segnet ""  that ignores a user-specified label in loss calculation in softmax.    Is there any similar functionality in Mask_RCNN or anyone confronted a similar issue?  Thanks. "
hi everybody  i installed tensorflow with 'tensorflow-gpu==1.5' but when i run the demo.ipynb i get this error:  ModuleNotFoundError                       Traceback (most recent call last)    in  ()       13 # Import Mask RCNN       14 sys.path.append(ROOT_DIR)  # To find local version of the library  ---> 15 from mrcnn import utils       16 import mrcnn.model as modellib       17 from mrcnn import visualize    ~/Mask_RCNN/mrcnn/utils.py in  ()       13 import random       14 import numpy as np  ---> 15 import tensorflow as tf       16 import scipy       17 import skimage.color    ModuleNotFoundError: No module named 'tensorflow'  what can i do?
"!     Hello, when i train the model on my own model, i came across this issue,  and the detailed configuration is as below, can anyone help?  ps: keras (2.1.5)   tensorflow (1.4.0)  !   "
"The file utils.py inside $ROOT_DIR/mrcnn tried fetching the pretrained model from link :   However the link is not working, Please update the working link for fetching the pretrained model .      Thanks!"
Hi  I am trying to run MaskR-CNN on google cloud. I created a VM with 2 GPUS (Tesla K80)    I have installed:       When I set `GPU_COUNT = 2`  in the configuration and try to initialise the model      i get the following error:         It can run on a GPU but after a while the training crashes due to memory issues.    Any idea?  Thanks for the help!    NB: The output of       is                 
"Here is the experimental result about drivable area segmentation with mask-rcnn, the test frame rate is 3-4 , I would like to ask about any good implementation of network acceleration.    Test Video:   "
"If you have 11 classes, you get the following error when you want to visualize:         Here is how I  try to visualize in the code:         and here is the function in which the error is occurring:     "
"Hello everyone, I'm a beginner in python and PyQt programming. I'm currently using Mask R-CNN algorithm on my own dataset. Thanks again for this amzing repo !    I have created an interface Gui with Qt Designer and when I launch the detection, an axis is created outside of my interface. That's normal, because the code :    from ""visualize.display_instances""(visualize.py) is:  ---------------------         I want to display results in a QFrame object instead of creating an axis. And I don't know how to call my QFrame object to replace the ax.    From my interface.py:  ---------------------        r = results     I hope your future help !"
"  !     While I am trying to implement the balloon model, I couldn't train the balloon.py file.  I always get this error  ""FileNotFoundError: [Errno 2] No such file or directory: '/path/to/balloon/dataset\\train\\via_region_data.json'"".  I have the balloon dataset stored in the root directory of the project.  Can someone please help me out with this issue."
"In `mrcnn_mask_loss_graph`, `y_true` and `y_pred`'s shape are both `[N, height, width]`. When using `K.binary_crossentropy` to calculate mrcnn_mask_loss, it will return a tensor with shape `[N, height]`, meaning that it treat every row in the mask as a sample, so there are `N*height` samples and each sample's length is `width`.   But the loss should be calculated on `N` samples with each sample of size `height*width`.  So I think `y_true` and `y_pred` should be reshaped into `[N, height*width]` before calculating the loss. Any ideas?"
"I am trying to get masks of instances (e.g., apple or orange) but faced some weired artifacts around predictions. I am using the default configuration as below (tested on/off mini mask but didn't help) and a screenshot of prediction is attached. Is there anyone encountered the similar issue?    > BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.8  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 1  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                14  IMAGE_MIN_DIM                  800  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'rpn_class_loss': 1.0, 'mrcnn_mask_loss': 1.0, 'rpn_bbox_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           apple  NUM_CLASSES                    2  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                100  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  False  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001    <img width=""415"" alt=""prediction"" src=""   "
"Hello everyone,    Thanks to author of this fine code/model. I am facing a problem when I try to train the model with provided balloon dataset. The training is within 30 min. through, but when I follow the provided notebook, namely ""inspect_balloon_model.ipynb"", I get zero detection. But when I specify the provided balloon weights h5 data as model, everything works fine and I can reproduce the model inspection notebook without a single problem.  I would appreciate any feedback regarding my problem. Thanks in advance.  PS: I should mention that I use docker and latest docker image of tensorflow.  IDE: pycharm  Environment: official docker image of tensorflow (tensorflow:latest-gpu-py3)  "
"It seems that the bounding box augmentation will not be correct in case of rotation using imgaug. Since we have to supply train_generator and val_generator in model.train. I am using the below code for rotation of images as well as bounding box  augmentation = iaa.Sequential([ iaa.OneOf([ iaa.Affine(rotate=(-5, 5)])  model.train(dataset_train, dataset_val,               learning_rate=config.LEARNING_RATE,               epochs=NUM_EPOCHS,               layers='all',              augmentation=augmentation.to_deterministic()))    Whereas using imgaug, the bounding box and image augmentation should be done as below  image_aug = seq_det.augment_images([image])  bbs_aug = seq_det.augment_bounding_boxes([bbs])    If I will supply one augmentation, I am not sure whether the current implementation will doing the bounding box augmentation correctly since it is internally using Keras data_generator in model.py            # Data generators          train_generator = data_generator(train_dataset, self.config, shuffle=True,                                           augmentation=augmentation,                                           batch_size=self.config.BATCH_SIZE,                                           no_augmentation_sources=no_augmentation_sources)          val_generator = data_generator(val_dataset, self.config, shuffle=True,                                         batch_size=self.config.BATCH_SIZE)    If anybody can suggest how can I use bounding box and image augmentation correctly here, I would be grateful. Thanks a lot."
The issue solved with python 3.6.6 (my case).    _Originally posted by @csyang6052 in  
"hi, when I run 'python3 parallel_model.py ', an error has happened.any ideas?"
"I have a set of images and png masks of each image. Interested in only one class. When I create tfrecord, should I provide both bounding box and masks? or only mask will be enough?"
"My platform is windows 10. python 3.6    I tried the following steps: 1) cloned: ""git clone   2) converted python to to 3 by typing this: ""2to3 . -w"" 3) then in the terminal typed ""python setup.py install""    Now I am having error like this:    C:\VS2017\VC\Tools\MSVC\14.15.26726\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Users\mk5n2\AppData\Local\Continuum\anaconda3\envs\tf_keras_env\lib\site-packages\numpy\core\include -I../common -IC:\Users\mk5n2\AppData\Local\Continuum\anaconda3\envs\tf_keras_env\include -IC:\Users\mk5n2\AppData\Local\Continuum\anaconda3\envs\tf_keras_env\include -IC:\VS2017\VC\Tools\MSVC\14.15.26726\ATLMFC\include -IC:\VS2017\VC\Tools\MSVC\14.15.26726\include ""-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\cppwinrt"" /Tc../common/maskApi.c /Fobuild\temp.win-amd64-3.6\Release../common/maskApi.obj -Wno-cpp -Wno-unused-function -std=c99 cl : Command line error D8021 : invalid numeric argument '/Wno-cpp' error: command 'C:\VS2017\VC\Tools\MSVC\14.15.26726\bin\HostX86\x64\cl.exe' failed with exit status 2    what shoud I do?  "
"The following piece of code taken from the inspection file:          for me results with nonsensical classification of my data:    !       I did specify 4 classes, yet there only 2 of them appear. On top of that, although the annotations had the same name (screw), their colors are different (look at the screw masks, having different color each). And I have no idea about the last two coloumn at all, like why are they even there if there isn't anything to show. And BG (background) is definitely messed up since it shows other components, like it makes no sense.    This is how I added my classes:       and it already outputs 5 classes when I print the number of classes. So it's not here which is messed up.    But anyway, there is something which is definitely wrong with this.     Did someone experience the same?"
"Hi all,    I am trying to use a Gui Interface (PyQt) to load an image, and launch the detection on it through a button.  But when I select my image, it displays this error:    !       My code for this is:    !       I don't know how to convert the ""list"" file (fileName) to the corresponding input for the network.  Sorry, I'm a beginner in Python.    EDIT: It works with the commented line:  `filePath = os.path.join(IMAGE_DIR, 'image56.jpg')`  But I want to load my image through the ""getOpenFileName"" :(    Regards,  Antoine"
"When my image input is .jpg - model works, but when I put the same image as .png it says     > operands could not be broadcast together with shapes (1024,1024,4) (3,)    Why?    I will close this issue in 2 days. Don't want to spam this repo. Thanks   "
"I'm trying to get Mask_RCNN to work on Windows with something beyond TF 1.4.  I actually don't have too many issues using 1.4, but today I tried to use Cuda and the proper dll for 1.4 isn't available with NVidia anymore.  I seem to have one problem that comes up with using TF 1.9.  It's this:    AssertionError                            Traceback (most recent call last)    in  ()       13     (""refined_anchors_clipped"", model.ancestor(pillar, ""ROI/refined_anchors_clipped:0"")),       14     (""post_nms_anchor_ix"", nms_node),  ---> 15     (""proposals"", model.keras_model.get_layer(""ROI"").output),       16 ])    D:\data\keras\Mask_RCNN\mrcnn\model.py in run_graph(self, images, outputs, image_metas)     2692         outputs = OrderedDict(outputs)     2693         for o in outputs.values():  -> 2694             assert o is not None     2695      2696         # Build a Keras function to run parts of the computation graph    AssertionError:     Can anybody point me in a reasonable direction for fixing that problem?      Thanks!"
"Hey,    When I run Detection from balloon_model on average it takes 25 sec. Why is it so slow? Because of my PC?    P.S  I don't want to spam this repo with stupid questions. I will delete this one in a day or two :)     "
"hi everyone   i run inspect_balloon_model.ipynb but in the Visualize Activations section, i get this error:  InvalidArgumentError                      Traceback (most recent call last)    in  ()        6     (""res4w_out"",          model.keras_model.get_layer(""res4w_out"").output),  # for resnet100        7     (""rpn_bbox"",           model.keras_model.get_layer(""rpn_bbox"").output),  ----> 8     (""roi"",                model.keras_model.get_layer(""ROI"").output),        9 ])    ~/Downloads/Mask_RCNN/mrcnn/model.py in run_graph(self, images, outputs, image_metas)     2687         if model.uses_learning_phase and not isinstance(K.learning_phase(), int):     2688             model_in.append(0.)  -> 2689         outputs_np = kf(model_in)     2690      2691         # Pack the generated Numpy arrays into a a dict and log the results.    ~/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in __call__(self, inputs)     2659                 return self._legacy_call(inputs)     2660   -> 2661             return self._call(inputs)     2662         else:     2663             if py_any(is_tensor(x) for x in inputs):    ~/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in _call(self, inputs)     2628                                 feed_symbols,     2629                                 symbol_vals,  -> 2630                                 session)     2631         fetched = self._callable_fn(*array_vals)     2632         return fetched[:len(self.outputs)]    ~/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in _make_callable(self, feed_arrays, feed_symbols, symbol_vals, session)     2580         callable_opts.target.append(self.updates_op.name)     2581         # Create callable.  -> 2582         callable_fn = session._make_callable_from_options(callable_opts)     2583         # Cache parameters corresponding to the generated callable, so that     2584         # we can detect future mismatches and refresh the callable.    ~/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/client/session.py in _make_callable_from_options(self, callable_options)     1478     """"""     1479     self._extend_graph()  -> 1480     return BaseSession._Callable(self, callable_options)     1481      1482     ~/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/client/session.py in __init__(self, session, callable_options)     1439           else:     1440             self._handle = tf_session.TF_DeprecatedSessionMakeCallable(  -> 1441                 session._session, options_ptr, status)     1442       finally:     1443         tf_session.TF_DeleteBuffer(options_ptr)    ~/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)      517             None, None,      518             compat.as_text(c_api.TF_Message(self.status.status)),  --> 519             c_api.TF_GetCode(self.status.status))      520     # Delete the underlying status object from memory otherwise it stays alive      521     # as there is a reference to status from this from the traceback due to    InvalidArgumentError: input_image:0 is both fed and fetched.  how can i solve it?"
None
"As described in the balloon example, I've used VIA tool to annotate my images. I've used circles, polygons, etc. and gave each class its respective name.    !   !       Now that I want to load the dataset, again, using the same code, and I get the following error:         What could be the problem? Has anyone tried loading the annotated data which contains circles? (not only polygons)"
"I have trained a lever segmentation. Each lever has its own mask plus a left hole and a right hole. I only want to get the detected levers with over 0.99 score, where its left and right holes are also detected.  !     Is there any way to filter out the masks based on my requirement? Thank you.  "
"I am getting the TypeError like this.  Can you please give the solution for this error.    dataset_train = DetectorDataset(image_fps_train, image_annotations, ORIG_SIZE, ORIG_SIZE)  Traceback (most recent call last):      File "" "", line 1, in        dataset_train = DetectorDataset(image_fps_train, image_annotations, ORIG_SIZE, ORIG_SIZE)      File "" "", line 48, in __init__      super().__init__(self)    TypeError: super() takes at least 1 argument (0 given)"
"Hi all,  I tried to just quickly remove the mask part, so we can use it for object detection easily.  Unfortunately I got very bad results for the simple shapes dummy dataset.    Implementation:       Result:  !       Could you give me hints why is it happening? Maybe the removal of the code was too fast 🤔 😢 "
"Hi everyone,    I'm training my own dataset using Mask_RCNN. My training steps are like the following:   `    But every time when it finished the first step, it did not proceed to the next training step. Is it because memory issue such as I need to release some memory from the last train?   Anyone has this kind of issue before?    Thanks"
"I have a question when I read codes of `PyramidROIAlign class` in `model.py`. The first dimension of pooled should be batch, but I didnt get a right result.  I constructed test inputs:     and feed them into PyramidROIAlign Layer         I checked the outputs shape and found mismatch between `x.shape` and `compute_output_shape()`         I reviewed the code inside `PyramidROIAlign` and found this. I thought it didn't ""re-add"" the batch dimension information back to `pooled`       Should this code be modified to code below?     "
For a project that I am working on I only need to generate masks for my objects. Is it possible to simplify the architecture by removing the Faster RCNN part that generates bounding boxes and only keep the Mask branch?
"Hi everyone,    I'm currently using this repo for an object detection problem with a lot of classes.  I trained on my own dataset (37 classes) and I used the splash balloon effect which works but I would like bounding boxes and scores confidence to see which object is detected.   That is why I have tried to test with the ""demo.ipynb"" code, to load my weights and to test the detection on my own dataset.  There is no error but I got this message : **""NO INSTANCE TO DISPLAY""**    I am a beginner in the computer vision field but I think I'm close to have results.  Here is my curves of my training :    !     !     But I think there is a problem when the weights are loaded.   In the original ""demo.ipynb"", **I have an error with these lines which load the weights:**    `model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)  model.load_weights(COCO_MODEL_PATH, by_name=True)`    **My error disappears when I replace with this line:**    `model.load_weights(COCO_MODEL_PATH, by_name=True, exclude=[ ""mrcnn_class_logits"", ""mrcnn_bbox_fc"", ""mrcnn_bbox"", ""mrcnn_mask""])`    But **""NO INSTANCE TO DISPLAY""**    Maybe somebody can help me with this ?    Regards,  Antoine    "
"In the semantic segmentation, we will set the colour of different class, but in the maskrcnn, we caculate the binary mask of each ROI, how to ensure the colour of every mask, and how to make the different object of the same class has the different colour mask."
"Seems like in model.py, tf.argmax does not have the argument 'output_type' anymore.    When I get rid of it I get some issue on the next line   `indices = tf.stack([tf.range(probs.shape[0]), class_ids], axis=1)`    returning   **Tensors in list passed to 'values' of 'Pack' Op have types [int32, int64] that don't all match.**    I am using tensorflow 1.2.1    Any idea how to solve this?"
"When I run the RPN graph and display its predictions in inspect_model.py, there is such a error:  the error lines in ` (""post_nms_anchor_ix"", nms_node)` How can I debug it?      `AssertionError                            Traceback (most recent call last)    in  ()       13     (""refined_anchors_clipped"", model.ancestor(pillar, ""ROI/refined_anchors_clipped:0"")),       14     (""post_nms_anchor_ix"", nms_node),  ---> 15     (""proposals"", model.keras_model.get_layer(""ROI"").output),       16 ])    ~/anaconda3/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py in run_graph(self, images, outputs, image_metas)     2692         outputs = OrderedDict(outputs)     2693         for o in outputs.values():  -> 2694             assert o is not None     2695      2696         # Build a Keras function to run parts of the computation graph    AssertionError: `  "
None
"I am trying to train Mask RCNN on a custom dataset (600x600) in a multi-GPU setting (GPU_COUNT > 1), but I quickly run out of RAM memory (MemoryError) right before the training process starts and the whole system freezes. The training works OK for GPU_COUNT = 1.    I figured this is something related to the multiprocessing module in fit_generator. When I disable multiprocessing, everything is working fine even with 8 GPUs, but the speedup compared to 1 GPU is so small that it's not worth it.    My setup is a machine with 16GB RAM memory and 8 GPUs (Tesla K80 12 GB memory)."
"when i run“inspect_balloon_model.py”，it reports error：    Traceback (most recent call last):    File ""/home/xupp/PycharmProjects/Mask_RCNN/samples/mountain/inspect_balloon_model.py"", line 137, in        weights_path = model.find_last()    File ""/home/xupp/PycharmProjects/Mask_RCNN/mrcnn/model.py"", line 2099, in find_last      errno.ENOENT, ""Could not find weight files in {}"".format(dir_name))  FileNotFoundError: [Errno 2] Could not find weight files in /home/xupp/PycharmProjects/Mask_RCNN/logs/balloon20180825T1144    Actually i have trained my own model by ”balloon.py“ and a logs file has been generated"
"How to fix this error in   “model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)”    Window anaconda  TF v1.8  keras 2.2    Thanks."
"**EDIT: Actually there is no problem when using multiple GPUs. I and several other people were merely confused about the time/steps, that are output during the training. We mixed them up with time/image, while it rather is time/batch, where we have to take note of the fact that the batchsize is proportional both to the `GPU_COUNT` and the `IMAGES_PER_GPU`. Therefore it is perfectly normal for the time/step to increase by a factor<=2, when increasing `GPU_COUNT` from 1 to 2.**     Hello everybody,  along with others (@waleedka, @ericj974, @schmidje, @liangbo-1, @dil, @kmh4321, @zgxsin, @YubinXie, @pieterbl86), I'm searching for the solution to the problem that the use of multiple GPUs can have a severely negative impact on the training speed (see #589, #676, #708, #710). There are however users, who claim that they are running or have run configurations, which don't have this problem. Therefore I'd like to make a poll how many users are affected by this problem.    ### Poll: I already encountered a configuration, with which I was able to train _significantly_ faster on multiple GPUs than on a single GPU.  !   !     Obviously, the poll alone does not help too much. Therefore, I'd like to gather working and non-working configurations. Please include the following information in your post:  - Did it work?  - Number of GPUs (GPU_COUNT)  - Number of images per GPU (IMAGES_PER_GPU)  - utilized script (e.g. train_shapes.ipynb)  - GPU model(s)  - OS  - CUDA version  - cuDNN version  - Driver version  - Mask-RCNN version (URL of the repo and hash specifying the state of the repo)  - Tensorflow version  - Keras version  - approx. speed up    Thank you very much for your help."
"Hi everyone,    I am training the Resnet 101  with my dataset (5100*3600*3). When I trained the network with `layers='heads'` it works well. But when I tried to train it with `layers='4+'or'all'` it always returned Resource exhausted error.    My graphics card is Nvidia TITAN XP with 12GB memory. Is it enough for training this model? Note that I have excluded three layers `exclude=[""mrcnn_class_logits"", ""mrcnn_bbox_fc"", ""mrcnn_bbox"", ""mrcnn_mask""])`    Any help would be greatly appreciated.  "
"fine-fine-fine and then  ......       I checked the annotation for polygons with the length zero, and they are all ok and equal.  Any ideas why this might happen? what to do with the local variable 'image_id'?"
"A quick question about the rpn_probs and rpn_class_logits. In the rpn_graph function when rpn_probs is first defined it is stated that the last dimension is separated into BG/FG. However, when using rpn_class_logits in the rpn_class_loss_graph function, it's last dimension is defined to be FG/BG, which is the reverse of rpn_probs. Is there a reason why it is swapped?"
"I edited `balloon.py` (following this:   ) and use it for multi classes. It took half a day to train (20 images for both training set and validating set) and i manually stoped the training. But after that I can't continue with this command   `python3 neural_code.py train --dataset=../../datasets/screens --weights=../../logs/test20180816T1444`    **Note that:**  test20180816T1444 is the file saved after training in `logs` directory                         and my laptop's specs:                         intel core i5 (4th gen) ,8gb ram                         nvidia geforce 830M (2gb) 256 cuda cores    And now I cannot start the training again from the beginning.    `python3 neural_code.py train --dataset=../../datasets/screens --weights=coco`    It give this error      `Traceback (most recent call last):`    `File ""/home/scit/anaconda3/lib/python3.6/site-packages/theano/tensor/type.py"", line 269, in ``dtype_specs`     ` }[self.dtype]`  `KeyError: "" ""`    `During handling of the above exception, another exception occurred:`    `Traceback (most recent call last):`    `File ""neural_code.py"", line 427, in        model_dir=args.logs)`    `File ""/home/scit/Desktop/My_work/object_detection/mask_rcnn/mrcnn/model.py"", line 1845, in __init__      self.keras_model = self.build(mode=mode, config=config)`    `File ""/home/scit/Desktop/My_work/object_detection/mask_rcnn/mrcnn/model.py"", line 1870, in build      shape=[None, 1], name=""input_rpn_match"", dtype=tf.int32)`    `File ""/home/scit/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py"", line 1439, in Input      input_tensor=tensor)`   ` File ""/home/scit/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 87, in wrapper      return func(*args, **kwargs)`    `File ""/home/scit/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py"", line 1348, in __init__      name=self.name)`    `File ""/home/scit/anaconda3/lib/python3.6/site-packages/keras/backend/theano_backend.py"", line 234, in placeholder`      `x = T.TensorType(dtype, broadcast)(name)`   ` File ""/home/scit/anaconda3/lib/python3.6/site-packages/theano/tensor/type.py"", line 51, in __init__      self.dtype_specs()  # error checking is done there`    `File ""/home/scit/anaconda3/lib/python3.6/site-packages/theano/tensor/type.py"", line 272, in dtype_specs`    `  % (self.__class__.__name__, self.dtype))`  `TypeError: Unsupported dtype for TensorType:    `  What may be the case here"
"Is it possible to combine the val_loss and loss graphs and display them in tensorboard? I know you can do it with tf.summary, but I'm not sure how to do it with Keras callbacks."
"First, thanks for sharing this amazing model and i learn a lot from it.  Briefly, my trouble is that predict seems just work after the model is loaded and not work for next coming requests.   I use a class to load the model. The test detecting inside `init` method works well.     I import this class in a flask script like this and run it with a uwsgi server.     I tried many times and every result is the same: only detecting inside `init` can work. I figure out by printing logs that image data is stuck at the `predict` method of the Keras model, in `mrcnn.model.py`.      Am I serve the model in wrong way or it's a bug of Keras?  Because I'm new in ML and English is not my first language, this issue may be silly.  Thanks for your reading, anyway. I'd be quite appreciated if anyone can help."
!   I take this error when I tried to start RPN Prediction part. What can I do for this?
"Trying to implement Mask-RCNN on my own dataset using the balloon example, I am having the stack trace error below.    `Epoch 1/60  ERROR:root:Error processing image {'id': '015.jpg', 'source': 'pot_crack', 'path': 'datasets/pot-crack/train/015.jpg', 'width': 480, 'height': 320, 'polygons': [{'name': 'rect', 'x': 11, 'y': 140, 'width': 127, 'height': 43}, {'name': 'rect', 'x': 233, 'y': 138, 'width': 133, 'height': 42}], 'names': [{'Class': 'crack'}, {'Class': 'crack'}]}  Traceback (most recent call last):    File ""/content/Pothole-Crack-Detection_MRCNN/mrcnn/model.py"", line 1699, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/content/Pothole-Crack-Detection_MRCNN/mrcnn/model.py"", line 1213, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""pot_crack.py"", line 169, in load_mask      rr, cc = skimage.draw.polygon(p['y'], p['x'])    File ""/usr/local/lib/python3.6/dist-packages/skimage/draw/draw.py"", line 457, in polygon      return _polygon(r, c, shape)    File ""skimage/draw/_draw.pyx"", line 217, in skimage.draw._draw._polygon  OverflowError: Python int too large to convert to C ssize_t  ERROR:root:Error processing image {'id': 'G0010033.JPG', 'source': 'pot_crack', 'path': 'datasets/pot-crack/train/G0010033.JPG', 'width': 3680, 'height': 2760, 'polygons': [{'name': 'rect', 'x': 1453, 'y': 1398, 'width': 120, 'height': 106}, {'name': 'rect', 'x': 1112, 'y': 1422, 'width': 87, 'height': 68}, {'name': 'rect', 'x': 909, 'y': 1408, 'width': 95, 'height': 74}, {'name': 'rect', 'x': 576, 'y': 1404, 'width': 128, 'height': 66}, {'name': 'rect', 'x': 315, 'y': 1414, 'width': 118, 'height': 66}], 'names': [{'Class': 'pothole'}, {'Class': 'pothole'}, {'Class': 'pothole'}, {'Class': 'pothole'}, {'Class': 'pothole'}]}  Traceback (most recent call last):    File ""/content/Pothole-Crack-Detection_MRCNN/mrcnn/model.py"", line 1699, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/content/Pothole-Crack-Detection_MRCNN/mrcnn/model.py"", line 1213, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""pot_crack.py"", line 169, in load_mask      rr, cc = skimage.draw.polygon(p['y'], p['x'])    File ""/usr/local/lib/python3.6/dist-packages/skimage/draw/draw.py"", line 457, in polygon      return _polygon(r, c, shape)    File ""skimage/draw/_draw.pyx"", line 217, in skimage.draw._draw._polygon  OverflowError: Python int too large to convert to C ssize_t`     Anyone come across this error while working on a different dataset? I am running this on google colab if it's of any use. "
"I'm using MICCAI 2017 EndoVis to do instance segmentation on surgical tools. The datasets includes 1800 images(25 of which have none instances and were excluded). I write my own tool.py as shown below. But when I started training the follow error occurs and the training process doesn't stop until sever iterations.  The same error happened while another user using 8-channel images.  But I'm only using simple 3-channel ones.  I could really use some help as I'm pretty bad in tf and python.   Really appreciate your help.  `ERROR:root:Error processing image {'id': '7frame032', 'path': '/home/user/MaskRCNN/Mask_RCNN-master/ReArranged_origin/Left_frames/7frame032.png', 'source': 'tools'}  Traceback (most recent call last):    File ""/home/user/.local/lib/python3.5/site-packages/mask_rcnn-2.1-py3.5.egg/mrcnn/model.py"", line 1704, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""/home/user/.local/lib/python3.5/site-packages/mask_rcnn-2.1-py3.5.egg/mrcnn/model.py"", line 1227, in load_image_gt      mask = utils.resize_mask(mask, scale, padding, crop)    File ""/home/user/.local/lib/python3.5/site-packages/mask_rcnn-2.1-py3.5.egg/mrcnn/utils.py"", line 517, in resize_mask      mask = scipy.ndimage.zoom(mask, zoom=[scale, scale, 1], order=0)    File ""/home/user/.local/lib/python3.5/site-packages/scipy/ndimage/interpolation.py"", line 573, in zoom      zoom = _ni_support._normalize_sequence(zoom, input.ndim)    File ""/home/user/.local/lib/python3.5/site-packages/scipy/ndimage/_ni_support.py"", line 65, in _normalize_sequence  `    And the following are my tools.py  `import os  import sys  import json  import datetime  import numpy as np  import skimage.io  from imgaug import augmenters as iaa    # Root directory of the project  ROOT_DIR = os.path.abspath(""../../"")    # Import Mask RCNN  sys.path.append(ROOT_DIR)  # To find local version of the library  from mrcnn.config import Config  from mrcnn import utils  from mrcnn import model as modellib  from mrcnn import visualize    # Path to trained weights file  COCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, ""mask_rcnn_coco.h5"")    # Directory to save logs and model checkpoints, if not provided  # through the command line argument --logs  DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, ""logs"")    # Results directory  # Save submission files here  # RESULTS_DIR = os.path.join(ROOT_DIR, ""results/tools/"")   # Under Linux  RESULTS_DIR = os.path.join(ROOT_DIR, r""results\tools"")  # To acquire val sets, I choose to segment the sequences with   # constant gaps.   templenum = 0  templename = ''  templename2 = ''  valname = ''  fullidname = ''  VAL_IMAGE_IDS = []  for idcount in range(1,226):      if idcount % 9 == 1:          templenum = str(idcount)          while len(templenum)   0:                  mask.append(m)                  classids.append(instancedic[instance_name])                          if np.sum(mask) > 0:                          mask = np.stack(mask, axis=-1)              classids = np.stack(classids)          elif np.sum(mask) == 0:              print(info['id'])          # Return mask, and array of class IDs of each instance. Since we have          # one class ID, we return an array of ones          return mask, classids        def image_reference(self, image_id):          """"""Return the path of the image.""""""          info = self.image_info[image_id]          if info[""source""] == ""tools"":              return info[""id""]          else:              super(self.__class__, self).image_reference(image_id)      ############################################################  #  Training  ############################################################    def train(model, dataset_dir, subset):      """"""Train the model.""""""      # Training dataset.      dataset_train = ToolsDataset()      dataset_train.load_tools(dataset_dir, ""train"")      dataset_train.prepare()        # Validation dataset      dataset_val = ToolsDataset()      dataset_val.load_tools(dataset_dir, ""val"")      dataset_val.prepare()        # If starting from imagenet, train heads only for a bit      # since they have random weights      print(""Train network heads"")      model.train(dataset_train, dataset_val,                  learning_rate=config.LEARNING_RATE,                  epochs=20,                  layers='heads')        print(""Train all layers"")      model.train(dataset_train, dataset_val,                  learning_rate=config.LEARNING_RATE,                  epochs=40,                  layers='all')        config = ToolsConfig()  config.display()  model = modellib.MaskRCNN(mode=""training"", config=config,                                    model_dir=DEFAULT_LOGS_DIR)  weights_path = COCO_WEIGHTS_PATH  model.load_weights(weights_path, by_name=True, exclude=[              ""mrcnn_class_logits"", ""mrcnn_bbox_fc"",              ""mrcnn_bbox"", ""mrcnn_mask""])  datasetsdir = os.path.join(ROOT_DIR,'ReArranged_origin')  train(model, datasetsdir, 'train')`"
"Hi,    I have written a training script using the balloons.py and the coco.py following the instructions on Medium.  Using the data inspection notebooks I have double-checked every sample in this small dataset (10 images, all classes are also present in COCO, angle is different).    If I use my own indices, total in 6 class + background then I have to remove the last mrcnn layers - which results in catastrophic forgetting.  I have experimented with training only the last layers of the mask_rcnn and adding layer after layer to the trainable params, eventually adding RPN to the trainable set as well, resulting in the ""heads"" preset used in the balloon experiment. Finally I have tried training all trainable parameters using ""all"" preset, with no luck.  Tried reducing the learning rate, switching to Adam... Still the results show that if I change anything from the COCO weights than it crashes.    I tried training on the same mini dataset with using the complete set of COCO weights and reindexing my classes to match the IDs in COCO annotations, therefore the shape of the last layers could be restored as well.  It performs ""OK"", which means only a very little difference from the original COCO weights and insignificant improvement.  Usually the losses reported by the script is the following:  `loss: 0.6037 - rpn_class_loss: 0.3280 - rpn_bbox_loss: 0.2757 - mrcnn_class_loss: 6.7830e-06 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: 0.4442 - val_rpn_class_loss: 0.3250 - val_rpn_bbox_loss: 0.1192 - val_mrcnn_class_loss: 4.7684e-06 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00`    On a larger dataset of 7k images the loss converges to a local minima, but visual output is completely wrong, barely finding any object, even on the training samples.    What should be the next step, where did I miss something?    Thank you,  Csabi"
"I am confused about the RPN loss implementation. Here is my understanding of the existing code:    **build_rpn_targets:**    From the ground truth data, it computes _rpn_match_ (batch, 4092, 1), where each value is -1, 1 or 0; and it computes _rpn_bbox_ (batch, 256, 1), tentatively made of 50/50 positive/negative boxes (with random sampling).    **rpn_losses:**    _rpn_class_loss_ computes the classification loss on all 4,092 anchors.  _rpn_bbox_loss_ computes the box loss on only the 256 anchors from _rpn_bbox_ above.      However, this is the opposite of what the paper mentions ...    _rpn_bbox_loss_ by definition computes a loss only for positive anchors, so there is no need to balance with negative ones. And we should not have a loss of zero for the ""extra"" positive anchors (if there are more than 128).    _rpn_class_loss_ is the one for which the selection is important, as otherwise the positive ones are overwhelmed by the negative ones.      Am I missing something ?"
"Balloon example considers only one class - balloon, how can I modify it to work with multiple classes?"
"Hi,    does anybody have hints on how should i modify my Dataset class in order to be able to use more than one class on the nucleus dataset?    Thanks!"
Im getting the following error while detecting     
"Hi there,  i trained the heads of the coco pretrained resnet 101. My dataset includes masks and i bring them into the correct form of [height, width, objects]. Training runs without errors on 1080TI for 40 Epochs with 200 step size.   Now i want to test the new model but got the following error:         i have no idea why i got this error. The dataset i used for training have 32 classes instead of the 80 coco and i didn't changed anything in the model definition in MASK_RCNN - class."
None
"I got the below mentioned error with ""Visualize Activations"" in ""inspect_balloon_model"".  Since I had a similar problem as mentioned in #799 before, which I could fix by switching to other module package versions, **I would be grateful if the the requirements.txt could be updated with exact version information** - which might help resolving this bug. Anyway, I am happy about any ideas of the source of the error ;-)  (OS: Windows10)    #Get activations of a few sample layers  activations = model.run_graph([image], [      (""input_image"",        model.keras_model.get_layer(""input_image"").output),      (""res2c_out"",          model.keras_model.get_layer(""res2c_out"").output),      (""res3c_out"",          model.keras_model.get_layer(""res3c_out"").output),      (""res4w_out"",          model.keras_model.get_layer(""res4w_out"").output),  # for resnet100      (""rpn_bbox"",           model.keras_model.get_layer(""rpn_bbox"").output),      (""roi"",                model.keras_model.get_layer(""ROI"").output),  ])  executed in 1.23s, finished 09:28:54 2018-07-23  ---------------------------------------------------------------------------  InvalidArgumentError                      Traceback (most recent call last)    in  ()        6     (""res4w_out"",          model.keras_model.get_layer(""res4w_out"").output),  # for resnet100        7     (""rpn_bbox"",           model.keras_model.get_layer(""rpn_bbox"").output),  ----> 8     (""roi"",                model.keras_model.get_layer(""ROI"").output),        9 ])    c:\users\myname\documents\maskrcnn\mrcnn\model.py in run_graph(self, images, outputs, image_metas)     2716         if model.uses_learning_phase and not isinstance(K.learning_phase(), int):     2717             model_in.append(0.)  -> 2718         outputs_np = kf(model_in)     2719      2720         # Pack the generated Numpy arrays into a a dict and log the results.    C:\Miniconda2\envs\MaskRCNN\lib\site-packages\keras\backend\tensorflow_backend.py in __call__(self, inputs)     2659                 return self._legacy_call(inputs)     2660   -> 2661             return self._call(inputs)     2662         else:     2663             if py_any(is_tensor(x) for x in inputs):    C:\Miniconda2\envs\MaskRCNN\lib\site-packages\keras\backend\tensorflow_backend.py in _call(self, inputs)     2628                                 feed_symbols,     2629                                 symbol_vals,  -> 2630                                 session)     2631         fetched = self._callable_fn(*array_vals)     2632         return fetched[:len(self.outputs)]    C:\Miniconda2\envs\MaskRCNN\lib\site-packages\keras\backend\tensorflow_backend.py in _make_callable(self, feed_arrays, feed_symbols, symbol_vals, session)     2580         callable_opts.target.append(self.updates_op.name)     2581         # Create callable.  -> 2582         callable_fn = session._make_callable_from_options(callable_opts)     2583         # Cache parameters corresponding to the generated callable, so that     2584         # we can detect future mismatches and refresh the callable.    C:\Miniconda2\envs\MaskRCNN\lib\site-packages\tensorflow\python\client\session.py in _make_callable_from_options(self, callable_options)     1478     """"""     1479     self._extend_graph()  -> 1480     return BaseSession._Callable(self, callable_options)     1481      1482     C:\Miniconda2\envs\MaskRCNN\lib\site-packages\tensorflow\python\client\session.py in __init__(self, session, callable_options)     1439           else:     1440             self._handle = tf_session.TF_DeprecatedSessionMakeCallable(  -> 1441                 session._session, options_ptr, status)     1442       finally:     1443         tf_session.TF_DeleteBuffer(options_ptr)    C:\Miniconda2\envs\MaskRCNN\lib\site-packages\tensorflow\python\framework\errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)      517             None, None,      518             compat.as_text(c_api.TF_Message(self.status.status)),  --> 519             c_api.TF_GetCode(self.status.status))      520     # Delete the underlying status object from memory otherwise it stays alive      521     # as there is a reference to status from this from the traceback due to    InvalidArgumentError: input_image:0 is both fed and fetched.        "
"     In the inspect_model notebook, the detect_molded() function fails, as it outputs an empty result and no masks. But later in the notebook I can run the step-by-step prediction and see the following bounding box and masks are predicted for the example image. (The image color is off due to how matplotlib does color stretching when many 0 values are present, can't seem to fix this).    !     !     Here is the code where the error occurs:       Below is the error, the shape of the mask is (1024, 1024, 0) while shape of groundtruth is (1024, 1024, 2).       This is my main question, but I'm also getting an error when inspecting anchors in the step-by-step detection. All other anchor cells work.       returns     Because sorted_anchor_ids contains values that are too high to serve as indices for the anchors array we get from        in an above cell. The first 20 values of sorted_anchor_ids are        Any advice is much appreciated!"
"Hi, I am still trying to read and understand the papers and the structure of the trained network. Mask RCNN trained on COCO is already showing promising results on my data but I was wondering how complicated it would be to present images as input augmented with a fourth channel (depth image). Would that require retraining the whole network or even changing the network's structure?"
"I am running inspect_balloon_model on my machine and got assertion error at 1.b RPN Predictions:  For me it looks close to issue #23 , but solution from it doesn't help. nms_node is empty.   There could be difference in tf versions. My setup: tf 1.9, keras 2.1.6, cuda 9.2, nvidia driver 396.37, Ubuntu 16.04. The only one change in the model by me was adding that 'print' on the screenshot.    !     Hope, you could give me an advice.  Thanks in advance       "
I am trying to run the balloon.py and am getting this error. Please advice as to how this can be resolved.  Thanks
I also have looked through all the issues in this repository and did not find find even one that was informative and in detail. LabelMe doesn't really work for the issue.
"There are many folders like ""coco20180718T1158"" in `logs`.I kown the weights will be saved in  these folders when training,but i can't find anything in these folders. "
"I got an Imablance data trouble ( 1 of classes have too many) .  First, I try to augmentation another classes to improve another classes data  How can I setting augmentation only selected classes ?  👍 "
"     Based on this implementation, the loss on line 1143 is a matrix of shape (number_boxes, 4).  So when taking the mean, we actually get the sum of all elements divided by (4 * number_boxes).    In the original paper, the loss for each box is defined as the sum over the 4 box components.    So we get a loss 4 times smaller than in the paper.    Has anyone noticed any impact ? Is it worth changing the weights in the config to compensate ?"
"Hi I am trying to learn the mask rcnn using balloon tutorial, however there are parts of the tutorial that I can't replicate even though i use the code straight from the repo.    The parts where I encounter the issue are these:  1. Show top anchors with refinement. Then with clipping to image boundaries  2. Show refined anchors after non-max suppression    Both of them show only the original image and no box at all. Is there something that is incorrect here?    Thx!"
"Hi, I'm able to run a training session, can  anyone advise?    Thanks.    (MASKR) john@john-H270N-WIFI:~/Mask_RCNN$ sudo python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=coco  Using TensorFlow backend.  Command:  train  Model:  coco  Dataset:  /path/to/coco/  Year:  2014  Logs:  /home/logs  Auto Download:  False    Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     2  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  COMPUTE_BACKBONE_SHAPE         None  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.7  DETECTION_NMS_THRESHOLD        0.3  FPN_CLASSIF_FC_LAYERS_SIZE     1024  GPU_COUNT                      1  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 2  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                93  IMAGE_MIN_DIM                  800  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'mrcnn_mask_loss': 1.0, 'rpn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'rpn_bbox_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           coco  NUM_CLASSES                    81  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                1000  TOP_DOWN_PYRAMID_SIZE          256  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001      Loading weights  /home/mask_rcnn_coco.h5  Traceback (most recent call last):    File ""samples/coco/coco.py"", line 474, in        model.load_weights(model_path, by_name=True)    File ""/home/john/Mask_RCNN/samples/coco/mrcnn/model.py"", line 2123, in load_weights      f = h5py.File(filepath, mode='r')    File ""/home/john/.local/lib/python3.5/site-packages/h5py/_hl/files.py"", line 312, in __init__      fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)    File ""/home/john/.local/lib/python3.5/site-packages/h5py/_hl/files.py"", line 142, in make_fid      fid = h5f.open(name, flags, fapl=fapl)    File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper    File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper    File ""h5py/h5f.pyx"", line 78, in h5py.h5f.open  OSError: Unable to open file (unable to open file: name = '/home/mask_rcnn_coco.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)  "
"Hi, sorry that i post kind of offtopic stuff here, but i have a few questions about the topology of the Mask R-CNN.    I want to use the pretrained model for transfer learning with a dataset based on cityscapes classes. To decide which of the layers i need to freeze and which one to train again with new dataset, i need some informations about the ResNet 101. Whats the easiest way to get the architecture of the network and are there any suggestions which layers i should replace with new one.     Dataset has 25000 Pictures with ground truth.      I would be very appreciate for your help.   Thanks"
"I'm looking to use the outputted masks from the model to extract the area of the image taken up by each object mask.   My issue is that I don't know how to interpret the r['masks'] array, for each mask it appears to be a list of lists with each element being a boolean value. I am not sure how to extract the information I need from this.  If you have any ideas, or know where I should be looking to learn more I'd be very grateful."
"Is there a confirmed way of running the MaskRCNN model here with Python 2.7? In the requirements it mentions Python 3.4, but I wanted to ask anyway.    I trained the model using said Python 3.4, but when using Python 2.7 for inference, the model does not output anything anymore.    Using Python 2.7 is rather crucial since I am interfacing the model with an architecture that relies upon Python 2.7."
Currently it only prints losses but I was wondering if it is possible to print accuracy too.
"E:\Programs\Python\lib\site-packages\tensorflow\python\ops\gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  Epoch 1/1    When run the traning，the console shows the warning above. How to solve this problem?"
"I want save only 0020.h5, 0040.h5, 0060.h5, 0080.h5, ...... instead of every .h5. How can I revise the code? I can't find out corresponding code. Thank you."
"Hi, have you considered   by assigning a DOI to it? I can imagine that quite some research is based on the code of your repository and a DOI makes it much easier to cite."
"Following the Python script `balloon.py`,  I created a similar script `card.py` and trained the network with my own dataset for 5 epochs. The trained weights are stored in `mask_rcnn_card_0005.h5`.    Then, I would like to continue to train the network by using the trained weights. (I run it in Spyder)  `run card.py train --dataset=/path/to/datasets/card/ --weights=/path/to/mask_rcnn_card_0005.h5`    It displays         ......      Loading weights  /path/to/mask_rcnn_card_0005.h5      Re-starting from epoch 5      Training network heads           Starting at epoch 5. LR=0.001        Checkpoint Path:  /path/to/mask_rcnn_card_{epoch:04d}.h5      Selecting layers to train      fpn_c5p5               (Conv2D)      ...      ...      mrcnn_mask          (TimeDistributed)    Then the program just stop here and does not produce any results.   Is there something that I am missing? Thanks"
"Hi,    i am trying to run the net together with an open world game. To get playable fps, i need to restrict the gpu which keras is using. Unfortunalety, i cannot find the part in the code where i can adjust the usage of the gpu.     I would be very appreciated for your help. Thanks. "
"I want to design my own training and I was wondering how to set the parameters in config in the right way, based on my data.    My image is 128 * 128, and there are around 100 objects in the image and only 1+1 classes in the data. How should I adjust the following parameters?       I am curious that how to find a suitable value for my data.   "
load_weights function from MaskRCNN is not working on suggested version 2.0.8 of Keras. This is because the topology package is no longer supported in Keras. Please update the requirements.txt with correct version of Keras or remove imports of topology and replace with saving. Below is the error I get wusing the demo code.    AttributeError: module 'keras.engine.topology' has no attribute 'load_weights_from_hdf5_group_by_name'
" I keep getting these errors in between training.However,training continues ..how do i solve this??      7/10 [====================>.........] - ETA: 3:16 - loss: 11.7525 - rpn_class_loss: 0.2370 - rpn_bbox_loss: 9.0595 - mrcnn_class_loss: 0.1269 - mrcnn_bbox_loss: 1.8972 - mrcnn_mask_loss: 0.4319ERROR:root:Error processing image {'id': 'TYPE_B_C-1638.jpg', 'source': 'balloon', 'path': 'C:\\Users\\praso\\OneDrive\\Desktop\\Mask\\Mask_RCNN\\samples\\balloon\\balloon\\dataset\\train\\TYPE_B_C-1638.jpg', 'width': 2048, 'height': 1536, 'polygons': [{'name': 'polygon', 'all_points_x': [978, 874, 933, 1081, 1379, 1395, 1306, 992, 978], 'all_points_y': [1513, 1352, 1186, 1131, 1204, 1415, 1522, 1527, 1513]}, {'name': 'polygon', 'all_points_x': [1998, 1825, 1725, 1761, 1923, 2041, 2043, 1998], 'all_points_y': [1090, 1070, 897, 692, 617, 667, 1063, 1090]}, {'name': 'polygon', 'all_points_x': [1639, 1584, 1662, 1866, 2028, 2044, 2048, 1639], 'all_points_y': [1529, 1424, 1233, 1199, 1288, 1410, 1538, 1529]}, {'name': 'polygon', 'all_points_x': [2037, 1927, 1743, 1741, 1923, 2034, 2037], 'all_points_y': [626, 678, 496, 232, 125, 162, 626]}]}  Traceback (most recent call last):    File ""C:\Users\praso\OneDrive\Desktop\Mask\Mask_RCNN\mrcnn\model.py"", line 1704, in data_generator      use_mini_mask=config.USE_MINI_MASK)    File ""C:\Users\praso\OneDrive\Desktop\Mask\Mask_RCNN\mrcnn\model.py"", line 1219, in load_image_gt      mask, class_ids = dataset.load_mask(image_id)    File ""balloon.py"", line 130, in load_mask      mask[rr, cc, i] = 1  IndexError: index 1536 is out of bounds for axis 0 with size 1536"
"Running model.py unmodified, called from a custom dataset implementation, results in following error:     For reference, I am using Python 3.6.5, TensorFlow 1.8 with nVidia GPU, Keras 2.2.    In line 1270, _idx is a boolean array used to filter out masks (and corresponding class_ids) that have been cropped and no longer contain the portion of the mask that constitutes the object in question. When  applying _idx to the mask array, it works fine because the mask array in a numpy array. However, when the same indexing operation is applied to class_ids, it fails because class_ids is a list, and boolean indexes cannot be used on Python lists.    I have implemented the following code fix (it is deliberately written on several lines for clarity) in my own branch:     This new code has been tested and does not exhibit this TypeError. There are multiple ways to resolve this error. For example, another way would be to replace line 1272 with a list comprehension.    I have not submitted a pull request with this fix because I cannot find any other bug reports with this error. If someone believes it is important to submit this fix, please let me know and I'll submit a pull request.  "
"Hello! I'm trying to train mrcnn on 8 channel imagery to detect agriculture. (channels are RGBN for on/off growing season). Here a notebook with my code:      I've followed the steps on the wiki for adjusting the mrcnn config and dataset classes for more than 3 channels, except for the last step since I don't want to use pregenerated weights and want the model to initialize those randomly:      The strange error is below, somewhere, something is expecting an array of shape (None,None, 3) but gets shape (256, 256, 8) instead which is the shape it should be expecting. You'll see that I've set the IMAGE_SHAPE channel to accept N, changed MEAN_PIXEL for 8 channels, and changed load_image to accept 8 channels.    I've tried to dig through the stacktrace to find why it would expect (None,None, 3) but am quite lost. Any help/suggestions are appreciated. I'm also confused why it is throwing `RuntimeError: sequence argument must have length equal to input rank` and if this issue is seperate or not from the other issue.    Please let me know if you need anymore information.     "
"I am training a network with part of the objects have intersections.  If I leave out the overlap objects.  I can train the model. If I include all objects, I will run into Nan loss in mrcnn_class_loss_graph. This is not a data problem as I can pass one Epoch training. By debug the code, I find in proposal network. The code maybe gets non-positive proposal. This result in Nan in mrcnn_class_loss_graph.   My question is if it is the limitation for this network or parameter issue?  "
My memory is always out.  Training used too much memory. And core dumps.  Can anybody give me a model trained with nucleus?  I want to try to demo.
"Hi there,    I am using tensorflow-gpu 1.8.0 and keras 2.2. (win10)  I followed the installation guide and successfully run the first 2 cells in demo.ipynb in sample folder.    When loading the weights from the 3rd cell, jupyter complained that module 'keras.engine.topology' has no attribute 'load_weights_from_hdf5_group_by_name'. I went to the keras source of topology.py and discovered that indeed probably this python file has been modifed during the latest releases:       How to fix this problem ?     Many thanks.      ---------------------------------------------------------------------------  AttributeError                            Traceback (most recent call last)    in  ()        3         4 # Load weights trained on MS-COCO  ----> 5 model.load_weights(COCO_MODEL_PATH, by_name=True)    C:\ProgramData\Anaconda3\lib\site-packages\mask_rcnn-2.1-py3.6.egg\mrcnn\model.py in load_weights(self, filepath, by_name, exclude)     2117      2118         if by_name:  -> 2119             topology.load_weights_from_hdf5_group_by_name(f, layers)     2120         else:     2121             topology.load_weights_from_hdf5_group(f, layers)    AttributeError: module 'keras.engine.topology' has no attribute 'load_weights_from_hdf5_group_by_name'"
"Hello,  I wanted to know if we can start with random weights or anything like that, or if we only have the coco option.    Thanks ! Nathan."
"Hi all,    On this year's CVPR they are presenting  , which basically improve the feature level selection from MaskRCNN.     I was wondering if someone had implemented these ideas based on this repo  (or is planning to do so). I think that it can be really interesting to have this option if what they report in the paper is true (+5mAP on COCO at low computational cost).     Best.  "
"hi everybody   I want to use Mask-RCNN demo.py in my jupyter notebook but when i ran the the demo code in part of Create Model and Load Trained Weights i faced with such problem      AttributeError                            Traceback (most recent call last)    in  ()        3         4 # Load weights trained on MS-COCO  ----> 5 model.load_weights(COCO_MODEL_PATH, by_name=True)    ~/Downloads/Mask_RCNN/mrcnn/model.py in load_weights(self, filepath, by_name, exclude)     2117      2118         if by_name:  -> 2119             topology.load_weights_from_hdf5_group_by_name(f, layers)     2120         else:     2121             topology.load_weights_from_hdf5_group(f, layers)    AttributeError: module 'keras.engine.topology' has no attribute 'load_weights_from_hdf5_group_by_name'      please help me"
"I'm trying to integrate a non local block (  with Mask R-CNN.    From the paper   ""We modify the Mask R-CNN backbone by adding one non-local block (right before the last residual block of res4). All models are fine-tuned from ImageNet pre-training. We evaluate on a standard baseline of ResNet-50/101 and a high baseline of ResNeXt-152 (X152)""    How do I go about this?"
"After performing training, I ran evaluation code on 500 images to which I received following results.  Can any one help me understand these results. Any help is much appreciated.       "
"I am getting the warning below         Is this warning common, or could there be an issue with loading the image and mask?"
"Hi,     Is there a way to load a coco-style dataset and make Mask-RCNN work? I've already checked nucleus and shape examples, but they don't help since shape generates its own dataset and nucleus has hardcoded numbers such as (657 for steps per epoch calculation, no idea where this number comes from) and other hardcoded values. Also, nucleus has only one class, it doesn't help at all when you want to load multiple classes in your dataset.    All in all, one cannot load the dataset without facing a problem, even if the dataset is coco-style dataset. You usually get an error such as:    `TypeError: list indices must be integers or slices, not str`    I've also tried to use the loader class `coco.py` has, and no luck:       Can someone help with a generic load function which loads any coco-style dataset? (The examples given such as shapes and nucleus aren't what I am asking for.)"
"I see the *** No instances to display ***  Error when i do detection on test images like in the demo notebook.    But in contrast to that when i do it over the coco dataset like in the inspect_model notebook i get normal results.    @waleedka do you have an idea why that is? Even if the test images are manually taken from the original coco dataset.    I know some of those errors are py2/3 related but as far as i know i solved all python version depending issues (like stated in my PR)    Edit: I solved it with casting all boxes to floats in the function norm_boxes.    I will include this into the PR, i hope finally my fork is completely py2/3 compatible"
I am training images on my own dataset and used VGG Image Annotator (VIA) for segmenting images but it cause error when i ran          Here's the traceback:     
"My masks continue to come out as relatively blocky and not dealing well with curves and points. For example the attached whale outline. I've turned off mini masks so I'm no longer downsampling those masks in training but does anyone have advice for increasing the output resolution of masks?    There is this line in config.py:  `MASK_SHAPE = [28, 28]`  Which I assume is part of the solution but I'm not sure what else I'll need to change to accomodate for a larger mask size because it says ""To change this mask size you also need to change the neural network mask branch.""    Any tips on what I need to alter to get this to put out a much higher resolution mask?      <img width=""418"" alt=""screen shot 2018-06-03 at 2 34 07 pm"" src=""   "
"Hi,    I was checking the Mask_RCNN/samples/balloon/README.md file. It asks to download the balloon_dataset.p3 file from the ""  page. However such file is not there on the page.   Further, while running the code it crashes with the error - ""Mask_RCNN/datasets/train/via_region_data.json file not found"".  Am I doing something wrongly?"
"I'm setting up the environment for running this in a Docker container. Currently my 1050ti  4GB is OOMing on the train_shapes example.     Can someone with a larger card please tell me how much memory this notebook requires when running with the default settings?     I'm trying to figure out if it's my CUDA/CuDNN being incompatible with the Tensorflow version, or if it truly requires more memory. "
"Hi all,   i'm encountering this error in the following simple code:     from rllab.spaces import Box   import tensorflow as tf     a = Box(1,4,shape(1,4))  b = a.new_tensor_variable('b',extra_dims=2)  c = tf.shape(b)[0]    I have already try to look at the keras backend and it's correct.  Any suggestion? Thank you!"
"Hello,    I tried training my custom dataset, which I converted to COCO's format. What I did is to copy `coco.py` and modify the name etc. Unfortunately I have never got the training to run successfully, due to out of memory issue. A strange issue is that when the problem breaks, it does not release the memory of the GPU. I had to reboot my server each time a training session failed :(    Lately I tried using resnet50 and reducing the image size:       it started to train but then error again:    > Epoch 1/40  >    1/1000 [..............................] - ETA: 17:10:49 - loss: 9.0742 - rpn_class_loss: 2.6839 - rpn_bbox_loss: 2.2965 - mrcnn_class_loss: 1.5021 - mrcnn_bbox_loss: 1.8991 - mrcnn_mask_loss: 0   2/1000 [..............................] - ETA: 8:41:46 - loss: 8.5584 - rpn_class_loss: 2.4432 - rpn_bbox_loss: 2.3522 - mrcnn_class_loss: 1.5195 - mrcnn_bbox_loss: 1.5785 - mrcnn_mask_loss: 0.   3/1000 [..............................] - ETA: 5:52:02 - loss: 7.8795 - rpn_class_loss: 2.4402 - rpn_bbox_loss: 2.2415 - mrcnn_class_loss: 1.1957 - mrcnn_bbox_loss: 1.3006 - mrcnn_mask_loss: 0.   4/1000 [..............................] - ETA: 4:26:59 - loss: 7.5652 - rpn_class_loss: 2.3533 - rpn_bbox_loss: 2.2754 - mrcnn_class_loss: 0.9323 - mrcnn_bbox_loss: 1.2874 - mrcnn_mask_loss: 0.   5/1000 [..............................] - ETA: 3:36:07 - loss: 7.0885 - rpn_class_loss: 2.2964 - rpn_bbox_loss: 2.1484 - mrcnn_class_loss: 0.7573 - mrcnn_bbox_loss: 1.1683 - mrcnn_mask_loss: 0.   6/1000 [..............................] - ETA: 3:02:14 - loss: 6.9631 - rpn_class_loss: 2.1952 - rpn_bbox_loss: 2.0658 - mrcnn_class_loss: 0.8401 - mrcnn_bbox_loss: 1.1630 - mrcnn_mask_loss: 0.   7/1000 [..............................] - ETA: 2:38:45 - loss: 6.6486 - rpn_class_loss: 2.0803 - rpn_bbox_loss: 2.0185 - mrcnn_class_loss: 0.7252 - mrcnn_bbox_loss: 1.1160 - mrcnn_mask_loss: 0.   8/1000 [..............................] - ETA: 2:20:35 - loss: 6.3481 - rpn_class_loss: 1.9904 - rpn_bbox_loss: 1.9701 - mrcnn_class_loss: 0.6393 - mrcnn_bbox_loss: 1.0321 - mrcnn_mask_loss: 0.   9/1000 [..............................] - ETA: 2:06:30 - loss: 6.0660 - rpn_class_loss: 1.8920 - rpn_bbox_loss: 1.8877 - mrcnn_class_loss: 0.5828 - mrcnn_bbox_loss: 0.9873 - mrcnn_mask_loss: 0.71622018-06-01 10:05:23.677419: F ./tensorflow/core/util/cuda_launch_config.h:127] Check failed: work_element_count > 0 (0 vs. 0)  > zsh: abort (core dumped)  CUDA_VISIBLE_DEVICES=2 python custom_coco.py train  --model=imagenet  >     The worst thing is that it never releases the memory. My GPU is showing `10953MiB / 11439MiB` although the program was stopped as shown above.    Is there a way to release the GPU memory without rebooting? I have other training sessions running at the same time so each time I reboot I have to restart them again, which is very annoying.    I would like to use resnet101 and higher image dimensions. Which parameters should I change to reduce further the memory usage?    I attached below the current full configuration.    Thank you very much in advance for your help!      > Configurations:  > BACKBONE                       resnet50  > BACKBONE_STRIDES               [4, 8, 16, 32, 64]  > BATCH_SIZE                     1  > BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  > DETECTION_MAX_INSTANCES        100  > DETECTION_MIN_CONFIDENCE       0.7  > DETECTION_NMS_THRESHOLD        0.3  > GPU_COUNT                      1  > GRADIENT_CLIP_NORM             5.0  > IMAGES_PER_GPU                 1  > IMAGE_MAX_DIM                  512  > IMAGE_META_SIZE                19  > IMAGE_MIN_DIM                  512  > IMAGE_MIN_SCALE                0  > IMAGE_RESIZE_MODE              square  > IMAGE_SHAPE                    [512 512   3]  > LEARNING_MOMENTUM              0.9  > LEARNING_RATE                  0.001  > LOSS_WEIGHTS                   {'mrcnn_mask_loss': 1.0, 'rpn_class_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'rpn_bbox_loss': 1.0}  > MASK_POOL_SIZE                 14  > MASK_SHAPE                     [28, 28]  > MAX_GT_INSTANCES               100  > MEAN_PIXEL                     [123.7 116.8 103.9]  > MINI_MASK_SHAPE                (56, 56)  > NAME                           qopius  > NUM_CLASSES                    7  > POOL_SIZE                      7  > POST_NMS_ROIS_INFERENCE        1000  > POST_NMS_ROIS_TRAINING         2000  > ROI_POSITIVE_RATIO             0.33  > RPN_ANCHOR_RATIOS              [0.5, 1, 2]  > RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  > RPN_ANCHOR_STRIDE              1  > RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  > RPN_NMS_THRESHOLD              0.7  > RPN_TRAIN_ANCHORS_PER_IMAGE    256  > STEPS_PER_EPOCH                1000  > TRAIN_BN                       False  > TRAIN_ROIS_PER_IMAGE           200  > USE_MINI_MASK                  True  > USE_RPN_ROIS                   True  > VALIDATION_STEPS               50  > WEIGHT_DECAY                   0.0001  "
"    # Shape of output mask      # To change this you also need to change the neural network mask branch      MASK_SHAPE = [28, 28]    I have changed the MASK_SHAPE to [14,14], but i cant find the code of the neural network mask branch about this. Can you tell me how to find the codes?  "
"Hi,  i am currently try to install pycocotools on windows 10.  I tried to install MSBuild Tools 2017 (unable to find working 2015 version) and cython and use  `pip install git+  to get the pycocotools but it didn't worked.    I tried to install after i cloned it running all methods     i am running everytime in the same error messaged added below:    NOTE: Das System kann die angegebene Datei nicht finden = The system cannot find the specified file         I hope somebody of you can help. Please let me know if you need further informations.     Best Regards CptK1ng  "
"I am training on my own dataset using imagenet pretrained weights. My image size is 512x512x3. I converted image masks into cocostyle json format and the annotation files for both training and validation loaded successfuly. However after that point I am getting file not found error on several files despite the files being present in the path. I checked my path several times to ensure that it is correct. The files on which error is occuring changes everytime i rerun the code.   Posting the traceback       What could be the possible cause of this error? it's wierd because everytime the image files are changing. Also, at two places this error is happening   1. line 1695 of model.py   2. line 502 of coco.py      "
"When I run `python3 setup.py install`, the error log comes out       from `easy_install --version` ,below is my setup tools version       somebody please give a help      "
"My environment is tf=1.4.0 ,keras=2.0.8,GPU=GTX1080Ti,RAM=8GB  Below is configuration:    (F:\soft\anaconda3) D:\UBUNTU\github\Mask_RCNN\samples\shapes>python train_shapes.py  Using TensorFlow backend.  success    Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES                                      "
None
"It runs with the balloons all good but when I want to train my own dataset, I get this error no matter what.    I have 2 classes apart from the background.    What might be the problem?    Edit: Full error message.     "
"I know this might be a newbie question,   is there anyway to keep model loaded while running detection again and again?    For example, there is a API which can feed images into the detection,  the detection function runs in like 200ms but the config and model-loading run in like 2mins.  So we want to keep config and model loaded so it will save much time for the next image detection.    Any help would be greatly appreciated"
"when i want to train on cityscapes dataset               class CityscapesDataset(utils.Dataset):              def load_cityscapes(self, dataset_dir, subset):                    for i in range(len(self.class_labels)):                      self.add_class(""cityscape"", i, list(self.class_labels.keys())[i])                    for idx in range(len(self.image_info)):                      self.add_image(                             source = self.image_info[idx]['source'],                             image_id = self.image_info[idx]['image_id'],                             path=self.image_info[idx]['path'],                             width=self.image_info[idx]['imgWidth'],                             height=self.image_info[idx]['imgHeight']                        )  I always got error message like this    Traceback (most recent call last):    File ""cityscape.py"", line 338, in        dataset_train.prepare()    File ""/home/world4jason/Mask_RCNN/mrcnn/utils.py"", line 316, in prepare      for info, id in zip(self.image_info, self.image_ids)}    File ""/home/world4jason/Mask_RCNN/mrcnn/utils.py"", line 316, in        for info, id in zip(self.image_info, self.image_ids)}  KeyError: 'id'    always stop here in utils.py            self.class_from_source_map = {""{}.{}"".format(info['source'], info['id']): id                                        for info, id in zip(self.image_info, self.image_ids))}  is there any clue?      "
"First of all, thanks for making this code public and thanks to everyone who has been contributing to it.    I am running into some issues and I wonder if some one here can enlighten me.     I am running anomaly detection on some images, and I only have one class (plus the background). All returned results seems to have detection scores higher than 0.5, even though I set `DETECTION_MIN_CONFIDENCE` to 0. How could I show detection with scores smaller than 0.5? Or if you are also doing something similar, are you able to showcase detection results with low scores?    Thanks!"
"Hi all,    I'm trying to remove the RPN part of the model and replace it with a Keras Input layer. Specifically, it is a new model apart from the original one, the RPN part and the RoiProposal layer replaced. I loaded the same weights and used both models in inference mode. The region proposals from the original network are extracted and then fed into the new network.    Normally I expect the outputs of the 2 networks to be the same, but they're not. The detection and mask generation of my custom model is always wrong.    I've narrowed down my bug to PyramidROIAlign class, since the tensor values from the two models start to differ after this step. But I don't understand how this bug comes about.    Any solution to this bug is welcome. Thanks!"
"Hi,    First of all thank you the authors of this great repo!     I am puzzled with something, I hope you can help me figuring out what's the problem.    My setup:  - Linux rhel 7  - 4x GeForce GTX 1080 8 GB  - tf 1.8.0, python 3.6, cuda 9.2.88.1, cuDNN 7.1.4 and nvidia driver 396.26   - 128x128 image size in the config  - own data composed of X-ray images with up to 4 masks saved in binary format for fast loading.    Basically, I started by noticing that a multi-gpu training was slower than a single GPU one....! So, I focused on a single GPU version and I noticed that the more I reduced the IMAGES_PER_GPU the faster was my training:  - ~10 min with 1 image per gpu,   - ~10 min 40 sec with 2 images per gpu,   - ~13 min with 4 images per gpu.  - ~20 min with 8 images per gpu,     I imagine that an excessive number of images per gpu will harm performance with possible out-of-memory issues, but my I pretty sure that my GPUs can properly handle e.g. 4 images per gpu and I thought that by increasing IMAGES_PER_GPU things will go faster.     Before I start digging the multi-gpu issue, what am I missing here?    Thanks!"
"When I ran the demo.ipynb, I got an error which said ""ValueError: Tensor Tensor(""mrcnn_detection/Reshape_1:0"", shape=(1, 100, 6), dtype=float32) is not an element of this graph"""
Is there anyway to run Mask_RCNN on Python 2.7? I tried updating references to Python 3 libraries but it seems like the code heavily depends on Python 3.
"My training has gone fine, but I'm getting the following error when I try to infer from an image. I'm using the balloon template to apply a splash image. Any help would be very much appreciated.       "
None
"The   states:         Who should be attributed as the author(s), GitHub, Inc.?  Would it be possible to provide a BibTex entry for the repository to allow easier citations?    @waleedka "
"In order to speed up the training rate on coco dataset, i set ""GPU_COUNT=2"". But I got the following problem:        Command:  train  Model:  coco  Dataset:  /data/liaoshisha/Mask_RCNN-master/coco  Year:  2014  Logs:  /data/liaoshisha/Mask_RCNN-master/logs  Auto Download:  False    Configurations:  BACKBONE                       resnet101  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     4  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.7  DETECTION_NMS_THRESHOLD        0.3  GPU_COUNT                      2  GRADIENT_CLIP_NORM             5.0  IMAGES_PER_GPU                 2  IMAGE_MAX_DIM                  1024  IMAGE_META_SIZE                93  IMAGE_MIN_DIM                  800  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              square  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           coco  NUM_CLASSES                    81  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                1000  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001      Traceback (most recent call last):    File ""coco.py"", line 455, in        model = modellib.MaskRCNN(mode=""training"", config=config, model_dir=args.logs)    File ""/home/liaoshisha/anaconda3/envs/tensorflow/lib/python3.4/site-packages/mask_rcnn-2.1-py3.4.egg/mrcnn/model.py"", line 1820, in __init__    File ""/home/liaoshisha/anaconda3/envs/tensorflow/lib/python3.4/site-packages/mask_rcnn-2.1-py3.4.egg/mrcnn/model.py"", line 2039, in build    File ""/home/liaoshisha/anaconda3/envs/tensorflow/lib/python3.4/site-packages/mask_rcnn-2.1-py3.4.egg/mrcnn/parallel_model.py"", line 37, in __init__    File ""/home/liaoshisha/anaconda3/envs/tensorflow/lib/python3.4/site-packages/mask_rcnn-2.1-py3.4.egg/mrcnn/parallel_model.py"", line 81, in make_parallel    File ""/home/liaoshisha/anaconda3/envs/tensorflow/lib/python3.4/site-packages/keras/engine/topology.py"", line 619, in __call__      output = self.call(inputs, **kwargs)    File ""/home/liaoshisha/anaconda3/envs/tensorflow/lib/python3.4/site-packages/keras/engine/topology.py"", line 2085, in call      output_tensors, _, _ = self.run_internal_graph(inputs, masks)    File ""/home/liaoshisha/anaconda3/envs/tensorflow/lib/python3.4/site-packages/keras/engine/topology.py"", line 2236, in run_internal_graph      output_tensors = _to_list(layer.call(computed_tensor, **kwargs))    File ""/home/liaoshisha/anaconda3/envs/tensorflow/lib/python3.4/site-packages/keras/layers/core.py"", line 663, in call      return self.function(inputs, **arguments)    File ""/home/liaoshisha/anaconda3/envs/tensorflow/lib/python3.4/site-packages/mask_rcnn-2.1-py3.4.egg/mrcnn/model.py"", line 1913, in      File ""/home/liaoshisha/anaconda3/envs/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/constant_op.py"", line 102, in constant      tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))    File ""/home/liaoshisha/anaconda3/envs/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/tensor_util.py"", line 376, in make_tensor_proto      _AssertCompatible(values, dtype)    File ""/home/liaoshisha/anaconda3/envs/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/tensor_util.py"", line 299, in _AssertCompatible      raise TypeError(""List of Tensors when single Tensor expected"")  TypeError: List of Tensors when single Tensor expected    I don't know what's wrong. Look forward to your help!    NOTE: Even if i change from tf.constant to tf.Variable, but i didn't deal with this problem."
None
"In the demo notebook, the line below  model.load_weights(COCO_MODEL_PATH, by_name=True)  is used to load weights, this no longer works in Keras. I tried with the latest github pull, as well as the pip default install on ubuntu 16.04. I had to revert to Keras 2.0.8 specifically to get this to work. Looks like model.load_weights and model.save_weights is the way going forward in keras.            if by_name:              topology.load_weights_from_hdf5_group_by_name(f, layers)          else:              topology.load_weights_from_hdf5_group(f, layers)  "
"Hello,i use the mobilenet as a backbone to train on the pretrained weights:mask_rcnn_coco.h5,  i met this mistake:  Loading weights  /root/data/mrcnn_test/mask_rcnn_coco.h5  Traceback (most recent call last):    File ""coco.py"", line 484, in        model.load_weights(model_path, by_name=True)    File ""/root/data/mrcnn_test/model.py"", line 2402, in load_weights      topology.load_weights_from_hdf5_group_by_name(f, layers)    File ""/usr/local/lib/python3.6/dist-packages/keras/engine/topology.py"", line 3201, in load_weights_from_hdf5_group_by_name      ' element(s).')  ValueError: Layer #2 (named ""conv1"") expects 1 weight(s), but the saved weights have 2 element(s).  How to fix it?"
None
"So, i'm using `compute_ap` to compute a mean average precision in the following way (similar to the example in `train_shapes`) :    - APs = []  - For all images in my validation dataset  - - Load image  - - Detect on image  - - - If ""rois"" is empty, AP = 0  - - - Else  `AP, precisions, recalls, overlaps = utils.compute_ap()`  - - Append AP to APs  - mAP = mean(APs)    My problem is that a lot of AP values i compute are over 1.0 (example of an APs of a small set : `[2.0, 2.0, 2.0, 2.0, 2.0, 0.3333333333333333, 2.0, 0.05555555555555555, 1.375, 2.0, 2.0, 2.0, 0, 1.375, 2.0, 2.0, 2.0, 2.0, 1.5, 1.3, 2.0, 1.138888920346896, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.5, 2.0, 2.0, 2.0, 1.25, 1.5, 2.0, 1.375, 1.6666666666666665, 2.0, 0.0, 0.25, 1.25, 2.0, 1.375, 1.375, 2.0, 2.0, 2.0, 0.5, 1.1666666666666667, 2.0, 2.0, 0.8333333333333333, 2.0, 1.5]` )    So my mAP is `mAP :  1.6073586750158326`, over 1.0 and i don't get it. Maybe it's because i have multiple detections for the same object? Maybe i don't understand what AP stands for and in this case i think i would like it very much if someone could help me :zipper_mouth_face:     Thanks.    PS : i think I'm simply mistaking AP for precisions but now the same issue appears with my recalls, that are spread between 0 and 2 and it makes no sense to me."
"Hi there,  I am confused with the document here:    > steps_per_epoch: Total number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch. It should **typically** be equal to the number of unique samples of your dataset divided by the batch size.    In my poor understanding, the steps_per_epoch should be ""**EXACTLY THE SAME**"" as ""NUM_OF_SAMPLES / batch_size"", so all the samples can be iterated once.    To understand more about this param, I really want to know what keras dose when step_per_epoch is set **un-typically** .    To be clear, my questions are:    1.  when step_per_epoch   if some of the samples were skipped? So, the trainer gets the input size   NUM_OF_SAMPLES / batch_size,      > if  some of the samples imported multiple times to the trainer, so the actual input size > NUM_OF_SAMPLES?    Thank you so much for your time.  "
"Hi,   I have an idea for such a good neural network implementation. I've been playing with it and I think that this idea is not implemented.    Sometimes, could be useful to take into account multi type of image for the same scene (so the mask is the same), like RGB image and depth images, if you have this data is very sad to not use it.     So, maybe, if we could train with more data, the network will be more precise.    Thanks,"
"It seems that the ""log_dir"" is created in the callback ""keras.callbacks.TensorBoard"".    But in my application, I don't need the ""tensorboard"" analysis which could be a really time consuming job for my hardware condition.    So, i comment the 2298-2299 lines in file ""/mrcnn/model.py"".         After that an error happened:  OSError: Unable to create file (unable to open file: name = 'D:\XXXX\mask_rcnn\logs\XXXX20180510T1347\mask_rcnn_XXXX_0001.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 302)    A workaround now is to add the following codes after line ~~2295~~ 2237 in ""model.py"":         But i am not sure that if there is a deeper consideration for the current behave, please kindly let me know if so@waleedka     "
Can I run your code with pytorch0.2 ?
"I use this function to show gt_masks and predicted_masks, however the iou is 4.74, which is larger than one.           iou is the area of overlap/ area of union, and in theory it should be range [0,1].  "
Are there any blogs/instruction to do semantic segmentation? 
"Traceback (most recent call last):    File ""samples/nucleus/nucleus.py"", line 487, in        train(model, args.dataset, args.subset)    File ""samples/nucleus/nucleus.py"", line 259, in train      dataset_train.load_nucleus(dataset_dir, subset)    File ""samples/nucleus/nucleus.py"", line 209, in load_nucleus      image_ids = next(os.walk(dataset_dir))[1]  StopIteration      thanks very much"
How can I solve the problem? I use the GTX950M in 2G memory to train data.    !   
"Hi, I want to change the coco dataset with just 3 classes.   So:  + is this ok to use pre-train?  + if use pre-train can I just place a softmax with 3 class at last?  + Can anyone give me some hints about how to do change the last layer? 😄   Thanks a lot."
"hey all  i have 16 class i wish to annotate and their are marked with x,y coordinates in the same way of the ""Balloon example"".  i didn't find any exact implementation such as the one i need and i'm struggling debugging what's wrong.  i tried combining some part from coco & balloon implementations, but i can't make it work.  i'm getting this error:  ""sequence argument must have length equal to input rank""  the full error log and my load_mask function are attached.  please help :)            "
"@waleedka   when ""Applies the given deltas to the given boxes."",     the author let:  center_y += deltas[:, 0] * height  height *= tf.exp(deltas[:, 2])    why use ""multiply"" not ""add"", i.e. center_y += deltas[:, 0], height += deltas[:, 2]?  why dy, ..., dh mean multiple, not increment?    Thanks!!"
"I am trying to run the demo.ipynb file, however upon running:       I receive the following error: module 'coco' has no attribute 'CocoConfig'    However, I have imported the coco module. Hence I wonder how to fix this?    "
None
"I have trained my own dataset and obtained my model.h5 in the log folder, how I could use this model to segment other picture? "
"When i try to continue training from a previous checkpoint, it always tries to load from this checkpoint:  `Checkpoint Path: /home/Mask_RCNN/logs/coco20180423T1626/mask_rcnn_coco_{epoch:04d}.h5`    Which is because of this call:     which seems to be broken  "
"i download stage1_test,stage1_train,stage1_train_labels.csv,stage2_test.However,it seems i still miss something.Can you show me how to set the files of the nucleus dataset which will be load in nucleus demo?thanks a lot."
"Hi,    I managed to run the demo notebook and got the results below,         I can work out the rest but could not work out how to get how to get the xmin, ymin, xmax and ymax?  Are the numbers in rois array the co-ordinates?  It is weird to have a 0 there if they are.    Cheers,  Lobbie    "
"Hi,    I realized that you put up a nice tutorial with ready-to-go implementation for training mask-rcnn on the data science bowl 2018. Unfortunately, I couldn't find any performance related figures of the implementation that you provided. Could you roughly tell me, what performance I would achieve, if training the mask-rcnn with the code?    Thanks a lot for your help!    "
"In the last version of this repo, when running an inference with  , a third tensor is needed for the input: `input_anchors`.    Are we supposed to build the `anchors` with a similar function at this one:   ?    Or is it only used for training? In that case, how can we disable it?    For example:         returns:     "
"I'm working with `demo.ipynb` and am having trouble running `model.detect()`. Specifically, when running the following block of code:       the following error is returned:        Any help on this issue would be much appreciated. Please let me know if I can provide extra information. **Thank you very much!**    "
"In the new augmentation function, I noticed that the augmentation is applied in the generator part as a replacement of original image form. Does this mean augmentation image will replace the original image, instead of increasing the overall training image amount?  It is possible that I mis-understand the codes. It would be nice if someone can sort this out. Thanks."
"I updated my clone and ran it on my previous codes, which worked great in previous MASK-R-CNN, but now it gives me error like when training:  InvalidArgumentError: Integer division by zero     ]]    Any ideas? Thanks!"
"I see a ZeroDivisionError when i start training, any idea what could be the reason?       "
"Hi,  This repo is very useful. thanks Matterport and other contributers.  I am getting nice results on low res images (mAP ~ 0.85), but when trying to train on higher resolution, process fails/stuck/not running.    I would love to get some tips on how to train for higher res. and if anyone can share configuration + GPU used to successfully train on high res images.    THANKS A LOT.        This is my HW -   name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582  pciBusID: 0000:01:00.0  totalMemory: 10.92GiB freeMemory: 10.38GiB  tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10044 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)    This is my last config details -    Configurations:  BACKBONE                       resnet50  BACKBONE_SHAPES                [[80 80]   [40 40]   [20 20]   [10 10]   [ 5  5]]  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     1  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  DETECTION_MAX_INSTANCES        20  DETECTION_MIN_CONFIDENCE       0.65  DETECTION_NMS_THRESHOLD        0.3  GPU_COUNT                      1  GRADIENT_CLIP_NORM             2  IMAGES_PER_GPU                 1  IMAGE_MAX_DIM                  320  IMAGE_META_SIZE                14  IMAGE_MIN_DIM                  320  IMAGE_MIN_SCALE                0  IMAGE_RESIZE_MODE              pad64  IMAGE_SHAPE                    [320 320   3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [138.25 128.33 118.01]  MINI_MASK_SHAPE                (56, 56)  NAME                           Post_Zeekit_320_  NUM_CLASSES                    2  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    128  STEPS_PER_EPOCH                3200  TRAIN_BN                       False  TRAIN_ROIS_PER_IMAGE           56  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               320  WEIGHT_DECAY                   0.0001"
"   why (y2, x2) shift by 1 pixel ?"
"I have trained an object detection model using Faster RCNN on a set of images where height and width or both of objects might be small as compared to the dimensions of the original image but the results are not good.     According to my hypothesis, this might have happened because after all the convolutions took place the feature map that remained might not have any information left regarding the objects as their dimensions are too small as compared to the dimension of the image.   So, I am just curious to know whether Mask RCNN will give me good detections or this hypothesis is true for Mask RCNN.     Thanks,  Sanyam Sharma"
"I cloned the Repo and tried to run demo.ipynb    I am consistently getting *** No instances to display ***     I have NOT trained my model. I simply ran the notebook, it downloaded ""mask_rcnn_coco.h5"" from ""     Attaching screen shots of output    !     "
"ValueError: Layer roi_align_classifier was called with an input that isn't a symbolic tensor. Received type:  . Full input: [[],  ,  ,  ,  ,  ]. All inputs to the layer should be tensors.    I get this error when I run the code:  mrcnn_model = model.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)    In my MacBook without GPU.  tf.__version__=1.3.0  keras.__version__=2.0.8    how can I solve this problem?"
"Thanks for your amazing work . Im trying to segment multiple smaller objects of few mm size contained in a bigger object. The problem arises when the size is so small. Im following BaloonConfig exactly to train the network. The model performs well but not always. it fails to detect certain smaller objects.  Im wondering if changing RPN_ANCHOR_SCALES to (8, 16, 32, 64, 128) could work    May I know How should I choose the various hyper parameters for detecting or masking such minute objects? If you can share your valuable suggestions, it will be helpful to gain deeper insights. Thanks."
"I want to make sure the RPN which is the first state of Mask RCNN work, so I only trained the RPN. I trained 40001 iterations, initial lr is 0.001, backbone is ResNet_V2_50. But I got several problems:    1. I found out the weight basically is not updated after 5000 iterations, and the gradients are really close to zero. wired weight  !    I have tried to use a lower lr, and the epsilon I use in the tf.train.AdamOptimizer is 0.0001. Still the weight seems not updated.    2. As for the prediction, I feel the bounding box prediction is quite ok   !    (green are the ground truth, red are the predicted box, these predicted boxes are selected if the anchors has a overlapping ratio larger than 0.7 with the ground truth bounding box), predicted bounding box, so it means at least the bounding box regression loss can make the prediction close to the ground truth. But as for the rpn_cls_loss, it classify too many wrong proposals to be object   !   . I have consider the class imbalance problem(in gland cell dataset, average number of positive anchors is only 20 per image, only 85 training images), so I random choose 128 positive anchor and 128 negative anchors (if the positive anchor smaller than 128, I random_shuffle the index so one positive anchor will appear several times), but still it doesn't work. One thing I don't understand is that when we calculate the loss, we only consider a minibatch, but at the same time, we classify all the anchors that we generated to be a object or non-object, so what about those anchors which are not contributing to the loss? Can the network also make correct decision for those anchors?  I would really appreciate your help!! Thanksss  a lot!!!!!!"
"The current detection bounding boxes are in the coordinates of resized image, how do you get the coordinates with respect to the original input size?    for example, My original image size is 1920x1080, where my network is trained with 1024x1024, so the image is first resized and padded to 1024x1024. The predicted bounding boxes coordinates is in within the 1024x1024 coordinates, but how do I get the coordinates in 1920x1080 coordinates?"
"along with plygons, my json also has circles and rect as follows. How do I parse this as well and use it while training baloon dataset?     "
"     I would think it would be `gt_masks.shape[0], gt_masks.shape[1]`"
"Hi, I was trying to follow the content in `deom.ipynb`, and found the following line:  `    utils.download_trained_weights(COCO_MODEL_PATH)`  should be  `    mrcnn.utils.download_trained_weights(COCO_MODEL_PATH)`"
"Im using the following code to train multiple classes after referring to  .          Im getting the following error.    >   >  File ""/home/rnd/miniconda3/envs/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper  >     return func(*args, **kwargs)  >   File ""/home/rnd/miniconda3/envs/lib/python3.6/site-packages/keras/engine/training.py"", line 2192, in fit_generator  >     generator_output = next(output_generator)  >   File ""/home/rnd/miniconda3/envs/lib/python3.6/site-packages/keras/utils/data_utils.py"", line 793, in get  >     six.reraise(value.__class__, value, value.__traceback__)  >   File ""/home/rnd/miniconda3/envs/lib/python3.6/site-packages/six.py"", line 693, in reraise  >     raise value  > IndexError: boolean index did not match indexed array along dimension 0; dimension is 0 but corresponding boolean dimension is 4  > "
"This is not an issue but a question.    How does the image size affect the accuracy of the results? I assume that with lower resolution we loose details and we can detect less features.  Does it make sense to continue training only the heads with pre-trained weights loaded from cocos with a different image resolution? I was thinking that since that model with pre-trained weights was trained with images of a certain resolution, there would be suboptimal results with features extracted from lower-resolution images."
"Hello,    Can I train the network without having the bbox of the objects? I know that I can theorically compute them, but it mostly is pipes, wires, small and narrow objects...    Thanks"
"I'm trying to train using a custom coco-style dataset. Running through the `train_shapes.ipynb` example substituting the dataset to my coco-style dataset, I get bounding box detection working, but masks do not. consequently `mAP` is 0.0.     Masks seem to show up fine when using the toy shapes dataset.     "
"I have a dataset , and I want to convert my labels into coco format.    I read all the posts and issues but still very confused.    My dataset has images and labels  Labels are in format of (x,y,w,h)    Can someone tell me in steps how can i convert these labels into coco format ?    "
"Hi,  Thanks for sharing such a project first!  And I find that in model.py, ""fpn_classfier_graph"" has already implemented ""softmax"" activations on mrcnn_class_logits, while in ""mrcnn_class_loss_graph"" the ""tf.nn.sparse_softmax_cross_entropy_with_logits"" API expects unscaled logits and internally performs a softmax on logits.  Thus the mrcnn_class_logits seems to be activated twice."
"I need to use other GPU to train instead of the first one. However, I tried different ways as follows to assign other GPU for this program but none of them succeeded. What should I do?    keras version: 2.1.5  tf version:1.4.1   system version: ubuntu 14.04.1 LTS  1:     2:     3:     Could anyone help me? Thanks in advance."
"Just cloned the repo, changed only the num of gpus from 1 to 2 and it produces the following error:   "
Same as the title.....
"If crop the object by Bounding Box, where I should go to find and fix the code about this?"
"I want to train in samples/nucleus with multi gpu -> GPU_COUNT = 2(NucleusConfig)  but it's show following error :  Traceback (most recent call last):    File ""/home/panotech/PycharmProjects/Mask_RCNN/samples/nucleus/nucleus.py"", line 455, in        model_dir=args.logs)    File ""/home/panotech/PycharmProjects/Mask_RCNN/mrcnn/model.py"", line 1820, in __init__      self.keras_model = self.build(mode=mode, config=config)    File ""/home/panotech/PycharmProjects/Mask_RCNN/mrcnn/model.py"", line 2039, in build      model = ParallelModel(model, config.GPU_COUNT)    File ""/home/panotech/PycharmProjects/Mask_RCNN/mrcnn/parallel_model.py"", line 37, in __init__      merged_outputs = self.make_parallel()    File ""/home/panotech/PycharmProjects/Mask_RCNN/mrcnn/parallel_model.py"", line 81, in make_parallel      outputs = self.inner_model(inputs)    File ""/usr/local/share/anaconda3/envs/zzg3.6/lib/python3.6/site-packages/keras/engine/topology.py"", line 619, in __call__      output = self.call(inputs, **kwargs)    File ""/usr/local/share/anaconda3/envs/zzg3.6/lib/python3.6/site-packages/keras/engine/topology.py"", line 2085, in call      output_tensors, _, _ = self.run_internal_graph(inputs, masks)    File ""/usr/local/share/anaconda3/envs/zzg3.6/lib/python3.6/site-packages/keras/engine/topology.py"", line 2236, in run_internal_graph      output_tensors = _to_list(layer.call(computed_tensor, **kwargs))    File ""/usr/local/share/anaconda3/envs/zzg3.6/lib/python3.6/site-packages/keras/layers/core.py"", line 663, in call      return self.function(inputs, **arguments)    File ""/home/panotech/PycharmProjects/Mask_RCNN/mrcnn/model.py"", line 1913, in        anchors = KL.Lambda(lambda x: tf.constant(anchors), name=""anchors"")(input_image)    File ""/usr/local/share/anaconda3/envs/zzg3.6/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 214, in constant      value, dtype=dtype, shape=shape, verify_shape=verify_shape))    File ""/usr/local/share/anaconda3/envs/zzg3.6/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py"", line 433, in make_tensor_proto      _AssertCompatible(values, dtype)    File ""/usr/local/share/anaconda3/envs/zzg3.6/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py"", line 341, in _AssertCompatible      raise TypeError(""List of Tensors when single Tensor expected"")  TypeError: List of Tensors when single Tensor expected    Training with Multi-GPUs is not supportted yet? "
"For NUM_CLASSES = 7, instead of 4:  ValueError: Error when checking input: expected input_image_meta to have shape (19,) but got array with shape (16,)"
"After the recent git pull, I am having this error  ""ModuleNotFoundError: No module named 'mrcnn'"""
"if  self.epoch = int(m.group(6)) + 1, then continue train from last model like epoch 5 will generate new model epoch 7, missing epoch 6."
"the same dataset I create of my own, doing well.  But after update to 2.1, in training,  I got this error message:  ValueError                                Traceback (most recent call last)    in  ()        2             learning_rate=config.LEARNING_RATE,        3             epochs=2,  ----> 4             layers='all')    /mnt/disks/sdb/Mask_RCNN/mrcnn/model.py in train(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation)     2312             max_queue_size=100,     2313             workers=workers,  -> 2314             use_multiprocessing=True,     2315         )     2316         self.epoch = max(self.epoch, epochs)    ~/tensorflow_GPU/lib/python3.5/site-packages/keras/legacy/interfaces.py in wrapper(*args, **kwargs)       89                 warnings.warn('Update your `' + object_name +       90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)  ---> 91             return func(*args, **kwargs)       92         wrapper._original_function = func       93         return wrapper    ~/tensorflow_GPU/lib/python3.5/site-packages/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)     2222                     outs = self.train_on_batch(x, y,     2223                                                sample_weight=sample_weight,  -> 2224                                                class_weight=class_weight)     2225      2226                     if not isinstance(outs, list):    ~/tensorflow_GPU/lib/python3.5/site-packages/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight)     1875             x, y,     1876             sample_weight=sample_weight,  -> 1877             class_weight=class_weight)     1878         if self.uses_learning_phase and not isinstance(K.learning_phase(), int):     1879             ins = x + y + sample_weights + [1.]    ~/tensorflow_GPU/lib/python3.5/site-packages/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)     1474                                     self._feed_input_shapes,     1475                                     check_batch_axis=False,  -> 1476                                     exception_prefix='input')     1477         y = _standardize_input_data(y, self._feed_output_names,     1478                                     output_shapes,    ~/tensorflow_GPU/lib/python3.5/site-packages/keras/engine/training.py in _standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)      121                             ': expected ' + names[i] + ' to have shape ' +      122                             str(shape) + ' but got array with shape ' +  --> 123                             str(data_shape))      124     return data      125     ValueError: Error when checking input: expected input_image_meta to have shape (13,) but got array with shape (14,)  Could anyone help me with this?  Thanks a lot"
"Hello everyone    First, it's a very detailed repo, thanks for everything the contributors have done here.    These days, I am working on training with my own datasets, but I get stuck on few questions.      I have read ""Training on Your Own Dataset"" section,  `train_shapes.ipynb` and `inspect_data.ipynb`.    Here's what I have now    ## My own dataset    ### Number of class  Only one  ### Images  It contains only one class of images, for example, a ball like stuff  ### Masks  I already have its mask images as ground truth.  ---  ## Questions  In ""Training on Your Own Dataset"" section   ### Q1  About Config, it said   > Subclass it and modify the attributes you need to change.     Does this mean that I should create a subclass of class `Config`(which is in `config.py`) in a python file like `coco.py`?     ### Q2  In the training part of  ,  what is inside `dataset_train` and `dataset_val` object in the snippet below? I'm not sure whether should I feed images with `bbox` intio the model if I use my own `Dataset` class?        model.train(dataset_train, dataset_val,                   learning_rate=config.LEARNING_RATE,                   epochs=1,                   layers='heads')  ---  If there is any training process document that I missed, pls tell me    I would be thankful for any suggestion, thank you for your time :)"
Is there already an implementation of Mask R-CNN with MobileNet V1 or V2?  If not is there a reason/ is it not possible?    As my plan is to implement a Mask R-CNN Model for Mobile usage (Jetson Tx2) I would like to know if there is something that would speak against it.    Thanks!    
"I have annotated hundreds of images and have trained for 30 epoch . However when I try to predict after the training, it is not segmenting the object...How can I be able to sort this out?"
"While constructing the model the batch-norm layers are set to be non-trainable when `train_bn = False`. The `train` function of the MaskRCNN class sets certain layers to trainable by calling `set_trainable`, including all batch-norm layers (if the regex is e.g. ""*"").    Due to this the batch-norm layers are (probably) always set to trainable if the train function is called with `layers = 'all'`.    To fix this the batch norm layers should be excluded from being affected by the set_trainable function, e.g. by changing the regex to always exclude certain names (even though that is a bit hacky...).     Somewhat related the parameter in the Layer constructor called in line 77 of model.py should be `trainable` not `training`. This means that  `return super(self.__class__, self).call(inputs, training=training)` should be `return super(self.__class__, self).call(inputs, trainable=training)`   .   Until this is changed setting train_bn unfortunately does not do anything.    I unfortunately don't have access to a GPU for a few weeks, so I can not check if those two issues really exist, but if they do, they probably affects the performance of the model a fair bit."
"utils uses scipy.misc.imresize() to resize any images with scale != 1. scipy.misc.imresize outputs type uint8 in the range 0 - 255 as default. This means that you can be sending in images that are float 0-1 range, say, but the result of the resizing will transform them to 0 -255 scale.    The problem manifests most severely when you have some images that are being resized (scale != 1) and some that are not (scale == 1). In this instance if the images you feed in are in a range that is not 0 - 255 you are going to get a mismatch in images being fed into the model.    This may be a problem if you are using greyscale images via skimage.color.rgb2gray(), for example. skimage.color.rgb2gray outputs type float in the range 0 - 1 as default."
No instances_valminusminival2017.json file found
"@waleedka.When I use ""mask_rcnn_balloon.h5"" to splash, everything goes perfect,but when I use the model I trained which based on coco,I can get the predict-bboxes and scores but no masks!  I changed nothing except model path,Do you know the reason,Thanks a lot"
"I have used Mask RCNN module for many images and it worked nicely. However, I received the following error recently and I have no clue how to solve it. Could someone please help me? I have used the code from the github repo.    Traceback (most recent call last):    File ""/home/omkar/pycharm-community-2017.3.4/helpers/pydev/pydev_run_in_console.py"", line 53, in run_file      pydev_imports.execfile(file, globals, locals)  # execute the script    File ""/home/omkar/pycharm-community-2017.3.4/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile      exec(compile(contents+""\n"", file, 'exec'), glob, loc)    File ""/home/omkar/PycharmProjects/mask_rcnn/demo.py"", line 149, in        result = model.detect([image])    File ""/home/omkar/PycharmProjects/mask_rcnn/model.py"", line 1599, in detect      detections, mrcnn_mask = self.predict([molded_images, image_metas], mode='inference')    File ""/home/omkar/PycharmProjects/mask_rcnn/model.py"", line 1674, in predict      detections = detection_layer(self.config, rpn_rois, mrcnn_class, mrcnn_bbox, image_metas)    File ""/home/omkar/PycharmProjects/mask_rcnn/model.py"", line 842, in detection_layer      detections = refine_detections(rois, mrcnn_class, mrcnn_bbox, window, config)    File ""/home/omkar/PycharmProjects/mask_rcnn/model.py"", line 787, in refine_detections      keep = torch.nonzero(keep_bool)[:,0]    File ""/home/omkar/anaconda3/envs/mask_rcnn/lib/python3.6/site-packages/torch/autograd/variable.py"", line 78, in __getitem__      return Index.apply(self, key)    File ""/home/omkar/anaconda3/envs/mask_rcnn/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py"", line 89, in forward      result = i.index(ctx.index)  IndexError: trying to index 2 dimensions of a 0 dimensional tensor    Thanking you in anticipation.  Regards,  Omkar."
None
"Hi @waleedka ,     I did a pull on master for Mask_RCNN. after this i get the stack-trace below which loading the weights for coco. Cannot figure out if it is a local issue or a bug in master ?     my command is:     elif init_with == ""coco"":      # Load weights trained on MS COCO, but skip layers that      # are different due to the different number of classes      # See README for instructions to download the COCO weights      model.load_weights(COCO_MODEL_PATH, by_name=True,                         exclude=[""mrcnn_class_logits"", ""mrcnn_bbox_fc"",                                  ""mrcnn_bbox"", ""mrcnn_mask""])    The stack-trace is:    File ""/home/mpsampat/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py"", line 686, in _call_cpp_shape_fn_impl      input_tensors_as_shapes, status)    File ""/home/mpsampat/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__      c_api.TF_GetCode(self.status.status))  tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimension 3 in both shapes must be equal, but are 12 and 6. Shapes are [1,1,512,12] and [1,1,512,6]. for 'Assign_642' (op: 'Assign') with input shapes: [1,1,512,12], [1,1,512,6].    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""./train_mask_rcnn.py"", line 266, in        ""mrcnn_bbox"", ""mrcnn_mask""])    File ""Mask_RCNN2/model.py"", line 2075, in load_weights      topology.load_weights_from_hdf5_group_by_name(f, layers)    File ""/home/mpsampat/miniconda3/lib/python3.6/site-packages/keras/engine/topology.py"", line 3468, in load_weights_from_hdf5_group_by_name      K.batch_set_value(weight_value_tuples)    File ""/home/mpsampat/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 2368, in batch_set_value      assign_op = x.assign(assign_placeholder)    File ""/home/mpsampat/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 599, in assign      return state_ops.assign(self._variable, value, use_locking=use_locking)    File ""/home/mpsampat/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py"", line 280, in assign      validate_shape=validate_shape)    File ""/home/mpsampat/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py"", line 58, in assign      use_locking=use_locking, name=name)    File ""/home/mpsampat/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper      op_def=op_def)    File ""/home/mpsampat/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3273, in create_op      compute_device=compute_device)    File ""/home/mpsampat/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3313, in _create_op_helper      set_shapes_for_outputs(op)    File ""/home/mpsampat/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2501, in set_shapes_for_outputs      return _set_shapes_for_outputs(op)    File ""/home/mpsampat/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2474, in _set_shapes_for_outputs      shapes = shape_func(op)    File ""/home/mpsampat/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2404, in call_with_requiring      return call_cpp_shape_fn(op, require_shape_fn=True)    File ""/home/mpsampat/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py"", line 627, in call_cpp_shape_fn      require_shape_fn)    File ""/home/mpsampat/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py"", line 691, in _call_cpp_shape_fn_impl      raise ValueError(err.message)  ValueError: Dimension 3 in both shapes must be equal, but are 12 and 6. Shapes are [1,1,512,12] and [1,1,512,6]. for 'Assign_642' (op: 'Assign') with input shapes: [1,1,512,12], [1,1,512,6]."
"The labelme generated files are label.png and info.yaml , but not instancess_train2014.json, how to make it"
"Hello, I keep getting ValueError. My setup is following: Ubuntu 16.04, python 3.6, tensorflow 1.5. Please help me with this error. Thanks!     "
"Briefly, even I set the following settings in inspect_model.ipynb, the gpu cannot be specified     but anything work well in train_shapes.ipynb, I wonder what is the difference and how can I fix it.  Maybe a leak happens somewhere I do not find...    Thanks!  "
"Some individuals in my training data are occluded so that they are divided into two parts. How do I go about labelling this kind of data? If I label each half separately, then I'm telling Mask-RCNN that they are different individuals. Do I just label the entire individual, including the parts that are occluded? The problem there is that when an individual is partly occluded, it's standard to draw around only the visible parts. I'm using VGG image annotator."
"Thanks, guys for this awesome repository and implementation of Mask R-CNN. This is a very high-quality work and could potentially be very useful to me.    In order to ease its use, would you be ok to convert it to a proper Python library?    It would just be a matter of moving all the Python file into a specific folder (let's say `mask_rcnn`) and add a `__init__.py` file into it so people can import it without running the code from inside the repository.    Then a `setup.py` file would allow upload on pypi so the library would easy to install with `pip` without having to clone the repository.    What do you think?    I can make a PR if you want."
"I passing the augmentation = imgaug.augmenters.Affine(rotate=(-45, 45)) in model.train() but the kernel always crash when in 2018 bowl dataset.    But it works in train_shapes.ipynb. Did the 2018 bowl dataset too big or any suggestion?    My environment are shown below:  two 1080ti   tensorflow-gpu==1.3.0  CUDA 8  cudnn 6"
"Hi,    Currently I'm training this algorithm on one class.  My images have between 50 and 300 objects on the same image and the goal is to detect them all.    Now I did a limited pilot test and detection/mask generation does work, however during this test I only annoted about 10 objects per image.  During detection it detects between 7 to 14 objects per image.    Now I was wondering if there is any relationship between this training and the detection or should I just get a larger training set.  Should I annotate my images completely for training for example all 300 objects? Or can I annotate a subset for training and will it then be able to detect everything or will the algorithm presume everything else I did not annotate is background?    Thanks in advance,  Kind regards,  Sören"
"Hello everyone,    I saw that in 'load_image_gt' it is possible to add an option for data augmentation and that this one is used in 'data_generator` used in 'inspect_data.ipynb'. Yet in the notebook I can't see where you can use it for data augmentation for training.  It seems that it is automatically called for training, is that right?    Thanks!  "
"In coco.py, annotation format is coco annotation format.  In balloon.py, the annotation is achieved by VIA which is different to coco format.  In shapes.py, the annotation is not needed coz there is no dataset  I successfully trained, evaluated and tested all samples and notebooks (coco, balloon, shapes) and I also created my own dataset, my annotation by VIA.  I could train the model using my own dataset successfully, but only for one class (one category).  I need to train for more class and I did try to modify funtions: load_objects() and load_mask() without success.  So please help."
"Hello all,    I would like to use the Mask R-CNN on brand new pictures for inference. Yet I can't figure out how to use an image that has not been used in a validation or train dataset object.    Can someone help me with this?    Thank you!"
"The mini mask is converted to all zeros(False) on the scipy imresize function when the mask contains only 1's.(ie., if the bounding box rectangle doesn't contain even one background pixel).    m = scipy.misc.imresize(m.astype(float), mini_shape, interp='bilinear')  mini_mask[:, :, i] = np.where(m >= 255, 1, 0) "
"Thank you for your code! @waleedka   To make it run for my own project, I have modified the code in these ways:  1. Using python 2.7, modify some functions related to download or other incompatible aspects.  2. Move the layers related to segmentation, as well as the mask related part in the input generators.  Then I run the code, and it can runs, the losses are all decreasing seemingly normally. But:  1. I noticed the losses decrease slowly after 10+ epochs in the first stage. So I stopped it and made it begin to run the second stage (layer 4+ trainable), but the total loss dramatically increased at the first epoch, compared with that before stopped in the first stage.  2. I used the trained model to detect in some sample images, and no right objections can be detected.    My dataset has 21 classes , each with ~2k images for training. The config is the same as that in your code for training coco.     Can you give some advice for make the model work as for my project?    Thank you!"
None
The docker image shared in the project supports CPU. Is there a docker image that can be used to run the code on GPU? Please share.
"@waleedka  Thank you for your code!  If I'm only interested in detection at present, what should I do to make the model only train for detection, but not segmentation?    Thank you !"
"I created my own dataset and annotation, however, I don't know how to calculate ""area"" in annotation. As I understand, it is not the image area, segmentation area, bounding box area."
"In [6]: elif config.NAME == ""coco"":    File "" "", line 1      elif config.NAME == ""coco"":         ^  SyntaxError: invalid syntax      Could you explain with I cannot run this commend line in ipython3 ?"
"UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape.   This may consume a large amount of memory.  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""    Hi. I use your code to train a mask-rcnn, but the training is ridiculous slow.  I google it and find someone meet the same problem   bu i still don't know how to fixed these problem, any advises? "
"Hello,    I am trying to get how defining my own dataset works. However I can't understand the aspect of a mask for a picture where you have several instances of the same class. How do you distinguish these two instances?    Can someone help me?    Thanks!"
!   As in  ， why we need the stop operation? 
"hi,  when i download the maks_rcnn_coco.h5 file,the error information message me ""Connection timed out"".i don't know wheather the url is invalid or some other reason. I will be grateful to you，if anyone can email the file to me .my email:shhuixi@qq.com"
"I can finish the shapes dataset in this project successfully,  but when i change the data to myself, something crashed.  it seems that the problems locates at line2234 , self.keras_model.fit_generator(), in the model.py.  Besides that , i have overwritten the load_image and load_mask function in the new ipynb.I print them and compare to the shapes dataset, i am sure that the format and content of my data is correct as the shapedataset.    I have tried change the parameters in this function.  Such as changing workers=1 and use_multiprocessing = False, but it still showed this problem.      Ps: My operation systerm is linux      so what's wrong ?      EOFError                                  Traceback (most recent call last)    in  ()        6             learning_rate=config.LEARNING_RATE,        7             epochs=1,  ----> 8             layers='heads')        9        10     ~/Kaggle/master/kerastf_maskrcnn/model.py in train(self, train_dataset, val_dataset, learning_rate, epochs, layers)     2242             max_queue_size=1000,     2243             workers=workers,  -> 2244             use_multiprocessing=True,     2245         )     2246         self.epoch = max(self.epoch, epochs)    ~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/legacy/interfaces.py in wrapper(*args, **kwargs)       89                 warnings.warn('Update your `' + object_name +       90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)  ---> 91             return func(*args, **kwargs)       92         wrapper._original_function = func       93         return wrapper    ~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)     2174                                                  use_multiprocessing=use_multiprocessing,     2175                                                  wait_time=wait_time)  -> 2176                 enqueuer.start(workers=workers, max_queue_size=max_queue_size)     2177                 output_generator = enqueuer.get()     2178             else:    ~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/utils/data_utils.py in start(self, workers, max_queue_size)      701             self.max_queue_size = max_queue_size      702             if self._use_multiprocessing:  --> 703                 self._manager = multiprocessing.Manager()      704                 self.queue = self._manager.Queue(maxsize=max_queue_size)      705                 self._stop_event = multiprocessing.Event()    ~/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/context.py in Manager(self)       53         from .managers import SyncManager       54         m = SyncManager(ctx=self.get_context())  ---> 55         m.start()       56         return m       57     ~/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/managers.py in start(self, initializer, initargs)      481         # get address of server      482         writer.close()  --> 483         self._address = reader.recv()      484         reader.close()      485     ~/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/connection.py in recv(self)      248         self._check_closed()      249         self._check_readable()  --> 250         buf = self._recv_bytes()      251         return ForkingPickler.loads(buf.getbuffer())      252     ~/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/connection.py in _recv_bytes(self, maxsize)      405       406     def _recv_bytes(self, maxsize=None):  --> 407         buf = self._recv(4)      408         size, = struct.unpack(""!i"", buf.getvalue())      409         if maxsize is not None and size > maxsize:    ~/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/connection.py in _recv(self, size, read)      381             if n == 0:      382                 if remaining == size:  --> 383                     raise EOFError      384                 else:      385                     raise OSError(""got end of file during message"")    EOFError: "
"I understand that the ROI outputs for each detected object correspond to [y1, x1, y2, x2]. I would like to convert the outputs to MOT Challenge format so that I can use the detections for an object detection task (using DeepSORT). The MOT Challenge format is described here:      ` ,  ,  ,  ,  ,  ,  ,  ,  ,  `"
"I see under `load_image_gt`:  `_idx = np.sum(mask, axis=(0, 1)) > 0`  ` mask = mask[:, :, _idx]`    Shouldn't we add as well the following ?  `class_ids = class_ids[_idx]`"
"I get the following error when using Spyder to run the code in the demo.    File "" "", line 10, in        import coco      File ""/Users/baozhang/Mask_RCNN/coco.py"", line 48, in        from config import Config    ImportError: cannot import name 'Config'"
"Hi there, what would I need to change in the config / loss function in order to train this on lower resolution images? I would like to train on (256, 256, 3) images instead of (1024, 1024, 3) images. When I changed     This seemed to work, but I started to get nan's for my rpn loss value. Any suggestions on what I might need to change in order to train on lower resolution images? (I am hoping that just by changing the IMAGE_MAX_DIM the code will actually resize my images for me). If that's not the case please let me know.    Thanks,  Edited 03/13/18 ===   "
"Hello,    I wanted to know what should change in the code so that we would be able to use the CPU instead of the GPU... I know we have to describe the config and put GPU to 0 but I can't figure out where in the code.    Thanks!"
Train_shapes.ipynb train my picture (pixel 192*192 and 100 total) later save the mask_rcnn_shapes_0001.h5 file.  Demo.ipynb load the mask_rcnn_shapes_0001.h5 weight testting picture as same as the trainning picture.  The test result show No instances to display error!  I'm not familiar with it. I wonder if you have any idea about solving this problem??  train picture show below:  !   I use labelme annotation and label_visulization piture show below:  !   Train the data using model train_shapes.ipynb each partially loss and picture show below:  !   Demo.ipynb test error show below:  !   
"I found that you have frozen the batch normalization in your model.py    Because I am working on a different task, likes medical image. Hence, I have to train the batch normalization. First, I will use the small size of input likes 128x128 to enable training with a batch size of 16. How could I modify the model.py?    Second question: After completed the training batch normalization with the batch size of 16 and image size of 128x128, I will freeze the batch size (as you did) and train the network with the size of 512x512 with checkpoint from the size of 128x128. Which layer should I ignore when I use my pre-trained model?"
"Hello!  I'd like to use Mask RCNN for satellite imagery analysis.  In addition to  aerial view (RGB input), I want model to learn from hillshade view (thus learn topography) as well as Indexes data (NWDI -- Normalized Difference Water Index, EVI --Enhanced Vegetation Index).  Could you please provide guidance how to update the code to support N channels (instead of 3)?  Also ideally I'd love  to use pretrained weights when possible rather start completely from scratch all the time.  Thanks a lot!"
!   I'm confused about the reshape in these two place. Any suggestion will be appreciated.
"I think i narrowed the problem to a small portion of my code but i really do not get why it is not working.  Probably it has something to do with some basic python string comparison operation.  The image_info dictionary objects get initialized correctly.  I did the following to check, if the image can be found in the image_info[] list:         For some reason the first way of getting the path to the image `i.get('path')` always works without a problem and the images will be loaded correctly. That means all the image_info dictionary objects are stored and loaded correctly, when I call `setVal.load_dataset(dataValPath)`  But the second way `setVal.image_reference(i.get('id')` only works for the first few and then at some random image it throws `""FileNotFoundError: [Errno 2] No such file or directory: 'not found'""`    So i guess there is a problem with the `image_reference(self, image_id)` method.  I think it is this exact line of code: `if(i.get('id') == image_id):`  Could it be the case that == does not compare the content but the two objects?  I am coming from C# and am new to python."
"Train_shapes.ipynb train my data later save the mask_rcnn_shapes_0001.h5 file. Demo.ipynb load the mask_rcnn_shapes_0001.h5 weight testting picture but run  this code happen error!    `model.load_weights(COCO_MODEL_PATH, by_name=True)`    I'm not familiar with it. I wonder if you have any idea about solving this problem??  Below is the Trackback:  `File ""/home/maoge/my/maskcode/demo.py"", line 46, in        model.load_weights(COCO_MODEL_PATH, by_name=True)    File ""/home/maoge/my/maskcode/model.py"", line 2005, in load_weights      topology.load_weights_from_hdf5_group_by_name(f, layers)    File ""/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py"", line 3248, in load_weights_from_hdf5_group_by_name      K.batch_set_value(weight_value_tuples)    File ""/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py"", line 2365, in batch_set_value      assign_op = x.assign(assign_placeholder)    File ""/home/maoge/.local/lib/python3.5/site-packages/tensorflow/python/ops/variables.py"", line 573, in assign      return state_ops.assign(self._variable, value, use_locking=use_locking)    File ""/home/maoge/.local/lib/python3.5/site-packages/tensorflow/python/ops/state_ops.py"", line 276, in assign      validate_shape=validate_shape)    File ""/home/maoge/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_state_ops.py"", line 57, in assign      use_locking=use_locking, name=name)    File ""/home/maoge/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper      op_def=op_def)    File ""/home/maoge/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2958, in create_op      set_shapes_for_outputs(ret)    File ""/home/maoge/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2209, in set_shapes_for_outputs      shapes = shape_func(op)    File ""/home/maoge/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2159, in call_with_requiring      return call_cpp_shape_fn(op, require_shape_fn=True)    File ""/home/maoge/.local/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py"", line 627, in call_cpp_shape_fn      require_shape_fn)    File ""/home/maoge/.local/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py"", line 691, in _call_cpp_shape_fn_impl      raise ValueError(err.message)  ValueError: Dimension 1 in both shapes must be equal, but are 324 and 8 for 'Assign_682' (op: 'Assign') with input shapes: [1024,324], [1024,8].`  "
while testing the images I want to draw mask or bounding box only on the class I want to.  for eg. I want to detect person in an image then bounding box should be drawn on person class only  it should not draw on any other class of coco dataset .    can u tell me how to do that?
"Hi everyone,  I am trying to run the code on my laptop, however everytime I run   modellib.MaskRCNN it will raise the TypeError.  My settings:    1. Windows10  2. python 3.6.3  3. Anaconda 64bit  4. Tensorflow1.3 and Keras latest    and my error log is  :        It's always the typeError, I wonder whether this is my Python version too high?or some other approach to solve it ? Can anybody help me out? Thanks .        "
"       This is the error I get:                           Everytime I run the code, the error occurs with different images, but always only with images of the validation set.  For some reason these images are not present in the image_info list.      "
"I have installed all packages and I'm following the demo to try to run Mask_RCNN, but running the demo I have the following errors:         and          Is this a problem with any version or is a missing package? Thanks."
"When I run this command  `model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)`  get this error message in the file Mask_RCNN-master\model.py"", line 294  ` normalized_boxes = boxes / np.array([[height, width, height, width]])`  `ValueError: operands could not be broadcast together with shapes (0,) (1,4)`  "
"Because my dataset is very different with imagenet and coco. I also want to design a new backbone model that did not provide pre-trained weight. So, I want to train the project from scratch? Is it correct if I just delete the line   in the coco.py? Anything else?   "
"  I get two separate type errors when I run the demo.ipynb or the train_shapes.ipynb. Anyone have any ideas?  I'm using GTX 1060, windows, CUDA v 9.0.        demo.ipynb     train_shapes.ipynb   "
"Hello all, I have to work with the arbitrary size of the testing image without using resize it to a fixed dimension. In the training, I randomly crop the image into `256x256` to obtain a trained model. After that, I will test the model with different size in inference phase.     To do it, I followed the FCN paper that replaces the last fully connected layer to 1x1 convolutional layer. Look inside the implementation, I found two lines used fully connected layer                To convert from fully connected to fully convolutional layer, Is it right if I change these above lines? Could you give me some advice?  This is my current solution but it does not work        "
"!   As show in the figure, when I  feed 2 same images into the model, the output batch_size of proposal is still 1. What's wrong with it？ "
"As described above, we just want to  applies an operation to every batch slice of an input. So can the  ""utils.batch_slice()""method been replace with a wallpapered defined layer ?  I'm new to keras. Could you please give some suggestion?"
"Hello folks,    I was getting an strange error when predicting the mask of some images. Tracing the error back I think I might have found a bug.    In the function ** unmold_detections** defined like:    `    def unmold_detections(self, detections, mrcnn_mask, image_shape, window):          """"""Reformats the detections of one image from the format of the neural          network output to a format suitable for use in the rest of the          application.            detections: [N, (y1, x1, y2, x2, class_id, score)]          mrcnn_mask: [N, height, width, num_classes]          image_shape: [height, width, depth] Original size of the image before resizing          window: [y1, x1, y2, x2] Box in the image where the real image is                  excluding the padding.            Returns:          boxes: [N, (y1, x1, y2, x2)] Bounding boxes in pixels          class_ids: [N] Integer class IDs for each bounding box          scores: [N] Float probability scores of the class_id          masks: [height, width, num_instances] Instance masks          """"""          # How many detections do we have?          # Detections array is padded with zeros. Find the first class_id == 0.          zero_ix = np.where(detections[:, 4] == 0)[0]          N = zero_ix[0] if zero_ix.shape[0] > 0 else detections.shape[0]`    This last line is intended to be like that? If the first detection is 0, then zero_ix[0] will be 0 as well (first index). It should be like:    `N = zero_ix.shape[0] if zero_ix.shape[0] > 0 else detections.shape[0]`    Am I right?"
"Hi,    I wonder why the data augmentation is applied after image resizing in `load_image_gt` instead of before resizing. I am thinking to do more augmentation like rotation, cropping, etc. Doing these augmentation after resizing will have different pb, like zero area mask, or change the size of image. What do you think?"
"Like several others I am trying to use the matterport/Mask_RCNN in the Data Science Bowl 2018.  I am running training in Google Cloud ML.  I adapted the ""shapes"" demo to run in Google Cloud and it runs fast and well.    I took the running Shapes adaptation and replaced the Config and Dataset with new definitions for the DSB 2018 data sets.   If I try to run the full datasets.  The model hangs somewhere after the call to model.train(...) and never comes back (at least I haven't waited longer than 2 hours for it start the first epoch).  When I scale the data down to 5 images in the train set and 2 in the validation set.  It runs the first epoch, and hangs in the 3rd step of the second epoch.  (Note the ~30 min period I gave it to move to step 4 before killing the job).    !     I note from the performance data that my master is running at 100% memory usage on scale tier complex_l_gpu which is the largest available machine in Google Cloud ML.  I have read @waleedka note on issue #116 which indicates that the sparse to dense tensor conversion is not an issue, but I am wondering if it may be...    I am using mini masks.  Here is the data from my config.      !     I have tried it with 8 GPUs, I have tried it with 1.  I have tried scaling down the images.  I am currently setting up a standalone instance to eliminate the build in Google Cloud ML as the issue. (Will report back with results). Update, I have tested it on a stand alone instance as well.  With 208GB of memory, it is still maxing out!  After 90 min still waiting for the first epoch to start.  Something definitely wrong.  Could be my code, but I don't know how to diagnose.    I am wondering if anyone has a suggestion for diagnosing...    "
"Using an AWS Tesla K80 gpu, with Ubuntu.     I ran into the error after training one epoch. The error (same as title) was preceded by this message:     ""UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.""     Here's the entire error message:      "
"This is really great project - love the level of comments, the working notebooks, and the simple shapes dataset.....all makes it exceptionally easy to read.    One thing that is not clear - In the RPN proposal layer the deltas are multiplied by BBOX_STD_DEV. There was a previous issue asking about this    The answer given refers to fastrcnn paper section on normalising the regression targets for the loss function.    However this is applied after the RPN loss function; and before the bbox spec is normalised to 0-1 range. Also two of the delta numbers are log(delta). I can possibly envisage that you might divide by the standard deviation as part of normalisation. However this is multiplying; by a log; and not in the right place for that.    "
"As shown in following image:  !   The predictions of triangle and circle are reasonable.  Have tried to train with more epochs, but it doesn't help at all. Even predicting in the training set, the prediction of squares is as bad as the test set."
"Hi everyone,  After training my own model, I found that the result of model.detect([image], verbose=1) is not as good as the result of inspect_model.  In inspect_model, I can get both detection but not by calling directly model.detect  Any suggestion please??  Thanks"
"You make a difference between rpn_reature_maps and mrcnn_feature_maps, however in the paper both the classification/bb network and the mask network get the ROI's as input right? So why do you split them up into two different inputs?"
"I'm using a nuclei dataset from Broad Institute in the  . I already have 670 training images and masks for each nucleus uploaded on my jupyter notebook. I understand how to subclass the configuration part, but not the dataset. I don't know how to use these three methods within the new subclass NucleiDataset(utils.Dataset) : (1) def load_nuclei(self), (2) def load_mask(self, image_id), and (3) image_reference(self, image_id).     Here's the code I already used to prepare the dataset for a U-Net:     `IMG_WIDTH = 256  IMG_HEIGHT = 256  IMG_CHANNELS = 3  TRAIN_PATH = 'train/'  TEST_PATH = 'test/'    train_ids = next(os.walk(TRAIN_PATH))[1]  test_ids = next(os.walk(TEST_PATH))[1]    X_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8)  Y_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)    for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):      path = TRAIN_PATH + id_      img = imread(path + '/images/' + id_ + '.png')[:,:,:3]      img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)      X_train[n] = img      mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)      #gets and resizes the masks that contain the individual segmented nuclei      #the targets for the model      for mask_file in next(os.walk(path + '/masks/'))[2]:          mask_ = imread(path + '/masks/' + mask_file)          mask_ = np.expand_dims(resize(mask_, (256, 256), mode='constant',                                         preserve_range=True), axis=-1)          mask = np.maximum(mask, mask_)      Y_train[n] = mask        X_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8)  sizes_test = []  for n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):      path = TEST_PATH + id_      img = imread(path + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]      sizes_test.append([img.shape[0], img.shape[1]])      img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)      X_test[n] = img    from sklearn.model_selection import train_test_split  x_train, x_valid, y_train, y_valid = train_test_split(X_train, Y_train, test_size=0.1, random_state=8011)`    So I tried to incorporate this code into those three functions, but I'm not sure how to correctly do so. The training data has the following file to each image in the training set: 'train/image_id/images/image_id.png'.   Here's the path to the masks: 'train/image_id/masks/image_id.png. Maybe important to note that there are several masks for each individual image as each mask is a single segmented nucleus without overlap with the other masks.     Maybe I'm overcomplicating this, because it seems that the current setup is expecting a user's own dataset to be more steps away as downloadable from an external website, but in my case, the entire dataset is already uploaded and unzipped to my jupyter notebook.     Would anyone be willing and able to guide me on how to subclass Dataset to fit the already uploaded dataset to this model? "
"my notebook has GTX950M with 2 GB memery,12G RAM.  cuda 9.0 and cuDNN v6.0 for cuda 8.0 are installed.  there is a issue when running the demo.py. when running the code below,kernel has died and have to restart.  `# Run detection  results = model.detect([image], verbose=1)`  there are some error information  ` could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED`  Is cuDNN's version error or Graphics memory small ?  could you help me,3Q  "
"Hi,    I got error below for the generator when running train_shapes Notebook. I am using Python 3.6.2, Tensorflow 1.4.0 and Keras 2.0.9.  Please advice. Thanks a lot!    StopIteration                             Traceback (most recent call last)    in  ()        6             learning_rate=config.LEARNING_RATE,        7             epochs=1,  ----> 8             layers='heads')    H:\My Documents\AlgoResearch\Kaggle\DataScienceBowl2018\Mask_RCNN-master\model.py in train(self, train_dataset, val_dataset, learning_rate, epochs, layers)     2236             max_queue_size=100,     2237             workers=workers,  -> 2238             use_multiprocessing=True,     2239         )     2240         self.epoch = max(self.epoch, epochs)    H:\MyPrograms\Anaconda3\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)       85                 warnings.warn('Update your `' + object_name +       86                               '` call to the Keras 2 API: ' + signature, stacklevel=2)  ---> 87             return func(*args, **kwargs)       88         wrapper._original_function = func       89         return wrapper    H:\MyPrograms\Anaconda3\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)     2044                 batch_index = 0     2045                 while steps_done   2046                     generator_output = next(output_generator)     2047      2048                     if not hasattr(generator_output, '__len__'):    H:\MyPrograms\Anaconda3\lib\site-packages\keras\utils\data_utils.py in get(self)      657                 all_finished = all([not thread.is_alive() for thread in self._threads])      658                 if all_finished and self.queue.empty():  --> 659                     raise StopIteration()      660                 else:      661                     time.sleep(self.wait_time)    StopIteration:   "
"I tried to run the Sloth GUI by using Git Bash / command line with the following command:  ""PYTHON_PATH/Lib/site-packages/sloth/bin/sloth""  or simply building the 'sloth' file. However, the following error was produced  **Traceback (most recent call last):    File ""E:\LiZhiYuan-Working\Segmentation\Sloth-image labeler\sloth\gui\annotationscene.py"", line 247, in onSelectionChangedInTreeView      self.editSelectedItems()    File ""E:\LiZhiYuan-Working\Segmentation\Sloth-image labeler\sloth\gui\annotationscene.py"", line 253, in editSelectedItems      self._labeltool.propertyeditor().startEditMode(items)  AttributeError: 'NoneType' object has no attribute 'startEditMode'**  Can I get some help with this. Thank you."
"In ROIAlign layer, the   for choosing fpn level seems incorrect, comparing with the equation in original paper without the ""/sqrt(image_area)"" item."
"<img width=""1127"" alt=""screen shot 2018-02-13 at 8 05 14 am"" src=""     The image shows an example of jumping validation loss (especially rpn loss) when the training goes to the next stage. (I was fine-tuning on the provided coco weights)    I use following config to stage the training (notice that I didn't change the learning rates or layers to freeze):         Since I saw that you are staging the training for your coco weights, did you observe the same jumping of validation loss when your training transit from one stage to the other?    Please help.    Thanks "
I wonder the way to know every MRCNN result came from which layer of feature map.    Do I need to modify the model.py to label every RoI in the RPN or it can be found somewhere in the code?    I want to use the advantage of FPN to input the BEST feature map into LSTM behind MRCNN.    This has troubled me for a long time.     Many thanks for any help!
Ran it on GPU with 12GB RAM and got OutOfMemory Error.  Now will try on 16GB RAM GPU. How much RAM on GPU does this library require?  (eg. train_shapes.ipynb)
None
"in the function data_generator, there yield inputs and outputs as below. But it's difficult for me to understand what's the use of the outputs.  in function data_generator:           and in the mrcnn build function:      so why do we generate outputs?   Thanks."
"I met the problem described in title when I tried to train this model on my custom dataset. The **gt_class_ids** is a python list representing the class ids of the objects as I print it out. For a regular python list, this argument(**_if not np.any(arbitray_list > 0 )**_) should not pass since it is comparing a list to a int. However,  the demo  '**train_shapes_.ipynb**' works just fine with the printed **gt_class_ids** as list such as [3,2,2]. I wonder why it works for the demo but not my custom dataset.     The error message looks like this:  `ERROR:root:Error processing image {'source': 'xxx', 'path': 'xxx', 'masks': array([[[0, 0, 0, ..., 0, 0, 0],], dtype=uint8), 'id': 'xxx'}    Traceback (most recent call last):    File ""/home/jeffreytan/Mask_RCNN/model.py"", line 1610, in data_generator      if not np.any(gt_class_ids > 0 ):  TypeError: unorderable types: list() > int()`    `---------------------------------------------------------------------------  TypeError                                 Traceback (most recent call last)    in  ()        2             learning_rate=config.LEARNING_RATE,        3             epochs=100,  ----> 4             layers='heads')    ~/Mask_RCNN/model.py in train(self, train_dataset, val_dataset, learning_rate, epochs, layers)     2205             steps_per_epoch=self.config.STEPS_PER_EPOCH,     2206             callbacks=callbacks,  -> 2207             validation_data=next(val_generator),     2208             validation_steps=self.config.VALIDATION_STEPS,     2209             max_queue_size=100,    ~/Mask_RCNN/model.py in data_generator(dataset, config, shuffle, augment, random_rois, batch_size, detection_targets)     1608             # have any of the classes we care about.     1609             print(gt_class_ids)  -> 1610             if not np.any(gt_class_ids > 0 ):     1611                 continue     1612     TypeError: unorderable types: list() > int()`  "
"Hi,    I am working on a task, where each image contains **120** objects at most. How should I set the following parameters? I believe their values are correlated    Thank you!    > RPN_TRAIN_ANCHORS_PER_IMAGE  >   > TRAIN_ROIS_PER_IMAGE  >   > MAX_GT_INSTANCES = 100  >   > DETECTION_MAX_INSTANCES"
I want to get rough segmentation for new labels basically.
"Hello, I found that your implementation uses Batch Norm layer. The training BN layer often takes some advantage that requires a larger batch size. I am running your code on TitanX (12GB) and I can run with a batch size of `8`, the image size of `256x256` by changing `IMAGES_PER_GPU = 8`. Do you think that the changing batch size to 8 will have better performance? Now, I saw that it takes more 10 times consuming than training with the batch size of 1. In my opinion, we will have no benefit because your code did not train the BN. Is it right if I just set `training=True` to train the BN?   "
"class ProposalLayer(KE.Layer):      """"""Receives anchor scores and selects a subset to pass as proposals      to the second stage. Filtering is done based on anchor scores and      non-max suppression to remove overlaps. It also applies bounding      box **_refinment detals_** to anchors.    -------------------------------------------------------    class PyramidROIAlign(KE.Layer):      """"""Implements ROI Pooling on multiple levels of the feature pyramid.      Params:      - pool_shape: [height, width] of the output pooled regions. Usually [7, 7]      - image_shape: [height, width, _**chanells**_]. Shape of input image in pixels"
"I'm trying to run `model.detect` on a numpy array of shape `(10, 720, 1280, 3)`, where the shape is `( no of frames, height, width, rgb )`. However, when I run it, I get the error       The code I used is as follows:     "
"In the inference phase, you resize the testing image into fixed dimensions as       I am wonder that why do we need fixed size in inference phase? Does it use fully connected layer in the network? Because I trained my image with size of `128x128` for saving memory, while my testing image is `512x1024`"
I check image preprocessing steps in model.py and only found mean subtraction. Do we need to further divide the image by its std?   I also checked the keras imagenet pretrained model. For tensorflow model the normalization steps are divide by 127.5 and subtract 1 for each channel. So do we need to maintain the same preprocessing steps when we use imagenet pretrained model?    In model.py   line 2504: function mold_image  line 2512: function unmold_image
"Hello all, I want to ask someone who has succeeded to apply random cropping in data augmentation. Does it help to increase the performance accuracy? Could you tell me some tips to implement it? I known that random cropping is useful in segmentation task"
"Has anyone experienced any issue with Mask_RCNN 2.0 on Python 3.6? I have the following issue on 3.6 (on Google Colab) but not on 3.5 (Ubunut 16.04 LTS stock), from the train_shapes.ipynb example. I notice that is an issue similar to this reported already, but there is no pull requested and I don't touch the code base unless there is no work around.     /content/model.py in detection_targets_graph(proposals, gt_class_ids, gt_boxes, gt_masks, config)      529     positive_count = tf.shape(positive_indices)[0]      530     # Negative ROIs. Add enough to maintain positive:negative ratio.  --> 531     r = 1.0 / config.ROI_POSITIVE_RATIO      532     negative_count = tf.cast(r * tf.cast(positive_count, tf.float32), tf.int32) - positive_count      533     negative_indices = tf.random_shuffle(negative_indices)[:negative_count]    TypeError: unsupported operand type(s) for /: 'Tensor' and 'float'"
"I am reading the code of this implementation and I found the number of anchors is much different from paper.  For example, in train_shapes.ipynb, I have a image which has three objects. I get the `inputs` of the image from `data_generator`.    `inputs[2]` is `batch_rpn_match`,  the shape of `inputs[2]` is `(1, 4092, 1)`:     Which means only three anchors and the rest of lines are all zeros, this is the same with `inputs[3]`, which is `batch_rpn_bbox`     In official paper *Mask R-CNN*, it says there are 512 anchors for FPN net with a ration of 1:3 of positive to negatives. In my opinion, this is a huge difference, I wonder how this implementation affects the performance.  "
"Hi   I want to train this mask rcnn on coco2017, but there are no valminusminival nor minival"
"Hi,     Could you explain why we need (224.0 / tf.sqrt(image_area)), instead of just using 224.0 at the following line?    roi_level = log2_graph(tf.sqrt(h * w) / (224.0 / tf.sqrt(image_area)))  // model.py line 364    !     When I see the equation in FPN paper, I do not see tf.sqrt(image_area) part.     Thank you very much."
"Is there a faster way to obtain the outputs of the RPN, at inference time, then model.run_graph?     `detections = model.detect([image], verbose=1)` runs in 196ms (on a 256x256 image), whereas    `proposals = model.run_graph([image], [(""proposals"", mask_rcnn.keras_model.get_layer(""ROI"").output)])` runs in 447ms.    We should be able to retrieve the outputs of the proposals layer at least as fast as the last layer of the network. Any insight on how to do this?"
"Hi,    I've trying to replace the ResNet 101 used as backbone with other architectures (e.g. VGG16, Inception V3, ResNeXt 101 or Inception ResNet V2) in order to check whether the results improve or not.    The problem is that, whenever I substitute the ResNet with any other architecture, the training losses of the mask branch are NaN or zero:    `loss: nan - rpn_class_loss: 0.6948 - rpn_bbox_loss: 0.3827 - mrcnn_class_loss: nan - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: nan - val_rpn_class_loss: 0.6931 - val_rpn_bbox_loss: 0.2744 - val_mrcnn_class_loss: nan - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00`    These are the implementations I have been using:    - Inception ResNet V2  -      - Inception V3  -      - ResNeXt 101  -      - VGG16  -      I do not get any errors during execution. Any suggestions?"
"Can we apply Mask_RCNN to detect objects in Depth images ? if not, is there a way to extend the algorithm to do this. thank you. "
"   PyramidROIAlign return a tensor whose shape is (1, ?, 7, 7, 256), but it's strange that after TimeDistributed operation, the tensor shape is (?, 200, 1, 1, 256), why it's happen? it's so confused to me.    The config I use is the default configuration in config.py"
"I am using the train of shapes code and uploaded my dataset with images of size 960x960 and there are only two classes background and water. The code works fine when using 1 GPU but anything beyond 1 GPU i get a broken pipe.     5/100 [>.............................] - ETA: 19:27 - loss: 3.0204 - rpn_class_loss: 0.0079 - rpn_bbox_loss: 0.8945 - mrcnn_class_loss: 0.0953 - mrcnn_bbox_loss: 1.1790 - mrcnn_mask_loss: 0.8437Traceback (most recent call last):            File ""/usr/local/lib/python3.5/dist-packages/keras/utils/data_utils.py"", line 655, in _data_generator_task                self.queue.put((True, generator_output))                                                                              File "" "", line 2, in put                                                                                         File ""/usr/lib/python3.5/multiprocessing/managers.py"", line 716, in _callmethod                                           conn.send((self._id, methodname, args, kwds))                                                                         File ""/usr/lib/python3.5/multiprocessing/connection.py"", line 206, in send                                                self._send_bytes(ForkingPickler.dumps(obj))                                                                           File ""/usr/lib/python3.5/multiprocessing/connection.py"", line 397, in _send_bytes                                         self._send(header)                                                                                                    File ""/usr/lib/python3.5/multiprocessing/connection.py"", line 368, in _send                                               n = write(self._handle, buf)                                                                                        BrokenPipeError: [Errno 32] Broken pipe                                                                                 Process Process-2:                                                                                                      BrokenPipeError: [Errno 32] Broken pipe                                                                                                                                                                                                         During handling of the above exception, another exception occurred:                                                                                                                                                                             Traceback (most recent call last):                                                                                        File ""/usr/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap                                             self.run()                                                                                                            File ""/usr/lib/python3.5/multiprocessing/process.py"", line 93, in run                                                     self._target(*self._args, **self._kwargs)                                                                             File ""/usr/local/lib/python3.5/dist-packages/keras/utils/data_utils.py"", line 665, in _data_generator_task                self.queue.put((False, e))                                                                                            File "" "", line 2, in put                                                                                         File ""/usr/lib/python3.5/multiprocessing/managers.py"", line 716, in _callmethod                                           conn.send((self._id, methodname, args, kwds))                                                                         File ""/usr/lib/python3.5/multiprocessing/connection.py"", line 206, in send                                                self._send_bytes(ForkingPickler.dumps(obj))                                                                           File ""/usr/lib/python3.5/multiprocessing/connection.py"", line 404, in _send_bytes                                         self._send(header + buf)                                                                                              File ""/usr/lib/python3.5/multiprocessing/connection.py"", line 368, in _send                                             6/100 [>.............................] - ETA: 18:50 - loss: 2.9845 - rpn_class_loss: 0.0074 - rpn_bbox_loss: 0.8183 - mrcnn_class_loss: 0.0954 - mrcnn_bbox_loss: 1.2383 - mrcnn_mask_loss: 0.8251Traceback (most recent call last):            File ""mask_test.py"", line 282, in                                                                                  layers='heads')                                                                                                       File ""/mnt/belal/Mask_RCNN/model.py"", line 2168, in train                                                                 use_multiprocessing=True,                                                                                             File ""/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py"", line 91, in wrapper                             return func(*args, **kwargs)                                                                                          File ""/usr/local/lib/python3.5/dist-packages/keras/engine/training.py"", line 2145, in fit_generator                       generator_output = next(output_generator)                                                                             File ""/usr/local/lib/python3.5/dist-packages/keras/utils/data_utils.py"", line 751, in get                                 if not self.queue.empty():                                                                                            File "" "", line 2, in empty                                                                                       File ""/usr/lib/python3.5/multiprocessing/managers.py"", line 716, in _callmethod                                           conn.send((self._id, methodname, args, kwds))                                                                         File ""/usr/lib/python3.5/multiprocessing/connection.py"", line 206, in send                                                self._send_bytes(ForkingPickler.dumps(obj))                                                                           File ""/usr/lib/python3.5/multiprocessing/connection.py"", line 404, in _send_bytes                                         self._send(header + buf)                                                                                              File ""/usr/lib/python3.5/multiprocessing/connection.py"", line 368, in _send                                               n = write(self._handle, buf)                                                                                        BrokenPipeError: [Errno 32] Broken pipe"
"Before giving a question, thank you for all of you who spent time on this big project.    I have two questions.  1. For me, evaluation results of segmentation does not come up. Let me upload a screenshot after type ..  `python3 coco.py evaluate --dataset=... --model=weight/mask_rcnn_coco.h5 --year=2017 `  !     2. I check the result with the paper ] but those are ... very different. I am not sure how do I treat this problem. ...   !   "
"What do I have to modify for the inference to run on GPU ?  In config.py , it's said GPU_COUNT=1, but according to the comment, setting GPU_COUNT to 1 is for CPU usage. So what do I have to do if I want to use one GPU to infer ?"
"I want to know the size of bounding box, such as height.. Thank U!"
"I just solved this problem so let me share the solution.    First the error comment is below.    > Traceback (most recent call last):  >   File ""coco.py"", line 493, in    >     layers='heads')  >   File ""/home/jaesungchoe/Dropbox/0_iDeaFactory/Mask_RCNN/model.py"", line 2189, in train  >     self.set_trainable(layers)  >   File ""/home/jaesungchoe/Dropbox/0_iDeaFactory/Mask_RCNN/model.py"", line 2093, in set_trainable  >     trainable = bool(re.fullmatch(layer_regex, layer.name))  > AttributeError: 'module' object has no attribute 'fullmatch'    I am not sure but there is a problem with  > but not with  >  ( Since I cannot solve the problem with _Py_ZeroStruct #196 , I run the code in python2.7 not python3.x )    You just can change  > into  >   For me, now it works well.      "
"Hi, thank you very much for your really awesome repo! I wanted to know how well your pretrained model does wrt to the actual accuracies obtained by the authors. Thanks again!"
"I am not sure. This problem is on my computer or the code.     After I run the code below,  `python3 coco.py train --dataset=/media/coco --model=coco`    >> Errors come up.    Traceback (most recent call last):    File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 16, in        from . import multiarray  ImportError: /usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so: undefined symbol: _Py_ZeroStruct    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""coco.py"", line 32, in        import numpy as np    File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in        from . import add_newdocs    File ""/usr/local/lib/python2.7/dist-packages/numpy/add_newdocs.py"", line 13, in        from numpy.lib import add_newdoc    File ""/usr/local/lib/python2.7/dist-packages/numpy/lib/__init__.py"", line 8, in        from .type_check import *    File ""/usr/local/lib/python2.7/dist-packages/numpy/lib/type_check.py"", line 11, in        import numpy.core.numeric as _nx    File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 26, in        raise ImportError(msg)  ImportError:   Importing the multiarray numpy extension module failed.  Most likely you are trying to import a failed build of numpy. If you're working with a numpy git repo, try `git clean -xdf` (removes all  files not under version control).  Otherwise reinstall numpy.    Original error was: /usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so: undefined symbol: _Py_ZeroStruct      "
StopIteration happens on the first step of epoch 1. What could be the reason?    !   
"I'm using   with the following modification for resolution for creating generic images like        class ShapesConfig(Config):         ...         IMAGES_PER_GPU = 1 #8           ...           # Use small images for faster training. Set the limits of the small side         # the large side, and that determines the image shape.         IMAGE_MIN_DIM = 256         IMAGE_MAX_DIM = 512           # Use smaller anchors because our image and objects are small         RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)  # anchor side in pixels         ...    and get the following error during training        ERROR:root:Error processing image {'source': 'shapes', 'path': None, 'shapes': [('circle', (29, 31, 44), (169, 63, 88)), ('triangle', (84, 87, 146), (34, 191, 29)), ('square', (251, 147, 186), (78, 236, 120)), ('circle', (208, 46, 20), (368, 136, 112))], 'height': 512, 'width': 512, 'id': 146, 'bg_color': array([ 37, 247, 114])}    Any ideas what I'm overlooking? "
"I am running into memory issues running the demo notebook on Nvidia Jetson TX1 (4GB RAM). I switched to Resnet50 (model.py) and played around with IMAGE_MIN_DIM and IMAGE_MAX_DIM (config.py), but am still running into resource issues. What is the minimum amount of RAM required to run Mask_RCNN???    Error:  InternalError: TopKOp: Could not launch cub::DeviceSegmentedRadixSort::SortPairsDescending to sort input, temp_storage_bytes: 1279, status: too many resources requested for launch     ]]    \ ]]  "
"I want to modified the structure of the backbone of resnet, so I can't load the pretrained coco model directly. Could you give me some advice on how to train a different backbone model utilizing the pretrained model or training from scratch? Thank you!"
"Hi,    I'm trying to train my own dataset. It has a different number of classes and it always fails at **model.load_weights(model_path, by_name=True)** (coco.py)  I train it like:  python3 own.py train --dataset=/own_dataset/ --model=coco    If I train it with ""--model=imagenet"" it's working!    The error message is:  `tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimension 1 in both shapes must be equal, but are 244 and 324 for 'Assign_682' (op: 'Assign') with input shapes:   [1024,244], [1024,324].  `  Any ideas?  "
"I tried to train on MS COCO dataset, using the command provided in the readme  python3 coco.py train --dataset=/path/to/coco/ --model=imagenet  failed due to insufficient GPU memory, so I wonder how much is GPU memory is needed to run the above command, without changing the default training setup.  "
"Hi,    I am trying to train for coco dataset.  The command I am using is: python3 coco.py train --dataset=/home/mask-rcnn/Mask_RCNN-master/dataset-coco --model=coco --download=false --year=2017    The dataset is already downloaded before. I am getting ""FileNotFoundError: [Errno 2] No such file or directory: '/home/mask-rcnn/Mask_RCNN-master/dataset-coco/annotations/instances_valminusminival2017.json' ""    Can anyone please help? TIA. Please see the logs below.     Using TensorFlow backend.  Command:  train  Model:  coco  Dataset:  /home/mask-rcnn/Mask_RCNN-master/dataset-coco  Year:  2017  Logs:  /home/mask-rcnn/Mask_RCNN-master/logs  Auto Download:  True    Configurations:  BACKBONE_SHAPES                [[256 256]   [128 128]   [ 64  64]   [ 32  32]   [ 16  16]]  BACKBONE_STRIDES               [4, 8, 16, 32, 64]  BATCH_SIZE                     2  BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]  DETECTION_MAX_INSTANCES        100  DETECTION_MIN_CONFIDENCE       0.7  DETECTION_NMS_THRESHOLD        0.3  GPU_COUNT                      1  IMAGES_PER_GPU                 2  IMAGE_MAX_DIM                  1024  IMAGE_MIN_DIM                  800  IMAGE_PADDING                  True  IMAGE_SHAPE                    [1024 1024    3]  LEARNING_MOMENTUM              0.9  LEARNING_RATE                  0.001  MASK_POOL_SIZE                 14  MASK_SHAPE                     [28, 28]  MAX_GT_INSTANCES               100  MEAN_PIXEL                     [123.7 116.8 103.9]  MINI_MASK_SHAPE                (56, 56)  NAME                           coco  NUM_CLASSES                    81  POOL_SIZE                      7  POST_NMS_ROIS_INFERENCE        1000  POST_NMS_ROIS_TRAINING         2000  ROI_POSITIVE_RATIO             0.33  RPN_ANCHOR_RATIOS              [0.5, 1, 2]  RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)  RPN_ANCHOR_STRIDE              1  RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]  RPN_NMS_THRESHOLD              0.7  RPN_TRAIN_ANCHORS_PER_IMAGE    256  STEPS_PER_EPOCH                1000  TRAIN_ROIS_PER_IMAGE           200  USE_MINI_MASK                  True  USE_RPN_ROIS                   True  VALIDATION_STEPS               50  WEIGHT_DECAY                   0.0001      Loading weights  /home/mask-rcnn/Mask_RCNN-master/mask_rcnn_coco.h5  2018-01-11 17:09:33.070611: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA  2018-01-11 17:09:33.262017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:   name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076  pciBusID: 0000:01:00.0  totalMemory: 11.91GiB freeMemory: 11.47GiB  2018-01-11 17:09:33.262044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)  Will use images in /home/mask-rcnn/Mask_RCNN-master/dataset-coco/train2017  Will use annotations in /home/mask-rcnn/Mask_RCNN-master/dataset-coco/annotations/instances_train2017.json  loading annotations into memory...  Done (t=12.45s)  creating index...  index created!  Will use images in /home/mask-rcnn/Mask_RCNN-master/dataset-coco/val2017  Will use annotations in /home/mask-rcnn/Mask_RCNN-master/dataset-coco/annotations/instances_valminusminival2014.json  loading annotations into memory...  Traceback (most recent call last):    File ""coco.py"", line 477, in        dataset_train.load_coco(args.dataset, ""valminusminival"", year=args.year, auto_download=args.download)    File ""coco.py"", line 108, in load_coco      coco = COCO(""{}/annotations/instances_{}{}.json"".format(dataset_dir, subset, year))    File ""/usr/local/lib/python3.5/dist-packages/pycocotools/coco.py"", line 79, in __init__      dataset = json.load(open(annotation_file, 'r'))  FileNotFoundError: [Errno 2] No such file or directory: '/home/mask-rcnn/Mask_RCNN-master/dataset-coco/annotations/instances_valminusminival2017.json'          "
"Thanks for your sharing, and I have a question about Resnet Architecture.   In Caffe prototxt, the conv_block is behind of the identity_block, which is different from your codes, so  could this affect the results?"
hi I want to use checkpoints I want that if the training stops in between if I start training then it will start from last checkpoint point any suggestions?
"This one thing keeps baffling me: if you use FCN for each RoI, it returns the binary mask by argmax'ing each pixel (like FCN in Long, Shelhamer (2015) for example). How does MaskRCNN learns to return exactly one mask in each bounding box? Are non-background pixles that don't belong to the largest mask suppressed in some way? "
"Can anyone tell me how I can set the path to train Mask-RCNN :  parser = argparse.ArgumentParser(  description='Train Mask R-CNN on MS COCO.')  parser.add_argument('-command',  metavar='train',  help=""'train' or 'evaluate' on MS COCO"")  parser.add_argument('-model',  metavar='COCO_DIR',  help='Directory of the MS-COCO dataset')  parser.add_argument('-dataset',  metavar='COCO_DIR/mask_rcnn_coco.h5',  help=""Path to weights .h5 file or 'coco'"")  args = parser.parse_args()  print(""Command: "", args.command)  print(""Model: "", args.model)  print(""Dataset: "", args.dataset)    It outputs : Command : None  Model : None  Dataset: None"
"Is BACKBONE_STRIDES the feature map size in each stage?  If I change the image size from 128\*128 to 512\*512, should I change the value of BACKBONE_STRIDES?  Thanks!    "
"Hi,    When I run trainning, with 10 steps per epoch, and max_gt_instances = 100 ,on 2 GPUs,  I get Memory error. Do you have an idea where is the problem?   If max_gt_instances = 20, it is working.     (I have 1 class and more than 100 objects in one image from the same class. My masks are numpy arrays from the shape (height, width , no of instances) )     Thank you,  Liron      File ""/home/liron/.local/lib/python3.5/site-packages/keras/utils/data_utils.py"", line 636, in data_generator_task      self.queue.put((True, generator_output))    File "" "", line 2, in put    File ""/usr/lib/python3.5/multiprocessing/managers.py"", line 716, in _callmethod      conn.send((self._id, methodname, args, kwds))    File ""/usr/lib/python3.5/multiprocessing/connection.py"", line 206, in send      self._send_bytes(ForkingPickler.dumps(obj))    File ""/usr/lib/python3.5/multiprocessing/reduction.py"", line 50, in dumps      cls(buf, protocol).dump(obj)  **MemoryError**      File ""my_data.py"", line 344, in        layers='heads')    File ""Mask_RCNN-master/model.py"", line 2095, in train      **fit_kwargs    File ""/home/liron/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py"", line 87, in wrapper      return func(*args, **kwargs)    File ""/home/liron/.local/lib/python3.5/site-packages/keras/engine/training.py"", line 2115, in fit_generator      generator_output = next(output_generator)    File ""/home/liron/.local/lib/python3.5/site-packages/keras/utils/data_utils.py"", line 735, in get      six.reraise(value.__class__, value, value.__traceback__)    File ""/usr/local/lib/python3.5/dist-packages/six.py"", line 693, in reraise      raise value  **MemoryError**  "
"can't install it on the windows, anybody have success?  when i excute ""python setup.py build_ext install"", it reports ""error D8021： invalid parameter/Wno-cpp"""
"How could I debug this model, say if I would like to get some intermediate values in PyramidROIAlign() func, seems like there's no sess.run(some_node) in the training process in keras    "
"Hi,    I try to run demo on TX2, but it comes out with error:       Does it mean the demo ran out of memory? I try to reduce the size of image but the error is still there.    Any idea? Thanks a lot!"
"Hi,     I tried to find out how exactly does the backbone architecture looks like, but I didn't understand it well from the `model.py`.     I understand that there is ResNet-101 with 101 layers divided to 4 stages used in the version from   (ResNet-50 if modified) and then it is somehow pushed into a function to make pyramids.      Can I ask about number of layers and pyramid levels? Each level of pyramid should be based on a stage of the ResNet except of the first one, so we get P2-P5, but I can see that there is also P6 in the script. How can we get this sixth level of pyramids from the five-stages 101 layers? Is it just some subsample of P5? And why isn't it used also for the mask branch?     Thank you in advance. "
"When i set init_with=""last"" ,the machine didn't do anything!But init_with=""coco"" work well."
" Hi @waleedka ,  Thanks for the impressive work.  I want to train the limited categories of coco other than the all 80 categories with the weight you trained on COCO as initial weight (mask_rcnn_coco.h5), what should I do?"
"Hi, @waleedka     I'm trying to fine-tune imagenet pre-trained model to coco task(detection and segmentation) with your code.  I already tried sample train, and to get more precise model, I start with my own pre-trained model(exactly resnet-101).  But I got nan at rpn bbox loss. the other loss seems not that bad,    what can be suspect of this phenomenon?  anything is okay, please give me advice.    Thank you."
"when I run the demo.ipynb in juptyer notebook, the error is  ""No module named request"" when import import urllib.request in coco.py"
"In the coco.py, I set the GPU_Count = 1. I got the following message when I run the program.     name: GeForce GTX TITAN X  major: 5 minor: 2 memoryClockRate (GHz) 1.076  pciBusID 0000:02:00.0  Total memory: 11.92GiB  Free memory: 11.60GiB  2017-12-26 09:52:42.406861: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x7f86a7909660 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.  2017-12-26 09:52:42.407910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties:   name: GeForce GTX TITAN X  major: 5 minor: 2 memoryClockRate (GHz) 1.076  pciBusID 0000:03:00.0  Total memory: 11.92GiB  Free memory: 11.80GiB  2017-12-26 09:52:42.638625: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x7f86a790db80 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.  2017-12-26 09:52:42.640497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 2 with properties:   name: GeForce GTX TITAN X  major: 5 minor: 2 memoryClockRate (GHz) 1.076  pciBusID 0000:82:00.0  Total memory: 11.92GiB  Free memory: 11.80GiB  2017-12-26 09:52:42.891295: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x7f86a79120a0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.  2017-12-26 09:52:42.892341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 3 with properties:   name: GeForce GTX TITAN X  major: 5 minor: 2 memoryClockRate (GHz) 1.076  pciBusID 0000:83:00.0  Total memory: 11.92GiB  Free memory: 11.80GiB    what does ""A non-primary context 0x7f86a7909660 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that."" mean?"
Is it possible to pass mutiple gt boxes for single class as input for train?
"I ran the evaluation on minival 5K based v2 model you provided, the mask AP is 0.233, which seems a bit low compared to the original paper, is this a normal number? "
None
None
"Hi, thanks for sharing your code.  You mentioned that you clipped the gradient in REAME.md  How much did you clip the gradient? I cannot find that in your code"
"Hi @waleedka,     Thanks for the impressive work.    I want to train the model using my own dataset (only two classes) with the weight you trained on COCO as initial weight (mask_rcnn_coco.h5). Basically, I added following code:    exclude_list = {'mrcnn_bbox_fc', 'mrcnn_mask_deconv', 'mrcnn_class_logits', 'mrcnn_mask'}  model.load_weights(model_path, by_name=True, exclude=exclude_list)    Is this correct, or should I add more layers into the ""exclude_list"". Thanks."
"Hi, I ran demo.ipynb using jupyter notebook and got error as following:    File ""coco.py"", line 340      config.print()                  ^  SyntaxError: invalid syntax"
"Hi matterport,    When I try to  use this Mask_RCNN source code to do  image segmentation for image size 1000*600, I found its performance is not good and about 1.5s per frame. It seems the python code of mask rcnn is not optimized by GPU.    I wonder if it could be optimized?  Matterport will be plan to optimize it?    Thanks.  "
"Hi,     I am trying to train the network on my own data, but I have a problem - my GPU fails (resource exhausted) when training 3+ layers on the image size which is still okay for the 4+ layers.     Is it possible to change config parameters during the process? Between 3+ and 4+ trainings?     I have tried something like    `CocoConfig.IMAGE_MAX_DIM = 704`    `CocoConfig.IMAGE_MIN_DIM = 700`    It didn't work. Neither did  `model.config.IMAGE_MAX_DIM = 704`    `model.config.IMAGE_MIN_DIM = 700`    I still got the error message saying that tensor [None, 1024, 1024, 3] was expected.     Thank you. "
"I want to train the model with only the bbox input, since my own dataset doesn't have segmentation labels.What should I do with the source code? (I have simply deleted the layers about segmentation, but then I was told that the pretrained weights can't be loaded) @waleedka ,Thanks~"
"First of all, thank you for sharing with the community this amazing repository!    I'm experimenting with the Mask R-CNN methodology on a dataset of greyscale images and couldn't help noticing that only RGB images are supported.  While the approach suggested in the default `load_image()` method of the `Dataset` class does the trick (i.e. converting from greyscale to RGB), it appears to me that repeating 3 times the same data would result in underutilizing GPU memory.  I played a bit with the `Dataset` and `Config` classes to be able to feed single-channel images, however I received errors from `model.py`, which made me realize that the 3-channel requirement might be hard-coded at a deeper level.    I was wondering if there are any plans on adding support for images with an arbitrary number of channels.  Thanks in advance!"
"This is a good implementation for Mask_RCNN. Thanks for that. To intuitive understand MaskRCNN, I tried to visualize the model's architecture. However, I failed.     The details are as follows:  In coco.py       And I got this error       I know MaskRCNN defines its weights using callable object, like x = KL.Activation('relu')(x). In this way, model doesn't have layers member. But I think there should be a way to visualize the model. Any ideas? Thanks.  "
"Hi,    First of all, thank you for this fantastic implementation about Mask R-CNN !    Just a quick question, which function calculates the Mask IoU ? I checked compute_ap in utils.py, but it seems not the function I am looking for. I believe this function is required to calculate segmentation AP.    Many thanks !"
"I download file from   Then I placed the file in Mask_RCNN-master, went to the coco-master/PythonAPI and run make.     BUT, when i run the demo.ipynb, there is an error said that "" ModuleNotFoundError: No module named 'pycocotools'."". is there anyone know why it is happened?     I also tried the replace python with python3 in Makefile."
"Hi! I have doubts that mask prediction and bbox prediction are done in paralle in Figure3 in paper while mask prediction is followed by bbox prediction in the code, right? "
"Hi,    Thanks for your project, I am training on my dataset and have some issues:  image of dataset is not fix size so mask is not fix size too. when I put it to form of you at line               self.add_image(""shapes"", image_id=id, path=imgs[id],                             width=width, height=height)  Do I need change to real size of image    Thanks"
"Hi,  Thanks a lot for this great implementation. I had some doubts on how you implemented the ROI ALign Layer.  Specifially this routine in :PyramidROIAlign :    !   !       I read the paper and understood how Kaiming interpolated using the 4 corner points.But I have trouble understanding how you have implemented it. Why is there a need of log2(in yellow box1)? I completely do not get the second yellow box? Do you mind explaining it with an example? Given an example image(800x800x3)  a single feature pyramid  map (1x256x100x100 : so 1/8th) and a single bounding box (x1:22,y1:22,x2:50,y2:50) how do i understand the interpolation?       Thanks a lot!"
"Really impressive work!   I've read the code in coco.py and train_shapes.ipynb, these examples are all doing on  datasets containing masks. How should I apply this model  to object location (bbox regression only) problems? What the format should my own datasets use?   Thanks a lot!"
"Hi ,  I'm a newer, using mask-rcnn for testing recently,  first of all, thank you for your contribution!  My final purpose is running it in andriod application, but I run it for a pitcure with a few time, it is obviously not feasible.    questions;  1. Can model used in actual project? If can, and how about that?(sketchy)  2. What are the special requirements for hardware?    Thank you!"
"This is not really an issue. Furthermore, I know this is not an official implementation and that neither of you is the author of the Mask R-CNN paper but I think I prefer to have a public discussion about this instead of correspond with the authors. So here goes...    Any ideas why the final output (mrcnn_mask) includes every class in it, resulting in a shape of `(None, 100, 28, 28, 81)`? Technically each of the 100 detections corresponds to a single object and the class probabilities for each detection are already known. Does this really give any noteworthy advantage compared to a simpler solution like a binary value for fg/bg (conditioned on the detected class) shaped `(None, 100, 28, 28)` and is it worth the additional memory requirements?    Thanks in advance"
I found your resnet units was conv-bn-relu structure  why dont you use bn-relu-conv?  Does the second one have some disadvantage？  Thanks  
"Thanks for your great work. I train maskrcnn on my own dataset, and it worked. but in the half of the training, it throws the warning as below:    /home/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""  /home/renren/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:2057: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.    UserWarning('Using a generator with `use_multiprocessing=True`'  /home/anaconda3/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:600: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.    ""the returned array has changed."", UserWarning)  /home/anaconda3/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:600: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.    ""the returned array has changed."", UserWarning)"
"@waleedka  Thanks very much for your great work. I am using your code to train mask rcnn on my own dataset that has only one class, say 'basketball' and background. Is it right to train using only images with 'basketball' instead using background images? Really appreciate your kind help."
Any function test my dataset mAP for detection and segmentation?
I am wondering why TRAIN_ROIS_PER_IMAGE affects GPU memory to be used a lot? And what's the difference or corelation between POST_NMS_ROIS_TRAINING and TRAIN_ROIS_PER_IMAGE?  Thanks a lot!
Does the Mask RCNN requires the per-pixel segmentation as the training data?    I have some object detection results (a rectangle for a object). Can I use those label as the training data?    Thank you for kindly answers.    Best wishes!  Qiang
"I was trying to run the coco.py.  In the coco.py, I saw ''cococonfig'' to set GPU_count.  Now, I wanna ask for help about   """"How to set some GPUs for training""""    In the server of our research group, anyone of us has several fixed GPUs.    So, I need to set the GPU number for training to avoid using other's GPU.  What should I do?    Thanks for answering."
"Thank you for your generous work on implementing Mask R-CNN.  when I run   # Train a new model starting from pre-trained COCO weights      python3 coco.py train --dataset=/path/to/coco/ --model=coco  there is something wrong look like this:   Traceback (most recent call last):    File ""coco.py"", line 40, in        from pycocotools.coco import COCO    File ""/mnt/Mask_RCNN-master/pycocotools/coco.py"", line 55, in        from . import mask as maskUtils    File ""/mnt/Mask_RCNN-master/pycocotools/mask.py"", line 3, in        import pycocotools._mask as _mask  ImportError: dynamic module does not define init function (PyInit__mask)    thank you very much.  "
"Thank you for your wonderful work on implementing Mask R-CNN. I have succeeded in training and testing on the COCO 2014 dataset.   Now, how can I train a model on a subset of COCO, e.g., I only want to train a model that only does segmentation on ""person"" and ""dog""?"
"Hello,    I would like to try this great implementation out on my own dataset and was wondering what the requirements for such a set are.    My set contains only a per-pixel class segmentation as ground truth, hence not instance bounding boxes or instance segmentations.    Would this mask-rcnn implementation be able to cope?     Best,  Ruud      "
"When I train net on 35kval images, StopIteration happens in the 295th step  of 6th epcho, like below.  295/1000 [=======>......................] - ETA: 11:56 - loss: 1.3895 - rpn_class_loss: 0.0333 -   rpn_bbox_loss: 0.5186 - mrcnn_class_loss: 0.3381 - mrcnn_bbox_loss: 0.2326 - mrcnn_mask_loss: 0.2669  Traceback (most recent call last):  File ""coco.py"", line 397, in    layers='heads')  File ""D:\work\code\matterportMask_RCNN\Mask_RCNN-master\model.py"", line 2183,in train    **fit_kwargs  File ""G:\Anaconda3\lib\site-packages\keras\legacy\interfaces.py"", line 87, in wrapper    return func(*args, **kwargs)  File ""G:\Anaconda3\lib\site-packages\keras\engine\training.py"", line 2083, in fit_generator    generator_output = next(output_generator)  StopIteration  I can't figur out what happened, can you help me? Changes I made in config.py BATCH_SIZE=1,IMAGE_MAX_DIM=512,IMAGE_MIN_DIM=400."
why val_loss is getting higher and higher？   How to choose the optimal model  
"Hi,  I get the below error when I start tensorflow_model_server.   ""Op type not registered 'PyFunc'""  And the only suggestion I get around web is to reimplement pyfunc as tensorflow operations. Do you know of any other ways?  "
"I use the code to train a coco model by the command: ""python3 coco.py train --dataset=/path/to/my/datasets/coco --model=imagenet"". Instead of using the train+val35K datasets, I use only train set of coco for training. However, the result is poor and only has an AP 0.082 after I finished the training. "
"Hi, I really like the demo which makes it super easy to segment some image, but I fail to use it for multiple images at once.  I thought that modifying predicting in `demo.ipynb`      to that:     would result in predicting for two images, but instead, segmentation and bounding boxes from first image are used also for the second one:   !   !   "
"The augment parameter is using random.randint(0,1), which always returns 0. The images are never getting flipped. It should be random.randint(0,2) in modely.py"
"# Run RPN sub-graph  pillar = model.keras_model.get_layer(""ROI"").output  # node to start searching from  rpn = model.run_graph([image], [      (""rpn_class"", model.keras_model.get_layer(""rpn_class"").output),      (""pre_nms_anchors"", model.ancestor(pillar, ""ROI/pre_nms_anchors:0"")),      (""refined_anchors"", model.ancestor(pillar, ""ROI/refined_anchors:0"")),      (""refined_anchors_clipped"", model.ancestor(pillar, ""ROI/refined_anchors_clipped:0"")),      (""post_nms_anchor_ix"", model.ancestor(pillar, ""ROI/rpn_non_max_suppression:0"")),      (""proposals"", model.keras_model.get_layer(""ROI"").output),  ])        ---------------------------------------------------------------------------  AssertionError                            Traceback (most recent call last)    in  ()        7     (""refined_anchors_clipped"", model.ancestor(pillar, ""ROI/refined_anchors_clipped:0"")),        8     (""post_nms_anchor_ix"", model.ancestor(pillar, ""ROI/rpn_non_max_suppression:0"")),  ----> 9     (""proposals"", model.keras_model.get_layer(""ROI"").output),       10 ])    ~\Mask_RCNN\model.py in run_graph(self, images, outputs)     2295         outputs = OrderedDict(outputs)     2296         for o in outputs.values():  -> 2297             assert o is not None     2298      2299         # Build a Keras function to run parts of the computation graph    AssertionError:     "
demo.ipynb  missing  `%matplotlib inline`  for image display in notebook
     next is returning just one item. Why aren't we passing the generator?  I tried to remove the `next` but it returns an error when keras is calling `np.avarage` due to some items having multiple dimensions instead of being scalars.
"Hello,    In prediction  stage, I have these questions:-      1-How can the FCN responsible for predicting masks deal with the different dimensions of the ROI?  and how m×m floating-number mask output is resized to the RoI size?    2- It seems that the classification does not benefit from segmentation at all, as the authors decoupled segmentation  and classification. I assumed better results will be from classification of a segment without any irrelevant pixels. Am i missing something?  May be ""indirectly"" as we try to improve segmentation results we will be forced to improve the detection results as the regions proposals should become more efficient  and this outputted better results than feeding segment to classifier in a cascade manner. After all the losses is the aggregation of masking,classification and detection.    Thanks"
"Hello,    I am trying to run the demo in CPU mode, before switching over the installation to my GPU setup. But I get the following error I cannot place!    Does anyone see this as familiar?    I am running Tensorflow 1.4.0, although it 1.3+ was stated as dependency.    Best,  Ruud         "
I know that ROI-align uses bilinear interpolation  to avoid error in rounding which causes inaccuracies in detection and segmentation     my understanding is that it helps twice:-    1-When the values of features for that region (h*w) are extracted from the feature map resulted from convolution.  2-When max pooling is applied on theses extracted features to produce the fixed W*H grid    Am I correct?
"May I ask a question, maybe silly.   What's the point for RPN_BBOX_STD_DEV and BBOX_STD_DEV in config.py? And where the value come from?"
None
None
Can the code run at windows?
"I want to nkow struct return method load_mask , i have 1 image size 2592x4068 and object with boxdingbox xmin: 279 ymin:723 xmax:2431 ymin2784 . I see masks : A bool array of shape [height, width, instance count] . i was confused . please give me good explain"
"I run with the example code in train_shapes.ipynb. It works fine with triangle and circle. However, it gets wrong mask prediction results on the square category (bounding boxes look OK). The errors seems to be caused by USE_MINI_MASK. "
"I run with the example code in train_shapes.ipynb. It works fine with triangle and circle. However, it gets much worse mask prediction results on the square category (bounding boxes look OK). "
"I am trying to train on 2 classes (including background).    `NUM_CLASSES = 1 + 1   `  The problem is that I can't load the starting coco weights any  more as I get the following error:-    > ValueError: Dimension 1 in both shapes must be equal, but are 8 and 324 for 'Assign_682' (op: 'Assign') with input shapes: [1024,8], [1024,324].    if I change the line` NUM_CLASSES = 80 + 1 ` , loading of weights is successful.    should something else be changed?  Thanks  "
None
"In config.py, TRAIN_ROIS_PER_IMAGE = 128  # TODO: paper uses 512  Is there any problem when considering this setting?"
"While running the demo.ipynb, I get the following error:    ValueError                                Traceback (most recent call last)    in  ()        4         5 # Run detection  ----> 6 results = model.detect([image], verbose=1)        7         8 # Visualize results    ~/Mask_RCNN/model.py in detect(self, images, verbose)     2315             final_rois, final_class_ids, final_scores, final_masks =\     2316                 self.unmold_detections(detections[i], mrcnn_mask[i],  -> 2317                                        image.shape, windows[i])     2318             results.append({     2319                 ""rois"": final_rois,    ~/Mask_RCNN/model.py in unmold_detections(self, detections, mrcnn_mask, image_shape, window)     2275         for i in range(N):     2276             # Convert neural network mask to full size mask  -> 2277             full_mask = utils.unmold_mask(masks[i], boxes[i], image_shape)     2278             full_masks.append(full_mask)     2279         full_masks = np.stack(full_masks, axis=-1)\    ~/Mask_RCNN/utils.py in unmold_mask(mask, bbox, image_shape)      475     y1, x1, y2, x2 = bbox      476     mask = scipy.misc.imresize(  --> 477         mask, (y2 - y1, x2 - x1), interp='bilinear').astype(np.float32) / 255.0      478     mask = np.where(mask >= threshold, 1, 0).astype(np.uint8)      479     ~/virtual_envs/py3_vir_env/lib/python3.5/site-packages/numpy/lib/utils.py in newfunc(*args, **kwds)       99             """"""`arrayrange` is deprecated, use `arange` instead!""""""      100             warnings.warn(depdoc, DeprecationWarning, stacklevel=2)  --> 101             return func(*args, **kwds)      102       103         newfunc = _set_function_name(newfunc, old_name)    ~/virtual_envs/py3_vir_env/lib/python3.5/site-packages/scipy/misc/pilutil.py in imresize(arr, size, interp, mode)      562         size = (size[1], size[0])      563     func = {'nearest': 0, 'lanczos': 1, 'bilinear': 2, 'bicubic': 3, 'cubic': 3}  --> 564     imnew = im.resize(size, resample=func[interp])      565     return fromimage(imnew)      566     ~/virtual_envs/py3_vir_env/lib/python3.5/site-packages/PIL/Image.py in resize(self, size, resample, box)     1743         self.load()     1744   -> 1745         return self._new(self.im.resize(size, resample, box))     1746      1747     def rotate(self, angle, resample=NEAREST, expand=0, center=None,    ValueError: height and width must be > 0  "
"Inside the method of fpn_classifier_graph, there is a dropout layer between the two fc layers.  I read the paper and couldn't find if the original implementation uses dropout.    My current understanding is that the dropout layer is there because the parameter count is really large, is this correct?"
"run code in inspect_model.ipynb:        _ = plt.imshow(activations[""input_image""][0])    then:        ~/Tools/tensorflow_python3/lib/python3.5/site-packages/matplotlib/cm.py in to_rgba(self, x, alpha, bytes, norm)          255                 if xx.dtype.kind == 'f':          256                     if norm and xx.max() > 1 or xx.min()   257                         raise ValueError(""Floating point image RGB values ""          258                                          ""must be in the 0..1 range."")          259                     if bytes:      ValueError: Floating point image RGB values must be in the 0..1 range.      I check the code and find that :        input_image              shape: (1, 1024, 1024, 3)    min: -123.70000  max:  148.10001    So I modify the code :        input_image_a = activations[""input_image""][0]+config.MEAN_PIXEL      input_image_a = A.astype(int)      input_image_a = A/255      log(""input_image_a:"",input_image_a)      _ = plt.imshow(input_image_a)    And then the image showed but there are some difference between your image in inspect_model.ipynb and mine.  My image is just the origin input image after resized and padded.  I think it should not be what you want to show."
I try the training code on my own dataset and the loss dosen't seem to go down whatever the learning rate is. Has anyone train on other dataset successfully? Give me some advice. Thx!
"Hi waleedka,    The file model.py is quite large at over 2200 lines, and could be split up into multiple files for better accessibility, since it already has multiple components such as Resnet, RPN, and FPN, as well as other utility functions. I was wondering if you had any plans to refactor the code.    Also, is the issue page the best place to ask questions about the code?    Thanks.    "
"I read the image from the images folder, and everything is ok. However, when read the '12283150_12d37e6389_z.jpg', I get the error, can you provide some advice?    Processing 1 images  image                    shape: (375, 500, 3)         min:    0.00000  max:  255.00000  ---------------------------------------------------------------------------  TypeError                                 Traceback (most recent call last)    in  ()        5 # Run detection        6 tic()  ----> 7 results = model.detect([image], verbose=1)        8 toc()        9 print results[0]['masks'][0]    /home/kecai/tf/Mask_RCNN/model.pyc in detect(self, images, verbose)     2205                 log(""image"", image)     2206         # Mold inputs to format expected by the neural network  -> 2207         molded_images, image_metas, windows = self.mold_inputs(images)     2208         if verbose:     2209             log(""molded_images"", molded_images)    /home/kecai/tf/Mask_RCNN/model.pyc in mold_inputs(self, images)     2111                 min_dim=self.config.IMAGE_MIN_DIM,     2112                 max_dim=self.config.IMAGE_MAX_DIM,  -> 2113                 padding=self.config.IMAGE_PADDING)     2114             molded_image = mold_image(molded_image, self.config)     2115             # Build image_meta    /home/kecai/tf/Mask_RCNN/utils.py in resize_image(image, min_dim, max_dim, padding)      393     if scale != 1:      394         image = scipy.misc.imresize(  --> 395             image, (round(h * scale), round(w * scale)))      396     # Need padding?      397     if padding:    /home/kecai/miniconda2/envs/kecai/lib/python2.7/site-packages/numpy/lib/utils.pyc in newfunc(*args, **kwds)       99             """"""`arrayrange` is deprecated, use `arange` instead!""""""      100             warnings.warn(depdoc, DeprecationWarning, stacklevel=2)  --> 101             return func(*args, **kwds)      102       103         newfunc = _set_function_name(newfunc, old_name)    /home/kecai/miniconda2/envs/kecai/lib/python2.7/site-packages/scipy/misc/pilutil.pyc in imresize(arr, size, interp, mode)      562         size = (size[1], size[0])      563     func = {'nearest': 0, 'lanczos': 1, 'bilinear': 2, 'bicubic': 3, 'cubic': 3}  --> 564     imnew = im.resize(size, resample=func[interp])      565     return fromimage(imnew)      566     /home/kecai/miniconda2/envs/kecai/lib/python2.7/site-packages/PIL/Image.pyc in resize(self, size, resample, box)     1743         self.load()     1744   -> 1745         return self._new(self.im.resize(size, resample, box))     1746      1747     def rotate(self, angle, resample=NEAREST, expand=0, center=None,    TypeError: integer argument expected, got float"
!   
"Hi,I made some masks for some 2048*2048 images.Then I found it's too big for my memory.So I change the IMAGE_MAX_DIM in config .But I couldn't find anywhere to change the size of the masks.Will the size of the masks change automatically?"
"Well, I have a voc-like dataset with __7000__ classes. So I use the following config:         Other config is just as the default config.    And~ I only take less than 15 objects in one image.    I run it on 2 Titan X, each of which has 12 GB memory. But still run out of memory during training:         So, anyone can tell me how to prevent it?      -----    OK~ set `TRAIN_ROIS_PER_IMAGE = 32`, goes well."
"I get an error when running the demon code : model.load_weights(COCO_MODEL_PATH, by_name=True)    IOError: Unable to open file (truncated file: eof = 255370586, sblock->base_addr = 0, stored_eoa = 257558264)        "
Can we use the CPU instead of GPU with your demo
"Hi, I want to train my own dataset consisting of 10 classes and 30000 images with segmentation annotation. Can you give me some advice on how many epoches for each training stage and how much time it may take to train on two TITAN X GPU? Thanks a lot!"
"i want to train on my own dataset. the image size is 1024*1024 and with a Titan xp GPU (12G). when i run the script . it wait .  `Configurations:`     when i use **Ctrl+c** it out_put:  `Traceback (most recent call last):    File ""train_own_data.py"", line 252, in        train_own_data()    File ""train_own_data.py"", line 189, in train_own_data      layers=""all"")    File ""/home/wuyudong/Project/scripts/Mask_RCNN/model.py"", line 2088, in train      **fit_kwargs    File ""/home/wuyudong/anaconda3/envs/python34/lib/python3.4/site-packages/keras/legacy/interfaces.py"", line 87, in wrapper      return func(*args, **kwargs)    File ""/home/wuyudong/anaconda3/envs/python34/lib/python3.4/site-packages/keras/engine/training.py"", line 2046, in fit_generator      generator_output = next(output_generator)    File ""/home/wuyudong/anaconda3/envs/python34/lib/python3.4/site-packages/keras/utils/data_utils.py"", line 661, in get      time.sleep(self.wait_time)  KeyboardInterrupt`  when i set the image size to 256*256 it works  what should i do  "
Hi could anyone kindly tell me where to download the pretrained imagenet model weights...thanks!
None
"The README mentions pre trained weights trained on the COCO dataset. However it doesn't seem to be there in the repo! Could you please commit/link it, it would be really useful."
"Hi, thanks a lot for publishing this code as open source project.    I trained the net without doing any changes to the code/configuration, and by initializing it with ImageNet weights. After training the weights output is different and gives worse results(but somewhat similar) compared to the one which was provided in the repository.   I have some thoughts on why the train could go wrong, although I’m not sure in my correctness. From the code, it seems to me that the mean is applied at the end of loss functions calculations and results from different GPUs are concatenated during the model parallelization, hence no need learning rate change. I have trained the net with 1 GPU, and as far I understand in such case in each step the net is trained with 2 images, but if the net in trained with 8 GPUs, then in each step the net will be trained with 16 images, and my thought is that in that case the train will be more stable, since the impact of “noise” will be smaller because the gradient direction will be determined by using 16 images instead of 2.   Please correct me if you think I made a wrong conclusions, and do you have any ideas on why the train could go wrong?     Thanks in advance!    Here is an inference result with my trained weights.    !     !   "
"Hi~ I am training Mask RCNN on my dataset. During training, an error occured:         Anyone can tell me how to fix it?"
"1.`2159         exclude_ix = np.where((boxes[:, 2] - boxes[:, 0]) * (boxes[:, 2] - boxes[:, 0]) <= 0)[0]` --line 2159 in function `unmold_detections(self, detections, mrcnn_mask, image_shape, window)` the bboxes area calculations was misspelt.  2.  Also in this function the **filtering out zero area process**  is better placed after the **translation coordinates backing to image domain process** for the reason that the latter process may still cause zero area."
"First of all I would like to thank the authors for such a great work.  When running the` train_shapes` notebook, I get the following error message 3 times in the first epoch.          I guess my humble 2GB GPU is not enough... how could I solve this? I do not see any batch size option implemented, is there any at all? Do you think that would that solve my issue? I would appreciate all kind of suggestions. Thanks."
"Hi,    For my master thesis I am working on  the topic of text detection in images and video frames. I implemented a modified version of Faster R-CNN/Mask R-CNN, which is very similar to your implementation, but tailored for text detection.     On average there are ~54 positive text anchors in the images I use, which result in a mini batch of ~54 positive and ~200 negative examples per image ( I use mini batches of size 254 per image). The problem I encounter is that my network overfits on only negative prediction (because on average, there are many more negative than positive examples). Now  a few simple solutions would be to 1) take a smaller mini batch size(for example 112), 2) to remove all the images in which there are not enough positive examples from the data set or 3) use a weighted loss function.     I inspected your code very carefully, but (as far as I can see) in your implementation this doesn't seem to be an issue. Was this positive/negative class imbalance also a problem in your implementation, and if so, how did you solve this problem?    Thanks!    Maurits "
Presumably `VALIDATION_STPES` should be `VALIDATION_STEPS` instead?
"I have a gtx 780 card which have 3Gb of memory, but when runing the demo example,  an error ocurred :""W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.07GiB.""    so how much gpu memory required to run an inference example ?"
"Firstly I want to congratulate you for this amazing work.    In your opinion, where modifications will impact most the performance (frame rate) of the method.     I'm thinking going with a smaller backbone network (for instance, squeeze net). Do you think it's there the performance bottleneck?"
"Please guide me on how to train the model and weights on custom images.I have set of images .How do I import those images,mask them and train this model."
"You used KL.UpSampling2D() to build the FPN network. Did you mean un-pooling or deconv?    Because when I looked into keras' document, it says this function is to repeats the rows and columns. (more like un-pooling). See from this:      But it seems like this process should be more like deconv. Am I wrong about understanding FPN ?"
"I am trying to run coco.py on a machine with 8 Tesla P100 GPUs... However, it seems like there is something going wrong when I try to use more than one GPU.   I was able to run the `parallel_model.py` file on all GPUs without a problem.  The error that is dumped in my terminal is the following one:         Is there anyone that has faced a similar error here?  My system is running python 3.5 and the latest keras (2.0.9) and tensorflow (1.4.0) versions.  Thanks!  "
"I test the detection speed with different batch_size, and it's surprising that the detection speed reduce when batch size increasing.  Then I check my device , and find it's OK.  Finally, I find batch_slice's annotation in utils.py.        Batch Slicing      Some custom layers support a batch size of 1 only, and require a lot of work      to support batches greater than 1. This function slices an input tensor      across the batch dimension and feeds batches of size 1. Effectively,      an easy way to support batches > 1 quickly with little code modification.      In the long run, it's more efficient to modify the code to support large      batches and getting rid of this function. Consider this a temporary solution      Although I have changed the code in DetectionLayer, it's the fact that the model support batch size > 1 superficially.    And I also find a bug, if we feed images of which the length is smaller than config.BATCH_SIZE to the function detect(self, images, verbose=0), program will raise exception.  I add ""assert"" to avoid it.        assert len(images) == self.config.BATCH_SIZE, ""len(images) must be equal to BATCH_SIZE""         "
"I use 'model.detect(image_batch, verbose=1)' to detect images, but I find the result is error.  The results of '1.jpg' and '10.jpg' are same.  I check the code and find that the value of input is ok but the value of detections[0] and detections[1] are duplicate.    code in model.py        detections, mrcnn_class, mrcnn_bbox, mrcnn_mask, \              rois, rpn_class, rpn_bbox =\          self.keras_model.predict([molded_images, image_metas], verbose=0)    my test code        image_batch = []      image_name_batch = []        image_path_1 = './1.jpg'      image_batch.append(skimage.io.imread(image_path_1))      image_path_10 = './10.jpg'      image_batch.append(skimage.io.imread(image_path_10))        results = model.detect(image_batch, verbose=1)      for j in range(len(image_batch)):          r = results[j]          visualize.display_instances(image_batch[j], r['rois'], r['masks'], r['class_ids'],                            coco_class_names, r['scores'],title=image_name_batch[j])"
"Hey @waleedka     Great work. Now I have man-crush on you.     A little PSA. If anyone is getting this error:    > ValueError: The channel dimension of the inputs should be defined. Found `None`.    you're most likely having backend issue ( in ~/.keras/keras.json), as it might be set to Theano instead of TF. And even if it it,s for some reason jupyter notebook is not reading it correctly.    Solution. You can do either    1) Before this cell is executed:         you can add:         2) Stick it in ipython notebook startup file (~/.ipython/profile_default/startup/ ) like  "
  !   
   AttributeError: Can't pickle local object 'GeneratorEnqueuer.start. .data_generator_task'
"During my training process, the loss of step 1 suddenly jumps from a low value (like 2.1) to a high value (like 13) in step 2. is that a normal situation? "
!   
"When I run demo ,there is an error:  scipy.misc has no attribute 'imread'.    I check the code and find that 'imread' is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.  But the fact is that when I use the scipy with version  1.0.0, error occurs.  I chage the scipy version to 1.0.0rc2 and then the demo run ok.    Could you provide the dependence list for the project to guide user preparing the environment.   "
"Hi @waleedka ,    Thanks a lot for an awesome repository. You've done good work.   I'm trying to trying to trigger a training on own dataset which is of Coco dataset type, but less number of classes(only 5).    When I changed the the number classes to 5+1 in the coco.py and trigger a training using Coco pretrained model, I'm getting the shape mismatch error.     Does this because the pretrained model is of 81 classes Or have I done any mistake? Please correct me if I'm wrong.    So does this mean, I need to start a new training from scratch for 5 classes?    Regards,  Sharath"
"I am running testing MASK_RCNN on my local computer and on a remote machine.  In `inspect_model` everything runs fine locally but on the remote machine I get an assertion error at `### 1.b RPN Predictions`:       =>       when printing the outputs:     I get the following output locally:     and the following remotely:     So looks like `(""post_nms_anchor_ix"", model.ancestor(pillar, ""ROI/rpn_non_max_suppression:0""))` is causing the issue.    Any suggestions? Thanks in advance"
"The code is here followed the demo.ipynb. But there are some errros.     import os  import sys  import random  import math  import numpy as np  import scipy.misc  import matplotlib  import matplotlib.pyplot as plt    import coco  import utils  import model as modellib  import visualize          # Create model object in inference mode.  model = modellib.MaskRCNN(mode=""inference"", model_dir='mask_rcnn_coco.h5', config=0)    # Load weights trained on MS-COCO  model.load_weights('mask_rcnn_coco.h5', by_name=True)      # COCO Class names  # Index of the class in the list is its ID. For example, to get ID of  # the teddy bear class, use: class_names.index('teddy bear')  class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',                 'bus', 'train', 'truck', 'boat', 'traffic light',                 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',                 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',                 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',                 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',                 'kite', 'baseball bat', 'baseball glove', 'skateboard',                 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',                 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',                 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',                 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',                 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',                 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',                 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',                 'teddy bear', 'hair drier', 'toothbrush']      # Load a random image from the images folder  file_names = next(os.walk('/media/wxl/00063DAF000FECC3/Mask_RCNN-master/images'))  image = scipy.misc.imread(os.path.join('/media/wxl/00063DAF000FECC3/Mask_RCNN-master/images', random.choice(file_names)))    # Run detection  results = model.detect([image], verbose=1)    # Visualize results  r = results[0]  visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'],                              class_names, r['scores'])    The errors locate in:   File ""/media/wxl/00063DAF000FECC3/Mask_RCNN-master/test.py"", line 20, in        model = modellib.MaskRCNN(mode=""inference"", model_dir='mask_rcnn_coco.h5', config=0)  I think the model is load incorrect. But I have no idea about the mistakes. Thanks for your helps"
Just a quick question: where can I find the minival set you used in to train with the coco dataset?  Currently I used a minival (5k images) set from here:      and the remaining 35K are in the following file:       awesome work by the way!
"The model seems to have problems with wings, limbs and holes. Is there theoretical awareness of this and proposed countermeasures? "
None
"When I want to run the demo.ipynb, I can't find the model file mask_rcnn_coco.h5. Wherer can I get the file or  I must train the model first? "
How to used it？Any one can help me？
Could you share the trained model for MS COCO?  Thank you very much
"Hi,    Thanks a lot for the awesome repository.     I went to train_shapes file which describes about how to train for our own dataset.     But all the things which you guys are doing over there is by generating randomly. Could explain the same by taking actual images which has ground truth of mask, class and bounding related information.     Regards,  Pirag"
"Hi  first thanks for sharing this great repo.    Now I tried the jupiter notebook but got an error on the first cell (not sure why):     then I did comment the line given it is just a print but then  I got a ton of more errors;    So back to the command line, but I get the same error...    can you please help to fix or give the steps to properly evaluate on a new image (not COCO data)?    thanks for your help.  Tets  "
"Hi Waleed,  This is wonderful work and I'm glad you made it available to the rest of us.  The two small edits to `README.md` might make it easier to experiment with your package:         Let me know it you'd like me to PR this.  Thanks again! "
"Thanks a lot  I am having the following error  ""ImportError: No module named 'pycocotools'""    Can you please advice me how to install it as I could not find it on pip or conda?    I tried the installation from   but did not work too "
"Hi @waleedka ,    I have a less powerful GPU and want to train my own data which contain only 3 classes. Therefore I would use Resnet50 to speed up the model. How can I switch to Resnet50?  Thanks for your help.  "
"Thanks for the great work!    In ""demo.ipynb"", It reminded  can be found 'mask_rcnn_coco.h5' in the  README file.  BUT, I can't find.    "
"I was wondering if it were possible to replicate the experiments using my own images. How do I do that? Are there limitations on, for example, the size of the images?  Thank you very much"
Did anyone has the deepdraw code
"Hi, ZhaoFan,    Thanks for your code.    Now I am running P3D model for the action similarity labeling (2nd task in your paper).     For some similarity index, we need to compute division (e.g. in  x1*(log(x1/x2)), and the output from layers could be zero. Therefore, the divisor is zero, leading to the problem that the similarity is NAN.     Did you meet such problem? If so, how did you address this case? Many thanks. "
" If each block is repeated three times in the network structure, it will be changed into three forms A, B, and C in turn, but each block of r is only repeated twice, how to change resnet18?    Resnet网络结构中如果每块重复三次，依次改成A,B,C三种形式，但是resnet18每一块只重复2次，该怎么改？"
Thank you for your grate work! I have some questions about classifier：Whether the experiments in your paper were implemented by P3D+SVM？If so，did you use softmax to optimize the P3D until convergence and then get the final feature to train SVM for final predict result？
"That is , [104, 117, 123]  is the mean-value list, which one is the blue-channel ? 104 or 123?"
"Could you tell me what's the version of `caffe` you use or the date when you download official caffe to your local machine?  I follow your instructions in README.md to modify the latest caffe. However, it occurs an error when loading pretrained model provided from you. Without the pretrained model, the network prototxt is good to use for training.    Here is the error info:    F0704 17:35:29.248016 42380 upgrade_proto.cpp:95] Check failed: ReadProtoFromBinaryFile(param_file, param) Failed to parse NetParameter   file: ~/p3d/p3d_resnet_sports1m_iter_150000.caffemodel                                                                       *** Check failure stack trace: ***                                                                                                           @     0x7faa0842bdaa  (unknown)                                                                                                          @     0x7faa0842bce4  (unknown)                                                                                                          @     0x7faa0842b6e6  (unknown)                                                                                                          @     0x7faa0842e687  (unknown)                                                                                                          @     0x7faa08a9145e  caffe::ReadNetParamsFromBinaryFileOrDie()                                                                          @     0x7faa08be4197  caffe::Net ::CopyTrainedLayersFromBinaryProto()                                                                   @     0x7faa08be4230  caffe::Net ::CopyTrainedLayersFrom()                                                                              @           0x407ca4  CopyLayers()                                                                            @           0x408654  train()                                                                                                            @           0x4059ec  main                                                                                                               @     0x7faa07657ec5  (unknown)                                                                                                          @           0x4063ab  (unknown)                                                                                                          @              (nil)  (unknown)                                                                                                   Aborted                                                      "
"Hi，  Thanks for your code about pesudo-3d.I have a question about the network.That's why the network has 5 dim([batch_size, t_size, n_size, n_size, out_dim]) at res4? but the output of res4 will be reshape 4 dim ,and the two last one is 10.what's the meaning of it ?it will be changed to conv2d .what's the meaning about it? @ZhaofanQiu "
"Hi, thank you for sharing this great work.  I want to training the P3D,but I facing some problem about training detail. If you release the train.prototxt and solver.prototxt, I will be very grateful to you.   Thank you !"
Would you share you deepdraw code.  Thank you
"Hi, thank you for sharing this great work.    Would you give more details about how the IDT is used to improve the results? Which library you used to calculate? How the merge with the pseudo features is done?"
"I'm having trouble casually testing the model. What label order was used for the Kinetics dataset when training?  I assumed alphabetic order for one attempt, but the results were not representative.  Thank you."
"Hello, when i try to draw the picture of network structure as the follow :  `python /caffe-master/python/draw_net.py deploy_p3d_resnet_sports1m.prototxt deploy_p3d_resnet_sports1m.jpg --rankdir=TB`    there has a error that `google.protobuf.text_format.ParseError: 65:3 : Message type ""caffe.LayerParameter"" has no field named ""bn_param"". `    Thanks~"
"Hi,Zhanfan,i found you did not mention the input format.Should the train data is showed as a training folder,then it include some clip floders?Could you show some training or testing folders as a demo file?  Thank you very much!!!"
"hi,  according to the paper,the input of the model is a 16-frames clip，but most video is much longer than 16 frames.Do you use the 16-frame clip  presenting the whole video clip to train the model?"
"Hello ZhaoFan,    Thanks for your great work.    I would like to consult whether your codes can be compiled and run on Caffe for Linux without any modifications except updating configurations. I suspect that it will be ""NO"" since I got many compiling error under Linux....Could you please help to point it?  I only need answer ""Yes"" Or ""NO"", a simple reply.  Thanks in advance.     "
"i am currently trying to reimplement  on UCF101.    first, when i do the training with p3d-b, i find that the total loss suddenly increases from around 4 to 11,  after four or five epoch. did you meet the same problems?    second, how do you come up with doing a two-stage training, finetune p3d on a trained tsn  model?     third, why do you decide to decrease the input from 224x224 to 160x160? "
"I'm really sorry to bother you,I want use P3D to extract the feature of video like C3D,  Can you show me some detail of implement?  In the caffe version of P3D, I have compiled the original caffe， shoud I  complie the caffe with some file you have supported？"
"Thank you for your master work! I thank it may help me a lot. I download the addition layers and merge into original caffe. then I compile it, but I get the follow errors. _# I am not very sure that the directory I put is right  ._      CXX src/caffe/layers/pooling3d_layer.cpp  src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer ::LayerSetUp(const std::vector *>&, const std::vector *>&) [with Dtype = float]’:  src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here  src/caffe/layers/pooling3d_layer.cpp:24:69: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’    Pooling3DParameter pool_param = this->layer_param_.pooling3d_param();                                                                       ^  In file included from ./include/caffe/common.hpp:6:0,                   from ./include/caffe/blob.hpp:8,                   from ./include/caffe/layers/pooling3d_layer.hpp:20,                   from src/caffe/layers/pooling3d_layer.cpp:14:  src/caffe/layers/pooling3d_layer.cpp:87:5: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’       == Pooling3DParameter_PoolMethod_AVE       ^  src/caffe/layers/pooling3d_layer.cpp:89:5: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’       == Pooling3DParameter_PoolMethod_MAX)       ^  src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer ::Reshape(const std::vector *>&, const std::vector *>&) [with Dtype = float]’:  src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here  src/caffe/layers/pooling3d_layer.cpp:140:51: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’     if (this->layer_param_.pooling3d_param().pool() ==                                                     ^  src/caffe/layers/pooling3d_layer.cpp:146:51: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’     if (this->layer_param_.pooling3d_param().pool() ==                                                     ^  src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer ::Forward_cpu(const std::vector *>&, const std::vector *>&) [with Dtype = float]’:  src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here  src/caffe/layers/pooling3d_layer.cpp:167:3: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’     switch (this->layer_param_.pooling3d_param().pool()) {     ^  src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer ::Backward_cpu(const std::vector *>&, const std::vector &, const std::vector *>&) [with Dtype = float]’:  src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here  src/caffe/layers/pooling3d_layer.cpp:288:3: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’     switch (this->layer_param_.pooling3d_param().pool()) {     ^  src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer ::LayerSetUp(const std::vector *>&, const std::vector *>&) [with Dtype = double]’:  src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here  src/caffe/layers/pooling3d_layer.cpp:24:69: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’    Pooling3DParameter pool_param = this->layer_param_.pooling3d_param();                                                                       ^  In file included from ./include/caffe/common.hpp:6:0,                   from ./include/caffe/blob.hpp:8,                   from ./include/caffe/layers/pooling3d_layer.hpp:20,                   from src/caffe/layers/pooling3d_layer.cpp:14:  src/caffe/layers/pooling3d_layer.cpp:87:5: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’       == Pooling3DParameter_PoolMethod_AVE       ^  src/caffe/layers/pooling3d_layer.cpp:89:5: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’       == Pooling3DParameter_PoolMethod_MAX)       ^  src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer ::Reshape(const std::vector *>&, const std::vector *>&) [with Dtype = double]’:  src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here  src/caffe/layers/pooling3d_layer.cpp:140:51: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’     if (this->layer_param_.pooling3d_param().pool() ==                                                     ^  src/caffe/layers/pooling3d_layer.cpp:146:51: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’     if (this->layer_param_.pooling3d_param().pool() ==                                                     ^  src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer ::Forward_cpu(const std::vector *>&, const std::vector *>&) [with Dtype = double]’:  src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here  src/caffe/layers/pooling3d_layer.cpp:167:3: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’     switch (this->layer_param_.pooling3d_param().pool()) {     ^  src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer ::Backward_cpu(const std::vector *>&, const std::vector &, const std::vector *>&) [with Dtype = double]’:  src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here  src/caffe/layers/pooling3d_layer.cpp:288:3: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’     switch (this->layer_param_.pooling3d_param().pool()) {     ^  Makefile:581: recipe for target '.build_release/src/caffe/layers/pooling3d_layer.o' failed  make: *** [.build_release/src/caffe/layers/pooling3d_layer.o] Error 1  "
"After adding your layers to my caffe , I met the following errors when I compile the caffe :    src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer ::LayerSetUp(const std::vector *>&, const std::vector *>&) [with Dtype = float]’:  src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here  src/caffe/layers/pooling3d_layer.cpp:24:69: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’    Pooling3DParameter pool_param = this->layer_param_.pooling3d_param();                                                                       ^  In file included from ./include/caffe/common.hpp:6:0,                   from ./include/caffe/blob.hpp:8,                   from ./include/caffe/layers/pooling3d_layer.hpp:20,                   from src/caffe/layers/pooling3d_layer.cpp:14:  src/caffe/layers/pooling3d_layer.cpp:87:5: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’       == Pooling3DParameter_PoolMethod_AVE       ^  src/caffe/layers/pooling3d_layer.cpp:89:5: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’       == Pooling3DParameter_PoolMethod_MAX)       ^  src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer ::Reshape(const std::vector *>&, const std::vector *>&) [with Dtype = float]’:  src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here  src/caffe/layers/pooling3d_layer.cpp:140:51: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’     if (this->layer_param_.pooling3d_param().pool() ==                                                     ^  src/caffe/layers/pooling3d_layer.cpp:146:51: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’     if (this->layer_param_.pooling3d_param().pool() ==                                                     ^  src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer ::Forward_cpu(const std::vector *>&, const std::vector *>&) [with Dtype = float]’:  src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here  src/caffe/layers/pooling3d_layer.cpp:167:3: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’     switch (this->layer_param_.pooling3d_param().pool()) {     ^  src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer ::Backward_cpu(const std::vector *>&, const std::vector &, const std::vector *>&) [with Dtype = float]’:  src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here  src/caffe/layers/pooling3d_layer.cpp:288:3: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’     switch (this->layer_param_.pooling3d_param().pool()) {     ^  src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer ::LayerSetUp(const std::vector *>&, const std::vector *>&) [with Dtype = double]’:  src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here  src/caffe/layers/pooling3d_layer.cpp:24:69: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’    Pooling3DParameter pool_param = this->layer_param_.pooling3d_param();                                                                       ^  In file included from ./include/caffe/common.hpp:6:0,                   from ./include/caffe/blob.hpp:8,                   from ./include/caffe/layers/pooling3d_layer.hpp:20,                   from src/caffe/layers/pooling3d_layer.cpp:14:  src/caffe/layers/pooling3d_layer.cpp:87:5: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’       == Pooling3DParameter_PoolMethod_AVE       ^  src/caffe/layers/pooling3d_layer.cpp:89:5: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’       == Pooling3DParameter_PoolMethod_MAX)       ^  src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer ::Reshape(const std::vector *>&, const std::vector *>&) [with Dtype = double]’:  src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here  src/caffe/layers/pooling3d_layer.cpp:140:51: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’     if (this->layer_param_.pooling3d_param().pool() ==                                                     ^  src/caffe/layers/pooling3d_layer.cpp:146:51: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’     if (this->layer_param_.pooling3d_param().pool() ==                                                     ^  src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer ::Forward_cpu(const std::vector *>&, const std::vector *>&) [with Dtype = double]’:  src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here  src/caffe/layers/pooling3d_layer.cpp:167:3: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’     switch (this->layer_param_.pooling3d_param().pool()) {     ^  src/caffe/layers/pooling3d_layer.cpp: In instantiation of ‘void caffe::Pooling3DLayer ::Backward_cpu(const std::vector *>&, const std::vector &, const std::vector *>&) [with Dtype = double]’:  src/caffe/layers/pooling3d_layer.cpp:368:1:   required from here  src/caffe/layers/pooling3d_layer.cpp:288:3: error: ‘class caffe::LayerParameter’ has no member named ‘pooling3d_param’     switch (this->layer_param_.pooling3d_param().pool()) {     ^  Makefile:581: recipe for target '.build_release/src/caffe/layers/pooling3d_layer.o' failed  make: *** [.build_release/src/caffe/layers/pooling3d_layer.o] Error 1    How can I solve these errors? Thank you ."
"Hi, I am learning how to extract the video feature from your proposed P3d.  What's the format of input? Can you show more details such as the usage.  Thank you very much. "
"Hi,@ZhaofanQiu,thanks for your nice job!    How do you implement the leave-one-video-out algorithm for the Dynamic Scene dataset?  This dataset has 130 videos(13 class,each class 10 videos), Just train 130 models with the libsvm ?  "
"sorry to impose,   have you ever reproduced result of P-3d in PyTorch?  "
"Hi,     I evaluated your provided P3D Resnet models trained on Kinetics-400 dataset. Specifically following clues mentioned in your paper 'Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks', my data layers for rgb and flow nets are as follow:    layer {    name: ""data""    type: ""VideoData""    top: ""data""    top: ""label_local""    video_data_param {      source: ""kinetics_400_vallist_rgb_25segs_s1_4eval.txt""      root_folder:""""      batch_size: 10      new_length: 16      new_width: 242      new_height: 182      sampling_rate: 1      show_data: false       use_image: true     }    transform_param{      crop_size: 160      mirror: false      mean_value: [104, 117, 123]    }    include: { phase: TEST }  }    layer {    name: ""data""    type: ""VideoData""    top: ""data""    top: ""label_local""    video_data_param {      source: ""kinetics_400_vallist_flow_25segs_s1_4eval.txt""      root_folder:""""      batch_size: 10      new_length: 16      new_width: 242      new_height: 182      sampling_rate: 1      show_data: false       use_image: true       use_flow: true    }    transform_param{      crop_size: 160      mirror: false      mean_value: 128    }    include: { phase: TEST }  }    I got 59.47%/81.19% and 69.38%/89.06% clip-level and (average pooling) video-level top1/top5 accuracies on kinetics-400 validation set using your RGB model (p3d_resnet_kinetics_iter_190000.caffemodel).  And I got 9.42%/22.21% and 15.95%/33.66% clip-level and video-level top1/top5 accuracies on Flow modality (p3d_resnet_kinetics_flow_iter_284000.caffemodel).    I guess there must be something wrong with my evaluation. Could you please kindly provide the corresponding performances of your released models for reference?  "
"  @ZhaofanQiu Hi,I am a ustcer too.  In your paper, it is said you trained P3D with multiple GPUs in parallel. And we know P3D resnet has batch norm layers.So I wonder whether the bacth norm layers use the data in each gpu or all gpus  (to calculate mean, variance , etc.)?  Any help will be greatly appreciated"
  I have this problem in the configuration of the network.   Check failed: this->layer_param_.param(i).lr_mult() == 0.f (1 vs. 0) Cannot configure batch normalization statistics as layer parameters.  Could you answer it for me?   Thank you  
"Hi Zhaofan,    Has been resolved."
How about Kinetics evaluation?  I can't find Kinetics evaluation in paper.
   128 frames equal to 8 clips (16 frames/clip). Whether these 8 clips come from one video or from 8 videos?
"I added your layers into the newest official caffe. Here is my code:     However, it gives this follow error    python/caffe/pycaffe.py"", line 68, in _Net_params  if len(lr.blobs) > 0])  OverflowError: long int too large to convert to int    my md5 of p3d_resnet_sports1m_iter_150000.caffemodel is:  9f28383e1d1f09df8f7a78831c3fc202"
I don't want add the layer into caffe-bvlc. So directly installing caffe-ms dose work.Am I right?
Thanks for your great work and nice sharing! @ZhaofanQiu   Is there the slides of the paper publicly available? Thanks!
"Why log loss[-sum(log f  (yis; xsi ))] is used in multi classification problem instead of cross entropy？  For example, I have three categories. When the prediction y of the network is correct, such as equal to (1,0,0) or (0,1,0), the loss is infinite."
"for example,in transfer task Amazon to Webcam  I1114 18:11:03.598723 15175 net.cpp:679] Copying source layer fc8-new  I1114 18:11:03.598791 15175 net.cpp:679] Copying source layer fc8-dial  I1114 18:11:03.598898 15175 net.cpp:679] Copying source layer fc8-sc  I1114 18:11:03.598990 15175 net.cpp:676] Ignoring source layer fc8-slice  I1114 18:11:03.599041 15175 net.cpp:676] Ignoring source layer loss-source  I1114 18:11:03.599086 15175 net.cpp:676] Ignoring source layer loss-target  F1114 18:11:03.763149 15175 accuracy_layer.cpp:71] Check failed: label_value < num_labels (24 vs. 10)     I am sure my data is correct and works in other caffe projects.Is it caused by an internal defect in alexnet_train_val.prototxt?  "
"Hello, I am a fresh learner. I have noticed that the model is trained on 4 GPUs for two days(In this paper).    So does that mean, if I don't have a good computer , I can't run the experiments on my computer?    Thanks!"
None
"I have some questions about three training stages.      1. I find you train novel class together with base class. However, the feature of base class far greater than novel class.  So it suffers a severe label imbalance during low shot training. why it still can work?  2. During base training, I find you use 389 class out of 1000 class, however you train a classifier with 1000 output, it is so weired.  3. When low shot training finished, I got a linear module, but its output is combine novel class with base class, so if I use imagenet1k for base training and use flowers102 for few-shot(one image per class) novel class training,  so I can get a model with a 1102 classifier? It's very weird.    "
It isn't clear to me ow I can train on my own dataset from novel classes I introduce. How would one go about this?
"Hi, I am trying to train a ResNet50 for the initial representation learning stage on Python 3.7 and PyTorch version 0.4.1.post2, and for some reason every time I run this I get an out of memory error, even when running on multiple GPUs with 24 GB memory. This most likely has to do with backwards passing, as the script finishes running if I just comment out loss.backward(). I've also looked into running with no_grad() but with errors. The command I used was:    python ./main.py --model ResNet50 \    --traincfg base_classes_train_template.yaml \    --valcfg base_classes_val_template.yaml \    --print_freq 10 --save_freq 10 \    --aux_loss_wt 0.02 --aux_loss_type sgm \    --checkpoint_dir checkpoints/ResNet50_sgm    The code worked perfectly with ResNet10, so I was wondering if there could be a solution to this issue? Do I have to run it on a different version of PyTorch or is it possible to fix with my current setup? Thank you for your help!"
"In the paper of matching network, it use cosine distance as metric. But in matching_networks.py, I didn't find cosine distance.  I wonder why this file didn't use cosine distance. Thanks"
"Hi, I'm trying to use the method on a completely different dataset. My implementation is in Keras and I built the generator model such that I can view the two losses (rms loss and cls loss) during the training phase. I trained for 10 epochs and I get suspicious results:    1. The classification loss/accuracy does not change at all (is it because I don't change the weights of the classifier?) and the accuracy is *very* small: `~0.0818` (the classifier achieved `~0.7` accuracy on a test set for trained categories).  2. The rms loss/accuracy improves - from `~0.6390` accuracy in the first epoch to `~0.7245` in the last epoch while I used `λ=1`.  3. The rms loss/accuracy converges to `~0.6286` in the second epoch (and does not change at all aftwards) while I used `λ=10` (as you did).    Is there something I should be worried about those results?   Do you have any enlightenments?    Thanks!"
"Hello, when running this code I am not able to reproduce your result.    For example, using ResNet10 with no auxiliary loss and no generation, these are my results.         A difference of 8 points for n=1 on Novel Top-1 seems very large. Also, this baseline model performs a lot worse than reported large n.    For reproducibility, I used the following command (and pytorch 0.4.0):         This is the code found on your website with the SGM option removed -- perhaps these are not the hyperparams you are using. If this is the case then please let me know.    Best,  Mitchell  "
"i dont know where i can find the dataset you used.And  please tell me what i should do if i want to set my own dataset? please help me,thanks!"
What version of ImageNet is necessary for saving features? I'm currently using the ILSVRC2012 Challenge dataset that has remain unchanged in challenges from 2012-now but I'm getting an error saying certain images are missing when I run the save_features command specified in the README.
"In this paper,1000 classes should be splited into (389,611),namely base class and novel class.  Is the ""base class""  ""train class""?"
"I want to use this code to train on CIFAR10 dataset. I'm confused with creating json files.    For 1st part (training the conv nets), do i need to specify image labels and image name for all the data that belong to base class.     For 2nd part (extracting features) what does train.json and val.json specify. I can see all the labels are present in this step, so why we extracting features for all the images. Also in this step all the image labels and image names are specified in json files, is that necessary for CIFAR10."
"Hi, thanks for your released code of the paper.But I got a problem.  When I trained ResNet50 or ResNet101 by 'main.py', nan error occured. But everything went well if I trained ResNet10 and ResNet34.   Except the dataset path, I didn't modify anything. The completed command I used is   """"""  python ./main.py --model ResNet50 \        --traincfg base_classes_train_template.yaml \        --valcfg base_classes_val_template.yaml \        --print_freq 10 --save_freq 10 \        --aux_loss_wt 0.02 --aux_loss_type sgm \        --checkpoint_dir checkpoints/ResNet50_sgm  """"""  And I worked well  with official resnet50. Is there something different on the implementation? Thank you very much！"
"Hi, I'm following your work.    In the second step 'Save features from the ConvNet', the configuration is train_save_data.yaml or val_save_data.yaml. However, I did not find '/mnt/vol/gfsai-east/ai-group/users/bharathh/imagenet_meta/train.json' and '/mnt/vol/gfsai-east/ai-group/users/bharathh/imagenet_meta/val.json'.    It seems that the order of the extracted features matters as the splitfile_{:d} specifies the order of indexes.     Or the order of the features is irrelevant？    Thank you very much！  "
"This project is really awesome and while I was running the step two, I found two unknown files.  Could you guys tell me how to get the   `/mnt/vol/gfsai-east/ai-group/users/bharathh/imagenet_meta/train.json`  file in _train_save_data.yaml_  and   `/mnt/vol/gfsai-east/ai-group/users/bharathh/imagenet_meta/val.json`  file in _val_save_data.yaml_?  pretty thanks"
I noticed that you manually added captions for KTH dataset.  It still needs a lot of work. Could you please share the captions?  Thank you very much!
"Could you please elaborate on how to perform the actual cap2vid generation, since the testing phase generates only arbitrary videos in the given code."
"They are mapped to unique integers while saving the captions in h5py file. However, before feeding them into the bidirectional are they represented as an embedding/fixed size vector? or the y_encode function in this   does it?"
where can I get the caption label?  Thank you.
"Hi, where should I download the dataset used in the code?   `datasets/mnist_single_gif_4exp.h5`  `dataset/mnist_single_gif.h5`    Thank you so much. "
!     !   !   !   
"Hi, @hfslyc. Thanks for your code.  Out of curiosity, could you tell me why is this Net called GCPNet?"
"In the paper, 3.3 joint training, a joint loss was defined. But in the code I saw the policy network and the intent prediction network was trained with separate loss. Where the joint loss was used during training? Thanks! "
"Hi, how do you perform multi-shot setting for Market1501? I am confused because in query set, there is only one image in one camera for one person, no multiple images."
@KovenYu 同学，你好！看了你的论文，自己非常感兴趣，想看看你的效果和代码，但是发现CAMEL_supervised.p打不开，同学，请问可不可以看看你这里面的源码？                                                祝好！
None
@mahyarnajibi Hi Mahyar: Thanks for sharing your code. I need to run a minimal example to see how the algorithm performs on my datasets. So I want to run your demo.py file. Do I need to run _make_ in the lib directory too? I am just doing the following steps only. Are they sufficient to be able to run the demo? Thanks    git clone --recursive    pip install -r requirements.txt  bash scripts/download_ssh_model.sh  python demo.py  
None
"I run the demo.py correctly,but when i try to train the module,it happened to rising this error message"
你好，我看你训练ssh的脚本中图片只减了均值。预训练vgg模型在训练时也只减了均值吗？
"Hello :)    I tried run demo.py in CPU mode), I modified file:    # Loading the network  #cfg.GPU_ID = args.gpu_id  #caffe.set_mode_gpu()  #caffe.set_device(args.gpu_id)  caffe.set_mode_cpu()     and I got the following problem:    root@adrian-H81M-S2V:/home/adrian/magisterka/SSH# sudo python3 demo.py   #################### Configuration ####################  TEST{      ORIG_SIZE:  False      SCALES:  [500, 800, 1200, 1600]      MAX_SIZE:  -1      PYRAMID_BASE_SIZE:  [800, 1200]      NMS_THRESH:  0.3      N_DETS_PER_MODULE:  1000      ANCHOR_MIN_SIZE:  0  }  DEBUG:  False  PIXEL_MEANS:  [[[102.9801 115.9465 122.7717]]]  RNG_SEED:  3  EPS:  1e-14  DATA_DIR:  /home/adrian/magisterka/SSH/data  MATLAB:  matlab  EXP_DIR:  ssh_pyramid  USE_GPU_NMS:  True  GPU_ID:  0  ROOT_DIR:  /home/adrian/magisterka/SSH  #######################################################    File ""/home/adrian/magisterka/SSH/SSH/layers/proposal_layer.py"", line 39      print 'feat_stride: {}'.format(self._feat_stride)                            ^  SyntaxError: invalid syntax  Loading the network...Traceback (most recent call last):    File ""demo.py"", line 50, in        net = caffe.Net(args.prototxt, args.model, caffe.TEST)  SystemError:   returned NULL without setting an error    What could be wrong?    Best regards,  Adrian"
   I used this code      The speed result from the network I was able to get average of 21 second from CPU with pre-trained model.  !     Can anyone share their speed result by using CPU?   
None
None
"when I run python demo.py, error occured like :  File ""demo.py"", line 8, in        from SSH.test import detect    File ""/home/fmming/test/tt/SSH-master/SSH/test.py"", line 14, in        from nms.nms_wrapper import nms    File ""lib/nms/nms_wrapper.py"", line 9, in        from nms.cpu_nms import cpu_nms  ImportError: No module named cpu_nms,   how can I fix it ?   Thanks a lot"
"Hello all,  I want to get landmark to align face. Can you tell me about the way do it ?  Thanks in advance"
"the training part of wider face dataset  is about 15K   and the batch size is 1, using 4 GPUs,  the iteration is setting to 21K to get the best result.  how did the 21K come from?  "
"I trained wider according to the introduction, but tested it on the generated model. Why didn't face be detected?"
"I try to train SSH using my own data.My training data is about 10k.    To get the better detection result,should I change the hyperparameters,    like the epochs, batch_size, learinging rate...? "
"Is i want to use the ssh + pyramid, what should i do?  just change the wider.yml to wider_pyramid.yml?"
F1030 11:30:52.953382  7873 blob.cpp:34] Check failed: shape[i] <= 0x7fffffff / count_ (2400 vs. 2333) blob size exceeds INT_MAX  *** Check failure stack trace: ***  Aborted (core dumped)  
"Dear author， when I use the pre-trained VGG-SSH model to detect the faces in  my own datasets, many heads or faces of some animals like cat, dog are extracted. It seems that the model has a relatived high false positive compared with cascade CNN. "
"hi,  I met the error of the compressed model file, SSH_models.tar.gz.  gzip: stdin: unexpected end of file  SSH_models/  SSH_models/SSH.caffemodel  tar: Unexpected EOF in archive  tar: Unexpected EOF in archive  tar: Error is not recoverable: exiting now  "
"@mahyarnajibi   @po0ya   HI    **Section 4.4** points out:  `Moreover, HR-ER and Conv3D both generate ellipses to decrease the localization error.  In contrast, SSH does not use FDDB for training, and is evaluated on this dataset out-of-the-box by generating bounding boxes`    Does this mean in the test, SSH use a rectangular result directly? There is no conversion of rectangular box to elliptical box"
"Hi, I was interested in using this on my MacBook, 2.3ghz, 16gb ram.  Do you think it could run (the pretrained model)?  Is there a lot in the model I would need to change?  Thanks!"
@po0ya   @mahyarnajibi          Why use all anchors when there are no inside anchors???  hope your reply!!!  
"@mahyarnajibi   @po0ya   HI    In **bbox_transform.py**, about **bbox_transform_inv(boxes, deltas)**:       **Why consider this judgment**(`dw[i][j]>50,` `dh[i][j]>50`)? **When does this happen?**    many many thanks!!"
I use ssh for training an industrial defects detection model. However this ssh model cannot converge. Which parameters I need to change？  !   
@mahyarnajibi Hello!Have you used video for testing?How does it work?
None
"SSH is proposed as binary classification (face and bg). Can it be extended smoothly for more than two classes with this implementation? Where will be the change in this implementation.     Currently, I am training for 6 classes and I changed the num_classes field in SSH/models/train.protobuf.     Any comments?"
Any idea/code/implementation of visualisation of training curves and images in caffe like tensorboard?
None
"Hi,    i am trying to compile your caffe branch to test the code but i get the following error, complaining that ""SmoothL1LossParameter"" is not found.. here is my output:      CXX src/caffe/layers/smooth_l1_loss_layer.cpp  src/caffe/layers/smooth_l1_loss_layer.cpp:10:3: error: unknown type name 'SmoothL1LossParameter'; did you mean 'SmoothL1LossLayer'?    SmoothL1LossParameter loss_param = this->layer_param_.smooth_l1_loss_param();    ^~~~~~~~~~~~~~~~~~~~~    SmoothL1LossLayer  ./include/caffe/layers/smooth_l1_loss_layer.hpp:22:7: note: 'SmoothL1LossLayer' declared here  class SmoothL1LossLayer : public LossLayer  {        ^  src/caffe/layers/smooth_l1_loss_layer.cpp:10:57: error: no member named 'smooth_l1_loss_param' in 'caffe::LayerParameter'    SmoothL1LossParameter loss_param = this->layer_param_.smooth_l1_loss_param();                                       ~~~~~~~~~~~~~~~~~~ ^  src/caffe/layers/smooth_l1_loss_layer.cpp:62:19: note: in instantiation of member function 'caffe::SmoothL1LossLayer ::LayerSetUp' requested here  INSTANTIATE_CLASS(SmoothL1LossLayer);                    ^  src/caffe/layers/smooth_l1_loss_layer.cpp:10:57: error: no member named 'smooth_l1_loss_param' in 'caffe::LayerParameter'    SmoothL1LossParameter loss_param = this->layer_param_.smooth_l1_loss_param();                                       ~~~~~~~~~~~~~~~~~~ ^  src/caffe/layers/smooth_l1_loss_layer.cpp:62:19: note: in instantiation of member function 'caffe::SmoothL1LossLayer ::LayerSetUp' requested here  INSTANTIATE_CLASS(SmoothL1LossLayer);                    ^  3 errors generated.  make: *** [.build_release/src/caffe/layers/smooth_l1_loss_layer.o] Error 1      do you have any idea what this could be? should i modify the .hpp file to add some new things?  any help is appreciated!  Thanks!!"
"the `ohem` layers in `m1@ssh_ohem` shares weight with `m1@ssh` and only provide scores for `anchor_target_layer`, can we remove these layers and use scores from `m1@ssh`. It seems the same.    !    "
"Hi, when I ran 'make all' to compile ssh-caffe, error as follows:    > /usr/local/cuda/include/cuda_fp16.h(3068): error: calling a constexpr __host__ function(""isinf"") from a __device__ function(""__float_simpl_sinf"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.  >   > /usr/local/cuda/include/cuda_fp16.h(3079): error: calling a constexpr __host__ function(""isinf"") from a __device__ function(""__float_simpl_cosf"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.  >   > 2 errors detected in the compilation of ""/tmp/tmpxft_00002bba_00000000-19_contrastive_loss_layer.compute_61.cpp1.ii"".  > make: *** [.build_release/cuda/src/caffe/layers/contrastive_loss_layer.o] 错误 1  > make: *** 正在等待未完成的任务....  > nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).  > /usr/local/cuda/include/cuda_fp16.h(3068): error: calling a constexpr __host__ function(""isinf"") from a __device__ function(""__float_simpl_sinf"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.  >   > /usr/local/cuda/include/cuda_fp16.h(3079): error: calling a constexpr __host__ function(""isinf"") from a __device__ function(""__float_simpl_cosf"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.  >   > 2 errors detected in the compilation of ""/tmp/tmpxft_00002bc4_00000000-19_exp_layer.compute_61.cpp1.ii"".  > make: *** [.build_release/cuda/src/caffe/layers/exp_layer.o] 错误 1  > nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).  > /usr/local/cuda/include/cuda_fp16.h(3068): error: calling a constexpr __host__ function(""isinf"") from a __device__ function(""__float_simpl_sinf"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.  >   > /usr/local/cuda/include/cuda_fp16.h(3079): error: calling a constexpr __host__ function(""isinf"") from a __device__ function(""__float_simpl_cosf"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.  >   > 2 errors detected in the compilation of ""/tmp/tmpxft_00002c1a_00000000-19_im2col_layer.compute_61.cpp1.ii"".  > make: *** [.build_release/cuda/src/caffe/layers/im2col_layer.o] 错误 1  > nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).  > /usr/local/cuda/include/cuda_fp16.h(3068): error: calling a constexpr __host__ function(""isinf"") from a __device__ function(""__float_simpl_sinf"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.  >   > /usr/local/cuda/include/cuda_fp16.h(3079): error: calling a constexpr __host__ function(""isinf"") from a __device__ function(""__float_simpl_cosf"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.  >   > 2 errors detected in the compilation of ""/tmp/tmpxft_00002c27_00000000-19_slice_layer.compute_61.cpp1.ii"".  > make: *** [.build_release/cuda/src/caffe/layers/slice_layer.o] 错误 1  > nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).  > /usr/local/cuda/include/cuda_fp16.h(3068): error: calling a constexpr __host__ function(""isinf"") from a __device__ function(""__float_simpl_sinf"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.  >   > /usr/local/cuda/include/cuda_fp16.h(3079): error: calling a constexpr __host__ function(""isinf"") from a __device__ function(""__float_simpl_cosf"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.  >   > 2 errors detected in the compilation of ""/tmp/tmpxft_00002c7e_00000000-19_crop_layer.compute_61.cpp1.ii"".  > make: *** [.build_release/cuda/src/caffe/layers/crop_layer.o] 错误 1  > nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).  > /usr/local/cuda/include/cuda_fp16.h(3068): error: calling a constexpr __host__ function(""isinf"") from a __device__ function(""__float_simpl_sinf"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.  >   > /usr/local/cuda/include/cuda_fp16.h(3079): error: calling a constexpr __host__ function(""isinf"") from a __device__ function(""__float_simpl_cosf"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.  >   > 2 errors detected in the compilation of ""/tmp/tmpxft_00002ce6_00000000-19_split_layer.compute_61.cpp1.ii"".  > make: *** [.build_release/cuda/src/caffe/layers/split_layer.o] 错误 1  > nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).  > /usr/local/cuda/include/cuda_fp16.h(3068): error: calling a constexpr __host__ function(""isinf"") from a __device__ function(""__float_simpl_sinf"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.  >   > /usr/local/cuda/include/cuda_fp16.h(3079): error: calling a constexpr __host__ function(""isinf"") from a __device__ function(""__float_simpl_cosf"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.  >   > 2 errors detected in the compilation of ""/tmp/tmpxft_00002cf3_00000000-19_hdf5_data_layer.compute_61.cpp1.ii"".  > make: *** [.build_release/cuda/src/caffe/layers/hdf5_data_layer.o] 错误 1  > nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).  > /usr/local/cuda/include/cuda_fp16.h(3068): error: calling a constexpr __host__ function(""isinf"") from a __device__ function(""__float_simpl_sinf"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.  >   > /usr/local/cuda/include/cuda_fp16.h(3079): error: calling a constexpr __host__ function(""isinf"") from a __device__ function(""__float_simpl_cosf"") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.  >   > 2 errors detected in the compilation of ""/tmp/tmpxft_00002d42_00000000-19_scale_layer.compute_61.cpp1.ii"".  > make: *** [.build_release/cuda/src/caffe/layers/scale_layer.o] 错误 1    Any help?Thanks"
"what if I want to write this?  `layer{`    `name: ""m3@ssh_output_ohem""`   ` type: ""Concat""`   ` bottom: ""m3@ssh_3x3_output_ohem""`    `bottom: ""m3@ssh_5x5_output_ohem""`    `bottom: ""m3@ssh_7x7_output_ohem""`    `top: ""m3@ssh_output_ohem""`    `concat_param{`        `axis: 1`    `}`    `propagate_down: false`    `propagate_down: false`    `propagate_down: false`  `}`  There are three#  **`propagate_down: false`**, and python can not receive the duplicate args.  How to write in python?      `name = 'm3@ssh_output_ohem'`      `net[name] = L.Concat(net['m3@ssh_3x3_output_ohem'], net['m3@ssh_5x5_output_ohem'],                            net['m3@ssh_7x7_output_ohem'], propagate_down=False,propagate_down=False,                           propagate_down=False,                           name='m3@ssh_output_ohem',                           concat_param=dict(axis=1))`"
"hi,     I want to know how much GPU memory we need when test the model? A problem has been encountered, ""Check failed: error == cudaSuccess (2 vs. 0)  out of memory"", our GPU is k80. I find the model just about 70M and just test one image, i don't why when i execute forwatd_net the error occured."
"After runnnig make in libs, i get this error in demo.py     I don't have caffe installed. Is it because of that or something else?  "
"Hi, besides VGG-16, have you tried any other Networks, such as ResNet, DenseNet or DualPathNet?  I have tried resnet101 and densenet92 by replacing the corresponding layers in vgg16. It's a pity these results are not good enough. Do you have any good suggestions?"
"hi, because you import nccl in train.py, when i debug anchor_target_layer.py, it reminds me that BdbQuit. So how can i use python -m pdb to debug it?"
"hi, i trained the model is more than 90m, and i use the demo.py to check the caffemodel i trained,it detect nothing,what is the problem .thanks"
Is there TensorFlow version?   who can share to me ?  thanks
"I compared the model described in the train_ssh.prototxt and test_ssh.prototxt, found that train_ssh.prototxt has extra layers such as m3@ssh_dimred_ohem, m3@ssh_7x7-1_ohem   ... Anyone else knows why?   @mahyarnajibi thanks for your excellent work."
"when i run the paper and the code.  i am confused about the batch size.  in the default_config.yml file, i saw the TRAING BATCH SIZE, which equals to 128.  and the default iter_size in the solver_ssh.prototxt is 2.  But in paper, the 4 GPUs are using a mini-batch of 4 images?    so, what's the number of mini-batch ?"
"I used the model by our training to test and no face frame is detected.  In the process of training, the loss of   layer m3@ssh_reg_loss is zero. I have no idea about it. Any help?"
"Thank you for your great work.  I compiled the caffe-ssh only on cpu successfully, but when i ran `import caffe`, encountered error:Segmentation fault: 11.  systerm: macos  Can you give me some advices?        "
"Thank you for your great job.  I tried to train my own model, when i ran `python main_train.py --gpus 2,3`, get the error below:       15:03:44.585690 12330 net.cpp:255] Network initialization done.  I0716 15:03:44.586588 12330 solver.cpp:56] Solver scaffolding done.  Loading pretrained model weights from data/imagenet_models/VGG16.caffemodel  Loading pretrained model weights from data/imagenet_models/VGG16.caffemodel  F0716 15:03:45.152788 12330 upgrade_proto.cpp:95] Check failed: ReadProtoFromBinaryFile(param_file, param) Failed to parse NetParameter file: data/imagenet_models/VGG16.caffemodel  F0716 15:03:45.152806 12331 upgrade_proto.cpp:95] Check failed: ReadProtoFromBinaryFile(param_file, param) Failed to parse NetParameter file: data/imagenet_models/VGG16.caffemodel  *** Check failure stack trace: ***  *** Check failure stack trace: ***      @     0x7fb0e5dfa5cd  google::LogMessage::Fail()      @     0x7fb0e5dfa5cd  google::LogMessage::Fail()      @     0x7fb0e5dfc433  google::LogMessage::SendToLog()      @     0x7fb0e5dfc433  google::LogMessage::SendToLog()      @     0x7fb0e5dfa15b  google::LogMessage::Flush()      @     0x7fb0e5dfa15b  google::LogMessage::Flush()      @     0x7fb0e5dfce1e  google::LogMessageFatal::~LogMessageFatal()      @     0x7fb0e5dfce1e  google::LogMessageFatal::~LogMessageFatal()      @     0x7fb0e68e92c1  caffe::ReadNetParamsFromBinaryFileOrDie()      @     0x7fb0e68e92c1  caffe::ReadNetParamsFromBinaryFileOrDie()      @     0x7fb0e69549ea  caffe::Net ::CopyTrainedLayersFromBinaryProto()      @     0x7fb0e69549ea  caffe::Net ::CopyTrainedLayersFromBinaryProto()      @     0x7fb0e6954ad7  caffe::Net ::CopyTrainedLayersFrom()      @     0x7fb0e6954ad7  caffe::Net ::CopyTrainedLayersFrom()      @     0x7fb0e705c508  boost::python::objects::caller_py_function_impl ::operator()()      @     0x7fb0e705c508  boost::python::objects::caller_py_function_impl ::operator()()      @     0x7fb0e58bb5cd  boost::python::objects::function::call()      @     0x7fb0e58bb5cd  boost::python::objects::function::call()      @     0x7fb0e58bb7c8  (unknown)      @     0x7fb0e58bb7c8  (unknown)      @     0x7fb0e58c3613  boost::python::handle_exception_impl()      @     0x7fb0e58c3613  boost::python::handle_exception_impl()      @     0x7fb0e58b8999  (unknown)      @           0x4c15bf  PyEval_EvalFrameEx      @           0x4b9ab6  PyEval_EvalCodeEx      @           0x4d55f3  (unknown)      @           0x4eebee  (unknown)      @           0x4ee7f6  (unknown)      @           0x4aa9ab  (unknown)      @           0x4c15bf  PyEval_EvalFrameEx      @           0x4b9ab6  PyEval_EvalCodeEx      @     0x7fb0e58b8999  (unknown)      @           0x4d55f3  (unknown)      @           0x4a577e  PyObject_Call      @           0x4bed3d  PyEval_EvalFrameEx      @           0x4c15bf  PyEval_EvalFrameEx      @           0x4c136f  PyEval_EvalFrameEx      @           0x4b9ab6  PyEval_EvalCodeEx      @           0x4c136f  PyEval_EvalFrameEx      @           0x4b9ab6  PyEval_EvalCodeEx      @           0x4d55f3  (unknown)      @           0x4d54b9  (unknown)      @           0x4eebee  (unknown)      @           0x4eebee  (unknown)      @           0x4ee7f6  (unknown)      @           0x4ee7f6  (unknown)      @           0x4aa9ab  (unknown)      @           0x4aa9ab  (unknown)      @           0x4c15bf  PyEval_EvalFrameEx      @           0x4b9ab6  PyEval_EvalCodeEx      @           0x4c15bf  PyEval_EvalFrameEx      @           0x4d55f3  (unknown)      @           0x4c136f  PyEval_EvalFrameEx      @           0x4a577e  PyObject_Call      @           0x4bed3d  PyEval_EvalFrameEx      @           0x4c136f  PyEval_EvalFrameEx      @           0x4c136f  PyEval_EvalFrameEx      @           0x4b9ab6  PyEval_EvalCodeEx      @           0x4d54b9  (unknown)      @           0x4eebee  (unknown)      @           0x4ee7f6  (unknown)      @           0x4aa9ab  (unknown)      @           0x4c15bf  PyEval_EvalFrameEx      @           0x4c136f  PyEval_EvalFrameEx  done solving!    I used the latest version of caffe-ssh.  can you give me some advices?"
"when i run python main_train.py --gpus 0,1,2,3. I have the problem   `Traceback (most recent call last):    File ""main_train.py"", line 63, in        max_iter=args.iters, gpus=gpus)    File ""/home/lavi/deepcode/face/SSH/SSH/train.py"", line 167, in train_net      uid = caffe.NCCL.new_uid()  AttributeError: type object 'NCCL' has no attribute 'new_uid'  `  How to solve it? Please help me."
None
"Hi @po0ya and @mahyarnajibi ,    Firstly, thank you for the discussion in my previous post earlier! I definitely benefited from learning from you!     I have a question regarding channel reduction and would be glad if you can share your suggestions. In the paper, it was written, ""...to decrease the memory consumption of the model, the number of channels in the feature map is reduced from 512 to 128 using 1 x 1 convolutions.""  I am trying to run an experiment when only feature map fusion is done (without reducing channels). However, by doing so, I encountered exploded gradient. Of course, one possible direction is to reduce learning rate but I try not to do that yet as I suspect I may have implemented prototxt incorrectly or I might have missed out something that I did not realize... may I know if  _nan_ loss was encountered too in your experiment when channels are not reduced?               Prototxt modification is as follows:     "
"hi, I tested one picture based on your pretrain model and compare the result with Tiny Face Detector, CVPR 2017(  In your paper, the performance is better than Tiny face,but it seems that the performance is not better than Tiny face. I am wonder if  anything wrong with the way I use the model Or something I need pay more attention when I use the model? Thank you very much."
"when I run ""download_ssh_model.sh"",it return   Length: 73481470 (70M), 530494 (518K) remaining [application/gzip]  Saving to: ‘./data/SSH_models.tar.gz’    ./data/SSH_models.tar.gz                100%[++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++>]  70.08M  28.0KB/s    in 23s         2018-06-08 08:04:51 (22.8 KB/s) - ‘./data/SSH_models.tar.gz’ saved [73481470/73481470]    Done!  Unzipping the file...  SSH_models/  SSH_models/SSH.caffemodel    gzip: stdin: invalid compressed data--format violated  tar: Unexpected EOF in archive  tar: Unexpected EOF in archive  tar: Error is not recoverable: exiting now  Cleaning up...  All done!      the model cannot be extraced, could you  please help me? or upload the model to another Network disk?  Thank you  "
"Dear @mahyarnajibi and @po0ya, I was wondering if during implementation, have you guys experimented adding another detection layer at conv3_3 which has feat_stride: 4? Since wider face consists a lot of small faces. If yes, could you please share some insights? many many thanks."
"Hi,  I have encounter several questions when training on one gpu.  1. Where to change learning rate? I cant find learning rate para in config files.  2. I keep encounter this error:  Traceback (most recent call last):    File ""main_train.py"", line 63, in        max_iter=args.iters, gpus=gpus)    File ""/SSH/SSH/train.py"", line 167, in train_net      uid = caffe.NCCL.new_uid()  AttributeError: type object 'NCCL' has no attribute 'new_uid'  Thanks"
"Using the .sh file to get the model,end up with a error and a less than 30M caffe.model that can't be parsed,bellow is the info.  ./data/SSH_models.tar.gz                     100%[+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++=>]  70.08M  56.1KB/s    in 38s         2018-03-20 21:13:17 (25.3 KB/s) - ‘./data/SSH_models.tar.gz’ saved [73481470/73481470]    Done!  Unzipping the file...  SSH_models/  SSH_models/SSH.caffemodel    gzip: stdin: invalid compressed data--format violated  tar: Unexpected EOF in archive  tar: Unexpected EOF in archive  tar: Error is not recoverable: exiting now  Cleaning up...  All done!    Also I try to manually downloaded the .tar.gz file ,no way to unzip it ,both Windows and  linux failed; "
Seems that the download link is broken. Anyone have a copy to share?
"Hi Guys,    I was wondering what GPU gpu card did you used in your experiments? I am using a GTX 970M. Even tried several configurations (/SSH/Configs) but none of them worked and I still finding out of memory error from cuda. I am using cuda 8.0 and cudnn 6.0.    #################### Configuration ####################  USE_GPU_NMS:  True  DATA_DIR:  /home/bockholt/probes/ssh-faceDetector/data  GPU_ID:  0  EPS:  1e-14  DEBUG:  False  MATLAB:  matlab  TEST{      NMS_THRESH:  0.3      SCALES:  [1200]      ANCHOR_MIN_SIZE:  0      N_DETS_PER_MODULE:  1000      PYRAMID_BASE_SIZE:  [800, 1200]      ORIG_SIZE:  False      MAX_SIZE:  1600  }  ROOT_DIR:  /home/bockholt/probes/ssh-faceDetector  EXP_DIR:  ssh  PIXEL_MEANS:  [[[102.9801 115.9465 122.7717]]]  RNG_SEED:  3  #######################################################  Loading the network...Done!  WARNING: Logging before InitGoogleLogging() is written to STDERR  F0119 16:57:22.624498  4045 syncedmem.cpp:71] Check failed: error == cudaSuccess (2 vs. 0)  out of memory  *** Check failure stack trace: ***  Aborted (core dumped)    "
"Hi,      Here's some problems may exist.      1. The total loss doesn't equal to the sum of all loss when training.      2. the reg_loss of m3 is 0 while the cls_loss of m3 is not.      Can you explain these?"
"@mahyarnajibi,  I tried detection using the latest code with model in download_ssh_model.sh. On WIDER validation set I got 92.5/90.9/73.7 respectively for easy/medium/hard subset. That is not consistent with results in paper: 91.9/90.9/81.4. Particularly for hard subset, the gap(81.4 vs. 73.7) is significant.  I also trained the model from scratch, the detection performances are 92.3/90.5/72.7. Result on hard subset differ much from 81.4.  Is there any specific setting for hard subset I am missing?  thank!"
"hi, sth bothers me is that when i make some changes in ur code (these changes work in other face detection network), the performance drops.  any tricks in ur code?  and i wonder why iter=21000, the model achieves the best performance. above 21000, it drops.    hope i can get ur reply asap."
"Hi， I use your released model make a evaluation on the FDDB datasets, I got a Disc curve precision 90 at False Positive 400. I dont know why.  !   "
"to detect the demo.jpg ,how much GPU needed?"
"After following your installation procedure, I tried to run `python demo.py`. Unfortunately, this crashes with the following error:       Can you reproduce this error?"
"hi Mahyar,    I've tried to achieve your paper before you release your code, but i didn't get the accuracy as you got. Mabye some details made my accuracy fall， I'm trying to figure it out. I see there is a 'con5_128_crop' layer in your net. What's work it does? How about if I remove it?     best regards!    "
Hi Mahyar:   what's the values of PYRAMID_BASE_SIZE and SCALES when evaluate FDDB？    thanks.
"Hello, I down the VGG16.caffemodel with the download_imgnet_model.sh,when I run the main_train.py, it comes with the error：   Error parsing text-format caffe.NetParameter: 849:24: Message type ""caffe.LayerParameter"" has no field named ""smooth_l1_loss_param"".  How can I deal with it，Thanks very much for your nice code and your help"
"I downloaded your data set, but there are no image in it."
Could you tell me how to calculate the loss of vp ？What method is better?
"Thanks for providing the dataset.  But, I have a few questions, how can I extract the mat file to get the jpg or jpeg image?  Besides, could you please provide the camera intrinsic and extrinsic parameters?  If so, it's really helpful for my research.  Thank you very much!"
"<img width=""667"" alt=""Screen Shot 2021-04-19 at 04 54 49"" src=""   "
"As per readme, there are steps for training on caltech and VPGNet Dataset.  Are there any scripts/steps written to run inference on a dataset using trained model."
"Dear authors,  #1 Can you show me how to perform training on your VPGNet dataset?   I don't know how to generate the dataset_list file of your VPGNet dataset in order to run the make_lmbd.sh    #2 The Caltech Lanes dataset seems to be removed from the official website. Can you share a snapshot of the dataset.    Thank you very much   "
"I am implementing VPGNET in keras. Task 2,3,4 are classification tasks, so they are learning and inferring okay. But grid box regression task isn't learning. It has a constant loss. I am using linear activation function on the last layer and using Mean Absolute error L1 loss function for bounding box regression task as per paper. I am not normalising any bounding box coordinates, so their range is 0-640 (width) and 0-480 height. The training MAE loss starts off really high, 17.5 to be exact and then stays there."
"VPGNet dataset is available.  Please fill out a   for the download link, and check the README.  Thanks."
"### Issue summary  Make runtest failed on Ubuntu 16.04 and CUDA 10.0    > *** Aborted at 1573612683 (unix time) try ""date -d @1573612683"" if you are using GNU date ***  > PC: @     0x7fad4225218e caffe::CuDNNConvolutionLayer ::LayerSetUp()  > *** SIGFPE (@0x7fad4225218e) received by PID 27175 (TID 0x7fad4d5b9740) from PID 1109729678; stack trace: ***  >     @     0x7fad41776390 (unknown)  >     @     0x7fad4225218e caffe::CuDNNConvolutionLayer ::LayerSetUp()  >     @           0x4a10b4 caffe::Layer ::SetUp()  >     @           0x7b36d8 caffe::CuDNNConvolutionLayerTest_TestSimpleConvolutionCuDNN_Test ::TestBody()  >     @           0x88cc8c testing::internal::HandleSehExceptionsInMethodIfSupported ()  >     @           0x887d99 testing::internal::HandleExceptionsInMethodIfSupported ()  >     @           0x8736d4 testing::Test::Run()  >     @           0x873ec6 testing::TestInfo::Run()  >     @           0x874511 testing::TestCase::Run()  >     @           0x8799bf testing::internal::UnitTestImpl::RunAllTests()  >     @           0x88de69 testing::internal::HandleSehExceptionsInMethodIfSupported ()  >     @           0x888a4a testing::internal::HandleExceptionsInMethodIfSupported ()  >     @           0x8785ce testing::UnitTest::Run()  >     @           0x496d9b main  >     @     0x7fad413bb830 __libc_start_main  >     @           0x496429 _start  >     @                0x0 (unknown)  > make: *** [Makefile:478: runtest] Floating point exception (core dumped)  >     ### Steps to reproduce  make clean  make all -j16  make test -j16  make runtest    ### Tried solutions  modify the makefile.config       ### System configuration    * Operating system: Ubuntu 16.04  * Compiler: GNU make 4.2  * CUDA version (if applicable): CUDA 10.0  * CUDNN version (if applicable): CUDNN 7.6.2  * BLAS: ATLAS  * Python version (if using pycaffe): python 3.5  * MATLAB version (if using matcaffe): R2018b  "
"The output size of multi label is 60 * 80, but the output size of grid box and object mask is 120 * 180. Can you explain the reason for this setting and how to generate grid box and object box through 8 * 8 grid? Thank you again for your work!"
"Hi @SeokjuLee,    I have been using VPGNet to do some lane detection on our own dataset. For the test loss, we are getting around 3.5. I'm just curious what would be a good number for the test loss?     Thanks!"
"Hi,    I'm a little confused since most of the files are solely about caffe. Is there a way to test the vpgnet on an image or video from other data sets (kitti, culane, etc)? If so, which script would I run, with the image/video as a file input?"
"Our **ENet-Label-Torch** has been released. More details can be found in  .    Key features:    (1) ENet-label is a **light-weight** lane detection model based on   and adopts **self attention distillation** (more details can be found in our paper which will be published soon).    (2) It has **20** × fewer parameters and runs **10** × faster compared to the state-of-the-art SCNN, and achieves **72.0** (F1-measure) on CULane testing set (better than SCNN which achieves 71.6).    (**Do not hesitate to try our model!!!**)    Performance on CULane testing set (F1-measure):    |Category| |SCNN-Tensorflow| |  |:---:|:---:|:---:|:---:|  |Normal|90.6|90.2|**90.7**|  |Crowded|69.7|71.9|70.8|  |Night|66.1|64.6|65.9|  |No line|43.4|45.8|44.7|  |Shadow|66.9|73.8|70.6|  |Arrow|84.1|83.8|**85.8**|  |Dazzle light|58.5|59.5|**64.4**|  |Curve|64.4|63.4|**65.4**|  |Crossroad|1990|4137|2729|  |Total|71.6|71.3|**72.0**|  |Runtime(ms)|133.5|--|**13.4**|  |Parameter(M)|20.72|--|**0.98**|"
None
"Hi seokju,    I have similar problem like the picture below  !   Even though I tried SCNN but I believe the postprocessing step is similar.  below is my result  !   the model confuse itself to see points from different lanes. Thus I want to cluster all points again using IPM to seperate the points near VP.    May i know which 4 src points did you use in cv2.getperspective() function. Since the prediction of mine has a large variety, I have some trouble finding these 4 points which encloses all points found.  Could you share some insight how you find these 4 points ?    Best regards,    ZX"
"CXX/LD -o .build_release/tools/compute_image_mean.bin  .build_release/lib/libcaffe.so: undefined reference to `void caffe::hdf5_load_nd_dataset (int, char const*, int, int, caffe::Blob *)'  .build_release/lib/libcaffe.so: undefined reference to `void caffe::hdf5_load_nd_dataset (int, char const*, int, int, caffe::Blob *)'  .build_release/lib/libcaffe.so: undefined reference to `caffe::hdf5_get_name_by_idx '  .build_release/lib/libcaffe.so: undefined reference to `void caffe::hdf5_save_nd_dataset (int, std::__cxx11::basic_string , std::allocator  > const&, caffe::Blob  const&, bool)'  .build_release/lib/libcaffe.so: undefined reference to `caffe::hdf5_get_num_links(int)'  .build_release/lib/libcaffe.so: undefined reference to `void caffe::hdf5_save_nd_dataset (int, std::__cxx11::basic_string , std::allocator  > const&, caffe::Blob  const&, bool)'  collect2: error: ld returned 1 exit status  Makefile:568: recipe for target '.build_release/tools/compute_image_mean.bin' failed  make: *** [.build_release/tools/compute_image_mean.bin] Error 1  "
Hello everyone.    I would like to know how I can visualize the output of the multi-label with an example image like it is done here:         !   
"Hi, SeokjuLee,  Recently, I am also doing some job about lane and road mask detection and reconnition. I am  so amazing about your job about it.  Now I have a question to ask you for some help. How do you annotate you trainning data set ?   what you steps are? "
"I have implemented SCNN using Tensorflow and put the full codes  . You can test the code in popular lane detection benchmarks like TuSimple, CULane and BDD100K or your custom dataset with minor modification. Welcome to raising issues if you have problems in reproducing the results. My code is based on   and  ."
"In the data layer, layer output is list as below:  data -> data     32 3 480 640 (29491200)  data -> label    32 8 120 160 (4915200)  data -> type     32 1 60 80 (153600)    I know the shape of ""type"" is depended on ""catalog_resolution""   But I can't find out where shape of ""label"" is depended.  If I want to change the output shape of binary-mask, I should change shape of ""label"" firstly, how can I change ""data -> label    32 8 120 160 (4915200)""  to  ""data -> label    32 8 240 320 (4915200)""?"
How can I find each label name of 64 multi-labels?
as the paper said. can the first task VPP be provide?
"Hi, I want to run the make_lmdb.sh file, but I got the following errors:  make_lmdb.sh: line 3: ../../build/tools/convert_driving_data: No such file or directory  make_lmdb.sh: line 4: ../../build/tools/compute_driving_mean: No such file or directory  make_lmdb.sh: line 6: ../../build/tools/convert_driving_data: No such file or directory    I checked the directories and find out that there is no folder named build, and the tools folder is under caffe. So I want to ask what does ../../build mean?    BTW, I use windows so I run this file on Git.      "
"I have trained the net two times with similar data which has two different type labels .One of the two type is Lane,conclude broken yellow, broken white and so on.Another type is about roadmarking,conclude straight arrow,right arrow and so on.But the test results were so strange.The lane detection result was good,such as pic 1.But the detection of roadmarking was  beyond my expectation，ａlmost nothing cｏｕｌｄ be detected，such as pic 2. I want to know why are there such a big difference. Can you explain the reason for me？ Thanks a lot @SeokjuLee   !   ｐｉｃ　１  !   ｐｉｃ　２"
"@SeokjuLee     VPGNet/caffe/tools/convert_driving_data.cpp        [line 115]    int resize_height = std::max (0, FLAGS_resize_height);   ///480+32=512   int resize_width = std::max (0, FLAGS_resize_width);       ///640+32=672      original  images are  resized to  672*512, should coordinates of lines  be  reseized also??    [line 96]             lines.push_back(std::make_pair(filename, boxes));  "
"Hello,I have finished train and test. Here is a output.log of test.How can I visualize test results without post-process."
Is the caffe used in this project different from the standard caffe?(do your add some special layers?)  can i replace the caffe in this project with the newest caffe? 
"I am now doing the evaluation of the results ,in your paper you said: we compute the minimum distance from the center of each cell to the sampled lane points for every cell. If the minimum distance is within the boundary R, we mark these sampled points as true positive and the corresponding grid cell as detected.  how about the R and the class FP?"
"I have trained VPGNet, but i don't know how to validate the codel by visualizing outputs. Do you smaple code to work on each image and visualize lane detection output? Kindly shared the code if you have."
"I have trained a caffemodel from your tutorial, but the test result is bad.  I think it is because of little dataset. Could you please tell me where I can download the lane and road marking benchmark which consists of about 20,000 images or give me the caffemodel described in your paper? Thank you very much!        "
"@SeokjuLee hi，In section 4.4 of this paper，how to understand the process of point sampling：“First, we subsample local peaks from the region where the probability of lane channels from the multi-label task is high.”，such as：  1、how to find ""the region""?  in one of the categories's feature map (60×80) or in original image (640×480)？  2、how to understand the peaks？what does its horizontal and vertical coordinates stand for？    thank you very much."
I tried to edit the make_lmdb.sh file as following:  ../../build/tools/convert_driving_data /media/aquarius/Backup/VPGNet/caltech-lanes-dataset/cordova1/ /media/aquarius/Backup/VPGNet/caltech-lanes-dataset/cordova1/list.txt LMDB_train  ../../build/tools/compute_driving_mean LMDB_train ./driving_mean_train.binaryproto lmd  ../../build/tools/convert_driving_data /media/aquarius/Backup/VPGNet/caltech-lanes-dataset/cordova2/ /media/aquarius/Backup/VPGNet/caltech-lanes-dataset/cordova2/list.txt LMDB_test  but it seem doesn't work.  Here is my output:  ./make_lmdb.sh   E1227 16:49:05.001368  4610 convert_driving_data.cpp:73] argv[1]: /media/aquarius/Backup/VPGNet/caltech-lanes-dataset/cordova1/  E1227 16:49:05.001581  4610 convert_driving_data.cpp:74] argv[2]: /media/aquarius/Backup/VPGNet/caltech-lanes-dataset/cordova1/list.txt  E1227 16:49:05.264470  4610 convert_driving_data.cpp:145] Total to be processed: 0.  F1227 16:49:05.354425  4617 compute_driving_mean.cpp:68] Unknown db backend lmd  *** Check failure stack trace: ***      @     0x7f38d4fba5cd  google::LogMessage::Fail()      @     0x7f38d4fbc433  google::LogMessage::SendToLog()      @     0x7f38d4fba15b  google::LogMessage::Flush()      @     0x7f38d4fbce1e  google::LogMessageFatal::~LogMessageFatal()      @           0x402c73  main      @     0x7f38d3ed9830  __libc_start_main      @           0x402f39  _start      @              (nil)  (unknown)  ./make_lmdb.sh: line 7:  4617 Aborted                 (core dumped) ../../build/tools/compute_driving_mean LMDB_train ./driving_mean_train.binaryproto lmd  E1227 16:49:05.457511  4619 convert_driving_data.cpp:73] argv[1]: /media/aquarius/Backup/VPGNet/caltech-lanes-dataset/cordova2/  E1227 16:49:05.457718  4619 convert_driving_data.cpp:74] argv[2]: /media/aquarius/Backup/VPGNet/caltech-lanes-dataset/cordova2/list.txt  E1227 16:49:05.694105  4619 convert_driving_data.cpp:145] Total to be processed: 0.  ---------------------------  Could you explain to me more details about this process? I am a newbie in caffe so it took me more than 3 days to fix this and nothing happened.  Thank you in advance.
"Here is an additional explanation about the lane post-processing. There are four steps basically. First, sampling seed points with the lane heat map from the multi-label task. Then, cluster the seeds in the IPM-ed coordinate with our clustering method (“We sequentially decide the cluster by the pixel distance. After sorting the points by the vertical index, we stack the point in a bin if there is a close point among the top of the existing bins. Otherwise, we create a new bin for a new cluster. By doing this, we can reduce the time complexity of the clustering.”). Lastly, we did polynomial line fitting for each cluster. If the cluster is near the VP, we include that VP while clustering for the stability. Related contents are described in our paper, Section 4.4.  ! "
"Hi, I found in your paper you declared your algorithm speed could achieve to 20Fps, so I am confused whether the speed you declared is just the speed of the network inference time or with the post process included clustering and curve fitting? If it is with the post-process, could you  share your post process code? Thanks for your good job!"
"Please ask installation, training and test issues in this panel."
"Hi everyone  I want to try to redo SeokjuLee's VPGNet Project. I try to compile the caffe and i failed. But when i try to compile caffe from   i found no problem and success. I figure the problem is because some modification in SeokjuLee's Caffe. there are some variables that are not declared so the compiling is failed. the content of some file also different from BVLC's Caffe  I'm using CUDA toolkit 10, Opencv 4, and Python 3.8  For anyone who have tried to redo this project, could you send me your caffe file? or is it because different version of my Opencv?"
does somebody have get the dataset and pretrained model ? 
Hi @SeokjuLee     Thanks you for your wonderful works.    Would you mind posting a license? (such as apache-license-2.0) I would like to use your code in my work.    Thank you very much.
!       I would like to know which version of cudnn you are using???    !   
None
"Hi  @SeokjuLee ,  Thanks you for your amazing works.    Would you mind releasing the model for caltech-lane dataset or your dataset ?  I would like to use it as pre-trained model in my personal side-project. Please~ :)"
"Hi, SeokjuLee  Thanks for your remarkable work.  I download your code from here and implemented training and deploying according to your Baseline Usage step by step completely.  I Compiled your caffe successfully on my ubuntu16.04.   I used all of your Caltech Lanes Dataset for training.  And of course no any change of your training and deploying prototxt and any other files in VPGnet.    But the 64X60X80 output of my 'multi-label' was not able to detected anything.  The first 60X80 channel of  'multi-label' is the background class. Each elements in this channel is above 0.999 at least (training for 3000 steps). And I know it's meaning that nothing to be detected by VPGnet.  Nothing to be detected, but by the training log, the pixel-acc and type-acc are all above 0.98. Does it meaning that Label data is nothing? But the training data preparation was according to your Baseline Usage step by step completely.    I have confused by this problems for several days, is there anything I missed?   I tried training and deploying according to your Baseline Usage step by step completely and didn't changed any files in VPGnet.    Did you meet this kind of problem? Or is there something missed in my implementation?"
which program can be use to test picture
"I was able to run make_lmdb.sh with cuda8, however, this shell gives me this error when running with cuda 9:  ~~~  E0731 17:34:52.597481 20328 common.cpp:103] Cannot create Cublas handle. Cublas won't be available.  E0731 17:34:52.653252 20328 common.cpp:110] Cannot create Curand generator. Curand won't be available.  ~~~  I'm not sure if this is the problem of cuda version, or something else.  This error alone does not prevent the shell from running, but it makes train.sh aborted:  ~~~  ./train.sh: line 1: 20354 Aborted                 (core dumped) ../../build/tools/caffe train --solver=./solver.prototxt >> ./output/output.log 2>&1  ~~~    Any thoughts?  Thank you!"
"src/caffe/layers/contrastive_loss_layer.cpp: In instantiation of ‘void caffe::ContrastiveLossLayer ::Forward_cpu(const std::vector *>&, const std::vector *>&) [with Dtype = float]’:  src/caffe/layers/contrastive_loss_layer.cpp:118:1:   required from here  src/caffe/layers/contrastive_loss_layer.cpp:56:30: error: no matching function for call to ‘max(float, double)’  "
"Several days ago, I reproduced the result of VPGNet and I put some frequently met installation problems here to facilitate other researchers:    1. Compile caffe:  If you use anaconda python, please uncomment the corresponding lines in Makefile.config (Line 55 ~ 58) and DO NOT uncomment Line 5 (# USE_CUDNN := 1) since current version of VPGNet does not use cudnn.    2. Run make_lmdb.sh:  If you meet problems about cudnn, please recompile caffe without cudnn. Besides, in my case, I need to write the absolute path of LMDB_train and LMDB_test in make_lmdb.sh.    Regards,  Cardwing"
"hello, @SeokjuLee ,when trying to bash make_lmdb.sh,it always reminds me of such mistakes. What should I do to solve them?  compute_driving_mean.cpp:58] Check failed: mdb_env_open(mdb_env, argv[1], 0x20000, 0664) == 0 (2 vs. 0) mdb_env_open failed"
I want to  know  how to test it? I just finish the train.Thanks.
"@SeokjuLee     How can I use my own dataset ? My dataset is some pictures with polygon labels (pixel-level , store in .json with the polygon's coordinates value ). I tried to figure out how you deal with the original pictures with labels, but I ... emmmm, I'm still confused with your implement of DriveData ...     So can I use my own data in a simple way ?    Thx a lot ~"
@SeokjuLee   My equipment configuration does not allow me to train a model and I'm very new to computer vision and deep-learning things. I will be very appreciate if there's a well-trained model available. Not mean to be lazy ^-^
"Hi,   I'm getting the below error while I try to train the model. I checked in internet, the sources say that it can happen because of multiple cuda versions. I delete other cudas and tried, the issue still persists. Can you please help me get through this?      F0402 17:15:39.522164 20718 math_functions.cu:422] Check failed: status == CURAND_STATUS_SUCCESS (201 vs. 0)  CURAND_STATUS_LAUNCH_FAILURE  *** Check failure stack trace: ***      @     0x7f4e3e0b35cd  google::LogMessage::Fail()      @     0x7f4e3e0b5433  google::LogMessage::SendToLog()      @     0x7f4e3e0b315b  google::LogMessage::Flush()      @     0x7f4e3e0b5e1e  google::LogMessageFatal::~LogMessageFatal()      @     0x7f4e3e80703a  caffe::caffe_gpu_rng_uniform()      @     0x7f4e3e80b2d2  caffe::DropoutLayer ::Forward_gpu()      @     0x7f4e3e7cfea2  caffe::Net ::ForwardFromTo()      @     0x7f4e3e7cffc7  caffe::Net ::ForwardPrefilled()      @     0x7f4e3e7f498d  caffe::Solver ::Step()      @     0x7f4e3e7f551a  caffe::Solver ::Solve()      @           0x40bbfb  train()      @           0x408408  main      @     0x7f4e3d53e830  __libc_start_main      @           0x408ab9  _start      @              (nil)  (unknown)  Aborted (core dumped)  "
"Hi,    Can you share a pretrained model, please?"
"@SeokjuLee Thanks for sharing this awesome work.  I have run the caffe version on a Tesla M40 GPU，CUDA version 8.0，cuDNN version v5. when compile  it，there is an error：  ./include/caffe/util/cudnn.hpp:124:41: error: too few arguments to function ‘cudnnStatus_t cudnnSetPooling2dDescriptor(cudnnPoolingDescriptor_t, cudnnPoolingMode_t, cudnnNanPropagation_t, int, int, int, int, int, int)’pad_h, pad_w, stride_h, stride_w));  solution：  1、Because the cuDNN head file is too old，so replace the file ./include/caffe/util/cudnn.hpp with /usr/local/cuda/include/cudnn.h  2、 editing ""caffe/cmake/Dependencies.cmake"", at line 29  replace list(APPEND Caffe_LINKER_LIBS ${HDF5_LIBRARIES}) with list(APPEND Caffe_LINKER_LIBS ${HDF5_LIBRARIES} ${HDF5_HL_LIBRARIES})"
"I am looking forward to meet the dataset. But I don't understand the grid-level annotations, what means?"
"I have a research on the lane detection , and do some work about it , could you open your code, that i can do some test on it , thank you !"
"Hello, thank you very much for your outstanding work!    My question is, the GT root node of the RHD data set in your paper is at the  position of the wrist. But the root node of the predicted hand pose is in the center of the palm.  So, how do you ensure the fairness of quantitative comparison?"
None
I would like to visualize a bounding box and visualize it on the full size input image. Is there a option to achieve that?  Thank you!
"Hi! I have ran your demo code which provides both 2D and normalized 3D coordinates. The first ones can be easily translated to pixel coordinates and be overlaed in the original image. Is there any way to do the same for the 3D coordinates? i.e. translate the coordinates to the pixel scale to overlap x,y values on the original image."
"According to   dataset home page, indices of index finger joints are 13 to 16.     However in `BinaryReaderDbSTB.py`, the `index_root_bone_length` is computed from 11 and 12th joint indices at this  .    Am I missing something?"
"Hi!  Thanks for you great work!  I was confused that how you get the weight folder which I directly download from the data you showed on Readme. when I run the run.py, it shows ""Loaded 102 variables from weights/posenet3d-rhd-stb-slr-finetuned.pickle"", ""Loaded 37 variables from weights/handsegnet-rhd.pickle"". I was confused how you get those pickle files, cause after I finish training, I cannot find any model saved like those pickle files.  Thank you     "
Hi.  Thanks for the great work!  Can this network detect both hands at a time?  I couldn't find a code that can pose estimate both hands simultaneously from run.py.  So I manually masked one side of image if both hands are present.  Would this be only option for detecting 2 hands? or would there be a more convenient way?  Thankyou
"Hello!  I'm trying to achieve the same results that you describe in your paper on the posenet stage when adding the STB dataset. However, the results are far from what you have achieved, and I can not find the reason why. I was hoping if you could enlighten me on this step.    After training with the RHD dataset using the pipeline you've published on `posenet_training.py`, I load `BinaryDbReaderST`B with the following parameters:    `dataset = BinaryDbReaderSTB(mode='training', batch_size=train_para['BATCH_SIZE'], shuffle=True, coord_uv_noise=True, hand_crop=True, crop_center_noise=True, use_wrist_coord=True)`    And proceed to run the session passing the tensors:    `_, loss_v = sess.run([train_op, loss])`    The `BinaryDbReaderSTB` class was not modified and I've processed the data using the scripts you provided.     I then proceed to evaluate the training, using:    `dataset = BinaryDbReaderSTB(mode='evaluation', shuffle=False, use_wrist_coord=True)`    When executing with `USE_RETRAINED=False`, the metrics are as expected:  `Average mean EPE: 18.581 pixels   `  However, when using my model trained with RHD+STB, the lowest mean EPE I got was ~40 pixels. Could you please point me to what I am forgetting?    I tried some ideas, as using different epochs combinations, tweaking the lr decay and different configurations on the data loader, but no effect.    Thank you for your attention"
"I try your create_db.m, I find uv coordinate is false in 'BB', how should I do?"
"Hi! I have a problem that while no hand in image, the script return weird coordinates, I think it can be resolved by setting the threshold on the result. Can you plz tell me how? "
"In create_db.m, i think you should also add ""anno_uv_l= anno_uv_l(1:2, :);"" in order to get 2x21 coordinate matrix for each sample, just like anno_uv_r. If the third dimension indicates the visibility of each keypoint shouldn't anno_uv_r be of the same shape?    "
"Hi @zimmerm   I have read your paper, and in supplementary material you wrote each section training procedure,     -  it means that you have trained each section separately not end to end?    - for example for training `pose prior net` , you use the ground-truth heatmaps as input to network to get `can pose & rot mat`?  -  for training the `posenet` you have used groundtruth bounding box around the hand ?     -  I mean you did not have end to end training you have trained each module separately ` (handseg, posenet, poseprior) ` with ground  truth input (not predictions from other modules as input) , right?       "
"Hi @zimmerm ,    in `RHD` dataset what does `visible` mean? does it mean to be occluded or not   Or does it mean the existence of hand (joint) in image?  "
"I saved frozen model, then converted to a transformed model optimized for inference using Tensorflow TransformGraph, but when I now try to inspect the .pb file created by TransformGraph, I get the following error       "
Asking because sometimes it can take a while for newer work to show prominently in google results 
"Hi @zimmerm,    Thanks  for your paper,   There is no a pretrained model, could you please you provide that?  "
how can the programmed detect two hands ?       thanks
"i saw in your training code only use the first batch of the dataset is used for training, why?  or just code is sample to show the idea, we need to write our own training loop? @zimmerm "
"Hello,  Thanks for your sharing.  I am working on an environment with only four 1080 titian GPUs, I am not sure whether it can be trained if it has other tasks running.  I am not sure whether the lmb-freiburg can provide a pre-trained model?  Thanks"
"Hello Team,    I tried to run the run.py file for the various images and the images that provided by you in the folder ""data"". But the execution time is more for each images. And processing image and getting result itself taking more time and its taking close to ""6 seconds per image"".    Even I tried tensorflow, and tensoflow-gpu and G3 AWS instance(It has below graphic card) , but no luck in execution time.    Graphic card details:           (Main goal is to explore this runi.py for live webcam video , but please help me reduce the execution time)    Please find installed packages:         Code where I am checking execution time in run.py:         Result:  for this below images        image_list.append('./data/img3.png')      image_list.append('./data/img4.png')      image_list.append('./data/img5.png')     "
"Hello!    I'm wokring on a academic project (bachelor) where I need to estimate a hands skeleton in 3D!  You work looks very interesting and I would like to use it as my backend for estimating hand skeletons.     I see it is built on top of tensorflow, so how difficult would this be to export to Java? E.g. Tensorflow Lite? I guess it still calls some of the native C++ functions.    Best regards,  Christian!    "
None
Does it takes video as input and yields output accordingly?
"I used another hand detector and keypoints detector. After that, I want to use your training_lifting.py to lift 2D to 3D coordinates. So, can I use only your posepriornetwork for 3D pose estimation. How can I do that?    And in your posenet network, what kinds of output of keypoints_map. As example, 21 keypoints for one image or all images. Can you show me your keypoints_map result as an example?"
"When running with ,,python  training_handsegnet.py""  it reported error:  ""  tensorflow.python.framework.errors_impl.InvalidArgumentError: Unsuccessful TensorSliceReader constructor: Failed to get matching files on ./weights/cpm-model-mpii: Not found: ./weights; No such file or directory.""    From the paper, some design idea come from open-pose, could you guide me where is the essential pre-downloaded model for training, e.g., cpm-model-mpii.    Thanks & Regards！  Neo"
"i tried to run on grey channel-averaged images and got nonsense results as compared to the color versions (my use case has only grey images). does this mean the model is fined tuned for caucasian hue, and won't work on grey images nor on people of color?"
"hi, thanks for sharing your code.   i'm confused that there are 42 key points in a frame even though the corresponding image in 'color' directory only shows one hand  in fact, i'm creating my own data, but how could i get 42 points by only one hand contained image ?"
"Hello,  what does the forward pass weights contains? Was it trained with the whole RHD dataset for handsegnet and posenet? Because it says minimal example should I download the dataset  and retrain all?"
"when I use only the model ""'./weights/posenet3d-rhd-stb-slr-finetuned.pickle'"",   I got one error ""ValueError: cannot reshape array of size 1049600 into shape (2562,512)""  Hmmm, 2562 x 512 is 1311744 and 1049600 = 2050 x 512. What should I do?  It seems to be related to PosePrior/fc_rel0/weights's shape "" 2050, 512 = 1049600.  I think I should call _inference_pose3d() as PosePrior, ViewPointNet's parameters are contained in the pickle and it was loaded already.     I don't use HandSegNet as below;      def inference(self, image, hand_side, evaluation):                    # detect keypoints in 2D          keypoints_scoremap = self.inference_pose2d(image)           keypoints_scoremap = keypoints_scoremap[-1]           # estimate most likely 3D pose          keypoint_coord3d = self._inference_pose3d(keypoints_scoremap, hand_side, evaluation)           # upsample keypoint scoremap           s = image.get_shape().as_list()          keypoints_scoremap = tf.image.resize_images(keypoints_scoremap, (s[1], s[2]))           return image, keypoints_scoremap      Can anybody give me any hint? Please help me ;/  "
"I would like to start to port it to caffe(My goal is to implement just forward passes, not for training).    I have read your paper roughly and I wonder there are some custom layers which are not included in the existing TF or caffe layers.     Plus, I wonder, HandSegNet can be replaced with SSD hand detector as I thought HandSegNet is just for detecting hands and after that the hand-cropped patchs from the original image(not from the feature map of the last layer of the HandSegNet) is transferred to PoseNet.   "
can I get any inference speed measurments? can it be poosible to run on TX2 in real time?
"Thanks for your works.  I have trained on tensorflow successfully, Do you have the trained caffe model?  If you do not have the plan to convert the model to caffe, do you have some suggestions that I should pay attention when  trained on caffe?  "
"Hey,I read the handsegnet part of your code,but after that ,I have a question.That is: in your paper,you said,  ""Our HandSegNet is a smaller version of the network from Wei et al. [19] trained on our hand pose dataset. "".  But in your code,I think you just did the first half part like the method in ""Fully Convolutional Networks for Semantic Segmentation""(FCN),the hand masks is not the same as heatmaps,at least in my opinion.Is my understanding right?  "
is there a way to extract your predicted keypoints
"run.py contains code like this:     The input image is ./data/img.png. but I get keypoint_coord3d_v like this:     The distance between wrist and thumb tip (keypoint_coord3d_v[0] and keypoint_coord3d_v[1]) is 2.160583, a weird number. What's the unit?? millimeter and meter both are not correct.   Does it need a conversion?      Thanks    --      "
I’d like to run hand3d using deeplearnjs but they accept .ckpt files see      Do you have those files or a script to convert the Pickle files to ckpt?  
"hand3d-master/nets/ColorHandPose3DNetwork.py"", line 52, in init      assert os.path.exists(file_name), ""File not found.""  AssertionError: File not found.      Missing weight files : ./weights/handsegnet-rhd.pickle', './weights/posenet3d-rhd-stb-slr-finetuned.pickle'  "
"With your RHD DB, you have mask images from color image. Could you tell me how to make mask image?  I would like to make the similar dataset from the custom dataset as well."
"Hi,    I extracted data in the root folder. I am getting the following error.    File ""run.py"", line 47, in        keypoints_scoremap_tf, keypoint_coord3d_tf = net.inference(image_tf, hand_side_tf, evaluation)    File ""/home/alex/dev/projects/hand3d-master/net.py"", line 37, in inference      hand_mask = single_obj_scoremap(hand_scoremap)    File ""/home/alex/dev/projects/hand3d-master/utils.py"", line 246, in single_obj_scoremap      max_loc = find_max_location(scoremap_fg)    File ""/home/alex/dev/projects/hand3d-master/utils.py"", line 228, in find_max_location      xy_loc.append(tf.concat(0, [x_loc, y_loc]))    File ""/home/alex/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 1062, in concat      ).assert_is_compatible_with(tensor_shape.scalar())    File ""/home/alex/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py"", line 737, in assert_is_compatible_with      raise ValueError(""Shapes %s and %s are incompatible"" % (self, other))  ValueError: Shapes (2, 1) and () are incompatible  "
How do i run this code to detect my hands in realtime?
"I had download the data files, weights_HandSegNet.pickle and weights_Pose3D.pickle.  I encountered the error ""ValueError: unsupported pickle protocol: 3"" when using the python 2.7.  The tensorflow package installed on my python 2.7,and I know that probably I reinstall tensorflow on python 3.5 maybe solve the issue.  If you help me provide the weights files  generated by pickle dump in protocol 2, it will help me a lot.    Thanks"
I wanted to play around with your work but im getting the above error when executing run.py  annysuggestions ? Its TF 1.1  tia
None
it's not good result.
"I would like to change some layers of a trained model for transition learning, but I don't know how to change only a part of the model.  Can someone please tell me how to do this?"
"when meet yi = yi*mm in contextual_attention,the error occurs.  tensorflow.python.framework.errors_impl.InvalidArgumentError: required broadcastable shapes [Op:Mul]  can anyone help me?"
Do I need to train the model with mask binary map and ground truth
!   
"Hi,    Congratulations for your outstanding papers, they contain several innovations and contributions.    I am conducting research on face de-occlusion and inpainting, and came across your papers. The research requires the exact replication of your DeepFill V2 experiments, since I'll use it as a baseline in my research. I couldn't find some key information neither in the paper nor in the code and YAML file.    I need the following information for CelebA and CelebA-HQ datasets:    1. Number of images used in the train and validation sets.  2. Image preprocessing, such as data augmentation, resizing, alignment, etc.  3. The CelebA-HQ paper says there are  , but the dataset comes with   . Where did you get this dataset? The link in   file is  .  4. The CelebA link in   file is also  .  5. Just to confirm, the image size during training is   and testing is  ?        I searched the closed issues, but couldn't find the flist files; I found just the code to create them.        Thanks,  Victor."
"Hello, I have this error during the test. How can I solve it？  Process finished with exit code -1073741819 (0xC0000005)"
"hello, how to get optical flow picture?Looking forward to your reply"
"Hi, @JiahuiYu   I modified your network structure, and train it. Also, this is my first time to build a model for image inpainting. So during training time, I have no idea if my model training is on the right way.     First of all, my data set is Places365, with a total amount of 1.8 million. Due to the limitation of GPU, I set the batchSize to 16, and the total number of iterations is 5,000,000. Within the total number of iterations, a data set can run about 44 epoch. It takes 112,500 iterations to run an epoch. My training method is: firstly, only using **content loss(L1 loss), perpetual loss(vgg loss) and style loss (style loss in style transfer)** to make the image inpainting network pre-converge, and then combining GAN to make it fully converge. Training methods refer to **globally and locally consistent image completion**.  !     Now it has run 22700 times and is still in the pre-convergence stage. My loss functions are as follows, namely **content loss(L1 loss), perpetual loss(vgg loss) and style loss**. Then I will check the repair results of the model with a verification diagram every once in a while, but I find that the effect is very unsatisfactory.    Content loss (L1 loss between the repair result and the real result): It can be seen that although the overall situation is declining, it fluctuates greatly. I don't know if this is normal.  !     Perpetual loss (it compares the feature obtained by vgg convolution of real pictures with the feature obtained by vgg convolution of generated pictures): This is also an overall decline, but the fluctuation is larger.  !     Style  loss (style loss in style transfer): This is also an overall decline, but the fluctuation range is super large.  !     This is the repair result of the verification diagram: **it can be seen that the repair part is like the epitome of the middle part of the original image**, and then it is continuously spliced and repaired. I don't know if this result is due to the fact that **the network hasn't converged yet, or because this is the normal situation in the training process, or there is a problem with my model structure or my code**.  !     "
"Hi,    I want to test pretrained celeba model. I run the code in Ubuntu.     When I wrote test code in terminal, model loaded succesfully.    <img width=""762"" alt=""1111"" src=""         But I want to test many images. So I put many command codes in .sh files.    command.sh file      !        When I tried to run command.sh file, it gives such error  : tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for logs/celeba    !       commmand.sh is in generative_inpainting folder.  pretrained model is located in generative_inpainting/logs/celeba      What can be the problem ? Any suggestions ?    Notes:  I had this problem also in Google Colab.  But there was no such problem in Windows.   "
"when I'm trying to set "" num_gpus_per_job: 2 "" in inpaint.yml   and  rewrite something in the train.py:   discriminator_training_callback = ng.callbacks.SecondaryMultiGPUTrainer(          num_gpus=FLAGS.num_gpus_per_job,          pstep=1,          optimizer=d_optimizer,          var_list=d_vars,          max_iters=1,          grads_summary=False,          graph_def=multigpu_graph_def,          graph_def_kwargs={              'model': model, 'FLAGS': FLAGS, 'data': data, 'loss_type': 'd'},      )      # train generator with primary trainer      # trainer = ng.train.Trainer(      trainer = ng.train.MultiGPUTrainer(          num_gpus=FLAGS.num_gpus_per_job,          optimizer=g_optimizer,          var_list=g_vars,          max_iters=FLAGS.max_iters,          graph_def=multigpu_graph_def,          grads_summary=FLAGS.GRADS_SUMMARY,          gradient_processor=False,          graph_def_kwargs={              'model': model, 'FLAGS': FLAGS, 'data': data, 'loss_type': 'g'},          spe=FLAGS.train_spe,          log_dir=FLAGS.log_dir,      there is something wrong while training:      File ""train.py"", line 91, in        log_dir=FLAGS.log_dir,    File ""/home/bo/anaconda3/envs/lwss/lib/python3.7/site-packages/neuralgym/train/multigpu_trainer.py"", line 24, in __init__      self._train_op, self._loss = self.train_ops_and_losses()    File ""/home/bo/anaconda3/envs/lwss/lib/python3.7/site-packages/neuralgym/train/multigpu_trainer.py"", line 84, in train_ops_and_losses      grads = process_gradients(grads, gradient_processor)    File ""/home/bo/anaconda3/envs/lwss/lib/python3.7/site-packages/neuralgym/ops/train_ops.py"", line 46, in process_gradients      grads = [gradient_processor(grad) for grad in grads]    File ""/home/bo/anaconda3/envs/lwss/lib/python3.7/site-packages/neuralgym/ops/train_ops.py"", line 46, in        grads = [gradient_processor(grad) for grad in grads]  TypeError: 'bool' object is not callable      I am a beginner. If someone can tell me what went wrong, I will be very grateful  My tensorflow -- GPU version is 1.14.0"
"Traceback (most recent call last):    File ""/home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call      return fn(*args)    File ""/home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1350, in _run_fn      target_list, run_metadata)    File ""/home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun      run_metadata)  tensorflow.python.framework.errors_impl.InvalidArgumentError: Need minval  = -384    [[{{node random_uniform_4}}]]    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""train.py"", line 102, in        trainer.train()    File ""/home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/neuralgym/train/multigpu_trainer.py"", line 59, in train      super().train()    File ""/home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/neuralgym/train/trainer.py"", line 130, in train      cb.run(sess, step)    File ""/home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/neuralgym/callbacks/secondary_multigpu_trainer.py"", line 24, in run      self.train()    File ""/home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/neuralgym/train/multigpu_trainer.py"", line 59, in train      super().train()    File ""/home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/neuralgym/train/trainer.py"", line 138, in train      feed_dict=self.context['feed_dict'])    File ""/home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 956, in run      run_metadata_ptr)    File ""/home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1180, in _run      feed_dict_tensor, options, run_metadata)    File ""/home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run      run_metadata)    File ""/home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call      raise type(e)(node_def, op, message)  tensorflow.python.framework.errors_impl.InvalidArgumentError: Need minval  = -384    [[node random_uniform_4 (defined at /home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]    Original stack trace for 'random_uniform_4':    File ""train.py"", line 76, in        'model': model, 'FLAGS': FLAGS, 'data': data, 'loss_type': 'd'},    File ""/home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/neuralgym/callbacks/secondary_multigpu_trainer.py"", line 20, in __init__      MultiGPUTrainer.__init__(self, primary=False, **context)    File ""/home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/neuralgym/train/multigpu_trainer.py"", line 25, in __init__      super().__init__(**self.context)    File ""/home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/neuralgym/train/trainer.py"", line 38, in __init__      self._train_op, self._loss = self.train_ops_and_losses()    File ""/home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/neuralgym/train/multigpu_trainer.py"", line 75, in train_ops_and_losses      gpu_id=gpu, **graph_def_kwargs)    File ""train.py"", line 18, in multigpu_graph_def      FLAGS, images, FLAGS, reuse=True)    File ""/DATA/scratch/khalid/gen_inpainting/inpaint_model.py"", line 140, in build_graph_with_losses      bbox = random_bbox(FLAGS)    File ""/DATA/scratch/khalid/gen_inpainting/inpaint_ops.py"", line 118, in random_bbox      [], minval=FLAGS.vertical_margin, maxval=maxt, dtype=tf.int32)    File ""/home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/ops/random_ops.py"", line 243, in random_uniform      shape, minval, maxval, seed=seed1, seed2=seed2, name=name)    File ""/home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_random_ops.py"", line 922, in random_uniform_int      seed=seed, seed2=seed2, name=name)    File ""/home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 794, in _apply_op_helper      op_def=op_def)    File ""/home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func      return func(*args, **kwargs)    File ""/home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 3357, in create_op      attrs, op_def, compute_device)    File ""/home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 3426, in _create_op_internal      op_def=op_def)    File ""/home/khalid/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 1748, in __init__      self._traceback = tf_stack.extract_stack()  "
"     Above link is where I tested with my own test data, but for some trials, if I mask a facial feature, it just removes that facial feature without generating anything. However, thank you for your work I learned a lot from your project!:D    For any observer of this repository that wonders how to mask your own data, I have written the code in the above link on how to mask your own data :).   So, from following the code on the above link, **you can make your own mask data** so that the SN-patchGAN works on your own test data.   I thought it was worth to share.    Thank you for your project :D"
"Hello, your code is to generate the mask image by yourself. I don't know where it is generated at present. Can you give me some pointers? I want to modify the mask part? Or how to input my own mask image like other questions."
"Is the model meant to sample from possible outputs? In other words, since it's a GAN, shouldn't the output be slightly different every time unless we provide a random seed? "
"When I train it on my own dataset, it paused on this step without feedback over a day as the pic shows. I wonder if this is normal. Maybe what should I do is just wait?  <img width=""666"" alt=""image1"" src=""   "
"I have an image that is masked already so i don't have the original image with me but. How to run inference on that?   Because in the test.py code it concatenates the image and its mask ""input_image = np.concatenate([image, mask], axis=2)"". How do I modify the test code to incorporate this changed scenario?"
"I have tried changing versions of tensor flow but the last problem that I am getting stuck is at the following.    Total size of trainable weights: 0G 9M 548K 958B (Assuming32-bit data type.)  2021-09-30 15:35:31.303671: E tensorflow/stream_executor/cuda/cuda_dnn.cc:396] Loaded runtime CuDNN library: 7102 (compatibility version 7100) but source was compiled with 7005 (compatibility version 7000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.  2021-09-30 15:35:31.303845: F tensorflow/core/kernels/conv_ops.cc:712] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo (), &algorithms)   Aborted (core dumped)    I couldnt downgrade cudnn to 7.0.5 because of the gcc and g++ versions used in ubuntu.    Please help "
"Traceback (most recent call last):    File ""train.py"", line 7, in        from inpaint_model import InpaintCAModel    File ""/home/myang47/generative_inpainting/inpaint_model.py"", line 13, in        from neuralgym.ops.gan_ops import gan_hinge_loss    File ""/home/myang47/.pyenv/versions/tf/lib/python3.6/site-packages/neuralgym/ops/gan_ops.py"", line 138, in        class Conv2DSepctralNorm(tf.layers.Conv2D):  AttributeError: module 'tensorflow.python.layers.layers' has no attribute 'Conv2D'    Any one have any ideas?"
"Hi, sorry for bothering you. I was looking at your code and I noticed that `xconv2_downsample` and `xconv4_downsample` and `pmconv2_downsample` have a different number of channels wrt the supmat (/2) that you submitted to cvpr2018. What is the correct version? Moreover, in the attention branch you use relu instead of elu as written in the paper. Why is that?"
"I have encountered `404: sorry, the page is here` when visiting the  , could you please fix it?"
"Hello, from the readme I see that ""all the pretrained models are trained with images of resolution 256x256 and largest hole size 128x128"".  However, as this work is often used in research papers, I would know which subset of ImageNet you used for training the ImageNet model of DeepFill v1.  Can you help me if you remember which set you used?  Thanks in advace"
"I want to know whether i can use my own random masks for training,thanks"
"num_gpus_per_job: 1  # number of gpus each job need  num_cpus_per_job: 4  # number of gpus each job need  num_hosts_per_job: 1  memory_per_job: 32  # number of gpus each job need  gpu_type: 'nvidia-tesla-p100'    i want  to train network in four Titanxp GPU  , how to change the config above ?  is it correct as below?  num_gpus_per_job: 4  # number of gpus each job need  num_cpus_per_job: 4  # number of gpus each job need  num_hosts_per_job: 4  memory_per_job: 32  # number of gpus each job need  gpu_type: 'titan xp'"
"Hi，    I want to use my own dataset for training, but instead of using a generative-adversarial network, I prefer only use the generator part to generate predictions and calculate the loss with the ground-truth. How can I turn off the discriminator part？    Thanks！"
"Hi,    I am fascinated by the results achieved by the model. I've been reading the paper and going through the implementation, and I just wanted to find out if there is any benefit (in terms of quality of results achieved) when using the hinge loss on the output of the discriminator as opposed to using the LSGAN (MSE) loss as used in the traditional PatchGAN implementation?    Thanks!"
How to download  the  pretranied modle ?
"Hey, thanks for releasing your code. I've been looking into the application of your work for virtual headset removal and I've been facing some issues with the kind of output I'm getting while testing with a nonsquare mask to in the wild images.    Some sample results I got on images in my dataset  !   !   !       I've been using the weights shared in the github repo. Is this issue regarding the shape of the mask? I could provide the files if you would like to recreate this issue?        "
"Hi thank you for your great project!       Why the propgation can encourage coherency of attention？Why a shift in attention can improve consistency？After reading the code and the explanation of the original text, I have not been able to figure out the principle."
"There's a typo in the **License** section of the README.  ""The software is for **educaitonal** and academic research purpose only.""  I guess it should be **educational**"
"I already fixed all error the pycharm mentioned.  Now there no error on terminal,  and I also checked the data is do input to model.  But it still no work...    Does anybody have experience about this?    Here is the terminal message:  =====================================================  2021-03-25 13:50:35.887101: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll  ---------------------------------- APP CONFIG ----------------------------------  num_gpus_per_job: 1  num_cpus_per_job: 1  num_hosts_per_job: 1  memory_per_job: 32  gpu_type: nvidia-tesla-p100  name: places2_gated_conv_v100  model_restore:  dataset: celebahq  random_crop: False  val: False  log_dir: logs/full_model_celeba_hq_256  gan: sngan  gan_loss_alpha: 1  gan_with_mask: True  discounted_mask: True  random_seed: False  padding: SAME  train_spe: 4000  max_iters: 100000000  viz_max_out: 10  val_psteps: 2000  data_flist:    celebahq: ['data/celebahq/train_shuffled.flist.txt', 'data/celebahq/val_shuffled.flist.txt']    celeba: ['data/celeba/train_shuffled.flist', 'data/celeba/validation_static_view.flist']    places2: ['data/places2/train_shuffled.flist', 'data/places2/validation_static_view.flist']    imagenet: ['data/imagenet/train_shuffled.flist', 'data/imagenet/validation_static_view.flist']  static_view_size: 30  img_shapes: [256, 256, 3]  height: 128  width: 128  max_delta_height: 32  max_delta_width: 32  batch_size: 4  vertical_margin: 0  horizontal_margin: 0  ae_loss: True  l1_loss: True  l1_loss_alpha: 1.0  guided: False  edge_threshold: 0.6  --------------------------------------------------------------------------------  --------------------------------- Dataset Info ---------------------------------  file_length: 14800  random: False  random_crop: False  filetype: image  shapes: [[256, 256, 3]]  dtypes: [tf.float32]  return_fnames: False  batch_phs: [ ]  enqueue_size: 32  queue_size: 256  nthreads: 1  fn_preprocess: None  index: 0  --------------------------------------------------------------------------------  WARNING:tensorflow:From F:\GGH\code\generative_inpainting-master\generative-inpainting\inpaint_ops.py:151: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.  Instructions for updating:  tf.py_func is deprecated in TF V2. Instead, there are two      options available in V2.      - tf.py_function takes a python function which manipulates tf eager      tensors instead of numpy arrays. It's easy to convert a tf eager tensor to      an ndarray (just call tensor.numpy()) but having access to eager tensors      means `tf.py_function`s can use accelerators such as GPUs as well as      being differentiable using a gradient tape.      - tf.numpy_function maintains the semantics of the deprecated tf.py_func      (it is not differentiable, and manipulates numpy arrays). It drops the      stateful argument making all functions stateful.    F:\GGH\code\run\venv\lib\site-packages\tensorflow\python\keras\legacy_tf_layers\convolutional.py:414: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.    warnings.warn('`tf.layers.conv2d` is deprecated and '  F:\GGH\code\run\venv\lib\site-packages\tensorflow\python\keras\engine\base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.    warnings.warn('`layer.apply` is deprecated and '  WARNING:tensorflow:From F:\GGH\code\run\venv\lib\site-packages\tensorflow\python\util\dispatch.py:201: calling extract_image_patches (from tensorflow.python.ops.array_ops) with ksizes is deprecated and will be removed in a future version.  Instructions for updating:  ksizes is deprecated, use sizes instead  F:\GGH\code\run\venv\lib\site-packages\tensorflow\python\keras\legacy_tf_layers\core.py:329: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.    warnings.warn('`tf.layers.flatten` is deprecated and '  ------------------------- Context Of Secondary Trainer -------------------------  num_gpus: 1  optimizer:    var_list: [ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ]  graph_def:    graph_def_kwargs: {'model':  , 'FLAGS': {}, 'data':  , 'loss_type': 'd'}  async_train: False  feed_dict: {}  max_iters: 1  log_dir: /tmp/neuralgym  spe: 1  grads_summary: False  log_progress: False  --------------------------------------------------------------------------------  2021-03-25 13:51:05.769605: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2  To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.  2021-03-25 13:51:05.772999: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll  2021-03-25 13:51:05.854404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:  pciBusID: 0000:06:00.0 name: TITAN V computeCapability: 7.0  coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 12.00GiB deviceMemoryBandwidth: 607.97GiB/s  2021-03-25 13:51:05.854655: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll  2021-03-25 13:51:06.902793: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll  2021-03-25 13:51:06.903006: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll  2021-03-25 13:51:07.036728: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll  2021-03-25 13:51:07.102033: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll  2021-03-25 13:51:07.652782: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll  2021-03-25 13:51:08.153744: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll  2021-03-25 13:51:08.189391: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll  2021-03-25 13:51:08.189793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0  2021-03-25 13:51:09.300279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:  2021-03-25 13:51:09.300500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0  2021-03-25 13:51:09.301232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N  2021-03-25 13:51:09.302256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10241 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:06:00.0, compute capability: 7.0)  2021-03-25 13:51:09.303468: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set  WARNING:tensorflow:Issue encountered when serializing queue_runners.  Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.  'QueueRunner' object has no attribute '_queue_closed_exception_types'  WARNING:tensorflow:From F:\GGH\code\run\venv\lib\site-packages\neuralgym\train\trainer.py:96: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.  Instructions for updating:  To construct input pipelines, use the `tf.data` module.  2021-03-25 13:51:13.450367: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)  test  -------------------------- Context Of Primary Trainer --------------------------  num_gpus: 1  optimizer:    var_list: [ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,    ,  ,  ,  ,  ,  ,  ,  ,   ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,   ]  graph_def:    gradient_processor: None  graph_def_kwargs: {'model':  , 'FLAGS': {}, 'data':  , 'loss_type': 'g'}  async_train: False  feed_dict: {}  max_iters: 100000000  log_dir: logs/full_model_celeba_hq_256  spe: 4000  grads_summary: False  log_progress: True  global_step:    global_step_add_one:    sess_config: gpu_options {    allow_growth: true  }  allow_soft_placement: true    sess:    summary_writer:    saver:    start_queue_runners: True  global_variables_initializer: True  --------------------------------------------------------------------------------  ?[32;1mTrigger callback: ?[0mTrigger WeightsViewer: logging model weights...  - weight name: inpaint_net/conv1/kernel:0, shape: [5, 5, 5, 48], size: 6000  - weight name: inpaint_net/conv1/bias:0, shape: [48], size: 48  - weight name: inpaint_net/conv2_downsample/kernel:0, shape: [3, 3, 24, 96], size: 20736  - weight name: inpaint_net/conv2_downsample/bias:0, shape: [96], size: 96  - weight name: inpaint_net/conv3/kernel:0, shape: [3, 3, 48, 96], size: 41472  - weight name: inpaint_net/conv3/bias:0, shape: [96], size: 96  - weight name: inpaint_net/conv4_downsample/kernel:0, shape: [3, 3, 48, 192], size: 82944  - weight name: inpaint_net/conv4_downsample/bias:0, shape: [192], size: 192  - weight name: inpaint_net/conv5/kernel:0, shape: [3, 3, 96, 192], size: 165888  - weight name: inpaint_net/conv5/bias:0, shape: [192], size: 192  - weight name: inpaint_net/conv6/kernel:0, shape: [3, 3, 96, 192], size: 165888  - weight name: inpaint_net/conv6/bias:0, shape: [192], size: 192  - weight name: inpaint_net/conv7_atrous/kernel:0, shape: [3, 3, 96, 192], size: 165888  - weight name: inpaint_net/conv7_atrous/bias:0, shape: [192], size: 192  - weight name: inpaint_net/conv8_atrous/kernel:0, shape: [3, 3, 96, 192], size: 165888  - weight name: inpaint_net/conv8_atrous/bias:0, shape: [192], size: 192  - weight name: inpaint_net/conv9_atrous/kernel:0, shape: [3, 3, 96, 192], size: 165888  - weight name: inpaint_net/conv9_atrous/bias:0, shape: [192], size: 192  - weight name: inpaint_net/conv10_atrous/kernel:0, shape: [3, 3, 96, 192], size: 165888  - weight name: inpaint_net/conv10_atrous/bias:0, shape: [192], size: 192  - weight name: inpaint_net/conv11/kernel:0, shape: [3, 3, 96, 192], size: 165888  - weight name: inpaint_net/conv11/bias:0, shape: [192], size: 192  - weight name: inpaint_net/conv12/kernel:0, shape: [3, 3, 96, 192], size: 165888  - weight name: inpaint_net/conv12/bias:0, shape: [192], size: 192  - weight name: inpaint_net/conv13_upsample/conv13_upsample_conv/kernel:0, shape: [3, 3, 96, 96], size: 82944  - weight name: inpaint_net/conv13_upsample/conv13_upsample_conv/bias:0, shape: [96], size: 96  - weight name: inpaint_net/conv14/kernel:0, shape: [3, 3, 48, 96], size: 41472  - weight name: inpaint_net/conv14/bias:0, shape: [96], size: 96  - weight name: inpaint_net/conv15_upsample/conv15_upsample_conv/kernel:0, shape: [3, 3, 48, 48], size: 20736  - weight name: inpaint_net/conv15_upsample/conv15_upsample_conv/bias:0, shape: [48], size: 48  - weight name: inpaint_net/conv16/kernel:0, shape: [3, 3, 24, 24], size: 5184  - weight name: inpaint_net/conv16/bias:0, shape: [24], size: 24  - weight name: inpaint_net/conv17/kernel:0, shape: [3, 3, 12, 3], size: 324  - weight name: inpaint_net/conv17/bias:0, shape: [3], size: 3  - weight name: inpaint_net/xconv1/kernel:0, shape: [5, 5, 3, 48], size: 3600  - weight name: inpaint_net/xconv1/bias:0, shape: [48], size: 48  - weight name: inpaint_net/xconv2_downsample/kernel:0, shape: [3, 3, 24, 48], size: 10368  - weight name: inpaint_net/xconv2_downsample/bias:0, shape: [48], size: 48  - weight name: inpaint_net/xconv3/kernel:0, shape: [3, 3, 24, 96], size: 20736  - weight name: inpaint_net/xconv3/bias:0, shape: [96], size: 96  - weight name: inpaint_net/xconv4_downsample/kernel:0, shape: [3, 3, 48, 96], size: 41472  - weight name: inpaint_net/xconv4_downsample/bias:0, shape: [96], size: 96  - weight name: inpaint_net/xconv5/kernel:0, shape: [3, 3, 48, 192], size: 82944  - weight name: inpaint_net/xconv5/bias:0, shape: [192], size: 192  - weight name: inpaint_net/xconv6/kernel:0, shape: [3, 3, 96, 192], size: 165888  - weight name: inpaint_net/xconv6/bias:0, shape: [192], size: 192  - weight name: inpaint_net/xconv7_atrous/kernel:0, shape: [3, 3, 96, 192], size: 165888  - weight name: inpaint_net/xconv7_atrous/bias:0, shape: [192], size: 192  - weight name: inpaint_net/xconv8_atrous/kernel:0, shape: [3, 3, 96, 192], size: 165888  - weight name: inpaint_net/xconv8_atrous/bias:0, shape: [192], size: 192  - weight name: inpaint_net/xconv9_atrous/kernel:0, shape: [3, 3, 96, 192], size: 165888  - weight name: inpaint_net/xconv9_atrous/bias:0, shape: [192], size: 192  - weight name: inpaint_net/xconv10_atrous/kernel:0, shape: [3, 3, 96, 192], size: 165888  - weight name: inpaint_net/xconv10_atrous/bias:0, shape: [192], size: 192  - weight name: inpaint_net/pmconv1/kernel:0, shape: [5, 5, 3, 48], size: 3600  - weight name: inpaint_net/pmconv1/bias:0, shape: [48], size: 48  - weight name: inpaint_net/pmconv2_downsample/kernel:0, shape: [3, 3, 24, 48], size: 10368  - weight name: inpaint_net/pmconv2_downsample/bias:0, shape: [48], size: 48  - weight name: inpaint_net/pmconv3/kernel:0, shape: [3, 3, 24, 96], size: 20736  - weight name: inpaint_net/pmconv3/bias:0, shape: [96], size: 96  - weight name: inpaint_net/pmconv4_downsample/kernel:0, shape: [3, 3, 48, 192], size: 82944  - weight name: inpaint_net/pmconv4_downsample/bias:0, shape: [192], size: 192  - weight name: inpaint_net/pmconv5/kernel:0, shape: [3, 3, 96, 192], size: 165888  - weight name: inpaint_net/pmconv5/bias:0, shape: [192], size: 192  - weight name: inpaint_net/pmconv6/kernel:0, shape: [3, 3, 96, 192], size: 165888  - weight name: inpaint_net/pmconv6/bias:0, shape: [192], size: 192  - weight name: inpaint_net/pmconv9/kernel:0, shape: [3, 3, 96, 192], size: 165888  - weight name: inpaint_net/pmconv9/bias:0, shape: [192], size: 192  - weight name: inpaint_net/pmconv10/kernel:0, shape: [3, 3, 96, 192], size: 165888  - weight name: inpaint_net/pmconv10/bias:0, shape: [192], size: 192  - weight name: inpaint_net/allconv11/kernel:0, shape: [3, 3, 192, 192], size: 331776  - weight name: inpaint_net/allconv11/bias:0, shape: [192], size: 192  - weight name: inpaint_net/allconv12/kernel:0, shape: [3, 3, 96, 192], size: 165888  - weight name: inpaint_net/allconv12/bias:0, shape: [192], size: 192  - weight name: inpaint_net/allconv13_upsample/allconv13_upsample_conv/kernel:0, shape: [3, 3, 96, 96], size: 82944  - weight name: inpaint_net/allconv13_upsample/allconv13_upsample_conv/bias:0, shape: [96], size: 96  - weight name: inpaint_net/allconv14/kernel:0, shape: [3, 3, 48, 96], size: 41472  - weight name: inpaint_net/allconv14/bias:0, shape: [96], size: 96  - weight name: inpaint_net/allconv15_upsample/allconv15_upsample_conv/kernel:0, shape: [3, 3, 48, 48], size: 20736  - weight name: inpaint_net/allconv15_upsample/allconv15_upsample_conv/bias:0, shape: [48], size: 48  - weight name: inpaint_net/allconv16/kernel:0, shape: [3, 3, 24, 24], size: 5184  - weight name: inpaint_net/allconv16/bias:0, shape: [24], size: 24  - weight name: inpaint_net/allconv17/kernel:0, shape: [3, 3, 12, 3], size: 324  - weight name: inpaint_net/allconv17/bias:0, shape: [3], size: 3  - weight name: discriminator/sn_patch_gan/conv1/kernel:0, shape: [5, 5, 4, 64], size: 6400  - weight name: discriminator/sn_patch_gan/conv1/bias:0, shape: [64], size: 64  - weight name: discriminator/sn_patch_gan/conv2/kernel:0, shape: [5, 5, 64, 128], size: 204800  - weight name: discriminator/sn_patch_gan/conv2/bias:0, shape: [128], size: 128  - weight name: discriminator/sn_patch_gan/conv3/kernel:0, shape: [5, 5, 128, 256], size: 819200  - weight name: discriminator/sn_patch_gan/conv3/bias:0, shape: [256], size: 256  - weight name: discriminator/sn_patch_gan/conv4/kernel:0, shape: [5, 5, 256, 256], size: 1638400  - weight name: discriminator/sn_patch_gan/conv4/bias:0, shape: [256], size: 256  - weight name: discriminator/sn_patch_gan/conv5/kernel:0, shape: [5, 5, 256, 256], size: 1638400  - weight name: discriminator/sn_patch_gan/conv5/bias:0, shape: [256], size: 256  - weight name: discriminator/sn_patch_gan/conv6/kernel:0, shape: [5, 5, 256, 256], size: 1638400  - weight name: discriminator/sn_patch_gan/conv6/bias:0, shape: [256], size: 256  ?[32;1mTrigger callback: ?[0mTotal counts of trainable weights: 9999294.  Total size of trainable weights: 0G 9M 548K 958B (Assuming32-bit data type.)  2021-03-25 13:51:17.153123: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll  2021-03-25 13:51:17.835695: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll  2021-03-25 13:51:17.874321: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll  2021-03-25 13:51:20.928005: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0    2021-03-25 13:51:21.060364: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0  "
"Dear author,may I ask how you calculated the L1 loss and L2 loss in the experimental results?The result of my statistical calculation of the experimental results is quite different from the data in your paper.Could you please give the specific calculation method?"
"Hi,  I'm a beginner and I don't know how do I download any of the dataset included in your yml file. If you can please help me I'll be thankful to you."
"Is this loss really 1 or  is it a mistake , because in deep fill_v1 , this loss is only 0.001  !   "
"Hello,  Where can I get the masks to remake the result of Table 2, in the paper ""Free-Form Image Inpainting with Gated Convolution""?    Thanks in advance."
"I'm trying to convert this model to a tensorflow.js format so I can perform in-browser client-side inference of the model. To do this, the model needs to be in the SavedModel format as specified  .     By using a simple python script to restore the model and save it in this SavedModel format, I've managed to obtain a .pb file as expected:         However, when I try to then convert this SavedModel format with tfjs converter, it seems that it expects some SignatureDefs to be saved, which have been lost in the conversion process so I get these error messages:    `RuntimeError: MetaGraphDef associated with tags 'serve' could not be found in SavedModel.`   `ValueError: Signature 'serving_default' does not exist. The following signatures are available: KeysView(_SignatureMap({}))`  `MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs: #no output`    It seems I need to provide some sort of signature for the inputs and outputs of the network, although I'm really unfamiliar with this process so am unsure how I would do so.    Does anybody have an insight into how I could solve this?"
"Can someone help me with training? I need to know folder hierarchy of dataset.  And should there be masks in one folder?    While training I am not getting any error but training not at all happening    Tried giving input images as 256X 256      If I will know the steps to train, it will be really helpful, as I am stuck  ################################  I edited inpaint.yml file for my data :    # =========================== Basic Settings ===========================  # machine info  num_gpus_per_job: 1  # number of gpus each job need  num_cpus_per_job: 4  # number of gpus each job need  num_hosts_per_job: 1  memory_per_job: 32  # number of gpus each job need  gpu_type: 'nvidia-tesla-p100'    # parameters  name: places2_gated_conv_v100  # any name  model_restore: ''  # logs/places2_gated_conv  dataset: 'peak'  # 'tmnist', 'dtd', 'places2', 'celeba', 'imagenet', 'cityscapes'  random_crop: False  # Set to false when dataset is 'celebahq', meaning only resize the images to img_shapes, instead of crop img_shapes from a larger raw image. This is useful when you train on images with different resolutions like places2. In these cases, please set random_crop to true.  val: False  # true if you want to view validation results in tensorboard  log_dir: logs/full_model_celeba_hq_256    gan: 'sngan'  gan_loss_alpha: 1  gan_with_mask: True  discounted_mask: True  random_seed: False  padding: 'SAME'    # training  train_spe: 4000  max_iters: 100000000  viz_max_out: 10  val_psteps: 2000    # data  data_flist:    #      celebahq: [      'data/celeba_hq/train_shuffled.flist',      'data/celeba_hq/validation_static_view.flist'    ]    #   please to use random_crop: True    celeba: [      'data/celeba/train_shuffled.flist',      'data/celeba/validation_static_view.flist'    ]    #   please download the high-resolution dataset and use random_crop: True    places2: [      'data/places2/train_shuffled.flist',      'data/places2/validation_static_view.flist'    ]    #   please use random_crop: True    imagenet: [      'data/imagenet/train_shuffled.flist',      'data/imagenet/validation_static_view.flist',    ]    peak: [      'data/peak/train_shuffled.flist',      'data/peak/validation_shuffled.flist',    ]    static_view_size: 30  img_shapes: [256, 256, 3]  height: 128  width: 128  max_delta_height: 32  max_delta_width: 32  batch_size: 16  vertical_margin: 0  horizontal_margin: 0    # loss  ae_loss: True  l1_loss: True  l1_loss_alpha: 1.    # to tune  guided: False  edge_threshold: 0.6  #################################      Thanks,  akanksh"
"         Hello, author, I have some questions about gated convolution, hope to answer！           In your paper, there is no bias in its formula (i.e. sigmoid (Gatingy,x)), so the probability value of the missing area after sigmoid is 0, which can shield the invalid value of the missing area.            But I read your code, and the bias default is biased, so the probability of the missing area is not 0. Isn't it impossible to shield?            The code of gated convolution is as follows， Please pay attention to “tf.layers.conv2d”  def gen_conv(x, cnum, ksize, stride=1, rate=1, name='conv',               padding='SAME', activation=tf.nn.elu, training=True):      assert padding in ['SYMMETRIC', 'SAME', 'REFELECT']      if padding == 'SYMMETRIC' or padding == 'REFELECT':          p = int(rate*(ksize-1)/2)          x = tf.pad(x, [[0,0], [p, p], [p, p], [0, 0]], mode=padding)          padding = 'VALID'      x = tf.layers.conv2d(          x, cnum, ksize, stride, dilation_rate=rate,          activation=None, padding=padding, name=name)      if cnum == 3 or activation is None:          # conv for output          return x      x, y = tf.split(x, 2, 3)      x = activation(x)      y = tf.nn.sigmoid(y)      x = x * y      return x              And the bias default of tf.layers.conv2d is as follows, Please pay attention to “use_bias=True”  def conv2d(inputs,             filters,             kernel_size,             strides=(1, 1),             padding='valid',             data_format='channels_last',             dilation_rate=(1, 1),             activation=None,             use_bias=True,             kernel_initializer=None,             bias_initializer=init_ops.zeros_initializer(),             kernel_regularizer=None,             bias_regularizer=None,             activity_regularizer=None,             kernel_constraint=None,             bias_constraint=None,             trainable=True,             name=None,             reuse=None):                Finally, wish you all the best, good job and good body!!"
"Hello, if I want to use the BN layer, where should I put the following code: update_ops = Tf.get_collection (TF.graphkeys.update_ops)"
"Hello, I want to do a comparative experiment with your results. However, I found that the code in the two papers is the same between ""Generative Image Inpainting with Contextual Attention"" and ""Free-Form Image Inpainting with Gated Convolutionv"". I want to compare with the paper of ""Generative Image Inpainting with Contextual Attention"", Can you give me any suggestions?"
"Hey,  I am getting this error when I run the train.py. If anyone can help me with this issue?    WARNING: tensorflow: Error encountered when serializing queue_runners.   Type is unsupported, or the types of the items don't match field type in CollectionDef.  'QueueRunner' object has no attribute '_queue_closed_exception_types'"
"Thank you for your great work. I have been applying your research to my problem now.  Here is my problem:    > DEM, stands for Digital Elevation Model, is a numerical matrix with each pixel represents its elevation correspond to a location.    SRTM is a globally DEM dataset. However, with a reservoir or a lake, SRTM showed the elevation of surface water at the time it collected (in 2000). Now I want to recover DEM below surface water of a lake or a reservoir.    Here is my step-by-step process:    1. My training set is 15k DEM images.    2. I generated masks correspond to input image. **An input image has a specific mask**. My mask is an connected area which has elevation lower than a random number. So it can describe a reservoir or a lake, I think so. Some of my masks with the input:    !     So I customized your code to read my input:   !     !     3. I also customized **data_from_fnames** in **neuralgym** toolkit to read '**.tif'** files. Then I used min-max normalize DEM image to 0-255 in order to pass it to your model.    4. Some first rows in my train.flist:    !     5. Because of limit memory of my GPU, I set image input shape (128, 128, 1) and batch size is 16 as your advice I saw in other issues.          6. I finally trained more than 70 epoches and get bad results:    My losses are not converging    !     !     Some of generated validation images:    !       **Questions**    1. Are all my customizations correct?   2. I saw your sample flist is shuffled and I didn't. Does it affect to the training result?  3. Could you give me some suggestion and your views on my problem?    Thanks a lot! <3     "
"Hello all, I create the thread to discuss about the gated convolution and its input. Many questions in the repo. have been asked for the problem        **First of all**, the input of gated convolution are: uncompleted image `img` and its `mask`. In which, the mask defined as  >for image inpainting, the input are composed of both regions with **valid pixels**/features **outside** holes and **invalid pixels/features** (in shallow layers) or synthesized pixels/features (in deep layers) **in masked** regions    >in which M is the corresponding binary mask, **1** represents pixel in the location (y, x) is **valid**, **0** represents the pixel is **invalid**    However, the official code has some confusing, that is the reason why many questions has been asked. In the code,               From these lines, we can see that the mask defined as pixels `0` for **outside** the box and `1` for **inside** the box. Hence, the paper has a typo, it should be   >in which M is the corresponding binary mask, **0** represents pixel in the location (y, x) is **valid**, **1** represents the pixel is **invalid**    So, the above lines answered the question         Secondly, the incompleted image `x` defined as       It shows that the incompleted input is obtained by multiply the original image and (1-mask). It means we want to keep the **original pixels of the image outside of the mask** and set to **zero pixels inside of the mask**    Now, we will concatenate the incompleted input `x` and `mask` to obtain `BxHxWx(channel_x + channel+mask)`. For example, the channel of x is 3 (RGB) and the channel of the mask is 1 (binary image). We have the input for gated convolution is `BxHxWx4`. Now, we will feed it to the gated convolution.           We can see that the concatenation of incompleted input and mask (defined as `x`) will go to a conv2d. The output then splits into two components: `x` is for convolution result of incompleted `x`, and `y` is for convolution result of mask. Then `y` goes to the sigmoid function to normalize it to [0,1] and multiply with `activation(x)` to perform gating.     Thanks for reading, please correct if I was wrong!   "
"Hello, @JiahuiYu .  There is no progress bar after ""Trigger callback: Total counts of trainable weights: 9999294.  Total size of trainable weights: 0G 9M 548K 958B (Assuming32-bit data type.)"" anymore.  What should i do to have it back?  "
"I have found some issues about this in   and   and according to the   provided by @JiahuiYu , I did some experiments and tried to explain it here. This also confuses me for several days and I hope this helps.    At first, I generated a specified image to do the experiment. The first channel of this image is 4*6, and the pixel value is like below, which helps us for confirmation.        The two images below correspond to the situation in one dimension. We can see in this situation, kernel size is always two times of stride. This explains why the code below has number 2.      !   !   Moreover, we can see that two values cover the same pixel so we want to average them by 2. Our situation is two dimension and we can image we should average them by 4 in the code below.     We can do the experiment and check whether the output y in function `contextual_attention` is the same as its input f or b(f and b are the same). It should be very closed from the intention of `contextual_attention`——We can borrow any patches from b and paste to y according to the similarity between b and f(Some may wonder that, in proposed model, the output y will always the same b. Actually, it won't if the mask is not all zero. In our experiment, mask is all zero, so y should be very closed to b. The mask stops y from borrowing patches in b in masked region. y will borrow patches outside masked region). Result is shown below, we can see that except some pixels in the boundary, other pixels are exactly the same.  !     Should convolution kernel size(w) the same as deconvolution kernel size(raw_w) as stated in  ? In my opinion, convolution and its subsequent softmax decide which the deconvolution kernel should be put where. Thus the kernel size of deconvolution does not matter much. We can try to change parameter ksize from 3 to 5. y should be the same as above. Actually it is as I did the experiment.    I wondered if we can change 2 or/and 4 in the code above to get the same effect. I found another pattern shown below.  !   !   According to the pattern shown above, I change 2 to 1(or 3) and change 4 to 1 (or 9). Finally I got y very closed to input b except some boundary pixel.   !   !   However, there is one more detail I did not mention above. While we were changing 2 and 4 above, we should keep kernel size is divisible by stride. Namely, we should keep  `kernel`  is divisible by `rate * stride` in the code below. As stated in      Besides, in  , @JiahuiYu said 'Basically larger patch mean more accurate of original resolution.'. In my opinion, convolution result gives rough location in low resolution where filters(raw_w) should be put in later deconvolution. Large patch covers more pixels and every pixel in y gets more to average in deconvolution, which may gives more accurate result. Below image shows larger patches.  !     I want to sum up the effect of variables `rate` and `stride` in function `contextual_attention`:  rate: downsample f,b,mask  stride: decrease number of convolution/deconvolution filters    Discussion above is based on the effect of deconvolution is to paste its kernels all around, which is decided by convolution and softmax output yi. I comprehend this from the perspective that deconvolution is another kind of convolution as stated in  "
"Whiel I was training, lines besides epoch and loss were printed redundantly like below. !   Do you know how to avoid this?  "
"    def build_inpaint_net(self, x, mask, config=None, reuse=False, training=True, padding='SAME', name='inpaint_net'):          xin = x          offset_flow = None          ones_x = tf.ones_like(x)[:, :, :, 0:1]          x = tf.concat([x, ones_x, ones_x * mask], axis=3)    I find the input contains 'ones_x',is there anything meaningful?    "
"Hi, Jiahui! After an one-week training on a GTX 1080TI, I found some interesting results from my own dataset.  There are 2 kinds of images in my dataset. One is the images with clearly texture like this:  !   !   The inpainting results of this kinds of images are semantic plausible:  !   !   !   !   Also, there are some images like this that contains more information and structure:  !   However, the result of this image from my pre-trained model is quite blurry and bad:  !   !   Here are my hypothesis:  1. The second kind of images is minority in my training set. Should I have to increase the ratio to 1:1? The total number of images in my dataset is 8000.  2. According to #21 and #53, I should do fine-tuning for my model. Could you please give my suggestions of changes of the hyper-parameters?  Here are the screenshots from Tensorboard:  !   !   !   !       "
Really nice work and great idea for the DeepFillv2!! Any plan and expected date to release the DeepFillv2 code?
"In the paper, it is written in Section 3 that ""we use mirror padding for all convolution layers"". However, the code seem to use 'SAME' padding (i.e. zero padding) for the generator since the 'PADDING' field of the .yml file is specified as 'SAME'. Which type of padding did you use exactly for the pretrained models?     Thank you for your help!"
" gpu        pid  type    sm   mem   enc   dec   command   Idx             C/G     %     %     %     %   name      0      14891     C    90    67     0     0   python3            Traceback (most recent call last):    File ""train.py"", line 37, in        ng.get_gpus(config.NUM_GPUS)    File ""/home/gpuguest/Downloads/generative_inpainting/neuralgym/utils/gpus.py"", line 70, in get_gpus      ' [(gpu id: num of processes)]: {}'.format(sorted_gpus))  SystemError: No enough gpus for dedicated usage. [(gpu id: num of processes)]: [(0, 1)]      iwhy is this error raised..?"
"I want to train network with new dataset.  For training, I tried to modify inpaint.yml file, but I'm not sure about how to set dataset path.  It seems that I have to add new dataset as 'flist' file to DATA_FLIST, but I cannot find how to make appropriate 'flist' files.  Is there any reference for flist file format, or some examples of them?"
"It's a great work. Does this code support multi-gpu training? I've tried to alter NUM_GPUS and GPU_ID, but it seems like that the code just selects one gpu for training. Is there any clue about it? Thanks."
"Thank you for your contribution,    At the moment, I have checked the test file and it only can run on 1 image/mask. I have tried to put code in the for loop, but I got error at this line :         output = model.build_server_graph(input_image)  output = model.build_server_graph(input_image)    File ""/home/ubuntu/trinh/generative_inpainting/inpaint_model.py"", line 307, in build_server_graph      config=None)    File ""/home/ubuntu/trinh/generative_inpainting/inpaint_model.py"", line 50, in build_inpaint_net      x = gen_conv(x, cnum, 5, 1, name='conv1')    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args      return func(*args, **current_args)    File ""/home/ubuntu/trinh/generative_inpainting/inpaint_ops.py"", line 45, in gen_conv      activation=activation, padding=padding, name=name)    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/convolutional.py"", line 608, in conv2d      return layer.apply(inputs)    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py"", line 671, in apply      return self.__call__(inputs, *args, **kwargs)    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py"", line 559, in __call__      self.build(input_shapes[0])    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/convolutional.py"", line 143, in build      dtype=self.dtype)    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py"", line 458, in add_variable      trainable=trainable and self.trainable)    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py"", line 1203, in get_variable      constraint=constraint)    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py"", line 1092, in get_variable      constraint=constraint)    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py"", line 425, in get_variable      constraint=constraint)    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py"", line 394, in _true_getter      use_resource=use_resource, constraint=constraint)    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py"", line 742, in _get_single_variable      name, """".join(traceback.format_list(tb))))  ValueError: Variable inpaint_net/conv1/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:      File ""/home/ubuntu/trinh/generative_inpainting/inpaint_ops.py"", line 45, in gen_conv      activation=activation, padding=padding, name=name)    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args      return func(*args, **current_args)    File ""/home/ubuntu/trinh/generative_inpainting/inpaint_model.py"", line 50, in build_inpaint_net      x = gen_conv(x, cnum, 5, 1, name='conv1')    Do you have any ideas what I have done wrong?  Thank you.      "
  The following error shows when I run the `python  train.py`   
None
"After successfully training with guided switched on, I thought it would be time to test it. I used the script `test.py`, with some small modifications, to add the guide input as additional input.       I assume that this might not be the correct way of doing this, because I'm getting the following error:   "
None
"Hi, Thanks for your beautiful work, i have a question about paper:  The reason that you didn't take global discriminator is that the recptive field of each Neurons is already satisify the training input size , so you mean the size of the image in the training set is smaller than the input to the discriminator? otherwise , if they are same size , the output is only one value but not a matirx. （because the recptive field is the size of training set images)"
Discriminator uses gated convolution.
"Hi, Thanks for your beautiful work,  i have a question about paper:  The reason that you didn't take global discriminator is that the recptive field is  already satisify the training input size , so you mean the size of the image in the  training set is smaller than the input to the discriminator? otherwise , if they are same size , the output is only one value but not a matirx. （because the recptive field is the size of training set images)"
None
"Hi there,   I'm using tensorflow 2.4.1 to try to run this code.  and I met a problem on file inpaint_ops.py from neuralgym library.    !     This is part of initializing class DataFromFNames(Dataset):    Changing tf.placeholder to tf.compat.v1.placeholder is not enough.  Here is terminal result:  =========================================================  2021-03-19 15:04:52.047612: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll  ---------------------------------- APP CONFIG ----------------------------------  num_gpus_per_job: 1  num_cpus_per_job: 4  num_hosts_per_job: 1  memory_per_job: 32  gpu_type: nvidia-tesla-p100  name: places2_gated_conv_v100  model_restore:  dataset: celebahq  random_crop: False  val: False  log_dir: logs/full_model_celeba_hq_256  gan: sngan  gan_loss_alpha: 1  gan_with_mask: True  discounted_mask: True  random_seed: False  padding: SAME  train_spe: 4000  max_iters: 100000000  viz_max_out: 10  val_psteps: 2000  data_flist:    celebahq: ['data/celeba_hq/train_shuffled.flist.txt', 'data/celeba_hq/validation_static_view.flist.txt']    celeba: ['data/celeba/train_shuffled.flist', 'data/celeba/validation_static_view.flist']    places2: ['data/places2/train_shuffled.flist', 'data/places2/validation_static_view.flist']    imagenet: ['data/imagenet/train_shuffled.flist', 'data/imagenet/validation_static_view.flist']  static_view_size: 30  img_shapes: [178, 218, 3]  height: 128  width: 128  max_delta_height: 32  max_delta_width: 32  batch_size: 4  vertical_margin: 0  horizontal_margin: 0  ae_loss: True  l1_loss: True  l1_loss_alpha: 1.0  guided: False  edge_threshold: 0.6  --------------------------------------------------------------------------------  Traceback (most recent call last):    File ""train.py"", line 38, in        nthreads=FLAGS.num_cpus_per_job)    File ""F:\GGH\code\run\venv\lib\site-packages\neuralgym\data\data_from_fnames.py"", line 85, in __init__      for dtype, shape in zip(self.dtypes, self.shapes)]    File ""F:\GGH\code\run\venv\lib\site-packages\neuralgym\data\data_from_fnames.py"", line 85, in        for dtype, shape in zip(self.dtypes, self.shapes)]    File ""F:\GGH\code\run\venv\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 3176, in placeholder      raise RuntimeError(""tf.placeholder() is not compatible with ""  RuntimeError: tf.placeholder() is not compatible with eager execution.  ========================================================  If you know anything about upper problem, please give me advice, sincerely.  "
"Hi @JiahuiYu ,  In the first place, your paper and model is great. But in some case, there is a hole in the result.  Image:  !     Mask:  !     Result:  !     what's the problem? Thanks.  I use your pretrained model    If someone else could tell me why, it is ok too. Thanks."
None
"Hello JiahuiYu,    I'm so happy to meet your code. Thanks again.  But I have one question!  I am curious about the option of static_view_size.   How does this work?    "
!   
What do they represent?    # training  train_spe: 4000  max_iters: 100000000  viz_max_out: 10  val_psteps: 2000    static_view_size: 30
"Hi! Thanks for sharing your code open-source, it makes it very easy to understand your paper!    I'm trying to reproduce your result on the Celebrity-A dataset by retraining your model.   I've split the Celeb-A dataset into 27K train / 3K val images and trained the model with the original source code and the config given in:      However, when validating on center mask images I'm seeing quite a performance drop compared to your original model (see below for metrics). I trained the model for ~24 hours on a V100-32GB, which is the same as 160K training steps on tensorboard.    **My questions are:**  1. Do you have the original tensorboard logs when you trained the model, then I can compare and try to see where the difference is.  2. What is the config file you used for Celeb-A (if it's not the one I linked).  3. How long did you train your model for in number of training steps?    ### Validation metrics on Celeba-HQ 256x256 center mask    **Your model checkpoint given in the github readme.md:**    L1: 0.019221259281039238  L2: 0.05457606166601181  PSNR: 25.547083021850874  SSIM: 0.8014083052799579  LPIPS: 0.058718740940093994    **Retrained model for 160K steps with the config file linked above:**   L1: 0.02299271896481514  L2: 0.06465483456850052  PSNR: 24.07686822468923  SSIM: 0.7864461851499979  LPIPS: 0.07174131274223328      "
"Hello!    I have a problem: I want to track training loss, but I do not see it in Tensorboard.  The training goes fine, I uncommented sys.stdout.write in neuralgym/utils/logger.py, and progress is shown. However, while Tensorboard launches and shows graphs panel, it does not show any scalars. Is this an intended behavior or not?    Thank you!    The log is the following:     "
"!   !   !     hey, can you meet this problem? i want to know, this problem cause by the training time not enough or others ?"
I used pretrained models and the command from the documentation but nothing happened  `!python test.py --image examples/places2/case1_input.png --mask examples/places2/case1_mask.png --output results/places2/case1_output.png --checkpoint_dir release_places2_256_deepfill_v2`   
"Hello, i am renting a 7 GPU (GTX 1080 TI) machine from vast.ai. I have changed inpaint.yml, changed the num_gpus_per_job to 7. Also tested it with value of 1.    When i run the train.py script, even 10 iterations takes a REALLY long time. When i check nvidia-smi, it doesn't show almost any usage. Here is the result from nvidia-smi:    +-----------------------------------------------------------------------------+  | NVIDIA-SMI 450.66       Driver Version: 450.66       CUDA Version: 11.0     |  |-------------------------------+----------------------+----------------------+  | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |  | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |  |                               |                      |               MIG M. |  |===============================+======================+======================|  |   0  GeForce GTX 108...  On   | 00000000:05:00.0  On |                  N/A |  |100%   19C    P8    14W / 250W |      0MiB / 11175MiB |      0%      Default |  |                               |                      |                  N/A |  +-------------------------------+----------------------+----------------------+  |   1  GeForce GTX 108...  On   | 00000000:06:00.0 Off |                  N/A |  |100%   21C    P8    14W / 250W |      0MiB / 11178MiB |      0%      Default |  |                               |                      |                  N/A |  +-------------------------------+----------------------+----------------------+  |   2  GeForce GTX 108...  On   | 00000000:07:00.0 Off |                  N/A |  |100%   22C    P8    21W / 250W |      0MiB / 11178MiB |      0%      Default |  |                               |                      |                  N/A |  +-------------------------------+----------------------+----------------------+  |   3  GeForce GTX 108...  On   | 00000000:0A:00.0 Off |                  N/A |  |100%   21C    P8    13W / 250W |      0MiB / 11178MiB |      0%      Default |  |                               |                      |                  N/A |  +-------------------------------+----------------------+----------------------+  |   4  GeForce GTX 108...  On   | 00000000:0B:00.0 Off |                  N/A |  |100%   21C    P8    15W / 250W |      0MiB / 11178MiB |      0%      Default |  |                               |                      |                  N/A |  +-------------------------------+----------------------+----------------------+  |   5  GeForce GTX 108...  On   | 00000000:0C:00.0 Off |                  N/A |  |100%   19C    P8    16W / 250W |      0MiB / 11178MiB |      0%      Default |  |                               |                      |                  N/A |  +-------------------------------+----------------------+----------------------+  |   6  GeForce GTX 108...  On   | 00000000:0D:00.0 Off |                  N/A |  | 90%   20C    P8    14W / 250W |      0MiB / 11178MiB |      0%      Default |  |                               |                      |                  N/A |  +-------------------------------+----------------------+----------------------+                                                                                   +-----------------------------------------------------------------------------+  | Processes:                                                                  |  |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |  |        ID   ID                                                   Usage      |  |=============================================================================|  |  No running processes found                                                 |  +-----------------------------------------------------------------------------+      The temperatures are all room temperature. No work is being done on the GPUs. How can i fix this??    Thank you in advance!"
"Hey, thank you for your excellent work and shared code. Before contextual attention in inpainting_model.py, the gated convolution layer uses the ReLU activation function instead of the ELU. Is there any benefit to doing this?  `            x = gen_conv(x, 4*cnum, 3, 1, name='pmconv6',                                  activation=tf.nn.relu)`"
"Thanks for the code. When I train the model, the  GPU-Util is zero (last 2nd column). I was wondering if the default deployment is correct? Look forward to the reply!  !   "
"Hi,    I get a big image. I want to split it to pieces as the generator inputs and finally join them as one image. Do you have any suggestion that the image pieces should overlap how many pixels among each other?    Thank you. "
"!   !   !     Why is the training result like this? I tried several times, but no good results. I used the places2 pre-training model."
the smeared ori area in input image how to creat?
"Hi,    Thanks for sharing your work!    I am trying out the network and according to the paper the average time per 512x512 image is 0.2s.    I am running the network on a 1080Ti gpu, with CUDA version 10.0.130 and cudNN 7.6.5 and the time per places2 image in the examples/places2 folder is 4+ seconds just for the line:    result = sess.run(output) in test.py (line 62).    I tried to crop the image to 512x512 and the time is still 4+ seconds.    Would you have any suggestions on what the reason for this might be? Does it have something to do with the library versions?"
Sorry to bother you but I have some problems while reimplement training the deepfillv2 model.  The d_loss is converged to 1.0 which means that the discriminator output the same value for both real and fake samples.  Should I train the netD for few times (maybe 5?) after once train the netG?  For some reason I can't use the tensorboard. Here is the manual loss pic.  !   !   !     The model results are not good  !       
"After you train a model, let's say it reaches epoch 10, at 4000iter/epoch, so 40000iter and you stop training.    When you resume training, it loads the model but starts again from epoch 0, 0 iterations. This makes is so that new checkpoints are (wrongly) saved with  `snap-4000` and `snap-8000` instead of `snap-44000` and `snap-48000` (which is the total number of iterations that model was trained for). Another problem is that the events emited would have the wrong number of iterations, so the graphs in tensorboard would be messed up.    !   "
"Hi, I am very interested in your work, especially in the Guided Image Inpainting part. Is there any way I can enter Sketch Channel?"
Can you also provide the original (uncut) images ? 
"I am trying to run it using Tensorflow v2, but I get several errors.  I used this   to try to fix it, but it still complains about ` module 'tensorflow' has no attribute 'contrib'`.    I noticed that the only functions used are `arg_scope` and `add_arg_scope` from `tensorflow.contrib.framework.python.ops`.    Any suggestions on how to fix/replace those calls?         EDIT:   Oh, neuralgym also seems to be using contrib.       "
I am getting this error when I run pre-trained model. Any idea about that issue? Thanks
"Hello :D     I have a basal question about, should we resize the test image resolution same to training date image resolution? during the training process, the image shapes are 256*256, but what about if i use a image of 1920*1080 for testing?    kind regards,  dD"
"Hi, can you explain and help me with a problem? If I do not use ae_loss to measure the error of background and l1 for the foreground, but I only use l1 for the generated image, what is the problem? Thanks."
"Dear JiahuiYu    Thank you very much for this great work and codes      We are aiming at training the inpainting method to re-construct specific objects. e,g, we aim to  train the network to inpaint only **birds**.   Thus we want to use a data set of 10000 images that contains birds, (with view in backround  -  trees, sky, etc ) mark and label the pixels with the birds, and than create masks only for the birds location in the images.   since the code in the git create random masks in batches, we are wondering whether it possible in the code to associate each mask to a specific image in the data set, where the location of the mask is paired with the location of the bird in the image, thus the coupling of image-mask is not random.       We were also wondering how can we utilise the image2edge function (located at inpaint_ops) if we want to integrate our own sketch masks in the code (for example, add the contour of the birds as sketch input)    Thanks in advance,   Efrat "
"Hi,  I want to apply different mask for each image within the same batch using the annotated facial landmarks.  If I understand correctly, I need to first read the fnames within each batch by setting `return_fnames=True` when creating the data object.    The data pipeline returns me ` `, however I have trouble with viewing/reading this tensor using tf.run(). What would be the correct way to obtain the filenames for each batch?    Thanks,"
"Sorry to bother you,I want to input some pictures during the training process and check the output effect of the network. What should I do?"
"my dataset is some 16 bits tiff pictures with 1 channel, I am wondering whether it can be used in you code or not.  And if it is effective,where can I change in you code."
I am trying to train it on MNIST dataset (with RGB channel) and I am getting the following error (on master branch code):     How can I fix this? I made appropriate changes in the `yml` file but it seems the there is some other problem!
"Hi sir,    I am reading your paper and reimplement your code. I see that in your Algorithm has sentence: ""while G has not converged"".   Q1: In your code, you write    I found and no see anything about difference of (pre_epoch_loss, current_epoch_loss) < threshold, or stop-when-updates-slow, ... If do the same, whether after the loop ended, loss has converged, doesn't it?  !     Q2: I know your code apply 1-Lipzchit countinous. However, will your model be always 100% converge?  Q3: What do you rely on to determine the best size of hole in raw_image (256x256)?  Thank you."
"Hi, I am researching on your great project, but I have some confusion about the convergence of D. I do not know that when D converges, which value will D come to? (How the curve will change). If possible, please send pictures of the curve that have D-convergenced. Thank you very much.     In below, there are 3 curve which I trained in 12 epochs (I am worry about the negative of loss)    !   "
"I have write the code to infer several images once, but it seems like some of these images will fail to inpainting. But when I use batchsize = 1, it works.  Can you tell my why,  I have checked the shape and content.   "
"Dear authors, could you please tell me what could be wrong, I use my own image and its generated mask for testing, I got the following error:     WARNING:tensorflow:  The TensorFlow contrib module will not be included in TensorFlow 2.0.  For more information, please see:    *      *      *   (for I/O related ops)  If you depend on functionality not listed there, please file an issue.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/neuralgym/ops/layers.py:142: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/neuralgym/callbacks/npz_model_loader.py:31: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/neuralgym/ops/gan_ops.py:138: The name tf.layers.Conv2D is deprecated. Please use tf.compat.v1.layers.Conv2D instead.    ---------------------------------- APP CONFIG ----------------------------------  num_gpus_per_job: 1  num_cpus_per_job: 4  num_hosts_per_job: 1  memory_per_job: 32  gpu_type: nvidia-tesla-p100  name: places2_gated_conv_v100  model_restore:   dataset: celebahq  random_crop: False  val: False  log_dir: logs/full_model_celeba_hq_256  gan: sngan  gan_loss_alpha: 1  gan_with_mask: True  discounted_mask: True  random_seed: False  padding: SAME  train_spe: 4000  max_iters: 100000000  viz_max_out: 10  val_psteps: 2000  data_flist:     celebahq: ['data/celeba_hq/train_shuffled.flist', 'data/celeba_hq/validation_static_view.flist']    celeba: ['data/celeba/train_shuffled.flist', 'data/celeba/validation_static_view.flist']    places2: ['data/places2/train_shuffled.flist', 'data/places2/validation_static_view.flist']    imagenet: ['data/imagenet/train_shuffled.flist', 'data/imagenet/validation_static_view.flist']  static_view_size: 30  img_shapes: [256, 256, 3]  height: 128  width: 128  max_delta_height: 32  max_delta_width: 32  batch_size: 16  vertical_margin: 0  horizontal_margin: 0  ae_loss: True  l1_loss: True  l1_loss_alpha: 1.0  guided: False  edge_threshold: 0.6  --------------------------------------------------------------------------------  Traceback (most recent call last):    File ""/content/generative_inpainting/test.py"", line 32, in        assert image.shape == mask.shape  AttributeError: 'NoneType' object has no attribute 'shape'"
Can we use this rep to train on my custom dataset which consists of .tif images having 16bit unsigned integer data in it (dtype = uint16)?  What changes do I need to make if I want to accomplish this?
"Hi @JiahuiYu,    I know this question asked many times. Actually I can run your code in my local properly. But in Google Colab I get this error.  I tried to create a DataFromFnames object and read_img within and this does not return an error.       This returns to an array:       But when I run train.py I still get the Image is None error. What can I do about this error?    Thank you so much for your work."
"First of all, I am very grateful to your significant study. Now, I have customized your model to my problem.   I have training data with predefined masks. So I customized your code to read it like:       And I also adding **masks** parameter to **build_graph_with_losses**.   Then I got stuck in **contextual_attention** with the error:    > Traceback (most recent call last):    File ""train.py"", line 47, in        g_vars, d_vars, losses = model.build_graph_with_losses(FLAGS, images, masks)    File ""/media/duongbao/data/Dropbox/code/HydroViet/git/dem-fill/generative_inpainting/inpaint_model.py"", line 159, in build_graph_with_losses      padding=FLAGS.padding)    File ""/media/duongbao/data/Dropbox/code/HydroViet/git/dem-fill/generative_inpainting/inpaint_model.py"", line 95, in build_inpaint_net      x, offset_flow = contextual_attention(x, x, mask_s, 3, 1, rate=2)    File ""/media/duongbao/data/Dropbox/code/HydroViet/git/dem-fill/generative_inpainting/inpaint_ops.py"", line 352, in contextual_attention      yi = tf.nn.conv2d_transpose(yi, wi_center, tf.concat([[1], raw_fs[1:]], axis=0), strides=[1,rate,rate,1]) / 4.    File ""/home/duongbao/miniconda3/envs/nu/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1232, in conv2d_transpose      filter.get_shape()[3]))  ValueError: input channels does not match filter's input channels, 16384 != 1024    I realized that images in a batch use a same mask. But I didn't know how to solve it.  Can you give me some ideas?  Thanks a lot."
Would you please provide the number of the image that is not used for training in Celeba-hq?  We want to use them to test.Thank you.
"Hi, Jiahui  I am grateful for you wonderful work and source code sharing.   I'm training a model  on my own datasets. and according to your advise, I didn't change any hyper parameter in` inpaint.yml` except the `num_gpus_per_job` from `1` to ` 1,2,3` with multi GPU. besides,  I have about 70,000 images, and 10% of them are for validation and 90% for training.  the model have trained about 48h until now. but the loss curves and inpaint results seem to be a little strange.   here are the loss curves  !   !   !     and here are the val inpainting results during training  some like these  !   !   !     and some like these seems to be worse  !   !   !     Q1. Is the loss curves normal? and what's meaning of these curves? the higher the value, the better the model ? or on the contrary?     Q2. the inpainting results are not very good. some are blur, and some are with random strange colors. and can u give me some advice about improvement? more data? more training time ? loss ratio? any method you think can be used to improve the results.    Thank u again and any help will be appreciated"
"cos similarity need calculate norm of the patch. But in the code, I am not find where to calculate the norm of the xi.  yi = tf.nn.conv2d(xi, wi_normed, strides=[1,1,1,1], padding=""SAME"")  I am confuse about this question, thank you very much~"
"I find the  the default parameter of train sep and iterate is a little big, so how much do you set for training of them?"
"Hi JiahuiYu, congratulations on the excellent inpainting work. I am trying to train the model on celebahq dataset, around 100k iterations into the training,  it is showing mode collapse behavior (For example, all the validation images on tensorboard having the same eye type in predictions when eyes are masked). Did you face any such issue? If yes, how did you resolve it? "
"Hello Jiahui,     First of all, thanks so much for your amazing work and courtesy of sharing the code.   However, I would appreciate it a lot if you could help check the error I got when training the model with my own database.     I am handling the single channeled images, in the dimensions of [64, 64, 1] (H, W, Ch). I made modifications to the inpaint_model.py     `x = gen_conv(x, FLAGS.img_shapes[2], 3, 1, activation=None, name='conv17')` (Line 66)   and   `x = gen_conv(x, FLAGS.img_shapes[2], 3, 1, activation=None, name='allconv17')` (Line 107)  also for the offset_flow, I simply slice a layer I feel like I have to keep the consistency of the dimensions between `x_stage1, x_stage2, offset_flow`.    But when I try to train the model each time it shows me the error:           I figured out the 32 is the `enqueue_size`. I doubt if it automatically squeezed the Tensor at the channel dimension. Could you please offer me some ideas to solve it?    My environment information:  NAME=""Ubuntu""  VERSION=""18.04.4 LTS (Bionic Beaver)""    python 3.7.4  tensorflow 1.14.0    Bests,  Yang"
"Hi @JiahuiYu,  First of all, thank you for your great work. I want to make a change in mask creation mechanism based on an epoch which currently training.  So I just want to ask you how to get epoch number in inpaint_ops.py?"
"Dear author:  I have two little questions for you：  1,  In the CoarseNetwork, the network input is:  x = tf.concat([x, ones_x, ones_x*mask], axis=3)    My question is tf.concat ""ones_x*mask"": 0 indicates the outside known region and 1 indicates the inside missing region.  Didn't it mean that this operation masks the known region and shows the missing region? Is it right？    Thank you for your answer。    2,ae_loss  batch_pos = batch_data / 127.5 - 1.  x1, x2, offset_flow = self.build_inpaint_net(xin, mask, reuse=reuse, training=training, padding=FLAGS.padding)    # local patches  losses['ae_loss'] = FLAGS.l1_loss_alpha * tf.reduce_mean(tf.abs(batch_pos - x1))  losses['ae_loss'] += FLAGS.l1_loss_alpha * tf.reduce_mean(tf.abs(batch_pos - x2))    Here x1 and x2 do not replace the known real region, so the loss here refers to the entire picture of network output as loss. May I ask why we should make loss on the known real region? Is it not the goal of inpainting to only make loss on the missing part?    Thank you for your answer。"
"I inpaint a cole flowers picture with a person mask using your trained model.cole flowers is colorful,and the inpaint region is uncoording to the whole picture .But the texture is ok.Could you give some advice?  "
"I rewrite train code into PyTorch, and test train result which is shown as follows.  **Mask**  !   **Train data result**  !   **Loss curve**  !   I'm not sure if this result is correct and should continue or not…      "
"Dear Dr.Yu,  Thank you for creating this awesome work. I am a little confusing about the object removal and user-guided editing. If we train the network using the object removal task, what is the ground truth? Or the network could be directly extended to object removal and creative editing tasks witout fine tunning?"
Dear author:      The online demo is really well-designed and I am absolutedly attracted by such a excelent tools to demonstrate the result of algorithm. Would you kindly publish the source code so that the community could benefit from it. Thank you again！
"Dear author:     Thanks for your insightful work again!     May I ask you for some tips. I am working the this project and want to boost the performance. It's not a research project, hence a lot of trivial approach can be taken into consideration.     I noticed that both the G and D network is rather small (~20MB) ， Have  you tried  larger models? does it help?     besides, I converted the contextual attention module to a rather simply self attention module, since self-attention module is relatively easy to work on embedded devices. Does the self attention harm the performance? did I have other better replacement?    Thank you for you help."
"Dear author:      Thanks for the context attention approach proposed.      Currently I'm trying to convert this model to mobile device. But I found the context attention module is not mobile device friendly. In other world, the extract_image_patches ops is not well supported in mobile device in tflite.       Hence I wonder Is there any alternative approach to replace contextual attention? Can we just remove the context attention module from the network? Does it significantly drop the performance?        Thank you very much ."
"Dear author:      Thanks for this paper and repo. it's really neat and well-documented!     However, Tensorflow 1 is extremely hard to use. It would be easy to use if you can adapt it to tensorflow 2.0 version. Thank you again."
"!   This is a list of my training files. On the left is a picture, and on the right is a sketch. When I change the guided parameter to true, the training is performed. The program is stuck in the interface below, and does not occupy the graphics card memory.  !   What is the problem?    "
"@JiahuiYu ,Thanks for your research!I am very sorry to bother you. when I train my custom dataset, the d_loss are fast converging, but the g_loss is negative and decreasing, Does the smaller g_loss(negative), the better the generated model performance?"
"Hi there!  I have a question: how to generate tensorflow model?. I downloaded a checkpoint and it's work really good, but I need generate a model.    Thank you!"
"When training a model, what image should be included in the training image folder and validation image folder ?    Is this correct?    -- training image   !     -- validation image   !     "
I don't understand the function of these two parameters. Can you tell me the the function of these two parameters(MAX_DELTA_WIDTH and MAX_DELTA_HEIGHT)
"Hi!    My experiments were performed on 30 pathological datasets with resolution of 1000X1000 pixel.     I want to remove the nucleus from the background and get a pure background picture, like this:  !     But the model I trained was bad. The nucleus cannot be completely removed from the background.  My result is:  !     The losses look like converged.  !     I trained the model on four pieces of 2080Ti.    Here is my profile :       I also searched for other issue, but I still don't know what the problem is. Could you give me some advice?"
"Thank you very much for sharing the code！  Using test.py, the effect of testing my own data is not consistent with your effect. Can you help me to have a look?  !   !   !   "
"Hello, @JiahuiYu   Thank you so much for doing this amazing research.    I had a problem while training with my own data set. I checked several issues but couldn't find a solution so I leave a post. I hope you understand it with a wide heart.    My data image size is 512x256 and my GPU is 2080Ti.    I checked GPU usage while training but I saw that it uses about 35% of the memory at the moment.    I changed the batch size because I thought this issue came from running out of memory, but I still get the phrase 'killed'.    I also confirmed that the file list was made properly.    There is no way to know where the problem is. I will wait for your advice.  This is my console output.         For reference, my inpaint.yml file is as below.     "
None
None
None
Could you give me evaluation results in terms of mean ℓ1 error and mean ℓ2 error on validation images of Celeba-HQ?  And is the pretrained model of Celeba-HQ rectangular mask or free-form mask?
None
I want to to train this model with some predefined masks and it's images.But this repository generates random masks for images.Can you tell me where I can put my code to train this model ? 
     this is comment here maybe ist for the last version
"Hi Jiahui,   I followed the instructions again. Now i have the following error.    File ""test.py"", line 66, in        output = model.build_server_graph(FLAGS, input_image)    File ""/home/staff/jireh/Year1-Ph.D/RD2/generative_inpainting-master/inpaint_model.py"", line 293, in build_server_graph      xin, masks, reuse=reuse, training=is_training)    File ""/home/staff/jireh/Year1-Ph.D/RD2/generative_inpainting-master/inpaint_model.py"", line 49, in build_inpaint_net      x = gen_conv(x, cnum, 5, 1, name='conv1')    File ""/home/staff/jireh/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 182, in func_with_args      return func(*args, **current_args)    File ""/home/staff/jireh/Year1-Ph.D/RD2/generative_inpainting-master/inpaint_ops.py"", line 48, in gen_conv      activation=None, padding=padding, name=name)    File ""/home/staff/jireh/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py"", line 417, in conv2d      return layer.apply(inputs)    File ""/home/staff/jireh/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 817, in apply      return self.__call__(inputs, *args, **kwargs)    File ""/home/staff/jireh/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 374, in __call__      outputs = super(Layer, self).__call__(inputs, *args, **kwargs)    File ""/home/staff/jireh/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 746, in __call__      self.build(input_shapes)    File ""/home/staff/jireh/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py"", line 165, in build      dtype=self.dtype)    File ""/home/staff/jireh/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 288, in add_weight      getter=vs.get_variable)    File ""/home/staff/jireh/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 609, in add_weight      aggregation=aggregation)    File ""/home/staff/jireh/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py"", line 639, in _add_variable_with_custom_getter      **kwargs_for_getter)    File ""/home/staff/jireh/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 1487, in get_variable      aggregation=aggregation)    File ""/home/staff/jireh/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 1237, in get_variable      aggregation=aggregation)    File ""/home/staff/jireh/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 540, in get_variable      aggregation=aggregation)    File ""/home/staff/jireh/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 492, in _true_getter      aggregation=aggregation)    File ""/home/staff/jireh/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 861, in _get_single_variable      name, """".join(traceback.format_list(tb))))  ValueError: Variable inpaint_net/conv1/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:      File ""/home/staff/jireh/Year1-Ph.D/RD2/generative_inpainting-master/inpaint_ops.py"", line 48, in gen_conv      activation=None, padding=padding, name=name)    File ""/home/staff/jireh/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 182, in func_with_args      return func(*args, **current_args)    File ""/home/staff/jireh/Year1-Ph.D/RD2/generative_inpainting-master/inpaint_model.py"", line 49, in build_inpaint_net      x = gen_conv(x, cnum, 5, 1, name='conv1')    Please here is my code:  import argparse    import os  import cv2  import numpy as np  import tensorflow as tf  import neuralgym as ng    from inpaint_model import InpaintCAModel    def dir_path(string):      if os.path.isdir(string):          return string      else:          raise NotADirectoryError(string)            parser = argparse.ArgumentParser()  parser.add_argument('--image', default='', type=str,                      help='The filename of image to be completed.')  parser.add_argument('--mask', default='', type=str,                      help='The filename of mask, value 255 indicates mask.')  parser.add_argument('--output', default='output.png', type=str,                      help='Where to write output.')  parser.add_argument('--checkpoint_dir', default='', type=str,                      help='The directory of tensorflow checkpoint.')      if __name__ == ""__main__"":      FLAGS = ng.Config('inpaint.yml')      # ng.get_gpus(1)      args, unknown = parser.parse_known_args()      #args = vars(parser.parse_args())      model = InpaintCAModel()      mask_paths = os.listdir(args.mask)      print(len(mask_paths))      img_paths = os.listdir(args.image)      for i in range(len(mask_paths)):          image = cv2.imread(args.image+img_paths[i])          #image = cv2.imread(args[""image""])          print(""Hi there {}, it's nice to meet you!"".format(args.image))          #print(image.shape[1])          #mask = cv2.imread(args[""mask""])          mask = cv2.imread(args.mask+img_paths[i])                    #mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)          #mask = mask[:,:,0:3]          print(mask.shape)          # mask = cv2.resize(mask, (0,0), fx=0.5, fy=0.5)                assert image.shape == mask.shape                h, w, _ = image.shape          grid = 8          image = image[:h//grid*grid, :w//grid*grid, :]          mask = mask[:h//grid*grid, :w//grid*grid, :]          print('Shape of image: {}'.format(image.shape))                image = np.expand_dims(image, 0)          mask = np.expand_dims(mask, 0)          input_image = np.concatenate([image, mask], axis=2)                sess_config = tf.ConfigProto()          sess_config.gpu_options.allow_growth = True          with tf.Session(config=sess_config) as sess:              input_image = tf.constant(input_image, dtype=tf.float32)              output = model.build_server_graph(FLAGS, input_image)              output = (output + 1.) * 127.5              output = tf.reverse(output, [-1])              output = tf.saturate_cast(output, tf.uint8)              # load pretrained model              vars_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)              assign_ops = []              for var in vars_list:                  vname = var.name                  from_name = vname                  var_value = tf.contrib.framework.load_variable(args.checkpoint_dir, from_name)                  assign_ops.append(tf.assign(var, var_value))              sess.run(assign_ops)              print('Model loaded.')              result = sess.run(output)              #cv2.imwrite('./places2_256/'+str(i)+'.jpg',((imgs[0]+1)*127.5).astype(""uint8""))              cv2.imwrite(args.output+str(i)+'.jpg', result[0][:, :, ::-1])  thanks    "
"Hi  JiahuiYu ,  I am currently following your paper and your inpainting algorithm is great. I have been trying to use the test method and I am getting this error ""AttributeError: 'Tensor' object has no attribute 'guided'  when testing using this code you posted. Please i need your help as i am trying to test a folder of images.    import argparse    import os  import cv2  import numpy as np  import tensorflow as tf  import neuralgym as ng  import time    from inpaint_model import InpaintCAModel    def dir_path(string):      if os.path.isdir(string):          return string      else:          raise NotADirectoryError(string)            parser = argparse.ArgumentParser()  parser.add_argument('--image', default='', type=str,                      help='The filename of image to be completed.')  parser.add_argument('--mask', default='', type=str,                      help='The filename of mask, value 255 indicates mask.')  parser.add_argument('--image_width', default='256', type=str,                      help='Image Width.')  parser.add_argument('--image_height', default='256', type=str,                      help='Image Height.')  parser.add_argument('--out', default='output.png', type=str,                      help='Where to write output.')  parser.add_argument('--checkpoint_dir', default='', type=str,                      help='The directory of tensorflow checkpoint.')      if __name__ == ""__main__"":      FLAGS = ng.Config('inpaint.yml')      # ng.get_gpus(1)      args, unknown = parser.parse_known_args()      sess_config = tf.ConfigProto()                                                                                                                                                                                                                  sess_config.gpu_options.allow_growth = True                                                                                                                                                                                                     sess = tf.Session(config=sess_config)                                                                                                                                                                                                                                                                                                                                                                                                                                                           model = InpaintCAModel()                                                                                                                                                                                                                        input_image_ph = tf.placeholder(                                                                                                                                                                                                                    tf.float32, shape=(1, args.image_height, args.image_width, 3))                                                                                                                                                                            output = model.build_server_graph(input_image_ph,1)                                                                                                                                                                                               output = (output + 1.) * 127.5                                                                                                                                                                                                                  output = tf.reverse(output, [-1])                                                                                                                                                                                                               output = tf.saturate_cast(output, tf.uint8)                                                                                                                                                                                                     vars_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)                                                                                                                                                                                    assign_ops = []                                                                                                                                                                                                                                 for var in vars_list:                                                                                                                                                                                                                               vname = var.name                                                                                                                                                                                                                                from_name = vname                                                                                                                                                                                                                               var_value = tf.contrib.framework.load_variable(                                                                                                                                                                                                     args.checkpoint_dir, from_name)                                                                                                                                                                                                             assign_ops.append(tf.assign(var, var_value))                                                                                                                                                                                                sess.run(assign_ops)                                                                                                                                                                                                                            print('Model loaded.')                                                                                                                                                                                                                                                                                                                                                                                                                                                                          with open(args.flist, 'r') as f:                                                                                                                                                                                                                    lines = f.read().splitlines()                                                                                                                                                                                                               t = time.time()                                                                                                                                                                                                                                 for line in lines:                                                                                                                                                                                                                                                                                                                                                                                                                                               image, mask, out = line.split()                                                                                                                                                                                                                 base = os.path.basename(mask)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   image = cv2.imread(image)                                                                                                                                                                                                                       mask = cv2.imread(mask)                                                                                                                                                                                                                         image = cv2.resize(image, (args.image_width, args.image_height))                                                                                                                                                                                mask = cv2.resize(mask, (args.image_width, args.image_height))                                                                                                                                                                                  # cv2.imwrite(out, image*(1-mask/255.) + mask)                                                                                                                                                                                                  # # continue                                                                                                                                                                                                                                    # image = np.zeros((128, 256, 3))                                                                                                                                                                                                               # mask = np.zeros((128, 256, 3))                                                                                                                                                                                                                                                                                                                                                                                                                                                                assert image.shape == mask.shape                                                                                                                                                                                                                                                                                                                                                                                                                                                                h, w, _ = image.shape                                                                                                                                                                                                                           grid = 4                                                                                                                                                                                                                                        image = image[:h//grid*grid, :w//grid*grid, :]                                                                                                                                                                                                  mask = mask[:h//grid*grid, :w//grid*grid, :]                                                                                                                                                                                                    print('Shape of image: {}'.format(image.shape))                                                                                                                                                                                                                                                                                                                                                                                                                                                 image = np.expand_dims(image, 0)                                                                                                                                                                                                                mask = np.expand_dims(mask, 0)                                                                                                                                                                                                                  input_image = np.concatenate([image, mask], axis=2)                                                                                                                                                                                                                                                                                                                                                                                                                                             # load pretrained model                                                                                                                                                                                                                         result = sess.run(output, feed_dict={input_image_ph: input_image})                                                                                                                                                                              print('Processed: {}'.format(out))             #cv2.imwrite(args.output+str(i)+'.jpg', result[0][:, :, ::-1])                                                                                                                                                                                                           cv2.imwrite(out, result[0][:, :, ::-1])                                                                                                                                                                                                                                                                                                                                                                                                                                                     print('Time total: {}'.format(time.time() - t)) "
"AttributeError: 'Tensor' object has no attribute 'guided'  when testing using this code    import argparse    import os  import cv2  import numpy as np  import tensorflow as tf  import neuralgym as ng  import time    from inpaint_model import InpaintCAModel    def dir_path(string):      if os.path.isdir(string):          return string      else:          raise NotADirectoryError(string)            parser = argparse.ArgumentParser()  parser.add_argument('--image', default='', type=str,                      help='The filename of image to be completed.')  parser.add_argument('--mask', default='', type=str,                      help='The filename of mask, value 255 indicates mask.')  parser.add_argument('--image_width', default='256', type=str,                      help='Image Width.')  parser.add_argument('--image_height', default='256', type=str,                      help='Image Height.')  parser.add_argument('--out', default='output.png', type=str,                      help='Where to write output.')  parser.add_argument('--checkpoint_dir', default='', type=str,                      help='The directory of tensorflow checkpoint.')      if __name__ == ""__main__"":      FLAGS = ng.Config('inpaint.yml')      # ng.get_gpus(1)      args, unknown = parser.parse_known_args()      sess_config = tf.ConfigProto()                                                                                                                                                                                                                  sess_config.gpu_options.allow_growth = True                                                                                                                                                                                                     sess = tf.Session(config=sess_config)                                                                                                                                                                                                                                                                                                                                                                                                                                                           model = InpaintCAModel()                                                                                                                                                                                                                        input_image_ph = tf.placeholder(                                                                                                                                                                                                                    tf.float32, shape=(1, args.image_height, args.image_width, 3))                                                                                                                                                                            output = model.build_server_graph(input_image_ph,1)                                                                                                                                                                                               output = (output + 1.) * 127.5                                                                                                                                                                                                                  output = tf.reverse(output, [-1])                                                                                                                                                                                                               output = tf.saturate_cast(output, tf.uint8)                                                                                                                                                                                                     vars_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)                                                                                                                                                                                    assign_ops = []                                                                                                                                                                                                                                 for var in vars_list:                                                                                                                                                                                                                               vname = var.name                                                                                                                                                                                                                                from_name = vname                                                                                                                                                                                                                               var_value = tf.contrib.framework.load_variable(                                                                                                                                                                                                     args.checkpoint_dir, from_name)                                                                                                                                                                                                             assign_ops.append(tf.assign(var, var_value))                                                                                                                                                                                                sess.run(assign_ops)                                                                                                                                                                                                                            print('Model loaded.')                                                                                                                                                                                                                                                                                                                                                                                                                                                                          with open(args.flist, 'r') as f:                                                                                                                                                                                                                    lines = f.read().splitlines()                                                                                                                                                                                                               t = time.time()                                                                                                                                                                                                                                 for line in lines:                                                                                                                                                                                                                                                                                                                                                                                                                                               image, mask, out = line.split()                                                                                                                                                                                                                 base = os.path.basename(mask)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   image = cv2.imread(image)                                                                                                                                                                                                                       mask = cv2.imread(mask)                                                                                                                                                                                                                         image = cv2.resize(image, (args.image_width, args.image_height))                                                                                                                                                                                mask = cv2.resize(mask, (args.image_width, args.image_height))                                                                                                                                                                                  # cv2.imwrite(out, image*(1-mask/255.) + mask)                                                                                                                                                                                                  # # continue                                                                                                                                                                                                                                    # image = np.zeros((128, 256, 3))                                                                                                                                                                                                               # mask = np.zeros((128, 256, 3))                                                                                                                                                                                                                                                                                                                                                                                                                                                                assert image.shape == mask.shape                                                                                                                                                                                                                                                                                                                                                                                                                                                                h, w, _ = image.shape                                                                                                                                                                                                                           grid = 4                                                                                                                                                                                                                                        image = image[:h//grid*grid, :w//grid*grid, :]                                                                                                                                                                                                  mask = mask[:h//grid*grid, :w//grid*grid, :]                                                                                                                                                                                                    print('Shape of image: {}'.format(image.shape))                                                                                                                                                                                                                                                                                                                                                                                                                                                 image = np.expand_dims(image, 0)                                                                                                                                                                                                                mask = np.expand_dims(mask, 0)                                                                                                                                                                                                                  input_image = np.concatenate([image, mask], axis=2)                                                                                                                                                                                                                                                                                                                                                                                                                                             # load pretrained model                                                                                                                                                                                                                         result = sess.run(output, feed_dict={input_image_ph: input_image})                                                                                                                                                                              print('Processed: {}'.format(out))             #cv2.imwrite(args.output+str(i)+'.jpg', result[0][:, :, ::-1])                                                                                                                                                                                                           cv2.imwrite(out, result[0][:, :, ::-1])                                                                                                                                                                                                                                                                                                                                                                                                                                                     print('Time total: {}'.format(time.time() - t)) "
"when i want to remove some objects,what's the input should i need?oral image ,mask and image whthout object?"
"Hi, JiahuiYu. Thanks for your useful code first.   I want to do batch testing on multi-GPU, could you please tell me how to enable GPU testing?  Sorry for the dumb question."
"set ""guided"" to true, and get a dataset from HED edge detection, and train it from scratch?    or    set ""guided"" to false, then train it with places256(or others), then change to ""guided: true"", and train it with HED?    if the first one, How big the dataset should be?  if the second one, it seems like different network between ""guided: true/false"", how to resume training?    Thanks a lot if you can answer me!"
"Dear Authors,  Thanks for open-sourcing your awesome paper.    I had some specific doubts regarding the input which is fed to the network.  I know ALMOST similar questions have been asked on this topic but, unfortunately, I could not get a full picture about the exact nature of the inputs which are fed to the network.    For example, if we consider the main input    tf.concat([x, ones_x, ones_x*mask], axis=3). My doubts are:    1. I understand (x) is the INCOMPLETE normalized image in [-1, 1]. Do you make the masked pixels = 0 in (x) or you make masked pixels = 1 in (x)    2. Why do we need to input the (ones_x) tensor ? It is simply an all-white input? Is it explicitly used anywhere for inpainting purpose?    3. Is the (mask) tensor = 0 on masked pixels or is it = 1 on masked pixels ?    Thanks,  Avisek    "
"When using the pretrained model with a custom mask, I get weird results:    !     <img width=""253"" alt=""PaintDotNet_2020-01-26_20-42-01"" src=""     !       ## Steps  1. `git clone    2. Download provided Places2 model to `model_logs\release_places2_256`  3. Install `tensorflow==1.7.0` and `git+   4. `python test.py --image examples/places2/case1_raw.png --mask examples/places2/case1_mask.png --output out.png --checkpoint_dir model_logs/release_places2_256`  5. `python test.py --image examples/places2/case1_raw.png --mask custom-mask.png --output out.png --checkpoint_dir model_logs/release_places2_256`    Step 4 is pretty much the command on the README and it works (result matches output example) but Step 5 generates the crazy output above. The `custom-mask.png` is the correct size with only 3 pure-white circles drawn on it.    Also not sure how this is even possible given      Raw mask file:  "
"Hi @JiahuiYu ,  I am eagerly waiting for your response to guide me for running your code including how to setup dataset  and how to setup environment.  "
what steps to create mask for this image    !   !       i need to test code on this image how can i create mask .png image  
"why when i test a new image result not good, i use your model     /content/drive/My Drive/Colab Notebooks/generative_inpainting-master/release_celeba_hq_256_deepfill_v2    i run this code in google colab and test image from your folder example result is very good    but when try this image using this code    !python test.py --image examples/input3.png --mask examples/mask3.png --output examples/output.png --checkpoint release_celeba_hq_256_deepfill_v2        this is an input image and mask    !   !     i get this result    !       why?      "
"Hi Jiahui, thanks for your amazing work and open source code. I try transfer learning based on the model which was pre-trained by places2. I use the original model as initialization parameters and the size of data file about 114MB. I retrain the model from allconv11 (the concat of the two branchs) and freeze the parameters of the former layers. However, the size of the final model only 88MB, it make me feel confused.    I change the these code of inpaint_model.py from:   g_vars = tf.get_collection(              tf.GraphKeys.TRAINABLE_VARIABLES, 'inpaint_net')  to:  g_vars = tf.get_collection(              tf.GraphKeys.TRAINABLE_VARIABLES, 'inpaint_net/allconv11/kernel:0')  and run the train.py.    Thank you for your help."
"Hi Jiahui, I want to know how to frozen the weights of the earlier layers and update only the weights of the last few layers? I want to try transfer learning in your pre-trained model. Thank you."
"Hi Jiahui, thanks for your amazing work and open source code. I hope this approach can also be used for the inpainting of the images with small missed area. I want to try transfer learning based on the model which was pre-trained by you as the initialization parameters. But I have no idea if I should retrain from the concat of the two branch in Refinement Network, or retrain from overall Refinement network and freeze the Coarse Network, or retrain the both Coarse Network and Refinement Network. Could you please provide some suggestions for me? BTW, I have a small dataset which is composed of 40 images with resolution 512 by 512."
Have you tried to use U-net as coarse generator ? Is there any reason you don't use Unet or Vgg in this model ? So thanks If you can tell me changing into Unet worth a try or not !
"Thank you for your awesome work. I am a beginner and I want use a small dataset for a transfer learning based on pretrained model of places2. The dataset has 50 images (resolution: 512 by 512), could you please rerecommend some parameter settings or a specific guidance?  The other question is why you set the beta1=0.5 for AdamOptimizer ? Thank you for your help"
None
"Hi Jiahui    Sometimes when I apply inpainting on some images, image is getting trimmed a little bit in its width or height.    please see this original image:  !     and the result:  !     width decreased from 1050px to 1048px.    Thanks in advance!"
"Thank you for your code. I trained the model with myself dataset, involve 4000 images for training and 1000 images for testing.  First, i have trained the model for ten days with four 16G GPU, and i test the model today, the result is unsatisfactory. I have no idea if there are some wrong options in my training process. This is my loss.         Second, i have trained the model for 121 epochs with the 4000 images for ten days. If the model have a lot of hyperparameters resulting in the training process so slow?"
"In the function of contextual_attention, i don't know which is the right grad flow?    1, y->wi_center->raw_w->b->x [in fact, b=x.  x, offset_flow = contextual_attention(x, x, mask_s, 3, 1, rate=2)]. This means tf.extract_image_patches (Line 283 in inpaint_ops.py) doesn't break grad flow.    2, y->yi->xi->f_groups->f->x [f =x. contextual_attention(x, x, mask_s, 3, 1, rate=2)] This mean resize operation (Line 289 and 290 in inpaint_ops.py) doesn't break grad flow.    Both guesses may be wrong. Thanks a lot!  "
"Hello, how can I get the model pretrained on imagenet(DeepFill v1)?"
"Hi @JiahuiYu     Sorry for disturbing you.    Does code support eagar execution?   I am trying to convert tensor into numpy in some part of your code but it does not support.   Hence, I used eagar execution then I got the the following error    ValueError: tf.enable_eager_execution must be called at program startup.    I tried and used the eagar_execution in the begining of the code. Again I got the same error.  I have tried many ways. I could not figured out. That's why, I approched you.    Again, sorry for disturbing you.  Kindly let me know my mistake and does code support eagar execution.    Thank you so much.      "
"Hi,    I hope to use this project on ImageNet dataset. I am wondering to know:  1. Have you used ImageNet to train the model before? Any suggestions on hyperparameters? Do you have pre-trained model?   2. I noticed that it seems I only need to provide images. Do I need to provide the masks during the training?    Thanks "
"hello,@JiahuiYu .I want to train model on my own dataset .Here is my understanding:  input of train are original images (flist format),don't need a mask.  input of test contain image(with free-form mask) and mask image.  Am I right？  Thank you ~"
"Hello Jiahui, thank you for sharing such wonderful works. I read through this paper and have some questions about the structure and implementation. Sorry, I'm new to Deep learning. My question seems a little naive.  I would like to continue training on my dataset with the pre training model you provided. My dataset is now 10000 images. I found that the loss is negative on the loaded pre training model, and the test results will introduce other colors.    Would you like to know if my data set is small or if I need to do wei'tiao.  Please give me some suggestions, thank you very much !!  !   !     !     The first picture is the result of the place 2 model test you provided  The second picture is the result of continuous training model test  The last one is the original picture    "
"What a wonderful work!  I am sorry to bother you,but I have some questions about user guide and model.  Q1:Could you share a model based on places2 (256 * 256) dataset and random mask?It is too long to retrain a model.  Q2:I have a question about user guide.I have searched  #375 #369,but I am still confused about it.I have corresponding places2 edge dataset.What should I do next?Set guieded to True  in inpaint.yml and rename the edge dataset as (image.jpg, guided_image.jpg)?  I would very appreciate if you could help me.  Thank you very much! "
" @JiahuiYu Hello, is the total number of high-resolution images of the Place2 dataset 1.8 million？"
"issue:tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:1 failed. Status: out of memory.    How to solve it?I have only one gpu.I have modified inpaint.yml,memory_per_job:1   I would very appreciate if you could help me."
"Hi, JiahuiYu:  Sorry to bother you. I am trying to using your network to inpaint tiny images, the losses is:  !   !   The gan loss really confuses me. And the results are not very satisfying(The size of image is 64x64) :  !   !   What's more, I found that in some case(Red rectangle border in result1.), the pixels near the reference pixels are fuzzy, while the piexls far away from the reference pixel (bottom right corner) are obvious dots. Could this be caused by Spatially_discounted_reconstruction_loss? In my case ,I set 0.95 to SPATIAL_DISCOUNTING_GAMMA.  And I also found that in very few case, these obvious dots appear in some area that should be supposed to be smooth! As shown:  !         I really want to know， according to to the loss, am I on the right way? If not, do you have any suggestions for me to solve the current dilemma?  Thank you so much!"
Thanks a  lot!
" In the contextual attention model, the cosine similarity is employed to measure the similarity, why not use other similarity measurement method?"
"May I add you on WeChat, I want to ask some questions"
"Does the contextual attention layer default to using the same mask for all images in a batch?  ` m = tf.extract_image_patches(          mask, [1,ksize,ksize,1], [1,stride,stride,1], [1,1,1,1], padding='SAME')`  `m = tf.reshape(m, [1, -1, ksize, ksize, 1])`  `m = tf.transpose(m, [0, 2, 3, 4, 1])  # transpose to b*k*k*c*hw`  `m = m[0]`"
"@JiahuiYu     Thanks for you continuous support.    **The results are good in Tensorboard view during the training  but while testing it was bad.**   Could you tell me, what is the reason for this.  I have tried many ways, but i could not find it.    Tensorboard view result during training process    !     Testing result  !   !     Thanks.           "
or how to see the loss curve and accuracy curve?  I read carefully the relevant code in the neuralgym trainer and logger sections
"Can you, please, add information to Read.me about that fact how many time requires training process using GPU (V100, for instance) on Celeba and Celeba-HQ datasets? Thanks!     (information about training on Places2 I found in the original paper)"
"Hello Jiahui, thanks for sharing such wonderful work. I read through the paper and have some questions regards to both structure and implementation. Sorry I am new to machine learning and my questions may seems a bit naive.    Q1. why did you use ELU for activation of the x and sigmoid activation for the mask? are these just on experimental trial? I searched around and most of the articles suggest to avoid use the sigmoid activation it kills the gradient in deep neural network. the ELU does not have the gradient kill issue but it is very computational heavy compare to ReLU or leaky ReLU.    Q2. the cnum is number of filters used right? is the number already reduced by 25% as indicated in the paper? like 48 = 64*0.75? i saw some post uses less filter numbers. is it going to affect the training result significantly? and what would be the suggested range to pick that number from?    Q3. what is are those parameters under #train?   train_spe: 4000  max_iters: 100000000  viz_max_out: 10  val_psteps: 2000  those are the default value i got and i am not sure what effect they have on the training process.    Q4, I am trying to train the ImageNet and for my local PC, i have Nvidia GTX 1060 6GB but it cannot go up to 16 batch_size, and i tried to train them with around 8 batch size but the result shows blurry inpainted region. should I adjust the l1_loss_alpha to the 0.5? or 2? I saw two issue post suggesting different values. and I only have these three parameters under #loss section.  ae_loss: True  l1_loss: True  l1_loss_alpha: 1.    Q5. Since I don't have enough graphic memories, I applied for google cloud compute engine and have VM created using their deep learning VM from the market place. with same version of the tensorflow and tensorflow-gpu from my local computer, the VM was not able to recognize the GPU installed. I tried change the trainer to multiGPU trainer and setup the parameters in inpaint.yml. I am only using one GPU for the VM and I was using the tesla V100 16gb and the network gpu utilization just blows up and the kernel got stuck and died. in the live nvidia-smi gpu monitoring I saw error like ""cannot allocate the memory"". after it blows up.    Thank you very much."
"I train the model and break it. And then I want to resume the model,but it did not load the checkpoint.   Could you please tell me how to resume the model.  I put the checkpoint in the ./logs, and the model_restore is also ./logs  "
"hello，thanks for  release deepfillv2. I have loade code and pretrained models. I want to train my dataset(256*256), so I use pretrained models to fine-tune train, I changes some trianing parameters，but the train loss is negative and decreasing，I suspect it's my own data problem. so I use place2 validation set to fine-tune train，the train loss is also negative，I don't know why ，can you tell me，Is code error？thanks  "
hello ，I fine tune mydataset in the trian model，18000 train images，so I modify  training parameters： train_spe and max_iters，other is unchanged，but the trian loss is negative and decreasing      
1)Taking more time and system is hanging to load images through neuralgym(ng.data.DataFromFNames) for the images of high resultion(1024x1024). Any alternative way is there?    _Originally posted by @sandhyalaxmiK in  
"hi @JiahuiYu   thanks for sharing your code.   when i run the train.py it has a error log like this.  ****  Instructions for updating:  To construct input pipelines, use the `tf.data` module.  image is None, sleep this thread for 0.1s.  ERROR:tensorflow:Exception in QueueRunner: 'NoneType' object has no attribute 'shape'  Exception in thread Thread-2:  Traceback (most recent call last):    File ""/usr/lib/python3.6/threading.py"", line 916, in _bootstrap_inner      self.run()    File ""/usr/lib/python3.6/threading.py"", line 864, in run      self._target(*self._args, **self._kwargs)    File ""/usr/local/lib/python3.6/dist-packages/neuralgym/data/feeding_queue_runner.py"", line 194, in _run      data = func()    File ""/usr/local/lib/python3.6/dist-packages/neuralgym/data/data_from_fnames.py"", line 142, in        feed_dict_op=[lambda: self.next_batch()],    File ""/usr/local/lib/python3.6/dist-packages/neuralgym/data/data_from_fnames.py"", line 179, in next_batch      random_h, random_w, align=False)  # use last rand    File ""/usr/local/lib/python3.6/dist-packages/neuralgym/ops/image_ops.py"", line 50, in np_random_crop      image = np_scale_to_shape(image, shape, align=align)    File ""/usr/local/lib/python3.6/dist-packages/neuralgym/ops/image_ops.py"", line 23, in np_scale_to_shape      imgh, imgw = image.shape[0:2]  AttributeError: 'NoneType' object has no attribute 'shape'  ****  i'm change some flag in inpaint.yml file (* guieded False --> True)  An error occurs only when the flag is changed to True(Guieded)  When it was flag 'False', it confirmed that it is good working"
Hello @JiahuiYu   Can I use this model in iOS and Android platform?  Would you explain me about the approach?  Thank you
"Firstly, thanks for the sharing the code.  when i run the train.py,it has a problem that TypeError: conv2d() got an unexpected keyword argument 'dilations' in the inpaint_ops.py.    It would be very nice if any hints or suggestion.  Thanks in advanced."
"Hi,  Thanks for sharing the code. Currently working on removing tattoo from human image. I am collecting raw image with tattoo, mask image as input (created using photo-shop) and an image by removing tattoo as an output. I need high quality images of 2k resolution. Can you help me of how to go further to continue my process??  Can you share pre-trained logdir (model directory) to view  the model in in tensor board which makes me to understand in an easy manner.    Thanks in advance.    Regards,  SandhyaLaxmi"
"hi,Does stage2's input include a mask? Just a 3-channel image in RGB?"
"Dear @JiahuiYu     Thanks for your continuous support.    I have seen many issues related to inpainting result is not good. You have answered those issues.  I have followed all those answers and tried with CelebA HQ dataset.   However, I also got bad results.    I have done the mask as small a region for better results  !       only 10 epoch with results. (seems to be somewhat good)    !       with 35 epoch results   !     **But by increasing an epoch, the result is deteriorating.**  **Till now i unable to produce good results after 100 epoch with CelebA HQ dataset.**    **Kinldy help me, where i have made a mistake and how to produce good results.**    Many thanks.    "
"Hello, @JiahuiYu     - Did you visualize the learned gating maps of different size just by resizing them to the size of the original image with interpolation method?    - It seems that different channels of the output feature map of the gated convolution layer focus on different parts (background, mask, and sketch) in the coarse network, is it exactly the same in the refinement network? Will the highlighted mask regions in the gating map shrink in the deeper layer of the refinement network?    Thank you very much :)"
"When testing with my own images, I don't know how to generate a mask file of the image.Please help me.I am a novice."
"In the test.py ，there are:  grid = 8  image = image[:h//grid*grid, :w//grid*grid, :]  mask = mask[:h//grid*grid, :w//grid*grid, :]  why do it ？ it changes output size，size of the output  image  is not equal to the size of the input image."
"I used pretrained place2 weights to train my own dataset. no training parameter was modified, just used the one the author provided.   there are about 10k images in my trainset with size 512X512. i trained nearly 200 epochs. the result is bad. it seems the all losses do not converge.  what's wrong? what should i do next?  !   !   !   "
"Hello @JiahuiYu   I'm sorry to bother you, but I have a problem need to solve. I want to train the images of a single channel now, like[256, 256, 1], but I don't know what should be modified to meet my needs. I have seen the solutions to some related problems before, but this problem still hasn't been solved. Could you please give me some detailed guidance on what needs to be modified? Thanks!"
"Hello @JiahuiYu   When I tried to train model as your guide, I can't see it work.  I already posted   about this issue to #52 , but I couldn't get answer.       Actually I've waited for few hours after I faced this issue. Also it never using GPU memory.  Not sure what I need to do.  Please help me."
"Hi @JiahuiYu   Sorry for the disturbance.    Previously, I have asked the questions about multi gpu for training. You have suggested that to use deepfill v2 for multi-gpu training.   I have done that work based on your suggestion and it is working fine now.  But the training speed is lower than single gpu. Then you have informed me that gpu speed is multiply by batch speed. (For example :  0.39 * number of GPUs batch / sec)  I have used 3 gpu but speed is not performed the above manner.  Kindly see the below picture for reference  Hence, kindly let me know, where i made a mistake and where should i change the code.    Thanks in advance    !   "
"@Hello, on deepfillv1 I replaced wgan-gp with relative GAN. During the training, I found that the blank part of the picture repaired by the rough network appeared to be a color. I thought for a long time, I don't know where the problem is, I hope you can help me!  !   !     "
It seems there should be 2 arguments.    !   
"Hi Jiahui, thanks so much for your great work and patiently answering my previous questions.     May I check in what format the flist should be in? Thanks in advance!    !   "
"Hi @JiahuiYu, thanks for your work.  I am a freshman in neural network.   At present, I am tring to using your model directly to train a model for the picture inpainting( The size of pictures is 32x32(or 16x16), and the inpaint block is 16x16(or 8x8 if the picture size is16x16) located in the bottom-right of the picture ), using DeepFill v1. The generative loss converges very quickly at first but it seems almost unchanged after several hours with gtx2080 (Even days of training, still almost unchanged) and the inpaint result is kind of bad... My training data is DTD dataset, and I crop it to smaller size(32x32) before training. The whole dataset contains about 4000000 pictures at last.  Do you have any Suggestions about this? Sorry to bother you, I was so puzzled what to do now. If I need to modify the model for this problem, how should I get started?"
"Hi @JiahuiYu     Thanks for your prompt responses,    I have done the multigpu training based on you are suggestion using deepfill2 . (issue #349 )  Now, I am currently working on deepfill1 not deepfill2.   You have suggested to use deepfill2 of multigpu in deepfill1.    It is working fine now. gpu's id :        **But Now, started the another issue that is: (issue #323)**     !     I have read the issue #323. still the issue is open.   Hence, kindly let me know, how to find the model is training and when it will be complete.    Thanks in advance      "
"Hi,  I am trying to use multi gpu for training. I have made significant changes on that based on the previous issues and as per your directions. I used the gpu's      **Kindly find the below batches speed:**    !     **Kinldy guide me, how to solve this or is it correct?**      **Note :  I have done the issues : #14**     **I got the following error:**  !     Kinldy guide me about the difference. Why I unable to use multigputrainer.  Thanks in advance  "
"Hi,can you tell me how to generate multiple patches on an image and a mask,I try to generate patches through inpaint.py, but only one patch can be generated on a mask.thanks"
"I am currently training an unmodified version of your architecture with a custom dataset. During training, I'm seeing unexpected output and an attention map where the unknown region is white, rather than the known region.    !   _TensorBoard image output at step 86,000 with batch size 16_    Is this normal behavior? To try to find out whether this is due to one of my own changes I have reverted back to your original DeepFill V2 version, but still am getting the same results.    I'd appreciate your opinion on this! Thank you!    "
thanks
Can the pretrained imagenet weights in DeepFill1 be used in Deepfill2?  thank you!
"Could you please publish some pre-trained model weights for some other datasets like ImageNet? For some reason, I need a pre-trained model which is trained on ImageNet. I don’t have conditions to train the model but still want to use it to Inpaint my masked images. Thanks a lot."
"Hello,  Thanks for the awesome repo. I was trying to run the demo for ImageNet; however I keep getting some dimension mismatch error. I am using the DeepFillv1;    Using the pre-trained Imagenet, we get conv weights mismatch error (refer ""error1.png""). Model is expecting weights of dimension    !   !   !   !   "
"Hello.  JiahuiYu, first of all, thanks a lot for your brilliant work!  I'm new in GAN training and would like to ask you a few questions about training.   Currently, I'm working on inpainting of texture of stone tiles and the goal is to remove cracks. To test the performance I've used the pre-trained places-2 models it gave me a relatively good result, but it still would be nice to improve it.  Obviously the domains of the places-2 and my dataset are different.   How do you think is it worth to try to continue the training of the places-2 model on my data?    "
"Hi,  After training on my dataset, the filled result seem to have more impact from contextual attention rather than the shapes. The model is not learning the feature and shape.  !   !      Is there a way to minimize the imapct in the contextual layer?    "
"I have tried to download ""snap-0.data-00000-of-00001"" from two different PCs, four different browsers, on two different network connections, both signed in and out of my Google Account. Unfortunately, the download always fail at around the 30MB mark; on *all* of my PCs, browsers and network environments.     Could you please try downloading this file? I think it is possible that it is corrupt in Google Drive. I have never seen a download fail halfway before from Google Drive.    Thank you!"
"@JiahuiYu Hi,    I trained this model with my own data and downsampled images were input.Before iters 10200 the mean L2 distance between predicted image and ground truth kept going down to 0.01(calculated by (-1,1)data,wasn't normalized to(0,255)), and subsequently which was up to 0.5 and even worse.The iters 10200 results are great,but I want higher accuracy actually.So do you have any suggestion?I really appreciate it.  Thank you.  !   !   !   "
"Hi,  can you explain and give me an example for input and mask images to the test.py.  It is little bit confusing with research paper and github code.  Thanks in advance. kindly make it clear."
"Thank you for your great work! Here is a problem I met when I was training on my own dataset consisting of images of width 256 and height 128. I cannot fully understand this error, but I found an issue about the same problem (#247 ). Unfortunately the OP closed it without explaining how he solved it.    There was no error when I was training on square images from Places2, so I suspect it might be related to the size of images or yml settings? I would greatly appreciate it if you could help me!    <img width=""1440"" alt=""Screen Shot 2019-11-05 at 10 59 10 PM"" src=""   "
"Some problem in ""get_conv"" function, it returns half of the expected output dimension.  "
"Sorry to bother again, I want to confirm how to set up train_spe and max_iters.   Now I have a dataset with 60,000 images. I want to run 40 epoches with batchsize of 16.  Conventionally, that means 1 epoch = 60,000 / 16 = 3750 iterations.  So I set train_spe=3750, max_iters=3750*40  But according to the training speed (0.43batches/sec) , finish one epoch will need 7 hours.    Comparing with other model training on Pytorch, that's too long.   "
"Hi,   I am trying to repeat your code that is contextual attention. but i got the following error.  Kindly let know, where i made a mistake    input image  !     mask image    !     output image  !     "
I dont know what's wrong about my tensorboard.  I just want to print something such as consumed time when an iteration is finished.   
"I am using the latest version as of 2019.11.3. 23:55 to train on Places2 dataset.  Random crop in the yml is set to True.  This is the error I am receiving, I will greatly appreciate your help!  <img width=""1440"" alt=""Screen Shot 2019-11-03 at 11 58 18 PM"" src=""   "
"Hi I am trying to train on Places2 dataset using the latest updated version, but encountered the following problem:  !   After this line appears, it just gets stuck there for a long time and I have to shut it off.  I read the issue about removing the progress bar, but I do not think this is the issue here because nvidia-smi shows that the GPU is not being utilized.  I checked to make sure that in train.py line 11 I use tf.device('/gpu:0'), and to use Trainer instead of MultiGPUTrainer since I am only using 1 GPU.   For the yml file, I changed dataset to 'places2', random_crop to True, reduced max_iters and  val_psteps, and kept everything else the same.  My flists are in the correct place.    Really thank you for your contribution, and I would greatly appreciate your insight on what might have caused this problem. "
"Hi,  I am a bit confused about mask_s = resize_mask_like(mask, x)  Here   you said it defines the background and foreground.     q1. Why is the mask_s generated after conv6 layer of stage1?    q2. i am also having difficulties in understanding how it is seperating f and b. f is the squared unknown region and b is the known outside region. If that is it then how the mask_s is generated by resizing?  "
"Hi @JiahuiYu   Thank you for your work.  I am excited with your work and tried to test your model on my machine.  I cloned your repository v2.0.0 and installed required dependencies successfully.  I downloaded pretrained model to model_logs directory and could run some example images in your folder.  But I can't get the result with my custom images, because its generating wrong result.  !   !   !     Would you explain me why I get such result? It worked with your sample images, but does not work with my sample images.  Also is there any restriction in input/mask image resolution?  Because I saw your sample images have same resolution.    Looking forward to hearing from you asap.  Thank you"
"i try the test script, but got error:  argmax() got an expected keyword argument 'output_type'"
"Hello @JiahuiYu ,  My g_loss, d_loss and l1_loss are as follows. I feel that g_loss and d_loss are fast converging, but my l1_loss feeling is still falling. I saw that the picture I fixed is good and bad, but the overall is not very good.May I continue to train? The convergence of the final model depends on the l1_loss   !   !         "
"Hi @JiahuiYu ， i have some trobule when i run the test.py , i dont know what happen?   Traceback (most recent call last):    File ""/home/clay/DL/generative_inpainting/test.py"", line 56, in        var_value = tf.contrib.framework.load_variable(args.checkpoint_dir, from_name)    File ""/usr/local/lib/python3.6/site-packages/tensorflow/contrib/framework/python/framework/checkpoint_utils.py"", line 80, in load_variable      return reader.get_tensor(name)    File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 334, in get_tensor      status)    File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 519, in __exit__      c_api.TF_GetCode(self.status.status))  tensorflow.python.framework.errors_impl.NotFoundError: Key inpaint_net/conv2_downsam-ple/kernel not found in checkpoint    thx for your reply"
"Hi, I am a beginer to Tensorflow.  I am trying to fine-tune a inpaint model based on your 'release_imagenet_256' model.  According to the instruction, i download the model and change the checkpoint names. I also change the MODEL_RESTORE parameter into : 'release_imagenet_256'.    However, When i start training, the output of tensorflow seems to say that the pretrained model's weights are all zeros! I got these:    Trigger callback: Trigger ModelRestorer: Load model from model_logs/release_imagenet_256/snap-0.  - restoring variable: inpaint_net/allconv11/bias:0  - restoring variable: inpaint_net/allconv11/kernel:0  - restoring variable: inpaint_net/allconv12/bias:0  - restoring variable: inpaint_net/allconv12/kernel:0  - restoring variable: inpaint_net/allconv13_upsample/allconv13_upsample_conv/bias:0  - restoring variable: inpaint_net/allconv13_upsample/allconv13_upsample_conv/kernel:0  - restoring variable: inpaint_net/allconv14/bias:0  - restoring variable: inpaint_net/allconv14/kernel:0  - restoring variable: inpaint_net/allconv15_upsample/allconv15_upsample_conv/bias:0  - restoring variable: inpaint_net/allconv15_upsample/allconv15_upsample_conv/kernel:0  - restoring variable: inpaint_net/allconv16/bias:0  - restoring variable: inpaint_net/allconv16/kernel:0  - restoring variable: inpaint_net/allconv17/bias:0  - restoring variable: inpaint_net/allconv17/kernel:0  - restoring variable: inpaint_net/conv1/bias:0  - restoring variable: inpaint_net/conv1/kernel:0  - restoring variable: inpaint_net/conv10_atrous/bias:0  - restoring variable: inpaint_net/conv10_atrous/kernel:0  - restoring variable: inpaint_net/conv11/bias:0  - restoring variable: inpaint_net/conv11/kernel:0  - restoring variable: inpaint_net/conv12/bias:0  - restoring variable: inpaint_net/conv12/kernel:0  - restoring variable: inpaint_net/conv13_upsample/conv13_upsample_conv/bias:0  - restoring variable: inpaint_net/conv13_upsample/conv13_upsample_conv/kernel:0  - restoring variable: inpaint_net/conv14/bias:0  - restoring variable: inpaint_net/conv14/kernel:0  - restoring variable: inpaint_net/conv15_upsample/conv15_upsample_conv/bias:0  - restoring variable: inpaint_net/conv15_upsample/conv15_upsample_conv/kernel:0  - restoring variable: inpaint_net/conv16/bias:0  - restoring variable: inpaint_net/conv16/kernel:0  - restoring variable: inpaint_net/conv17/bias:0  - restoring variable: inpaint_net/conv17/kernel:0  - restoring variable: inpaint_net/conv2_downsample/bias:0  - restoring variable: inpaint_net/conv2_downsample/kernel:0  - restoring variable: inpaint_net/conv3/bias:0  - restoring variable: inpaint_net/conv3/kernel:0  - restoring variable: inpaint_net/conv4_downsample/bias:0  - restoring variable: inpaint_net/conv4_downsample/kernel:0  - restoring variable: inpaint_net/conv5/bias:0  - restoring variable: inpaint_net/conv5/kernel:0  - restoring variable: inpaint_net/conv6/bias:0  - restoring variable: inpaint_net/conv6/kernel:0  - restoring variable: inpaint_net/conv7_atrous/bias:0  - restoring variable: inpaint_net/conv7_atrous/kernel:0  - restoring variable: inpaint_net/conv8_atrous/bias:0  - restoring variable: inpaint_net/conv8_atrous/kernel:0  - restoring variable: inpaint_net/conv9_atrous/bias:0  - restoring variable: inpaint_net/conv9_atrous/kernel:0  - restoring variable: inpaint_net/pmconv1/bias:0  - restoring variable: inpaint_net/pmconv1/kernel:0  - restoring variable: inpaint_net/pmconv10/bias:0  - restoring variable: inpaint_net/pmconv10/kernel:0  - restoring variable: inpaint_net/pmconv2_downsample/bias:0  - restoring variable: inpaint_net/pmconv2_downsample/kernel:0  - restoring variable: inpaint_net/pmconv3/bias:0  - restoring variable: inpaint_net/pmconv3/kernel:0  - restoring variable: inpaint_net/pmconv4_downsample/bias:0  - restoring variable: inpaint_net/pmconv4_downsample/kernel:0  - restoring variable: inpaint_net/pmconv5/bias:0  - restoring variable: inpaint_net/pmconv5/kernel:0  - restoring variable: inpaint_net/pmconv6/bias:0  - restoring variable: inpaint_net/pmconv6/kernel:0  - restoring variable: inpaint_net/pmconv9/bias:0  - restoring variable: inpaint_net/pmconv9/kernel:0  - restoring variable: inpaint_net/xconv1/bias:0  - restoring variable: inpaint_net/xconv1/kernel:0  - restoring variable: inpaint_net/xconv10_atrous/bias:0  - restoring variable: inpaint_net/xconv10_atrous/kernel:0  - restoring variable: inpaint_net/xconv2_downsample/bias:0  - restoring variable: inpaint_net/xconv2_downsample/kernel:0  - restoring variable: inpaint_net/xconv3/bias:0  - restoring variable: inpaint_net/xconv3/kernel:0  - restoring variable: inpaint_net/xconv4_downsample/bias:0  - restoring variable: inpaint_net/xconv4_downsample/kernel:0  - restoring variable: inpaint_net/xconv5/bias:0  - restoring variable: inpaint_net/xconv5/kernel:0  - restoring variable: inpaint_net/xconv6/bias:0  - restoring variable: inpaint_net/xconv6/kernel:0  - restoring variable: inpaint_net/xconv7_atrous/bias:0  - restoring variable: inpaint_net/xconv7_atrous/kernel:0  - restoring variable: inpaint_net/xconv8_atrous/bias:0  - restoring variable: inpaint_net/xconv8_atrous/kernel:0  - restoring variable: inpaint_net/xconv9_atrous/bias:0  - restoring variable: inpaint_net/xconv9_atrous/kernel:0    And here is my 'inpaint.yml' , i set other parameters as default:    DATASET: 'moire'  # 'tmnist', 'dtd', 'places2', 'celeba', 'imagenet', 'cityscapes', 'moire'  RANDOM_CROP: False  VAL: False  LOG_DIR: moire_256  MODEL_RESTORE: 'release_imagenet_256'  # '20180115220926508503_jyugpu0_places2_NORMAL_wgan_gp_full_model'    NUM_GPUS: 1  GPU_ID: [1]  # -1 indicate select any available one, otherwise select gpu ID, e.g. [0,1,3]  TRAIN_SPE: 10000  MAX_ITERS: 1000000  VIZ_MAX_OUT: 10  GRADS_SUMMARY: False  GRADIENT_CLIP: False  GRADIENT_CLIP_VALUE: 0.1  VAL_PSTEPS: 1000    DATA_FLIST:    moire: [      'data_folder/moire/train_shuffled.flist',      'data_folder/moire/validation_static_view.flist'    ]  Did i set any parameters wrong? Could you help me please @JiahuiYu   "
None
"Hi，Jiahui Yu @JiahuiYu   ， thanks for your code   i have some problem in the test ，i use this command to test ‘python3.6 test.py --image examples/celeba/celebahr_patches_164036_input.png --mask examples/center_mask_256.png --output examples/output.png --checkpoint_dir model_logs/20191009214622181966_clay-MS-7B47_celeba_NORMAL_wgan_gp_full_model_celeba_256 ’     but have some trouble as follow:  Traceback (most recent call last):    File ""test.py"", line 23, in        ng.get_gpus(1)    File ""/usr/local/lib/python3.6/site-packages/neuralgym/utils/gpus.py"", line 70, in get_gpus      ' [(gpu id: num of processes)]: {}'.format(sorted_gpus))  SystemError: No enough gpus for dedicated usage. [(gpu id: num of processes)]: [(0, 7)]    Thanks for your reply  "
"Thank you for your code, @JiahuiYu   Here , I will train the CelebA,  i want to know , should i  crop the image to 128x128 or resize the image to 128x128 ？     Thanks again"
"I have a GPU. I modified  inpaint.yml as follow:   DATASET: 'celeba'   RANDOM_CROP: True  and then I run train.py.  but something is wrong, anyone can help me?    WARNING:tensorflow:  The TensorFlow contrib module will not be included in TensorFlow 2.0.  For more information, please see:    *      *      *   (for I/O related ops)  If you depend on functionality not listed there, please file an issue.    WARNING:tensorflow:From D:\Anaconda3\lib\site-packages\neuralgym\ops\layers.py:142: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.    WARNING:tensorflow:From D:\Anaconda3\lib\site-packages\neuralgym\callbacks\npz_model_loader.py:31: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.    WARNING:tensorflow:From D:\Anaconda3\lib\site-packages\neuralgym\ops\gan_ops.py:138: The name tf.layers.Conv2D is deprecated. Please use tf.compat.v1.layers.Conv2D instead.    ---------------------------------- APP CONFIG ----------------------------------  DATASET: celeba  RANDOM_CROP: True  VAL: False  LOG_DIR: full_model_celeba  MODEL_RESTORE:   GAN: wgan_gp  PRETRAIN_COARSE_NETWORK: False  GAN_LOSS_ALPHA: 0.001  WGAN_GP_LAMBDA: 10  COARSE_L1_ALPHA: 1.2  L1_LOSS_ALPHA: 1.2  AE_LOSS_ALPHA: 1.2  GAN_WITH_MASK: False  DISCOUNTED_MASK: True  RANDOM_SEED: False  PADDING: SAME  NUM_GPUS: 1  GPU_ID: -1  TRAIN_SPE: 10000  MAX_ITERS: 1000000  VIZ_MAX_OUT: 10  GRADS_SUMMARY: False  GRADIENT_CLIP: False  GRADIENT_CLIP_VALUE: 0.1  VAL_PSTEPS: 1000  DATA_FLIST:     celebahq: ['data/celeba_hq/train_shuffled.flist', 'data/celeba_hq/validation_static_view.flist']    celeba: ['data/celeba/train_shuffled.flist', 'data/celeba/validation_static_view.flist']    places2: ['data/places2/train_shuffled.flist', 'data/places2/validation_static_view.flist']    imagenet: ['data/imagenet/train_shuffled.flist', 'data/imagenet/validation_static_view.flist']  STATIC_VIEW_SIZE: 30  IMG_SHAPES: [256, 256, 3]  HEIGHT: 128  WIDTH: 128  MAX_DELTA_HEIGHT: 32  MAX_DELTA_WIDTH: 32  BATCH_SIZE: 16  VERTICAL_MARGIN: 0  HORIZONTAL_MARGIN: 0  AE_LOSS: True  L1_LOSS: True  GLOBAL_DCGAN_LOSS_ALPHA: 1.0  GLOBAL_WGAN_LOSS_ALPHA: 1.0  LOAD_VGG_MODEL: False  VGG_MODEL_FILE: data/model_zoo/vgg16.npz  FEATURE_LOSS: False  GRAMS_LOSS: False  TV_LOSS: False  TV_LOSS_ALPHA: 0.0  FEATURE_LOSS_ALPHA: 0.01  GRAMS_LOSS_ALPHA: 50  SPATIAL_DISCOUNTING_GAMMA: 0.9  --------------------------------------------------------------------------------  Error reading GPU information, set no GPU.  'nvidia-smi' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���  ���������ļ���  Traceback (most recent call last):    File ""C:/Users/dell/PycharmProjects/generative_inpainting/train.py"", line 39, in        with open(config.DATA_FLIST[config.DATASET][0]) as f:  FileNotFoundError: [Errno 2] No such file or directory: 'data/celeba/train_shuffled.flist'    Process finished with exit code 1  "
None
"Hello, when I trained myself on the celeba dataset, my generator loss first dropped normally, but it went up until 50,000 steps,And when I tested the image in the example, the result was very bad.Can you help me?"
"I want to use the pretrained model said in ReadMe file. But, Where is the models dir?"
"Awesome work and very thanks @JiahuiYu for each answer, which has greatly helped my work.  However, I still have some questions to ask.  Q1：I see you say, the model that only contains the generator is only 4.1M. Does it contain two stages, stage1 and stage2? Are the two branches of Stage2 (normal series and attetion modules) included?  Q2: Is the end of stage1 also a tanh and clip_by_value end?  Q3: You said,”So the output of our discriminator is (ImageWidth / 32) * (ImageHeight / 32) * 256.” The number of discriminator layers is 6, so the stride of the first or the last layers is 1?   Q4: Does the discriminator need a clip_by_value to keep the value between -1 and 1？  "
"Hello, I wonder when training the Places2 dataset, whether part of the data set is used or the whole data set? And how long it takes for the training? Thanks."
"Excuse me, how did you adjust the 178*218 image to 256*256 in the CelebA model you trained?"
"Excuse me,         I use the following code to change the CelebA image with resolution 178*218 to 256*256, and then use your trained CelebA model to inpaint the image. What's wrong? Why the completion effect is so bad? and how do you adjust the image to 256*256?         Thank you very much!!    code:  import neuralgym as ng  import tensorflow as tf  import cv2    config = ng.Config('inpaint.yml')  fnames = ['3.png']  data=ng.data.DataFromFNames(fnames,config.IMG_SHAPES,random_crop=config.RANDOM_CROP)  images = data.data_pipeline(1)  sess = tf.Session(config=tf.ConfigProto())  tf.train.start_queue_runners(sess)  image = sess.run(images)  print(image[0].shape)  cv2.imwrite('256.png', image[0])  "
"It's a great work! While I running 'train.py' in GPU, some error happens:    Traceback (most recent call last):  File ""train.py"", line 87, in         'model': model, 'data_flist': data, 'config': config, 'loss_type': 'd'},  File ""/usr/local/lib/python3.5/dist-packages/neuralgym/callbacks/secondary_trainer.py"", line 22, in __init__       Trainer.__init__(self, primary=False, **context)  File ""/usr/local/lib/python3.5/dist-packages/neuralgym/train/trainer.py"", line 41, in __init__       self._train_op, self._loss = self.train_ops_and_losses()  File ""/usr/local/lib/python3.5/dist-packages/neuralgym/train/trainer.py"", line 250, in train_ops_and_losses        loss = self.context   TypeError: multigpu_graph_def() got an unexpected keyword argument 'data_flist'    Why is this error raised?"
"I have tried deepFill v1's demo,it's funny.Can v2's demo be released someday?"
!   Shown at the end of the paper.
"I tried training your network with my own data and it worked fine. After that, I trained on the pre trained places2 model and got a negative loss after half of the first epoch. How are negative loss values even possible?"
"In the test.py, the following line concatenate image and mask along axis = 2. But I think it should be axis 3, ie the channel dimension. Thanks.     "
"Hi, thank you for the great work! And I am very sorry to bother you. After I modified some layers based on your model and trained it again(Training process seems to be ok, I can print the value of all the variable ). It seems that during test the model I trained doesn't work and just output the input image as follows. Could you please give me some advice? Thank you so much!      !   "
"Sorry to bother you, I tried to repair the grayscale image with your network, and my training set is a small amount of grayscale image, but the restored image is colored. I tried changing the number of channels in the image to 1 in inpaint.yml, which would throw an error, and I tried to find the answer in the issue, but I didn't find it. Looking forward to your reply! Thank you"
"!   !     Here is the result of my model, which uses gated conv and trained on rectangles，and you can see the texture is strange if zoom in. Have you ever met this situation in deepfill2?   "
"please sir i really want the gui . I took forward this project as a final year project of bs degree graduation. My supervisor is asking for gui . i tried my best to make gui but i am using the static mask image . i want to draw mask at run time just as your's gui .    screen shot of ui that i created but it is not appreciated by supervisor    !      If you provide it to me this will really help me. Please sir provide it , I will be very thankful for you   "
"Hello,     I struggle to understand the meaning of the attention map when the colors do not belong to the background. I don't know if I am clear, but for example in the following :   !    !     To reconstruct the patch, we borrow information from the patch itself (pale colors), so I am a bit confused. You wrote in the paper that white means the pixel attends on itself, but I am not sure of what it means.      "
its difficult for me to interact with your code.  Please  share the gui code also as you posted in your site   jiahuiyu.com/deepfill/
"In pretrained places2 model, Which scenes in 365 scenes he trains? "
1. Were you seeing any different results with zero-padded vs symmetric convolution kernels?  2. Have you experimented with pixel shufflers for upsampling instead of deconvolutions/upsampling? 
"Hello, my question is what does the mask used for testing on places2 looks like when you do quantitative experiment?"
"Considering that the model don't use any post processing, too many losses (GAN loss and l1 loss only) and that training pictures don't aligned key points, just random cropping, I was amazed by some results. However, results are not very good for mask in left eye. Completed left eyes are not quite the same as rights eyes. This may be solved using parsing loss in papar——Generative Face Completion.  I trained the models using the default parameters for 5 days. I used one Ti2080 GPU, with which the model runs 0.57 baches/sec.  !   !   !   "
"batch_incomplete = batch_pos*(1.-mask), generate bbox missing regions seem like is black???    x = tf.concat([x, ones_x, ones_x*mask], axis=3), why input of model is this? , thank you "
None
"hi, if I want to use this model at logo  removal, when retraining model ,Can you give me some advice? thank you "
"!   i have gpu,but get_gpus method can not select.  !   "
"Hi Jiahui,  I would like to appreciate your excellent work of inpainting. I am trying to use it on the recovery of the compressed images.    I have a few questions about training weights:    1)  In the default .yml file, the GAN_LOSS_ALPHA is set to 0.001.      Is it a typo and supposed to be 0.0005 for wgan?  Does it mean you have a fixed step size value rather than decreased fixed value?    2) What is the loss value printed out in the logger?      Is it for discriminator only? Is it for **training loss** or **validation loss**?    3) If it is training loss, could you help to explain the increasing loss like this?   !     Although from   you mentioned it is normal to have small turbulence of training loss, but I am assuming it shouldn't keep increasing.   How do you set a wiser way of stop training **except for manually stop training** or **manually set MAX_ITERS**?    4) If there is no way to stop training automatically, what if I would like to exact the optimal weights manually (e.g. when the loss is the minimum). Then how could we save the weights for **all epochs**.   In the model_log folder, it only showed the lastest 5 snap- of the weights, (i.e. the previous weights has removed)            "
"I would like to fine tune your release_places2_256 model for training on 16 bits images, do you have any idea of which layers weigths I could freeze ? Like the ones that realize a general task.     Thank you"
"Hi, I followed the instructions in readme to run the pre-trained model given ""release_celeba_hq_256"" but the results don‘t seem very good:  python test.py --image 003983.png --mask examples/center_mask_256.png  --output output.png --checkpoint_dir model_logs/release_celeba_hq_256/    The input image i used is:  !    , which I randomly selected from the data set Celeba-HQ（256x256）.      And the result I got:  !   Can you tell me what I did wrong?Thanks."
"Hi, I followed the instructions in readme to run the pre-trained model given ""release_celeba_hq_256"" but the results don‘t seem very good:  python test.py --image 003983.png --mask examples/center_mask_256.png  --output output.png --checkpoint_dir model_logs/release_celeba_hq_256/    The input image i used is:  !    , which I randomly selected from the data set Celeba-HQ（256x256）.      And the result I got:  !   Can you tell me what I did wrong?Thanks."
"Awesome work and thanks @JiahuiYu for being super responsive and helpful with all the issues here.    I try to check various closed issues, but wasn't able to go trough all, so please excuse in advance if some of my questions have been answered already.  Q1: You wrote in the deepfillv2 paper, that     > To maintain the same efficiency with our baseline model, we slim the base model width by 25%    So that means, you cut out 25% of the layers in coarse and refine net? If so, which layers?    Q2: This has been partly discussed in #182 , I think. Do you include ones also in deepfillv2? So that G input is **cat[img, mask, sketch, ones] (6 dims)** and D input is **cat[img, mask, sketch] (5 dim)**?    Q3: As far as I understood, you used L1-loss on the coarse and refined image + SN-Patch loss on refined image only. If so, how are these 3 losses added up? You mentioned a 1:1 relation, does that mean for **final loss L = L1_coarse + L1_refine + SN-Patch_refine**?    Q4: I am still quite confused about implementation of contex-attten layer, so not sure if this question is actually valid. Why are you extracting 4x4 patches from the background for _raw_w_ which later gets convolved with 3x3 patches from foreground. So why **4x4 and not 3x3**?    "
"I want to input 16-bit images as input.    So, i changed int32, int to float32, float    but     i received this error message.    TypeError: Tensors in list passed to 'input' of 'PyFuncStateless' Op have types [ , int32, int32, int32, int32] that are invalid.    do you have an idea of the reason ?    Thanks for reading my question.  "
"I find the result from model of release_places2_256 is diffrent from your papper  !      !       in your papper  !     is the model   "" release_places2_256""  in your papper?    I test some other images ,some results are like this has boundary traces. How should I improve?  "
!   How can the coefficient |x| be canceled in the right term. Is there something that i misunderstand?
"`yi = tf.nn.conv2d(xi, wi_normed, strides=[1,1,1,1], padding=""SAME"")    yi = tf.nn.softmax(yi*scale, 3)    yi = tf.nn.conv2d_transpose(yi, wi_center, tf.concat([[1], raw_fs[1:]], axis=0), strides=[1,rate,rate,1]) / 4.  `  The foreground patch are used in these lines, but I don't understand where normalization is canceled. Could u explain it in detail?"
"Hello,     I have trained a model with my own dataset composed of 20000 16-bits images. The results are  quite coherent in structures and colors but the inpainted square limits are still very noticeable, do you have an idea of the reason ?    Here are the results :     The image :       !     The inpainting result :     !   "
"Hello,     I would like to resume training of your release_places2 model, for training on 16 bits images, is there a mean to freeze some layers or to modify the learning rate for the network to focus on learning dataset-specific features in the subsequent layers ?    "
"Hi,  Great Work.    I have a doubt regarding the pre-trained weight on Places2 Dataset. When I opened the file it contains the pre-trained weights of ""inpaint_net""/ generator variable, I couldn't find the weight of Discriminator network. Is there a specific reason why you didn't store the discriminator network weights?     2 - If so, discriminator weights doesn't matter while resuming the training?    !   "
"Wonderful work!  It is so awesome a project!  I have read DeepFillv2 release #62 carefully,but I still have some questions about deepfillv2.  Why we use sketch channel(user guide)?Given a broken picture,if we use HED to generate the edge flist,and training it with edge flist.That's equal to know the broken area in advance.So what is the meaning to use sketch channel?By changing the broken area in a user guide way to generate the picture we want?How do you do in this sketch channel?Do you just put the original picture,mask and edge flist as input and train?  I would very appreciate if you could help me,thank you very much!  Best wishes to you!"
"Hi I don't want to use random masks. In my dataset, each image has a predefined mask. Could you tell me the easiest way to do that? Thanks."
"Why do you normalize background patch by wi_normed = wi / tf.maximum(tf.sqrt(tf.reduce_sum(tf.square(wi), axis=[0,1,2])), 1e-4), but not normalize foreground. (Cosine similarity mentioned in your paper both normalizes these two kinds of patches.)"
"Hello @JiahuiYu, first thank you for your work,  I have a question about the content of the ""raw_incomplete_predicted_complete"" data in tensorboard. I assume that when you train your model with validation data (VAL=true), it shows the result of inpainting on validation images , but if you don't have validation data, does it show the result of inpainting on training images ?     "
"Hi, I found that you  have transposed twice in contextual_attention layer, but I can't understand why. Could you tell me the reason?    Thanks a lot !!!     "
"[2019-06-02 08:27:32 @weights_viewer.py:43] - weight name: discriminator/dout_local_fc/kernel:0, shape: [5120, 1], size: 5120  [2019-06-02 08:27:32 @weights_viewer.py:43] - weight name: discriminator/dout_local_fc/bias:0, shape: [1], size: 1  [2019-06-02 08:27:32 @weights_viewer.py:43] - weight name: discriminator/dout_global_fc/kernel:0, shape: [286720, 1], size: 286720  [2019-06-02 08:27:32 @weights_viewer.py:43] - weight name: discriminator/dout_global_fc/bias:0, shape: [1], size: 1    Weights in last dense layer of discriminator are fixed, how can I train the model with different shape images and masks?  When I train with my own masks with different shape, the error occurs:  The last dimension of the inputs to `Dense` should be defined. Found `None`.    Thanks for your reading!"
Dear team  I hope you are fine. could you please inform me about your dataset. did you use the same images for training and testing?
!   
"Hi @JiahuiYu     Thanks for your wonderful work. I am working on GANs, I used a free form mask like the below -    !       And below is the result of it.  !     What could be the possible reason for the appearance of bridge feature in the inpainted area?  Why free form mask doesn't work in DeepFillV1?     "
"### Question to DeepFillV2 implementation  Hey,  so I implemented DeepFill2 as well as I could based on your papers and the questions you answered here.   I trained on a facade dataset for around 6 days on a 1080.   In addition to groundtruth, image with mask, result and flow I also output the coarse image and the whole image after contextual attention (so x1 and x2).    Here is an example I am quite happy with:  !         But in other cases there are these weird artifacts that the inpainted pixels look like they are not really taken from the picture, but pretty independent, e.g. in the top of this image:  !     So regarding my results I have the following questions:    **Q1**: In the output of the coarse image the mask is still clearly visible and seems to not really have a lot of detail in it, but it still seems just gray. Did your network also learn to just leave the mask area gray in the coarse image?    **Q2**: Do you have an idea what might cause the artifacts in the second image?    ### Questions to Extension of DeepfillV2  Additionally to reimplementing DeepfillV2 I want to extend it by not only providing an edge image like you did for the guided inpainting part, but I want to provide a complete segmentation map.    What I do is that I take a segmentation map with 8 classes, then one_hot encode it and attach it to the rgb image as additional channels as an input. I provide segmentation information for the masked out area as well.   So I attach the segmentation information to the stage1 input, the stage2 input and the discriminator input.     The problem is that the results during training are quite bad for this use case.  !   This result is after 2.5 days of training on the cityscapes dataset.        !   This result is after 1.5 days on a facade dataset.      As you can see it looks like the contextual attention stage is matching features which are not in the image itself.    My assumption is that since I give the result from coarse as well as the segmentation as input the contextual attention module is ""confused"" and matches features from the segmentation map. The rgb image has only 3 channels, while the segmentation has 8 channels, therefore it might not focus on the rgb image features.    **Q3**: Do you think in this case it might make sense to not give the segmentation as input to the contextual attention state so that it focuses on image features, or are there any other changes that you think might help?    Thank you very much for reading this long issue with lots of questions.  I am looking forward to your thoughts.    Best regards  ThJOD  "
"Hi, congrats on this very interesting work! I just wanted to point out that whenever the input images size is not a multiple of 4, the output of the convolution will not be the same shape, resulting in an error. For instance, keeping track of the shapes of the image we see:     I understand this can be quite obvious when convolving with stride>1, but maybe it would be worth to mention this in the documentation or add a more explanatory error message to avoid this being a recurrent error (I lost quite some time trying to understand it😅).    Thanks!"
"During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""train.py"", line 6, in        import tensorflow as tf    File ""C:\Users\admin\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 24, in        from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import    File ""C:\Users\admin\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 49, in        from tensorflow.python import pywrap_tensorflow    File ""C:\Users\admin\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in        raise ImportError(msg)  ImportError: Traceback (most recent call last):    File ""C:\Users\admin\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in        from tensorflow.python.pywrap_tensorflow_internal import *    File ""C:\Users\admin\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in        _pywrap_tensorflow_internal = swig_import_helper()    File ""C:\Users\admin\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper      _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)    File ""C:\Users\admin\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module      return load_dynamic(name, filename, file)    File ""C:\Users\admin\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic      return _load(spec)  ImportError: DLL load failed: 找不到指定的模块。      Failed to load the native TensorFlow runtime.    See      for some common reasons and solutions.  Include the entire stack trace  above this error message when asking for help."
"Traceback (most recent call last):  File ""test.py"", line 23, in   ng.get_gpus(1)  File ""C:\Users\admin\AppData\Local\Programs\Python\Python37\lib\site-packages\neuralgym\utils\gpus.py"", line 54, in get_gpus  gpu_id = int(s[0])  IndexError: list index out of range      But my cpu have 20"
"Traceback (most recent call last):    File ""test.py"", line 23, in        ng.get_gpus(1)    File ""C:\Users\admin\AppData\Local\Programs\Python\Python37\lib\site-packages\neuralgym\utils\gpus.py"", line 54, in get_gpus      gpu_id = int(s[0])  IndexError: list index out of range"
"There are so many losses, I am confused about which loss curves are used to judge that the model has converged, the result picture is complete_raw_incomplete.  !   !   !   !         "
"Your work is very great and novel, but I have some questions about deppfill2.  If we process the image with holes by HED, the regions around the boundary of holes will be labeled with thickly white lines obviously, how do u handle these problems? I am looking forward to your answers, thank you very much."
"hi, i trained the model first ,then the training interrupted once. so i rusume training based on my model first, but the process stopped here for hours. i just change the MODEL_RESTORE in inpaint.yml.   This is the logs first time i trained  !   This is the first model_logs file name and the changed MODEL_RESTORE code after  !   This is where the training process stopped  !   so i want to know   1. why my training stopped and how to solve it  2. if i just change the IMG_SHAPE to [64,64,3] in inpaint.yml to speed up the training, do i still need to change other code?   thanks a lot!"
"I want to use a 16 bit TIFF image file for training.    I changed the code 'uint8' to 'uint16', '(pixel / 127.5) - 1' to '(pixel / 32768) - 1' and changed 'cv2.imread' to 'cv2,imread(..., -1)'.    The training was run without problems, but the test stuffed all the missing region with zeros.    Could I get advice on how to use a 16-bit training image to get a 16-bit image with a value between 0 and 65535?    Thank you for your interesting research.  Thank you for reading the questions."
" File ""/home/anaconda3/envs/tensorflow/lib/python3.6/site-packages/neuralgym/utils/gpus.py"", line 70, in get_gpus      ' [(gpu id: num of processes)]: {}'.format(sorted_gpus))  SystemError: No enough gpus for dedicated usage. [(gpu id: num of processes)]: [(0, 4)]"
"Hey,  First of all thank you for publishing the code to DeepFillV1 and answering so many questions.  I still have one question I could not find answered in the other issues.    Below is the architecture of DeepFillV2 from the paper.  !     In the paper you mention that     > the output is a 3-D feature of shape R h×w ×c , where h, w, c  > representing the height, width and number of channels respectively.    Q1:  Do you mean with height, width and channels the height, width and channels of the last layer of the discriminator, so (ImageWidth / 32) * (ImageHeight / 32) * 256 ?    If so you can ignore question 2 and 3.    In case you mean with the quote, that h = image height, w = image width, c = number of input channels:   Q2:  If the input to the discriminator is an RGB image + mask + guidance channel I would assume c = 3 + 1 + 1 = 5 , correct? So basically with channels you mean the sum of all channels in the input, or maybe just of the image, so 3 channels for rgb?    Q2:   In the image the last layer has size 256 * (H / 32) * (W / 32). How do you get from there to h * w * c? Do you use upsampling / deconvolution? Or do you add another layer that has so many layers so you can then reshape? E.g. the next layer will have shape (32 * 32 * c) * (H / 32) * (W / 32) = C * H * W     Best regards  ThJOD"
"Hai @JiahuiYu . Thanks for your work, it really helps me a lot in my research. But I have a question, how can I generate the attention map color coding as it was mentioned in your paper?       !   "
"My checkpoint file doesn't have `.txt`.But it still say:  `Couldn't find 'checkpoint' file or checkpoints in given directory`  My os is Windows8.1,doesn't this matters?And I run test.py with only cpu.  Thanks for your reading!"
"Hello,     I am trying to train a model with my own dataset composed of tiff images thus with IMG_SHAPE set to (3,256,256). To make things work I try to reshape the tensor data in train.py to (256,256,3), but I get the following error when I launch the training : ""Cannot feed value of shape (32, 3, 256, 3) for Tensor 'Placeholder:0', which has shape '(?, 3, 256, 256)'"" And I am quite not sure how to resolve it.    Thank, you, regards"
"I have put the model file(you provide through 4 links) in the `model_logs`,but after running   `python test.py --image examples/celeba/celebahr_patches_164036_input.png --mask examples/center_mask_256.png --output examples/output.png --checkpoint_dir model_logs/release_celeba_256/`,  an error occurred:  ValueError: Couldn't find 'checkpoint' file or checkpoints in given directory  model_logs/release_celeba_256/  Please help me.Thanks for your reading!"
!   How to understand these two lines of code？
"  !     why is foreground parameter X and background parameter X the same? and I think the size of background parameter X and the size of parameter mask_s is the same, so how does the parameter mask_s  to indicate patches not available for background?  "
None
"my gpu is P6000, but training speed is very low.    it's a 0.02bathes/sec( batches:16)    i think gpu is not working...    what should i do? please give me answer"
"Hello JiahuiYu! Thank you for your excellent work.There a question when I run the train.py which has been bothering me for a long time,so I'll be very grateful if you could watch at it for me.  Here is the error case:        ]]    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""train.py"", line 113, in        trainer.train()    File ""C:\ProgramData\Anaconda3\envs\tensorflow-gpu\lib\site-packages\neuralgym\train\trainer.py"", line 133, in train      cb.run(sess, step)    File ""C:\ProgramData\Anaconda3\envs\tensorflow-gpu\lib\site-packages\neuralgym\callbacks\secondary_trainer.py"", line 26, in run      self.train()    File ""C:\ProgramData\Anaconda3\envs\tensorflow-gpu\lib\site-packages\neuralgym\train\trainer.py"", line 141, in train      feed_dict=self.context ]]    Caused by op 'random_uniform_5', defined at:    File ""train.py"", line 88, in        'model': model, 'data': data, 'config': config, 'loss_type': 'd'},    File ""C:\ProgramData\Anaconda3\envs\tensorflow-gpu\lib\site-packages\neuralgym\callbacks\secondary_trainer.py"", line 22, in __init__      Trainer.__init__(self, primary=False, **context)    File ""C:\ProgramData\Anaconda3\envs\tensorflow-gpu\lib\site-packages\neuralgym\train\trainer.py"", line 41, in __init__      self._train_op, self._loss = self.train_ops_and_losses()    File ""C:\ProgramData\Anaconda3\envs\tensorflow-gpu\lib\site-packages\neuralgym\train\trainer.py"", line 250, in train_ops_and_losses      loss = self.context     File ""train.py"", line 23, in multigpu_graph_def      images, config, reuse=True)    File ""D:\learning\generative_inpainting-master\inpaint_model.py"", line 148, in build_graph_with_losses      bbox = random_bbox(config)    File ""D:\learning\generative_inpainting-master\inpaint_ops.py"", line 114, in random_bbox       ]]    Thx a lot!"
"Hi, JiahuiYu,  we'd like to convert the release checkpoint file to frozen PB. BUT we don't find the pipeline.conf for export_inference_graph.py tool in your repo.  could you do us a favor and send out the pipeline.conf ?    Thx."
"Outstanding work!!! Thanks for your excellent work, I have some questions about the code.     First of all,in inpaint_model.py,I  have some questions in the network.As is shown in the picture,   the python sentence  begining with the symbol of # is what I  thought and modified , I wonder if I was wrong.  !   !     In addition,what is the meaning of build_server_graph and build_static_infer_graph?Especially in   build_server_graph.How could we use our own mask to train,such irregular mask?could we get better results in this way?    Last but not the least,by simply modifying Network Structure(adding some layers etc) and the function loss rather than use the thought of gated conv could we improve  image inpainting quality ?    I would appreciate if you could help me,thank you"
Really awesome  project . I want to buy your project .How can i contact with you .
"I'm noticing that in the refinement network, every time there is a downsample convolution, the number of output channels only increases in the output of the subsequent layer. Isn't the point of doubling the channels with each downsample to mitigate information loss in the bottleneck? Is it intentional to wait one layer before doing this, for example:     "
"I have 4 GTX 1080Ti GPUs and each gpu can handle batch size of 16 that means if I use all the gpus I can change batch size to 64. But when I do that my GPUs ran out of memory.  Am assuming here that   uses data parallelism to split input data (64 batch size) in to 4 gpus where each gpu gets 16 batch of images.       Because of that Issue I can only train on batch size of 16, whether I use 4 gpus or 1 gpu.   What are your thoughts about this?"
"Hi,  Thanks for the awesome repo. Just wanted to clarify a doubt. By reading your paper, I got a feel (I might be wrong) that you used masks of random size with max hole size being 128X128. But from the code it appears that mask size is always restricted to be 128X128.  Can you please comment on  1. what you actually did while reporting the results in paper  2.  If I train with on 128X128 holes, is there is a down side for that?    Regards,  Avisek"
"Hello,@JiahuiYu.  I'm curious about the performance of deepfill2 on celeba-HQ, when the mask is regular, so could you share the pretrained model?Or would you mind sending me the inpaintng results when the hole is rectangular with the size of 128*128 and located in the middle of the mask?Here is my e-mail:luminouscmy@163.com  Thx!"
None
test results：  !   !   !   !   !   !   !   !   training data like this：  !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !                   
Sorry to bother you. How long time do you train the imagenet model ? and how many epochs?Do you train the model with only 1 GPU ?  Thank you very much.
"I've tried change the test.py to detect each frame in the video with opencv, but out of memory happened after couple frames. Could you take a look?   `"
"Heyy here's another question!    When you are implementing PatchGan loss in DeepFillv2, will you try to discriminate the scores in the output feature map of a inpainted image? I mean, in the original paper, since the entire input is fake, they can just label all feature points as fake. But the output of an inpianter is partially real and partially fake, so is it the case that we only take those fake pixels (inside the mask) into consideration when we calculate loss?      Thank you!!"
"I am trying out test_contextual_attention() from inpaint_ops.py. How can I get the attention map from the function?     To do this, I tried to evaluate (i.e tf.Session.run(.)) the ""offsets"" and ""flow"" from contextual_attention, but I came across an error:  >  Incompatible shapes:  ]]    Any idea about how to settle this? Thank you.  "
"Hey Jiayu,    Here is another question! (Thank you for your speedy response this afternoon lol)    In inpaint_ops.py line 271, it says `mm = tf.cast(tf.equal(tf.reduce_mean(m, axis=[0,1,2], keep_dims=True), 0.), tf.float32)`    The 0th axis should be the batch size, I remember. I understand why we are averaging through the kernel, but why are we averaging along the batch as well? Don't different images have different masks in a batch?    Thanks!"
"Hello,@JiahuiYu.  You said all layer in discriminator apply like below:  x = leaky_relu(conv2d_spectral_norm(x))  But in original patchGAN as well as discriminator in SNGAN,there is no activation in last layer.   Why you add an activation in last layer?"
Hi @JiahuiYu   I have a few doubts related to training of the model and its speed     1. The training stops at 10% on google colab. We were training for 100 images just as a sample. It is taking the same amount of time to reach 10% irrespective of the number of images used for training (We tried using 50 images as well). What could be the reason for this?    !     2. We could not find the epoch value as well. Which parameter of the inpaint.yml file indicates the epoch value?  3. How much time would it usually take to train the model for celeba dataset on google colab using 1 GPU?
"Hi, JiahuiYu! Thank you for your research and sharing!    According to your instructions, all models you provided are trained with images of resolution 256x256 and largest hole size 128x128.But the dataset Places2 contains images which short side is 512.    Did you randomly crop 256x256 images from the original images as training set?     Can I train the model with images have random resolution just like the images in Places2 dataset? "
"With reference to the issue #125. I have been trying to perform the training of the model through google colab but for some reason when i try to execute the train.py file it gives an assertion error similar to issue #125     The error thrown is:   <img width=""868"" alt=""Screenshot 2019-03-19 at 11 54 27"" src=""     "
"Hi, Jiahui,   Thanks for sharing this repo! With respect to the hole sizes, when you say the hole sizes should be smaller than 128x128, does this mean that performance for non-square holes of size 100 x 163 (which have an area less than 128x128 ) will deteriorate?  "
Can this model be trained on other mask dataset?
Could you provide a pretrained model on Paris?
"Hi, JiahuiYu. There are only two lines in every neuralgym log, like this:    > [INFO  2019-03-10 20:51:45 @__init__.py:79] Set root logger. Unset logger with neuralgym.unset_logger().  > [INFO  2019-03-10 20:51:45 @__init__.py:80] Saving logging to file: neuralgym_logs/20190310205145801144.    I want to see more log info, because I am training my model on cloud gpu server. If there are no more info, I can not know where the training process are when I log in next time.  Hos should I add more info in the log file?"
"When I try to run train.py it shows this error:    File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/neuralgym/data/data_from_fnames.py"", line 13, in        from ..yolo3.yolo.YOLO import detect_image  ModuleNotFoundError: No module named 'neuralgym.yolo3'    <img width=""1144"" alt=""Screenshot 2019-03-15 at 12 17 21 PM"" src=""   "
"Hi,  Congrats on the awesome work.    I was trying to train the model on DTD texture dataset. I had several doubts about DTD implementation:  1. During training did you randomly crop images to 256X256 or resized to 256X256?  2. Since the dataset size is small did you use any data augmentation?  3. During testing, did you run on the model on the original image resolution or did you resize the test images to 256X256?    Thanks in advance.  Avisek  "
None
"I am training the DeepFillv2 with my own data. During the training, something is wrong in the visualized offset flow of attention map. There are color pixels in the sky region where is not in the mask. I assume that there should be white in all non-mask regions. Here are my results.  !     In my implementation, masks are different in one batch. I change the attention code as following and take mi in m_groups as mm in the original code:  `   int_ms = mask.get_shape().as_list()      m = tf.extract_image_patches(          mask, [1,ksize,ksize,1], [1,stride,stride,1], [1,1,1,1], padding='SAME')      m = tf.reshape(m, [int_ms[0], -1, ksize, ksize, int_ms[3]])      m = tf.transpose(m, [0, 2, 3, 4, 1])  # transpose to b*k*k*c*hw        mm = tf.cast(tf.equal(tf.reduce_mean(m, axis=[1,2,3], keep_dims=True), 0.), tf.float32)      m_groups = tf.split(mm, int_ms[0], axis=0)`"
I would like to ask you if I use some irregular face data and irregular mask to retrain the network? Is it feasible? And I am not very clear about the place where I should use my mask code to replace the code of your rectangular mask.
I would like to ask if your test face images are included in the training set? Why are my test results like this:  
None
"when I do python train.py, error occur:  Traceback (most recent call last):    File ""train.py"", line 7, in        import neuralgym as ng    File ""/home/zhangyongle/.conda/envs/Inpainting/lib/python3.6/site-packages/neuralgym/__init__.py"", line 75, in        from .utils.config import Config    File ""/home/zhangyongle/.conda/envs/Inpainting/lib/python3.6/site-packages/neuralgym/utils/config.py"", line 4, in        import yaml  ModuleNotFoundError: No module named 'yaml'  WHY?"
"Hi there, I've been following your work for a while and I ran into a new problem now, your help would be sincerely thanked.     Here's what I want to do and what my problems are:  I'm trying to save a frozen pb file so that I can use it without rebuilding graph every time I use it. That one I achieved and it went well. But I realized that while I freeze graph, the input of the whole network is settled. I have to resize images to 256x256 so that I can use them as the network's input. That's not what I'm hoping for because my images' sizes are random. You mentioned in #8 that placeholder can be used, I tried that and stuck there.  Training process didn't report any errors, per se, but it stuck here and didn't go on:    > 2019-02-21 15:12:45.789528: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA  > 2019-02-21 15:12:48.551802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:   > name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582  > pciBusID: 0000:02:00.0  > totalMemory: 10.91GiB freeMemory: 10.75GiB  > 2019-02-21 15:12:48.551832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0  > 2019-02-21 15:12:48.747683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:  > 2019-02-21 15:12:48.747743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0   > 2019-02-21 15:12:48.747751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N   > 2019-02-21 15:12:48.747988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10409 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)  >     that's where the _feed_dict_ is:     I also changed here:     My guess is that I didn't get how dataflow works here, you said that you   is based on a static graph, could you give me a hint where I did wrong and how should I change it? Thanks in advance."
None
"Hi  Your work is great, and thanks for sharing the code with us.    I would like to modify the test.py code to take multiple images as input and save the outputs in a folder.  The code in this repo takes one image at a time.    I made the following changes to the code :    if __name__ == ""__main__"":      ng.set_gpus(1)      args = parser.parse_args()      model = InpaintCAModel()        image_listing = os.listdir(args.image)      mask_listing = os.listdir(args.mask)        count = 0        sess_config = tf.ConfigProto()      sess_config.gpu_options.allow_growth = True      sess = tf.Session(config=sess_config)        for input_img , mask_img in zip(image_listing, mask_listing):          count +=1          image = cv2.imread(os.path.join(args.image ,input_img))          mask = cv2.imread(os.path.join(args.mask ,mask_img))            assert image.shape == mask.shape            h, w, _ = image.shape          grid = 8          image = image[:h//grid*grid, :w//grid*grid, :]          mask = mask[:h//grid*grid, :w//grid*grid, :]          print('Shape of image: {}'.format(image.shape))            image = np.expand_dims(image, 0)          mask = np.expand_dims(mask, 0)          input_image = np.concatenate([image, mask], axis=2)              input_image = tf.constant(input_image, dtype=tf.float32)          output = model.build_server_graph(input_image)          output = (output + 1.) * 127.5          output = tf.reverse(output, [-1])          output = tf.saturate_cast(output, tf.uint8)          # load pretrained model          vars_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)          assign_ops = []          for var in vars_list:              vname = var.name              from_name = vname              var_value = tf.contrib.framework.load_variable(args.checkpoint_dir, from_name)              assign_ops.append(tf.assign(var, var_value))          sess.run(assign_ops)          #print('Model loaded.')          result = sess.run(output)          cv2.imwrite(args.output+'output_'+str(count)+'.jpg', result[0][:, :, ::-1])    But I get the following error since I reuse the variable.    ValueError: Variable inpaint_net/conv1/kernel already exists, disallowed.     I'm unsure how to go forward. Could you please help ?  Thanks"
"Hi JiahuiYu,     Thanks for your wonderful work! I want to apply this model to grayscale images. Can you give me some pointers what do I need to modify on the code? I already changed IMG_SHAPES in .yml file to [256, 256, 1] and the generator model so the output images are single channel, but I am still facing some errors.     Thanks for your help again! "
"Hi, I've read in another thread that you have implemented the DeepFillv2. Would you please be so nice and share it?"
"Hello,@JiahuiYu .  Q1:What's the number of images in validation set? 2K?  Q2:When you compute evaluation metrics on places2, were the images size 512×680 and with a 128×128 hole in the center?   Thanks."
"@JiahuiYu How do you generate incoplete image,mine like below:  !   Your is fine.  !   "
thanks for sharing your work. i want to train using my own mask instead of creating the mask during training. what to change in order to be take the mask as mask.png ? i attached the mask i want to train with.  your help is really appreciated   !   
 pip install  neuralgym  Looking in indexes:    Collecting neuralgym    Could not find a version that satisfies the requirement neuralgym (from versions: )  No matching distribution found for neuralgym
"i train this model with the place2 (data_256),  but i find the result of this model is not realize the effect of the 'release_places2_256' model  so i want to know how to train this?"
"`Traceback (most recent call last):    File ""train.py"", line 47, in        images, config=config)    File ""/Users/utkarsh_1/Documents/Machine_Learning/AI_Tech/channel_reconstruction/scripts/generative_inpainting/inpaint_model.py"", line 153, in build_graph_with_losses      padding=config.PADDING)    File ""/Users/utkarsh_1/Documents/Machine_Learning/AI_Tech/channel_reconstruction/scripts/generative_inpainting/inpaint_model.py"", line 97, in build_inpaint_net      x, offset_flow = contextual_attention(x, x, mask_s, 3, 1, rate=2)    File ""/Users/utkarsh_1/Documents/Machine_Learning/AI_Tech/channel_reconstruction/scripts/generative_inpainting/inpaint_ops.py"", line 307, in contextual_attention      yi = tf.nn.conv2d_transpose(yi, wi_center, tf.concat([[1], raw_fs[1:]], axis=0), strides=[1,rate,rate,1]) / 4.    File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1228, in conv2d_transpose      filter.get_shape()[3]))  ValueError: input channels does not match filter's input channels, 1369 != 1444`"
"I checked out your demo and it works nicely for small distortions. Did you try use this model for ""outpainting""? I mean extending the field of view by providing photo with frame and mask the frame. Something like ""extend"" in snapseed. I tested it on your demo but the results were not so good. Do you think finetuning the model is enough or does the model architecture need to be changed?"
I would love to have docker image for this. It would help test the code smoothly without fighting errors.    Nice work tho :)
"hi,i want to use test.py to iteration my own homework a lot of times ,so i add a 'for method',but it olny make a error when it begin the second section.the error seem to be a init of neuralgym,the parameter has been load.    ValueError: Variable inpaint_net/conv1/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:      File ""/home/dingyuyang/generative_inpainting-master/inpaint_ops.py"", line 45, in gen_conv      activation=activation, padding=padding, name=name)    File ""/home/dingyuyang/.conda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args      return func(*args, **current_args)    File ""/home/dingyuyang/generative_inpainting-master/inpaint_model.py"", line 50, in build_inpaint_net      x = gen_conv(x, cnum, 5, 1, name='conv1')  is there any method to solve it? thanks a lot ."
"Hi, @JiahuiYu!    I am interested in your awesome work and trying to reimplement it with PyTorch framework. However, I got some worse results when inpainting 256x256 image patches from ImageNet validation dataset with  . When testing the models, only 128x128 center hole mask has been used.    Here are some results for comparison. From the left to the right, they are **original image patches**, **pre-trained model results** and **reproduce results** respectively.     !   !   !   !   !     Like the last two images show, many inpainted results of my reproduce contain strange ""color blocks"", which make the results worse. Of course, there may be a lot of bugs in my codes and I am reviewing them.    I wonder if you have also met the same problem during your work. It would be helpful if you could give me some possible guesses or explanation for it. Thank you!  "
"@JiahuiYu   I want to save random generated masks,and the code like below(write in build_graph_with_losses)  But it just saved 8 pictures（now iter:6000）    Could you give me some suggestion?Looking forward your reply.  "
@JiahuiYu Do you use gradient penalty in deepfillv2?
"Hi there, I was trying to transfer the author's pre-trained checkpoint to pb file and ran into weird error. I used the following code to do it, yet I failed to load the transferred pb file:    `constat_graph = graph_util.convert_variables_to_constants(sess, sess.graph_def, ['save/restore_all'])  with tf.gfile.FastGFile(self._dump_prefix, mode='wb'):      f.write(constant_graph.SerializeToString())`    I'm a little confused by the neuralgym and wondering if you could tell me how to transfer checkpoint to pb file, or how to save pb file during training process(tried, failed to load it). Thanks very much."
"Hi there, I'd like to know if I'll achieve good performance using irregular masks during training process? I've tested the model you provided and its performance on irregular masks is a little worse than on regular masks. So I want to retrain a model with irregular masks. The fact is that I'm not sure if its ok to use irregular ones as input, can you answer this question?    FYI, looking forward to the release of Deep Fill V2."
Thank you for sharing your great achievements. When i tried to make some improvements based on your code，I have encountered some problems.  !   !   Why are there obvious boxes on my test results?Have you ever encountered this situation before?I would be very grateful if you can answer my doubts.    
"Hi,@JiahuiYu .I have some questions about training from scratch on places2.  Q1:i set parameters as follows.Is it ok?FYI:i use high-resolution pictures ,and i have one available gpu.   Q2:What's the meanings of GAN_WITH_MASK and DISCOUNTED_MASK?When should i use them?  DATASET: 'places2'  # 'tmnist', 'dtd', 'places2', 'celeba', 'imagenet', 'cityscapes'  RANDOM_CROP: True  VAL: False  LOG_DIR: full_model_places2_512  MODEL_RESTORE: ''  # '20180115220926508503_jyugpu0_places2_NORMAL_wgan_gp_full_model'    GAN: 'wgan_gp'  # 'dcgan', 'lsgan', 'wgan_gp', 'one_wgan_gp'  PRETRAIN_COARSE_NETWORK: False  GAN_LOSS_ALPHA: 0.001  # dcgan: 0.0008, wgan: 0.0005, onegan: 0.001  WGAN_GP_LAMBDA: 10  COARSE_L1_ALPHA: 1.2  L1_LOSS_ALPHA: 1.2  AE_LOSS_ALPHA: 1.2  GAN_WITH_MASK: False  DISCOUNTED_MASK: True  RANDOM_SEED: False  PADDING: 'SAME'      NUM_GPUS: 1  GPU_ID: -1  # -1 indicate select any available one, otherwise select gpu ID, e.g. [0,1,3]  TRAIN_SPE: 10000  MAX_ITERS: 1000000  VIZ_MAX_OUT: 10  GRADS_SUMMARY: False  GRADIENT_CLIP: False  GRADIENT_CLIP_VALUE: 0.1  VAL_PSTEPS: 1000    "
Really great works your deepfill!  Is it possible to build graph once then feed it with multi images/masks with different resolutions?    Similar to    a loop was created to process multi images but he built graph for every input image.    In your code   graph only has to be built once but all image/mask should be resized to same size then fed to the graph.    Can we build graph once and feed images/masks with different resolutions?    Thanks.
"Hi @JiahuiYu I'm getting the following error when trying to run the Train.py code. I have previously installe d the **neuralgym** pip module via your git repo.    !     Any ideas what's going on here?    Also, when can we get access to V2? I'd prefer to use polygon based inpainting instead o just rectangles. Thanks!"
@JiahuiYu Why use batch_complete(left) as the final result， instead of batch_predict(right)？I think the picture on the right is better.  !   !   !     
"When I used deepfillv2 code to fix irregular occlusion, the results are  as follows. What do you think caused this phenomenon?Looking forwarding your replay!  !   !   "
"If it is okay, please leave your e-mail and I send you some examples.  Thanks!"
"I generated a mask database and use CelebA-HQ to test some indexes.  But the result is not good.I dont know why.  I found you generated one mask for every image of the same batch, did it matter？  !   !         "
"When I run train.py with my own dataset, it‘s warning: unknown JFIF revision number 0.02  what's the problem?    !   "
@JiahuiYu  I have a question to consult you.My training set has 5000 pictures. It performs very well in training set and very poorly in test set. How to deal with this situation?
"Hi, @JiahuiYu            I wonder what data augmentation did you use in this project? I could not find this part of code.           Thanks in advance."
"Hi Jiahui,     Was looking at the implementation of the contextual attention layer, and I think I spotted a bug.    Stride is used here:   `w = tf.extract_image_patches(b, [1,ksize,ksize,1], [1,stride,stride,1], [1,1,1,1], padding='SAME')`  and here:  `m = tf.extract_image_patches(mask, [1,ksize,ksize,1], [1,stride,stride,1], [1,1,1,1], padding='SAME')`  These lines extract a certain number of channels that is inversely proportional to the stride.     Attention is extracted here:   `yi = tf.nn.conv2d(xi, wi_normed, strides=[1,1,1,1], padding=""SAME"")`  and reshaped here:  `yi = tf.reshape(yi, [1, fs[1]*fs[2], bs[1]*bs[2], 1])`    The shapes fs and bs are not changed by stride at all. However, the number of channels in w and m, and therefore in yi, decreases inversely to the stride. As a result, if the stride is anything other than 1, the reshape in the fuse section will fail because there are not enough elements in the tensor.    A quick test on my end with a stride set to 3 failed. Let me know what the correct solution is for this!"
"Hi, I trained the deepfill v2 based on your open source. As i trained many iterations, I have found the output of stage1 and stage2 is similar. Does anyone has the same phenomenon？And How to solve it? I  reimplemented with pytorch and I have got the same results by testing the CA model. "
"@JiahuiYu When I run test.py, I made the following error. What do you think is the reason?  !   "
"Q1: According to the paper discriminator's input is image, mask and optional user input. Just like for generator net.  And according to this comment   you do NOT use ones in discriminator's input. So it's looks like that?       Q2: Do you use gated convolution for ALL layers? Even for last one of coarse and refine nets?"
"I try to use GPU for training but it don't work when using default setting：  `NUM_GPUS: 1  GPU_ID: -1`  The training process  only used the CPU.    Then I follow #68 #63 #14 to change ng.train.Trainer to  MultiGPUTrainer, add `num_gpus=config.NUM_GPUS` and set  `NUM_GPUS: 2  GPU_ID: [6,7]`  But the training process  still only used the CPU.    Did I ignore something?  Actually, my objective is to check the value of GAN_loss when training, cause I am trying to produce a pytorch version but my loss of discriminator and generator is very small(e-03) from beginning to end and the result is very blurry"
"The change of loss.  the loss of D_net is:  g_loss, d_loss = self.gan_hinge_loss(pos_global, neg_global)  the complete loss of G loss is :   losses   !   !     _Originally posted by @huangqianfirst in    "
"@JiahuiYu I want to use the smaller mask,and how to modify the code？I look forward to your reply."
Appreciate your contribution！    Q1：When you train the coarse net separately，you use l1 reconstruction loss only？    Q2：When you train the refinement net，the parameters of coarse net are fixed or updated together？
"image = image / 127.5 -1  x = image *  (1. - mask)  ones_x = tf.ones_like(x)[:, :, :, 0:1]   input = tf.concat([x, ones_x, ones_x*mask], axis=3)    1. Why we need to concat a map whose values are only ones?    2. After processing, the hole's value of masked_image is 0, but the hole's value of mask is 1,   does it matters?    3.Why we need to normalize to [-1,1]  but not [0,1]?    "
"While running the following:  python inpaint_ops.py --imageA examples/style_transfer/bnw_butterfly.png  --imageB examples/style_transfer/bike.jpg --imageOut examples/style_transfer/bike_style_out.png    I get the following error:  Traceback (most recent call last):    File ""/opt/virtual_tensorflow/g_tf_neuralgym/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 558, in set_shape      unknown_shape)  tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimension 1 in both shapes must be equal, but are 166 and 64. Shapes are [1,166,250,2] and [1,64,64,2].    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""inpaint_ops.py"", line 513, in        test_contextual_attention(args)    File ""inpaint_ops.py"", line 360, in test_contextual_attention      training=False, fuse=False)    File ""inpaint_ops.py"", line 313, in contextual_attention      offsets.set_shape(int_bs[:3] + [2])    File ""/opt/virtual_tensorflow/g_tf_neuralgym/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 561, in set_shape      raise ValueError(str(e))  ValueError: Dimension 1 in both shapes must be equal, but are 166 and 64. Shapes are [1,166,250,2] and [1,64,64,2].    I can see this has been logged before here:    But cant find any fix. Can you please let me know how I can fix this?"
"I am a fisherman of inpainting.This article can repair lage missing region of image.And I am intersted in this I want to know what aspect I can major in in inpainting.That means what aspect need to improvie,because I think this passage is perfect and there is no space to rearch."
"Hello,    You did a really interesting work. I have a question that how can I resume training based on the pre-trained ImageNet model. I mean where can I find the restored model and what should I modify of MODEL_RESTORE flag in inpaint.yml.    Thanks for your time."
I've run one epoch on places2 dataset. The loss and result of  repair are as follows. Do you think I should run more epoches?  !   !     
_Originally posted by @JiahuiYu in  
"Dear author, thank you for your excellent work.  I trained the model with nearly 30,000 pictures. A week later, when I tested it, I found my test output very strange. I used DeepFillv2 to train the model.  I hope you can help me out, thank you very much.  !   "
@JiahuiYu There are more iterations. Why does it look worse?（places2 dataset，still not one epoch）  Separate iteration278000 and 483000  !   !   
MY process is killed automatically when i run train.py. I have created .flist file from the code which you wrote. please help me  !   
"Hello Jiahui,    Thanks for very well written code and great description!   I have a dataset with around 60000 images and I would like to use genarative_inpainting as a feature extractor. I am planning to use the output of encoder for the task of image classification and I would be grateful if you could tell me which layer should I use?   "
Hi~  My questions comes from here.   Q1:What's the definition of score fusion?  Q2:How can it encourage larger patches?  Thx!
"Do they have to be the same size?  Do I have to do some initialization before training?  if I want to inpaint landscape painting， Is transfer learning based on your pre-trained model of places2 possible effect?    And sorry for disturbing you with these  simple question, I'm really new to DL especially GANs."
"def dis_conv(x, cnum, ksize=5, stride=2, name='conv', training=True):      x_shape=x.get_shape().as_list()      print(""x_shape................................"",x_shape)      #print(""The shape for x_shape "",x_shape)      w=tf.get_variable(name=name+'_w',shape=[ksize,ksize,x_shape[-1]]+[cnum])      w=conv2d_spectral_norm(w,cnum,ksize,name=name)         x = tf.layers.conv2d(x,w,strides=[1,stride,stride,1],padding='SAME',data_format='channels_last')      bias=tf.get_variable(name=name+'_bias',shape=[cnum])      return  tf.nn.leaky_relu(x+bias)      This is my code for dis_conv. I have used spectral norm from neuralgyms dev branch. The w tensor returned has different shape than expected. "
"Hi I have opened a new issue for you since the question is not relevant to the issue of deep fill v2.  @khemrajrathore    hello! On running the following line of code,  x = x*mask + xin*(1.-mask)  I am getting an error:  Dimensions must be equal, but are 3 and 256 for 'inpaint_net/mul_15' (op: 'Mul') with input shapes: [16,3,256,2], [1,256,256,1].  however, my generative layers are the same as yours.  I didn't understand why  x = gen_conv(x, 3, 3,1, activation=None, name='conv17') works for 3 cnum value too."
"my test code:  `python3 test.py --image examples/celeba/celebahr_patches_163050_input.png --mask examples/center_mask_256.png --output out3.png --checkpoint_dir model_logs/release_celeba_hq_256/`  and  `python3 test.py --image examples/celeba/celebahr_patches_165230_input.png --mask examples/center_mask_256.png --output out1.png --checkpoint_dir model_logs/release_celeba_hq_256/`    I only changed line23 in test.py from ""ng.get_gpus(1)"" to ""ng.get_gpus(0)""    those are input, mask, output:  !     !     !     !   !     It's really similar to #111 , so I changed tf1.10 to 1.7, but nothing happened.    ps: the performance of places2 seems better:  !   !     Do yo know why this happened, please!"
Does the random mask created during the training process change from per 1000 step?
"imagenet: [  your_file.flist,  ]    What means ""your_file.flist""? How can I create a file.flist? How can I add 'data\imagenet\buffer010.png' to a file? Can u show me? Thx a lot.  "
"Hi Jiahui,    experiment on DTD was mentioned in your paper (CVPR2018)  But I didn' find the release model for DTD?    Will you release the model of DTD as well as flist file?"
"  # training data      #""""""      #with open(config.DATA_FLIST[config.DATASET][0]) as f:          #fnames = f.read().splitlines()      fnames=['img010.png','img000.png']    I  change the code in train.py.  Then it can run now.  In the end it will give me information like this:  2018-10-15 15:52:31 @weights_viewer.py:60] Total size of trainable weights: 0G 10M 108K 392B (Assuming32-bit data type.)  But it also produced the model.  Then I run the test.    tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file model_logs\20181015153040966844_imagenet_NORMAL_wgan_gp_LOG_DIR\events.out.tfevents.1539588647.NBA1903: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?    Is this means my training fails?  Should I increase my training dataset or doing something else to solve the problem?  I am still trying it on Win10."
"@aiueogawa Hi, I have opened a specific issue for you. You have asked five questions and I have answered all your questions. If you do not understand each one, please ask here and we can communicate here. Thanks.    "
I run it on the window
"Can u tell me how to install pip install git+  I try to install it on Anaconda prompt. But it will send me the msg ""cannot find command 'git'"". I try to use conda install git+  It will send me the msg PackagesNotFoundError."
The program has not run for 4 hours.What do you think is wrong?  !   
"I am wondering why the network accepts different input sizes. Within my knowledge, test input sizes must equal to train input size."
"In the 'bbox2mask' function to create a random mask, I modified the code part corresponding to the random mask in order to add the desired our mask value to the mask value by like below.    We added our_mask in 'npmask' function to set the white area of ​​our_mask to '1.', the black area to '0', and the initial value of mask to our_mask.    >>code  def bbox2mask(bbox, config, name='mask'):      """"""Generate mask tensor from bbox.        Args:          bbox: configuration tuple, (top, left, height, width)          config: Config should have configuration including IMG_SHAPES,              MAX_DELTA_HEIGHT, MAX_DELTA_WIDTH.        Returns:          tf.Tensor: output with shape [1, H, W, 1]        """"""      def npmask(bbox, height, width, delta_h, delta_w):          our_mask = cv2.imread('addmask.png', cv2.IMREAD_GRAYSCALE)          our_mask = np.place(our_mask, our_mask == 254, 1.)            mask = np.zeros((1, height, width, 1), np.float32)          mask[0,:, :, 0] = our_mask          h = np.random.randint(delta_h//2+1)          w = np.random.randint(delta_w//2+1)          mask[:, bbox[0]+h:bbox[0]+bbox[2]-h,               bbox[1]+w:bbox[1]+bbox[3]-w, :] = 1.          return mask      with tf.variable_scope(name), tf.device('/cpu:0'):          img_shape = config.IMG_SHAPES          height = img_shape[0]          width = img_shape[1]          mask = tf.py_func(              npmask,              [bbox, height, width,               config.MAX_DELTA_HEIGHT, config.MAX_DELTA_WIDTH],              tf.float32, stateful=False)          mask.set_shape([1] + [height, width] + [1])      return mask      However, the following error code has occurred.  >>error        Traceback (most recent call last):    File ""train.py"", line 112, in        trainer.train()    File ""/home/user/Documents/SSS/generate3/lib/python3.5/site-packages/neuralgym/train/trainer.py"", line 133, in train      cb.run(sess, step)    File ""/home/user/Documents/SSS/generate3/lib/python3.5/site-packages/neuralgym/callbacks/secondary_trainer.py"", line 26, in run      self.train()    File ""/home/user/Documents/SSS/generate3/lib/python3.5/site-packages/neuralgym/train/trainer.py"", line 143, in train      assert not np.isnan(loss_value)  AssertionError    Can i ask your advice about the cause of the error and the problem of the code modification?  "
"@JiahuiYu   Interrupted during training, continue training on the same flist, and it will start on the first line of the flist or the line that is interrupted？  Looking forward to your reply."
"Hi Jiahui,    You mentioned context-aware-fill method in your paper for comparison.   To my best known is that context-aware-fill is a tool in Photoshop.  I wonder how did you get/use the model of context-aware-fill ?  (Because I didn't find out the source code for context-aware-fill)    Thank you for your kindly help!"
"I encountered this error when doing python train.py  I followed the instruction to setup a proper flist, but the error stills  File ""/home/wxh/anaconda3/envs/tf/lib/python3.6/site-packages/neuralgym/ops/image_ops.py"", line 23, in np_scale_to_shape  imgh, imgw = image.shape[0:2]  AttributeError: 'NoneType' object has no attribute 'shape'"
"Understand that this uses your own implementation to train; my questions are -  1. Can Imagenet weight (training result) be directly used (replaced with one of the Imagenet weights, such as VGG16)?  2. If yes, how?  Thanks in advance.  Jay"
"hi JiahuiYu，when I train your model followiing your default setting ，the large mask does not work well as your result。I notice that the parameter HEIGHT：128 and WIDTH：128，means the maximum size of the mask is 128*128,but in your examples in places2 the hole of grass_mask.png is larger than 128*128. do you make larger  HEIGHT and WIDTH setting to train this model on places2?  !     !   !   !   !   !   "
"Hi, I encounter a problem about gpu. There is already a GTX 960 in my computer and I can use gpu to run other code. But I can not run this code. Looking forward to your reply and thanks.    File ""/home/jason/anaconda3/lib/python3.6/site-packages/neuralgym/utils/gpus.py"", line 70, in get_gpus      ' [(gpu id: num of processes)]: {}'.format(sorted_gpus))  SystemError: No enough gpus for dedicated usage. [(gpu id: num of processes)]: [(0, 3)]  "
"Hi @JiahuiYu     I saw the numeric results report in your paper includes mean L1 loss, mean L2 loss and mean TV loss.  I wonder why they all have percentage ?  for example, I used to know L1_loss=abs(real-fake),  does the percentage comes from dividing 255 like L1_loss=abs(real-fake)/255?"
"  @JiahuiYu The picture on the top is the result of my training（ iteration = 25000，other parameters remain unchanged）.The picture on the bottom is the result of your model.       And I have been training for three days(0.11batches/sec),do you think continuing training will make the result better?Look forward to your! #75   !   !     "
"I have tried to inpaint images of resolution 1920x1080 using CPU only. But the inpainting consumes so much memory, nearly 24GB. May I ask why is it? Is it possible to come up with some fixations? It is a problem when trying to inpaint a single image consumes 24 GB memory.    FYI, attached are my mask of resolution 1920x1080.  !     Also, when I try to inpaint image of resolution 2560x1600, lots of swap memory has been consumed, nearly 60GB. And eventually the process got killed, so inpainting didn't make it.  <img width=""853"" alt=""screenshot 2018-09-27 at 4 25 06 pm"" src=""   <img width=""226"" alt=""screenshot 2018-09-27 at 4 24 58 pm"" src=""     Below is my mask of size 2560x1600:  !   "
"I set NUM_GPUS=2, while the all data is set to GPU0, GPU1 is almost empty, so can this code supports multi-gpus?"
"I have a gpu (GK104 [GeForce GTX 760]) and I would like to test your code. I used pretrained data...but i get the following error.  Any help?   Thanks in advance.    python test.py --image examples/input.png --mask examples/center_mask  _256.png --output examples/output.png --checkpoint model_logs/release_celeba_hq_256/  [2018-09-21 15:45:25 @__init__.py:79] Set root logger. Unset logger with neuralgym.unset_logger().  [2018-09-21 15:45:25 @__init__.py:80] Saving logging to file: neuralgym_logs/20180921154525035034.  Not supported on the device(s)  Failed to process command line       Traceback (most recent call last):    File ""test.py"", line 23, in        ng.get_gpus(1)    File ""/home/u64353/venvpytorch/lib/python3.6/site-packages/neuralgym/utils/gpus.py"", line 54, in get_gpus      gpu_id = int(s[0])  IndexError: list index out of range"
I want to use an image with transparent values as the training data.    How do I read the image values for transparent images?
None
"Hi, Jiahui! Great results and thank you for sharing.    It seems that your implementation is for images of shape (?, ?, 3), is this true? If so, I wanted to ask if you have tried adapting this for grayscale images, i.e. of shape (?, ?, 1), before I start making my own adaptation."
"Hi @JiahuiYu ,           After finetuning,  I found the model saved (snap-10000.data-00000-of-00001) was 123M，but the imagenet pretrained model is 14M. I think extra params (like discriminator ) are including in it. How could I slimming the saved model? "
"While doing training, five snap-***.data-00000-of-00001 are saved by default. How can I save 10 or 20 snap-***.data-00000-of-00001? Thank you."
"I want to add specific image values to the existing mask.  The code for the existing mask part has been modified as shown below.    >> code      def npmask(bbox, height, width, delta_h, delta_w):          mask = np.zeros((1, height, width, 1), np.float32)          h = np.random.randint(delta_h//2+1)          w = np.random.randint(delta_w//2+1)          mask ]]        ]]  During handling of the above exception, another exception occurred:  Traceback (most recent call last):    File ""train.py"", line 112, in        trainer.train()    File ""/home/user/Documents/SSS/generate3/lib/python3.5/site-packages/neuralgym/train/trainer.py"", line 133, in train      cb.run(sess, step)    File ""/home/user/Documents/SSS/generate3/lib/python3.5/site-packages/neuralgym/callbacks/secondary_trainer.py"", line 26, in run      self.train()    File ""/home/user/Documents/SSS/generate3/lib/python3.5/site-packages/neuralgym/train/trainer.py"", line 141, in train      feed_dict=self.context ]]        ]]  Caused by op 'mask_c_1/DecodePng', defined at:    File ""train.py"", line 87, in        'model': model, 'data': data, 'config': config, 'loss_type': 'd'},    File ""/home/user/Documents/SSS/generate3/lib/python3.5/site-packages/neuralgym/callbacks/secondary_trainer.py"", line 22, in __init__      Trainer.__init__(self, primary=False, **context)    File ""/home/user/Documents/SSS/generate3/lib/python3.5/site-packages/neuralgym/train/trainer.py"", line 41, in __init__      self._train_op, self._loss = self.train_ops_and_losses()    File ""/home/user/Documents/SSS/generate3/lib/python3.5/site-packages/neuralgym/train/trainer.py"", line 250, in train_ops_and_losses      loss = self.context     File ""train.py"", line 23, in multigpu_graph_def      images, config, reuse=True)    File ""/home/user/Documents/SSS/generative_inpainting-master/inpaint_model.py"", line 149, in build_graph_with_losses      mask = bbox2mask(bbox, config, name='mask_c')    File ""/home/user/Documents/SSS/generative_inpainting-master/inpaint_ops.py"", line 150, in bbox2mask      train_mask_image = tf.image.decode_png('/home/user/Documents/SSS/generative_inpainting-master/mask_using_train_mask.png')    File ""/home/user/Documents/SSS/generate3/lib/python3.5/site-packages/tensorflow/python/ops/gen_image_ops.py"", line 1067, in decode_png      name=name)    File ""/home/user/Documents/SSS/generate3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper      op_def=op_def)    File ""/home/user/Documents/SSS/generate3/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func      return func(*args, **kwargs)    File ""/home/user/Documents/SSS/generate3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3155, in create_op      op_def=op_def)    File ""/home/user/Documents/SSS/generate3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1717, in __init__      self._traceback = tf_stack.extract_stack()  InvalidArgumentError (see above for traceback): Expected image (JPEG, PNG, or GIF), got unknown format starting with '/home/user/Docum'        ]]        ]]      Q. How do you modify the code to add the desired image values to the random mask?    I always appreciate your interesting research."
"We did different levels of training, compared with testings on those trained models, and found that overfitting is quite easy to happen.  Since we did not find much overfitting discussion in either original paper or github repository, so we ask here:    - where can we find the overfitting discussion - conditions to happen, and the ways to avoid?  Thanks in advance! - Jay"
"I trained my 1883 images about 380 epochs with parameters like below:  #parameters  DATASET: 'celebahq'  RANDOM_CROP: False  VAL: False  LOG_DIR: 'logs'  MODEL_RESTORE: 'a'  #    GAN: 'wgan_gp'  # 'dcgan', 'lsgan', 'wgan_gp', 'one_wgan_gp'  PRETRAIN_COARSE_NETWORK: False # unused  GAN_LOSS_ALPHA: 0.001  # dcgan: 0.0008, wgan: 0.0005, onegan: 0.001  WGAN_GP_LAMBDA: 10  COARSE_L1_ALPHA: 0.5  L1_LOSS_ALPHA: 0.5  AE_LOSS_ALPHA: 0.5  GAN_WITH_MASK: False  DISCOUNTED_MASK: True  RANDOM_SEED: False  PADDING: 'SAME'    #training  NUM_GPUS: 1  GPU_ID: 3  # -1 indicate select any available one, otherwise select gpu ID, e.g.    And imges with bigger masks were inpainted unsatisfing.  !   !   Could you please give me some help?"
Thank you again for your excellent work！  In GatedConvolution:Free-Form inpainting   Can you tell me in inpaint.yml the irrmask_flist.txt looks like？How should I generate it？
"Hi, I have 2346 masks the same size as train images like below and I want apply them to my train images instead of randomly producing masks.  !   So I changed codes in inpaint_ops.py:  def random_bbox(config):         img_shape = config.IMG_SHAPES         img_height = img_shape   My inpait.yml is:  ##parameters  DATASET: 'celebahq'  RANDOM_CROP: False  VAL: False  LOG_DIR: 'logs'  MODEL_RESTORE: ''  #    GAN: 'wgan_gp'  # 'dcgan', 'lsgan', 'wgan_gp', 'one_wgan_gp'  PRETRAIN_COARSE_NETWORK: False  GAN_LOSS_ALPHA: 0.001  # dcgan: 0.0008, wgan: 0.0005, onegan: 0.001  WGAN_GP_LAMBDA: 10  COARSE_L1_ALPHA: 2  L1_LOSS_ALPHA: 2  AE_LOSS_ALPHA: 2  GAN_WITH_MASK: False  DISCOUNTED_MASK: True  RANDOM_SEED: False  PADDING: 'SAME'    ##training  NUM_GPUS: 1  GPU_ID: -1  # -1 indicate select any available one, otherwise select gpu ID, e.g. [0,1,3]  TRAIN_SPE: 750  MAX_ITERS: 100000  VIZ_MAX_OUT: 10  GRADS_SUMMARY: False  GRADIENT_CLIP: False  GRADIENT_CLIP_VALUE: 0.1  VAL_PSTEPS: 50    ##data  DATA_FLIST:    celebahq: [      'training_data/train_shuffled.flist',      'training_data/validation_static_view.flist'    ]    STATIC_VIEW_SIZE: 30  IMG_SHAPES: [224, 224, 3]  HEIGHT: 224       WIDTH: 224     MAX_DELTA_HEIGHT: 0  MAX_DELTA_WIDTH: 0  BATCH_SIZE: 2  VERTICAL_MARGIN: 0  HORIZONTAL_MARGIN: 0    ##loss  AE_LOSS: True  L1_LOSS: True  GLOBAL_DCGAN_LOSS_ALPHA: 1.  GLOBAL_WGAN_LOSS_ALPHA: 1.    ##loss legacy  LOAD_VGG_MODEL: False  VGG_MODEL_FILE: data/model_zoo/vgg16.npz  FEATURE_LOSS: False  GRAMS_LOSS: False  TV_LOSS: False  TV_LOSS_ALPHA: 0.  FEATURE_LOSS_ALPHA: 0.01  GRAMS_LOSS_ALPHA: 50  SPATIAL_DISCOUNTING_GAMMA: 0.9  "
"hi，dear  Jiahui! I notice that the name of your test image is *.png ,but the imagenet data is named *.jpeg. do you only change image's name from the jpeg to png or do some process on .jpeg to get the .png"
"Hi JiaHui,  1. When we run the train.py of generative Inpainting, should we keep the VAL setting in inpaint.YML as False? (and why?)  2. Does this code contain the changes foe the ""version 2"" implementation?  2.1 if yes, how should we handle the masks?  2.2 if not, where can we find the ""version 2"" implementation?  Thanks,  Jay"
"hi, thanks for your work.  I note there are several different mask types used. I wonder that if the model is trained one time or trained each time for each mask type?  so can the trained model using random mask inpaint arbitrary broken image or only subject to the given mask type images?  "
"trying to accumulate training results progressively:   - did set inpaint.yml: MODEL_RESTORE accordingly   - but everytimes when re-run ""python train.py"", it did not take the pre-trained model pointed by inpaint.yml: MODEL_RESTORE; instead, it produces a new log in the ""model_logs""   ? what's the condition that inpaint.yml: MODEL_RESTORE is not taking effects?   ? was ""progressively training"" (on different sets of inputs) possible from current design?  Thanks,  Jay"
"Thank you for your excellent work! I have some trouble when I ran the Free-Form codes.  And I use python3.5+tensorflow 1.4.0, I also resized celeba picture to 256*256*3.  Here are the parameters in the inpaint.yml:    STATIC_VIEW_SIZE: 30  IMG_SHAPES: [256, 256, 3]  HEIGHT: 128  WIDTH: 128  MAX_DELTA_HEIGHT: 32  MAX_DELTA_WIDTH: 32  BATCH_SIZE: 16  VERTICAL_MARGIN: 0  HORIZONTAL_MARGIN: 0  MAXVERTEX: 5  MAXANGLE: 4.0 #pi  MAXLENGTH: 40  MAXBRUSHWIDTH: 10      And here are the errors :+1:     [2018-09-07 20:41:41 @__init__.py:79] Set root logger. Unset logger with neuralgym.unset_logger().  [2018-09-07 20:41:41 @__init__.py:80] Saving logging to file: neuralgym_logs/20180907204141238258.  [2018-09-07 20:41:42 @config.py:92] ---------------------------------- APP CONFIG ----------------------------------  [2018-09-07 20:41:42 @config.py:119] GRAMS_LOSS: False  [2018-09-07 20:41:42 @config.py:119] GAN_LOSS_ALPHA: 0.001  [2018-09-07 20:41:42 @config.py:111] DATA_FLIST:   [2018-09-07 20:41:42 @config.py:119]   celeba: ['/home/kjm/Desktop/inpainting/GatedConvolution/celeba_data/data_flist/train_shuffled.flist', '/home/kjm/Desktop/inpainting/GatedConvolution/celeba_data/data_flist/validation_shuffled.flist']  [2018-09-07 20:41:42 @config.py:119] COARSE_L1_ALPHA: 1.2  [2018-09-07 20:41:42 @config.py:119] GPU_ID: [0]  [2018-09-07 20:41:42 @config.py:119] RANDOM_SEED: False  [2018-09-07 20:41:42 @config.py:119] MAXBRUSHWIDTH: 10  [2018-09-07 20:41:42 @config.py:119] VIZ_MAX_OUT: 10  [2018-09-07 20:41:42 @config.py:119] WGAN_GP_LAMBDA: 10  [2018-09-07 20:41:42 @config.py:119] MAX_DELTA_HEIGHT: 32  [2018-09-07 20:41:42 @config.py:119] TV_LOSS: False  [2018-09-07 20:41:42 @config.py:119] MAX_DELTA_WIDTH: 32  [2018-09-07 20:41:42 @config.py:119] PADDING: SAME  [2018-09-07 20:41:42 @config.py:119] LOAD_VGG_MODEL: False  [2018-09-07 20:41:42 @config.py:119] AE_LOSS: True  [2018-09-07 20:41:42 @config.py:119] VAL: True  [2018-09-07 20:41:42 @config.py:119] TV_LOSS_ALPHA: 0.0  [2018-09-07 20:41:42 @config.py:119] MODEL_RESTORE: 201809_celeba_model  [2018-09-07 20:41:42 @config.py:119] GAN: sn_pgan  [2018-09-07 20:41:42 @config.py:119] GRADIENT_CLIP: False  [2018-09-07 20:41:42 @config.py:119] MAXVERTEX: 5  [2018-09-07 20:41:42 @config.py:119] BATCH_SIZE: 16  [2018-09-07 20:41:42 @config.py:119] DISCOUNTED_MASK: True  [2018-09-07 20:41:42 @config.py:119] GRAMS_LOSS_ALPHA: 50  [2018-09-07 20:41:42 @config.py:119] TRAIN_SPE: 10000  [2018-09-07 20:41:42 @config.py:119] GRADS_SUMMARY: False  [2018-09-07 20:41:42 @config.py:119] GLOBAL_DCGAN_LOSS_ALPHA: 1.0  [2018-09-07 20:41:42 @config.py:119] GLOBAL_WGAN_LOSS_ALPHA: 1.0  [2018-09-07 20:41:42 @config.py:119] STATIC_VIEW_SIZE: 30  [2018-09-07 20:41:42 @config.py:119] MAX_ITERS: 1000000  [2018-09-07 20:41:42 @config.py:119] DATASET: celeba  [2018-09-07 20:41:42 @config.py:119] VAL_PSTEPS: 1000  [2018-09-07 20:41:42 @config.py:119] HEIGHT: 128  [2018-09-07 20:41:42 @config.py:119] L1_LOSS_ALPHA: 1.2  [2018-09-07 20:41:42 @config.py:119] HORIZONTAL_MARGIN: 0  [2018-09-07 20:41:42 @config.py:119] VERTICAL_MARGIN: 0  [2018-09-07 20:41:42 @config.py:119] GAN_WITH_GUIDE: False  [2018-09-07 20:41:42 @config.py:119] GAN_WITH_MASK: True  [2018-09-07 20:41:42 @config.py:119] GRADIENT_CLIP_VALUE: 0.1  [2018-09-07 20:41:42 @config.py:119] MASKDATASET: irrmask  [2018-09-07 20:41:42 @config.py:119] PRETRAIN_COARSE_NETWORK: False  [2018-09-07 20:41:42 @config.py:119] MAXANGLE: 4.0  [2018-09-07 20:41:42 @config.py:119] RANDOM_CROP: False  [2018-09-07 20:41:42 @config.py:119] WIDTH: 128  [2018-09-07 20:41:42 @config.py:119] LOG_DIR: full_model_celeba_256  [2018-09-07 20:41:42 @config.py:119] NUM_GPUS: 1  [2018-09-07 20:41:42 @config.py:119] MASKFROMFILE: False  [2018-09-07 20:41:42 @config.py:119] L1_LOSS: True  [2018-09-07 20:41:42 @config.py:119] IMG_SHAPES: [256, 256, 3]  [2018-09-07 20:41:42 @config.py:119] AE_LOSS_ALPHA: 1.2  [2018-09-07 20:41:42 @config.py:119] VGG_MODEL_FILE: data/model_zoo/vgg16.npz  [2018-09-07 20:41:42 @config.py:119] MAXLENGTH: 40  [2018-09-07 20:41:42 @config.py:119] SPATIAL_DISCOUNTING_GAMMA: 0.9  [2018-09-07 20:41:42 @config.py:119] FEATURE_LOSS_ALPHA: 0.01  [2018-09-07 20:41:42 @config.py:119] FEATURE_LOSS: False  [2018-09-07 20:41:42 @config.py:94] --------------------------------------------------------------------------------  [2018-09-07 20:41:42 @gpus.py:20] Set env: CUDA_VISIBLE_DEVICES=[0].  [2018-09-07 20:41:42 @dataset.py:26] --------------------------------- Dataset Info ---------------------------------  [2018-09-07 20:41:42 @dataset.py:36] fn_preprocess: None  [2018-09-07 20:41:42 @dataset.py:36] queue_size: 256  [2018-09-07 20:41:42 @dataset.py:36] file_length: 32766  [2018-09-07 20:41:42 @dataset.py:36] return_fnames: False  [2018-09-07 20:41:42 @dataset.py:36] shapes: [[256, 256, 3]]  [2018-09-07 20:41:42 @dataset.py:36] random: False  [2018-09-07 20:41:42 @dataset.py:36] dtypes: [tf.float32]  [2018-09-07 20:41:42 @dataset.py:36] index: 0  [2018-09-07 20:41:42 @dataset.py:36] enqueue_size: 32  [2018-09-07 20:41:42 @dataset.py:36] random_crop: False  [2018-09-07 20:41:42 @dataset.py:36] nthreads: 8  [2018-09-07 20:41:42 @dataset.py:36] batch_phs: [ ]  [2018-09-07 20:41:42 @dataset.py:36] filetype: image  [2018-09-07 20:41:42 @dataset.py:37] --------------------------------------------------------------------------------  Traceback (most recent call last):    File ""train.py"", line 77, in        images, masks, guides, config=config)    File ""/home/kjm/Desktop/inpainting/GatedConvolution/inpaint_model_gc.py"", line 161, in build_graph_with_losses      batch_incomplete, batch_mask, batch_mask, config, reuse=reuse, training=training, padding=config.PADDING)    File ""/home/kjm/Desktop/inpainting/GatedConvolution/inpaint_model_gc.py"", line 114, in build_inpaint_net      x, offset_flow = contextual_attention(x, x, mask_s, 3, 1, rate=2)    File ""/home/kjm/Desktop/inpainting/GatedConvolution/inpaint_ops.py"", line 512, in contextual_attention      offset = tf.argmax(yi, axis=3, output_type=tf.int32)  TypeError: argmax() got an unexpected keyword argument 'output_type'    This is 'validation_shuffled.flist' file:  /home/kjm/Desktop/inpainting/GatedConvolution/data/validation/validation/047710.png  /home/kjm/Desktop/inpainting/GatedConvolution/data/validation/validation/047871.png  /home/kjm/Desktop/inpainting/GatedConvolution/data/validation/validation/046974.png    Thank you for your help!"
"Thank you for your excellent work!When I ran the Free-Form code.The following error occurred.Can you help me ?Thank you very much.  [2018-09-06 19:01:29 @__init__.py:79] Set root logger. Unset logger with neuralgym.unset_logger().  [2018-09-06 19:01:29 @__init__.py:80] Saving logging to file: neuralgym_logs/20180906190129318090.  Traceback (most recent call last):    File ""train.py"", line 49, in        config = ng.Config(sys.argv[1])  IndexError: list index out of range  "
"Hi，I am confused about the result of stage 1.  In your paper, the result of stage 1 is:  !     But, in the code, it is:  `# stage2, paste result as input`   `x = x*mask + xin*(1.-mask)`  ` x.set_shape(xin.get_shape().as_list())`    In my understanding，xin in code is x in paper, and x in code is G(z, m) in paper. I am very confused about this. Hope for your reply！  "
"Exception in thread Thread-6:  Traceback (most recent call last):    File ""/usr/lib/python3.5/threading.py"", line 914, in _bootstrap_inner      self.run()    File ""/usr/lib/python3.5/threading.py"", line 862, in run      self._target(*self._args, **self._kwargs)    File ""/home/chandu/akshay/dev_py36/lib/python3.5/site-packages/neuralgym/data/feeding_queue_runner.py"", line 197, in _run      sess.run(enqueue_op, feed_dict)    File ""/home/chandu/akshay/dev_py36/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 877, in run      run_metadata_ptr)    File ""/home/chandu/akshay/dev_py36/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1076, in _run      str(subfeed_t.get_shape())))  ValueError: Cannot feed value of shape (32, 576, 768, 3) for Tensor 'Placeholder:0', which has shape '(?, 768, 576, 3)'    ### In inpaint.yml   I have given  **IMG_SHAPES: [768,576, 3]**      **It work well when I have give height and width same**    I'm new to Deep learning and tensorflow  can you please throw a light where I can make the modification in code   "
"I thought input for training data is **two**,one is image and other one was mask like input in testing(test.py)   but Its looks like only one input. I was confused     I have dataset contains   2 part 1 is mask other one is image  please help me.How to solve this problem"
"error:  During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""inpaint_ops.py"", line 513, in        test_contextual_attention(args)    File ""inpaint_ops.py"", line 360, in test_contextual_attention      training=False, fuse=False)    File ""inpaint_ops.py"", line 313, in contextual_attention      offsets.set_shape(int_bs[:3] + [2])    File ""/anaconda3/envs/dev_py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 528, in set_shape      raise ValueError(str(e))  **ValueError: Dimension 1 in both shapes must be equal, but are 166 and 64. Shapes are [1,166,250,2] and [1,64,64,2]..**    can you help me please where I was going wrong"
"Hi, thank you for your code!  I haved reimplement your network which separates core-code and neuralgym lib, the training code is based on Algorithm 1 in your excellent paper.  !   In my understanding， it means that firstly the discriminator is trained for 5 iters, then the generator is trained once, loop in this way. Is that right?     The training code is below:  `           # batch_queue is sample batch fifo              images = batch_queue.dequeue()              _ = Loss.build_graph_with_losses(images, FLAGS)              g_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'inpaint_net')              d_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'discriminator')                losses = Loss.build_graph_with_losses(images, FLAGS, reuse=True)              g_loss = losses['g_loss']              d_loss = losses['d_loss']                learning_rate = _configure_learning_rate(FLAGS.num_samples, global_step)              g_optimizer = _configure_optimizer(learning_rate)              d_optimizer = g_optimizer                g_target = g_optimizer.minimize(g_loss, global_step=global_step, var_list=g_vars)              d_target = d_optimizer.minimize(d_loss, global_step=global_step, var_list=d_vars)                g_update_op_list = tf.get_collection(tf.GraphKeys.UPDATE_OPS)              g_update_op_list.append(g_target)              if g_update_op_list:                  updates = tf.group(*g_update_op_list)                  with tf.control_dependencies([updates]):                      g_loss = tf.identity(g_loss, name='g_loss')              d_update_op_list = tf.get_collection(tf.GraphKeys.UPDATE_OPS)              d_update_op_list.append(d_target)              if d_update_op_list:                  updates = tf.group(*d_update_op_list)                  with tf.control_dependencies([updates]):                      d_loss = tf.identity(d_loss, name='d_loss')            .....                    d_iter = 0                  for batch_index in xrange(FLAGS.max_number_of_steps):                      start_time = time()                      if d_iter   step:%05d G_loss:%.5f time_cost:%.4f' % (count, generative_loss, end_time - start_time))  `  And the output is like this, do you think it's correct?  thanks!    step:00001 D_loss:18.17612 time_cost:14.1205  step:00002 D_loss:17.93037 time_cost:12.5227  step:00003 D_loss:17.72539 time_cost:12.6372  step:00004 D_loss:16.56054 time_cost:12.7271  step:00005 D_loss:15.68778 time_cost:12.7708  ==> step:00006 G_loss:1.60540 time_cost:56.7784  step:00007 D_loss:15.38677 time_cost:13.1365  step:00008 D_loss:15.38883 time_cost:12.4622  step:00009 D_loss:12.38719 time_cost:13.7129  step:00010 D_loss:10.36194 time_cost:12.3818  step:00011 D_loss:-2.97061 time_cost:13.4681  ==> step:00012 G_loss:1.03093 time_cost:27.2360  step:00013 D_loss:10.91956 time_cost:13.7071  step:00014 D_loss:10.10634 time_cost:13.7555  step:00015 D_loss:6.00499 time_cost:13.6784  step:00016 D_loss:-8.38905 time_cost:13.6509  step:00017 D_loss:-22.11833 time_cost:13.9234  ==> step:00018 G_loss:1.02747 time_cost:28.3925  step:00019 D_loss:78.22955 time_cost:13.6580  step:00020 D_loss:51.37152 time_cost:13.8747  step:00021 D_loss:33.22484 time_cost:13.7412  step:00022 D_loss:21.71333 time_cost:13.7939  step:00023 D_loss:31.06818 time_cost:13.6712  ==> step:00024 G_loss:0.96862 time_cost:28.2964  step:00025 D_loss:28.41310 time_cost:13.9727  step:00026 D_loss:26.28073 time_cost:13.7324  step:00027 D_loss:21.02436 time_cost:14.3333  step:00028 D_loss:17.93679 time_cost:14.3002  step:00029 D_loss:15.28124 time_cost:13.7549  ==> step:00030 G_loss:0.51390 time_cost:28.5788  step:00031 D_loss:5.08658 time_cost:13.8227  step:00032 D_loss:8.47297 time_cost:14.4835  step:00033 D_loss:2.40379 time_cost:13.9880  step:00034 D_loss:-5.77701 time_cost:13.7595  step:00035 D_loss:-23.29973 time_cost:13.6249  ==> step:00036 G_loss:0.76133 time_cost:28.4427  step:00037 D_loss:1.38272 time_cost:13.7218  step:00038 D_loss:-7.99489 time_cost:13.6285  step:00039 D_loss:-2.82512 time_cost:13.9723  step:00040 D_loss:-1.76494 time_cost:13.7711  step:00041 D_loss:20.16182 time_cost:14.4152  ==> step:00042 G_loss:0.71855 time_cost:28.4300  step:00043 D_loss:9.34317 time_cost:13.6513  step:00044 D_loss:24.80732 time_cost:13.9349  step:00045 D_loss:1.21541 time_cost:13.7895  step:00046 D_loss:10.25472 time_cost:14.4245  step:00047 D_loss:5.82635 time_cost:14.2184  ==> step:00048 G_loss:0.60195 time_cost:28.1839  step:00049 D_loss:-3.02651 time_cost:13.8890  step:00050 D_loss:-11.74504 time_cost:13.8308  step:00051 D_loss:-0.14545 time_cost:14.4897  step:00052 D_loss:-8.71203 time_cost:14.2027"
"1. When opencv was installed, it's 3.4.1 already;  2. When train.py was run, from neuralgym/ops/image_ops.py"", got ""import cv2: ImportError"": /libopencv_dnn.so.3.4: undefined symbol: _ZNK6google8protobuf7Message25InitializationErrorStringB5cxx11Ev  3. uninstall opencv, reinstall ""pip3 install opencv-python"" did not help    Please help on how to avoid, thanks.  Jay"
"I want to inpaint a special box in some images,and I get the location.I can change the function of random_bbox,but should  I change codes about the section of batch_data.I am confused.I need your help.Thank you. @JiahuiYu "
"Thank you for your code!  Now I want to train the model by using Places2, how do I set IMG_SHAPES, HEIGHT, WIDTH, MAX_DELTA_HEIGHT, MAX_DELTA_WIDTH?    IMG_SHAPES: [**512, 680**, 3]  HEIGHT: **128**  WIDTH: **128**  MAX_DELTA_HEIGHT: 32  MAX_DELTA_WIDTH: 32    Is that right?  thanks!  "
FYI.
"We are rushing for time, and trying to estimate the efforts needed and possible risks.    Thanks,  Jay"
"Hi, thank you for your great work!  I run your test demo code(test.py), but the results looks not so good. It looks like there is a shift to right-bottom.  !   !   !   !   !   !     I just follow your guide, ""python3 test.py --image examples/places2/building_input.png --mask examples/places2/building_mask.png --output examples/output.png --checkpoint_dir model_logs/release_places2_256"", do you know why?  thanks!"
None
"why is the tensor shape of the bbox2mask function's output [1, H, W,1]"
"Dear Jiahui,  thanks a lot for your great work first.  I am not sure about the process of training/testing user-guided inpainting.   Is it enough to just add a user-guided sketch channel in training/testing?  any additional loss function needed?  and in training, use full HED edged image or masked HED edged image? how about in testing?    Thank you~"
"Hello, the author. I am also study image inpainting. I am suffering from the poor ability of the machine and the long time of model training. Could you send me a copy of the model of DeepFill v1 training? My email is love666666shen@163.com . Thank you very much! "
"Hi, is the testing image which you use in this project, is inside training image?  Because you just take the image from celeba dataset which you trained with model to test your model Thats why the result look good, but if I test your model with new testing image, the result looks not good.  Example:  This is the result with your testing image which inside training dataset:  Inpainting_image  !   Result:  !   But if i use image not inside your training dataset the result looks not good:  Inpaingin_Image:  !   Result:  !   Groundtruth:  !       - For testing image, i take from this      "
"Traceback (most recent call last):    File ""test.py"", line 28, in        ng.get_gpus(1)    File ""/usr/local/lib/python3.5/dist-packages/neuralgym-0.0.1-py3.5.egg/neuralgym/utils/gpus.py"", line 54, in get_gpus  ValueError: invalid literal for int() with base 10: 'gpu'      Sometimes the above problems appear in the test. Have you ever encountered them？"
There are many color inconsistencies in my results.  Do you have any good Suggestions? Should other loss functions be added
"The paper of contextual attention says that for the spatial discounting mask, the wight of each pixel is calculated with the distance of the nearest known pixel. However, your implementation seems to calculate the weight with the distance from the outermost pixel of the image.         Could you explain the reason for this?"
"Hello,  Thanks for sharing the code. So, after I clone the repo, I downloaded the pretrained models for  CelebA and tried running the test.py code (Tensorflow version:1.6.0). I created a folder model_logs and placed all the files of the downloaded pre-trained model in that folder.  I run using the command:  python3 test.py --image examples/celeba/celebahr_patches_164036_input.png --mask examples/center_mask_256.png --output examples/output.png --checkpoint_dir model_logs/  i got the following error:   Traceback (most recent call last):    File ""test.py"", line 6, in        import neuralgym as ng  ImportError: No module named 'neuralgym'   although i installed neuralgym using :  pip install git+   and the output is:    /usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/__init__.py:83: RequestsDependencyWarning: Old version of cryptography ([1, 2, 3]) may cause slowdown.    warnings.warn(warning, RequestsDependencyWarning)  Collecting git+     Cloning   to /tmp/pip-req-build-dWpUvh  Requirement already satisfied (use --upgrade to upgrade): neuralgym==0.0.1 from git+  in /home/nermin/.local/lib/python2.7/site-packages  Building wheels for collected packages: neuralgym    Running setup.py bdist_wheel for neuralgym ... done    Stored in directory: /tmp/pip-ephem-wheel-cache-B8JcDK/wheels/bc/dd/bc/fbaacf774dfc69223a01b3e037b8539f24318193a37221bb3b  Successfully built neuralgym  so how to install neuralgym correctly? as apparently the program can't find it "
"hi jianhui,sorry to bother you again. we are in the image is none problem,and we check our path very carefully,and if we directly use cv2.imread(), the pictures load successfully  !   after checking the code in the  data_from_fnames.py,   !   !     so i was wondering the problem might be here cause the fnamelist we send is a list,and this func will change it into a (""image.png"",)tuple, then cause the problem.    we still can't solve it, would you please help us?  thanks for your time  "
"Hi, There is `GRAMS_LOSS_ALPHA` in inpaint.yml. So I'm curious, did you tried the style loss (as in the partial convolution paper) on your model? or maybe deepfill2? If so, what's the effect of the style loss?"
"Hi, Jiahui. I read your new paper 'Free-Form Image Inpainting with Gated Convolution', and mentioned that sketch was got by HED with threshold 0.6. You used the caffe model from HED's author or trained by yourself, and how did you get binary edge from the output?"
"First of all, thank you for sharing the project. I would like to ask about the problem of setting superparameters. Do you have any detailed analysis materials on the effects of the following four parameters on the training results  WGAN_GP_LAMBDA: 10  COARSE_L1_ALPHA: 1.3  L1_LOSS_ALPHA: 1.3  AE_LOSS_ALPHA: 1.3"
"Hi JiahuiYu，I want to train this model on celebA dataset, but I can not get the results as good as your provided results. I used the default train set on celebA and I trained our model about 5 days on TITAN X.   !   !   I set the ""random_crop"" as True like this  !   I changed the path of celebA dataset as below  !   The content of the celebA_trian.txt is showed below  !   Is there anything I miss?"
"Hi, thank you for sharing, because I saw you use a fixed mask during training. I would like to know how to use a random mask during training"
Why did you use ae_loss and l1_loss with x2  Is it smooth results?
Could you please explain offset flow?
Hi  This issue more convenient for neuralgym framework but when I use both of MultiGPUTrainer and SecondaryMultiGPUTrainer I got the deadlock  How can I place both of generator and discriminator at GPU?
"Hi, I am trying to understand the contextual attention layer and its back-propagation mechanism. My question is, if we extract patches (or filters) for each image separately then what is the network learning here?  Please if you can provide me some more details or any supporting material about it (in addition to what is mentioned in the paper), I will be thankful to you.    Further more in connection to this, I was checking the code and I noticed that there is a 'training' flag in the function arguments but it is not used anywhere.   "
Sorry to bother you...  Your test image is mask.png + image_with_mask.png  Maybe mask.png + raw_image.png is also OK?    
"Hello, sorry to bother you...  You said it can not solve irregular mask in the version(#77)，you v2 paper is difficult for me...   So, if I add irregular mask to your v1 model when training, it may be works well or not ?    What's more, when training Places2 model, all High-resolution images were used? It has 105G ...  If I want to train my own dataset with the given Places2 pre-trained model given, how many images and size(256*256 mentioned in paper) are required?   !   "
"Hi, jiahui, See the previous question, the image is 256*256, the mask is 128 * 256, the result will not be very good. If the picture is 256 × 256, the mask is  (12-60)*(40-230), eg, 40*220,  will the result be  not good?"
None
"I have conducted a experiment on the CelebA dataset, With randomly spliting the training subset(199399 samples) and testing subset(3200 samples), I have trained the model from scratch with 500k iteration. However, I can not obtain a satisfactory result.  !   !   I wonder if you can provide more training configurations on the CeleA dataset, such as training/testing split, total iteration and other related trianing hyperparameters. Thank you a lot!"
"Hello,   During training random fixed masks are applied on batches. It possible to use different masks for images in the batch?  Thank you"
Hi!  I have a few questions  1) Why did you not use Dropout at your network  2) Can I replace upsample convolutions to pixel shuffling?
`python test.py --image examples/input.png --mask examples/mask.png --output examples/output.png --checkpoint model_logs/your_model_dir`  The  input image for demo in upper code (input.png) has the white square hole.    Is the square hole necessary or only for clearer demo?
"Hi, Jiahui! i want to test the pretrain model on celebA. I download the pretrain model of celebA and put it at ./model_logs/celebA_model.      when I run  python test.py --image examples/celeba/celebahr_patches_164787_input.png --mask examples/center_mask_256.png --output examples/output.png --checkpoint_dir model_logs/celebA_model  ,I get the very bad result.  !     !   I don't know why it happens. can you help me?    "
"Hi, I saw you input the args when testing (corrupted image, mask).   `python test.py --image examples/input.png --mask examples/mask.png --output examples/output.png --checkpoint model_logs/your_model_dir`  I want to ask how you generate these images files? what tool do you use?  Thanks,"
"Hi, I tried to resume my model when training but it didnt load my `checkpoint` to continue train the model. It will start from beginning with epoch 0. I changed my `MODEL_RESTORE `in inpaint.yml file   for sure.  Do i need to do anything to make model continue training with my checkpoint?  This is my `checkpoint `folder  !   Thanks"
I have three question  1. how can i save and check loss plot?    2. how can we using tensorboard?    3. how many numbers data proper for add train data? we using 125 images.    thank you very much
"Hello,  Is it possible to modify the code for a loss different than wasserstein?   Thank you.  "
"Hi Jiahui,    Thanks for your work very much! It's wonderful !  And I wonder are the released model full trained? I tested the release_imagenet_256 model in some cases as below    !   !   !   !     They looks not very good. And I think these are simple case, because the masked region are mostly background.  Do you have any idea why it failed in such cases ?"
"Thank you for your work.  My question is about the input image size manipulation done during the test.py in the following code segment:  ` h, w, _ = image.shape`  `grid = 8`   ` image = image[:h//grid*grid, :w//grid*grid, :]`   `mask = mask[:h//grid*grid, :w//grid*grid, :]`  ` print('Shape of image: {}'.format(image.shape))`    I have run the code without these lines but of course I got errors. I could not understand the reason for this manipulation. Thank you."
"quoted from the paper: `gamma is set to 0.99 in all experiment`  about in inpaint.yml  `SPATIAL_DISCOUNTING_GAMMA: 0.9`    So have you tested which is better, 0.9 or 0.99? or it doesn't matter?"
"    kernel = 2*rate      raw_w = tf.extract_image_patches(          b, [1,kernel,kernel,1], [1,rate*stride,rate*stride,1], [1,1,1,1], padding='SAME')  for convolution, the kernel size is 3, but for deconvolution, why the kernel is `2*rate=4` with `stride=rate`?  why not use `3*3` kernel? Is there a reason for that?"
"Hi,  I want to ask question about how i can see the result of mask, input image, result stage 1 and stage 2 in every epoch or iteration when training model ?  Thanks"
I want to save 'c' every 10 epochs during training. Which parts of the code should be modified?  thank you.
"Hi, Thank for your work.  I want to generate the fixed mask for my image with the mask's size = 1/2 image's size and the mask is on the right side of original image.  so what i should change to get that because so far your code generate random mask in r`andom_bbox` and `bbox2mask` function.  Original  !     mask  !   Thanks  "
"I run test.py which points out “Error reading GPU information, set no GPU.” Actually, my computer has one GPU： GeForce GT 710, and I have run some tensorflow-based programs on it.    Can you give me some advice?    tensorflow 1.8+cuda 9+ cudnn7"
None
" inpaint_ops.py --imageA examples/style_transfer/bnw_butterfly.png  --imageB examples/style_transfer/bike.jpg --imag       File ""D:/work/work11/generative_inpainting-master/inpaint_ops.py"", line 313, in contextual_attention      offsets.set_shape(int_bs[:3] + [2])    File ""C:\Users\caocao\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 538, in set_shape      raise ValueError(str(e))  ValueError: Dimension 1 in both shapes must be equal, but are 166 and 64. Shapes are [1,166,250,2] and [1,64,64,2]."
"# downscaling foreground option: downscaling both foreground and  # background for matching and use original background for reconstruction.        f = resize(f, scale=1./rate, func=tf.image.resize_nearest_neighbor)      b = resize(b, to_shape=[int(raw_int_bs[1]/rate), int(raw_int_bs[2]/rate)], func=tf.image.resize_nearest_neighbor)  #      It should be raw_int_fs instead of raw_int_bs because of mathing b and f  Moreover if you try python inpaint_ops.py on images which have different shapes it will be error at offsets.set_shape(int_bs[:3] + [2])"
"Excuse me, I want to output the result of the first step. What should I do?"
"hello!    We did not get good results when We trained with new 1000 images, without changing the hyper-parameters   We are trying to fine-tune it using pre-train model.    When trying to resume training new images based on pretrained model(restored model), do i have to train with not only new images but also previous trained images or with only new images?    I want to fine tune 1,000 additional images in the pre-trained model with places dataset    Can we initialize the weight value of pre-train model to weight value of train?   By initializing the weight value of the 'training' to the weight value of the pre-trained model, is it possible to do additional training?   Is there another way to 'train' with the information value of 'pre-train model'?    thank you  "
"Hi, Jiahui. I see your comments on #14 but it cannot solve my problem. Same situation happens again. When I changed the the line you mentioned in #14  into ""MultiGPUTrainer"". The error is   Traceback (most recent call last):    File ""train.py"", line 100, in        log_dir=log_prefix,    File ""/home/mc/miniconda3/envs/DRL/lib/python3.6/site-packages/neuralgym/train/multigpu_trainer.py"", line 27, in __init__      self._train_op, self._loss = self.train_ops_and_losses()    File ""/home/mc/miniconda3/envs/DRL/lib/python3.6/site-packages/neuralgym/train/multigpu_trainer.py"", line 70, in train_ops_and_losses      assert loss is None('For multigpu training, graph_def and kwargs'  TypeError: 'NoneType' object is not callable, which is the same as other users in #14 .  I thought the reason why this kind of error came out is the line 70 of multigpu_trainer.py so that I commented out this line. However, a new error showed :  Traceback (most recent call last):    File ""train.py"", line 101, in        num_gpus=2    File ""/home/mc/miniconda3/envs/DRL/lib/python3.6/site-packages/neuralgym/train/multigpu_trainer.py"", line 27, in __init__      self._train_op, self._loss = self.train_ops_and_losses()    File ""/home/mc/miniconda3/envs/DRL/lib/python3.6/site-packages/neuralgym/train/multigpu_trainer.py"", line 99, in train_ops_and_losses      grads = average_gradients(tower_grads)    File ""/home/mc/miniconda3/envs/DRL/lib/python3.6/site-packages/neuralgym/ops/train_ops.py"", line 22, in average_gradients      grad = tf.add_n([x[0] for x in grad_and_vars])  NameError: name 'tf' is not defined    Therefore, I need your help.  Thanks a lot!    "
"I am very grateful to the author's contribution. Recently I have collected some pictures of the clothing of the electric business. The result is not very good, so I want to know how to make a fine tuning on the Imagenet model. Thank you very much!!!"
"Hi,   Congratulations on successfully carrying out this great work! I am trying to train the model from scratch on ~200,000 celebA images. Apparently, its stuck at this point with no further output in 2 days (see output below). Seems it hasn't finished even first epoch so far. Is it normal?     I have 2 GPUs installed but I am only using 1 by setting NUM_GPUS: 0 in inpaint.yml (because for 1, gpus.py was giving GPU unavailability error). Please correct me if I am wrong, but here, by 0 I remember I read an issue where you said it means one GPU.      > [2018-06-13 01:43:34 @logger.py:43] Trigger callback: Trigger WeightsViewer: logging model weights...  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv1/kernel:0, shape: [5, 5, 5, 32], size: 4000  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv1/bias:0, shape: [32], size: 32  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv2_downsample/kernel:0, shape: [3, 3, 32, 64], size: 18432  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv2_downsample/bias:0, shape: [64], size: 64  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv3/kernel:0, shape: [3, 3, 64, 64], size: 36864  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv3/bias:0, shape: [64], size: 64  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv4_downsample/kernel:0, shape: [3, 3, 64, 128], size: 73728  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv4_downsample/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv5/kernel:0, shape: [3, 3, 128, 128], size: 147456  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv5/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv6/kernel:0, shape: [3, 3, 128, 128], size: 147456  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv6/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv7_atrous/kernel:0, shape: [3, 3, 128, 128], size: 147456  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv7_atrous/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv8_atrous/kernel:0, shape: [3, 3, 128, 128], size: 147456  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv8_atrous/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv9_atrous/kernel:0, shape: [3, 3, 128, 128], size: 147456  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv9_atrous/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv10_atrous/kernel:0, shape: [3, 3, 128, 128], size: 147456  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv10_atrous/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv11/kernel:0, shape: [3, 3, 128, 128], size: 147456  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv11/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv12/kernel:0, shape: [3, 3, 128, 128], size: 147456  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv12/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv13_upsample/conv13_upsample_conv/kernel:0, shape: [3, 3, 128, 64], size: 73728  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv13_upsample/conv13_upsample_conv/bias:0, shape: [64], size: 64  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv14/kernel:0, shape: [3, 3, 64, 64], size: 36864  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv14/bias:0, shape: [64], size: 64  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv15_upsample/conv15_upsample_conv/kernel:0, shape: [3, 3, 64, 32], size: 18432  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv15_upsample/conv15_upsample_conv/bias:0, shape: [32], size: 32  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv16/kernel:0, shape: [3, 3, 32, 16], size: 4608  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv16/bias:0, shape: [16], size: 16  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv17/kernel:0, shape: [3, 3, 16, 3], size: 432  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/conv17/bias:0, shape: [3], size: 3  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/xconv1/kernel:0, shape: [5, 5, 5, 32], size: 4000  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/xconv1/bias:0, shape: [32], size: 32  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/xconv2_downsample/kernel:0, shape: [3, 3, 32, 32], size: 9216  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/xconv2_downsample/bias:0, shape: [32], size: 32  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/xconv3/kernel:0, shape: [3, 3, 32, 64], size: 18432  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/xconv3/bias:0, shape: [64], size: 64  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/xconv4_downsample/kernel:0, shape: [3, 3, 64, 64], size: 36864  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/xconv4_downsample/bias:0, shape: [64], size: 64  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/xconv5/kernel:0, shape: [3, 3, 64, 128], size: 73728  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/xconv5/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/xconv6/kernel:0, shape: [3, 3, 128, 128], size: 147456  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/xconv6/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/xconv7_atrous/kernel:0, shape: [3, 3, 128, 128], size: 147456  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/xconv7_atrous/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/xconv8_atrous/kernel:0, shape: [3, 3, 128, 128], size: 147456  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/xconv8_atrous/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/xconv9_atrous/kernel:0, shape: [3, 3, 128, 128], size: 147456  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/xconv9_atrous/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/xconv10_atrous/kernel:0, shape: [3, 3, 128, 128], size: 147456  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/xconv10_atrous/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/pmconv1/kernel:0, shape: [5, 5, 5, 32], size: 4000  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/pmconv1/bias:0, shape: [32], size: 32  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/pmconv2_downsample/kernel:0, shape: [3, 3, 32, 32], size: 9216  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/pmconv2_downsample/bias:0, shape: [32], size: 32  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/pmconv3/kernel:0, shape: [3, 3, 32, 64], size: 18432  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/pmconv3/bias:0, shape: [64], size: 64  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/pmconv4_downsample/kernel:0, shape: [3, 3, 64, 128], size: 73728  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/pmconv4_downsample/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/pmconv5/kernel:0, shape: [3, 3, 128, 128], size: 147456  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/pmconv5/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/pmconv6/kernel:0, shape: [3, 3, 128, 128], size: 147456  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/pmconv6/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/pmconv9/kernel:0, shape: [3, 3, 128, 128], size: 147456  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/pmconv9/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/pmconv10/kernel:0, shape: [3, 3, 128, 128], size: 147456  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/pmconv10/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/allconv11/kernel:0, shape: [3, 3, 256, 128], size: 294912  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/allconv11/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/allconv12/kernel:0, shape: [3, 3, 128, 128], size: 147456  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/allconv12/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/allconv13_upsample/allconv13_upsample_conv/kernel:0, shape: [3, 3, 128, 64], size: 73728  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/allconv13_upsample/allconv13_upsample_conv/bias:0, shape: [64], size: 64  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/allconv14/kernel:0, shape: [3, 3, 64, 64], size: 36864  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/allconv14/bias:0, shape: [64], size: 64  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/allconv15_upsample/allconv15_upsample_conv/kernel:0, shape: [3, 3, 64, 32], size: 18432  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/allconv15_upsample/allconv15_upsample_conv/bias:0, shape: [32], size: 32  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/allconv16/kernel:0, shape: [3, 3, 32, 16], size: 4608  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/allconv16/bias:0, shape: [16], size: 16  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/allconv17/kernel:0, shape: [3, 3, 16, 3], size: 432  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: inpaint_net/allconv17/bias:0, shape: [3], size: 3  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: discriminator/discriminator_local/conv1/kernel:0, shape: [5, 5, 3, 64], size: 4800  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: discriminator/discriminator_local/conv1/bias:0, shape: [64], size: 64  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: discriminator/discriminator_local/conv2/kernel:0, shape: [5, 5, 64, 128], size: 204800  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: discriminator/discriminator_local/conv2/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: discriminator/discriminator_local/conv3/kernel:0, shape: [5, 5, 128, 256], size: 819200  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: discriminator/discriminator_local/conv3/bias:0, shape: [256], size: 256  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: discriminator/discriminator_local/conv4/kernel:0, shape: [5, 5, 256, 512], size: 3276800  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: discriminator/discriminator_local/conv4/bias:0, shape: [512], size: 512  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: discriminator/discriminator_global/conv1/kernel:0, shape: [5, 5, 3, 64], size: 4800  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: discriminator/discriminator_global/conv1/bias:0, shape: [64], size: 64  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: discriminator/discriminator_global/conv2/kernel:0, shape: [5, 5, 64, 128], size: 204800  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: discriminator/discriminator_global/conv2/bias:0, shape: [128], size: 128  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: discriminator/discriminator_global/conv3/kernel:0, shape: [5, 5, 128, 256], size: 819200  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: discriminator/discriminator_global/conv3/bias:0, shape: [256], size: 256  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: discriminator/discriminator_global/conv4/kernel:0, shape: [5, 5, 256, 256], size: 1638400  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: discriminator/discriminator_global/conv4/bias:0, shape: [256], size: 256  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: discriminator/dout_local_fc/kernel:0, shape: [32768, 1], size: 32768  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: discriminator/dout_local_fc/bias:0, shape: [1], size: 1  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: discriminator/dout_global_fc/kernel:0, shape: [65536, 1], size: 65536  > [2018-06-13 01:43:34 @weights_viewer.py:43] - weight name: discriminator/dout_global_fc/bias:0, shape: [1], size: 1  > [2018-06-13 01:43:34 @logger.py:43] Trigger callback: Total counts of trainable weights: 10674312.  > [2018-06-13 01:43:34 @weights_viewer.py:60] Total size of trainable weights: 0G 10M 184K 136B (Assuming32-bit data type.)        "
"Hi,    Great work! I would like to use your code to complete textures, could you provide the pretrained model of DTD dataset?     Thanks.    "
"Hi. Should I use corrupted image with patches as input or clear image when I use your pretrained model to test?In my experiment,the result seems different. Thanks for a lot."
Why you are giving mask  and clear image as input.  why not  only corrupted  image as input.
can't connect to your server
"I am writing you a question because I need to get your project 'generative_inpainting' trained quickly.    The following error occurs first when you train. : --(1)  error>>  2018-06-04 15:35:43.372845: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA  2018-06-04 15:35:43.479918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties:   name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531  pciBusID: 0000:05:00.0  totalMemory: 11.90GiB freeMemory: 9.25GiB  2018-06-04 15:35:43.479951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0  2018-06-04 15:35:43.665966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:  2018-06-04 15:35:43.666006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0   2018-06-04 15:35:43.666012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N   2018-06-04 15:35:43.666232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8949 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:05:00.0, compute capability: 6.1)    Refer to the answer to question 41 and I corrected the 'train.py' file as shown below. :  train.py>>      if config.GPU_ID != -1:          ng.set_gpus(config.GPU_ID)      else:          ng.get_gpus(config.NUM_GPUS)      # training data      os.environ['CUDA_VISIBLE_DEVICES'] = '0'    Secondly, there is an error loading the file.:  error>>  [2018-06-04 15:35:46 @data_from_fnames.py:153] image is None, sleep this thread for 0.1s.  [2018-06-04 15:35:46 @data_from_fnames.py:153] image is None, sleep this thread for 0.1s.  [2018-06-04 15:35:46 @data_from_fnames.py:153] image is None, sleep this thread for 0.1s.  [2018-06-04 15:35:46 @data_from_fnames.py:153] image is None, sleep this thread for 0.1s.  [2018-06-04 15:35:46 @data_from_fnames.py:153] image is None, sleep this thread for 0.1s.  [2018-06-04 15:35:46 @data_from_fnames.py:153] image is None, sleep this thread for 0.1s.  [2018-06-04 15:35:46 @data_from_fnames.py:153] image is None, sleep this thread for 0.1s.  [2018-06-04 15:35:46 @data_from_fnames.py:153] image is None, sleep this thread for 0.1s.  [2018-06-04 15:35:46 @data_from_fnames.py:153] image is None, sleep this thread for 0.1s.  OpenCV(3.4.1) Error: Assertion failed (ssize.width > 0 && ssize.height > 0) in resize, file /io/opencv/modules/imgproc/src/resize.cpp, line 4044  OpenCV(3.4.1) Error: Assertion failed (ssize.width > 0 && ssize.height > 0) in resize, file /io/opencv/modules/imgproc/src/resize.cpp, line 4044  OpenCV(3.4.1) Error: Assertion failed (ssize.width > 0 && ssize.height > 0) in resize, file /io/opencv/modules/imgproc/src/resize.cpp, line 4044    I modified the list of 'train_shuffled.flist' files to match place2 data path. :  ex) data/places2_real/data_large/a/airfield/00000001.jpg  ﻿  and I also printed the path during the training.:  consol>>  file_load  ['/home/pmi/Documents/GAN/generative_inpainting-master/data/places2_real/data_large/a/airfield/00000001.jpg', '/home/pmi/Documents/GAN/generative_inpainting-master/data/places2_real/data_large/a/airfield/00000002.jpg', '/home/pmi/Documents/GAN/generative_inpainting-master/data/places2_real/data_large/a/airfield/00000003.jpg', '/home/pmi/Documents/GAN/generative_inpainting-master/data/places2_real/data_large/a/airfield/00000004.jpg', '/home/pmi/Documents/GAN/generative_inpainting-master/data/places2_real/data_large/a/airfield/00000005.jpg', '/home/pmi/Documents/GAN/generative_inpainting-master/data/places2_real/data_large/a/airfield/00000006.jpg', '/home/pmi/Documents/GAN/generative_inpainting-master/data/places2_real/data_large/a/airfield/00000007.jpg', '/home/pmi/Documents/GAN/generative_inpainting-master/data/places2_real/data_large/a/airfield/00000008.jpg', '/home/pmi/Documents/GAN/generative_inpainting-master/data/places2_real/data_large/a/airfield/00000009.jpg', '']    Where can I get a flist file?  Why did an error such as ""--(1)""?    Thank you for reading through.  I'll be waiting for the reply.  "
"How do I get it to run 1000's of iterations?  When I run   it seems to only do a first pass. I have a Tensorboard client running on my local machine ok, but can't seem to figure out any more due to my lack of experience here.  Any help appreciated."
"Could I know how many epoch you have used to train your model (e.g., release_places2_256 model)? Any suggest min epoch to re-produce your impainting result?    Also, I observe the loss kept increasing during my training (e.g., from -0.47 to -0.21 and -0.06 ...). Does that indicate that I am not on the right track? (I test the resulted model and they do have bad accuracy.) What parameter should I check if it is not normal? Thanks!      |####################| 100.00%, 74660/0 sec. train epoch 1, iter 10000/10000, loss -0.472547, 0.13 batches/sec.  |####################| 100.00%, 74743/0 sec. train epoch 2, iter 10000/10000, loss -0.217382, 0.13 batches/sec.  |####################| 100.00%, 74793/0 sec. train epoch 3, iter 10000/10000, loss -0.061427, 0.13 batches/sec.  |#################---| 87.40%, 65406/9362 sec. train epoch 4, iter 8740/10000, loss -0.031020, 0.13 batches/sec."
"Fantastic work here, can't wait to get it running for some cultural preservation work.    I'm trying to run on an AWS deep learning GPU compute instance under ubuntu 16.04.  Anyway, I've got a few issues getting everything to work that I'm hoping are simple to fix.    Starting with training, I've copied the contents of the google drive into a data/celeba_hq/ folder.  When I run train.py, the process freezes with:     (aborting via control-c won't work, so I have to suspend the task and then      Does anything need to be altered in   to get the default celeb data set to work?  Your notes say to alter DATA_FLIST, LOG_DIR, IMG_SHAPES, however I can only see LOG_DIR as a parameter.     Sorry for the questions which I'm sure will have obvious answers!      "
I wonder if you have implemented it with pytorch? I want to implement it using pytorch. Will the performance be decreased a lot? 
"Hello. I have another question.  In each epoch, snap file is created and added the line in checkpoint.  `model_checkpoint_path: ""snap-30000""  all_model_checkpoint_paths: ""snap-10000""  all_model_checkpoint_paths: ""snap-20000""  all_model_checkpoint_paths: ""snap-30000""  `  What does it mean? each epoch has different loss value.  In your pretrained model, there is only one snap model.  Do I choose one snap with low loss value or   Is it a structure that uses all snap files together?    Regarding, JiyoungAn"
"During the testing stage (test.py), the network has to be built with a fixed size of image, such as    ------------------      #the image size is determined here       input_image = np.concatenate([image, mask], axis=2)        sess_config = tf.ConfigProto()      sess_config.gpu_options.allow_growth = True      with tf.Session(config=sess_config) as sess:          input_image = tf.constant(input_image, dtype=tf.float32)          #the inference can now only accept a pre-defined size of image          output = model.build_server_graph(input_image)  ------------------    Just wander can I build an inference network that can accept various input size (i.e., with no need to pre-define the image size)?    or to be more specific, can I build a inference network that can achieve    output = model.build_server_graph(input_image)    where the input_image is a tensor of (batch_size, width, height, channel) = (1, None, None, 3)    "
"Hello, JiahuiYu.  Thank you for answering me.  However, I got this error again..  `  [2018-05-25 17:43:37 @data_from_fnames.py:153] image is None, sleep this thread for 0.1s.  Exception in thread Thread-16: Traceback (most recent call last):    File ""/usr/lib/python3.5/threading.py"", line 914, in _bootstrap_inner      self.run()    File ""/usr/lib/python3.5/threading.py"", line 862, in run      self._target(*self._args, **self._kwargs)    File ""/usr/local/lib/python3.5/dist-packages/neuralgym/data/feeding_queue_runner.py"", line 194, in _run      data = func()    File ""/usr/local/lib/python3.5/dist-packages/neuralgym/data/data_from_fnames.py"", line 143, in        feed_dict_op=[lambda: self.next_batch()],    File ""/usr/local/lib/python3.5/dist-packages/neuralgym/data/data_from_fnames.py"", line 182, in next_batch      img = cv2.resize(img, tuple(self.shapes[i][:-1]))  cv2.error: /io/opencv/modules/imgproc/src/resize.cpp:4044: error: (-215) ssize.width > 0 && ssize.height > 0 in function resize`    It seems like there is no image.  I used code in #15 for creating flist.  They are all in ""data_flist"" file.  At inpaint.yml, I add DATA_FLIST: city: ['data_flist/city/train_shuffled.flist','data_flist/city/validation_shuffled.flist']  and Set dataset as city.    Below is log about my Dataset  `[2018-05-25 17:42:59 @dataset.py:26] --------------------------------- Dataset Info ---------------------------------  [2018-05-25 17:42:59 @dataset.py:36] fn_preprocess: None  [2018-05-25 17:42:59 @dataset.py:36] file_length: 726  [2018-05-25 17:42:59 @dataset.py:36] return_fnames: False  [2018-05-25 17:42:59 @dataset.py:36] nthreads: 16  [2018-05-25 17:42:59 @dataset.py:36] batch_phs: [ ]  [2018-05-25 17:42:59 @dataset.py:36] dtypes: [tf.float32]  [2018-05-25 17:42:59 @dataset.py:36] index: 0  [2018-05-25 17:42:59 @dataset.py:36] queue_size: 256  [2018-05-25 17:42:59 @dataset.py:36] filetype: image  [2018-05-25 17:42:59 @dataset.py:36] enqueue_size: 32  [2018-05-25 17:42:59 @dataset.py:36] random_crop: False  [2018-05-25 17:42:59 @dataset.py:36] random: False  [2018-05-25 17:42:59 @dataset.py:36] shapes: [[256, 256, 3]]  [2018-05-25 17:42:59 @dataset.py:37] --------------------------------------------------------------------------------`    What should I do?  Thank you."
"Hello, I have a question and I am writing.   I want to 'test' the 'canyon_input.png' image in the 'examples' folder.  I also want to 'train' the 'places2' dataset  Look for your github and save the files to be stored in 'model_logs'  I wrote an error below the article saying that an error occurred in 'neuralgym'.  Your answers I'll wait.           ng.get_gpus(1)    File ""/home/pmi/Documents/Songhee/songhee/lib/python3.5/site-packages/neuralgym/utils/gpus.py"", line 70, in get_gpus      ' [(gpu id: num of processes)]: {}'.format(sorted_gpus))  SystemError: No enough gpus for dedicated usage. [(gpu id: num of processes)]: [(0, 3)]  "
"Hello,  Thanks for sharing the codebase publicly. So, after I clone the repo, I downloaded the pretrained models for HQ CelebA and tried running the test.py code (Tensorflow version:1.6.0). I created a folder model_logs and placed all the files of the downloaded pre-trained model in that folder. However, I get the error that:  ` Couldn't find 'checkpoint' file or checkpoints in given directory model_logs/'     I run using the command:  python test.py --image examples/celeba/celebahr_patches_164036_input.png --mask examples/center_mask_256.png --output examples/output.png --checkpoint_dir model_logs/      And the contents of model_logs are:    checkpoint.txt    snap-0.data-00000-of-00001    snap-0.index    snap-0.meta    train_shuffled.flist    val_shuffled.flist    Can you please let me know the correct way to just run your pretrained models.    Thanks,  Avisek"
"Hi Jiahui,    Nice work!  I have a question about the model size. In your provided trained model, the size is pretty small (e.g., around 14 MB). However, the output after running your train.py is far larger (e.g., 126M). Just wonder did you conduct any post-processing to the model? or is there any parameters in your code to tune the model size? (any trade-off for such a compression?)    [Released model]         69 May 22 16:15 checkpoint*    14M May 22 16:15 snap-0.data-00000-of-00001*   3.6K May 22 16:15 snap-0.index*    14M May 22 16:15 snap-0.meta*      [output of train.py]    77 May 23 15:50 checkpoint   84M May 23 20:01 events.out.tfevents.1527040937.dgx1-server2  126M May 23 15:50 snap-10000.data-00000-of-00001   12K May 23 15:50 snap-10000.index   14M May 23 15:50 snap-10000.meta  "
"First of all, Thank you for your good research.    When I run the code, it looks like g_loss is the only loss that is printed, is that correct?    and d_loss is sometimes negative, is it ok?    "
"when I run the code, the error module 'tensorflow.python.ops.nn' has no attribute 'leaky_relu'. how can I fix it?"
"In your paper, I can only find the l1 loss, I am confused about the ae loss?"
"Hi, I am a beginner in tensorflow and I'm very interested in your paper.  I'll be very grateful if you could answer my question.    1. run only using CPU  I am trying to run train.py using CPU only.  So I changed NUM_GPUS to 0 (inpaint.yml).  However error related with reading gpu still has occurred and it stops after below sentence.  `32m[2018-05-23 19:02:54 @weights_viewer.py:60][0m Total size of trainable weights: 0G 10M 184K 136B (Assuming32-bit data type.)`  Are there any settings that need to be changed or Is it impossible to run only using cpu?    2. Adding train image in existing model  I'd like to add some training images(#800(shape 255,255,3)) in your place2 model.  1) downloading the place2 model you've built  2) locate the model into the model_logs file   3) train  In this way, will my image be added to the existing model?    Regarding, jiyoungAn"
"In the code of contextual attention, foreground is x, background also x. I'm confused what's the sense to match x with x. Can you help me?"
"when I running the program ,there are some errors like this:  [2018-05-20 20:13:24 @logger.py:43] Trigger callback: Total counts of trainable weights: 10582152.  [2018-05-20 20:13:24 @weights_viewer.py:60] Total size of trainable weights: 0G 10M 94K 136B (Assuming32-bit data type.)  |##############################---------------------| 58.00%, 36937/26403 sec. train epoch 1, iter 2900/5000, loss 0.891349, 0.08 batches/sec.Traceback (most recent call last):    File ""train.py"", line 112, in        trainer.train()    File ""/home/anaconda3/lib/python3.6/site-packages/neuralgym/train/trainer.py"", line 133, in train      cb.run(sess, step)    File ""/home/anaconda3/lib/python3.6/site-packages/neuralgym/callbacks/secondary_trainer.py"", line 26, in run      self.train()    File ""/home/anaconda3/lib/python3.6/site-packages/neuralgym/train/trainer.py"", line 143, in train      assert not np.isnan(loss_value)  AssertionError    I can't solve it. is it machine problem? I hope you give me some ideas. thanks.  "
"When trying to resume training new images based on pretrained model(restored model), do i have to train with not only new images but also previous trained images or with only new images?  In the former case, i wonder if it can cause overfitting with previous trained images as training iteration increases."
I think your demo web is very good.Can you put the demo web code to let me study it.
"inpaint.yml setting:       During training, i met OOM error :        any possible suggestions..? (e.g. reduce BATCH_SIZE or IMG_SHAPES. In case of reducing IMG_SHAPES, i wonder if it downgrades output quality..)  "
"I tried to run the test script on the pre-trained models for the image net task on cpu on Mac OS X Sierra with Tensorflow 1.7.0 and it gave me an error that there is not output_type argument for tf.arg_max in inpainting_ops.py on line 302, I was able to fix that bug using this line.     `offset = tf.cast(tf.argmax(yi, axis=3),tf.int32)`    Just bringing this to notice if anyone faces similar bug in the future and would be better to cast it for preventing the bug from causing. "
"Hi,     Great work! I would like to play with your code and I was wondering the following:    1) How do you prepare the data in the flist file?  2) I want to create my own masked images, what steps should I follow?    Thanks,  Alex"
I think It would be better if this repo had requirements.txt file and added description on README :D
I am sorry I am a little confused about the training of the WGAN-GP. Does it train the discriminator five times and the generator one time alternately?
"when I input my picture shape (63,44,3) and running train.py .  But It shows some errors like this:  Traceback (most recent call last):    File ""train.py"", line 47, in        images, config=config)    File ""/opencv/generative_inpainting/inpaint_model.py"", line 153, in build_graph_with_losses      padding=config.PADDING)    File opencv/generative_inpainting/inpaint_model.py"", line 97, in build_inpaint_net      x, offset_flow = contextual_attention(x, x, mask_s, 3, 1, rate=2)    File ""/opencv/generative_inpainting/inpaint_ops.py"", line 307, in contextual_attention      yi = tf.nn.conv2d_transpose(yi, wi_center, tf.concat([[1], raw_fs[1:]], axis=0), strides=[1,rate,rate,1]) / 4.    File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1187, in conv2d_transpose      filter.get_shape()[3]))  ValueError: input channels does not match filter's input channels, 40 != 48  I don't know how to fix it. can you give me any suggestions? Thanks.  "
I saw you uploaded more results on faces that look good. How long have you trained? I have trained model with celeba_hq dataset for 8 days with the same hyper-parameters as in your inpain.yml on a Titan X(Pascal). But the test result is terrible.  !    Can you give me some advices? Thank you!
"Recently ,I read your paper and want to run your code. but I don't know how to create a .flist file? can you tell me in detail? Thank you."
i wanted to use custom dataset  so edited 'inpaint.yml'  as following:     and also added absolute file path of training img to 'data/buildings/train_shuffled.flist' as following example:       but ng.data_from_fnames.py > function 'read_img'        why is img None..? 
"In the function gradients_penalty, I can't understand the code:  gradients = tf.gradients(y, x)[0]  why do we need to get the dimension [0], not all of them?"
"When you train the model, do you optimize the params of stage-1 and stage-2 jointly?   Or you first pre-train the stage-1 and then train stage-2 and finally optimize them jointly?"
"Hello there,  first of all congratulations for this awesome project!    I need help regarding the training process.  I learned from #15 that we need a training and a validation dataset but I still have some confusion.    Is the training dataset just a set of pictures with white rectangles covering some parts and the validation dataset the same pictures without any occlusion?"
"In my opinion, the kernel size when extract patches to generate deconvolution filters (`raw_w`) should be the same with the kernel size to generate the convolution filters `(w).`  But why raw_w use `kernel=2*rate`, but w use `ksize`?"
"Although you have released pretrained models, I want to train a model by myself with the celeba_hq dataset. With images of size 1024*1024, I set the hyper-parameter  IMG_SHAPES: [256, 256, 3]  HEIGHT: 128  WIDTH: 128  MAX_DELTA_HEIGHT: 64  MAX_DELTA_WIDTH: 64  RANDOM_CROP: False  other hyper-parameters same as yours in inpaint.yml,  but I can't get a good model like yours.  Can you give me some advice with the hyper-parameter?"
"First I want to say congratulations for the amazing job you did!    I did not find any example of what the `Prepare training images filelist.` step consists on. How must the images be listed? What is the format? One file name per line, like this?       "
"    Thanks for your great work on inpaiinting! I am trying to train the inpainting network according to your guide on the README. The training seems okay.      However, the training is so slow. Below shows the training log:  |####################| 100.00%, 109326/0 sec. train epoch 1, iter 10000/10000, loss 0.089837, 0.09 batches/sec.  [2018-04-20 02:47:04 @logger.py:43] Trigger callback: Trigger ModelSaver: Save model to model_logs/20180418202435371480_node1_imagenet_NORMAL_wgan_gp_full_model_image_256/snap-10000.  |####################| 100.00%, 108851/0 sec. train epoch 2, iter 10000/10000, loss 0.030459, 0.09 batches/sec.  [2018-04-21 09:01:15 @logger.py:43] Trigger callback: Trigger ModelSaver: Save model to model_logs/20180418202435371480_node1_imagenet_NORMAL_wgan_gp_full_model_image_256/snap-20000.  |####################| 100.00%, 108510/0 sec. train epoch 3, iter 10000/10000, loss 0.030714, 0.09 batches/sec.  [2018-04-22 15:09:45 @logger.py:43] Trigger callback: Trigger ModelSaver: Save model to model_logs/20180418202435371480_node1_imagenet_NORMAL_wgan_gp_full_model_image_256/snap-30000.  |####################| 100.00%, 108276/0 sec. train epoch 4, iter 10000/10000, loss 0.038624, 0.09 batches/sec.  [2018-04-23 21:14:21 @logger.py:43] Trigger callback: Trigger ModelSaver: Save model to model_logs/20180418202435371480_node1_imagenet_NORMAL_wgan_gp_full_model_image_256/snap-40000.  |############--------| 61.50%, 66691/41605 sec. train epoch 5, iter 6150/10000, loss 0.038147, 0.09 batches/sec.        I use one K80 to train the network.       I have a question about the training, how can I speed up the training? Thanks a lot!"
"    Thanks for your great work on inpaiinting! I am trying to train the inpainting network according to your guide on the README. The training seems okay.      However, the training is so slow. Below shows the training log:  |####################| 100.00%, 109326/0 sec. train epoch 1, iter 10000/10000, loss 0.089837, 0.09 batches/sec.  [2018-04-20 02:47:04 @logger.py:43] Trigger callback: Trigger ModelSaver: Save model to model_logs/20180418202435371480_node1_imagenet_NORMAL_wgan_gp_full_model_image_256/snap-10000.  |####################| 100.00%, 108851/0 sec. train epoch 2, iter 10000/10000, loss 0.030459, 0.09 batches/sec.  [2018-04-21 09:01:15 @logger.py:43] Trigger callback: Trigger ModelSaver: Save model to model_logs/20180418202435371480_node1_imagenet_NORMAL_wgan_gp_full_model_image_256/snap-20000.  |####################| 100.00%, 108510/0 sec. train epoch 3, iter 10000/10000, loss 0.030714, 0.09 batches/sec.  [2018-04-22 15:09:45 @logger.py:43] Trigger callback: Trigger ModelSaver: Save model to model_logs/20180418202435371480_node1_imagenet_NORMAL_wgan_gp_full_model_image_256/snap-30000.  |####################| 100.00%, 108276/0 sec. train epoch 4, iter 10000/10000, loss 0.038624, 0.09 batches/sec.  [2018-04-23 21:14:21 @logger.py:43] Trigger callback: Trigger ModelSaver: Save model to model_logs/20180418202435371480_node1_imagenet_NORMAL_wgan_gp_full_model_image_256/snap-40000.  |############--------| 61.50%, 66691/41605 sec. train epoch 5, iter 6150/10000, loss 0.038147, 0.09 batches/sec.        I use one K80 to train the network.       I have a question about the training, how can I speed up the training? Thanks a lot!"
"    Thanks for your great work on inpaiinting! I am trying to train the inpainting network according to your guide on the README. The training seems okay.      However, the training is so slow. Below shows the training log:  |####################| 100.00%, 109326/0 sec. train epoch 1, iter 10000/10000, loss 0.089837, 0.09 batches/sec.  [2018-04-20 02:47:04 @logger.py:43] Trigger callback: Trigger ModelSaver: Save model to model_logs/20180418202435371480_node1_imagenet_NORMAL_wgan_gp_full_model_image_256/snap-10000.  |####################| 100.00%, 108851/0 sec. train epoch 2, iter 10000/10000, loss 0.030459, 0.09 batches/sec.  [2018-04-21 09:01:15 @logger.py:43] Trigger callback: Trigger ModelSaver: Save model to model_logs/20180418202435371480_node1_imagenet_NORMAL_wgan_gp_full_model_image_256/snap-20000.  |####################| 100.00%, 108510/0 sec. train epoch 3, iter 10000/10000, loss 0.030714, 0.09 batches/sec.  [2018-04-22 15:09:45 @logger.py:43] Trigger callback: Trigger ModelSaver: Save model to model_logs/20180418202435371480_node1_imagenet_NORMAL_wgan_gp_full_model_image_256/snap-30000.  |####################| 100.00%, 108276/0 sec. train epoch 4, iter 10000/10000, loss 0.038624, 0.09 batches/sec.  [2018-04-23 21:14:21 @logger.py:43] Trigger callback: Trigger ModelSaver: Save model to model_logs/20180418202435371480_node1_imagenet_NORMAL_wgan_gp_full_model_image_256/snap-40000.  |############--------| 61.50%, 66691/41605 sec. train epoch 5, iter 6150/10000, loss 0.038147, 0.09 batches/sec.        I use one K80 to train the network.       I have a question about the training, how can I speed up the training? Thanks a lot!"
"I'm training network with Stanford Cars dataset, and now it's on 10th epoch.  But the loss showed in terminal does not change from 0.14xx to 0.19xx.  Also, in TensorBoard, there are many losses but all of them are just oscillating and not converging.  Should I modify some of parameters in inpaint.xml to solve this problem?"
Great work!  I have a question about the  STN-based attention for image inpainting (Fig. 8 in the paper). I wonder what the loss function of STN-based attention network is ? L1 or WGAN-GP + L1?    Thank you.
"Hi, JiahuiYu    Can I ask for the discriminator ckpt file?    Thanks!"
"Hi, JiahuiYu    Thank you for your contribution and release those code!  I have a question about `inpaint_ops.py#L283`,  `xi` should be normed?    您好根據您的 paper, `xi` 是否應該也要 normed?  另外你的 work 非常有趣!    Thanks!  "
"Hi,    Thank you so much for such an impressive work and sharing it.    I would like to train this approach on a new dataset. However during training, I get the following error:  Exception in thread Thread-13:    Traceback (most recent call last):    File ""/usr/lib/python3.5/threading.py"", line 914, in _bootstrap_inner  self.run()  File ""/usr/lib/python3.5/threading.py"", line 862, in run  self._target(*self._args, **self._kwargs)  File ""/usr/local/lib/python3.5/dist-packages/neuralgym/data/feeding_queue_runner.py"", line 194, in _run  data = func()  File ""/usr/local/lib/python3.5/dist-packages/neuralgym/data/data_from_fnames.py"", line 143, in    feed_dict_op=[lambda: self.next_batch()],  File ""/usr/local/lib/python3.5/dist-packages/neuralgym/data/data_from_fnames.py"", line 180, in next_batch  random_h, random_w, align=False) # use last rand  File ""/usr/local/lib/python3.5/dist-packages/neuralgym/ops/image_ops.py"", line 50, in np_random_crop  image = np_scale_to_shape(image, shape, align=align)  File ""/usr/local/lib/python3.5/dist-packages/neuralgym/ops/image_ops.py"", line 23, in np_scale_to_shape  imgh, imgw = image.shape[0:2]  AttributeError: 'NoneType' object has no attribute 'shape'  [32m[2018-03-29 10:02:11 @data_from_fnames.py:153][0m image is None, sleep this thread for 0.1s.     Do you have any idea what might be causing that?     During training, the model is saved in model_logs directory, however, I cannot observe any generated/inpainted images since no images are being saved during training. Is that right?    Sorry for many questions!     Thank you so much for your help!    Regards,  Ecem"
"Thank you for shaing your code!    I am trying to do batch infilling but I find it crashes in the function 'contextual_attention' in the file inpaint_op.py. I run the test.py with batch size set as 5, then I get the error:    ``   ""/home/zzzace2000/Googledrive/Lab/nl_markov_network/exp/vbd_imagenet/generative_inpainting/inpaint_ops.py"", line 308, in contextual_attention      strides=[1,rate,rate,1]) / 4.    File ""/home/zzzace2000/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1187, in conv2d_transpose      filter.get_shape()[3]))  ValueError: input channels does not match filter's input channels, 5120 != 1024  ``    If I set the batch size as 1, the code runs flawlessly.  I can see from the code that it should be able to handle situation when batch size > 1.  Can you let me know where I might get it wrong?    Thank you!"
"When I try to run the inference with test.py using the given command for CelebA 256 HQ images from the examples, I get the following error    File ""/home/tbalaji/.local/lib/python2.7/site-packages/neuralgym/utils/config.py"", line 21      class Loader(yaml.Loader, metaclass=LoaderMeta):                                         ^  SyntaxError: invalid syntax    Please guide me with fixing this.  I am using python 2.7 with TensorFlow 1.3 in Virtual env. Thanks"
Is it trained against all the categories or a subset of categories?
"Just a minor thing. After downloading the pre-trained models, the ""checkpoint.txt"" file must be changed to ""checkpoint"" before it works as described in the README.    Great work btw.!"
!   !   !   !   
!   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   
!   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   
!   !   !   !   !   !   !   !   !   !   
"When calculating IIC, let's say there are n samples in the batch and the corresponding pairs.   Is it correct that the sample of the ""same class"" is in one batch ?  Otherwise, what does it mean to divide by n when calculating CxC matrix P ?       "
"RuntimeError: unsupported operation: more than one element of the written-to tensor refers to a single memory location. Please clone() the tensor before performing the operation.    Can you tell me, what's causing this issue"
"First of all, thank you very much for your contribution. I have some doubts when I read the paper. The training data set used in the Auxiliary overclustering section of Section 3.2 includes two parts, one known to contain only relevant classes and the other known to contain irrelevant or distractor classes. Isn't there no label in unsupervised learning? Why do I know this information? Maybe I didn't understand it thoroughly. I hope you can give me some suggestions, thank you very much!!!"
"Hello, I computed IID_loss, but the result is negative （approximate to zero). Is it right?"
Why need to shrink and resize to 128x128 for coco-stuff input image in segmentation task? Can I get a segmentation map having the same size with the original input image without resizing?
"Hi,     Thank you for this amazing work. Just to let you know that the suggested fork still has some python 2 code in the clustering part.     "
"Hi, can you please tell me where I can find the original code for rendering CoCo Segmentation results? I only find the Potsdam-verion in code/scripts/segmentation/analysis/render_potsdam.py . Thank you!"
"Thanks for your great work firstly. I use your model in nlp tasks, but after training some batches, the loss becomes 0 and doesn't change anymore. Can you provide some ideas for this problem？Thank you in advance."
"Hi, thanks for your great work. I had an issue that when I used my own data set to conduct fully unsupervised clustering. The outputted accuracy showed a random choice. For example, I have 3 classes and the numbers of these data are balanced. But the accuracies in 5 sub_heads were all around 0.333. Did I set the parameters wrong? Could you help me with this? Thanks a lot!"
"It'll be a great help if the conda environment.yml file (or any equivalent format) could be provided.    package_versions.txt file is not of any help.     Thank you, "
"I am trying to reproduce your segmentation research recently. However, I have some problems.  Firstly, I noticed that in the paper, you mentioned lamb was set as 1.5 for COCO3 and PostDam3 and 1.0 for other experiments. However, when I checked the setting for COCO3, I notice the lamd_A = 1.0 and lamd_B = 1.5, and I am a little confused.  In your shared model, I also noticed that take 555 for example, the num_epoches = 4800, while the loss data seems to have 109 rows in total. Does this mean you record it every 4800/109 epochs?  Another question is that shouldn't the minimum of loss _no_lamd be -1 since it is -1*mutual_information? Why the loss drops to a little higher than -1.5? Is the ideal minimum loss affected by the number of clusters?"
"In the case of STL10, it says that using overclustering with an extra 100000 images helps improve clustering of those initial 5000 images significantly.    What would have been the trade off if you had ran the algorithm with all 105000 images and showed each head the same images.  Is there a better clustering achieved when you use auxillary head performing overclustering.  If I have 200,000 images for fully unsupervised clustering, would it be better to show both head A and head B all 200,000 images?  Or would it better to shown one head 20,000 and the other head 200,000 (with more clusters)?    Furthermore, if I show both heads the same images, I might as well use one head right?    Thank you. "
"Hi Xu-Ji,    Currently, I have modified the Segmentation Script to train another dataset. Both the dataset loader B and A are the same but I noticed the avg loss returned (between head B and A) have large margin of difference:  !     Any idea why this happened? Am I training it correctly    Thank you and regards"
"Hi, when I tried to run ""cluster part“,  the command I used is :  `python -m code.scripts.cluster.cluster_greyscale_twohead --model_ind 685 --arch ClusterNet6cTwoHead --mode IID --dataset MNIST --dataset_root /scratch/local/ssd/xuji/MNIST --gt_k 10 --output_k_A 50 --output_k_B 10  --lamb_A 1.0 --lamb_B 1.0 --lr 0.0001 --num_epochs 3200 --batch_sz 700 --num_dataloaders 5 --num_sub_heads 5 --crop_orig --crop_other --tf1_crop centre_half --tf2_crop random --tf1_crop_sz 20  --tf2_crop_szs 16 20 24 --input_sz 24 --rot_val 25 --no_flip --head_B_epochs 2 > out/sh10_gpu3_m685.out`,  However, the result suggests that there is a KeyError, can you help me with this problem, thanks a lot!  "
"When I ran the command:python -m code.scripts.cluster.cluster_greyscale --dataset MNIST --dataset_root \Users\myhome\PycharmProjects\IIC\datasets --model_ind 665 --arch ClusterNet6c --num_epochs  3200 --output_k 25 --gt_k 10 --lr 0.0001 --lamb 1.0 --num_sub_heads 5 --batch_sz 350 --num_dataloaders 5 --crop_orig --crop_other --tf1_crop centre_half --tf2_crop random --tf1_crop_sz 20  --tf2_crop_szs 16 20 24 --input_sz 24 --r  ot_val 25 --no_flip --mode IID+ --batchnorm_track  I got the following error:ValueError: batch_size should be a positive integer value, but got batch_size=70.0.  I'm using python 3.7.  "
The loss is all minus zeros during training. What's the matter?
"When I run the file on my own dataset. I get an error ""xrange module not found"". When I googled this error, it said that I needed to run the program in python2 but when I do that I'm unable to import everything needed to run the program. What version of Python should I run this program on? Thanks for your help"
"Hey. I'm trying to use the unsupervised segmentation on my own images. After reading the old thread, I learned I needed to create dataloaders, dataloaders_head_A, mapping_assignment_dataloader, and mapping_test_dataloader. I understand that my images should go into dataloaders_head_A, but what do I do for the other two datasets? Also, to run the unsupervised segmentation on my own images, do I just need to return these three dataloaders? Do I have to send where my folder when using -dataset. Thanks for the help."
"I read through the segmentation data loader class ""cocostuff.py"" very carefully, however, I can not find any code to prevent loading fully masked images (which are sub-region of the whole image where all the pixel doesn't belong to preset classes), is there anything that I missed?"
"I read through the segmentation data loader class ""cocostuff.py"" very carefully, however, I can not find any code to prevent loading fully masked images (which are sub-region of the whole image where all the pixel doesn't belong to preset classes), is there anything that I missed?"
"Hi,    Thank you for your great work!     I have the same problem as @nhuvan, with the error:    `TypeError: batch must contain tensors, numbers, dicts or lists; found    `    This happens when I substitute line 155 in ""IIC/code/scripts/cluster/cluster_greyscale.py "" from ""`cluster_create_dataloaders`"" to ""`create_basic_clustering_dataloaders`"". The error is in line 222 ""`for tup in itertools.izip(*iterators): `"" also in the file ""IIC/code/scripts/cluster/cluster_greyscale.py"".    Thanks a lot for any possible solutions!    Best  "
"Do you have any advice on how to find the best transformation of a given dataset if there are no labels?    Right now, I am training with a subset of data of only about 300 images and visualizing the clusters.  I am training this until I get a reasonable loss value, then visualizing the groupings.  Then, I'm tweaking a value such as changing rotation from 30 to 60 and visualizing it again and seeing what's changed.  Is there a better way to find best transformations that will work best for a dataset?  Thank you."
"Dear @xu-ji ,  Thank you for your brilliant work. I have trained the IIC network, and have some questions about reproducing the final results:  1) Are the parameter settings of CIFAR-10-clustering shown in ""IIC/examples/commands.txt"" same as the settings in paper? Or are there always exists some fluctuations in final results? It seems like I can't reproduce the best acc 61.7 on CIFAR-10 dataset. I only got 59.01% best acc.  2) Is there any possibility you update a python3 version code? Or have you test the performance of code in this repo  ?    Sorry to bother. Looking forward to your reply."
"Dear @xu-ji   Thank you for your great work. I have read your papers and trained your network, and I need to evaluate the trained network to regenerate papers tables.   1 - If I do not misunderstand, I think that during training, the code train network on trainset (for example cocostuff-3-train2017 ) and compute and print accuracy on the validation set (cocostuff-3-val2017) each epoch. Is it right?  2 - you have reported IoU on these datasets on your paper, and it is different with accuracy. However, I found IoU evaluation on a trained model on the ""render_general.py"" script. Would I utilize this script to regenerate your paper's results or I should follow another approach?"
"Hi @xu-ji ,  Thank you so much for your work and sharings.  I have tried to train the clustering model on a custom dataset using your function  ""create_basic_clustering_dataloaders"" but I can not get out of this error:    `TypeError: batch must contain tensors, numbers, dicts or lists; found  `    Do you have any idea on this ?  "
"Hi! All training scripts produce two checkpoints: ""latest.pytorch"" and ""best.pytorch"". Could you clarify, please, which of them is used to report figures in the paper?  "
" After performing unsupervised clustering on all of my image data samples, I would like to use the categorical class labels from ImageFolder to see how well they clustered.  I think accuracy can be obtained from cluster_eval where mapping_test_dataloader == mapping_train_dataloader.    Do I just set the mapping dataloaders equal to all the data to get the accuracy statistics in stats_dict, and then optionally add in other metrics Im interested in like F1 score, recall, cluster homogeneity, etc?  Thank you!"
"Hi, could you tell me how I can just run inference given a folder containing raw images to create segmentation maps? Thanks in advance"
"Hi,    I had a question regarding trying to run inference using pretrained weights for fully unsupervised clustering on the STL10 data.  Since STL10 is RGB and I have a greyscale dataset, I tried doing a grayscale to RGB transformation in a simpler dataloader I created.       However, when I try to run this code, I get the following error.     Unexpected key(s) in state_dict: ""head_A.heads.1.0.weight"", ""head_A.heads.1.0.bias"", ""head_A.heads.2.0.weight"", ""head_A.heads.2.0.bias"", ""head_A.heads.3.0.weight"", ""head_A.heads.3.0.bias"", ""head_A.heads.4.0.weight"", ""head_A.heads.4.0.bias"", ""head_B.heads.1.0.weight"", ""head_B.heads.1.0.bias"", ""head_B.heads.2.0.weight"", ""head_B.heads.2.0.bias"", ""head_B.heads.3.0.weight"", ""head_B.heads.3.0.bias"", ""head_B.heads.4.0.weight"", ""head_B.heads.4.0.bias"".          size mismatch for trunk.conv1.weight: copying a param with shape torch.Size([64, 2, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 1, 3, 3]).      My question is why is the parameter of the checkpoint (64,2,3,3)  Since it is 3 channel, shouldn't it be (64,3,3,3)?  Also, what are the unexpected keys showing up in the state_dict.  Thank you!"
"Why is the loss function, IID_loss(), negative ?"
"I am trying to run fully unsupervised cluster without any labels on greyscale image data.    In the create_basic_clustering_dataloaders function, I see that there is only one dataloader per head.  Each dataloader consists of a dataloader for the original image x and other dataloader(s) for the transformed image.   I am wondering how the network knows which image corresponds to which transformed image.  How will it know between which images to calculate the mutual information?    Thank you."
"Hi  When `RENDER_DATA` is set to `True` in *potsdam.py*, an `AssertionError` arising from the data having a wrong shape is returned:       Loading a potsdam example's ground truth mat file this way:     returns an array, `label`, with shape `(200, 200)`, which has length 2.       "
"There are a few issues on running with a  custom dataset:  1) Deterministic Random Sampler needs an additional argument  (data_source)  2) print(""Pre: time %s: \n %s"" % (datetime.now(), nice(config.epoch_stats[-1])))  IndexError: list index out of range  How do I fix these?  "
"Sorry for bothering,    Recently, I change the number of dataloader to train the MNIST. I notice if I reduce the number of dataloader, the accuracy increasing rate seems to be slower than before. I'm curious whether the result is able to achieve oustanding performance with longer time when I use 1 dataloader, other than 5?    Thank you so much for your support."
"Hi,     I want to run your codes with your dataset, and I've installed most of the required packages you've mentioned. But it tells me that 'Dataset not found or corrupted', and I cannot figure out how to deal with it.   <img width=""463"" alt=""image"" src=""   <img width=""906"" alt=""image"" src=""     Is this because the out_root and dataset_root are wrong? How should I change them?    I'm new to ML and really wish you could help me with this question.     Thank you. "
"Hi, thank for your awesome work 👍 . when I reading your paper, i have a little question, How to compute the acc for dataset like postdam3?  the code your write a little abstract for me, I am sorry for this.  As i understood, I should compute the acc for each image to get `acc_i`, and then, the total `acc_total` equal to `acc_i/num`. Am i right? "
"Sorry for bothering again,     thanks so much for providing script to load the custom dataset, and I have one question that if i use the imageFolder to load the image from various classes, the sequence seems really regular and organized.    Do I need to change the sequence and make them distribute ramdomly ilke MNIST/CIFAR, or I can just take the data into your script?    Thank you so much for your help."
"Sorry for bothering again,     may I roughly know how to understand 'best_train_sub_head_match'  ex. the number is like: [(0, 0), (1, 19), (2, 14), (3, 8), (4, 18), (5, 9), (6, 17), (7, 11), (8, 16), (9, 10), (10, 4), (11, 12), (12, 5), (13, 3), (14, 7), (15, 2), (16, 13), (17, 6), (18, 1), (19, 15)]    Thank you so much for your support. "
"Hi @xu-ji, I am new to this field so please bear with me. I am trying to perform unsupervised IIC on the MNIST 2d point cloud dataset. I have made the necessary changes that you have mentioned in your previous posts for the custom dataset. As transformations, I am only using rotation and translation for now.    I am extracting the dataset from a .csv file which consists of a label, x-coordinate, and y-coordinate. The dimension of the training set is (60000, 703) and that of the test set is (10000, 703). You have hardcoded the network according to the input_sz. could you please specify the changes to be made to make it work for my dataset. "
"Hi,   I have been trying to break the algorithm and found confusing on loss calculation(IIC), my doubt is, lets assume my batch size is 16 and cluster heads are 8 , so my final layer prediction shape is (16,8), from here my question should i pass the matrix for computation with same(including batch) or perform loss operation in individual pairs. your guidance would be helpful !    Thanks,"
I want to run your code on my custom datasets and I'd like to use your pretrained weights of semantic image segmentation. Thanks in advance.
What happen if we don't use 'Coco - curated datasets'?
"Sorry for bothering again,     I notice there are 5 results in each training epoch, whose format is just like  train_accs: [0.9918857, 0.97852856, 0.9005857, 0.97854286, 0.97852856]     May I roughly know what's the difference of those results? I experimented to change the tf2, and some of the results would increase, so are they assoicated with the transformation format?    Thank you so much for your help."
"Sorry for bothering again,     I'm trying to create my own dataset for the training, and may I roughly know the detail of the training partitions (how many partitions, and is that used for cross-value validation?)     Thank you so much, sorry again for coutinuous questions."
"I am trying to use my own dataset. However, the dataset file is large (~7 GB) and I am running the scripts on Google Colab, so my memory is somewhat limited. When running, the script is killed and the last line in the .out file is:  `Creating auxiliary dataloader ind 0 out of 5 time 2020-07-01 14:04:59.398052`  and I suspect it is due to excessive use of memory. Is there a way I could segment out my dataset so that it is loaded in smaller pieces on demand? Or would the recommendation be to simply reduce the size of my dataset? Currently, it is stored as a large pytorch tensor. Would it be best practice to instead save each item as its own image/tensor to load individually?"
"Sorry for asking,    I'm really wondering what is in the ""mapping/assigment dataloader"" in the evaluation phase, and is the same content in the ""mapping dataloader"" and ""assignment dataloader""?    Thanks for your support."
"Hi, thanks a lot for your great work! I find it really interesting!     However, I have a question regarding the loss function. To understand what kind of probability distribution between original `(z)` and augmented image `(z')` can maximise the MI, I calculated the heatmap using your provided function `IID_loss` (I assume there are only two clusters).     I understand that the second term `H(z|z')` encourage one-hot prediction, and we want `(z)`  and `(z')`  have a similar probability distribution and fall into the same cluster, but then shouldn't the MI being maximised on the top-left and bottom right corner instead of top-right and bottom-left corners?      "
None
"Hi, I am a historian who would like to employee the clustering function for clustering different types of signatures like these (around 500 images in total, some of them are made by the same person):  !   !   !   I think the most likely dataset for my task is MNIST, so I would like to know the format of the data in the data loader function for MNIST. Say, should I save all of the images as csv matrix or just png files after I do all the resizing, rgb2gray and whitening task?  Also, as there are no labels for the data and we are not sure how many clusters are rational for this task, may I ask how to drop off the evaluation flag and output the relative distance between samples?  I have read your answer in   and am still puzzled at this stage. Thanks a lot."
"Reading through the code, it seems that one needs to provide the number of output classes, for example:  `--output_k_B = 10`  Would it be possible to do this without that? I'm attempting to cluster similar images together in an undefined number of possible classes to identify outliers.  "
"Hello, Xu.     Very interesting work here! But I have one issue with the results in the Table 1.  In table 1, how do you evaluate the ""Results obtained using our experiments with authors’ original code""?  Or more specifically, how do you do the many-to-one mapping using DeepCluster code?     Thank you very much.  Sen"
In figure 2 of the paper it's said that  > Dashed line denotes shared parameters    But reading the code I can't find where de parameters of a fully connected layer are shared. ClusterNet6cTrunk doesn't have any linear layer and ClusterNet6cHead's linear layers don't share parameters between them. The same thing happens with ClusterNet5g. Am I missing something here?
"Hi!    I am trying to reproduce the results on POTSDAM using the example command you provided:         I keep running into variations on the following output:  `only got 0.44026533 4th, restarting`    1.) I've tried multiple times to see if I get lucky enough to pass this catch, but every time the accuracy is too low. I understand that the initialization probably matters a lot, so is it better to restart than to continue training? Or should I comment out this check and just train for longer? How many times should I expect to have to rerun the script to be able to reproduce the results?    2.) Did you by any chance experiment with doing segmentation with more classes? The results in the paper seem to be for relatively few ground truth classes, and I was wondering if there is any hope in using this method for a dataset with 16+ ground truth classes.    Thanks for your time, and this is really superb work! "
"Hi,    I have been playing around with the code for a while and I discovered that when I try to load the config files of the pretrained models, some of them fail with the error:    `AttributeError: 'Namespace' object has no attribute 'batchnorm_track'`    It seems that the pretrained models are saved without `batchnorm_track` attribute, but the   expect it to be there.    The models that give this error are 482, 487, 496, 521 (there may be more). Out of the models I have tried, only model 544 successfully loads.    Is there any workaround that issue?    Thanks!"
"Hi, Thank you for great results as your research.  I have tried to use these code, but I don't understand how to use those because of no description on README.  Could you show me how to use the code as samples, and help me which part I should change for own data?    I found the train command in examples.  e.g.    > export CUDA_VISIBLE_DEVICES=2 && nohup python -m code.scripts.cluster.cluster_sobel_twohead --model_ind 640  --arch ClusterNet5gTwoHead --mode IID --dataset CIFAR10 --dataset_root /scratch/local/ssd/xuji/CIFAR --gt_k 10 --output_k_A 70 --output_k_B 10 --lamb 1.0 --lr 0.0001  --num_epochs 2000 --batch_sz 660 --num_dataloaders 3 --num_sub_heads 5 --crop_orig --rand_crop_sz 20 --input_sz 32 --head_A_first --head_B_epochs 2 > out/gnoded1_gpu2_m640_r1.out &    When I run this code, Ive got some results in ./640 directly. However I did not understand that result meanings. And Im not sure the detail of these args, and the procedure.  Please help me for that."
For an unlabelled dataset how should one use the loss function? It seem to rely on all_mask_img1 which is not available for my dataset.
"In the segmentation transforms custom_greyscale_numpy() uses RGB2GRAY, but opencv opens images by default with BGR. Did I miss a conversion from BGR2RGB somewhere?     "
"Hi Xu,     Thanks so much for your great work!     I am wondering if you could provide more information about the VAE model you obtain 83.2 on MNIST in your Table 1?     I am trying to reproduce the number but it seems a naive implementation (i.e., a 784-200-200 enc and 200-200-784 dec, with k-means performed on the 200D mean of the Normal distribution for q(z|x) ) only gets 59% on MNIST. "
"Hello,    I noticed that in the script, while using restart_from_best, there is a check for whether the config.last_epoch correspond to the best epoch.     However I did not see anywhere in the code that update config.last_epoch when is_best? Such update only happen when the model saving is done via save frequency.    Many thanks."
"Hi, thanks to your amazing work!  I'm curious about why semi-supervised overclustering (IIC+) could have around 30 percentage higher accuracy than unsupervised learning (IIC) on STL10. Combined with your code and the paper, my personal understanding is:  At training stage, IIC has two heads, A (let's call it overhead whose k is larger than kgt) and B. A and B are trained with 113k and 13k data alternatively. While IIC+ has only one overhead and is trained by 105k data.   At evaluation stage, IIC directly uses 13k labeled data to get a match for head B and simutaniously calculate the accuracy. But IIC+ uses 5k labeled data to get a match which is further used to calculate the accuracy on 8k data. (So the supervised part for ""semi-"" dose not modify the trained parameters but find a more accurate match)  From the training stage, the IIC+ seems to be simpler than IIC in both training procedure and data. So the 30 percentage improvement is mainly derived from the evaluation stage where IIC+ has a ""better"" match than IIC. Is that right?   Thank you."
"Just read your paper, amazing work ! I am just about to finish my keras implementation, but have a small understanding problem. When, and in which order do i train the different heads ?    Am i going to:    1) freeze the weigths of all subheads of aux and main , and than just backprop over one subhead for a whole epoch ?    or     2) calculate the loss over all main heads for one epoch, sum the loss of them up and backprop than, next epoch do the same for aux ?    or sth completely different?    Sorry i tried to reverse-engineer your pytorch code but i am not into pytorch, and the forks also didnt help me. thanks a lot for your time!"
"Traceback (most recent call last):    File ""/local_home/guang_local/anaconda3/envs/py2/lib/python2.7/runpy.py"", line 163, in _run_module_as_main      mod_name, _Error)    File ""/local_home/guang_local/anaconda3/envs/py2/lib/python2.7/runpy.py"", line 102, in _get_module_details      loader = get_loader(mod_name)    File ""/local_home/guang_local/anaconda3/envs/py2/lib/python2.7/pkgutil.py"", line 462, in get_loader      return find_loader(fullname)    File ""/local_home/guang_local/anaconda3/envs/py2/lib/python2.7/pkgutil.py"", line 472, in find_loader      for importer in iter_importers(fullname):    File ""/local_home/guang_local/anaconda3/envs/py2/lib/python2.7/pkgutil.py"", line 428, in iter_importers      __import__(pkg)    File ""code/scripts/cluster/cluster_greyscale_twohead.py"", line 496, in        train()    File ""code/scripts/cluster/cluster_greyscale_twohead.py"", line 313, in train      for tup in itertools.izip(*iterators):    File ""/local_home/guang_local/anaconda3/envs/py2/lib/python2.7/site-packages/torch/utils/data/dataloader.py"", line 345, in __next__      data = self._next_data()    File ""/local_home/guang_local/anaconda3/envs/py2/lib/python2.7/site-packages/torch/utils/data/dataloader.py"", line 385, in _next_data      data = self._dataset_fetcher.fetch(index)  # may raise StopIteration    File ""/local_home/guang_local/anaconda3/envs/py2/lib/python2.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch      data = [self.dataset[idx] for idx in possibly_batched_index]    File ""/local_home/guang_local/anaconda3/envs/py2/lib/python2.7/site-packages/torch/utils/data/dataset.py"", line 207, in __getitem__      return self.datasets[dataset_idx][sample_idx]    File ""/local_home/guang_local/anaconda3/envs/py2/lib/python2.7/site-packages/torchvision/datasets/mnist.py"", line 97, in __getitem__      img = self.transform(img)    File ""/local_home/guang_local/anaconda3/envs/py2/lib/python2.7/site-packages/torchvision/transforms/transforms.py"", line 70, in __call__      img = t(img)    File ""/local_home/guang_local/anaconda3/envs/py2/lib/python2.7/site-packages/torchvision/transforms/transforms.py"", line 367, in __call__      img = t(img)    File ""/local_home/guang_local/anaconda3/envs/py2/lib/python2.7/site-packages/torchvision/transforms/transforms.py"", line 1003, in __call__      return F.rotate(img, angle, self.resample, self.expand, self.center, self.fill)    File ""/local_home/guang_local/anaconda3/envs/py2/lib/python2.7/site-packages/torchvision/transforms/functional.py"", line 729, in rotate      return img.rotate(angle, resample, expand, center, fillcolor=fill)    File ""/local_home/guang_local/anaconda3/envs/py2/lib/python2.7/site-packages/PIL/Image.py"", line 1871, in rotate      return self.transform((w, h), AFFINE, matrix, resample, fillcolor=fillcolor)    File ""/local_home/guang_local/anaconda3/envs/py2/lib/python2.7/site-packages/PIL/Image.py"", line 2161, in transform      im = new(self.mode, size, fillcolor)    File ""/local_home/guang_local/anaconda3/envs/py2/lib/python2.7/site-packages/PIL/Image.py"", line 2331, in new      return Image()._new(core.fill(mode, size, color))  TypeError: function takes exactly 1 argument (3 given)  "
"Hello,    I have a small dataset of around 2000 images, 10 classes(approximately 200 per class). I am trying to use the settings for CIFAR10 to train IIC on this dataset, but after 50+ epochs it is still stucking at around 16% accuracy, which is pretty much the same as after the first epoch. I'm wondering if it is because of dataset size?    Many thanks."
Are there any pretrained models for the baselines available?  Respectively can you provide the exact settings that were used to reproduce the results presented in the paper?
"As you show the fig.2 in your paper, auxiliary overclustering can used to improve cluster quality.  I read your code, and i notice that you produce 2 datasets called **""dataloaders_head_A""** and **""dataloaders_head_B""**.   I think you alternately use these two datasets for model training and share parameters among them.  But the model need have a fixed ""num_classes"", which means parameters of the last few layers of the model cannot be shared.   Specifically, I want to use the ""auxiliary overclustering"" to my image clustering for a specific task.   But i think just creating a dataloader will be ok.  If I just modify the num_classes of the model alternately during training on the same dataloader, can I achieve this ""auxiliary overclustering""?    Namely, can you explain brief steps of the ""auxiliary overclustering"" ?"
"Great work. This has a lot of applicability.    I am attempting to cluster (fully unsupervised) some binary data I have, but I want to cluster it into 10 groups (so setting gt_k and output_k_B to 10 without caring about the output accuracies).    I wanted to extract the assigned cluster label so that I could then review the images relative to their semantic group. I noticed you did something similar in your paper - is there some code in the repo that I didnt notice which can output these labels?    Also - is it possible to generate segmented images with your code without providing Ground Truths? I initially believed this was the case but noticed you report GT images - was this just to compare to human performance?    Again thanks for your time. I really look forward to experimenting with your code!"
"I am loading the MNIST 685 model. According to `.cluster_eval()` I have around 99% accuracy. Evaluating it manually with the coded listed below returns 85% accuracy. Am I doing something wrong?    1) Load model:     2) Load MNIST:       3. Set the mappings from semantic clusters to actual classes as the `.cluster_eval()` recommends:  `mappings = {0: 9, 1: 3, 2: 1, 3: 4, 4: 7, 5: 8, 6: 5, 7: 6, 8: 0, 9: 2}`    4. Manually test for accuracy:   "
"Dear IIC group,    Thank you for sharing with us this wonderful project. Just have a simple question.     The model in this link '  is for mnist or STL dataset? I want to try it on STL dataset. And do you have a example code for inference?    Thank you very much for your help.    Best Wishes,    Alex"
"How do you recommend loading a pre-trained model? In particular, I am working with model-ind 685 (MNIST - unsupervised)."
"Hi Xu,    A well-written paper! Thanks for the code as well.    I am trying to perform timing analysis by loading a pretrianed segmentation model. I have the following question W.R.T  .    In the function `segmentation_create_dataloaders(config)`, the train and test partitions use all the data (train/test/validation) for `mode == 'IID'`. **IID** seems to be the required mode by the code. Does this mean entire data was used for training?    Thanks,  Kantha Girish"
"Hi,   Could you please tell me what is the different between `IID_segmentation_loss` and `IID_segmentation_loss_uncollapsed`?  I saw the difference as the normalization towards `padding`."
None
"I just had a question about the case where one uses NO transformation g for image clustering (i.e. x = x'). I am playing around with the mutual information loss and it appears to be learning something when trained without transformations. For starters, it learns to predict a one-hot distribution to minimize H(z|z'), but why then is it able to get larger than random chance on the validation set? (10 classes, 18% accurate after a few epochs)    Could anyone help me understand why the network learns something when we maximize the mutual information between an image and the exact same image? Or is it more likely that there is a bug in my code? Thanks! and great work :) "
"Hello Xu, thanks for the great work! I'm adopting your approach in my unsupervised segmentation task and encountered several problems about the scripts.    1) About the 'local spatial invariance' technique in Section 3.3, where's the code implementing it? In dataloader of coco-stuff-3 and potsdam, the paired patches only contains photometric transformation (one raw img, and the other is color jitter version) instead of pairs with shifts. See  .    2) I'm implementing unsupervised image segmentation for RGB images, which should be the same as  . After checking the code, I found the ground truth   is loaded in dataloader  and  . And in the dataloader for Potsdam, the corresponsive label/mask is  , which I think makes sense. So which should be the right choice for mask_img1 in unsupervsed segmentation task?    3) I noticed that the function pad_and_or_crop is used both for training and testing part in dataloader. Does cropping images into size (h,w) performs better than directly resizing them into (h,w)? And if you crop them when training, to obtain the whole segmentation map for testing images, we need to generate masks using sliding window with size (h,w) instead of just   to (h,w), right?    Please correct me if there's any misunderstanding. Thanks for your time!"
"hi~  I am training the segmentation code with Potsdam dataset. When the model starts to iterate the GPU memory increasing rapidly and will always out of memory,  !   I'm using 8 1080TI and my bach size is 2.  My training scrip is   python -u -m code.scripts.segmentation.segmentation_twohead --mode IID --dataset Potsdam --dataset_root /mnt/lustre/lichuchen/lily/unsupervise/dataset/POTSDAM --model_ind 544 --arch SegmentationNet10aTwoHead --num_epochs 4800 --lr 0.000001 --lamb_A 1.0 --lamb_B 1.0 --num_sub_heads 1 --batch_sz 2 --num_dataloaders 1 --output_k_A 36 --output_k_B 6 --gt_k 6 --input_sz 200 --half_T_side_sparse_min 0 --half_T_side_sparse_max 0 --half_T_side_dense 5  --include_rgb --no_sobel --jitter_brightness 0.1 \   --jitter_contrast 0.1 --jitter_saturation 0.1 --jitter_hue 0.1 --use_uncollapsed_loss --batchnorm_track    Could you tell me the memory usage of your segmentation code?  Thanks~"
"Thank you for releasing your codes! I tried to run your codes on CIFAR10 recently. According to the provided command, the totally epochs you set is 2000. I am using 4 RTX 2080ti GPUs to run your codes. It need to take about 10 minutes for each epoch. Therefore, the totally time cost for training CIFAR10 with 4 GPUs will be about 14 days, which is too long. I am not whether I did something wrong, or it indeed need 14 days? Thank you!"
"Hi xuji,  In your paper 3.3 segmentation, in order to build the spatial relationships between patches, you did a convolution within every gt-k layer of prediction. Maybe this will lead to detail loss. The main class in local will dominate the region, is that right? do you have any idea about that?"
"in the file (code/datasets/segmentation/cocostuff.py), what operations is done with datsets.    between the following datasets, what's the difference? or where should i refer to   Coco10kFull  Coco10kFew  Coco164kFull  Coco164kFew  Coco164kCuratedFew  Coco164kCuratedFull  "
"hello  thank you for that great work.  i have 38 K images  size 128*128 , and i want to classify them into   7 groups using  your model.  unfortunately i cannot know how i can run your model for this problem , i can say am try all way for that and everything not work.  now i have your code with all package  installed in my machine.  so can you tell me how what i can do ."
"Thank you for sharing the source code for this great work!    I am trying to replicate your unsupervised image clustering results on the MNIST dataset. In the ArXiv paper, the avg. and lowest loss sub-head accuracies are reported to be 98.4% and 99.2% respectively. And in one of your answers to an issue, you stated that the loss function goes down to -2.20, which is also what I obtained after running the training for a few hundred epochs. However, I was never able to reach to a 98% or 99% accuracy. After training from scratch five times, the best accuracy I obtained was around 97.87% on the training and test datasets for all heads.    Do you know what I might have done wrong? Did you get the accuracies you reported on the paper with the same set of random transformations used in the training?     Here is the command I used for training:    `export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.cluster_greyscale_twohead --model_ind 685 --arch ClusterNet6cTwoHead --mode IID --dataset MNIST --dataset_root /root/IIC/dataset/MNIST --out_root /root/IIC/results --gt_k 10 --output_k_A 50 --output_k_B 10  --lamb_A 1.0 --lamb_B 1.0 --lr 0.0001 --num_epochs 3200 --batch_sz 700 --num_dataloaders 5 --num_sub_heads 5 --crop_orig --crop_other --tf1_crop centre_half --tf2_crop random --tf1_crop_sz 20  --tf2_crop_szs 16 20 24 --input_sz 24 --rot_val 25 --no_flip --head_B_epochs 2`      And here is the result I got at the 606th epoch:    > Starting e_i: 606  Model ind 685 epoch 606 head B batch: 0 avg loss -2.212163 avg loss no lamb -2.212163 time 2019-12-13 09:59:16.075421  Model ind 685 epoch 606 head B batch: 100 avg loss -2.227755 avg loss no lamb -2.227755 time 2019-12-13 09:59:43.266961  Model ind 685 epoch 606 head B batch: 200 avg loss -2.232833 avg loss no lamb -2.232833 time 2019-12-13 10:00:10.573869  Model ind 685 epoch 606 head B batch: 300 avg loss -2.246190 avg loss no lamb -2.246190 time 2019-12-13 10:00:37.024877  Model ind 685 epoch 606 head B batch: 400 avg loss -2.200957 avg loss no lamb -2.200957 time 2019-12-13 10:01:02.510503  Model ind 685 epoch 606 head B batch: 0 avg loss -2.246450 avg loss no lamb -2.246450 time 2019-12-13 10:01:30.403862  Model ind 685 epoch 606 head B batch: 100 avg loss -2.209442 avg loss no lamb -2.209442 time 2019-12-13 10:01:57.819133  Model ind 685 epoch 606 head B batch: 200 avg loss -2.216221 avg loss no lamb -2.216221 time 2019-12-13 10:02:25.090045  Model ind 685 epoch 606 head B batch: 300 avg loss -2.232495 avg loss no lamb -2.232495 time 2019-12-13 10:02:52.324494  Model ind 685 epoch 606 head B batch: 400 avg loss -2.208003 avg loss no lamb -2.208003 time 2019-12-13 10:03:20.211105  Model ind 685 epoch 606 head A batch: 0 avg loss -2.237204 avg loss no lamb -2.237204 time 2019-12-13 10:03:47.252569  Model ind 685 epoch 606 head A batch: 100 avg loss -2.212928 avg loss no lamb -2.212928 time 2019-12-13 10:04:14.799001  Model ind 685 epoch 606 head A batch: 200 avg loss -2.218388 avg loss no lamb -2.218388 time 2019-12-13 10:04:42.189923  Model ind 685 epoch 606 head A batch: 300 avg loss -2.255437 avg loss no lamb -2.255437 time 2019-12-13 10:05:09.242370  Model ind 685 epoch 606 head A batch: 400 avg loss -2.210399 avg loss no lamb -2.210399 time 2019-12-13 10:05:36.609692  Pre: time 2019-12-13 10:06:12.301906:          std: 2.8002994e-05          best_train_sub_head_match: [(0, 7), (1, 9), (2, 0), (3, 8), (4, 3), (5, 4), (6, 6), (7, 5), (8, 2), (9, 1)]          test_accs: [0.97877145, 0.97877145, 0.9787143, 0.9787143, 0.9787143]          train_accs: [0.97877145, 0.97877145, 0.9787143, 0.9787143, 0.9787143]          best_train_sub_head: 0          worst: 0.9787143          avg: 0.9787372          best: 0.97877145"
"Hi there,    As mentioned in the paper, some details you mentioned in the paper are referred to supplementary material. However, I cannot find the supplementary material after the paper. Can you provide it?"
"Hi @xu-ji     I managed to use IID for segmentation on Potsdam (3 and 6) but was wondering how you performed objective measures, e.g. pixel accuracy, to compare my results with yours.     Do you map ground-truth masks to cluster predictions taken from a network maximising IID?    Thanks in advance."
"Hello @xu-ji ,    May I ask you why estimating the joint `P(X,Y) =P(X)×P(Y)` holds in your equation?   It holds when `X` and `Y` are independent, It is obvious that `P(X)` and `P(T(X))` are dependent. I may regard it as `biased`.  You maximize the MI based on the average on `z` of `P(X|Z)×P(Y|Z)`, where `z` is an image, does the average of the product helps to reduce this `bias`?"
"Hi, I have read your paper and I'm interested in your approach. In my opinion, the invariant information shares  similar idea with consistency training in semi-supervised learning [1].     In [1], they use KL-divergence to force consistent prediction between data and augmented ones. I think KL-divergence may be also proper in your implementation without degenracy.     Have you tried some experiments on KL-divergence or cross-entropy as objective function? Could you explain the differences among them?      [1] Unsupervised Data Augmentation for Consistency Training, Xie et al."
Hi! I apply the function of  IID_loss in the object classification and the loss is negative.  And the accuracy decrease in the training process. Is there something wrong? Thank you for answering!
None
"Running this command on a gpu colab notebook:  `export CUDA_VISIBLE_DEVICES=0 && python -m code.scripts.cluster.cluster_greyscale_twohead --model_ind 0 --arch ClusterNet6cTwoHead --mode IID --dataset_root /code/utils/cluster/MNIST.py --gt_k 10 --output_k_A 10 --output_k_B 10 --lamb_A 1.0 --lamb_B 1.0 --lr 0.0001 --num_epochs 3200 --batch_sz 10 --num_dataloaders 1 --num_sub_heads 1 --crop_orig --crop_other --tf1_crop centre_half --tf2_crop random --tf1_crop_sz 20 --tf2_crop_szs 16 20 24 --input_sz 24 --rot_val 25 --no_flip --head_B_epochs 2 `    Gets this error:  Traceback (most recent call last):    File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main      ""__main__"", fname, loader, pkg_name)    File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code      exec code in run_globals    File ""/content/code/code/scripts/cluster/cluster_greyscale_twohead.py"", line 495, in        train()    File ""/content/code/code/scripts/cluster/cluster_greyscale_twohead.py"", line 378, in train      avg_loss_batch.backward()    File ""/usr/local/lib/python2.7/dist-packages/torch/tensor.py"", line 150, in backward      torch.autograd.backward(self, gradient, retain_graph, create_graph)    File ""/usr/local/lib/python2.7/dist-packages/torch/autograd/__init__.py"", line 99, in backward      allow_unreachable=True)  # allow_unreachable flag  RuntimeError: unsupported operation: more than one element of the written-to tensor refers to a single memory location. Please clone() the tensor before performing the operation."
I cannot pip install javapackages==1.0.0?  _ERROR: Could not find a version that satisfies the requirement javapackages==1.0.0 (from versions: none)  ERROR: No matching distribution found for javapackages==1.0.0_
"Hi, first thanks for the amazing work and providing the code.    I am testing my model on potsdam-3 for unsupervised segmentation, with only two P100 (so 24 GB), the batch size if quite small, and in your example command you have 4800 epochs, which is a lot (that might take me like more than a month to complete), I was wandering if I am doing something wrong or is the training time is indeed this long? and can you please provide just some metrics, like the number of epochs needed for good performance in your experiments and the minimal loss values we can expect?     Thanks again for this work."
"Hi, first of all, thanks for the amazing work,    Sorry if this is not the place to ask this type of question, feel free to close the issue; Since you've mentioned the sum. mat in the paper quite a few time, I was wandering where can I find the supplementary material of your paper.     Thanks."
Do you think for this dataset     Does 18% acc for unsupervised make sense?    Do you have any idea for setting parameter?
Is it possible to load network in pytorch independently from your code?  how can I test the output network (trained by my data) with my inputs?
"Hi @xu-ji, could you please describe the hardware requirement and the training time for clustering of STL10? Thank you."
"Great work!  I'll refer to your code and paper and reproduce it. However, the results were not satisfactory.  First, define three transformations in __init__ method:          self.transform_tf1= transforms.Compose([transforms.RandomCrop(20),                                      transforms.Resize(32),                                      custom_greyscale_to_tensor(include_rgb=False)                                      ])            self.transform_tf2= transforms.Compose([transforms.RandomCrop(20),                                      transforms.Resize(32),                                      transforms.RandomHorizontalFlip(),                                      transforms.ColorJitter(brightness=0.4, contrast=0.4,saturation=0.4, hue=0.125),                                      custom_greyscale_to_tensor(include_rgb=False)                                      ])            self.transform_tf3=transforms.Compose([transforms.CenterCrop(20),                                      transforms.Resize(32),                                      custom_greyscale_to_tensor(include_rgb=False)                                 ])    Second, apply these transformations in __getitem__ method:            img1, target = self.all_data[index], self.all_labels[index]          img1 = Image.fromarray(img1)          if not self.mapping_test_dataloader_flag:                 #build the head data              img1_temp=[]              img2_temp=[]              img1_=self.transform_tf1(img1)              for i in range(config.num_loaders):                  img1_temp.append(img1_)                  img2_temp.append(self.transform_tf2(img1))              return img1_temp , img2_temp          else:                                                                        #build the mapping data              return self.transform_tf3(img1),target      Third, all settings refer to your code, including optimizer, learning rate, etc. But in training stage, the best acc is low (on epoch 10, the best acc=15.14%), and the accuracy rate of each epoch increases very slowly(about 0.5%).    So did I ignore any important settings? Look forward to your reply."
"Hello, thanks for sharing these very nice results and code. It's up and running for some of my data now. I am kinda new to the field of Deep Learning and I was wondering if this method is suitable for 3D segmentation. Would it require too many modifications?    Thank you,    Giovanni"
"Hi,  first of all, thank you for your great work.  I'm currently trying to set up the segmentation model for my custom data set. The training script is running, but I'm receiving a negative loss and the model is not really converging. (Accuracy still around 0.5 after 100 epochs.) Might that be an indication that something is wrong with my code?  Any help is greatly appreciated.  Thanks!  !   "
"Hello,    When I run the following command, I get the error ""ModuleNotFoundError: No module named 'cluster'"".    export CUDA_VISIBLE_DEVICES=0,1 && nohup python -m code.scripts.segmentation.segmentation --mode IID+ --dataset Potsdam --dataset_root /scratch/local/ssd/xuji/POTSDAM --model_ind 487 --arch SegmentationNet10a --num_epochs 4800 --lr 0.00001 --lamb 1.0 --num_sub_heads 1 --batch_sz 60 --num_dataloaders 1 --output_k 24 --gt_k 6 --input_sz 200 --half_T_side_sparse_min 0 --half_T_side_sparse_max 0 --half_T_side_dense 10  --include_rgb --no_sobel --jitter_brightness 0.1 --jitter_contrast 0.1 --jitter_saturation 0.1 --jitter_hue 0.1 --use_uncollapsed_loss --batchnorm_track > gnoded2_gpu01_m487_r1.out &  "
"Have you investigated how the lamb affects the performance of model? Tuning it helps me getting a better result, but I don't see anything about it on the paper?"
"Using the pre-trained model 544, I want to visualize the results of the image segmentation on Potsdam-3 as given in Figure 7 of the IIC research paper. However, downloading the models from the link given in README.MD only gives me the config files for the training. How do I execute the models to see the unsupervised segmentation in action?"
"Hi and thanks for your work,    I was just trying to plug-in the IID loss function for the MNIST example but the loss seems to stabilize to -0.55 just after 1 epoch. If I remove the NaN check, instead of stabilizing the network it starts to output NaNs and fails to learn.    I did consider this in the last version of your paper: ""Mutual information (3) expands to I(z, z') = H(z) - H(z | z')."" And also, the largest value for H(z) is ln(C) and minimum value for H(z|z') is 0. If the network has randomly assigned weights, its fair to say that the first set of predictions from the first set of mini-batches will have equal likelihood for each class and that H(z) will in fact be ln(C). Which checks out! But the conditional cluster assignment entropy remains at -6 throughout the entire training process and maximizing IID doesn't trade-off between individual and conditional assignments.    Here is a link to the repo, its just something simple:        Just looking for a bit of insight as to what I may be doing wrong. Perhaps I should add an additional clustering head but I'm not sure if this will help."
great work
"I've been trying to run the unsupervised MNIST example. I took the command from  , and adjusted `dataset_root` and `out_root` appropriately.    But when I run it the program hangs, and to stop it I have to restart the computer (no response from ctrl c/z). Htop shows it's using one core for only kernel calls, the longest I've let it run is one hour. I tried looking into the code but haven't figured out what is going wrong. Is there an additional parameter one should change before calling the command?    Side question: What are normal loss values when using the IIC loss function?"
I see in your commands that all experiments are done with lr=1e-4 and much more epochs. I wonder why since _deepcluster_ only train on imagenet for 200 epochs with an initial lr=0.05. Does this make much difference on the model performance?
"Hi @xu-ji,  Thanks for this wonderful work. I am rerunning your code here and noticed that in the `commands.txt`, the setting with cifar10 is without --batchnorm_track, while most other commands are with this flag. I can understand that freezing the BN can be used in a finetuning setting but here apparently not the case. Can you tell me why the BN has been freezed for this particular setting for training Cifar10 from scratch?   Thanks for your help in advance."
"Hi Xu, thanks for the great code and really novel paper!  I didn't understand the segmentation implementation too well - hoping you can clarify:    * I want to know how to run your code for a segmentation dataset where I have 4 classes. Would I require 4 heads for this task?   * Why does `SegmentationNet10aTwoHead` have two output heads with different sized channels? (Talking about `output_k_A` and `output_k_B`).   * In your experience, does it take a long time to run the `IID` loss for say COCO-Stuff 512 settings? It takes me roughly 7 seconds to compute the loss **per batch** for a 4-class dataset, `batch=32` and `imgsize= 3x128x128` - I'm not sure if this is normal (My setup is GTX1080 Ti, Cuda 10, pytorch 1.0.1)  **EDIT:** The time for loss computation was solved for me by removing the float   and by clipping the minimum value to EPS by using the following instead of  :   "
"Thanks for your great work! I check the code and find its hard coded for the benchmark given in the paper, whether  could i run on my custom data? thanks a lot!"
"Hi, Thanks for sharing your work.    I had a question regarding the input size used for evaluation for STL10. Looking at the code in IID_semisup_STL10.py (698), the test data uses TenCrop evaluation with input image size =  old_config.input_sz. This is 64x64 from the command used in 650.   Could you please confirm if the numbers reported in Table 3 are all using 64x64 as the input image size with TenCrop evaluation, including the supervised baseline for Cutout networks?"
"Hey, I was reading through the segmentation code and I notice that you utilize the pixel labels in the loss through some sort of mask. Is this really unsupervised, or are you just using that as a prior or filter before computing the loss? Any insights will be appreciated, thanks a lot!"
"Why is it required for these two tensors to be set with requires_grad for clustering yet not for segmentation? More so do not understand why one would set requires_grad to True for input images into a conv net, as I expect they should remain static. Thanks in advance!"
"I'm having quite a bit of trouble running this, from simple stuff between python2.7 and 3.6 to weird behavior from my version of PyTorch not broadcasting types properly."
"When I  run on 4 gpus   CUDA_VISIBLE_DEVICES=0,1,2,3 python -m code.scripts.segmentation.segmentation_twohead --mode IID --dataset Coco164kCuratedFew --dataset_root /vulcan/scratch/shlok/IIC/datasets --model_ind 555 --arch SegmentationNet10aTwoHead --num_epochs 4800 --lr 0.0001 --lamb_A 1.0 --lamb_B 1.5 --num_sub_heads 1 --batch_sz 1 --num_dataloaders 1 --use_coarse_labels --output_k_A 15 --output_k_B 3 --gt_k 3 --pre_scale_all --pre_scale_factor 0.33 --input_sz 128 --half_T_side_sparse_min 0 --half_T_side_sparse_max 0 --half_T_side_dense 10 --include_rgb  --coco_164k_curated_version 6 --use_uncollapsed_loss --batchnorm_track > gnoded1_gpu0123_m555_r1.out    I receive following error.     File ""/vulcan/scratch/shlok/Ana/envs/python2.7pytorch/lib/python2.7/runpy.py"", line 174, in _run_module_as_main      ""__main__"", fname, loader, pkg_name)    File ""/vulcan/scratch/shlok/Ana/envs/python2.7pytorch/lib/python2.7/runpy.py"", line 72, in _run_code      exec code in run_globals    File ""/vulcan/scratch/shlok/IIC/code/scripts/segmentation/segmentation_twohead.py"", line 451, in        train()    File ""/vulcan/scratch/shlok/IIC/code/scripts/segmentation/segmentation_twohead.py"", line 216, in train      using_IR=config.using_IR)    File ""code/utils/segmentation/segmentation_eval.py"", line 25, in segmentation_eval      verbose=verbose)    File ""code/utils/cluster/cluster_eval.py"", line 100, in cluster_subheads_eval      verbose=verbose)    File ""code/utils/cluster/cluster_eval.py"", line 160, in _get_assignment_data_matches      verbose=verbose)    File ""code/utils/segmentation/segmentation_eval.py"", line 127, in _segmentation_get_data      xrange(config.num_sub_heads)]  RuntimeError: CUDA out of memory. Tried to allocate 4.48 GiB (GPU 0; 10.92 GiB total capacity; 6.56 GiB already allocated; 3.77 GiB free; 1.39 MiB cached)    Do you know why this might occur, I have reduced my batch size to 2.   Also, how many gpu did you use?"
"Hi, over the past few days I put together a minimal tensorflow version to study your information based clustering algorithm.          The MNIST example can be run in the `tf_cluster.py` script. It wants a directory called `pointcloud` to draw the outputs to.     The loss functions are tested to give nearly identical results in tensorflow and pytorch (see the readme there). Still, I'm having trouble training very robust clustering as shown in the MNIST example. The differences would be in the conv net architecture, and the sub-head & auxiliary overclustering heads, unfortunately I'm not fluent in pytorch and it's hard for me to parse the reference package. Currently I only have the MNIST example in place, with regular python iteration to feed the data, instead of a tf.data.dataset. This results in some slow training when image augmentations like flips and rotates are turned on.     I'll keep working on this and make updates to my fork linked above.    Best,  "
"Hi, I'm getting import issues. Also, I have changed code to code1 as it was causing some dependency issues.  CUDA_VISIBLE_DEVICES=0,1,2,3 python -m code1.scripts.segmentation.segmentation_twohead --mode IID --dataset Coco164kCuratedFew --dataset_root /scratch/local/ssd/xuji/COCO/CocoStuff164k --model_ind 714 --arch SegmentationNet10aTwoHead --num_epochs 4800 --lr 0.0001 --lamb_A 1.0 --lamb_B 1.0 --num_sub_heads 1 --batch_sz 120 --num_dataloaders 1 --use_coarse_labels --output_k_A 15 --output_k_B 3 --gt_k 3 --pre_scale_all --pre_scale_factor 0.33 --input_sz 128 --half_T_side_sparse_min 0 --half_T_side_sparse_max 0 --half_T_side_dense 10 --include_rgb  --coco_164k_curated_version 6 --use_uncollapsed_loss --batchnorm_track > gnoded2_gpu0123_m714.out  Traceback (most recent call last):    File ""/vulcan/scratch/shlok/Ana/envs/pytorch/lib/python3.5/runpy.py"", line 193, in _run_module_as_main      ""__main__"", mod_spec)    File ""/vulcan/scratch/shlok/Ana/envs/pytorch/lib/python3.5/runpy.py"", line 85, in _run_code      exec(code, run_globals)    File ""/vulcan/scratch/shlok/IIC/code1/scripts/segmentation/segmentation_twohead.py"", line 18, in        import code1.archs as archs    File ""/vulcan/scratch/shlok/IIC/code1/archs/__init__.py"", line 1, in        from cluster import *  ImportError: No module named 'cluster'"
"The network structure in core/models.py is inconsistent with the content in the paper. For example, the activation function PReLU, the number of times the sliding window is used."
"I ran the test_fc_aide_ft.py without any modification, but the following error occurs:  ``   tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  tracking   mask  Traceback (most recent call last):    File ""test_fc_aide_ft.py"", line 21, in        denoised_img, psnr, ssim = t_ft.fine_tuning()    File ""/home/bob/FC-AIDE-Keras/core/test_ft.py"", line 128, in fine_tuning      self.model.load_weights('./weights/' + 'sigma' + str(self.noise_sigma) + '.hdf5')    File ""/home/bob/anaconda3/envs/bobsense/lib/python3.7/site-packages/keras/engine/saving.py"", line 492, in load_wrapper      return load_function(*args, **kwargs)    File ""/home/bob/anaconda3/envs/bobsense/lib/python3.7/site-packages/keras/engine/network.py"", line 1230, in load_weights      f, self.layers, reshape=reshape)    File ""/home/bob/anaconda3/envs/bobsense/lib/python3.7/site-packages/keras/engine/saving.py"", line 1235, in load_weights_from_hdf5_group      ' elements.')  ValueError: Layer #0 (named ""naide__conv2d__q1_1"" in the current model) was found to correspond to layer naide__conv2d__q1_1 in the save file. However the new layer naide__conv2d__q1_1 expects 3 weights, but the saved weights have 2 elements.  ``    Is there some problem with the pretrained model?    thanks!      "
None
"In preprocessing function, the noise image would normalize to -2.5--2.5? I can‘t understand that.       self.noisy_img /= 255  self.X_data = (self.noisy_img - 0.5) / 0.2"
"The paper gives the number of epochs to train for each stage as follows:  5 epochs for global stage  30 epochs for local stage  5 epochs for FaceGAN    For each stage, the code divides the training epochs into two parts `niter` (epochs at starting learning rate) and `niter_decay` (epochs at decaying learning rate). For a given stage, how do we divide the total epochs (say 30 for the local stage) into these two parts. Equally? Some other ratio?"
"I downloaded the dataset from   as mentioned on the project website. I faced some discrepancies in the numbers I read in the paper, and what I found in the dataset:  1] In `Section 3: Dataset Collection` of the appendix, the paper mentions that first 20% of the filmed footage is used for training and the last 80% for testing. However, when I calculated the number of frames of data (for the 5 subjects) in the downloaded datasets, I see the following numbers:  Subject  :   train    test    ratio   Subject 1: 11642  3598  3.23  Subject 2: 10623  2794  3.80  Subject 3: 9948    1848  5.38  Subject 4: 23410  4546  5.15  Subject 5: 25214  4998  5.04  Why the mismatch. Is it a typo in the paper?    2] In the same section, the paper mentions that for every subject, 120fps videos were shot, with the duration of each video being somewhere between 8 to 17 minutes.  However, looking at the number of frames, the total runtime of all frames is only 13 min 41 seconds (assuming 120 fps), whereas it would be expected to be at least around 40+ minutes. Is only part of the filmed footage used in the final dataset?"
"I have PyTorch 1.11.0 with CUDA 10.2 installed, but when attempting to run training, the process dies due to a lack of a Tensorflow dependency. Where should the configuration be changed to ensure PyTorch is used. Perhaps this code does not work with PyTorch 1.11?"
"I want to make a set of detected pose   from  some source dancing videos ,   and cache them .     when somebody give me a photo , i use  one detected pose from  the cache , and drive the photo to dance "
@carolineec Can you highlight where the temporal smoothing loss has been used in the provided code?
"Thank you for your excellent work!   It seems 'everybody dance now' is able to apply to face motion transfer? If so, I just need to replace the skeleton frame to face landmark frame int 'sample_data/train/train_label'?  Looking forward to your reply."
"Hi,  Could you please upload a dataset of synthesized frames or just some videos that you obtained as a result of this algorithm? It would be very important for my project.  Thank you"
"Hi, has anyone trained to run the script graph_posenorm.py, obtaining good results as shown in the paper?  I have tried it, but it doesn't work at all. Could someone suggest to me how to do it?  I'm trying to modify it, but I still don't have good results.    Thank you in advance "
It's really important for dance transfer to my cat.
None
!   
None
so with these dependencies it works?    --python3.6  --torch 1.6.0+cu101  --torchvision 0.7.0+cu101  --tensorflow 1.15.0       !   
"Hi, @carolineec Thank you for your sharing. I noticed that you mentioned that this work includes detecting whether the video is real or forged by EDN. Is it included in your open source code? If there is any, could you indicate its location; if not, could you add it?  Thanks for your time!  Best  IItaly  "
None
"Hi, @carolineec Thanks for sharing your code! But I have some questions about your training dataset AlignedDataset (in the aligned_dataset.py).    It seems that it only allows using one subject for training. In this   of code, one can see that whether we have the next sample depends on the size of the dataset. So I guess that we can only use one subject for one training dataset. Is it correct? If we want to test it on another subject, we need to train the model again?    Thanks for your time!  Best  Haomiao  "
None
Can someone explain the file structure/tree for training?  I'm really trying to get this working...I've tried other repos but they're not as good as this one.  Any help would be appreciated.
"Hi, I am an undergraduate from Peking Univ. currently reimplementing the project, everything's going fine but I really need the method/code to evaluate the result, specifically, the SSIM/IPIPS evaluation metrics. Besides, I'd like to know how you guys crop the face and body region for all test images. Thanks!    Feel free to contact me by email at rickycs97@pku.edu.cn     "
will the pretrained models be released?
None
"I trained a model with dataset that shared in   subject4.  I used same train codes in readme.    In paper, writing that training is 30 epochs for local stage.  So only i changed --niter and --niter_decay as 15.  But results are not good as in videos.    Where i did mistake?  Has anyone trained this dataset and gets successful results?    ! "
None
"I'm training with `python3 train_fullts.py \  --name ./model_global/ \  --dataroot ./dataset/train/  \  --checkpoints_dir ./checkpoints/ \  --loadSize 512 \  --no_instance \  --no_flip \  --tf_log \  --label_nc 6 \  --resize_or_crop scale_width \  --save_latest_freq 100 \  `  and inferencings with `python3 test_fullts.py \  --name model_global \  --dataroot ./dataset/test/ \  --checkpoints_dir ./checkpoints/ \  --results_dir ./result/ \  --loadSize 512 \  --no_instance \  --how_many 10000 \  --label_nc 6 \  --aspect_ratio 2.0 \`, but when inferencing, the result is a square 256x256, no matter how I change the output. How do I output in a non square aspect ratio and at a resolution greater than 256?"
I am getting this error while training subject1 local state and subject2 global stage:    `AttributeError: 'int' object has no attribute 'numel'`
"Hi,     I'm a beginner, I don't really understand the process( in README ) to implement this project, can someone give me advice about how to train my own module ( like where to place my source & target video or should run which file ) and get the result from it?    Thanks!"
"Hi,    Thanks for your nice work!   I have several questions about the following table     1. Does the off-the-shelf here include (ImageNet pretrained) VGG16+VLAD+PCA?   2. What does trained/shelf mean?  3. I am now trying to implement your results by PyTorch, but the results seem similar to the raw NetVLAD. So do you have any suggestions?    Thank you very much!"
"Hi Liuliu,  Thanks for your amazing job.   As stated in the paper, you took pitts30k-train as the training dataset and others for the test.  But in the sample script, the code shows that pitts250k-train is taken as training dataset. Which one  should we follow to reproduce the results? Or the model trained with pitts250k is used for classification?  Another question is that: are the results in table 2 of the paper gotten after implementing pca whitening with dim 4096?  "
"Hello, I am a little confused now, that is, what participates in the loss calculation is the prediction_horizon of the input trajectory and the trajectory predicted by the model, so what does labels mean?  I really appreciate your answer."
"When I run train.py, it occurs that:      Traceback (most recent call last):    File ""/mnt/d/try/Trajectron-master/code/train.py"", line 522, in        main()    File ""/mnt/d/try/Trajectron-master/code/train.py"", line 352, in main      train_loss = stg.train_loss(train_inputs, train_labels, hyperparams['prediction_horizon']) / args.batch_multiplier    File ""/mnt/d/try/Trajectron-master/code/model/dyn_stg.py"", line 76, in train_loss      if np.any(inputs['traj_lengths']  "", line 5, in any    File ""/home/lyl/miniconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py"", line 2330, in any      return _wrapreduction(a, np.logical_or, 'any', axis, None, out, keepdims=keepdims)    File ""/home/lyl/miniconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py"", line 85, in _wrapreduction      return reduction(axis=axis, out=out, **passkwargs)  TypeError: any() received an invalid combination of arguments - got (out=NoneType, axis=NoneType, ), but expected one of:   * ()   * (name dim, bool keepdim)        didn't match because some of the keywords were incorrect: out, axis   * (int dim, bool keepdim)        didn't match because some of the keywords were incorrect: out, axis      Can you tell me how to solve it?Thanks."
"I have read your paper , but the repo does not exist anymore. So, will you share this great work again?    Thanks"
"Hi, author. Thank you very much for your work. I would like to ask you about the quantitative ADE and FDE results. You said in your paper that you used your own implementations of the ADE and FDE metrics and made a comparison in Table 1, but I don't seem to see any script that can be used to evaluate the results in Table 1. Could you share the script that you evaluated in table 1? thank you "
"Hi can you explain a bit about the parameters in       `  desired_max_time = 100  pred_indices = [2, 3]  state_dim = 6  frame_diff = 10  desired_frame_diff = 1   `  what do they mean ? "
"when i run train.py , it lost debug_eval_data.pkl and debug_train_data.pkl . i can't get the files."
"Hi, @BorisIvanovic ,  I was working with the  , and noticed frame_diff, I want to know what if I have different frame differences? I have a dataset where the same pedestrian exits and enters the scene after some time. How should I deal with that?"
"Hi, @BorisIvanovic I read the paper and I have a few doubts regarding it. Can you explain what are the types of edges T1, T2 and T3? I went through STG in structured RNN paper as well but was not able to figure out properly if you have similar kind of edges like spatial edge and temporal edge.     For categories, you consider Humans as the only categories, right? and no other object-human interactions?"
"hi @BorisIvanovic how do you preprocess the datasets? I from the mean you take the x, y, vx, vy and ax, ay (please correct me if I am wrong) but what is standard deviation(does it record the history of the pedestrian and then calculates the std deviation)?     If possible can you share the data preprocessing code as well please?  "
"Hi,    Can you make a code public that you used to convert ETH dataset into given preprocessed data? Also, can you explain what do the last four columns in   represent?"
"Hi, I solved the previous error by changing it to CPU but I still want to use the GPU for the same.   After I change it to the CPU then I get the following output:    `Loading from ../ewap-dataset/logs/models_28_Jan_2019_15_35_20\model_registrar-1999.pt  Loaded!    At t=0, have 24 nodes, 17 edges which uses 112.96 MBs of RAM.  t=1: took 0.39 s (= 2.58 Hz) and 24 nodes, 16 edges uses 116.66 MBs of RAM.  t=2: took 0.34 s (= 2.95 Hz) and 24 nodes, 16 edges uses 116.83 MBs of RAM.  t=3: took 0.31 s (= 3.21 Hz) and 24 nodes, 16 edges uses 116.97 MBs of RAM.  t=4: took 0.32 s (= 3.12 Hz) and 25 nodes, 16 edges uses 117.14 MBs of RAM.  t=5: took 0.43 s (= 2.30 Hz) and 22 nodes, 12 edges uses 130.80 MBs of RAM.  t=6: took 0.26 s (= 3.84 Hz) and 22 nodes, 11 edges uses 137.32 MBs of RAM.  t=7: took 0.52 s (= 1.92 Hz) and 22 nodes, 11 edges uses 146.02 MBs of RAM.  t=8: took 0.24 s (= 4.10 Hz) and 22 nodes, 10 edges uses 142.48 MBs of RAM.  t=9: took 0.25 s (= 3.98 Hz) and 22 nodes, 10 edges uses 148.12 MBs of RAM.  t=10: took 0.25 s (= 3.97 Hz) and 22 nodes, 10 edges uses 142.60 MBs of RAM.  t=11: took 0.23 s (= 4.35 Hz) and 22 nodes, 11 edges uses 148.10 MBs of RAM.  t=12: took 0.23 s (= 4.27 Hz) and 22 nodes, 11 edges uses 142.95 MBs of RAM.  t=13: took 0.23 s (= 4.29 Hz) and 22 nodes, 12 edges uses 144.37 MBs of RAM.  t=14: took 0.23 s (= 4.33 Hz) and 22 nodes, 13 edges uses 148.92 MBs of RAM.  t=15: took 0.22 s (= 4.57 Hz) and 22 nodes, 14 edges uses 146.37 MBs of RAM.  t=16: took 0.25 s (= 4.08 Hz) and 23 nodes, 17 edges uses 150.79 MBs of RAM.  t=17: took 0.25 s (= 4.03 Hz) and 23 nodes, 16 edges uses 144.25 MBs of RAM.  t=18: took 0.24 s (= 4.08 Hz) and 23 nodes, 16 edges uses 144.77 MBs of RAM.  t=19: took 0.26 s (= 3.85 Hz) and 23 nodes, 18 edges uses 147.86 MBs of RAM.  t=20: took 0.23 s (= 4.26 Hz) and 23 nodes, 20 edges uses 144.71 MBs of RAM.  t=21: took 0.25 s (= 4.02 Hz) and 23 nodes, 20 edges uses 149.22 MBs of RAM.  t=22: took 0.25 s (= 4.08 Hz) and 23 nodes, 21 edges uses 144.71 MBs of RAM.  t=23: took 0.24 s (= 4.15 Hz) and 23 nodes, 20 edges uses 145.87 MBs of RAM.  t=24: took 0.25 s (= 4.03 Hz) and 23 nodes, 20 edges uses 149.62 MBs of RAM.  t=25: took 0.24 s (= 4.22 Hz) and 23 nodes, 21 edges uses 145.03 MBs of RAM.  t=26: took 0.26 s (= 3.79 Hz) and 23 nodes, 21 edges uses 144.78 MBs of RAM.  t=27: took 0.24 s (= 4.17 Hz) and 23 nodes, 18 edges uses 147.97 MBs of RAM.  t=28: took 0.26 s (= 3.88 Hz) and 23 nodes, 19 edges uses 144.60 MBs of RAM.  t=29: took 0.24 s (= 4.17 Hz) and 23 nodes, 19 edges uses 148.77 MBs of RAM.  t=30: took 0.24 s (= 4.24 Hz) and 23 nodes, 18 edges uses 145.20 MBs of RAM.  t=31: took 0.25 s (= 4.03 Hz) and 23 nodes, 18 edges uses 146.63 MBs of RAM.  t=32: took 0.23 s (= 4.41 Hz) and 23 nodes, 18 edges uses 149.96 MBs of RAM.  t=33: took 0.24 s (= 4.12 Hz) and 23 nodes, 18 edges uses 145.87 MBs of RAM.  t=34: took 0.23 s (= 4.41 Hz) and 23 nodes, 18 edges uses 145.72 MBs of RAM.  t=35: took 0.24 s (= 4.10 Hz) and 23 nodes, 17 edges uses 148.70 MBs of RAM.  t=36: took 0.25 s (= 4.00 Hz) and 23 nodes, 16 edges uses 145.04 MBs of RAM.  t=37: took 0.24 s (= 4.11 Hz) and 23 nodes, 16 edges uses 145.39 MBs of RAM.  t=38: took 0.40 s (= 2.49 Hz) and 23 nodes, 16 edges uses 148.45 MBs of RAM.  t=39: took 0.23 s (= 4.42 Hz) and 23 nodes, 15 edges uses 145.41 MBs of RAM.  t=40: took 0.26 s (= 3.83 Hz) and 24 nodes, 17 edges uses 148.25 MBs of RAM.  t=41: took 0.24 s (= 4.22 Hz) and 23 nodes, 16 edges uses 146.03 MBs of RAM.  t=42: took 0.25 s (= 4.03 Hz) and 23 nodes, 17 edges uses 147.18 MBs of RAM.  t=43: took 0.24 s (= 4.12 Hz) and 23 nodes, 17 edges uses 150.51 MBs of RAM.  t=44: took 0.25 s (= 4.07 Hz) and 23 nodes, 16 edges uses 146.31 MBs of RAM.  t=45: took 0.24 s (= 4.13 Hz) and 23 nodes, 16 edges uses 148.22 MBs of RAM.  t=46: took 0.24 s (= 4.18 Hz) and 23 nodes, 15 edges uses 150.94 MBs of RAM.  t=47: took 0.24 s (= 4.22 Hz) and 23 nodes, 16 edges uses 146.42 MBs of RAM.  t=48: took 0.25 s (= 4.07 Hz) and 23 nodes, 18 edges uses 151.20 MBs of RAM.  t=49: took 0.23 s (= 4.27 Hz) and 23 nodes, 18 edges uses 146.47 MBs of RAM.  t=50: took 0.24 s (= 4.20 Hz) and 23 nodes, 19 edges uses 147.48 MBs of RAM.  t=51: took 0.24 s (= 4.18 Hz) and 23 nodes, 21 edges uses 150.88 MBs of RAM.  t=52: took 0.24 s (= 4.24 Hz) and 23 nodes, 22 edges uses 146.18 MBs of RAM.  t=53: took 0.23 s (= 4.33 Hz) and 23 nodes, 21 edges uses 146.69 MBs of RAM.  t=54: took 0.24 s (= 4.18 Hz) and 23 nodes, 20 edges uses 149.60 MBs of RAM.  t=55: took 0.25 s (= 4.05 Hz) and 23 nodes, 20 edges uses 146.55 MBs of RAM.  t=56: took 0.23 s (= 4.39 Hz) and 23 nodes, 21 edges uses 151.00 MBs of RAM.  t=57: took 0.23 s (= 4.26 Hz) and 23 nodes, 20 edges uses 146.32 MBs of RAM.  here  t=58: took 0.26 s (= 3.82 Hz) and 25 nodes, 23 edges uses 145.86 MBs of RAM.  Traceback (most recent call last):      File ""C:\DynSTGModeling\code\model\online_dyn_stg.py"", line 54, in _remove_node_model      raise ValueError('%s is not in this graph!' % str(node))  ValueError: Pedestrian/0 is not in this graph!    Process finished with exit code 1`    How can I deal with this? I tried with the NBA dataset as well but I get different errors there. "
"Hi, thank you for making the code public.     I am getting an error related to tensor type when I run test_online.py    `  File ""C:\Users\vora\Desktop\TressPass\Code\DynSTGModeling\code\model\online_node_model.py"", line 141, in obtain_encoded_tensor_dict      TD[""joint_present_orig""] = torch.cat([TD[robot_present + ""_orig""], our_prediction_present], dim=1) # [bs/nbs, state_dim+pred_dim]  RuntimeError: Expected a Tensor of type torch.FloatTensor but found a type torch.cuda.FloatTensor for sequence element 1 in sequence argument at position #1 'tensors'  `    PS- can I also know the data format needed to be given to the network? "
"Hi @pengsongyou , amazing work :) Thanks for open-sourcing it!    If I may, I have a question regarding the following paragraph of the paper:    > Consider a corner point extracted in an image; the uncertainty of its position can be estimated by computing the autocorrelation matrix C for a window of a given size around the point (see for instance  ]).   **Concretely, C is an estimate of the inverse of the covariance matrix of the corner position**.    I am struggling to understand this. Given that the autocorrelation matrix can be computed as explained e.g. in this  ).  I don't see where this result of being the inverse covariance matrix comes from.    I would highly appreciate if you could shed some light on this,  Thanks in advance!"
"In the 'Result and Evaluation' section of the paper, it said:         But how to simulate the process of camera calibration in Matlab? Which Toolbox do you use?      Thanks.  "
"Firstly, thanks for your great work!    background:  When I use Opencv camera calibration node to calibrate my camera, I found that the intrinsic's center is different in each time, about >20px error, So I guess whether it is because the chessboard position can influence the result, I found your paper and I want to use this method to solve my question.    So the first question is When I use matlab calculate the next position of chessboard, I found that the pose is out of border.  !   and is there any method to display a pose in the range of image? (I modified the image size as 1280x720 to replace the origin size of 640x480)    The second question is whether could you provide the cplusplus version of estimate-pose module? it is a great work but has some difficult for using , bacuse it's matlab code. (although I have succeed run the code on ubuntu 16.04, but there is also some matlab functions error)    Thanks for your great work again!  "
"!     I changed the width and height of chessboard in 'config/wizard.xml'. But when I chose the 0(initial images capture) mode, this code was aborted. Could you give me some help?"
Where are codes?  When are they available?
"Hi dear developer,    I was trying to run 'train.py' but received an error-'No module named 'libs.box_utils.cython_utils.cython_bbox' when trying to import a 'bbox_overlaps' file.   I have checked the folder and there is no such folder named cython_bbox nor the bbox_overlaps file.   Could you please provide them?    Thanks"
你好，我训练的是自己数据集，图像大小参差，有大小为6000*4000，于是我修改train_crop的图像大小为6000，希望这样不会裁剪的太碎，削弱了框选的特征，我也没有大改什么文件参数，但是训练的时候，loss就是不收敛，并且预测的图像中有密集的绿框，但它们并没有正确框选。  请问您能提供一些思路吗？十分感谢！
我发现您的代码里面resnet.py中将senet注意力模块以及inception模块给注释掉了。  采用自己数据集 我加回来之后发现损失变为nan  您是将这两个模块写在了其他的地方吗
修改哪里
"Please ask the author, why my visualization result is so bad?  !   !   "
你好！我想问这个代码是否不包含iou-smoothl1LOss函数的代码，我用一块1080的GPU训练33W步对这个repo复现，始终得到的map只有0.60，对比你发的2019ICCV的SCRDet中的0.72还是有着较大的差距  ，按道理没什么改动不应该有这么大的出入。我想请教一下这其中是哪里有问题  还是这个repo只是部分代码要我自己去整合其它的代码块？麻烦给出一下回复，谢谢你！
"At DEMO step (img1), an error occurred. error message is ""/.pyxbld/temp.linux-x86_64-2.7/pyrex/libs/box_utils/rbbox_overlaps.c:600:10: fatal error: rbbox_overlaps.hpp: No such file or directory"":  !     I dont know what is the reason here?    Pls help me, im new either at python and linux.    This is the error image (img2):  ! "
你好 我想问下，我在训练的时候出现这个问题，基本检测不到目标，看了生成的anchor是没有问题的，但是却出现了这个问题，你知道这是怎么回事吗？改怎么修改呀  !     
你好 我想问下你在SCRDet里面的损失函数里面包括RPN的loss吗，你论文里的total_loss=rpn_total_loss+iou_smoothL1_loss+attention_loss+ fastrcnn_cls_loss 我这样理解不知道对不对 
"I have tried to obtain similar results to the ones reported in the HBB table:    ### Task2 - Horizontal Leaderboard  | Approaches | mAP | PL | BD | BR | GTF | SV | LV | SH | TC | BC | ST | SBF | RA | HA | SP | HC |  |------------|:---:|:--:|:--:|:--:|:---:|:--:|:--:|:--:|:--:|:--:|:--:|:---:|:--:|:--:|:--:|:--:|  |[R 2 CNN++]|**75.35**|**90.18**|**81.88**|**55.30**|73.29|72.09|77.65|78.06|**90.91**|**82.44**|**86.39**|**64.53**|**63.45**|75.77|**78.21**|**60.11**|    I am using the validation set instead of the testing set because of the test annotations have not been released yed ... Could you provide the results with the validation set? Because they are too much worse compared to the ones that you report in the table ...    Am I missing something? I didn't change anything in your eval.py code .... But the mAP results are really dissappointing. I would like to know if someone has obtained similar results to the ones that the authors report.    Best,  Roberto Valle  "
"The following error occurred while running train.py. Can anyone give me some advice?    Traceback (most recent call last):    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2084, in cond      nest.assert_same_structure(orig_res_t, orig_res_f)    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/nest.py"", line 183, in assert_same_structure      _pywrap_tensorflow.AssertSameStructure(nest1, nest2, check_types)  ValueError: The two structures don't have the same nested structure.    First structure: type=tuple str=([800, 600, 700, 900, 1000, 1100, 1200],  )    Second structure: type=tuple str=( , [800, 600, 700, 900, 1000, 1100, 1200])    More specifically: Substructure ""type=list str=[800, 600, 700, 900, 1000, 1100, 1200]"" is a sequence, while substructure ""type=Tensor str=Tensor(""get_batch/cond/floordiv_1:0"", shape=(7,), dtype=int32)"" is not    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""/media/ys/000F3E52000A91C0/zheng/R2CNN-Plus-Plus_Tensorflow-master (2)/tools/train.py"", line 230, in        train()    File ""/media/ys/000F3E52000A91C0/zheng/R2CNN-Plus-Plus_Tensorflow-master (2)/tools/train.py"", line 37, in train      is_training=True)    File ""/media/ys/000F3E52000A91C0/zheng/R2CNN-Plus-Plus_Tensorflow-master (2)/data/io/read_tfrecord.py"", line 87, in next_batch      is_training=is_training)    File ""/media/ys/000F3E52000A91C0/zheng/R2CNN-Plus-Plus_Tensorflow-master (2)/data/io/read_tfrecord.py"", line 61, in read_and_prepocess_single_img      target_shortside_len=shortside_len)    File ""/media/ys/000F3E52000A91C0/zheng/R2CNN-Plus-Plus_Tensorflow-master (2)/data/io/image_preprocess.py"", line 24, in short_side_resize      false_fn=lambda: (target_shortside_len * h//w,  target_shortside_len))    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func      return func(*args, **kwargs)    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2090, in cond      ""Incompatible return values of true_fn and false_fn: {}"".format(e))  ValueError: Incompatible return values of true_fn and false_fn: The two structures don't have the same nested structure.    First structure: type=tuple str=([800, 600, 700, 900, 1000, 1100, 1200],  )    Second structure: type=tuple str=( , [800, 600, 700, 900, 1000, 1100, 1200])    More specifically: Substructure ""type=list str=[800, 600, 700, 900, 1000, 1100, 1200]"" is a sequence, while substructure ""type=Tensor str=Tensor(""get_batch/cond/floordiv_1:0"", shape=(7,), dtype=int32)"" is not  "
"First of all, thank you very much for your patience, but I have some questions  first: I'm still a little behind the results you gave me,  cls : ship|| Recall: 0.34129464285714284 || Precison: 0.8827944572748267|| AP: 0.3243042815122047  AP is only half of yours, and the others are a little different  two:Which of the modified fast RCNN is used to predict the skew box?"
"在demo_rh_pyramid.py中的157行如下面代码：  det_boxes_r_ = forward_convert(det_boxes_r_, False)  det_boxes_r_[:, 0::2] *= (w_len / ss)  det_boxes_r_[:, 1::2] *= (h_len / ss)  det_boxes_r_ = back_forward_convert(det_boxes_r_, False)  有时会报IndexError: too many indices for array，查看有时为空。当用if做排除后还会出现此错误，请指教一下。如果方便的话可以给下微信？谢谢了"
@yangxue0827 大佬，你好。我看了你的代码，发现postprocess_fastrcnn_r这里面没有根据得分来限定做nms box的个数。在nms之后做了一个显示的阈值限制。我尝试加过，但是没有成功，所以想问你我应该怎么改，谢了啊！
"why does run ""python setup.py build_ext --inplace"" compile /libs/box_utils and /libs/box_utils/cython_utils have error? I have tried R2CNN-PLUS-PLUS, R2CNN_Faster-RCNN and R2CNN_FPN, all of them have the same error!! Details is as follows.        In file included from /home/ivis/anaconda3/lib/python3.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1824:0,                   from /home/ivis/anaconda3/lib/python3.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:12,                   from /home/ivis/anaconda3/lib/python3.7/site-packages/numpy/core/include/numpy/arrayobject.h:4,                   from rbbox_overlaps.cpp:449:  /home/ivis/anaconda3/lib/python3.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning ""Using deprecated NumPy API, disable it with "" ""#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION"" [-Wcpp]   #warning ""Using deprecated NumPy API, disable it with "" \    ^~~~~~~  rbbox_overlaps.cpp: In function ‘void __Pyx__ExceptionSave(PyThreadState*, PyObject**, PyObject**, PyObject**)’:  rbbox_overlaps.cpp:5670:21: error: ‘PyThreadState {aka struct _ts}’ has no member named ‘exc_type’; did you mean ‘curexc_type’?       *type = tstate->exc_type;                       ^~~~~~~~                       curexc_type  rbbox_overlaps.cpp:5671:22: error: ‘PyThreadState {aka struct _ts}’ has no member named ‘exc_value’; did you mean ‘curexc_value’?       *value = tstate->exc_value;                        ^~~~~~~~~                        curexc_value  rbbox_overlaps.cpp:5672:19: error: ‘PyThreadState {aka struct _ts}’ has no member named ‘exc_traceback’; did you mean ‘curexc_traceback’?       *tb = tstate->exc_traceback;                     ^~~~~~~~~~~~~                     curexc_traceback  rbbox_overlaps.cpp: In function ‘void __Pyx__ExceptionReset(PyThreadState*, PyObject*, PyObject*, PyObject*)’:  rbbox_overlaps.cpp:5679:24: error: ‘PyThreadState {aka struct _ts}’ has no member named ‘exc_type’; did you mean ‘curexc_type’?       tmp_type = tstate->exc_type;                          ^~~~~~~~                          curexc_type  rbbox_overlaps.cpp:5680:25: error: ‘PyThreadState {aka struct _ts}’ has no member named ‘exc_value’; did you mean ‘curexc_value’?       tmp_value = tstate->exc_value;                           ^~~~~~~~~                           curexc_value  rbbox_overlaps.cpp:5681:22: error: ‘PyThreadState {aka struct _ts}’ has no member named ‘exc_traceback’; did you mean ‘curexc_traceback’?       tmp_tb = tstate->exc_traceback;                        ^~~~~~~~~~~~~                        curexc_traceback  rbbox_overlaps.cpp:5682:13: error: ‘PyThreadState {aka struct _ts}’ has no member named ‘exc_type’; did you mean ‘curexc_type’?       tstate->exc_type = type;               ^~~~~~~~               curexc_type  rbbox_overlaps.cpp:5683:13: error: ‘PyThreadState {aka struct _ts}’ has no member named ‘exc_value’; did you mean ‘curexc_value’?       tstate->exc_value = value;               ^~~~~~~~~               curexc_value  rbbox_overlaps.cpp:5684:13: error: ‘PyThreadState {aka struct _ts}’ has no member named ‘exc_traceback’; did you mean ‘curexc_traceback’?       tstate->exc_traceback = tb;               ^~~~~~~~~~~~~               curexc_traceback  rbbox_overlaps.cpp: In function ‘int __Pyx__GetException(PyThreadState*, PyObject**, PyObject**, PyObject**)’:  rbbox_overlaps.cpp:5739:24: error: ‘PyThreadState {aka struct _ts}’ has no member named ‘exc_type’; did you mean ‘curexc_type’?       tmp_type = tstate->exc_type;                          ^~~~~~~~                          curexc_type  rbbox_overlaps.cpp:5740:25: error: ‘PyThreadState {aka struct _ts}’ has no member named ‘exc_value’; did you mean ‘curexc_value’?       tmp_value = tstate->exc_value;                           ^~~~~~~~~                           curexc_value  rbbox_overlaps.cpp:5741:22: error: ‘PyThreadState {aka struct _ts}’ has no member named ‘exc_traceback’; did you mean ‘curexc_traceback’?       tmp_tb = tstate->exc_traceback;                        ^~~~~~~~~~~~~                        curexc_traceback  rbbox_overlaps.cpp:5742:13: error: ‘PyThreadState {aka struct _ts}’ has no member named ‘exc_type’; did you mean ‘curexc_type’?       tstate->exc_type = local_type;               ^~~~~~~~               curexc_type  rbbox_overlaps.cpp:5743:13: error: ‘PyThreadState {aka struct _ts}’ has no member named ‘exc_value’; did you mean ‘curexc_value’?       tstate->exc_value = local_value;               ^~~~~~~~~               curexc_value  rbbox_overlaps.cpp:5744:13: error: ‘PyThreadState {aka struct _ts}’ has no member named ‘exc_traceback’; did you mean ‘curexc_traceback’?       tstate->exc_traceback = local_tb;"
"Hi,   I'm trying to run the demo code and I followed all the step that you mentioned in the Markdown. But I keep on getting this error         But when I go and set the SHOW_SCORE_THRSHOLD to 0 in the cfgs.py I'm getting different error.      "
"Q1. Can i use this file as pretrained_weights =       Q2. Do i have to make changes in the code also ?    Q3. If yes, where i need to change ?    Thanks.  "
Accuracy in simple terms does not give the real picture.   F1 score gives a complete picture.   Can we add this in the chart as well ?
"OutOfRangeError (see above for traceback): PaddingFIFOQueue '_1_get_batch/batch/padding_fifo_queue' is closed and has insufficient elements (requested 1, current size 0)   ]]  I have this error when i train the model, i use the default cfgs, can u give some suggestions?"
Could anyone pls tell me how to build setup.py on window 10 ?
@yangxue0827 大佬，你好。我在测试图片的时候，怎么inference时间运行时间差异很大，这是什么原因？我有的图片是100多毫秒，有的就直接成500ms了。我是自己的数据集，我对数据的处理是先resize成800*800的图片的，并不是滑框式的。
"在测试demo_rh_pyramid.py中出现如下错误    Traceback (most recent call last):    File ""demo_rh_pyramid.py"", line 393, in        args.h_overlap, args.w_overlap,  args.save_res)    File ""demo_rh_pyramid.py"", line 69, in inference      mask_batch=None)    File ""../libs/networks/build_whole_network.py"", line 447, in build_whole_detection_network      img_shape=img_shape)    File ""../libs/networks/build_whole_network.py"", line 157, in postprocess_fastrcnn_r      use_gpu=cfgs.ROTATE_NMS_USE_GPU)    File ""../libs/box_utils/nms_rotate.py"", line 31, in nms_rotate      device_id=gpu_id)    File ""../libs/box_utils/nms_rotate.py"", line 112, in nms_rotate_gpu      Tout=tf.int64)    File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/script_ops.py"", line 384, in py_func      func=func, inp=inp, Tout=Tout, stateful=stateful, eager=False, name=name)    File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/script_ops.py"", line 199, in _internal_py_func      token = _py_funcs.insert(func)    File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/script_ops.py"", line 100, in insert      self._funcs[token] = func    File ""/usr/lib/python2.7/weakref.py"", line 108, in __setitem__      self.data[key] = KeyedRef(value, self._remove, key)    File ""/usr/lib/python2.7/weakref.py"", line 278, in __new__      self = ref.__new__(type, ob, callback)  TypeError: cannot create weak reference to 'builtin_function_or_method' object"
"Is there a way to store the best checkpoint among many so if somebody wants to store the best and not necessarily among every 20,000 checkpoints ? This way should be ideal if the dataset is not balanced. "
I am using the default configuration file and getting multiple detections on the same object. NMS is not working properly. Can you tell me why ?
"The images in my dataset are 2400x2400.    Do i have to change the following items:  cfgs.py  IMG_SHORT_SIDE_LEN = [800, 600, 700, 900, 1000, 1100, 1200]    Can you please confirm what changes i need to make for my dataset ?"
"@yangxue0827     你好, 我用自己的训练数据时会出现nan, 我的学习率是0.001, res50网络, 单卡, 这个现象是跟学习率有关吗? 你有什么好的建议解决下吗? "
how to get the feature map or heatmap in the paper?
why the  ap  of  swimming pool is so low ?while  other ap is normal  close  to the  result  served by the author .my  swimming pool's ap is just 0.007.i have tried   changes  three times  including  rectify the dataset  and parameters . what's the reason? i don't understand.
簡單說一下我利用了此方法加上我自製的exp(IoU)使得旋轉框問題有明顯降低，但仍然還是有5~10%會出現偏移的狀況，或許是因為你說的角度週期性以及長寬比例的縮放，我和我的教授討論了許久，並嘗試解決此問題，最後我嘗試在coordinate_convert.py裡面固定了ground truth的長寬，也就是說固定長短邊，長邊w，短邊h，要是 h > w 則做交換動作角度也加上90度，在實驗結果出乎意料地不錯。在此我想跟你討論，是否是因為RPN產出的proposal與ground truth非常相關，也就是說proposal的框大致是參考ground truth使得預測結果在長方形的框幾乎不會出現偏移的狀況，但在框要是接近正方形偏移的情形其實還是有。
你好，我看到代码和论文中在pixel attention部分都有相关表述，就是特征图进来之后先经过一个inception，之后通过卷积变成一个w*h*2的特征图，attention loss是用这个两通道的特征图计算的，就是不太清楚这里为什么需要2个通道呢？  我进一步查看build_attention_loss相关代码，发现这里用两个通道是因为mask标签是两个通道的？但是mask为什么要两个通道呢，mask不是一个二值标签吗，用0，1表示背景和前景不需要两个通道呀，我是遗漏了什么吗？
论文中，MDA-Net部分有两个通道，pixel attention的部分有Inception模块。但是我读了代码之后，发现attention.py这个文件中只有build_attetion这个函数被调用过，其他函数并没有被调用。而build_attetion这个函数也只是五层卷积，并没有使用Inception的结构。请问是我理解有误，还是作者出于什么样的考量没有采用原文所说的结构呢？谢谢！
"How can I sovle this problem?  train.py    2020-05-12 00:31:06.260514: W tensorflow/core/framework/op_kernel.cc:1158] Out of range: PaddingFIFOQueue '_2_get_batch/batch/padding_fifo_queue' is closed and has insufficient elements (requested 1, current size 0)     ]]  2020-05-12 00:31:06.262274: W tensorflow/core/framework/op_kernel.cc:1158] Out of range: PaddingFIFOQueue '_2_get_batch/batch/padding_fifo_queue' is closed and has insufficient elements (requested 1, current size 0)     ]]  2020-05-12 00:31:06.262326: W tensorflow/core/framework/op_kernel.cc:1158] Out of range: PaddingFIFOQueue '_2_get_batch/batch/padding_fifo_queue' is closed and has insufficient elements (requested 1, current size 0)     ]]  2020-05-12 00:31:06.263071: W tensorflow/core/framework/op_kernel.cc:1158] Out of range: PaddingFIFOQueue '_2_get_batch/batch/padding_fifo_queue' is closed and has insufficient elements (requested 1, current size 0)     ]]  2020-05-12 00:31:06.263099: W tensorflow/core/framework/op_kernel.cc:1158] Out of range: PaddingFIFOQueue '_2_get_batch/batch/padding_fifo_queue' is closed and has insufficient elements (requested 1, current size 0)     ]]  Traceback (most recent call last):    File ""/home/xxx/R2CNN-Plus-Plus_Tensorflow-master/tools/train.py"", line 230, in        train()    File ""/home/xxx/R2CNN-Plus-Plus_Tensorflow-master/tools/train.py"", line 210, in train      _, global_stepnp, summary_str = sess.run( ]]    Caused by op u'get_batch/batch', defined at:    File ""/home/xxx/R2CNN-Plus-Plus_Tensorflow-master/tools/train.py"", line 230, in        train()    File ""/home/xxx/R2CNN-Plus-Plus_Tensorflow-master/tools/train.py"", line 37, in train      is_training=True)    File ""/home/xxx/R2CNN-Plus-Plus_Tensorflow-master/data/io/read_tfrecord.py"", line 102, in next_batch      dynamic_pad=True)    File ""/home/xxx/anaconda3/envs/py2.7/lib/python2.7/site-packages/tensorflow/python/training/input.py"", line 919, in batch      name=name)    File ""/home/xxx/anaconda3/envs/py2.7/lib/python2.7/site-packages/tensorflow/python/training/input.py"", line 716, in _batch      dequeued = queue.dequeue_many(batch_size, name=name)    File ""/home/xxx/anaconda3/envs/py2.7/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py"", line 457, in dequeue_many      self._queue_ref, n=n, component_types=self._dtypes, name=name)    File ""/home/xxx/anaconda3/envs/py2.7/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 946, in _queue_dequeue_many_v2      timeout_ms=timeout_ms, name=name)    File ""/home/xxx/anaconda3/envs/py2.7/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op      op_def=op_def)    File ""/home/xxx/anaconda3/envs/py2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2506, in create_op      original_op=self._default_original_op, op_def=op_def)    File ""/home/xxx/anaconda3/envs/py2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1269, in __init__      self._traceback = _extract_stack()    OutOfRangeError (see above for traceback): PaddingFIFOQueue '_2_get_batch/batch/padding_fifo_queue' is closed and has insufficient elements (requested 1, current size 0)     ]]    "
I want to add IoU Smooth L1 loss to SCRDet             Is this correct?
Fast-RCNN/resnet_v1_50/block4/unit_1/bottleneck_v1/conv1/BatchNorm/beta not found in checkpoint    
"我這邊原本就有labelme標記好的data, 我現在要轉成VOC格式  附上我的XML檔:      這樣的格式 正確嗎 還是需要修改?  是不是要改成 x1, y1, x2, y2, x3, y3, x4, y4 的正矩形還是 斜矩形的GT"
"Hi, I would like to know how to change the batch size for training. It gives this error when I try to change the batch size to 8.    ValueError: Can not squeeze dim[0], expected a dimension of 1, got 8 for 'get_batch/Squeeze_1' (op: 'Squeeze') with input shapes: [8,?,9]."
I am using GTX 1080 Ti 11GB to train this model and it is using roughly 50% of my GPU.  I want to increase my batch size so that it utilize more GPU memory and train a little faster. I have not made much changes in config or yaml files.
"Hi @yangxue0827. Thanks for the amazing work!    I have tried to follow your instruction in the README file to test the pretrained model but I always get empty result from it.    I have tried the inference.py, demo_rh.py and demo_rh_pyramid.py script but the result are still empty like this:    !     The SHOW_SCORE_THRSHOLD value inside config is 0.5 because I turn on save res.    Am I wrong at any step ? Can you help me out a little bit here. Thanks a lot"
Does the smooth L1 loss in this algorithm include IOU?
Hello!     I wonder why C1-C3 layers in resnet101 backbone were freezed in codes? Did you guys try to unfreezed them and train the model? Thank you!
"When I test the Val data of DOTA with eval.py, where  cls : swimming-pool|| Recall: 0.0 || Precison: 0.0|| AP: 0.0  I used the weight you gave me"
"@yangxue0827 I download your pretrained ckpt file (R2CNN_20180922_DOTA_v28/voc_280002model.ckpt), but meet the following errors? Do you have some tips to solve this problem?  ===========Error hints like this:    `NotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:    Key resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/BatchNorm/beta not found in checkpoint     ]]     ]]  `"
"Thanks for publishing this repository.    After studying your code, i have noticed that the implementation of SCRDet is existed in your code, but it is commented. For example, current code and pretrained weight have many parts stated in your paper (SCRDet) as (SF-Net C3-UPsampling and Inception Module) and (MDA-Net Channel attention) are commented.    My question is: can i uncomment all these parts and train on DOTA or ICDAR dataset using the full SCRDet implementation? "
"hi ,Xue Yang, when SCRDet code will be released?"
"Basically, Is Installing Caffe on Linux recommended to run the code as a must step for running Fast RCNN here? Thanks!!"
论文里面写你们用的是1080是嘛？我也是。 我训练了一天两夜还没训练完，不知道是出了问题还是确实需要这么长时间，特向大佬求教，谢谢。
你好，测试的时候对各类采用的置信度阈值是一样的还是不同的
"In your paper, you propose a noval regression loss called IoU-smooth L1 loss, which make a big deal in performance. But in your  code I have no idea what is the IoU-smooth L1 loss. Coulde you give some more detailed illumination about this, Thanks a lot."
None
"When I run vaild data of DOTA with the weight of demo_rh.py and R2CNN_20180922_DOTA_v28 given by the author, why are some categories of AP still relatively low?"
@yangxue0827 大佬你好，我使用mobilenetv2作为backbone进行训练，出现了上面的问题，请问是什么原因？
请问论文中提到的为了改善角度回归加入了log(iou)的权重，那么iou也一起参与梯度下降了吗
大佬你好，请问paper里OBB的 map72.61结果是用train+val数据训练得出的吗？  这个代码里的71.16的结果又是在什么数据集上训练和测试的呢？
How i can configure the number of MAX Iterations for training ?
what kind of data augmentation is implemented in this project ?
@yangxue0827 大佬，你好。我看了这个代码，你使用了DOTA这个数据集，是将里面的图片截成了800*800大小。然后模型的输入就是800*800的。如果我使用自己的数据集，我不用截图的，我想直接用原图，那我模型输入尺寸，应该在哪修改？我看了你写的训练自己的数据集的修改方式，没有在哪看到修改模型的尺寸，是默认的吗？
It seems that the implementation of IoU smooth-l1 loss mentioned in the paper is missing in this repo. While it is implemented in  
"PS F:\PycharmProjects\R2CNN-Plus-Plus_Tensorflow-master\libs\box_utils> python setup.py build_ext --inplace  Traceback (most recent call last):    File ""setup.py"", line 59, in        CUDA = locate_cuda()    File ""setup.py"", line 47, in locate_cuda      raise EnvironmentError('The nvcc binary could not be '  OSError: The nvcc binary could not be located in your $PATH. Either add it to your path, or set $CUDAHOME"
"first ,i change cfg:    NET_NAME = 'MobilenetV2'  then, throw error:  Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /home/evga/wjj/R2CNN-Plus-Plus_Tensorflow/data/pretrained_weights/mobilenet/mobilenet_v2_1.0_224.ckpt"
None
"用dota 1411张图片按照代码裁剪800*800的子图，有 55178个子图。转成tfrecord有98.4G。训练内存溢出了，应该减少训练图片个数还是修改cfg.py 的那些参数？  FIXED_BLOCKS = 2  # allow 0~3，  IMG_SHORT_SIDE_LEN = [800, 600, 700, 900, 1000, 1100, 1200]，  IMG_MAX_LENGTH = 1000，  RPN_MINIBATCH_SIZE = 512，  是修改这些参数么？  "
     I am using RTX 2080TI and still getting these messages. Which GPU you would recommend to train this model ?
W tensorflow/core/framework/allocator.cc:107] Allocation of 829440000 exceeds 10% of system memory.
@yangxue0827 你好，请问你提供的 train model可以直接使用吗？我用你给的，在demo_rh.py保存的检测图片，发现，都是不行的，检测不到的，请问是不是自己重新训练才行？？
您好，我看到您的code里把inception module给注释掉了？会对结果造成什么影响吗？我的应用场景是textdetection～  谢谢～
你好，用DOTA数据集进行训练时，到step300 就卡到这里了，不再往下训练了，卡了一个晚上不动。。是什么原因引起的？ 训练过程如下  2019-09-11 19:35:25: step200    image_name:b'P0000_4352_0512.png' |                                                     rpn_loc_loss:0.004083531908690929 |                                                   rpn_cla_loss:0.4112159013748169 |                                                     rpn_total_loss:0.4152994453907013 |                                                     fast_rcnn_loc_loss:0.2768683433532715 |                                                          fast_rcnn_cla_loss:0.07029825448989868 |                                                     fast_rcnn_loc_rotate_loss:0.46062421798706055 |                                                     fast_rcnn_cla_rotate_loss:0.07179155200719833 |                                                     fast_rcnn_total_loss:0.879582405090332 |                                                          attention_loss:0.11331246793270111 |                                                     total_loss:1.9829115867614746 |                                                          pre_cost_time:1.2178001403808594s  2019-09-11 19:39:05: step300    image_name:b'P0002_1280_0000.png' |                                                     rpn_loc_loss:0.002683267230167985 |                                                      rpn_cla_loss:0.571774959564209 |                                                     rpn_total_loss:0.5744582414627075 |                                                     fast_rcnn_loc_loss:0.6731187105178833 |                                                           fast_rcnn_cla_loss:0.556395947933197 |                                                     fast_rcnn_loc_rotate_loss:1.3842848539352417 |                                                           fast_rcnn_cla_rotate_loss:0.5610043406486511 |                                                     fast_rcnn_total_loss:3.1748037338256836 |                                                          attention_loss:0.22223511338233948 |                                                     total_loss:4.546215534210205 |                                                             pre_cost_time:1.202199935913086s  
如果我训练dota数据集中某一类数据，大概150张图片，估计迭代多少次之后能稳定下来？
None
"@yangxue0827 @yangJirui     代码中的,  ws, hs = enum_ratios(enum_scales(base_anchor, anchor_scales),                               anchor_ratios)  # per locations ws and hs    子函数的返回值是hs,ws, 这里是不是写错了?    def enum_ratios(anchors, anchor_ratios):      '''      ratio = h /w      :param anchors:      :param anchor_ratios:      :return:      '''      ws = anchors[:, 2]  # for base anchor: w == h      hs = anchors[:, 3]      sqrt_ratios = tf.sqrt(tf.constant(anchor_ratios))        ws = tf.reshape(ws / sqrt_ratios[:, tf.newaxis], [-1, 1])      hs = tf.reshape(hs * sqrt_ratios[:, tf.newaxis], [-1, 1])        return hs, ws"
!   
!   你好，想问一下这个值该如何设置，感谢！
None
None
请问您是用原图进行训练的还是裁剪之后的？如果内存溢出，哪些参数可以调整呢？
"It seems that you randomly choose one of the two channel in code,why?thank you"
"OutOfRangeError (see above for traceback): PaddingFIFOQueue '_1_get_batch/batch/padding_fifo_queue' is closed and has insufficient elements (requested 1, current size 0)     ]]    I have this error when i train the model, i use the default cfgs, can u give some suggestions?"
用demo.py进行了测试，数据集是dota的val，并且进行了val_crop.py裁剪，用dota_devkit中的测试脚本进行了测试，测试结果如下：  classname: plane  ap:  0.8043356143149806    classname: baseball-diamond  ap:  0.7632814847658935    classname: bridge  ap:  0.45506307334937024    classname: ground-track-field  ap:  0.6750099483352051    classname: small-vehicle  ap:  0.5591377615596839    classname: large-vehicle  ap:  0.6746200558900223    classname: ship  ap:  0.6556901466589511    classname: tennis-court  ap:  0.9075544170664117    classname: basketball-court  ap:  0.7445786918243077    classname: storage-tank  ap:  0.6838217965433654    classname: soccer-ball-field  ap:  0.5940273758469495    classname: roundabout  ap:  0.7472942076726852    classname: harbor  ap:  0.7384752552414509    classname: swimming-pool  ap:  0.0013568521031207597    classname: helicopter  ap:  0.5403029396351005    map: 0.6363033080538332  其中swimming-pool得分只有0.13，但是作者测试的数据是63，了解作者测试的数据集是dota的test集，并且提交的官方系统，不过测试集的影响应该不大。我并没有对cfgs进行参数修改。请问这个异常如何解决呢？
None
"论文里的scale the original image to 600,800,1000,1200。这一步是指将图片切分为600*600\800*800这样不同的大小，送入网络；还是我先讲图片切成800*800，然后resize成不同大小，还是我将图片切成1200*1200再resize，或者其他方法。"
Admire your work very much. But when will the codes be released?
"Hello dear,  first, thank you to share your code with us  the problem is I don't find the page of  trained model when I try to download it."
None
"Hi,    I have a quick question toward get_L2norm_loss_self_driven in the example of Office 31. As you mentioned in #4, the loss value will only relate to delta r and it would be a constant value during the whole training process.     If that is the case, then basically the total loss at the end would be (classification loss + entropy loss + constant generated by get_l2norm_loss_self_driven). Since that constant is only depended on delta R and has nothing to do with L2_norm,  I am wondering what is the role of L2 norm in this case?    Looking forward to hearing from you soon!    Thank you,"
"Hi there. I wonder why we should the L2 norm the  feature 'x'  first?  The L2 norm of feature 'x' will always be 1,  and the true norm of feature 'x' is lost.  "
"Hi, the paper says you have used 10-crop during evaluation. But it didn't appear in the code. In addition, I cannot reproduce the results on both office31 and visda:  1) For office31, I have tried 10-crop evalution and the result is 86.1. SAFN+ENT is 87.6 reported by the paper.  2) For visda, I have tried both 10-crop and your default eval code (which is not 10-crop), and the results are 69.8 and 71.7 respectively (resnet101 base), which is far from the result reported by the paper.   All the hyperparameters I have used are the defaults in the code. I have not tried imageCLEF. Hope you can help, thanks."
"Dear Yang,    Thank you so much for sharing your code.    I have a question regarding the proposed get_L2norm_loss_self_driven loss. In your code, for example in Office31,   def get_L2norm_loss_self_driven(x):      radius = x.norm(p=2, dim=1).detach()      assert radius.requires_grad == False      radius = radius + 1.0      l = ((x.norm(p=2, dim=1) - radius) ** 2).mean()      return args.weight_L2norm * l  x.norm will be cancelled out when you compute L. This results L always be 1 and the loss will always be args.weight_L2norm which is 0.05 during the training. I also print L when I train your model, and it indeed always one.   Could you please explain this to me so that I can understand your paper better?    Thank you so much in advance for your help."
"Hi, thanks for your sharing the code!  As shown in the AFN/vanilla/Visda2017/SAFN/code/model/net.py ,  the ""ResClassifier"" contains three FC layers with 2048 * 1000, 1000 * 1000, 1000 * class_num neurons, respectively. While as shown in the AFN/vanilla/Office31/SAFN/code/model/net.py /, the ""ResClassifier "" contains two FC layers with 2048 * 1000, and 1000 * class_num neurons, respectively.  1. Could you explain that why the ""ResClassifier"" has diffirent  FC layers in diffirent datasets?   2.  As shown in Table 1,  CDAN is one of the compaired models. But as far as I know, the corresponding ""ResClassifier"" in CDAN has two FC layers with 2048 * 256, and 256 * class_num neurons in all datasets, respectively.  It's unclear to show the superiority of the proposed methods because the increase of parameters can also obtain some gain. Do you have additional experimental results about the model has the same ""ResClassifer"" ?  By the way,  according to the  section 3.3 in the paper,   you cited literature [7, 26] in paper to illustrate the so-called bottleneck. But in the respository of  [7] ,  I find the description in the corresponding code that [7] has the same ""ResClassifer"" as CDAN."
"Hi jihan,  It's a great work, and I am now reproducing the results.   I am a little bit confused about the result of the source only ResNet-50. I believe this result can be achieved by simply setting the weights of L2_norm loss and ent loss to be zero. However, it turned out that the results are much better. For example, in Office-31 A to W, the accuracy is about 0.79 (0.68 reported). Though, I saw that the same results are also reported in other papers.   Looking forward to your reply.  "
"Hi~ Where can I get ""IAFN/result"" folder and ""/data/da"" folder? As they are needed in eval.py.  Thank you."
">>training A->W epoch : 20  25it [00:08,  3.00it/s]  >>training A->W epoch : 21  25it [00:08,  2.99it/s]  >>training A->W epoch : 22  25it [00:08,  2.90it/s]  >>training A->W epoch : 23  25it [00:08,  3.00it/s]  >>training A->W epoch : 24  25it [00:08,  2.93it/s]  >>training A->W epoch : 25  25it [00:08,  2.95it/s]  >>training A->W epoch : 26  25it [00:08,  2.96it/s]  Exception ignored in:  >  Traceback (most recent call last):    File ""/home/v-lew/anaconda3/envs/PyTorch0.4Python3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 399, in __del__      self._shutdown_workers()    File ""/home/v-lew/anaconda3/envs/PyTorch0.4Python3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 378, in _shutdown_workers      self.worker_result_queue.get()    File ""/home/v-lew/anaconda3/envs/PyTorch0.4Python3.6/lib/python3.6/multiprocessing/queues.py"", line 337, in get      return _ForkingPickler.loads(res)    File ""/home/v-lew/anaconda3/envs/PyTorch0.4Python3.6/lib/python3.6/site-packages/torch/multiprocessing/reductions.py"", line 151, in rebuild_storage_fd      fd = df.detach()    File ""/home/v-lew/anaconda3/envs/PyTorch0.4Python3.6/lib/python3.6/multiprocessing/resource_sharer.py"", line 57, in detach      with _resource_sharer.get_connection(self._id) as conn:    File ""/home/v-lew/anaconda3/envs/PyTorch0.4Python3.6/lib/python3.6/multiprocessing/resource_sharer.py"", line 87, in get_connection      c = Client(address, authkey=process.current_process().authkey)    File ""/home/v-lew/anaconda3/envs/PyTorch0.4Python3.6/lib/python3.6/multiprocessing/connection.py"", line 493, in Client      answer_challenge(c, authkey)    File ""/home/v-lew/anaconda3/envs/PyTorch0.4Python3.6/lib/python3.6/multiprocessing/connection.py"", line 732, in answer_challenge      message = connection.recv_bytes(256)         # reject large message    File ""/home/v-lew/anaconda3/envs/PyTorch0.4Python3.6/lib/python3.6/multiprocessing/connection.py"", line 216, in recv_bytes      buf = self._recv_bytes(maxlength)    File ""/home/v-lew/anaconda3/envs/PyTorch0.4Python3.6/lib/python3.6/multiprocessing/connection.py"", line 407, in _recv_bytes      buf = self._recv(4)    File ""/home/v-lew/anaconda3/envs/PyTorch0.4Python3.6/lib/python3.6/multiprocessing/connection.py"", line 383, in _recv      raise EOFError  EOFError:  >>training A->W epoch : 27  25it [00:08,  2.99it/s]  Exception ignored in:  >  Traceback (most recent call last):    File ""/home/v-lew/anaconda3/envs/PyTorch0.4Python3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 399, in __del__      self._shutdown_workers()    File ""/home/v-lew/anaconda3/envs/PyTorch0.4Python3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 378, in _shutdown_workers      self.worker_result_queue.get()    File ""/home/v-lew/anaconda3/envs/PyTorch0.4Python3.6/lib/python3.6/multiprocessing/queues.py"", line 337, in get      return _ForkingPickler.loads(res)    File ""/home/v-lew/anaconda3/envs/PyTorch0.4Python3.6/lib/python3.6/site-packages/torch/multiprocessing/reductions.py"", line 151, in rebuild_storage_fd      fd = df.detach()    File ""/home/v-lew/anaconda3/envs/PyTorch0.4Python3.6/lib/python3.6/multiprocessing/resource_sharer.py"", line 57, in detach      with _resource_sharer.get_connection(self._id) as conn:    File ""/home/v-lew/anaconda3/envs/PyTorch0.4Python3.6/lib/python3.6/multiprocessing/resource_sharer.py"", line 87, in get_connection      c = Client(address, authkey=process.current_process().authkey)    File ""/home/v-lew/anaconda3/envs/PyTorch0.4Python3.6/lib/python3.6/multiprocessing/connection.py"", line 493, in Client      answer_challenge(c, authkey)    File ""/home/v-lew/anaconda3/envs/PyTorch0.4Python3.6/lib/python3.6/multiprocessing/connection.py"", line 732, in answer_challenge      message = connection.recv_bytes(256)         # reject large message    File ""/home/v-lew/anaconda3/envs/PyTorch0.4Python3.6/lib/python3.6/multiprocessing/connection.py"", line 216, in recv_bytes      buf = self._recv_bytes(maxlength)    File ""/home/v-lew/anaconda3/envs/PyTorch0.4Python3.6/lib/python3.6/multiprocessing/connection.py"", line 407, in _recv_bytes      buf = self._recv(4)    File ""/home/v-lew/anaconda3/envs/PyTorch0.4Python3.6/lib/python3.6/multiprocessing/connection.py"", line 383, in _recv      raise EOFError  EOFError:  >>training A->W epoch : 28  25it [00:08,  2.91it/s]  >>training A->W epoch : 29  25it [00:08,  2.99it/s]"
"Hi,  Thanks for releasing the code for your work.    One quick question: In section 3.2 of your ICCV paper, you wrote about an L2-preserved dropout operation (to meet the adaptive L2 feature norm goal). However, I can't seem to find it in this repository. For example, the model from the   seems to use ordinary nn.Dropout - which, if I recall correctly, is the L1 preserved dropout that you mention in the paper. So is the L2 preserved dropout not used, or is it simply omitted in this release of the code?    Thanks"
You have done a great work.    I am confused that whether you can publish the codes about visualization in figure 1 and figure 4?    Thank you very much. Looking forward to your replay.
"Hello,thank you for sharing your codes. It is so useful.  I am confused that whether you can publish the codes about visualization in figure 1 and figure 4?  Thank you very much."
"Hi,  Did you use pretrained model on Imagenet as other domain adaptation methods?    "
"Hi,    may I get the code for producing figure 1 of your paper, please?"
"Hi,   May I know that how we can test the results with Alexnet model? It would be really nice if you compare your results with Alexnet model?    Thanks and regards,"
it seems that the 'test' and 'trainval' of trian.py is not fit the data loader.    ###  train.py       ###  data_utils.py   
"hi , could you please offer some results of this Network? thanks :-)"
"Hi, after I successfully trained the model as suggested in the README, I encounter overflow when running `test.py` by `python test.py --model_path log/best_model_epoch_174.ckpt`:     Then the visualization becomes a white point in the middle, which is definitely caused by the overflow problem. So i checked the raw output of the network in `test.py` (`preds` in function `generate_shapse_from_vectors`), and it turns out its values are of `1e21` magnitude... Is it reasonable? Or did I miss something?    Thanks if you can help."
"Hi,    Thank you for making the code available for TRN. I am having a very hard time reproducing the results using features pretrained on Kinetics. Also I am noticing huge variation in performance based on minor changes in the data generation code? Could you please share the script you used to generate the training and test split for training TRN? Without the actual code to generate data, there are inconsistencies in the experiments and this hurts in making any progress. Please help!    Thanks  Gaurav"
"For THUMOS'14 dataset,  when I get chunk-level feature vectors(e.g. Lx2048) for each video,  how could I get chunk-level labels for each video?    Groundtruth of BasketballDunk_test for THUMOS'14 dataset:  > video_test_0000179  7.8 9.0  > video_test_0000179  10.5 12.3  > video_test_0000179  17.3 18.7  > video_test_0000179  20.2 22.9    Cause groundtruth of raw video is the category of action and the interval of action, should I get the frames'labels from each interval (seconds) firstly, then get the union of 6 frames' labels as a chunk's label and combine all chunk-level labels for each video?"
"Hi Ming, thank you for releasing the code.    May I request you to share features of test set or at least frame-level annotations of test set?  Since OAD task does not have a uniform annotation for test, it is difficult to make a fair comparison.  It would be very helpful if you share the annotations for fair comparison.    Thank you.  "
Could you offer the dataset features because I lack the GPU source？Thanks a lot!
"We have been attempting to train the TRN model on the THUMOS'15 dataset, and have used resnet and inception to extract the features from the RGB and optical flow images respectively. The optical flow was computed using Farneback algorithm on OpenCV.     We were not training using the full dataset, only a subset of 120 videos, with a batch size of 8. However, we are unable to get any reasonable results, with the mAP score for any given class not exceeding 8%, after over 60 epochs. Would you have any insights as to why this is?    Additional note:- We also tried extracting features using different models, but attained the same results. "
"  From Where I can get the  inceptionresnetv2/ 201702271017.npy , I have access to the HDD dataset but I couldn't find the camera feature data. "
"Hi Mingze:  I was reading the code in  , and found there is a   the following code:     I think it could be some sort of data augmentation ?  If so, what's the purpose of the   ? Why its maximum should be  ?"
"Hi Mingze,    According to your LSTR paper, we found a kinetics pretrained model in mmaction2 library is used to extract motion features.  ""Motion features are extracted from pre-computed stacked optical flow fields between 6 consecutive frames.""    However, we found that the TSN motion extraction model in mmaction2 is pretrained using 5 consecutive frames, e.g.       Can you let me know which pretrained model you have used to extract flow features?    Many thanks.  "
"Hi,    First of, thanks for making TRN's code available. Will you also be release the code for LSTR?    Thanks"
"We ran into this error with HDD dataset,     FileNotFoundError: [Errno 2] No such file or directory: 'data/HDD/inceptionresnetv2/201703011016.npy'    I wonder where to get the 201703011016.npy file?"
"Hi,    I am glad to hear that the new result on THUMOS reaches 62.1%.  Can you let me know what TSN weights pretrained by Kinetics are used?   It would be better if the link to the weights is provided, as there are many Kinetics pretrained weights?"
"Your paper mentions that training was conducted in an end-to-end manner. However, the pytorch code does not link the feature extraction models to the TRN model. Were the feature extraction models that you used fine-tuned on the THUMOS or HDD Datasets, or were the pre-trained ImageNet weights used?    If they were fine-tuned, were they fine-tuned separately, treating each image as a classification problem, or were they linked to the TRN and trained as part of the TRN training, end-to-end?    Thank you. "
"Thanks for your excellent work, I have a question about the prepared data format. Assume a 5 seconds video with fps 24, it has total 5\*24=120 original frames, so is the L is 5\*4=20? so the actually inputs of the video are 20 frames (or chunks)?"
"Excuse me , but I want to know that whether you changed the fps of videos in thumos14 dataset,since the original fps of the video is 30 and what you mentioned in your articles is that the fps is 24.Another question is that is the axis 0 of camera inputs minus the axis 0 of motions inputs is 5?For example,the camera inputs is (200,22),the motion inputs is (195,22),is that correct?Thank you a lot!"
"I want to train the ResNet-200 model on my RGB dataset which consists of 22 classes as you mentioned before. When I run the classification with my RGB model, I met the ""ShapeEquals(proto) shape mismatch (reshape not set)"" problem.                is my prototxt for running the classification.     is my prototxt for training TSN model. The first part of my my prototxt for training, including ""data"" and ""reshape_data"" layer is referenced from  . The second part, the layers between ""caffe.SpatialConvolution_0"" and ""caffe.Flatten_673"", is referenced from  . And the third part, including layers after ""caffe.Flatten_673"" is referenced from this  .     Actually, I asked the question over   in anet2016-cuhk. No response so far. Could you give me some directions for setting the prototxt? I think the problem should be in the first part or third part, but I have no idea which layer goes wrong. Sorry for bothering, but this problem really stuck me for a period of time. Thank you!"
"According to the following structure, I need to prepare 3 .npy files for training on THUMOS'14 dataset.     * THUMOS'14 dataset:         In this  , we could do video classification with the two-stream model, which includes one reset-200 model for RGB input and one BN-Inception model for optical flow input. For reset-200 model, should I get the parameters from ""caffe.Flatten_673"" layer? I have checked the shape of the array, which is (1, 1, 2048). For BN-Inception model, should I get the parameters from ""global_pool"" layer? The shape of the array is (1, 10, 1024, 1, 1). Furthermore, I have no idea what .npy file I need to put in the ""target"" folder. Could you share what is the data format for training on THUMOS'14 dataset? Thank you!"
"Hello, I followed your instructions to first run the code posed by yjxiong to extract features, but I found that when I run his code, there is always a problem of ""ANET_HOME"" and ""ANET_CFG"", it seems that they are not in environment, did you meet this problem when extracting features, and if I use that code to extract features of THUMOS14, do I need to change his code? It seems his code is suitable for ActivityNet, thank you a lot!"
"I try to extract feature representations for THUMOS'14 dataset by following this  . In the  , OpenCV's version is 2.4.13, and the Caffe's version is their specified version. My GPU is Nvidia GeForce RTX 2060, I install CUDA 10.0 and cuDNN 7.6.4 for GPU acceleration. I found out that I could not build OpenCV 2.4.13 in my environment. Therefore, I built OpenCV 4.1.0 and denseflow by following this  . And I built Caffe from the official  . When I run the example      , I met the  . I tried to build the specified version Caffe with no luck. I have noticed that your code is developed with CUDA 9.0 and Python 3.6. I suppose you have met similar environment issues because the   is developed with Python 2. Would you mind sharing how you solved the Caffe LayerParameter issue? Thank you!"
Thanks for your great work.    Would you please upload the feature files (.npy) for THUMOS'14 dataset so we can directly run your code?    Thanks.
"Thanks for the great work.  I want to ask the questions about the code of online demo/main.py!!  How does the buffer mean??   !     And another question is about the input of model and tvm，  As I know，when I want to compile the torch2tvm ，  the shape have to same with the input shape of model = (4,3,224,224) [If I use num_segment = 4]，  So I am curios about why it can still compile??   Could you tell me the possible reason?    Thank you so much!!    "
If I want to extract other features. What should I do?  And I need kinetics pre-trained model。
"Thanks for sharing this great resources.  I am trying to play with different frame rates for TSM.  I noticed there are 3 important attributes here:  frame_count,  num_segments and shift_div.    For example,  if I reduced frame_count from 8 to 4 (which means the video is split into 4 segments this time, so the equivalent frame rate is reduced),  should I also adjust ""shift_div"" and ""num_segments""?   Am I right to say ""shift_div"" should always be equal or smaller than ""frame_count""?"
"Hi, thanks for your great work and sharing. I encounter some problems when I compiled the mobilenet_v2_tfslim.py after initialized to tensorflow-slim submodule (git submodule update --init --recursive). And build online_model fold succesfully.     However, the error was happened in this line         "
None
"Hi, i guess using two conditions in the code doesn't make same sense. in _ops/dataset_ if average is >0, it is by default true that record.num_frames > self.num_segments. So whats the point of adding again as a condition. Someone please put some light on it."
"Hi,       A very interesting work. I saw your presentation video on youtube and wondered how were you able to link the neural network model with online google maps? I searched your paper but could not find any reference to API calls of google maps.  I am working on a project where I want to access google maps from my jetson device in real-time.        It would be helpful if you could point me in the right direction.     Many thanks,  Niranjan              "
coulde you provide the pretrain mode of mobilenetv2 in k400?   the url is wrong
"I use this to train 16 frames TSM on something-something v2, but validation accuracy is only 61%, and test accuracy with twice sample is only 62.26%    python3 main.py somethingv2 RGB --arch resnet50 \  --num_segments 16 --gd 20 --lr 0.04 --lr_steps 20 30 --epochs 40 \  --batch-size 256 -j 16 --dropout 0.5 --consensus_type=avg --eval-freq=1 \  --shift --shift_div=8 --shift_place=blockres    How to get the same performance as the paper?"
"When I want try my model on my dataset, I want to know when should i empty the shift_buffe. Cause in the online demo, shift_buffer never got empty? Is that same to train module?"
"  params = []      for child in list(model.children())[:-1]:          params.extend(list(child.parameters()))      model.avg_pool = nn.Identity()      print(len(X),X.shape)      # shape of x: [N, T, C, H, W]       d = torch.rand((64,8,3,256,256),dtype=torch.float32).to(device)      preds = model(d,8)       File ""/home/muhammadbsheikh/anaconda3/envs/open-mmlab2/lib/python3.7/site-packages/torch/nn/functional.py"", line 1370, in linear      ret = torch.addmm(bias, input, weight.t())  RuntimeError: size mismatch, m1: [64 x 1572864], m2: [2048 x 101] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:290  (open-mmlab2) muhammadbsheikh@LambdaBlade:~/workspace/projects/AV_HAR_MBS$ "
None
This dataset is unavailable now on TwentyBN website. Could anyone share it on drive please? Thanks a lot.
None
"when i use two gpus to run the main.py to train model on sthv2 dataset, got error as below:  -----------------------------------------------------------------------------------------------------    Traceback (most recent call last):    File ""main.py"", line 378, in        main()    File ""main.py"", line 194, in main      train(train_loader, model, criterion, optimizer, epoch, log_training, tf_writer)    File ""main.py"", line 244, in train      output = model(input_var)    File ""/home/simon/anaconda3/envs/tsm/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 532, in __call__      result = self.forward(*input, **kwargs)    File ""/home/simon/anaconda3/envs/tsm/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py"", line 151, in forward      replicas = self.replicate(self.module, self.device_ids[:len(inputs)])    File ""/home/simon/anaconda3/envs/tsm/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py"", line 156, in replicate      return replicate(module, device_ids, not torch.is_grad_enabled())    File ""/home/simon/anaconda3/envs/tsm/lib/python3.8/site-packages/torch/nn/parallel/replicate.py"", line 88, in replicate      param_copies = _broadcast_coalesced_reshape(params, devices, detach)    File ""/home/simon/anaconda3/envs/tsm/lib/python3.8/site-packages/torch/nn/parallel/replicate.py"", line 71, in _broadcast_coalesced_reshape      tensor_copies = Broadcast.apply(devices, *tensors)    File ""/home/simon/anaconda3/envs/tsm/lib/python3.8/site-packages/torch/nn/parallel/_functions.py"", line 21, in forward      outputs = comm.broadcast_coalesced(inputs, ctx.target_gpus)    File ""/home/simon/anaconda3/envs/tsm/lib/python3.8/site-packages/torch/cuda/comm.py"", line 39, in broadcast_coalesced      return torch._C._broadcast_coalesced(tensors, devices, buffer_size)  RuntimeError: NCCL Error 2: unhandled system error"
"Hi han:  Could you tell me why the total number of parameters of TSM is only 24.3M? And ResNet50 has 25.56M, 24.3 ＜ 25.56,  Is this reasonable? Looking forward to your answer. Thanks !!!"
"I notice that the parameter ""epoches"" is 50 when training on sth-sth v1. Why the model of ""e45.pth"" not ""e50.pth"" is used when testing?     I'll appreciate that if someone can reply me."
Are the results of the UCF101 and HMDB51 data sets in Table 1 in your TSM paper only the results of split 1?
None
"As for how to train my data set, I have the folder of video frame and the label of the corresponding folder. How can I make train_videofolder.txt and val_videofolder.txt."
"When transform torch model to onnx model:  for this line of code:x1, x2 = x[:,:,c//8],x[:,c//8:]  TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!   I think we can't ignore this warning  "
     Cannot find expanded_conv_shift in conv_blocks.py at 4dd1fd3 commit
"Hi, thank you for providing the code, I have some questions about file 'mobilenetv2_jester_online.pth.tar'    1. how to train 'mobilenetv2_jester_online.pth.tar'? Is its training process related to TSM?  2. how to train a online model? (deal with buffer)"
None
"I trained  2 classes of behavior recognition using MobilenetV2 and then used Torch and TensorRT on the Jetson Nano to get vastly different results. Excuse me, what's the problem?    model = TSN(      num_class=2,       num_segments=8,       modality=""RGB"",      base_model=""mobilenetv2"",       dropout=0.8,      is_shift=True,       before_softmax=True)  state_dict = torch.load(""checkpoint/epoch_59_ckpt.pth.tar"")['state_dict']  model.load_state_dict(state_dict)  model.cuda()  model.eval()    x = torch.randn((24, 224, 224)).cuda()  model_trt = torch2trt_dynamic(model, [x])  ......  out = model(buffer)   # buffer shape [24,224,224]  out_trt = model_trt(buffer)    results：  out          tensor([[ 1.6837, -1.6845]], device='cuda:0')  out_trt     tensor([[-4.6641,  4.7363]], device='cuda:0')  ====================================  out           tensor([[ 1.6734, -1.6750]], device='cuda:0')  out_trt      tensor([[-4.6915,  4.7649]], device='cuda:0')    tensorrt (7.1.3.0)  topi (0.6.0)  torch (1.6.0)  torch2trt (0.2.0)  torch2trt-dynamic (0.3.0)        "
"I want to run online_demo  but error happend    python3 main.py  Open camera...     Build transformer...  /home/ice/.local/lib/python3.6/site-packages/torchvision-0.9.0-py3.6-linux-aarch64.egg/torchvision/transforms/transforms.py:286: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.    ""please use transforms.Resize instead."")  /home/ice/.local/lib/python3.6/site-packages/torchvision-0.9.0-py3.6-linux-aarch64.egg/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.    ""Argument interpolation should be of type InterpolationMode instead of int. ""  Build Executor...  [libprotobuf ERROR google/protobuf/descriptor_database.cc:57] File already exists in database: onnx/onnx-ml.proto  [libprotobuf FATAL google/protobuf/descriptor.cc:1164] CHECK failed: generated_database_->Add(encoded_file_descriptor, size):   terminate called after throwing an instance of 'google::protobuf::FatalException'    what():  CHECK failed: generated_database_->Add(encoded_file_descriptor, size):   Aborted (core dumped)      How to solve?"
"As stated in the paper, I got the same size x and x_shift. But the layer_func() changes the shape of x_shift which leads to an error when add it to x. Is there someone have the same problem?"
"I am trying to use your scripts for preprocessing the kinetics dataset, but I realized that the dataset structure I have does not match with your code.   May I know what are the contents of the following?  kinetics/images  kinetics/labels/train_videofolder.txt    Thank you in advance!  "
"In the TABLE 1 of the paper reports the TSN and TSM results on UCF101 and HMDB51 dataset, however, could you give more details about the experiments settings:  1、Did you follow TSN to process the video data to extract rgb frames? What  the scale of the rgb frames are?  2、The pre-trained model you use is dense sampling or uniform sampling?  3、The results on UCF101 and HMDB51 are reported on only split 1 or the 3 splits average?  4、The results on UCF101 and HMDB51 are reported on only rgb input or fusion the flow?  "
thanks for the nice job!  But i wonder how to recognize mulit actions meanwhile in one video usingTSM
"Hello,    while running the Demo I receive an FatalException from Google Protobuf. Did anyone before solved this issue?     Thanks for help!    python3 main.py  Open camera...  [ WARN:0] global /home/nvidia/host/build_opencv/nv_opencv/modules/videoio/src/cap_gstreamer.cpp (933) open OpenCV | GStreamer warning: Cannot query video position: status=0, value=-1, duration=-1     Build transformer...  /usr/local/lib/python3.6/dist-packages/torchvision-0.8.0a0+45f960c-py3.6-linux-aarch64.egg/torchvision/transforms/transforms.py:280: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.    ""please use transforms.Resize instead."")  Build Executor...  /home/kea/temporal-shift-module/online_demo/mobilenet_v2_tsm.py:95: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!    x1, x2 = x[:, : c // 8], x[:, c // 8:]  [libprotobuf ERROR google/protobuf/descriptor_database.cc:57] File already exists in database: onnx/onnx-ml.proto  [libprotobuf FATAL google/protobuf/descriptor.cc:1164] CHECK failed: generated_database_->Add(encoded_file_descriptor, size):   terminate called after throwing an instance of 'google::protobuf::FatalException'    what():  CHECK failed: generated_database_->Add(encoded_file_descriptor, size):   Aborted (core dumped)  "
"**The TSM test scripts on Something-Something v1 dataset is as follow:**  efficient setting: center crop and 1 clip     accurate setting: full resolution and 2 clips (--twice sample)       But, there is no argument about test_list. We can know the acc is on the val dataset from the code in line 177 of test_models.py :  `test_file if test_file is not None else val_list`    The test data is offered on the official site:    So how to get the results on the test dataset? Or, the acc on the paper is based on the val dataset ?  Thanks.  "
"Hi, @tonylins . First thing first, thanks for your amazing work     I have been paying attention to tsm for a long time.  However i don't have any end device which has gpu to run online-demo,  so can the online-demo run on google colab? Thanks in advance~"
None
"Does anyone train tsm on ucf101  only using imagenet pretrained resnet50? No Kinetics pretrained.  I train tsm on ucf101 only using RGB stream, imagenet pretrained resnet50,  with hyper-parameters same as TSN, only get 83.5 accuracy."
"Hi Lin,    Thanks for your impressive work. However, when I want to have a try of your online model on Jetson Nano. It shows only 0.7 vid/s, extremely slow. And I do have followed your guidance. And my environment is as follows:  - CUDA 10.2  - CUDNNv8  - openCV 4.1.1  - pytorch 1.6 and torchvision 0.7    Is there any operation for me to speed up  the recognition speed?    I am looking forward to your reply.     Best wishes,  Bin"
"Thanks for your contribution! I want to know the number in the table below is used for training or used for testing or both them? In other words, Is the number of frames used for training is same as  the number of frames used for testing?    !   "
"What does Vid/s mean for online demo? Isn't it FPS? because the script takes alternative frames and calculate the resizing plus inference time for each of those frames and then divide 1 by the calculated time. So, its FPS, no?"
"thanks your work,i'm Interesting in gesture recognition，could you provide dataset  of online_demo?  thanks"
"When I run the online demo main.py, the code runs, but it cannot recognize any gesture and the camera screen it shows is completely green. When I tried another code to see if my camera was the issue, it showed it working perfectly fine. Can someone help me?"
Sorry to disturb you. Really excellent work!  May I  get a copy of ”kinetics_label_map.txt“?
"I want to fine-tune on my dataset with 'TSM_kinetics_RGB_mobilenetv2_shift8_blockres_avg_segment8_e100_dense.pth'  I can train successfully with 'TSM_kinetics_RGB_mobilenetv2_shift8_blockres_avg_segment8_e100_dense.pth' , but when I train with command:  python main.py mydata RGB \       --arch mobilenetv2 --num_segments 8 \       --gd 20 --lr 0.001 --lr_steps 10 20 --epochs 25 \       --batch-size 8 -j 16 --dropout 0.8 --consensus_type=avg --eval-freq=1 \       --shift --shift_div=8 --shift_place=blockres \       --tune_from=pretrained/TSM_kinetics_RGB_mobilenetv2_shift8_blockres_avg_segment8_e100_dense.pth    I got error:  Traceback (most recent call last):    File ""main.py"", line 378, in        main()    File ""main.py"", line 123, in main      model.load_state_dict(model_dict)    File ""/home/cgim/anaconda3/envs/centernet/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 830, in load_state_dict      self.__class__.__name__, ""\n\t"".join(error_msgs)))  RuntimeError: Error(s) in loading state_dict for DataParallel:   Unexpected key(s) in state_dict: ""base_model.features.0.0.weight"",...    What should I change?  "
"Thanks for the code.  I have some questions:  Table2 in the paper, 1.What is the result of training optical flow only and what is the training script？ 2.How to get the results of rgb and optical flow, can you provide a reproducible script? @tonylins Thanks!"
"I am trying to run the online demo on GeForce RTX 2080. I am getting this error that I cannot solve:    **conv2d: requires that `23`, the input channels (23) divided by groups (1),   must match the input channels of the weight `24`, where the weight shape is ([144, 24, 1, 1]).  The type inference pass was unable to infer a type for this expression.  This usually occurs when an operator call is under constrained in some way, check other reported errors for hints of what may of happened.**    The issue seems to be during model initialization for 3.conv and 4.conv layers, particularly in the Inverted ResidualWithShift Module.    Running a randomly initialized tensor gives the expected output, is there an issue with the input dimension from the webcam that might be giving this error?  Please help.  I reinstalled an older version of tvm to see if that resolves the issue, but it gave me the following error in the torch2tvm_module  `graph, tvm_module, params = tvm.relay.build(relay_module, target, params=params)`    [15:19:08] /home/cwc/incubator-tvm/src/relay/ir/doc.h:50: text node: ' an internal invariant was violated while typechecking your program [15:19:08] /home/cwc/incubator-tvm/src/relay/op/tensor/transform.cc:1919: Check failed: begin_v   const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)+0xfae) [0x7f6978f3455e]    [bt] (2) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(std::_Function_handler  const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>::AssignTypedLambda  const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>(bool (*)(tvm::Array  const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&))::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)+0xd6) [0x7f6978e41466]    [bt] (3) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(tvm::relay::TypeSolver::Solve()+0xad4) [0x7f69790b1294]    [bt] (4) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(tvm::relay::TypeInferencer::Infer(tvm::relay::Expr)+0x55) [0x7f697909b0c5]    [bt] (5) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(tvm::relay::InferType(tvm::relay::Function const&, tvm::relay::Module const&, tvm::relay::GlobalVar const&)+0x1c5) [0x7f697909b825]    [bt] (6) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(tvm::relay::ModuleNode::Add(tvm::relay::GlobalVar const&, tvm::relay::Function const&, bool)+0x356) [0x7f6979182826]    [bt] (7) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(tvm::relay::ModuleNode::FromExpr(tvm::relay::Expr const&, tvm::Map  const&, tvm::Map  const&)+0x24f) [0x7f697918336f]    [bt] (8) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(+0xa5377b) [0x7f697918577b]    ; ' should not has tab or newline.  Traceback (most recent call last):      File ""main.py"", line 309, in        main()      File ""main.py"", line 202, in main      executor, ctx = get_executor()      File ""main.py"", line 113, in get_executor      return torch2executor(torch_module, torch_inputs, target)      File ""main.py"", line 68, in torch2executor      graph, tvm_module, params = torch2tvm_module(torch_module, torch_inputs, target)      File ""main.py"", line 44, in torch2tvm_module      relay_module, params = tvm.relay.frontend.from_onnx(onnx_model, shape=input_shapes)      File ""/usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/relay/frontend/onnx.py"", line 1497, in from_onnx      mod, params = g.from_onnx(graph, opset)      File ""/usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/relay/frontend/onnx.py"", line 1344, in from_onnx      return _module.Module.from_expr(func), self._params      File ""/usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/relay/module.py"", line 233, in from_expr      return _module.Module_FromExpr(expr, funcs, defs)      File ""tvm/_ffi/_cython/./function.pxi"", line 304, in core.FunctionBase.__call__      File ""tvm/_ffi/_cython/./function.pxi"", line 239, in core.FuncCall      File ""tvm/_ffi/_cython/./function.pxi"", line 228, in core.FuncCall3      File ""tvm/_ffi/_cython/./base.pxi"", line 160, in core.CALL    tvm._ffi.base.TVMError: Traceback (most recent call last):    [bt] (7) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(TVMFuncCall+0x65) [0x7f697926a195]    [bt] (6) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(+0xa5377b) [0x7f697918577b]    [bt] (5) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(tvm::relay::ModuleNode::FromExpr(tvm::relay::Expr const&, tvm::Map  const&, tvm::Map  const&)+0x24f) [0x7f697918336f]    [bt] (4) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(tvm::relay::ModuleNode::Add(tvm::relay::GlobalVar const&, tvm::relay::Function const&, bool)+0x356) [0x7f6979182826]    [bt] (3) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(tvm::relay::InferType(tvm::relay::Function const&, tvm::relay::Module const&, tvm::relay::GlobalVar const&)+0x1c5) [0x7f697909b825]    [bt] (2) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(tvm::relay::TypeInferencer::Infer(tvm::relay::Expr)+0x71) [0x7f697909b0e1]    [bt] (1) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(tvm::relay::ErrorReporter::RenderErrors(tvm::relay::Module const&, bool)+0x19bb) [0x7f697915f9cb]    [bt] (0) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f6978a82b53]    [bt] (8) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(+0xa5377b) [0x7f697918577b]    [bt] (7) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(tvm::relay::ModuleNode::FromExpr(tvm::relay::Expr const&, tvm::Map  const&, tvm::Map  const&)+0x24f) [0x7f697918336f]    [bt] (6) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(tvm::relay::ModuleNode::Add(tvm::relay::GlobalVar const&, tvm::relay::Function const&, bool)+0x356) [0x7f6979182826]    [bt] (5) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(tvm::relay::InferType(tvm::relay::Function const&, tvm::relay::Module const&, tvm::relay::GlobalVar const&)+0x1c5) [0x7f697909b825]    [bt] (4) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(tvm::relay::TypeInferencer::Infer(tvm::relay::Expr)+0x55) [0x7f697909b0c5]    [bt] (3) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(tvm::relay::TypeSolver::Solve()+0xad4) [0x7f69790b1294]    [bt] (2) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(std::_Function_handler  const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>::AssignTypedLambda  const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>(bool (*)(tvm::Array  const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&))::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)+0xd6) [0x7f6978e41466]    [bt] (1) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(tvm::relay::StridedSliceRel(tvm::Array  const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)+0xfae) [0x7f6978f3455e]    [bt] (0) /usr/local/lib/python3.6/dist-packages/tvm-0.6.0-py3.6-linux-x86_64.egg/tvm/libtvm.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f6978a82b53]    File ""/home/cwc/incubator-tvm/src/relay/ir/error.cc"", line 132  TVMError:       Please help me. Thanks!        "
"I can install onnx-simplifier on Jetson nano on Nov 10, but I can't install onnx-simplifier on Jetson nano on Nov 11, a mistake ‘Could not find a version that satisfies the requirement onnxoptimizer>=0.1.1 (from onnx-simplifier) (from versions: ) No matching distribution found for onnxoptimizer>=0.1.1 (from onnx-simplifier)’ .  Does onnxoptimizer-0.1.1-cp37-cp37m-win_amd64.whl can't fit Jetson nano(aarch64) ?"
"hello，how to train and test jester v1 dataset, incluing how to set parameters (eg.test_segments, batch_size)"
"How can I deal with RWF-2000 when following the instruction of Data Preparation? My dataset is RWF-2000 which contatins train and val folder,label is only Fight and NonFight."
"Hi, I wonna use TSM on my own dataset, which is a video-like input(each gesture have 32 frames,so my input shape is (N,C,T,H,W)).  But when I use a 2D conv backbone (such as resnet50), it needs a 4 dimensional input. So what should I do to merge my own input to a 4-D input?   If I use x.view(NT,C,H,W), my data got the 4-D input, but the label size is still N, so here comes the mismatch.  I don't know how to solve the problem."
"Hi Developers,  Thanks for this amazing repository. I am working on a video captioning task.     - [ ] How can i use the pretrained models in this repo as feature extractor for my architecture?   - [ ] is there a script to do that?   - [ ] Which functions do i need to call?    Any help would be appreciable. Thanks"
hello，as marked in your paper， the flops and params of eco is 64g & 47.5M，could you share the code how to get the results?
Did you use the optim policy for traning the model for charades dataset  or did you train without optim policy to get the mentioned maP values in the paper?
I trained the TSM-resnet50 with 8 segments on SomethingV1 datasets with optical flow inputs only. The single crop accuracy for the flow model is around 37% and the two-stream result is around 48%. Is this performance reasonable or not?
"ｉmeet this error,my enviornment is ,can you give some advice  onnx=1.7.0     onnx-simplifier=0.2.9   torch= 1.4.0     torchvision=0.5.0   tvm= 0.6.0  opencv-python=4.4.0.44      Open camera...     GLib-GIO-Message: 19:00:58.906: Using the 'memory' GSettings backend.  Your settings will not be saved or shared with other applications.  Build transformer...  /home/sunxu/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:219: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.    warnings.warn(""The use of the transforms.Scale transform is deprecated, "" +  Build Executor...  /home/sunxu/下载/未命名文件夹/temporal-shift-module-master/online_demo/mobilenet_v2_tsm.py:95: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!    x1, x2 = x[:, : c // 8], x[:, c // 8:]  [19:01:01] /home/sunxu/下载/未命名文件夹/incubator-tvm/src/relay/ir/doc.h:50: text node: ' an internal invariant was violated while typechecking your program [19:01:01] /home/sunxu/下载/未命名文件夹/incubator-tvm/src/relay/op/tensor/transform.cc:1919: Check failed: begin_v        main()      File ""main.py"", line 341, in main      executor, ctx = get_executor()      File ""main.py"", line 155, in get_executor      return torch2executor(torch_module, torch_inputs, target)      File ""main.py"", line 111, in torch2executor      graph, tvm_module, params = torch2tvm_module(torch_module, torch_inputs, target)      File ""main.py"", line 58, in torch2tvm_module      relay_module, params = tvm.relay.frontend.from_onnx(onnx_model, shape=input_shapes)      File ""/home/sunxu/anaconda3/lib/python3.7/site-packages/tvm-0.6.0-py3.7-linux-x86_64.egg/tvm/relay/frontend/onnx.py"", line 1497, in from_onnx      mod, params = g.from_onnx(graph, opset)      File ""/home/sunxu/anaconda3/lib/python3.7/site-packages/tvm-0.6.0-py3.7-linux-x86_64.egg/tvm/relay/frontend/onnx.py"", line 1344, in from_onnx      return _module.Module.from_expr(func), self._params      File ""/home/sunxu/anaconda3/lib/python3.7/site-packages/tvm-0.6.0-py3.7-linux-x86_64.egg/tvm/relay/module.py"", line 233, in from_expr      return _module.Module_FromExpr(expr, funcs, defs)      File ""tvm/_ffi/_cython/./function.pxi"", line 304, in tvm._ffi._cy3.core.FunctionBase.__call__      File ""tvm/_ffi/_cython/./function.pxi"", line 239, in tvm._ffi._cy3.core.FuncCall      File ""tvm/_ffi/_cython/./function.pxi"", line 228, in tvm._ffi._cy3.core.FuncCall3      File ""tvm/_ffi/_cython/./base.pxi"", line 160, in tvm._ffi._cy3.core.CALL      ....    File ""/home/sunxu/下载/未命名文件夹/incubator-tvm/src/relay/ir/error.cc"", line 132  TVMError:   Error(s) have occurred. The program has been annotated with them:    In `main`:   v0.0.4  fn (%i0: Tensor[(1, 3, 224, 224), float32], %v788: Tensor[(32, 3, 3, 3), float32], %v790: Tensor[(32), float32], %v792: Tensor[(32, 1, 3, 3), float32], %v794: Tensor[(32), float32], %v796: Tensor[(16, 32, 1, 1), float32], %v798: Tensor[(16), float32], %v800: Tensor[(96, 16, 1, 1), float32], %v802: Tensor[(96), float32], %v804: Tensor[(96, 1, 3, 3), float32], %v806: Tensor[(96),"
"First of all, thank you for releasing this amazing work + trained models that are not easily obtainable by many researcher!    I do have some conceptual questions regarding your work (the questions themselves may not be that closely related ... ). I hope they make sense:    1. If the objective of TSM is to partially shuffle 2D spatial features for temporal cues, why not considering only inserting temporal shift at the last block/layer of ResNet?     2. (Related to 1). Say at block 1 of ResNet, a partially shifted feature is obtained (which in theory should already take into account temporal cues from multiple frames), then what is the intuition of further shifting at block 2 (3, 4, ... and so on)?    3. In the case of TSM for video object detection, I read from another issue that 8 frames were used to train the model. I wonder, are the groundtruths (class, box coordinates) of ALL 8 frames be considered? Or did you perform certain temporal pooling technique and only take the groundtrurh of a particular frame?     Thank you in advance!"
"When I want to train the TSM model on kinetics, I do not find the file 'kinetics_val.csv', 'kinetics_train.csv'"
"I would like to save the result but when I pass in `--csv_file results.csv` it errors that `test_file_list[0] is None`.    If I pass in `--test_file test_videofolder.txt` then it errors on line 302 at test_models.py: `assert len(vid_names) == len(video_pred)`.     After digging in the code, it seems that video_pred is the number of rows in test_file that have the second value >=3 while vid_names is just all number of rows in test_file, which don't match:       Am I doing something wrong? What is supposed to be passed as test_file?"
"Hi, I was running the test_models on somethingv2 but had to decrease the batch_size from 72 to 12 since I was getting a CUDA issue.    The accuracy is lower as expected, but I just wanted to double check that the results are correct. For TSM ResNet101 | 8 * 2clip I am getting 43.5 compared to your reported 63.3 where you used batch size 72.       Could you just confirm this?"
Your paper mentioned that you used your online TSM for object detection on ImageNet VID.    Could you provide instructions on how to use it exactly with pretrained models?
Can you provide the training script files?  THANKS
tks  tks  tks
"Hi, dear author:     Thank you for the well-done work!     How to extract flow in dataset prepration and set it up for training? I did not see the guides in data preparation ?"
"Hi, I find a bug in the code: ops/models.py  In pytorch, lr_mult, decay_mult is not available parameter, I guess this is because the original code is transformed from caffe which support these paramters.     "
"Thanks for your code, but I have some questions.    Q1：ops/temporal_shift.py Line 107. Is the code an implementation of ""In-place TSM""? and Line 120, is the code an implementation of ""Residual TSM"" in your paper?    Q2：ops/temporal_shift.py Line 130. How is the “n_round” parameter determined? Why use n_round=2 for Resnet101 and deeper Resnets? And why not add TSM before each convolution, but just before the ""n_round"" convolution?    Looking forward to your answer~"
I find that the output of inplace shift is different from the normal shift. is that right?
"input_view = input.view((-1, self.num_segments, self.new_length + 1, input_c,) + input.size()[2:])  RuntimeError: shape '[-1, 16, 6, 3, 18, 224, 224]' is invalid for input of size 231211008    my segments num is 16, and training stage has no error."
Many thanks to the pre-trained models that have been released.  Is there any plan of releasing a checkpoint for ResNet-101 (NL) model trained in Kinetics dataset? 
"I add some code in online_demo/main.py as below:       I test in jester data, but all the pred idx is 21.  Could help me solve the problem? wish..."
None
"Hi,  I am running the online demo on the Jetson TX2 and i find that the result is not really real time. This might be because the released code for the demo does not use TVM autotuning. There seems to be no pretuned logs for the Nano on TVM tophub. Hence i am not able to reproduce the results on the paper.  @tonylins : Could you please let me know how you achieved 25-30 FPS on the Jetson Nano?     Thank you for your help and looking forward to your reply."
"I'm sorry. Although I've looked at other people's questions, I still wonder how to get val_videofolder.txt  "
"hi  ,Thank you very much for uploading the code of TSM. Now I want to trainuni-direction TSM . May I modify the code in this way     def shift(x, n_segment, fold_div=3, inplace=False):          nt, c, h, w = x.size()          n_batch = nt // n_segment          x = x.view(n_batch, n_segment, c, h, w)            fold = c // fold_div          if inplace:              # Due to some out of order error when performing parallel computing.               # May need to write a CUDA kernel.              raise NotImplementedError                # out = InplaceShift.apply(x, fold)          else:              out = torch.zeros_like(x)              out[:, :-1, :fold] = x[:, 1:, :fold]  # shift left              #out[:, 1:, fold: 2 * fold] = x[:, :-1, fold: 2 * fold]  # shift right              #out[:, :, 2 * fold:] = x[:, :, 2 * fold:]  # not shift              out[:, :, fold:] = x[:, :, fold:]  # not shift          return out.view(nt, c, h, w)  "
I trained a model myself. How do I test it online in real time？Should I rewrite a file like mobilenet_v2_tsm.py
"Hello, thanks for your great job!!!  Today when I tried to fine-tuning your model and test the accuracy of your model, something strange happened.  I used uniform sampling, but it seemed that both the pretrained model for sth-sth v1 and kinetics got lower accuracy.    **sth-sth v1**       **kinetics**     The process is stopped for it runs more and more slowly, but it really don't work well......  Are there some errors in the `test_model.py` or the models? Hopefully to your reply."
I see that the TSM `inplace` parameter defaults to `false` and that setting it to `true` yields a `NotImplementedError`.  When will this be fixed?
"Hi,    thank you for sharing your great work and the code!    I would like to use the TSM in an online scenario for gesture recognition, but the camera angle is a bit different than the one used in the jester dataset. So my idea is to create my own dataset and finetune the model on that.    I see that the model architecture for online inference is a bit different than the one for training. Some layers are a `InvertedResidualWithShift` layer instead of a `InvertedResidual` layer. So I guess I can not finetune from the weights provided for the online demo? I actually tried that, but I get errors according to the state dict keys that I can not resolve.    Are you willing to share the weights for the mobilenet_v2 online model for the jester dataset?    If that is not possible, should I then finetune from the kinetics weights? When I try that, I also get an error on the state dict keys. First on almost all keys and I think that is because in the `main.py` file `model = torch.nn.DataParallel(model, device_ids=args.gpus).cuda()` is used before loading the weights. When I move the line to after loading the weights, I get the following error:         There I can set `model.load_state_dict(model_dict, strict=False)` and then the script runs further until it fails at the data loader. I guess the jester dataset changed a bit. Now the description files is a .csv file and does not contain the number of frames. I guess that is expected by the dataloader.      What is the expected format for the dataset?    Looking forward for your reply."
"Hi there,    Thanks for the code and pretrained models, it's been very useful.  I was wondering whether you had any TSN resnet-50 models trained on optical flow on Kinetics that you could share?    Many thanks"
"hi, dear    If I want to get the embedding features from the TSM pretrained model ,  could you please help me ?  thx"
"Hi, Author.  I set the param 'num_segment=16; batch-size=32' and train the Optical Flow Model on Kinetics dataset. The model can  converg, but the training speed is very slow. Do you have the same question? or how to solve? Looking forward to your reply, thank you very much!"
"Hi, will you release the training code for video object detection? thx. "
"Hi Ji,    Thanks for your novel work. I wonder that, have you tried to train MobileNetV2 from scratch (without any pre-trained weight) on Kinetics or UCF-101? Could you share the configs of this setting like lr and bs?    Thanks!"
"Hi, thanks for sharing this work    I'm having a fault when running online_demo code. Here is the error:      Traceback (most recent call last):      File ""main.py"", line 349, in        main()      File ""main.py"", line 323, in main      idx, history = process_output(idx_, history)      File ""main.py"", line 249, in process_output      if not (history[-1] == history[-2]): #  and history[-2] == history[-3]):    IndexError: list index out of range      "
"hi,dear,  for I have no the  dataset, and I just want to use my own dataset ,then how to modify the code in    ,  or you can supply the usage of the script and tell me the what's the kinetics400 configure,  thx  "
"Thank you very much  for your codebase. I have trained my own data  with resnet50 successfully,but I when train it with mobilenet, the accuracy is very low.    python main.py ucf101 RGB --arch mobilenetv2 --num_segments 8 --gd 20 --lr 0.001 --lr_steps 10 20 --epochs 25 --batch-size 2 -j 16 --dropout 0.8 --consensus_type=avg --eval-freq=1 --shift --shift_div=8 --shift_place=blockres      Freezing BatchNorm2D except the first one.  Epoch: [24][0/104], lr: 0.00001 Time 15.333 (15.333) Data 15.214 (15.214) Loss 0.6946 (0.6946) Prec@1 50.000 (50.000) Prec@5 100.000 (100.000)  Epoch: [24][20/104], lr: 0.00001 Time 0.085 (0.815) Data 0.000 (0.725) Loss 0.6946 (0.6896) Prec@1 50.000 (54.762) Prec@5 100.000 (100.000)  Epoch: [24][40/104], lr: 0.00001 Time 0.084 (0.459) Data 0.000 (0.371) Loss 0.6947 (0.6907) Prec@1 50.000 (53.659) Prec@5 100.000 (100.000)  Epoch: [24][60/104], lr: 0.00001 Time 0.086 (0.336) Data 0.000 (0.250) Loss 0.6946 (0.6894) Prec@1 50.000 (54.918) Prec@5 100.000 (100.000)  Epoch: [24][80/104], lr: 0.00001 Time 0.082 (0.274) Data 0.000 (0.188) Loss 0.6391 (0.6893) Prec@1 100.000 (54.938) Prec@5 100.000 (100.000)  Epoch: [24][100/104], lr: 0.00001 Time 0.084 (0.236) Data 0.000 (0.151) Loss 0.6946 (0.6926) Prec@1 50.000 (51.980) Prec@5 100.000 (100.000)  Test: [0/12] Time 2.424 (2.424) Loss 0.7487 (0.7487) Prec@1 0.000 (0.000) Prec@5 100.000 (100.000)  Testing Results: Prec@1 52.174 Prec@5 100.000 Loss 0.69226  Best Prec@1: 52.174    why?   "
"Thank you for the wonderful work.    For Uni-directional TSM for online video detection what is the network backbone used? Resnet 101 or mobilenetV2?  Also can you elaborate on the below lines from the paper. Like how the training and validation is carried out?  I am trying to reproduce the same result.    > We show that we can significantly improve the performance of video detection by simply modifying the backbone with online TSM, without changing the detection module design or using optical flow features    > For TSM experiments, we inserted uni-directional TSM to the backbone, while keeping other settings the same.       And if possible please release the online training script.    "
"I download the pretrained model:  TSM_kinetics_RGB_resnet50_shift8_blockres_avg_segment8_e50.pth  and finetune it on UCF101-split1 using the command below:  '''  python main.py ucf101 RGB \       --arch resnet50 --num_segments 8 \       --gd 20 --lr 0.001 --lr_steps 10 20 --epochs 25 \       --batch-size 64 -j 16 --dropout 0.8 --consensus_type=avg --eval-freq=1 \       --shift --shift_div=8 --shift_place=blockres \       --tune_from=pretrained/TSM_kinetics_RGB_resnet50_shift8_blockres_avg_segment8_e50.pth  '''  but official UCF101 dataset doesn't provide validation dataset, so I split  UCF101-split1 into 9:1 , 9 for training , 1 for validation.    after the training process, I test the model on UCF101-split 1 using the command below:  '''  python test_models.py ucf101\      --weights=checkpoint/TSM_ucf101_RGB_resnet50_shift8_blockres_avg_segment8_e25/ckpt.best.pth.tar \      --test_segments=8 --batch_size=72 -j 24 --test_crops=3  --twice_sample --full_res  '''    and I only get Acc1 93.4%, I want to know what I did wrong, and how can i reproduce your result in the paper Acc1 95.9%.   I really appreciate your reply, thank you very much!"
When I run this command:     I got only 57.85% Overall Prec@1. Looking forward to your reply.
I have 1060ti graphics card 16gb ram i7 processor but Build Executor... is taking more than 5min don't know why plus its using 1 cpu 100% and ram 2 gb even graphics memory 500mb   
I have followed   to install opencv but getting error `aarch64: libgomp.so.1: cannot allocate memory in static TLS block`  `import cv2 in line 8`
Hi!  Thanks for the impressive work with publicly accessible source code here :)   I am trying to train another application for online TSM with different datasets and adding a few adjustments. The repo currently only has the offline version of training script. Is it possible for you also providing the training script for the online TSM?   Thank you very much!
"The pretrained model links are broken. Downloading does not start, the pth file shows up in a webpage as texts.    "
Can you download the pretrained models by the link?
"I am trying to implement the naive version of TSM on one of my model. I can't understand what value should I set for the fold_div.  My tensor x has a shape of (1,8,3,224,224)."
"I use the code(also the script for ucf) train the HMDB51 datasets and the resnet50 pretrained on imagenet. Finally I got 51% top1 Acc, is that reasonable?  I just want to know whether i got a reasomable results. Thank you very much"
None
你好， 我训练4类别数据， 不过发现模型的固定输出类别是3， 请问可能是什么原因呢    mobilenetv2_tsm
"hi， 我训练ucf101时发现：  训练ucf101时__getitem__函数得返回值跟dataloader的返回值不一致    1. 打印__getitem__函数的返回值如下：  `    def __getitem__(self, index):          record = self.video_list   我检查了很多次， 发现两者的返回都是对不上的，请问可能时什么原因呢"
why the network's input shape is 1 24 224 224 but bot 1 8 3 224 224? how to understand 24?
"Hi, I have deployed the projected successfully on the Jeston Nano, but the testing speed is just 0.7vid/s. I noticed that this was no accident. I think it may be caused by onnx. I used onnx==1.6.0 and onnx-simplifier==0.2.18.  Could you offer some suggestion on this issue? Thanks!"
None
None
I got cuda error previously so someone suggested that I should install a lower version of torch and respective torchvision so I chose to install python 1.4 and torchvision v0.5.0 from the link but I am getting the 'transform' module missing error.    Which version of torch and torchvision have you installed?
When trying to download a checkpoint it links to a website that a textual version of the content inside the pth file. I think some of the content is also encoded. Would it be possible to fix this?
All the download links encounter the 404 Not Found error!
This dataset is unavailable now on TwentyBN website. could you share it on drive please? Thanks a lot.
"@JoshNoel Hi, when running mobilenet_v2_tfslim.py, there is an error on line 119  mobilenet_v2_tsm_v2.py is missing in the tsm fpga folder  !   "
"the  show of train only  to offline train , is that mean online train is same to offline train?"
"If any body is getting problem with installing onnxruntime on Jetson devices, Nvidia has uploaded whl file in its       "
I have trained my own model with a custom dataset that is not related to action recognition. I would like to use my best trained model to predict on a single video and get its class. Sort of like how we do usually for 2D CNN. How can I modify the test file? Has anyone managed to do it before?
"I've trained multiple TSM models on my own dataset. The backbone I've used is always a ResNet model. Since ResNet uses CNN layers, it should be possible to visualize the activations with the Grad-CAM method ( ).    However, when I evaluate the same image multiple times through the trained model, the Grad-CAM yields different activation results. This **only** happens with the TSM model, proving that the Grad-CAM implementation does work correctly for non-TSM models.     I'm wondering whether this inconsistency is because I only process a single image, instead of the ""batches"" that TSM samples (and the actual shift that is not performed on a subset of the sampled frames).    Any suggestions or insights on this issue? "
"I have read your code and paper, in your paper, *For testing, when pursue high accuracy, we followed the common setting in  , but I cannot find 10 clips test for kinetics.    I'll appreciate that if you could reply me!    "
how to set  hyper parameter， train mobilenetv2 in UCF101
"Environment:  Anaconda3  Python: 3.7  PyTorch: 1.3.1  torchvision: 0.4.2  cudatoolkit: 11.0  Server: Linux  GPU: 4*Nvidia 2080Ti    Trouble:  After installing the environment and downloading the pretrained models, try to run 'test_tsm_kinetics_rgb_8f.sh' and get some trouble.  When change 'map_location=None' to 'map_location=lambda storage, loc: storage.cuda(1))',something wrong with '_cuda_getDevice' and test  !   !     When change 'map_location=None' to 'map_location={'cuda:1':'cuda:0'})',something wrong with 'torch.cuda.is_available()'.  !   !     If I change map_location to torch.device('cpu'), test will run but cant get a result because I havent upload the test video and labels and annotations to the server. This is not a problem but Im wondering why cant I use gpu to do the test, if I can only run test.sh with cpu then I cant train my own dataset.    Really appreciate any help!  "
"hey   I want to run the online demo in windows.   So, i tried the code as mentioned in #114  @dragen1860 with change mentioned by @gd2016229035   MY CODE (main.py):     I got the following error       I think it has someting to do with the forward func in `mobilenet_v2_tsm.py`   line 93     Please help."
"I am facing this error while training with mobilenet architecture...any solutions..?  -------------------------------------------------------------------------------------------------------------------------------------------------------------------  ucf101: 6 classes  storing name: TSM_ucf101_RGB_mobilenetv2_shift8_blockres_avg_segment8_e25  creating folder log  creating folder checkpoint  creating folder log/TSM_ucf101_RGB_mobilenetv2_shift8_blockres_avg_segment8_e25  creating folder checkpoint/TSM_ucf101_RGB_mobilenetv2_shift8_blockres_avg_segment8_e25        Initializing TSN with base model: mobilenetv2.      TSN Configurations:          input_modality:     RGB          num_segments:       8          new_length:         1          consensus_module:   avg          dropout_ratio:      0.8          img_feature_dim:    256                => base model: mobilenetv2  Downloading: ""  to /root/.cache/torch/hub/checkpoints/mobilenetv2_1.0-f2a8633.pth.tar  100% 13.5M/13.5M [00:00       main()    File ""/content/temporal-shift-module/main.py"", line 65, in main      non_local=args.non_local)    File ""/content/temporal-shift-module/ops/models.py"", line 59, in __init__      self._prepare_base_model(base_model)    File ""/content/temporal-shift-module/ops/models.py"", line 144, in _prepare_base_model      if self.print_spec:    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 772, in __getattr__      type(self).__name__, name))  **torch.nn.modules.module.ModuleAttributeError: 'TSN' object has no attribute 'print_spec**  "
"Hi, maybe I am missing something, but according to log like the below one, looks like when using TSN 2D model, only 1 test_segment, i.e. 1 frame is used? I thought it is supposed to average results from input frames, say 8 frames?         Thanks!"
"!     Hi  all,    Please check the attached error am getting while running the tutorial , Please help me out  how to solve it.    am so new to it.      Regards ,  kaavya"
"Hi, I have succeeded to deploy the online demo on my tx2.  But I found some gestures not easy to recognized. I am not sure whether it is because my gesture is wrong.     I wonder what dataset is used to train the model used in the  demo? I wonder if you could provide some images（sign） of the gesture. Thank you very much!"
None
I get the following error        when running       I'm not sure what it's looking for?
"hi, I want to input a video and predict what type to event occured in the video, how to do that? how do i edit test_models.py  ?  ... anyone .. ?  "
None
i trained the model with ucf101    how to test it ? what should be the parameters?
"I am getting the following accuracy on ucf101 after 7 epochs .... should i be worried ?    Freezing BatchNorm2D except the first one.  Epoch: [7][0/71], lr: 0.00100 Time 2.873 (2.873) Data 0.743 (0.743) Loss 0.0004 (0.0004) Prec@1 100.000 (100.000) Prec@5 100.000 (100.000)  Epoch: [7][20/71], lr: 0.00100 Time 1.726 (1.768) Data 0.000 (0.036) Loss 0.0012 (0.0312) Prec@1 100.000 (98.810) Prec@5 100.000 (100.000)  Epoch: [7][40/71], lr: 0.00100 Time 1.718 (1.742) Data 0.000 (0.018) Loss 0.0025 (0.0176) Prec@1 100.000 (99.390) Prec@5 100.000 (100.000)  Epoch: [7][60/71], lr: 0.00100 Time 1.709 (1.733) Data 0.000 (0.012) Loss 0.0000 (0.0140) Prec@1 100.000 (99.590) Prec@5 100.000 (100.000)  Test: [0/29] Time 1.707 (1.707) Loss 0.0000 (0.0000) Prec@1 100.000 (100.000) Prec@5 100.000 (100.000)  Test: [20/29] Time 0.461 (0.534) Loss 0.3756 (0.0719) Prec@1 87.500 (98.810) Prec@5 100.000 (100.000)  Testing Results: Prec@1 98.673 Prec@5 100.000 Loss 0.10979  Best Prec@1: 100.000"
Dear all:    I wonder what does 8*10 clips means? I know the 8 means 8 frames for each clips. I dont understand what does 10 means? thank you very much.
"Thanks for your opening code.I want to fusion the RGB stream and optical flow stream like the TSN,but I don't know how to save the score when testing ."
"Can someone tell me what should fileList look like so that i can start training my model. i have attached the list i generated,            PS: My rest of the list_file look like the above ."
In your paper you mention that TSM can run on a device like the Samsung Galaxy S8. What changes are needed to setup the online demo on such a device?
hi I am getting the following errror while traning on UCF101.  below are the details ... can someone guide me ...? @tonylins     !   !     
"Could you please provide your training config for I3D NL and TSM NL in this  ? Besides,the training scripts in this repo are not full provided and it is hard to reproduce the results for each setting."
"Herein each of the videos, the total number of frames are divided by 2 when image_tmpl is form  '{:06d}-{}_{:05d}.jpg'. I couldn't understand what's the specific reason behind it?   "
"Dear author:     I noticed the bi-directional model can not be exported directly to Coreml/MNN mobile engines, since it used 5-dim tensors and the Scatter ops. I wonder have you done some modification to the original bi-directional models, or did you use other powerful mobile inference engines？ thank you."
"I test several videos, but the predict time increase linearly, may increase from 200ms to 3s (even more). Then the test script crashed for RecursionError: maximum recursion depth exceeded. I this the reason is out of memory in net. Any suggestions?"
Is it possible to train a new network without TSN? Maybe like EfficientNet.   Not necessarily using this repository but in general.   I guess the question is if TSN is necessary to get the desired performance? 
Dear all:     i have a doubt. What does new_length means? i feel confused. thank you.
"Dear author:    Since the TVM can not run on windows, i just make some modification to `online_demo/main.py` to make it work on windows. However,  i found my code can work normaly ,but no gestures detected at all. the output like this:    !       My modified code is as follows:        You just need to replace this to `online_demo/main.py` and it will run .   Please give me some tips on how to resolve this bug? thank you."
"Dear author:    you make a gesture demo using mobv2, which is really helpful!    I wonder have you tried trainining on other large actions space datasets, such as full sthsthv2 , using mobilenetv2？ could you give some intuitive performance descrption of mobv2 on these datasets? thank you."
"Can you provide your training configs for sth V1 & V2?  I try to train the TSN and TSM in your repo, but I do not find the training config (lr, batch_size, wd, etc). So I just use the setting in the default script, but I didn't get the top1 & top5 score in your modelzoo. Could you provide them?"
"Why is the shift direction implemented in the code opposite to what the paper describes? In the code, some channels are moved up in the frame dimension and then another part is moved down. But the original shift operation described in Figure 1 is to move down and then up. There is a difference between the two, because I experimented with these two schemes and got different accuracy.  !   !   "
learning rate is multiplied by 0.1 while printing in logs(  while printing after each epoch authors are not multiplying it by 0.1(   Is it a typo in there?
"I have succees converting the tsm pytorch to onnx. but the onnx output is different with the pytorch. when converting. it show me that:                 out[:, :-1, :fold] = x[:, 1:, :fold] # shift left              out[:, 1:, fold: 2 * fold] = x[:, :-1, fold: 2 * fold]  # shift right              out[:, :, 2 * fold:] = x[:, :, 2 * fold:] # not shift     I think this may be caused by the shift operation. but I dont think it is a inplace op. it seems that the onnx not support the slice op. "
"Hello, Could you tell me the training script of the TSN ResNet50 (2D) model(1-crop acc 68.8%)? THX."
"If I want to run your code on UCF101, what format should UCF101 be organized into? This confuses me. Hope you can tell me. Otherwise, I will need to spend a lot of time to study how to organize the UCF101 data set to meet the code. Looking forward to your reply, thanks"
environment required
"I was running online demo after build TSM according to readme.md, when I run ""python main.py"",   an error occurs:"" unsupported export of dynamic inputs. The dynamic slice is deprecated experimental op  ,pls change to higher op version"" ,   So I change the default opset version to 10/11/12, however , it says "" keyerror: ' ' "",  it seems there is confliction somewhere between the online demo and onnx, but I could not locate it. Could you tell me how to solve it? Thanks.  My environment is as follows:  pytorch=1.4.0 torchvision=0.5.0 onnx=1.5.0 and I also tried different versions for pytorch and onnx such as pytorch=1.5.0,onnx=1.7.0/1.6.0."
"Thanks for you to public your code ! If  the UCF101 dataset need to extract frames ,the ""tools/vid2img_kinetics.py"" didn't woark .It  just  build  a lot of empty  folders.  can you tell me the version of the  FFmpeg.  I think it the  the key  cuase. "
"Thanks for your great jobs ! When I try to reproduce the result of sth-sth-v1 dataset, I get some  problem. This is my args.txt content:    `Namespace(arch='resnet50', batch_size=20, clip_gradient=20, consensus_type='avg', dataset='something', dense_sample=False, dropout=0.5, epochs=50, eval_freq=1, evaluate=False, flow_prefix='', gpus=[0, 1], img_feature_dim=256, k=3, loss_type='nll', lr=0.01, lr_steps=[20, 40], lr_type='step', modality='RGB', momentum=0.9, no_partialbn=False, non_local=False, num_segments=8, pretrain='imagenet', print_freq=20, resume='', root_log='log', root_model='checkpoint', root_path='', shift=True, shift_div=8, shift_place='blockres', snapshot_pref='', start_epoch=0, store_name='TSM_something_RGB_resnet50_shift8_blockres_avg_segment8_e50', suffix=None, temporal_pool=False, train_list='/media/a0/18A02F4CA02F3024/gt/data/train_videofolder.txt', tune_from=None, val_list='/media/a0/18A02F4CA02F3024/gt/data/val_videofolder.txt', weight_decay=0.0005, workers=16) `    At last, the accuracy of the trained model is 41.6%, I am really confused where is wrong?"
"Hello,     When I try to run the _finetune_tsm_ucf101_rgb_8f.sh_ script, I get the following error:    > Traceback (most recent call last):    File ""main.py"", line 380, in        main()    File ""main.py"", line 196, in main      train(train_loader, model, criterion, optimizer, epoch, log_training, tf_writer)    File ""main.py"", line 247, in train      loss = criterion(output, target_var)    File ""/anaconda3/envs/ml/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 493, in __call__      result = self.forward(*input, **kwargs)    File ""/anaconda3/envs/ml/lib/python3.6/site-packages/torch/nn/modules/loss.py"", line 942, in forward      ignore_index=self.ignore_index, reduction=self.reduction)    File ""/anaconda3/envs/ml/lib/python3.6/site-packages/torch/nn/functional.py"", line 2056, in cross_entropy      return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)    File ""/anaconda3/envs/ml/lib/python3.6/site-packages/torch/nn/functional.py"", line 1871, in nll_loss      ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)  RuntimeError: Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at ../aten/src/THNN/generic/ClassNLLCriterion.c:92    I have encountered this error before in other projects, and back then it meant that I had a mismatch between the amount of classes in my final layer, and the amount of classes that are present in the dataset.    Since I do not have access to the actual model here (at least I could only find that it loads in the PyTorch ResNet), I cannot make these changes.    Anybody had a similar error? and how did you fix it?"
"I've been trying to run with HMDB51, however the split file that is provided on the HMDB website   comes in an entirely different format than what is used in the dataset-config.py for the HMDB51 dataset.     Can anybody help me to get this working? Doesn't seem like it should be difficult since the dataset seems to be working for the authors (and others)?"
"I've been trying to run with HMDB51, however the split file that is provided on the HMDB website   comes in an entirely different format than what is used in the dataset-config.py for the HMDB51 dataset.     Can anybody help me to get this working? Doesn't seem like it should be difficult since the dataset seems to be working for the authors (and others)?"
"Hello, in the online demo, InvertedResidualWithShift returned  return x + self.conv(torch.cat((shift_buffer, x2), dim=1)), x1  but in the offline shift part of MobileNet V2, it returned  return out.view(nt, c, h, w) but without plus initial x, why here is not the same.    "
Neat work!    There is sample command for finetuning on UCF. Would you also share the command or training hyper-parameters for finetuning on HMDB?    Thanks a lot!
I'm currently struggling how I can try to run this with the Charades dataset. Can somebody point me in the right direction?
"I build TVM and other files according to on-line-demo readme, however ,when I run the main.py, an error always occurs: bilinear_sample_nchw could not be imported from ..cpp.util.  I found several ways through Internet, many methods mentioned did not work for me.   Could you tell me how to solve it ? I think this is a basic problem and the temporal-shift-module building should not be so complicated."
"In table 2 of the original paper. The last line shows results of 'RGB+Flow', I want to know how to apply this?  Can I just train 2 models based on RGB and optical flow respectively and then ensemble them?  Thanks a lot!"
"     the model is unavailabel, could you check it?"
"before this, I have changed the code in vid2img_kinetics.py to create dataset of frame format to train TSM and get accuracy of 94.528.   Now I want to train TSM on UCF101 based on optical flow.   In data_config I found img_{:05d}.jpg, flow_{}_{:05d}.jpg:        filename_categories = 'UCF101/labels/classInd.txt'      if modality == 'RGB':          root_data = ROOT_DATASET + 'UCF101/jpg'          filename_imglist_train = 'UCF101/file_list/ucf101_rgb_train_split_1.txt'          filename_imglist_val = 'UCF101/file_list/ucf101_rgb_val_split_1.txt'          prefix = 'img_{:05d}.jpg'      elif modality == 'Flow':          root_data = ROOT_DATASET + 'UCF101/jpg'          filename_imglist_train = 'UCF101/file_list/ucf101_flow_train_split_1.txt'          filename_imglist_val = 'UCF101/file_list/ucf101_flow_val_split_1.txt'          prefix = 'flow_{}_{:05d}.jpg'      else:          raise NotImplementedError('no such modality:' + modality)      return filename_categories, filename_imglist_train, filename_imglist_val, root_data, prefix    However, I haven't found the script to create optical flow dataset.   How can I do this, to create the right optical-flow dataset of ucf101?   Thanks a lot!  "
"hi, is there a reason why the number of segments used in creating the pretrained architectures in the example of tasing TSN vs TSM  is different (8 vs 5)? shouldn't they be the same?  # test TSN  python test_models.py kinetics \      --weights=pretrained/TSM_kinetics_RGB_resnet50_avg_segment5_e50.pth \      --test_segments=8 --test_crops=1 \      --batch_size=64    # test TSM  python test_models.py kinetics \      --weights=pretrained/TSM_kinetics_RGB_resnet50_shift8_blockres_avg_segment8_e50.pth \      --test_segments=8 --test_crops=1 \      --batch_size=64"
"Hi,   this is how you train with TSM:  python main.py ucf101 RGB \       --arch resnet50 --num_segments 8 \       --gd 20 --lr 0.001 --lr_steps 10 20 --epochs 25 \       --batch-size 64 -j 16 --dropout 0.8 --consensus_type=avg --eval-freq=1 \       --shift --shift_div=8 --shift_place=blockres \    is the right way to train without TSM is to remove this line:   -shift --shift_div=8 --shift_place=blockres \  ?  "
"I'm trying to run the following script:   `python test_models.py hmdb51     --weights=pretrained/resnet50.pth     --test_segments=8 --test_crops=1  --batch_size=64`    where resnet50.pth is just the default resnet model as can be found here:     >      However,  when I try to run the above command, I keep on getting the following Traceback:       Which relates to the following code:        I already tried to add map_location='cpu' on line 141 as an argument, but that doesn't help."
"Hello, unfortunately all the links for the pretrained models are not accessible. Could you please fix this issue or provide an alternative way to download the checkpoints?"
I want to reproduce by ncnn
"I've trying to download pre-trained models.  However, all link seem to be blocked.  Please check that if links are available properly.    Thanks."
"Could you show me some examples about how to train a flow model, such as: --lr, --lr_steps, --epochs"
"Hello,     I have installed all the software as described in the instraction on a Jetson Nano. But when I run the main.py in online_demo (pyhton3 main.py) I get a very long error message. (see below).     The file ""nvcc"" seems to be missing. Does anyone know how I can correct the error      File ""main.py"", line 395, in        main()      File ""main.py"", line 287, in main      executor, ctx = get_executor()      File ""main.py"", line 96, in get_executor      return torch2executor(torch_module, torch_inputs, target)      File ""main.py"", line 52, in torch2executor      graph, tvm_module, params = torch2tvm_module(torch_module, torch_inputs, target)      File ""main.py"", line 37, in torch2tvm_module      graph, tvm_module, params = tvm.relay.build(relay_module, target, params=params)      File ""/usr/local/lib/python3.6/dist-packages/tvm-0.7.dev1-py3.6-linux-aarch64.egg/tvm/relay/build_module.py"", line 251, in build      graph_json, mod, params = bld_mod.build(mod, target, target_host, params)      File ""/usr/local/lib/python3.6/dist-packages/tvm-0.7.dev1-py3.6-linux-aarch64.egg/tvm/relay/build_module.py"", line 120, in build      self._build(mod, target, target_host)      File ""tvm/_ffi/_cython/./packed_func.pxi"", line 312, in tvm._ffi._cy3.core.PackedFuncBase.__call__      File ""tvm/_ffi/_cython/./packed_func.pxi"", line 247, in tvm._ffi._cy3.core.FuncCall      File ""tvm/_ffi/_cython/./packed_func.pxi"", line 236, in tvm._ffi._cy3.core.FuncCall3      File ""tvm/_ffi/_cython/./base.pxi"", line 160, in tvm._ffi._cy3.core.CALL    tvm._ffi.base.TVMError: Traceback (most recent call last):    [bt] (8) /usr/local/lib/python3.6/dist-packages/tvm-0.7.dev1-py3.6-linux-aarch64.egg/tvm/libtvm.so(std::_Function_handler , std::allocator  > const&, tvm::runtime::ObjectPtr  const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)+0x2c) [0x7f99ed9bcc]    [bt] (7) /usr/local/lib/python3.6/dist-packages/tvm-0.7.dev1-py3.6-linux-aarch64.egg/tvm/libtvm.so(tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string , std::allocator  > const&, tvm::runtime::ObjectPtr  const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const+0x1ac) [0x7f99ed9ac4]    [bt] (6) /usr/local/lib/python3.6/dist-packages/tvm-0.7.dev1-py3.6-linux-aarch64.egg/tvm/libtvm.so(tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, std::unordered_map , std::allocator  >, tvm::runtime::NDArray, std::hash , std::allocator  > >, std::equal_to , std::allocator  > >, std::allocator , std::allocator  > const, tvm::runtime::NDArray> > > const&)+0xb6c) [0x7f99ed93dc]    [bt] (5) /usr/local/lib/python3.6/dist-packages/tvm-0.7.dev1-py3.6-linux-aarch64.egg/tvm/libtvm.so(tvm::build(tvm::Map , std::allocator  >, tvm::IRModule, void, void> const&, tvm::Target const&, tvm::BuildConfig const&)+0x388) [0x7f99b18978]    [bt] (4) /usr/local/lib/python3.6/dist-packages/tvm-0.7.dev1-py3.6-linux-aarch64.egg/tvm/libtvm.so(tvm::build(tvm::Map  const&, tvm::Target const&, tvm::BuildConfig const&)+0x448) [0x7f99b18120]    [bt] (3) /usr/local/lib/python3.6/dist-packages/tvm-0.7.dev1-py3.6-linux-aarch64.egg/tvm/libtvm.so(tvm::codegen::Build(tvm::IRModule, tvm::Target const&)+0x26c) [0x7f99b46444]    [bt] (2) /usr/local/lib/python3.6/dist-packages/tvm-0.7.dev1-py3.6-linux-aarch64.egg/tvm/libtvm.so(std::_Function_handler ::AssignTypedLambda (tvm::runtime::Module (*)(tvm::IRModule))::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)+0xfc) [0x7f99b90744]    [bt] (1) /usr/local/lib/python3.6/dist-packages/tvm-0.7.dev1-py3.6-linux-aarch64.egg/tvm/libtvm.so(tvm::codegen::BuildCUDA(tvm::IRModule)+0x760) [0x7f99fc09f8]    [bt] (0) /usr/local/lib/python3.6/dist-packages/tvm-0.7.dev1-py3.6-linux-aarch64.egg/tvm/libtvm.so(+0xc60d28) [0x7f9a022d28]    File ""tvm/_ffi/_cython/./packed_func.pxi"", line 55, in tvm._ffi._cy3.core.tvm_callback    File ""/usr/local/lib/python3.6/dist-packages/tvm-0.7.dev1-py3.6-linux-aarch64.egg/tvm/autotvm/measure/measure_methods.py"", line 597, in tvm_callback_cuda_compile      ptx = nvcc.compile_cuda(code, target=target, arch=AutotvmGlobalScope.current.cuda_target_arch)    File ""/usr/local/lib/python3.6/dist-packages/tvm-0.7.dev1-py3.6-linux-aarch64.egg/tvm/contrib/nvcc.py"", line 96, in compile_cuda      cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)    File ""/usr/lib/python3.6/subprocess.py"", line 729, in __init__      restore_signals, start_new_session)    File ""/usr/lib/python3.6/subprocess.py"", line 1364, in _execute_child      raise child_exception_type(errno_num, err_msg, err_filename)  FileNotFoundError: [Errno 2] No such file or directory: 'nvcc': 'nvcc'  "
"Hi,    I tried to test sth-sth v2 pretrained model (TSM ResNet50 | 8 * 2clip), but I got much lower accuracy than that in your paper and Readme. Should I use different arguments?        * Single Clip (59.1 on your paper)    Script:       Test Log:       * 2 Clips (61.2 according to Readme)  Script:       Test Log:       Thank you."
"If you have done it, when will the parameters of TSM-Bniception pretrained on Kinetics be released?"
"Hello! Thanks for your good job!  I have a simple question. How do you caculate the parameters of your model? I have tried but I got 23.9MB, which is same in paper **STM**.   Hopefully to your reply!"
"Hello, thanks for your great job again!!!  I have noticed that in your paper firtly published in arxiv, the accuracy are lower than your results in ICCV. And I find that in this repo, the TSM-Sth-Sth-v1 is tued from the TSM-Kinetics400. Could you please tell me is there any pre-training in Kinetics-400 to get 45.6% top-1 in Sth-Sth V1?   BTW, the pretrained model of Sth-Sth V1 only gets ~20% top1, and I trained it for another epoches to get 45.5% top1......  #65 "
"Hi,    I am trying to run the online demo on the NVIDIA Jetson TX2.     I get the following warning multiple times: … WARNING:autotvm:Cannot find config for target=llvm -target=aarch64-linux-gnu, workload=(‘conv2d’, (1, 3, 224, 224, ‘float32’), (32, 3, 3, 3, ‘float32’), (2, 2), (1, 1), (1, 1), ‘NCHW’, ‘float32’). A fallback configuration is used, which may bring great performance regression. …    I also noticed that the performance is degraded as the warning suggests.    Some info about setup: The model uses MobileNetV2 as backbone, and I am trying to use only the CPU to run the model. I tried modifying the -target argument after finding the target from running “gcc -v” on the device. Also i do not use RPC from the host. I am running the demo on the device.    Can someone tell me how to fix this?"
I'm getting this error when I try to execute python3 main.py within `~/temporal-shift-module/online_demo` folder..       
Hi! Thank you for your code!    I have a question: What is the reason for the resizing to 331-pixel images in vid2img_kinetics.py (line 37)?
"hi, can I get the probability of prediction result for each class?  I have 7 classes, and what I got is like this [-2.3080198764801025,1.7756237983703613,1.0015809535980225,0.9872103929519653,0.1296948343515396,3.5421037673950195,-5.409295558929443]  which is not the probability.  "
"I'm trying to convert .pth file to trace file, and relate code is:          but it give me waring and error：         "
"Hi,  I can tell from main.py that specifying this option with a path to the pretrained weights will reset the final fc layer after loading the weights. My question is will all layers be updated during training or are some layers frozen with the tune-from option?     Thank you"
"Installing everything on a nano with a jetson sd card image r32.2  when launching /onlinedemo/main.py on python3 here the raise error  CUDAError: Check failed: ret == 0 (-1 vs. 0) : cuModuleLoadData(&(module_[device_id]), data_.c_str()) failed with error: CUDA_ERROR_INVALID_PTX    cuda is my path    Thanks for helping    notice that   1/ when making tvm, no /nnvm/python directory is generated.  2/ tried first to install on last release sd card image r32.3.1 and didn't succeed  => on what version of jetson nano sd card you made it worked ?  "
"Appreciated for your great work and kind code sharing!  I notice that there is a complex optimizer policy in the TSN model. A part of that is like:   {'params': first_conv_weight, 'lr_mult': 5 if self.modality == 'Flow' else 1, 'decay_mult': 1,               'name': ""first_conv_weight""},  However, I suppose that the embedded pytorch SGD optimizer cannot identify the parameters like 'lr_mult' and 'decay_mult', which are from Caffe framework. Considering there is no specific function to override the 'step' func in the original SGD class, I deem that those complex optimizer policy is indeed without efficiency.  Please disabuse me if I misunderstand this part.  "
"Can the test results on the somethingv1 dataset and the hyperparameter Settings achieve the accuracy of 47.3% in the paper? Num - segments = 8? Epoch25 isn't enough, is it? My 25 epochs are only 45.98 percent    Python main.py something RGB \    --arch resnet50 --num_segments 8 \    --gd 20 --lr 0.001 --lr_steps 10 20 --epochs 25 \    --batch-size 1-j 16 --dropout 0.5 --consensus_type=avg --eval-freq=1 \    - shift - shift_div = 8 -- shift_place = blockres \    -- tune_from = pretrained/TSM_kinetics_RGB_resnet50_shift8_blockres_avg_segment8_e50. PTH"
"Hi, Author! Thank you for sharing such great jobs! I'm very interesting in your paper. Could you provid the pretrain model or the trainng script on jester dataset?"
"Hi Dr Lin,  Did you train TSM on Kinetics dataset using optical flow as the input modality?  If so, could you please release the pre-trainded model on Kinetics with the input of optical flow?  Thank you!"
"hi,dear  in the script    I find the **archs.small_resnet.ResNet**  but in the   I couldn't find it   So could you help me ?  thx  "
"hi,dear  Do not want test on the txt file's images  Just want to test other images or videos ,so how to ?  pls supply the file  ### kinetics/labels/val_videofolder.txt  or tell me what's on it ?    thx"
"hi, dear,  have tested the test_models.py a liittle difficult to read,  if I just want to test on some movies ,then how should I modify the codes,  thx    any advice or suggsetion will be appreciated.    if have a key argument 'video_path' will be more convenient   do not want test on the txt file below     "
"Thanks for your good work!     I follow TSM early after submit to arxiv first time. And I find the result in older version   Table 2,  `TSM ResNet50  16 65G 24.3M 44.8 74.5`   while in your ICCV paper under the same setting, the result is .   `TSM ResNet50  16 65G 24.3M 47.2 77.1`     While there is no difference in performance between kinetics pretrained model in these two versions, If the reason is using different hyperparmater or training the data more sufficient?  Looking forward to your reply! :)"
This one works     
"Hi, thanks for the code release. In your first version of Arxiv paper,     > We then fine-tuned the model to other target datasets like Something- Something [12], UCF101 [34], and HMDB51 [22]    In the most current version  > For most of the datasets, the model is fine-tuned from ImageNet pre-trained weights; while HMDB-51 [26] and UCF-101 [40] are too small and prone to over-fitting [48], we followed the common practice [48, 49] to fine-tune from Kinetics [25] pre-trained weights and freeze the Batch Normalization [22] layers.     Which dataset is trained using the pre-trained model to get the score reported in the paper? Jester, UCF101 and HDMB? Are the parameters set for Jester and HDMB the same as UCF101?    Thanks again."
"Hi,    Thanks for sharing the mobilenetV2 pretrained weights for online TSM on kinetics.   Can we fine-tune it on HMDB51 dataset the same way mentioned in the GitHub repository? If so what might be the expected accuracy on the smaller data sets?"
"Thanks a lot for the author's code, the effect is amazing. But I have a problem and I am looking forward to helping: I have implemented the real-time pre-processing of the video frame captured by the webcam, and then how to use this model for motion recognition?  Can someone give me some advice, thank you very much!"
"I got into a trouble when I run the online_demo by using my trained model.  My trained model has only 4 classes, but an unexpected error occurred. The error message is as follows.     I have modified catigories as 4 classes.    It seems like a simple bug but I don't have a clue about it."
"Hello！Thanks for your excellent work.I find there are very little 3D works for sthv1/v2 datastes.  I check the leaderboard of sthv2.The top methods nearly all 2D.The performance of 3D is far away 2D works. In fact ,3D conv is proved more suitable for capturing space-time information.The top Acc of UCF/HMDB/Kinetics are all 3D methods.  So what's your opinions about there are fewer 3D works and their Acc are lower on sthv1/v2?  Looking forward to your reply soon.Thanks."
"Hey Hi,    Thank you for your work.   I am trying to train TSM **online version on kinectics with resnet 50** and it has been two days and it has not passed two epocs.     How long did it take to train TSM network from scratch online version for both for resnet 50 and mobilenetv2? I just wanted to make sure if i am in the right path."
"Hi,    Thanks for your amazing work!    I'm new in video analysis, I'm wondering the model performance if you do not load ImageNet pretrained weights? And what if you load pretrained weights on other task dataset, e.g, detecton on MS-COCO?    I did not find you report this issue in your paper or your code, thanks for your help!"
我想试试双向TSM的效果，不知道在哪里修改可以实现？希望得到作者大大的回复，感谢！
None
"I just ran the training script  **""python3 main.py kinetics RGB --arch mobilenetv2 --num_segments 8 --gd 20 --lr 0.02 --wd 1e-4 --lr_steps 20 40 --epochs 50 --batch-size 128 -j 16 --dropout 0.5 --consensus_type=avg --eval-freq=1 --shift --shift_div=8 --shift_place=blockres --npb --gpus 1""**    But i ran into below error.     File ""main.py"", line 249, in train      output = model(input_var)    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 493, in __call__      result = self.forward(*input, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py"", line 146, in forward      ""them on device: {}"".format(self.src_device_obj, t.device))  **RuntimeError: module must have its parameters and buffers on device cuda:1 (device_ids[0]) but found one of them on device: cuda:0**    Also the loss,model,input and target are in GPU . What are the real expected settings? Please let us know  "
None
我没有加载预训练模型，backbone用的resnet50，但是一直不收敛。希望作者大大帮忙解答，谢谢！
"Hi Ji, thank you for publishing the work.    I want to double-check training parameters for training TSM on something-v2 which should achieve at least 58.8 which is the performance I got when testing with your weight. (tested with single crop and single clip)    According to your paper, the training parameters for the something-something-v2 dataset are: 50 training epochs, initial learning rate 0.01 (decays by 0.1 at epoch 20&40), weight decay 1e-4, batch size 64, and dropout 0.5. And the model is fine-tuned from ImageNet pre-trained weights.    However, the script in the git repository indicates that initial learning rate is 0.001, weight decay is 5e-4, and the model is tuned-from Kinetics pre-trained weight.    Due to this disparity, I am confusing which parameters should I use to reproduce the number. Could you provide accurate parameters for training TSM on something-v2?    Thank you."
"when run the repo, it will keep poping the UserWarning:  /pytorch/torch/csrc/autograd/python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example:      os:win7  pytorch:1.2  python:3.5"
"Hi,    Thank you for your work . I was trying to use the TSM module and also check with the reported accuracy but the test_models.py is expecting a val_folder.txt and train_folder.txt(basically train and validation file list).  I tried to download the kinetics 400 data set(download from official code from the script in activity net) but the recent one has so many expired/broken YouTube links . If possible could you please give access to the kinetics data set that you used for training?.     "
"Hi!  Thanks for your interesting work and the source code.  I find that the performance on Sthv1 of TSM with 8-frames and ResNet-50 backbone, efficient test setting is much better than your paper. Have you made any improvements to the original paper?  And can you share the training script on Sthv1  for TSM  with 8-frames and ResNet-50 backbone, which can get the same Top-1 Acc in the GitHub?    Thanks very much"
"I used this command to train the TSM model:     And I got a ckpt.best.pth.tar and ckpt.pth.tar, likely including the model parameter, the model structure information, but the test_models.py only need the parameter. I tried to save the model parameter in ckpt.pth.tar and deleted the lines in test_models.py:      However, I got very low accuracy. Please tell me how to load the parameter rightly. THX."
I think `online_demo` will only work for jetson nano how can i run this on my laptop I install all the packages on laptop getting this error.   
"I'm trying to reproduce the two-stream results of TSM on Something v1, but the performance of my flow model is far below. (segment based sampling method)    I understand the 10 channels stacked optical flow (TV-L1) / learning rate 5 times in the first conv layer.    Is there any difference between RGB and Flow model in setting params?  (e.x, epochs, learning rate...)"
Thanks for your great work!    I tried to run      and encountered the following error message       I solved this problem by changing the line `n_round = 1` in `ops/temporal_shift.py` to `n_round = 2`.
"I got how to finetune TSM in UCF101, but there is not the accuracy posted. Could you tell me the accuracy? THX."
Thanks for the repo! I'm wondering is there a command to train a RGB + Flow model on a pretrained model?
"Hi, thanks for sharing this work    I'm having a segmentation fault when running online_demo code. Here is the error:  ~~~~  UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.    warnings.warn(""The use of the transforms.Scale transform is deprecated, "" +  Build Executor...  Segmentation fault (core dumped)  ~~~~    I found that the program is crashing in line 35 of the main.py file and I'm currently using the LLVM-4.0.0 on ubuntu 16.04.  ~~~~  relay_module, params = tvm.relay.frontend.from_onnx(onnx_model, shape=input_shapes)  ~~~~    Can anyone replicate this problem?    Thanks for your help."
"Congratulations on the great work!    As noted in the supplementary section: ""... we inserted uni-directional TSM to the backbone, while keeping other settings the same. We used the official training code of [60] to conduct the experiments"".    May I ask a few questions on online video object detection:  1. How many frames are used during training? 21 frames the same as FGFA?  2. What is your learning rate policy? and optimizer? the same as FGFA?"
Anyway you can release a Keras version of the Temporal Shift in `ops/temporal_shift.py`. I am looking at implementing this in a custom layer to use with my project.
None
"Hi, Thank you for your TSM code!    But I'm wondering if there is code for resnet50 pretrained models (not the weights)."
"Hello,    I am testing your code on generated images from the pre-trained models you have provided. There are quite a few issues encountered while attempted to run the classifier on this dataset we have creating (testing portion of your code).   - The environment had to be build as CUDA 10.0, Python3.7, and tensorflow-gpu 1.13.1, as previous versions were incompatible  - once run with the pk pre-trained model provided, in file GANFingerprints/classifier/util_scripts.py, line 23 `C_im = misc.load_network_pkl(model_path)` cannot be used as is anymore when executing line 57 `logits = C_im.run(im, minibatch_size=1, num_gpus=1, out_dtype=np.float32)` as this ends up being a tuple. I ended up changing line 23 to `G, D, Gs = misc.load_network_pkl(model_path)` and using `logits = D.run(im, minibatch_size=1, num_gpus=1, out_dtype=np.float32)` to run the classifier.  - upon running the classifier, assignment of labels gets skipped over. This would mean it does not match the labels at the top. What could we use as labels if logits.shape[1] = 1?  - If I use labels as list(labels_1) or list(labels_2) , it classifies everything as from CelebA, which is incorrect as the dataset I am passing in is all fake images generated from sngan model.    Please let me know how I can fix this.  Thanks"
"When I tested it with my own data set, I found it wasn't in lexicographic order。    for exmaple，in util_scripts.py     labels_1 = ['CelebA_real_data', 'ProGAN_generated_data', 'SNGAN_generated_data', 'CramerGAN_generated_data', 'MMDGAN_generated_data'].    But in what order should I use my own data set?What is the rule for this order? Thank you  "
"for example  labels_1 = ['CelebA_real_data', 'ProGAN_generated_data', 'SNGAN_generated_data', 'CramerGAN_generated_data', 'MMDGAN_generated_data']  in util_scripts.py    I've found that if I use a lexicographical order with my own data set, I get an error response  "
"Both CramerGAN and MMDGAN export images with wrong colors:    ! ! !     The error is located in the export images function (equivalent for both GANs):         Values passed to `save_images` are in the range [0,1]. The call to `inverse_transform` rescales them to [0.5, 1], which in turn causes the pixel values to be only in the range [127.5, 255], instead of [0, 255]. This results in colors only from the brighter part of the spectrum, as shown above."
"After carefully reading the author's paper, the idea of using the difference of the offset of different frames to represent the time information is very inspiring to me. I am very interested in your implementation code.When will you share you codes?"
"Hi,  I find your article super cool and really enjoyed reading it!    I am willing to add a second set of tuning blocks in order to check if the interpolation is still working, in order to do that I need to train it, can you please send the data that you've trained your nets on?    Regards,  Noam"
"Hi, I've been having lots of trouble to get this program to work and I keep getting many errors, is there any chance you could provide a more detailed explanation of how to get the program running and what versions of python and dependencies are being used as I think that's what's causing lots of the issues.     `(image) D:\dynamic_net-master\dynamic_style_transfer>python inference.py --network_name=mosaic --use_saved_config=False --set_net_version=normal  Traceback (most recent call last):    File ""inference.py"", line 2, in        from models.inference_model import InferenceModel    File ""D:\dynamic_net-master\dynamic_style_transfer\models\inference_model.py"", line 1, in        from models.base_model import BaseModel    File ""D:\dynamic_net-master\dynamic_style_transfer\models\base_model.py"", line 1, in        import torch    File ""D:\anaconda\envs\image\lib\site-packages\torch\__init__.py"", line 81, in        from torch._C import *  ImportError: DLL load failed: The operating system cannot run %1.`    This is one of the error codes I have got"
"Hi, when I run the demo the main GUI comes up and when I try to select 'take photo' button the photo window can't be closed and the input image is never updated. When I force close the progrm I see the following error:    > File ""[MyFileTree]\dynamic_net\dynamic_style_transfer\gui\webcam_style_transfer_widget.py"", line 23, in on_take_photo_click      >     self.input_image = self.webcam.take_photo()  >   File ""[MyFileTree]\dynamic_net\dynamic_style_transfer\gui\webcam\webcam.py"", line 17, in take_photo  >     cv2.imshow(winname, image)  > cv2.error: OpenCV(3.4.7) C:\projects\opencv-python\opencv\modules\highgui\src\window.cpp:358:   > error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'    Which version of OpenCV are you using and what should be the correct result of pressing 'take photo'?"
"Thank you so much for the fantastic work.    -> I was wondering if the pre-trained models for the other styles would be provided as well and the same for the dual network?  -> The trained_models I believe has only one style available, or is there something I'm missing?    Thank you!"
"Hi,    thank you for your contribution and for releasing your work.    I tried to rerun your model on Cityscape and IDD datasets. What I noticed particularly is that the Unsupervised Loss (US1, US2)  is extremely low in comparison to typical loss. Did you put a weight vector on that Loss or is that a bug? Can you help me to understand this?    Epoch: 50  train loss : 0.7892   S1: 0.3645   S2: 0.3907   US1: 0.0006   US2:0.0004   Total: 0.7892  Best epoch :  Val-IoU-CS= 0.2694  Val-IoU-IDD= 0.2173    Thank you  Best wishes"
"Hi, thanks for sharing your work.  I would like to know what is the self.d_id and self._dataset_index in the EntropyLoss?    Thanks in advance.    Best,"
Thanks for your contribution to this work. The IDD dataset used in this paper is IDD 20k Part I or IDD 20k Part II?
Thanks for releasing the codes. Could you also release the implementation of label embedding?
"First of all, thank you for this wonderful  work and for providing the implementation    I am was interested in seeing how the unsupervised loss was implemented, in the paper, the loss is the entropy of sigmoid(v), the v is the similarity between the embedding of two labels set (either within or across the dataset).    I have two main questions if I may:  - In the implementation I see that we only calculate the entropy between the element products of embeddings directly, is the sigmoid applied before the loss calculation because I see that it was omitted in this line.  `loss_unsup = {d:beta*torch.mean(-1* torch.sum(log_probs * tensor_prob , dim=1)).view(-1)}`    - I see that in the intialization, we initialize the weights of the embedding layer using `torch.randn(NUM_LABELS , em_dim)`, does this help the training and it better than using the initiale weights of the conv layer in the beginning. And during training we don't update the weights of this conv1x1, can you please explain why is that.      Thank you."
"Hi,    Thank you for the interesting paper and sharing the code.   I am currently working on a project based on your implementation, and before publishing it I need to figure out what kind of licence your work is under. Since I understood it's based on open-reid and DomainAdaptiveReID, whose licenses are MIT, I figured yours was also, although I can't find any license information in your project.    Thank you again for your great work,"
你好，刚看完论文，还没来及看代码。对SSG+以及SSG++的训练过程，这点有些疑问。SGG+是不是先利用无监督方法将SSG模型完全收敛以后，利用稳定的聚类来选取label参考集，对target上面每个数据取标签然后再去fine-tuneSSG？
None
"Hi, I am reproducing your work, could you please update the code and fix the error? thanks"
" File ""C:\Project\Self-Similarity-Grouping-master\reid\metric_learning\__init__.py"", line 25, in get_metric      return __factory   TypeError: Can't instantiate abstract class Euclidean with abstract methods get_metric, score_pairs    I meet this question in    Self-Similarity-Grouping-master\reid\metric_learning\__init__.py    line25"
None
写的冗长又完全没有必要，谁会去从解压文件开始
"I cannot find the ""source_train.py"" in this project, where can I find it?"
"To my understanding, should it be ""loss += loss_tri"" instead of ""loss + loss_tri""?"
"Hi, I would like to thank you for releasing the codes in the first place.    Following the readme.md, we directly run the run.sh without any modification, but fail to obtain the reported performance. Can you help us figure out what the problem is?    To be more specific, we obtain Mean AP: 54.3% and top1: 77.3% (best) after training for 30 epoches by running the run.sh for SSG in Duke->Market1501, which should have Mean AP: 58.3% and top1: 80.0%."
你好，有两个问题想要请教一下  1. 由于每张图片根据三个聚类结果有三个标签，代码里是根据第一个标签进行PK采样得到一个batch的图片数据，来进行triplet loss的计算。  但是，如何保证这个batch里的每一张图片在根据后两个聚类结果打上伪标签时，在这个batch中能找到正样本来保证triplet loss的正确计算呢？  2. 在selftraining.py文件中的compute_dist(）函数中采用了源域数据特征来计算距离，怎么解释呢？我看论文里并没有提到
"why used yt as the label for feature ft, ft_up, ft_low,it's different from your paper.  here is the code,calculate the loss Lsemi  ""loss_global_eug, prec_global_eug = self.criterions    for i, output_p in enumerate(outputs_eug  loss_os += loss_tri""  and it is different from loss Lssg,  code here  ""loss_global, prec_global = self.criterions    for i, output_p in enumerate(outputs  loss_uns += loss_tri""  I'm tring to recurrent your paper,pls help me,Thank you"
"you don't need cluster_list on line 366,it takes up 6GB,beacuse cluster.components_ is a is a matrix shape of (15xxx,16552),you can see this in site-packages/sklearn/cluster/dbscan_.py.  I think eps_list can have the same effect.  I don't know if I am right, please tell me"
"I am confused that why computing the distance between the source features and target ones in SSG in your codes. And it is similar to SSG+.  Isn`t it that Self-similarity Grouping is just about target features grouping?  if you could, please point out my mistakes in the comprehension of your paper or code.       thanks for your great works!  "
"Usually rerank is used in one dataset,but in your rerank.py you rerank source feature with target feature? Why? Or my understanding is fault? At this moment, what's the mean of your e_dist and r_dist? Is it still distence between each sample in target datasets?"
"  in you train file       parser.add_argument('--no-rerank', action='store_true', help=""train without rerank"")      parser.add_argument('--dce-loss', action='store_true', help=""train without rerank"")  both args.dce_loss and args.no-rerank default is False, it seems like you default to use rerank, but if I don't want to use rerank how can I set the parameter about --no-rerank and --dce-loss, I can't find the instuction about --dce-loss in your paper"
SGG++:use rerank both in selftraining.py and in semitrain.py  SGG:only  use rerank in selftraining.py and don't train semitrain.py  Is my understanding right?
"Following is my problem, I download your pretrained model from google drive.   I load dukemtmc.pth.tar successful but fall at market1501.  Any suggestion about this? Anyway, I could retrain a market1501 model using source_train.py  Thanks!    Files already downloaded and verified  Market1501 dataset loaded    subset   | # ids | # images    ---------------------------    train    |   676 |    11744    val      |    75 |     1192    trainval |   751 |    12936    query    |   750 |     3368    gallery  |   751 |    15913  Files already downloaded and verified  DukeMTMC dataset loaded    subset   | # ids | # images    ---------------------------    train    |   632 |    14923    val      |    70 |     1599    trainval |   702 |    16522    query    |   702 |     2228    gallery  |  1110 |    17661  Resuming checkpoints from finetuned model on another dataset...    Traceback (most recent call last):    File ""/home/node3/xxx/SSG/selftraining.py"", line 405, in        main(parser.parse_args())    File ""/home/node3/xxx/SSG/selftraining.py"", line 136, in main      checkpoint = load_checkpoint(args.resume)    File ""/home/node3/xxx/SSG/reid/utils/serialization.py"", line 34, in load_checkpoint      checkpoint = torch.load(fpath)['state_dict']    File ""/home/node3/anaconda3/envs/py36_cu10/lib/python3.6/site-packages/torch/serialization.py"", line 386, in load      return _load(f, map_location, pickle_module, **pickle_load_args)    File ""/home/node3/anaconda3/envs/py36_cu10/lib/python3.6/site-packages/torch/serialization.py"", line 573, in _load      result = unpickler.load()  UnicodeDecodeError: 'ascii' codec can't decode byte 0xe8 in position 0: ordinal not in range(128)  "
"Hi, thanks for your excellent work and your release code.  However, I found the DukeMTMC dataset is broken, can you share your data download link?  "
"when I run semitrain.py no matter I use cluster or random it will report :No such file/directory: 'random_split/random_marker1501.pkl', the error code is in eug.py line406:      with open(load_path, ""wb"") as fp:          pickle.dump({""label set"": label_dataset, ""unlabel set"":unlabel_dataset}, fp)    and another question: what's the mean of parser --sample cluster and --sample random"
"When I run the selftraining.py, it is very time-consuming to calculate the source distance and original distance, and why?"
"I run the source_train.py, and there is a error, as follows:  Traceback (most recent call last):    File ""source_train.py"", line 311, in        main(parser.parse_args())    File ""source_train.py"", line 238, in main      top1 = rank_score.allshots[0]  AttributeError: 'numpy.float64' object has no attribute 'allshots'.  Why?"
"I runned the code in Market2Duke and Duke2Market, the result of Duke2Market is a match to the reported numbers while the result of Market2Duke has a drop in performance. The result is showed as below.(I runned on Linux LTS 16.04 with pytorch 0.4.0 and python3.6)  |SSG method| rank-1 |  mAP |  |    reported   |73.0%  |53.4% |  |   observed   |70.2% | 49.8% |    |SSG++ method| rank-1 |  mAP |  |       reported    | 76.0% | 60.3% |  |      observed    | 72.7% | 53.7% |  No change was been made to the training codes, can you please give me some advice about what the reasons probably be?  Thank you."
"hello, when I read your code in source_train.py, I can not find the definition of the function ""get_one_shot_in_cam1"", so I can't not understanding the meaning of the variable ""l_data"" & ""u_data"". Thank you"
"when i run selftraining.py  it shows  ValueError: => No checkpoint found at '/data/liangchen.song/models/torch/trained/model_best.pth.tar' ,can you give me some advices? "
"In the selftraining.py line 276, we can observe that the generate_selflabel only generates the pseudo label once through the pre-trained model rather than every iteration. Is that reasonable? "
"Hi,  Thank you for your great work and the provided code.  I've got a bug while following your suggestion with batch_size=32 and two gpus:     Do you have any clues about the error? And just to be sure, you suggest 2 gpus with batch size=32, you mean batch_size =32 for each gpu or for both two gpus ?    Thank you again,  sincerely,"
"Traceback (most recent call last):    File ""source_train.py"", line 315, in        main(parser.parse_args())    File ""source_train.py"", line 241, in main      rank_score = evaluator.evaluate(val_loader, dataset.val, dataset.val)    File ""/data0/network/SSG-master/SSG-master/reid/evaluators.py"", line 190, in evaluate      features, _ = extract_features(self.model, data_loader, print_freq=self.print_freq)    File ""/data0/network/SSG-master/SSG-master/reid/evaluators.py"", line 28, in extract_features      outputs = extract_cnn_feature(model, imgs, for_eval)    File ""/data0/network/SSG-master/SSG-master/reid/feature_extraction/cnn.py"", line 16, in extract_cnn_feature      outputs = model(inputs, for_eval)[0]    File ""/home/dongwh/anaconda3/envs/SSG/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 489, in __call__      result = self.forward(*input, **kwargs)    File ""/home/dongwh/anaconda3/envs/SSG/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 143, in forward      outputs = self.parallel_apply(replicas, inputs, kwargs)    File ""/home/dongwh/anaconda3/envs/SSG/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 153, in parallel_apply      return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])    File ""/home/dongwh/anaconda3/envs/SSG/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 83, in parallel_apply      raise output    File ""/home/dongwh/anaconda3/envs/SSG/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 59, in _worker      output = module(*input, **kwargs)    File ""/home/dongwh/anaconda3/envs/SSG/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 489, in __call__      result = self.forward(*input, **kwargs)  TypeError: forward() takes 2 positional arguments but 3 were given  "
"   The `table 2` says `trained DukeMTMC-reID dataset and tested  on Market1501 dataset`, but the result in table 2 (mAP: 53.4, R1: 73.0) actually is the result of `Market1501 → DukeMTMC-ReID` in table 1"
"why used yt as the label for feature ft,   ft_up,   ft_low,it's different from your paper.  here is the code,calculate the loss Lsemi  `loss_global_eug, prec_global_eug = self.criterions   for i, output_p in enumerate(outputs_eug                   loss_os += loss_tri`  and it is different from loss Lssg,  code here  `loss_global, prec_global = self.criterions   for i, output_p in enumerate(outputs                   loss_uns += loss_tri`  I'm tring to recurrent your paper,pls help me,Thank you"
None
"Hi; thank you for sharing  But your code has a lot of mistake; I'm inclined to say; it can't execute. Except we modified them. Pls, make sure you upload a running code out of the box?  For example; here; you used `cross_entropy` without `self.cross_entropy`.        in reid/loss/triplet.py L35, the code is unreachable, you used `if False`"
"Hi,    I run the code and got the following error.     Traceback (most recent call last):    File ""source_train.py"", line 311, in        main(parser.parse_args())    File ""source_train.py"", line 237, in main      rank_score = evaluator.evaluate(val_loader, dataset.val, dataset.val)    File ""/media/mahfuj/DATA/preid/SSG-master/reid/evaluators.py"", line 190, in evaluate      features, _ = extract_features(self.model, data_loader, print_freq=self.print_freq)    File ""/media/mahfuj/DATA/preid/SSG-master/reid/evaluators.py"", line 28, in extract_features      outputs = extract_cnn_feature(model, imgs, for_eval)    File ""/media/mahfuj/DATA/preid/SSG-master/reid/feature_extraction/cnn.py"", line 16, in extract_cnn_feature      outputs = model(inputs, for_eval)[0]    File ""/home/mahfuj/pytorch_new_python3/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 477, in __call__      result = self.forward(*input, **kwargs)    File ""/home/mahfuj/pytorch_new_python3/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py"", line 121, in forward      return self.module(*inputs[0], **kwargs[0])    File ""/home/mahfuj/pytorch_new_python3/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 477, in __call__      result = self.forward(*input, **kwargs)  TypeError: forward() takes 2 positional arguments but 3 were given  "
"Hi, I again got a problem when running the source_train.py    Files already downloaded and verified  Market1501 dataset loaded    subset   | # ids | # images    ---------------------------    train    |   676 |    11744    val      |    75 |     1192    trainval |   751 |    12936    query    |   750 |     3368    gallery  |   751 |    15913  Traceback (most recent call last):    File ""/home/node3/xxxxxx/SSG/source_train.py"", line 311, in        main(parser.parse_args())    File ""/home/node3/xxxxxx/SSG/source_train.py"", line 196, in main      metric = DistanceMetric(algorithm=args.dist_metric)    File ""/home/node3/xxxxxx/SSG/reid/dist_metric.py"", line 13, in __init__      self.metric = get_metric(algorithm, *args, **kwargs)    File ""/home/node3/xxxxxx/SSG/reid/metric_learning/__init__.py"", line 25, in get_metric      return __factory   TypeError: Can't instantiate abstract class Euclidean with abstract methods get_metric, score_pairs    Thanks for your kindly reply!"
None
None
None
"I follow your step like this:  git clone    $ cd inplace_abn  $ python setup.py install  File ""/public/home/cxj123456/anaconda3/envs/tcx/lib/python3.8/site-packages/torch/utils/cpp_extension.py"", line 1354, in _write_ninja_file_and_compile_objects      _run_ninja_build(    File ""/public/home/cxj123456/anaconda3/envs/tcx/lib/python3.8/site-packages/torch/utils/cpp_extension.py"", line 1683, in _run_ninja_build      raise RuntimeError(message) from e  RuntimeError: Error compiling objects for extension  errors happened,but I don't know how to install it correctly.thanks"
"excuse me . Is there an another method? the torch.diag methon isn't supported by onnx version and when i use torch.eye ,my tensorrt doesn‘t support it either."
"Hello, how does RCCA compare with nolocal in image classification tasks? Is the effect improved?"
看代码怎么感觉和论文中结构不一样啊，没看见有WH×（H+W-1）的结构啊
"** !   **I have some questions about 'Criss-Cross Attention'. Here should be C×W×H or C×H×W in the place marked by the red circle？**    !   **At the same time,** **I have some questions about Affinity.** ****Which variables corresponds to the codes in the place marked by the blue line，proj_query_H and proj_key_H , or proj_query_W and proj_key_W?** I don’t understand the definition of Qu and Ωu very well. Is it convenient for you to answer? **Looking forward to your answer, thank you very much!****      "
Hello，thanks for your code.Do you have a  CCA written by tensorflow version?
"Hello, your cc_ attention have conv1d?"
why use INF function?i want to know that.
"I noticed that the trained models in README using pytorch 0.4.1 and I want to evaluate the model to make sure my envirnment is ok when using pytorch 1.5+.Since I did not have enough memories to train the model by myself with res101.I try to replace res101 by res18 which performance is far away from the paper.So,I want to get the trained models with pytorch 1.5+ to reproduce the performance.Thanks!"
"您好，我打算将您公布的pytorch版本的RCCA模块应用到视频的不同帧之间，以获得帧与帧之间的注意力进而增强视频帧的特征表示。主要问题是loss没有完全收敛，维持在1-2中间。我想排除一下是不是我网络改的有问题，需要您的帮助！！！    主要任务是视频的显著性检测，取同一视频中任意两帧经过同一ResNet-101，获得 B x 256 x 47 x 47的特征，然后再输入到RCCA模块，先得到 Q_X , K_X , V_X , Q_Y, K_Y, V_Y,即得到两帧映射到Q,K,V空间的特征。然后再用 Q_X 和 K_Y 做相关性矩阵，作用到V_Y，然后是Q_Y 和 K_X 做相关性，作用到 V_X。 代码的实现如下，几乎没怎么改动，希望您能帮我看一眼，或者能提供给我一点训练或者调参的建议吗？感谢！！！    `class RCCAModule(nn.Module):      def __init__(self, in_channels, out_channels = 256):          super(RCCAModule, self).__init__()            #inter_channels = in_channels // 4              self.cca = CrissCrossAttention(in_channels)            self.convbX = nn.Sequential(nn.Conv2d(in_channels, in_channels, 3, padding=1, bias=True),                                     nn.BatchNorm2d(in_channels))            self.convbY = nn.Sequential(nn.Conv2d(in_channels, in_channels, 3, padding=1, bias=True),                                      nn.BatchNorm2d(in_channels))            self.bottleneckX = nn.Sequential(              nn.Conv2d(in_channels * 2, out_channels, kernel_size=3, padding=1, dilation=1, bias=True),              nn.BatchNorm2d(out_channels),              #nn.Dropout2d(0.1),  # dropout在这也会有用吗？？              )            self.bottleneckY = nn.Sequential(              nn.Conv2d(in_channels * 2, out_channels, kernel_size=3, padding=1, dilation=1, bias=True),              nn.BatchNorm2d(out_channels),              #nn.Dropout2d(0.1),  # dropout在这也会有用吗？？              )            def forward(self, x, y, recurrence=2):          #outputX = self.convaX(x)          #outputY = self.convaY(y)          outputX = x          outputY = y          for i in range(recurrence):              outputX, outputY = self.cca(outputX, outputY)            outputX = self.convbX(outputX)          outputY = self.convbY(outputY)            outputX = self.bottleneckX(torch.cat([x, outputX], 1))          outputY = self.bottleneckY(torch.cat([y, outputY], 1))            return outputX, outputY`      `  import torch  import torch.nn as nn  import torch.nn.functional as F  from torch.nn import Softmax      def INF(B,H,W):        return -torch.diag(torch.tensor(float(""inf"")).cuda().repeat(H),0).unsqueeze(0).repeat(B*W, 1, 1)  # 主对角线为-inf的矩阵      class CrissCrossAttention(nn.Module):      """""" Criss-Cross Attention Module""""""      def __init__(self, in_dim):          super(CrissCrossAttention,self).__init__()          #　下面三个是转成Q,K,V之前的降维，V不变          self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//2, kernel_size=1)          self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//2, kernel_size=1)          self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)          self.softmax = Softmax(dim=3)          self.INF = INF          self.gamma1 = nn.Parameter(torch.zeros(1)) # 虽然初始化为0了，但是它是一个可以学习的参数，当插入在模型中时，最开始可以保证从          self.gamma2 = nn.Parameter(torch.zeros(1))          # self.gamma2 = torch.zeros(1).cuda().requires_grad_()            # ImageNet上学来的特征，然后再慢慢学习，会得到一个值，这可以使得整个训练过程更加的平滑          def forward(self, x, y):            m_batchsize, _, height, width = x.size()  # B x 2C x H x W ,m_batchsize = 2, _ = 256, height = 47, width = 47          proj_query_X = self.query_conv(x) # 降维,我改成了128，即降维一半， B,C,H,W          proj_query_X_H = proj_query_X.permute(0,3,1,2).contiguous().view(m_batchsize*width,-1,height).permute(0, 2, 1) # BW,H,C          proj_query_X_W = proj_query_X.permute(0,2,1,3).contiguous().view(m_batchsize*height,-1,width).permute(0, 2, 1) # BH,W,C          proj_key_X = self.key_conv(x) # 降维  B,C,H,W          proj_key_X_H = proj_key_X.permute(0,3,1,2).contiguous().view(m_batchsize*width,-1,height) # 12,8,5, BW,C,H          proj_key_X_W = proj_key_X.permute(0,2,1,3).contiguous().view(m_batchsize*height,-1,width) # 10,8,6, BH,C,W          proj_value_X = self.value_conv(x)  # 2,64,5,6 就是没有降维而已          proj_value_X_H = proj_value_X.permute(0,3,1,2).contiguous().view(m_batchsize*width,-1,height) # 12,64,5 BW,2C,H          proj_value_X_W = proj_value_X.permute(0,2,1,3).contiguous().view(m_batchsize*height,-1,width) # 10,64,6 BH,2C,W              proj_query_Y = self.query_conv(y) # 降维 B,C,W,H          proj_query_Y_H = proj_query_Y.permute(0,3,1,2).contiguous().view(m_batchsize*width,-1,height).permute(0, 2, 1) # BW,H,C          proj_query_Y_W = proj_query_Y.permute(0,2,1,3).contiguous().view(m_batchsize*height,-1,width).permute(0, 2, 1) # BH,W,C          proj_key_Y = self.key_conv(y) # 降维  B,C,W,H          proj_key_Y_H = proj_key_Y.permute(0,3,1,2).contiguous().view(m_batchsize*width,-1,height) # 12,8,5, BW,C,H          proj_key_Y_W = proj_key_Y.permute(0,2,1,3).contiguous().view(m_batchsize*height,-1,width) # 10,8,6, BH,C,W          proj_value_Y = self.value_conv(y)  # 2,64,5,6 就是没有降维而已          proj_value_Y_H = proj_value_Y.permute(0,3,1,2).contiguous().view(m_batchsize*width,-1,height) # 12,64,5 BW,2C,H          proj_value_Y_W = proj_value_Y.permute(0,2,1,3).contiguous().view(m_batchsize*height,-1,width) # 10,64,6 BH,2C,W            A = torch.bmm(proj_query_X_H, proj_key_Y_H)          B = self.INF(m_batchsize, height, width)          C = A+B          # BW,H,H的注意力图中每一列包含了查询帧中的每一个H信息，BH,W,W同理          energy_X_H = (torch.bmm(proj_query_X_H, proj_key_Y_H)+self.INF(m_batchsize, height, width)).view(m_batchsize,width,height,height).permute(0,2,1,3) # B,H,W,H          energy_X_W = torch.bmm(proj_query_X_W, proj_key_Y_W).view(m_batchsize,height,width,width)  # B,H,W,W          concateX = self.softmax(torch.cat([energy_X_H, energy_X_W], 3))  # B,H,W,H+W            att_X_H = concateX[:,:,:,0:height].permute(0,2,1,3).contiguous().view(m_batchsize*width,height,height)  # BW,H,H          att_X_W = concateX[:,:,:,height:height+width].contiguous().view(m_batchsize*height,width,width)  # BH,W,W            # 与X一样          energy_Y_H = (torch.bmm(proj_query_Y_H, proj_key_X_H)+self.INF(m_batchsize, height, width)).view(m_batchsize,width,height,height).permute(0,2,1,3)          energy_Y_W = torch.bmm(proj_query_Y_W, proj_key_X_W).view(m_batchsize,height,width,width)  # 2,5,6,6          concateY = self.softmax(torch.cat([energy_Y_H, energy_Y_W], 3))  # 2,5,6,11            att_Y_H = concateY[:,:,:,0:height].permute(0,2,1,3).contiguous().view(m_batchsize*width,height,height)  # 12,5,5          att_Y_W = concateY[:,:,:,height:height+width].contiguous().view(m_batchsize*height,width,width)  # 10,6,6          # 因为这边permute()相当于做了个转置，所以应当是每一行，包含了查询帧中的每一个H信息          out_Y_H = torch.bmm(proj_value_Y_H, att_X_H.permute(0, 2, 1)).view(m_batchsize,width,-1,height).permute(0,2,3,1)  # 2,64,5,6          out_Y_W = torch.bmm(proj_value_Y_W, att_X_W.permute(0, 2, 1)).view(m_batchsize,height,-1,width).permute(0,2,1,3)  # 2,64,5,6            out_X_H = torch.bmm(proj_value_X_H, att_Y_H.permute(0, 2, 1)).view(m_batchsize,width,-1,height).permute(0,2,3,1)  # 2,64,5,6          out_X_W = torch.bmm(proj_value_X_W, att_Y_W.permute(0, 2, 1)).view(m_batchsize,height,-1,width).permute(0,2,1,3)  # 2,64,5,6              return (self.gamma1 * (out_X_H + out_X_W) + x), (self.gamma2 * (out_Y_H + out_Y_W) + y)  `    另外这部分的初始化，卷积层为kaiming初始化，偏置0。BN层权重设为1，偏置0."
"   I think the overlapped elements should be masked out twice for 3d cc attention module, if  your paper describes 3d version as below.  !   I wrote 3   of inplementation for it. You can check my inplementations."
"Section 3.4 in your TPAMI version paper,  **_Learning Category Consistent Features_**   Is there any code for this part ? "
"The original author wasn't sure what it was.    <img width=""708"" alt=""Screen Shot 2021-02-07 at 10 48 15 pm"" src=""   "
"Hello, i use the command 'python3 train.py --data-dir ./dataset/cityscapes/ --random-mirror --random-scale --restore-from ./dataset/resnet101-imagenet.pth --gpu 4,5,6,7 --learning-rate 1e-2 --input-size 769,769 --weight-decay 1e-4 --batch-size 8 --num-steps 60000 --recurrence 2', and has some wrong as follows:  /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.    warnings.warn(warning.format(ret))  481950 images are loaded!  Traceback (most recent call last):    File ""train.py"", line 245, in        main()    File ""train.py"", line 209, in main      preds = model(images, args.recurrence)    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 477, in __call__      result = self.forward(*input, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py"", line 123, in forward      outputs = self.parallel_apply(replicas, inputs, kwargs)    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py"", line 133, in parallel_apply      return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/parallel_apply.py"", line 77, in parallel_apply      raise output    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/parallel_apply.py"", line 53, in _worker      output = module(*input, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 477, in __call__      result = self.forward(*input, **kwargs)    File ""/home/cc/networks/ccnet.py"", line 196, in forward      x = self.relu1(self.bn1(self.conv1(x)))    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 477, in __call__      result = self.forward(*input, **kwargs)    File ""/home/cc/libs/bn.py"", line 184, in forward      self.activation, self.slope)    File ""/home/cc/libs/functions.py"", line 183, in forward      _check(_ext.bn_mean_var_cuda, x, mean, var)    File ""/home/cc/libs/functions.py"", line 16, in _check      raise RuntimeError(""CUDA Error encountered in {}"".format(fn))  RuntimeError: CUDA Error encountered in    Can you give some  suggestions?  @speedinghzl @honghuis "
"Hi I am having this problem while running I have built the build file by installing the setup.py, but while running the command   ""./run_local.sh YOUR_CS_PATH ccnet 60000 256,256 [0|1]"". But I am geeting this error of   extra_cuda_cflags= [""--expt-extended-lambda""]). My log is printed here.    1]: command not found  Traceback (most recent call last):    File ""train.py"", line 14, in        from networks.ccnet import ResNet    File ""/home/srutsth/PycharmProjects/PolypCCNet/networks/ccnet.py"", line 11, in        from cc_attention.functions import CrissCrossAttention    File ""/home/srutsth/PycharmProjects/PolypCCNet/cc_attention/functions.py"", line 20, in        extra_cuda_cflags= [""--expt-extended-lambda""])    File ""/home/srutsth/miniconda3/envs/myenv/lib/python3.6/site-packages/torch/utils/cpp_extension.py"", line 644, in load      is_python_module)    File ""/home/srutsth/miniconda3/envs/myenv/lib/python3.6/site-packages/torch/utils/cpp_extension.py"", line 813, in _jit_compile      with_cuda=with_cuda)    File ""/home/srutsth/miniconda3/envs/myenv/lib/python3.6/site-packages/torch/utils/cpp_extension.py"", line 847, in _write_ninja_file_and_build      verbose)    File ""/home/srutsth/miniconda3/envs/myenv/lib/python3.6/site-packages/torch/utils/cpp_extension.py"", line 904, in _prepare_ldflags      print('Detected CUDA files, patching ldflags')          "
"Hello Everyone!  I am trying to find the code to generate figure 6 in the paper, but I am not able to find the piece of code which can generate figure 6 in the paper.  Can anyone help in this issue?"
"Dear authors,    I have created a version of CCNet that can work with Pytorch >= 1.5. Please consider adding this branch to your repo.        Thank you!"
"Thanks for sharing. I compile out RuntimeError: Error building extension 'rcca'"" . Could you please give me some suggestions?"
"Observed that mIoU is not improving beyond **28%** on  Cityscapes. (on the branch pytorch-1.1)    Changes made in run configs  * Reduced batch size to 4  * Number of GPUs is 2  * Reduced input size to 512*512  * Trained for 1,20,000 iterations  !     I trained in different phases.  For eg. trained the model for 20,000 iterations. Evaluated the model and also **recorded the learning rate** after 20,000 iterations    Then  restart training  the model with   `--restore-from ./snapshots/CS_scenes_20000.pth`  `LR={learning rate after 20000 iterations}`    Am I missing something?"
" class CrissCrossAttention(nn.Module):      """""" Criss-Cross Attention Module""""""      def __init__(self,in_dim):          super(CrissCrossAttention,self).__init__()          self.chanel_in = in_dim            self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)          self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)          self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)          self.gamma = nn.Parameter(torch.zeros(1))        def forward(self, x):          proj_query = self.query_conv(x)          proj_key = self.key_conv(x)          proj_value = self.value_conv(x)            energy = ca_weight(proj_query, proj_key)          attention = F.softmax(energy, 1)          out = ca_map(attention, proj_value)          out = self.gamma*out + x          return out   Thanks for your reply. I am confused with self.gamma is zero?"
"请问数据集是要下载哪几个文件? 下载完后直接解压吗, 目录需要怎样组织? 新手求教"
None
"from torch.utils.ffi import _wrap_function  from _ext import lib as _lib, ffi as _ffi     These two pieces of code always report errors, what can I do, can you help me?"
"Hi,    In the RCCA, you used a single attention layer multiple times. I would like to know if you have tried different attention layers for each recurrent step (i.e. one `CrissCrossAttention` per step). I wonder if this can improve the results...    Thank you in advance!"
"I run the code branch CCNet-pytorch1.1, my environments are:  pytorch 1.4  torchvision 0.5.0  cuda 10.2  python 3.6  However, this error happend:  root@3178500507b3:/workspace/CODE/CCNet-pytorch-1.1# python3 train.py  Detected CUDA files, patching ldflags  Emitting ninja build file /workspace/CODE/CCNet-pytorch-1.1/cc_attention/build/build.ninja...  Building extension module rcca...  ninja: no work to do.  Loading extension module rcca...  Traceback (most recent call last):    File ""train.py"", line 17, in        import networks    File ""/workspace/CODE/CCNet-pytorch-1.1/networks/__init__.py"", line 3, in        import networks.ccnet    File ""/workspace/CODE/CCNet-pytorch-1.1/networks/ccnet.py"", line 13, in        from cc_attention import CrissCrossAttention    File ""/workspace/CODE/CCNet-pytorch-1.1/cc_attention/__init__.py"", line 1, in        from .functions import CrissCrossAttention, ca_weight, ca_map    File ""/workspace/CODE/CCNet-pytorch-1.1/cc_attention/functions.py"", line 24, in        extra_cuda_cflags=[""--expt-extended-lambda""])    File ""/usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py"", line 680, in load      is_python_module)    File ""/usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py"", line 877, in _jit_compile      return _import_module_from_library(name, build_directory, is_python_module)    File ""/usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py"", line 1088, in _import_module_from_library      return imp.load_module(module_name, file, path, description)    File ""/usr/lib/python3.6/imp.py"", line 243, in load_module      return load_dynamic(name, filename, file)    File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic      return _load(spec)  ImportError: libcudart.so.9.2: cannot open shared object file: No such file or directory   Could you help me what should I do?"
None
"I have some trouble when reproducing this code on the ADE20K dataset. When testing the mIoU , I got extremely low result. Could you please share the code for ADE20K?"
How to compute the Memory usage of RCCA?  Could you please share the details?  Thanks a lot !
"The function `rcca.ca_forward_cuda` seems doesn't support mixed-precision training and raise error:     RuntimeError: expected scalar type Float but found Half (data  at /data/.conda/envs/pytorch/lib/python3.5/site-packages/torch/include/ATen/core/TensorMethods.h:1821)     when the **opt_level** of NVIDIA APEX is set to ""O1""    While it goes well when I shift **opt_level** to ""O0"".    So it should be the reason of data type."
"so nice job, but when I visual the attention map follow the #3 , however, I get the bellow result, someone can help me? Thanks  !   "
"这里多尺度预测说对于大于原图的采用裁剪的方式，请问是否也采用的滑动窗口预测呢？但是代码里没有滑动窗口的代码啊  def predict_multiscale(net, image, tile_size, scales, classes, flip_evaluation, recurrence):      """"""      Predict an image by looking at it with different scales.          We choose the ""predict_whole_img"" for the image with less than the original input size,          **for the input of larger size, we would choose the cropping method to ensure that GPU memory is enough**.      """"""      image = image.data      N_, C_, H_, W_ = image.shape      full_probs = np.zeros((H_, W_, classes))        for scale in scales:          scale = float(scale)          print(""Predicting image scaled by %f"" % scale)          scale_image = ndimage.zoom(image, (1.0, 1.0, scale, scale), order=1, prefilter=False)          scaled_probs = predict_whole(net, scale_image, tile_size, recurrence)          if flip_evaluation == True:              flip_scaled_probs = predict_whole(net, scale_image[:,:,:,::-1].copy(), tile_size, recurrence)              scaled_probs = 0.5 * (scaled_probs + flip_scaled_probs[:,::-1,:])          full_probs += scaled_probs      full_probs /= len(scales)      return full_probs"
您好，我想请问下您的attention module的可视化图是怎样得出来的？是否也包含在代码里？谢谢
"   - I do not know why return [x, x_dsn]? why not return (torch.cat(x, x_dsn))"
"Hi, @speedinghzl ,      I try to train on Cityscapes dataset using pytorch 0.4.0, but I got an error as follows:    Traceback (most recent call last):    File ""train.py"", line 253, in        main()    File ""train.py"", line 217, in main      preds = model(images, args.recurrence)    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 491, in __call__      result = self.forward(*input, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py"", line 112, in forward      return self.module(*inputs[0], **kwargs[0])    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 491, in __call__      result = self.forward(*input, **kwargs)    File ""/userhome/segmentation/CCNet/networks/ccnet.py"", line 196, in forward      x = self.relu1(self.bn1(self.conv1(x)))    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 491, in __call__      result = self.forward(*input, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py"", line 301, in forward      self.padding, self.dilation, self.groups)  RuntimeError: CUDNN_STATUS_MAPPING_ERROR    Could you give me some advise? Thanks"
"I noticed that there is no test.py  in pytorch1.1 branch.  Can you add test.py to support pytorch1.1 branch?    Thanks,"
"Hello All,    I can't access the resnet101-imagenet.pth.   When i click the link it doesn't work. Could you please share any alternative procedure.     Kind regards      Please download MIT imagenet pretrained resnet101-imagenet.pth, and put it into dataset folder."
"my pytorch 0.4.1 with  cuda 9.0   and i use voc dataset, the image can be read correctly , but got some problem in forward     i debug the code :        def forward(self, x, recurrence=1):          x = self.relu1(self.bn1(self.conv1(x)))          x = self.relu2(self.bn2(self.conv2(x)))          x = self.relu3(self.bn3(self.conv3(x)))          x = self.maxpool(x)          x = self.layer1(x)          x = self.layer2(x)          x = self.layer3(x)          print('self.layer3:', x.shape)          x_dsn = self.dsn(x)          print('self.dsn:', x.shape)          x = self.layer4(x)          x = self.head(x, recurrence)          return [x, x_dsn]    the output is followed:    xxxxxxxxxxxxx     (bottleneck): Sequential(        (0): Conv2d(2560, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (1): InPlaceABNSync(512, eps=1e-05, momentum=0.1, affine=True, devices=[0], activation=leaky_relu slope=0.01)        (2): Dropout2d(p=0.1)        (3): Conv2d(512, 21, kernel_size=(1, 1), stride=(1, 1))      )    )    (dsn): Sequential(      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))      (1): InPlaceABNSync(512, eps=1e-05, momentum=0.1, affine=True, devices=[0], activation=leaky_relu slope=0.01)      (2): Dropout2d(p=0.1)      (3): Conv2d(512, 21, kernel_size=(1, 1), stride=(1, 1))    )  )  /home/yuxi.xt/software/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.    warnings.warn(warning.format(ret))  self.layer3: torch.Size([4, 1024, 97, 97])  Segmentation fault    it means : x_dsn = self.dsn(x) can not be processed    how can i solve it ?"
"Hi guys!    I configured my pyenv with python3.6.5 and torch0.4.1. After the complie, I got the following error when I tried to train the model:    File ""/home/zhourong/.pyenv/versions/env3.6.5-pytorch0.4.1/lib/python3.6/site-packages/torchvision/models/inception.py"", line 8, in    from torch.jit.annotations import Optional  ImportError: cannot import name 'Optional'    I am using cuda9.0, is this the problem.  Does anyone else get this problem as well?     Thanks in advance!    "
@speedinghzl  thanks for sharing the code and a wonderful just have few queries     1. can ccnet be extended to perform instance segmentation feature   2.what is the performance speed on cpu and gpu     thanks in advance 
"I have set batch_size = 1 and input_size = 713,713.  Use the following command:  python train.py --data-dir ．/dataset/image/ --random-mirror --random-scale --restore-from ./dataset/resnet101-imagenet.pth --gpu 1,3.6 --learning-rate 1e-2 --input-size 713,713 --weight-decay 1e-4 --batch-size 1 --num-steps 60000 --recurrence 2 --ohem 1 --ohem-thres 0.7 --ohem-keep 100000    However, the following situations occur:   UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.    warnings.warn(warning.format(ret))  62475 images are loaded!    Then it stuck here！！！！Do you know why?　What do I need to do to make it work?    "
"error shows that  cffi doesnt work in pytorch1.x,"
"After read the train.py and evaluate.py, I don't find a option that turn torch.distributed off.(I try to run your code in one GPU)"
"Hello, thank you for sharing your impressive work!  However, I got an issue on running your code.  Deadlock always occurs at loss.backward() of second iteration(batch index 1).  I think this is multiprocessing issue, isn't it?  My torch version is 0.4.1 and python version is 3.6.1  I hope this issue be resolved soon! "
"Hello, Why is my program not outputting after outputting a taking snapshot, and the program cannot end?  I use two k80, and the build.sh does not report any error. But when printing 'taking snapshot', the program cannot show anything.  !   "
"Hi, thanks for the great work and code.    When I try to build the code, the build.sh is fine but when I run the build.py, an error occurs:  ""ImportError: torch.utils.ffi is deprecated. Please use cpp extensions instead.""    I'm currently using pytorch 1.2.0. Can you please let me know some solution for this problem? I have searched some solutions but all of them failed.    Thanks."
None
When I use     ` sh build.sh`    then I have many error about cuda version.  I checked the version of cuda and found the version is 8.0 which is your recommendation.  I checked the error log and found the path for cuda is not what I used then I checked the build.sh.   I change bulid.sh    `nvcc -I/usr/local/cuda/include --expt-extended-lambda -O3 -c -o bn.o bn.cu -x cu -Xcompiler -fPIC -std=c++11 ${CUDA_GENCODE}`  into my path then worked.  Maybe others will meet the same problem. Wish what i did can help you.
The new support for pytorch 1.x is much better to use for multi-gpu.   So does it achieve the same performance for training as previous version for pytorch 0.4.1?  @speedinghzl 
"The code includes some scripts on VOC dataset. However, there is no VOC-related training schemes inside the code.    Could someone kindly provide training details (or code) on the VOC dataset?    > e.g. base learning rate, learning rate decreasing strategy, batchsize    注意到代码中有部分PASCAL VOC的脚本，但是代码中没有提供 VOC 上训练的一些超参数细节。    请问有人能提供一下 VOC 数据集上训练的相关训练代码或者超参数吗？    > 例如 初始学习率，学习率下降方式，batchsize等"
I don't know which is YOUR_CS_PATH.Does it mean the CCNet folder?  Thank you.
"************************************************************************************************************************  when ./libs/build.py is excuted:  cc1: warning: command line option ‘-std=c++11’ is valid for C++/ObjC++ but not for C  gcc -pthread -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/wj/anaconda3/envs/torch0.4.0/lib/python3.6/site-packages/torch/utils/ffi/../../lib/include -I/home/wj/anaconda3/envs/torch0.4.0/lib/python3.6/site-packages/torch/utils/ffi/../../lib/include/TH -I/home/wj/anaconda3/envs/torch0.4.0/lib/python3.6/site-packages/torch/utils/ffi/../../lib/include/THC -I/usr/local/cuda/include -I/home/wj/anaconda3/envs/torch0.4.0/include/python3.6m -c /media/wj/bangong/wang/CCNet-master/libs/src/lib_cffi.cpp -o ./media/wj/bangong/wang/CCNet-master/libs/src/lib_cffi.o -std=c++11  cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++  g++ -pthread -shared -L/home/wj/anaconda3/envs/torch0.4.0/lib -Wl,-rpath=/home/wj/anaconda3/envs/torch0.4.0/lib,--no-as-needed ./__ext.o ./media/wj/bangong/wang/CCNet-master/libs/src/lib_cffi.o /media/wj/bangong/wang/CCNet-master/libs/src/bn.o -L/home/wj/anaconda3/envs/torch0.4.0/lib -lpython3.6m -o ./__ext.so    ************************************************************************************************************************  when ./cc_attention/bulid.py is excuted:  cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++  cc1plus: warning: command line option ‘-std=c99’ is valid for C/ObjC but not for C++  g++ -pthread -shared -L/home/wj/anaconda3/envs/torch0.4.0/lib -Wl,-rpath=/home/wj/anaconda3/envs/torch0.4.0/lib,--no-as-needed ./__ext.o ./media/wj/bangong/wang/CCNet-master/cc_attention/src/lib_cffi.o /media/wj/bangong/wang/CCNet-master/cc_attention/src/ca.o -L/home/wj/anaconda3/envs/torch0.4.0/lib -lpython3.6m -o ./__ext.so          "
None
"AttributeError: Traceback (most recent call last):    File ""/home/ydc/anaconda3/envs/tl_pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 106, in _worker_loop      samples = collate_fn([dataset[i] for i in batch_indices])    File ""/home/ydc/anaconda3/envs/tl_pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 106, in        samples = collate_fn([dataset[i] for i in batch_indices])    File ""/home/ydc/tl/pytorch/CCNet-master/dataset/datasets.py"", line 178, in __getitem__      size = image.shape  AttributeError: 'NoneType' object has no attribute 'shape'    HOW TO SOLVE IT?"
"I have a question when I make the training dataset. I want to use my own dataset, but my dataset is  in coco format，So I converted the dataset to voc format and use the class VOCDataSet. But when I begin to train, there was a strange problem. It fills up all the video memory，but not use and not report any errors.I am eager to get your help.   !   "
"Hello, thank you very much for your reply to my last question. I appreciate your CCNET very much, but I encounter difficulties in testing after training. I want to ask how you test the test set. Why are the other list files corresponding to image and label, while the test. list file contains only test pictures, without labels? I am eager to get your help. I am a student. Thank you.  !   !     "
"I understand the question about the evaluation result of ccnet. Thank you very much for your reply. I also have another question about the improvement point of ccnet. Whether we can only pay cross attention to four nodes around the blue information point when we do cross-focus, can we improve the computing efficiency? I do a test as if it is feasible. I am eager to get your academic guidance. Thank you very much."
"Hi  !     I would like to ask why the evaluation result of ccnet source code will be 19 arrays, how do these 19 arrays represent the results of 500 proof sets?Another problem is that the ratio and number of training sets, validation sets, and test sets I use are the same for you. Why is my IU 51% and you are 80%?I am an undergraduate student and I urgently need your help. Thank you very much."
"Hi  I have cd ../cc_attention an run python build.py but i get error ""cffi.error.VerificationError: CompileError: command 'gcc' failed with exit status 1"" I have 4 1080Ti, and my cuda version is 9.0 and cudnn is 7.1.4 and gcc version is 4.9.3. Is this error produced by the version. Thank for your code."
"Hello, the GPU I am using is TITAN XP, batch_size=2, other parameters are the same as yours, why can't I run it?`/home/jason/.local/lib/python3.5/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.    warnings.warn(warning.format(ret))  121975 images are loaded!  /home/jason/.local/lib/python3.5/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.    warnings.warn(""nn.functional.upsample is deprecated. Use nn.functional.interpolate instead."")  iter = 0 of 60000 completed, loss = 4.349814414978027  taking snapshot ...  Traceback (most recent call last):    File ""train.py"", line 313, in        main()    File ""train.py"", line 276, in main      loss.backward()    File ""/home/jason/.local/lib/python3.5/site-packages/torch/tensor.py"", line 93, in backward      torch.autograd.backward(self, gradient, retain_graph, create_graph)    File ""/home/jason/.local/lib/python3.5/site-packages/torch/autograd/__init__.py"", line 90, in backward      allow_unreachable=True)  # allow_unreachable flag  RuntimeError: CUDA error: out of memory  `"
None
"I'm cuda toolkit 11.2 - ubuntu 20 / python 3.8  nightly build of pytorch.     Linux pop-os 5.8.0-7630-generic #32~1609193707~20.10~781bb80-Ubuntu SMP Tue Jan 5 21:29:56 UTC 2 x86_64 x86_64 x86_64 GNU/Linux  Sat 06 Feb 2021 06:56:10 AEDT  *****************************************  Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.   *****************************************  Detected CUDA files, patching ldflags  Emitting ninja build file /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/build/build.ninja...  Building extension module rcca...  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)  [1/2] c++ -MMD -MF lib_cffi.o.d -DTORCH_EXTENSION_NAME=rcca -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\""_gcc\"" -DPYBIND11_STDLIB=\""_libstdcpp\"" -DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" -isystem /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include -isystem /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH -isystem /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda-11.2/include -isystem /home/jp/miniconda3/envs/torch/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -c /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp -o lib_cffi.o   FAILED: lib_cffi.o   c++ -MMD -MF lib_cffi.o.d -DTORCH_EXTENSION_NAME=rcca -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\""_gcc\"" -DPYBIND11_STDLIB=\""_libstdcpp\"" -DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" -isystem /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include -isystem /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH -isystem /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda-11.2/include -isystem /home/jp/miniconda3/envs/torch/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -c /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp -o lib_cffi.o   /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp: In function ‘int ca_forward_cuda(const at::Tensor&, const at::Tensor&, at::Tensor&)’:  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:10:25: error: ‘THCState_getCurrentStream’ was not declared in this scope     10 |   cudaStream_t stream = THCState_getCurrentStream(state);        |                         ^~~~~~~~~~~~~~~~~~~~~~~~~  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:13:34: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data () is deprecated. Please use Tensor.data_ptr () instead. [-Wdeprecated-declarations]     13 |   float * t_data = t.data ();        |                                  ^  In file included from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:2,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensorApply.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensor.h:5,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THCTensor.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THC.h:13,                   from /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:3:  /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:363:7: note: declared here    363 |   T * data() const {        |       ^~~~  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:14:34: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data () is deprecated. Please use Tensor.data_ptr () instead. [-Wdeprecated-declarations]     14 |   float * f_data = f.data ();        |                                  ^  In file included from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:2,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensorApply.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensor.h:5,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THCTensor.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THC.h:13,                   from /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:3:  /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:363:7: note: declared here    363 |   T * data() const {        |       ^~~~  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:15:44: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data () is deprecated. Please use Tensor.data_ptr () instead. [-Wdeprecated-declarations]     15 |   float * weight_data = weight.data ();        |                                            ^  In file included from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:2,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensorApply.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensor.h:5,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THCTensor.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THC.h:13,                   from /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:3:  /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:363:7: note: declared here    363 |   T * data() const {        |       ^~~~  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp: In function ‘int ca_backward_cuda(const at::Tensor&, const at::Tensor&, const at::Tensor&, at::Tensor&, at::Tensor&)’:  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:21:25: error: ‘THCState_getCurrentStream’ was not declared in this scope     21 |   cudaStream_t stream = THCState_getCurrentStream(state);        |                         ^~~~~~~~~~~~~~~~~~~~~~~~~  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:24:34: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data () is deprecated. Please use Tensor.data_ptr () instead. [-Wdeprecated-declarations]     24 |   float * t_data = t.data ();        |                                  ^  In file included from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:2,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensorApply.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensor.h:5,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THCTensor.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THC.h:13,                   from /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:3:  /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:363:7: note: declared here    363 |   T * data() const {        |       ^~~~  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:25:34: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data () is deprecated. Please use Tensor.data_ptr () instead. [-Wdeprecated-declarations]     25 |   float * f_data = f.data ();        |                                  ^  In file included from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:2,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensorApply.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensor.h:5,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THCTensor.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THC.h:13,                   from /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:3:  /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:363:7: note: declared here    363 |   T * data() const {        |       ^~~~  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:26:36: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data () is deprecated. Please use Tensor.data_ptr () instead. [-Wdeprecated-declarations]     26 |   float * dt_data = dt.data ();        |                                    ^  In file included from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:2,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensorApply.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensor.h:5,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THCTensor.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THC.h:13,                   from /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:3:  /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:363:7: note: declared here    363 |   T * data() const {        |       ^~~~  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:27:36: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data () is deprecated. Please use Tensor.data_ptr () instead. [-Wdeprecated-declarations]     27 |   float * df_data = df.data ();        |                                    ^  In file included from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:2,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensorApply.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensor.h:5,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THCTensor.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THC.h:13,                   from /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:3:  /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:363:7: note: declared here    363 |   T * data() const {        |       ^~~~  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:28:36: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data () is deprecated. Please use Tensor.data_ptr () instead. [-Wdeprecated-declarations]     28 |   float * dw_data = dw.data ();        |                                    ^  In file included from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:2,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensorApply.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensor.h:5,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THCTensor.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THC.h:13,                   from /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:3:  /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:363:7: note: declared here    363 |   T * data() const {        |       ^~~~  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp: In function ‘int ca_map_forward_cuda(const at::Tensor&, const at::Tensor&, at::Tensor&)’:  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:33:25: error: ‘THCState_getCurrentStream’ was not declared in this scope     33 |   cudaStream_t stream = THCState_getCurrentStream(state);        |                         ^~~~~~~~~~~~~~~~~~~~~~~~~  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:38:49: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data () is deprecated. Please use Tensor.data_ptr () instead. [-Wdeprecated-declarations]     38 |   const float *weight_data = weight.data ();        |                                                 ^  In file included from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:2,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensorApply.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensor.h:5,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THCTensor.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THC.h:13,                   from /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:3:  /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:363:7: note: declared here    363 |   T * data() const {        |       ^~~~  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:39:39: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data () is deprecated. Please use Tensor.data_ptr () instead. [-Wdeprecated-declarations]     39 |   const float *g_data = g.data ();        |                                       ^  In file included from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:2,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensorApply.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensor.h:5,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THCTensor.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THC.h:13,                   from /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:3:  /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:363:7: note: declared here    363 |   T * data() const {        |       ^~~~  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:40:37: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data () is deprecated. Please use Tensor.data_ptr () instead. [-Wdeprecated-declarations]     40 |   float *out_data = out.data ();        |                                     ^  In file included from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:2,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensorApply.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensor.h:5,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THCTensor.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THC.h:13,                   from /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:3:  /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:363:7: note: declared here    363 |   T * data() const {        |       ^~~~  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp: In function ‘int ca_map_backward_cuda(const at::Tensor&, const at::Tensor&, const at::Tensor&, at::Tensor&, at::Tensor&)’:  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:47:25: error: ‘THCState_getCurrentStream’ was not declared in this scope     47 |   cudaStream_t stream = THCState_getCurrentStream(state);        |                         ^~~~~~~~~~~~~~~~~~~~~~~~~  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:52:45: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data () is deprecated. Please use Tensor.data_ptr () instead. [-Wdeprecated-declarations]     52 |   const float *dout_data = dout.data ();        |                                             ^  In file included from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:2,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensorApply.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensor.h:5,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THCTensor.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THC.h:13,                   from /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:3:  /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:363:7: note: declared here    363 |   T * data() const {        |       ^~~~  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:53:49: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data () is deprecated. Please use Tensor.data_ptr () instead. [-Wdeprecated-declarations]     53 |   const float *weight_data = weight.data ();        |                                                 ^  In file included from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:2,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensorApply.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensor.h:5,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THCTensor.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THC.h:13,                   from /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:3:  /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:363:7: note: declared here    363 |   T * data() const {        |       ^~~~  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:54:39: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data () is deprecated. Please use Tensor.data_ptr () instead. [-Wdeprecated-declarations]     54 |   const float *g_data = g.data ();        |                                       ^  In file included from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:2,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensorApply.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensor.h:5,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THCTensor.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THC.h:13,                   from /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:3:  /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:363:7: note: declared here    363 |   T * data() const {        |       ^~~~  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:55:35: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data () is deprecated. Please use Tensor.data_ptr () instead. [-Wdeprecated-declarations]     55 |   float *dw_data = dw.data ();        |                                   ^  In file included from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:2,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensorApply.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensor.h:5,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THCTensor.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THC.h:13,                   from /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:3:  /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:363:7: note: declared here    363 |   T * data() const {        |       ^~~~  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:56:35: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data () is deprecated. Please use Tensor.data_ptr () instead. [-Wdeprecated-declarations]     56 |   float *dg_data = dg.data ();        |                                   ^  In file included from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:2,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensorApply.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH/THTensor.h:5,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THCTensor.h:4,                   from /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC/THC.h:13,                   from /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:3:  /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:363:7: note: declared here    363 |   T * data() const {        |       ^~~~  ninja: build stopped: subcommand failed.  Traceback (most recent call last):    File ""/home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/cpp_extension.py"", line 1533, in _run_ninja_build      subprocess.run(    File ""/home/jp/miniconda3/envs/torch/lib/python3.8/subprocess.py"", line 512, in run      raise CalledProcessError(retcode, process.args,  subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.    The above exception was the direct cause of the following exception:    Traceback (most recent call last):    File ""train.py"", line 17, in        import networks    File ""/home/jp/Documents/gitWorkspace/CCNet-1/networks/__init__.py"", line 3, in        import networks.ccnet    File ""/home/jp/Documents/gitWorkspace/CCNet-1/networks/ccnet.py"", line 13, in        from cc_attention import CrissCrossAttention    File ""/home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/__init__.py"", line 1, in        from .functions import CrissCrossAttention, ca_weight, ca_map    File ""/home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/functions.py"", line 16, in        rcca = load(name=""rcca"",    File ""/home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/cpp_extension.py"", line 986, in load      return _jit_compile(    File ""/home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/cpp_extension.py"", line 1193, in _jit_compile      _write_ninja_file_and_build_library(    File ""/home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/cpp_extension.py"", line 1297, in _write_ninja_file_and_build_library      _run_ninja_build(    File ""/home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/cpp_extension.py"", line 1555, in _run_ninja_build      raise RuntimeError(message) from e  RuntimeError: Error building extension 'rcca'  Loading extension module rcca...  Traceback (most recent call last):    File ""train.py"", line 17, in        import networks    File ""/home/jp/Documents/gitWorkspace/CCNet-1/networks/__init__.py"", line 3, in        import networks.ccnet    File ""/home/jp/Documents/gitWorkspace/CCNet-1/networks/ccnet.py"", line 13, in        from cc_attention import CrissCrossAttention    File ""/home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/__init__.py"", line 1, in        from .functions import CrissCrossAttention, ca_weight, ca_map    File ""/home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/functions.py"", line 16, in        rcca = load(name=""rcca"",    File ""/home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/cpp_extension.py"", line 986, in load      return _jit_compile(    File ""/home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/cpp_extension.py"", line 1213, in _jit_compile      return _import_module_from_library(name, build_directory, is_python_module)    File ""/home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/cpp_extension.py"", line 1560, in _import_module_from_library  Loading extension module rcca...      Traceback (most recent call last):  file, path, description = imp.find_module(module_name, [path])    File ""train.py"", line 17, in      File ""/home/jp/miniconda3/envs/torch/lib/python3.8/imp.py"", line 296, in find_module      import networks      raise ImportError(_ERR_MSG.format(name), name=name)    File ""/home/jp/Documents/gitWorkspace/CCNet-1/networks/__init__.py"", line 3, in    ImportError:     No module named 'rcca'import networks.ccnet      File ""/home/jp/Documents/gitWorkspace/CCNet-1/networks/ccnet.py"", line 13, in        from cc_attention import CrissCrossAttention    File ""/home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/__init__.py"", line 1, in        from .functions import CrissCrossAttention, ca_weight, ca_map    File ""/home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/functions.py"", line 16, in        rcca = load(name=""rcca"",    File ""/home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/cpp_extension.py"", line 986, in load      return _jit_compile(    File ""/home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/cpp_extension.py"", line 1213, in _jit_compile      return _import_module_from_library(name, build_directory, is_python_module)    File ""/home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/cpp_extension.py"", line 1560, in _import_module_from_library  Loading extension module rcca...      file, path, description = imp.find_module(module_name, [path])    File ""/home/jp/miniconda3/envs/torch/lib/python3.8/imp.py"", line 296, in find_module      raise ImportError(_ERR_MSG.format(name), name=name)  ImportError: No module named 'rcca'  Traceback (most recent call last):    File ""train.py"", line 17, in        import networks    File ""/home/jp/Documents/gitWorkspace/CCNet-1/networks/__init__.py"", line 3, in        import networks.ccnet    File ""/home/jp/Documents/gitWorkspace/CCNet-1/networks/ccnet.py"", line 13, in        from cc_attention import CrissCrossAttention    File ""/home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/__init__.py"", line 1, in        from .functions import CrissCrossAttention, ca_weight, ca_map    File ""/home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/functions.py"", line 16, in        rcca = load(name=""rcca"",    File ""/home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/cpp_extension.py"", line 986, in load      return _jit_compile(    File ""/home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/cpp_extension.py"", line 1213, in _jit_compile      return _import_module_from_library(name, build_directory, is_python_module)    File ""/home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/cpp_extension.py"", line 1560, in _import_module_from_library      file, path, description = imp.find_module(module_name, [path])    File ""/home/jp/miniconda3/envs/torch/lib/python3.8/imp.py"", line 296, in find_module      raise ImportError(_ERR_MSG.format(name), name=name)  ImportError: No module named 'rcca'  Traceback (most recent call last):    File ""/home/jp/miniconda3/envs/torch/lib/python3.8/runpy.py"", line 194, in _run_module_as_main      return _run_code(code, main_globals, None,    File ""/home/jp/miniconda3/envs/torch/lib/python3.8/runpy.py"", line 87, in _run_code      exec(code, run_globals)    File ""/home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/distributed/launch.py"", line 260, in        main()    File ""/home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/distributed/launch.py"", line 255, in main      raise subprocess.CalledProcessError(returncode=process.returncode,  subprocess.CalledProcessError: Command '['/home/jp/miniconda3/envs/torch/bin/python', '-u', 'train.py', '--local_rank=3', '--data-dir', '--model', '--random-mirror', '--random-scale', '--restore-from', './dataset/resnet101-imagenet.pth', '--input-size', '--gpu', '0,1,2,3', '--learning-rate', '1e-2', '--weight-decay', '5e-4', '--batch-size', '8', '--num-steps', '--ohem']' returned non-zero exit status 1.  *****************************************  Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.   *****************************************  Detected CUDA files, patching ldflags  Emitting ninja build file /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/build/build.ninja...  Building extension module rcca...  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)  [1/2] c++ -MMD -MF lib_cffi.o.d -DTORCH_EXTENSION_NAME=rcca -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\""_gcc\"" -DPYBIND11_STDLIB=\""_libstdcpp\"" -DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" -isystem /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include -isystem /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH -isystem /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda-11.2/include -isystem /home/jp/miniconda3/envs/torch/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -c /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp -o lib_cffi.o   FAILED: lib_cffi.o   c++ -MMD -MF lib_cffi.o.d -DTORCH_EXTENSION_NAME=rcca -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\""_gcc\"" -DPYBIND11_STDLIB=\""_libstdcpp\"" -DPYBIND11_BUILD_ABI=\""_cxxabi1011\"" -isystem /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include -isystem /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/TH -isystem /home/jp/miniconda3/envs/torch/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda-11.2/include -isystem /home/jp/miniconda3/envs/torch/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -c /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp -o lib_cffi.o   /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp: In function ‘int ca_forward_cuda(const at::Tensor&, const at::Tensor&, at::Tensor&)’:  /home/jp/Documents/gitWorkspace/CCNet-1/cc_attention/src/lib_cffi.cpp:10:25: error: ‘THCState_getCurrentStream’ was not declared in this scope     10 |   cudaStream_t stream = THCState_getCurrentStream(state);    "
"I  found add a module of Non-local attention which just add a little extra time cost about 0.4s each iter. But if I add a CC-attention take R==1 , the train time each iter about 0.7s, and 1.0s if R==2. It's not like the description in your paper.  I dont know why.  Can anyone explain it ."
报错为：RuntimeError: Error compiling objects for extension  请问这是哪个方面的问题呢？
None
"Hi,   thanks for your work. Can I perform the evaluation of your pre-trained network using only one GPU?   I test the Pytorch 1.1 implementation installing all requirements in a miniconda environment. I used cuda 10.0. I installed Apex using conda and Inplace-ABN with `pip install inplace-abn` (as indicated here:    I tried changing the `run_local.sh` file as the following:       But I got the following error:       Where is the problem? Can anyone help me?"
i dont know why size of  Ωu  = (H+W-1)×C‘  how to got it (H+W-1) ?  looking forward to your reply!
"I have got the error below, how can i fix it?    [00:00       main()    File ""train.py"", line 226, in main      loss = model(images, labels)    File ""/usr/local/miniconda3/envs/dl10/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 547, in __call__      result = self.forward(*input, **kwargs)    File ""/usr/local/miniconda3/envs/dl10/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 152, in forward      outputs = self.parallel_apply(replicas, inputs, kwargs)    File ""/usr/local/miniconda3/envs/dl10/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 162, in parallel_apply      return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])    File ""/usr/local/miniconda3/envs/dl10/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py"", line 85, in parallel_apply      output.reraise()    File ""/usr/local/miniconda3/envs/dl10/lib/python3.6/site-packages/torch/_utils.py"", line 369, in reraise      raise self.exc_type(msg)  AssertionError: Caught AssertionError in replica 0 on device 0.  Original Traceback (most recent call last):    File ""/usr/local/miniconda3/envs/dl10/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py"", line 60, in _worker      output = module(*input, **kwargs)    File ""/usr/local/miniconda3/envs/dl10/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 547, in __call__      result = self.forward(*input, **kwargs)    File ""/zmz/ccnet/networks/ccnet.py"", line 189, in forward      x = self.relu1(self.bn1(self.conv1(x)))    File ""/usr/local/miniconda3/envs/dl10/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 547, in __call__      result = self.forward(*input, **kwargs)    File ""/usr/local/miniconda3/envs/dl10/lib/python3.6/site-packages/inplace_abn-1.1.0-py3.6-linux-x86_64.egg/inplace_abn/abn.py"", line 323, in forward      self.group,    File ""/usr/local/miniconda3/envs/dl10/lib/python3.6/site-packages/inplace_abn-1.1.0-py3.6-linux-x86_64.egg/inplace_abn/functions.py"", line 307, in inplace_abn_sync      group,    File ""/usr/local/miniconda3/envs/dl10/lib/python3.6/site-packages/inplace_abn-1.1.0-py3.6-linux-x86_64.egg/inplace_abn/functions.py"", line 91, in forward      ctx.world_size = distributed.get_world_size(group=group)    File ""/usr/local/miniconda3/envs/dl10/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 584, in get_world_size      return _get_group_size(group)    File ""/usr/local/miniconda3/envs/dl10/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 200, in _get_group_size      _check_default_pg()    File ""/usr/local/miniconda3/envs/dl10/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 191, in _check_default_pg      ""Default process group is not initialized""  AssertionError: Default process group is not initialized  "
None
"i use voc datasets to train, but i got this error ,how can i solve it      torch.Size([4, 64, 385, 385])  torch.Size([4, 64, 385, 385])  torch.Size([4, 128, 385, 385])  torch.Size([4, 128, 193, 193])  Traceback (most recent call last):    File ""train.py"", line 246, in        main()    File ""train.py"", line 210, in main      preds = model(images, args.recurrence)    File ""/home/yuxi.xt/software/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__      result = self.forward(*input, **kwargs)    File ""/home/yuxi.xt/software/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 121, in forward      return self.module(*inputs[0], **kwargs[0])    File ""/home/yuxi.xt/software/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__      result = self.forward(*input, **kwargs)    File ""/home/yuxi.xt/experinment/CCNet/networks/ccnet.py"", line 204, in forward      x=self.bn1(self.conv1(x))    File ""/home/yuxi.xt/software/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__      result = self.forward(*input, **kwargs)    File ""/home/yuxi.xt/software/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py"", line 301, in forward      self.padding, self.dilation, self.groups)  RuntimeError: Given groups=1, weight of size [64, 3, 3, 3], expected input[4, 128, 193, 193] to have 3 channels, but got 128 channels instead          "
None
None
   Can you tell me abou it ? why do you use it?
"I'm using the pytorch-1.1 branch, and meet some problem,would you like to tell me your torchvision?(my pytorch version is 1.1.0, torchvision is 0.3.0)"
"build.sh: 11: build.sh: nvcc: not found  !   hello，such problem hapeened,i have write the path in  ~/.bashrc ,do you have some solution?  export CUDA_HOME=/usr/local/cuda  export PATH=$PATH:/usr/local/cuda-9.0/bin  export PATH=/usr/local/cuda-9.0/bin${PATH:+:${PATH}}  export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}  "
"!       I recently read your paper and have some questions about the formula. What does the ∑ above mean? How does it work？   !   !   I found that the number of channels between them was not the same, while the number of channels to H‘ was C.  "
"Thanks for your great work. I have tried to use the RCCA module you developed in my own segmentation network, in particular, I place this module in the end of segmentation network. However, it cannot improve the performance. Can you please give some suggestions for using the RCCA module?    Thanks."
"CCnet is not computation efficiency in current implementation. Given the same input image,  CCNet(R=2) needs **more computation** than DAnet(Dual Attention Networks) which use two non-local operators. The main point lies in  **RCCAModule** and it have two 3 *3 convolution operated on the 2048 channel though CC module itself is effeciency.  "
"I have tried multiscale-test of the master branch.  There seems no improvment of performance.  For branch-pytorch1.1, it seems bug remains (prob shape mismatch), if replace [1.0] with [0.75, 1.0, 1.25] directly."
"Hi, Thanks for you work.  I am curious that why the preformance of CCNET on table 1 is 81.3 while that on table 4 is 79.8. It seems they are under the same setting and should be the same network. Do I miss some details?    Thanks! "
None
"Hi, @speedinghzl ,    Here is my command for training:  `python train.py --data-dir /media/root/WDdata/dataset/cityscapes_dataset/leftImg8bit_trainvaltest --random-mirror --random-scale --restore-from ./dataset/resnet101-imagenet.pth --gpu 0,1,2,3 --learning-rate 1e-2 --input-size 769,769 --weight-decay 1e-4 --batch-size 8 --num-steps 60000 --recurrence 2`    But the error shows that annotation labels are missing.          Could you elaborate on the directory structure for cityscapes dataset? e.g. the location on  img and labels.    THX!"
None
"Is the mean_IU in the program mIoU?  IU_array = (tp / np.maximum(1.0, pos + res - tp))  mean_IU = IU_array.mean()"
"Dear author,  thank you for your excellent work. I have a question. If I want to split only the specific scenes in the image, how do I change the code? For example, I just want to split the car in the picture.Should I modify the“--num-classes” ？"
"Hello, I would like to quote your RecurrentCriss-CrossAttention separately, but the main framework has CNN feature extraction, so the CNN feature extraction and segmentation module is no longer needed, but I do not know how to get independent RecurrentCriss-CrossAttention  code, I hope to get your guidance.Thank you very much"
None
"hi~  i using the project in your pytorch-segmentation-toolbox.  when i training the pspnet in the project.  i get the result follow:  1) val:0.7883(ss) / 0.7930(ms) | test:0.7756(ss) / 0.7846(ms)  2) val:0.7897(ss) / 0.8003(ms) | test:0.7800(ss) / 0.7826(ms)  so i want to make sure it is normal? because it seem like we can easily get a good result in val set, but have some trouble in test set.    i want to know how's your performance in val and test set in the same experiment.(pspnet as well as ccnet)~thanks for your reply!"
"Hello, the GPU I am using is TITAN XP, batch_size=2, other parameters are the same as yours, why can't I run it?!   "
"When I compiled the ""cc_atttenion"", I get the error ""cffi.VerificationError: CompileError: command 'gcc' failed with exit status 1"". My environment is Ubuntu 16.04, CUDA 8.0, python3.6 with pytorch 0.4.1. Could you help me?  !   "
"ImportError: /home/denny/github_open_Sources/CCNet/libs/_ext/__ext.so: undefined symbol: __cudaPopCallConfiguration        Complete Traceback:-      Traceback (most recent call last):    File ""evaluate.py"", line 14, in        from networks.ccnet import Res_Deeplab    File ""/home/denny/github_open_Sources/CCNet/networks/ccnet.py"", line 15, in        from libs import InPlaceABN, InPlaceABNSync    File ""/home/denny/github_open_Sources/CCNet/libs/__init__.py"", line 1, in        from .bn import ABN, InPlaceABN, InPlaceABNWrapper, InPlaceABNSync, InPlaceABNSyncWrapper    File ""/home/denny/github_open_Sources/CCNet/libs/bn.py"", line 15, in        from .functions import inplace_abn, inplace_abn_sync    File ""/home/denny/github_open_Sources/CCNet/libs/functions.py"", line 5, in        from . import _ext    File ""/home/denny/github_open_Sources/CCNet/libs/_ext/__init__.py"", line 3, in        from .__ext import lib as _lib, ffi as _ffi  ImportError: /home/denny/github_open_Sources/CCNet/libs/_ext/__ext.so: undefined symbol: __cudaPopCallConfiguration    "
"Hi, the idea of cross-criss attention is impressive and inspiring and the code released is very helpful to me.     I am a starter in the DL area, and I want to further learn how to implement some computational codes in CUDA just like you implementing computational part of cross-criss attention in CUDA. So is there any resources in English or Chinese that you suggest to learn?     Thank you in advance for helping me!"
"Hello，  Now I try to use CCNet to segment Road.  In my mask images, road is set 255 while others are set 0.  NUM_CLASSES=2, self.id_to_trainid={0:0, 255:255}.  It quickly convergence.But when I run test.py,the output image is all black.  I want  to knows what's wrong?  Thx @speedinghzl "
"Hi !Because I always make mistakes in compiling, I want to implement these two modules by myself, where can I see their implementation? Thanks !"
"I try to training the CCnet on the ADE20K dataset. Unfortunately, I get a bad result.  Could you release your training code on the ADE20K ?"
"See #12 .         I think it is because the ATen does not support `std=c99` in 0.4.1. By the way, the cc_attention does not support 1.0.0 too while the latter does not support `torch.utils.ffi`."
!     !     !   
"Hi, Zilong:    Thanks for your contribution to this amazing repo! I really appreciate of this!  By the way, I want to reproduce the highest score in your paper: 81.4 on Cityscapes test set.(with 4 V100 GPUs).  **I have already reproduced the default setting single-scale result on val set: 79.7 on my machine.**  To reach the 81.4, as far as I have known: besides the default settings, OHEM loss function and multi-scale inference should also be used. But when I re-train the code with setting --ohem True, the evaluation results drop a lot - about 10 points are decreased.   So my problem is, what also should I modify to achieve the result as proposed in the original paper?    Thanks a lot for your help!"
"When I run run_local.sh, I get the problem as followed. How can I solve it? Thanks.  `Traceback (most recent call last):    File ""train.py"", line 249, in        main()    File ""train.py"", line 212, in main      preds = model(images, args.recurrence)    File ""/home/ym/anaconda3/envs/py3.5_pytorch0.4.0/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 491, in __call__      result = self.forward(*input, **kwargs)    File ""/home/ym/anaconda3/envs/py3.5_pytorch0.4.0/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py"", line 114, in forward      outputs = self.parallel_apply(replicas, inputs, kwargs)    File ""/home/ym/anaconda3/envs/py3.5_pytorch0.4.0/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py"", line 124, in parallel_apply      return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])    File ""/home/ym/anaconda3/envs/py3.5_pytorch0.4.0/lib/python3.5/site-packages/torch/nn/parallel/parallel_apply.py"", line 65, in parallel_apply      raise output    File ""/home/ym/anaconda3/envs/py3.5_pytorch0.4.0/lib/python3.5/site-packages/torch/nn/parallel/parallel_apply.py"", line 41, in _worker      output = module(*input, **kwargs)    File ""/home/ym/anaconda3/envs/py3.5_pytorch0.4.0/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 491, in __call__      result = self.forward(*input, **kwargs)    File ""/home/ym/CCNet/networks/ccnet.py"", line 196, in forward      x = self.relu1(self.bn1(self.conv1(x)))    File ""/home/ym/anaconda3/envs/py3.5_pytorch0.4.0/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 491, in __call__      result = self.forward(*input, **kwargs)    File ""/home/ym/CCNet/libs/bn.py"", line 184, in forward      self.activation, self.slope)    File ""/home/ym/CCNet/libs/functions.py"", line 183, in forward      _check(_ext.bn_mean_var_cuda, x, mean, var)    File ""/home/ym/CCNet/libs/functions.py"", line 16, in _check      raise RuntimeError(""CUDA Error encountered in {}"".format(fn))  RuntimeError: CUDA Error encountered in    `"
"Hi, thanks for sharing the code for your insightful work!     From your code I didn't find any explicit param initialization, so do you initialize your weights using the default initialization? (which is kaiming_uniform for convolution layers)    Thanks!"
"Hi, i am new to pytorch. My environment is python2.7 cuda8.0 pytorch 0.4.1 gcc4.8.5. When i run python build.py (in cc_attention dir), it throws   cffi.error.VerificationError: CompileError: command 'x86_64-linux-gnu-gcc' failed with exit status 1  how can i fix this"
I would like to know its full name.
"Hi, thanks for your work.  I wonder if I edit the `ca.cu` file, how to generate a new `ca.o` for `python build.py install`, which requires `ca.o` as `extra_objects`.  Thanks."
"Thanks for this grateful implementation. I am training cityscapes dataset. But I am facing this issue. seeking help. I will be very thankful. Regards  File ""/adata/Naushad/CCNet/dataset/datasets.py"", line 164, in id2trainId      label_copy = label.copy()  AttributeError: 'NoneType' object has no attribute 'copy'"
There are two parts to your semantic segmentation loss. What are the benefits and theoretical basis?
None
Some BNs are `InPlaceABNSync` with `activation='none'` (in backbone) and others are `InPlaceABNSync` with `activation='leaky_relu'` (in `head` and `dsn`).  Why?
"Thank you very much for the source code provided. I have some questions about the parameter settings in your code. The value of gamma is set to zero?    class CrissCrossAttention(nn.Module):      """""" Pixel-wise attention module""""""      def __init__(self,in_dim):          super(CrissCrossAttention,self).__init__()          self.chanel_in = in_dim            self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)          self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)          self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)          self.gamma = nn.Parameter(torch.zeros(1))        def forward(self,x):          proj_query = self.query_conv(x)          proj_key = self.key_conv(x)          proj_value = self.value_conv(x)            energy = ca_weight(proj_query, proj_key)          attention = F.softmax(energy, 1)          out = ca_map(attention, proj_value)          out = self.gamma*out + x            return out"
"Thx for the nice job.  However I downloaded the code and trained the model, but the results in the paper were not well reproduced.    #### Setting  Dataset: Cityscapes  Train with `train` split, 2975 images.  Evaluate with `val` split.  Follow all details in this repo.   Train models with different `max_iteration`s (60000 as default setting in this repo.)    #### Results in paper  !     #### Result  | model | Max Iter | mIoU |  |---|---|---|  | Resnet101-RCCA(R=2) | 40000 | 75.85% |  | Resnet101-RCCA(R=2) | 60000 | 76.81% |  | Resnet101-RCCA(R=2) | 100000 | 76.36% |  | Resnet101-PSP | 40000 | 76.92% |  | Resnet101-PSP | 60000 | 76.85% |  | Resnet101-PSP | 100000 | 76.90% |      #### Env  pytorch 0.4.0  torchvision 0.2.1  4*TITAN XP    Is there any tricks in the implementation?  "
nice jobs!  Do you mind to provide the code or details about how to implement the visualization of the attention map?
Nice Work! Can you provide the code or details about how to calculate the flops of the CCNet?
"Hi, I'm interested in the GTA data, but I'm not able to download the images due to this error, which I've gotten two days in a row now:     Can you please make the data available in some other way? Thanks!"
"@mikittt Dear Mikihiro, thank you for the share of your work. I have a question about the uploaded models, sp.h5, splm.h5, and spve.h5 are for RefGTA, it is right? could you also please provide the generation models from Refcoco, Refcoco+ and Refcocog? Thank you a lot."
"It does not appear to contain the code to calculate the MMD and Accracy in paper's Table1,2.   Do you plan to share that code?"
Thank you for the great code base!  I try to do STEP1 and STEP2.  I seem to be consuming very few GPUs(500MB).  How much GPU do you consume in your experience?
The repo is missing   that is discussed in the README.
"Hi, friends, I meet a problem when I run  ""python main.py --data_type ""event"" --use_vfeatures --use_siamese --use_gfeatures --use_gcn --use_cd""  the error as follow:       By searching online, I find that this problem may due to the version of pytorch( when pytorch version >1.3, torch.autograd.Function need to be static instead of non-static). So I  try to solve this problem by changing class""SparseMM"" from  non-static to static, My modified results are as follows：       However, the problem has not been solved and the error prompt has not changed.  I don't know what to do next.（Unless I change the pytorch version） I hope to get the help of the author and everyone. Thanks a lot!  "
" WARNING **: 01:45:59.539: Failed to load shared library 'libgdk-3.so.0' referenced by the typelib: libXcursor.so.1: cannot open shared object file: No such file or directory  Traceback (most recent call last):    File ""feature_extractor.py"", line 8, in        from graph_tool.all import *    File ""/opt/conda/lib/python3.6/site-packages/graph_tool/all.py"", line 34, in        from graph_tool.draw import *    File ""/opt/conda/lib/python3.6/site-packages/graph_tool/draw/__init__.py"", line 835, in        from . cairo_draw import graph_draw, cairo_draw, \    File ""/opt/conda/lib/python3.6/site-packages/graph_tool/draw/cairo_draw.py"", line 1496, in        from gi.repository import Gtk, Gdk, GdkPixbuf    File "" "", line 971, in _find_and_load    File "" "", line 955, in _find_and_load_unlocked    File "" "", line 656, in _load_unlocked    File "" "", line 626, in _load_backward_compatible    File ""/opt/conda/lib/python3.6/site-packages/gi/importer.py"", line 144, in load_module      importlib.import_module('gi.repository.' + dep.split(""-"")[0])    File ""/opt/conda/lib/python3.6/importlib/__init__.py"", line 126, in import_module      return _bootstrap._gcd_import(name[level:], package, level)    File "" "", line 994, in _gcd_import    File "" "", line 971, in _find_and_load    File "" "", line 955, in _find_and_load_unlocked    File "" "", line 656, in _load_unlocked    File "" "", line 626, in _load_backward_compatible    File ""/opt/conda/lib/python3.6/site-packages/gi/importer.py"", line 145, in load_module      dynamic_module = load_overrides(introspection_module)    File ""/opt/conda/lib/python3.6/site-packages/gi/overrides/__init__.py"", line 118, in load_overrides      override_mod = importlib.import_module(override_package_name)    File ""/opt/conda/lib/python3.6/importlib/__init__.py"", line 126, in import_module      return _bootstrap._gcd_import(name[level:], package, level)    File ""/opt/conda/lib/python3.6/site-packages/gi/overrides/Gdk.py"", line 83, in        Color = override(Color)    File ""/opt/conda/lib/python3.6/site-packages/gi/overrides/__init__.py"", line 195, in override      assert g_type != TYPE_NONE  AssertionError"
None
!   
Is there similar work in English?  Is this dataset available for English as well?
None
假如我现在要使用该模型做文章搜索的功能，搜索相似的文章。首先通过一些TextRank、Ner的等模块提取了特征，然后是不是要和已有库中的所有文章都调用一次模型，这样的效率是不是太慢了
"Dr. Liu, Thanks for you great work!    When I try to apply it to an application, I found it's quite slow and not suitable for calculating the similarity of massive sentence pairs(it takes around 6 hrs to process 30k sentence pairs with `use_cd=False` on i7-8700K CPU).     I'll appreciate it if you can provide a way to accelerate this progress."
"Traceback (most recent call last):    File ""main.py"", line 102, in        labels, idx_train, idx_val, idx_test = load_graph_data(path, word_to_ix, MAX_LEN, args.num_data)    File ""/content/ArticlePairMatching/src/models/CCIG/loader.py"", line 81, in load_graph_data      fin = open(path, ""r"", encoding=""utf-8"")  FileNotFoundError: [Errno 2] No such file or directory: '../../../data/processed/event-story-cluster/same_event_doc_pair.no_cd.json'"
你好，我正在使用git-lfs克隆仓库以期获得数据集，但是在克隆时出现一些问题。请问您能不能将数据集以别的方式发布一下呢，谢谢。
"<img width=""1112"" alt=""image"" src=""   "
None
$ conda config --add channels conda-forge  $ conda config --add channels ostrokach-forge  $ conda install graph-tool
"Hi, BangLiu,      I use Bert to fine-tuning, extract two long document pair first 256 words, feed them bert, output is feed classifcation layer;      My model batch_size is 6, epoch 2; but the result of train is accuracy = 0.5519683, global_step = 517, loss = 0.72958165, precision = 0.49228394, recall = 0.2029262; The differences with papers's result is very big.      So, I want to know how do you do use bert-text-matching?  "
"您好，想问一下数据集标注的标准是什么，以下的标注是否会影响结果？  论文提到对于event_pair 是描述同一件事，而story_pair是一个一些有关联的事件（比如一些话题）。  标注的same_story_doc_pair数据集中，  一些相关事件并未标注相关，如：  0|9501|14721|“ 出轨 后 ” 的 宋 喆 买 豪宅 母亲 背 名牌 包 马蓉 这边 却 惨不忍睹|  马蓉 出轨 宋 喆 后 一直 没 露面 ， 这次 终于 要 露面 了|  0|10706|10751|详解 特朗普 就职 典礼 全程 安排 具有 多 个 看点|  名流 大腕 拒绝 出席 总统 就职 典礼 特朗普 ： 我 想 要 人民|  0|14176|14297|"" 台风 """" 海马 """" 本周 或 带来 严重 风雨 影响 ""|  “ 海马 ” 或 直 扑 闽粤 19日 至 23日 将 带来 严重 的 风雨 影响|  一些不相关事件标注为相关，如：  1|13109|13110|肇庆 这 部分 路段 封闭 施工 , 车主 请 绕行 !|  肇庆 打掉 一特大 贩毒 犯罪 团伙 缴 毒 6000 多 克|  "
"As mentioned before #2, installing ""graph_tool""  may be very troublesome.    Here is my solution to install ""graph_tool"" on my ubuntu 16.04, hoping can be helpful to those still using ubuntu 16.04 as the server.    We could go to the   for debian&ubuntu, but only ubuntu 18.04(bionic), ubuntu 18.10(cosmic) and ubuntu 19.04(disco) are listed blow the instructions.    I tried to open the source url    in my browser,  and found there was still a folder ""xenial"" which is for ubuntu 16.04.    So, I just replace the `DISTRIBUTION` with `xenial` in the following lines, and added them to  `/etc/apt/sources.list`.      Then follow the official guidance, I finally installed it successfully.    **Note**: The code may encounter a “Error”when import `cairo`, so the graph drawing will not work. But it can work normally if we don't use it for visualization.  "
运行feature_extractor.py时出现  python: symbol lookup error: /home/ubuntu/anaconda3/envs/pytorch/lib/python3.6/site-packages/graph_tool/draw/libgraph_tool_draw.so: undefined symbol: _ZN5Cairo7Context16select_font_faceERKSsNS_9FontSlantENS_10FontWeightE  请问这个如何解决？
"运行feature_extractor.py报错  Traceback (most recent call last):    File ""feature_extractor.py"", line 9, in        from ccig import *    File ""/home/ubuntu/Desktop/CIG-GCN/ArticlePairMatching-master/src/models/CCIG/data/ccig.py"", line 13, in        IDF = load_IDF(""event_story"")    File ""/home/ubuntu/Desktop/CIG-GCN/ArticlePairMatching-master/src/models/CCIG/data/resource_loader.py"", line 18, in load_IDF      ""|"", ""|"", keep_header=False)    File ""/home/ubuntu/Desktop/CIG-GCN/ArticlePairMatching-master/src/models/CCIG/util/pd_utils.py"", line 11, in export_columns      df = pd.read_csv(fin, sep=sep_in)    File ""/home/ubuntu/anaconda3/envs/pytorch/lib/python3.6/site-packages/pandas/io/parsers.py"", line 688, in read_csv      return _read(filepath_or_buffer, kwds)    File ""/home/ubuntu/anaconda3/envs/pytorch/lib/python3.6/site-packages/pandas/io/parsers.py"", line 454, in _read      parser = TextFileReader(fp_or_buf, **kwds)    File ""/home/ubuntu/anaconda3/envs/pytorch/lib/python3.6/site-packages/pandas/io/parsers.py"", line 948, in __init__      self._make_engine(self.engine)    File ""/home/ubuntu/anaconda3/envs/pytorch/lib/python3.6/site-packages/pandas/io/parsers.py"", line 1180, in _make_engine      self._engine = CParserWrapper(self.f, **self.options)    File ""/home/ubuntu/anaconda3/envs/pytorch/lib/python3.6/site-packages/pandas/io/parsers.py"", line 2010, in __init__      self._reader = parsers.TextReader(src, **kwds)    File ""pandas/_libs/parsers.pyx"", line 382, in pandas._libs.parsers.TextReader.__cinit__    File ""pandas/_libs/parsers.pyx"", line 674, in pandas._libs.parsers.TextReader._setup_parser_source  FileNotFoundError: [Errno 2] No such file or directory: '../../../../data/raw/event-story-cluster/event_story_cluster.txt'  请问这个不存在的文件在哪里呢？是需要生成的吗？"
您好，我在git clone时遇到如下问题：  Error downloading object: data/raw/event-story-cluster/same_event_doc_pair.txt (e5c2482): Smudge error: Error downloading data/raw/event-story-cluster/same_event_doc_pair.txt (e5c2482c410f19418256839d7158d18244e9630466f262b064fb5a69e6f7dddc): batch response: Post   dial tcp: lookup github.com: no such host  请问如何解决？
None
"Hi ! I love your work !  I want to use it on other dataset, but i don't know how you get the ner words. Would you tell me ? Thanks!  In the papaer, it said ""Given a document D, we ﬁrst extract the named entities and keywords by TextRank"".  Did you use TextRank to get keywords, and then match whether the key words in your own word-type dictionary?  e.g. 广东 in your dictionary is ""广东-site"".So when you get ""广东"" is a keyword, you search it and find it is a ""site""? Or you use other ner tools? I need a good and fast ner method...Can you tell me about that?  Thanks for listen to me."
"I have changed this        pos = sfdp_layout(g)       graph_draw(g, pos=pos,                 vertex_text=g.vertex_properties[""name""],                 vertex_fill_color=c,                 vertex_font_family=""STKaiti"",                 vertex_font_size=18,                 edge_font_family=""STKaiti"",                 edge_font_size=10,                 edge_text=g.edge_properties[""name""],                 output_size=(1000, 1000),                 output=fig_name)    to this        pos = gt.sfdp_layout(g)      gt.graph_draw(g, pos=pos,                 vertex_text=g.vertex_properties[""name""],                 vertex_fill_color=c,                 vertex_font_family=""STKaiti"",                 vertex_font_size=18,                 edge_font_family=""STKaiti"",                 edge_font_size=10,                 edge_text=g.edge_properties[""name""],                 output_size=(1000, 1000),                 output=fig_name)  Even though I have everything imported from graph_tool, I face this error. I also imported graph_tool as gt.  Kindly help me with this"
I get this error. 
"作者您好，在feature_extractor.py中有如下语句  if __name__ == ""__main__"":      #debug with a few lines      dataset2featurefile(          ""../../../../data/raw/event-story-cluster/same_event_doc_pair.txt"",          ""../../../../data/processed/event-story-cluster/same_event_doc_pair.cd.debug.json"",          ""label"", ""category1"", ""time1"", ""time2"", ""content1"", ""content2"",          [""keywords1"", ""ner_keywords1""], [""keywords2"", ""ner_keywords2""],          col_title1=None, col_title2=None, use_cd=True,          draw_fig=True, parallel=False, extract_range=range(2), print_fig=True)        # process data      dataset2featurefile(          ""../../../../data/raw/event-story-cluster/same_event_doc_pair.txt"",          ""../../../../data/processed/event-story-cluster/same_event_doc_pair.cd.json"",          ""label"", ""category1"", ""time1"", ""time2"", ""content1"", ""content2"",          [""keywords1"", ""ner_keywords1""], [""keywords2"", ""ner_keywords2""],          col_title1=""title1"", col_title2=""title2"", use_cd=True,          draw_fig=False, parallel=True, extract_range=None,          betweenness_threshold_coef=1.0, max_c_size=6, min_c_size=2)      dataset2featurefile(          ""../../../../data/raw/event-story-cluster/same_story_doc_pair.txt"",          ""../../../../data/processed/event-story-cluster/same_story_doc_pair.cd.json"",          ""label"", ""category1"", ""time1"", ""time2"", ""content1"", ""content2"",          [""keywords1"", ""ner_keywords1""], [""keywords2"", ""ner_keywords2""],          col_title1=""title1"", col_title2=""title2"", use_cd=True,          draw_fig=False, parallel=True, extract_range=None,          betweenness_threshold_coef=1.0, max_c_size=6, min_c_size=2)      dataset2featurefile(          ""../../../../data/raw/event-story-cluster/same_event_doc_pair.txt"",          ""../../../../data/processed/event-story-cluster/same_event_doc_pair.no_cd.json"",          ""label"", ""category1"", ""time1"", ""time2"", ""content1"", ""content2"",          [""keywords1"", ""ner_keywords1""], [""keywords2"", ""ner_keywords2""],          col_title1=""title1"", col_title2=""title2"", use_cd=False,          draw_fig=False, parallel=True, extract_range=None,          betweenness_threshold_coef=1.0, max_c_size=6, min_c_size=2)      dataset2featurefile(          ""../../../../data/raw/event-story-cluster/same_story_doc_pair.txt"",          ""../../../../data/processed/event-story-cluster/same_story_doc_pair.no_cd.json"",          ""label"", ""category1"", ""time1"", ""time2"", ""content1"", ""content2"",          [""keywords1"", ""ner_keywords1""], [""keywords2"", ""ner_keywords2""],          col_title1=""title1"", col_title2=""title2"", use_cd=False,          draw_fig=False, parallel=True, extract_range=None,          betweenness_threshold_coef=1.0, max_c_size=6, min_c_size=2)`  `    其中多次调用了dataset2featurefile这一方法，除第一个参数extract_range设置为range(2)，其余后后面几次都是一样的；请问这样做是否是必要的，实际运行时只保留其中一次调用可以吗，如果是，保留extract_range=range(2)的，还是extract_range=None的呢？谢谢您！"
想问下aggregation layer那里的term based similarity以及Bert做encode层部分代码会放出来吗？  另外就是这里没考虑用pytorch的自带dataloader作为数据加载吗，这样整个模型的Batchsize这部分不好调整
"您好，我在使用graph_tool中的draw_graph画图的时候，画出来的图像没法显示中文，    我看您的code中用了vertex_font_family=""STKaiti""，请问是因为我graph_tool版本的问题吗？"
I want to ask about how to assign document pair to label  ?   Crowdsourcing or other ways?
"Hi, in resource_loader.py,  'event_story_cluster.txt' is not provided and I don't know how this file generate."
can you give me the version of graph-tool and torch 
None
"begin loading DATA............../../../data/processed/event-story-cluster/same_event_doc_pair.cd.json  Traceback (most recent call last):  File ""main.py"", line 102, in  labels, idx_train, idx_val, idx_test = load_graph_data(path, word_to_ix, MAX_LEN, args.num_data)  File ""/home/wting/Documents/code/ArticlePairMatching-master/src/models/CCIG/loader.py"", line 108, in load_graph_data  sent_idx = right_pad_zeros_1d([word_to_ix[w.decode(""utf-8"")] for w in val], max_len)  File ""/home/wting/Documents/code/ArticlePairMatching-master/src/models/CCIG/loader.py"", line 108, in  sent_idx = right_pad_zeros_1d([word_to_ix[w.decode(""utf-8"")] for w in val], max_len)  KeyError: ')'  device is cpu    I meet the keyerror problem, and found the json contains "" ""v_texts_mat"": [["") 之所以 说 圆满 ， 是 因为 这 是 各方 都 能 接受 的 方案 笔者 认为 ， 这 也 算 另外 一 种 意义 的 混 改 吧"", """"], [""对于 王石 为首 的 万科 来说 ，""  I wander why the json failed, thank you for you explainations"
"begin loading DATA............../../../data/processed/event-story-cluster/same_event_doc_pair.cd.json  Traceback (most recent call last):    File ""main.py"", line 102, in        labels, idx_train, idx_val, idx_test = load_graph_data(path, word_to_ix, MAX_LEN, args.num_data)    File ""/home/wting/Documents/code/ArticlePairMatching-master/src/models/CCIG/loader.py"", line 108, in load_graph_data      sent_idx = right_pad_zeros_1d([word_to_ix[w.decode(""utf-8"")] for w in val], max_len)    File ""/home/wting/Documents/code/ArticlePairMatching-master/src/models/CCIG/loader.py"", line 108, in        sent_idx = right_pad_zeros_1d([word_to_ix[w.decode(""utf-8"")] for w in val], max_len)  KeyError: ')'  device is cpu    I meet the keyerror problem, and found the json contains "" ""v_texts_mat"": [["") 之所以 说 圆满 ， 是 因为 这 是 各方 都 能 接受 的 方案 笔者 认为 ， 这 也 算 另外 一 种 意义 的 混 改 吧"", """"], [""对于 王石 为首 的 万科 来说 ，""  I wander why the json failed, thank you for you explainations"
"def load_IDF(data):      if data == ""event_story"":          datafile = ""../../../../data/raw/event-story-cluster/event_story_cluster.txt""          contentfile = ""../../../../data/processed/event-story-cluster/content.txt""          idffile = ""../../../../data/processed/event-story-cluster/IDF.txt""    I meet mistakes : these three txt  no such file or directory"
"As proposed in your paper, you generated local matching vectors in each concept, but what if the concept only contains sentences from one document, do you just ignore these concepts?"
"ub16c9@ub16c9-gpu:/media/ub16c9/fcd84300-9270-4bbd-896a-5e04e79203b7/ub16_prj/ArticlePairMatching/src/models/CCIG$ python3.6 main.py --data_type ""event"" --use_gfeatures  device is cpu  begin loading W2V............  Company  W2V loaded!   Vocab size: 2, Embedding size: 200  Namespace(adjacent='tfidf', beta1=0.8, beta2=0.999, betweenness_threshold_coef=1.0, combine_type='separate', data_type='event', dropout_siamese=0.1, dropout_vfeat=0.1, ema_decay=0.9999, epochs=10, gcn_type='valina', gfeatures_type='features', hidden_final=16, hidden_siamese=128, hidden_vfeat=16, inputdata='event-story-cluster/same_event_doc_pair.no_cd.json', lr=0.001, lr_warm_up_num=1000, max_c_size=6, max_grad_norm=5.0, min_c_size=2, no_cuda=False, no_grad_clip=False, num_data=1000000000, num_gcn_layers=2, outputresult='event-story-cluster/same_event_doc_pair.no_cd.result.txt', pool_type='mean', seed=42, use_cd=False, use_ema=False, use_gcn=False, use_gfeatures=True, use_siamese=False, use_vfeatures=False, vertice='pagerank')  begin loading DATA............../../../data/processed/event-story-cluster/same_event_doc_pair.no_cd.json  Traceback (most recent call last):    File ""main.py"", line 102, in        labels, idx_train, idx_val, idx_test = load_graph_data(path, word_to_ix, MAX_LEN, args.num_data)    File ""/media/ub16c9/fcd84300-9270-4bbd-896a-5e04e79203b7/ub16_prj/ArticlePairMatching/src/models/CCIG/loader.py"", line 81, in load_graph_data      fin = open(path, ""r"", encoding=""utf-8"")  FileNotFoundError: [Errno 2] No such file or directory: '../../../data/processed/event-story-cluster/same_event_doc_pair.no_cd.json'  ub16c9@ub16c9-gpu:/media/ub16c9/fcd84300-9270-4bbd-896a-5e04e79203b7/ub16_prj/ArticlePairMatching/src/models/CCIG$   "
None
"I am interested to use Pretrained Model for Drawer and Teller to accomplish the CoDraw task and generate the conversation and intermediate drawings from Drawer. However, I didn't see an instruction provided in README on how to do it.    I wonder if you could help me to give me some hints on which code script should I take a look to understand how to produce the intermediate conversation and drawings. Thanks."
"Thanks for the great work!   The benchmark mainly use style of binary class (e.g. positive v.s. negative).   Could the model work on style of multiple classes? (e.g. write style 1, write style 2, ... ,write style N)"
"   1. The output of `global_config.predefined_word_index.values()` are indices of some words, not words.  2. At this point, the actual value of this `global_config.predefined_word_index` is equal to `word_index`, not only `{' ': 0,' ': 1,' ': 2}`.  3. Therefore, I think that this `blacklisted_words ` contains unnecessary words and does not match the meaning of the blacklist."
"Hello. Thanks for sharing your work.   I trained a model following the steps in README and ran  the evaluation using the run_all_evaluator.sh  It turns out most of the metrics are identicle to the results reported in your paper except PPL.   The results for my trained model are :  ll_scores: [(-9.701861720617387, 106.5074394250216), (-10.269295644873736, 120.9065905583248)]  The mean PPL is 113.7  However, the results should be around 32.  I think it may attribute to a different vocabulary or training KenLM with different corpus. I directly used the yelp_corpus_adapter for data preparation and  yelp/reviews-train.txt to train KenLM.   Did I miss something ? "
"Hi,    I wanted to pre-train word embedding models like these as you wrote in README.    ./scripts/run_word_vector_training.sh \  --text-file-path ${TRAINING_TEXT_FILE_PATH} \  --model-file-path ${WORD_EMBEDDINGS_PATH}      but, why do you differentiate the **word_embeddings_path** and **validation__word_embedding_path** ?    Then, my question is   - Can I use glove.6B.100d.txt for pre-training word embedding step?  - If not, which embedding files should I use in this case?    Thank you for releasing your code :)"
"Hi,    I came back to kenlm/build after installing boost and eigen.  But there was no CMakeCache.txt file in that folder, so 'rm CMakeCache.txt' does not work.  Is removing that file necessary?    Thank you so much for reading this issue :)  Best regards,  Hyeseon"
"Hi,    I am using Ubuntu 16.04.5 LTS and tried to install Boost & Eigen from (  and I have a question about the location of Boost & Eigen. where should I locate Boots & Eigen? or it doesn't matter?    and /path/to/ means the location of zip file for boost? am I correct?    Thank you so much for releasing your code and reading my issue :)    Best regards,  Hyeseon"
"kl loss may be computed by    def kl_divergence(p, q):         return tf.reduce_sum(p * tf.log(p/q))    So I don't understand what get_kl_loss and sample_prior mean.....    def get_kl_loss(self, mu, log_sigma):          return tf.reduce_mean(              input_tensor=-0.5 * tf.reduce_sum(                  input_tensor=1 + log_sigma - tf.square(mu) - tf.exp(log_sigma),                  axis=1))"
"File ""linguistic_style_transfer_model/models/adversarial_autoencoder.py"",         line 645, in train         shuffled_one_hot_labels, inverse_word_index, current_epoch, sess)  File ""linguistic_style_transfer_model/models/adversarial_autoencoder.py"",         line 655, in run_validation         glove_model = content_preservation.load_glove_model(options.validation_embeddings_file_path)  File ""inguistic_style_transfer_model/evaluators/content_preservation.py""        , line 22, in load_glove_model         embedding = np.array([float(val) for val in split_line[1:]])    I didn't find any glov model for validation, so i just use train word embedding"
"- Use a single empirical style vector, and sample from the unit Gaussian to set the content vector   - Use the decoder weights to generate a sentence"
Hello.  Where I can find the file: data/opinion-lexicon/sentiment-words.txt?  
"Hello.  Thank you for your work.    I tried to reproduce the results on the Yelp data.  I used the latest version of the code.  I didn't change the model parameters.  Data from:    I train w2v models.  I train style classifier (its quality I reproduced).     But I have strange results:     `{""word-overlap"": 0.24729206951072533, ""epoch"": 26, ""style-transfer"": 0.4400895856662934, ""content-preservation"": 0.978698748140737}  `    I use DEBUG mode and I have this output:  `09-05T18:16:44: validating label 0  2018-09-05 18:16:46.028885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0  2018-09-05 18:16:46.028951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:  2018-09-05 18:16:46.028975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0   2018-09-05 18:16:46.028996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N   2018-09-05 18:16:46.029157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5143 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)  09-05T18:16:46: style_transfer_score: 0.0  09-05T18:16:46: confusion_matrix:  [[1819  181]   [   0    0]]  09-05T18:16:46: Skipped lines: [] :-: []  09-05T18:16:46: Skipped lines: ['well', 'done'] :-: ['nope']  09-05T18:16:46: Skipped lines: ['foods', 'great'] :-: ['strike', 'num']  09-05T18:16:46: Skipped lines: ['great', 'wings'] :-: ['num']  09-05T18:16:46: Skipped lines: ['great'] :-: ['num']  09-05T18:16:46: Skipped lines: [] :-: []  09-05T18:16:46: Skipped lines: ['well', 'done'] :-: ['nope']  09-05T18:16:46: Skipped lines: ['great', 'subs'] :-: ['num']  09-05T18:16:46: 8 lines skipped due to errors  09-05T18:16:46: content_preservation_score: 0.9862392492341229  09-05T18:16:46: word_overlap_score: 0.27697874119639415    09-05T18:16:46: validating label 1  2018-09-05 18:16:48.106678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0  2018-09-05 18:16:48.106732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:  2018-09-05 18:16:48.106755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0   2018-09-05 18:16:48.106770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N   2018-09-05 18:16:48.106938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5143 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)  09-05T18:16:48: style_transfer_score: 0.9301952393688153  09-05T18:16:48: confusion_matrix:  [[   0    0]   [ 261 1739]]  09-05T18:16:48: Skipped lines: [] :-: ['do', 'yourself', 'a', 'favor', 'and', 'work', 'here']  09-05T18:16:48: Skipped lines: ['i', 'enjoyed', 'their', 'pollo', 'bowl'] :-: []  09-05T18:16:48: Skipped lines: ['great', 'place', 'to', 'take', 'a', 'family', 'or', 'business', 'partners'] :-: []  09-05T18:16:48: Skipped lines: [] :-: ['my', 'favorite', 'place', 'to', 'have', 'a', 'great', 'choice']  09-05T18:16:48: Skipped lines: ['some', 'nice', 'coins', 'priced', 'at', 'market', 'values'] :-: []  09-05T18:16:48: 5 lines skipped due to errors  09-05T18:16:48: content_preservation_score: 0.9648154781145771  09-05T18:16:48: word_overlap_score: 0.016776839826839827  09-05T18:16:48: Aggregate Style Transfer: 0.46509761968440766  09-05T18:16:48: Aggregate Content Preservation: 0.97552736367435  09-05T18:16:48: Aggregate Word Overlap: 0.14687779051161698  `    Could you help me, please?"
None
"Move from a linear graph for the priors, to a set of dynamic priors based on the number of labels."
Align encoded and sampled spaces using a cross-entropy loss
None
"Hi:    Thanks for the repo. I am interested in testing  the lyrics generating option and would appreciate links to the files required for ""lyrics_adapter.py""    artist-song-line.top30artists.txt  lyrics-val.txt  artist-val.txt  lyrics-test.txt  artist-test.txt  lyrics-train.txt  artist-train.txt  lyrics-all.txt  artist-all.txt    If this is not possible could you please provide an example of the text format required for processing.  I would like to use my own corpus files, for example, Project Gutenberg etexts.    Cheers    "
None
None
"Posterior collapse, KL loss term is too low"
None
None
None
* Remove sentiment and stop words  * score = len(intersection) / len(union)
None
None
None
Pickle and reuse vocab details
None
"Merge the output and saved models and pickled files under a single directory, re-runnable for inference"
None
None
None
None
None
None
None
None
None
None
None
None
None
Options:  * Decoding RNN  * MLP BoW
None
None
None
Only for the label classifiers
None
There should be no UNK and EOS tokens in the ground truth.
None
None
None
None
None
* ReLU - avoid due to it being a sparse gradient  * Leaky ReLU
None
None
None
None
None
No EOS (end of sentence) tokens are being predicted by the decoder.
None
None
None
None
None
None
Add a masking layers of 0s and 1s to the RNN encoder.
* Dropout  * Batch normalization
Word2Vec + GloVe + FastText
"The decoder RNN output at each time-step should be conditioned not only on the rnn_output but also the prediction of the previous step.     The ground truth shouldn't be fed, because at inference time, we expect the decoder to output words that are different from the source sentences."
The gradients should be propagated through separate sets of differentiable variables for both the Adversarial component and the Reconstruction component.    The sets are not mutually exclusive. They will have some common variables to update.
"Your WordGCN paper is very exciting and very well written, so I want to try to use your code in my current work, and I would like to ask you some questions.  For training SynGCN and SemGCN, If I try to use other text data such as transcripts of speech recognition benchmark corpus (AMI) rather than the Wikipedia corpus and receive the AMI corpus-based SynGCN and SemGCN word embeddings, what is the first step I need to do, or how to process my own text data.  Thanks!    Shih-Hsuan"
"I downloaded the pretrained SynGCN embeddings from your WordGCN github and then run the script ""python semgcn.py -embed ./embeddings/syngcn_embeddings.txt  -gpu 0 -epoch 10  -name fine_tuned_embeddings"", but after the model was successfully trained, I cannot find/get the finetuned SemGCN embeddings. What should I do?  Thanks!"
why i will have this question？   
`from models import model`  where is the `models`
"In Figure 2 of your paper, for `hypernym` relation the edge direction is `water -> liquid`. In NLTK WordNet API, `liquid` is the `hypernym` of `water`, why the edge direction is not `water <- liquid`?"
I wonder how did you obtained lists of semantic relation pairs in the folder `semantic_info`?
"I am trying to replicate your code with the same dataset which you used for training. My code stops running once it enters run_epoch function in syngcn.py code.  The issue seems to appear in self.getBatches(shuffle), however I ran make command and BatchGen.so is created. So not really sure why my code stops running without any error.   <img width=""1038"" alt=""Screen Shot 2020-03-13 at 10 27 40 PM"" src=""   "
"I think your work is really interesting. But Does it have any meaning after the BERT model?   It seems your work is like word2vec or Glove, these static word embedding. Can it encode text dynamically like BERT?   "
I can‘t find a package for “from web.embedding import Embedding”
Described in detail here:          Using python 3.5.0 resolved the issue
"If i just want to train the model, do i need to replace the data with the web_data you provided, such as synGCN?  And if i fine-tuned the symGCN using your provided embedding, should I replace the data?"
"If I pre-train a model, how can I use to process the text? Is it same as word2vec or Glove?"
"Hello, I am running ""python syngcn.py -name test_embeddings -gpu 0 -dump ""to get the URL error. I want to ask how to solve this error and how to crawl the dataset after it has been downloaded? Thank you very much for your help.    !   "
"Hi @svjan5 ,    Thanks for your paper, as well as releasing the code.    I follow your current code and default settings, after several runs it seems hard to reproduce your reported result on test set.     My results on five runs with deviation are like below:    **Analogy task:**    |           | Google | MSR  | SemEval2012_2|  |-----------|-------|-------|-------|  | our    | 45.16±1.61 | 49.41±0.60 | 16.28±1.73 |  | reported |        | 52.8 | 23.4 |    **Similarity task**    |          | MEN        | WS353      | WS353R     | WS353S     | SimLex999  | RW         | RG65       | MTurk      | TR9856     |  |----------|------------|------------|------------|------------|------------|------------|------------|------------|------------|  | our      | 69.99±﻿0.19 | 58.35±0.52 | 43.51±﻿1.68 | 70.68±0.51 | 47.61±0.29 | 37.91±0.47 | 58.19±1.33 | 59.90±0.86 | 17.23±0.26 |  | reported |            |            | 45.7       | 73.2       | 45.5       | 33.7       |            |            |            |    **Categorisation task**    |          | AP         | BLESS      | Batting    | ESSLI_2c   | ESSLI_2b   | ESSLI_1a   |  |----------|------------|------------|------------|------------|------------|------------|  | our      | 59.22±1.97 | 69.04±0.86 | 39.50±1.34 | 67.41±3.63 | 77.78±6.20 | 80.67±1.33 |  | reported | 69.3       | 85.2       | 45.2       |            |            |            |    As you can see, there is a large gap for tasks like SemEval2012_2 and categorisation tasks. The deviations for several tasks are also a little bit large.    I wonder where did I go wrong? Forgive my carelessness, Is there anything I missed?  "
"Hi @svjan5 ,    I am curious about the stopping criteria for training you used in the paper. Is it the same as in the code by depending on the average score of all sorts of word similarity/analogy/categorisation tasks? Because I found that using the average score to save best model brought so much stochastics. Though the final average scores are similar for multiple runs, the score for specific tasks could have huge differences. Besides, do you think it is plausible to use those intrinsic tasks to decide the best model during training given the fact that you would evaluate your model on those tasks for comparison with other models?    Best,  Qiwei"
"Hi @svjan5 ,  After reading your source code, I have found some places that make me confused. In the paper, the formula you mentioned is like:  !   And where:  !   while in your code, it is like this:     It seems to me that the calculated `in_t` or `inp_in` is never used when enable gating which might not align with the formula where there is a multiplication in between. And the weight `w_in` and `w_out` would never be updated in the code. May you please give me some information how the calculated `in_t` or the `w_in` and `w_out`  are used under gating mechanism in your code?    Many thanks.    "
"Hi,    Thanks very much for your work, it's really impressive. I have managed to run the code with default setting on given dataset which consists of 57 million sentences on a Titan V, it takes around 18 hours to go over just one epoch(I noticed that your negative samples are set to be 100, wouldn't it be too large?). I wonder would it be possible for you to also release a pre-trained checkpoint and may I also ask your gpu and runtime?    Many thanks.  "
"Hi,Thank you for your amazing work  However,this code doesn't seem to contain the part of how to build the syntactic graph. can i ask for sharing the code of this part?   thank you very much."
"I clone the bug fixed code and run it with setting max length to 50, 70, 90, 150, **segmentation fault** comes as before. My TF version is gpu-1.12 and run it on two pieces of Tesla K80. "
"Hi,  Thank you for your paper, as well as releasing the code.  I follow your source code with your default settings, we obtain a poor result. The experimental setup as shown below:    2019-09-22 11:04:39,606 - test_embeddings_22_09_2019_11:04:39 - [INFO] - {'embed_loc': None, 'gcn_layer': 1, 'batch_size': 512, 'sample': 0.0001, 'lr': 0.001, 'config_dir': './config/', 'dropout': 1.0, 'max_epochs': 5, 'total_sents': 56974869, 'num_neg': 25, 'log_dir': './log/', 'side_int': 10000, 'log_db': 'aaai_runs', 'emb_dir': './embeddings/', 'opt': 'adam', 'onlyDump': False, 'restore': False, 'l2': 0.0, 'context': False, 'gpu': '0', 'seed': 1234, 'name': 'test_embeddings_22_09_2019_11:04:39', 'embed_dim': 300}      | WS353S | WS353R | SimLex999 | RW | AP | Battig | BLESS | SemEval2012 | MSR|  SynGCN | 73.2 | 45.7 | 45.5 | 33.7 | 69.3 | 45.2 | 85.2 | 23.4 | 52.8  our imp.  | 75.4 | 39.9 | 44.7 | 30.1 | 66.8 | 44.9 | 77.0 | 21.5  | 41.3    Where did I go wrong?      "
"Hi,  I got a problem while trying to generate my own data.txt.  Specifically, I found that the initial data.txt is not of the format you have mentioned in README.md. (As follows)  `    tok1 tok2 tok3 ... tokn dep_e1 dep_e2 .... dep_em`  They are actually organized like this (the first line of the initial data.txt file)  `15 14 15 24351 24351 10 7 436 2083 26 8385 121958 4986 215 13 6932 2293 2 1|0|26 5|1|11 5|2|23 5|3|34 5|4|7 7|6|11 5|7|9 9|8|7 7|9|38 9|10|13 13|11|2 13|12|7 10|13|16 5|14|10 21854 21854 3 15 659 2324 0 2397 0 479 328 4 5905 7965 0`  which have 4 parts, the first part '15 14 15' --I guess they are the numbers of the latter three parts? So what does the latter three parts represent?    I re-read the 'batch_generator.cpp', and it seems the last part of each line (i.e. the sequence of numbers after the dependency relations) are read but not stored.  Therefore, would it work if I set the first three numbers as (number of words in the sentence, number of dependency relations, 0), and leave the last part empty?    This problem confuses me for a long time... I tried setting the last part the same as the sentence tokens, and it kept showing segmentation fault...     Would you please give a description of data.txt, and also update the README.md?    Thanks!  @svjan5  "
"Hi,     I came up with a problem while using semgcn to fine tune the given 'syngcn_embeddings.txt'.   `sudo python3 semgcn.py -embed ./embeddings/syngcn_embeddings.txt -semantic synonyms -embed_dim 300 -name fine_tuned_embeddings -epoch 10 -gpu 0`    Everything seems going well. However, after the training finished (and it printed success message), I found nothing in the ./embeddings directory except for syngcn_embeddings.txt, which I put it there as the training set.   In addition, the log files were output to ./log successfully, and those evaluations in them accord with the data given in the paper.     I tried several times and it turned out the same.  Could anyone please tell me why this happened? Thanks!"
"Hi,  Nice paper!     I had some problem during running 'semgcn.py'.  I downloaded pretrained 300-dimensional SynGCN embeddings from your README.md, and tried to fine tune them using semgcn.py. Here is how I typed:  `sudo python3 semgcn.py -embed ./embeddings/syngcn_embeddings.txt -semantic synonyms -embed_dim 300 -name fine_tuned_embeddings -gpu 0`    However, after the percentile hit 100, some Error occurred:         I'm sure that my Ubuntu is properly connected to the Internet. So why does this happen?    Thanks!"
"Hello,  Thank you for your paper, as well as releasing the code.     My question is about Processing of edges in the GCN, the original paper differentiated incomings and outgoing arcs by modeling two matrices, each one for each direction. They do so to avoid overparametrize the model (when adding reversed edges), also I saw that you compute the self loop vector, but you did not include it in the update formulae.     Could you tell me if I misunderstood the code.    Thank you    Kindly regards"
"Hi,   To test and validate the embeddings, I need to ""replace the original ~/web_data folder with the provided one"", but I don't know who to replace the dataset in word-embeddings-benchmark project. The evaluation tool will automatically download datasets from google drive.   Can you provide a more detailed instruction?   Thank you very much!"
Do you have scripts available/any easy way to convert raw data to your processed dataset files . So that i can test your on my own dataset .
"Hi, when I run ""python syngcn.py -name test_embeddings -gpu 0"". A problem has arisen, Segmentation fault (core dumped).Have you encountered such a problem?  Thank you a lot."
"Hello,there are some problem with me.  When I run ""python syngcn.py -name test_embeddings -gpu 0"", It has a error   ""ModuleNotFoundError: No module named 'web""  So I run ""pip install web.py"",  Then ""ModuleNotFoundError: No module named 'web.embedding"".  I wanna to know how can I use web.embedding.  Thanks."
"Hi,    The WordGCN is an interesting thing. However, I can't compile the batch_generator.cpp with the command ""make"" following the Readme. Meanwhile, the requirements can not be found in the repository.    **g++ batch_generator.cpp -o batchGen.so -fPIC -shared -pthread -O3 -march=native -std=c++11   batch_generator.cpp:96:3: error: expected identifier before ‘)’ token     ) {     ^  makefile:2: recipe for target 'all' failed  make: *** [all] Error 1**    Thanks."
"     As the attachment, the train_splits/dia125_utt3.mp4 is corrupted, and could not be used to extract features.    I put it here in case someone has a similar problem.  "
"I could run audio unimodal and bimodal BC-LSTM pretrained model, but got the following error when running text unimodal BC-LSTM.       The command I'm running is:  ` python baseline.py -classify sentiment -modality text -test`.   "
"Dear professor    i am really sorry to disturb you. I add the class_weight={0: 4.0, 1: 15.0, 2: 15.0, 3: 3.0, 4: 1.0, 5: 6.0, 6: 3.0}.  the prceision is zero                   precision    recall  f1-score   support               0     0.4919    0.9889    0.6570      1256             1     0.0000    0.0000    0.0000       281             2     0.0000    0.0000    0.0000        50             3     0.0000    0.0000    0.0000       208             4     0.0000    0.0000    0.0000       402             5     0.0000    0.0000    0.0000        68             6     0.4353    0.1072    0.1721       345       micro avg     0.4900    0.4900    0.4900      2610     macro avg     0.1325    0.1566    0.1184      2610  weighted avg     0.2942    0.4900    0.3389      2610    Weighted FScore:    (0.2942449206381084, 0.4900383141762452, 0.3388985544501019, None)    How to fix it?  thanks, best wishes  "
"Hi, I have tried the bc_LSTM baseline with bimodal in emotion classification, but the F1-score and accuracy of 'fear' and 'disgust' are always zero, so I can't reproduce the result in paper.    The command I use:    python baseline.py -classify emotion -modality bimodal -train    The results:              precision    recall  f1-score   support           0     0.7322    0.7795    0.7551      1256         1     0.4799    0.4662    0.4729       281         2     0.0000    0.0000    0.0000        50         3     0.2781    0.2019    0.2340       208         4     0.4813    0.5448    0.5111       402         5     0.0000    0.0000    0.0000        68         6     0.3832    0.4377    0.4087       345  The emotion labels:    Emotion - {'neutral': 0, 'surprise': 1, 'fear': 2, 'sadness': 3, 'joy': 4, 'disgust': 5, 'anger': 6}.    I know the main strategy is to adjust the class weight. To be hnoest, I'm new to tensorflow. I don't know what codes are needed to add to achieve it. Could you please give me some suggestion?      Best wishes>  "
"when i run python baseline.py -classify [Sentiment|Emotion] -modality [text|audio|bimodal] [-train|-test].    it's errors. how to fix it? thanks    error                                     Traceback (most recent call last)    in    ----> 1 get_ipython().run_line_magic('run', 'baseline.py -classify [Sentiment|Emotion] -modality [text|audio|bimodal] [-train|-test]')    F:\Anaconda\lib\site-packages\IPython\core\interactiveshell.py in run_line_magic(self, magic_name, line, _stack_depth)     2325                 kwargs['local_ns'] = self.get_local_scope(stack_depth)     2326             with self.builtin_trap:  -> 2327                 result = fn(*args, **kwargs)     2328             return result     2329       in run(self, parameter_s, runner, file_finder)    F:\Anaconda\lib\site-packages\IPython\core\magic.py in  (f, *a, **k)      185     # but it's overkill for just that one bit of state.      186     def magic_deco(arg):  --> 187         call = lambda f, *a, **k: f(*a, **k)      188       189         if callable(arg):    F:\Anaconda\lib\site-packages\IPython\core\magics\execution.py in run(self, parameter_s, runner, file_finder)      736         else:      737             # tilde and glob expansion  --> 738             args = shellglob(map(os.path.expanduser,  arg_lst[1:]))      739       740         sys.argv = [filename] + args  # put in the proper filename    F:\Anaconda\lib\site-packages\IPython\utils\path.py in shellglob(args)      324     unescape = unescape_glob if sys.platform != 'win32' else lambda x: x      325     for a in args:  --> 326         expanded.extend(glob.glob(a) or [unescape(a)])      327     return expanded      328     F:\Anaconda\lib\glob.py in glob(pathname, recursive)       19     zero or more directories and subdirectories.       20     """"""  ---> 21     return list(iglob(pathname, recursive=recursive))       22        23 def iglob(pathname, *, recursive=False):    F:\Anaconda\lib\glob.py in _iglob(pathname, recursive, dironly)       55             yield from _glob2(dirname, basename, dironly)       56         else:  ---> 57             yield from _glob1(dirname, basename, dironly)       58         return       59     # `os.path.split()` returns the argument itself as a dirname if it is a    F:\Anaconda\lib\glob.py in _glob1(dirname, pattern, dironly)       83     if not _ishidden(pattern):       84         names = (x for x in names if not _ishidden(x))  ---> 85     return fnmatch.filter(names, pattern)       86        87 def _glob0(dirname, basename, dironly):    F:\Anaconda\lib\fnmatch.py in filter(names, pat)       50     result = []       51     pat = os.path.normcase(pat)  ---> 52     match = _compile_pattern(pat)       53     if os.path is posixpath:       54         # normcase on posix is NOP. Optimize it away from the loop.    F:\Anaconda\lib\fnmatch.py in _compile_pattern(pat)       44     else:       45         res = translate(pat)  ---> 46     return re.compile(res).match       47        48 def filter(names, pat):    F:\Anaconda\lib\re.py in compile(pattern, flags)      250 def compile(pattern, flags=0):      251     ""Compile a regular expression pattern, returning a Pattern object.""  --> 252     return _compile(pattern, flags)      253       254 def purge():    F:\Anaconda\lib\re.py in _compile(pattern, flags)      302     if not sre_compile.isstring(pattern):      303         raise TypeError(""first argument must be string or compiled pattern"")  --> 304     p = sre_compile.compile(pattern, flags)      305     if not (flags & DEBUG):      306         if len(_cache) >= _MAXCACHE:    F:\Anaconda\lib\sre_compile.py in compile(p, flags)      762     if isstring(p):      763         pattern = p  --> 764         p = sre_parse.parse(p, flags)      765     else:      766         pattern = None    F:\Anaconda\lib\sre_parse.py in parse(str, flags, state)      946       947     try:  --> 948         p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)      949     except Verbose:      950         # the VERBOSE flag was switched on inside the pattern.  to be    F:\Anaconda\lib\sre_parse.py in _parse_sub(source, state, verbose, nested)      441     start = source.tell()      442     while True:  --> 443         itemsappend(_parse(source, state, verbose, nested + 1,      444                            not nested and not items))      445         if not sourcematch(""|""):    F:\Anaconda\lib\sre_parse.py in _parse(source, state, verbose, nested, first)      832             sub_verbose = ((verbose or (add_flags & SRE_FLAG_VERBOSE)) and      833                            not (del_flags & SRE_FLAG_VERBOSE))  --> 834             p = _parse_sub(source, state, sub_verbose, nested + 1)      835             if not source.match("")""):      836                 raise source.error(""missing ), unterminated subpattern"",    F:\Anaconda\lib\sre_parse.py in _parse_sub(source, state, verbose, nested)      441     start = source.tell()      442     while True:  --> 443         itemsappend(_parse(source, state, verbose, nested + 1,      444                            not nested and not items))      445         if not sourcematch(""|""):    F:\Anaconda\lib\sre_parse.py in _parse(source, state, verbose, nested, first)      596                     if hi   598                         raise source.error(msg, len(this) + 1 + len(that))      599                     setappend((RANGE, (lo, hi)))      600                 else:    error: bad character range \|-t at position 12    "
Adding requirements.txt will be very helpful to fix environment issues.
The download link to the audio data doesn't appear to be working. Am I missing something?
"In MELD/data_emotion.p, why is word_idx_map[','] = 6459, whereas W.shape = (6336, 300)?  Is this a bug?"
"Obviously this issue was already brought up at      The alignment is pretty bad. It's hard for me to go multimodal at the moment, because of this issue.    I have two questions:    1. Has this been fixed? Or are you planning on using a better alignment tool?  1. Can I have access to the original friends videos? I wonder if I can cut the videos into utterances myself using ASR. "
"Hello! I noticed that there is a ""visual_features.tar.gz"" file in the   file url you provided. I have downloaded it and decompressed it, and the file structure is as follows:      - matlab_resnet_faces      - train          - frame_1_1.mat          - frame_1_2.mat          - frame_x_x.mat          ...      - dev      - test    Because there is no README file, so I cannot know the meaning or source of these files. Can you explain it to me? Hope to hear from you. Thank you!"
"The raw dataset, in dev dataset is missing the dialog 110-utterance 7  video."
"Hi professor,    I find that there is no No.60 dialogue in data/MELD/train.csv, can you check it?    Best wishes,  Mian"
"Dear Prof. Poria,  I've downloaded the raw dataset from the offical   because I want to extract multimodal features by myself. However, I find that the video 'dia110_utt7.mp4' (Sr No. 1153 in the 'dev_sent_emo.csv')  does not exist in the 'dev_splits_complete' folder. Could you verify this problem and update the dataset?     Besides, I notice that there are additional videos in video folders which have no corresponding annotations in csv files. For example,  'dia66_utt9', 'dia49_utt5', 'dia49_utt4', 'dia66_utt10' in the dev set and 'dia108_utt2', 'final_videos_testdia101_utt0' in the test set. Could you tell me what are videos used for?    Thanks very much!  "
"I used   to download the audio files for this data set.    However, there are a few problems with at least a few of the video files and/or their transcriptions:    -   is way off as it's Ross and Julie meeting Rachel at the airport, not Phoebe talking to Joey )    -   still persist (e.g. dia793_utt0.mp4).     Have they been solved? Have I perhaps downloaded an old version of the data set?"
"Do you have any expectations for how the GPL v3 license should apply to weights in a model trained using the data as part of a training set, with no use of the software or the pretrained models?  I was looking to include this as training data either in Stanford's stanza package, which has an apache v2 license, or CoreNLP software, which has a separate license for commercial applications."
"Hello, i have downloaded the pre-trained models from the link provided in the repository, but baseline.py doesn't provide any way to use my own audio/video (.mp3/.mp4) files directly. The authors are loading pickle files instead.     Can somebody give me any scripts so i can use the model with my own audio/video files.    Any help would be really appreciated."
"You mention that feature selection was done using opensmile with initial feature set of 6373, and then feature selection was performed.    What is the config file used for feature extraction ? is it Compare_2016? and how exactly did you do the feature selection? is it possible to provide the indices or names of selected features? Also, audio_emotion.pkl has 122 features that are all zeros out of the 300 selected so they do not provide any information"
"Hi, I have tried the bc_LSTM baseline with bimodal in emotion classification, but the F1-score and accuracy of 'fear' and 'disgust' are always zero, so I can't reproduce the result in paper.     The command I use:    `python baseline.py -classify emotion -modality bimodal -train`    The results:                  precision    recall  f1-score   support               0     0.7322    0.7795    0.7551      1256             1     0.4799    0.4662    0.4729       281             2     0.0000    0.0000    0.0000        50             3     0.2781    0.2019    0.2340       208             4     0.4813    0.5448    0.5111       402             5     0.0000    0.0000    0.0000        68             6     0.3832    0.4377    0.4087       345    The emotion labels:    Emotion - {'neutral': 0, 'surprise': 1, 'fear': 2, 'sadness': 3, 'joy': 4, 'disgust': 5, 'anger': 6}.    Is there something wrong with my understanding?"
"First of all, sorry if this is not the correct place to post this question.    I tried the following commands several times but could not download the raw data.       And I confirmed that I also cannot access Prof. Mihalcea's website, where the data is located.    Could you please check if you can access this download link?  Thank you."
"Whenever below is called,       tensorflow throws an error         Perhaps `utter` is not the right name of the layer?      Running on python3.6, running below python packages       "
!python baseline.py -classify Emotion -modality bimodal -train  Wanted to run baseline script after having download feature file in ./data/pickles/ dir. But going into this issue. data_emotion.p file is missing and I wonder where I can get it.
Thank you for releasing the baseline code and the feature files.   Can you open the feature extraction code for speech and text of the MELD dataset?  Thank you.
I need the Raw Data to extract features according to my project needs. I am unable to Download the features from       The link   is not loading. Kindly look into it. Thanks
I am trying to run the baseline.py file to test the model for Emotion classification and text modality by using the already trained models (the source of which were provided in the README file).    I am using the following command:  `python baseline.py -classify Emotion -modality text -test`    The error that I am getting after running this command is as follows:       Can someone please help me resolve this issue?
"The following error is popping up     Traceback (most recent call last):    File ""C:/Users/Sanil Andhare/.PyCharm2019.1/FFP/baseline/baseline.py"", line 10, in        from baseline.data_helpers import Dataloader    File ""C:\Users\Sanil Andhare\.PyCharm2019.1\FFP\baseline\baseline.py"", line 10, in        from baseline.data_helpers import Dataloader  ModuleNotFoundError: No module named 'baseline.data_helpers'; 'baseline' is not a package    Process finished with exit code 1  "
    from  baseline.data_helpers import Dataloader  ModuleNotFoundError: No module named 'baseline.data_helpers'; 'baseline' is not a package
"Hi, thanks for making this open source project.    I'm trying to use the pre-trained models you provide on my own audio files in order to extract the emotion and sentiment labels, but the `baseline.py` does not seem to provide a way to use my own files.    Moreover, the `baseline.py` file loads `.pkl` instead of `.wav` or `.mp4` files. How would I go about using my own files an generating similar `.pkl` files in order to be used with the pre-trained models?    Thanks."
"1. **Audio**    There is a disturbance in audio which would have affected the audio features.    Few Examples:  dia793_utt0.mp4  dia164_utt5.mp4  dia682_utt1.mp4  dia529_utt2.mp4  dia1029_utt1.mp4  dia1008_utt1.mp4    Mostly all videos with size > 2.5 MB (around 200 videos in train_set)    2. **Video and text are not matching.**    For example     a) dialogue 241. In utterance 1 the sync breaks between the text and the video  utterance 2 in text is ""I asked him."" while video dia241_utt2.mp4 has just word ""now"" and the sync issues goes on.    b) dialogue 757 utterance 7 is also not synced with the text.    c) diaglogue 485 utterance 0 in text ""Hey, this-  Heyy..."" but the video is a long clip.    There are many more video-text sync issues.     Is this dataset usable?  Please help me with this.  "
"Hi,  I am struggling to find any support for developing a model for a multi-model dataset .  Can you please guide or give me some reference for using this dataset and developing a LSTM or CNN model.  I am new to this field but for now i am able to develop model for images and text separately but having trouble in using a merged input (image+text or image+audio ).    Please provide some direction or explain with respect to the baseline model you provided.  Thanks"
"Hi,   I tried to reload the pretrained models, but failed. I assume it is a keras version problem. Would you like to describe the running enviroment? Sorry, I cannot find it in Readme.  Thanks."
"Hi, Meld crew.   I tried running baseline code with text-only sentiment classification, and it worked. However, I got one question in _baseline_.py (Line 124). It's about the input_length in Embedding layer. I think it should be the sentence_length instead of the sequence_length, since the 2nd dimension of the _concatenated_tensor_ is a negative number (-48) in my case. "
"Hi there,   I've downloaded MELD.Raw.tar.gz, the dev and test data is ok, but the training data is missing.  when I untar train.tar.gz, it always shows the following issue:  gzip: stdin: unexpected end of file  tar: Unexpected EOF in archive  tar: Unexpected EOF in archive  tar: Error is not recoverable: exiting now  and after that, I got 'split' directory which contains 4648 files. So I believe the training data is definitely missing.  I tried many times, always the same results.  I wish I can get some help with that."
"!   Can you exlpain what features each file represents？I'm a little confused about their file names. After reading the codes in basline.py and data_helper.py, you use cnn to extract the textual-features. Does it mean the vedio features? In your fusion model, I can't find the vedio branch.  And what does text_glove_average_emotion.pkl mean?  And what's the difference between audio_embeddings_feature_selection_emotion.pkl and audio_emotion.pkl?"
Got this error while untar     gzip: stdin: unexpected end of file  tar: Unexpected EOF in archive  tar: Unexpected EOF in archive  tar: Error is not recoverable: exiting now  
"I am learning emotion recognition.So I downloaded your project,but the website of   that I can not enter.Can you send to my .And the others should I download,I can not enter either. Sincerely thank you .Email:982905429@qq.com"
There are important files that Microsoft projects should all have that are not present in this repository. A pull request has been opened to add the missing file(s). When the pr is merged this issue will be closed automatically.  Microsoft teams can   within the open source guidance available internally.    
"I'm trying to replicate your code but I get an error when I load the CoNLL dataset for either ESP or NED, I get the error that 'utf-8' codec can't decode byte .... in position .....    This I can solve by specifying encoding in open() but I am curious if you did any preprocessing of the CoNLL files such that you don't get the same error. I took the CoNLL 2002 data from the official website       Thank you!"
Thanks for sharing the code and data.    It seems that you didn't include the multilingual slot-filling dataset in this repo. Is this dataset still **NOT** publicly available?
"We have options `--no_fix_emb` and `--no_fix_charemb` for not fixing the word and character embeddings respectively. But the argument parser misses the ""dest"" option due to which the embeddings are always fixed despite specifying the options.    Listing below the line numbers in the options file corresponding to the same.        "
"Merry Christmas!    In the beginning, I run the repo on 1080TI with 11GB memory, the out of memory error has been shown. Then, I switch to Titan RTX with 24GB memory. This situation is the same. After that, I choose to run on CPU. Memory is not enough neither.       Could you please tell me how much memory do this repo need? And how to reduce memory consumption? Such as changing the batch size of other operations? Thanks."
"I managed to collect and construct data to run the repo. Now I am facing the following issues, when I executed `./scripts/train_conll_ner_3to1.sh CoNLLE1`         Which version of cuda and cudnn do you use? And in what way do you install PyTorchNet?    Thanks."
"I am afraid that I cannot get `eng.train` from the links of  CoNLL 2002, 2003. Could you give me some suggestions?    By the way, the original link of   is broken. Could you please upload it please? Thanks."
"Thanks for your great work.  Could you please descript the details commands and parameter to constructing word embeddings of MUSE, VECMAP and UMWE. I am not sure the source and target embedding files and their parameters.  Relly thanks."
"hello,the following question has been bothering me  './scripts/train_conll_ner_3to1.sh {exp_name}'    what is the specific '{exp_name}' example?  "
"Hello, thank you for the source code. I am very interested in your thesis and source code, but I am still a newbie. For the data download, I have some questions about the download of multilingual word embeddings. For the download of MUSE, VecMap, UMWE three multilingual word embeddings, it has been a long time to explore for me but it is still unreasonable. If you are convenient, can you give me some guidance on how to download these multilingual word embeddings? Grateful!"
"In the code it has 'train.pkl', but the original data was in xml.  Could you specify how to process the dataset please?      "
"Requirements are not documented. A requirements.txt file would be very helpful. Also, it appears that pytorch must be compiled with CUDA support, implying the need for CUDA at the system level.    Somewhat related to dependencies in general: the use of Spacy corpus shortcuts is obsolete. The corpus reference in similarity_scorer.py should be to a specific corpus rather than ""en"""
"the paper shows that there are 720k easy and hard sentences sepecifically,  but the download data has 2000000 row hard and 2000000 row easy sentences sepecifically(the file is tsdata/fkdifficpart-2m-1.lower and tsdata/fkeasycpart-2m-1.lower).  WHy?    !     "
"hello,  I meet this error when run the train.sh  !   Can someone explain the reason,Thx!"
"Hi,    Thanks for sharing your work.  Can you please indicate which of the prediction files correspond to which scores reported in the paper?    Prediction files I am talking about:  `gen_lower.sen.src2trg.wgan.onlyback.denoi.back1.singleclassf.rho1.0.10k.22000.test.noredund`  `gen_lower.sen.src2trg.wgan.semisup10k-sel-6-4.noadvcompl.control1.allclass.denoi.singleclassf.rho1.0.10k.10000.test.noredund`  `gen_lower.sen.src2trg.wgan.unsup.noadvcompl.control1.allclass.denoi.singleclassf.rho1.0.10k.13500.test.noredund`    Thank you for your help."
What is the recommended system to run the code?
"Dear  The following error occurs when I set ""enable_mgan"" parameter as true randomly and at different times.      STEP 6400 x 36 no of loggers 6    time for 100 steps : 294.9260549545288 sec    .    .    .    File ""...\UnsupNTS\undreamt\undreamt\discriminator.py"", line 63, in forward      x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)    File ""...\UnsupNTS\undreamt\undreamt\discriminator.py"", line 63, in        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)    File ""...\torch\nn\modules\module.py"", line 547, in __call__      result = self.forward(*input, **kwargs)    File ""...\torch\nn\modules\conv.py"", line 343, in forward      return self.conv2d_forward(input, self.weight)    File ""...\torch\nn\modules\conv.py"", line 340, in conv2d_forward      self.padding, self.dilation, self.groups)    **RuntimeError: Calculated padded input size per channel: (4 x 600). Kernel size: (5 x 600). Kernel size can't be greater than actual input size**"
None
Running translate.sh can't find references.tsv file? I downloaded tsdata.zip and it doesn't have it. What does references.tsv should be?
"Hello! Your work is great, but I am confused about the annotations:    In the readme file of data file folder, you give one example from the files of the form `2007-12-17.train-a.* and the corresponding annotations are 993 1000 -.    But when I use python to process raw/ascii/tok file, the line at 992 is not  '[03:41]   amitprakash...'. I also use my VSCode to check these txt files and I think these problems are caused by ""when to start a new line"": In your given example, the response from 'ubotu' is split into six lines while in the raw/ascii/tok file there is only one line for this response.     So I would like to know how to fix these disorders or if I misunderstand anything.    Thanks,  "
"I converted the output (auto) to graphs with `output-from-py-to-graph.py` and then to clusters with `graph-to-cluster.py`.     I am now attempting to use the `conversation-eval.py` file to get cluster metrics but I am running into an issue finding how to create the `gold` file which is input to this script. What would be the proper way to convert the annotation files to ""gold clusters""?    Thank for any help."
"Hi, I was trying to reproduce your results in the paper, and I might be wrong, but it seems that this part of the code from `majority_vote.py` (54-58) is inconsistent with what's mentioned in the paper?     In the paper you said:  > ... combine output by keeping the edges they all agree on. Link messages with no agreed antecedent to themselves.    In the code, it seems that you:  1. do not keep the edges that all agree on if they are self-links, and  2. if there's no agreed antecedent, you link it to the most-voted link IF the most-voted one is not a self-link?    As a result, I wasn't able to reproduce the results reported on your paper."
"As noted in the README.md, the test data is being withheld until the end of the DSTC 8 shared task. This issue is a reminder to add them after that (ie. in November)."
@jkkummerfeld can you please check the irc-disentanglement/tools/ folder which is empty ?
"Thank you so much for creating this great dataset.    I have uploaded the NLVR dataset to tagtog for easier visualization and exploration of the data.    Here the project's link with its guidelines/README:      Here for instance a sample:      It looks like this:    <img width=""500"" alt=""64972786-2c40e080-d8aa-11e9-9f17-abb79826997c"" src=""     --    Do you have some thoughts? Feedback? It would be interesting to entirely explore the NLVR2 dataset too.  "
"I am looking at the distributions of labels for sentences across structured representations, and wanted to check if my observations are correct. I grouped structured representations by gathering those in examples whose identifiers have the same ""m"" values, where each identifier is of the form ""m-n"".    I imagined I would find each sentence occurring with four different structured representations, two in which it is true and two in which it is false. However, I see that  1) Only 3358 out of 3696 sentences in train, dev and test occur with 4 different structured representations, and others have less than four structures.  2) Only 2176 out of the 3696 sentences have equal numbers of true and false labels.  3) 465 out of the 3696 sentences have all true or all false labels.    I can see how (1) may have happened during post-processing to remove examples with low agreement, but I am not sure about (2) and (3). Please let me know if this is expected.    Thanks!"
"This isn't necessarily an issue, but rather an observation. It seems there are quite a few misspelt words in the example sentences.     Here is the output of a script which takes all examples, extracts their sentences, splits on spaces, lowercases all words, and uniqfies the words to form a simple vocabulary:     > ['1', 'more', '', '**isa**', '**yelow**', '.', 'has', 'numbers', 'block', 'having', 'have', 'no', 'base', 'leats', 'item,', 'back', 'other.', 'all', 'triangle,', 'odd', 'circles', 'circle', '**od**', 'but', 'two', '**adge**', 'black.', 'another', 'at', 'every', '**nealy**', 'and', 'bases', 'touching', 'underneath', 't', 'sthe', 'box', 'same', 'one.', 'count', 'one', 'other', 'among', 'directly', 'objects', '**color**.', 'do', '**wirh**', 'bow', 'to', 'only.', '3', 'left', 'third', 'middle', 'than', 'that', 'less', 'closely', 'line', 'items,', 'shape.', 'out', 'boxes.', 'line.', 'bellow', 'ones.', 'triangle.', '**wwith**', 'item', 'first', 'lot', 'nearly', '**ablue**', 'triangle', 'coloured', 'not', 'both', 'box,', 'lease', 'box.', 'square,', 's', 'set', 'different.', 'over', 'colors.', 'alternately', 'them', 'just', 'in', 'between', 'smaller', '2', '**yelloe**', 'objects,', '**squere**', 'which', 'small', 'bottom-right', 'blue.', 'where', 'most', 'traingles.', '**ble**', 'either', '**blicks**', 'near', 'each', 'side', 'grey', ',', 'traingle', 'block.', 'bottom', 'beneath', '5', 'an', 'touhing', 'blue,', 'items.', 'bottom.', 'containing', 'positioned.', 'tocuhing', 'they', 'corner', 'height', 'base.', 'lest', 'square', 'three.', 'blocks,', 'consecutive', 'are', 'one,', 'total', 'yellow', 'different', 'towers', 'top.', 'item.', '**yelllow**', '**block/**', 'none', 'size.', '**bule**', 'shapes', 'objects.', 'triangles.', 'only', 'number', 'some', 'size', 'least', 'ans', 'their', 'colour', '**objetcs**', 'yellow.', '**ciircles**', 'black,', 'any', 'second', 'corners.', 'middle.', 'contain', 'six', 'wih', 'medium', 'including', 'black', 'color', 'yellow,', 'circles.', 'attach', 'under', 'shape', 'all.', 'wth', 'height.', 'blacks', 'attached.', 'blocks', 'right', '**atleast**', 'four', '**exacty**', 'tow', 'theer', 'block,', 'each.', 'ones', '**bloxk**', '**ia**', 'bases.', 'big', 'exactly', 'items', 'then,idle', '**blccks**', 'roof', 'squares.', '**eactly**', 'this', 'attached', 'wall.', 'that.', 'colors', '**exacrly**', 'is', 'corner.', 'ha', 'triangles', 'below', '**bo**', '**opis**', '6', 'edge', 'squares', 'thee', 'or', 'with', 'a', 'there', 'colored', 'exacts', 'towers.', 'boxes', 'hte', 'lower', 'positions', 'made', 'three', 'kinds', '**egde**', 'top', 'almost', 'it.', 'four.', 'it', 'single', 'type', 'most.', '**cirlce**', 'i', '**abox**.', 'blocks.', 'same.', 'square.', 'blue', 'after', 'colours', 'bkack', 'together', 'colour.', 'ad', 'its', 'even', 'close', '**tleast**', '4', 'the', 'five', 'seven', '**trianlge**', 'circle,', 'position.', 'tower.', 'as', 'above', 'stacked', 'without', 'al', 'many', '**circ;e**', 'tower', 'blocks..', 'side.', 'from', 'multiple', 'object', 'level.', 'stack', 'rectangle', '**b;ue'**, 'of', 'tower,', 'being', 'object.', 'circle.', 'on', '**sqaures**', 'contains', 'wall', '**ll**']    I was just wondering whether there's any agreed upon convention for preprocessing the text? "
"Would you be able to license the repository data and code? For the data, I might suggest the possible combination of ODbL (  for the database) and CC0-1.0 (  for the individual images)."
"In image features + RNN method, you use color, shape and size etc. to construct a set of feature for  every object index. Let's take color and shape into account for example. Given a image with two objects:  Object 1 is represented as： color[0,0,1] shape [1,0,0]  Object 2 is represented as :  color [0,1,0] shape[0,1,0]    You said  you use the concatenation of the one-hot image features to compute the image embeddings  with two layers of size 32.    Does it mean that  you first concatenate the features of a Object, namely Object 1 ->[0,0,1,1,0,0]  and Object 2 ->[0,1,0,0,1,0]. And then they will be  put into Embedding Layer1 to produce vector e1 and e2.  e1 and e2 will be  concatenated again and then put into Embedding Layer2 to produce final image Embedding?    Or it means concatenates all features of all Objects,namely image->[0,0,1,1,0,0,0,1,0,0,1,0] and then it will be embedded by two Embedding Layers?    Maybe both of my comprehensions are wrong.     Thanks you for this cool corpus.    "
"I found that there are many typos in the dataset (e.g., ciircles, yelllow, yelloe, ...). I can send out a small PR that fixes some of the sentences with typos. But I was wondering whether you just want to keep the dataset the same as it is now (i.g., just leave the typos as they are) ?    Thanks for creating this cool dataset."
"The given images have 4 colour channels and the last channel looks like this,    !     Is there any particular reason for this?  "
"When I try to parse any of json files in python or julia, I got the following error:        "
"I'm trying to get the image feature tar files, but the links in the readme don't seem to work anymore.  Is there any place I can access these files?"
"Hello ，it seems that in run_*.sh  `MODEL_PATH=""/path/to/variational-multimodal-nmt-model-snapshots""  MODEL_FILE_NAME=""MMT_VI_Model_TranslatedM30K""`    are not exist.    > FileNotFoundError: [Errno 2] No such file or directory: '/home2/zy/ali/variational_mmt/data/model/MMT_VI_Model_TranslatedM30K.fixed-prior_BestModelBleu.pt'  > Using GPU  > Traceback (most recent call last):  >   File ""translate_mm_vi.py"", line 183, in    >     main()  >   File ""translate_mm_vi.py"", line 70, in main  >     map_location=lambda storage, loc: storage)  >   File ""/home2/zy/miniconda3/envs/pt13/lib/python3.6/site-packages/torch/serialization.py"", line 419, in load  >     f = open(f, 'rb')  > FileNotFoundError: [Errno 2] No such file or directory: '/home2/zy/ali/variational_mmt/data/model/MMT_VI_Model_TranslatedM30K.conditional-prior_BestModelBleu.pt'  > -ne Finished. Translations of valid/test 2016/test 2017 (Flickr and ambiguous MSCOCO) can be found in:  > /home2/zy/ali/variational_mmt/data/model/MMT_VI_Model_TranslatedM30K.{fixed,conditional}-prior_BestModelBleu.pt.{validation,test2016,test2017,test2017_mscoco}-translations  "
This code looks illogical.    If `new_opt` doesn't have `fasttext` set `new_opt.fasttext` on `saved_opt`.      
In google colab this command raises exception.       Found fix at      Notebook is here        
"Hello since you can't provide the dataset, in what format should we prepare the data from reddit to match the program requirements  "
Where can I get Reddit embeddings and Reddit Data?
Do you provided the model trained?
"Since there are still no generation codes released, it would be nice to have some generation results.   Could you provide some generation results of the baseline models?  "
"When I tried to load the train.csv, I observed these errors:  1. Rows 58466, 2355, 37523, 67237 have the same text for 'prompt' and 'utterance' columns. The real text for utterance is in a wrong column.  2.  Due to these wrong indentations, pandas always throws a error as these rows have unequal number of columns with the rest.    I haven't checked for valid.csv and test.csv. please, fix these in the files.    Thank you"
"Hello, I fine-tuned your model and now I would like to evaluate it, to have a feeling of how it works. When I give the command          I get the following error:       How do you suggest to fix this? Thanks!"
"Hi @EricMichaelSmith, I was going through the data preparation for ED dataset in empchat.py file and found that the speaker utterance is also taken as label with prev conversations utterances as context. I am a little confused as to why would you take speaker utterance as a label when you want a response in listener role only?    Is data preparation different for generation and retrieval tasks?    Please clarify this.  Thank you."
Where should I download reddit data? 
"Hi, I'm confused about the dataset since there is no readme file. I found the column names are the following:    conv_id | utterance_idx | context | prompt | speaker_idx | utterance | selfeval | tags    does 'context' means the sentiment of each utterence ? And what do 'prompt', 'selfeval' and 'tags' mean ?     thanks,             "
"Hi,  May I know what uid & p2c in reddit.py ?"
"Hi,  I would like to ask in which dataset the classifier was pretrained and what was the architecture of the classifier."
"I successfully ran the retrieval model (transformer and BERT), where is the generative model?"
"As I understand, ED used   - bert tokenizer to embed   - and use embedding output as bert encoder input,   - bert encoder try to minimized negative loglikelihood of y* and y^ , in this case, y^ is the responses ground truth for each input y and x, y* is response predicted through bert encoder model? > is that right?    and another phase is generative base I marked it like a bert decoder - because bert doen't have a tokenizer decoder , so we train a transformer like a decoder to get a sentence from bert encoder output?    I also mention before that transformer has many architure right now (huggingface), so it makes confuse to everybody come up with this method.  Hope you answer these questions"
"Hello, I was going through the dataset and I noticed that the valid and test sets have an additional column containing candidate responses. I don't see this additional column being referenced in the dataloader.     The original paper mentions that the candidates were sampled from three sources - the training set, dailydialogs and reddit conversations and the validation code seems to do exactly that.     I'm not sure if I'm misreading the code or if this repo's evaluation code does not represent the current state of how training and evaluation is done for this dataset. Can I get some clarity on this? Thanks in advance for your help!"
"I saw in paper said that you use the full Transformer architecture (Vaswani et al.,  2017), but you know transformers had many architectures right now, GPT2, Bert, Robert, ... and each architecture has its own tasks."
"Hi,  I have a problem for the generative model. In the paper, you used the full-transformer model for pre-train and fine-tune. I am wondering another generative model - GPT2. Since you haven't released the fine-tuned full-transformer generative model and I don't have enough resources to replicate your outcome for comparison, I would like to ask:    From your perspective, if I use ED to fine-tune GPT2 model, what will be the performance for that(both automated metric and human ratings)? will there be a sacrifice compared with full-transformer model? Since there is no encoder parts in GPT2, but there are multiple layer of decoders. Thanks"
"Thanks for reading this post~  I wonder how you design your human-evaluation standard?  Here is a case that responsor give a responce in very empathetic way but totally not at the point.  A: Oh,Shit!I fell down just now,and hurt myself.  B:It must be horrible for you to stand under the sun at such a hot day.  Will this dialogue get high empathy score in human evaluation?"
"the original name and parent_id of a commemt are all a str like ""t1_XXX"", and when will it be 6 as a deleted comment?  `        if ""bert_tokenizer"" in dict_:              self.using_bert = True              assert BERT_ID == ""bert-base-cased""              deleted_uid = -1          else:              self.using_bert = False              deleted_uid = 6`"
"Hi, I am getting after running the retrieval command I am getting the error:    File ""/Users/ikram/opt/anaconda3/lib/python3.7/site-packages/pytorch_pretrained_bert/tokenization.py"", line 109, in tokenize      if self.do_basic_tokenize:  AttributeError: 'BertTokenizer' object has no attribute 'do_basic_tokenize'    However I cant locate the file to make requisite changes. What would you advise? Thanks! "
"Hi,    I was wondering if the generative prepend models (EmoPrepend and TopicPrepend) involve any pre-trained BERT weights? From my understanding, it seems that you first trained the prepend models on Reddit and then fine-tuned them on ED, right? And for prepend models, you only experimented with 4-layer transformers but not 5-layer (denoted as ""Large"" in the paper)?    Another related question would be, when you trained the prepend models on Reddit, you still predicted the labels based on the input context and prepended them in the front, am I correct?    Thanks for your time!  Yubo"
"What's the interpretation of the `selfeval` field in the dataset?    For example, what's the meaning of `4|3|4_3|5|5` in        Thanks"
"Sorry, maybe a little wiered. I wonder to know how you collected 25k situations? just built by human‘s imagination？  I think it is a big work to build data by self."
"Hello, I recently read your paper. Your work is very meaningful for the dialogue system, so I want to track your work. However, due to my limited technical level, I have some questions about the code and hope to get your advice.  When using your code, how should the files in the data folder mentioned in the parameters be obtained? For example, how does ""word dictionary"" in ""reddit dir"" get?"
"I am kind of confused about where the 32 emotion labels from. Because some emotions are really close to another, I try to find some resources to find the difference between them.  I read all of the references mentioned in the image below but I failed to find all of the 32 emotions in these papers.   Would you provide some information about this?    !     "
"I don't really understand the relationship betwwen hit id and conv id, and can you explain the case in dataset below :  !   I think in hit 1: conv 3, speaker and listener should talk about  ""I showed a guy how to run a good bead in welding class and he caught on quick"" with ""proud"" emotion"
"Hi, may I know how to convert the raw data in Reddit dataset to chunk.pth loaded in reddit.py? I have downloaded reddit dataset, but I have no idea how to process the raw data so that this raw data can work in RedditDataset class in reddit.py.    I have checked  , but I still can not understand how to deal with the format in the required file."
None
"Hi, May I know what files should be present in the REDDIT _DATA_FOLDER and what are the formats of those files? so that it becomes easy for me to convert raw Reddit dataset into required files necessary for pre-training the model."
"Hi, I construct my dataset for train, valid and test split for multi-turn response selection task:  for a session: a, b, c, d, e, f  get dataset using this way(for reactonly, randomly sample 99 negative responses for one positive sample):  a, b  a, b, c, d  a, b, c, d, e, f  I conduct experiment using interaction-based bert model (easy to conduct). Concretely, I concatenate context and response, and find this task is too easy as P100@1, MAP is 0.9~. There is a huge difference between 0.9~ and your result 0.5~. I think there shouldn't be such a large difference between biencoder-type and concatenate-type results. But I cann't explain it. Could you analysis the reason? Thanks."
"Hi, I have several questions regarding retrieval-based model        1. How do you get 100 candidates at inference time in calculating P@1, 100      2. At training time, you use all of the utterances from the batch as candidates to minimize the negative log-likelihood of selecting the correct candidate. Why not sample negative examples of a certain proportion. For example, sample 9 negative examples for one positive example. Did you compare these two methods?    Looking forward to your reply.  Best wishes"
"Hi, I have several questions regarding retrieval-based model        1. How do you get 100 candidates at inference time in calculating P@1, 100      2. At training time, you use all of the utterances from the batch as candidates to minimize the negative log-likelihood of selecting the correct candidate. Why not sample negative examples of a certain proportion. For example, sample 9 negative examples for one positive example. Did you compare these two methods?    Looking forward to your reply.  Best wishes  "
"Hi there,    I have a few questions regarding the emotion classifier in the EmoPrepend model:  1. Is the pre-trained classifier available?  2. What data did you use to train the emotion classifier? Is it the context-prompt pairs in the ED dataset?  3. It is mentioned in the paper that history utterances are concatenated into one and sent to the encoder, so in the case of EmoPrepend, is it correct that the input has the following format: [emotion_label_1] utterance_1 [emotion_label_2] utterance_2 ...?    Thanks for your response!"
"Hi! Is there a way to obtain the 1.7B Reddit dataset? Thanks!    Best,  Yubo"
are pre-trained models available?
"Do you have any plans to release the code for the generative model using the Transformer. I trained the full Transformer on Reddit dataset but got random responses. I got low cross-entropy loss for my validation set, so I don't know why is the case.    Thanks,  Peixiang"
"I am curious about the emotion classification accuracy on the test set after multi-task fine-tuning training. Currently, I am running experiments on your dataset and would like to compare the results.    Looking forward to your reply. Appreciate your help and time.    Thank you  "
"Can we download pretrained models to use them for inference?   (Also what is time-to-response for this?)   As i see in paper, it takes 2-3 days to train myself"
Thanks for your response.
Please add a requirements.txt with compatible third-party versions or some means to create conda evnironments.  Installing the latest versions breaks the `bucc.sh` script and it's tedious to debug and figure out the mismatches of installed libraries.    I have run these commands:     but I still get an Attribute error when running           
"Hi,  At the end of section 4.3 it says there is a special procedure for high resource languages where only fwd scores are calculated.    in the code if we set ""fwd"" instead of ""max"" it is supposed to calculate only the fwd stuff BUT when scoring it need both the x2y_mean AND the y2x_mean, the later is ot being calculated.    So what should it be in the scoring formula ?    Many thanks."
"I tried to mine from two wmt news files (en, de) - generated the embeddings with bilstm.93langs.2018-12-26.pt - then mine_bitexts.py with defaults settings (""mine"", ""ratio"") but the results is very bad even though the margin seems good:  top lines:  any clue ?      1.6953904090950678 Affordable and flexible fee payment structure. Inzwischen sieht das schon ganz anders aus.  1.6196460840402447 Having toured as Beyoncé""s bassist and assistant musical director, she also has her own career as a soloist and songwriter, not to mention a new, much younger audience. Er schreibt: ""Mit unglaublicher Trauer muss ich die Nachricht über den Tod meiner wunderschönen Tochter Maia teilen.  1.6135828150470815 As their popularity continues to increase, we will probably end up more focused on what happens after our favourite reality shows than what happens on them. Der Kreativität seien mithin keine Grenzen gesetzt, betont er.  1.566564394910487 With steroid medication making little difference, Karina realised that she was suffering from topical steroid addiction (TSA) and topical steroid withdrawal (TSW), when the skin reacts adversely after long-term use of topical steroids is stopped, Timo Werner kam hingegen nur auf sechs Tore und wurde mit seiner Chancenverschwendung in England phasenweise zur Witzfigur.  1.555322152863677 ""I would like to thank the management and backroom team for their unwavering support and commitment and the clubs, supporters, Club Iarmhi and Westmeath County Board. SN/www.picturedesk.com Vizekanzler Werner Kogler und Klubchefin Sigrid Maurer.  1.5525301272930254 The next two corners of finance to feel the invasion of quants will be the corporate bond market - where systematic strategies are now beginning to spread - and private equity, Rattray predicts. Ismaning - Die Dramatik mit den Ismaninger Abschlussversuchen zeichnete sich in der ersten Hälfte noch nicht ab, denn die war nur wenig aufregend. Unter dem Strich hatten die Dachauer etwas mehr vom Spiel, Ismaning lauerte auf Konter und de facto neutralisierten sich alle. Der FCI präsentierte sich im defensiven Verhalten deutlich besser als zuletzt gegen Hallbergmoos und ließ nur Schüsse zu, die Torwart Radic sicher hatte.  1.5299819314567646 He's not a typical freshman, though. Das teilten Polizei und Staatsanwaltschaft am Sonntag gemeinsam mit.  1.5084004481504976 Move over hard seltzer, a new beverage is poised to become the drink of summer. Die gesellschaftliche Spaltung Israels, so viel steht fest, kann mit dieser Wahl nicht überwunden werden.  1.5046071137197825 The store is advising customers to visit its website in the first instance, before making a trip to store, where there is also an opportunity to utilise its click and collect service. If you choose this option, the team will have your items ready for you to collect within one hour. Extreme Änderungen wolle er aber nicht vornehmen: ""Jetzt noch mal alles über den Haufen zu schmeißen, wäre auch nicht zielführend"". Freilich werde er weiterhin jungen Spielern ""eine Plattform geben, sich zu zeigen.  1.4632964789078255 We are psychologists, data scientists and HR consultants who screen, select, develop, and engage talent worldwide. Wem in der Pandemie etwas langweilig geworden und das Rasenmähen oder andere laute Gartenarbeiten eine willkommene Abwechslung ist, sollte nicht übermütig an die Sache herangehen. Denn bei Geräten wie einem motorbetriebenen Rasenmäher müssen die Ruhezeiten eingehalten sowie die Sonntags- und Feiertagsruhe respektiert werden. Haben die Nachbarn es sich nach dem Mittagessen gerade draußen bequem gemacht, sollte man besser noch etwas warten, bevor man den Rasenmäher aufheulen lässt.  "
"For my university FYP project related to text simplification, there's a requirement for me to generate LASER embeddings for a large number of sentences. (15.7 million) However when I try to generate LASER embeddings using the `SentenceEncoder` in the `embed.py`, the program stays fully utilized for around 12 hours and then exits without any error (I assume it is because of the high CPU and GPU utilization). I'm using the `SentenceEncoder` in the following way.    Initialize the `SentenceEncoder` with the following params. I'm using the pretrained encoder (`models/bilstm.93langs.2018-12-26.pt` )      And then generate LASER embeddings as follows.        I tried to execute the setup with above params in a GCP compute engine with 16 cores with 102 GB memory and 1 Nvidia Tesla T4 GPU. The CPU utilization reaches `100%` while the GPU utilization is somewhere around `90%`. It stays like that for around 12 hours and exits without any error. (no error in `nohup.out`).     Any idea about what could go wrong ? I'm stucked at this point for several weeks and really appreciate if someone can help me.       cc @hoschwenk "
"I just looked at   and the availablility/unavailability of certain language combinations is so unexpected, it almost certainly points to deep flaws in the pipeline that produced them.    So if we pick a random medium-resource language, like Armenian, we find that there is no Armenian-English nor Armenian-Russian (nor -French, -German, -Spanish...) where we would expect a lot of parallel data.    But there is Armenian-Burmese and Armenian-Khmer, which seems unlikely given that they are very low-resource languages, and have zero contact or cultural overlap or bureacratic overlap (e.g. Council of Europe, Eurovision, CIS...).    In fact, all the pairings of Burmese `my` and Khmer `km` are unlikely and suspicious.    <img width=""779"" alt=""Screenshot 2021-07-13 at 12 28 23"" src=""   <img width=""624"" alt=""Screenshot 2021-07-13 at 12 28 32"" src=""     "
"Hello,    I'm sending a list of sentences to the Flask API being served in the Docker container.  It looks as though it get an embedding for the first sentence and nothing more.     HTTP request looks good, with all my queries showing up...     However, when i inspect my objects I'm only seeing one embedding for the first sentence in my dataset     "
"What is the utility of the lower_case parameter for the TokenLine function ? In a related question, what is the correct way for generating Laser embeddings: should the inputs to this function already be in lower case, or do we need to lower_case the output from this function before applying BPE ?       "
I have been reading   paper to replicate the architecture for my unsupported language pretraining. I was able to grasp everything except learning of language id embedding. Can anyone tell me how embedding is learned? I am assuming it is learned during training as well. Is this true?
"Hello,    i'm new to LASER and machine learning in general. So LASER transforms sentence into 1024-dimensional vector. What are the max/min values for each item (or feature?) in the vector? Are those floats between -0.5 and 0.5?"
This was mentioned internal use of this here:   Will this be publicly released? 
"Hi there,    I was wondering whether it makes sense to ""trick"" LASER to consider a whole document made out of multiple sentences as a single sentence? That way I'd get a whole document embedding and wouldn't need to devise any aggregation method.    I know there's a limit of 12000 tokens on sentences (as per   but let's forget this for now please :)   "
"Good Morning!  Would you release the script/link on how to get the training corpus of LASER?  Since it is a corpus combination, some corpus is cut from the original corpus. It is very difficult to replicate the original training corpus of LASER.    So I want to ask for a script/detailed guide to cut, combine, and process all the original corpus as the LASER does. If there a readily available corpus combination then it would be more wonderful.  I actually just want a make a subset of LASER training corpus containing 'en, es, fr, de, it.'                                                 Regards!                                                          Wei  "
How can I finetune the LASER model for my task? I can't find any sample about it.
"As suggested in the paper, 1024 is used. Could I use other dimension like 256/128 for the final output embedding?"
"Hi, I have some questions about the training details of LASER. In Appendix A it is stated that:    > OpenSubtitles2018: A parallel corpus of movie subtitles in 57 languages. The corpus size varies from a few thousand sentences to more than 50 million. We keep at most 2 million entries for each language pair.    For Chinese and Portuguese, there are separate entries depending on the locale:      * For Chinese we have 2 locales: zh_cn, zh_tw  * For Portuguese we also have 2 locales: pt_br, pt    I'm wondering if in this case we keep 2 million for each locale, for a total of 4 million for Chinese and 4 million for Portuguese, or do we pick 1 million for each locale for a total of 2 million per language.    In addition, how are the 2 million sentences sampled? Is it just the first 2 million for each language pair?    Thank you!"
"class Laser_embedding(nn.Module):    def __init__(self):          super().__init__()          self.out_features = 1024          self.laser = Laser()    def forward(self, input):          print(input)            return self.laser.embed_sentences(sentences= input,lang=[""ar"",""eng""])    laser = Laser_embedding()  list = [""go"",""fast"",""test""]  ouput_embed = np.array(laser(list))  print(ouput_embed.shape)    output is:  ['go', 'fast', 'test']  (2, 1024)"
"I really liked the paper and I'm glad the code is published, but I have one minor problem.  When evaluating the model on a text classification task, I wanted to print not only validation accuracy but also training acc.  To do that I added a line ""corr_train, nbex_train = net.TestCorpus(train_loader, 'Train')"" before line 247 in sent_classif.py. Surprisingly enough, after adding that line the results (on the test set) has changed. I am new to Pytorch, so I'm probably overlooking something, but I can't understand why adding this line has such impact. Would be great to hear if you have any idea @hoschwenk "
"`combine_bidir` is a function in  .  It's used to concatenate the forward and backward hidden tensors from a bidirectional LSTM.         Here `outs` is a tensor of the shape `[num_dir * num_layers, bsz, hidden_size]`.  The goal is to `combine` the tensor to the form `[num_layers, bsz, num_dir * hidden_size]`.    The error is clear as the inner concatenate function should join the tensors on the final dimension instead of the first. Significantly current version of the implementation would mix up tensor values between different entries in the same batch. A quick fix would be to change `dim=0` to `dim=-1`.         However, the code is still rather convoluted and includes one too many for loop which hampers the readability of the code. I suggest using purely reshape and transpose operations for this task.     "
Hello.I need to get multiple languages sentence embedding frequently，so I don't want input/output through file.Is there a way for me to passing input via parameters and get the value returned from the function?Thank you.
"Some language pairs are oddly missing from ...WikiMatrix/list_of_bitexts.txt, against my intuitions on which ones would have more data and thus more matching sentences.    For example, Armenian (`hy`) pairings exist with German, French, Russian, Italian, Spanish and Portuguese, but not with English.    Another odd thing about it is that Armenian Wikipedia is not exactly a low-resource Wikipedia - 30th by number of articles -     ""Southern"" Azerbaijani Turkish (`azb`, the variant spoken in Iran and still written in its original Perso-Arabic script) has a pairing only with French!    For comparison, `azb` has many articles as (post-Soviet) Azerbaijani Turkizh (`az`), which is now written in the Latin alphabet and has pairings with 34 languages.    Similarly, it smells a bit fishy that Chinese and Hindi have only as many pairings as Galician or Esperanto.    But still, not as suspicious as a language having pairs with French but not with English.    I know you used a relatively objective cutoff, and there are many factors in how articles are created that would affect the number of true match, I'm just wondering if there may be a bug.  Most likely related to the handling of non-Latin scripts.  "
"I have a question but not sure if this is the right forum.     The encoder starts with word embeddings to generate a sentence embedding. The closer the network moves to the sentence embedding, the further it is from the original language. Same applies to the decoder.     Has anybody considered a 2-stage encoder + 2-stage decoder where:  - first stage of the encoder is language-specific (LS)  - second stage of the encoder is language-agnostic (LA)  - decoder is 'symmetric'    Why would that be useful?     The first intuition is that this could be used to have 2 separate training processes. one stage is extremely LS; the other is not. Use same-language translation to train the two LA stages (and only them). Use language pairs to train the two LS stages. Here same language translation just means that the sentence getting into the encoder has to be reproduced by the decoder. Impossible to imagine a bigger training corpus of training examples.    The second intuition is that the two can be done in parallel, and that training 2 half models is cheaper than training the full one.     An alternative way to think about it is to 'explode' the current stage where the encoder output exits into the decoder without any changes. Take the the encoded output, run it through a few stages (BLSTM +/- attention) and, only then, get that into the decoder. The existing encoder/decoder parts would be trained on same-language paris. Intermediate stage trained on language pairs.      Let me know if that's been done elsewhere  or if I should ask on another forum.     Thanks.      "
"If there is no tokenization with `embed.py`, running the script fails:         This is due to the fact that the  . The tokenizing step can handle input from STDIN, but the fastBPE step tries to execute the following command:        ./fast applybpe [TWO SPACES HERE] /tmp/tmpgh9sgiy_/bpe \          tools/laser/models/93langs.fcodes \          tools/laser/models/93langs.fvocab    While the general recipe is        ./fast applybpe output input codes vocab    The temporary output file `/tmp/tmpgh9sgiy_/bpe` is mistaken for the codes file, which makes this assertion fail."
"I am trying to embed some strings, but facing an issue. When passing a list of string containing different language (like French), the imput and output counts are not matching. Like if I pass 4 French strings, I get output of shape (2,1024). In some cases the output increases. It works fine for English, but emoticons or different language characters are resulting in this issue. Any help?     "
"When not using the ""--unify"" option, there still will be an index of unique items built in TextLoadUnify that is based on unique lines. This messes up (at least) the ""--score"" option of scoring sentence pairs.     So, this:        for line in fin:          new_ind = len(sent2ind)          inds.append(sent2ind.setdefault(line, new_ind))          if args.unify:              if inds[-1] == new_ind:                  sents.append(line[:-1])                  nu += 1          else:              sents.append(line[:-1])              nu += 1    should be changed to:        for line in fin:          if args.unify:              new_ind = len(sent2ind)              inds.append(sent2ind.setdefault(line, new_ind))              if inds[-1] == new_ind:                  sents.append(line[:-1])                  nu += 1          else:              sents.append(line[:-1])              inds.append( nu )              nu += 1   "
"I found that in CPU mode, the speed of the generation of embedding is about 26 sentences per hour. Is that slow or normal?"
"Hello,  I have the training data with labels in English. Now, I want to use this data to predict for other languages. I saw XLM and LASER both support the cross-lingual classification. However, they don't have the benchmark on the same dataset, therefore, it's difficult to know which model is better. Does someone help me in determining which(XLM or LASER) is better for cross-lingual classification?    "
"Since `Token()`  , it may be vulnerable to some shell injection attacks.    The Python documentation explicitly warns against this.  "
"Hi,    Is there any option or improvement on finetuning LASER for specific areas?    Thank you!"
We have this error occasionally. It's hard to reproduce it and get significative logging. A very similar error has been found and fixed here:       This may help to find a possibile solution for LASER too.  
"In   an error is produced because `--wildcards` is not a valid option. This is on mac OS with bsdtar 2.8.3. The command can be reduced to: `tar -xf dev.tgz ""dev/newstest2012.??""`"
"  testing of   on Python 3.7.1    $ __flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics__     __E901,E999,F821,F822,F823__ are the ""_showstopper_""   issues that can halt the runtime with a SyntaxError, NameError, etc. These 5 are different from most other flake8 issues which are merely ""style violations"" -- useful for readability but they do not effect runtime safety.  * F821: undefined name `name`  * F822: undefined name `name` in `__all__`  * F823: local variable name referenced before assignment  * E901: SyntaxError or IndentationError  * E999: SyntaxError -- failed to compile a file into an Abstract Syntax Tree  "
"Hello, I found that Classical Chinese sentences do not match their English in WikiMatrix.en-zh.tsv . e.g.    num of lines | English | Chinese | score | Chinese Reference | Translation in Mandarin | Translation in English  -|-|-|-|-|-|-  9 | He is sometimes seen having arguments with his lord. | 正月，朝見其主（王）龔。| 1.20 |  A historical record ( 后汉书 quoted by 风俗通义校注 ) | 一月份，他拜见了他的君主(王)龚。 | In January, he had an audience with his load (Wang) Gong.  11 | Owain is appointed lord. | 以邑名為氏。| 1.19 |  A record about the origin of last names | 以地方的名字作为姓氏 | Take the County Name as their first name.   12 |  Nevertheless when it shall turn to the Lord, the vail shall be taken away. | 當體聖主好生之德，俟其向化。| 1.19 | A war in Qing danysty: 清平王辅臣之战 | 应当体现君主的好生之德，等待他们归服。| It is appropriate to embody the King's mercy on livings, and wait for their come over and pledge allegiance.    The Classical Chinese differs with Modern Chinese on vocabulary, grammar, and genre. And ellipsis phenomenons in Classical Chinese make it quite hard be understood, even for people like me, who have learned it from primary school to college.     In my point of view, we should consider Classical Chinese as a independent language.  It will improve the quality of extracted en-zh bitexts."
Is there any plan to release the code/scripts for training the encoder? I would like to train using my own data. Thanks!
"When trying to install the MLDoc dataset (I assumed that means cloning the MLDoc repo in the /tasks/mldoc directory), I encounter the following error when I run ""bash ./mldoc.sh"":         mldoc.sh seems to be checking for files in the form of ${mldir}/${part}.${l}   (e.g. MLDoc/mldoc.train1000.en or MLDoc/mldoc.dev.fr, etc.)    Any idea where to get these files? All I see in the mldoc packages are files that are named like ""english.train.2000"".    I probably misunderstood what installing the MLDoc package entails...  Thank you!    "
These two runtime error comes out randomly while during an inference. It cannot be reproduced programmatically.       and       Both it seems to be related to the following Pytorch errors:  The `setStorage` error could be related to:      while the latter could directly be related to:      We are using **torch==1.0.0**. My wonder is if this could be also implicit in the pre-trained model since it seems that only some pytorch versions are/were affected.
I got the mecab setup in the right location as mentioned in the docs. But I am not able to get the japanese tokenization working. Anyone seen this before ?          
"Hi,  I am trying to mine some parallel sentences from two large monolingual corpora (over 40M sentences each). In the first step I encoded the two sides and then called `mine_bitexts.py` to do the magic and extract the most probable sentence pairs. However, I faced a memory issue so I decided to just load the embeddings of the target side and to keep the memory footage minimal at each time I just encode one source and try to mine the candidates of that single sentence. But, still I get the following error:    `Faiss assertion 'err__ == cudaSuccess' failed in virtual void faiss::gpu::StandardGpuResources::initializeForDevice(int) at StandardGpuResources.cpp:168; details: CUDA error 2  go-align.sh: line 56: 23772 Aborted                 (core dumped) python ${LASER}/source/mine_bitexts.py`    To reduce even further the memory usage I decrease the batch size so that at each time it just reads a small batch of target embeddings and compares the source embedding with them. But still no success.  This issue seems to be related to  FAISS and I found the following thread in the FAISS issues:     But, couldn't find a solution which works for me. Any ideas about this?  I am running my experiments on 4 Tesla k80 gpus and the corpora contain about 50-60M sentences each  The only other solution that I could think of is to split to target corpus into smaller batches of say 10M sentences and for each source sentence get its most probable candidate in each batch. Then I need to go through the list of all the extracted candidates for each source sentence and return the best as the most similar candidate.  May I ask you if you ever faced this issue and if you have a better solution for it?    Thank you,  Amin"
Is it normal that dropout is applied during the forward inference pass for the sentence embedding?
"The `install_models.sh` file downloads 3 files, one is the `blstm.ep7.9langs-v1.bpej20k.model.py` file and the other two are `ep7.9langs-v1.bpej20k.bin.9xx` & `ep7.9langs-v1.bpej20k.codes.9xx`. `mlenc.py` file says that bpe_codes is ""File with BPE codes (created by learn_bpe.py)."" and on the research paper it is mentioned as ""20k joint vocabulary for all the nine languages"" I created this using learn_bpe.py as mentioned with my own data but I don't quite understand how to create the other two, hash_table ""File with hash table for binarization."" and model ""File with trained model used for encoding.""  Any idea on how I can create hash_table and model? I couldn't find any documentation about them or code sample to train them. Thanks in advance."
"Hi,    I hope you're doing well. I was trying to download the LASER 3 `eng_Latn` for one of my experiments and it seems the link is broken and not active anymore. Can you please take a look and update the link for the checkpoint?  "
I'm currently using LASOR to embed my documents.   My command is:      But I got the error.   ! .     Can anyone face the problem? How can I fix that? 
"Hi,  I follow the instructions from LASER   and train a LASER model with my own data (Chinese-English Bitext).  But when I embed sentences with the trained model, an error occurs as follows:  !     **from the logs and codes, I found the checkpoint file of my trained models is different from laser2.pt,  and the state_dict of my checkpoint file has no ""params"".**  !     So is there something wrong when I trained the LASER model (actually I want to train a Teacher model)?  Thanks for your reply.  "
"Hi, I keep having this issue and am not sure where went wrong. Can anyone take a look at this?     <img width=""939"" alt=""Screen Shot 2022-08-04 at 1 27 31 AM"" src=""     "
!   
"hi,    if i want to embed text in Chinese and text in English,    which model should i download ?"
"hi,    when i tried to run bucc task,    Processing BUCC data in .  2022-07-19 15:46:23,944 | INFO | embed | loading encoder: /Users/liguangyu/LASER/models/bilstm.93langs.2018-12-26.pt  ./bucc.sh: line 82: 78886 Broken pipe: 13         cat ${txt}       78887 Segmentation fault: 11  | python3 ${LASER}/source/embed.py --encoder ${encoder} --token-lang ${ll} --bpe-codes ${bpe_codes} --output ${enc} --verbose  2022-07-19 15:46:25,718 | INFO | embed | loading encoder: /Users/liguangyu/LASER/models/bilstm.93langs.2018-12-26.pt  ./bucc.sh: line 82: 78890 Broken pipe: 13         cat ${txt}       78891 Segmentation fault: 11  | python3 ${LASER}/source/embed.py --encoder ${encoder} --token-lang ${ll} --bpe-codes ${bpe_codes} --output ${enc} --verbose  /Users/liguangyu/LASER/source/mine_bitexts.py:224: SyntaxWarning: ""is not"" with a literal. Did you mean ""!=""?    if args.retrieval is not 'bwd':  /Users/liguangyu/LASER/source/mine_bitexts.py:230: SyntaxWarning: ""is not"" with a literal. Did you mean ""!=""?    if args.retrieval is not 'fwd':  "
"hi,    when i tried to run bucc task,  there are two errors,    what is .enc. file and what is candidates.tsv file ?    FileNotFoundError: [Errno 2] No such file or directory: './embed/bucc2018.fr-en.train.candidates.tsv'  FileNotFoundError: [Errno 2] No such file or directory: './embed/bucc2018.fr-en.train.enc.fr'"
"  ERROR: Command errored out with exit status 1:     command: /Users/liguangyu/opt/anaconda3/envs/laserr/bin/python /Users/liguangyu/opt/anaconda3/envs/laserr/lib/python3.6/site-packages/pip/_vendor/pep517/in_process/_in_process.py build_wheel /var/folders/f3/8jrjgbyj6rb39q8f6hm9fmzr0000gn/T/tmpdkv8cntm         cwd: /private/var/folders/f3/8jrjgbyj6rb39q8f6hm9fmzr0000gn/T/pip-install-5xda94xt/tokenizers_754c49cea1724635a231beb70759f8bd    Complete output (51 lines):    running bdist_wheel    running build    running build_py    creating build    creating build/lib.macosx-10.7-x86_64-3.6    creating build/lib.macosx-10.7-x86_64-3.6/tokenizers    copying py_src/tokenizers/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/tokenizers    creating build/lib.macosx-10.7-x86_64-3.6/tokenizers/models    copying py_src/tokenizers/models/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/tokenizers/models    creating build/lib.macosx-10.7-x86_64-3.6/tokenizers/decoders    copying py_src/tokenizers/decoders/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/tokenizers/decoders    creating build/lib.macosx-10.7-x86_64-3.6/tokenizers/normalizers    copying py_src/tokenizers/normalizers/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/tokenizers/normalizers    creating build/lib.macosx-10.7-x86_64-3.6/tokenizers/pre_tokenizers    copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/tokenizers/pre_tokenizers    creating build/lib.macosx-10.7-x86_64-3.6/tokenizers/processors    copying py_src/tokenizers/processors/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/tokenizers/processors    creating build/lib.macosx-10.7-x86_64-3.6/tokenizers/trainers    copying py_src/tokenizers/trainers/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/tokenizers/trainers    creating build/lib.macosx-10.7-x86_64-3.6/tokenizers/implementations    copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.macosx-10.7-x86_64-3.6/tokenizers/implementations    copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.macosx-10.7-x86_64-3.6/tokenizers/implementations    copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.macosx-10.7-x86_64-3.6/tokenizers/implementations    copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.macosx-10.7-x86_64-3.6/tokenizers/implementations    copying py_src/tokenizers/implementations/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/tokenizers/implementations    copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.macosx-10.7-x86_64-3.6/tokenizers/implementations    copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.macosx-10.7-x86_64-3.6/tokenizers/implementations    creating build/lib.macosx-10.7-x86_64-3.6/tokenizers/tools    copying py_src/tokenizers/tools/__init__.py -> build/lib.macosx-10.7-x86_64-3.6/tokenizers/tools    copying py_src/tokenizers/tools/visualizer.py -> build/lib.macosx-10.7-x86_64-3.6/tokenizers/tools    copying py_src/tokenizers/__init__.pyi -> build/lib.macosx-10.7-x86_64-3.6/tokenizers    copying py_src/tokenizers/models/__init__.pyi -> build/lib.macosx-10.7-x86_64-3.6/tokenizers/models    copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.macosx-10.7-x86_64-3.6/tokenizers/decoders    copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.macosx-10.7-x86_64-3.6/tokenizers/normalizers    copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.macosx-10.7-x86_64-3.6/tokenizers/pre_tokenizers    copying py_src/tokenizers/processors/__init__.pyi -> build/lib.macosx-10.7-x86_64-3.6/tokenizers/processors    copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.macosx-10.7-x86_64-3.6/tokenizers/trainers    copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.macosx-10.7-x86_64-3.6/tokenizers/tools    running build_ext    running build_rust    error: can't find Rust compiler        If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.        To update pip, run:            pip install --upgrade pip        and then retry package installation.        If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at   is the recommended way to download and update the Rust compiler toolchain.    ----------------------------------------    ERROR: Failed building wheel for tokenizers  Failed to build tokenizers  ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects"
"embed.py  1. class HuggingFaceEncoder():      def __init__(self, encoder_name: str, verbose=False):          from sentence_transformers import SentenceTransformer // -------here !!!!!!!!! ------- line 188 , Unresolved reference 'sentence_transformers    2. def EncodeFilep(      encoder, inp_file, out_file, buffer_size=10000, fp16=False, verbose=False  ):      n = 0      t = time.time()      for sentences in buffered_read(inp_file, buffer_size):          encoded = encoder.encode_sentences(sentences)          if fp16:              encoded = encoded.astype(np.float16)          encoded.tofile(out_file)          n += len(sentences)          if verbose and n % 10000 == 0:              loger.info(""encoded {:d} sentences"".format(n)) // -------here !!!!!!!!! ------- line 409, NameError: name 'loger' is not defined      if verbose:          logger.info(f""encoded {n} sentences in {EncodeTime(t)}"")      indexing.py  .format(args.langs[i1], args.langs[i2], // -------here !!!!!!!!! ------- line 126 Unresolved reference 'args'"
"hi,    i noticed that "" , fast C++ implementation of byte-pair encoding (installed automatically)""    but it seems that there is no fastbpe,    and then i tried to install it manually, but it said that :  g++ -std=c++11 -pthread -O3 fastBPE/main.cc -IfastBPE -o fast  clang: error: no such file or directory: 'fastBPE/main.cc'  clang: error: no input files    could anyone know how to solve this problem ?   thanks !  "
"Originally from   even to curl 1000 urls took almost 2 hours, why not just include the text data?"
"I am trying to generate embedding with newly released LASER3 in google colab.    `! /bin/bash ./laser/tasks/embed/embed.sh ./input_text.txt ./output_2.bin ind_Latn`    <img width=""790"" alt=""Screenshot 2022-07-11 at 9 42 35 AM"" src=""     Although my input_text.txt contains 100s of lines of text to encode but, output showing 0 embeddings.     May anybody help ?"
"I am really hyped from the progress meta has made in this much needed offer.     Regardless my background in coding etc i cannot work my way around how these tools work with each other to be able to set everything up for translation from albanian to english.    I managed to encode a sentence in albanian but that was all, dont know any there is no resource in the internet explaining what should be done next.  "
"## :bangbang: This issue is finally solved by myself. Please refer to the solution  . :bangbang:    I follow the instruction in          to embed `example.eng` to `example.eng.emb`:         It runs successfully, but in fact, it doesn't encode any sentences at all (`example.eng` is not empty).    How to resolve this issue? Any suggestions are highly appreciated :pray:!    ---    **UPD:** I guess the models are well-downloaded as well.        Model File Sizes                   Model SHA256 Sums           "
"I pulled the GitHub repo, set the environment variable 'LASER,' and downloaded the encoders and third-party software as instructed. However, whenever I run the `tasks/embed/embed.sh` script, I get the following error message:         The `wmt22` folder under `models` was never installed in the first place. How can I get the embedding script to work? Thank you!"
"Hello  I want to **finetune LASER** using fairseq, it need complete checkpoint (need both encoder and decoder) but provided pretrained checkpoints **contain only encoder**.  Is there any complete checkpoint ?!"
"Hi,    I tried to download cc-matrix data, but it seems that the website is down. Can you please check it?    Thanks."
"Hi,    I'm trying to use the embed.py file and I cannot figure out how to solve the package import issue resulted by   `from text_processing import Token, BPEfastApply`    I have used `pip install text_processing` but the installed package has no Token or BPEfastApply inside. I'm wondering if anyone can point me to the correct package to install here? Thanks,"
"first time using LASER, trying to create embeddings from tatoeba data, getting these errors when I try to run:    ModuleNotFoundError: No module named 'transliterate'    ...    FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmp4p3sxtv8/bpe'    I ran the install .sh scripts before use, but it was still missing dependencies, so I installed numpy, torch, and fastBPE with pip    What can I do to get the setup done properly here?"
"Hi, hoping to use a subset of ccmatrix for research, but running into issues even before downloading:    from the README running pip3 install cc_net returns this:    ERROR: Packages installed from PyPI cannot depend on packages which are not also hosted on PyPI.  cc-net depends on kenlm@ git+      installing that package (kenlm) makes no difference. Any suggestions?"
When running `bash install_external_tools.sh`  fastBPE installation does not succeed in the absense of Cython in the environment. This can be fixed with   `pip install cython`
Thanks for providing the download script but I still have trouble fetching the data.  After patiently waiting for the download to finish and fixing issues with newline characters and malformed Unicode I now get bitexts that do not match in the number of lines when running finalize:         The numbers are close but none of them is the same. I wonder whether anyone already succeeded to actually download and compile the dataset with the provided script? Thanks for any further hints on getting this done.
"We only need the CCMatrix, however, the script “dl_cc_matrix.py” will download the whole CCNet, which is too big for my network bandwidth.    So , it'll be great If you could provide a dumped version of the CCMatrix."
"I have test the embedding time, it is too long for me. Anyone can tell me how to load encoder before embedding.  thank you."
"Hello,    I noticed that Kurdish is only available as one language in this library. You may know, Kurdish is a macro language and consists of three main dialects/languages; Northern Kurdish (Kurmanji), Central Kurdish (Sorani), and Southern Kurdish.    Most Kurdish texts online are written in Kurdish (Kurmanji) (ku + kmr) and Kurdish (Sorani) (ckb).  I suggest adding both Kurmanji and Sorani as two languages. Note that Kurmanji uses the Latin script and Sorani uses the Arabic script.    A few months ago, we fixed this problem with Tatoeba too. Now it supports the three dialects of Kurdish.  "
"hey man.  i have read source/embed.py, i have used embed.sh, and i test the inputfile value in source/embed.py, but i dont find the value of inputfile in embed.sh command i used.  any one can tell me what cause that?  thank you."
"Hi,  thanks for releasing the script to download CCMatrix. I have a few questions regarding the dl_cc_matrix.py  script:  I started the download of CCNet a couple of weeks ago by calling ‘dl_cc_matrix.py dl’. By now it downloaded about 29GB. I’m sure the internet connection is a lot faster than that. Is that download speed normal or did I do something wrong? How much data will roughly be downloaded in total? (to estimate how much longer it will take)    Finally, what happens when I kill and restart the script? Should it continue where it was or start again from the beginning?    I used the script version from commit 9b33d3a1021ed37e02cb5fd590eec70cb54f1a77 since it was the latest version when I started.  "
"Reproduction steps for version 1.0.0:  1. Download sentences from `2019-09_0000.tsv.gz` meta file.       Replace in `dl` function:     `file_list = [l.strip() for l in open_remote_file(metadata_dir + ""/list.txt"")]` with `file_list = [""2019-09_0000.tsv.gz""]`  2.  After that run finalize           I have added code to print last processed lineno.  `zcat  data/v1.0.0/raw/2019-09_0000.tsv.gz | grep -A3 -m1 '2239378'`     `  You can see there are newlines in the eng. text."
"**I have downloaded the ""v1.0.beta"" version of the data and get all the language pairs and I got these numbers:**    !     I don't know why the numbers aren't the same? Is this means that the ""v1.0"" will include the missing data?    Also when will ""v1.0"" be available?    Also regardless the missing languages like English, Japanese and Chinese The other language sentences don't fully aligned for example in ""ar-fr"" we have:  -  3,195,877 Sentences in fr  -  3,134,825 Sentences in ar  - Only 680,546 Sentences are aligned Which is 20% only of the data size  Why not the full data aligned? Is this will be solved in ""v1.0""?    @gwenzek "
"Hi    I've downloaded files listed in `  and extracted the entries in the language information column (e.g., cs-ko/ko) in there.    Then, I've found that there is no English sentence (i.e., ""bla-en/en"" or ""en-bla/en"").    Furthermore, some popular language pairs like es-pt (Spanish and Portuguese) are missing.    Is there something wrong or these pairs are not provided intentionally?"
"What is the shorthand for the Serbian language that the tokeniser understand? I've tried sr, rs, srp, serp, serbian, Serbian and SERBIAN. None works.     - Encoder: loading /xxx/xxx/projects/laser/models/bilstm.93langs.2018-12-26.pt   - Tokenizer:  in language sr    WARNING: No known abbreviations for language 'sr', attempting fall-back to English version..."
"when I run  similarity.py.it raise error normalize-punctuation.perl: not found. I have already run install_external_tools to download it. I found that the URL “  where we download normalize-punctuation.perl is unreachable.when we visit it,it raise 404 . how can I do to solve this problem.  Thanks  "
I found eng-nep model is already trained via   paper. It would be great help for me if it is provided as well. Is there's any possibility to upload this model as well?
"Hi, I wonder when you release the M2M-100 training data? Here said that it would be available on Oct 19th. But I don't found any download link now. "
"CC_ Matrix  Three days ago, some data in v1.0.alpha may not meet the requirements, and the data is the same as the previous v1.0 release. But when I want to run it, all the data is skipped, at least 2018-34-0000 tsv.gz And 2018-34-0001 tsv.gz incorrect. I found that they didn't have a score for each row, so they would jump out when they ran to assert error.  Is it the wrong way I use it?  !     !   "
Some decode error encounter when using `dl_cc_matrix.py` to download:   
"Hi  I am running WMT.sh example in similarity task, here is the output:  thanks for your help  Best  Rabeeh     (internship) rkarimi@italix17:/idiap/user/rkarimi/dev/internship/seq2seq/temp/LASER/tasks/similarity$ bash wmt.sh   LASER: similarity search    Processing:   - loading encoder /idiap/user/rkarimi/dev/internship/seq2seq/temp/LASER/models/bilstm.93langs.2018-12-26.pt   - Tokenizer: newstest2012.tok.cs exists already   - fast BPE: newstest2012.bpe.cs exists already   - Encoder: newstest2012.enc.cs exists already   - embedding: ./embed/newstest2012.enc.cs 0 examples of dim 1024   - creating FAISS index   - Tokenizer: newstest2012.tok.de exists already   - fast BPE: newstest2012.bpe.de exists already   - Encoder: newstest2012.enc.de exists already   - embedding: ./embed/newstest2012.enc.de 0 examples of dim 1024   - creating FAISS index   - Tokenizer: newstest2012.tok.en exists already   - fast BPE: newstest2012.bpe.en exists already   - Encoder: newstest2012.enc.en exists already   - embedding: ./embed/newstest2012.enc.en 0 examples of dim 1024   - creating FAISS index   - Tokenizer: newstest2012.tok.es exists already   - fast BPE: newstest2012.bpe.es exists already   - Encoder: newstest2012.enc.es exists already   - embedding: ./embed/newstest2012.enc.es 0 examples of dim 1024   - creating FAISS index   - Tokenizer: newstest2012.tok.fr exists already   - fast BPE: newstest2012.bpe.fr exists already   - Encoder: newstest2012.enc.fr exists already   - embedding: ./embed/newstest2012.enc.fr 0 examples of dim 1024   - creating FAISS index  /idiap/user/rkarimi/dev/internship/seq2seq/temp/LASER/source/lib/indexing.py:123: RuntimeWarning: invalid value encountered in long_scalars    .astype(int).sum()) / nbex  Confusion matrix:  langs   cs       de       en       es       fr       avg       cs     0.00%     nan%     nan%     nan%     nan%     nan%  de      nan%    0.00%     nan%     nan%     nan%     nan%  en      nan%     nan%    0.00%     nan%     nan%     nan%  es      nan%     nan%     nan%    0.00%     nan%     nan%  fr      nan%     nan%     nan%     nan%    0.00%     nan%  avg     nan%     nan%     nan%     nan%     nan%     nan%      "
"Hi! I'm trying to download the CCMatrix using the script `dl_cc_matrix.py` but it I get this error    `requests.exceptions.HTTPError: 403 Client Error: Forbidden for url:      Any idea to fix it?     Anyway, I'm interested specifically on English-Japanesse data, is there anyway to only download this language pair? Thank you very much!    David"
"Hi @bittlingmayer , @hoschwenk    After confirming that English and French languages work fine, for Serbian, Croatian and Macedonian, none of the language codes works:    bash ./embed.sh ${LASER}/data/tatoeba/v1/tatoeba.srp-eng.srp sr my_embeddings_srp.raw  bash ./embed.sh ${LASER}/data/tatoeba/v1/tatoeba.srp-eng.srp srp my_embeddings_srp.raw  bash ./embed.sh ${LASER}/data/tatoeba/v1/tatoeba.hrv-eng.hrv hrv  my_embeddings_hrv.raw  bash ./embed.sh ${LASER}/data/tatoeba/v1/tatoeba.hrv-eng.hrv hr  my_embeddings_hrv.raw  bash ./embed.sh ${LASER}/data/tatoeba/v1/tatoeba.mkd-eng.mkd mk my_embeddings_mkd.raw  bash ./embed.sh ${LASER}/data/tatoeba/v1/tatoeba.mkd-eng.mkd mkd my_embeddings_mkd.raw  WARNING: No known abbreviations for language 'sr', attempting fall-back to English version...    Similar to   but there was no precise answer where we can find all language codes neither?"
In this file    (   line 9 (from cc_net.process_wet_file import CCSegmentsReader)    The code raises an error.    It seems there is no CCSegmentsReader to import.  (     Am I doing something wrong?  
"Hi, I failed to install fastBPE with error like `cc1plus: error: unrecognized command line option ""-std=c++11""`. Does anyone has similar issue and share your experience?    Thank you."
None
"Hi,    I am learning about Neural Machine Translation and am undertaking a project. I am trying to find similar sentences between my Training and dev/valid sets. I made sentence embeddings using LASER and I'm going to use those to find the k-nearest neighbours of each sentence using 'faiss' (The similarity search library -       My question is what are the embeddings? I know in NMT, word embeddings are vector representations of words, with the distance representing how similare they are). How are sentences translations handles? And how can I use them?    Apologies if this is a simple question. I'm fairly new to all this."
"I split my text files for efficient purposes but I don't how to combine word embeddings.  How to combine these embeddings, is there any advice for this?  "
"Hello all,    Turkish has been shown in the supported languages list. But when I try to use it in Turkish it says    WARNING: No known abbreviations for language 'tr', attempting fall-back to English version...    I have used, tr, TR, tur, TUR, turkish, tr-tr, TR-tr     None of them works.    Can you please help me?  "
"Hi,   Given a sentence s = [x_1, x_2, ...., x_N] is there a maximum length above which we truncate the sentence?   By that, I mean: let n be the maximum length. If N>n, do we map sentence s to map(s) = [x_1, x_2, ..., x_n] and hence remove all words at indices above n?     Regards, "
"After toying around with some of the data, I found out that quite some data comes from the wrong language. As an example, here are the top 100 lines from the German/English data (`de-en.tsv`).         It seems that there is quite some dialogue data, where people narrate things in other languages, leading to some English sentences being German and so on.    After some preliminary research using `langdetect` from Google, it appears that about 7% of the lines contain text coming from the wrong language."
"LASER embeddings are really good, but for the use case:  Suppose:  I have text in ENGLISH and I have to match that provided text in English in any german language document. So, the laser multilingual embeddings will help me find that similar sentence in the German document.  BUT, my question is how do I translate that german similar sentence into English sentence using LASER?"
Can anyone point me as to where can I find the list of supported languages and id (English -> en)?
"Hi,    I have a domain (health and/or finance) data set of 100k English sentences, I need synthetic parallel sentences for these in multiple languages (Spanish, Arabic, Indonesian etc.). I wondering how I can use LASER toolkit to generate this. I looked through the tasks folders but did not find an exact tutorial to do so.     Can I just download the multilingual encoder, embed my custom data set and start finding similar sentences ? Guessing I need embed target sentences as well. Not sure where to start.     New to this, any help is appreciated.     Cheers !"
"The WikiMatrix README says:    > One can specify the threshold on the margin score. The higher the value, the more likely the sentences are mutual translations, but the less data one will get. A value of 1.04 seems to be good choice for most language pairs. Please see the analysis in the paper for more information [1].    I assume CCMatrix will have similar when the extraction script is ready.    But what are the min and max?  "
"Hello,  When will the CCMatrix data or relevant extraction scripts be released?  Thanks."
"Is there a way I can check if a word (in any language) was considered when the training happened?  For example, a word like ""Vaibhav"", will not probably be embedded correctly and is best ignored while performing embedding and comparisons."
"Using the python integration of fastBPE could fasten up the code, because bootstraping in unix implies a fork everytime you are trying to use the cpp only implementation. This slows down the execution depending on the used ram.     "
"When I run `embed.sh` with any language code other than ""en"", the following warning comes up, I am able to get the embeddings but I doubt whether the correct tokenization happened? Is there any fix for this?   "
"hi,   we know that Laser has used different tokenizers for languages like English, Chinese, Japanese.  if one sentence has multiple languages  e.g.   a mix of English and Chinese in one sentence [""我喜欢Nike""]  or multilingual batch [""I like Nike"", ""我喜欢耐克""]    how to use a mix of tokenizer in these cases?"
"Hi and thank you for releasing the bitext mining code!    The approach looks very inspiring! However, it seems that the code you are providing is only applicable to the data from BUCC. Would it be possible for you to make it work for some general monolingual plain texts? That could be very helpful, thanks!    My research focuses on the parallel sentences from the revision of multi-lingual Wiki-texts. I know the Wikimatrix is out there, maybe it will also be very helpful if the code for mining Wiki-texts is available there!    Sorry, I'm just a beginner on digital humanity. I will be very grateful if you could help me out.  "
"I try to process zh-en data (  using bucc listed in tasks, but get this error ValueError: attempt to get argmax of an empty sequence, here is my trace back:  `Processing BUCC data in .   - Encoder: loading /home/asd/Project/LASER/models/bilstm.93langs.2018-12-26.pt   - Tokenizer:  in language zh    Building prefix dict from the default dictionary ...  Loading model from cache /tmp/jieba.cache  Loading model cost 0.701 seconds.  Prefix dict has been built succesfully.   - fast BPE: processing tok   - Encoder: bpe to bucc2018.zh-en.train.enc.zh   - Encoder: 0 sentences in 0s   - Encoder: loading /home/asd/Project/LASER/models/bilstm.93langs.2018-12-26.pt   - Tokenizer:  in language en     - fast BPE: processing tok   - Encoder: bpe to bucc2018.zh-en.train.enc.en   - Encoder: 0 sentences in 0s  Loading faiss with AVX2 support.  LASER: tool to search, score or mine bitexts   - knn will run on all available GPUs (recommended)   - loading texts ./embed/bucc2018.zh-en.train.txt.zh: 0 lines, 0 unique   - loading texts ./embed/bucc2018.zh-en.train.txt.en: 0 lines, 0 unique   - Embeddings: ./embed/bucc2018.zh-en.train.enc.zh, 0x1024   - unify embeddings: 0 -> 0   - Embeddings: ./embed/bucc2018.zh-en.train.enc.en, 0x1024   - unify embeddings: 0 -> 0   - perform 4-nn source against target  /home/asd/Project/LASER/source/mine_bitexts.py:228: RuntimeWarning: Mean of empty slice.    x2y_mean = x2y_sim.mean(axis=1)   - perform 4-nn target against source  /home/asd/Project/LASER/source/mine_bitexts.py:234: RuntimeWarning: Mean of empty slice.    y2x_mean = y2x_sim.mean(axis=1)   - mining for parallel data   - scoring 0 candidates   - scoring 0 candidates  Traceback (most recent call last):    File ""/home/asd/Project/LASER/source/mine_bitexts.py"", line 270, in        fwd_best = x2y_ind[np.arange(x.shape[0]), fwd_scores.argmax(axis=1)]  ValueError: attempt to get argmax of an empty sequence   - Encoder: loading /home/asd/Project/LASER/models/bilstm.93langs.2018-12-26.pt   - Tokenizer:  in language zh    Building prefix dict from the default dictionary ...  Loading model from cache /tmp/jieba.cache  Loading model cost 0.666 seconds.  Prefix dict has been built succesfully.   - fast BPE: processing tok   - Encoder: bpe to bucc2018.zh-en.test.enc.zh   - Encoder: 0 sentences in 0s   - Encoder: loading /home/asd/Project/LASER/models/bilstm.93langs.2018-12-26.pt   - Tokenizer:  in language en     - fast BPE: processing tok   - Encoder: bpe to bucc2018.zh-en.test.enc.en   - Encoder: 0 sentences in 0s  Loading faiss with AVX2 support.  LASER: tool to search, score or mine bitexts   - knn will run on all available GPUs (recommended)   - loading texts ./embed/bucc2018.zh-en.test.txt.zh: 0 lines, 0 unique   - loading texts ./embed/bucc2018.zh-en.test.txt.en: 0 lines, 0 unique   - Embeddings: ./embed/bucc2018.zh-en.test.enc.zh, 0x1024   - unify embeddings: 0 -> 0   - Embeddings: ./embed/bucc2018.zh-en.test.enc.en, 0x1024   - unify embeddings: 0 -> 0   - perform 4-nn source against target  /home/asd/Project/LASER/source/mine_bitexts.py:228: RuntimeWarning: Mean of empty slice.    x2y_mean = x2y_sim.mean(axis=1)   - perform 4-nn target against source  /home/asd/Project/LASER/source/mine_bitexts.py:234: RuntimeWarning: Mean of empty slice.    y2x_mean = y2x_sim.mean(axis=1)   - mining for parallel data   - scoring 0 candidates   - scoring 0 candidates  Traceback (most recent call last):    File ""/home/asd/Project/LASER/source/mine_bitexts.py"", line 270, in        fwd_best = x2y_ind[np.arange(x.shape[0]), fwd_scores.argmax(axis=1)]  ValueError: attempt to get argmax of an empty sequence  usage: bucc.py [-h] [--encoding ENCODING] --src-lang SRC_LANG --trg-lang                 TRG_LANG --bucc-texts BUCC_TEXTS --bucc-ids BUCC_IDS                 --candidates CANDIDATES [--gold GOLD] [--threshold THRESHOLD]                 [--output OUTPUT] [--verbose]  bucc.py: error: argument --threshold: expected one argument`    I just started to use this repo, here is what I did:  step1: installing the dependencies:    step2: export LASER=""${HOME}/projects/laser""  step3:bash ./install_models.sh  step4: bash ./install_external_tools.sh (I didn't install mecab)  step5: download the data    step6: run bash bucc.sh    Then I got the above error. Thanks  "
tools-external/moses-tokenizer/tokenizer/remove-non-printing-char.perl: not found    When running embed.sh
Can you provide a model file bilstm.93langs.2018-12-26.pt?  Thank you very much!
"How can we find the index of sentence that has the same index in the file or for the sentence where it doesn't match ?    Can we get a output file like binary variable, match/mis-mismatch ?"
"I want to tokenize and apply ape for a single line so i used  Tokenline for tokenizing a line and BPEfastApplyLine for applying bpe to a tokenized line  the code is   ``sen = ""He is inclined to be lazy.""   token =  TokenLine(sen ,  lang = 'en' ,lower_case= True , romanize= False)          token=BPEfastApplyLine(token, args.bpe_codes)``    and the value of bpe_codes is ""/home/LASER//models/93langs.fcodes""    But i get the following error  ``Traceback (most recent call last):    File ""/home/LASER//source/embed.py"", line 402, in        token=BPEfastApplyLine(token, args.bpe_codes)    File ""/home/LASER//source/lib/text_processing.py"", line 123, in BPEfastApplyLine      return bpe.apply([line])[0]  AttributeError: 'str' object has no attribute 'apply'  ``   can anyone help me   thanks in advance!!!!!!!!!!!!  "
"Hi, I am trying to get sentence embeddings on Japanese via the docker. However, the output is **empty** since Mecab is not installed.     **Output in python**:  {'content': JAPANESE_SENTENCE, 'embedding': []}    **Output in shell:**        Any idea on how to deal with that? :) "
"Perhaps related to #62, the `install_external_tools.sh` script fails with the current master:         The file is indeed missing:     "
LASER does not consider language while giving sentence representation.But what does it consider to give embeddings for the input sentence such that the similar sentences in different language end up together in same neighbourhood.  Kindly explain.
How to use this laser tool kit to produce multilingual embeddings.Please help
Hi    Is there any way to recover articles boundaries from your corpus ?    Thanks
"when I install all of these tools offline, FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmp498vksrc/bpe' appears........"
   Thank you very much!
"Are there informations available from which article springs the sentence pair? It would be very helpful for multi-domain translations.  Nevertheless, thanks for sharing the results!"
"While trying to download the corpora, we are receiving 429 errors:     "
"Hi, I have been trying to use LASER for product category prediction based on product reviews. The initial dataset is in English and I was able to train it to get an accuracy of 88% using a Siamese Network and 5 Nearest Neighbour. The F1 score for all the categories were reasonable as well.    However, when I used the German dataset for  similar Product Category prediction, the accuracy values are really bad, 8% when there are 22 classes and one can expect ~4% random accuracy. Now, at this point to be honest, I am running out of ideas.    The training is completely in English and the prediction is in German that too Zero Shot. So, I depend completely on the Sentence Embedding. Can anyone give me tips for improving the model since frankly, I am completely confused"
"Hi,  I am trying to reproduce the experiment of text classification by executing the script ./mldoc.sh. However, when training starts I got this kind of error:  Training MLDoc classifier (log files in embed)   - train on en, dev on en  Traceback (most recent call last):    File ""LASER/source/sent_classif.py"", line 188, in        dim=args.dim, bsize=args.bsize, shuffle=True)    File ""LASER/source/sent_classif.py"", line 34, in LoadData      lbl = np.loadtxt(bdir + lfn, dtype=np.int32)    File ""/lib/python3.6/site-packages/numpy/lib/npyio.py"", line 1141, in loadtxt      for x in read_data(_loadtxt_chunksize):    File ""/lib/python3.6/site-packages/numpy/lib/npyio.py"", line 1068, in read_data      items = [conv(val) for (conv, val) in zip(converters, vals)]    File ""/lib/python3.6/site-packages/numpy/lib/npyio.py"", line 1068, in        items = [conv(val) for (conv, val) in zip(converters, vals)]    File ""/lib/python3.6/site-packages/numpy/lib/npyio.py"", line 785, in        return lambda x: int(float(x))  ValueError: could not convert string to float: '0CAT'    "
"I have a use case where I am training LASER over 10 million sentences to predict 23 categories. Currently, I run it over 230k samples (ie 230k sentences) and a single iteration of batch size 1000 is taking almost 20-25 minutes which I think is kind of large considering I am using GPU support (Google Colab). My only query is, apart from using `Sentence Embedding` instance with CPU as false, do I need to change some other values. I think the result given by `Sentence Embedding` is a numpy array which indicates that the value is moved away from GPU and sent onto CPU. A single epoch on the dataset is taking 3 hours. I think it's quite a high number    Or is this amount of time expected? I am actually kind of stuck in this regard. Any help shall be highly appreciated "
"Hi, I am bit of a noivce in the field so many of the things might seem trivial but I would appreciate some help in here. I am  trying to perform a product category identification on a multilingual dataset (there are statements about a product and the network needs to guess its category). For this :    1. Use LASER to get sentence embeddings  2. Train a FC Network on the 1024 dimensional output to scale it a bit  3. Train a softmax at the end  However, I was thinking if it would make sense to start using convolutions on the 1024 dimensional output vector from LASER? I was thinking on using things like a RESNET block with 1-D convolutions but I am not sure what is the implication of that.   Frankly, we would be using such convolution mostly in language modeling. Is it even correct to try and use anything other than a FC layer?    I would really appreciate a response since I am basically stuck"
"In order to use embed.sh, a 2 character language code is needed. Is there a list of these language codes that are allowed? is a general guideline followed or something custom? For example, to get embeddings in Spanish, should I use ""es"" or ""sp""?"
Currently Laser has dependency to FastBPE which has to be compiled from C to be usable. It makes it hard to use it in serverless manner. Do you know maybe some way to overcome this? I would like to use it in AWS environment.
"Hi,  as in previous issue I explained, I am using LASER embedding to compare semantic cross-lingual similarities between sentences.  It works well for the language we are facing.  I have a particular case that I cannot understand why its result is that.  This is a normal case in my pipeline                 And here it's all ok: so the model is able to handle the punctuation.   I have a case when some strange things happen.                     And the most strange thing is that if I remove commas both from source and target:       I could remove all the punctuation from both but I don't know if it can be a good idea, maybe it can break other cases that worked before.              "
"I get one vector after sentence embedding for japanese, for other languages everything works well.    **input:**  echo jap_text.txt | python laser/source/embed.py --encoder laser/models/bilstm.93langs.2018-12-26.pt --token-lang jpn --bpe-codes laser/models/93langs.fcodes --output jptest.vec --verbose    **output:**   - Encoder: loading laser/models/bilstm.93langs.2018-12-26.pt   - Tokenizer:  in language ja    WARNING: No known abbreviations for language 'ja', attempting fall-back to English version...  ja   - fast BPE: processing tok   - Encoder: bpe to jptest.vec   - Encoder: 1 sentences in 0s    but the actual number of lines (sentences) in jap_text.txt is 32     after Token() (there is no problem with MeCab) and BPEfastApply() number of lines doesn't change, so I think there is a problem in SentenceEncoder()"
What is the best way to get the word embeddings out of LASER instead of Sentence embeddings?   -  We may use the last BiLSTM Layer's output (just before max pooling) as word-level representations. My understanding is Encode.forward() at   ;  can be modified to provide such a utility. 
"Hi,  we are trying to build a cross-lingual sentiment classifier starting from LASER embedding.   The first step we did was to build a simple classifier that has as input the embedding with size 1024 of a sentence and ,adding some hidden layers, makes a prediction between 5 classes.  We take as example the MLDOC task you provide and we saw good results in that, but also something to improve.    Some example of errors:         I think that, especially for the sentiment, it's important to maintain a context that is updated everytime that a word is analyzed inside the sentence, because potentially every word could overturn the sentiment of the entire sentence.     So I think we have to move to RNNs. In this sense, I would like to create an embedding LASER for every word and use an RNN to create an hidden state that can change based on words the network sees.   Do you think this approach could work with LASER embedding at word level?   Thanks  "
I tried to use the script of 'embed.sh' to get the sentence embedding in Persian and Arabic. And is the sentence embedding in Persian and Arabic the same as the sentence embedding in english？if not，are there abbreviations for Persian and Arabic?    
"Hi @hoschwenk ,    We are experimenting with using LASER for translation of low-resource languages. In order to get reliable results, we have to be sure if the training data didn't contain the subsets from our test data. We are currently concerned with English-Hindi language pair. Our test data comes from WMT NewsWire 2014 which is a part of Opus collection that you have used. Is it possible for you to share the training data used for LASER for en-hi language pair?     I have gone through the paper where its mentioned that around 288k sentences have been used for hindi. In Appendix A, there's no mention of WMT NewsWire 2014, so is it true that you have not used any data from WMT NewsWire for hindi?   Anyways it would be great if we can have a look at the training data (for hindi) that you used so that we can create a new test set by excluding all of these sentences?     Thanks,  Ashim"
In the shell script intsall_external_tools.sh under install fastBPE section the command   `g++ -std=c++11 -pthread -O3 fast.cc -o fast`  has fast.cc in it. but there is no file named fast.cc in repository of fastBPE. it contains `main.cc`.    it is showing error while trying to execute `bash ./install_external_tools.sh`   
"I use LASER to achieve this with the combination of cosine similarity and it works fine for near or exact translations, but I get medium percentages (40-50%) between irrelevant translations.   I use SentenceEncoder, TokenLine to tokenize each sentence, BPEfastApplyLine, I am calculating embeddings and then cosine similarity.  "
Hi and thank you for releasing the LASER code.     It seems that the `mine_bitexts.py` throws index errors like the following when either or both of the documents to be examined have fewer unique sentences than the `args.neighborhood` argument passed to the `knn` function.           A workaround would perhaps be to call the `knn` function like       
"In the `Token` I see that the `transliteration` package is used          and piped to the output like:         While for `zh` and `ja` it is optional. Could it be improved checking the input language to be romanized here as well? Also the `transliteration` handles not only Greek but Russian/Armenian and other languages, so it will be applied in this case as well."
This problems happens when running multiple concurrent calls to the process of calculating embeddings in `TokenLine`         Here is the detailed strack trace:     
"Hello,    **Task**: Given a document (each document has ~ 8-30 sentences), find k most similar documents from the dataset.      **Language**: English      **Approach**: Each document is converted to a embedding vector using LASER. Given N query vectors, I am using FAISS library for finding k nearest neighbours. Then the experiment is repeated by using TFIDF (nltk TfidfVectorizer) for embedding documents instead of LASER.      **Observation**: TFIDF approach perform better than the LASER. I am using recall@k where k = 1..5 as evaluation metric. However, I was expecting LASER to perform better.      Am I missing something? Can someone share some insight on how this might be the case?  Any revenant reference is also welcome.      Thanks."
"For this sentence `Que le dé, que le dé, que le dé` (that is in `ES`, Spanish) and a closer translation to `EN` English that sounds like `To give it to her, to give it to her, to give it to her`, the dot product of the embedding (see   gives out a score of `0.44`. Typically for similar sentences I get a score `>0.5` (likely similar), `>0.7` (very likely to be similar), while everything `<0.5` seems to be unlikely similar.    Now in this specific case, it seems that at first glance `Que le dé, que le dé, que le dé` is considered like `FR`, French from most of the langId neural network I run (FastText, Google Cloud, etc.). If so, this means that in the vector space the `EN` translation would be not so closer as it would be the `ES` one.    The second consideration is that, another possibile translation in Spanish is `""to be given,to be given,to be given""`, and in this case we get a `0.79` score, that means that source and targer are actually similar.    Last, if I add to the sentence more `ES` context like `Que le dé, que le dé, que le dé, come estas`, now the similarity gets closer as well to `0.68`.    So, which would be a reasonable approach to disambiguates sentences like this, that could have very close vectors in different languages?  "
"We are using LASER line by line so using          where the new `EncodeLine` looks line         Now everything works ok: `TokenLine`, `BPEfastApplyLine`, but we are not sure about the `EncodeLine` nd.array shape:    For a given sentence like  like `donde podremos estar solos, solos, solos` we get as BPE `donde pod@@ remos estar sol@@ os , sol@@ os , sol@@ os` (size:55), then the encoded shape was `(55, 1024)` after `EncodeLine`. Is that correct?        Full logging on two sentences:         Second question, we still need to call `IndexCreate` before calculating the similarity between embeddings like:         Is this step actually needed? Assumed we have the embeddings from the `EncodeLine` we have to actually run `IndexCreate`?"
None
"Does embed.py support multi-gpu? I need to get the sentence embeddings of a rather large corpus (+40M sentences) and was looking into embed.py to see if it supports multi-gpu to speedup the process. But, it seems that currently it can run only on a single gpu. right? May I ask you if there is any script for this purpose?    Thank you,  Amin"
"I would like to test LASER with a set of very low resource languages: Sanskrit, Classical Tibetan and Classical Chinese. Is it possible to train it with my own data? It is already tokenized and I was able to calculate fasttext embedding for it.   With best wishes,     Sebastian "
"In order to support interactive mode and/or run-time querying from other languages, it would be ideal if the code under `if __name__ == '__main__':` in a task like embed had an initial load and then processed each line from stdin as soon as it arrived.    "
"During installing the external library (bash install_external_tools.sh) I found that wget somehow the gave me 302 responses and redirected me to   so I ended up downloaded ""master"" instead of ""master.zip"". Therefore, the installation failed because the script was trying to find a file named ""master.zip"" which doesn't exist. So it's better to add an -O option for wget command I think.   "
"Hi,   I am trying to use LASER embedding in order to calculate similarity between sentences. I tried with inner product and results are not bad, but maybe others are better.   What similarity would you recommend?   Thanks"
"Hi,    Thank you for amazing work!    We have a cross lingual document classification (Binary) use-case where each documents is more than 500 words and we have labelled data for 2 foreign languages along with English. Few Questions:    1.  How is document embeddings being computed currently? Does entire sequence get passed through biLSTM in one go? If that is true, can we compute it in a better way specially for documents since semantic representation of document might not be good enough while passing entire sequence?  2. Will training on combined multilingual data is any better than training on Only English data and then doing inference over other languages?  3. Do we have to pass any explicit encoding while calling `embed.sh` in case of languages like French and Spanish? I see default encoding is `utf-8`, Is there something I should be worried about as pointed out in      TIA! :)    Regards,  Ashvini"
"Hi,  I can't get bpe codes in Chinese:    ./fast applybpe  data/embd/lcqmc.dev.prem.bpe.zh  data/embd/lcqmc.dev.prem.tok.zh LaserModel/models/93langs.fcodes  LaserModel/models/93langs.fvocab    will result error:   Read 2369862597 words (73636 unique) from vocabulary file.  Loading codes from LaserModel/models/93langs.fcodes ...  Read 50000 codes from the codes file.  Loading vocabulary from data/embd/lcqmc.dev.prem.tok.zh ...  Read 328980 words (235 unique) from text file.  fast: fast.cc:486: void decompose(std::__cxx11::string, std::vector  >&, const std::unordered_map , std::pair , std::__cxx11::basic_string  > >&, const std::unordered_map , unsigned int>&, bool): Assertion `count == 1' failed.  fast: fast.cc:486: void decompose(std::__cxx11::string, std::vector  >&, const std::unordered_map , std::pair , std::__cxx11::basic_string  > >&, const std::unordered_map , unsigned int>&, bool): Assertion `count == 1' failed.  Aborted (core dumped)    But in english, it work well:  ./fast applybpe  data/embd/mrpc.dev.prem.bpe.en data/embd/mrpc.dev.prem.tok.en LaserModel/models/93langs.fcodes LaserModel/models/93langs.fvocab   Read 2369862597 words (73636 unique) from vocabulary file.  Loading codes from LaserModel/models/93langs.fcodes ...  Read 50000 codes from the codes file.  Loading vocabulary from /data/embd/mrpc.dev.prem.tok.en ...  Read 38776 words (7710 unique) from text file.  Applying BPE to data/embd/mrpc.dev.prem.tok.en ...  Modified 38776 words from text file.    Thanks.  "
Hi.    I found to refer wrong path in LASER/tasks/xnli/xnli.py line 29.    now       maybe better   
Thank you for open sourcing your work. We are trying to test MLDoc zeroshort performance and simply running mldoc.sh does not reproduce the results. I've run them twice and got the same results that are far from expected:    | Train |   en  |   de  |   es  |   fr  |   it  |   ru  |   zh  |  |-------|-------|-------|-------|-------|-------|-------|-------|  |  en:  | 90.88 | 86.48 | 67.62 | 61.98 | 69.95 | 22.95 | 11.65 |  |  de:  | 73.23 | 92.90 | 77.23 | 74.05 | 72.30 | 24.80 |  9.93 |  |  es:  | 65.62 | 80.58 | 92.03 | 73.28 | 69.03 | 34.10 | 12.58 |  |  fr:  | 78.35 | 85.45 | 78.20 | 89.68 | 69.85 | 33.88 |  9.68 |  |  it:  | 73.93 | 84.58 | 79.23 | 76.73 | 84.03 | 34.48 | 11.83 |  |  ru:  | 57.33 | 63.78 | 45.80 | 52.78 | 51.15 | 66.08 | 36.28 |  |  zh:  | 26.15 | 28.13 | 21.88 | 29.33 | 30.58 | 34.38 | 75.62 |    Do you happen to have the original mldoc models stored somewhere? If so could you share them It would help us to debug the issue.
"First of all thanks for providing LASER.  I am trying to get the embedding of some sentences in hindi.  The question is that - how the trained biLSTM model is handling an unseen word ?  if all the words in the sentence is new how the sentence embedding is calculated?    I understood following:-  BPE - encoder takes care of unknown words. for every word it tries to break the word into subwords. e.g. ""loved"" will be broken into ""lov"" and ""ed"" and ""loving"" will be broken into ""lov"" and ""ing"". So if a new word comes it breaks it into known sub words and gives the output regarding those sub-words    <img width=""670"" alt=""bpe_example"" src=""     correct me if i am wrong"
"I used ` ./embed.sh ./es_tass1.txt es es_tass1.raw` to run the embedding example.  It executed successfully with the following info:     The encoder encoded the given 7217 sentences .    But while reading the raw embedding as per the script give  , I'm getting `7219` embeddings.     What could be the possible reason for getting +2 number of embeddings? "
I think this should use some sort of env variable or something else.    
how can i use LASER to translate english into hindi or vice versa ?
"I tried sentence embedding example, and I observed that it doesn't understand sentences with opposite meaning.     Is there any way to tune it or use different models?"
"while running bash ./xnli.sh i am getting this error  Traceback (most recent call last):    File ""/home/appuser/sheshank.k/projects/laser/source/nli.py"", line 228, in        dim=args.dim, bsize=args.bsize, shuffle=True)    File ""/home/appuser/sheshank.k/projects/laser/source/nli.py"", line 57, in LoadDataNLI      D = data_utils.TensorDataset(torch.from_numpy(nli), torch.from_numpy(lbl))    File ""/home/appuser/.local/lib/python3.6/site-packages/torch/utils/data/dataset.py"", line 36, in __init__      assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)  AssertionError"
"First of all, definitely a great framework, I would like to congratulate you and thank you also for sharing it, the idea of multilingual embeddings really is great.     My doubt is, I was able to run the sample to obtain the embeddings for sentences in CLI, is there a way to get these embeddings in Python without the need to enter the sentences from one file and read the result as a numpy array, from another.     That is to call some function like:     encoder.embed (""sentence"") or something similar.    Thank you, kind regards  Alejandro."
Hi! Could you please open source the code that was used to train full encoder-decoder system?     It would be extremely helpful for those who work on improving embeddings itself and want to use your work as a starting point.    Thank you
"Hi,  I am trying to understand how to organize my dataset for training using the `sent_classif.py` script.  I have 2 classes (spam/not-spam), and I have created the following files:         The `XX_corpus.txt` files contain only the text, while the `XX_labels.txt` are the corresponding labels. First question: Is this correct? Should the files be splitted like that?    I have also encoded my labels as numerical since I encountered the error `ValueError: could not convert string to float: 'SPAM'`. So now I have binary labels.    I am launching the script as follows:         But I have this error:     "
Thank you for releasing the code.     Any idea when the code for bitext mining is getting released?
"Hi, I am working on a cross-lingual spam detection task. I have a dataset containing spam/non-spam sentences in different languages. My goal is to detect the spam no matter in which language it is.   I was wondering how to approach this problem using LASER. I saw a number of examples but each of them require the language for training/testing the model. Is there a way I can generalize the usage of LASER for my purpose?"
"I tried to get the embeddings for hindi language using `./embed.sh ${LASER}/data/tatoeba/v1/tatoeba.hin-eng.hin hi check.raw`, but I got the following output on my console:       I also tried different abbreviations like hn, hin, hindi etc but none of them worked.    Also, if can you provide the mappings for all the languages to their respective abbreviations, that would be helpful."
While trying to execute the example for sentence embeddings I get this console log:        The resulting raw file is empty (0 Bytes).    I also tried to invoke the python script manually to no avail. Unfortunately there seems no further explanation being logged.     How can I fix this?    Btw: Many thanks for sharing your work and code!
In the README.md **«LASER: calculation of sentence embeddings»** seems to be a small code typo:    Shouldn't this:     `bash ./embed.sh INPUT-FILE LANGUGAE OUTPUT-FILE`    be this?     `bash ./embed.sh INPUT-FILE LANGUAGE OUTPUT-FILE`    By the way: Thanks very much for sharing LASER! Much appreciated.
"Hello  Looks like today pretrained models are in pythorch pt formats :   ""bilstm.eparl21.2018-11-19.pt""    ""bilstm.93langs.2018-12-26.pt""   Would it be possible to also offer it in onnx format :      ?   Kind"
I'm trying to install and run the similarity task but I got this error:    `bash ./wmt.sh  `  `python3: can't open file '/LASER//source/similarity_search.py': [Errno 2] No such file or directory`
Hi!  I've tried to calculate sentence embeddings for a file in Russian language and failed with this error: `'ascii' codec can't decode byte 0xd0 in position 0: ordinal not in range(128)`    The problem is that the data is UTF-8 encoded and default python `open` function doesn't support non-ASCII characters.    
"Hello LASER Team!  I try to run the ""Sentence Embedding for text files"" for fra language but it gives me the following error. may you please guide me....    command :::::>        ./embed.sh ${LASER}/data/tatoeba/v1/tatoeba.fra-eng.fra fr my_embeddings.raw       - Encoder: loading /home/farhat/LASER/models/bilstm.93langs.2018-12-26.pt   - Tokenizer:  in language fr     - fast BPE: processing tok   - Encoder: bpe to my_embeddings.raw  Traceback (most recent call last):    File ""/home/farhat/LASER/source/embed.py"", line 364, in        buffer_size=args.buffer_size)     File ""/home/farhat/LASER/source/embed.py"", line 295, in EncodeFile      fin = open(inp_fname, 'r') if len(inp_fname) > 0 else sys.stdin  FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmp5mjbrkfz/bpe'"
"I tried running xnli.sh to replicate the XNLI results and got the following errors:     LASER: training and evaluation for XNLI   - loading encoder /group/project/s1782911/LASER/models/bilstm.93langs.2018-12-26.pt    Processing train:   - Tokenizer: xnli.train.prem.tok.en exists already   - fast BPE: processing xnli.train.prem.tok.en   - Encoder: xnli.train.prem.bpe.en to xnli.train.prem.enc.en  Traceback (most recent call last):    File ""xnli.py"", line 91, in        buffer_size=args.buffer_size)     File ""/group/project/s1782911/LASER/source/embed.py"", line 295, in EncodeFile      fin = open(inp_fname, 'r') if len(inp_fname) > 0 else sys.stdin  FileNotFoundError: [Errno 2] No such file or directory: 'embed/xnli.train.prem.bpe.en'    Training the classifier (see embed/xnli.log)  Traceback (most recent call last):    File ""/group/project/s1782911/LASER/source/nli.py"", line 228, in        dim=args.dim, bsize=args.bsize, shuffle=True)    File ""/group/project/s1782911/LASER/source/nli.py"", line 36, in LoadDataNLI      x = np.fromfile(fn1, dtype=np.float32, count=-1)  FileNotFoundError: [Errno 2] No such file or directory: 'embed/xnli.train.prem.enc.en'    Do you have an idea what might cause these errors and what I can do about them?  Thanks in advance!"
"I follow the instuction README, but still can't get it running.          Thanks for such a great work, There are very little resource in cantonese (yue chinese). This model should be a great help."
"The ""pip install"" failed but the external tools installer said it was better to build from source.  Unfortunately, I haven't been able to find clear documentation for how to do this.    Any help would be appreciated."
"If you plan on supporting the Tigrinya language (similar to Amharic) in the future, I would be happy to help you find sources of training data. Thanks!          "
"I'm using fastText for both unsupervised language model and as supervised text classifier. Currently for cross-language models fastText has several limitations, due to the approach in sub-words creations, that is limited in terms of text tokenization and it does not handle properly languages without word boundary etc. I was wondering how to combine LASER multilingual text embedding with a fastText model: fastText supports pre-trained vectors, so would it be possible to load the LASER bi-LSTM embeddings for training? Or maybe a different approach could be followed.  _Cross-side note._ In   recent paper we have exploited a fastText+biLSTM+Attention model and we have found pretty interesting results (mono-lingual), and I would like to extend these results including LASER at some point!"
It seems that the multilingual decoder mentioned in the   is not currently in this repository. Do you have plans to open source it?
"TREC website has been down for several days. So we cannot access the MLdoc dataset, which was used in the paper for zero shot text classification. Any other way to download the data?"
"Hi,    The title of   suggests that LASER is an open-source library, but the license file doesn't say so. It is not   and even does not permit commercial use.    Is there any plan to change the license to an OSI-approved license (MIT, BSD, Apache)?"
"README.md Has Latavian as supported language, but it should say Latvian. Your blogpost   also has an animation with the same typo."
"This is such a cool project, thanks for putting it out! I was wondering what the process is to compute our own embeddings on text. For example, if I have a list of sentences in Portuguese, what format do they need to be in and what do I need to do to compute their embeddings?"
"Hello,  Thanks for open-sourcing & maintaining this useful package. I've been using it well for gathering parallel sentences from Wikipedia.    I was wondering, would it be possible to make the pre-trained multilingual encoder model for Chinese and English (as used in the paper, ""_Learning joint multilingual sentence representations with neural machine translation_"") publicly available?    "
None
Hi Holger!      links to   which is a 404.      Bet you weren't expecting that as the first issue...    Kenneth
     This line should be:  `       data['cpos'] = cand_tokens['pos']  `  
"Hi  I cannot find the wikipedia dump I should use, and cannot recognize which path is correct, I really appreciate sharing me the path to the data. thanks "
"What is your experimental machine ? The number of GPUs, memory size of GPUs, CPU and its memory size. Hoping for solution~"
"你好，我是一名计算机研究生，阅读了Open Vocabulary Learning for Neural Chinese Pinyin IME论文之后，其中更新vocabulary的算法在手动模拟之后仍感觉没有理解到位。在 www.runoob.com 查看简单lua语法之后，我使用""vocabulary""/""ngram""/""break""/""6""分别进行了搜索并查看了大部分搜索位置，但是没有定位到源代码。于是有三个问题想要请教一下：    1. 我按照文章中的伪代码的理解，举了两个例子，我理解对了吗？  2. 文中Online Vocabulary Updating Algorithm算法在代码的什么位置？  3. 我尝试理解算法的过程有哪里是亟待提高的？    例1：    |      | 模型预测   | 用户选择   |  | ---- | ---------- | ---------- |  | 句子 | 我是中国人 | 我是种果人 |  | n=5  | 我是种果人 | 我是种果人 |    比较结果不符合注释`// both the first and last characters are different at least`所以执行第10行的break跳出循环，没有新词增加    例2：    |      | 模型预测                       | 用户选择                         |  | ---- | ------------------------------ | -------------------------------- |  | 句子 | 湖州看见美丽                   | 胡诌看见魅力                     |  | n=6  | 湖州看见美丽                   | **胡诌看见魅力**                 |  | n=5  | 湖州看见美、州看见美丽         | **胡诌看见魅**、**诌看见魅力**   |  | n=4  | 湖州看见、州看见美、看见美丽   | 胡诌看见、**诌看见魅**、看见魅力 |  | n=3  | 湖州看、州看见、看见美、见美丽 | 胡诌看、诌看见、看见魅、见魅力   |    表中粗体字被考虑加入vocabulary；n=3后由于没有mismatch而break出循环        "
"Hello, I found a performance issue in the definition of `masked_routing_iter`, capsule_masked.py,  two `tf.matmul` and three `tf.shape` in   will be created repeatedly during program execution, resulting in reduced efficiency. I think they should be created before the loop.    Looking forward to your reply. Btw, I am very glad to create a PR to fix it if you are too busy."
对比实验中有without intent predicting，请问with intent predicting且without re-routing的效果怎么样呢？
None
 我这边论文复现，snips数据集按照论文的参数设置，效果能达到预期效果，但是atis数据集的Overall Acc只能达到0.704，与之对应的Intent Acc只有0.776，并且训练过程intent loss 一直震荡不收敛，已经尝试了不同的学习率和优化器，效果均达不到论文水平，请问一下作者论文关于atis数据集的超参是怎么设置的，在此十分感谢！！
Hi!How do I understand the parsing logic for sentences with a semicolon?For example  `Earnings The coming week will be busy as a total of 129 companies will release their March quarter earnings including key ones;Markets show a repeated downward slope`  These sentences are not split into two clauses
"Hi there, thanks for putting together this awesome repo!    I'm wondering if it's possible to save the output somewhere on the disk (e.g., with `pickle` or `spacy` serialization methods).    For example:         would yield the error:       Is there any workaround? This would be really useful when dealing with large datasets. Thanks for any guidance!  "
"For some reason, when the span has a length of one,   returns an empty tuple. I would expect it to return the part of speech of the individual word (which can be done by taking the token of the word in the span and then taking  .    Reproduction:       From this code pos will be an empty tuple, but I would expect it to be equal to   which is ""NNP"""
"Is there a sentence length threshold beyond which parse quality is known to deteriorate significantly? I am asking before initiating my own tests, in case anyone did some work on the topic. In the same spirit, does parse time grow linearly with sentence length? "
"Hi.  Is there any chance you can make benepar_en2 available also for version 0.2?  I am asking since on the types of inputs I am working on, which are more ""noisy"" in the sense they often contain errors, colloquial language and such the benepar_en2 does consistently better than benepar_en3 and benepar_en3_wsj. There is real value there for this type of input.     Thank you,  Alex."
None
"I'm trying to make the multi-processing spacy pipeline works with the berkeley parser as I assume it will boost the performance. How can I get it to work? I tried the suggestion from  , but it didn't work for me.    !          Error message     "
"Hi,  is it possible to write the parse string spacy into the spacy ext.  I want to compute the tree height of the parsed const. string and I want to use NLTK embedded commands like `tree.height` of the structure of Tree or ParentedTree.    I've solved it with the following code snipped    parse_tree = ParentedTree.fromstring(sent._.parse_string)  parse_tree.height()    Is it possible to load the tree from the benepar spacy interface in a direct way, e.g. in the NLTK commands `tree = parser.parse(input_sentence)`?    Best Regards, Christina Lohr"
"I’d like to visualize the parse tree from Berkeley Neural Parser. But I couldn’t find the function in the package. Could you give me the hint how to generate the tree graph as in the online demo site:       if this function is not part of the package, please consider adding it. thanks a lot!  "
"There is currently one bug when using fast tokenizer.   If I run it to multi-thread, a bug will occur, so could you add the option use_fast = False that doesn't use fast tokenizer?     "
"Hi, first thank you for this plugin :D     I am unable to serialize a spacy doc with this benepar in the pipeline :  Will this feature be available ?    Thank you         !   "
I get this warning each time when I run parsing:         It seems like the parser is working fine still.  Is that something I can ignore?
"Hello, thank you for making this amazing tool opensource. I keep on receiving the following error with your latest version on benepar_en3 and spacy 3.0.    The same code that I'm using works with a shorter length of text. Thus, it certainly seems that the issue is coming from max-token(or length) allowed from the pretrained model.     The weird thing is that the passage (also provided below) seems to run under certain code implementations. I ran the same corpus several times just a few days ago and constituency parsing worked just fine. The issue arose when I removed virtualenv and re-installed everything for migration.    Is there a given max-token threshold for benepar_en3? Assuming that it is based on T5, there shouldn't be a maximum input sequence like Bert does...    error:       failing passage:   "
"Hi. Thanks for writing benepar. The web rendering demo is super useful for visualizing sentence structure. It'd be lovely if it was made available as something like `benepar.render()` (inspired from spaCy) for use within Jupyter notebooks, etc. I expect most of the code on the website that does this rendering could be reused as is, too."
"Hey,    First - thanks for this awesome package and the work which was needed to build it.  However currently I am make a small annotation project, which uses spaCy (3.0.3) with this repo (0.2.0) and I am experience segfaults. torch is up to date (1.7.1) and I am running on python 3.9.2.  Also I did not experience this behavior on spaCy 2.x.  This also effects the nltk variant."
"Dear authors,    I currently want to use   as a BERT pre-trained language model for this repo. PhoBERT was built and deploy in huggingface/transformers library  .  As I know, self-attentive-parser is using `pytorch_pretrained_bert` for getting BERT Model. I have tried to change the code of function `get_bert` in `parse_nk.py` to use PhoBERT:     But get this error:     I read a   using PhoBert for training in this repo so I am pretty sure this can be done, but do not know how to do it.  Any solution? thanks!"
"for example, extract ""house"" from the span (NN house)  Is it possible? "
@nikitakit   How do I deploy the trained parser model `benepar_en2` on server?
The issue as mentioned in the title. I am wondering if this is normal since English Constituency Parsing requires way fewer GPU resources (can be done with less than 12 GPU RAM).    My code (following     
"In some cases, the parser recognizes the token 'left' as VBD by mistake, which is most likely ALWAYS ADJ in my corpus. How can I force 'left' to be tagged as a ADJ in or out spaCy?  "
The syntax has changed in V2 of tf.   Here's the correct piece:    tf.compat.v1.GraphDef()   # -> instead of tf.GraphDef()  tf.compat.v2.io.gfile.GFile()   # -> instead of tf.gfile.GFile()
"Hello,  I have tensor flow 1.14 and I get the following error on this command:nlp.add_pipe(BeneparComponent('benepar_en')). Please let us know how to resolve this?      197                 graph_def = tf.GraphDef.FromString(model['model'])      198             else:  --> 199                 graph_def = tf.GraphDef.FromString(model)      200             tf.import_graph_def(graph_def, name='')      201     AttributeError: module 'tensorflow' has no attribute 'GraphDef'  "
"Hi, this is Brian.  After reading your paper (""Multilingual Constituency Parsing with Self-Attention and Pre-Training), I've been searching for the code used in training the model.  However, I couldn't find the code for the multilingual model in this repository.  Is it available to see the code for training the multilingual model?"
Shouldn't the parameter `preserve_line=True` being added to the call of `nltk.word_tokenize` since there should be only one sentence everytime?     
"Hi @nikitakit ,    I was wondering - what is the result of the parsing when the model is given an invalid sentence according to the English grammar?    Thank you very much in advance."
None
"When I use ""run_parse"" routine of the script ""src/main.py"" and use the self-trained model, it outputs parse trees with many ""UNK""s. like that:  > (S (INTJ (UNK No)) (UNK ,) (NP (UNK it)) (VP (UNK was) (UNK n't) (NP (UNK Black) (UNK Monday))) (UNK .))  (S (UNK But) (SBAR (UNK while) (S (NP (UNK the) (UNK New) (UNK York) (UNK Stock) (UNK Exchange)) (VP (UNK did) (UNK n't) (VP (UNK fall) (ADVP (UNK apart)) (NP (UNK Friday)) (UNK as) (S (NP (UNK the) (UNK Dow) (UNK Jones) (UNK Industrial) (UNK Average)) (VP (UNK plunged) (NP (UNK 190.58) (UNK points)) (PRN (UNK --) (NP (NP (UNK most)) (PP (UNK of) (NP (UNK it))) (PP (UNK in) (NP (UNK the) (UNK final) (UNK hour)))) (UNK --)))))))) (NP (UNK it)) (VP (ADVP (UNK barely)) (UNK managed) (S (VP (UNK to) (VP (UNK stay) (NP (NP (UNK this) (UNK side)) (PP (UNK of) (NP (UNK chaos)))))))) (UNK .))    And it's simillar when I use  . How to deal with that?"
"When my sentence length exceeds 300, I get an error, then I modified SENTENCE_MAX_LEN = 3000, but another error occurred. “tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[483] = 483 is not in [0, 300)  [[{{node GatherV2_1}}]],” what is the cause, is there any way to solve it?"
"Hi,  what is the full tagset used by the parser?  Thanks!"
Python 3.7.3     
"I have a copy of the revised PennTreebank that looks like the format of the files in `data/`.  However, the code breaks when I try to use these files. On further inspection, I'm guessing I need to insert a ""TOP"" tag at the start of every sentence? I did that and the model starts training, but then the EVAL script doesn't work. My copy of the treebank is somehow also missing a sentence. Is this what's causing the problem for the EVAL script? Can I just copy and paste the sentence that's missing from the silver trees you provided?"
"To start, thanks for making this available, putting it on pypi and having good documentation.    ---    The following code raises an `NonConstituentException`, which is unexpected since we're accessing the tree on the sentence level.         Without the newlines there is no exception.    I'm of course aware that analyzing a French sentence with an English model isn't useful but when analyzing large corpora from the web it happens from time to time that texts in different languages sneak in.    The following library versions were used:  * spaCy 2.0.18  * benepar 0.1.1  * en-core-web-sm (spaCy model) 2.0.0"
"Dear authors,    Is it possible to output a packed-forest instead of just a 1-best tree for each sentence? A relevant paper is written by Liang Huang in this (  paper.     Thanks. "
"This library appears very useful but it appears there hasn't been activity on it since around May 2021.    I had a couple installation issues which seemed to stem from the library not being up to date with newer versions of its dependencies, such as protobuf.    I think it could be good if people could see that this library is no longer maintained."
"I am getting a sub-optimal parse for a certain sentence while in the demo page it is parsed well (    What can be the source of the difference?     The sentence is:   Its more easy to write him an email  I tried with both benepar_en3 and benepar_en3_large. Both tag ""Its"" as VBZ while the demo page sets it to PRP.     Any idea what I need to change to get the parse I see on the demo page?   I am using a custom tokenizer but I doubt in this specific sentence it makes a difference, as each word is a token as I expect is the case also with Spacey. I also made sure I am using the latest versions of the models.   "
"When I try to download the French model, as indicated in the documentation, I get:       Can you please tell me how to download the French model? Thanks in advance!  "
"Hi,  Is it possible to disable nltk tagger, and use SpaCy tags with benepar 0.2 ?  Thank you for the great work."
"I am trying to run the code from the README which creates a simple doc instance with nlp object from spacy and I get the following error:  RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasCreate(handle)`    - Interestingly enough, the older version benepar 0.1.3, spacy 2.3 works fine with the GPU (with tensorflow).  - Other Torch based models run fine on my computer.  - Running with CUDA_VISIBLE_DEVICES="""" makes it work, with no exception, so the problem is related to the GPU.  - I am wondering if this can be something like this:   (where only a portion of the model is committed to cuda?)    Any idea why this might be happening with this library in particular?    ---------------------------------------------------------------------------------------------------------------------------  output of nvidia-smi while the model is loaded:  +-----------------------------------------------------------------------------+  | NVIDIA-SMI 460.56       Driver Version: 460.56       CUDA Version: 11.2     |  |-------------------------------+----------------------+----------------------+  | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |  | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |  |                               |                      |               MIG M. |  |===============================+======================+======================|  |   0  GeForce RTX 207...  Off  | 00000000:01:00.0 Off |                  N/A |  | N/A   34C    P5     9W /  N/A |   1309MiB /  7982MiB |      0%      Default |  |                               |                      |                  N/A |  +-------------------------------+----------------------+----------------------+                                                                                   +-----------------------------------------------------------------------------+  | Processes:                                                                  |  |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |  |        ID   ID                                                   Usage      |  |=============================================================================|  |    0   N/A  N/A      2786      G   /usr/lib/xorg/Xorg                  4MiB |  |    0   N/A  N/A     11405      C   python                           1301MiB |  +-----------------------------------------------------------------------------+      Full stacktrace:  Traceback (most recent call last):    File "" "", line 1, in      File ""/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/spacy/language.py"", line 995, in __call__      error_handler(name, proc,      File ""/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py"", line 540, in forward      attention_output = self.SelfAttention(    File ""/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 889, in _call_impl      result = self.forward(*input, **kwargs)    File ""/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py"", line 468, in forward      query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)    File ""/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 889, in _call_impl      result = self.forward(*input, **kwargs)    File ""/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/torch/nn/modules/linear.py"", line 94, in forward      return F.linear(input, self.weight, self.bias)    File ""/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/torch/nn/functional.py"", line 1753, in linear      return torch._C._nn.linear(input, weight, bias)  RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasCreate(handle)`  "
I clone this repo and train model using the following code:         Then a RuntimeError occurred:      
"Hi,    I'm trying to parse input text which has already been tagged using a model that includes a tagger. For this experiment, I'd like to disregard the tagger included in the parsing model but make the parser use the existing tags for tagging the text. Is this possible?"
"I am operating on a mac10.15 system and  have tensorflow 2.3, python 3.7 in anaconda  I've tried to change tf.Session to tf.compat.v1.Session but it still doesn't work, please help!"
"Hey,   I tried to load the model:   `import spacy  from benepar.spacy_plugin import BeneparComponent    nlp = spacy.load(""en"")  nlp.add_pipe(BeneparComponent(""benepar_en2""))`    But I got error from tensorflow:   `ValueError: NodeDef mentions attr 'Truncate' not in Op  y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: ToInt32_1 = Cast . (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).`    My tf version is 1.8.0, which was from one of the answer the author posted, but I still got this weird error. Anyone has idea about what's going wrong? Thanks a lot! "
"Hi,    I met several problems when executing this script. I'd be appreciate if anyone can help me out.    First, I saw ""PTB_TOKEN_UNESCAPE"" does not exist in ""parse_nk.py"", and I just commented that out.    Second, I observe the following error:     By looking into the code, it seems that my model does not have named parameters with ""f_tag"" prefix, and my model does have these with ""f_label"" prefix.  Does this mean that my model require gold POS tags as the input as well?"
"I have successfully trained a model and am trying to parse sentences but I get the following error when parsing some of the files:         This error also occurred when training the model but I removed some sentences to get around it. I have tried changing `max_len` in several places to find out where the error can be handled but nothing seems to work.  I have `sentence_max_len=300` unchanged in `main.py`, which I would think solved this problem but obviously not. Is there a way to handle this error?"
"When I run `python3 src/main.py train --model-path-base models/ --train-path data/gamalt_icepahc/train_random.clean --dev-path data/gamalt_icepahc/dev_random.clean --predict-tags --epochs 20 --use-words` the following error occurs:         I am using a Mac with python 3.6, pytorch 1.0.0 and Cython 0.29.14. Training is successful when the number of input sentences for training is small but this error comes up quickly when the number is increased.    Does anyone know why this error occurs and how it can be fixed?"
When I tried to run the following:  `python3 src/main.py train --model-path-base models/ --train-path data/gamalt_icepahc/train_random.clean --dev-path data/gamalt_icepahc/dev_random.clean --predict-tags --use-bert`    An error arose:   
I'm getting the error `AttributeError: module 'tensorflow' has no attribute 'GraphDef'` which appears to be an error resulting from using tensorflow 2.0. Is there any plan to add support for tensorflow 2.0? In the meantime what should I do to fix this?
For Future use tensorflow for use with cpu only is called `tensorflow-cpu`.  Maybe the setup.py should be updated to reflect this.   
"Hi,  First of all, thanks for your great contributions to your work and for releasing awesome codes.  I have a few questions about the SPMRL dataset, which has been used for testing your model in your paper.  It would be very appreciated if you could answer my inquiry below:    - Is it possible to get the dataset from  ?  I'm not sure I can obtain the dataset to replicate your result reported on the paper because the SPMRL organizers require one who wants to download their dataset to be certificated and it's actually been a while since the real workshop (or competition) was held.  - I am wondering you've recently got the dataset directly from the workshop organizers or have used in-house one (which may exist in your school or lab), or have fortunately obtained with a kind of indirect route.    Thanks for your response in advance."
"Training...      main()    File ""src/main.py"", line 608, in main      args.callback(args)    File ""src/main.py"", line 564, in        subparser.set_defaults(callback=lambda args: run_train(args, hparams))    File ""src/main.py"", line 312, in run_train      _, loss = parser.parse_batch(subbatch_sentences, subbatch_trees)    File ""/home/test/project/self-attentive-parser-master/src/parse_nk.py"", line 1095, in parse_batch      p_i, p_j, p_label, p_augment, g_i, g_j, g_label = self.parse_from_annotations(fencepost_annotations_start[start:end,:], fencepost_annotations_end[start:end,:], sentences[i], golds[i])    File ""/home/test/project/self-attentive-parser-master/src/parse_nk.py"", line 1148, in parse_from_annotations      p_score, p_i, p_j, p_label, p_augment = chart_helper.decode(False, **decoder_args)    File ""chart_helper.pyx"", line 11, in chart_helper.decode (/home/test/.pyxbld/temp.linux-x86_64-3.6/pyrex/chart_helper.c:1674)      def decode(int force_gold, int sentence_len, np.ndarray[DTYPE_t, ndim=3] label_scores_chart, int is_train, gold, label_vocab):  TypeError: decode() takes exactly 6 positional arguments (5 given)"
"I used default parameters and 8 layers bert ，trained on data ctb5.1.  I got 'FScore=90.50, CompleteMatch=28.82'  But the paper 'Cross-Domain Generalization of Neural Constituency Parsers'   refered F1=92.14, exactmatch=44.42  I was confused if there were anything wrong on my training, have you fine-tuned bert?"
"I got the error from tensorflow: module 'tensorflow' has no attribute 'GraphDef'. My tensorflow version is 2.0.0, is it any quick way to fix it?      "
"The current version of the code does not run with pytorch 1.2, which is the current latest version. I am running the training script on the ptb data with `--use-words` as the only flag.    The error is in the call of `FeatureDropoutFunction.apply()`, in the line    .  output is of shape ([2016, 1024]) and ctx.noise is of shape ([1379, 1024]), due to which the mul operation fails.    Note that this does not happen in every call of `FeatureDropoutFunction.apply()`. While stepping through, this exception is seen in the second call only. In the first time it's called, both the dimensions match and there is no exception thrown.    With Pytorch 1.1, these errors do not seem to appear. In a trial run, output and ctx.noise are of shape (1413, 1024) and there is no problem.    I can provide further stack traces if needed.    "
"when I changed the data from tree to binary tree  I got   Error reading EVALB results.  Gold path: /tmp/evalb-jndjlzq5/gold.txt  Predicted path: /tmp/evalb-jndjlzq5/predicted.txt  Output path: /tmp/evalb-jndjlzq5/output.txt  dev-fscore (Recall=nan, Precision=nan, FScore=nan, CompleteMatch=nan) dev-elapsed 0h00m00s total-elapsed 0h00m03s  "
"I follow the instructions on Spacy (  and use the `benepar_en` model.     When I use this model, it basically uses all the GPUs although they just utilize 163 MB for each GPU. Can I specify which GPU to use in practice? "
"When I test the sentence, I am prompted that the sentence is too long. Why is this?"
"I am using your parser to get the `parse_string` of a lot of data to calculate their parse tree depth. Most sentences work fine, but the exception is sentences that contain brackets. Take as input the following sentence.    > The Solano(r) range is phthalate-free and contains no heavy metals.    This will be parsed and stringified as          Prettifying this, gives us the following:         As is clear, the tree is incomplete. If you try to parse it with NLTK, it will fail. The parentheses don't match. As far as I can see, the NP should've been closed with a parenthesis **and** the `-RBR-` should be removed, so the correct parse looks like this:         The issue seems to be the word with the parentheses in it. When a parenthesis is not part of a token (like this), it works fine and it is parsed as -RBR-. However, if it's part of the token, things go wrong. Another example sentence is     > (They like(d) it a lot.)     The outer parentheses are parsed correctly, but `(d)` seems to give errors when generating the parse_string.     The above seems to be a bug of how the parse string is generated. Of course everything depends on the parsing early on in the pipeline, but even then the parse_string method should at least return a valid string (even if it is not 100% accurate).    However, another issue arises even when the parse string is corrected. Assume the 'correct' parse         The NLTK parse would look like         You can see that the `(d)` is, unsurprisingly, interpreted as a node. We don't want that. This is rather an issue with NLTK - but I am not sure how to ""escape"" parentheses here. Suggestions welcome, perhaps better suited on  ."
"I am curious to find out what the reason is for this inconsistency. Why does the package depend on Tensorflow, where you require PyTorch for training? Did you start off with Tensorflow and then made the switch to PT?"
"Hi,    Not sure what impact these messages have (so far) - I am reporting them FYI:    (On macosx 10.14.5, python 3.6.7, tensorflow 1.14.0)    >>> nlp.add_pipe(BeneparComponent(""benepar_en""))      WARNING: Logging before flag parsing goes to stderr.      W0718 09:41:51.815104 4736062912 deprecation_wrapper.py:119] From       /Users/.../anaconda3/lib/python3.6/site-packages/benepar/base_parser.py:199: The name       tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.        W0718 09:41:53.919924 4736062912 deprecation_wrapper.py:119] From       /Users/.../anaconda3/lib/python3.6/site-packages/benepar/base_parser.py:202: The name       tf.Session is deprecated. Please use tf.compat.v1.Session instead.    Regards    Colin Goldberg  "
">>> import benepar  /home/jhy/.pyxbld/temp.linux-x86_64-3.6/pyrex/benepar/chart_decoder.c:598:31: fatal error: numpy/arrayobject.h: No such file or directory  compilation terminated.  Traceback (most recent call last):    File ""/usr/lib/python3.6/distutils/unixccompiler.py"", line 118, in _compile      extra_postargs)    File ""/usr/lib/python3.6/distutils/ccompiler.py"", line 909, in spawn      spawn(cmd, dry_run=self.dry_run)    File ""/usr/lib/python3.6/distutils/spawn.py"", line 36, in spawn      _spawn_posix(cmd, search_path, dry_run=dry_run)    File ""/usr/lib/python3.6/distutils/spawn.py"", line 159, in _spawn_posix      % (cmd, exit_status))  distutils.errors.DistutilsExecError: command 'x86_64-linux-gnu-gcc' failed with exit status 1    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""/home/jhy/py3.6/lib/python3.6/site-packages/pyximport/pyximport.py"", line 215, in load_module      inplace=build_inplace, language_level=language_level)    File ""/home/jhy/py3.6/lib/python3.6/site-packages/pyximport/pyximport.py"", line 191, in build_module      reload_support=pyxargs.reload_support)    File ""/home/jhy/py3.6/lib/python3.6/site-packages/pyximport/pyxbuild.py"", line 102, in pyx_to_dll      dist.run_commands()    File ""/usr/lib/python3.6/distutils/dist.py"", line 955, in run_commands      self.run_command(cmd)    File ""/usr/lib/python3.6/distutils/dist.py"", line 974, in run_command      cmd_obj.run()    File ""/home/jhy/py3.6/lib/python3.6/site-packages/Cython/Distutils/old_build_ext.py"", line 186, in run      _build_ext.build_ext.run(self)    File ""/usr/lib/python3.6/distutils/command/build_ext.py"", line 339, in run      self.build_extensions()    File ""/home/jhy/py3.6/lib/python3.6/site-packages/Cython/Distutils/old_build_ext.py"", line 194, in build_extensions      self.build_extension(ext)    File ""/usr/lib/python3.6/distutils/command/build_ext.py"", line 533, in build_extension      depends=ext.depends)    File ""/usr/lib/python3.6/distutils/ccompiler.py"", line 574, in compile      self._compile(obj, src, ext, cc_args, extra_postargs, pp_opts)    File ""/usr/lib/python3.6/distutils/unixccompiler.py"", line 120, in _compile      raise CompileError(msg)  distutils.errors.CompileError: command 'x86_64-linux-gnu-gcc' failed with exit status 1    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File "" "", line 1, in      File ""/home/jhy/project/self-attentive-parser-master/benepar/__init__.py"", line 6, in        from .nltk_plugin import Parser    File ""/home/jhy/project/self-attentive-parser-master/benepar/nltk_plugin.py"", line 4, in        from .base_parser import BaseParser, IS_PY2, STRING_TYPES, PTB_TOKEN_ESCAPE    File ""/home/jhy/project/self-attentive-parser-master/benepar/base_parser.py"", line 8, in        from . import chart_decoder    File ""/home/jhy/py3.6/lib/python3.6/site-packages/pyximport/pyximport.py"", line 462, in load_module      language_level=self.language_level)    File ""/home/jhy/py3.6/lib/python3.6/site-packages/pyximport/pyximport.py"", line 231, in load_module      raise exc.with_traceback(tb)    File ""/home/jhy/py3.6/lib/python3.6/site-packages/pyximport/pyximport.py"", line 215, in load_module      inplace=build_inplace, language_level=language_level)    File ""/home/jhy/py3.6/lib/python3.6/site-packages/pyximport/pyximport.py"", line 191, in build_module      reload_support=pyxargs.reload_support)    File ""/home/jhy/py3.6/lib/python3.6/site-packages/pyximport/pyxbuild.py"", line 102, in pyx_to_dll      dist.run_commands()    File ""/usr/lib/python3.6/distutils/dist.py"", line 955, in run_commands      self.run_command(cmd)    File ""/usr/lib/python3.6/distutils/dist.py"", line 974, in run_command      cmd_obj.run()    File ""/home/jhy/py3.6/lib/python3.6/site-packages/Cython/Distutils/old_build_ext.py"", line 186, in run      _build_ext.build_ext.run(self)    File ""/usr/lib/python3.6/distutils/command/build_ext.py"", line 339, in run      self.build_extensions()    File ""/home/jhy/py3.6/lib/python3.6/site-packages/Cython/Distutils/old_build_ext.py"", line 194, in build_extensions      self.build_extension(ext)    File ""/usr/lib/python3.6/distutils/command/build_ext.py"", line 533, in build_extension      depends=ext.depends)    File ""/usr/lib/python3.6/distutils/ccompiler.py"", line 574, in compile      self._compile(obj, src, ext, cc_args, extra_postargs, pp_opts)    File ""/usr/lib/python3.6/distutils/unixccompiler.py"", line 120, in _compile      raise CompileError(msg)  ImportError: Building module benepar.chart_decoder failed: [""distutils.errors.CompileError: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n""]"
" import benepar  Traceback (most recent call last):    File "" "", line 1, in      File ""/project/self-attentive-parser-acl2018/benepar/__init__.py"", line 6, in        from .nltk_plugin import Parser    File ""/project/self-attentive-parser-acl2018/benepar/nltk_plugin.py"", line 4, in        from .base_parser import BaseParser    File ""/project/self-attentive-parser-acl2018/benepar/base_parser.py"", line 6, in        from . import chart_decoder  ImportError: cannot import name 'chart_decoder'  "
"Hello! I need to access each tree node label while traversing the tree. And the problem I faced is that I don't have an access to the leaf node's labels.    I have following code:         which gives such output:     As you can see, last parse string has NNP, but the labels tuple is empty    is it a bug or am I doing it wrong?"
"spaCy has several models that are capable of dependency parsing in English: `en_core_web_sm`, `en_core_web_md`, `en_core_web_lg` (  There's a pretty good demo available of them through  . Are there any performance comparisons for dependency parsing with benepar vs these?"
"First of all, thank you for sharing the code for this great work.    I'm trying to train a model in the most simple setup using the following command-line arguments:  (python self-attentive-parser-master/src/main.py) train --model-path-base . --train-path self-attentive-parser-master\data\02-21.10way.clean --use-words    using python 3.6 on windows 10 with the latest pytorch, cython etc. I get the following error after ""Training..."":    Traceback (most recent call last):    File ""self-attentive-parser-master/src/main.py"", line 612, in        main()    File ""self-attentive-parser-master/src/main.py"", line 608, in main      args.callback(args)    File ""self-attentive-parser-master/src/main.py"", line 564, in        subparser.set_defaults(callback=lambda args: run_train(args, hparams))    File ""self-attentive-parser-master/src/main.py"", line 312, in run_train      _, loss = parser.parse_batch(subbatch_sentences, subbatch_trees)    File ""self-attentive-parser-master\src\parse_nk.py"", line 1010, in parse_batch      annotations, _ = self.encoder(emb_idxs, batch_idxs, extra_content_annotations=extra_content_annotations)    File ""venv_parsing_36\lib\site-packages\torch\nn\modules\module.py"", line 489, in __call__      result = self.forward(*input, **kwargs)    File ""self-attentive-parser-master\src\parse_nk.py"", line 607, in forward      res, timing_signal, batch_idxs = emb(xs, batch_idxs, extra_content_annotations=extra_content_annotations)    File ""venv_parsing_36\lib\site-packages\torch\nn\modules\module.py"", line 489, in __call__      result = self.forward(*input, **kwargs)    File ""self-attentive-parser-master\src\parse_nk.py"", line 486, in forward      for x, emb, emb_dropout in zip(xs, self.embs, self.emb_dropouts)    File ""self-attentive-parser-master\src\parse_nk.py"", line 486, in        for x, emb, emb_dropout in zip(xs, self.embs, self.emb_dropouts)    File ""venv_parsing_36\lib\site-packages\torch\nn\modules\module.py"", line 489, in __call__      result = self.forward(*input, **kwargs)    File ""venv_parsing_36\lib\site-packages\torch\nn\modules\sparse.py"", line 118, in forward      self.norm_type, self.scale_grad_by_freq, self.sparse)    File ""venv_parsing_36\lib\site-packages\torch\nn\functional.py"", line 1454, in embedding      return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)  RuntimeError: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.cuda.IntTensor instead (while checking arguments for embedding)    Process finished with exit code 1    Trying to run with 'use_cuda = False' in parse_nk.py I get the same error (with 'torch.IntTensor' instead of 'torch.cuda.IntTensor'), so it doesn't seem to be cuda-related.    To make sure this is not a compatibility issue, I tried running in another virtual environment with python 3.6, cython 0.25.2 and pytorch 0.4.1 (with which the code was originally tested, according to the documentation), and I get the same error, with 'torch.cpu.IntTensor' replaced by 'CUDAIntTensor'.    I found some references for this error on the web but nothing helpful. Have you encountered this error? Any idea what's causing it?    Thanks"
"Just curious if the default models (specifically `benepar_en2` or `benepar_en2_large`) include BERT embeddings? I see references to BERT and ELMo in the training section. Do we need to do that training ourselves if we want a parser to use those embeddings, or are they already included?    And more importantly, even if the default models don't include BERT/ELMo embeddings, is it worth doing the training?    Thanks!"
When I tried to run as the follow:  `python src/main.py train --use-elmo --model-path-base models/en_elmo --num-layers 4`     There was a Error:         
"First, thank you for making this wonderful tool available. I can't say enough good things about it. Very impressive indeed.     That makes these bizarre tagger errors all the more surprising.          Output:       In all three, spaCy's tagger was right and BNP's was wrong. The third one was the most troubling. With all due respect to the famous mariner Zheng He, it's not very wise to guess that ""He"" at the beginning of a sentence is any thing other than a pronoun.     When I read that ""tagger in models such as benepar_en2 gives better results,"" I actually went out of my way to make sure that BNP's tags are used instead of  spaCy's. Now I'm not so sure. Can you quantify or describe how BNP's tagging is better, please? I don't have time to do an exhaustive test, so your input will carry a lot of weight.  How do you think `benepar_en2` compares with spaCy 2.1.3's `en_core_web_lg` for tagging?    Again, I emphasize that these tagging errors in no way detract from the project, which to me is about parsing, and BNP is excellent at that. "
"As I read the benepar documents, benepar should not update pos_ attribute, it should only set `span._.*` and `token._.*` custom attributes.              The output is like this;       Why does this happen?"
"Hi,    Just letting you know that parsing Hungarian should be available through spaCy, as it can   and      Thanks,  Gyuri"
>>> benepar.download('benepar_en')  [nltk_data] Error loading benepar_en:    False    Attempts to download the models as in the instructions fail due to the above. Any ideas?
"Hi, I'm using the pretrained benepar model as described in  . It does not produce (-LRB- -LRB-)/(-RRB- -RRB-) as other standard parsers for cases of parentheticals. For example, parsing this sentence:    _Representative George Hansen (R., Idaho) drew a reprimand in nineteen eighty-four after a felony conviction for falsifying his financial disclosures._    gives    (S     (NP      (NP (JJ Representative) (NNP George) (NNP Hansen))      **(PRN (( () (NP (NNP R.)) (, ,) (NP (NNP Idaho)) () ))))**    (VP      (VBD drew)      (NP (DT a) (NN reprimand))      (PP (IN in) (NP (JJ nineteen) (JJ eighty-four)))      (PP        (IN after)        (NP          (NP (DT a) (NN felony) (NN conviction))          (PP            (IN for)            (S              (VP                (VBG falsifying)                (NP (PRP$ his) (JJ financial) (NNS disclosures))))))))    (. .))    The empty labels are particularly problematic when used with the trees.py module in this repo. Is this a bug or is this your own label convention? "
"One of the most problematic things in Py2 is `unicode` and `str`. To backport several features, I (and so do many) usually use      This means when I import your model `benepar_en_small`, I get an error.         This is because of      I think making it `basestring` instead of just `str` can resolve this issue. Please let me know if this is comfortable. "
"hi, i'm using the cpu version in nltk, i hope to parallel the parser with python's multiprocessing module, but it fails. Is there any way to solve this?"
"Hi,     I want to use benepar, so I try to download benepar_en with this code:       I have this result:     And no download (I waited 30min), when I go to this folder, I have the file benepar_en created, but its size is 0KB...    How can I solve that please ?    I try this on another computer, same things...    Best,  "
"This is the error    $ pip install benepar[cpu]  Collecting benepar[cpu]    Using cached    Requirement already satisfied: cython in /anaconda/lib/python3.6/site-packages (from benepar[cpu]) (0.28.5)  Requirement already satisfied: numpy in /anaconda/lib/python3.6/site-packages (from benepar[cpu]) (1.14.5)  Requirement already satisfied: nltk>=3.2 in /anaconda/lib/python3.6/site-packages (from benepar[cpu]) (3.2.5)  Requirement already satisfied: tensorflow>=1.8.0 in /anaconda/lib/python3.6/site-packages (from benepar[cpu]) (1.10.1)  Requirement already satisfied: six in /anaconda/lib/python3.6/site-packages (from nltk>=3.2->benepar[cpu]) (1.11.0)  Requirement already satisfied: absl-py>=0.1.6 in /anaconda/lib/python3.6/site-packages (from tensorflow>=1.8.0->benepar[cpu]) (0.4.1)  Requirement already satisfied: protobuf>=3.6.0 in /anaconda/lib/python3.6/site-packages (from tensorflow>=1.8.0->benepar[cpu]) (3.6.1)  Requirement already satisfied: astor>=0.6.0 in /anaconda/lib/python3.6/site-packages (from tensorflow>=1.8.0->benepar[cpu]) (0.7.1)  Requirement already satisfied: tensorboard =1.10.0 in /anaconda/lib/python3.6/site-packages (from tensorflow>=1.8.0->benepar[cpu]) (1.10.0)  Requirement already satisfied: wheel>=0.26 in /anaconda/lib/python3.6/site-packages (from tensorflow>=1.8.0->benepar[cpu]) (0.31.1)  Requirement already satisfied: gast>=0.2.0 in /anaconda/lib/python3.6/site-packages (from tensorflow>=1.8.0->benepar[cpu]) (0.2.0)  Requirement already satisfied: termcolor>=1.1.0 in /anaconda/lib/python3.6/site-packages (from tensorflow>=1.8.0->benepar[cpu]) (1.1.0)  Requirement already satisfied: grpcio>=1.8.6 in /anaconda/lib/python3.6/site-packages (from tensorflow>=1.8.0->benepar[cpu]) (1.14.2)  Collecting setuptools =1.8.0->benepar[cpu])    Using cached    Requirement already satisfied: werkzeug>=0.11.10 in /anaconda/lib/python3.6/site-packages (from tensorboard =1.10.0->tensorflow>=1.8.0->benepar[cpu]) (0.14.1)  Requirement already satisfied: markdown>=2.6.8 in /anaconda/lib/python3.6/site-packages (from tensorboard =1.10.0->tensorflow>=1.8.0->benepar[cpu]) (2.6.11)  Building wheels for collected packages: benepar    Running setup.py bdist_wheel for benepar ... error    Complete output from command /anaconda/bin/python -u -c ""import setuptools, tokenize;__file__='/private/var/folders/gr/plmtcxzd19705bxnz4k6hqz00000gn/T/pip-install-v5le71uh/benepar/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" bdist_wheel -d /private/var/folders/gr/plmtcxzd19705bxnz4k6hqz00000gn/T/pip-wheel-vtm6_ah6 --python-tag cp36:    running bdist_wheel    running build    running build_py    creating build    creating build/lib.macosx-10.9-x86_64-3.6    creating build/lib.macosx-10.9-x86_64-3.6/benepar    copying benepar/downloader.py -> build/lib.macosx-10.9-x86_64-3.6/benepar    copying benepar/__init__.py -> build/lib.macosx-10.9-x86_64-3.6/benepar    copying benepar/base_parser.py -> build/lib.macosx-10.9-x86_64-3.6/benepar    copying benepar/spacy_plugin.py -> build/lib.macosx-10.9-x86_64-3.6/benepar    copying benepar/nltk_plugin.py -> build/lib.macosx-10.9-x86_64-3.6/benepar    copying benepar/chart_decoder.pyx -> build/lib.macosx-10.9-x86_64-3.6/benepar    running build_ext    building 'benepar.chart_decoder' extension    creating build/temp.macosx-10.9-x86_64-3.6    creating build/temp.macosx-10.9-x86_64-3.6/benepar    clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/anaconda/include -mmacosx-version-min=10.9 -m64 -fPIC -I/anaconda/include -mmacosx-version-min=10.9 -m64 -fPIC -I/anaconda/lib/python3.6/site-packages/numpy/core/include -I/anaconda/include/python3.6m -c benepar/chart_decoder.c -o build/temp.macosx-10.9-x86_64-3.6/benepar/chart_decoder.o    In file included from benepar/chart_decoder.c:17:    /anaconda/include/python3.6m/Python.h:25:10: fatal error: 'stdio.h' file not found    #include               ^~~~~~~~~    **1 error generated.    error: command 'clang' failed with exit status 1**        ----------------------------------------    Failed building wheel for benepar    Running setup.py clean for benepar  Failed to build benepar  Installing collected packages: benepar, setuptools    Running setup.py install for benepar ... error      Complete output from command /anaconda/bin/python -u -c ""import setuptools, tokenize;__file__='/private/var/folders/gr/plmtcxzd19705bxnz4k6hqz00000gn/T/pip-install-v5le71uh/benepar/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /private/var/folders/gr/plmtcxzd19705bxnz4k6hqz00000gn/T/pip-record-fy8y1cv0/install-record.txt --single-version-externally-managed --compile:      running install      running build      running build_py      creating build      creating build/lib.macosx-10.9-x86_64-3.6      creating build/lib.macosx-10.9-x86_64-3.6/benepar      copying benepar/downloader.py -> build/lib.macosx-10.9-x86_64-3.6/benepar      copying benepar/__init__.py -> build/lib.macosx-10.9-x86_64-3.6/benepar      copying benepar/base_parser.py -> build/lib.macosx-10.9-x86_64-3.6/benepar      copying benepar/spacy_plugin.py -> build/lib.macosx-10.9-x86_64-3.6/benepar      copying benepar/nltk_plugin.py -> build/lib.macosx-10.9-x86_64-3.6/benepar      copying benepar/chart_decoder.pyx -> build/lib.macosx-10.9-x86_64-3.6/benepar      running build_ext      building 'benepar.chart_decoder' extension      creating build/temp.macosx-10.9-x86_64-3.6      creating build/temp.macosx-10.9-x86_64-3.6/benepar      clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/anaconda/include -mmacosx-version-min=10.9 -m64 -fPIC -I/anaconda/include -mmacosx-version-min=10.9 -m64 -fPIC -I/anaconda/lib/python3.6/site-packages/numpy/core/include -I/anaconda/include/python3.6m -c benepar/chart_decoder.c -o build/temp.macosx-10.9-x86_64-3.6/benepar/chart_decoder.o      In file included from benepar/chart_decoder.c:17:      /anaconda/include/python3.6m/Python.h:25:10: fatal error: 'stdio.h' file not found      #include                 ^~~~~~~~~      1 error generated.      error: command 'clang' failed with exit status 1            ----------------------------------------  Command ""/anaconda/bin/python -u -c ""import setuptools, tokenize;__file__='/private/var/folders/gr/plmtcxzd19705bxnz4k6hqz00000gn/T/pip-install-v5le71uh/benepar/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /private/var/folders/gr/plmtcxzd19705bxnz4k6hqz00000gn/T/pip-record-fy8y1cv0/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /private/var/folders/gr/plmtcxzd19705bxnz4k6hqz00000gn/T/pip-install-v5le71uh/benepar/  "
"Hello,     Thanks a lot for making your code available and for the thorough documentation!     I have pytorch 0.4.x, which is also the default version when installing allennlp 0.6.1. A couple of questions/comments I have:     1. [EDIT: never mind]    2. When using the option `--use-elmo`, I was able to load the weights and everything seems to be running fine, but I get this warning at the beginning:     > In file included from /atm/turkey/vol/transitory/ttmt001/envs/py3.6-gpu/lib/python3.6/site-packages/numpy/core/include/numpy/ndarraytypes.h:1821:0,                   from /atm/turkey/vol/transitory/ttmt001/envs/py3.6-gpu/lib/python3.6/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,                   from /atm/turkey/vol/transitory/ttmt001/envs/py3.6-gpu/lib/python3.6/site-packages/numpy/core/include/numpy/arrayobject.h:4,                   from /homes/ttmt001/.pyxbld/temp.linux-x86_64-3.6/pyrex/thinc/neural/gpu_ops.c:568:  /atm/turkey/vol/transitory/ttmt001/envs/py3.6-gpu/lib/python3.6/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning ""Using deprecated NumPy API, disable it by "" ""#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION"" [-Wcpp]   #warning ""Using deprecated NumPy API, disable it by "" \    ^  /homes/ttmt001/.pyxbld/temp.linux-x86_64-3.6/pyrex/thinc/neural/gpu_ops.c:570:24: fatal error: _cuda_shim.h: No such file or directory   #include ""_cuda_shim.h""                          ^  compilation terminated.    So the issue seems to be somewhere in the interaction between allennlp, numpy, and perhaps cython (.pyx)? I'm using a virtual environment (python 3.6) without gpu, so I'm not sure why there's a warning about cuda and gpu_ops. I don't get this warning if I just do `from  allennlp.modules.elmo import Elmo` in the command line (without .pyx dependencies). "
Ubuntu 16  anaconda install  pip install benepar  ---this yields  RuntimeError: module compiled against API version 0xc but this version of numpy is 0xa
"Hi,    When I trying to use NLTK with benepar, I have this problem. I can't download the benepar_en.gz. Can anyone help me with this? Thanks a lot!!!      Resource benepar_en.gz not found.    Please use the NLTK Downloader to obtain the resource:      >>> import nltk    >>> benepar.download('benepar_en.gz')  "
"I'm trying to run the parse using spacy on the American Constitution (  with this code:       I'm getting an error - the error is different for using CPU or GPU. I've also tried longer text, and there is no problem. If I remove `nlp.add_pipe(BeneparComponent(""benepar_en""))`, it works.    ### CPU Error       ### GPU Error   "
"Hi,     thanks for this great work. I installed the parser (""benepar_en"") using pip and tested its performance the WSJ 23 section. What I found is that, the performance is higher than the number claimed in README (in my side, it shows 95.98 (vs 95.07 in README)). I am wondering if the pre-trained model in pip has used WSJ 23  in training? It seems unclear what is used to train the pip model.    Thanks in advance."
"I am not sure about the following - please clarify - or report a bug (?)    The text I am parsing:  when a notification is received that a driver is available then update that fact in the database    Result from parse_string:  >>> print(sent._.parse_string)  (FRAG (SBAR (WHADVP (WRB when)) (S (NP (DT a) (NN notification)) (VP (VBZ is) (VP (VBN received) (SBAR (IN that) (S (NP (DT a) (NN driver)) (VP (VBZ is) (ADJP (JJ available))))))))) (ADVP (RB then)) (VP (NN update) (NP (DT that) (NN fact)) (PP (IN in) (NP (DT the) (NN database)))))    In it, I see that the word 'update' is marked as NN. Should it not be a verb?    [The following added later, after further exploration]    A test script (below) produces a dict that is missing the entry for the word ""update"" - I think because no label was available for the word.    Please excuse the naivete of the test - I am a newcomer to Python as well.    Note: spaces replaced by dots.    `  import benepar  import spacy  from benepar.spacy_plugin import BeneparComponent  nlp = spacy.load('en')  nlp.add_pipe(BeneparComponent(""benepar_en""))    doc = nlp(""when a notification is received that a driver is available then update that fact in the database"")  sent = list(doc.sents)[0]  print(sent._.parse_string)    def.get_children(parent):  ..ret_dict.=.{}  ..if.len(list(parent._.children)).>.0:  ....for.child.in.parent._.children:  ......print(child)  ......try:  ........if.len(list(child._.labels)).>.0:  ..........lab.=.list(child._.labels)[0]  ..........print(lab)  ..........child_dict.=.{""label"":.lab,.""text"":.str(child)}  ..........gc.=.get_children(child)  ..........child_dict[""children""].=.gc  ..........ret_dict[lab].=.child_dict  ......except.Exception.as.e:  ...... pass  ..return.ret_dict    gc = get_children(sent)  print(gc)  `    The following gc result was output:  `  {   ""SBAR"": {    ""label"": ""SBAR"",    ""text"": ""when a notification is received that a driver is available"",    ""children"": {     ""WHADVP"": {      ""label"": ""WHADVP"",      ""text"": ""when"",      ""children"": {}     },     ""S"": {      ""label"": ""S"",      ""text"": ""a notification is received that a driver is available"",      ""children"": {       ""NP"": {        ""label"": ""NP"",        ""text"": ""a notification"",        ""children"": {}       },       ""VP"": {        ""label"": ""VP"",        ""text"": ""is received that a driver is available"",        ""children"": {         ""VP"": {          ""label"": ""VP"",          ""text"": ""received that a driver is available"",          ""children"": {           ""SBAR"": {            ""label"": ""SBAR"",            ""text"": ""that a driver is available"",            ""children"": {             ""S"": {              ""label"": ""S"",              ""text"": ""a driver is available"",              ""children"": {               ""NP"": {                ""label"": ""NP"",                ""text"": ""a driver"",                ""children"": {}               },               ""VP"": {                ""label"": ""VP"",                ""text"": ""is available"",                ""children"": {                 ""ADJP"": {                  ""label"": ""ADJP"",                  ""text"": ""available"",                  ""children"": {}                 }                }               }              }             }            }           }          }         }        }       }      }     }    }   },   ""ADVP"": {    ""label"": ""ADVP"",    ""text"": ""then"",    ""children"": {}   },   ""VP"": {    ""label"": ""VP"",    ""text"": ""update that fact in the database"",    ""children"": {     ""NP"": {      ""label"": ""NP"",      ""text"": ""that fact"",      ""children"": {}     },     ""PP"": {      ""label"": ""PP"",      ""text"": ""in the database"",      ""children"": {       ""NP"": {        ""label"": ""NP"",        ""text"": ""the database"",        ""children"": {}       }      }     }    }   }  }  `  Note: Word ""update"" is missing.    Colin Goldberg  "
"Hi,    On macosx 10.13.5 I got an error at the 'nlp.add_pipe(BeneparComponent(""benepar_en""))' statement:    `  $ python  Python 3.6.4 |Anaconda custom (64-bit)| (default, Dec 21 2017, 15:39:08)   [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin  Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.  >>> import benepar  /Users/colingoldberg/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.    from ._conv import register_converters as _register_converters  >>> benepar.download('benepar_en')  [nltk_data] Downloading package benepar_en to  [nltk_data]     /Users/colingoldberg/nltk_data...  True  >>> import spacy  >>> from benepar.spacy_plugin import BeneparComponent  >>> nlp = spacy.load('en')  >>> nlp.add_pipe(BeneparComponent(""benepar_en""))  Traceback (most recent call last):    File "" "", line 1, in      File ""/Users/colingoldberg/anaconda3/lib/python3.6/site-packages/benepar/spacy_plugin.py"", line 73, in __init__      super(BeneparComponent, self).__init__(filename, batch_size)    File ""/Users/colingoldberg/anaconda3/lib/python3.6/site-packages/benepar/base_parser.py"", line 163, in __init__      tf.import_graph_def(graph_def, name='')    File ""/Users/colingoldberg/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 316, in new_func      return func(*args, **kwargs)    File ""/Users/colingoldberg/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 541, in import_graph_def      raise ValueError('No op named %s in defined operations.' % node.op)  ValueError: No op named ClipByValue in defined operations.    `    I also tried this on an AWS EC2 (t2.small) instance (2GB memory), and got the following error:    `  # python  Python 3.6.3 |Anaconda custom (64-bit)| (default, Oct 13 2017, 12:02:49)   [GCC 7.2.0] on linux  Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.  >>> import benepar  >>> benepar.download('benepar_en')  [nltk_data] Downloading package benepar_en to /root/nltk_data...  [nltk_data]   Package benepar_en is already up-to-date!  True  >>> import spacy  >>> from benepar.spacy_plugin import BeneparComponent  >>> nlp = spacy.load('en')  >>> nlp.add_pipe(BeneparComponent(""benepar_en""))  Traceback (most recent call last):    File "" "", line 1, in      File ""/usr/local/share/anaconda3/lib/python3.6/site-packages/benepar/spacy_plugin.py"", line 73, in __init__      super(BeneparComponent, self).__init__(filename, batch_size)    File ""/usr/local/share/anaconda3/lib/python3.6/site-packages/benepar/base_parser.py"", line 163, in __init__      tf.import_graph_def(graph_def, name='')    File ""/usr/local/share/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func      return func(*args, **kwargs)    File ""/usr/local/share/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 486, in import_graph_def      with c_api_util.tf_buffer(graph_def.SerializeToString()) as serialized:  MemoryError    `    I look forward to having this issue resolved so that I can continue testing.    Thank you.    Colin Goldberg  "
"hello,I want to run ' parser = benepar.Parser(""benepar_en"")', but it raise  the error like this ""ValueError: No op named GatherV2 in defined operations"".  could you tell me your python version and tensorflow-gpu version and cudnn version. I don't know how to fix this error . thank you very much."
"I meet this error and really can not solve it:  [W C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Indexing.cu:963] Warning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (function masked_fill__cuda)  Could you somehow fix it? Thanks!!"
I am trying to find the hyperparameters  .sh file for the mid-size enwiki8 model with 88M parameters for torch. Searched the web but unfortunately found nothing.
is it some mask mechanism as in transformer decoder
"I have followed the required tensorflow 1.12 and python 2.7, but the following errors still raised. I wonder if you could help me. By the way, it is suggested by the internet that the CUBLAS_STATUS_EXECUTION_FAILED raises when tensorflow version does not match the cuda version. Could you please tell me the gpu type and cuda version you used to train? Looking forward to your reply.  `2021-09-08 23:23:08.258752: E tensorflow/stream_executor/cuda/cuda_blas.cc:652] failed to run cuBLAS routine cublasSgemm_v2: CUBLAS_STATUS_EXECUTION_FAILED  Traceback (most recent call last):    File ""train_gpu.py"", line 475, in        tf.app.run()    File ""/home/caoyq/anaconda3/envs/tensorflow_cp27/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run      _sys.exit(main(argv))    File ""train_gpu.py"", line 471, in main      evaluate(n_token, cutoffs, ""/gpu:0"")    File ""train_gpu.py"", line 446, in evaluate      fetched = sess.run(fetches, feed_dict=feed_dict)    File ""/home/caoyq/anaconda3/envs/tensorflow_cp27/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 929, in run      run_metadata_ptr)    File ""/home/caoyq/anaconda3/envs/tensorflow_cp27/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1152, in _run      feed_dict_tensor, options, run_metadata)    File ""/home/caoyq/anaconda3/envs/tensorflow_cp27/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run      run_metadata)    File ""/home/caoyq/anaconda3/envs/tensorflow_cp27/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call      raise type(e)(node_def, op, message)  tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(31424, 1024), b.shape=(1024, 3072), m=31424, n=3072, k=1024            ]]    Caused by op u'transformer/layer_0/rel_attn/qkv/Tensordot/MatMul', defined at:    File ""train_gpu.py"", line 475, in        tf.app.run()    File ""/home/caoyq/anaconda3/envs/tensorflow_cp27/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run      _sys.exit(main(argv))    File ""train_gpu.py"", line 471, in main      evaluate(n_token, cutoffs, ""/gpu:0"")    File ""train_gpu.py"", line 400, in evaluate      mems=mems_i)    File ""train_gpu.py"", line 218, in single_core_graph      is_training=is_training)    File ""train_gpu.py"", line 186, in model_fn      proj_same_dim=FLAGS.proj_same_dim)        return self.__call__(inputs, *args, **kwargs)    File ""/home/caoyq/anaconda3/envs/tensorflow_cp27/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 374, in __call__      outputs = super(Layer, self).__call__(inputs, *args, **kwargs)    File ""/home/caoyq/anaconda3/envs/tensorflow_cp27/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 757, in __call__      outputs = self.call(inputs, *args, **kwargs)    File ""/home/caoyq/anaconda3/envs/tensorflow_cp27/lib/python2.7/site-packages/tensorflow/python/keras/layers/core.py"", line 963, in call      outputs = standard_ops.tensordot(inputs, self.kernel,  ]]  `"
"Hi,     I have a quick question with respect to the relative shift operation:         In the transformer-xl paper, Appendix B (  we see that the upper right triangular of matrix B consists of zeros. In the above code and throughout the model implementation `zero_triu == False` so that after performing the relative shift, the upper right triangle is not filled with zeros as described in the paper.   In the huggingface implementation of this function, this unused parameter is completely removed (see      Is the upper right triangle masked at a later place no matter what, or why can `zero_triu` be neglected? "
Is the project running on Linux or Windows?
您好，我下载您的代码，环境都一样，只是在windows环境中运行，执行对应的训练发现不行，可以详细赐教下不。 
"  File ""/data/lw/2Dxiangao/data_parallel_my_v2.py"", line 89, in scatter      bsz = inputs[0].size(self.dim)  AttributeError: 'list' object has no attribute 'size'    "
"I'm very interested in your work. I think your work is very pioneering and meaningful. Does this package run only with Python scripts? I can't run with shell script, maybe I'm not proficient in shell script. Expert, can you provide an example program running with Python script? Thank you very much."
"The positional encoding in the code is:        pos_seq = tf.range(klen - 1, -1, -1.0)      inv_freq = 1 / (10000 ** (tf.range(0, d_model, 2.0) / d_model))      pos_emb = positional_embedding(pos_seq, inv_freq)    Then used to build the `r_head_k` tensor and finally used in the BD term:        BD = tf.einsum('ibnd,jnd->ijbn', rr_head_q, r_head_k)      BD = rel_shift(BD)    In the paper, the left-shift is done on a Lx(M+L) (i.e. `[qlen, qlen+klen]`) matrix, but here it is on a LxM if I'm not mistaken. The upper right relative positional encoding are thus erroneous, no ?     I understand that this is not a problem as these ones are masked afterwards, but if we were not using the mask, the `pos_seq` should be computed with `klen + qlen`, and then truncated after left-shift before adding to term `AC` ?    Or did I miss something ?"
"First time ever posting an issue, apologies if I've written something incorrectly or missing obvious things.    On lines 82-88 of the file `transformer-xl/pytorch/eval.py`, the perplexity is being computed by computing the total loss and the total segment size.       Rather than adding to the total loss the term `loss.sum()`, the implementation instead multiplies the mean by `seq_len`. However when computing loss, there should only `seq_len - 1` losses in the output of the model (in language modeling you predict the next token based on the previous tokens, so this excludes computing a loss value for the very first token).     (Compare this against the TF implementation in file `transformer-xl/tf/train_gpu.py`         Here this issue is avoided because all losses are appended into a list `tower_losses` and then summed over and divided by the length of that list.)    This is subtle because it will make your perplexity value _seem_ correct, but in actuality your perplexity computation is pretending to include one extra term. I think this is the correct implementation:         Or         Is this a bug? Am I missing something? Thanks!  "
I ran the model training program using   script on 3 Tesla M40 GPUs (every GPU has a memory of 24 GB). But I ran into this problem.  !   What's the problem behind this phenomenon?  
"Hi First of all, I really appreciate you for your great code.  I was following your code and got question about the reason why you took bpc for enwik8 and text8.  Is there any reason choosing bpc for enwik8 and text8, and ppl for the others?  Also, is there any problem if I change the original enwik8 and text8 code to get ppl.      Thank you."
"Hello,    The copyright seems to be missing in the license:       I used this repository as a starting point for my project (which I am about to release),  so I would like to get this information.    Thank you!"
"Hi, thanks for the great code. Since LMShuffledIterator are putting different samples (unrelated sentences) in different batches, how is the memory still useful? Thanks"
"I have this issue with Positional Embedding at this line:  `sinusoid_inp = torch.ger(pos_seq, self.inv_freq)`    The following error message is like this       I want to use Transformer-XL in my problem is binary classification for document context."
None
None
Is there an implementation that is based on tf2.x and python3.x? 
"Hi,   I was trying to reproduce the WT_103 lm results with TF models. I was running pytorch ones with batchsize 60 4xV100 16G GPUs fine. However if I change to tensorflow. There is always out-of-memory error even if I changed the batch size to 40 with 4 GPUs    May I know what is the estimated GPU requirements for training both tf_wt103 base and large models.      ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape ]]  Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.      ]]  Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.  "
This is how the positional embeddings matrix is constructed in the code:   `  This basically creates a matrix of  [sin | cos] whereas the implementation in other papers including the original Attention is all you need had a positional embedding in which the sin and cos alternated between each other along the embedding dimension. Does this have anything to do with the relative positional embedding?    Thanks! 
"In transformer-xl/pytorch/mem_transformer.py,  I found the argument order of _update_mems ftn(member of class MemTransformerLM) is wrong!     619th line :   def _update_mems(self, hids, mems, qlen, mlen):  733th line :  new_mems = self._update_mems(hids, mems, mlen, qlen)    I tested the code when qlen == mlen, so the code worked without any problem,  but the code should be corrected, for the case when qlen != mlen.  Thanks!"
"Hi,    Can you provide sample code for fine-tuning Transformer-XL for classification task (Just like BERT)?    Thanks!"
"I use torch=1.4.0, and python=3.6.4  running sh on only one GPU (the corporation condition is poor) -.-  it indicates   /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.    how can i solve this error?"
"Hi, I am using the eval script by changing the `tgt_len` parameter. It ideally changes the ""number of tokens to predict"" and even changes the way data is pre-processed. However, tweaking this parameter during evaluation does not perplexity even slightly. What could be the reason? And how can we test this model's performance for different context windows? Thanks."
"Hi authors,  Thanks for sharing this great code. I am wondering why the input index of positional embedding, pos_seq, needs to be in descending order as [klen-1, ..., 1, 0], so that pos_emb is also in descending order along sequence dimension:    > pos_seq = torch.arange(klen-1, -1, -1.0, device=word_emb.device, dtype=word_emb.dtype)  > pos_emb = self.pos_emb(pos_seq)         Is this an important setting for Transformer model? What will happen if pos_seq is in ascending order as `torch.arange(0, klen)` ?    Thank you very much!"
"TRAIN_BSZ=64 is used in text8_large_tpu.sh.    During training data preparation it is used as  ""--per_host_train_bsz=${TRAIN_BSZ}""  During training it is used as ""--train_batch_size=${TRAIN_BSZ}"" and when calling data_utils.get_input_fn() it is used as ""per_host_bsz=FLAGS.train_batch_size // FLAGS.num_hosts,""    Maybe I missed something so get a bit confused on this, wonder if anyone could explain this a bit?"
"In   , I am wondering why only `key` and `value` are layernormed, while `query` is not normed? In other variants (such as RelMultiHeadAttn), the qkv computation is implemented by a single self.qkv_net layer.     "
"`mlen = mems[0].size(0) if mems is not None else 0  klen = mlen + qlen  if self.same_length:      all_ones = word_emb.new_ones(qlen, klen)      mask_len = klen - self.mem_len      if mask_len > 0:          mask_shift_len = qlen - mask_len      else:          mask_shift_len = qlen      dec_attn_mask = (torch.triu(all_ones, 1+mlen)              + torch.tril(all_ones, -mask_shift_len)).byte()[:, :, None] # -1  else:      dec_attn_mask = torch.triu(          word_emb.new_ones(qlen, klen), diagonal=1+mlen).byte()[:,:,None]`    what if `mems` is None? that is, mlen is zero.  Actually, `dec_attn_mask` will be all-one matrix.   My test demo shows that it seems to cause bad results in eval phase (same_length is True) ."
"Hi, kimiyoung.  Your transformer-xl idea is interesting. I have some confusion about the git resp. In pytorch and tensorflow branch, 1-billion experiment parameters seems have some difference. First, mem_len and target_len is 32 in pytorch branch, but 256 in tensorflow branch. Second, warmup_steps is 20000 in pytorch branch, but 0 in tensorflow branch.  I ran two branch default experiments. 1) When I ran tensorflow resp, it occurs OOM in Tesla v100 32GB. If I modify mem_len and target_len from 256 to 32(the same to pytorch branch), it will run successfully. 2) When I ran pytorch resp, ppl will not be converged well after global_steps > warmup_steps. Experiment result log is following.         I think I only need to set batch_size smaller or mem_len smaller from 256 to 32 because of GPU memory limitation. How mem_len and target_len affects the experiment result? How 1-billion experiment can be converged? Would you please give me some advices?"
"Hi, thanks for the high-quality code and paper.  A hopefully quick question, in Section 3.2, you said ""faster evaluation""  I don't quite understand why the vanilla model is very slow, could you clarify a bit or give some concrete example?  Thanks!"
How to we extend the model for sentiment analysis task?
"Hi, thank you for the open source.    After I finished running run_wt103_base.sh with 4 GPUs, my final result looked like this:         Would you mind tell me which test ppl is the correct one? (The one reported in your paper&github) If 24.248 is the correct test ppl, I wonder why it cannot achieve 24.03 as you said in your github (since every random seed has been fixed).    By the way, will the result be the same if I use only 2 GPUs to train wt103 base ?  Thanks a lot!"
"Can anyone point out how I can get only the log probability (for actual decoding) from the Adaptive Softmax without getting the loss. The forward method in the class in this repo only gives the loss, but not the log probs.     Thanks"
"     Function signature is:  `def _update_mems(self, hids, mems, qlen, mlen):`    And the call is:  `new_mems = self._update_mems(hids, mems, mlen, qlen)`    `mlen` and `qlen` probably misordered in the function call?"
no decoder found in model.py in tf implementation.
"Thanks for the repo！  It seems you don't have validation step in training loop and I can only assume that you have tuned the `train_steps` so the models won't overfit when training on those corpora memtioned in your paper, right?    But what if I want to train on my own corpus? Do this mean I need to write validation code myself？    Burce"
"Hello,    I'm trying to train a XL-model on a sub-word corpus of the Finnish Language, im facing a issue that the loss just jumps to 8 after going down to 4, attaching is a log from the restart point of my previous attempt, it still jumps to 8.28(i let my previous attempt complete). i dont use adaptive as my vocab is just 34K words, my corpus has 20 million sentences, iam training on 2 tesla V100's 32gb each, please let me know if there is a fix , or some parameter issue(have based my parameters on lm1b_large reducing the layers due to memory issues)     "
@kimiyoung Can you explain the meaning and usage of the parameter `cutoffs` in the function `single_core_graph`? Can you provide some examples? Thanks
"Hi,  Is it specified somewhere in the paper that a major difference with BERT is that you're using position embeddings at every layer ? If I see it clear :)  Thank you"
"Greetings,     I deleted the old question on the input&vocabulary, I solved that on my own    The remaining doubt is: How to retrieve the output word predicted by the LM?    I can surmise that one has to extract the `softmax(logits)` output specified in `model.transformer` and then randomly sample a vocabulary index from the distribution, converting it to a word.    Is that correct? Are there any better ways?"
Have you done any inference time comparison with LSTM based LMs? Our experiments show vanilla transformer based LMs are 50-60% slower compared to LSTM based LMs. Any such comparison data for Transformer XL will really help. 
"Hey,  Thank you so much for the code. If it's not too much to ask, could you put in an example on how to train with my own data? I'd be very grateful."
Is there a pre-training model in Chinese
"There has been   suggesting that a human can reach a perplexity score of about 12 so a 21.8 on the BW Corpus is a huge step towards that goal.      I'm wondering if you've considered something like a   to try to get closer to this?  The basic idea is to limit the vocab to dictionary words and group others such as proper nouns, times, locations, etc.. under categories to represent their class of word.  The idea being that we expect a person to be able to predict the next token is going to be a time but not a specific time.      I've some done some   that suggest this improves perplexity scores but don't have the GPU power to see how close a model like TransformerXL can get us to human level performance.  "
The shape of data in this initialization doesn't match comments or later reshape            should it be instead this?       
I initially want to train T-xl at the character level on my own database a la text8 where individual words in the dataset are unrelated and separated by a special character     Later i want to be able to train this on another dataset where the words are related and separated by another special character (I don't have this dataset yet).    How can I configure the initial training such that this would be workable? Would adding ` ` in the tokenizer when i hit a separator character be ok?
"In pytorch implementation there is a mix between para_model and model.  Should not you use only para_model?    For example in training function line number 422 you used ""model.zero_grad()"" but afterward you used in line number 436 ""ret = para_model(data_i, target_i, *mems[i])"".    Should not the whole program use para_model ??  "
We are interested in your experiment. We used the scripts provided to verify the your experiments on different corpora. We got the SoTA results in the paper. But We didn't find the script for the experiment on the word-level PTB corpus. We don't know how I can verify your experiment on the word-level PTB corpus. Please provide the scripts or the methods. Thanks.
"hello    could someone give me an advice to understand the class AdaptiveEmbedding. In particular such variables as div_val, d_proj    thanks!"
"I want to use it to calculate ppl for sentences of different lengths to check the fluent of sentence in some Specific application scenarios. Howerer i can't set the input for seq_len,  e.g: if seq_len max_len cut of it. And i suspect that transformerXL training segment by segment limit the calculate variance sentences ppl. Please provide some solutions, Thanks!"
"Hi , I am trying to use --attn_type 2 with enwik8 dataset. Here is the exact command I am using : python train.py         --cuda         --data ../data/enwik8/         --dataset enwik8         --n_layer 12         --d_model 512         --n_head 8         --d_head 64         --d_inner 2048         --dropout 0.1         --dropatt 0.0         --optim adam         --lr 0.00025         --warmup_step 0         --max_step 400000         --tgt_len 512         --mem_len 512         --eval_tgt_len 128  --batch_size 12 --attn_type 2    It throws the following error:   Traceback (most recent call last):    File ""train.py"", line 539, in        train()    File ""train.py"", line 445, in train      ret = para_model(data, target, *mems)    File ""/network/home/vermavik/miniconda2/envs/al/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__      result = self.forward(*input, **kwargs)    File ""/network/home/vermavik/github/transformer-xl/pytorch/mem_transformer.py"", line 745, in forward      hidden, new_mems = self._forward(data, mems=mems)    File ""/network/home/vermavik/github/transformer-xl/pytorch/mem_transformer.py"", line 706, in _forward      mems_i += pos_emb[:mlen]  RuntimeError: The size of tensor a (0) must match the size of tensor b (512) at non-singleton dimension 2"
"I am trying training with a fixed vocab(10k bpe symbols). I tried with auto-generated bpe vocab as well. The model doesn't converge. Are there any other considerations to be taken care of?  Initially there was an issue with cutoffs, I made cutoffs=[], still facing the issue with model convergence."
Can we extend this model to do some kind of summarization. Can we have decoders so that we can train it similar to a seq2seq method for a summarization task.   Maybe use a pretrained model and fine tune it with added decoders?
Replacing DataParallel -> DistributedDataParallel makes step time go from from 350ms to 260ms on p3.8xlarge machine for the `run_wt103_base.sh` example ( )
"When I look at the code of Transformer-XL for TensorFlow, in model.py, in function ""transformer"", the parameters contain ""dec_inp"", which seems to be the decoder input. I can't find the parameters for encoder in the code file. Is there an encoder process? If so, where is the encoder process?"
"When I try to train the model using train_gpu.py under folder tf (using TensorFlow), the result shows out of memory. Even when I add the statement ""config.gpu_options.allow_growth = True"", the out-of-memory issue still occurs.  I wonder why this issue happens, and is there a good way to solve this?"
"I have 6 titan GPUs machine with 12 GB memory, I changed the code to add my own dataset.  However, I always get cuda out of memory:       It doesn't matter whatever, I reduced the model size or the target length, or even add batch chunk.  Here is my bash file:       It seems the script wants to load the whole data file into the GPU memory at once."
absl.flags._exceptions.UnparsedFlagAccessError: Trying to access flag --data_dir before flags were parsed.    when running   sudo python data_utils.py     --data_dir=${DATA_DIR}/     --dataset=wt103     --tgt_len=${TEST_TGT_LEN}     --per_host_test_bsz=${TEST_BSZ}     --num_passes=1     --use_tpu=False
"Hello,    I have few questions regarding adding and training new corpus:    **_Adding New Corpus:_**    1. Function ""vocab.encode_file"" second parameter is ""ordered"", what is the purpose of this parameter and when we should set it to false or true.    2. Function ""vocab.encode_file"" third and fourth parameters are ""add_eos"" and ""add_double_eos"", when we should use them ? for example in character datasets you didn't add any of them while in the ""ptb"" you add a single eos, and in in the ""lm1b"" you added two.  Does that mean the character level datasets doesn't need eos or it is already included in the dataset, while it is not included in the ""ptb"" dataset ? But then why did you add two eos in ""lm1b"" dataset.     **_Training New Corpus:_**    1. Is it better to only use the ""adaptive"" parameter for adaptive softmax for word level only, but not for character level?    2. What is the difference between ""tgt_len"" ""mem_len"" and ""eval_tgt_len"" ?  As far as I understood ""tgt_len"" is the maximum length for the vanilla transformer, ""men_len"" is the length of previous tokens to look into, and the ""eval_tgt_len"" is the extended length during evaluation, correct ?"
"Hello, very nice work, and thank you for sharing the source.    I was looking at the PyTorch implementation and I was wondering how you are able to make the multi-GPU work with sparse updates, especially when fp16 is activated. Because none of the sparse/fp16 or sparse/distributed are currently implemented in PyTorch. My feeling is that in the current code, you have an optimizer that synchronizes the parameters across GPUs as expected, but the sparse updates are never synchronized which should result in slightly different models in each process. Or maybe I am missing something?    Thank you"
"On the invocation of _update_mems(self, hids, mems, qlen, mlen), swap of parameters seems a typo?        "
"So after I have trained this model and replicated it, How could i go about getting the sentence level log probability given a sentence?"
Seems VLIDA_BSZ should be replaced with VALID_BSZ in a couple places. For example:     
"I noticed that there are two flags in the pytorch `train.py` script (`--finetune_v2` and `--finetune_v3`) that don't seem to be used in any of the code. These flags suggest that there might be something special I need to do for finetuning Transformer XL. Might I be missing something?    Currently, I am running finetuning experiments simply by specifying `--restart` as well as `--restart_dir` and changing the dataset"
None
"with config: python train.py --cuda --data ../data/one-billion-words/ --dataset lm1b --adaptive --n_layer 18 --d_model 1024 --div_val 4 --n_head 8 --d_head 128 --d_inner 4096 --dropout 0.0 --dropatt 0.0 --optim adam --log-interval 5 --eval-interval 20 --warmup_step 20000 --max_step 500000 --lr 0.00025 --tgt_len 32 --mem_len 32 --eval_tgt_len 32 --batch_size 240 --batch_chunk 8 --work_dir exps, so run on only one GPU, do you think possible to achieve similar results as in the paper?"
"Hi,  Does anyone know about the function of parameters 'bin_sizes' and 'cutoffs' used for lm1b model?  Thanks for your help"
None
"Can you include a simple script for generating text with a pretrained Transformer-XL language model?  We are primarily using the PyTorch codebase but I am sure Tensorflow users would also appreciate this example.    If including this script is outside the scope of the project repository, could an informal example be provided in this issue thread?"
RuntimeError: CUDA out of memory.     My GPU is 11441MiB.    How to reproduce 128M-model?    Thank you  @kimiyoung @zihangdai 
"when I read your tf code,i am really confused about the codes below?        rw_head_q = w_head_q + r_w_bias      rr_head_q = w_head_q + r_r_bias            AC = tf.einsum('ibnd,jbnd->ijbn', rw_head_q, w_head_k)      BD = tf.einsum('ibnd,jnd->ijbn', rr_head_q, r_head_k)      BD = rel_shift(BD)    could you tell me what are the variables r_w_bias and r_r_bias mean in your paper?  and could you explain about this code shortly?  Thanks for your effort"
Do you have any plans to release the trained models listed in the paper?
1. python data_utils.py receive `--dataset=enwik8` but dataset name - `lm1b`     2 If I change `dataset` parameter to `lm1b` and run `sh sota/lm1b.sh` I have this error:     
"Hello!    Could you, please, provide hyperparameters for training models with close to SOTA perplexity on PTB and WT2 (if you experimented with the latter, as it has the corresponding choice in data utils)? Am I right that two changes I need to make to the released code is to add variational dropout and ASGD optimizer? If you have a code which produces necessary changes, it would be great.    Thanks"
"Hi, thanks for your excellent work. Transformer-xl is the most elegant model for long sequences by now. Do you plan to finetune pretrained models for document classification just like Bert?"
"Thank you for your transformer code!  When I ran the code, I encountered such issue:    Traceback (most recent call last):    File ""train.py"", line 539, in        train()    File ""train.py"", line 451, in train      loss.backward()    File ""/data1/baiye/miniconda3/envs/torch04py3/lib/python3.6/site-packages/torch/tensor.py"", line 93, in backward      torch.autograd.backward(self, gradient, retain_graph, create_graph)    File ""/data1/baiye/miniconda3/envs/torch04py3/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 89, in backward      allow_unreachable=True)  # allow_unreachable flag  RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation    My enviroment is pytorch 0.4. And I checked the code and did not find any inplace operation.  "
"In the past couple of months I've been trying to get a vanilla Transformer to be able to do text generation, more specifically to generate a long sentence starting from a small prime sentence.      This has thus far failed, and in multiple discussions it was pointed out that a Transformer only works on fixed-length context, while text generation is an incomplete-sentence task, therefor it is not fixed-length.       In your educated guess, is there merit in trying to fit this new architecture to the task of text generation now that the fixed-length problem has been solved?"
"Hi,  I observed values to be slighty different when evaluating the perplexity of set of sentences with `batch_size = 1` vs looping through the sentences one by one. (all other parameters being same)  `difference in loss is 0.7707 vs 0.7564 in other`    I created the data iterator using `dataset=""lm1b""`   Note: I modified the `corpus.vocab.encode_file` to encode the input sentence instead of reading from file  Any particular reason why this is observed."
"Hi, I'm getting NAN values in the first forward pass of the model (in the first layer), generally caused by the first AC calculation.  I'm wondering if this is an issue with the initial weights of the model?  If so, any advice to help with this issue?  I have made some changes to the model and this will help me determine if this is a known issue or if I have introduced a bug.  Thanks.  "
First of all thanks for providing the training scripts.    Is there any script available to do the fine tuning for sentence classification?
None
None
"Hello,     Thanks for the pytorch version of transformer-XL. I trained a model on my own corpus and it ran smoothly but I can't seem to load the model back from the checkpoint. I tried loading it the same way as in the `eval.py` script. Printing the model gives an attribute error.     "
"Hello!    Can you, please, provide the bash script for training Transformer-XL on PTB dataset with PyTorch?    Thanks!"
"I want to train a model with attn_type=2, and here is my configure.  Experiment dir : wt103_workdir/-wt103/20190121-201645  Loading cached dataset...  ====================================================================================================      - data : ../data/wikitext-103/      - dataset : wt103      - n_layer : 16      - n_head : 10      - d_head : 41      - d_embed : 410      - d_model : 410      - d_inner : 2100      - dropout : 0.1      - dropatt : 0.0      - init : normal      - emb_init : normal      - init_range : 0.1      - emb_init_range : 0.01      - init_std : 0.02      - proj_init_std : 0.01      - optim : adam      - lr : 0.00025      - mom : 0.0      - scheduler : cosine      - warmup_step : 0      - decay_rate : 0.5      - lr_min : 0.0      - clip : 0.25      - clip_nonemb : False      - max_step : 200000      - batch_size : 60      - batch_chunk : 1      - tgt_len : 150      - eval_tgt_len : 150      - ext_len : 0      - mem_len : 0      - not_tied : False      - seed : 1111      - cuda : True      - adaptive : True      - div_val : 1      - pre_lnorm : False      - varlen : False      - multi_gpu : True      - log_interval : 200      - eval_interval : 4000      - work_dir : wt103_workdir/-wt103/20190121-201645      - restart : False      - restart_dir :      - debug : False      - same_length : False      - attn_type : 2      - clamp_len : -1      - eta_min : 0.0      - gpu0_bsz : 4      - max_eval_steps : -1      - sample_softmax : -1      - patience : 0      - finetune_v2 : False      - finetune_v3 : False      - fp16 : False      - static_loss_scale : 1      - dynamic_loss_scale : False      - tied : True      - n_token : 267735      - n_all_param : 148417118      - n_nonemb_param : 38376800  But it seems to diverge. Can anyone give me some advice? Thanks very much!  | epoch   1 step      200 |    200 batches | lr 0.00025 | ms/batch 702.80 | loss  7.64 | ppl  2088.928  | epoch   1 step      400 |    400 batches | lr 0.00025 | ms/batch 621.46 | loss  7.46 | ppl  1730.912  | epoch   1 step      600 |    600 batches | lr 0.00025 | ms/batch 621.39 | loss  7.45 | ppl  1728.002  | epoch   1 step      800 |    800 batches | lr 0.00025 | ms/batch 621.19 | loss  7.45 | ppl  1717.449  | epoch   1 step     1000 |   1000 batches | lr 0.00025 | ms/batch 621.40 | loss  7.45 | ppl  1720.351  | epoch   1 step     1200 |   1200 batches | lr 0.00025 | ms/batch 620.97 | loss  7.44 | ppl  1700.753  | epoch   1 step     1400 |   1400 batches | lr 0.00025 | ms/batch 620.69 | loss  7.44 | ppl  1695.095  | epoch   1 step     1600 |   1600 batches | lr 0.00025 | ms/batch 635.92 | loss  7.44 | ppl  1711.255  | epoch   1 step     1800 |   1800 batches | lr 0.00025 | ms/batch 620.47 | loss  7.44 | ppl  1710.397  | epoch   1 step     2000 |   2000 batches | lr 0.00025 | ms/batch 619.97 | loss  7.44 | ppl  1695.020  | epoch   1 step     2200 |   2200 batches | lr 0.00025 | ms/batch 620.45 | loss  7.43 | ppl  1690.592  | epoch   1 step     2400 |   2400 batches | lr 0.00025 | ms/batch 620.02 | loss  7.44 | ppl  1702.485  | epoch   1 step     2600 |   2600 batches | lr 0.00025 | ms/batch 620.64 | loss  7.43 | ppl  1689.785  | epoch   1 step     2800 |   2800 batches | lr 0.00025 | ms/batch 619.92 | loss  7.43 | ppl  1693.790  | epoch   1 step     3000 |   3000 batches | lr 0.00025 | ms/batch 620.23 | loss  7.43 | ppl  1684.638  | epoch   1 step     3200 |   3200 batches | lr 0.00025 | ms/batch 619.65 | loss  7.42 | ppl  1666.079  | epoch   1 step     3400 |   3400 batches | lr 0.00025 | ms/batch 619.42 | loss  7.41 | ppl  1659.356  | epoch   1 step     3600 |   3600 batches | lr 0.00025 | ms/batch 619.80 | loss  7.41 | ppl  1649.053  | epoch   1 step     3800 |   3800 batches | lr 0.00025 | ms/batch 620.29 | loss  7.45 | ppl  1711.666  | epoch   1 step     4000 |   4000 batches | lr 0.00025 | ms/batch 620.61 | loss  7.42 | ppl  1661.076  ----------------------------------------------------------------------------------------------------  | Eval   1 at step     4000 | time: 2506.77s | valid loss  7.41 | valid ppl  1654.046  ----------------------------------------------------------------------------------------------------  | epoch   1 step     4200 |   4200 batches | lr 0.00025 | ms/batch 662.27 | loss  7.43 | ppl  1682.745  | epoch   1 step     4400 |   4400 batches | lr 0.00025 | ms/batch 619.80 | loss  7.42 | ppl  1672.333  | epoch   1 step     4600 |   4600 batches | lr 0.00025 | ms/batch 619.94 | loss  7.42 | ppl  1668.553  | epoch   1 step     4800 |   4800 batches | lr 0.00025 | ms/batch 619.89 | loss  7.42 | ppl  1669.587  | epoch   1 step     5000 |   5000 batches | lr 0.00025 | ms/batch 620.04 | loss  7.44 | ppl  1705.483  | epoch   1 step     5200 |   5200 batches | lr 0.00025 | ms/batch 619.59 | loss  7.44 | ppl  1707.168  | epoch   1 step     5400 |   5400 batches | lr 0.00025 | ms/batch 619.41 | loss  7.41 | ppl  1656.997  | epoch   1 step     5600 |   5600 batches | lr 0.00025 | ms/batch 619.81 | loss  7.43 | ppl  1682.111  | epoch   1 step     5800 |   5800 batches | lr 0.000249 | ms/batch 620.20 | loss  7.44 | ppl  1695.797  | epoch   1 step     6000 |   6000 batches | lr 0.000249 | ms/batch 619.68 | loss  7.43 | ppl  1691.197  | epoch   1 step     6200 |   6200 batches | lr 0.000249 | ms/batch 619.43 | loss  7.41 | ppl  1654.504  | epoch   1 step     6400 |   6400 batches | lr 0.000249 | ms/batch 620.16 | loss  7.43 | ppl  1688.890  | epoch   1 step     6600 |   6600 batches | lr 0.000249 | ms/batch 620.38 | loss  7.43 | ppl  1678.386  | epoch   1 step     6800 |   6800 batches | lr 0.000249 | ms/batch 619.64 | loss  7.42 | ppl  1664.799  | epoch   1 step     7000 |   7000 batches | lr 0.000249 | ms/batch 619.72 | loss  7.42 | ppl  1670.465  | epoch   1 step     7200 |   7200 batches | lr 0.000249 | ms/batch 620.02 | loss  7.42 | ppl  1667.845  | epoch   1 step     7400 |   7400 batches | lr 0.000249 | ms/batch 619.56 | loss  7.42 | ppl  1670.860  | epoch   1 step     7600 |   7600 batches | lr 0.000249 | ms/batch 620.25 | loss  7.42 | ppl  1664.089  | epoch   1 step     7800 |   7800 batches | lr 0.000249 | ms/batch 619.83 | loss  7.42 | ppl  1661.560  | epoch   1 step     8000 |   8000 batches | lr 0.000249 | ms/batch 619.85 | loss  7.43 | ppl  1689.380  ----------------------------------------------------------------------------------------------------  | Eval   2 at step     8000 | time: 2484.77s | valid loss  7.39 | valid ppl  1616.071  ----------------------------------------------------------------------------------------------------  | epoch   1 step     8200 |   8200 batches | lr 0.000249 | ms/batch 672.76 | loss  7.43 | ppl  1680.628  | epoch   1 step     8400 |   8400 batches | lr 0.000249 | ms/batch 620.21 | loss  7.43 | ppl  1685.528  | epoch   1 step     8600 |   8600 batches | lr 0.000249 | ms/batch 619.91 | loss  7.43 | ppl  1684.851  | epoch   1 step     8800 |   8800 batches | lr 0.000249 | ms/batch 620.02 | loss  7.44 | ppl  1699.004  | epoch   1 step     9000 |   9000 batches | lr 0.000249 | ms/batch 619.53 | loss  7.42 | ppl  1667.265  | epoch   1 step     9200 |   9200 batches | lr 0.000249 | ms/batch 620.79 | loss  7.43 | ppl  1684.868  | epoch   1 step     9400 |   9400 batches | lr 0.000249 | ms/batch 620.06 | loss  7.42 | ppl  1672.693  | epoch   1 step     9600 |   9600 batches | lr 0.000249 | ms/batch 619.62 | loss  7.43 | ppl  1689.861  | epoch   1 step     9800 |   9800 batches | lr 0.000249 | ms/batch 619.44 | loss  7.41 | ppl  1652.922  | epoch   1 step    10000 |  10000 batches | lr 0.000248 | ms/batch 620.06 | loss  7.43 | ppl  1692.675  | epoch   1 step    10200 |  10200 batches | lr 0.000248 | ms/batch 619.46 | loss  7.41 | ppl  1653.468  | epoch   1 step    10400 |  10400 batches | lr 0.000248 | ms/batch 619.62 | loss  7.41 | ppl  1651.442  | epoch   1 step    10600 |  10600 batches | lr 0.000248 | ms/batch 620.05 | loss  7.41 | ppl  1652.406  | epoch   1 step    10800 |  10800 batches | lr 0.000248 | ms/batch 619.74 | loss  7.41 | ppl  1658.664  | epoch   1 step    11000 |  11000 batches | lr 0.000248 | ms/batch 619.59 | loss  7.44 | ppl  1694.259  | epoch   1 step    11200 |  11200 batches | lr 0.000248 | ms/batch 619.55 | loss  7.42 | ppl  1672.915  | epoch   1 step    11400 |  11400 batches | lr 0.000248 | ms/batch 619.08 | loss  7.42 | ppl  1664.737  | epoch   2 step    11600 |    130 batches | lr 0.000248 | ms/batch 620.83 | loss  7.38 | ppl  1601.478  | epoch   2 step    11800 |    330 batches | lr 0.000248 | ms/batch 621.36 | loss  7.31 | ppl  1490.017  | epoch   2 step    12000 |    530 batches | lr 0.000248 | ms/batch 621.48 | loss  7.33 | ppl  1523.110  ----------------------------------------------------------------------------------------------------  | Eval   3 at step    12000 | time: 2485.37s | valid loss  7.42 | valid ppl  1674.380  ----------------------------------------------------------------------------------------------------  | epoch   2 step    12200 |    730 batches | lr 0.000248 | ms/batch 648.06 | loss  7.31 | ppl  1498.382  | epoch   2 step    12400 |    930 batches | lr 0.000248 | ms/batch 621.16 | loss  7.33 | ppl  1527.507  | epoch   2 step    12600 |   1130 batches | lr 0.000248 | ms/batch 621.00 | loss  7.33 | ppl  1530.506  | epoch   2 step    12800 |   1330 batches | lr 0.000247 | ms/batch 620.95 | loss  7.33 | ppl  1525.309  | epoch   2 step    13000 |   1530 batches | lr 0.000247 | ms/batch 621.32 | loss  7.33 | ppl  1527.929  | epoch   2 step    13200 |   1730 batches | lr 0.000247 | ms/batch 621.35 | loss  7.34 | ppl  1543.376  | epoch   2 step    13400 |   1930 batches | lr 0.000247 | ms/batch 621.04 | loss  7.33 | ppl  1523.908  | epoch   2 step    13600 |   2130 batches | lr 0.000247 | ms/batch 621.29 | loss  7.34 | ppl  1546.512  | epoch   2 step    13800 |   2330 batches | lr 0.000247 | ms/batch 620.99 | loss  7.34 | ppl  1545.263  | epoch   2 step    14000 |   2530 batches | lr 0.000247 | ms/batch 621.08 | loss  7.34 | ppl  1540.291  | epoch   2 step    14200 |   2730 batches | lr 0.000247 | ms/batch 620.93 | loss  7.34 | ppl  1540.285  | epoch   2 step    14400 |   2930 batches | lr 0.000247 | ms/batch 621.68 | loss  7.34 | ppl  1540.759  | epoch   2 step    14600 |   3130 batches | lr 0.000247 | ms/batch 621.22 | loss  7.32 | ppl  1512.795  | epoch   2 step    14800 |   3330 batches | lr 0.000247 | ms/batch 621.04 | loss  7.32 | ppl  1506.678  | epoch   2 step    15000 |   3530 batches | lr 0.000247 | ms/batch 621.31 | loss  7.33 | ppl  1530.028  | epoch   2 step    15200 |   3730 batches | lr 0.000246 | ms/batch 621.44 | loss  7.34 | ppl  1537.768  | epoch   2 step    15400 |   3930 batches | lr 0.000246 | ms/batch 621.56 | loss  7.33 | ppl  1532.047  | epoch   2 step    15600 |   4130 batches | lr 0.000246 | ms/batch 622.21 | loss  7.34 | ppl  1535.568  | epoch   2 step    15800 |   4330 batches | lr 0.000246 | ms/batch 621.75 | loss  7.34 | ppl  1537.776  | epoch   2 step    16000 |   4530 batches | lr 0.000246 | ms/batch 621.52 | loss  7.33 | ppl  1524.707  ----------------------------------------------------------------------------------------------------  | Eval   4 at step    16000 | time: 2490.61s | valid loss  7.42 | valid ppl  1664.061  ----------------------------------------------------------------------------------------------------  | epoch   2 step    16200 |   4730 batches | lr 0.000246 | ms/batch 648.01 | loss  7.33 | ppl  1531.670  | epoch   2 step    16400 |   4930 batches | lr 0.000246 | ms/batch 621.81 | loss  7.35 | ppl  1561.311  | epoch   2 step    16600 |   5130 batches | lr 0.000246 | ms/batch 621.62 | loss  7.35 | ppl  1558.448  | epoch   2 step    16800 |   5330 batches | lr 0.000246 | ms/batch 621.31 | loss  7.34 | ppl  1544.213  | epoch   2 step    17000 |   5530 batches | lr 0.000246 | ms/batch 621.27 | loss  7.33 | ppl  1521.180  | epoch   2 step    17200 |   5730 batches | lr 0.000245 | ms/batch 621.11 | loss  7.36 | ppl  1577.214  | epoch   2 step    17400 |   5930 batches | lr 0.000245 | ms/batch 620.95 | loss  7.35 | ppl  1551.961  | epoch   2 step    17600 |   6130 batches | lr 0.000245 | ms/batch 620.91 | loss  7.34 | ppl  1546.448  | epoch   2 step    17800 |   6330 batches | lr 0.000245 | ms/batch 621.03 | loss  7.34 | ppl  1534.776  | epoch   2 step    18000 |   6530 batches | lr 0.000245 | ms/batch 621.68 | loss  7.36 | ppl  1571.506  | epoch   2 step    18200 |   6730 batches | lr 0.000245 | ms/batch 621.27 | loss  7.34 | ppl  1535.712  | epoch   2 step    18400 |   6930 batches | lr 0.000245 | ms/batch 621.65 | loss  7.34 | ppl  1538.308  | epoch   2 step    18600 |   7130 batches | lr 0.000245 | ms/batch 620.88 | loss  7.34 | ppl  1541.480  | epoch   2 step    18800 |   7330 batches | lr 0.000245 | ms/batch 621.06 | loss  7.34 | ppl  1539.062  | epoch   2 step    19000 |   7530 batches | lr 0.000244 | ms/batch 621.02 | loss  7.35 | ppl  1556.423  | epoch   2 step    19200 |   7730 batches | lr 0.000244 | ms/batch 621.01 | loss  7.33 | ppl  1530.237  | epoch   2 step    19400 |   7930 batches | lr 0.000244 | ms/batch 621.38 | loss  7.35 | ppl  1560.169  | epoch   2 step    19600 |   8130 batches | lr 0.000244 | ms/batch 621.06 | loss  7.34 | ppl  1543.635  | epoch   2 step    19800 |   8330 batches | lr 0.000244 | ms/batch 621.31 | loss  7.34 | ppl  1546.412  | epoch   2 step    20000 |   8530 batches | lr 0.000244 | ms/batch 620.97 | loss  7.36 | ppl  1573.621  ----------------------------------------------------------------------------------------------------  | Eval   5 at step    20000 | time: 2490.28s | valid loss  7.40 | valid ppl  1642.454  ----------------------------------------------------------------------------------------------------  | epoch   2 step    20200 |   8730 batches | lr 0.000244 | ms/batch 648.18 | loss  7.35 | ppl  1552.783  | epoch   2 step    20400 |   8930 batches | lr 0.000244 | ms/batch 621.19 | loss  7.35 | ppl  1561.782  | epoch   2 step    20600 |   9130 batches | lr 0.000244 | ms/batch 621.56 | loss  7.35 | ppl  1551.505  | epoch   2 step    20800 |   9330 batches | lr 0.000243 | ms/batch 621.11 | loss  7.35 | ppl  1550.757  | epoch   2 step    21000 |   9530 batches | lr 0.000243 | ms/batch 621.23 | loss  7.36 | ppl  1576.482  | epoch   2 step    21200 |   9730 batches | lr 0.000243 | ms/batch 621.01 | loss  7.34 | ppl  1534.469  | epoch   2 step    21400 |   9930 batches | lr 0.000243 | ms/batch 621.09 | loss  7.35 | ppl  1552.626  | epoch   2 step    21600 |  10130 batches | lr 0.000243 | ms/batch 621.28 | loss  7.35 | ppl  1550.348  | epoch   2 step    21800 |  10330 batches | lr 0.000243 | ms/batch 621.55 | loss  7.35 | ppl  1555.845  | epoch   2 step    22000 |  10530 batches | lr 0.000243 | ms/batch 620.96 | loss  7.34 | ppl  1533.085  | epoch   2 step    22200 |  10730 batches | lr 0.000242 | ms/batch 620.96 | loss  7.35 | ppl  1556.160  | epoch   2 step    22400 |  10930 batches | lr 0.000242 | ms/batch 621.35 | loss  7.35 | ppl  1562.793  | epoch   2 step    22600 |  11130 batches | lr 0.000242 | ms/batch 620.82 | loss  7.35 | ppl  1563.720  | epoch   2 step    22800 |  11330 batches | lr 0.000242 | ms/batch 621.20 | loss  7.36 | ppl  1566.230  | epoch   3 step    23000 |     60 batches | lr 0.000242 | ms/batch 620.74 | loss  7.34 | ppl  1541.840  | epoch   3 step    23200 |    260 batches | lr 0.000242 | ms/batch 621.55 | loss  7.28 | ppl  1453.898  | epoch   3 step    23400 |    460 batches | lr 0.000242 | ms/batch 622.00 | loss  7.30 | ppl  1479.356  | epoch   3 step    23600 |    660 batches | lr 0.000242 | ms/batch 621.62 | loss  7.29 | ppl  1458.763  | epoch   3 step    23800 |    860 batches | lr 0.000241 | ms/batch 621.74 | loss  7.31 | ppl  1488.324  | epoch   3 step    24000 |   1060 batches | lr 0.000241 | ms/batch 622.03 | loss  7.30 | ppl  1476.624  ----------------------------------------------------------------------------------------------------  | Eval   6 at step    24000 | time: 2490.67s | valid loss  7.44 | valid ppl  1703.611  ----------------------------------------------------------------------------------------------------  | epoch   3 step    24200 |   1260 batches | lr 0.000241 | ms/batch 648.34 | loss  7.30 | ppl  1478.781  | epoch   3 step    24400 |   1460 batches | lr 0.000241 | ms/batch 621.87 | loss  7.30 | ppl  1476.575  | epoch   3 step    24600 |   1660 batches | lr 0.000241 | ms/batch 621.75 | loss  7.31 | ppl  1499.300  | epoch   3 step    24800 |   1860 batches | lr 0.000241 | ms/batch 621.83 | loss  7.30 | ppl  1477.016  | epoch   3 step    25000 |   2060 batches | lr 0.00024 | ms/batch 622.08 | loss  7.31 | ppl  1500.889  | epoch   3 step    25200 |   2260 batches | lr 0.00024 | ms/batch 621.62 | loss  7.31 | ppl  1495.962  | epoch   3 step    25400 |   2460 batches | lr 0.00024 | ms/batch 621.89 | loss  7.31 | ppl  1492.161  | epoch   3 step    25600 |   2660 batches | lr 0.00024 | ms/batch 621.87 | loss  7.31 | ppl  1492.371  | epoch   3 step    25800 |   2860 batches | lr 0.00024 | ms/batch 621.40 | loss  7.31 | ppl  1491.645  | epoch   3 step    26000 |   3060 batches | lr 0.00024 | ms/batch 621.84 | loss  7.30 | ppl  1484.346  | epoch   3 step    26200 |   3260 batches | lr 0.00024 | ms/batch 621.87 | loss  7.29 | ppl  1466.224  | epoch   3 step    26400 |   3460 batches | lr 0.000239 | ms/batch 621.72 | loss  7.29 | ppl  1471.563  | epoch   3 step    26600 |   3660 batches | lr 0.000239 | ms/batch 621.46 | loss  7.30 | ppl  1484.499  | epoch   3 step    26800 |   3860 batches | lr 0.000239 | ms/batch 621.93 | loss  7.31 | ppl  1492.562  | epoch   3 step    27000 |   4060 batches | lr 0.000239 | ms/batch 621.78 | loss  7.30 | ppl  1474.048  | epoch   3 step    27200 |   4260 batches | lr 0.000239 | ms/batch 621.84 | loss  7.31 | ppl  1498.078  | epoch   3 step    27400 |   4460 batches | lr 0.000239 | ms/batch 621.81 | loss  7.30 | ppl  1477.633  | epoch   3 step    27600 |   4660 batches | lr 0.000238 | ms/batch 621.69 | loss  7.31 | ppl  1491.983  | epoch   3 step    27800 |   4860 batches | lr 0.000238 | ms/batch 621.52 | loss  7.32 | ppl  1507.707  | epoch   3 step    28000 |   5060 batches | lr 0.000238 | ms/batch 621.40 | loss  7.32 | ppl  1507.557  ----------------------------------------------------------------------------------------------------  | Eval   7 at step    28000 | time: 2492.27s | valid loss  7.44 | valid ppl  1706.144  ----------------------------------------------------------------------------------------------------  | epoch   3 step    28200 |   5260 batches | lr 0.000238 | ms/batch 648.62 | loss  7.32 | ppl  1502.753  | epoch   3 step    28400 |   5460 batches | lr 0.000238 | ms/batch 621.67 | loss  7.31 | ppl  1487.829  | epoch   3 step    28600 |   5660 batches | lr 0.000238 | ms/batch 621.82 | loss  7.33 | ppl  1518.292  | epoch   3 step    28800 |   5860 batches | lr 0.000237 | ms/batch 621.37 | loss  7.32 | ppl  1510.123  | epoch   3 step    29000 |   6060 batches | lr 0.000237 | ms/batch 621.82 | loss  7.31 | ppl  1501.527  | epoch   3 step    29200 |   6260 batches | lr 0.000237 | ms/batch 621.80 | loss  7.31 | ppl  1488.194  | epoch   3 step    29400 |   6460 batches | lr 0.000237 | ms/batch 621.68 | loss  7.32 | ppl  1513.990  | epoch   3 step    29600 |   6660 batches | lr 0.000237 | ms/batch 621.60 | loss  7.32 | ppl  1503.472  | epoch   3 step    29800 |   6860 batches | lr 0.000237 | ms/batch 621.45 | loss  7.31 | ppl  1491.761  | epoch   3 step    30000 |   7060 batches | lr 0.000236 | ms/batch 621.66 | loss  7.31 | ppl  1500.846  | epoch   3 step    30200 |   7260 batches | lr 0.000236 | ms/batch 621.62 | loss  7.31 | ppl  1494.195  | epoch   3 step    30400 |   7460 batches | lr 0.000236 | ms/batch 621.88 | loss  7.31 | ppl  1501.704  | epoch   3 step    30600 |   7660 batches | lr 0.000236 | ms/batch 621.50 | loss  7.31 | ppl  1493.181  | epoch   3 step    30800 |   7860 batches | lr 0.000236 | ms/batch 622.06 | loss  7.31 | ppl  1493.227  | epoch   3 step    31000 |   8060 batches | lr 0.000235 | ms/batch 621.64 | loss  7.31 | ppl  1501.180  | epoch   3 step    31200 |   8260 batches | lr 0.000235 | ms/batch 621.87 | loss  7.31 | ppl  1501.493  | epoch   3 step    31400 |   8460 batches | lr 0.000235 | ms/batch 621.95 | loss  7.33 | ppl  1518.169  | epoch   3 step    31600 |   8660 batches | lr 0.000235 | ms/batch 621.71 | loss  7.31 | ppl  1502.388  | epoch   3 step    31800 |   8860 batches | lr 0.000235 | ms/batch 621.56 | loss  7.32 | ppl  1511.796  | epoch   3 step    32000 |   9060 batches | lr 0.000235 | ms/batch 621.97 | loss  7.31 | ppl  1500.025  ----------------------------------------------------------------------------------------------------  | Eval   8 at step    32000 | time: 2492.28s | valid loss  7.44 | valid ppl  1708.262  ----------------------------------------------------------------------------------------------------  | epoch   3 step    32200 |   9260 batches | lr 0.000234 | ms/batch 648.48 | loss  7.32 | ppl  1502.871  | epoch   3 step    32400 |   9460 batches | lr 0.000234 | ms/batch 621.60 | loss  7.33 | ppl  1527.820  | epoch   3 step    32600 |   9660 batches | lr 0.000234 | ms/batch 621.64 | loss  7.31 | ppl  1492.826  | epoch   3 step    32800 |   9860 batches | lr 0.000234 | ms/batch 621.63 | loss  7.31 | ppl  1495.967  | epoch   3 step    33000 |  10060 batches | lr 0.000234 | ms/batch 622.10 | loss  7.33 | ppl  1524.507  | epoch   3 step    33200 |  10260 batches | lr 0.000233 | ms/batch 621.53 | loss  7.30 | ppl  1485.690  | epoch   3 step    33400 |  10460 batches | lr 0.000233 | ms/batch 621.17 | loss  7.31 | ppl  1492.452  | epoch   3 step    33600 |  10660 batches | lr 0.000233 | ms/batch 621.04 | loss  7.31 | ppl  1498.407  | epoch   3 step    33800 |  10860 batches | lr 0.000233 | ms/batch 621.77 | loss  7.32 | ppl  1512.605  | epoch   3 step    34000 |  11060 batches | lr 0.000233 | ms/batch 621.23 | loss  7.32 | ppl  1504.775  | epoch   3 step    34200 |  11260 batches | lr 0.000232 | ms/batch 621.52 | loss  7.33 | ppl  1523.020  | epoch   3 step    34400 |  11460 batches | lr 0.000232 | ms/batch 620.80 | loss  7.31 | ppl  1491.825  | epoch   4 step    34600 |    190 batches | lr 0.000232 | ms/batch 621.64 | loss  7.29 | ppl  1465.376  | epoch   4 step    34800 |    390 batches | lr 0.000232 | ms/batch 621.66 | loss  7.28 | ppl  1450.339  | epoch   4 step    35000 |    590 batches | lr 0.000232 | ms/batch 621.68 | loss  7.28 | ppl  1457.213  | epoch   4 step    35200 |    790 batches | lr 0.000231 | ms/batch 621.64 | loss  7.28 | ppl  1456.289  | epoch   4 step    35400 |    990 batches | lr 0.000231 | ms/batch 621.70 | loss  7.29 | ppl  1464.555  | epoch   4 step    35600 |   1190 batches | lr 0.000231 | ms/batch 621.51 | loss  7.28 | ppl  1455.969  | epoch   4 step    35800 |   1390 batches | lr 0.000231 | ms/batch 621.76 | loss  7.29 | ppl  1460.029  | epoch   4 step    36000 |   1590 batches | lr 0.000231 | ms/batch 622.02 | loss  7.29 | ppl  1469.949  ----------------------------------------------------------------------------------------------------  | Eval   9 at step    36000 | time: 2491.61s | valid loss  7.45 | valid ppl  1727.050  ----------------------------------------------------------------------------------------------------  | epoch   4 step    36200 |   1790 batches | lr 0.00023 | ms/batch 648.55 | loss  7.29 | ppl  1465.296  | epoch   4 step    36400 |   1990 batches | lr 0.00023 | ms/batch 621.63 | loss  7.29 | ppl  1470.353  | epoch   4 step    36600 |   2190 batches | lr 0.00023 | ms/batch 621.60 | loss  7.29 | ppl  1466.556  | epoch   4 step    36800 |   2390 batches | lr 0.00023 | ms/batch 621.81 | loss  7.30 | ppl  1481.549  | epoch   4 step    37000 |   2590 batches | lr 0.000229 | ms/batch 621.74 | loss  7.29 | ppl  1459.671  | epoch   4 step    37200 |   2790 batches | lr 0.000229 | ms/batch 622.05 | loss  7.30 | ppl  1475.031  | epoch   4 step    37400 |   2990 batches | lr 0.000229 | ms/batch 621.83 | loss  7.29 | ppl  1471.230  | epoch   4 step    37600 |   3190 batches | lr 0.000229 | ms/batch 621.65 | loss  7.28 | ppl  1444.171  | epoch   4 step    37800 |   3390 batches | lr 0.000229 | ms/batch 621.73 | loss  7.27 | ppl  1439.950  | epoch   4 step    38000 |   3590 batches | lr 0.000228 | ms/batch 621.45 | loss  7.28 | ppl  1454.786  | epoch   4 step    38200 |   3790 batches | lr 0.000228 | ms/batch 622.01 | loss  7.30 | ppl  1479.319  | epoch   4 step    38400 |   3990 batches | lr 0.000228 | ms/batch 621.87 | loss  7.28 | ppl  1451.045  | epoch   4 step    38600 |   4190 batches | lr 0.000228 | ms/batch 622.00 | loss  7.30 | ppl  1475.214  | epoch   4 step    38800 |   4390 batches | lr 0.000227 | ms/batch 621.46 | loss  7.29 | ppl  1460.207  | epoch   4 step    39000 |   4590 batches | lr 0.000227 | ms/batch 621.24 | loss  7.29 | ppl  1462.538  | epoch   4 step    39200 |   4790 batches | lr 0.000227 | ms/batch 621.45 | loss  7.29 | ppl  1468.254  | epoch   4 step    39400 |   4990 batches | lr 0.000227 | ms/batch 621.85 | loss  7.31 | ppl  1493.102  | epoch   4 step    39600 |   5190 batches | lr 0.000227 | ms/batch 622.29 | loss  7.31 | ppl  1493.513  | epoch   4 step    39800 |   5390 batches | lr 0.000226 | ms/batch 621.24 | loss  7.29 | ppl  1459.525  | epoch   4 step    40000 |   5590 batches | lr 0.000226 | ms/batch 621.47 | loss  7.30 | ppl  1474.168  ----------------------------------------------------------------------------------------------------  | Eval  10 at step    40000 | time: 2492.18s | valid loss  7.46 | valid ppl  1730.644  ----------------------------------------------------------------------------------------------------  | epoch   4 step    40200 |   5790 batches | lr 0.000226 | ms/batch 648.23 | loss  7.31 | ppl  1495.160  | epoch   4 step    40400 |   5990 batches | lr 0.000226 | ms/batch 621.79 | loss  7.30 | ppl  1486.213  | epoch   4 step    40600 |   6190 batches | lr 0.000225 | ms/batch 621.62 | loss  7.29 | ppl  1462.185  | epoch   4 step    40800 |   6390 batches | lr 0.000225 | ms/batch 621.89 | loss  7.30 | ppl  1473.741  | epoch   4 step    41000 |   6590 batches | lr 0.000225 | ms/batch 621.52 | loss  7.31 | ppl  1488.068  | epoch   4 step    41200 |   6790 batches | lr 0.000225 | ms/batch 621.60 | loss  7.29 | ppl  1458.729  | epoch   4 step    41400 |   6990 batches | lr 0.000224 | ms/batch 621.76 | loss  7.30 | ppl  1480.750  | epoch   4 step    41600 |   7190 batches | lr 0.000224 | ms/batch 621.84 | loss  7.29 | ppl  1471.271  | epoch   4 step    41800 |   7390 batches | lr 0.000224 | ms/batch 621.72 | loss  7.30 | ppl  1479.743  | epoch   4 step    42000 |   7590 batches | lr 0.000224 | ms/batch 621.64 | loss  7.30 | ppl  1473.643  | epoch   4 step    42200 |   7790 batches | lr 0.000224 | ms/batch 621.37 | loss  7.28 | ppl  1447.959  | epoch   4 step    42400 |   7990 batches | lr 0.000223 | ms/batch 621.54 | loss  7.31 | ppl  1495.434  | epoch   4 step    42600 |   8190 batches | lr 0.000223 | ms/batch 621.55 | loss  7.29 | ppl  1462.527  | epoch   4 step    42800 |   8390 batches | lr 0.000223 | ms/batch 621.60 | loss  7.31 | ppl  1489.709  | epoch   4 step    43000 |   8590 batches | lr 0.000223 | ms/batch 621.65 | loss  7.30 | ppl  1484.012  | epoch   4 step    43200 |   8790 batches | lr 0.000222 | ms/batch 621.62 | loss  7.31 | ppl  1491.229  | epoch   4 step    43400 |   8990 batches | lr 0.000222 | ms/batch 621.14 | loss  7.30 | ppl  1476.093  | epoch   4 step    43600 |   9190 batches | lr 0.000222 | ms/batch 621.67 | loss  7.31 | ppl  1487.912  | epoch   4 step    43800 |   9390 batches | lr 0.000222 | ms/batch 621.81 | loss  7.30 | ppl  1474.515  | epoch   4 step    44000 |   9590 batches | lr 0.000221 | ms/batch 621.78 | loss  7.31 | ppl  1489.621  ----------------------------------------------------------------------------------------------------  | Eval  11 at step    44000 | time: 2491.86s | valid loss  7.44 | valid ppl  1706.983  ----------------------------------------------------------------------------------------------------  | epoch   4 step    44200 |   9790 batches | lr 0.000221 | ms/batch 648.36 | loss  7.29 | ppl  1464.462  | epoch   4 step    44400 |   9990 batches | lr 0.000221 | ms/batch 621.91 | loss  7.31 | ppl  1492.388  | epoch   4 step    44600 |  10190 batches | lr 0.000221 | ms/batch 621.52 | loss  7.29 | ppl  1465.266  | epoch   4 step    44800 |  10390 batches | lr 0.00022 | ms/batch 621.12 | loss  7.30 | ppl  1477.299  | epoch   4 step    45000 |  10590 batches | lr 0.00022 | ms/batch 620.76 | loss  7.29 | ppl  1464.611  | epoch   4 step    45200 |  10790 batches | lr 0.00022 | ms/batch 621.72 | loss  7.30 | ppl  1475.448  | epoch   4 step    45400 |  10990 batches | lr 0.00022 | ms/batch 621.22 | loss  7.31 | ppl  1493.140  | epoch   4 step    45600 |  11190 batches | lr 0.000219 | ms/batch 621.56 | loss  7.30 | ppl  1486.864  | epoch   4 step    45800 |  11390 batches | lr 0.000219 | ms/batch 621.13 | loss  7.29 | ppl  1470.398  | epoch   5 step    46000 |    120 batches | lr 0.000219 | ms/batch 621.16 | loss  7.29 | ppl  1462.640  | epoch   5 step    46200 |    320 batches | lr 0.000219 | ms/batch 622.12 | loss  7.27 | ppl  1429.701  | epoch   5 step    46400 |    520 batches | lr 0.000218 | ms/batch 622.40 | loss  7.28 | ppl  1455.915  | epoch   5 step    46600 |    720 batches | lr 0.000218 | ms/batch 621.56 | loss  7.27 | ppl  1429.803  | epoch   5 step    46800 |    920 batches | lr 0.000218 | ms/batch 621.79 | loss  7.28 | ppl  1447.379  | epoch   5 step    47000 |   1120 batches | lr 0.000217 | ms/batch 621.45 | loss  7.28 | ppl  1449.542  | epoch   5 step    47200 |   1320 batches | lr 0.000217 | ms/batch 621.70 | loss  7.28 | ppl  1444.158  | epoch   5 step    47400 |   1520 batches | lr 0.000217 | ms/batch 622.01 | loss  7.27 | ppl  1441.529  | epoch   5 step    47600 |   1720 batches | lr 0.000217 | ms/batch 621.49 | loss  7.28 | ppl  1456.560  | epoch   5 step    47800 |   1920 batches | lr 0.000216 | ms/batch 621.47 | loss  7.27 | ppl  1440.568  | epoch   5 step    48000 |   2120 batches | lr 0.000216 | ms/batch 621.64 | loss  7.29 | ppl  1461.721  ----------------------------------------------------------------------------------------------------  | Eval  12 at step    48000 | time: 2491.63s | valid loss  7.47 | valid ppl  1752.609  ----------------------------------------------------------------------------------------------------"
"Good works! I have two question about your tf codes.    The first:  In the paper, query vector is calculated using previous layer's hidden state rather than the concatenated pre-layer's memory and hidden state. However, in the tf code, I found the query vector is calculated the same as key vector and value vector.     !   !     The second:  Each layers has memory tensor with shape [mem_len, batch_size, d_model].  When calculating query, key and value vector, the input vector of `tf.layers.dens` is the concatenation of current layer's memory and pre-layers' output. which seems be conflict with the paper. Besides, why stop gradient in `_cache_mem` method rather than in `rel_multihead_attn`, the later seems to make better sense.      "
"Hi, there:  So nice that you release the original code. Maybe a little difficult for me to reproduce: (  After nearly 1.5 days for matching your paper and code, still... some questions about model structure, hope you could help, maybe some foolish ...    1. What's the difference between `RelLearnableMultiHeadAttn` and `RelPartialLearnableMultiHeadAttn` ?    Seem the most important part is the construction of embedding (A+B+C+D), but the first one doesn't use the position embedding in ""Attention is all you need""?    2. Can you explain the function `_rel_shift` in detail for me?    Especially the top -4 line code, I don't know why we need this?    3. What happens when the param `div_val > 1` and what's the meaning of the `cutoff_xxx`?   More specifically, I think what we need is the part of code when `div_val==1`.      Hope you could help me, thx.    "
"Thank you for such easy to read code & repo - can be seen that a lot of hard work has gone into it! Secondly, found your work from Sebastian Ruder NLP newsletter and as he put it as: ""Peer review is an imprecise process and gems may sometimes fall through the cracks."" Your work was under one of the gems and I totally agree!      Now specifically, I tried using wt103 in Tensor2Tensor and I'm getting an error of:         I suppose it comes from the wrong hparams I am using?            "
"Hi,    thanks for the releasing the TensorFlow and PyTorch code for your Transformer-XL :heart:     I would like to ask, if you plan to provide some pre-trained models for the PyTorch implementation? I was only able to find the TensorFlow checkpoints...    Thanks in advance,    Stefan"
How to obtain the data? Thanks!
"According to your paper, q is always multiplied with the previous output and not the concatenated version. This is implemented correctly in the DecoderLayer but not in the RelPartial- and RelDecoderLayer. Is this intended?    Thank you!"
"Hi, thank you for sharing your code here on github. I tried to understand you math in detail and stumbled over two versions of your decoder. What exactly is the difference between the partial and normal learnable decoder? I figured out what the difference in math is but what was the motivation behind these versions?    Thank you!"
"`    def _rel_shift(self, x):          x_size = shape_list(x)    # (q,k,bsz,n_head)            x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])          x = tf.reshape(x, [x_size[1] + 1, x_size[0], x_size[2], x_size[3]])          x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])          x = tf.reshape(x, x_size)            return x  `  try a simple test:   x = tf.reshape(tf.range(1,33), [4,8]), it looks like:         Feeding into function '_rel_shhift(..)', output is:  array([[ 4,  5,  6,  7,  8,  0,  9, 10],         [11, 12, 13, 14, 15, 16,  0, 17],         [18, 19, 20, 21, 22, 23, 24,  0],         [25, 26, 27, 28, 29, 30, 31, 32]], dtype=int32)>    I guess the first line of output should be: [ 4,  5,  6,  7,  8,  0,  0, 0], and 2nd is: [11, 12, 13, 14, 15, 16,  0, 0], and so on."
"The inputs of Decoderlayer is a set of batch data or a single data ?  So as the code in pytorch/mem_trandformer.py,  'qlen=h.size(0)'. But if the input is a batch, h.size(0) is not the qlen."
"Hi,  I notice that the training steps for the base_wt103 in PyTorch codes is 200K, while this number is 400K in the TF scripts. However, for the large wt103, both of them are 4M.     I am confused about the training steps as I am training the large PyTorch model with 16*32GB v100. The speed is too slow to finish the 4000000 steps(1300ms per step,2 months, is it right? ).    By the way, will the tf codes be faster than PyTorch in this project?    Thanks for your help!  "
"i use the pretrained-xl weights and same vocab to build transformer-xl large(we use tensorflow2.0) to eval the test set. But in my experiments, I find the {tgt_len=128, mem_len=1600, clamp_len=1000} just can reach test ppl around 35, and {tgt_len=384, mem_len=384, clamp_len=1000} can reach test ppl around 24, and {tgt_len=2048, mem_len=2048, clamp_len=1000} can reach test ppl around 20, but all of these settings can not reach the paper result 18.3, why?    `#!usr/bin/env python  # -*- coding:utf-8 -*-  import tensorflow as tf  from tensorflow import keras  import numpy as np  import pickle  from DataService import DataObjForWT_PTB as DataObj  import os    os.environ               if layer != self.n_layers - 1:                  new_cache.append(cur_layer_out               linear_proj = tf.matmul(embed, self.projection_list ], dtype=tf.int32)          )  # (c0, dim)            all_tail_cluster_embedding = self.tail_clusters_embedding(              inputs=tf.convert_to_tensor( ], dtype=tf.int32)                  )                  cur_inputs = tf.matmul(cur_inputs, self.projection_list[i], transpose_b=True)                  cur_logits = tf.matmul(cur_inputs, all_cur_cluster_embedding, transpose_b=True)                  cur_logits += self.bias_list[i]                    cur_softmax = tf.nn.softmax(cur_logits, axis=-1)                  cur_out_prob = tf.gather_nd(cur_softmax, r_s)                  cur_out_prob = tf.where(cur_out_prob >= 1e-9, cur_out_prob,                                          tf.ones_like(cur_out_prob, dtype=tf.float32) * 1e-9)                  cur_log_prob = -tf.math.log(cur_out_prob)                    cur_log_prob += pre_log_prob              x.append(cur_log_prob)          return tf.concat(x, axis=0)      class SingleTransformerBlock(keras.layers.Layer):      def __init__(self, d_model, ffn_size, n_heads, dropout_attn, dropout_norm, cur_layer):          super(SingleTransformerBlock, self).__init__()          self.n_heads = n_heads          self.cur_layer = cur_layer          self.d_model = d_model            self.w_query = keras.layers.Dense(              units=d_model, use_bias=False,              kernel_initializer=tf.constant_initializer(                  pre_train_weights[""transformer/layer_%d/rel_attn/qkv/kernel:0"" % cur_layer][:, 0:d_model]              )          )          self.w_key = keras.layers.Dense(              units=d_model, use_bias=False,              kernel_initializer=tf.constant_initializer(                  pre_train_weights[""transformer/layer_%d/rel_attn/qkv/kernel:0"" % cur_layer][:, d_model:2 * d_model]              )          )          self.w_value = keras.layers.Dense(              units=d_model, use_bias=False,              kernel_initializer=tf.constant_initializer(                  pre_train_weights[""transformer/layer_%d/rel_attn/qkv/kernel:0"" % cur_layer][:, 2 * d_model:]              )          )          self.w_rel_pos = keras.layers.Dense(              units=d_model, use_bias=False,              kernel_initializer=tf.constant_initializer(                  pre_train_weights[""transformer/layer_%d/rel_attn/r/kernel:0"" % cur_layer]              )          )            self.w_attn = keras.layers.Dense(              units=d_model, use_bias=False,              kernel_initializer=tf.constant_initializer(                  pre_train_weights[""transformer/layer_%d/rel_attn/o/kernel:0"" % cur_layer]              )          )            self.w_ffn_up = keras.layers.Dense(              units=ffn_size, activation=""relu"",              kernel_initializer=tf.constant_initializer(                  pre_train_weights[""transformer/layer_%d/ff/layer_1/kernel:0"" % cur_layer]              ),              bias_initializer=tf.constant_initializer(                  pre_train_weights[""transformer/layer_%d/ff/layer_1/bias:0"" % cur_layer]              )          )          self.w_ffn_down = keras.layers.Dense(              units=d_model,              kernel_initializer=tf.constant_initializer(                  pre_train_weights[""transformer/layer_%d/ff/layer_2/kernel:0"" % cur_layer]              ),              bias_initializer=tf.constant_initializer(                  pre_train_weights[""transformer/layer_%d/ff/layer_2/bias:0"" % cur_layer]              )          )            x_ut = tf.convert_to_tensor(pre_train_weights[""transformer/r_w_bias:0""][cur_layer],                                      dtype=tf.float32)  # (head, dim // head)          x_vt = tf.convert_to_tensor(pre_train_weights[""transformer/r_r_bias:0""][cur_layer],                                      dtype=tf.float32)  # (head, dim // head)            self.ut = tf.Variable(initial_value=tf.reshape(              x_ut, shape=(d_model,)          ), dtype=tf.float32, trainable=True)          self.vt = tf.Variable(initial_value=tf.reshape(              x_vt, shape=(d_model,)          ), dtype=tf.float32, trainable=True)            self.attn_drop = keras.layers.Dropout(rate=dropout_attn)          self.ffn_drop = keras.layers.Dropout(rate=dropout_norm)            self.attn_ln = keras.layers.LayerNormalization(              gamma_initializer=tf.constant_initializer(                  pre_train_weights[""transformer/layer_%d/rel_attn/LayerNorm/gamma:0"" % cur_layer]              ), beta_initializer=tf.constant_initializer(                  pre_train_weights[""transformer/layer_%d/rel_attn/LayerNorm/beta:0"" % cur_layer]              )          )          self.ffn_ln = keras.layers.LayerNormalization(              gamma_initializer=tf.constant_initializer(                  pre_train_weights[""transformer/layer_%d/ff/LayerNorm/gamma:0"" % cur_layer]              ), beta_initializer=tf.constant_initializer(                  pre_train_weights[""transformer/layer_%d/ff/LayerNorm/beta:0"" % cur_layer]              )          )        def call(self, inputs, **kwargs):          cache = kwargs[""cache""]          fusion_inputs = tf.concat([cache, inputs], axis=1)            query = tf.concat(tf.split(self.w_query(inputs) + self.ut, axis=2, num_or_size_splits=self.n_heads), axis=0)          q2 = tf.concat(tf.split(self.w_query(inputs) + self.vt, axis=2, num_or_size_splits=self.n_heads), axis=0)          key = tf.concat(tf.split(self.w_key(fusion_inputs), axis=2, num_or_size_splits=self.n_heads), axis=0)          value = tf.concat(tf.split(self.w_value(fusion_inputs), axis=2, num_or_size_splits=self.n_heads), axis=0)            pos_enc = G.create_pre_relative_encoding(seq_length=fusion_inputs.shape[1], dim=fusion_inputs.shape[2])          pos_enc = tf.tile(self.w_rel_pos(pos_enc)[tf.newaxis, ...], multiples=[fusion_inputs.shape[0], 1, 1])          pos_enc = tf.concat(tf.split(pos_enc, axis=2, num_or_size_splits=self.n_heads), axis=0)            attn_out = self.rel_scaled_dot_product_attention(query=query, key=key, value=value, pos_enc=pos_enc,                                                           padding_mask=kwargs[""padding_mask""], q2=q2,                                                           look_ahead_mask=G.create_look_ahead_mask(                                                               q_len=inputs.shape[1], k_len=fusion_inputs.shape[1]))          attn_out = tf.concat(tf.split(attn_out, axis=0, num_or_size_splits=self.n_heads), axis=2)            attn_out = self.w_attn(attn_out)          attn_out = self.attn_drop(attn_out, training=kwargs[""is_training""])          res_out_1 = attn_out + inputs          ln_out_1 = self.attn_ln(res_out_1)            ffn_up = self.w_ffn_up(ln_out_1)          ffn_down = self.w_ffn_down(ffn_up)            ffn_out = self.ffn_drop(ffn_down, training=kwargs[""is_training""])          res_out_2 = ln_out_1 + ffn_out          ln_out_2 = self.ffn_ln(res_out_2)          return ln_out_2        @staticmethod      def rel_scaled_dot_product_attention(query, q2, key, value, pos_enc, padding_mask, look_ahead_mask):          matmul_qk = tf.matmul(query, key, transpose_b=True)          matmul_qp = tf.matmul(q2, pos_enc, transpose_b=True)            pad_zero_1 = tf.zeros(shape=(query.shape[0], key.shape[1] - query.shape[1], key.shape[1]),                                dtype=tf.float32)          pad_zero_2 = tf.zeros(shape=(query.shape[0], key.shape[1], 1), dtype=tf.float32)          matmul_qp = tf.concat([pad_zero_2, tf.concat([pad_zero_1, matmul_qp], axis=1)], axis=2)            matmul_qp = tf.reshape(matmul_qp, shape=(matmul_qp.shape[0], matmul_qp.shape[2], matmul_qp.shape[1]))[:, 1:, :]            matmul_qp = matmul_qp[:, -query.shape[1]:, :]            matmul_out = matmul_qk + matmul_qp          dk = tf.cast(tf.shape(value)[-1], tf.float32)          scaled_attention_logits = matmul_out / tf.math.sqrt(dk)            pad_one = tf.ones(shape=(padding_mask.shape[0], key.shape[1] - query.shape[1]), dtype=tf.float32)          padding_mask = tf.concat([pad_one, padding_mask], axis=1)           padding_mask = tf.tile(padding_mask[:, tf.newaxis, :],                                 multiples=[query.shape[0] // padding_mask.shape[0], query.shape[1], 1])          look_ahead_mask = tf.tile(look_ahead_mask[tf.newaxis, :], multiples=[query.shape[0], 1, 1])          mask = tf.multiply(padding_mask, look_ahead_mask)          scaled_attention_logits += (1 - mask) * -1e9          attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)            output = tf.matmul(attention_weights, value)            return output      class GeneralFunction:      @staticmethod      def create_look_ahead_mask(q_len: int, k_len: int, same_length=True):          mask = tf.linalg.band_part(tf.ones(shape=(k_len, k_len), dtype=tf.float32), -1, 0)[-q_len:, ...]          if same_length:              x = mask[:, 0: q_len]              y = mask[:, q_len:]              x = tf.linalg.band_part(x, 0, -1)              mask = tf.concat([x, y], axis=1)          return mask        @staticmethod      def create_pre_relative_encoding(seq_length: int, dim: int):          pos = np.arange(start=seq_length - 1, step=-1, stop=-1, dtype=np.float32)[..., np.newaxis]          pos = np.minimum(pos, 1000)          all_i = np.arange(dim, dtype=np.float32)[np.newaxis, ...]          angle_rates = 1 / np.power(10000, (2 * (all_i // 2)) / np.float32(dim))          angle_rads = pos * angle_rates          x = np.sin(angle_rads[:, 0::2])          y = np.cos(angle_rads[:, 1::2])          pos_enc = tf.convert_to_tensor(tf.concat([x, y], axis=-1), dtype=tf.float32)          return pos_enc      class Main:      def __init__(self, **kwargs):          self.kwargs = kwargs          self.data_obj = DataObj(dataset_name=kwargs[""dataset_name""], segment_size=kwargs[""segment_size""],                                  pad_id=PAD, batch_size=batch_size)          self.cache = self.get_init_cache()          self.model = Vanilla_XL(              dataset_name=kwargs[""dataset_name""], n_heads=kwargs[""n_heads""], n_layers=kwargs[""n_layers""],              dropout_norm=kwargs[""dropout_norm""], dropout_attn=kwargs[""dropout_attn""],              d_embed=kwargs[""d_embed""], ffn_mul=kwargs[""ffn_mul""], segment_size=kwargs[""segment_size""],              cutoffs=kwargs[""cutoffs""], d_model=kwargs[""d_model""]          )        def train(self):          ppl, count = self.eval(is_valid=True)          print(""valid_ppl: %.3f, all_tokens:%d"" % (ppl, count))          ppl, count = self.eval(is_valid=False)          print(""test_ppl: %.3f, all_tokens:%d"" % (ppl, count))        def eval(self, is_valid):          sum_loss, sum_count = 0.0, 0          dic = self.data_obj.get_next_valid_test_segment(is_valid=is_valid)          self.cache = self.get_init_cache()          while dic is not None:              loss, count, new_cache = self.eval_step(inputs=dic[""input_ids""], ground_truth=dic[""ground_truth""],                                                      padding_mask=dic[""input_mask""])              self.cache = tf.concat(                  [self.cache[:, :, self.kwargs[""segment_size""]:, :], new_cache], axis=2              )              sum_loss += loss              sum_count += count              dic = self.data_obj.get_next_valid_test_segment(is_valid=is_valid)          ppl = tf.exp(sum_loss / sum_count)          return ppl, sum_count        @tf.function      def eval_step(self, inputs, ground_truth, padding_mask):          log_prob, new_seg_cache = self.model(inputs=inputs, training=False, padding_mask=padding_mask,                                               cache=self.cache, ground_truth=ground_truth)          total_loss = tf.reduce_sum(log_prob)          count = tf.cast(tf.shape(log_prob)[0], dtype=tf.float32)          return total_loss, count, new_seg_cache        def get_init_cache(self):          return tf.zeros(              shape=(batch_size, self.kwargs[""n_layers""], self.kwargs[""mem_len""], self.kwargs[""d_model""]),              dtype=tf.float32)      if __name__ == ""__main__"":      with open(""InitWeights/WT103/weights.p"", ""rb"") as f:          pre_train_weights = pickle.load(f)      dataset = ""wikitext-103""      PAD = 0      batch_size = 1      G = GeneralFunction()      _cutoffs = [          1, 20001, 40001, 200001, vocab_size_dic[dataset]      ]      a_epoch_segment = {          ""384"": 268820 // batch_size,          ""512"": 201615 // batch_size,          ""256"": 403230 // batch_size      }      E = Main(dataset_name=dataset, segment_size=128, mem_len=1600, n_heads=16, d_model=1024, n_layers=18,               d_embed=1024, batch_size=batch_size, dropout_attn=0.2, dropout_norm=0.2,               ffn_mul=4, cutoffs=_cutoffs, method=""AC001"")      E.train()  `"
计算相对位置编码时，inv_freq变量不是整型数，torge.ger方法要求参数是Long类型，不太理解这个计算步骤
     Open-Science NLP Bounty: ($100 + $100 to charity)    Task: A notebook demonstrating experiments of this widely cited LM baseline on PTB.    It seems many people on twitter have not been able to replicate anything near the PTB numbers reported in this paper. I would love for someone to prove me wrong and am happy to pay for it.
灰色的线代表什么？  隐层之间的线怎么不是双向的呢？   还有绿色的线代表的是啥？  十分感谢！
"Having read of other people's attempts (like in  ), and the sensitivity of Transformer-XL to hyperparameters,    if anyone has tried to train a Transformer-XL Language Model on a new corpus, I would like to hear your opinions and experiences on:    what were your hyperparameters?   Did you use any warm-up, and if so spanning how many epochs?   From my grid-search I would say that a learning rate in [0.0001, 0.0005] is a reasonable value.  How many epochs did you set it to train for? 10, 20, 100?"
"Thanks for the code for Pytorch.    I'm wondering is there any technique I can use to speed up the inference. Could anyone please share your inference time for  evaluation using the given .sh files? I got more than 700 seconds here for run_text8_base.sh. When I change the default batch_size 10 to a larger one, for example, batch_size=40, the inference time is still more than 600 seconds, I don't know if this is the ""quick"" inference as what the author said.  Also, I wanna know how to use multi gpus for evaluation.     I really want to get a quick respond for each inference iteration  to verify the effectiveness of some modifications of this model, could you please help me? Thanks a lot!"
"Hi,     Assuming for the vanilla transformer, I can set the tgt_len=512 due to the GPU limitation.    If I would like to apply the transformer-xl (concatenate the memory from the previous segment with the current segment along the length dimension), but due to the GPU limitation, I have to decrease the tgt_len=256 (that is, after concatenation, mem_len+tgt_len=256+256)    So, the overall history length is the same (512 and 256+256).   The critical idea is trying to ""enhance"" the memory representations by the recurrence mechanism (the first 256 in the segment after concatenation).    Does it make sense?    Thanks!"
None
"   when same_length is not true, I can understand the mask, for current token x_n(n include mlen), mask is 0 for all target x_<=n.   But for same_length, what it is doing to take the first qlen mask from ret calculated for same_length is False and set all these to 1 for target x_<n?  if want to atten only the last qlen memory should it be below?       "
"Dear authors,  This script transformer-xl/tf/scripts/wt103_base_gpu.sh use enwiki8 as the test data, is it correct?"
"It's great how low the perplexity score is on the BWCorpus.  I'd like to try getting actual word predictions out of it but I'm having difficulty figuring out how to extract a traditional softmax from the ""adaptive softmax"" used here.      For the Tensforflow model, does anyone have code for this?  Is there another project on GitHub where that is done that might serve as an example?    btw... I'm using the GPU methods (not the TPU).  The method `model::mask_adaptive_logsoftmax`  looks like it's somewhat close to what I'd need but I'm struggling with some of the specifics."
"Hi, thanks for the great job !  May I ask how you visualize the attention of different head ? I mean the Part D in the appendix. Would you please give me some help and that would help me a lot, thanks!"
I wanted to know the expected test perplexity for the lm1b base model. It would be especially great if you could upload the training log file if possible. I wanted to include the results in my ICML paper.  
"In the cutoff index dependent conditional for updating loss, head_logprob appears to be used before it is ever defined.      "
"I am trying to train with 1 billion corpus on Tesla P40.  Following are the values being used.    N_LAYER = 12  D_MODEL = 512,   D_EMBED = 512,   D_INNER = 2048,  D_HEAD = 64    I also tried with a BSZ of 128, it still gives OOM error."
"when i use bash sota/enwik8.sh     Preprocess test set...  Loading cached dataset...  Traceback (most recent call last):    File ""data_utils.py"", line 586, in        tf.app.run(main)    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 125, in run      _sys.exit(main(argv))    File ""data_utils.py"", line 382, in main      corpus = get_lm_corpus(FLAGS.data_dir, FLAGS.dataset)    File ""data_utils.py"", line 345, in get_lm_corpus      corpus = pickle.load(fp)  UnicodeDecodeError: 'ascii' codec can't decode byte 0x88 in position 153592: ordinal not in range(128)  Run evaluation on test set...  I0304 14:20:51.687984 139744890042112 tf_logging.py:115] n_token 204  Traceback (most recent call last):    File ""train_gpu.py"", line 475, in        tf.app.run()    File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 125, in run      _sys.exit(main(argv))    File ""train_gpu.py"", line 471, in main      evaluate(n_token, cutoffs, ""/gpu:0"")    File ""train_gpu.py"", line 366, in evaluate      use_tpu=False)    File ""/home/gaodihe/PycharmProjects/transformer-xl/tf/data_utils.py"", line 424, in get_input_fn      num_core_per_host, use_tpu=use_tpu)    File ""/home/gaodihe/PycharmProjects/transformer-xl/tf/data_utils.py"", line 415, in load_record_info      with open(record_info_path, ""r"") as fp:  FileNotFoundError: [Errno 2] No such file or directory: './/pretrained_xl/tf_enwik8/data/tfrecords/record_info-test.bsz-16.tlen-128.json'    this error happens.    How could i fix it?  Thanks"
预测速度快是因为state reuse？是用 两个32seq_len+state-reuse 和 一个64seq_len+no-state-reuse 比吗？  !     多谢多谢多谢！  @kimiyoung   @zihangdai 
I use the code here:      ------------  I run `run_classifier.py` with `bert-base-uncased` and `max_seq_length=128` on the MRPC task.  The log:     The speed is about 1.7 s/batch    ------------------  I run `run_transfo_xl.py` on the `wikitext-103` task.  The log:     The speed is about 2.4 s/batch  
What changes we need to perform inside the script to train in a new corpus ?    I have checked the script and there is a lot of if condition depend on each corpus.
Thank you very much!  !   I have not get the main reason.  Thank you very much!
"I think it's good to find out transformer-xl's potential abilities of supporting different languages, thus, any plan to release multilingual models? "
!     Thank you!!
"> Hence, the model lacks necessary contextual information needed to well predict the first few symbols.    So the backward direction part of model can not do:     > predict the first few symbols"
"Hi:      I am reading the code of the model, but I can not find which part is encoder and which is decoder. So i want to know that is transformer-xl like a word-embedding layer as BERT instead of a seq2seq model? Thanks a lot."
"Hi,  How do i create a tfrecord for a new sentence and get its perplexity using the evaluate function.   Thanks."
"ub16c9@ub16c9-gpu:~/ub16_prj/transformer-xl/pytorch$ bash run_enwik8_base.sh train --work_dir enwiki8_task  Run training...  Experiment dir : enwiki8_task-enwik8/20190127-180347  Loading cached dataset...  ====================================================================================================      - clip : 0.25      - eta_min : 0.0      - finetune_v3 : False      - n_layer : 12      - pre_lnorm : False      - n_head : 8      - proj_init_std : 0.01      - emb_init_range : 0.01      - fp16 : False      - n_nonemb_param : 40949760      - scheduler : cosine      - work_dir : enwiki8_task-enwik8/20190127-180347      - batch_size : 22      - debug : False      - dropatt : 0.0      - init_std : 0.02      - lr : 0.00025      - cuda : True      - data : ../data/enwik8/      - emb_init : normal      - ext_len : 0      - sample_softmax : -1      - eval_tgt_len : 128      - restart_dir :       - mom : 0.0      - clamp_len : -1      - max_eval_steps : -1      - batch_chunk : 1      - multi_gpu : True      - mem_len : 512      - dynamic_loss_scale : False      - d_embed : 512      - max_step : 400000      - attn_type : 0      - lr_min : 0.0      - static_loss_scale : 1      - init : normal      - patience : 0      - dropout : 0.1      - finetune_v2 : False      - d_head : 64      - same_length : False      - dataset : enwik8      - init_range : 0.1      - d_model : 512      - tgt_len : 512      - optim : adam      - d_inner : 2048      - warmup_step : 0      - restart : False      - seed : 1111      - adaptive : False      - n_token : 204      - log_interval : 200      - varlen : False      - tied : True      - clip_nonemb : False      - decay_rate : 0.5      - div_val : 1      - gpu0_bsz : 4      - not_tied : False      - eval_interval : 4000      - n_all_param : 41055436  ====================================================================================================  #params = 41055436  #non emb params = 40949760  Traceback (most recent call last):    File ""train.py"", line 539, in        train()    File ""train.py"", line 445, in train      ret = para_model(data, target, *mems)    File ""/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py"", line 477, in __call__      result = self.forward(*input, **kwargs)    File ""/home/ub16c9/ub16_prj/transformer-xl/pytorch/utils/data_parallel.py"", line 64, in forward      inputs, kwargs = self.scatter(inputs, kwargs, device_ids)    File ""/home/ub16c9/ub16_prj/transformer-xl/pytorch/utils/data_parallel.py"", line 80, in scatter      bsz_unit = (bsz - gpu0_bsz) // (num_dev - 1)  ZeroDivisionError: integer division or modulo by zero  ub16c9@ub16c9-gpu:~/ub16_prj/transformer-xl/pytorch$ "
"Hi,    I would like to train a model on TPU, but I'm not able to find the correct settings for a *v2-8* TPU.    What parameters are needed for `NUM_HOST` and `NUM_CORE`? I tried different values, but I always get `num_replicas should be (8), got (XXX).` error messages.    What TPU model did you use for the 1 Billion word benchmark?    Can I create the `tfrecords` locally (on a non-TPU) in the `train_data` step?    Thanks :)"
"I am not sure where it is wrong but I have been training enwik8 with your TensorFlow code and default parameters on 4 GPUs for 4 days and the loss never drops below 4.2 meanwhile the learning rate already drops to 0.000001. Is there any special tricks to replicate the experiment?    Thanks.    P.S. I am using Python 3 and TensorFlow 1.11.0. I have not tried on the other 3 datasets yet. I also tried transformer-xl on a private dataset (where a single-layer word-level LSTM can achieve around 60%+ accuracy), and its loss also never drops below 4.2 and accuracy never goes higher than 15%."
"Thanks for the codes! I am sure my question will be asked over and over and over again in near future. And I also read your paper which is all about comparison against vanilla transformer.    But still, in terms of performance, have you compared your great model against BERT? I mean it may not be a 100% fair comparison. But at the end of the day... which one (BERT or Tranformer-XL) is better on typical NLP tasks? Thanks."
None
"Hi,  Thanks for this great piece of work (research and code), it's very impressive!  I am wondering why the PyTorch version has the additional parameter `ext_len` which doesn't seems to be used in the TensorFlow version."
"hello,I have read this paper but I'm confused at the relative attention score. I don't know why the transpose(u) and the transpose(v) were defined separately as they seemed have the same meaning. Is there anything that I should consider ?"
"Thank you for releasing such an awesome and easy to use code!    Could you, please, elaborate a little bit on PyTorch implementation multi GPU setup? More concretely, what does the parameter ""gpu0_bsz"" mean and what parameters should I change to scale this code to setups with the number of GPUs more (or less) than 4?    From the description it seems that ""gpu0_bsz"" is the batch size for GPU0, but it is not clear to me why it should differ from batch sizes on other GPUs."
Love the work and really glad to see `getdata.sh` was useful and that you extended it! Wrangling datasets is never the fun part ;)    Can you clarify the license for the released code by adding a LICENSE file?
"I am quite new to deep learning and coding in general but am keen to make use of this library to explore a possible dissertation topic on MT.  I have cloned the repository onto my hard drive but when I try to install with pip I get an error to the effect that pip cannot find version 1.5.0 of pytorch - the earliest available is 1.7.0.  Does it need to be 1.5.0 or can I tweak requirements.txt to choose a different version and if so, which version should be used?  Many thanks for your support. Rob"
"It seems that it is not convenient to put mtdnn in other work, and it can be more convenient to use mtdnn with the hugging face. But I searched the official website and found that no one published relevant content. Can you provide the corresponding file format? thank you!"
"First observation: the Python version on the instructions is 3.6, but the transformer version in the requirements.txt is 4.20.0, which is only available for python >= 3.7.   The second is: I cannot find the `scripts` directory.  I'm looking for the embedding extractor.  So, I'm assuming that is the `/experiments/dump_embedding` directory. It's correct?    And finally, nice job! You have very interesting results. "
"Hello,  I am trying to download and test the repository, but I keep getting an error with docker nvidia runtime. So I am just guessing: is it possible to run the project on Windows?  Thank you in advance!  Simone      !   "
Hello! Your work is very effective. I wonder if tasks must be separated from the benchmark model before they can be used as pre training language models for other tasks? Does the provided. PT file split the task?
"Hi, In **mt-dnn**, inappropriate dependency versioning constraints can cause risks.    Below are the dependencies and version constraints that the project is using         The version constraint **==** will introduce the risk of dependency conflicts because the scope of dependencies is too strict.  The version constraint **No Upper Bound** and **\*** will introduce the risk of the missing API Error because the latest version of the dependencies may remove some APIs.    After further analysis, in this project,  The version constraint of dependency **numpy** can be changed to *>=1.8.0, =4.36.0, =0.1.1, =0.15.0, =0.12.0, =2.0.0,  The calling methods from the numpy     numpy.ma.masked_array          The calling methods from the tqdm     tqdm.tqdm          The calling methods from the pytorch-pretrained-bert     pytorch_pretrained_bert.modeling.BertLayerNorm          The calling methods from the scikit-learn     sklearn.metrics.f1_score  random.choice  sklearn.metrics.accuracy_score  sklearn.metrics.matthews_corrcoef  sklearn.metrics.roc_auc_score  sklearn.metrics.confusion_matrix          The calling methods from the future     time.strftime  datetime.datetime.now          The calling methods from the transformers     transformers.AutoTokenizer.from_pretrained          The calling methods from the all methods     plt.clf  _make_qid_to_has_ans  grad.abs.add_  sorted.append  torch.nn.Tanh  score.reshape.tolist.numpy  os.path.isdir  torch.erf  logging.StreamHandler.setFormatter  self._task_def_dic.items  layer_module  module.register_forward_pre_hook  subprocess.call.strip  iter  make_qid_to_has_ans.items  self._prepare_model_input  rnn_cls  m.contiguous  src.items  set  data_utils.log_wrapper.create_logger.warning  numpy.ma.masked_array  extra_indices.tolist.tolist  mask.size  output_dir.os.path.join.open.read  self.shadow.items  find_best_thresh  self.qkv_head_dim.self.num_heads.bsz.src_len.k.contiguous.view.transpose  find_all_best_thresh  min  truncate_seq_pair  data.get.str.lower  score.numpy.numpy  set_config  self.dropout.transpose  transformers.get_polynomial_decay_schedule_with_warmup  key_padding_mask.unsqueeze.unsqueeze  input_lengths.append  self.tok2ind.keys  final_predict.append  passage.replace.replace  prefix.format.opt.get.upper  logging.StreamHandler  f.readlines  logging.StreamHandler.setLevel  _get_raw_scores  torch.nn.functional.cross_entropy  line.strip.split  tqdm.tqdm  prepare_validation_features  AttentionWrapper  range  format.startswith  self._tokenizer.vocab.keys  tokens_b.extend  scores.data.masked_fill_  qb.new_dict.cpu.numpy  rnn_type.upper.upper  delattr  score.numpy.tolist  data_utils.task_def.EncoderModelType  mlm_p.view.size  shutil.rmtree  load_stsb  self.AdamaxW.super.__init__  self.kd_task_loss_criterion.append  self.dropout.bmm  partial_feature  data_utils.log_wrapper.create_logger.error  x.size  self.SmartPerturbation.super.__init__  torch.tanh  self.eff_perturb.update  copy.deepcopy.item  rows.append  key_padding_mask.unsqueeze  score.reshape.tolist  self.linear.contiguous  test_prefix.lower  self.score_func  load_qqp  self.SimpleFlatSim.super.__init__  self.BilinearSum.super.__init__  make_qid_to_has_ans.values  json.dumps  extract_answers_from_features  x_flat.self.linear.view  self.rnn.new  os.path.exists  TaskDef  text.lower  round  self.Pooler.super.__init__  self.decoder  self.dropout  copy.deepcopy.extend  self.DropoutWrapper.super.__init__  self.y_linear  logging.FileHandler  inputs.append  data_utils.vocab.Vocabulary  SanPooler  numpy.random.seed  x1.contiguous.view  args.train_datasets.split  self._global_map.get  i.multi_task_train_data.len.i.start.datetime.now.str.split  search_index  torch.float32.target.detach.F.softmax.sqrt_  kb.new_dict.cpu.numpy  mt_dnn.batcher.Collater  torch.device  open  key.transpose.size  masked_lm_positions.append  collections.OrderedDict  torch.max  torch.sum  text.split  random.randint  self.dropout.contiguous  torch.unbind  x1_flat.self.proj_1.self.f.view.bmm  self.qkv_head_dim.self.num_heads.bsz.src_len.v.contiguous.view.transpose.contiguous  experiments.superglue.superglue_utils.save  all_encoder_layers.append  y.unsqueeze.expand_as.unsqueeze  self.optimizer.zero_grad  data_utils.tokenizer_utils.create_tokenizer  self.encode  qa.get  mt_dnn.batcher.DistTaskDataset  batch_meta.batch_data.detach.cpu.numpy.tolist  self.classifier  sklearn.metrics.roc_auc_score  self.SANBertNetwork.super.__init__  grad.abs.max  self.optimizer.load_state_dict  self.qkv_head_dim.self.num_heads.bsz.tgt_len.q.contiguous.view.transpose  self.init  self.setup  self.qkv_head_dim.self.num_heads.bsz.src_len.k.contiguous.view.transpose.transpose  self.alpha.data.new  torch.autograd.grad  common.activation  kb.new_dict.cpu  load_json  super  labels.append  tasks.get_task_obj.train_forward  super.__init__  beta2.exp_inf.mul_.unsqueeze  logits.size.logits.data.new.zero_.torch.diag.byte  train_datasets.append  torch.nn.functional.kl_div  self.f  self.train_loss.update  seqeval.metrics.classification_report  FlatSimV2  embed.detach.abs.mean  subprocess.check_output  scipy.stats.spearmanr  x.torch.std.expand_as  numpy.argmax  make_eval_dict  mt_dnn.batcher.DistMultiTaskBatchSampler  score.reshape.tolist.reshape  mt_dnn.model.MTDNNModel.update  k.contiguous.view  token.startswith  p.transpose  module.my_optim.weight_norm  noise.detach.requires_grad_  self.DotProduct.super.__init__  mt_dnn.batcher.Collater.patch_data  load_mrpc  SimpleFlatSim  eff_noise.detach  plt.hist  adv_logits.view.view  load_copa_mtdnn  run_precision_recall_analysis  trim  torch.zeros_like  batch_info.pin_memory.to  self.DeepAttentionWrapper.super.__init__  target.contiguous.view.view  int  self.parameters  self.qkv_head_dim.self.num_heads.bsz.src_len.k.contiguous.view.transpose.contiguous  torch.manual_seed  merge_eval  x2_flat.self.proj_2.self.f.view.transpose  scores.items  shutil.copytree  plot_pr_curve  self.__str__  grad.sign  json.loads.get  time.strftime  BilinearFlatSim  compute_f1  reduce_features_to_examples.append  closure  end.numpy.tolist.contiguous  data_utils.load_data  self.bert  create_bins  self.SANClassifier.super.__init__  get_tokens  logging.FileHandler.setLevel  self._tokenizer.convert_tokens_to_ids.get  y.x.torch.abs.y.x.y.x.torch.cat.contiguous  x.size.idx.mask.view.expand_as  self.rnn.size  embedding_weights.size  torch.nn.functional.mse_loss  uid.split  torch.LongTensor  sample.get.strip  os.getcwd  self._rnn  att_scores.F.softmax.unsqueeze  temp_answers.append  test_prepro_std  self.MLPSelfAttn.super.__init__  self.FlatSimV2.super.__init__  torch.nn.ModuleList  main  load_data  os.mkdir  self.compute_weight  self._setup_lossmap  setattr  x1.size.x1.size.x1.contiguous.view.self.x_dot_linear.view.expand_as.bmm  plt.savefig  epsilon.epsilon.y.detach  module.similarity.SelfAttnWrapper  torch.distributed.all_reduce  ground_truth.normalize_answer.split  batch_meta.batch_data.size  self.mnetwork.bert  self.query_wsum  prediction.normalize_answer.split  dim.p.transpose._norm.transpose  self._norm_grad  torch.nn.Linear  start_scores.squeeze.squeeze  adv_lc  model_config  remove_punc  self.config.update  torch.isnan  batch_meta.batch_data.detach  para.strip  tensorboardX.SummaryWriter.close  data_utils.roberta_utils.update_roberta_keys  sub_part.pin_memory.to  start.data.cpu  self.x_dot_linear  NotImplementedError  self.attn  set.add  sequence_outputs.append  slen.idx.yidx.embeddings.tolist  self.load  model.to.to  exp_avg.mul_  self.task_types.append  self.encoder  numpy.array  task_id.self.scoring_list.split  self.qkv_head_dim.self.num_heads.bsz.tgt_len.q.contiguous.view.transpose.contiguous  task_id.self.scoring_list  y.x.torch.abs.y.x.y.x.torch.cat.contiguous.view  y.view.view  module.common.activation  self.score_func.size  experiments.common_utils.dump_rows  self.emb_val.update  value.transpose.transpose  list.extend  tasks.get_task_obj.train_build_task_layer  start.append  vars.update  transformers.get_cosine_schedule_with_warmup  train_data_list.append  transformers.get_linear_schedule_with_warmup  predict.reshape.tolist  dataset.get_task_id  self._tokenizer.convert_tokens_to_ids  end.append  self.WeightNorm.super.__init__  self.linear  idx.lines.strip  torch.mean  argparse.ArgumentParser.print_help  mt_dnn.optim.AdamaxW  score.np.argmax.reshape  qb.new_dict.cpu  self._dataset.get_task_id  plt.xlim  line.strip  self.network.eval  dump_data  train_config.parse_args  vw.new_dict.cpu  end.numpy.tolist.numpy  load  alpha.unsqueeze.bmm  collections.defaultdict  self.num_hid.size.size.tmp_output.view.max  dropout_layer  question.strip  self._get_param_groups  bool  newbatch.append  s.normalize_answer.split  out_f.write  self.qkv_head_dim.self.num_heads.bsz.src_len.v.contiguous.view.transpose  input_ids.index  normalize_answer  any  key_padding_mask.size  torch.nn.utils.weight_norm.unsqueeze  machines.append  tasks.get_task_obj.input_parse_label  stable_kl  pred.pop  adv_loss.item  tensor.to  delta_grad.norm  torch.nn.utils.weight_norm  issubclass  self.FlatSimilarityWrapper.super.__init__  e.to  start.numpy.tolist.numpy  score.data.cpu  max  x.att_scores.F.softmax.unsqueeze.torch.bmm.squeeze  tasks.get_task_obj  torch.nn.parallel.DistributedDataParallel  self.config.get.backward  self.Bilinear.super.__init__  task_id.self.task_loss_criterion  logging.FileHandler.setFormatter  float  experiments.exp_def.TaskDefs.get_task_names  load_multirc_mtdnn  len.get  result.strip.split  model.to.parameters  _norm  self.dropout.size  torch.norm  hasattr  torch.diag  temp_1.pop  self.LinearSelfAttn.super.__init__  data_utils.mrc_eval.squadv2_evaluate_func  TypeError  self.scalar.expand_as  initialize_distributed  task_cls  LinearSelfAttn  sorted  start.numpy.tolist.contiguous  torch.utils.data.DataLoader  model_class.from_pretrained  state.items  enumerate  sys.exit  self.bert.generate  self._setup_kd_lossmap  self._gen_task_indices  plt.title  data_utils.mrc_eval.squadv1_evaluate_func  weight.cpu  join  os.path.abspath  numpy.max  tasks.get_task_obj.test_predict  target.size.target.view.float  pdb.set_trace  key.transpose.transpose  torch.cat  random.uniform  literal_model_type.lower.lower  exp_inf.new  get_raw_scores  self.adv_task_loss_criterion.append  epsilon.epsilon.p.detach.log  emb_val.item  features.append  x.torch.mean.expand_as  new_data.zero_.size  qw.new_dict.cpu  load_mlm_data  tok_len.batch_size.torch.BoolTensor.fill_  self.proj_1  torch.abs  json.loads.split  dump  q.contiguous.view  eff_noise.detach.abs  metric_fn  self.network.cuda  batch_meta.get  search_bin  bais.cpu.numpy  type_ids_list.append  make_precision_recall_eval  create_masked_lm_predictions  attn_weights.data.masked_fill_  logging.Formatter  y.x.torch.cat.contiguous.view  self.proj  ValueError  logits.size.logits.data.new.zero_.torch.diag.byte.unsqueeze.expand_as  i.tgt.extend  query.replace  time.gmtime  load_mnli  histogram_na_prob  collections.Counter  logit.size.logit.view.float.size  generate_mask  print_message  local_task_idx.self._datasets.get_task_id  torch.load.size  part.pin_memory  positions.append  self.activation  torch.pow  assert_file_equal  tasks.get_task_obj.train_prepare_label  attn_weights.size.attn_weights.data.new.zero_.torch.diag.byte  torch.isinf  x1.size.x1.contiguous.view.self.x_dot_linear.view  numpy.array_equal  torch.nn.utils.clip_grad_norm_  grounds.items  Vocabulary  attn_weights.float.masked_fill  self.embed_encode  type  cls  mt_dnn.model.MTDNNModel.load  mt_dnn.model.MTDNNModel.save  self.__if_pair__  task.split  format  module._parameters.keys  merge  logits.detach  logit.F.log_softmax.exp  model.detach  subprocess.call  dropout_wrapper.DropoutWrapper  eval  config_class.from_dict  isinstance  scaled_loss.backward  datetime.datetime.now  self.dropout.view  self._get_max_len  kd_lc  args.test_datasets.split  dataset.items  input_ids_list.append  logits.size.logits.data.new.zero_.torch.diag.byte.unsqueeze  tokenizer.vocab.keys  load_record_mtdnn  evaluate  logging.getLogger.addHandler  beta1.exp_avg.mul_.add_  load_sst  batch_info.pin_memory  locals  qw.new_dict.cpu.numpy  new_batch.append  state.keys  batch_data.append  expected_file.open.read  end_position.append  laod_function  FlatSim  attn_weights.data.new  torch.nn.functional.log_softmax  i.self.attn_list  self.embeddings  process_data  plt.xlabel  load_xnli  mt_dnn.model.MTDNNModel.cuda  mt_dnn.batcher.MultiTaskBatchSampler  scores.extend  map  score.contiguous.view  self.rnn  self.proj_2  vars  data_utils.load_score_file.keys  temp_1.append  self._get_index_batches  mt_dnn.model.MTDNNModel  y.x.y.x.torch.cat.contiguous.view  data_utils.metrics.calc_metrics  MultiheadAttentionWrapper  feature.example_id_to_index.features_per_example.append  para.clone  self.beta.expand_as  opt.EncoderModelType.name.lower  scores_for_ground_truths.append  sentence.append  self.pooler  self.network.train  numpy.zeros  label.append  scores_list.append  os.getenv  copy.deepcopy  tokens.append  p.norm  numpy.exp  loss.stable_kl  self._dropout_p_map.get  self.AttentionWrapper.super.__init__  json.loads  new_sequence_outputs.append  parse_args  re.compile  transformers.AutoTokenizer.from_pretrained.tokenize  prefix.format.opt.get.lower  x_flat.self.FC.self.f.self.linear.view  plt.ylabel  docs.append  torch.float32.input.detach.F.softmax.sqrt_  ry.rp.p.sum  data_config  list  v.contiguous.view  pytorch_pretrained_bert.modeling.BertEmbeddings  idx.mask.view  apply_no_ans_threshold  self.adv_teacher.forward  module.dropout_wrapper.DropoutWrapper  v.cpu  config.get  dataset.split  idx.all_encoder_layers.detach  train_config.add_argument  self.FC  sample_id_2_label_dic.keys  weight.reshape  proj.unsqueeze.x.bmm.squeeze  self.SelfAttnWrapper.super.__init__  rng.randint  data_utils.load_score_file  numpy.exp.sum  self.dropout_list.append  data_utils.log_wrapper.create_logger  torch.cuda.set_device  dev_data_list.append  query.transpose.size  mt_dnn.inference.eval_model  target.contiguous.view  h0.size.h0.new.zero_  locals.items  TASK_REGISTRY.get  plt.step  n_best_size.start_logits.np.argsort.tolist  Vocabulary.add  attn.transpose.transpose  tokens.extend  temp_2.pop  experiments.ner.ner_utils.load_conll_pos  self.Trilinear.super.__init__  logits.data.new  numpy.ones_like  p.nelement  self.network.named_parameters  pool.map  numpy.power  self.network.parameters  rvl.append  random.sample  self.attn_list.append  compare_output  self.FlatSim.super.__init__  self.network.encode  torch.bernoulli  assert_dir_equal  torch.save  json.dump  self.SimilarityWrapper.super.__init__  y_pred.append  pooled_output.contiguous.view  torch.distributed.init_process_group  task_def.get  args.task.lower  embed.data.new  vw.new_dict.cpu.numpy  xWy.data.masked_fill_  self.DotProductProject.super.__init__  next  batch_meta.batch_data.detach.cpu  MaskedLmInstance  self.num_hid.size.size.tmp_output.view.max.view  Classifier  x2.x1.abs  score.np.argmax.tolist  predict.strip.reshape  self.reset  query.transpose.transpose  DotProductProject  self._setup_adv_lossmap  vb.new_dict.cpu  generate_noise  torch.cuda.device_count  experiments.mlm.mlm_utils.load_loose_json.append  batch_meta.batch_data.cuda  torch.optim.SGD  model.state_dict.items  logit.size.logit.view.float  self.x_linear  rng.shuffle  task_id.self.dropout_list.size  sum  tensorboardX.SummaryWriter  os.access  sys.path.append  encoding_0.size.input_length.torch.LongTensor.fill_  x.size.x.size.x.data.new.zero_  sigmoid  uid.gold_map.append  x2_flat.self.proj_2.self.f.view  rng.random  golds.append  weight.cpu.numpy  tokenizer  argparse.ArgumentParser  WeightNorm  p.size.p.contiguous.view.norm  task_def_list.append  self.qkv_head_dim.self.num_heads.bsz.src_len.k.contiguous.view.transpose.size  experiments.exp_def.TaskDefs  self._setup_adv_training  line.strip.startswith  self.optimizer.step  self.adv_loss.update  logit.size.logit.view.float.view  args.model.split  TrainingInstance  torch.ones  config_class.from_pretrained  flat_scores.contiguous.view  collections.namedtuple  torch.log  scipy.stats.pearsonr  logging.getLogger.info  grounds.append  x1.size.x1.size.x1.contiguous.view.self.x_dot_linear.view.expand_as  lang_map.items  tasks.get_task_obj.input_is_valid_sample  attn.transpose.contiguous  self.att  torch.optim.AdamW  mt_dnn.perturbation.SmartPerturbation  doc.split  data_utils.tokenizer_utils.create_tokenizer._convert_token_to_id  tokenizer.append  embed.size.embed.data.new.normal_  torch.load  numpy.concatenate  p.dim  torch.nn.functional.softmax  build_data  y.unsqueeze.expand_as  self.Classifier.super.__init__  SimilarityWrapper  logits.size.logits.data.new.zero_  attn.transpose.contiguous.view  attn_weights.size.attn_weights.data.new.zero_  uid.predict_map.append  key_padding_mask.unsqueeze.expand_as  self._layer_norm  self.EMA.super.__init__  start_position.append  p.contiguous.view  ent_strs.append  torch.BoolTensor  idx.lines.strip.split  multiprocessing.Pool  load_qnli  torch.cuda.is_available  sklearn.metrics.accuracy_score  x.alpha.unsqueeze.bmm.squeeze  logging.getLogger  lower  mask.size.score.np.argmax.reshape.tolist  score.numpy.reshape  group.setdefault  mt_dnn.batcher.DistSingleTaskBatchSampler  load_wnli  human.items  self.decoder_opt.append  y.x.y.x.torch.cat.contiguous  os.path.splitext  pred.items  predictions.append  random.Random  self.rebatch  apex.amp.master_params  numpy.ma.masked_array.mean  mt_dnn.model.MTDNNModel.extract  end.numpy.tolist  load_scitail  numpy.random.choice  test_encoder  tokenizer.pop  attn_weights.float.F.softmax.type_as  MLPSelfAttn  data_utils.vocab.Vocabulary.add  compute  writer.write  self.linear.unsqueeze  tok_len.batch_size.torch.LongTensor.fill_  self.mnetwork  model.to.state_dict  end.data.cpu  mt_dnn.inference.extract_encoding  WeightNorm.compute_weight  sklearn.metrics.f1_score  target.contiguous.view.float  instances.append  output_file.open.read  load_squad  target.contiguous.view.detach  experiments.glue.glue_utils.submit  attention_mask_list.append  model.encode  load_qnnli  self.nsp  self.BilinearFlatSim.super.__init__  tqdm.auto.tqdm  proj  self.AdamaxW.super.__setstate__  mlm_y.view.view  hyp_mask.size  expected_dir.os.path.join.open.read  masked_lm_labels.append  p.contiguous  epsilon.epsilon.y.detach.log  test_metrics.items  idx.all_encoder_layers.detach.cpu.numpy  sub_part.pin_memory  epsilon.epsilon.p.detach  data_utils.utils.set_environment  cand_indexes.append  self._to_cuda  len  data_utils.utils.AverageMeter  hid_shape.weight.new.zero_  apex.amp.scale_loss  torch.no_grad  torch.nn.functional.dropout  sklearn.metrics.matthews_corrcoef  model_class  make_qid_to_has_ans  remove_articles  x1.size.x1.contiguous.view.self.x_linear.view.expand_as  test_data_list.append  vb.new_dict.cpu.numpy  self.self_att  task_id.self.dropout_list.contiguous  x2.size.x2.contiguous.view.self.y_linear.view  suffix.split.upper  self.encoder_type.EncoderModelType.name.lower  self.init_hidden  logging.getLogger.setLevel  self.tokenizer.batch_decode  weight.size  os.path.split  self.scheduler.step  input.view.float  recalls.append  torch.zeros  self.MaskLmHeader.super.__init__  SanEncoder  p.data.mul_  start.numpy.tolist  current_chunk.append  random.seed  p.size.p.contiguous.view.norm.view  filecmp.cmp  sorted.insert  eval.split  load_rte  functools.partial  self._rnn.flatten_parameters  model.predict  v.cuda.cuda  self._get_shuffled_index_batches_bin  module.similarity.FlatSimilarityWrapper  target.contiguous.view.size  score.numpy.contiguous  build_data_single  positives.random.sample.pop  qa_entry.get  torch.ones_like  torch.distributed.is_initialized  kw.new_dict.cpu  random.choice  generate_decoder_opt  encoding_0.size.input_lengths.max.input_lengths.len.torch.LongTensor.fill_  DotProduct  torch.bmm  mask.unsqueeze.expand_as  format.replace  train_config  module.pooler.Pooler  p.data.addcdiv_  self._get_shuffled_index_batches  self.alpha.expand_as  attn_weights.size.attn_weights.data.new.zero_.torch.diag.byte.unsqueeze  load_boolq  metric_max_over_ground_truths  cs.LOSS_REGISTRY  mask.sum  self._get_batch_size  getattr.cuda  self.optimizer.state_dict  argparse.ArgumentParser.parse_args  opt.get  attn_weights.size.attn_weights.data.new.zero_.torch.diag.byte.unsqueeze.expand_as  load_cb  bin_idx.data.append  grad.abs  dict  argparse.ArgumentParser.add_argument  exp_inf.mul_  len.pop  data_utils.utils_qa.postprocess_qa_predictions  x.data.new  yaml.safe_load  torch.nn.functional.softmax.unsqueeze  WeightNorm.apply  transformers.get_constant_schedule_with_warmup  experiments.ner.ner_utils.load_conll_ner  _mg  RuntimeError  noise.detach.detach  label_tokenize  prepare_train_feature  SanLayer  common.init_wrapper  args.layers.split  temp_2.append  self.dropout.float  plt.fill_between  next.extend  pytorch_pretrained_bert.modeling.BertLayerNorm  n_best_size.end_logits.np.argsort.tolist  self.task_loss_criterion.append  float.key_padding_mask.unsqueeze.unsqueeze.attn_weights.float.masked_fill.type_as  feature_extractor  part.pin_memory.to  load_cola  input.view.detach  x1.size.x1.contiguous.view.self.x_linear.view  segment_ids.append  sklearn.metrics.confusion_matrix  tokens_a.extend  experiments.mlm.mlm_utils.load_loose_json  torch.cuda.manual_seed_all  feature_index.features.get  Wy.unsqueeze.x.bmm.squeeze  str  encoding_1.encoding_0.abs  self.score_func.view  torch.std  y.x.torch.cat.contiguous  self._task_type_map.keys  embed.detach.abs  task_id.self.dropout_list  exact_scores.values  seqeval.metrics.f1_score  apex.amp.initialize  plt.ylim  torch.nn.parameter.Parameter  blocks.lang_dict.append  self.dense  self.bert.parameters  torch.nn.Parameter  os.makedirs  entry.get  random.shuffle  BilinearSum  tasks.get_task_obj.test_prepare_label  experiments.exp_def.TaskDef.from_dict  line.strip.split.split  self.tok2ind.get  x1_flat.self.proj_1.self.f.view  f1_scores.values  sample_id_2_label_dic.items  generate_golds_predictions_scores  context.strip  re.sub  transformers.AutoTokenizer.from_pretrained.convert_tokens_to_ids  self.config.get  args.model.split.split  mask.sum.tolist  torch.nn.DataParallel  input.view.squeeze  transformers.AutoTokenizer.from_pretrained  Trilinear  register_task  x2.contiguous.view  self.network.load_state_dict  module.register_parameter  target.contiguous.view.contiguous  experiments.exp_def.TaskDefs.get_task_def  torch.FloatTensor  tensorboardX.SummaryWriter.add_scalar  sizes.append  self.rnn_type.nn.getattr  module.san.SANClassifier  metric_func  self.ind2tok.get  self._setup_optim  re.compile.findall  kw.new_dict.cpu.numpy  score_path.open.read  prelim_predictions.append  mt_dnn.matcher.SANBertNetwork  glob.glob  arch.config_class.from_pretrained.to_dict  torch.bernoulli.unsqueeze  Bilinear  self.__random_select__  torch.distributed.get_rank  self.model.named_parameters  data_utils.roberta_utils.patch_name_dict  model  input.view.view  line.strip.strip  max_answer_seq_len.batch_size.torch.LongTensor.fill_  mt_dnn.batcher.MultiTaskDataset  collections.Counter.values  getattr  math.sqrt  logits.data.masked_fill_  qa_sample  ImportError  args.transformer_cache.init_model.config_class.from_pretrained.to_dict  examples.append  x2.size.x2.contiguous.view.self.y_linear.view.expand_as  self.bert.embeddings  data_utils.log_wrapper.create_logger.info  experiments.mlm.mlm_utils.create_instances_from_document  eff_perturb.item  white_space_fix  reduce_features_to_examples  end_scores.squeeze.squeeze  compute_acc  feature_index.features.get.get  experiments.ner.ner_utils.load_conll_chunk  next.new  predict.strip.strip  exp_inf.new.long  new_data.zero_.zero_  eff_noise.detach.abs.mean  torch.stack  json.load  evaluation  tasks.get_task_obj.train_prepare_soft_labels  mt_dnn.batcher.SingleTaskDataset  idx.all_encoder_layers.detach.cpu  target.F.log_softmax.exp  task_id.self.dropout_list.view  self.config.get.item  flat_squad  load_wic_mtdnn  batch_meta.batch_data.detach.cpu.numpy  attn.transpose.size  zip  self.LayerNorm.super.__init__  task_layer  p.size  bais.cpu  tokenizer.sequence_ids  compute_exact  os.path.join  precisions.append  y_true.append  eps.grad.abs.add_.unsqueeze_  mlm_p.view.view  repr  model.cuda  tuple  x.contiguous.view  print  numpy.argsort  load_snli  model.size  self.scoring_list.append          @developer  Could please help me check this issue?  May I pull a request to fix it?  Thank you very much."
"For branch master, I modify the output dimension in matcher.py to output one piece of data in different tasks at the same time, but it cannot be implemented. Where should I modify the network?Thanks!"
"For branch v0.1, I want to get the complete pre-trained model without task-specific layer from the checkpoint file `mt_dnn_base_uncased.pt`, however, I am not able to convert it to `MTDNNModel`. Is there a script that can help me get the multi-task trained MT-DNN model from the checkpoint file? Thanks!"
"Thank you for provided code of SMART.  SMART uses the following  code to get the embeddings, which is then used to get noisy embeddings and feed bert as `inputs_embeds`.       But for `inputs_embeds` in transformers, it should be the output of `bert.embeddings.word_embedding` not `bert.embeddings`.   Please refer to the following code for `BertEmbedding.forward`:       "
I was trying to finetune on STS-B using SMART like follows:    `python ../train.py --task_def ../experiments/glue/glue_task_def.yml --data_dir ${DATA_DIR} --init_checkpoint ${BERT_PATH} --batch_size ${BATCH_SIZE} --output_dir ${model_dir} --log_file ${log_file} --train_datasets ${train_datasets} --test_datasets ${test_datasets} --adv_train --adv_opt 1  `  The following error pops out.        RuntimeError: Boolean value of Tensor with more than one value is ambiguous'    I think the problem is that at line 106 of perturbation.py:       The arguments do not match the forward function at line 165 in matcher.py:         I think the 'y_input_ids' is missing in perturbation.py but I am not sure how to fix it. Any solutions or suggestions? Thanks!
"Figure a in Figure 1 shows the decision boundary without Rs, and Figure b shows the decision boundary with Rs.But it gives me the feeling that the decision boundary of Figure a is smoother, while the decision boundary of Figure b is a bit over-fitting. In the paper, you want to use Figure 1 to show that after the disturbance is added, the output f of the neural network will not change too much, but the figure also gives the decision boundary, and the amount of change is whether to add Rs, Rs itself It's not a disturbance, but a regular term, so it makes me feel very confused, can you explain it?    Finally, my feeling is that in Figure a, the Rs regular term is not added, but the decision boundary is smoother, that is, the generalization performance is better; Figure b, the Rs regular term is added, but it is over-fitting, and the generalization performance Deterioration; and the added disturbance and the output f of the neural network are not shown on the graph;    In addition, it is mentioned in Figure 1 that the output f will not change too much in the vicinity of the training data, but this sentence is not visible on the figure;    Want to understand what you mean, please explain, thank you!"
"Hi,   I use SMART.   In the SMART paper, the gradient is divided by the infinity norm of the gradient first, but in the perturbation.py, the gradient is multiplied by the step_size first, and then the sum of the initial noise and the gradient is divided by the infinity norm of it.  According to the algorithm on the paper, I think that the only gradient is divided by the infinity norm of the gradient, and then the initial noise should be added to it   Could you let me know which is correct?"
"The init_checkpoint is trained by using `scripts/run_mt_dnn.sh`, and only the random seed changed, the train loss:     "
"In the paper in section 3, there are equations from (1) to (5)  for Single-Sentence Classification Output (1)  Text Similarity Output (2)  Pairwise Text Classification Output (3-4)  and Relevance Ranking Output (5)  Could you please explain what should be the  formula for the sequence labeling task that you now have implemented in this framework?"
"When using a Portuguese Translation of squad 1.1 with initial checkpoint as neuralmind/bert-base-portuguese-cased, the prediction step throws a KeyError specifically in squad_utils.py extract_answer function:    !      Noticing that the error was related to questions with biggest context paragraph, I set +10 in DOC_STRIDE in prepro_std.py and, additionally, executed training with batch_size = 20. The first epoch ran without errors, however in the second about 7 KeyErrors errors related to these answers bound were encountered. I tried increasing DOC_STRIDE in 20, 30, 40, 50 and it still did not work.    The dataset was also run on hugging face using run_squad.py without errors.  "
"It would be nice to have a capability that I can give a dataset with labels at sentence level, but which will be treated as a sequential task.  example:  [B-label1, I-label1, O], [""this is a sentence"", ""this is another sentence""]    Thank you"
"Hi, do you use a crf on top, for ner? This could provide a boost in performance?"
None
"     When running on the CuDNN backend, there might be reproducibility issue.  According to pytorch document, two further options must be set:   "
"I train 5 tasks at the same. In the prediction, I use this command, --task_id=1 is triggered an error. I check and see that the output is an array like this [0] only? I though the ouput should be something like [0,1,0,0,1]?    `python3 predict.py --task accident --task_def dataset/all_task_def.yml  --max_seq_len 128 --batch_size_eval 8 --checkpoint checkpoint/model_0.pt --prep_input dataset/bert_base_cased/test_train.json --task_id 1 --score dataset/bert_base_cased/test_pred_1.json `"
"Running on an internal data set. Only 1 multi-classification task.    BERT F1 score: ~85%   ELECTRA / ROBERTA F1 score: ~75%      model.network of ELECTRA (last ~10 lines)    !     - Changing learning rates does not help ELECTRA/ROBERTA to match BERT performance  - Calling the model using this config.     - electra.pt is directly from   (with state and config as keys)  - Similar issue as reported here:    - Full parameters below  `02/09/2022 01:12:40 {'log_file': 'log.log', 'tensorboard': False, 'tensorboard_logdir': 'tensorboard_logdir', 'init_checkpoint': 'electra.pt', 'data_dir': 'data', 'data_sort_on': False, 'name': 'farmer', 'task_def': 'config.yml', 'train_datasets': ['agg_data'], 'test_datasets': ['agg_data'], 'glue_format_on': False, 'mkd_opt': 0, 'do_padding': False, 'update_bert_opt': 0, 'multi_gpu_on': False, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'max_answer_len': 10, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': 1, 'pooler_actf': 'tanh', 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 512, 'init_ratio': 1, 'encoder_type': 7, 'num_hidden_layers': -1, 'bert_model_type': 'bert-base-uncased', 'do_lower_case': False, 'masked_lm_prob': 0.15, 'short_seq_prob': 0.2, 'max_predictions_per_seq': 128, 'bin_on': False, 'bin_size': 64, 'bin_grow_ratio': 0.5, 'local_rank': -1, 'world_size': 1, 'master_addr': 'localhost', 'master_port': '6600', 'backend': 'nccl', 'cuda': True, 'log_per_updates': 500, 'save_per_updates': 10000, 'save_per_updates_on': False, 'epochs': 5, 'batch_size': 8, 'batch_size_eval': 8, 'optimizer': 'adamax', 'grad_clipping': 0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 5e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'adam_eps': 1e-06, 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'model_ckpt': 'checkpoints/model_0.pt', 'resume': False, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'lr_gamma': 0.5, 'scheduler_type': 'ms', 'output_dir': 'result/exp2', 'seed': 2018, 'grad_accumulation_step': 4, 'fp16': False, 'fp16_opt_level': 'O1', 'adv_train': False, 'adv_opt': 0, 'adv_norm_level': 0, 'adv_p_norm': 'inf', 'adv_alpha': 1, 'adv_k': 1, 'adv_step_size': 1e-05, 'adv_noise_var': 1e-05, 'adv_epsilon': 1e-06, 'encode_mode': False, 'debug': False, 'transformer_cache': '.cache', 'task_def_list': [{'adv_loss': ' ', 'kd_loss': ' ', 'loss': ' ', 'dropout_p': 'None', 'enable_san': 'False', 'split_names': ""['train', 'dev', 'test']"", 'metric_meta': '( ,  )', 'task_type': ' ', 'data_type': ' ', 'n_class': '147', 'label_vocab': ' ', 'self': '{}', '__class__': "" ""}]}`    Will try to reproduce on opensource data once I have time."
"@namisan Please help.    When I am running predict.py on test set for evaluation score using my mt-dnn trained model. I am facing this error.  The reason I think they model architecture and checkpoints architecture are not the same.     Command:  `python predict.py --task=""mdd"" --task_id=0 --prep_input=""data//mdd_test.json"" --with_label --score=""mdd_score.txt""`    Error:       tasks.yml file:    > fid:  >   data_format: PremiseOnly  >   n_class: 2  >   labels:  >     - irony  >     - not_irony  >   task_type: Classification  >   enable_san: False  >   metric_meta:  >     - F1  >   loss: CeCriterion  >   > xnli:  >   data_format: PremiseAndOneHypothesis  >   enable_san: false  >   labels:  >     - contradiction  >     - neutral  >     - entailment  >   metric_meta:  >     - ACC  >   loss: CeCriterion  >   n_class: 3  >   task_type: Classification  >   > mdd:  >   data_format: PremiseOnly  >   n_class: 26  >   labels:  >     - RAB  >     - DAM  >     - BEI  >     - DOH  >     - JED  >     - SFX  >     - BAS  >     - MOS  >     - FES  >     - BAG  >     - ALG  >     - ALX  >     - JER  >     - TRI  >     - RIY  >     - MSA  >     - SAN  >     - ALE  >     - MUS  >     - CAI  >     - AMM  >     - ASW  >     - TUN  >     - BEN  >     - SAL  >     - KHA  >   task_type: Classification  >   enable_san: False  >   metric_meta:  >     - F1MAC  >   loss: CeCriterion        "
@namisan I have tweets dataset with multi-labels.    How can I define multi-labels in the task_def.yml file?  
"Code refactoring is requird.      File ""prepro_std.py"", line 13, in        from experiments.squad import squad_utils  ImportError: cannot import name 'squad_utils' from 'experiments.squad' (unknown location)  "
"Cheers,    trying to follow the SMART implementation, I fail to comprehend how the projection step in PGS algorithm on the epsilon max-norm ball around the noise is implemented.   Please correct me if I'm wrong, but it should be done in this step (in SmartPerturbation):  `noise, eff_noise = self._norm_grad(delta_grad, eff_grad=eff_delta_grad, sentence_level=self.norm_level)`    where the norm_grad returns:  `direction = grad / (grad.abs().max(-1, keepdim=True)[0] + self.epsilon)`    **Question:**     > how is x/(MaxNorm(x)+eps.) a projection on the epsilon max-norm ball.    Thank you for your support!     Cheers  Wassim  "
"Hello,     I am wondering why there are two different implementations of $\ell_s$, the symmetric KL divergence in the code (SmartPerturbation):    first in the loop:   `adv_loss = stable_kl(adv_logits, logits.detach(), reduce=False) `  and then outside of the loop:  `adv_loss = adv_lc(logits, adv_logits, ignore_index=-1)`  where adv_lc is the SymKlCriterion that is implemented differently from stable_kl.     am I missing something?   Really appreciate your help!  "
"When you fine-tune MT-DNN for GLUE, you mention applying task-specific data. I wonder if these data originate from the training set. Do you have a percentage of how much training data you will remain for the task-specific fine-tuning?    Thanks"
"For example, I need to use the pretrained BioBERT model from  . It has 3 files: pytorch_model.bin, vocab.txt, config.json. How should I convert it to your .pt model? Thanks!"
"I have run train.py for three models, bert-large, bert-base, and roberta-base, on two different multi-class classification datasets, one with 3 labels and one with 5 labels. The results for BERTs are pretty good, but the results for RoBERTa are pretty bad. By printing the confusion matrices at each epoch, it is understandable what the differences are. For instance, at epoch 2, the results for BERT base uncased for 5-class classification are:  07/05/2021 08:47:44 Task fiveClass -- epoch 2 -- Dev ACC: 84.121  07/05/2021 08:47:44 Task fiveClass -- epoch 2 -- Dev F1MAC: 81.951  07/05/2021 08:47:44 Task fiveClass -- epoch 2 -- Dev F1MIC: 84.121  07/05/2021 08:47:44 Task fiveClass-- epoch 2 -- Dev CMAT:   [[328  37   7  39  13]   [ 20 545  26  20  12]   [  4  16 118  14   5]   [ 26  15  21 405  33]   [  9   5   9  15 437]]    And the results for RoBERTA-base are:  07/05/2021 08:56:05 Task fiveClass -- epoch 3 -- Dev ACC: 47.958  07/05/2021 08:56:05 Task fiveClass -- epoch 3 -- Dev F1MAC: 35.243  07/05/2021 08:56:05 Task fiveClass -- epoch 3 -- Dev F1MIC: 47.958  07/05/2021 08:56:05 Task fiveClass -- epoch 3 -- Dev CMAT:   [[ 65 242   0  56  61]   [ 44 512   0  43  24]   [ 29  89   0  16  23]   [ 62 226   0 111 101]   [ 21  39   0  58 357]]    I have gone up to epoch 20; the results improve a little for RoBERTa but start to degrade after some epochs.   If you notice, the RoBERTa has absolute zero capability to detect label 2. I ran this experiment on the 3-class classification as well and found that there also RoBERTa cannot detect label 2 at all.    Does anybody have any solution for this?"
"Hello ,thanks for sharing of your work. I' m using MT-DNN on a custom datasets. I'm following the tutorials so I did single task learning and MTL, but I fail to do KD. I don't know if  it is necessary a processing of data or others previous steps. I 've simply used   `!python train.py --task_def tutorials/tutorial_task_def.yml --data_dir tutorials/bert_base_uncased/  --init_checkpoint=""mt_dnn_models/mt_dnn_kd_large_cased.pt"" --train_datasets MyTask --test_datasets MytTask --epochs=1 --batch_size=1`   Later, I have had an error:   `RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`.  I'm using Google Colab and I've tried differents batch size. "
"In predict.py, it's still using **use_cuda=args.cuda,**  which is no longer used in the eval_model function.    `eval_model(model, test_data,metric_meta=metric_meta,**use_cuda=args.cuda,** with_label=args.with_label)`    In inference.py    `def eval_model(model, data, metric_meta, device, with_label=True, label_mapper=None, task_type=TaskType.Classification):`"
I have a dataset for 3 tasks in csv file. How to train the model
Once I try to run train.py it gives the below error:  ImportError: cannot import name 'UnencryptedCookieSessionFactoryConfig' from 'pyramid.session' (unknown location)    I have installed everything in a new conda env with pip install -r requirements.txt but still getting the above error.    Thanks in advance.  
"Hi,  Im looking for the PDR (Posterior Differential Regularization ) implementation and couldnt find it in the repo,  Can you please advice?"
I would like to try SMART model in a jupyter notebool a'la Colab with the idea to extract sentence embeddings and use them on my own dataset. Is it possible to bootstrap that with few lines of code similar to for example :         Thank you!    
"Hi,   If I unserstand correctly, the code in link below should be ""**score_mask[i][j] = 1.0**""?        Thanks  "
"`parser.add_argument(""--local_rank"", type=int, default=-1, help=""For distributed training: local_rank"")`  I don‘t know it’s meaning and what role playing in model training，  Looking forward to your answer！  Thanks！"
"While I run the script for preproduce HNN. I got:  `Traceback (most recent call last):    File ""/content/mt-dnn/hnn/script/../src/apps/run_hnn.py"", line 588, in        main(args)    File ""/content/mt-dnn/hnn/script/../src/apps/run_hnn.py"", line 476, in main      train_model(args, device, n_gpu, model, train_data, eval_data, num_train_steps)    File ""/content/mt-dnn/hnn/script/../src/apps/run_hnn.py"", line 133, in train_model      prefix='{}-{}'.format(n_epoch, args.num_train_epochs))    File ""/content/mt-dnn/hnn/script/../src/apps/run_hnn.py"", line 266, in run_eval      sm_logits, lm_logits, en_logits, tmp_eval_loss = model(input_ids, tids, label_ids)    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 722, in _call_impl      result = self.forward(*input, **kwargs)    File ""/content/mt-dnn/hnn/src/apps/hnn_model.py"", line 360, in forward      sm_logits, sm_loss = self.sm_matcher(self.bert, sm_inputs, sm_labels)    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 722, in _call_impl      result = self.forward(*input, **kwargs)    File ""/content/mt-dnn/hnn/src/apps/hnn_model.py"", line 150, in forward      encoder_layers = bert(token_ids, type_ids, mask_ids, output_all_encoded_layers=True, return_att=return_att)    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 722, in _call_impl      result = self.forward(*input, **kwargs)    File ""/content/mt-dnn/hnn/src/apps/hnn_model.py"", line 52, in forward      embedding_output = self.embeddings(input_ids.to(torch.long), token_type_ids.to(torch.long), position_ids, input_mask)    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 722, in _call_impl      result = self.forward(*input, **kwargs)    File ""/content/mt-dnn/hnn/src/bert/modeling.py"", line 171, in forward      words_embeddings = self.word_embeddings(input_ids)    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 722, in _call_impl      result = self.forward(*input, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py"", line 126, in forward      self.norm_type, self.scale_grad_by_freq, self.sparse)    File ""/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py"", line 1814, in embedding      return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)  RuntimeError: Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select`  I ran on Google Colab with Nvidia V100, with previous tasks they all recognized the cuda device but not this one. Appreciate the help"
do you have any idea about how to train on the HotpotQA dataset?
"Adversarial Training for Commonsense Inference this paper used mt-dnn framework and I guess that you are in the same lab. It is hard and time consuming to reproduce the result without proper instruction or script. Any plans in updating MC-TACO reuslt reproducing guidance like other datasets in the project home page? Since as I noticed for now , there is no information about MC-TACO dataset."
"Hi,     Thanks for releasing the code.    In the Mean teacher of SMART, The model parameters is updated as theta = (1-beta)*theta_new + beta * theta_old. But I can't find corresponding code. Could you please point out the code?    Thanks!  Best,  Deming"
`## DOWNLOAD GLUE DATA  ## Please refer glue-baseline install requirments or other issues.  git clone    cd jiant-v1-legacy  python scripts/download_glue_data.py --data_dir $DATA_DIR --tasks all    cd ..  rm -rf jiant-v1-legacy  #########################`
"!   !   modify to loss.item() can work   `self.train_loss.update(loss.item(), batch_size) `       "
"I loaded all the previous steps, but encountered a BUG in the process data that the relevant file could not be found：  _**can not find the josn :mt-dnn-master/data/canonical_data/bert_uncased_lower/trian_mnli.json**_  The bert_uncased_lower file is not created in the process data step, which installation package is missing or other reasons?  "
I got an error when I run train.py with pair task and using SAN.  The error was caused by the following error occurred at line 247 in module/batcher.py.    ”RuntimeError: Expected object of scalar type Byte but got scalar type Bool for argument #2 ‘mask’”      Is there any reason to use ‘BoolTensor’ instead of ‘ByteTensor’ at lines 365-366 in mt_dnn/batcher.py?
"Hi, do you forget to add _truncate_seq_pair to prepro_std.py?"
"Great work, and thanks for the repo.    I cannot seem to find a pre-trained, nor fine-tuned, ALUM models. Can anyone help out?    Thanks :)"
"About the Algorithm 1 in paper,      !     - I think the Gadv is uesd to update the noise? But, i saw that Gadv don't add any noise in the code below         - And the variable adv_direct addad to a embed, which doesn't match the paper i think.  !      "
I ran docker image:     and get the following error:       The same error would happen if I installed and run train.py on my machine.
adamax很少看到有人用，bert论文中也用的adam
"Hi,   In this file glue_task_def.yml, the qnli task use PremiseAndOneHypothesis and the loss is CeCriterion. however, in the paper, section 3.1, you said that using relevance ranking is more helpful, given a query Q, we make a list of candidates, where only one of them are positive and the rest are negative. So I think we should use PremiseAndMultipleHypothesis and ranking loss for qnli task, right?"
在运行download.sh之后并没有下载SNLI数据集，我在sample_data中找到了SNLI的样例是train.tsv文件，但是特别小，就一行，所以很快train.py就运行结束了。我想问是否是我的数据集不对，那么SNLI数据集我应该去哪里找呢？如果我只看文本蕴涵的结果，也就是处理SNLI数据集的结果，那我应该怎么做呢？
"hi, adversarial training for LM is very interesting.  In you apper: ""alpha = 10 for pre-training, and alpha = 1 for fine-tuning  in all our experiments.""    So, when you apply standard finetuning, you also add VAT-objective function?  "
"I downloaded the codebase and tried to reproduce the results on a linux machine. Here is the bash script:    #!/bin/bash  #SBATCH -e slurm.err  #SBATCH -p carin-gpu --gres=gpu:1  #SBATCH --mem=30G # adjust as needed  #SBATCH -c 2 # adjust as needed  module load Python-GPU/3.6.5    prefix=""mt-dnn""  BATCH_SIZE=2  gpu=0  echo ""export CUDA_VISIBLE_DEVICES=${gpu}""  export CUDA_VISIBLE_DEVICES=${gpu}  tstr=$(date +""%FT%H%M"")    train_datasets=""mnli,rte,qqp,qnli,mrpc,sst,cola,stsb""  test_datasets=""mnli_matched,mnli_mismatched,rte""  MODEL_ROOT=""checkpoints""  BERT_PATH=""mt_dnn_models/bert_model_base_uncased.pt""  DATA_DIR=""data/canonical_data/bert_uncased_lower""    answer_opt=1  optim=""adamax""  grad_clipping=0  global_grad_clipping=1  lr=""5e-5""    model_dir=""checkpoints/${prefix}_${optim}_answer_opt${answer_opt}_gc${grad_clipping}_ggc${global_grad_clipping}_${tstr}""  log_file=""${model_dir}/log.log""  python train.py --data_dir ${DATA_DIR} --init_checkpoint ${BERT_PATH} --batch_size ${BATCH_SIZE} --output_dir ${model_dir} --log_file ${log_file} --answer_opt ${answer_opt} --optimizer ${optim} --train_datasets ${train_datasets} --test_datasets ${test_datasets} --grad_clipping ${grad_clipping} --global_grad_clipping ${global_grad_clipping} --learning_rate ${lr} --grad_accumulation_step 4 --do_lower_case      Here is the error:    CUDA 9.0  Python-GPU 3.6.5  /opt/apps/rhel7/Python-GPU-3.6.5/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.    from ._conv import register_converters as _register_converters  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.  Traceback (most recent call last):    File ""train.py"", line 360, in        main()    File ""train.py"", line 291, in main      model.update(batch_meta, batch_data)    File ""/work/ss1043/mt-dnn/mt_dnn/model.py"", line 168, in update      logits = self.mnetwork(*inputs)    File ""/opt/apps/rhel7/Python-GPU-3.6.5/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__      result = self.forward(*input, **kwargs)    File ""/work/ss1043/mt-dnn/mt_dnn/matcher.py"", line 131, in forward      logits = task_obj.train_forward(sequence_output, pooled_output, premise_mask, hyp_mask, decoder_opt, self.dropout_list[task_id], self.scoring_list[task_id])    File ""/work/ss1043/mt-dnn/tasks/__init__.py"", line 48, in train_forward      logits = task_layer(sequence_output, hyp_mem, premise_mask, hyp_mask)    File ""/opt/apps/rhel7/Python-GPU-3.6.5/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__      result = self.forward(*input, **kwargs)    File ""/work/ss1043/mt-dnn/module/san.py"", line 88, in forward      att_scores = self.attn(x, h0, x_mask)    File ""/opt/apps/rhel7/Python-GPU-3.6.5/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__      result = self.forward(*input, **kwargs)    File ""/work/ss1043/mt-dnn/module/similarity.py"", line 463, in forward      scores = self.score_func(x1, x2, mask)    File ""/opt/apps/rhel7/Python-GPU-3.6.5/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__      result = self.forward(*input, **kwargs)    File ""/work/ss1043/mt-dnn/module/similarity.py"", line 350, in forward      xWy.data.masked_fill_(x_mask.data, -float('inf'))  RuntimeError: The expanded size of the tensor (66) must match the existing size (25) at non-singleton dimension 1.  Target sizes: [2, 66].  Tensor sizes: [2, 25]  "
"I doubt that whether mt-dnn can work normally with multi-GPU？    For example (2 gpu), at the same time:     * GPU 0: train task_a  * GPU 1: train task_b    Does it work?"
"Through the paper I know that is first to pretrain a roberta(it is already done by fb), second to continue train with alum, but I wonder how to merge to code below the alum into fairseq to run the trainning code? "
How much video card memory do you need for pre-training?
"I have used :  BERT_PATH=""checkpoints/mt-dnn-rte_adamax_answer_opt1_gc0_ggc1_2020-04-26T0324/model_0.pt""  and the scores for dev set and test set are zero.    Seems that the error goes up:  04/26/2020 03:38:34 At epoch 0  04/26/2020 03:38:34 Task [ 0] updates[     1] train loss[0.56227] remaining[0:13:10]  04/26/2020 03:39:24 Task [ 5] updates[   500] train loss[0.60000] remaining[0:11:02]  04/26/2020 03:40:15 Task [ 3] updates[  1000] train loss[1.04799] remaining[0:10:12]  04/26/2020 03:41:06 Task [ 0] updates[  1500] train loss[1.98074] remaining[0:09:22]  04/26/2020 03:41:56 Task [ 2] updates[  2000] train loss[2.98191] remaining[0:08:31]  04/26/2020 03:42:47 Task [ 1] updates[  2500] train loss[3.95106] remaining[0:07:40]  04/26/2020 03:43:37 Task [ 0] updates[  3000] train loss[5.01362] remaining[0:06:49]  04/26/2020 03:44:28 Task [ 5] updates[  3500] train loss[6.07541] remaining[0:05:59]  04/26/2020 03:45:19 Task [ 2] updates[  4000] train loss[7.19914] remaining[0:05:08]  04/26/2020 03:46:09 Task [ 2] updates[  4500] train loss[8.43027] remaining[0:04:17]  04/26/2020 03:46:59 Task [ 4] updates[  5000] train loss[9.66741] remaining[0:03:27]  04/26/2020 03:47:50 Task [ 1] updates[  5500] train loss[10.88766] remaining[0:02:36]  04/26/2020 03:48:41 Task [ 1] updates[  6000] train loss[12.26786] remaining[0:01:45]  04/26/2020 03:49:31 Task [ 4] updates[  6500] train loss[nan] remaining[0:00:55]  04/26/2020 03:50:20 Task [ 3] updates[  7000] train loss[nan] remaining[0:00:04]  "
!   
What is loss and what is kd_loss ?    example:  -------------------------------------------  mnli:    data_format: PremiseAndOneHypothesis    encoder_type: BERT    dropout_p: 0.3    enable_san: true    labels:    - contradiction    - neutral    - entailment    metric_meta:    - ACC    loss: CeCriterion <============    kd_loss: MseCriterion <==============    n_class: 3    split_names:    - train    - matched_dev    - mismatched_dev    - matched_test    - mismatched_test    task_type: Classification  
You have the following as labels:    - X    - CLS    - SEP  Can you explain what - X is? Is this for padding?
"Let's say that I have two tasks of the same type, for example, two pairwise text classification.  (e.g RTE, MNLI). Will those tasks share the same task-specific module? Or all tasks even of the same type have different weights? This question is based on Figure 1."
"Hi,  you have the shared layers and the task specific layers.  Are the shared layers the layers that bert has? "
"When using the extractor.py and set the --layers to ""0,1,2,3,4,5,6,7,8,9,10,11"", the script is able to extract the embedding for all 12 layers for each token. I'm wondering if there is a way to extract the embedding for all 12 layers for all 12 heads in the model for each token. If not, I'm wondering if the current 768 dimension array for each layer is taking an average of all 12 head's embedding or using other forms of aggregation.    Also, I'm wondering if there is a script to extract the attention matrices for each input sample."
"Hi,  when will the SMART code be released ？  "
Does mt-dnn support a general question-answering task? Given a question and multiple answers?
"Specifically on this commit f444fe9109d5a9980c9d825a24576c8d873bdf33 (21 days ago), which version of mt-dnn is implemented? So that I can cite it."
I uesd this model to predict squad2.0. But I foud that it seem to throw error when the anwser is empty.  So I would like to know【 if mt-dnn model can predict the questions that have no anwsers】?
"Hi,   When I run the run_mt_dnn.sh, I had the following error.    >   >   File ""/path/mt-dnn/module/similarity.py"", line 350, in forward  >     xWy.data.masked_fill_(x_mask.data, -float('inf'))  > RuntimeError: The expanded size of the tensor (59) must match the existing size (28) at non-singleton dimension 1.  Target sizes: [16, 59].  Tensor sizes: [16, 28]    I found that premise_masks in batcher.py seemed to have wrong vector size.    `premise_masks = torch.ByteTensor(batch_size, premise_len).fill_(1)`    at line 363 in batcher.py should be    `premise_masks = torch.ByteTensor(batch_size, tok_len).fill_(1)`?  "
"Hi,  if we run for example for four epochs training, how should I evaluate the performance of the model? I have the predictions of the model at the end of each epoch, for validation and for test set. What should I report as *the* performance of the model? Should I report the best  performance of the four epochs? Should I report the test set performance for the epoch where the model has the best performance on validation test? Does the framework reports automatically the performance of the model? "
"calc_metrics fails when trying to use F1 as a metric in multiclass classification:         The equivalent to what accuracy_score does in multiclass would be:     but I'm not sure if that's the desired behaviour or it would be better to add extra metrics F1macro, F1micro, etc.  "
When trying to execute prediction.py it fails with the following error:         because the call to mt_dnn.batcher.Collater creator is still using the old required arguments (as it happened with #146)
"When running with mt_dnn_base_uncased,   `python extractor.py --do_lower_case --finput dataset.txt --foutput dataset.json --bert_model mt_dnn_models/mt_dnn_base_uncased.pt  --checkpoint mt_dnn_models/mt_dnn_base_uncased.pt`    it gets the following error:    Better speed can be achieved with apex installed from    Traceback (most recent call last):    File ""extractor.py"", line 199, in        main()    File ""extractor.py"", line 154, in main      data, is_single_sentence = process_data(args)    File ""extractor.py"", line 136, in process_data      tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)    File ""/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/tokenization.py"", line 153, in from_pretrained      tokenizer = cls(resolved_vocab_file, *inputs, **kwargs)    File ""/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/tokenization.py"", line 83, in __init__      self.vocab = load_vocab(vocab_file)    File ""/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/tokenization.py"", line 56, in load_vocab      token = reader.readline()    File ""/usr/lib/python3.6/codecs.py"", line 321, in decode      (result, consumed) = self._buffer_decode(data, self.errors, final)  UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
"During validation in each epoch, do you use the entire validation set each time, or a subset?"
"Hi, mt-dnn developers, recently, there are some new state of art pretraining model likes ROBERTA and XLM which supports more than 100 languages, do you guys have plan to publish verison 3.0 paper for MT-DNN, and add the feature which supports ROBERTA, XLM or other models which based on hugging face transformers?    And I know that the berf in mt-dnn is also based on hugging face, so it is not hard to implement that, right?    Hope to get your reply, thanks a lot."
"When I run experiments, in checkpoints there is a directory with logs as well as models.pt.  Can I save only logs but not model.pt files? "
"Hi,    Based on  `update` function in `mt_dnn/model.py`, I do not find the code snippet related to the optimization techniques introduced in the ""SMART"" paper.     Could you please point out the related code in this repo?     Thanks!"
"When running the extractor.py with the command   python extractor.py --do_lower_case --finput input_examples/pair-input.txt --foutput input_examples/pair-output.json --bert_model bert-base-uncased --checkpoint mt_dnn_models/mt_dnn_base.pt     I got Traceback (most recent call last):    File ""extractor.py"", line 248, in        main()    File ""extractor.py"", line 198, in main      collater = Collater(gpu=args.cuda, is_train=False, data_type=data_type)  TypeError: __init__() got an unexpected keyword argument 'gpu'    Please help to advise, thanks!"
"I was trying to use custom data for passage ranking by creating a task with data_format 'PremiseAndMultiHypothesis'. While running train.py with RankCeCriterion, I'm getting the following error:     I tried by reshaping/commenting the reshaping of input and target in RankCeCriterion mentioned in loss.py,        But this is triggering cuda device-side assert error with following trace     I'm using the following command to run train.py     task_def.yml         My tsv data row before running prepo_std.py looks like following     That is ""id""\t""ruids""\t""label""\t""premise""\t""hypothesis1""\t""hypothesis2""...    I also have couple of more concerns  1. What is the ruids for ranking ? (the 2nd field of data. To make it work, I'd generated a range sequence as dummy ids).  2. How to decide n_class for ranking? (It seemed to me that Ranking is modeled as pairwise binary classification, so choose n_class as 2).     Other tasks of Classification for PremiseOnly and PremiseAndOneHypothesis worked fine for me. This issue comes only during Ranking.    Please help me find a solution. I can provide any additional information required on the issue.  Thanks in advance!!    "
"1. Which function does g in Equation 5 represent?  2. In table 3, why is ST-DNN has no results on dataset QNLI(v2) ? And why is MT-DNN has results on  QNLI(v2) ?    3. Is the sample_data for qnli classification task? I didn't see any ranking sample data.   Thanks for answering the questions."
"After adding dev.json and test.json:  01/26/2020 12:41:24 Task [ 0] updates[     1] train loss[10.20193] remaining[0:00:05]  predicting 0  Traceback (most recent call last):    File ""train.py"", line 402, in        main()    File ""train.py"", line 361, in main      task_type=task_defs.task_type_map[prefix])    File ""/home/ /mt-dnn/mt_dnn/inference.py"", line 18, in eval_model      score, pred, gold = model.predict(batch_info, batch_data)    File ""/home/ /mt-dnn/mt_dnn/model.py"", line 248, in predict      score = score.data.cpu()  AttributeError: 'tuple' object has no attribute 'data'  "
"I have mlm together with other 4 tasks, but mlm does not run at all.  for example:     I do not see task [ 0]    Keep in mind that I used only train.json (for training) and if I add dev.json (and test.json) then there is another error: issue 141"
"{""id"": ""316"", ""url"": ""..."", ""title"": ""..."", ""text"": ""...""}  Which are mandatory?  "
"I trained with a dataset, and in   folder, there are saved models from each epoch.  But what I would expect is the following: use latest model_  from epoch n, as a starting point and in epoch 0 of the new training you will have the metrics that you had in epoch n. (you will continue from where you left it).  Instead of this, in epoch 0 I have worse results than ever. What is happening? Can you elaborate?  It seems like it does not use it. Like it is trained from scratch?  edit: Also, I understand that this supposed to be the fine-tuning face, training using task-specific data."
Do you have a specific mlm dataset? Will you update the download.sh?
"In `checkpoints` folder, I see that for the test set, there is no result for `metrics` field.   Compare this to dev set, which is ok e.g {""metrics"": {""ACC"": 76.69902912621359}"
???
I have an error for chunk task. Could you please check it?  Are you able to run it OK?
"Hi, in the reported results at the end of each epoch,  how can I choose f1-score instead of accuracy for a task?   "
None
Try to provide more clear support when an issue is raised.
"Hi, could you give an example of how to use the code to evaluate the pretrained model (no finetuning) on a GLUE task?"
Why is there such a big improvement?   What is the difference?
I'm interested in this model and I was wondering will the pre-trained model of smart be released (the `.pt`  file) ?
"Hey,   Great work on the SMART paper.   I have a very quick questions about numbers reported in Table 2 of the paper.    1) Is `RTE` model on the last row of table 2, initialized from `MNLI` checkpoint or just from `RoBERTa`?    2) Are the reported numbers, mean/ median / best across seeds? If so, how many seeds?    Thanks,  Naman"
"We have a billion pieces of data， so, source code put all data to memory is not work. i use dataloader + yield , every batch load data to memory, but gpu memory continue increase. i just test load data, not begin train. I try all clear memory methods（torch.cuda.empty_cache() ... ）, but not work.  This is my read data code     this  is run independently code  pytorch 1.2       !   "
smart code will be opened?
"Here the last of all_encoder_layers, is called pooled_output     but some lines before, it is called sequence_output, which I think is the right name       I believe so, due to the following       This caused some confusion to me, and still, I am not pretty sure. Can you clarify?    edit: also   case will not play for the case of    , because   is only declared in the   (not roberta).  "
"Hey guys,    I couldn't find this issue in the issues list (closed or open), hence raising a new one. Kindly pardon if it is duplicate    Issue: When i do `pip install -r requirements.txt` my installation errors out on fairseq0.8    . can you please tell me    Qn)  If anyone else had this issue while on mac osx mojave and an anaconda environment?     Qn) or if this might be something am doing wrong/ specific to my OS?    Qn) what machines do you/people train/test your code on usually? I can switch to an ubuntu based machine if needed, hence asking.    Thank you all in advance,  mithun      "
"running prepro_std.py  for sequence task, gives an error:  (also the spelling is incorrect: Seqence --> Sequence)       The missing implementation I think is in load data, in final else (implementation for data_format `Seqence` is missing):   "
"Hello,  could you guide me on how I could add a new task?  Thanks"
"Hi,  you have three folders,     where in each one you have an .yml file, containing tasks.  We can train all tasks for a yaml file, jointly. But could I train jointly a task of one folder with a task of another folder?  An example is to train     from glue_task_def.yml in glue/, with   from ner_task_def.yml in ner/  (or specifically for my case, a different sequence labeling task instead of ner)    Is this supported?  Could you guide me on that?   Thanks a lot!  "
I followed instruction for       I had the following warning:       Can you provide more feedback? Should I ignore it? Should I check something?
"I wonder if the scripts could be enhanced in order to detect hardware (gpu) and print warning that there is not enough hardware for the experiment (or ideally to suggest settings e.g batch size etc).    For example the following run lasted from 08:27:29 to 08:53:12, and then crashed.    Could you help me understand why the specific experiment needs so much gpu memory?  I used 'init_checkpoint': 'mt_dnn_models/mt_dnn_base_uncased.pt'  and 'init_checkpoint': 'mt_dnn_models/bert_model_base_uncased.pt', with the same result bellow.    Thanks a lot.     "
"Can I use instead of bert, your final model as initial point of training?"
"I think that script run_mt_dnn.sh has updated paths,  but script run_rte.sh is not       with git blame:            "
"python3 train.py --data-dir ner/ --init_checkpoint mt_dnn_models/bert_model_base_uncased.pt -train_dataset ner --test_dataset ner --task_def experiments/ner/ner_task_def.yml   Traceback (most recent call last):    File ""train.py"", line 10, in        from pytorch_pretrained_bert.modeling import BertConfig    File ""/home/user/.local/lib/python3.7/site-packages/pytorch_pretrained_bert/__init__.py"", line 7, in        from .modeling import (BertConfig, BertModel, BertForPreTraining,    File ""/home/user/.local/lib/python3.7/site-packages/pytorch_pretrained_bert/modeling.py"", line 218, in        from apex.normalization.fused_layer_norm import FusedLayerNorm as BertLayerNorm    File ""/home/user/.local/lib/python3.7/site-packages/apex/__init__.py"", line 18, in        from apex.interfaces import (ApexImplementation,    File ""/home/user/.local/lib/python3.7/site-packages/apex/interfaces.py"", line 10, in        class ApexImplementation(object):    File ""/home/user/.local/lib/python3.7/site-packages/apex/interfaces.py"", line 14, in ApexImplementation      implements(IApex)    File ""/usr/lib64/python3.7/site-packages/zope/interface/declarations.py"", line 483, in implements      raise TypeError(_ADVICE_ERROR % 'implementer')  TypeError: Class advice impossible in Python3.  Use the @implementer class decorator instead.    I have the following version of APEX  pip3 show apex  Name: apex  Version: **0.9.10.dev0**  "
"ls ner/  chunk_dev.tsv  chunk_test.tsv  chunk_train.tsv  ner_dev.tsv  ner_test.tsv  ner_train.tsv  pos_dev.tsv  pos_test.tsv  pos_train.tsv  test.txt  train.txt  valid.txt    python3 train.py --data_dir ner/ --init_checkpoint ./mt_dnn_models/bert_model_large_uncased.pt --train_dataset ner --test_dataset ner --task_def experiments/ner/ner_task_def.yml   Better speed can be achieved with apex installed from    Namespace(adam_eps=1e-06, answer_att_hidden_size=128, answer_att_type='bilinear', answer_dropout_p=0.1, answer_mem_drop_p=0.1, answer_mem_type=1, answer_merge_opt=1, answer_num_turn=5, answer_opt=0, answer_rnn_type='gru', answer_sum_att_type='bilinear', answer_weight_norm_on=False, batch_size=8, batch_size_eval=8, bert_dropout_p=0.1, bert_l2norm=0.0, cuda=True, data_dir='ner/', data_sort_on=False, dropout_p=0.1, dropout_w=0.0, dump_state_on=False, ema_gamma=0.995, ema_opt=0, embedding_opt=0, encoder_type= , epochs=5, fp16=False, fp16_opt_level='O1', freeze_layers=-1, global_grad_clipping=1.0, glue_format_on=False, grad_accumulation_step=1, grad_clipping=0, have_lr_scheduler=True, init_checkpoint='./mt_dnn_models/bert_model_large_uncased.pt', init_ratio=1, label_size='3', learning_rate=5e-05, log_file='mt-dnn-train.log', log_per_updates=500, lr_gamma=0.5, max_seq_len=512, mem_cum_type='simple', mix_opt=0, model_ckpt='checkpoints/model_0.pt', momentum=0, mtl_opt=0, multi_gpu_on=False, multi_step_lr='10,20,30', name='farmer', optimizer='adamax', output_dir='checkpoint', ratio=0, resume=False, save_per_updates=10000, save_per_updates_on=False, scheduler_type='ms', seed=2018, task_def='experiments/ner/ner_task_def.yml', tensorboard=False, tensorboard_logdir='tensorboard_logdir', test_datasets=['ner'], train_datasets=['ner'], update_bert_opt=0, vb_dropout=True, warmup=0.1, warmup_schedule='warmup_linear', weight_decay=0)  10/19/2019 12:30:52 0  10/19/2019 12:30:52 Launching the MT-DNN training  10/19/2019 12:30:52 Loading ner/ner_train.json as task 0  Traceback (most recent call last):    File ""train.py"", line 434, in        main()    File ""train.py"", line 207, in main      train_data = BatchGen(BatchGen.load(train_path, True, task_type=task_type, maxlen=args.max_seq_len),    File ""/home/ant/multitask/mt-dnn/mt_dnn/batcher.py"", line 53, in load      with open(path, 'r', encoding='utf-8') as reader:  FileNotFoundError: [Errno 2] No such file or directory: 'ner/ner_train.json'  "
Probably it means .yaml instead of .py  
" python experiments/ner/prepro.py  Traceback (most recent call last):    File ""experiments/ner/prepro.py"", line 7, in        from experiments.ner.ner_utils import load_conll_chunk, load_conll_ner, load_conll_pos, dump_rows  ImportError: cannot import name 'dump_rows' from 'experiments.ner.ner_utils' (/home/ant/multitask/mt-dnn/experiments/ner/ner_utils.py)    I tried to fix it with the below changes  --from experiments.ner.ner_utils import load_conll_chunk, load_conll_ner, load_conll_pos, dump_rows  ++from experiments.ner.ner_utils import load_conll_chunk, load_conll_ner, load_conll_pos    BUT, I think that it is just needed a specific for ner implementation in experiments/ner/ner_utils.py    "
"hello, I'm a newer. Each mini-batch dataset only coms from one task, right? If not, how to combine the different types of loss."
"(mt-dnn) ant@ant-Z97M-DS3H:~/multitask/mt-dnn$ python train.py  Better speed can be achieved with apex installed from    Namespace(adam_eps=1e-06, answer_att_hidden_size=128, answer_att_type='bilinear', answer_dropout_p=0.1, answer_mem_drop_p=0.1, answer_mem_type=1, answer_merge_opt=1, answer_num_turn=5, answer_opt=0, answer_rnn_type='gru', answer_sum_att_type='bilinear', answer_weight_norm_on=False, batch_size=8, batch_size_eval=8, bert_dropout_p=0.1, bert_l2norm=0.0, cuda=True, data_dir='data/canonical_data/mt_dnn_uncased_lower', data_sort_on=False, dropout_p=0.1, dropout_w=0.0, dump_state_on=False, ema_gamma=0.995, ema_opt=0, embedding_opt=0, encoder_type= , epochs=5, fp16=False, fp16_opt_level='O1', freeze_layers=-1, global_grad_clipping=1.0, glue_format_on=False, grad_accumulation_step=1, grad_clipping=0, have_lr_scheduler=True, init_checkpoint='mt_dnn_models/bert_model_base.pt', init_ratio=1, label_size='3', learning_rate=5e-05, log_file='mt-dnn-train.log', log_per_updates=500, lr_gamma=0.5, max_seq_len=512, mem_cum_type='simple', mix_opt=0, model_ckpt='checkpoints/model_0.pt', momentum=0, mtl_opt=0, multi_gpu_on=False, multi_step_lr='10,20,30', name='farmer', optimizer='adamax', output_dir='checkpoint', ratio=0, resume=False, save_per_updates=10000, save_per_updates_on=False, scheduler_type='ms', seed=2018, task_def='experiments/glue/glue_task_def.yml', tensorboard=False, tensorboard_logdir='tensorboard_logdir', test_datasets=['mnli_mismatched', 'mnli_matched'], train_datasets=['mnli'], update_bert_opt=0, vb_dropout=True, warmup=0.1, warmup_schedule='warmup_linear', weight_decay=0)  10/15/2019 01:45:12 0  10/15/2019 01:45:12 Launching the MT-DNN training  10/15/2019 01:45:12 Loading data/canonical_data/mt_dnn_uncased_lower/mnli_train.json as task 0  Traceback (most recent call last):    File ""train.py"", line 432, in        main()    File ""train.py"", line 207, in main      train_data = BatchGen(BatchGen.load(train_path, True, task_type=task_type, maxlen=args.max_seq_len),    File ""/home/ant/multitask/mt-dnn/mt_dnn/batcher.py"", line 53, in load      with open(path, 'r', encoding='utf-8') as reader:  **FileNotFoundError: [Errno 2] No such file or directory: 'data/canonical_data/mt_dnn_uncased_lower/mnli_train.json'**    where:  ls data  canonical_data  domain_adaptation  MRPC  QQP  SciTail  squad     SST-2  WNLI  CoLA            MNLI               QNLI  RTE  SNLI     squad_v2  STS-B  "
"Hello,  regarding the suggested command for NER  python train.py --data_dir   --init_checkpoint   --train_dataset ner --test_dataset ner --task_def experiments\ner\ner_task_def.py    1) could you provide a specific example for the command ? It is not clear what we should put here:   . To my understadning, here   where   we should put the filename.    2) When running the command, what will happen? It will train NER as single task, using the shared layers that have been trained with the multitask?    My intention is to adapt it for a different sequence labeling task, from NER, and to use multitask learning for that.    Could you please help me understand?  Thanks  "
Thanks for sharing the pretrained model!    I tried the below steps in the README.md    1. Download data and pretrained model.    2. Run Extract embeddings    (had to change to   instead of   because only the first one is downloaded.)    And it gives error      
"Hi，  I want to use MTDNN train a single bert model use ‘sgd’ optimizer，but an error occured as follows：  self.optimizer = optim.sgd(optimizer_parameters, opt['learning_rate'],  AttributeError: module 'torch.optim' has no attribute 'sgd  I want to know how to use  optimizer sgd correctly？  best regard！"
Hi Namisan    The code page said that you use 4 V100 to train the a toy example and 8 V100 to fine-tune MT-DNN.  Could you provide the rough estimate for the training time required for both cases?      Thank you!    SH
"Hi @namisan!    I came across your repository from paperswithcode.com, while looking for SOTA NLI models. I am trying to get entailment scores between a given premise and hypothesis using your pre-trained models.    Could you please provide the steps I would need to follow to achieve this? I went through your README but could not figure out how this could be done. The README seems to focus more on training our own model from scratch or on reproducing the results of your paper, but there does not seem to be any API provided for using the pre-trained model to get entailment scores.    If I have missed any part of the source code or README that might be talking about this, could you kindly point me to it? If this has already not been covered in the README or implemented at all, I would be more than happy to implement and document this API if you could just guide me on how to start.    I would be extremely grateful if you could kindly help me out in this.     Thanks!    Best,  Aditya"
"Hi,    Is it possible to release MT-DNN no-fine-tune checkpoint?    Currently released mt-dnn-large.pt only contains shared layers, lacks task-specific layers.    Thanks!"
!     Can MT-DNN  load pre-trained BERT model ? Why convert bert model to mt-dnn format ?
"Hello, Thanks for your great work! I have three questions.         Q1: Which piece of code is the Ensemble?         Q2: Is this code used for Knowledge Distillation?                 【batcher.py】                  # soft label generated by ensemble models for knowledge distillation                  if self.soft_label_on and (batch[0].get('softlabel', None) is not None):                      sortlabels = [sample['softlabel'] for sample in batch]                      sortlabels = torch.FloatTensor(sortlabels)                      batch_info['soft_label'] = self.patch(sortlabels.pin_memory()) if self.gpu else sortlabels          Q3: How is the 'softlabel' added in 'batch[0]'?    Looking forward to your reply. Thanks very much."
"Hi,  Congratulations on your amazing work.    I wanted to fine tune the model on a custom dataset. Can you please provide instructions on how to train the model on custom dataset.    Thank you."
I downloaded the data using the download script and ran the prepro.sh script as given in the readme and it runs into the following error:     
"Hi all,  when I run  run_stsb.sh, an error occurred.  Does anybody have the same issue?  !   "
"In training model, when I use `python train.py --init_checkpoint='mt_dnn_models/bert_model_base_chinese.pt'`,   Some error occur: `RuntimeError: CUDA error: device-side assert triggered`    However, when I replace this hyper-parameter with, for example, `bert_model_base_uncased.pt; bert_model_base_uncased.pt`, it can train normally.    But, it is obviously not what I want, I train it for a simple four-class classification task in chinese dataset, and want to use `bert_model_base_chinese.py` you provided.    Note that:  -  all model mentioned above are download by `download.sh`  - I have check out this   previously in pytorch repo, also it could not give me any helps."
How can I use this code do multi-task text classification?
Is it possible to convert pt file to tf bert model(or pytorch_model.bin) and how? Thanks!
"I have applied the model to my own dataset, and trying to explore using hinge loss instead of cross entropy loss for doing binary classification (to determine is the answer is a suitable answer for my question).    However, am not too sure i am doing this correctly.      I have changed this line in model.py:  loss = F.cross_entropy(logits, y)    to:  loss = my_hinge_loss(logits, y.reshape(y.shape   !      "
Thanks for your work.  I have a question:  What is the optimizer and the learning rate for fine-tuning the single task after mt-dnn?
"Thanks for your great work!  Actually, I am curious about how to define the value of factor for each task, could you share some experience about it ?  Thank you !  "
"I am trying to run task 1. Now science I removed the data and json file related to other tasks, I am getting an error for missing file. But I don't want to run the whole task as I am very specific just for one task. Please help me."
"While running the train.py, I encountered this error. Apex 0.1 is installed."
"Hi,   I'm tring to reproduce the reported result. Since we don't have many large RAM GPUs, adding gradient accumulation steps would be of great help. Should the standard gradient accumulation version be modified to adapt to multi-task setting?     Thanks.  "
"While printing the values of test_predictions in stsb task I was getting 0 for all cases. Then I printed the scores. I thought the scores were from 0 to 5 where 5 representing most similar but when I printed them I found that some of the similarity scores were negative. Can you clarify what is the range of similarity score.  Another problem I am getting is that the test results change if I run the model again( I am not training the model, I am just predicting the test set). I thought it was due to the dropout, so I made the dropout 0 but I am still getting different results every time I ran it.  I even saw that you have used self.network.eval() which makes the network in testing mode thus even if there is any dropout layer then it ignores it."
Pytorch-0.4.1 cannot be installed with cuda-10. How can I proceed? As it is giving an error.  please help. Thank you
"Is there any way to load the model after training and classify based on the task-specific dense layers (I assume for each task a specific dense layer is learned for classification) or are these not stored with the model.pt file and lost? E.g. if one wants to predict classes for samples of an unseen dataset on the, let's say, specific MNLI dense layer."
"I wonder why adding the number of gpus will increase the remaining training time under the same batch_size condition.  !   The picture above shows the situation of a single GPU.When I used 4 v100s to train the network, the time increased to two hours."
"Better speed can be achieved with apex installed from    Namespace(answer_att_hidden_size=128, answer_att_type='bilinear', answer_dropout_p=0.1, answer_mem_drop_p=0.1, answer_mem_type=1, answer_merge_opt=1, answer_num_turn=5, answer_opt=0, answer_rnn_type='gru', answer_sum_att_type='bilinear', answer_weight_norm_on=False, batch_size=8, batch_size_eval=8, bert_dropout_p=0.1, bert_l2norm=0.0, cuda=True, data_dir='data/mt_dnn', data_sort_on=False, dropout_p=0.1, dropout_w=0.0, dump_state_on=False, ema_gamma=0.995, ema_opt=0, embedding_opt=0, epochs=5, freeze_layers=-1, global_grad_clipping=1.0, grad_clipping=0, have_lr_scheduler=True, init_checkpoint='mt_dnn_models/bert_model_base.pt', init_ratio=1, label_size='3', learning_rate=5e-05, log_file='mt-dnn-train.log', log_per_updates=500, lr_gamma=0.5, max_seq_len=512, mem_cum_type='simple', mix_opt=0, momentum=0, mtl_opt=0, multi_gpu_on=False, multi_step_lr='10,20,30', name='farmer', optimizer='adamax', output_dir='checkpoint', pw_tasks=['qnnli'], ratio=0, scheduler_type='ms', seed=2018, task_config_path='configs/tasks_config.json', test_datasets=['mnli_mismatched', 'mnli_matched'], train_datasets=['mnli'], update_bert_opt=0, vb_dropout=True, warmup=0.1, warmup_schedule='warmup_linear', weight_decay=0)  07/01/2019 01:37:37 0  07/01/2019 01:37:37 Launching the MT-DNN training  07/01/2019 01:37:37 Loading data/mt_dnn/mnli_train.json as task 0  Traceback (most recent call last):    File ""train.py"", line 350, in        main()    File ""train.py"", line 178, in main      train_data = BatchGen(BatchGen.load(train_path, True, pairwise=pw_task, maxlen=args.max_seq_len),    File ""/home/ubuntu/paraphrase/mt-dnn/mt_dnn/batcher.py"", line 55, in load      with open(path, 'r', encoding='utf-8') as reader:  FileNotFoundError: [Errno 2] No such file or directory: 'data/mt_dnn/mnli_train.json'        I use the tutorial to train, what does this error mean? What does the Json file contain?"
"Hello!  I'd like to know more of the details of ensembling multiple MT-DNN models.  Specifically for the STS-B task, could you describe the steps you took to get the ensemble? Given the large model sizes, did you load them to the GPUs sequentially and just saved the individual model outputs to be averaged later? I'd appreciate your response! "
"If I first finetune BERT in task_1, then finetune the same BERT in task_2, and final get the finetuend BERT in task_1 & task_2. Is it same as mt-dnn in task_1 & task_2?"
"As per the current `scripts/run_rte.sh`, you are giving `answer_opt=0` for `rte`. I am curious what was the reason of throwing away the `SAN` module for finetuning of the pair tasks and going back to using `pooler`.    1) Is this what you use for glue submission or you use SAN for finetuning for submission?  2) If not, do you know the perf difference?"
"When the procedure is interrupted accidentally. I want to resume the training procedure with the checkpoint in `checkpoints/model_*.pt`. But I don't know why it comes out some error:    Traceback (most recent call last):    File ""../train.py"", line 352, in        main()    File ""../train.py"", line 314, in main      model.update(batch_meta, batch_data)    File ""/home/mt-dnn/mt_dnn/model.py"", line 171, in update      self.optimizer.step()    File ""/home/mt-dnn/module/bert_optim.py"", line 120, in step      exp_avg.mul_(beta1).add_(1 - beta1, grad)  RuntimeError: Expected object of type torch.FloatTensor but found type torch.cuda.FloatTensor for argument #4 'other'    I didn't change any other code but just hyperparameters. How to deal with such a problem?"
wget      Note that it is converted from Google's BERT model:      Hi namisan. How do you convert bert model to yours ? I am very insteresting in that. Can you share your covert method. Thank you very much.
"Hi, I have couple of questions about the test set numbers, can you please clarify:    1) What's the test set number of original single MT-DNN model (non KD)? The paper says it's `82.7` vs leaderboard says `85.1`? The difference seems more than just WNLI.    2) What's the difference between MTDNN-KD vs MTDNN-ensemble in Table 3 in   paper.   "
What are the changes we have to make in the tran.py file  in order to train for specific purpose(STSB). Some of the changes I have made are following:-  1.  train_dataset=stsb  2.  test_dataset=stsb  Apart from that is there anything else I need to change because right now I am getting all predictions as 0.  Even the run_stsb.sh is giving all zero prediction instead of predicting similarity score between 0 and 5.  Please help!!!!
"Hello,    Do you have a rough timeline for when you might release:  1. a tutorial for using pre-trained MT-DNN models,  2. code/models for MT-DNN with knowledge distillation?    Thank you!"
None
"Dear,    while a nn.ModuleList() is used for the scoring_list, this is not the case for the dropout_list:         This seems to have an impact when switching the network mode in class MTDNNModel from training to evaluation. The scoring_list appears to be correctly switched to evaluation mode while this is not happening for the dropout_list. As a consequence, dropout is active at prediction time.    Jan Luts"
None
"Hi,    In Pytorch bert I used to put txt file and run the extract_features and embeddings,py file...how should I use this for the same.    Abhinandan"
What is the preprocessing/changes we need to do in order to get the prediction on NLI task for any two user defined input statements??
"Hi,    I am now working on a sentiment analysis task and found that this architecture might be a good choice for my application.    However, I am quite confused on how to use this architecture on my custom data. More specifically,  if I would like to **directly** apply this architecture to my custom data. What I should do? Is there a more detailed tutorial for the use of the architecture?    "
I just noticed that the performance on WNLI have got a huge improvement! That's really marvelous. Can't wait to know more details about it. 
"I'm on commit 25c3bd1cfee2db14c7e8dbec4a369acaa95b15f6    05/27/2019 03:38:53 Loaded 5463 QNLI test samples  Traceback (most recent call last):    File ""prepro.py"", line 352, in        main(args)    File ""prepro.py"", line 193, in main      qnnli_train_data = load_qnnli(qnli_train_path, GLOBAL_MAP['qnli'])    File ""/home/markus/mt-dnn/data_utils/glue_utils.py"", line 113, in load_qnnli      assert len(lines) % 2 == 0  AssertionError"
"On the glue leaderboard, the QNLI performance can get 96.0. But when I train the network, the dev accuracy can only get 92.2. I don't have V100 GPU. The training setting is batch size 8 on 8 TITAN X. Can you provide some details when you train the network on QNLI dataset?"
"I had tried to do multitask learning on original BERT for seven Chinese tasks. I tried to feed data with task sampling and data sampling. The results are somehow confusing. When feeding model using task sampling strategy just like MTDNN, the mtl trained model can't achieve better results than single-task-training, But when i apply data sampling, mtl trained model can achieve better results than BERT, EIRNE on lcqmc, chnsenticorp and xnli. Is there any theoritical guidance for mtl on different datasets"
"I tried to finetune the pretrained mt-dnn model on a specific task, but the GPU memory usage keeps growing. After training for 1000 iterations, the memory is fully occupied, causing 'run out of memory' error. The experiment was conducted on 1080TI. "
"I'm trying to finetune this model on a specific task. But when I run the example scripts of run_rte.sh, the multi-gpu training is not working when I set CUDA_VISIBLE_DEVICES=""0,1,2,3"", it seems that only the first gpu is working.    I have only 4 1080 ti gpus, when I run the run_rte.sh script, I can only adjust the batch size to 1 to avoid out of memory, can this be possible training on 1080Ti?"
"<img width=""1145"" alt=""image"" src=""   "
"Hi,    Thanks for the repo and your model. Thanks for your time.     So I have an intro question about the model.     So this model takes a pretrained BERT model and then jointly fine tunes it on 4 tasks: single sentence classification, pairwise text similarity, pairwise text classification and pairwise ranking.  For pairwise text classification it has an additional stochastic answer network attached to the head but for the other three tasks it just attaches the specific loss function for the three tasks.    The whole network is trained end to end.    Is this correct?    Thanks for the help and God bless!    "
"Looking forward to inclusion of an open source license, otherwise the default copyrights apply which doesn't allow reproduction."
求解答
Will there be a tensorflow verison of mt-dnn?
"Thanks for your work and we have a problem with the stochastic prediction dropout.     When call this function as above, the element in the matrix will be greater than one, I want to known why multiply 'Variable(1.0/(1 - dropout_p)'. It will cause the element greater than one."
"Hello,    I have trained the mt-dnn, initialized with mt_dnn_large.pt, with some new input and with the following parameters     > `{'log_file': 'checkpoints/mt-dnn-NL-labels_adamax_answer_opt1_gc0_ggc1_2019-04-15T2138/log.log', 'init_checkpoint': '../mt_dnn_models/mt_dnn_large.pt', 'data_dir': '../data/mt_dnn', 'data_sort_on': False, 'name': 'farmer', 'train_datasets': ['mnli', 'rte', 'qnli'], 'test_datasets': ['mnli_matched', 'mnli_mismatched', 'rte'], 'pw_tasks': ['qnnli'], 'update_bert_opt': 0, 'multi_gpu_on': True, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': [1, 1, 1], 'label_size': '3,2,2', 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 512, 'init_ratio': 1, 'cuda': True, 'log_per_updates': 500, 'epochs': 5, 'batch_size': 16, 'batch_size_eval': 8, 'optimizer': 'adamax', 'grad_clipping': 0.0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 5e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'ema_opt': 0, 'ema_gamma': 0.995, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'freeze_layers': -1, 'embedding_opt': 0, 'lr_gamma': 0.5, 'bert_l2norm': 0.0, 'scheduler_type': 'ms', 'output_dir': 'checkpoints/mt-dnn-NL-labels_adamax_answer_opt1_gc0_ggc1_2019-04-15T2138', 'seed': 2018, 'task_config_path': 'configs/tasks_config.json', 'tasks_dropout_p': [0.1, 0.1, 0.1]}``    I train this model for e.g., 4 epochs and I want to perform the fine tuning and domain adaptation experiments based on the resulting model, ""checkpoint/model_4.pt"". Therefore, in run_rte.sh or snli_domain_adaptation_bash.sh I replace the initial checkpoint from ../mt_dnn_models/mt_dnn_base.pt to checkpoint/model_4.pt.    Doing so, I get the following error         > export CUDA_VISIBLE_DEVICES=4,5  Better speed can be achieved with apex installed from    Namespace(answer_att_hidden_size=128, answer_att_type='bilinear', answer_dropout_p=0.1, answer_mem_drop_p=0.1, answer_mem_type=1, answer_merge_opt=1, answer_num_turn=5, answer_opt=0, answer_rnn_type='gru', answer_sum_att_type='bilinear', answer_weight_norm_on=False, batch_size=32, batch_size_eval=8, bert_dropout_p=0.1, bert_l2norm=0.0, cuda=True, data_dir='../data/mt_dnn/', data_sort_on=False, dropout_p=0.1, dropout_w=0.0, dump_state_on=False, ema_gamma=0.995, ema_opt=0, embedding_opt=0, epochs=5, freeze_layers=-1, global_grad_clipping=1.0, grad_clipping=0.0, have_lr_scheduler=True, init_checkpoint='checkpoints/mt-dnn-NL-labels_adamax_answer_opt1_gc0_ggc1_2019-04-15T2138/model_4.pt', init_ratio=1, label_size='3', learning_rate=2e-05, log_file='checkpoints/mt-dnn-rte_adamax_answer_opt0_gc0_ggc1_2019-04-16T1636/log.log', log_per_updates=500, lr_gamma=0.5, max_seq_len=512, mem_cum_type='simple', mix_opt=0, momentum=0, mtl_opt=0, multi_gpu_on=False, multi_step_lr='10,20,30', name='farmer', optimizer='adamax', output_dir='checkpoints/mt-dnn-rte_adamax_answer_opt0_gc0_ggc1_2019-04-16T1636', pw_tasks=['qnnli'], ratio=0, scheduler_type='ms', seed=2018, task_config_path='configs/tasks_config.json', test_datasets=['rte'], train_datasets=['rte'], update_bert_opt=0, vb_dropout=True, warmup=0.1, warmup_schedule='warmup_linear', weight_decay=0)  04/16/2019 04:36:15 0  04/16/2019 04:36:15 Launching the MT-DNN training  04/16/2019 04:36:15 Loading ../data/mt_dnn/rte_train.json as task 0  Loaded 2490 samples out of 2490  04/16/2019 04:36:15 2  Loaded 277 samples out of 277  Loaded 3000 samples out of 3000  04/16/2019 04:36:15 ####################  04/16/2019 04:36:15 {'log_file': 'checkpoints/mt-dnn-rte_adamax_answer_opt0_gc0_ggc1_2019-04-16T1636/log.log', 'init_checkpoint': 'checkpoints/mt-dnn-NL-labels_adamax_answer_opt1_gc0_ggc1_2019-04-15T2138/model_4.pt', 'data_dir': '../data/mt_dnn/', 'data_sort_on': False, 'name': 'farmer', 'train_datasets': ['rte'], 'test_datasets': ['rte'], 'pw_tasks': ['qnnli'], 'update_bert_opt': 0, 'multi_gpu_on': False, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': [0], 'label_size': '2', 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 512, 'init_ratio': 1, 'cuda': True, 'log_per_updates': 500, 'epochs': 5, 'batch_size': 32, 'batch_size_eval': 8, 'optimizer': 'adamax', 'grad_clipping': 0.0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 2e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'ema_opt': 0, 'ema_gamma': 0.995, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'freeze_layers': -1, 'embedding_opt': 0, 'lr_gamma': 0.5, 'bert_l2norm': 0.0, 'scheduler_type': 'ms', 'output_dir': 'checkpoints/mt-dnn-rte_adamax_answer_opt0_gc0_ggc1_2019-04-16T1636', 'seed': 2018, 'task_config_path': 'configs/tasks_config.json', 'tasks_dropout_p': [0.1]}  04/16/2019 04:36:15 ####################  04/16/2019 04:36:35   ############# Model Arch of MT-DNN #############  SANBertNetwork(    (bert): BertModel(      (embeddings): BertEmbeddings(        (word_embeddings): Embedding(30522, 1024)        (position_embeddings): Embedding(512, 1024)        (token_type_embeddings): Embedding(2, 1024)        (LayerNorm): BertLayerNorm()        (dropout): Dropout(p=0.1)      )      (encoder): BertEncoder(        (layer): ModuleList(          (0): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )          (1): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )          (2): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )          (3): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )          (4): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )          (5): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )          (6): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )          (7): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )          (8): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )          (9): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )          (10): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )          (11): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )          (12): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )          (13): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )          (14): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )          (15): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )          (16): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )          (17): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )          (18): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )          (19): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )          (20): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )          (21): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )          (22): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )          (23): BertLayer(            (attention): BertAttention(              (self): BertSelfAttention(                (query): Linear(in_features=1024, out_features=1024, bias=True)                (key): Linear(in_features=1024, out_features=1024, bias=True)                (value): Linear(in_features=1024, out_features=1024, bias=True)                (dropout): Dropout(p=0.1)              )              (output): BertSelfOutput(                (dense): Linear(in_features=1024, out_features=1024, bias=True)                (LayerNorm): BertLayerNorm()                (dropout): Dropout(p=0.1)              )            )            (intermediate): BertIntermediate(              (dense): Linear(in_features=1024, out_features=4096, bias=True)            )            (output): BertOutput(              (dense): Linear(in_features=4096, out_features=1024, bias=True)              (LayerNorm): BertLayerNorm()              (dropout): Dropout(p=0.1)            )          )        )      )      (pooler): BertPooler(        (dense): Linear(in_features=1024, out_features=1024, bias=True)        (activation): Tanh()      )    )    (scoring_list): ModuleList(      (0): SANClassifier(        (dropout): DropoutWrapper()        (query_wsum): SelfAttnWrapper(          (att): LinearSelfAttn(            (linear): Linear(in_features=1024, out_features=1, bias=True)            (dropout): DropoutWrapper()          )        )        (attn): FlatSimilarityWrapper(          (att_dropout): DropoutWrapper()          (score_func): BilinearFlatSim(            (linear): Linear(in_features=1024, out_features=1024, bias=True)            (dropout): DropoutWrapper()          )        )        (rnn): GRUCell(1024, 1024)        (classifier): Classifier(          (dropout): DropoutWrapper()          (proj): Linear(in_features=4096, out_features=3, bias=True)        )      )      (1): SANClassifier(        (dropout): DropoutWrapper()        (query_wsum): SelfAttnWrapper(          (att): LinearSelfAttn(            (linear): Linear(in_features=1024, out_features=1, bias=True)            (dropout): DropoutWrapper()          )        )        (attn): FlatSimilarityWrapper(          (att_dropout): DropoutWrapper()          (score_func): BilinearFlatSim(            (linear): Linear(in_features=1024, out_features=1024, bias=True)            (dropout): DropoutWrapper()          )        )        (rnn): GRUCell(1024, 1024)        (classifier): Classifier(          (dropout): DropoutWrapper()          (proj): Linear(in_features=4096, out_features=2, bias=True)        )      )      (2): SANClassifier(        (dropout): DropoutWrapper()        (query_wsum): SelfAttnWrapper(          (att): LinearSelfAttn(            (linear): Linear(in_features=1024, out_features=1, bias=True)            (dropout): DropoutWrapper()          )        )        (attn): FlatSimilarityWrapper(          (att_dropout): DropoutWrapper()          (score_func): BilinearFlatSim(            (linear): Linear(in_features=1024, out_features=1024, bias=True)            (dropout): DropoutWrapper()          )        )        (rnn): GRUCell(1024, 1024)        (classifier): Classifier(          (dropout): DropoutWrapper()          (proj): Linear(in_features=4096, out_features=2, bias=True)        )      )    )  )    04/16/2019 04:36:35 Total number of params: 357215242    04/16/2019 04:36:36 At epoch 0  Traceback (most recent call last):    File ""../train.py"", line 354, in        main()    File ""../train.py"", line 316, in main      model.update(batch_meta, batch_data)    File ""../mt_dnn/model.py"", line 151, in update      self.optimizer.step()    File ""../module/bert_optim.py"", line 119, in step      exp_avg.mul_(beta1).add_(1 - beta1, grad)  RuntimeError: expected type torch.FloatTensor but got torch.cuda.FloatTensor      May I ask you what is the problem?  I can run either of these scripts using ../mt_dnn_models/mt_dnn_large.pt but not using the model that  trained myself, i.e., model_4.pt  "
你好  想请教你们两个问题。      1. multi-task带来的促进应该和task之间的联系有关，什么时候task能够带来好的提升，以至于什么时候task反而会影响对方的表现？有没有一些理论的分析呢？      2.文章中的多个task的objective function，是否考虑过给他们设置不同的权重呢，比如最终的loss = 权重1*loss1+权重2*loss2....  
"Thanks for your work and we have a problem with the reproduce. We use the released pretrained model ""mt_dnn_large.pt"" and directly run ""run_stsb.sh"", ""run_rte"" and so on, however, we get low accuracy. I want to know is there any problem with the running procedure or the released model? Or something else such as the number of GPU and bach size?"
"Hi,     Is there any online demo interface for mt-dnn? I am working on a task similar to textual entailment and I would like to see how mt-dnn performs on a very small subset of my dataset?"
"If ""ratio"" argument is provided, then in each epoch only `random_picks=min(len(MNLI task batches) * args.ratio, len(all other tasks batches))` from `len(all other tasks batches)` are used, instead of using all other tasks batches. Such behavior seems strange to me, considering the fact that MNLI has most examples throughout the GLUE dataset. Could you please explain, what is the benefit of not using some of the examples from smaller datasets on each epoch?  Thank you very much."
"`prepro.py` uses the tokenizer for `bert-base-uncased`. Then if we use a large model and/or an uncased model, would there be inconsistencies?"
"When mtl_opt is on (by default), do all GLUE tasks with the same n_class share the same task-specific classifier? Intuitively this seems a little strange that as distinct tasks as SST and QQP use the same classifier.  Another question is, answer_opt is set to 0 in all scripts, so when does SAN apply?  Thank you."
"I specified `--multi_gpu_on` and set `CUDA_VISIBLE_DEVICES` to multiple devices, but it looks like only one GPU is used? Why is that?"
"Looks like by default SciTail does not use SAN, because it uses the default value for `--answer_opt`, which is 0. Why is that?"
"Is the downloaded `mt_dnn_base.pt` the same as the resulting model after running `scripts/run_mt_dnn.sh`? If not, what is their relationship?"
None
"Hi, I am having trouble reproducing the result on the QNLI dataset. How did you choose the hyper-parameters?"
"In the arxiv paper it is stated:  > In the multi-task fine-tuning stage, we use minibatch based stochastic gradient descent (SGD) to  learn the parameters of our model (i.e., the parameters of all shared layers and task-specific layers) as shown in Algorithm 1. In each epoch, a  mini-batch b_t  is selected(e.g., among all 9 GLUE  tasks), and the model is updated according to the  task-specific objective for the task t. This approximately optimizes the sum of all multi-task objectives.    If I understand it correctly, in your code this multi-task fine-tuning stage is called `MTL refinement`. Then why do you fine-tune for each task in single task setting in your `fine-tuning` stage? There is no such stage in the original paper.  Also, in `run_mt_dnn.sh` there are lines:  `train_datasets=""mnli,rte,qqp,qnli,mrpc,sst,cola,stsb""  test_datasets=""mnli_matched,mnli_mismatched,rte""`  Why do you only test on `mnli` and `rte` and not test on all other tasks?  I would also like to ask if I can switch from `BERT large` to `BERT base` there because i only have one 1080 GTX card.    Thank you.  "
"In the line 45 of mt_dnn\model.py,  `self.optimizer = optim.sgd(parameters, opt['learning_rate'],`    `parameters `should be `optimizer_parameters`?"
`  !   `
None
"New one is:      The current one is deprecated, and would fail to download some of the datasets."
"root@n6qc287nfs:/notebooks# python ./xling-eval/code/map.py -m b -d --lang_src English --lang_trg Arabic ./English_embeds ./English_vocab ./Arabic_embeds ./Arabic_vocab ./shared_vec_space  usage: map.py [-h] [-m MODEL] [-d TRANS_DICT] [--lang_src LANG_SRC] [--lang_trg LANG_TRG] embs_src vocab_src embs_trg vocab_trg output  map.py: error: argument -d/--trans_dict: expected one argument  root@n6qc287nfs:/notebooks# python ./xling-eval/code/map.py -m b --lang_src English --lang_trg Arabic ./English_embeds ./English_vocab ./Arabic_embeds ./Arabic_vocab ./shared_vec_space  Loading source embeddings and vocabulary...  Loading target embeddings and vocabulary...  Loading translation dictionary...  Traceback (most recent call last):    File ""/notebooks/./xling-eval/code/map.py"", line 60, in        trans_dict = [x.lower().split(""\t"") for x in util.load_lines(args.trans_dict)]    File ""/notebooks/xling-eval/code/util.py"", line 8, in load_lines      return [l.strip() for l in list(codecs.open(path, ""r"", encoding = 'utf8', errors = 'replace').readlines())]    File ""/usr/lib/python3.9/codecs.py"", line 905, in open      file = builtins.open(filename, mode, buffering)  TypeError: expected str, bytes or os.PathLike object, not NoneType  root@n6qc287nfs:/notebooks# "
Solved!
We get a 404 (Not Found) error when trying to get your data file from the Dropbox link:         Could you please tell us if you are planning to fix this link? We use it from Hugging Face Datasets library.
"Hello,    First of all, thank you for creating and sharing your datasets with the community!    As far as I can tell, you provide 176 WikiANN datasets in your Amazon Cloud drive and I was wondering why this isn't the 282 quoted in the Pan et al paper?     Also, for some experiments (e.g. on Google Colab) it would be very convenient to be able to download the datasets using curl / wget, but this doesn't seem to be possible with Amazon Drive - do you happen to know if there's a way to download the datasets from the terminal?    Thanks! "
"Hi,    I already have the target sentences and annotated labels.  I just want to change my file format to run BEA, which means I can't just run:  `python main.py  -m uaggexport -dir_input mydata -dir_output outputdata`  So my problem is what format should my data be to run the code above. Or is there another way to change my file to the format of the raw data which you provide in the bea_code folder?  BTW, my file format is,  WORD SOURCE1 SOURCE2 SOURCE3 GOLD  Bill O B-PER B-PER B-PER  Gates O I-PER I-PER I-PER"
"> first pretrain on top k (here 10) annotations (sorted by f1 results in taglangtag.json) and then fine-tune on lowres gold data from the target language.    Is this taglangtag.json missing? I would like to see the ranking for the languages, especially for the Germanic language family"
"In the readme it says, ""In addition, you can also evaluate the model in the same settings as in our paper using the evaluate.py script."" -- note that there is no evaluate.py script in this repo."
"While training monoses I got an error in Step 7 which is------    > Traceback (most recent call last):  >   File ""/home/xyz/monoses/training/tuning/tune.py"", line 335, in    >     main()  >   File ""/home/xyz/monoses/training/tuning/tune.py"", line 322, in main  >     extract_zmert_params(tmp + '/dcfg.txt.ZMERT.final'))  >   File ""/home/xyz/monoses/training/tuning/tune.py"", line 73, in extract_zmert_params  >     with open(path, encoding='utf-8', errors='surrogateescape') as f:  > FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpv1m8y_i1/dcfg.txt.ZMERT.final'  > clean-corpus.perl: processing /home/xyz/models/monoses/src-tgt/tmpzbtcque6/train.bt & .trg to /home/xyz/models/monoses/src-tgt/tmpzbtcque6/train-supervised/clean, cutoff 3-80, ratio 9      From the log file and intermediate results, I find out that     - It successfully generated phrase tables.   - However, in step 6 it filtered 0% which I suspect.     > P(f|e) filter limit: 100  > Filtering using P(e|f) only. n=100  >   > ..................................................[n:500000]  > ..................................................[n:1000000]  > ..................................................[n:1500000]  > ..................................................[n:2000000]  > ..................................................[n:2500000]  > ..................................................[n:3000000]  > ..................................................[n:3500000]  > ..................................................[n:4000000]  > ..................................................[n:4500000]  > ..................................................[n:5000000]  > ..................................................[n:5500000]  > ..................................................[n:6000000]  > ..................................................[n:6500000]  > ..................................................[n:7000000]  > ..................................................[n:7500000]  > ..................................................[n:8000000]  > ..................................................[n:8500000]  > ..................................................[n:9000000]  > ..................................................[n:9500000]  > ..................................................[n:10000000]  >  >  unfiltered phrases pairs: 10000000  >   >      P(f|e) filter [first]: 0   (0%)  >        significance filter: 0   (0%)  >             TOTAL FILTERED: 0   (0%)  >  >     FILTERED phrase pairs: 10000000   (100%)    - Then, in Step 7 while running decoder, it printed -    > Call to decoder returned 1; was expecting 0.  >  Z-MERT exiting prematurely (MertCore returned 30)...    "
"Hi, I'm interested in your paper Bilingual Lexicon Induction through Unsupervised Machine Translation. I understand that the paper is based on the code in this repository, but I think there might be a minor difference. In the paper, frequent n-grams are assigned the centroid of the words they contain, whereas the SMT paper uses phrase2Vec, correct? Is there a difference in performance if phrase2Vec is used for BLI? "
"I got this warning for both kazakh and turkish with monoses, WARNING: No known abbreviations for language 'kz', attempting fall-back to English version..., this because Moses does not support kazakh and trukish,  here I did not know how to ignore tokenizer, please could you point me?"
"Hi,  Sorry to trouble you, but I had this error:    ERROR: compile contrib/sigtest-filter at /home/zhang/data-short/monoses/third-party/moses/scripts/generic/binarize4moses2.perl line 34.  ERROR: compile contrib/sigtest-filter at /home/zhang/data-short/monoses/third-party/moses/scripts/generic/binarize4moses2.perl line 34.  Using SCRIPTS_ROOTDIR: /home/zhang/data-short/monoses/third-party/moses/scripts  Not executable: /home/zhang/data-short/monoses/third-party/moses/bin/moses2 at /home/zhang/data-short/monoses/third-party/moses/scripts/training/mert-moses.pl line 466.  Traceback (most recent call last):    File ""/home/zhang/.pyenv/versions/3.7.2/lib/python3.7/shutil.py"", line 563, in move      os.rename(src, real_dst)  FileNotFoundError: [Errno 2] No such file or directory: 'cj-word-smt/tmpou08sq8q/mert/moses.ini' -> 'cj-word-smt/step7/src2trg.it1.moses.ini'    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""train.py"", line 484, in        main()    File ""train.py"", line 478, in main      unsupervised_tuning(args)    File ""train.py"", line 366, in unsupervised_tuning      shutil.move(args.tmp + '/mert/moses.ini', config[(src, trg)])    File ""/home/zhang/.pyenv/versions/3.7.2/lib/python3.7/shutil.py"", line 577, in move      copy_function(src, real_dst)    File ""/home/zhang/.pyenv/versions/3.7.2/lib/python3.7/shutil.py"", line 263, in copy2      copyfile(src, dst, follow_symlinks=follow_symlinks)    File ""/home/zhang/.pyenv/versions/3.7.2/lib/python3.7/shutil.py"", line 120, in copyfile      with open(src, 'rb') as fsrc:  FileNotFoundError: [Errno 2] No such file or directory: 'cj-word-smt/tmpou08sq8q/mert/moses.ini'    Is that the moses2 didn't been compiled?"
"Thank you for your sharing.I trained to the fifth step and there was a problem.    Traceback (most recent call last):    File ""monoses/training/induce-phrase-table.py"", line 153, in        main()    File ""monoses/training/induce-phrase-table.py"", line 140, in main      .format((epoch + j/n)/args.epochs, t.detach().cpu().numpy(), loss.detach().cpu().numpy()),  TypeError: unsupported format string passed to numpy.ndarray.__format__  sort: cannot read: MODEL-DIR/tmpvkx1vuhd/src2trg.phrase-table: No such file or directory"
"Hey everyone,  I am trying to reproduce your result but can't get past step 7. I am using the WMT English-Polish language but facing error in step 8.  Attaching my error.  !     Thank u in advance..."
I am getting this error while making phrase tables.  I tried resolving this error by installing cmph2.0.2 and xmlrpc-c-1.39.12 but couldn't make it.  !   
"hello,    I did not know how to compile Phrase2vec, this site    you pointed, but there is any instruction  how to install it, so because that I got these bash: /media/adminonur/bee/monoses/third-party/phrase2vec/word2vec: No such file or directory  head: cannot open 'MODEL-DIR/tmpk7yaqfp0/vocab-full.txt' for reading: No such file or directory  bash: /media/adminonur/bee/monoses/third-party/phrase2vec/word2vec: No such file or directory  bash: /media/adminonur/bee/monoses/third-party/phrase2vec/word2vec: No such file or directory  head: cannot open 'MODEL-DIR/tmpk7yaqfp0/vocab-full.txt' for reading: No such file or directory  bash: /media/adminonur/bee/monoses/third-party/phrase2vec/word2vec: No such file or directory    please could you point me?"
"Your system is very good, I am learning to use it, but now I have a problem! In step 7 unspervised_tuning, I can't find the moses2 file in the bin directory. Is the moses2  in train.py  is the bin/moses in the updated moses system? If not, can you send me a moses2 file?"
"Hi, I got this error during running train.py         It seems a divide by zero error during the mapping step. Any ideas why this happen?    Thank you!"
"Hi, thanks for your great work. I noticed that , in the  step of Induce phrase-table,  the first score is direct phrase translation probability， the second is inverse phrase translation probability. However， moses 's phrases table is that the first is inverse phrase translation probability. the third is direct phrase translation probability.  Unfortunately, I am not familiar with statistical machine translation. I look forward to your reply. "
Training embedding is too slow. Could I train it with GPU ？How to set it? Thank you.
"Hi, thanks for publishing this code -- just wanted to point out that I had issues with compiling moses2. These were strange compilation problems relating to HypothesisColl.h and an unordered set and whether or not to import it from boost or std. It turns out that since the version 4 release of Moses (which is used in this repo), they have rolled back changes in this file relating to this very issue.     I finally fixed the problem by checking out the latest version of moses. (As of writing, that commit hash was: 187a75cb5596c8e4362c66c62de395e2b7d3a64a)"
@artetxem Great work on the project. I am trying to reproduce your results but cant get past step 7  I am using wmt14 news.2010 french and english text. After the first iteration of step 7 the features in the features list are all 0. Therefore throwing an error of Illegal division by zero at moses/scripts/training/mert-moses.pl line 1290    I have tried with other datasets but still getting the same error
"Hello,Truly appreciated with work.I am using a corpora with 10lakh sentences.In the ""induce-phrase-table.py"" file there is default option of using cuda,,but after checking gpu usage its 0 %.So pls can out clarify what does induce-phrase-table is actually doing ,or how can I increase traning speed.    Thanks a lot."
"Hi, thank you for your great work. I run this code and encounter error, segmentation fault,  during STEP 7.  More specifically,   the error occurred in code is       and part of log file is    Could you give some advice about this problem?  Thank you."
"@artetxem Hi Mikel, Great work. I am actually having a problem where in STEP 4: Map Embeddings, all the cores are not being used. Instead, less than half of the available cores are being used despite specifying the number of available threads as double of the available number of cores. This is when I train on a small corpus with a 8 core CPU.    If I use a system with 96 cores, the number of cores utilised even in previous steps such as 1,2 and 3 don't use even 20% of the available cores.    Do you have any pointers for the same?    Thanks"
Hello @artetxem thank you for this work! 💯 🥇   Do you have any pre-trained model (dummy or not) in order to check the decoding before starting a new training from scratch?  Thanks a lot!
It would be great if the conversion to   is supported. This can most probably be done by using the `scripting` concept of   introduced by  . 
"With large tag sets (e.g. lexical types, of which there can be thousands), unknown labels are possible at decoding time. In production, it is not possible to include the test set into the alphabet during training.     Is there any reason to explicitly disallow unknown labels then?     "
Hi!    I was wondering if there was a way to resume retraining model training from a checkpoint. So for example if a model should be trained for 500 iterations and for some reason was interrupted in between.    Thank you very much for this tool.
Thank you very much for response with   I tried the suggestion of using nbest=0 with two different types of models and see errors. Happy to have your recommendations to resolve them. Thank you. Please find the experiment run logs below.    - **Attempt 1: ccnn-wbilstm model**         - **Attempt 2: wbilstm model**     
"Greetings,    was wondering if there are plans to support Transformer models such as BERT, SciBERT, XLNet ... with this tool? This is not so much an issue, just a question.    Thank you :)"
"Hi,    I am trying to train and test a model for chunking. For training, I am using data in the format of CONLL 2000 i.e.    Word POStag Chunktag    However, I am not able to predict chunks for new data as I am getting an unknown key error. What should be the format for that?    Thank you!"
"大神你好，我的配置都没问题，数据是韩文中文和英文，数据都是用B/I/E-ORG标记的，在train期间是正常的，但是decode就出错，我的raw_dems数据是三种文字，后边标记全是O，请问下面报错是什么原因啊。我的python3.8和pytorch1.6.。。。  下面是我decode后报错的问题，请问  这是版本的问题么？还是其他的问题。谢谢。    `DATA SUMMARY END.  ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++  Traceback (most recent call last):    File ""/tmp/test_ljx/2/NCRFpp-master/utils/alphabet.py"", line 49, in get_index      return self.instance2index[instance]  KeyError: 'O'    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""main.py"", line 562, in        data.generate_instance('raw')    File ""/tmp/test_ljx/2/NCRFpp-master/utils/data.py"", line 306, in generate_instance      self.raw_texts, self.raw_Ids = read_instance(self.raw_dir, self.word_alphabet, self.char_alphabet, self.feature_alphabets, self.label_alphabet, self.number_normalized, self.MAX_SENTENCE_LENGTH, self.sentence_classification, self.split_token)    File ""/tmp/test_ljx/2/NCRFpp-master/utils/functions.py"", line 109, in read_instance      label_Ids.append(label_alphabet.get_index(label))    File ""/tmp/test_ljx/2/NCRFpp-master/utils/alphabet.py"", line 56, in get_index      return self.instance2index[self.UNKNOWN]  KeyError: ' '  `      "
"     In the forward method, when calling _viterbi_decode, a mask argument is not provided... "
Hello there!    This research is fascinating. I was wondering what layers were used in the architecture of your model. I see that there are biLSTM layers and a dropout layer: are there any other layers? I am specifically interested in the adam optimizer's layers.    Thanks!
Can you explain a little bit more about the following code? Why need to do this?     
"Hello,          I want to introduce a pre-training vector (trained with bert) in demo.train.config file, word_emb_dir=xxx(bert), where does NCRF++ need to be changed? Or when do you release bert embedding?"
"Scenario:  use My own RNN model (very simple model ) on my dataset by using (ORG, PER, LOC, O)tag scheme my model gives score about (F1 = 0.775) but when i use NCRFpp to train my model and it gives me F1 score = -1  and after debugging  I changed my data format according to the requirement (BIOES tag sheme used)   and then it gives my very poor results (F1 = 0.554)  Q-1  how do you compute f1-Score replacing tag with original or just weighted average?  Q-2 if I want to use bert Embedding How can I use? "
None
"I see in `utils/data.py` the `MAX_SENTENCE_LENGTH` parameter is fixed as a constant at 250 words. In my training and test data, I have maybe two or three instances that exceed 250 words to about 300 or so. I was just curious if there is a technical reason that the parameter is a constant? Thank you very much."
"A user reported success using a fasttext model (ref:   However, I can't seem to get it to work. I share a part of my training model summary stats below.         I am downloading the English text file model from here      Happy to know any comments. Thank you!"
Hello.    Below are the lines in my decode file.       And below is a summary preview of the loaded trained model during decoding.       Would appreciate knowing how to fix the problem `Nbest output is currently supported only for CRF! Exit...` Thank you!
"Greetings,    Is this expected behavior that the hyperparameters which obtain good results on the CPU will not transfer to experiments run on GPU? At the moment, we obtain a sufficiently strong sequence labeler with default lr (0.015) and in 10 iterations. However, we see very low results using the same parameters when training on a GPU. Maybe this is expected and normal, I am just not aware...    Thank you for any help with clarification."
"Greetings,    There was some discussion on `main_parse.py` in this issue   however, it is not clear to me the difference between `main.py` and `main_parse.py`    The   in point 3 talks about setting hyperparameters in command line arguments that is possible with `main_parse.py`. However, I notice two things in this regard: 1) this can also be done with `main.py`; and 2) it is currently not possible for all hyperparameters, e.g., `word_emb_dim` or `char_emb_dim` or `lr` etc.     Would be happy to have a clarification discussion on this.    Many thanks ..."
"Greetings,    Thank you for this great tool!    I am trying to train a new model on my data. My data file is 14.8MB and has 981,163 lines, 6 unique tags, and uses the IOBES tagging scheme. With my training attempt, the training aborts prematurely. I share here the screenshot with the message before it aborts.    <img width=""920"" alt=""error-screen"" src=""     Also, shared below is a screenshot of the end of my training file as my data sample.    !     Would making batches of the data help? And how would one go about it if so?    Happy to know a resolution to this.    Many thanks in advance!            "
"I want just use the CRF layer, because of the lack of data."
"hello,I have got the code of the NCRFpp. I installed the python 3.6 and pytorch gpu 1.7.1. But it can not work. So I want to know about the pytorch and python version you installed."
"Hello, I see that you allow for adding POS and capitalization as features. I was wondering if this could be augmented with other features? For instance I plan on using wordshape (eg 123&* -> ddd*& ; heE-123 -> xxX-ddd) as an additional feature. Is there a way to do this with NCRF++?     Thanks"
I have got nbst=6 result (below)   my question is how can i interpret this result?     is it actually the answer of the best 6 models to our testing data?       how can I have the same chart for a best score as you had in your GitHub?   ``  # 0.9998 0.0000 0.0000 0.0000 0.0000 0.0000  International O O O O O O  tourism O O O O O O  is O O O O O O  now O O O O O O  more O O O O O O  common O O O I-P O I-P  than O I-P I-P I-P O I-P  ever O I-P O I-P I-P O  before O O O O O O    # 0.9962 0.0015 0.0011 0.0004 0.0004 0.0003  The O O O O O O  last O O O O O O  50 O O O O O O  years O O O O O O  have O O O O O O  seen O O O O O O  a O O O O O O  significant O O O O O O  increase O O O O O O  in O O O O O O  the O O O O O O  number O O O O O O  of O O O O O O  tourist O O O O O O  traveling O O O O O O  worldwide O O O O O O  . O O O O O O`  ```
你好。我目前使用你的框架来做NER的实验，用的数据集conll2013。但在实验过程中我发现程序的内存占用很高，能高达40G，而且CPU占用也很高，几乎把机器的核数都占完了。    因为实验需要固定住预训练的embedding，所以对model/wordrep.py的48行进行了修改，但这应该影响不大。      
!   but it improve F1 in your paper 
"Hi,I want to know how the n best decodeing work to increase F1 on NER task, I don't know why n best=10 will increase F1"
"hi, how can we have a metric on span level? is there any way to use that?  (my labels are Claim and Premise)    I mean, I have a result on iob tokens like          how can I find the result on the Claim and Premise span?   "
"I have used your repo , it works well, I have kind of this result       firstly how can I have the best result? e.g based on   r: 0.4792 or... in training secondly how can I do hyperparameter optimization, confusion matrix and...    do you have a notebook using your repo?     is there any way to go inside your model and customize that?       many thnaks"
I faced this error!      `      can someone have a clue? 
"I have faced with mismatch, error what I have done, I add my data in the desired format as in train01 and dev01 and test01.   do you know what is the problem?   (detail in below)    maybe  i should add embedding also? if yes, can you give me a hint how can I do that    **log file**     `                **config file**           **sample data**         I have also add  errors='ignore' to     in funvtion and data       "
Edit: I solved the problem.
"您好,我在复现NER任务 WLSTM+CRF+CCNN模型的时候,有两个问题想请教您  1.我发现您论文中的NER数据统计部分和CONLL 2003数据集在dev和test句子数量上结果不一致,  论文统计结果:  !   CONLL2003统计结果:  !   想请问您的论文中的数据集是怎么构建的呢    2.我使用CONLL 2003 NER数据集,复现WLSTM+CRF+CCNN模型结果时,未能达到您论文中的F1:91.35结果,我的最好的结果是F1:91.02(epochs: 97),数据已经转换为BIOES格式,对于demo.train.config只修改了word_emb_dim=100和Iteration=100,其他参数未改动,附上log信息和数据格式  log:     数据格式:     想请问您,我的复现结果达不到91.35是否跟数据量不一致有关呢,还是由于其他什么原因引起的呢    如有打扰到您,还请谅解,谢谢你!"
"Hi, even after 10 epochs, f score < 0.01 and accuracy stay around 0.83.  I checked my data format, and I don't think it is wrong like past issues.    could you give me a hand with this problem?    my config:      test data (head -100):        train log :   "
"When running NCRF++ in training mode with the demo file, it spews out a massive amount of the following error:    `UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.`    Looking at previously closed issues, I see that manually modifying the code to cast all variables named ""mask"" to bool by using `.bool()` in stead of `.byte()` fixed the issue then, and apparently the issue was closed without a fix.    However, in the current version there are also other masks like this out there. I found those that are named ""mask"" and recast them by using `.astype(bool)` where `.bool()` wouldn't work, but I'm still getting a lot of warnings of the same type (also there does seem to be less of them).    I really don't want to go through the code with a fine-toothed comb to find every instance where an array of integers is actually used as a mask (but not named one) - can someone who's worked on the code before provide a fix?"
"Hi, could u help me find out the definition of oracle score? I just can't find the related meaning of it . Many thanks."
您好，我希望能利用该工具包训练中文标点恢复任务，在查阅说明以及issues区后进行了如下准备  数据的格式如下，有四个标点的标签（S-COMMA，S-PERIOD，S-QUESTIONMARK，S-QUESTIONMARK）EXCLAMATIONMARK）:         在使用项目的默认配置（gpu=True）训练时，训练数据在8千万句（12Gb），运行任务一段时间后显示被kill掉。  我将训练数据降至1000万句（2Gb），任务运行成功，但最终结果显示为loss 爆炸    查看issues后我修改了config文件，  主要是改变的地方是将CNN换成了LSTM，     但是运行系统显示1000万句的任务还是被kill掉。    之后我按照#107修改了config，同时设置seg=False，  仍然出现任务被kill的情况。    请问这主要是什么问题呢？数据的标注格式存在什么问题么？训练中文标点想利用字符（char）作为输入的时候还有什么需要注意的么？
"there is no pretrained embedding, such as sample_data/lstmcrf in the config file"
"Hi, This sounds to be reported by other developers in       and   I have checked all of those explanations.    I am trying to build and NER system with  . I used the `train_fold1.txt` as the training set, `test_fold1.txt` as dev set and `test_fold2.txt` as the test set. The corpus is in IOB format by default. I used the `tagSchemeConverte.py` for conversion to BOI format. The corpus includes various labels, there are some `B-X` in the labels too.    The problem is the model can not predict correct labels of entities, everything sounds to be predicated as `O`. Also, there are lots of tokens with `O` tags, but removing many of samples which all tokens are labelled as `O` didn't help. Below are my log files:         and this is train logs for some epochs ...       This is my configuraions:         For some epochs very small positive F1 has been seen. I thought maybe using different configurations could help the problem but none worked,  It would be a great help if you could share your ideas. Thanks"
请问您的WSJ数据集是在哪下载的？我在网上没有找到WSJ数据集，谢谢
"I am trying to run the main script, but get the following error.          The data summary says that there are no data instances.       I have tried to make my data as similar as possible to the data in sample_data. Here is an example, using a BIO scheme.         So I am not sue why my data is not getting recognized at all. Any ideas? "
"I have try to train punctuation restoration using NCRFpp, my data has three label type:    - C: comma (,)  - P: period (.)  - and O    For example:       But when training, i always get precision, recall and f1 = -1. And after 20 epochs training, decode result almost give me label `O`.    Here is my training logs:         What am i wrong?"
"Hi @jiesutd,  Thanks for sharing your great sequence labeling implementation on PyTorch.    I'm using your code for my sequential tagging problem but struggling in finding viterbi score in function __viterbi_decode_ in module _crf.py_.    Particularly, when decoding optimal tag sequence using function __viterbi_decode_ in _crf.py_, I would like to not only return the optimal sequence but also the **viterbi score for each tag in the optimal output sequence**.    I attached my implementation below. Hope to receive your useful comments on my implementation.    Thanks and regards,    <img width=""1182"" alt=""Screen Shot 2019-11-10 at 7 08 47 PM"" src=""    "
"hi，i trained a segment model on a gpu cluster,while my local machine has no gpu,  I'm wandering whether I can make some configuration to run the decode step in this situation  thx"
"I've noticed some   concerning the use of the test set for initializing vocabularies.     Even if it won't affect the performance, why should we monitor the test results during training?    Thanks  "
"hi, best wishes to you!   i have a question about pos labeling. now you give a tag format(BIO, BMES,BIOES), but for ner,chunking etc on. so for pos, what is that? it seems no support. if it support, can you give a sample? data format samples? thanks."
"Hello, where can I download the actual size of data, for the same comparison and results presented in the paper? In this repository, I see only a sample of data being used.    Thank you,  Vasiliki  "
您好 博主  它们的参数是required_grad=True吧？  就是训练过程中会更新吗？    有没有对比过 word_embedding 使用glove 300d的 然后embedding层 freeze的效果呢？
"Hi,  I am running the sample training and decoding configuration with the sample training data. I get this warning cluttering my command line (but training and decoding still seams to work). I use the newest PyTorch version 1.2.0. Can this be fixed easily? (I am not very familiar with PyTorch)    Thank you for your help in advance :)     "
+2 is used for the CRF later which needs start and end padding.  The results in demo data doesn’t mean anything as the data is too small.    _Originally posted by @jiesutd in  
"I confused about the operation: data.label_alphabet_size += 2 in seqlabel.py, Why add two more label for downlayer lstm ?    thanks for your work."
"I look into the viterbi decoding implementation and find that when backtracing the best path, argmax is done based on the current emission probability + previous score + transition probability. If my understanding is correct, the argmax should be done only based on previous score + transition probability, which is also suggested by   and the official crf implementation  . Please clarify this."
Are there any paper for reference?
作者你好，非常非常非常感谢你的工作。    最近想基于 NCRFpp 做一个断句的模型，具体来说就是： 来一个句子，判断这个句子是不是完整的句子。类似给句子标上逗号和句号。    尝试使用 Ncrfpp，实验参数及数据表示如下：    **参数配置：**       **部分数据：**       **数据说明：**  1. 数据中临时使用 S-LOC 作为断句标志，意思是这里是逗号或者句号。  2. 之前使用 crfpp 做过这个功能，当时的数据组织为：每句话都是完整的句子，然后把逗号，句号（句尾）标注成断句符号。训练之后发现不管是不是完整句子句尾都会被标注成 断句。  3. 当前的数据组织方式为，当前句子句尾加上下一句子的句首，大约 10 个字。训练后发现，完整的句子句尾会标注成不是断句，必须等待后续内容才能确定之前的句子是不是完整的句子。（这个是符合预期的）  4. 尝试训练数据一半是完整句子，一半是同一完整句子 + 下一句话的句首 10 个字，但是训练出来的性能非常差。    **我的问题是：**  如何组织训练数据才能真正达到断句的目的，也就是说来一个句子，判断是 完整句子 or 不是完整句子。    感谢！这个 issue 我持续提供实验结果，无论作者是否回复，已经非常感谢啦！
"Hi, don't know whether for the version of pytorch, I use torch=1.10. and happend to this error, please help me, thanks.  File ""E:\desktop\KG\NCRFpp\model\wordrep.py"", line 45, in __init__      self.word_embedding.weight.data.copy_(torch.from_numpy(data.pretrain_word_embedding))  TypeError: expected np.ndarray (got numpy.ndarray)"
"hi @jiesutd thanks for your great NCRFpp, when i modified my train.config, i'm confused about what dset_dir exactly means in TRAIN. what will your NCRFpp use dset_dir do?"
如题。
"您好，我的数据集有B-SIGNS，I-SIGNS等五个实体，请问可以得到每个实体的Precision,Recall和F值吗？  非常感谢~"
"作者，你好，感谢您的分享。  今天在引入char_embeddings和word_embeddings，模型是charCNN + wordLSTM + CRF，发现一个疑似bug。训练过程中，“ERROR: LOSS EXPLOSION (>1e8) ! PLEASE SET PROPER PARAMETERS AND STRUCTURE! EXIT....”导致退出，回看日志发现是sample_loss: nan导致，不知是否是bug问题，还请解答一下。  data.seg: True  status: train  Seed num: 42  MODEL: train  total_column: 2  tagScheme: BIO  tagScheme: BIO  tagScheme: BIO  train_texts_num: 23911  dev_texts_num: 2986  test_texts_num: 2987  Load pretrained word embedding, norm: False, dir: renmindata/embeddings/sgns.renmin.char  Embedding:       pretrain word:355792, prefect match:44646, case_match:1, oov:9674, oov%:0.1780862265748684  Load pretrained char embedding, norm: False, dir: renmindata/embeddings/sgns.renmin.char  Embedding:       pretrain word:355792, prefect match:4590, case_match:0, oov:88, oov%:0.018807437486642445  positive:negtive 3.896784763465083:1  Training model...  ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++  DATA SUMMARY START:   I/O:       Start   Sequence   Laebling   task...       Tag          scheme: BIO       Split         token:  |||        MAX SENTENCE LENGTH: 250       MAX   WORD   LENGTH: -1       Number   normalized: True       Word  alphabet size: 54322       Char  alphabet size: 4679       Label alphabet size: 6       Word embedding  dir: renmindata/embeddings/sgns.renmin.char       Char embedding  dir: renmindata/embeddings/sgns.renmin.char       Word embedding size: 300       Char embedding size: 300       Norm   word     emb: False       Norm   char     emb: False       Train  file directory: renmindata/dataset/train.txt       Dev    file directory: renmindata/dataset/dev.txt       Test   file directory: renmindata/dataset/test.txt       Raw    file directory: None       Dset   file directory: renmindata/model/lstmcrf       Model  file directory: renmindata/model/lstmcrf       Loadmodel   directory: None       Decode file directory: None       Train instance number: 23911       Dev   instance number: 2986       Test  instance number: 2987       Raw   instance number: 0       FEATURE num: 0   ++++++++++++++++++++++++++++++++++++++++   Model Network:       Model        use_crf: True       Model word extractor: LSTM       Model       use_char: True       Model char extractor: CNN       Model char_hidden_dim: 50   ++++++++++++++++++++++++++++++++++++++++   Training:       Optimizer: SGD       Iteration: 1       BatchSize: 10       Average  batch   loss: False   ++++++++++++++++++++++++++++++++++++++++   Hyperparameters:       Hyper              lr: 0.015       Hyper        lr_decay: 0.05       Hyper         HP_clip: None       Hyper        momentum: 0.0       Hyper              l2: 1e-08       Hyper      hidden_dim: 200       Hyper         dropout: 0.5       Hyper      lstm_layer: 1       Hyper          bilstm: True       Hyper             GPU: False  DATA SUMMARY END.  ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++  build sequence labeling network...  use_char:  True  char feature extractor:  CNN  word feature extractor:  LSTM  use crf:  True  build word sequence feature extractor: LSTM...  build word representation...  build char sequence feature extractor: CNN ...  lstm_hidden: 100  self.lstm: LSTM(350, 100, batch_first=True, bidirectional=True)  self.word_hidden: WordSequence(    (droplstm): Dropout(p=0.5)    (wordrep): WordRep(      (char_feature): CharCNN(        (char_drop): Dropout(p=0.5)        (char_embeddings): Embedding(4679, 300)        (char_cnn): Conv1d(300, 50, kernel_size=(3,), stride=(1,), padding=(1,))      )      (drop): Dropout(p=0.5)      (word_embedding): Embedding(54322, 300)      (feature_embeddings): ModuleList()    )    (lstm): LSTM(350, 100, batch_first=True, bidirectional=True)    (hidden2tag): Linear(in_features=200, out_features=8, bias=True)  )  build CRF...  word2id has written down  char2id has written down  label2id has written down  Epoch: 0/1   Learning rate is set as: 0.015  Before Shuffle: first input word list: [2, 3, 4]  Shuffle: first input word list: [3571, 12526, 48391]  train_num: 23911  total_batch: 2392  running calculate loss..  loss: tensor(nan, grad_fn= ) tag_seq: tensor([[0, 0, 0,  ..., 0, 0, 0],          [0, 0, 0,  ..., 0, 0, 0],          [0, 0, 0,  ..., 0, 0, 0],          ...,          [0, 0, 0,  ..., 0, 0, 0],          [0, 0, 0,  ..., 0, 0, 0],          [0, 0, 0,  ..., 0, 0, 0]])  sample_loss: nan  ...  sample_loss: nan       Instance: 500; Time: 27.81s; loss: nan; acc: 0.0/16339.0=0.0000  ERROR: LOSS EXPLOSION (>1e8) ! PLEASE SET PROPER PARAMETERS AND STRUCTURE! EXIT...."
"Hi, I am facing **an issue regarding to the file format**. I want to train NER MODEL by using this toolkit for URDU (an asian language) which is supported by **'UTF-8'** format. when I start to train (after changing in config file for my Dataset) it shows the following error.    > UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 5: character maps to '\ '    after searching on internet for this problems most recommend solution is that     set the encoding type to 'UTF-8' when reading a file.    please look into this issue and help me how to resolve this issue.   "
"Hi,is there a version of tensorflow implementation?"
"Hi, my research interest is computer vision. I want to use crf module in my work, Also I don't familiar with NLP. So I want to know how to get mean nll loss insteaf of sum nll loss"
您好，想请问可以用CharCNN+CRF或者是只用CRF吗？  非常感谢回答~
"THCudaCheck FAIL file=/pytorch/torch/lib/THC/generic/THCStorage.cu line=58 error=2 : out of memory  Traceback (most recent call last):    File ""main.py"", line 445, in        train(data)    File ""main.py"", line 357, in train      loss.backward()    File ""/home/jhy/py2.7/local/lib/python2.7/site-packages/torch/autograd/variable.py"", line 167, in backward      torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)    File ""/home/jhy/py2.7/local/lib/python2.7/site-packages/torch/autograd/__init__.py"", line 99, in backward      variables, grad_variables, retain_graph)  RuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58  "
"你好，在使用decode模式时，我想预测一个文件夹下所有文件而不是指定的一个文件，这样的场景对我更合适。但是预测到第二个文件就会出错，请问该怎样解决呢？谢谢  Traceback (most recent call last):    File ""main.py"", line 567, in        decode_results, pred_scores = load_model_decode(data, 'raw')    File ""main.py"", line 491, in load_model_decode      model.load_state_dict(torch.load(data.load_model_dir))    File ""C:\install\anaconda\lib\site-packages\torch\nn\modules\module.py"", line 777, in load_state_dict      self.__class__.__name__, ""\n\t"".join(error_msgs)))  RuntimeError: Error(s) in loading state_dict for SeqLabel:          size mismatch for word_hidden.hidden2tag.weight: copying a param with shape torch.Size([17, 200]) from checkpoint, the shape in current model is torch.Size([19, 200]).          size mismatch for word_hidden.hidden2tag.bias: copying a param with shape torch.Size([17]) from checkpoint, the shape in current model is torch.Size([19]).          size mismatch for crf.transitions: copying a param with shape torch.Size([17, 17]) from checkpoint, the shape in current model is torch.Size([19, 19]).  "
None
How this toolkit handle OOV? Where I could find the implementation?
"when encoding  crf.py"", line 27, in log_sum_exp      return max_score.view(-1, m_size) + torch.log(torch.sum(torch.exp(vec - max_score.expand_as(vec)), 1)).view(-1, m_size)  # B * M  RuntimeError: CUDA out of memory. Tried to allocate 158.25 MiB (GPU 0; 11.17 GiB total capacity; 10.56 GiB already allocated; 130.88 MiB free; 1.29 MiB cached)  "
None
"When I use this awesome pro, I found that when I decode the model with different raw data, I get results in below which makes me confused. like:  result (1)  build CRF...  Decode raw data, nbest: 1 ...  gold_num =  0  pred_num =  15  right_num =  0  raw: time:0.24s, speed:178.83st/s; acc: 0.9732, p: 0.0000, r: -1.0000, f: -1.0000  result (2)  build CRF...  Decode raw data, nbest: 1 ...  gold_num =  0  pred_num =  96  right_num =  0  raw: time:0.30s, speed:73.46st/s; acc: 0.8833, p: 0.0000, r: -1.0000, f: -1.0000  In result (1), the acc is high(acc:0.9732) and the NER predicted perfect such as PER which I want. However, in result (2), the acc(which is 0.8833) is not that good, and there are many PER predicted wrong. So if the acc value already shows that there is something wrong when predicting, why should it still gives the wrong decoding result. like  '王益' is the right PER in raw data, the model founds '王益作' as PER because there is '王益作出****' in the sentences."
path_score that _viterbi_decode function returned in crf.py is None    !   
您好！我在您的Chinese NER Using Lattice LSTM论文的resume数据集上进行实验，只修改了配置文件，训练了100个epoch，但是训练过程中每次在dev和test上评测的f1值都是10%~20%，十分不理想。配置文件如下：  ### use # to comment out the configure item    ### I/O ###  train_dir=data/ResumeNER/train.char.bmes  dev_dir=data/ResumeNER/dev.char.bmes  test_dir=data/ResumeNER/test.char.bmes  model_dir=result/lstmcrf  word_emb_dir=data/gigaword_chn.all.a2b.uni.ite50.vec.txt    #raw_dir=  #decode_dir=  #dset_dir=  #load_model_dir=  #char_emb_dir=    norm_word_emb=True  norm_char_emb=False  number_normalized=True  seg=True  word_emb_dim=50  char_emb_dim=30    ###NetworkConfiguration###  use_crf=True  use_char=False  word_seq_feature=LSTM  char_seq_feature=CNN  #feature=[POS] emb_size=20  #feature=[Cap] emb_size=20  #nbest=1    ###TrainingSetting###  status=train  optimizer=SGD  iteration=100  batch_size=10  ave_batch_loss=False    ###Hyperparameters###  cnn_layer=4  char_hidden_dim=50  hidden_dim=200  dropout=0.5  lstm_layer=1  bilstm=True  learning_rate=0.015  lr_decay=0.05  momentum=0  l2=1e-8  #gpu  #clip=    请求您的帮助~~十分感谢！
"I trained the model on GPU, python 2.7 installed. After that I try to decode this model on window with python 3.7.    (base) D:\NER\NCRFpp>python main.py --config demo.decode.config  Seed num: 42  MODEL: decode  Traceback (most recent call last):    File ""main.py"", line 442, in        data.load(data.dset_dir)    File ""D:\NER\NCRFpp\utils\data.py"", line 305, in load      tmp_dict = pickle.load(f)  UnicodeDecodeError: 'ascii' codec can't decode byte 0xa0 in position 2400: ordinal not in range(128)    How can I solve it? Thanks in advance.  "
"Hi!    Thank you for making this toolkit available!    I have a collection of pre-trained embeddings with different vector sizes, and I have a question regarding the **word_emb_dim** setting in the configuration file.    Let's say we load a 300-dimensional pre-trained embedding, and keep the default **word_emb_dim** value, which is 50. What will happen then? Will **word_emb_dim** be overridden?"
None
"你好，我用你的模型跑了13个epoch后对best model解码raw_data是测试集，用评测工具对结果评价，发现p、r、f1都较低：  processed 173615 tokens with 6145 phrases; found: 5280 phrases; correct: 1057.  accuracy:  80.64%; precision:  20.02%; recall:  17.20%; FB1:  18.50                LOC: precision:  21.13%; recall:  20.13%; FB1:  20.62  2721                ORG: precision:  17.99%; recall:  16.99%; FB1:  17.48  1256                PER: precision:  19.65%; recall:  13.07%; FB1:  15.70  1303    为何在训练中测试集的p、r、f那么高，但评测后这么低呢?  评测工具为conlleval.pl  -------------------------------------------------------------  log:    Seed num: 42  MODEL: train  Find feature:  [pos]  Find feature:  [wb]  Load pretrained word embedding, norm: True, dir: sample_data/char_vec.txt  Embedding:       pretrain word:4769, prefect match:4756, case_match:0, oov:67, oov%:0.013888888888888888  Training model...  ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++  DATA SUMMARY START:   I/O:       Start   Sequence   Laebling   task...       Tag          scheme: BIO       Split         token:  |||        MAX SENTENCE LENGTH: 1000       MAX   WORD   LENGTH: -1       Number   normalized: True       Word  alphabet size: 4824       Char  alphabet size: 4826       Label alphabet size: 8       Word embedding  dir: sample_data/char_vec.txt       Char embedding  dir: None       Word embedding size: 100       Char embedding size: 50       Norm   word     emb: True       Norm   char     emb: False       Train  file directory: sample_data/2f.train       Dev    file directory: sample_data/2f.dev       Test   file directory: sample_data/2f.test       Raw    file directory: None       Dset   file directory: None       Model  file directory: sample_data/lstmcrf       Loadmodel   directory: None       Decode file directory: None       Train instance number: 45000       Dev   instance number: 4537       Test  instance number: 3441       Raw   instance number: 0       FEATURE num: 2           Fe: [pos]  alphabet  size: 61           Fe: [pos]  embedding  dir: None           Fe: [pos]  embedding size: 20           Fe: [pos]  norm       emb: False           Fe: [wb]  alphabet  size: 7           Fe: [wb]  embedding  dir: None           Fe: [wb]  embedding size: 20           Fe: [wb]  norm       emb: False   ++++++++++++++++++++++++++++++++++++++++   Model Network:       Model        use_crf: True       Model word extractor: LSTM       Model       use_char: False   ++++++++++++++++++++++++++++++++++++++++   Training:       Optimizer: adagrad       Iteration: 1000       BatchSize: 64       Average  batch   loss: True   ++++++++++++++++++++++++++++++++++++++++   Hyperparameters:       Hyper              lr: 0.015       Hyper        lr_decay: 0.05       Hyper         HP_clip: None       Hyper        momentum: 0.0       Hyper              l2: 1e-08       Hyper      hidden_dim: 200       Hyper         dropout: 0.5       Hyper      lstm_layer: 1       Hyper          bilstm: True       Hyper             GPU: False  DATA SUMMARY END.  ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++  build sequence labeling network...  use_char:  False  word feature extractor:  LSTM  use crf:  True  build word sequence feature extractor: LSTM...  build word representation...  build CRF...  Epoch: 0/1000  Shuffle: first input word list: [46, 108, 10, 725, 95, 97, 95, 95, 39, 1392, 795, 591, 687, 1208, 453, 191, 27, 28, 260, 702, 837, 417, 9, 29, 72, 198, 425, 375, 136, 731, 1092, 18, 497, 53, 530, 78, 38, 2248, 278, 1296, 53, 530, 1090, 770, 577, 1307, 9, 451, 96, 1088, 1068, 591, 1265, 345, 350, 217, 1306, 1688, 218, 191, 27, 96, 307, 308, 18, 544, 95, 96, 26, 65, 18, 135, 709, 307, 1912, 2986, 117, 145, 146, 1360, 363, 9, 266, 1116, 448, 345, 827, 265, 206, 813, 2981, 451, 96, 1088, 1068, 591, 18, 191, 348, 95, 354, 95, 96, 9, 818, 1124, 53, 117, 848, 486, 20, 452, 278, 78, 270, 117, 19, 399, 117, 773, 185, 129, 266, 901, 9, 1210, 1211, 70]       Instance: 8000; Time: 299.00s; loss: 1586.4518; acc: 360710.0/391012.0=0.9225       Instance: 16000; Time: 252.92s; loss: 854.9906; acc: 726976.0/775678.0=0.9372       Instance: 24000; Time: 239.02s; loss: 718.7530; acc: 1097478.0/1162269.0=0.9443       Instance: 32000; Time: 272.94s; loss: 640.5142; acc: 1467619.0/1547160.0=0.9486       Instance: 40000; Time: 237.99s; loss: 588.6535; acc: 1838839.0/1932301.0=0.9516       Instance: 45000; Time: 140.48s; loss: 352.6502; acc: 2069830.0/2171573.0=0.9531       Instance: 45000; Time: 0.34s; loss: 0.0000; acc: 2069830.0/2171573.0=0.9531  Epoch: 0 training finished. Time: 1442.69s, speed: 31.19st/s,  total loss: 4742.013214111328  totalloss: 4742.013214111328  Right token =  211777  All token =  217223  acc =  0.9749289900240766  Dev: time: 28.46s, speed: 160.45st/s; acc: 0.9749, p: 0.6666, r: 0.6413, f: 0.6537  Exceed previous best f score: -10  Save current best model in file: sample_data/lstmcrf.0.model  Right token =  164811  All token =  170174  acc =  0.9684851975037315  Test: time: 25.85s, speed: 133.99st/s; acc: 0.9685, p: 0.7245, r: 0.6901, f: 0.7069  Epoch: 1/1000  Shuffle: first input word list: [248, 18, 1549, 2511, 353, 60, 109, 942, 378, 1371, 3756, 18, 1549, 2389, 1408, 2212, 74, 1408, 382, 53, 9, 182, 317, 2866, 1299, 278, 822, 817, 83, 56, 18, 1238, 624, 1769, 1247, 9, 1769, 1242, 785, 689, 291, 122, 798, 18, 274, 47, 18, 378, 1371, 3756, 40, 307, 195, 154, 178, 55, 153, 1769, 1242, 167, 50, 18, 220, 135, 732, 1299, 236, 275, 180, 95, 95, 793, 3022, 135, 136, 70]       Instance: 8000; Time: 227.76s; loss: 540.5916; acc: 374918.0/388321.0=0.9655       Instance: 16000; Time: 287.22s; loss: 504.1164; acc: 751064.0/777062.0=0.9665       Instance: 24000; Time: 218.37s; loss: 464.7418; acc: 1118086.0/1156130.0=0.9671       Instance: 32000; Time: 244.40s; loss: 466.9012; acc: 1488194.0/1538504.0=0.9673       Instance: 40000; Time: 272.75s; loss: 440.6490; acc: 1864421.0/1926563.0=0.9677       Instance: 45000; Time: 209.71s; loss: 278.1708; acc: 2102140.0/2171573.0=0.9680       Instance: 45000; Time: 0.10s; loss: 0.0000; acc: 2102140.0/2171573.0=0.9680  Epoch: 1 training finished. Time: 1460.32s, speed: 30.82st/s,  total loss: 2695.1707305908203  totalloss: 2695.1707305908203  Right token =  212894  All token =  217223  acc =  0.9800711711006661  Dev: time: 28.40s, speed: 160.81st/s; acc: 0.9801, p: 0.7604, r: 0.6979, f: 0.7278  Exceed previous best f score: 0.6536741946578012  Save current best model in file: sample_data/lstmcrf.1.model  Right token =  165455  All token =  170174  acc =  0.9722695593921515  Test: time: 25.68s, speed: 134.84st/s; acc: 0.9723, p: 0.7831, r: 0.7303, f: 0.7558  Epoch: 2/1000  Shuffle: first input word list: [324, 1901, 2372, 331, 1047, 488, 240, 184, 571, 1604, 145, 74, 96, 1726, 83, 56, 9, 309, 117, 484, 96, 495, 486, 185, 53, 1526, 1308, 95, 95, 149, 161, 486, 590, 1393, 9, 486, 185, 251, 1300, 1301, 887, 488, 9, 95, 95, 149, 486, 185, 61, 14, 1435, 8, 273, 1047, 70]       Instance: 8000; Time: 265.00s; loss: 414.1249; acc: 373120.0/384562.0=0.9702       Instance: 16000; Time: 296.40s; loss: 407.2514; acc: 748736.0/771391.0=0.9706       Instance: 24000; Time: 304.65s; loss: 405.6964; acc: 1125830.0/1159816.0=0.9707       Instance: 32000; Time: 297.83s; loss: 392.1353; acc: 1499516.0/1544834.0=0.9707       Instance: 40000; Time: 258.83s; loss: 379.1874; acc: 1873266.0/1929417.0=0.9709       Instance: 45000; Time: 144.59s; loss: 234.2878; acc: 2108992.0/2171573.0=0.9712       Instance: 45000; Time: 0.18s; loss: 0.0000; acc: 2108992.0/2171573.0=0.9712  Epoch: 2 training finished. Time: 1567.48s, speed: 28.71st/s,  total loss: 2232.683151245117  totalloss: 2232.683151245117  Right token =  213443  All token =  217223  acc =  0.9825985277802074  Dev: time: 28.62s, speed: 159.66st/s; acc: 0.9826, p: 0.7836, r: 0.7337, f: 0.7578  Exceed previous best f score: 0.7278057933622609  Save current best model in file: sample_data/lstmcrf.2.model  Right token =  165913  All token =  170174  acc =  0.9749609223500653  Test: time: 26.10s, speed: 132.68st/s; acc: 0.9750, p: 0.7984, r: 0.7575, f: 0.7774  Epoch: 3/1000  Shuffle: first input word list: [1169, 964, 9, 39, 6, 475, 1256, 226, 162, 797, 257, 18, 1229, 178, 514, 53, 22, 455, 1192, 193, 40, 531, 514, 72, 238, 9, 964, 157, 431, 382, 178, 408, 299, 9, 2322, 599, 93, 351, 18, 584, 60, 610, 653, 642, 178, 931, 198, 53, 22, 78, 270, 516, 404, 9, 282, 203, 70]       Instance: 8000; Time: 232.16s; loss: 364.5571; acc: 373455.0/384059.0=0.9724       Instance: 16000; Time: 241.86s; loss: 341.3476; acc: 745776.0/766286.0=0.9732       Instance: 24000; Time: 280.22s; loss: 335.9779; acc: 1124209.0/1154680.0=0.9736       Instance: 32000; Time: 283.85s; loss: 358.8347; acc: 1501418.0/1542644.0=0.9733       Instance: 40000; Time: 263.40s; loss: 352.5958; acc: 1879932.0/1931808.0=0.9731       Instance: 45000; Time: 174.51s; loss: 217.0028; acc: 2113133.0/2171573.0=0.9731       Instance: 45000; Time: 0.11s; loss: 0.0000; acc: 2113133.0/2171573.0=0.9731  Epoch: 3 training finished. Time: 1476.11s, speed: 30.49st/s,  total loss: 1970.3158416748047  totalloss: 1970.3158416748047  Right token =  213627  All token =  217223  acc =  0.9834455835708005  Dev: time: 28.09s, speed: 162.61st/s; acc: 0.9834, p: 0.7925, r: 0.7515, f: 0.7714  Exceed previous best f score: 0.7578115925194564  Save current best model in file: sample_data/lstmcrf.3.model  Right token =  165950  All token =  170174  acc =  0.9751783468684992  Test: time: 25.55s, speed: 135.54st/s; acc: 0.9752, p: 0.8023, r: 0.7703, f: 0.7860  Epoch: 4/1000  Shuffle: first input word list: [260, 47, 53, 22, 10, 96, 198, 199, 398, 129, 20, 452, 901, 273, 392, 503, 164, 1526, 415, 9, 29, 217, 822, 852, 18, 47, 1637, 577, 846, 577, 1724, 848, 1377, 514, 1088, 72, 274, 1143, 257, 132, 48, 29, 190, 111, 188, 544, 9, 85, 203, 56, 486, 18, 369, 317, 653, 251, 22, 390, 2298, 442, 9, 85, 91, 623, 596, 74, 392, 9, 118, 89, 26, 146, 9, 445, 246, 278, 195, 450, 18, 87, 47, 59, 22, 577, 1307, 19, 20, 21, 22, 415, 635, 117, 577, 198, 53, 123, 207, 1161, 1704, 56, 366, 21, 9, 85, 56, 2742, 1887, 70]       Instance: 8000; Time: 277.93s; loss: 334.9919; acc: 380129.0/390202.0=0.9742       Instance: 16000; Time: 271.79s; loss: 315.0048; acc: 757504.0/777026.0=0.9749       Instance: 24000; Time: 269.80s; loss: 311.9272; acc: 1133465.0/1162616.0=0.9749       Instance: 32000; Time: 235.91s; loss: 314.7539; acc: 1509071.0/1548080.0=0.9748       Instance: 40000; Time: 216.21s; loss: 312.0599; acc: 1880446.0/1929213.0=0.9747       Instance: 45000; Time: 157.34s; loss: 197.1122; acc: 2116945.0/2171573.0=0.9748       Instance: 45000; Time: 0.18s; loss: 0.0000; acc: 2116945.0/2171573.0=0.9748  Epoch: 4 training finished. Time: 1429.16s, speed: 31.49st/s,  total loss: 1785.8500518798828  totalloss: 1785.8500518798828  Right token =  210553  All token =  217223  acc =  0.9692942275910009  Dev: time: 28.21s, speed: 161.89st/s; acc: 0.9693, p: 0.7008, r: 0.7528, f: 0.7259  Right token =  162041  All token =  170174  acc =  0.9522077403128563  Test: time: 25.63s, speed: 135.11st/s; acc: 0.9522, p: 0.7235, r: 0.7473, f: 0.7352  Epoch: 5/1000  Shuffle: first input word list: [259, 1590, 178, 18, 260, 47, 276, 22, 1180, 56, 9, 554, 1287, 166, 1296, 74, 188, 4468, 70]       Instance: 8000; Time: 232.55s; loss: 306.9022; acc: 372532.0/382248.0=0.9746       Instance: 16000; Time: 270.74s; loss: 284.7562; acc: 747907.0/766755.0=0.9754       Instance: 24000; Time: 285.25s; loss: 295.1732; acc: 1126502.0/1154799.0=0.9755       Instance: 32000; Time: 273.36s; loss: 291.7760; acc: 1505567.0/1543020.0=0.9757       Instance: 40000; Time: 226.25s; loss: 291.5304; acc: 1881639.0/1928376.0=0.9758       Instance: 45000; Time: 182.02s; loss: 171.9740; acc: 2119163.0/2171573.0=0.9759       Instance: 45000; Time: 0.10s; loss: 0.0000; acc: 2119163.0/2171573.0=0.9759  Epoch: 5 training finished. Time: 1470.27s, speed: 30.61st/s,  total loss: 1642.1119689941406  totalloss: 1642.1119689941406  Right token =  214116  All token =  217223  acc =  0.9856967264055833  Dev: time: 28.29s, speed: 161.43st/s; acc: 0.9857, p: 0.8451, r: 0.7571, f: 0.7987  Exceed previous best f score: 0.7714153775109674  Save current best model in file: sample_data/lstmcrf.5.model  Right token =  166362  All token =  170174  acc =  0.9775993982629544  Test: time: 25.74s, speed: 134.51st/s; acc: 0.9776, p: 0.8398, r: 0.7770, f: 0.8072  Epoch: 6/1000  Shuffle: first input word list: [569, 280, 251, 624, 2102, 2483, 2484, 9, 72, 167, 964, 157, 55, 716, 47, 1568, 642, 171, 667, 1162, 382, 793, 2006, 9, 237, 238, 964, 157, 70]       Instance: 8000; Time: 262.43s; loss: 274.2701; acc: 377686.0/386546.0=0.9771       Instance: 16000; Time: 237.48s; loss: 271.5450; acc: 751968.0/769658.0=0.9770       Instance: 24000; Time: 280.85s; loss: 270.2705; acc: 1130277.0/1156854.0=0.9770       Instance: 32000; Time: 270.58s; loss: 266.4450; acc: 1507709.0/1543064.0=0.9771       Instance: 40000; Time: 243.58s; loss: 273.1657; acc: 1885914.0/1930429.0=0.9769       Instance: 45000; Time: 166.70s; loss: 167.4845; acc: 2121566.0/2171573.0=0.9770       Instance: 45000; Time: 0.18s; loss: 0.0000; acc: 2121566.0/2171573.0=0.9770  Epoch: 6 training finished. Time: 1461.80s, speed: 30.78st/s,  total loss: 1523.180892944336  totalloss: 1523.180892944336  Right token =  214219  All token =  217223  acc =  0.9861708935057522  Dev: time: 28.41s, speed: 160.77st/s; acc: 0.9862, p: 0.8368, r: 0.7821, f: 0.8085  Exceed previous best f score: 0.7986712540040337  Save current best model in file: sample_data/lstmcrf.6.model  Right token =  166419  All token =  170174  acc =  0.9779343495481095  Test: time: 25.95s, speed: 133.46st/s; acc: 0.9779, p: 0.8355, r: 0.7947, f: 0.8146  Epoch: 7/1000  Shuffle: first input word list: [818, 1124, 9, 153, 2633, 280, 117, 1217, 1870, 280, 278, 551, 5, 280, 95, 95, 1883, 497, 342, 479, 191, 348, 18, 74, 2654, 206, 812, 1664, 398, 275, 95, 95, 1883, 70]       Instance: 8000; Time: 247.24s; loss: 253.3324; acc: 381440.0/389921.0=0.9782       Instance: 16000; Time: 286.34s; loss: 257.5703; acc: 760024.0/777107.0=0.9780       Instance: 24000; Time: 265.89s; loss: 258.9609; acc: 1134505.0/1160540.0=0.9776       Instance: 32000; Time: 286.05s; loss: 255.1125; acc: 1513505.0/1548190.0=0.9776       Instance: 40000; Time: 227.32s; loss: 255.2852; acc: 1889785.0/1932885.0=0.9777       Instance: 45000; Time: 151.64s; loss: 163.8296; acc: 2123051.0/2171573.0=0.9777       Instance: 45000; Time: 0.09s; loss: 0.0000; acc: 2123051.0/2171573.0=0.9777  Epoch: 7 training finished. Time: 1464.56s, speed: 30.73st/s,  total loss: 1444.0909881591797  totalloss: 1444.0909881591797  Right token =  214289  All token =  217223  acc =  0.9864931429913039  Dev: time: 28.25s, speed: 161.64st/s; acc: 0.9865, p: 0.8320, r: 0.7987, f: 0.8150  Exceed previous best f score: 0.8085106382978723  Save current best model in file: sample_data/lstmcrf.7.model  Right token =  166360  All token =  170174  acc =  0.9775876455862823  Test: time: 25.62s, speed: 135.16st/s; acc: 0.9776, p: 0.8304, r: 0.8015, f: 0.8157  Epoch: 8/1000  Shuffle: first input word list: [300, 432, 154, 1559, 9, 53, 22, 479, 2863, 149, 1871, 248, 283, 540, 455, 571, 367, 329, 1704, 18, 369, 20, 1741, 511, 2264, 513, 18, 20, 1741, 2131, 1088, 1240, 117, 2412, 1704, 1180, 18, 571, 353, 95, 1294, 3308, 3309, 117, 95, 1294, 477, 3315, 117, 95, 1294, 181, 2235, 2230, 117, 95, 1294, 2419, 2009, 117, 95, 1294, 2419, 258, 1397, 117, 95, 1294, 893, 2447, 117, 95, 95, 1294, 1428, 217, 117, 95, 95, 1294, 181, 250, 2210, 117, 95, 95, 1294, 2179, 162, 162, 117, 95, 95, 1294, 854, 737, 117, 95, 95, 1294, 511, 3320, 122, 117, 95, 95, 1294, 274, 3312, 70]       Instance: 8000; Time: 260.89s; loss: 249.9745; acc: 378210.0/386633.0=0.9782       Instance: 16000; Time: 253.64s; loss: 239.8650; acc: 754222.0/770816.0=0.9785       Instance: 24000; Time: 244.65s; loss: 239.4943; acc: 1132179.0/1157059.0=0.9785       Instance: 32000; Time: 258.29s; loss: 238.7275; acc: 1510134.0/1543383.0=0.9785       Instance: 40000; Time: 255.57s; loss: 247.0385; acc: 1889634.0/1931291.0=0.9784       Instance: 45000; Time: 174.89s; loss: 159.2359; acc: 2124470.0/2171573.0=0.9783       Instance: 45000; Time: 0.22s; loss: 0.0000; acc: 2124470.0/2171573.0=0.9783  Epoch: 8 training finished. Time: 1448.14s, speed: 31.07st/s,  total loss: 1374.335693359375  totalloss: 1374.335693359375  Right token =  214073  All token =  217223  acc =  0.9854987731501729  Dev: time: 28.62s, speed: 159.56st/s; acc: 0.9855, p: 0.8470, r: 0.7557, f: 0.7988  Right token =  165586  All token =  170174  acc =  0.9730393597141749  Test: time: 25.82s, speed: 134.08st/s; acc: 0.9730, p: 0.8161, r: 0.7503, f: 0.7818  Epoch: 9/1000  Shuffle: first input word list: [497, 325, 164, 2248, 117, 1178, 462, 1249, 2091, 952, 18, 2163, 30, 152, 266, 901, 199, 1247, 84, 2248, 353, 18, 1090, 14, 449, 375, 1178, 1179, 2248, 353, 375, 18, 1070, 1671, 273, 1767, 1750, 1224, 108, 431, 382, 543, 1334, 708, 510, 18, 225, 114, 26, 448, 1224, 108, 193, 241, 2812, 708, 510, 81, 1282, 70]       Instance: 8000; Time: 252.97s; loss: 226.9855; acc: 373706.0/381690.0=0.9791       Instance: 16000; Time: 204.65s; loss: 230.0659; acc: 749044.0/764994.0=0.9792       Instance: 24000; Time: 227.49s; loss: 233.0966; acc: 1127953.0/1152088.0=0.9791       Instance: 32000; Time: 282.76s; loss: 235.7867; acc: 1506578.0/1539036.0=0.9789       Instance: 40000; Time: 307.61s; loss: 231.4924; acc: 1886241.0/1926802.0=0.9789       Instance: 45000; Time: 181.43s; loss: 151.5118; acc: 2125720.0/2171573.0=0.9789       Instance: 45000; Time: 0.14s; loss: 0.0000; acc: 2125720.0/2171573.0=0.9789  Epoch: 9 training finished. Time: 1457.05s, speed: 30.88st/s,  total loss: 1308.9389190673828  totalloss: 1308.9389190673828  Right token =  214500  All token =  217223  acc =  0.9874644950120384  Dev: time: 28.45s, speed: 160.51st/s; acc: 0.9875, p: 0.8485, r: 0.8027, f: 0.8250  Exceed previous best f score: 0.8150103282074821  Save current best model in file: sample_data/lstmcrf.9.model  Right token =  166521  All token =  170174  acc =  0.9785337360583873  Test: time: 25.72s, speed: 134.65st/s; acc: 0.9785, p: 0.8446, r: 0.8076, f: 0.8257  Epoch: 10/1000  Shuffle: first input word list: [56, 236, 348, 577, 1334, 38, 18, 325, 425, 9, 2298, 442, 74, 785, 56, 6, 233, 136, 1247, 198, 84, 19, 399, 9, 2298, 442, 70]       Instance: 8000; Time: 265.88s; loss: 222.4381; acc: 379677.0/387575.0=0.9796       Instance: 16000; Time: 278.53s; loss: 224.5166; acc: 760091.0/776150.0=0.9793       Instance: 24000; Time: 238.49s; loss: 223.6321; acc: 1137837.0/1161706.0=0.9795       Instance: 32000; Time: 254.93s; loss: 227.8724; acc: 1515157.0/1547246.0=0.9793       Instance: 40000; Time: 272.89s; loss: 217.0261; acc: 1890553.0/1930450.0=0.9793       Instance: 45000; Time: 170.78s; loss: 138.6168; acc: 2126778.0/2171573.0=0.9794       Instance: 45000; Time: 0.09s; loss: 0.0000; acc: 2126778.0/2171573.0=0.9794  Epoch: 10 training finished. Time: 1481.59s, speed: 30.37st/s,  total loss: 1254.1020812988281  totalloss: 1254.1020812988281  Right token =  214629  All token =  217223  acc =  0.9880583547782693  Dev: time: 30.98s, speed: 147.33st/s; acc: 0.9881, p: 0.8629, r: 0.8068, f: 0.8339  Exceed previous best f score: 0.8250115580212668  Save current best model in file: sample_data/lstmcrf.10.model  Right token =  166694  All token =  170174  acc =  0.979550342590525  Test: time: 26.18s, speed: 132.24st/s; acc: 0.9796, p: 0.8530, r: 0.8075, f: 0.8296  Epoch: 11/1000  Shuffle: first input word list: [162, 22, 56, 117, 766, 162, 56, 278, 22, 207, 208, 129, 208, 813, 9, 451, 96, 1090, 770, 278, 513, 162, 486, 185, 56, 375, 239, 548, 193, 18, 1752, 28, 74, 1147, 3584, 1562, 448, 2405, 1756, 1562, 2664, 1982, 226, 921, 164, 165, 1459, 143, 178, 906, 2386, 18, 135, 1247, 190, 273, 1755, 689, 591, 9, 3508, 165, 70]       Instance: 8000; Time: 285.10s; loss: 211.1272; acc: 380313.0/387858.0=0.9805       Instance: 16000; Time: 268.76s; loss: 214.6739; acc: 758287.0/773522.0=0.9803       Instance: 24000; Time: 325.18s; loss: 209.7188; acc: 1135616.0/1158515.0=0.9802       Instance: 32000; Time: 258.09s; loss: 217.7040; acc: 1512803.0/1543532.0=0.9801       Instance: 40000; Time: 288.31s; loss: 215.8403; acc: 1892153.0/1930661.0=0.9801       Instance: 45000; Time: 172.24s; loss: 130.8118; acc: 2128347.0/2171573.0=0.9801       Instance: 45000; Time: 0.17s; loss: 0.0000; acc: 2128347.0/2171573.0=0.9801  Epoch: 11 training finished. Time: 1597.86s, speed: 28.16st/s,  total loss: 1199.8760070800781  totalloss: 1199.8760070800781  Right token =  214688  All token =  217223  acc =  0.9883299650589487  Dev: time: 31.36s, speed: 145.51st/s; acc: 0.9883, p: 0.8679, r: 0.8099, f: 0.8379  Exceed previous best f score: 0.8338951528536557  Save current best model in file: sample_data/lstmcrf.11.model  Right token =  166671  All token =  170174  acc =  0.9794151868087957  Test: time: 26.81s, speed: 129.18st/s; acc: 0.9794, p: 0.8598, r: 0.8116, f: 0.8350  Epoch: 12/1000  Shuffle: first input word list: [300, 348, 171, 392, 347, 3005, 931, 49, 537, 135, 368, 18, 1988, 335, 1720, 398, 129, 486, 488, 5, 188, 2248, 353, 375, 91, 722, 261, 1720, 95, 95, 48, 398, 488, 95, 95, 11, 56, 486, 185, 18, 486, 740, 95, 149, 56, 486, 185, 1767, 931, 47, 162, 793, 38, 117, 1252, 738, 729, 143, 117, 314, 338, 193, 207, 1047, 682, 1239, 9, 1393, 661, 161, 235, 18, 514, 26, 307, 450, 178, 135, 324, 338, 454, 117, 729, 143, 18, 442, 869, 549, 503, 164, 1393, 661, 56, 486, 185, 331, 9, 369, 157, 566, 317, 70]       Instance: 8000; Time: 235.12s; loss: 208.8093; acc: 375896.0/383644.0=0.9798       Instance: 16000; Time: 277.37s; loss: 207.2930; acc: 755929.0/771413.0=0.9799       Instance: 24000; Time: 286.95s; loss: 205.1098; acc: 1133844.0/1156868.0=0.9801       Instance: 32000; Time: 248.53s; loss: 205.7095; acc: 1515016.0/1545549.0=0.9802       Instance: 40000; Time: 236.95s; loss: 207.1384; acc: 1892844.0/1931009.0=0.9802       Instance: 45000; Time: 168.23s; loss: 128.2052; acc: 2128749.0/2171573.0=0.9803       Instance: 45000; Time: 0.12s; loss: 0.0000; acc: 2128749.0/2171573.0=0.9803  Epoch: 12 training finished. Time: 1453.26s, speed: 30.96st/s,  total loss: 1162.2650909423828  totalloss: 1162.2650909423828  Right token =  214675  All token =  217223  acc =  0.9882701187259176  Dev: time: 28.42s, speed: 160.68st/s; acc: 0.9883, p: 0.8760, r: 0.8041, f: 0.8385  Exceed previous best f score: 0.8379290285049449  Save current best model in file: sample_data/lstmcrf.12.model  Right token =  166815  All token =  170174  acc =  0.9802613795291878  Test: time: 25.66s, speed: 134.99st/s; acc: 0.9803, p: 0.8685, r: 0.8105, f: 0.8385  Epoch: 13/1000  Shuffle: first input word list: [300, 485, 55, 28, 18, 410, 1452, 546, 250, 454, 9, 95, 3208, 454, 3394, 87, 1023, 299, 88, 1990, 9, 533, 192, 379, 343, 70]       Instance: 8000; Time: 262.30s; loss: 200.5388; acc: 379833.0/387317.0=0.9807       Instance: 16000; Time: 277.50s; loss: 201.5161; acc: 759113.0/774017.0=0.9807       Instance: 24000; Time: 234.44s; loss: 196.7661; acc: 1136447.0/1158779.0=0.9807       Instance: 32000; Time: 247.29s; loss: 203.0335; acc: 1516604.0/1546681.0=0.9806       Instance: 40000; Time: 265.18s; loss: 194.8583; acc: 1893686.0/1930949.0=0.9807       Instance: 45000; Time: 171.57s; loss: 130.8293; acc: 2129455.0/2171573.0=0.9806       Instance: 45000; Time: 0.16s; loss: 0.0000; acc: 2129455.0/2171573.0=0.9806  Epoch: 13 training finished. Time: 1458.44s, speed: 30.85st/s,  total loss: 1127.5420989990234  totalloss: 1127.5420989990234  Right token =  214755  All token =  217223  acc =  0.9886384038522624  Dev: time: 28.37s, speed: 160.96st/s; acc: 0.9886, p: 0.8657, r: 0.8207, f: 0.8426  Exceed previous best f score: 0.8385129588366365  Save current best model in file: sample_data/lstmcrf.13.model  Right token =  166675  All token =  170174  acc =  0.97943869216214  Test: time: 25.75s, speed: 134.50st/s; acc: 0.9794, p: 0.8576, r: 0.8154, f: 0.8360"
None
"您好，非常感谢您能分享代码，但是我在用您的代码时出现一点问题，希望您能帮忙解答。  我使用的是conll2002西班牙语的数据集，词向量是从MUSE中下载的 fastText Wikipedia embeddings。在跑CNN+LSTM+CRF时发生了梯度爆炸，但是跑CLSTM+LSTM+CRF和GRU+LSTM+CRF就没有问题，想请问一下，为什么会这样，这是由什么原因造成的？  非常感谢！  Seed num: 42  MODEL: train  Find feature:  NP]  Load pretrained word embedding, norm: False, dir: sample_data/wiki.es.vec  Embedding:       pretrain word:983630, prefect match:18012, case_match:8758, oov:1742, oov%:0.0610949391505629  Training model...  ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++  DATA SUMMARY START:   I/O:       Start   Sequence   Laebling   task...       Tag          scheme: BIO       Split         token:  |||        MAX SENTENCE LENGTH: 250       MAX   WORD   LENGTH: -1       Number   normalized: True       Word  alphabet size: 28513       Char  alphabet size: 84       Label alphabet size: 10       Word embedding  dir: sample_data/wiki.es.vec       Char embedding  dir: None       Word embedding size: 300       Char embedding size: 30       Norm   word     emb: False       Norm   char     emb: False       Train  file directory: sample_data/esp.train       Dev    file directory: sample_data/esp.testa       Test   file directory: sample_data/esp.testb       Raw    file directory: None       Dset   file directory: None       Model  file directory: sample_data/lstmcrf       Loadmodel   directory: None       Decode file directory: None       Train instance number: 8320       Dev   instance number: 1915       Test  instance number: 1517       Raw   instance number: 0       FEATURE num: 1           Fe: NP]  alphabet  size: 62           Fe: NP]  embedding  dir: None           Fe: NP]  embedding size: 20           Fe: NP]  norm       emb: False   ++++++++++++++++++++++++++++++++++++++++   Model Network:       Model        use_crf: True       Model word extractor: LSTM       Model       use_char: True       Model char extractor: CNN       Model char_hidden_dim: 50   ++++++++++++++++++++++++++++++++++++++++   Training:       Optimizer: SGD       Iteration: 100       BatchSize: 10       Average  batch   loss: False   ++++++++++++++++++++++++++++++++++++++++   Hyperparameters:       Hyper              lr: 0.015       Hyper        lr_decay: 0.05       Hyper         HP_clip: None       Hyper        momentum: 0.0       Hyper              l2: 1e-08       Hyper      hidden_dim: 200       Hyper         dropout: 0.5       Hyper      lstm_layer: 1       Hyper          bilstm: True       Hyper             GPU: False  DATA SUMMARY END.  ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++  build sequence labeling network...  use_char:  True  char feature extractor:  CNN  word feature extractor:  LSTM  use crf:  True  build word sequence feature extractor: LSTM...  build word representation...  build char sequence feature extractor: CNN ...  build CRF...  Epoch: 0/100   Learning rate is set as: 0.015  Shuffle: first input word list: [112, 122, 37, 86, 39, 29, 123, 23, 21, 124, 6, 86, 39, 37, 56, 125, 21, 126, 127, 6, 128, 129, 37, 21, 130, 131, 132, 133, 134, 86, 135, 68, 136, 137, 65, 39, 51, 10]       Instance: 500; Time: 17.48s; loss: 66744.3432; acc: 12547.0/16152.0=0.7768       Instance: 1000; Time: 17.76s; loss: 36117.9189; acc: 25307.0/32388.0=0.7814       Instance: 1500; Time: 18.82s; loss: 33063.2673; acc: 38117.0/48536.0=0.7853       Instance: 2000; Time: 17.73s; loss: 22335.1410; acc: 49616.0/63012.0=0.7874       Instance: 2500; Time: 18.96s; loss: 23658.9066; acc: 62782.0/79203.0=0.7927       Instance: 3000; Time: 18.44s; loss: 23325.6564; acc: 75591.0/95059.0=0.7952       Instance: 3500; Time: 18.57s; loss: 25712.5104; acc: 87833.0/110454.0=0.7952       Instance: 4000; Time: 19.91s; loss: 22649.9701; acc: 100420.0/126189.0=0.7958       Instance: 4500; Time: 19.22s; loss: 24327.3322; acc: 112567.0/141422.0=0.7960       Instance: 5000; Time: 19.85s; loss: 26658.7230; acc: 124379.0/156725.0=0.7936       Instance: 5500; Time: 18.37s; loss: 25029.7343; acc: 136149.0/172008.0=0.7915       Instance: 6000; Time: 18.65s; loss: 29637.6336; acc: 149560.0/188728.0=0.7925       Instance: 6500; Time: 19.05s; loss: 24267.2758; acc: 161762.0/204421.0=0.7913       Instance: 7000; Time: 19.61s; loss: 22461.7405; acc: 175534.0/221536.0=0.7923       Instance: 7500; Time: 18.50s; loss: 22996.7426; acc: 187587.0/237142.0=0.7910       Instance: 8000; Time: 18.71s; loss: 21329.9551; acc: 199753.0/252760.0=0.7903       Instance: 8320; Time: 12.32s; loss: 12823.4778; acc: 208061.0/262902.0=0.7914  Epoch: 0 training finished. Time: 311.95s, speed: 26.67st/s,  total loss: 463140.3287963867  totalloss: 463140.3287963867  Right token =  10789  All token =  52923  acc =  0.20386221491601006  Dev: time: 14.95s, speed: 128.68st/s; acc: 0.2039, p: 0.0070, r: 0.0097, f: 0.0081  Exceed previous best f score: -10  Save current best model in file: sample_data/lstmcrf.0.model  Right token =  9090  All token =  51533  acc =  0.17639182659654978  Test: time: 13.96s, speed: 110.87st/s; acc: 0.1764, p: 0.0035, r: 0.0059, f: 0.0044  Epoch: 1/100   Learning rate is set as: 0.014285714285714285  Shuffle: first input word list: [4192, 33, 41, 5335, 102, 3068, 6, 41, 16619, 8961, 16625, 33, 8284, 157, 7171, 23, 8823, 157, 3, 850, 235, 304, 5, 62, 12097, 109, 7593, 235, 39, 157, 8307, 23, 21, 900, 157, 3, 850, 235, 304, 5, 6, 1404, 23, 361, 23, 21, 16399, 3, 850, 235, 304, 5, 62, 37, 114, 16626, 261, 16627, 3, 850, 235, 304, 5, 6, 16628, 39, 16623, 10]       Instance: 500; Time: 20.07s; loss: 22066.5989; acc: 12368.0/15718.0=0.7869       Instance: 1000; Time: 19.20s; loss: 15910.0819; acc: 25232.0/31352.0=0.8048       Instance: 1500; Time: 19.42s; loss: 16232.9181; acc: 37775.0/46995.0=0.8038       Instance: 2000; Time: 19.53s; loss: 14678.2134; acc: 51808.0/63706.0=0.8132       Instance: 2500; Time: 19.44s; loss: 15756.1388; acc: 64652.0/79451.0=0.8137       Instance: 3000; Time: 19.07s; loss: 18944.6857; acc: 77324.0/95315.0=0.8112       Instance: 3500; Time: 18.91s; loss: 15556.8204; acc: 89668.0/110505.0=0.8114       Instance: 4000; Time: 18.46s; loss: 18086.7941; acc: 101804.0/125771.0=0.8094       Instance: 4500; Time: 19.44s; loss: 16134.2563; acc: 114591.0/141392.0=0.8104       Instance: 5000; Time: 19.51s; loss: 19310.2681; acc: 126960.0/157040.0=0.8085       Instance: 5500; Time: 19.66s; loss: 20971.3447; acc: 139678.0/173303.0=0.8060       Instance: 6000; Time: 19.37s; loss: 16266.7186; acc: 152363.0/188912.0=0.8065       Instance: 6500; Time: 19.58s; loss: 21224.6567; acc: 164716.0/204945.0=0.8037       Instance: 7000; Time: 20.10s; loss: 14005.5008; acc: 177883.0/220961.0=0.8050       Instance: 7500; Time: 19.66s; loss: 16057.6001; acc: 190685.0/236599.0=0.8059       Instance: 8000; Time: 20.49s; loss: 14424.4894; acc: 203964.0/252622.0=0.8074       Instance: 8320; Time: 13.11s; loss: 9627.8608; acc: 212431.0/262902.0=0.8080  Epoch: 1 training finished. Time: 325.03s, speed: 25.60st/s,  total loss: 285254.94677734375  totalloss: 285254.94677734375  Right token =  45356  All token =  52923  acc =  0.8570186875271621  Dev: time: 16.07s, speed: 119.56st/s; acc: 0.8570, p: -1.0000, r: 0.0000, f: -1.0000  Right token =  45355  All token =  51533  acc =  0.8801156540469214  Test: time: 14.67s, speed: 103.71st/s; acc: 0.8801, p: -1.0000, r: 0.0000, f: -1.0000  Epoch: 2/100   Learning rate is set as: 0.013636363636363634  Shuffle: first input word list: [4629, 4641, 4642, 33, 41, 3264, 23, 50, 29, 23, 2373, 4643, 37, 39, 3269, 2392, 4644, 834, 109, 3449, 86, 4645, 168, 109, 2036, 3280, 23, 1215, 6, 70, 37, 6, 913, 39, 4639, 4640, 6, 3487, 50, 4646, 4647, 23, 21, 4484, 15, 4648, 10]       Instance: 500; Time: 19.82s; loss: 20885.3103; acc: 12893.0/16169.0=0.7974       Instance: 1000; Time: 20.22s; loss: 18628.8053; acc: 26086.0/32552.0=0.8014       Instance: 1500; Time: 19.46s; loss: 15202.7120; acc: 38418.0/47930.0=0.8015       Instance: 2000; Time: 19.52s; loss: 18382.5779; acc: 50024.0/62925.0=0.7950       Instance: 2500; Time: 18.99s; loss: 15396.2651; acc: 62468.0/78217.0=0.7986       Instance: 3000; Time: 19.76s; loss: 19577.7411; acc: 74929.0/94184.0=0.7956       Instance: 3500; Time: 19.24s; loss: 18076.7267; acc: 87007.0/109485.0=0.7947       Instance: 4000; Time: 19.66s; loss: 16454.8464; acc: 99954.0/125322.0=0.7976       Instance: 4500; Time: 19.88s; loss: 16385.4592; acc: 112478.0/140793.0=0.7989       Instance: 5000; Time: 20.46s; loss: 14677.9603; acc: 125713.0/157081.0=0.8003       Instance: 5500; Time: 20.97s; loss: 21058.0332; acc: 139035.0/173825.0=0.7999       Instance: 6000; Time: 19.35s; loss: 11340.9529; acc: 152017.0/188874.0=0.8049       Instance: 6500; Time: 20.22s; loss: 16436.0164; acc: 164729.0/204922.0=0.8039       Instance: 7000; Time: 20.76s; loss: 17523.4183; acc: 177801.0/221258.0=0.8036       Instance: 7500; Time: 20.60s; loss: 14272.2194; acc: 191142.0/237650.0=0.8043       Instance: 8000; Time: 20.56s; loss: 12585.5363; acc: 203910.0/253135.0=0.8055       Instance: 8320; Time: 12.77s; loss: 8050.8893; acc: 211887.0/262902.0=0.8060  Epoch: 2 training finished. Time: 332.23s, speed: 25.04st/s,  total loss: 274935.4700317383  totalloss: 274935.4700317383  Right token =  45356  All token =  52923  acc =  0.8570186875271621  Dev: time: 16.64s, speed: 115.44st/s; acc: 0.8570, p: -1.0000, r: 0.0000, f: -1.0000  Right token =  45355  All token =  51533  acc =  0.8801156540469214  Test: time: 15.29s, speed: 99.48st/s; acc: 0.8801, p: -1.0000, r: 0.0000, f: -1.0000"
"您好，metric.py中的get_ner_fmeasure函数有段代码如下：     这里第一个print应该是用于报告NER的预测情况，如果是这样的话，条件似乎应该改为     否者无论是'BIO','BIOES'还是‘BMES’标注格式，都会执行第二个print，输出acc值。"
"hello, I noticed you use the pytorch function  ""clone()"" in crf layer.  `partition = inivalues[:, START_TAG, :].clone().view(batch_size, tag_size, 1)`  I want to know why use the clone function, thanks."
"Hi,  I tried this for our language Malayalam pos tagging by creating different models. I got best accuracy approx 87% with soft+ LSTM+LSTM model for 30000 (24000 train, 300 Dec,and 3000 test) pos tagged instances without embeddings. Could please help me to make sure my experiments are correct or not ? Also this data set is enouph for my experiments? Any suggestions please ..    Hyper-parameter Value  character dimension 50  word dimension 200  update function SGD  learning rate 0.05  learning decay rate 0.05  dropout rate 0.5  lstm-layer 1  batch size 10  iterations 10  gpu False"
"你好！感觉您分享的代码  目前我在做一些实验，想更改一下ACC的计算方式。对metric.py内的函数get_ner_fmeasure内的for idx in range(0,sent_num)的循环体进行了改动，但实验后ACC并没有发生改变，您能否给我一些建议，是否要对其它地方进行改动。我的语料是中文BIO标注的，目前测试集能在NCRF++上得到78%的ACC，还在进一步调参，这方面能否给些建议，万分感谢！！！"
"(torch) zhengyi_ma@ubuntu:~/NCRFpp-ner-2/NCRFpp$ python main.py --config demo.decode.config   Seed num: 42  MODEL: decode  sample_data/mytest.bmes  ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++  DATA SUMMARY START:   I/O:       Start   Sequence   Laebling   task...       Tag          scheme: BMES       Split         token:  |||        MAX SENTENCE LENGTH: 250       MAX   WORD   LENGTH: -1       Number   normalized: True       Word  alphabet size: 375876       Char  alphabet size: 8168       Label alphabet size: 6       Word embedding  dir: sample_data/sgns.baidubaike.bigram-char       Char embedding  dir: None       Word embedding size: 300       Char embedding size: 30       Norm   word     emb: False       Norm   char     emb: False       Train  file directory: sample_data/mytrain.bmes       Dev    file directory: sample_data/mydev.bmes       Test   file directory: sample_data/mytest.bmes       Raw    file directory: sample_data/mytest.bmes       Dset   file directory: sample_data/lstmcrf.dset       Model  file directory: sample_data/lstmcrf       Loadmodel   directory: sample_data/lstmcrf.7.model       Decode file directory: sample_data/raw.out       Train instance number: 149900       Dev   instance number: 21623       Test  instance number: 23099       Raw   instance number: 0       FEATURE num: 1           Fe: [POS]  alphabet  size: 9292           Fe: [POS]  embedding  dir: None           Fe: [POS]  embedding size: 20           Fe: [POS]  norm       emb: False   ++++++++++++++++++++++++++++++++++++++++   Model Network:       Model        use_crf: True       Model word extractor: LSTM       Model       use_char: True       Model char extractor: CNN       Model char_hidden_dim: 50   ++++++++++++++++++++++++++++++++++++++++   Training:       Optimizer: SGD       Iteration: 25       BatchSize: 10       Average  batch   loss: False   ++++++++++++++++++++++++++++++++++++++++   Hyperparameters:       Hyper              lr: 0.015       Hyper        lr_decay: 0.05       Hyper         HP_clip: None       Hyper        momentum: 0.0       Hyper              l2: 1e-08       Hyper      hidden_dim: 200       Hyper         dropout: 0.5       Hyper      lstm_layer: 1       Hyper          bilstm: True       Hyper             GPU: True  DATA SUMMARY END.  ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++  nbest: 10  Load Model from file:  sample_data/lstmcrf  build sequence labeling network...  use_char:  True  char feature extractor:  CNN  word feature extractor:  LSTM  use crf:  True  build word sequence feature extractor: LSTM...  build word representation...  build char sequence feature extractor: CNN ...  build CRF...  Decode raw data, nbest: 10 ...  Traceback (most recent call last):    File ""main.py"", line 573, in        decode_results, pred_scores = load_model_decode(data, 'raw')    File ""main.py"", line 502, in load_model_decode      speed, acc, p, r, f, pred_results, pred_scores = evaluate(data, model, name, data.nbest)    File ""main.py"", line 171, in evaluate      scores, nbest_tag_seq = model.decode_nbest(batch_word,batch_features, batch_wordlen, batch_char, batch_charlen, batch_charrecover, mask, nbest)    File ""/home/zhengyi_ma/NCRFpp-ner-2/NCRFpp/model/seqlabel.py"", line 81, in decode_nbest      scores, tag_seq = self.crf._viterbi_decode_nbest(outs, mask, nbest)    File ""/home/zhengyi_ma/NCRFpp-ner-2/NCRFpp/model/crf.py"", line 318, in _viterbi_decode_nbest      partition, cur_bp = torch.topk(cur_values, nbest, 1)  RuntimeError: invalid argument 5: k not in range for dimension at /opt/conda/conda-bld/pytorch_1550784378911/work/aten/src/THC/generic/THCTensorTopK.cu:21      anyone get this error ? "
"Hello,  Thank you for providing this amazing framework to the community.  I have a question related to input vectors?   Could I use FastText embeddings instead of Word2vec/Glove embeddings?  Also, could I use this framework for other languages like Arabic, Turkish or Czech for example or it supports English only?"
"Hi,  I'm unable to reproduce results for ConLL2003 NER data.  I even tried converting my data from BIO-BIOES using your script. Please help me understand what I've done wrong.    Below is the log with BIOES train:     Seed num: 42  MODEL: train  Load pretrained word embedding, norm: False, dir: ../data/glove.6B/glove.6B.100d.txt  Embedding:       pretrain word:400000, prefect match:3, case_match:0, oov:1, oov%:0.2  Training model...  ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++  DATA SUMMARY START:   I/O:       Start   Sequence   Laebling   task...       Tag          scheme: BMES       Split         token:  |||       MAX SENTENCE LENGTH: 250       MAX   WORD   LENGTH: -1       Number   normalized: True       Word  alphabet size: 5       Char  alphabet size: 3       Label alphabet size: 18       Word embedding  dir: ../data/glove.6B/glove.6B.100d.txt       Char embedding  dir: None       Word embedding size: 100       Char embedding size: 30       Norm   word     emb: False       Norm   char     emb: False       Train  file directory: ../data/modified_data/eng.train.bioes       Dev    file directory: ../data/modified_data/eng.testa.bioes       Test   file directory: ../data/modified_data/eng.testb.bioes       Raw    file directory: None       Dset   file directory: None       Model  file directory: ../data/charcnnwordlstmcrftestbioes       Loadmodel   directory: None       Decode file directory: None       Train instance number: 14986       Dev   instance number: 3465       Test  instance number: 3683       Raw   instance number: 0       FEATURE num: 0   ++++++++++++++++++++++++++++++++++++++++   Model Network:       Model        use_crf: True       Model word extractor: LSTM       Model       use_char: True       Model char extractor: CNN       Model char_hidden_dim: 50   ++++++++++++++++++++++++++++++++++++++++   Training:       Optimizer: SGD       Iteration: 100       BatchSize: 10       Average  batch   loss: True   ++++++++++++++++++++++++++++++++++++++++   Hyperparameters:       Hyper              lr: 0.015       Hyper        lr_decay: 0.05       Hyper         HP_clip: None       Hyper        momentum: 0.0       Hyper              l2: 1e-08       Hyper      hidden_dim: 200       Hyper         dropout: 0.5       Hyper      lstm_layer: 1       Hyper          bilstm: True       Hyper             GPU: True  DATA SUMMARY END.  ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++  build sequence labeling network...  use_char:  True  char feature extractor:  CNN  word feature extractor:  LSTM  use crf:  True  build word sequence feature extractor: LSTM...  build word representation...  build char sequence feature extractor: CNN ...  build CRF...  Epoch: 0/100   Learning rate is set as: 0.015  Shuffle: first input word list: [2, 2, 2, 2, 2, 2, 2]       Instance: 500; Time: 5.65s; loss: 688.5860; acc: 5410/6734=0.8034       Instance: 1000; Time: 5.89s; loss: 572.2787; acc: 11774/14229=0.8275       Instance: 1500; Time: 5.42s; loss: 527.8886; acc: 18033/21551=0.8368       Instance: 2000; Time: 5.59s; loss: 551.2242; acc: 23748/28433=0.8352       Instance: 2500; Time: 5.40s; loss: 512.4623; acc: 29489/35255=0.8364       Instance: 3000; Time: 5.25s; loss: 481.4681; acc: 35058/41839=0.8379       Instance: 3500; Time: 5.73s; loss: 554.1080; acc: 41089/49084=0.8371       Instance: 4000; Time: 5.31s; loss: 514.6246; acc: 46476/55640=0.8353       Instance: 4500; Time: 5.15s; loss: 521.3245; acc: 51936/62275=0.8340       Instance: 5000; Time: 5.42s; loss: 526.3831; acc: 57730/69309=0.8329       Instance: 5500; Time: 5.54s; loss: 520.3846; acc: 63471/76261=0.8323       Instance: 6000; Time: 5.72s; loss: 480.5155; acc: 69338/83262=0.8328       Instance: 6500; Time: 5.51s; loss: 495.4523; acc: 75078/90162=0.8327       Instance: 7000; Time: 5.60s; loss: 488.3297; acc: 81048/97261=0.8333       Instance: 7500; Time: 5.44s; loss: 469.5476; acc: 86391/103745=0.8327       Instance: 8000; Time: 5.51s; loss: 477.6908; acc: 92115/110597=0.8329       Instance: 8500; Time: 5.34s; loss: 466.7520; acc: 97842/117457=0.8330       Instance: 9000; Time: 5.42s; loss: 460.8916; acc: 103260/124017=0.8326       Instance: 9500; Time: 5.39s; loss: 421.7527; acc: 108522/130318=0.8327       Instance: 10000; Time: 5.57s; loss: 444.7986; acc: 114220/137079=0.8332       Instance: 10500; Time: 5.37s; loss: 438.1787; acc: 119643/143589=0.8332       Instance: 11000; Time: 5.47s; loss: 443.6167; acc: 125421/150452=0.8336       Instance: 11500; Time: 5.66s; loss: 465.6778; acc: 131328/157521=0.8337       Instance: 12000; Time: 5.70s; loss: 441.7232; acc: 137153/164439=0.8341       Instance: 12500; Time: 5.23s; loss: 456.5177; acc: 142728/171152=0.8339       Instance: 13000; Time: 5.33s; loss: 443.0195; acc: 148391/177974=0.8338       Instance: 13500; Time: 5.07s; loss: 436.1295; acc: 153527/184243=0.8333       Instance: 14000; Time: 5.61s; loss: 488.2037; acc: 159389/191383=0.8328       Instance: 14500; Time: 5.53s; loss: 435.6175; acc: 164958/198074=0.8328       Instance: 14986; Time: 5.28s; loss: 398.8262; acc: 170415/204566=0.8331  Epoch: 0 training finished. Time: 164.08s, speed: 91.33st/s,  total loss: 14623.973976135254  totalloss: 14623.973976135254  Right token =  42974  All token =  51577  acc =  0.8332008453380383  Dev: time: 6.14s, speed: 565.80st/s; acc: 0.8332, p: -1.0000, r: 0.0000, f: -1.0000  Exceed previous best f score: -10  Save current best model in file: ../data/charcnnwordlstmcrftestbioes.0.model  Right token =  38553  All token =  46665  acc =  0.8261652201864352  Test: time: 5.83s, speed: 635.29st/s; acc: 0.8262, p: -1.0000, r: 0.0000, f: -1.0000  Epoch: 1/100   Learning rate is set as: 0.014285714285714285  Shuffle: first input word list: [2, 2]       Instance: 500; Time: 5.08s; loss: 431.2016; acc: 5391/6521=0.8267       Instance: 1000; Time: 5.47s; loss: 439.1820; acc: 10921/13222=0.8260       Instance: 1500; Time: 5.50s; loss: 423.6805; acc: 16460/19866=0.8286       Instance: 2000; Time: 5.46s; loss: 425.2254; acc: 22446/26950=0.8329       Instance: 2500; Time: 5.90s; loss: 435.6300; acc: 28570/34178=0.8359       Instance: 3000; Time: 5.35s; loss: 444.1788; acc: 34283/41039=0.8354       Instance: 3500; Time: 5.36s; loss: 430.6110; acc: 39931/47798=0.8354       Instance: 4000; Time: 5.26s; loss: 431.6148; acc: 45356/54376=0.8341       Instance: 4500; Time: 5.55s; loss: 450.8224; acc: 51271/61468=0.8341       Instance: 5000; Time: 5.46s; loss: 429.3036; acc: 57317/68630=0.8352       Instance: 5500; Time: 5.60s; loss: 449.7797; acc: 63241/75743=0.8349       Instance: 6000; Time: 5.27s; loss: 433.6448; acc: 68693/82380=0.8339       Instance: 6500; Time: 5.71s; loss: 456.3143; acc: 74874/89753=0.8342       Instance: 7000; Time: 5.25s; loss: 436.8700; acc: 80510/96573=0.8337       Instance: 7500; Time: 5.63s; loss: 430.2619; acc: 86279/103481=0.8338       Instance: 8000; Time: 5.72s; loss: 412.8583; acc: 92142/110434=0.8344       Instance: 8500; Time: 5.54s; loss: 398.9669; acc: 97755/117099=0.8348       Instance: 9000; Time: 5.58s; loss: 401.6885; acc: 103315/123734=0.8350       Instance: 9500; Time: 5.71s; loss: 401.0224; acc: 109076/130581=0.8353       Instance: 10000; Time: 5.61s; loss: 436.5150; acc: 114616/137309=0.8347       Instance: 10500; Time: 5.47s; loss: 422.0465; acc: 120169/144025=0.8344       Instance: 11000; Time: 5.45s; loss: 392.8255; acc: 125509/150463=0.8342       Instance: 11500; Time: 5.66s; loss: 430.9805; acc: 131225/157359=0.8339       Instance: 12000; Time: 5.81s; loss: 429.5823; acc: 137266/164574=0.8341       Instance: 12500; Time: 5.47s; loss: 409.2201; acc: 143011/171457=0.8341       Instance: 13000; Time: 5.26s; loss: 393.7457; acc: 148514/178056=0.8341       Instance: 13500; Time: 5.70s; loss: 444.4959; acc: 154245/184981=0.8338       Instance: 14000; Time: 5.38s; loss: 411.1087; acc: 159572/191452=0.8335       Instance: 14500; Time: 5.20s; loss: 397.2859; acc: 165118/198095=0.8335       Instance: 14986; Time: 5.40s; loss: 389.1057; acc: 170523/204566=0.8336  Epoch: 1 training finished. Time: 164.81s, speed: 90.93st/s,  total loss: 12719.768818855286  totalloss: 12719.768818855286  Right token =  42974  All token =  51577  acc =  0.8332008453380383  Dev: time: 6.16s, speed: 564.23st/s; acc: 0.8332, p: -1.0000, r: 0.0000, f: -1.0000  Right token =  38553  All token =  46665  acc =  0.8261652201864352  Test: time: 5.80s, speed: 638.96st/s; acc: 0.8262, p: -1.0000, r: 0.0000, f: -1.0000  Epoch: 2/100   Learning rate is set as: 0.013636363636363634  Shuffle: first input word list: [2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]       Instance: 500; Time: 5.41s; loss: 436.1395; acc: 5824/7012=0.8306       Instance: 1000; Time: 5.48s; loss: 385.2749; acc: 11357/13599=0.8351       Instance: 1500; Time: 5.53s; loss: 415.0793; acc: 17094/20450=0.8359       Instance: 2000; Time: 5.55s; loss: 413.8247; acc: 22636/27161=0.8334       Instance: 2500; Time: 5.40s; loss: 420.5595; acc: 28451/34152=0.8331       Instance: 3000; Time: 5.18s; loss: 362.7070; acc: 33769/40470=0.8344       Instance: 3500; Time: 5.71s; loss: 408.6057; acc: 39442/47317=0.8336       Instance: 4000; Time: 5.42s; loss: 415.7353; acc: 45278/54329=0.8334       Instance: 4500; Time: 5.26s; loss: 390.8038; acc: 50917/61030=0.8343       Instance: 5000; Time: 5.49s; loss: 410.7181; acc: 56446/67705=0.8337       Instance: 5500; Time: 5.55s; loss: 417.0718; acc: 62245/74659=0.8337       Instance: 6000; Time: 5.48s; loss: 416.0790; acc: 68118/81681=0.8340       Instance: 6500; Time: 5.19s; loss: 404.0871; acc: 73511/88224=0.8332       Instance: 7000; Time: 5.23s; loss: 430.6882; acc: 79219/95148=0.8326       Instance: 7500; Time: 5.26s; loss: 414.4986; acc: 84977/102097=0.8323       Instance: 8000; Time: 5.44s; loss: 372.8214; acc: 90537/108671=0.8331       Instance: 8500; Time: 5.29s; loss: 376.0572; acc: 95977/115146=0.8335       Instance: 9000; Time: 5.74s; loss: 393.9897; acc: 101715/122009=0.8337       Instance: 9500; Time: 5.60s; loss: 444.6560; acc: 107654/129181=0.8334       Instance: 10000; Time: 5.45s; loss: 422.0748; acc: 113211/135972=0.8326       Instance: 10500; Time: 5.55s; loss: 415.9887; acc: 118967/142896=0.8325       Instance: 11000; Time: 5.37s; loss: 392.2800; acc: 124211/149289=0.8320       Instance: 11500; Time: 5.67s; loss: 405.8225; acc: 130222/156428=0.8325       Instance: 12000; Time: 5.65s; loss: 414.1149; acc: 136121/163498=0.8326       Instance: 12500; Time: 5.58s; loss: 410.7859; acc: 141971/170495=0.8327       Instance: 13000; Time: 5.44s; loss: 406.8974; acc: 147548/177214=0.8326       Instance: 13500; Time: 5.27s; loss: 398.6541; acc: 153373/184149=0.8329       Instance: 14000; Time: 5.46s; loss: 404.1680; acc: 159096/190973=0.8331       Instance: 14500; Time: 5.27s; loss: 383.6540; acc: 165023/197968=0.8336       Instance: 14986; Time: 5.25s; loss: 397.4585; acc: 170523/204566=0.8336  Epoch: 2 training finished. Time: 163.20s, speed: 91.83st/s,  total loss: 12181.295450210571  totalloss: 12181.295450210571  Right token =  42974  All token =  51577  acc =  0.8332008453380383  Dev: time: 6.18s, speed: 563.14st/s; acc: 0.8332, p: -1.0000, r: 0.0000, f: -1.0000  Right token =  38553  All token =  46665  acc =  0.8261652201864352  Test: time: 5.81s, speed: 637.34st/s; acc: 0.8262, p: -1.0000, r: 0.0000, f: -1.0000  Epoch: 3/100   Learning rate is set as: 0.013043478260869566  Shuffle: first input word list: [2, 2, 2, 2]       Instance: 500; Time: 5.49s; loss: 401.5520; acc: 5588/6744=0.8286       Instance: 1000; Time: 5.47s; loss: 423.1180; acc: 11382/13726=0.8292       Instance: 1500; Time: 5.69s; loss: 397.5806; acc: 17080/20559=0.8308       Instance: 2000; Time: 5.37s; loss: 402.0758; acc: 22700/27326=0.8307       Instance: 2500; Time: 5.26s; loss: 373.3217; acc: 28362/34040=0.8332       Instance: 3000; Time: 5.38s; loss: 382.8468; acc: 33965/40726=0.8340       Instance: 3500; Time: 5.50s; loss: 377.1780; acc: 39843/47660=0.8360       Instance: 4000; Time: 5.24s; loss: 395.6849; acc: 45476/54430=0.8355       Instance: 4500; Time: 5.25s; loss: 400.7815; acc: 50939/61028=0.8347       Instance: 5000; Time: 5.53s; loss: 401.5997; acc: 57020/68232=0.8357       Instance: 5500; Time: 5.31s; loss: 384.0676; acc: 62393/74702=0.8352       Instance: 6000; Time: 5.23s; loss: 418.5926; acc: 68060/81529=0.8348       Instance: 6500; Time: 5.55s; loss: 428.4656; acc: 73992/88702=0.8342       Instance: 7000; Time: 5.52s; loss: 424.5894; acc: 79857/95815=0.8334       Instance: 7500; Time: 5.31s; loss: 385.3072; acc: 85075/102162=0.8327       Instance: 8000; Time: 5.55s; loss: 396.4581; acc: 90761/108985=0.8328       Instance: 8500; Time: 5.66s; loss: 360.9902; acc: 96596/115831=0.8339       Instance: 9000; Time: 5.41s; loss: 387.4009; acc: 102245/122568=0.8342       Instance: 9500; Time: 5.20s; loss: 387.1995; acc: 107829/129260=0.8342       Instance: 10000; Time: 5.58s; loss: 426.1336; acc: 113663/136333=0.8337       Instance: 10500; Time: 5.61s; loss: 390.8738; acc: 119387/143206=0.8337       Instance: 11000; Time: 5.63s; loss: 422.8063; acc: 125286/150325=0.8334       Instance: 11500; Time: 5.34s; loss: 387.0369; acc: 130685/156860=0.8331       Instance: 12000; Time: 5.58s; loss: 396.0940; acc: 136412/163692=0.8333       Instance: 12500; Time: 5.25s; loss: 367.3678; acc: 141830/170170=0.8335       Instance: 13000; Time: 5.46s; loss: 387.3940; acc: 147733/177130=0.8340       Instance: 13500; Time: 5.49s; loss: 405.8708; acc: 153590/184181=0.8339       Instance: 14000; Time: 5.58s; loss: 407.3771; acc: 159170/190949=0.8336       Instance: 14500; Time: 5.41s; loss: 380.1521; acc: 164731/197603=0.8336       Instance: 14986; Time: 5.46s; loss: 409.9660; acc: 170522/204566=0.8336  Epoch: 3 training finished. Time: 163.31s, speed: 91.76st/s,  total loss: 11909.882559418678  totalloss: 11909.882559418678  Right token =  42974  All token =  51577  acc =  0.8332008453380383  Dev: time: 6.23s, speed: 559.48st/s; acc: 0.8332, p: -1.0000, r: 0.0000, f: -1.0000  Right token =  38553  All token =  46665  acc =  0.8261652201864352  Test: time: 5.78s, speed: 639.40st/s; acc: 0.8262, p: -1.0000, r: 0.0000, f: -1.0000  Epoch: 4/100   Learning rate is set as: 0.0125  Shuffle: first input word list: [2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3]       Instance: 500; Time: 5.45s; loss: 404.0416; acc: 5866/7016=0.8361       Instance: 1000; Time: 5.30s; loss: 382.1240; acc: 11447/13708=0.8351       Instance: 1500; Time: 5.42s; loss: 381.1682; acc: 16952/20317=0.8344       Instance: 2000; Time: 5.23s; loss: 387.5546; acc: 22407/26895=0.8331       Instance: 2500; Time: 5.16s; loss: 381.2725; acc: 27803/33383=0.8328       Instance: 3000; Time: 5.33s; loss: 417.8431; acc: 33474/40260=0.8314       Instance: 3500; Time: 5.40s; loss: 389.9610; acc: 39179/47080=0.8322       Instance: 4000; Time: 5.62s; loss: 412.6207; acc: 45268/54354=0.8328       Instance: 4500; Time: 5.53s; loss: 406.7833; acc: 51011/61310=0.8320       Instance: 5000; Time: 5.39s; loss: 382.0359; acc: 56519/67895=0.8324       Instance: 5500; Time: 5.06s; loss: 382.4959; acc: 61973/74477=0.8321       Instance: 6000; Time: 5.46s; loss: 377.8582; acc: 67960/81522=0.8336       Instance: 6500; Time: 5.27s; loss: 367.1264; acc: 73301/87945=0.8335       Instance: 7000; Time: 5.19s; loss: 355.6229; acc: 78982/94648=0.8345       Instance: 7500; Time: 5.44s; loss: 394.8074; acc: 84629/101438=0.8343       Instance: 8000; Time: 5.37s; loss: 375.2287; acc: 90165/108049=0.8345       Instance: 8500; Time: 5.71s; loss: 376.5849; acc: 95960/114906=0.8351       Instance: 9000; Time: 5.66s; loss: 405.4678; acc: 101777/121910=0.8349       Instance: 9500; Time: 5.18s; loss: 391.5243; acc: 107120/128427=0.8341       Instance: 10000; Time: 5.39s; loss: 380.5630; acc: 112713/135117=0.8342       Instance: 10500; Time: 5.81s; loss: 431.6293; acc: 118645/142308=0.8337               Instance: 11000; Time: 5.37s; loss: 402.0669; acc: 124340/149140=0.8337       Instance: 11500; Time: 5.63s; loss: 409.2975; acc: 130087/156080=0.8335       Instance: 12000; Time: 5.36s; loss: 378.2141; acc: 135491/162592=0.8333       Instance: 12500; Time: 5.42s; loss: 376.7511; acc: 141190/169413=0.8334       Instance: 13000; Time: 5.36s; loss: 400.4109; acc: 146848/176215=0.8333       Instance: 13500; Time: 5.53s; loss: 404.5539; acc: 152630/183183=0.8332       Instance: 14000; Time: 5.68s; loss: 401.6745; acc: 158861/190583=0.8336       Instance: 14500; Time: 5.57s; loss: 413.2835; acc: 164735/197642=0.8335       Instance: 14986; Time: 5.51s; loss: 391.3215; acc: 170524/204566=0.8336  Epoch: 4 training finished. Time: 162.79s, speed: 92.06st/s,  total loss: 11761.887456417084  totalloss: 11761.887456417084  Right token =  42974  All token =  51577  acc =  0.8332008453380383  Dev: time: 6.23s, speed: 559.71st/s; acc: 0.8332, p: -1.0000, r: 0.0000, f: -1.0000  Right token =  38553  All token =  46665  acc =  0.8261652201864352  Test: time: 5.81s, speed: 638.25st/s; acc: 0.8262, p: -1.0000, r: 0.0000, f: -1.0000  Epoch: 5/100   Learning rate is set as: 0.012  Shuffle: first input word list: [2, 2, 2, 2, 2]       Instance: 500; Time: 5.66s; loss: 381.6468; acc: 5618/6734=0.8343       Instance: 1000; Time: 5.42s; loss: 400.6623; acc: 11431/13682=0.8355       Instance: 1500; Time: 5.40s; loss: 384.2276; acc: 17341/20712=0.8372       Instance: 2000; Time: 5.25s; loss: 356.9555; acc: 22822/27231=0.8381       Instance: 2500; Time: 5.37s; loss: 364.6856; acc: 28444/33907=0.8389       Instance: 3000; Time: 5.56s; loss: 416.1254; acc: 34933/41557=0.8406       Instance: 3500; Time: 5.26s; loss: 381.0427; acc: 40571/48293=0.8401       Instance: 4000; Time: 5.57s; loss: 398.7148; acc: 46353/55262=0.8388       Instance: 4500; Time: 5.44s; loss: 402.5182; acc: 52054/62129=0.8378       Instance: 5000; Time: 5.19s; loss: 385.0131; acc: 57506/68698=0.8371       Instance: 5500; Time: 5.38s; loss: 390.2033; acc: 63247/75573=0.8369       Instance: 6000; Time: 5.39s; loss: 383.5077; acc: 68634/82073=0.8363       Instance: 6500; Time: 5.45s; loss: 382.4222; acc: 74221/88769=0.8361       Instance: 7000; Time: 5.37s; loss: 383.9613; acc: 80011/95671=0.8363               Instance: 7500; Time: 5.45s; loss: 384.6937; acc: 85412/102232=0.8355       Instance: 8000; Time: 5.68s; loss: 378.3377; acc: 91017/108940=0.8355       Instance: 8500; Time: 5.64s; loss: 370.2925; acc: 96633/115631=0.8357       Instance: 9000; Time: 5.49s; loss: 408.3350; acc: 102144/122371=0.8347       Instance: 9500; Time: 5.56s; loss: 403.2952; acc: 107538/128995=0.8337       Instance: 10000; Time: 5.42s; loss: 362.4672; acc: 113294/135794=0.8343       Instance: 10500; Time: 5.35s; loss: 356.3109; acc: 118877/142433=0.8346       Instance: 11000; Time: 5.50s; loss: 406.9036; acc: 124881/149641=0.8345       Instance: 11500; Time: 5.48s; loss: 390.2478; acc: 130577/156486=0.8344       Instance: 12000; Time: 5.56s; loss: 402.3553; acc: 136459/163544=0.8344       Instance: 12500; Time: 5.40s; loss: 382.0685; acc: 142281/170469=0.8346       Instance: 13000; Time: 5.74s; loss: 445.6279; acc: 148165/177684=0.8339       Instance: 13500; Time: 5.15s; loss: 389.8673; acc: 153435/184116=0.8334       Instance: 14000; Time: 5.53s; loss: 417.4627; acc: 159433/191332=0.8333       Instance: 14500; Time: 5.19s; loss: 365.1926; acc: 164977/197924=0.8335       Instance: 14986; Time: 5.24s; loss: 381.3668; acc: 170524/204566=0.8336  Epoch: 5 training finished. Time: 163.08s, speed: 91.89st/s,  total loss: 11656.511207222939  totalloss: 11656.511207222939  Right token =  42974  All token =  51577  acc =  0.8332008453380383  Dev: time: 6.21s, speed: 562.23st/s; acc: 0.8332, p: -1.0000, r: 0.0000, f: -1.0000  Right token =  38553  All token =  46665  acc =  0.8261652201864352  Test: time: 5.77s, speed: 642.81st/s; acc: 0.8262, p: -1.0000, r: 0.0000, f: -1.0000  Epoch: 6/100   Learning rate is set as: 0.011538461538461537  Shuffle: first input word list: [2, 2, 2]       Instance: 500; Time: 5.48s; loss: 401.9486; acc: 5781/6965=0.8300       Instance: 1000; Time: 5.62s; loss: 384.5986; acc: 11536/13845=0.8332       Instance: 1500; Time: 5.51s; loss: 417.3854; acc: 17780/21308=0.8344       Instance: 2000; Time: 5.65s; loss: 393.7084; acc: 23749/28397=0.8363       Instance: 2500; Time: 5.35s; loss: 370.2277; acc: 29072/34825=0.8348       Instance: 3000; Time: 5.30s; loss: 349.6673; acc: 34605/41363=0.8366       Instance: 3500; Time: 5.32s; loss: 377.4291; acc: 40084/47947=0.8360       Instance: 4000; Time: 5.33s; loss: 369.7106; acc: 45505/54460=0.8356       Instance: 4500; Time: 5.75s; loss: 399.7611; acc: 51303/61446=0.8349       Instance: 5000; Time: 5.33s; loss: 376.3262; acc: 57062/68271=0.8358       Instance: 5500; Time: 5.10s; loss: 347.2517; acc: 62332/74578=0.8358       Instance: 6000; Time: 5.23s; loss: 375.7463; acc: 67894/81244=0.8357       Instance: 6500; Time: 5.36s; loss: 368.8458; acc: 73614/88035=0.8362       Instance: 7000; Time: 5.55s; loss: 380.6469; acc: 79431/94975=0.8363       Instance: 7500; Time: 5.43s; loss: 401.6817; acc: 85078/101798=0.8358       Instance: 8000; Time: 5.60s; loss: 391.9794; acc: 91319/109161=0.8366       Instance: 8500; Time: 5.53s; loss: 394.1160; acc: 97110/116108=0.8364       Instance: 9000; Time: 5.34s; loss: 377.3278; acc: 102649/122766=0.8361       Instance: 9500; Time: 5.46s; loss: 376.5606; acc: 107906/129159=0.8355       Instance: 10000; Time: 5.62s; loss: 369.9259; acc: 113081/135448=0.8349       Instance: 10500; Time: 5.52s; loss: 364.3385; acc: 118776/142220=0.8352       Instance: 11000; Time: 5.25s; loss: 396.5493; acc: 124348/148986=0.8346       Instance: 11500; Time: 5.75s; loss: 401.7784; acc: 130586/156383=0.8350       Instance: 12500; Time: 5.41s; loss: 381.1171; acc: 141723/169843=0.8344       Instance: 13000; Time: 5.46s; loss: 413.3094; acc: 147394/176781=0.8338       Instance: 13500; Time: 5.59s; loss: 400.8582; acc: 153411/183974=0.8339       Instance: 14000; Time: 5.29s; loss: 372.7107; acc: 159000/190650=0.8340       Instance: 14500; Time: 5.47s; loss: 391.3594; acc: 164541/197357=0.8337       Instance: 14986; Time: 5.59s; loss: 425.4650; acc: 170524/204566=0.8336  Epoch: 6 training finished. Time: 163.43s, speed: 91.70st/s,  total loss: 11572.986675977707  totalloss: 11572.986675977707  Right token =  42974  All token =  51577  acc =  0.8332008453380383  Dev: time: 6.17s, speed: 562.71st/s; acc: 0.8332, p: -1.0000, r: 0.0000, f: -1.0000  Right token =  38553  All token =  46665  acc =  0.8261652201864352  Test: time: 5.78s, speed: 640.24st/s; acc: 0.8262, p: -1.0000, r: 0.0000, f: -1.0000  Epoch: 7/100   Learning rate is set as: 0.01111111111111111  Shuffle: first input word list: [2, 2, 2]       Instance: 500; Time: 5.47s; loss: 374.9266; acc: 5570/6658=0.8366       Instance: 1000; Time: 5.37s; loss: 362.9837; acc: 11014/13176=0.8359       Instance: 1500; Time: 5.67s; loss: 386.5207; acc: 17108/20378=0.8395       Instance: 2000; Time: 5.62s; loss: 389.0672; acc: 22785/27220=0.8371       Instance: 2500; Time: 5.47s; loss: 382.5923; acc: 28687/34279=0.8369       Instance: 3000; Time: 5.26s; loss: 368.1319; acc: 34190/40901=0.8359       Instance: 3500; Time: 5.45s; loss: 393.2582; acc: 39740/47614=0.8346       Instance: 4000; Time: 5.34s; loss: 392.7045; acc: 45394/54422=0.8341       Instance: 4500; Time: 5.31s; loss: 377.3566; acc: 50738/60893=0.8332       Instance: 5000; Time: 5.41s; loss: 376.6756; acc: 56354/67626=0.8333       Instance: 5500; Time: 5.42s; loss: 389.5069; acc: 62048/74474=0.8331       Instance: 6000; Time: 5.48s; loss: 390.0989; acc: 67624/81203=0.8328       Instance: 6500; Time: 5.69s; loss: 403.9725; acc: 73701/88454=0.8332       Instance: 7000; Time: 5.58s; loss: 382.9669; acc: 79345/95223=0.8333       Instance: 7500; Time: 5.52s; loss: 385.3155; acc: 85046/102050=0.8334       Instance: 8000; Time: 5.37s; loss: 371.5117; acc: 90620/108739=0.8334       Instance: 8500; Time: 5.53s; loss: 390.2922; acc: 96421/115696=0.8334       Instance: 9000; Time: 5.51s; loss: 375.7071; acc: 101971/122356=0.8334       Instance: 9500; Time: 5.29s; loss: 368.7694; acc: 107760/129228=0.8339       Instance: 10000; Time: 5.38s; loss: 355.8612; acc: 113385/135885=0.8344       Instance: 10500; Time: 5.19s; loss: 381.4508; acc: 119086/142691=0.8346       Instance: 11000; Time: 5.58s; loss: 412.3629; acc: 124810/149675=0.8339       Instance: 11500; Time: 5.66s; loss: 386.1944; acc: 130869/156865=0.8343       Instance: 12000; Time: 5.53s; loss: 401.3127; acc: 136732/163900=0.8342       Instance: 12500; Time: 5.58s; loss: 404.7056; acc: 142454/170845=0.8338       Instance: 13000; Time: 5.33s; loss: 395.0213; acc: 147989/177557=0.8335       Instance: 13500; Time: 5.34s; loss: 366.7315; acc: 153658/184281=0.8338       Instance: 14000; Time: 5.26s; loss: 359.2290; acc: 159212/190901=0.8340       Instance: 14500; Time: 5.52s; loss: 375.3674; acc: 164716/197525=0.8339       Instance: 14986; Time: 5.51s; loss: 417.3557; acc: 170520/204566=0.8336  Epoch: 7 training finished. Time: 163.63s, speed: 91.58st/s,  total loss: 11517.950781226158  totalloss: 11517.950781226158  Right token =  42974  All token =  51577  acc =  0.8332008453380383  Dev: time: 6.18s, speed: 563.97st/s; acc: 0.8332, p: -1.0000, r: 0.0000, f: -1.0000  Right token =  38553  All token =  46665  acc =  0.8261652201864352  Test: time: 5.90s, speed: 627.84st/s; acc: 0.8262, p: -1.0000, r: 0.0000, f: -1.0000  "
Thanks for sharing this projection!   I have a question about the data you used to train the model.     train the model use training set combine with dev set and the result is on test set?   or train the model only on the training set?  because I find that on this web   the reference said you use the combination of training and dev set. And many papers also use the combination of the data set.  I get confused ! can you help me?
"I am a bit confused about the line:          Doesn't shuffling the data throw away useful information about the sequence? For example, the data in `sample_data` consists of sequences of sentences which are delimited by document delimiter tokens (`-DOCSTART-`). If the sentences are shuffled the model cannot learn from sequential relationships between sentences.     I would imagine that for sentence classification tasks, shuffling in this way is probably worse still?  "
"Hello,    I opened an issue on Jan 24th asking about sentence/document classification functionality to which you replied there would be a release soon. Can you provide any updates on this? possibly an estimated release date?    Thank you"
How can we use the generated model to predict labels for a new raw data ?
您好，我发现您在alphabet.py中，将default index=0也当做了词表的一个元素，并计入词表的大小。这样做会导致模型的预测label中包含label 0，而在获取标签0对应的真实标签时，您是返回了词表中的第一个真实标签，请问这么做是否会影响到模型的准确性？即模型的分类结果会增加一个默认标签，且您将该默认标签等同于数据中的第一种真实标签。
"Why mapping the size of word representation vectors into the size of CNN hidden units?Can `self.word2cnn = nn.Linear(self.input_size, data.HP_hidden_dim)` be deleted and modify`self.cnn_list.append(nn.Conv1d(data.HP_hidden_dim, data.HP_hidden_dim, kernel_size=kernel, padding=pad_size))`to`self.cnn_list.append(nn.Conv1d(self.input_size, data.HP_hidden_dim, kernel_size=kernel, padding=pad_size))`?"
"Hi. First of all, thank you for your great project!    I just run your code with CoNLL dataset.  In the closed issues, you said that we should just change the iteration (1->100) in your demo.train.config file.   However, in your COLING 2018 paper, it states that it used 100-d glove word embedding for CCNN+WLSTM+CRF.   Is it right that we also have to change your demo config file as below?    word_emb_dir=glove  word_emb_dim=100    and also, I get 90.09 F-1 score on conll 2003 NER with 100d glove dataset.  Could you tell me what was the ""minimum"" score for CCNN+WLSTM+CRF..?    Thanks!"
"您好；    十分荣幸看到您的github，我在跑您代码的时候看到您的utils/metric.py中30行是不是要改成“if label_type == ""BMES"" or label_type == ""BIOES"":”这样？  !   "
"When word_feature_extractor== ""CNN"", why add a full connection layer before the convolution layer?"
"Instead of manually tweaking learning rate for SGD, use PyTorch built-in `lr_scheduler`:         StepLR or LambdaLR seem appropriate"
Use DataLoader to do shuffling and batching:     
Bug: wrong indentation:    
"Your code is overly optimistic when comparing two **different** labelings. This becomes important when CRF decoding is **not** used and predicted BIOES labels can be invalid.    For example, your function `get_ner_fmeasure` will return F1=1.0, but token accuracy acc=0.5. This does not make any sense to me.    Here is the test code that fails, because computed f1 is 1.   "
"Hello,    I noticed that there is an option `sentence_classification` that is not mentioned in the README.    Could you provide some details on how to use this option? Does this only use sentence level labels? and if so how should the training data look?    Thanks"
"Why the default setting of ""ave_batch_loss"" in demo.train.config is False?  In what case I need to use the whole batch's loss? Doesn't that equal to increase the learning rate?"
"Hi, thanks for the great project!   So when I ran the demo.decode.config, it says 'SeqModel' is not defined.  it's in File ""main.py"", line 500, in load_model_decode, model = SeqModel(data);  Can you help with the issue?  Thanks a lot!"
"When I was doing decoding (sequence labeling), the last sentence from the input file is missing from the output file. This is not because of the MAX_SENT_LENGTH (I change that from 250 to 2500, but this problem is still there).    Then I tried to add a new blank line at the end of the input file, and then the problem is solved. It happens that none of the files inside sample_data ends with a blank line, and that's probably the reason. I guess the problem is in the `read_instance` function inside `function.py`, but haven't figured out where exactly it is yet."
"my config is   `   ### I/O ###  train_dir=data/example.train  dev_dir=data/example.dev  test_dir=data/texample.test  model_dir=save/lstmcrf  #word_emb_dir=data/vocab_column_300d_w2v.bin.zip    #raw_dir=  #decode_dir=  #dset_dir=  #load_model_dir=  #char_emb_dir=    norm_word_emb=False  norm_char_emb=False  number_normalized=True  seg=True  word_emb_dim=300  char_emb_dim=300    ###NetworkConfiguration###  use_crf=True  use_char=True  word_seq_feature=LSTM  char_seq_feature=CNN  #feature=[POS] emb_size=20  #feature=[Cap] emb_size=20  #nbest=1    ###TrainingSetting###  status=train  optimizer=SGD  iteration=20  batch_size=128  ave_batch_loss=True    ###Hyperparameters###  cnn_layer=4  char_hidden_dim=200  hidden_dim=200  dropout=0.5  lstm_layer=1  bilstm=True  learning_rate=0.015  lr_decay=0.05  momentum=0  l2=1e-8  gpu=True  #clip=    ` .   and my sample data is:  `  在 O   此人 O   的 O   一再 O   推荐 O   下 O   ， O   小张 B_PER   的 O   母亲 O   换 O   了 O   一个 O   一千多元 O   的 O   燃气灶 O   。 O     不过 O   ， O   等 O   小张 B_PER   回家 O   之后 O   ， O   上网 O   搜索 O   才 O   知道 O   换 O   的 O   这个 O   燃气灶 O   并 O   不值钱 O   。 O   `  I change use_char=False,  p,r,f still -1"
Do you know the cause of this problem?Thanks.
"Hi,    Even after running for multiple epochs (>15), I get the precision, recall and F1 score to be -1. The data I'm working with is in the BIO format.     The train config file :    Output log :      Weird element in output log : gold_num =  0  pred_num =  0  right_num =  0    Train Dataset Size : 50063 Tokens  Val Dataset Size : 5563 Tokens  Test Dataset Size : 4970 Tokens    The loss does seem to going down, so the model is learning but the predicted results messed up.    Note : Although the data is formatted in the BIO format, there are no 'I' tags for this instance of the dataset. Could that be causing an issue here?    Regards"
Is there a way to use to multiple GPUs?  
"Hi,     Can you please add a requirements.txt tp the repository?  When I followed your current instructions to use python 2 and pytorch 0.3, and ran `python main.py --config demo.train.config` I got the following error    !     Here's output of `pip freeze`  !   "
我在使用框架完成中文NER任务时，数据是用BIO格式标注的，训练一切正常，但是decode的时候生成的文件有部分数据会丢失。比如一个5219行的文件，decode之后只剩下前3429行。一个50W行的文件decode之后的文件只有30多W行，decode过程没有任何报错信息，请问有可能是什么原因造成的？是数据的问题吗？感谢您    下面是train的config文件  ### I/O ###  train_dir=data/train.data  dev_dir=data/dev.data  test_dir=data/test.data  model_dir=data/lstmcrf  word_emb_dir=data/sample.word.emb    #raw_dir=  #decode_dir=  #dset_dir=  #load_model_dir=  #char_emb_dir=    norm_word_emb=False  norm_char_emb=False  number_normalized=True  seg=True  word_emb_dim=50  char_emb_dim=30    ###NetworkConfiguration###  use_crf=True  use_char=False  word_seq_feature=LSTM  char_seq_feature=CNN  #feature=[POS] emb_size=20  #feature=[Cap] emb_size=20  #nbest=1    ###TrainingSetting###  status=train  optimizer=SGD  iteration=50  batch_size=256  ave_batch_loss=False    ###Hyperparameters###  cnn_layer=4  char_hidden_dim=50  hidden_dim=200  dropout=0.5  lstm_layer=1  bilstm=True  learning_rate=0.015  lr_decay=0.05  momentum=0  l2=1e-8  gpu=True  #clip=  下面是decode的config文件  ### Decode ###  status=decode  raw_dir=data/33.data  nbest=0  decode_dir=data/33.out  dset_dir=data/lstmcrf.dset  load_model_dir=data/lstmcrf.20.model    
"Hi,    I have changed the repository to allow for 100 iterations instead of 1. And there is no change in performance. What do I do ? I get a test f1-score of about 70% which is very poor."
"In NCRFpp/model/charbilstm.py     Is there any reason for ``char_hidden[0].transpose(1,0).contiguous().view(batch_size,-1)``?    I suppose we should concatenate ``char_hidden[0]`` and ``char_hidden[1]`` to form the feature of char BiLSTM"
"Hi, jie!  To reproduce the results of  `Char CNN + Word LSTM + CRF` model on CoNLL2003 NER task, I set the `iteration=1` as `iteration=100` in configuration file `demo.train.config` , put glove.6B.100d.txt  in directory ""sample_data""  and revised I/O paths. Then I ran ""python main.py --config demo.train.config"".  After 100 epochs, the test F1-score was only 81.86, which was much lower than reported result 91.11. Could you give me some suggestions? (I didn't revise any hyper-parameters in your default configuration file ""demo.train.config"" as you mentioned in your ""README.md"")"
"For the data you give, how do you set the train.config and decode.config to get the best result?  Thanks for your open code."
"In your comments within the CRF model:     to me, it seems there is a contradiction (to i from j vs from f_tag to t_tag). "
我使用LatticeLSTM项目中您提供的ResumeNER数据集，训练了多次，结果都是不收敛，请问可能是什么问题？多谢您啊！以下是我的日志记录：     谢谢！
"Hi @jiesutd,    This is just a grammar mistake, the code and print statements are full of the word ""setted"", but that word does not exist, the present and the past of set is set also :).    Regards,  Guillermo"
"Hi,    I have a question about CRF's initialization in module`crf.py`.    This content is cited from line 39 to line 43, where `START_TAG` is -2 and `STOP_TAG` is -1.       It is clear that `init_transitions[:,START_TAG]` and `init_transitions[STOP_TAG,:]` are set as -10000. That indicate that `any tag` to `start_tag` is almost impossible and `stop_tag` to `any tag` is almost impossible.    But I couldn't figure out why you set `init_transitions[:,0]` and `init_transitions[0,:]` to -10000.0? It seems that you don't allow `any tag` to be `tags[0]`, as well as not `tags[0]` to be `any tag`.    Best,    Junjie"
"gold_num =  992  pred_num =  0  right_num =  0  Dev: time: 2.86s, speed: 98.30st/s; acc: 0.8030, p: -1.0000, r: 0.0000, f: -1.0000  Exceed previous best f score: -10  Save current best model in file: sample_data/lstmcrf.0.model  gold_num =  930  pred_num =  0  right_num =  0  Test: time: 2.48s, speed: 103.37st/s; acc: 0.8011, p: -1.0000, r: 0.0000, f: -1.0000  why?"
"Hi again,  I want to get **label probability** for each token like that,    > Rockwell        NNP     B       B/0.992465  > International   NNP     I       I/0.979089  > Corp.   NNP     I       I/0.954883  > 's      POS     B       B/0.986396  > Tulsa   NNP     I       I/0.991966    Is it possible?"
"Hi Jie,           Thanks for making this awesome tool publicly available. Can you provide the parameter settings of the LSTM-CRF models you use to reproduce the results on CoNLL 03 NER dataset?           Thanks in advance~"
"Hi @jiesutd     For conll-2003 dataset, did you get optimal results for 1 layer lstm (if not, then how many did you stack to get optimal results?)"
"I have some OCR dataset and want to include numerical features of the data such as their position in the document, but don't want an embedding of it.  "
"Sorry for this repeative post, I'd just clicked submit accidently before I finished writting description.     As we can see in the _main.py_     words in the test and dev file are used to build **data.word_alphabet**  in _data.py_       So that the oov problem is hidden as a result of this operation.   If we comment _data.build_alphabet(data.dev_dir)_ and _data.build_alphabet(data.test_dir)_ in _main.py_, the f1 score will decrease by about 5% in test data.  So is this just a mistake or there are some other considerations concerning this operation?   Thanks"
"As we can see in the _main.py_     words in the test and dev file are used to build **data.word_alphabet**  in _data.py_       So that the oov problem is hidden as a result of this operation.   If we comment _data.build_alphabet(data.dev_dir)_ and _data.build_alphabet(data.test_dir)_ in _main.py_, the f1 score will decrease by about 5% in test data.  So is this just a mistake or there are some other considerations concerning this operation?   Thanks"
"The same issue as 22#. We use our dataset to train the NER model. The tag scheme is BIOES (The only difference is we used ""M-"" instead of ""I-""). These data have been test on your ""Lattice LSTM model"". They can get accurate p,r,f1 value. So I am confused about this. Why our f1 score is -1 and pred_num = 0 on this   model?"
"Hi,  I want to get probability of each output sequence ( not n-best score ) when decoding.  How to get this?   (How to get partition function Z when decoding?)"
"If you using glove 100 on Windows, it will probably have some errors about gbk encoding.  So the solution is changing the code in functions.py, line 128:  `with open(embedding_path, 'r') `as file:  add encoding = ""utf-8"" like the following:  `with open(embedding_path, 'r',encoding=""utf-8"") as file:`  "
    I think it should be 'total_batch = (train_num-1)//batch_size+1'.
I'm trying to get a pre-trained model to run via the command line using main_parse but am having issues.      It would be helpful to have some documentation on the command line arguments for this function.    Thanks
"Hi   @jiesutd jiesutd,    Thanks for sharing your work, it is an incredible job!  My GPU is too old to run this program, I was wondering that if I can use CPU to run this program?  I set the parameter ""#gpu"" to ""gpu = 0"" in ""demo.train.config"" file, and it does not work.    Regards,  Thanks!    "
"Hi,  I am using ncrfpp on my own dataset.  Adam can converge normally in fewer than 20 epochs.    However, optimizing with SGD is extremely hard. I got gradient explosion or non-convergence most of the time.   Removing dropouts and l2 regularization and using very small lr makes the training converge, but extremely slow.    Could you share your parameters used for training with SGD?   Many thanks!  "
"As the title states, the behavior is different on python2.   A better option is to use a standard logging module."
"hello,      In this experiment, if the development set is too small, will it have a big impact on the experimental results?thanks"
"I have tried running the configuration mentioned in readme on  GPU, with 10 different seeds.  I am still not able to hit f score of 90+ (for non lstm based results) or 91+ for lstm based result. "
"hello,  Can I change raw.bmes to test.bmes during decoding? And,  I want to know the role of raw.bmes."
How to do POS tagging (and NOT NER)? Is there any option to set that?
"Hi,  what am I doing wrong that the training with the same hyperparameters results always in the same performance? There should be some variance because of the random initialization or am i wrong?     Config looks like this:   "
#41     results are not reproducible by using params written in paper. Can you better provide config file for your results.
"Hi,    Can you provide your config files for the reported CoNLL numbers (under section 3. Performance)?    Thanks,  Yi"
"hi, i use this to do a sequence tag task, but when i decode, the resulf of 1 best if the same of 10-best, how to use n-best??"
"Hi,  Can you please give some guidelines as to how can I unfreeze the word vectors?    I have the embedding file, word gets converted to embedding, but I want this embedding to get tweaked while training ('hence unfreezing'). Can you help?    Thanks"
"Thanks for the nice work. A minor issue, you have implemented support for Python 3 by catching the `ModuleNotFoundError` exception, which is fine for Python version 3.6 but will cause an error in versions <=`3.5`.    A quick solution would be to use `ImportError` instead of `ModuleNotFoundError`, at lines 24 and 14 in `main.py` and `utils/data.py`, respectively.     "
"Hello, I want to integrate your code with my system ""for academic purposes""  So I want to enable the system to take input a stream of tokens and output their respective POS tags  how can I do that ?    thanks"
"It appears from your sample that there may be a bug in your IOBES converter, which Im assuming will affect your paper findings slightly?    An example would be at line 6845 of `train.bmes.txt`:     The original file in IOB1 has this:     "
"Hi,     I am trying to run the training demo using python 3.5 and torch 0.40 (with cuda on an NVIDIA 1050 GTX). I get the following error:  `  THCudaCheck FAIL file=/pytorch/aten/src/THC/generic/THCTensorCopy.c line=70 error=59 : device-side assert triggered  /pytorch/aten/src/THC/THCTensorScatterGather.cu:97: void THCudaTensor_gatherKernel(TensorInfo , TensorInfo , TensorInfo , int, IndexType) [with IndexType = unsigned int, Real = long, Dims = 2]: block: [0,0,0], thread: [0,0,0] Assertion `indexValue >= 0 && indexValue  , TensorInfo , TensorInfo , int, IndexType) [with IndexType = unsigned int, Real = long, Dims = 2]: block: [0,0,0], thread: [1,0,0] Assertion `indexValue >= 0 && indexValue  , TensorInfo , TensorInfo , int, IndexType) [with IndexType = unsigned int, Real = long, Dims = 2]: block: [0,0,0], thread: [2,0,0] Assertion `indexValue >= 0 && indexValue  , TensorInfo , TensorInfo , int, IndexType) [with IndexType = unsigned int, Real = long, Dims = 2]: block: [0,0,0], thread: [3,0,0] Assertion `indexValue >= 0 && indexValue  , TensorInfo , TensorInfo , int, IndexType) [with IndexType = unsigned int, Real = long, Dims = 2]: block: [0,0,0], thread: [4,0,0] Assertion `indexValue >= 0 && indexValue  , TensorInfo , TensorInfo , int, IndexType) [with IndexType = unsigned int, Real = long, Dims = 2]: block: [0,0,0], thread: [5,0,0] Assertion `indexValue >= 0 && indexValue        main()    File ""/home/spike/Software/PyCharm/helpers/pydev/pydevd.py"", line 1662, in main      globals = debugger.run(setup['file'], None, None, is_module)    File ""/home/spike/Software/PyCharm/helpers/pydev/pydevd.py"", line 1072, in run      pydev_imports.execfile(file, globals, locals)  # execute the script    File ""/home/spike/Software/PyCharm/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile      exec(compile(contents+""\n"", file, 'exec'), glob, loc)    File ""/home/spike/Projects/NCRFpp/main.py"", line 434, in        train(data)    File ""/home/spike/Projects/NCRFpp/main.py"", line 326, in train      loss, tag_seq = model.neg_log_likelihood_loss(batch_word,batch_features, batch_wordlen, batch_char, batch_charlen, batch_charrecover, batch_label, mask)    File ""/home/spike/Projects/NCRFpp/model/seqmodel.py"", line 43, in neg_log_likelihood_loss      total_loss = self.crf.neg_log_likelihood_loss(outs, mask, batch_label)    File ""/home/spike/Projects/NCRFpp/model/crf.py"", line 262, in neg_log_likelihood_loss      gold_score = self._score_sentence(scores, mask, tags)    File ""/home/spike/Projects/NCRFpp/model/crf.py"", line 247, in _score_sentence      tg_energy = tg_energy.masked_select(mask.transpose(1,0))  RuntimeError: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generated/../THCReduceAll.cuh:339`    Does anyone know what might be causing this error?"
"`Traceback (most recent call last):    File ""main.py"", line 436, in        train(data)    File ""main.py"", line 326, in train      batch_word, batch_features, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask  = batchify_with_label(instance, data.HP_gpu)    File ""main.py"", line 234, in batchify_with_label      mask[idx, :seqlen] = torch.Tensor([1]*seqlen)  TypeError: mul() received an invalid combination of arguments - got (list), but expected one of:   * (Tensor other)        didn't match because some of the arguments have invalid types: (list)   * (float other)        didn't match because some of the arguments have invalid types: (list)`"
"By any chance, do you know if this is compatible with Python3+? I noticed you mentioned 2.7 in your requirements. "
which word embeddings did you use to get the results as displayed?
"It seems like for the CNN word feature extractor, you are simply applying 1D convolution to each word embedding directly, it doesn't look at the neighboring words (which I think will be useful). Please correct me if I am wrong.    Can you point to the reference for the CNN method? I found this seminal work      should it not consider the neighboring words (something like the diagram shown)?  !    "
None
"Hi,  In the cnn feature extractor, is it the case that in a batch you are assuming all the words to be of the same length? If so, then there must have been padding to the smaller words, will it not disturb the char level features?"
"why is tagScheme is initalised with `noSeg`?     and it is not getting updated upon reading the config file, so far in my experiments tagscheme always defaults to `BIO` and not `BMES` how do you set the tagScheme? through config file (I don't think you've even written code for that)"
"I think `get_ner_BIO()` in `metric.py` is wrong.    consider the example where `label_list = [I-MISC, I-MISC, O, I-PER, I-PER, O, O, O, O, O I-ORG, O]` according to current function the following will happen :    Since there is no tag involving **B-**, `whole_tag` and `tag_index` will always be **`[] `** and hence the output of the function is **`[]`** which is wrong?"
"In the file `demo.train.config` I changed the iterations to 100 and batch_size to 32, dev and test scores are almost always -1. (Note this is on the sample_dataset that you have provided with the embeddings that you have provided) "
先膜拜大佬：  我想把这个模型用在一个中文的序列标注问题上：这里面有POS的标记：这个和CNN_character的特征冲突吗，你的项目里面是手动标记特征和CNN_character的特征可以共存吗？另外看了一下数据的预处理的格式：Friday [Cap]1 [POS]NNP O ，我只用到了POS的特征数据是不是应该写成Friday [POS]NNP O ，[POS]是必须要的吗，还是你只是作为一个标记？  烦请指教
"Hello,I have a problem about the alphabet.  In the file `alphabet.py` ,the function `size` returns `len(self.instances) + 1`,I think it is cause of the padding /pad,but in the file `seqmodel.py`,why we have to add two more labels for down stream lstm?Though we use the original label size for CRF,actually in the file CFR model,in the transition matrix ,still add ""start"" and ""end"".this confused me.  And if I do not use CRF,`_, tag_seq = torch.max(outs, 1)`maybe lead to the wrong index.  Thank you~"
can we expect python=3.6 and pytorch=0.4 ? would you like me to help port it for you?
          血崩 这个bug拖了我3天
"I found the sample number of decode output is less than decode input.  Do you just skip the sentence with no tag predicted?  Can I have all predicted result?       My input count is **1514142**, and output count is **1510350**."
"Excuse me, I can't find the conll data with BIOES tags.Where can I get the data to perform your score?  The score of BIO tags is something worse than BIOES ' s.    Can you give me a url to download the data?    thanks a lot for your help."
"Hi I'm trying to run the demo traning:   , then got      Any suggestion would be appreciated. Thanks!"
"Thanks for share this good work, can you help me?  I set the iteration to be 15000 times and find that the training process is too slow.And the F1 score always be 0.7 percent . So can you help me with the training problems?     BTW, I find that your f1 depends on p and recall. But I found that others's work calculates it with acc and recall. May be I am wrong. So how can I get your score.And after how many epoch did you get the best score, so that I can set the iteration param.    I will appreciate for you help! "
"Hello. I want to know if the performance on CONLL 2003 English NER reported in Section 2 is the average performance or the maximum performance? Did you done the significant test? I think 91.20/91.26 is good, but the result of the significant test is needed for CONLL 2003 English NER."
"Thank you for this excellent open source code.  But I have one question about the pre-trained embedding for charaters,In the class ""Data"",we load the pre-trained embedding for characters,but i donot known where to use it,maybe I have to add one parameter called ""pretrained_char_embedding"",and pass it into the class CharBilstm(for example),and modify the code like below:  `        if pretrain_char_embedding is not None:              self.char_embeddings.weight.data.copy_(torch.from_numpy(pretrain_char_embedding))          else:              self.char_embeddings.weight.data.copy_(                  torch.from_numpy(self.random_embedding(alphabet_size, embedding_dim)))`"
"Hi, I run the demo code successfully. It worked pretty well.  And thus I wish to use it in my application. But I encounter a problem here.  Even if I set the *iteration=30*, the training process will stop after the first Epoch.  What should I do in this case?"
Is it possible that you add a entity level sentiment analysis wrapper to this?
In log sum exp why take argmax and then gather instead of just taking max ? any gradient flow issues ?
"In case when crf is false, you do not use mask in calculation of loss. Is there any reason for that?"
There are 2 sets of features for each sentence. What are those? One looks like capitalization ?
"Hi, for NER what's the format of the file that the script expects:       like the above? I saw that you need some .bme files. "
"There are two questions I want to ask you:  1, The numbers of sentences of data used in your code are 14987, 3466, 3684 or 14041, 3250, 3453 (train, dev, test respectively). Can you tell me?  2, Whether your code can obtain comparable performances in CoNLL03 Germany NER data as   and in WSJ data as  . That is, how does your code perform in other tasks compared to existing related works? Did you have a try?"
"Hi,Thank you for this perfect code,and I have learned a lot from this code.But when I run this code,I get one problem ：  `Traceback (most recent call last):    File ""main.py"", line 438, in        train(data, save_model_dir, seg)    File ""main.py"", line 265, in train      batch_charrecover, batch_label, mask)    File ""/Users/fengxiachong/Desktop/PyTorchSeqLabel-master/model/bilstmcrf.py"", line 33, in neg_log_likelihood_loss      scores, tag_seq = self.crf._viterbi_decode(outs, mask)    File ""/Users/fengxiachong/Desktop/PyTorchSeqLabel-master/model/crf.py"", line 162, in _viterbi_decode      partition_history = torch.cat(partition_history).view(seq_len, batch_size, -1).transpose(1,  RuntimeError: invalid argument 0: Tensors must have same number of dimensions: got 3 and 2 at /Users/soumith/minicondabuild3/conda-bld/pytorch_1518371252923/work/torch/lib/TH/generic/THTensorMath.c:2888`  could you please help me？"
None
"After 100 epochs on the train-dev-test splits of CoNLL 2003, dataset with LSTM and CNN character features, I get the following results for the best dev f-score:    - **LSTM**  **Dev:** time: 5.59s, speed: 627.24st/s; acc: 0.9891, p: 0.9460, r: 0.9465, **f: 0.9463**  Exceed previous best f score: 0.945258548088  **Test:** time: 5.57s, speed: 712.46st/s; acc: 0.9808, p: 0.9102, r: 0.9107, **f: 0.9104**  - **CNN**  **Dev:** time: 5.32s, speed: 660.84st/s; acc: 0.9891, p: 0.9458, r: 0.9460, **f: 0.9459**  Exceed previous best f score: 0.945809491754  **Test:** time: 4.88s, speed: 788.98st/s; acc: 0.9804, p: 0.9081, r: 0.9068, **f: 0.9074**    I'm trying to understand what it takes to reproduce the reported numbers, and also use as a baseline for my experiments. Let me know what are the other parameters that I need to change.      Also, thanks for open-sourcing the code!"
"Hello, I am a student from SJTU. I am very interested in this work and recently I am working on sememes prediction. Could you please send me your word embedding and character embedding since training a GloVe embedding with 20G corpus cost too much time. My email address is 853601704@qq.com. Thank you very much！"
"Dear Authors, Many thanks for making your code available. I was wondering if you have the Python code for the neat example you have on r/The_Donald vs. /r/SandersForPresident.    Kind regards  sbs "
"when I run sh src/models/run_main.sh I got this bug, do you know how to fix?   "
I'm now updating the issue after a bit of debugging.    This is the problem while training:           This is the error while training and is the root cause of my previous issue reported.
"sh src/models/run_load_data.sh    Namespace(no_of_candidates=10, post_data_tsv='data/superuser.com/post_data.tsv', qa_data_tsv='data/superuser.com/qa_data.tsv', test_ids='data/superuser.com/test_ids', train_ids='data/superuser.com/train_ids', tune_ids='data/superuser.com/tune_ids', vocab='embeddings/vocab.p')    Traceback (most recent call last):    File ""src/models/load_data.py"", line 82, in        main(args)    File ""src/models/load_data.py"", line 61, in main      train_ids = read_ids(args.train_ids)    File ""src/models/load_data.py"", line 37, in read_ids      ids = [curr_id.strip('\n') for curr_id in open(ids_file, 'r').readlines().split()]  AttributeError: 'list' object has no attribute 'split'      I don't know what to do, I hope I can have a suggestion. thank you very much  "
"Hi, ransudha89.  in evpi.py, where is 'pq_out' defined?  Did you run you code sucessfully?"
"Hi, raosudha89, I am very glad that you share you code. Your paper is interesting and I am trying to run your code. But something is wrong when I run 'run_load_data.sh'.    In the function 'generate_neural_vectors',  **get_indices()** is unresolved reference.      def generate_neural_vectors(posts, titles, ques_lists, ans_lists, post_ids, vocab, N, split):      post_vectors = []      ques_list_vectors = []      ans_list_vectors = []      for post_id in post_ids:          post_vectors.append(get_indices(titles[post_id] + ' ' + posts[post_id], vocab))          ques_list_vector = [None] * N          ans_list_vector = [None] * N          for k in range(N):              ques_list_vector[k] = get_indices(ques_lists[post_id][k], vocab)              ans_list_vector[k] = get_indices(ans_lists[post_id][k], vocab)          ques_list_vectors.append(ques_list_vector)          ans_list_vectors.append(ans_list_vector)      dirname = os.path.dirname(args.train_ids)      p.dump(post_ids, open(os.path.join(dirname, 'post_ids_' + split + '.p'), 'wb'))      p.dump(post_vectors, open(os.path.join(dirname, 'post_vectors_' + split + '.p'), 'wb'))      p.dump(ques_list_vectors, open(os.path.join(dirname, 'ques_list_vectors_' + split + '.p'), 'wb'))      p.dump(ans_list_vectors, open(os.path.join(dirname, 'ans_list_vectors_' + split + '.p'), 'wb'))"
"Hi,    I got this error when trying to run the train.py file    TypeError: add_weight() got multiple values for keyword argument 'name'      ANy ideas plz    thank u"
"Hi,  I tried retraining the model, it went on training and showed the loss during every epoch but at the end of the training it  didn't save anything (Model or Word vector file), which I could use for further predictions. Also is there a way to use model and find the sentiment of the input sentence?  "
"hello, I modified the code to run with python3 and it can run,  but I can't get the same precision you mentioned in the paper, the precision just was 50%, when I adjust the learning rate the precision improve to 67% but still has a gap with your experiment. I used your preprocessed_data. Do you have any idea about improving the precision? thank you!"
"Hello, I have already run your program, and the error shown in the title appears when train.py runs the second args.epochs loop. I don't know why the number of all input array (x) samples has changed. I have been watching for a long time, but I can't find a mistake. Can you help me see why this is? Thank you"
"Hello, I have already run your program and I get the following error when running the second loop in train.py. I have been watching for a long time, but I can't find a mistake. Can you help me see why this is? Thank you"
"Thanks for sharing.  From the preprocessed data, I realized the counts of examples (from my script) are not the same as reported in the paper.     For example, the training data of SemEval 2014 is like this:  lt Counter({'positive': 987, 'negative': 866, 'neutral': 460})  res Counter({'positive': 2164, 'negative': 805, 'neutral': 633})    Did I make any mistake?"
"Thank you for your excellent work, the code is really helpful. The code went well and showed very good results, but I've found the following line different from the equation in the paper.   They're line 34 and 35 in ./race/comatch.py.  34.  trans_q = self.trans_linear(proj_q)  35.  att_weights = proj_p.bmm( torch.transpose(proj_q, 1, 2) )  It seems that 'trans_q' should replace the 'proj_q' in line 35, according to equation(2) in the paper. But here 'trans_q' is not used anywhere.   The result is good though, which confused me. Is it because this linear transformation actually doesn't matter a lot? "
"First of all, thank you for making source code public. May i know the test accuracy of this repository? "
您好，请问给出的emb文件，是用来做什么的？
!   
"I can find how the program computes the reward as follows, but it's not easy for me to understand.     Here are some of my question:  1. What does `gold` stand for?  2. Why are `prec`, `rec`, `sc` negative numbers?  3. ..."
"大神，pretrain.py 文件中引用的 train_data不存在啊    read_f = file(""./data/train_data"",""rb"")   train_generater = cPickle.load(read_f)   read_f.close()    "
"subprocess.CalledProcessError: Command '['/home/ghx/PycharmProjects/pythonProject/Global-Encoding-master/RELEASE-1.5.5/ROUGE-1.5.5.pl', '-e', '/home/ghx/PycharmProjects/pythonProject/Global-Encoding-master/RELEASE-1.5.5/data', '-c', '95', '-2', '-1', '-U', '-r', '1000', '-n', '4', '-w', '1.2', '-a', '-m', '/tmp/tmpj5l8j6ve/rouge_conf.xml']' returned non-zero exit status 255    Looking for a solution to this problem, thanks"
"Could you tell me what preprocessing steps you perform on the dataset itself before you use your tokenization script, by that i mean how do you tokenize?"
使用的是LCSTS全部的数据集，训练了一个epoch之后，生成的摘要和原文一样……不知道有没有人遇到同样的情况？    我做的一点修改就是不再使用pyrouge，因为pyrouge测不了中文。我改成了用rouge，这个应该没什么影响吧。    数据处理过程应该是没什么问题:  (Global-Encoding) [ychuang@gpu18 data]$ cat train.src | head -n 1  新华社受权于18日全文播发修改后的《中华人民共和国立法法》，修改后的立法法分为“总则”“法律”“行政法规”“地方性法规、自治条例和单行条例、规章”“适用与备案审查”“附则”等6章，共计105条。  (Global-Encoding) [ychuang@gpu18 data]$ cat train.tgt | head -n 1  修改后的立法法全文公布  (Global-Encoding) [ychuang@gpu18 data]$ cat test.src | head -n 1  日前，方舟子发文直指林志颖旗下爱碧丽推销假保健品，引起哗然。调查发现，爱碧丽没有自己的生产加工厂。其胶原蛋白饮品无核心研发，全部代工生产。号称有“逆生长”功效的爱碧丽“梦幻奇迹限量组”售价高达1080元，实际成本仅为每瓶4元！  (Global-Encoding) [ychuang@gpu18 data]$ cat test.tgt | head -n 1  林志颖公司疑涉虚假营销无厂房无研发
"Hello, I've set up pyrouge according to the tutorial below:       And used the command you propose, having RELEASE-1.5.5 at home:  pyrouge_set_rouge_path RELEASE-1.5.5/    When trying to train the first epoch works, but when it comes to validation the script gets stuck when trying to deal with the files. I'm not sure why this happens, but it always gets blocked on the same line of execution (as I show in the image).    !     I should notice you that I'm using a reduced training and validation set, consisting of only the first 100 lines of each of the files (.src and .tgt). I'd like to mention, aswell, that the original valid.src and valid.tgt had more than 180K lines; which doesn't match with the settings you mentioned in your paper.     If you have any idea as to why this doesn't work, it would be very helpful. Thanks in advance and congratulations on the work.    "
"您好 每个epoch都出现 Use of uninitialized value in division (/) at script/multi-bleu.perl line 127,   line 1106.  而且每个epoch的BLEU都是0.00 0/0/0/0  请问是什么问题？  输出如下：  !     数据处理，以valid.src为例，如下：  !     valid.tgt:  !     "
None
LCSTS数据集处理需要去掉标点符号或者空格吗，能否给一个处理后的小demo
您好，请问您的问题解决了吗？我也是rouge得分异常高，前几个epoch55+。我是对数据分字后用的preprogress.py处理的    _Originally posted by @wyyNLP in  
"基于字级别的数据，从数据集里把摘要和原文本提取出来以后分别保存，再对他们进行分字处理。在这里标点没有去掉。然后用preprogress.py.在使用rouge评价是把reference和candidate根据字典dict转成index数字进行评分的，rouge结果异常高。  epoch:   3, loss: 23146.946, time: 3100.882, updates:   110000, accuracy: 57.57  F_measure: [57.11, 30.26, 43.7] Recall: [56.44, 29.56, 43.05] Precision: [62.93, 33.76, 48.4]  请问我是数据处理的有问题吗？或者您方便透漏一下您的实验的bleu值吗？"
"Hi,    I have downloaded Gigaword dataset from  , and there are 4 directories: **train**, **Giga**, **DUC2004**, and **DUC2003**.    I use  valid.title.filter.txt, valid.article.filter.txt, train.title.txt and train.article.txt in **train** as the tgt/src of valid/train data, and use two txt files in **Giga** as test data. However, the valid data has wrong size (189651) after preprocess. The weird thing is that when I run preprocess.py, the result shows ""(0 and 0 ignored due to length == 0 or > )"".    Would you know any method to fix this?  Thanks a lot!"
您好：  我采用PARTI作为训练，PARTIII中得分大于3的作为测试，得到的结果非常高，想问您一下，是如何划分的训练和测试的？  
"所执行的命令：  /root/Global-Encoding/RELEASE-1.5.5/ROUGE-1.5.5.pl -e /root/Global-Encoding/RELEASE-1.5.5/data -c 95 -2 -1 -U -r 1000 -n 4 -w 1.2 -a -m /tmp/tmphr15dtvf/rouge_conf.xml    异常信息：  Illegal division by zero at /root/Global-Encoding/RELEASE-1.5.5/ROUGE-1.5.5.pl line 2450.     返回代码255    虽然有  #4 讨论过，但没有明白问题根源在哪儿。按讨论说法是出现了非法字符，我用以下代码清洗了数据集，只保留了汉字、英文字符、数字、普通标点符号，但依然出现此问题。  `  for line in f:          line = re \              .compile( \              u""[^""              u""\u4e00-\u9fa5""               u""\u0041-\u005A""               u""\u0061-\u007A""              u""\u0030-\u0039""              u""\!\@\#\$\%\^\&\*\(\)\-\=\[\]\{\}\\\|\;\'\:\""\,\.\/\ \?\/\*\+""              u""]+"") \              .sub('', line)    `  同时用`grep ""^$""  * | wc -l `检测空行，并没有空行。  所以不是很清楚哪些算非法字符。或者如果把汉字映射为数字，原文中的数字又如何处理？"
As above..Thanks!
请问一下你们的模型在LCSTS数据集训练好大概需要多长时间呀？
"“Traceback (most recent call last):    File ""train.py"", line 332, in        main()    File ""train.py"", line 324, in main      train_model(model, data, optim, i, params)    File ""train.py"", line 179, in train_model      score = eval_model(model, data, params)    File ""train.py"", line 252, in eval_model      score[metric] = getattr(utils, metric)(reference, candidate, params['log_path'], params['log'], config)    File ""/home/zhengxin/Global-Encoding/Global-Encoding-master/utils/metrics.py"", line 58, in rouge      rouge_results = r.convert_and_evaluate()    File ""/root/miniconda3/lib/python3.7/site-packages/pyrouge/Rouge155.py"", line 361, in convert_and_evaluate      rouge_output = self.evaluate(system_id, rouge_args)    File ""/root/miniconda3/lib/python3.7/site-packages/pyrouge/Rouge155.py"", line 336, in evaluate      rouge_output = check_output(command).decode(""UTF-8"")    File ""/root/miniconda3/lib/python3.7/subprocess.py"", line 389, in check_output      **kwargs).stdout    File ""/root/miniconda3/lib/python3.7/subprocess.py"", line 466, in run      with Popen(*popenargs, **kwargs) as process:    File ""/root/miniconda3/lib/python3.7/subprocess.py"", line 769, in __init__      restore_signals, start_new_session)    File ""/root/miniconda3/lib/python3.7/subprocess.py"", line 1516, in _execute_child      raise child_exception_type(errno_num, err_msg, err_filename)  PermissionError: [Errno 13] Permission denied: 'RELEASE-1.5.5/ROUGE-1.5.5.pl'”  Tried in several different machine, but get the same mistake, and I'm wondering the reason QwQ. "
"Once we downloaded the gigaword dataset, how are we supposed to preprocess it so we have the train and valid tgt and src?"
"When infering test.src, there are too much '  ' output."
错误如下：
"  File ""train.py"", line 322, in        main()    File ""train.py"", line 316, in main      print_log(""Best %s score: %.2f\n"" % (metric, max(params[metric])))  ValueError: max() arg is an empty sequence"
您好，我看到代码中最后输出的是验证集上的结果，请问论文中的结果是验证集上的结果还是测试集上的结果呢？希望得到您的回复，谢谢！
请教几个问题，  一，你们中文跑的粒度是词粒度还是字符粒度呀，哪种效果好呢？  二，原始数据要做预处理，比如去掉标点符号啥的。  三，这个方法你们有在标题生成任务测过吗    我的程序还在跑目前先用的char，跑了好久一直没有loss日志输出来，不知道为啥，是要跑很久才能看到loss输出吗？我用的数据量也不大，nlpcc2017摘要数据试的
"Traceback (most recent call last):    File ""train.py"", line 322, in        main()    File ""train.py"", line 316, in main      print_log(""Best %s score: %.2f\n"" % (metric, max(params[metric])))"
"When beam is 1, candidate likes [None, None, None...], and causes error, because you didn't transform tensor ""samples(line 207 of train.py)""  into list.  You can change line 86 of seq2seq.py like this to solve this problem  `sample_ids = torch.index_select(outputs, dim=1, index=reverse_indices).t().tolist()`  "
It seems that the `from .loss import *` in `models/__init__.py`could be deleted ?
关于Gigaword dataset的问题，论文中提到，你们用的是Rush et al. (2015)处理好的数据。我想问一下这个数据在哪里可以找到？我去Rush et al. (2015)找了，找不到。关于源数据，我给LDC写了申请，也没有回复。所以我想问一下你们，这个数据怎么获取的。非常感谢你们的工作！
你好：我想请教一下关于LCSTS的word-based和character-based测rouge的问题。举个例子:  训练结果：上 市 公 司 朋 友 圈 合 作 背 后 利 益 均 沾  实际结果：揭 秘 “ 老 虎 们 ” 的 上 市 公 司 朋 友 圈 合 作 背 后 利 益 均 沾  是直接用 files2rouge测两者之间的rouge么？ 还是需要改变什么，比如说将中文字改成数字之类的，或者需要其他的改变。  谢谢！
     Is there need to share dictionary between article and title file ?  
"  The model can be trained successful in first epoch. However, it will use validation set to test model performance , which  caused a bug.  the log is as followed.         total number of parameters: 83289940      epoch:   1, loss: 51426.711, time: 2488.591, updates:    10000, accuracy: 27.78    218 Traceback (most recent call last):  219   File ""train.py"", line 332, in    220     main()  221   File ""train.py"", line 324, in main  222     train_model(model, data, optim, i, params)  223   File ""train.py"", line 179, in train_model  224     score = eval_model(model, data, params)  225   File ""train.py"", line 252, in eval_model  226     score[metric] = getattr(utils, metric)(reference, candidate, params['log_path'], params['log'], config)  227   File ""/data/mmyin/Global-Encoding/utils/metrics.py"", line 58, in rouge  228     rouge_results = r.convert_and_evaluate()  229   File ""/home/mmyin/anaconda3/lib/python3.6/site-packages/pyrouge-0.1.3-py3.6.egg/pyrouge/Rouge155.py"", line 367,     in convert_and_evaluate  230     rouge_output = self.evaluate(system_id, rouge_args)  231   File ""/home/mmyin/anaconda3/lib/python3.6/site-packages/pyrouge-0.1.3-py3.6.egg/pyrouge/Rouge155.py"", line 342,     in evaluate  232     rouge_output = check_output(command, env=env).decode(""UTF-8"")  233   File ""/home/mmyin/anaconda3/lib/python3.6/subprocess.py"", line 336, in check_output  234     **kwargs).stdout  235   File ""/home/mmyin/anaconda3/lib/python3.6/subprocess.py"", line 418, in run  236     output=stdout, stderr=stderr)  237 subprocess.CalledProcessError: Command '['/data/mmyin/ROUGE/RELEASE-1.5.5/ROUGE-1.5.5.pl', '-e', '/data/mmyin/ROU    GE/RELEASE-1.5.5/data', '-c', '95', '-2', '-1', '-U', '-r', '1000', '-n', '4', '-w', '1.2', '-a', '-m', '/tmp/tmp    436z9iyl/rouge_conf.xml']' returned non-zero exit status 255.            "
None
"我用的是srush 的数据集    “Training and evaluation data for Gigaword is available      我将下载的文件命名为train.src, train.tgt, valid.src, valid.tgt, test.src and test.tgt后放入 giga文件夹中，并在这个giga文件夹中新建了一个data空文件夹。    之后运行 python3 preprocess.py -load_data giga -save_data newgiga    再之后 将giga_yam中的data 改成 ‘newgiga/’    模型成功训练，与验证，但验证完毕开始计算rouge分数的时候，    显示    Command '['script/RELEASE-1.5.5/ROUGE-1.5.5.pl', '-e', 'script/RELEASE-1.5.5/data', '-c', '95', '-2', '-1', '-U', '-r', '1000', '-n', '4', '-w', '1.2', '-a', '-m', '/tmp/tmp6fmy4hlt/rouge_conf.xml']' returned non-zero exit status 2    我尝试了        “  cd pythonrouge/RELEASE-1.5.5/data/  rm WordNet-2.0.exc.db  ./WordNet-2.0-Exceptions/buildExeptionDB.pl ./WordNet-2.0-Exceptions ./smart_common_words.txt ./WordNet-2.0.exc.db  ”    均无法解决这个问题，请问应该怎么做呢？  "
同学，你好，打扰了。  想问问你， 关于 giaword 你是取全部单词当作字典大小吗？   还是高频词前5W个？
请问一下，你在论文中model使用的字典大小是5W吗？    打算做对比实验，想设定baseline model一样的参数，但是发现字典大小没有给。。。
你好，在测试中文摘要的时候，会遇到如下报错：  Illegal division by zero at /home/zhumaokun/ROUGE/RELEASE-1.5.5/ROUGE-1.5.5.pl line 2450.    是不是该对中文转换成英文字符？？
"错误的代码：   def unbottle(m):              return m.view(beam_size, batch_size, -1)  修改后：  def unbottle(m):              return m.view(batch_size, beam_size, -1)"
"* in function sortFinished in models/beam.py, when the parameter minimum is not None, we need add from beam, but during the iteration, the beam index i is not updated       * in function sample in models.seq2seq.py line 125, is the variable alignmentsi misspelled?"
你好，请问train.src和train.tgt等格式是怎么样呢？
"Hi there,     I'm reaching out to ask a few questions about the SciDTB dataset that I couldn't find answers to in this repository or the paper:     **Firstly**, the   and   directories are clear in distinguishing the gold annotations and the second annotations while   seems to have everything lumped together since you can see file names like `P14-1024_anno1.edu.txt.dep,` `P14-1024_anno2.edu.txt.dep,` and `P14-1024_anno3.edu.txt.dep`. Could you please clarify which one of the multiple annotated files are considered the gold annotation file?      **Secondly**, just wanted to double check: are `same-unit`, `joint`, and `comparison` the only discourse relations that are considered multinuclear (i.e. symmetric) in SciDTB?         **Thirdly**, there are 4 files that contain textual EDUs whose parent head is `-1`, as exemplified below by `eduid=1` (dev file: D14-1080) in the figure. Could this be an error from the original annotation file? The file names for these are as follows:     - `train`: P16-1069_anno2  - `dev`: D14-1080; D14-1099  - `test`: D14_1042    !       **Lastly**, is there an annotation manual and/or detailed documentation of this dataset that is publicly available for reference regarding various aspects of the data?       Looking forward to hearing from you soon!       Cheers,  Janet "
"  Hello, I'd like to ask why there is no train fold in the data set for the Hypernym Detection task"
"""generate_table.sh"" returns both Validation and test performance, so I'm confused which is the final result you presented in your paper. "
Hi! The url (  in `download_data.sh` for `BLESS` and `LEDS` seems unavailable now. It returns 403 forbidden when trying to access. I wonder if there is any suggestion on a replacement? Thanks!
Would you please undate  valid url？Thanks a lot
"I see this parameter in another paper[  said used your setting,but Mentioned parameter “rank” I have not seen in your paper.Do you have any idea anbout it?Looking forward to your reply."
Is this code implements the heast-pattern to extract hypo and hyper from a sentence?
"The hyperlex url is not available now. It will show ""The DS-Web service has been decommissioned"" when visit the url. Could you offer the hyperlex data in this repo?"
None
"Perhaps add to the readme that when running the file download_data.sh on macOS, it is necessary to ensure that you have the gnu versions of sort and sed installed. Two immediate problems with the native version are that macOS sort does not have the option random-sort, and macOS sed does not recognise \t as a tab."
None
"since learning rate [0.1] for fasttext, and 1e-5 here"
"Hello,    I am interested in saving the variance in the unimodal case. So I implemented the negativeSampling in the case where multi is false and var is true.    For some reason the variance does not seem to change a lot, it stays around the initial value. I tried to use the variance without the setting it to the log, but then the loss blows up. Is this the reason you store the log?     Also, it seems to me that when var is false, you never change its value. I get that in the spherical case, it simplifies the energy (equation 5), but shouldn't you still update the variance (just a scalar, not a vector anymore).    Many thanks"
"Hi,    I try to download the pretrained embeddings for the model but both 7z and zip link failed.  here is the error message    wget   -P modelfiles/  --2018-07-23 19:46:29--     Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.129.173  Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.129.173|:443... connected.  HTTP request sent, awaiting response... 403 Forbidden  2018-07-23 19:46:30 ERROR 403: Forbidden.    Would you mind solving this link issue?    Best,  Guoyin"
"Hi, I trained the model on 50k files with same arguments given in `train_text8_multi.sh` , which took around 7 to 8 hours for training. Now I am trying to load it in python, I used below code:     The size of modelfiles folder is around 5GB.   But now it is giving me error:  `EmptyDataError: No columns to parse from file`    "
Is PTF embedding better than word2vec embedding on text classification and sentence similarity task?  Thanks!
Where can I download the original paper? THANKS
"Hi,    I am a fresher of word2vec research. When I copied this code to my own jupyter, I found there was the error existed as below:    NameError                                 Traceback (most recent call last)    in          1 # caching  ----> 2 if self.cache and self.bucket != 0:        3       print (""Cache subword"")        4       self.subword_emb = self.cache_subword_rep(basename=basename)        5 if multi:    NameError: name 'self' is not defined    I tried to find a method to address this issue, but did not make any sense. Just wandering whether you have got the same experience? I am looking forward to your reply. Thank you.    Kind Regards,  Siyue"
"I tried to run the experiment code. First, I find it can't find the correct data. The data it expects it load is starget-train, starget-val, and starget-test. But in the data folder, all the files don't have the prefix ""starget-"". After I fix it, there is decode error:  UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 282: character maps to      The command I run is   python run_experiment.py --model att --vectors GoogleNews-vectors-negative300.bin.gz --data ../data/1-Sent/    Can you try your code and fix the errors?  "
Epoch 0 complete! Average Training loss: nan  Training got into NaN values...      Model initialized  Epoch 0 complete! Average Training loss: nan  Training got into NaN values...      Model initialized  Epoch 0 complete! Average Training loss: nan  Training got into NaN values...  
"Hi,          The train, test, valid numbers are not matching with the paper. And also, the pickle contains the binary labels. how do you run the scripts for multilabel. Can I get the scripts that you used to generate the features for audio, video and text data.   Thanks  sarala"
"Hello, there is an error ' Cannot pickle local object 'load_mosi. .MOSI',Is there a problem with the torch version?"
"Thank you for sharing your work !  I'm confused about the data read from dataset IEMOCAP.pkl    `   iemocap_data = pickle.load(open(data_path + ""iemocap.pkl"", 'rb'), encoding='bytes')      print(np.sum(iemocap_data[b'happy'][TRAIN][TEXT]))        print(""------------------------------------"")        print(np.sum(iemocap_data[b'sad'][TRAIN][TEXT]))        print(""------------------------------------"")        print(np.sum(iemocap_data[b'angry'][TRAIN][TEXT]))        print(""------------------------------------"")        print(np.sum(iemocap_data[b'neutral'][TRAIN][TEXT]))  `  I use the above ""print"" to show the vector value in four classes, but I get the same result like the following:    `  Temp location for models: models/model__b'angry'.pt  Grid search results are in: results/results__b'angry'.csv  -7770.0356465404475  ------------------------------------  -7770.0356465404475  ------------------------------------  -7770.0356465404475  ------------------------------------  -7770.0356465404475  Audio feature dimension is: 74  Visual feature dimension is: 35  Text feature dimension is: 300  `    Meanwhile, I try another keys ""VIDEO"" and ""AUDIO"", the four classes value are still same.  No matter how I choose the emotion value, the dataset is the same.    The reason I find this is that I want to know how many data in each emotion type,  but I get the same length of ""train_set"" when I switch the parameter ""emotion"".    And may I ask why don't you train with all emotions together     Thanks"
train iemocap keyerror happy
"Hello. How do you fix the nan and inf value in the mosi dataset? I just replace the inf value with 0, but the result is bad. Acc-2 is only 72%. Please help! It's very urgent!"
"I tried to save this model on colab.  model_save_name = 'classifier.pt'  path = F""/content/gdrive/My Drive/{model_save_name}""   torch.save(LMF.state_dict(), path)    but found error  TypeError                                 Traceback (most recent call last)    in  ()        1 model_save_name = 'classifier.pt'        2 path = F""/content/gdrive/My Drive/{model_save_name}""  ----> 3 torch.save(LMF.state_dict(), path)    TypeError: state_dict() missing 1 required positional argument: 'self'"
"  how to solve this problem?please  Traceback (most recent call last):  File ""train_pom.py""， line 274, in  main(PARAMS)  File ""train _pom.py"", line 248, in main  results = [ahid, vhid,thid，adr，vd, tdr，factor_lr，lr， x， batch_sz, decay， min_valid_loss.cpu().data.numpy()]  AttributeError : 'float' object has no attribute 'cpu'"
Can't pickle local object 'load_mosi. .MOSI'
"Why can the order of summation and Hadamard product be exchanged in the last step of formula (6)?  I tried the simplest case (when w is a vector, r=2, m=2) and found that exchange the order will bring extra interleaved items."
" The completion epoch is less than I set, and then the following error is generated:  `Traceback (most recent call last):    File ""train_mosi.py"", line 246, in        main(PARAMS)    File ""train_mosi.py"", line 226, in main      min_valid_loss.data.cpu().numpy(), mae, corr, multi_acc, bi_acc, f1])  AttributeError: 'float' object has no attribute 'data'    `  "
"having a hard time matching the math with the code:       here is my understanding   * `self.video_hidden` = $d_m$ in the paper, i.e., the dimension of the video embedding, similar for other modality   * `self.output_dim` = $d_h$ in the paper     `_audio_h` is of dim `(batch_size, self.audio_hidden + 1)`,   `self.audio_factor` is of dim `(self.rank, self.audio_hidden + 1, self.output_dim)`,   so `torch.matmul(_audio_h, self.audio_factor)` is of dim `(self.rank, batch_size, self.output_dim)`, which is also the dimension of `fusion_zy`, then `fusion_zy` goes through a linear transformation to collapse the ""rank"" dimension and get `output`. Is `output` the `h` in the paper? It seems the implementation following Eq (6) in the paper, but not exactly.   (1) the summation is replaced by a weighted summation  (2) the product and summation order is exchanged. Eq (6) first does the summation then product, but the code reversed this.   Did I understand it correctly? seems these changes make it not equivalent to the math? What are the rationale of these changes?     Thanks! "
!     what is \hat{z}_m here? couldn't find its definition. Is it z_m augmented with 1?   
"Very interesting work! A quick question, can we understand this method as a generalization of factorization machine (  it appears to me the high level idea is similar, but generalized to high-dim tensor. Let me know if I misunderstood :) "
"hi:      First of all thank you very much for your work. But I don't quite understand some processing about iemocap dataset.      - First, iemocap.pkl is divided into four parts according to emotion. The label of each part is a 2-D array with 0 or 1.  What this label refers to？And IEMOCAP should be a four-category multi-label classification task. Why divide the data set like this ?    - Second, I don't quite understand the code in train_iemocap.py :         `torch.max(y, 1)[1]` .   This line of code also confuses me because I don’t understand the meaning of the label.    I will be very grateful If you can help me.  "
  WIth this:    python train_mosi.py --run_id 19260817 --epochs 50 --patience 20 --output_dim 1 --signiture test_run_big_model    I have three pickles files under the data folder.     I have the following output.                                                                Model initialized  Epoch 0 complete! Average Training loss: nan  Training got into NaN values...      Model initialized  Epoch 0 complete! Average Training loss: nan  Training got into NaN values...      Model initialized  Epoch 0 complete! Average Training loss: nan  Training got into NaN values...    
"In this code,  the parameters of  fusion_weights , fusion_bias is not contain in optimizer,  Don't they need to be learned? thanks !  "
"Hello, when I run the train_mosi.py script, I get NAN from the very beginning. Is that normal?    Best!"
"My friend uses your code directly as a multimodal decision, but I think your code is just to generate a tensor that reduces the amount of computation, and the output in the code is meant to illustrate that this tensor is valid, not for classification. Is my understanding correct?"
"Hi Justin,    since you give the 3-D and 2-D array directly, it is difficult to know the clear train/val/test set.    And I find some value in the feature array is inf, causing the training loss ""NAN"", is this normal ?    Thanks very much!"
Recently I want to use your LMF to fuse my three modes for training. I want to know how long does it take for the whole process?  
"Hi, Justin,        Can you share the original face features and audio features with averaging along the temporal dimension?        and where can I download the original video data of the three datasets?    Thanks very much!  "
"Hi  I am not able to understand why the training word vector dimension is (1283, 20, 300)    Each word vector is of size 300, and there are 1283 data points in training data set. But what does 20 represent? Does that mean there is one audio and video feature vector for every 20 words? and one opinion label for every 20 words?    Dimensions of training data points -   train_audio: (1283, 74)  train_visual: (1283, 46)  train_text: (1283, 20, 300)  train_labels: (1283, 1)  "
"Hi Justin,    Thank you for your codes. It is helpful for me.    I want to replicate the results for research purposes. But the performance of the model can only reach about 72(F1-score on test set, MOSI dataset), while the best is 75.7 according to the paper. So could you please tell me the hyper-parameters settings in the model?    Thank you!    Best Wishes,  Lan-qing"
"Hi jiesutd,  I've got ontonotes-4.0 copyright  from LDC, and tryed to split the NER data set by myself.  But I've got a different size of data set, especially on dev and test set.  I want to reimplement the same as your split on OntoNotes-4.0 dataset.  I can prove that i have ontonotes-4.0 copyright.  Could you please send me your split  gaoyunbo923@buaa.edu.cn  best wish  "
数据怎么换成自己的数据集呢
   这里为什么`f = 1 - i`？
"   > model\latticelstm.py"", line 107, in reset_parameters  >     self.weight_hh.data.set_(weight_hh_data)  > RuntimeError: set_storage is not allowed on a Tensor created from .data or .detach().  > If your intent is to change the metadata of a Tensor (such as sizes / strides / storage / storage_offset)  > without autograd tracking the change, remove the .data / .detach() call and wrap the change in a `with torch.no_grad():` block.  > For example, change:  >     x.data.set_(y)  > to:  >     with torch.no_grad():  >         x.set_(y)"
大家好呀，有没有人知道run demo之后要训练多久才能出结果呢？
你好，我想问一下Character-Based Model部分 Char+softword中segmentation label embedding lookup table这个表用的是什么文件呢？
您好，请问您可以分享一下ontonote4.0的数据集么，我的邮箱是a874214105@163.com，我可以先把授权发给您。
"when i run test/decode, there is an error on ""load_model_decode"". Information like:  _size mismatch for lstm.hidden2tag.weight: copying a param with shape torch.Size([20, 200]) from checkpoint, the shape in current model is torch.Size([22, 200])._   "
您好！我想请问下ner任务上在构建char2id的字典映射时，这个除了使用训练集的语料构建，是否可以额外补充预训练char向量中的char到上面的char2id里面？正确的构建方法是什么呢？
"事情是这样的，MAX_SENTENCE_LENGTH我看您默认设置为了250。默认的时候不管是训练还是decode都是正常的，而且效果很不错。  而我的数据集中的句子可能会有长度为1000左右的，如果按照默认的250，该句子就会消失在decode生成的结果文件中。源码中我也看到有个if判断，当MAX_SENTENCE_LENGTH设置为负数时会让所有长度的句子通过。因此我设置了MAX_SENTENCE_LENGTH为-1。结果在训练模型的时候没有报错，在decode的时候显示CRF层的一个语句报错，详细报错信息如下。    File ""C:\Users\jmy\PycharmProjects\LatticeLSTM\model\crf.py"", line 171, in _viterbi_decode      last_partition = torch.gather(partition_history, 1, last_position).view(batch_size,tag_size,1)  RuntimeError: Invalid index in gather at C:\w\1\s\tmp_conda_3.7_021303\conda\conda-bld\pytorch_1565316900252\work\aten\src\TH/generic/THTensorEvenMoreMath.cpp:472    报错提示gather函数的index是无效的，然后我打印了gather函数的第一个参数和第三个参数，也就是partition_history和last_position。  打印的代码是这样写的：             print(partition_history.size())          print(last_position.size())          last_partition = torch.gather(partition_history, 1, last_position).view(batch_size,tag_size,1)          print(""success"")    打印过程如下：  torch.Size([1, 312, 7])  torch.Size([1, 1, 7])  success  torch.Size([1, 164, 7])  torch.Size([1, 1, 7])  success  torch.Size([1, 219, 7])  torch.Size([1, 1, 7])  success  torch.Size([1, 256, 7])  torch.Size([1, 1, 7])  Traceback (most recent call last):    File ""main.py"", line 449, in        decode_results = load_model_decode(model_dir, data, 'raw', gpu, seg)    File ""main.py"", line 355, in load_model_decode      speed, acc, p, r, f, pred_results = evaluate(data, model, name)    File ""main.py"", line 153, in evaluate      tag_seq = model(gaz_list,batch_word, batch_biword, batch_wordlen, batch_char, batch_charlen, batch_charrecover, mask)    File ""C:\Users\jmy\Anaconda3\envs\python3.7\lib\site-packages\torch\nn\modules\module.py"", line 547, in __call__      result = self.forward(*input, **kwargs)    File ""C:\Users\jmy\PycharmProjects\LatticeLSTM\model\bilstmcrf.py"", line 42, in forward      scores, tag_seq = self.crf._viterbi_decode(outs, mask)    File ""C:\Users\jmy\PycharmProjects\LatticeLSTM\model\crf.py"", line 171, in _viterbi_decode      last_partition = torch.gather(partition_history, 1, last_position).view(batch_size,tag_size,1)  RuntimeError: Invalid index in gather at C:\w\1\s\tmp_conda_3.7_021303\conda\conda-bld\pytorch_1565316900252\work\aten\src\TH/generic/THTensorEvenMoreMath.cpp:472      请问作者知道是哪里有问题吗？为什么MAX_SENTENCE_LENGTH设置会导致这里有问题。250长度的默认值就可以完美train和decode。已经困惑了很久了，望作者解惑，十分感谢。      "
No such file or directory: 'data/model/saved_model.lstmcrf..dset'
 7NX6](   0 O  0 O  0 O  0 O  年 O  0 O  0 O  月 O  0 O  日 O  （ O  新 B-ORG  华 M-ORG  社 E-ORG  北 B-LOC  京 E-LOC  0 O  0 O  月 O  0 O  日 O  电 O  ） O  江 S-PER  主 O  席 O  离 O  开 O      疑惑住了
 7NX6](   0 O  0 O  0 O  0 O  年 O  0 O  0 O  月 O  0 O  日 O  （ O  新 B-ORG  华 M-ORG  社 E-ORG  北 B-LOC  京 E-LOC  0 O  0 O  月 O  0 O  日 O  电 O  ） O  江 S-PER  主 O  席 O  离 O  开 O      疑惑住了
我发现在mrsa数据训练预测后，输出的预测文件raw.out比test文件少了600句，而在其他数据集上并没有出现这种情况。这是为什么呢？太长的句子截断吗？还是去重了
我在论文中看到嵌入词作为一个嵌入词（南京市）作为一个静态词，公式13中，也给这个静态词分配了一个权重，这个权重是依赖句子中“南”这个词存在的吗？因为这个句子中，嵌入的位置可能是随机的，也就是说“南京市”匹配的那部分权重并不是每次都起作用的？那么如何保证能够激活“南京市”这个静态词，或者说如何使用“南京市”对应的权重？这个权重可以直接放弃吗？反向传播的时候，如何更新这个权重？
FileNotFoundError: [Errno 2] No such file or directory: 'data/model/saved_model.lstmcrf.dset'  是缺少相关文件吗，还是路径有问题，该如何修改呢
您好！最近我在做实验对比，已取得了Ontonotes4的access，但是不知道是如何划分成train、dev、test？您方便分享一下预处理的代码吗（或者是划分后的数据集吗）。谢谢您！
训练起来非常的慢，我发现主要是因为batch_size在main.py中被设置为1了，而且改了这个参数后，会在model/latticelstm.py：142行提醒报错，初步猜想是作者并没有进行batch_size的其他值的调试，或许会导致一些其他的bug产生。哪位大佬有空能够修正下batch_size的问题吗？
"  File ""main.py"", line 197, in batchify_with_label      mask[idx, :seqlen] = torch.Tensor([1]*seqlen)  TypeError: mul(): argument 'other' (position 1) must be Tensor, not list"
请问该如何测试自己的数据集，对于那种没有标签的初始数据
None
"在代码中, main.py 模块,      /code    `    def batchify_with_label(input_batch_list, gpu, volatile_flag=False):         """""" pass """"""                 word_seq_lengths, word_perm_idx = word_seq_lengths.sort(0, descending=True)                  """""" pass """"""                 char_seq_lengths, char_perm_idx = char_seq_lengths.sort(0, descending=True)         char_seq_tensor = char_seq_tensor[char_perm_idx]         _, char_seq_recover = char_perm_idx.sort(0, descending=False)         _, word_seq_recover = word_perm_idx.sort(0, descending=False)             """""" pass """"""  `    这几行代码的排序有什么意义, 尤其是 关于对 char_seq_lengths 数据的排序, 由于每个 char 的长度都是 1, 这个排序我没看明白有什么作用, 而且我做了测试, 这个排序会打乱数据都是 1 的 tensor, 这是不应该的.  我去 torch 社区提问了这个问题, 得到以下答复, 可以参考:      以及可以参考:       总之就是在说 torch 的 sort() 是不稳定的, 而且代码也确实把数据都是 1 的 char_seq_lengths 进行了一个排序, 但不是完全的升序也不是完全的降序.    所以还是希望作者能答复一下我这个关于代码中的疑问, 尤其是  char_seq_recover  这个数据有什么含义.    ====================================    另外, 结合数据的实际情况, 我暂时修改了代码, 因为每次只传入了 1 条 sequence 数据, 而且每个 character  的长度都是 1 , 因此我取消了排序, 取而代之的是直接写定了代码如下:  char_seq_recover = torch.arange(0, len(char_seq_lengths), 1).   看起来的结果好像没什么问题, 但是我不确定这之中是否会有其它细节出现偏差.  git 上的 markdown 用不好. 希望大家能看明白我的描述.    谢谢~          "
"在 latticelstm,py 中定义的 LatticeLSTM 类,  在它的 foward() 方法中,   `    def forward(self, input, skip_input_list, hidden=None):          """"""          input:              variable (batch, seq_len), batch = 1              skip_input_list: [skip_input, volatile_flag]              skip_input: three dimension list, with length is seq_len. Each element is a list of matched word id and its length.                           example: [[], [[25,13],[2,3]]] 25/13 is word id, 2,3 is word length .               skip_input == gaz_list          """"""          volatile_flag = skip_input_list[1]  # gaz_list          skip_input = skip_input_list[0]          if not self.left2right:              skip_input = convert_forward_gaz_to_backward(skip_input)          input = input.transpose(1, 0)          seq_len = input.size(0)  # character 信息          batch_size = input.size(1)          assert (batch_size == 1)          hidden_out = []          memory_out = []          if hidden:              (hx, cx) = hidden          else:              hx = autograd.Variable(torch.zeros(batch_size, self.hidden_dim))              cx = autograd.Variable(torch.zeros(batch_size, self.hidden_dim))              if self.gpu:                  hx = hx.cuda()                  cx = cx.cuda()            id_list = range(seq_len)  # (0, 57)          if not self.left2right:              id_list = list(reversed(id_list))          input_c_list = init_list_of_objects(seq_len)  # len list = 57 : [[],[],[],[],....]          for t in id_list:              (hx, cx) = self.rnn(input[t], input_c_list[t], (hx, cx))  # input character 数据 sequence              hidden_out.append(hx)              memory_out.append(cx)              if skip_input[t]:                  matched_num = len(skip_input[t][0])  # word  idx in gaz # 处理匹配到的词汇信息                  word_var = autograd.Variable(torch.LongTensor(skip_input[t][0]), volatile=volatile_flag)                  if self.gpu:                      word_var = word_var.cuda()                  word_emb = self.word_emb(word_var)  # 3 * 50 # x1 时刻有 3 个单词                  word_emb = self.word_dropout(word_emb)                  ct = self.word_rnn(word_emb, (hx, cx))  # words 词汇信息 input                  assert (ct.size(0) == len(skip_input[t][1]))                  for idx in range(matched_num):  # matched_num: 匹配到的词汇数量                      length = skip_input[t][1][idx]  # character 得到的词汇的 length 信息,帮助确定词汇截至位置                      if self.left2right:                          # if t+length   1 * 100                          input_c_list[t + length - 1].append(ct[idx, :].unsqueeze(0))                      else:                          # if t-length >=0:                          input_c_list[t - length + 1].append(ct[idx, :].unsqueeze(0))          if not self.left2right:              hidden_out = list(reversed(hidden_out))              memory_out = list(reversed(memory_out))          output_hidden, output_memory = torch.cat(hidden_out, 0), torch.cat(memory_out, 0)          return output_hidden.unsqueeze(0), output_memory.unsqueeze(0)`    当模型的下述部分计算完毕后:  ` ct = self.word_rnn(word_emb, (hx, cx)) `  这个 ct 数据在哪里被使用了, 我实在是看不懂了, 看了好多文章, 都讲的是和一个 LSTM 输出做拼接进一步计算了, 那么这里说的 LSTM 是在哪里被引入的, 这个 ct 数据的使用是又是在代码的哪里体现的呢?  求求解答. 谢谢啊~"
None
None
How to view the .model files
None
1、您在论文中分别提到了bioes和bmeso两种格式，在其他的issues中您提到训练时使用了bioes，请问bmeso有什么作用呢？  2、关于weibo数据集的版本， 
请问模型输入中的gaz_list代表什么
在对Ontonotes的测试集进行decode时  从55747行开始，decode出来的内容与原测试集的内容(文本内容，而非标签)不一致  总体而言decode出来的内容比原测试集内容要少一些  其他数据集(除了msra)均有类似情况
"如下图，公式中15在计算当前字符j的隐状态Cj,c的时候，用到了当前字符的输入门和基于词典匹配到的词的输入门的信息，但是这个时候如下方公式11中红框中的字符的遗忘门的公式没有加到公式15中，是直接不用前一个状态信息遗忘门吗？    !   !   "
我想问下 对于测试集长度长于训练集的训练长度的文本时？对于这部分长度比训练集设定的最大训练文本长度长的话，在计算P\R\F1时是怎么进行处理的呀？  是截断成两个句子分别预测吗？ 还是对于长于训练集部分的长度进行抛弃，计算PRF1时候也不用它们？
你好，在cpu上要训练多长时间啊
None
巨佬您好... 我从去年 follow 您的这篇工作，一直跟到 NAACL19 的 Lattice 分词器，一直在学习您出色的数学理论和优秀的代码功底，一直超崇拜你！但是，一直有一个问题困扰着我，像“南京市长江大桥”这个例子中，“市长” 这个词也会跟随 “长” 字进入 Lattice-LSTM 中去训练，Lattice-LSTM 是如何去处理诸如这样的错误信息呢？另外，论文中与每一个 Xc 做匹配的 lexicon 就是预训练好的词向量吗？恳请巨佬能够不吝赐教！
None
"I already  have a trained model.   And I know how to use the model to decode a file.  But I want to use the model to decode online text to get entities.  that is , decode a line and get some entities at a time.  How to change code to achieve this ?    Thank u very much."
"您好，我用的自带的weibo.conll的数据进行训练，参照84#也统一了一下参数，但是好久不能收敛，同时loss好几十万，您知道大概什么参数美调对么。谢谢，log如下：  CuDNN: True  GPU available: True  Status: train  Seg:  True  Train file: data/weiboNER.conll.train.txt  Dev file: data/weiboNER.conll.dev.txt  Test file: data/weiboNER.conll.test.txt  Raw file: None  Char emb: data/gigaword_chn.all.a2b.uni.ite50.vec  Bichar emb: None  Gaz file: data/ctb.50d.vec  Model saved to: data/model/weibo.  Load gaz file:  data/ctb.50d.vec  total size: 704368  gaz alphabet size: 10798  gaz alphabet size: 12235  gaz alphabet size: 13671  build word pretrain emb...  Embedding:       pretrain word:11327, prefect match:3281, case_match:0, oov:79, oov%:0.023504909253198453  build biword pretrain emb...  Embedding:       pretrain word:0, prefect match:0, case_match:0, oov:42651, oov%:0.999976554440589  build gaz pretrain emb...  Embedding:       pretrain word:704368, prefect match:13669, case_match:0, oov:1, oov%:7.31475385853266e-05  Training model...  DATA SUMMARY START:       Tag          scheme: BIO       MAX SENTENCE LENGTH: 250       MAX   WORD   LENGTH: -1       Number   normalized: True       Use          bigram: False       Word  alphabet size: 3361       Biword alphabet size: 42652       Char  alphabet size: 3358       Gaz   alphabet size: 13671       Label alphabet size: 16       Word embedding size: 50       Biword embedding size: 50       Char embedding size: 30       Gaz embedding size: 50       Norm     word   emb: True       Norm     biword emb: True       Norm     gaz    emb: False       Norm   gaz  dropout: 0.1       Train instance number: 1350       Dev   instance number: 270       Test  instance number: 270       Raw   instance number: 0       Hyperpara  iteration: 100       Hyperpara  batch size: 1       Hyperpara          lr: 0.015       Hyperpara    lr_decay: 0.05       Hyperpara     HP_clip: 5.0       Hyperpara    momentum: 0       Hyperpara  hidden_dim: 200       Hyperpara     dropout: 0.5       Hyperpara  lstm_layer: 1       Hyperpara      bilstm: True       Hyperpara         GPU: True       Hyperpara     use_gaz: True       Hyperpara fix gaz emb: False       Hyperpara    use_char: False  DATA SUMMARY END.  Data setting saved to file:  ResumeNER/save.dset  build batched lstmcrf...  build batched bilstm...  build LatticeLSTM...  forward , Fix emb: False  gaz drop: 0.1  /home/ztl/PycharmProjects/latticeLSTM/model/latticelstm.py:104: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.    init.orthogonal(self.weight_ih.data)  load pretrain word emb... (13671, 50)  /home/ztl/PycharmProjects/latticeLSTM/model/latticelstm.py:105: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.    init.orthogonal(self.alpha_weight_ih.data)  /home/ztl/PycharmProjects/latticeLSTM/model/latticelstm.py:117: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.    init.constant(self.bias.data, val=0)  /home/ztl/PycharmProjects/latticeLSTM/model/latticelstm.py:118: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.    init.constant(self.alpha_bias.data, val=0)  /home/ztl/PycharmProjects/latticeLSTM/model/latticelstm.py:37: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.    init.orthogonal(self.weight_ih.data)  /home/ztl/PycharmProjects/latticeLSTM/model/latticelstm.py:43: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.    init.constant(self.bias.data, val=0)  build LatticeLSTM...  backward , Fix emb: False  gaz drop: 0.1  load pretrain word emb... (13671, 50)  build batched crf...  finished built model.  Epoch: 0/100   Learning rate is setted as: 0.015       Instance: 1350; Time: 152.99s; loss: 4618066.5410; acc: 57990.0/73780.0=0.7860  Epoch: 0 training finished. Time: 152.99s, speed: 8.82st/s,  total loss: 4618066.541015625  /home/ztl/PycharmProjects/latticeLSTM/main.py:218: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.    char_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len, max_word_len)), volatile =  volatile_flag).long()  /home/ztl/PycharmProjects/latticeLSTM/model/latticelstm.py:261: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.    word_var = autograd.Variable(torch.LongTensor(skip_input_[t][0]),volatile =  volatile_flag)  gold_num =  301  pred_num =  9  right_num =  0  Dev: time: 13.57s, speed: 19.92st/s; acc: 0.9338, p: 0.0000, r: 0.0000, f: -1.0000  gold_num =  310  pred_num =  16  right_num =  0  Test: time: 13.97s, speed: 19.35st/s; acc: 0.9361, p: 0.0000, r: 0.0000, f: -1.0000  Epoch: 1/100   Learning rate is setted as: 0.014249999999999999       Instance: 1350; Time: 154.49s; loss: 2459108.2500; acc: 67563.0/73780.0=0.9157  Epoch: 1 training finished. Time: 154.49s, speed: 8.74st/s,  total loss: 2459108.25  gold_num =  301  pred_num =  26  right_num =  0  Dev: time: 13.57s, speed: 19.92st/s; acc: 0.9338, p: 0.0000, r: 0.0000, f: -1.0000  gold_num =  310  pred_num =  26  right_num =  0  Test: time: 13.86s, speed: 19.50st/s; acc: 0.9371, p: 0.0000, r: 0.0000, f: -1.0000  Epoch: 2/100   Learning rate is setted as: 0.0135375       Instance: 1350; Time: 153.91s; loss: 1731192.8750; acc: 67235.0/73780.0=0.9113  Epoch: 2 training finished. Time: 153.91s, speed: 8.77st/s,  total loss: 1731192.875  gold_num =  301  pred_num =  23  right_num =  2  Dev: time: 13.63s, speed: 19.83st/s; acc: 0.9378, p: 0.0870, r: 0.0066, f: 0.0123  Exceed previous best f score: -1  gold_num =  310  pred_num =  19  right_num =  0  Test: time: 13.88s, speed: 19.48st/s; acc: 0.9420, p: 0.0000, r: 0.0000, f: -1.0000  Epoch: 3/100   Learning rate is setted as: 0.012860624999999997       Instance: 1350; Time: 154.79s; loss: 1379898.4375; acc: 67332.0/73780.0=0.9126  Epoch: 3 training finished. Time: 154.79s, speed: 8.72st/s,  total loss: 1379898.4375  gold_num =  301  pred_num =  14  right_num =  0  Dev: time: 13.75s, speed: 19.65st/s; acc: 0.9397, p: 0.0000, r: 0.0000, f: -1.0000  gold_num =  310  pred_num =  12  right_num =  0  Test: time: 14.25s, speed: 18.97st/s; acc: 0.9418, p: 0.0000, r: 0.0000, f: -1.0000  Epoch: 4/100   Learning rate is setted as: 0.012217593749999998       Instance: 1350; Time: 152.44s; loss: 1097173.6875; acc: 67249.0/73780.0=0.9115  Epoch: 4 training finished. Time: 152.44s, speed: 8.86st/s,  total loss: 1097173.6875  gold_num =  301  pred_num =  27  right_num =  1  Dev: time: 13.43s, speed: 20.13st/s; acc: 0.9393, p: 0.0370, r: 0.0033, f: 0.0061  gold_num =  310  pred_num =  23  right_num =  0  Test: time: 13.85s, speed: 19.52st/s; acc: 0.9411, p: 0.0000, r: 0.0000, f: -1.0000  Epoch: 5/100   Learning rate is setted as: 0.011606714062499995       Instance: 1350; Time: 153.71s; loss: 928970.1250; acc: 67332.0/73780.0=0.9126  Epoch: 5 training finished. Time: 153.71s, speed: 8.78st/s,  total loss: 928970.125  gold_num =  301  pred_num =  32  right_num =  1  Dev: time: 13.67s, speed: 19.77st/s; acc: 0.9396, p: 0.0312, r: 0.0033, f: 0.0060  gold_num =  310  pred_num =  24  right_num =  0  Test: time: 13.99s, speed: 19.31st/s; acc: 0.9411, p: 0.0000, r: 0.0000, f: -1.0000  Epoch: 6/100   Learning rate is setted as: 0.011026378359374997       Instance: 1350; Time: 155.16s; loss: 815103.9062; acc: 67232.0/73780.0=0.9112  Epoch: 6 training finished. Time: 155.16s, speed: 8.70st/s,  total loss: 815103.90625  gold_num =  301  pred_num =  22  right_num =  1  Dev: time: 13.47s, speed: 20.06st/s; acc: 0.9396, p: 0.0455, r: 0.0033, f: 0.0062  gold_num =  310  pred_num =  10  right_num =  0  Test: time: 13.82s, speed: 19.55st/s; acc: 0.9430, p: 0.0000, r: 0.0000, f: -1.0000"
请问acl 2018 论文Chinese NER Using Lattice LSTM里正文的中文是怎么在不影响acl模板格式的情况下打出来的呢？网上查到的方法都会改变原有的模板格式。  谢谢。
您好，我想问一下，你的训练集是要被分词么，我看函数里面有word，biword，我不知道哪里用到，是训练集本身里面就分好了么。谢谢您
您好，您的词典是通过对大量语料进行分词得到的吗？能分享一下词典吗，文中给的链接似乎打不开了
如题
你好，PyTorch版本是0.3.1，应该怎么改一下呢，按issues8中的那个改还是有问题
您好！  想请教一下论文里面的Lattice LSTM模型有提到是用的双向还是单向LSTM吗？希望您解答，谢谢。
您好！请问在Lattice LSTM中用的是单向LSTM吗？或者说是通过改进LSTM-CRF模型得到的Lattice LSTM模型的吗？虚心请教。
rt..
I want to reimplement your experientment but I can't find the OntoNotes-4 dataset same as your split.
"日志文件如下：      D:\Anaconda3\python.exe G:/experiment/main.py  CuDNN: True  GPU available: False  Status: train  Seg:  True  Train file: ./data/onto4ner.cn/weiboNER.conll.train  Dev file: ./data/onto4ner.cn/weiboNER.conll.dev  Test file: ./data/onto4ner.cn/weiboNER.conll.test  Raw file: ./data/onto4ner.cn/weiboNER.conll.test  Char emb: data/gigaword_chn.all.a2b.uni.ite50.vec  Bichar emb: None  Gaz file: data/ctb.50d.vec  Model saved to: ./data/onto4ner.cn/saved_model  Load gaz file:  data/ctb.50d.vec  total size: 629353  gaz alphabet size: 677  gaz alphabet size: 767  gaz alphabet size: 858  build word pretrain emb...  Embedding:       pretrain word:11327, prefect match:84, case_match:0, oov:3161, oov%:0.9738139248305607  build biword pretrain emb...  Embedding:       pretrain word:0, prefect match:0, case_match:0, oov:42277, oov%:0.9999763470362837  build gaz pretrain emb...  Embedding:       pretrain word:704368, prefect match:847, case_match:0, oov:10, oov%:0.011655011655011656  Training model...  DATA SUMMARY START:       Tag          scheme: BIO       MAX SENTENCE LENGTH: 250       MAX   WORD   LENGTH: -1       Number   normalized: True       Use          bigram: False       Word  alphabet size: 3246       Biword alphabet size: 42278       Char  alphabet size: 158       Gaz   alphabet size: 858       Label alphabet size: 16       Word embedding size: 50       Biword embedding size: 50       Char embedding size: 50       Gaz embedding size: 50       Norm     word   emb: True       Norm     biword emb: True       Norm     gaz    emb: False       Norm   gaz  dropout: 0.5       Train instance number: 1350       Dev   instance number: 270       Test  instance number: 270       Raw   instance number: 0       Hyperpara  iteration: 100       Hyperpara  batch size: 1       Hyperpara          lr: 0.015       Hyperpara    lr_decay: 0.05       Hyperpara     HP_clip: 5.0       Hyperpara    momentum: 0       Hyperpara  hidden_dim: 200       Hyperpara     dropout: 0.5       Hyperpara  lstm_layer: 1       Hyperpara      bilstm: True       Hyperpara         GPU: False       Hyperpara     use_gaz: True       Hyperpara fix gaz emb: False       Hyperpara    use_char: False  DATA SUMMARY END.  Data setting saved to file:  ./data/onto4ner.cn/saved_model.dset  build batched lstmcrf...  build batched bilstm...  build LatticeLSTM...  forward , Fix emb: False  gaz drop: 0.5  load pretrain word emb... (858, 50)  build LatticeLSTM...  backward , Fix emb: False  gaz drop: 0.5  load pretrain word emb... (858, 50)  build batched crf...  finished built model.  Epoch: 0/100   Learning rate is setted as: 0.015       Instance: 500; Time: 48.44s; loss: 6876.1025; acc: 25840/27265=0.9477       Instance: 1000; Time: 34.44s; loss: 5515.9680; acc: 51667/54737=0.9439       Instance: 1350; Time: 25.46s; loss: 3288.7032; acc: 69619/73780=0.9436  Epoch: 0 training finished. Time: 108.34s, speed: 12.46st/s,  total loss: 15680.77370262146  gold_num =  301  pred_num =  0  right_num =  0  Dev: time: 6.29s, speed: 43.05st/s; acc: 0.9428, p: -1.0000, r: 0.0000, f: -1.0000  gold_num =  310  pred_num =  0  right_num =  0  Test: time: 6.66s, speed: 40.62st/s; acc: 0.9448, p: -1.0000, r: 0.0000, f: -1.0000  Epoch: 1/100   Learning rate is setted as: 0.014249999999999999       Instance: 500; Time: 37.29s; loss: 4565.2451; acc: 25885/27435=0.9435       Instance: 1000; Time: 35.23s; loss: 3729.4443; acc: 51309/54336=0.9443       Instance: 1350; Time: 25.63s; loss: 2592.1435; acc: 69696/73780=0.9446  Epoch: 1 training finished. Time: 98.14s, speed: 13.76st/s,  total loss: 10886.832878112793  gold_num =  301  pred_num =  50  right_num =  34  Dev: time: 5.96s, speed: 45.42st/s; acc: 0.9481, p: 0.6800, r: 0.1130, f: 0.1937  Exceed previous best f score: -1  gold_num =  310  pred_num =  35  right_num =  22  Test: time: 6.93s, speed: 44.14st/s; acc: 0.9477, p: 0.6286, r: 0.0710, f: 0.1275  Epoch: 2/100   Learning rate is setted as: 0.0135375       Instance: 500; Time: 39.36s; loss: 2878.0598; acc: 25730/26938=0.9552       Instance: 1000; Time: 37.79s; loss: 3445.0646; acc: 52187/55068=0.9477       Instance: 1350; Time: 24.20s; loss: 2031.7176; acc: 69970/73780=0.9484  Epoch: 2 training finished. Time: 101.34s, speed: 13.32st/s,  total loss: 8354.8420753479  gold_num =  301  pred_num =  110  right_num =  62  Dev: time: 5.87s, speed: 46.02st/s; acc: 0.9298, p: 0.5636, r: 0.2060, f: 0.3017  Exceed previous best f score: 0.19373219373219375  gold_num =  310  pred_num =  82  right_num =  34  Test: time: 5.76s, speed: 47.29st/s; acc: 0.9341, p: 0.4146, r: 0.1097, f: 0.1735  Epoch: 3/100   Learning rate is setted as: 0.012860624999999997       Instance: 500; Time: 37.55s; loss: 2718.9896; acc: 25936/27331=0.9490       Instance: 1000; Time: 36.35s; loss: 2820.8742; acc: 51557/54327=0.9490       Instance: 1350; Time: 25.09s; loss: 1802.1964; acc: 70014/73780=0.9490  Epoch: 3 training finished. Time: 98.99s, speed: 13.64st/s,  total loss: 7342.060092926025  gold_num =  301  pred_num =  28  right_num =  21  Dev: time: 5.84s, speed: 46.27st/s; acc: 0.9466, p: 0.7500, r: 0.0698, f: 0.1277  gold_num =  310  pred_num =  18  right_num =  11  Test: time: 5.91s, speed: 45.79st/s; acc: 0.9472, p: 0.6111, r: 0.0355, f: 0.0671  Epoch: 4/100   Learning rate is setted as: 0.012217593749999998       Instance: 500; Time: 37.22s; loss: 2447.0826; acc: 27172/28385=0.9573       Instance: 1000; Time: 32.56s; loss: 2286.7070; acc: 51892/54389=0.9541       Instance: 1350; Time: 24.49s; loss: 1831.1641; acc: 70284/73780=0.9526  Epoch: 4 training finished. Time: 94.27s, speed: 14.32st/s,  total loss: 6564.953800201416  gold_num =  301  pred_num =  152  right_num =  83  Dev: time: 6.27s, speed: 43.16st/s; acc: 0.9513, p: 0.5461, r: 0.2757, f: 0.3664  Exceed previous best f score: 0.3017031630170316  gold_num =  310  pred_num =  116  right_num =  55  Test: time: 7.61s, speed: 43.71st/s; acc: 0.9504, p: 0.4741, r: 0.1774, f: 0.2582      Epoch: 99/100   Learning rate is setted as: 9.348204032106312e-05       Instance: 500; Time: 36.70s; loss: 255.0433; acc: 28084/28217=0.9953       Instance: 1000; Time: 34.23s; loss: 216.6641; acc: 54065/54314=0.9954       Instance: 1350; Time: 25.20s; loss: 176.6801; acc: 73434/73780=0.9953  Epoch: 99 training finished. Time: 96.13s, speed: 14.04st/s,  total loss: 648.3875198364258  gold_num =  301  pred_num =  214  right_num =  89  Dev: time: 6.04s, speed: 44.78st/s; acc: 0.9436, p: 0.4159, r: 0.2957, f: 0.3456  gold_num =  310  pred_num =  209  right_num =  87  Test: time: 6.19s, speed: 43.72st/s; acc: 0.9491, p: 0.4163, r: 0.2806, f: 0.3353    Process finished with exit code 0  请问一下问题出在哪里了呢？"
"As the title shows, if you want to train LatticeLSTM by mini-batch, please refer to  "
"D:\Anaconda3\python.exe G:/experiment/main.py  CuDNN: True  GPU available: False  Status: train  Seg:  True  Train file: ./data/onto4ner.cn/weiboNER_2nd_conll.train  Dev file: ./data/onto4ner.cn/weiboNER_2nd_conll.dev  Test file: ./data/onto4ner.cn/weiboNER_2nd_conll.test  Raw file: ./data/onto4ner.cn/weiboNER_2nd_conll.test  Char emb: data/gigaword_chn.all.a2b.uni.ite50.vec  Bichar emb: None  Gaz file: data/ctb.50d.vec  Model saved to: ./data/onto4ner.cn/saved_model  Load gaz file:  data/ctb.50d.vec  total size: 704368  gaz alphabet size: 2  gaz alphabet size: 2  gaz alphabet size: 2  build word pretrain emb...  Embedding:       pretrain word:11327, prefect match:0, case_match:0, oov:3387, oov%:0.9997048406139315  build biword pretrain emb...  Embedding:       pretrain word:0, prefect match:0, case_match:0, oov:45491, oov%:0.9999780181130749  build gaz pretrain emb...  Embedding:       pretrain word:704368, prefect match:0, case_match:0, oov:1, oov%:0.5  Training model...  DATA SUMMARY START:       Tag          scheme: BIO       MAX SENTENCE LENGTH: 250       MAX   WORD   LENGTH: -1       Number   normalized: True       Use          bigram: False       Word  alphabet size: 3388       Biword alphabet size: 45492       Char  alphabet size: 3357       Gaz   alphabet size: 2       Label alphabet size: 18       Word embedding size: 50       Biword embedding size: 50       Char embedding size: 50       Gaz embedding size: 50       Norm     word   emb: True       Norm     biword emb: True       Norm     gaz    emb: False       Norm   gaz  dropout: 0.5       Train instance number: 1350       Dev   instance number: 270       Test  instance number: 270       Raw   instance number: 0       Hyperpara  iteration: 100       Hyperpara  batch size: 1       Hyperpara          lr: 0.015       Hyperpara    lr_decay: 0.05       Hyperpara     HP_clip: 5.0       Hyperpara    momentum: 0       Hyperpara  hidden_dim: 200       Hyperpara     dropout: 0.5       Hyperpara  lstm_layer: 1       Hyperpara      bilstm: True       Hyperpara         GPU: False       Hyperpara     use_gaz: True       Hyperpara fix gaz emb: False       Hyperpara    use_char: False  DATA SUMMARY END.  Data setting saved to file:  ./data/onto4ner.cn/saved_model.dset  build batched lstmcrf...  build batched bilstm...  build LatticeLSTM...  forward , Fix emb: False  gaz drop: 0.5  load pretrain word emb... (2, 50)  build LatticeLSTM...  backward , Fix emb: False  gaz drop: 0.5  load pretrain word emb... (2, 50)  build batched crf...  finished built model.  Epoch: 0/100   Learning rate is setted as: 0.015       Instance: 500; Time: 43.23s; loss: 8353.7121; acc: 25399/27265=0.9316       Instance: 1000; Time: 36.27s; loss: 6507.9950; acc: 50956/54737=0.9309       Instance: 1350; Time: 23.01s; loss: 3835.8206; acc: 68546/73778=0.9291  Epoch: 0 training finished. Time: 102.51s, speed: 13.17st/s,  total loss: 18697.527671813965  gold_num =  389  pred_num =  7  right_num =  0  Dev: time: 5.64s, speed: 48.13st/s; acc: 0.9330, p: 0.0000, r: 0.0000, f: -1.0000  gold_num =  414  pred_num =  2  right_num =  1  Test: time: 6.57s, speed: 41.16st/s; acc: 0.9274, p: 0.5000, r: 0.0024, f: 0.0048  Epoch: 1/100   Learning rate is setted as: 0.014249999999999999       Instance: 500; Time: 32.57s; loss: 4907.2505; acc: 25699/27435=0.9367       Instance: 1000; Time: 31.39s; loss: 4013.4990; acc: 50852/54336=0.9359       Instance: 1350; Time: 22.50s; loss: 2766.0821; acc: 69083/73778=0.9364  Epoch: 1 training finished. Time: 86.46s, speed: 15.61st/s,  total loss: 11686.831638336182  gold_num =  389  pred_num =  144  right_num =  93  Dev: time: 5.27s, speed: 51.43st/s; acc: 0.9453, p: 0.6458, r: 0.2391, f: 0.3490  Exceed previous best f score: -1  gold_num =  414  pred_num =  107  right_num =  56  Test: time: 5.30s, speed: 51.53st/s; acc: 0.9361, p: 0.5234, r: 0.1353, f: 0.2150  Epoch: 2/100   Learning rate is setted as: 0.0135375       Instance: 500; Time: 31.27s; loss: 3183.9019; acc: 25468/26936=0.9455       Instance: 1000; Time: 32.47s; loss: 3497.6632; acc: 51754/55066=0.9399       Instance: 1350; Time: 22.58s; loss: 2232.6319; acc: 69466/73778=0.9416  Epoch: 2 training finished. Time: 86.32s, speed: 15.64st/s,  total loss: 8914.197044372559  gold_num =  389  pred_num =  173  right_num =  106  Dev: time: 5.64s, speed: 48.05st/s; acc: 0.9482, p: 0.6127, r: 0.2725, f: 0.3772  Exceed previous best f score: 0.34896810506566606  gold_num =  414  pred_num =  123  right_num =  78  Test: time: 5.92s, speed: 46.38st/s; acc: 0.9402, p: 0.6341, r: 0.1884, f: 0.2905  Epoch: 3/100   Learning rate is setted as: 0.012860624999999997       Instance: 500; Time: 32.10s; loss: 2750.2932; acc: 25878/27331=0.9468       Instance: 1000; Time: 32.04s; loss: 2797.1717; acc: 51392/54325=0.9460       Instance: 1350; Time: 24.10s; loss: 1805.7985; acc: 69830/73778=0.9465  Epoch: 3 training finished. Time: 88.24s, speed: 15.30st/s,  total loss: 7353.263328552246  gold_num =  389  pred_num =  140  right_num =  89  Dev: time: 5.60s, speed: 48.35st/s; acc: 0.9478, p: 0.6357, r: 0.2288, f: 0.3365  gold_num =  414  pred_num =  87  right_num =  62  Test: time: 6.24s, speed: 43.38st/s; acc: 0.9375, p: 0.7126, r: 0.1498, f: 0.2475  Epoch: 4/100   Learning rate is setted as: 0.012217593749999998       Instance: 500; Time: 38.97s; loss: 2368.9777; acc: 27038/28385=0.9525       Instance: 1000; Time: 35.29s; loss: 2310.6736; acc: 51716/54387=0.9509       Instance: 1350; Time: 25.06s; loss: 1737.6990; acc: 70130/73778=0.9506  Epoch: 4 training finished. Time: 99.33s, speed: 13.59st/s,  total loss: 6417.350269317627  gold_num =  389  pred_num =  242  right_num =  139  Dev: time: 5.80s, speed: 46.66st/s; acc: 0.9506, p: 0.5744, r: 0.3573, f: 0.4406  Exceed previous best f score: 0.3772241992882562  gold_num =  414  pred_num =  205  right_num =  120  Test: time: 6.18s, speed: 44.02st/s; acc: 0.9450, p: 0.5854, r: 0.2899, f: 0.3877  Epoch: 5/100   Learning rate is setted as: 0.011606714062499995       Instance: 500; Time: 37.95s; loss: 1882.3516; acc: 25622/26726=0.9587       Instance: 1000; Time: 35.38s; loss: 2224.0402; acc: 51347/53790=0.9546       Instance: 1350; Time: 23.87s; loss: 1504.2289; acc: 70438/73778=0.9547  Epoch: 5 training finished. Time: 97.19s, speed: 13.89st/s,  total loss: 5610.620803833008  gold_num =  389  pred_num =  254  right_num =  146  Dev: time: 5.90s, speed: 45.86st/s; acc: 0.9515, p: 0.5748, r: 0.3753, f: 0.4541  Exceed previous best f score: 0.44057052297939775  gold_num =  414  pred_num =  229  right_num =  140  Test: time: 6.11s, speed: 44.64st/s; acc: 0.9471, p: 0.6114, r: 0.3382, f: 0.4355  Epoch: 6/100   Learning rate is setted as: 0.011026378359374997       Instance: 500; Time: 32.10s; loss: 1676.2667; acc: 25272/26259=0.9624       Instance: 1000; Time: 32.24s; loss: 1810.9099; acc: 51960/54019=0.9619       Instance: 1350; Time: 23.36s; loss: 1524.6076; acc: 70781/73778=0.9594  Epoch: 6 training finished. Time: 87.70s, speed: 15.39st/s,  total loss: 5011.784099578857  gold_num =  389  pred_num =  255  right_num =  161  Dev: time: 5.46s, speed: 49.55st/s; acc: 0.9553, p: 0.6314, r: 0.4139, f: 0.5000  Exceed previous best f score: 0.4541213063763608  gold_num =  414  pred_num =  225  right_num =  148  Test: time: 6.74s, speed: 47.90st/s; acc: 0.9486, p: 0.6578, r: 0.3575, f: 0.4632  Epoch: 7/100   Learning rate is setted as: 0.010475059441406245       Instance: 500; Time: 32.21s; loss: 1650.2751; acc: 26363/27373=0.9631       Instance: 1000; Time: 32.79s; loss: 1681.9573; acc: 52655/54691=0.9628       Instance: 1350; Time: 22.37s; loss: 1277.5512; acc: 70933/73778=0.9614  Epoch: 7 training finished. Time: 87.37s, speed: 15.45st/s,  total loss: 4609.783710479736  gold_num =  389  pred_num =  301  right_num =  164  Dev: time: 5.38s, speed: 50.37st/s; acc: 0.9515, p: 0.5449, r: 0.4216, f: 0.4754  gold_num =  414  pred_num =  324  right_num =  162  Test: time: 5.61s, speed: 48.31st/s; acc: 0.9444, p: 0.5000, r: 0.3913, f: 0.4390  Epoch: 8/100   Learning rate is setted as: 0.009951306469335933       Instance: 500; Time: 33.81s; loss: 1308.9993; acc: 26565/27332=0.9719       Instance: 1000; Time: 34.60s; loss: 1639.5016; acc: 52881/54695=0.9668       Instance: 1350; Time: 23.46s; loss: 1182.6076; acc: 71262/73778=0.9659  Epoch: 8 training finished. Time: 91.87s, speed: 14.70st/s,  total loss: 4131.108478546143  gold_num =  389  pred_num =  314  right_num =  180  Dev: time: 5.28s, speed: 51.31st/s; acc: 0.9540, p: 0.5732, r: 0.4627, f: 0.5121  Exceed previous best f score: 0.5  gold_num =  414  pred_num =  315  right_num =  172  Test: time: 5.49s, speed: 49.49st/s; acc: 0.9481, p: 0.5460, r: 0.4155, f: 0.4719  Epoch: 9/100   Learning rate is setted as: 0.009453741145869136       Instance: 500; Time: 33.62s; loss: 1346.7611; acc: 26282/27151=0.9680       Instance: 1000; Time: 34.71s; loss: 1505.3554; acc: 53071/54864=0.9673       Instance: 1350; Time: 23.36s; loss: 976.1559; acc: 71384/73778=0.9676  Epoch: 9 training finished. Time: 91.69s, speed: 14.72st/s,  total loss: 3828.2723808288574  gold_num =  389  pred_num =  304  right_num =  174  Dev: time: 5.52s, speed: 49.01st/s; acc: 0.9526, p: 0.5724, r: 0.4473, f: 0.5022  gold_num =  414  pred_num =  297  right_num =  161  Test: time: 5.78s, speed: 46.87st/s; acc: 0.9460, p: 0.5421, r: 0.3889, f: 0.4529  以上是我跑了微博相关数据集的部分代码结果，其中我想问一下日志文件中gaz alphabet size: 为什么一直是2呢？"
"    def forward(self, input, skip_input_list, hidden=None):          """"""              input: variable (batch, seq_len), batch = 1              skip_input_list: [skip_input, volatile_flag]              skip_input: three dimension list, with length is seq_len. Each element is a list of matched word id and its length.                           example: [[], [[25,13],[2,3]]] 25/13 is word id, 2,3 is word length .           """"""          volatile_flag = skip_input_list[-1]          skip_input = skip_input_list[0:-1]            max_seq_len = input.size(1)          #max_seq_len = 128    我在latticelstm.py这里直接改报错"
你好，感谢您分享的代码。代码中batch_size设置的为1，训练速度太慢。请问如何修改batch_size呢？谢谢！！
请问data目录下的数据集是完整的 resume data 吗？
None
您好，我这边已经通过学校注册上了LDC也下到了ontoNotes4的原始数据集，但是在切分和整合出训练/测试数据方面实在有点苦手，能麻烦您提供一份你们论文中使用的训练/测试数据集吗？我的邮箱是wayne_li@bupt.edu.cn，需要看LDC的License和Corpora Invoiced记录的话我可以另外发给您。麻烦了！
"作者您好：  我用weibo数据集（BMEOS模式）在您的原始代码（参数等均未改动）上跑了lattice model的实验，在NE,NM,ALL测试集上的F值都没有达到您论文汇报的结果（分别为：51.77vs53.04，60.00vs62.25，56.00vs58.79）。  我查看了之前的issues，看到您回复由于weibo数据规模小，所以存在不稳定的问题。因此我又尝试了8个不同的random seed，在weibo的all数据集上跑了实验，发现在weibo.all的test上最好的结果能达到57.03，仍然低于您论文汇报的结果58.79。请问我还可以进行哪些尝试以达到您论文汇报的结果？您论文的结果使用的是代码里的参数吗，我是否还需要调参？非常感谢！"
"请问论文里的公式13中的h_{b}^c是否应为h_{b-1}^c，比如“大桥”用输入了“江”后得到的hidden state和""大桥""的词向量作为输入。目前是输入了""大""后的hidden state和""大桥""的词向量作为lstm的输入。"
您好，ontonotes数据集我参考了您文章中提到的那篇论文《Named Entity Recognition with Bilingual Constraints》，但实际切分出的跟您的统计数据有一些差距，请问您能发给我一份切分的数据吗，我已经获得access，如果您需要看access我可以通过邮箱发给您，我的邮箱是xuemengge@iie.ac.cn  另外，想麻烦您跟您确认一下，论文中四份数据集都是用的BMES标注标准吗  若您回复，不胜感激！
您好，我在 
您好，为什么我把数据换为医疗数据后，效果极低，F1值低到1%，这是为什么呢？我在BILESTM-CRF上面的可以到80%以上
如何对于新的数据集进行预测呢？  我看LatticeLSTM/run_demo.sh只是对你的测试集进行预测
"您好，我最近在实验对比，能给我发一份OntoNote4数据集么？我发现我找到的数据和您论文中的句子数不一致，根据您论文中提供的参考文献找到的数据只有train和test，句子的数量分别为15724，8249,而您论文中的数据在三个数据集上的统计结果分别15.7K，,4.3K，4.3K，似乎后面两项加起来为8.6K，差距有点大。感谢！ nlp_zhang@qq.com"
对于超过max_seq_len的句子，是怎么处理的呢
您好，您的论文与实验对我做NER提供了很大的帮助！  请问您的msra与ontonotes的数据集是怎么划分训练集测试集的呢，这两份数据集好像没有官方的划分标准，如果您方便的话，能麻烦您发一下您这两份的训练开发测试集吗，非常感谢，我的邮箱是xuemengge@iie.ac.cn。如果您方便的话，resume数据集能麻烦您一起分享一下吗，谢谢。
您好，  那篇论文《Named Entity Recognition with Bilingual Constraints》我看了，但是还是想咨询一下，是不是Ontonotes4中的./chinese/annotations文件夹下的数据集，除了./nw/xinhua中的chtb001-0325 和另一个文件下的chtb1000-1078按奇偶分为dev和test，./chinese/annotations中其它文件夹里面的数据全部作为train？
"因为此模型运行在我的一块GPU上（共有两块TITAN X），训练很慢（大概3000s/epoch）,请问此模型是否可以进行多GPU并行训练？比如应该如何修改代码?或者其它方法的优化来提高收敛速度。期待您的回复~"
您好，请问你在latticelstm.py中自己实现的lstm部分是用的lstm的变形GRU吗
找到了，用latticelstm去重构了lstm类
有人在python3+pytorc1.0.0运行过该程序吗？感觉改代码改到崩溃了。
"Hi Jie,    I am curious about how you conduct the word-based NER experiments. As you see, both Weibo and Resume do not have the word-based labeled training dataset, how can we train a word-based model?      I'd like to know if you use some methods to transform the character-based dataset to word-based dataset?     I am willing to hear from you soon!    Regards.  Wei"
请问一下您的代码里有用来加载模型以预测新的测试集的那部分代码吗，我找了很久没有找到，是需要自己写吗？
"运行main.py后出现如下问题  build batched crf...  THCudaCheck FAIL file=/pytorch/torch/lib/THC/generic/THCStorage.cu line=58 error=2 : out of memory  Traceback (most recent call last):    File ""main.py"", line 458, in        train(data, save_model_dir, seg)    File ""main.py"", line 252, in train      model = SeqModel(data)    File ""/home/xfbai/LatticeLSTM/model/bilstmcrf.py"", line 24, in __init__      self.crf = CRF(label_size, self.gpu)    File ""/home/xfbai/LatticeLSTM/model/crf.py"", line 47, in __init__      init_transitions = init_transitions.cuda()    File ""/home/xfbai/anaconda3/envs/py2/lib/python2.7/site-packages/torch/_utils.py"", line 69, in _cuda      return new_type(self.size()).copy_(self, async)  RuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58  请问是什么问题呢，试过网上很多方法都没用，batch_size已经是1了。  谢谢。"
"It seems that there has been some changes in Pan Baidu policies, it makes it very difficult (if even possible) to download the models from outside China and reproduce your experiments and/or benefit from your work. Would it be possible to upload these files somewhere else on the Internet ?  Many thanks !"
"您好，在看代码时，我发现在utils/data.py里面的 def build_gaz_alphabet(self, input_file)函数中，您设置条件： if len(line) > 3:    但其实每行数据都多出个换行符，是否应该将其改为：if len(line)>4， 或者利用strip()先去掉换行符？？"
None
None
首先很感谢您提供的代码，您论文中提到使用词分割的标签作为soft feature，并将分词标签转化为了标签向量，请问标签向量是随机初始化并随着训练一起fine tune的还是使用了某种预训练的方式得到的？
" # build iter   seq_iter = enumerate(scores)   # bat_size * from_target_size * to_target_size   _, inivalues = seq_iter.next()  "
您好，我分别使用了：  1、您data目录下的demo数据集  2、您ResumeNER下的数据集  3、MSRA数据集（BIO）  4、人民日报数据集（BIO）  无论是否放入预训练的词向量（ctb.50d.vec  && gigaword_chn.all.a2b.uni.ite50.vec），只有ResumeNER目录下的数据集（2）结果达标，其余的召回率都在75%（3）、50%（1、4）左右。  请问我应该怎么修改代码 来使用这些数据集。  谢谢
None
"您好，我发现在test模式下，在对test数据集加载模型时会报错：  RuntimeError: While copying the parameter named lstm.hidden2tag.weight, whose dimensions in the model are torch.Size([22, 200]) and whose dimensions in the checkpoint are torch.Size([20, 200]).    经过分析，报错原因如下：  在test模式下，会分别对验证集和测试集进行测试，也就是说会利用data构建两次模型，由于在构建model的时候，会执行如下操作：  data.label_alphabet_size += 2  所以在第二次利用data构建模型的时候，label size会比第一次构建的时候多2，因此，在第二次加载模型的时候会报维度不对的错误。  bug修改：  在对‘dev’测试后，需要重新加载data，或者执行如下操作：  data.label_alphabet_size -= 2      "
"weibo log:  CuDNN: True  GPU available: True  Status: train  Seg:  True  Train file: WeiboNER/train.ne.bmes  Dev file: WeiboNER/dev.ne.bmes  Test file: WeiboNER/test.ne.bmes  Raw file: None  Char emb: data/gigaword_chn.all.a2b.uni.ite50.vec  Bichar emb: None  Gaz file: data/ctb.50d.vec  Model saved to: WeiboNER/saved_model.all  Load gaz file:  data/ctb.50d.vec  total size: 704368  gaz alphabet size: 10798  gaz alphabet size: 12235  gaz alphabet size: 13671  build word pretrain emb...  Embedding:       pretrain word:11327, perfect match:3281, case_match:0, oov:75, oov%:0.0223413762288  build biword pretrain emb...  Embedding:       pretrain word:0, perfect match:0, case_match:0, oov:42646, oov%:0.999976551692  build gaz pretrain emb...  Embedding:       pretrain word:704368, perfect match:13669, case_match:0, oov:1, oov%:7.31475385853e-05  Training model...  DATA SUMMARY START:       Tag          scheme: BMES       MAX SENTENCE LENGTH: 250       MAX   WORD   LENGTH: -1       Number   normalized: True       Use          bigram: False       Word  alphabet size: 3357       Biword alphabet size: 42647       Char  alphabet size: 3357       Gaz   alphabet size: 13671       Label alphabet size: 16       Word embedding size: 50       Biword embedding size: 50       Char embedding size: 30       Gaz embedding size: 50       Norm     word   emb: True       Norm     biword emb: True       Norm     gaz    emb: False       Norm   gaz  dropout: 0.5       Train instance number: 1350       Dev   instance number: 270       Test  instance number: 270       Raw   instance number: 0       Hyperpara  iteration: 100       Hyperpara  batch size: 1       Hyperpara          lr: 0.015       Hyperpara    lr_decay: 0.05       Hyperpara     HP_clip: 5.0       Hyperpara    momentum: 0       Hyperpara  hidden_dim: 200       Hyperpara     dropout: 0.5       Hyperpara  lstm_layer: 1       Hyperpara      bilstm: True       Hyperpara         GPU: True       Hyperpara     use_gaz: True       Hyperpara fix gaz emb: False       Hyperpara    use_char: False  DATA SUMMARY END.  Data setting saved to file:  WeiboNER/saved_model.all.dset  build batched lstmcrf...  build batched bilstm...  build LatticeLSTM...  forward , Fix emb: False  gaz drop: 0.5  load pretrain word emb... (13671, 50)  ...  ...  ...  Epoch: 98 training finished. Time: 379.70s, speed: 3.56st/s,  total loss: 638.227500916  gold_num =  169  pred_num =  138  right_num =  83  Dev: time: 23.04s, speed: 11.73st/s; acc: 0.9752, p: 0.6014, r: 0.4911, f: 0.5407  gold_num =  216  pred_num =  140  right_num =  94  Test: time: 23.75s, speed: 11.38st/s; acc: 0.9706, p: 0.6714, r: 0.4352, f: 0.5281  Epoch: 99/100   Learning rate is setted as: 9.34820403211e-05       Instance: 500; Time: 138.24s; loss: 222.6014; acc: 26713.0/26857.0=0.9946       Instance: 1000; Time: 143.49s; loss: 236.1890; acc: 54543.0/54865.0=0.9941       Instance: 1350; Time: 98.02s; loss: 162.5528; acc: 73351.0/73778.0=0.9942  Epoch: 99 training finished. Time: 379.75s, speed: 3.55st/s,  total loss: 621.343292236  gold_num =  169  pred_num =  139  right_num =  83  Dev: time: 23.04s, speed: 11.73st/s; acc: 0.9748, p: 0.5971, r: 0.4911, f: 0.5390  gold_num =  216  pred_num =  142  right_num =  94  Test: time: 23.48s, speed: 11.51st/s; acc: 0.9705, p: 0.6620, r: 0.4352, f: 0.5251  问题出在哪？在overall上的值也没有达到文中的58.79%，只有56点几的  "
None
如题，谢谢。
通常c的计算，都会考虑前一个字符的c状态，为什么论文中这部分没有呢？为什么论文只考虑来当前字符相关的所有词的c状态呢？
你好，请问在预训练embedding时有什么tricks吗？我发现自己使用word2vec训练的字向量在NER上效果并不好。
您好，我现在想用自己的语料库训练，标签集必须要改成BIOES吗，还是BIO也可以，在哪里改标签集合呢？  谢谢
请问要下的两个词向量文件夹中的所有文件都要下吗，还是只要分别下一个就好？  谢谢
I want to use this model for my project.Thx
None
"CuDNN: True  GPU available: False  Status: decode  Seg:  True  Train file: data/conll03/train.bmes  Dev file: data/conll03/dev.bmes  Test file: data/conll03/test.bmes  Raw file: ./rd_data/test/test.txt  Char emb: data/gigaword_chn.all.a2b.uni.ite50.vec  Bichar emb: None  Gaz file: data/ctb.50d.vec  Data setting loaded from file:  ./rd_data/test/test.dset  DATA SUMMARY START:       Tag          scheme: BMES       MAX SENTENCE LENGTH: 250       MAX   WORD   LENGTH: -1       Number   normalized: False       Use          bigram: False       Word  alphabet size: 2596       Biword alphabet size: 31940       Char  alphabet size: 2596       Gaz   alphabet size: 13634       Label alphabet size: 18       Word embedding size: 50       Biword embedding size: 50       Char embedding size: 30       Gaz embedding size: 50       Norm     word   emb: True       Norm     biword emb: True       Norm     gaz    emb: False       Norm   gaz  dropout: 0.5       Train instance number: 0       Dev   instance number: 0       Test  instance number: 0       Raw   instance number: 0       Hyperpara  iteration: 100       Hyperpara  batch size: 1       Hyperpara          lr: 0.015       Hyperpara    lr_decay: 0.05       Hyperpara     HP_clip: 5.0       Hyperpara    momentum: 0       Hyperpara  hidden_dim: 200       Hyperpara     dropout: 0.5       Hyperpara  lstm_layer: 1       Hyperpara      bilstm: True       Hyperpara         GPU: False       Hyperpara     use_gaz: True       Hyperpara fix gaz emb: False       Hyperpara    use_char: False  DATA SUMMARY END.  Load Model from file:  ./rd_data/test/demo_test.6.model  build batched lstmcrf...  build batched bilstm...  build LatticeLSTM...  forward , Fix emb: False  gaz drop: 0.5  load pretrain word emb... (13634, 50)  build LatticeLSTM...  backward , Fix emb: False  gaz drop: 0.5  load pretrain word emb... (13634, 50)  build batched crf...  Traceback (most recent call last):    File ""main_test.py"", line 454, in        decode_results = load_model_decode(model_dir, data, 'raw', gpu, seg)    File ""main_test.py"", line 348, in load_model_decode      model.load_state_dict(torch.load(model_dir))    File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 487, in load_state_dict      .format(name, own_state[name].size(), param.size()))  RuntimeError: While copying the parameter named lstm.word_embeddings.weight, whose dimensions in the model are torch.Size([2596, 50]) and whose dimensions in the checkpoint are torch.Size([2527, 50]).    只修改过main文件,run_demo.sh文件"
"CuDNN: True  GPU available: False  Status: train  Seg:  True  Train file: ./rd_data/train.txt  Dev file: ./rd_data/dev.txt  Test file: ./rd_data/test.txt  Raw file: None  Char emb: data/gigaword_chn.all.a2b.uni.ite50.vec  Bichar emb: None  Gaz file: data/ctb.50d.vec  Model saved to: ./rd_data/demo_test  Load gaz file:  data/ctb.50d.vec  total size: 704368  gaz alphabet size: 31572  gaz alphabet size: 33642  gaz alphabet size: 35512  build word pretrain emb...  Embedding:       pretrain word:11327, prefect match:2497, case_match:0, oov:29, oov%:0.0114760585675  build biword pretrain emb...  Embedding:       pretrain word:0, prefect match:0, case_match:0, oov:91271, oov%:0.999989043737  build gaz pretrain emb...  Embedding:       pretrain word:704368, prefect match:35510, case_match:0, oov:1, oov%:2.81594953818e-05  Training model...  DATA SUMMARY START:       Tag          scheme: BIO       MAX SENTENCE LENGTH: 250       MAX   WORD   LENGTH: -1       Number   normalized: False       Use          bigram: False       Word  alphabet size: 2527       Biword alphabet size: 91272       Char  alphabet size: 2527       Gaz   alphabet size: 35512       Label alphabet size: 5       Word embedding size: 50       Biword embedding size: 50       Char embedding size: 30       Gaz embedding size: 50       Norm     word   emb: True       Norm     biword emb: True       Norm     gaz    emb: False       Norm   gaz  dropout: 0.5       Train instance number: 28185       Dev   instance number: 5885       Test  instance number: 5977       Raw   instance number: 0       Hyperpara  iteration: 100       Hyperpara  batch size: 1       Hyperpara          lr: 0.015       Hyperpara    lr_decay: 0.05       Hyperpara     HP_clip: 5.0       Hyperpara    momentum: 0       Hyperpara  hidden_dim: 200       Hyperpara     dropout: 0.5       Hyperpara  lstm_layer: 1       Hyperpara      bilstm: True       Hyperpara         GPU: False       Hyperpara     use_gaz: True       Hyperpara fix gaz emb: False       Hyperpara    use_char: False  DATA SUMMARY END.  Traceback (most recent call last):    File ""main_test.py"", line 444, in        train(data, save_model_dir, seg)    File ""main_test.py"", line 240, in train      save_data_setting(data, save_data_name)    File ""main_test.py"", line 90, in save_data_setting      new_data = copy.deepcopy(data)    File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy      y = copier(x, memo)    File ""/usr/lib/python2.7/copy.py"", line 298, in _deepcopy_inst      state = deepcopy(state, memo)    File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy      y = copier(x, memo)    File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict      y[deepcopy(key, memo)] = deepcopy(value, memo)    File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy      y = copier(x, memo)    File ""/usr/lib/python2.7/copy.py"", line 230, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy      y = copier(x, memo)    File ""/usr/lib/python2.7/copy.py"", line 230, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy      y = copier(x, memo)    File ""/usr/lib/python2.7/copy.py"", line 230, in _deepcopy_list      y.append(deepcopy(a, memo))    File ""/usr/lib/python2.7/copy.py"", line 192, in deepcopy      memo[d] = y  MemoryError  "
我用tensorflow复现文中的字模型baseline效果，使用了相同的实验配置和embeding，ontonotes 4 test数据集上只得到了61左右的f值，微博语料则要高一些。请问如果用您的代码，需要做什么改动来跑字模型的baseline效果呢？
"In character-based NER, which one is used as the pretrained embeding?gigaword_chn.all.a2b.uni.ite50.vec or joint4.all.b10c1.2h.iter17.mchar?"
please tell me why?    
"dev和test是否只是对chtb 0001-0325, chtb 1001-1078根据奇数偶数编号来做了一下划分呢？剩余的用来做训练集？"
"I read in the paper that you set a weight-decay in the optimizer, but I didn't see that term in your initialization of optimizer in main.py here. I wonder if I have skipped something or you really didn't set the regularization in your code? Thanks."
"As title shows, I upgraded the code.If you want to use the code on pytorch 0.4.1 ,please refer to this.   "
Where can i get weibo and MSRA data?
Hi. I'm now dealing with some clinical unannotated data and I wonder how did you manually annotate the resume data in your experiment. Did you use some tricks or ML based annotatation? Thanks XD.
"gpu=True，但是也没用GPU, 哪儿需要改动，model后面加个cuda()就完了吗？还是还有其他也要改。"
"Sorry that I am new  to pytorch , but here in the class WordLSTMCell ,I found that     f, i, g = torch.split(wh_b + wi, split_size=self.hidden_size, dim=1)     In the formula of your paper, wh_b and wi are not added , so Did I misunderstand your code?    def forward(self, input_, hx):          """"""          Args:              input_: A (batch, input_size) tensor containing input                  features.              hx: A tuple (h_0, c_0), which contains the initial hidden                  and cell state, where the size of both states is                  (batch, hidden_size).          Returns:              h_1, c_1: Tensors containing the next hidden and cell state.          """"""            h_0, c_0 = hx          batch_size = h_0.size(0)          bias_batch = (self.bias.unsqueeze(0).expand(batch_size, *self.bias.size()))          wh_b = torch.addmm(bias_batch, h_0, self.weight_hh)            wi = torch.mm(input_, self.weight_ih)            f, i, g = torch.split(wh_b + wi, split_size=self.hidden_size, dim=1)           c_1 = torch.sigmoid(f)*c_0 + torch.sigmoid(i)*torch.tanh(g)          return c_1"
"Hi, can you share me with pretrained biword-embedding?"
"我根据您的代码，仅仅用char embedding来复现基于char的weibo和MSRA实验，我发现webo和MSRA的结果都达不到论文中的引用值，weibo test只有: 0.475, 论文中是0.5277； MSRA test只有85.75，论文中是88.81。所以，我想请教一下作者，这大概是什么原因造成的？我调试了很久，但是始终没有太大的提升。"
"In main python script, bichar_emb is set to be none. What is this embedding?"
None
"hello, why we should fixed the batch_size to 1? I have not read your code carefully, can you say something to me in advance?"
您好：        我在GitHub上下载的MSRA数据集是BIO格式的标签，您好像使用的是BMES格式的标签，请问能不能通过百度云或者其他方式分享一下呢
0. 我尝试复现论文中的weibo数据集overall的结果，但是test集的F1值仅达到了54，论文是58，没有达到论文的精度；  1. 我从这里    下载了weibo数据集，使用data/weiboNER_2nd_conll.*文件作为数据集，我使用BIO方式；  我想实现overall的效果，没有对数据进行修改，直接用了全部的数据。  2. 我的命令如下：     3. 这是相关的log输出：     @jiesutd 请问你一下，问题可能出在那里啊？多谢！
None
1.这种边界信息对分词应该也有帮助啊，有尝试过吗？  2.paper中lattice用到的分词，是现成的分词器，还是用的无监督分词来产生词表啊？    谢谢。
"Epoch: 0/100   Learning rate is setted as: 0.015  Traceback (most recent call last):    File ""main.py"", line 436, in        train(data, save_model_dir, seg)    File ""main.py"", line 281, in train      loss, tag_seq = model.neg_log_likelihood_loss(gaz_list, batch_word, batch_biword, batch_wordlen, batch_char, batch_charlen, batch_charrecover, batch_label, mask)    File ""/root/receiveData/LatticeLSTM/model/bilstmcrf.py"", line 32, in neg_log_likelihood_loss      scores, tag_seq = self.crf._viterbi_decode(outs, mask)    File ""/root/receiveData/LatticeLSTM/model/crf.py"", line 159, in _viterbi_decode      partition_history = torch.cat(partition_history,0).view(seq_len, batch_size,-1).transpose(1,0).contiguous() ## (batch_size, seq_len. tag_size)  RuntimeError: invalid argument 0: Tensors must have same number of dimensions: got 2 and 3 at /pytorch/torch/lib/THC/generic/THCTensorMath.cu:102  "
你好，读了你的论文，感觉很棒，刚入坑的小白一枚，有几个地方想请教一下。  1. 论文中提到没有使用分词，但word-embedding难道不是分词后训练得到的吗？如果没有用分词，那么word-embedding是怎么得到的呢？  2. 如果使用了分词，那么一种分词方法对于“南京市长江大桥”只能得到一种分词结果，为什么在论文的模型中，会出现“大桥”“长江大桥”这些输入到“桥”的cell中呢？  希望得到您的回复，非常感谢。
你好。在查阅ACL2018时，看见您的论文。按照您论文以及实验代码中的思想，我想确认几点问题：  1、完全摒弃了char特征（ps：代码中未看见通过lstm提取字特征），是不是没有结合char特征和Lattice提取出的特征，只是单单使用Lattice提取出的特征？  2、代码中的bi_word也就是代表了词信息，但是在LatticeLSTM中并未参与计算？这个是没有使用么？  3、仅仅使用Lattice网络在长期依赖的问题处理上，能不能保证和lstm能达到相同的效果？有没有一些论证呢？  
"Excuse me, I have some trouble training your model on MSRA dataset with a GTX 1080Ti card. I've found the speed of training is quite slow. So, may I know your solution to this problem? (Note: The video memory almost runs out, but there is still much unused computing power left.)"
"Have you tried setting data.HP_batch_size to a value greater than 1?   I set data.HP_batch_size  to 100, but the training results are not good. On the msra dataset, f1 is  stable around 0.74. I switched to the adam optimization method, the effect is not as good as the previous default setting, f1 is  about 0.91- 0.92."
"语句如下：  输入语句：在全国高等医药教材建设研究会和卫生部教材办公室的指导和组织下，在第6版的基础上，经过编委们的精心修改、编撰，完成了本教材的第7版。  通过使用训练出来的模型文件（xxx.model）,使用decode后：  输出：在全国高等医药教材建设研究会和卫生部教材办公室的指导和组织下，在第0版的基础上，经过编委们的精心修改、编撰，完成了本教材的第0版。  我可以保证的的是词嵌入和字典里面都是有数字vec的。  请问：为什么输出后数字变成了“0”？"
"Hi Jie, thank for your works on Chinese NER, I downloaded the code and embedding files, and run the demo, however the training spent a long time, the performance is not good, f-value is adout 0.4 after 50 epoch. But I didn't know where I did wrong.  Environment: python2.7, pytorch0.3.0, gpu1080  Thanks!"
"我们用decode去做序列标注时，发现得到的raw.out中缺失了大概10处字符，没有字符也没有标签。这十几处不连续，也不是以句子为单位的缺失，每处缺失的字符数不同，多的地方缺失近200个字符。我们用的saved_model是中间某一次new score对应的，因为目前训练还没有结束。问题：1、一定要用最高的score对应的save_model也就是最后一次保存的模型才可以得到正确结果吗？2、目前这种句子有缺失的情况，代码得出的指标p,r,f还是正确的吗？3、出现这种缺失的原因是？"
Hi！  我们在使用ResumeNER的数据时，发现标签列是“B-”，“M-”，“E-”和“O”，可是在LatticeLSTM/utils/metric.py中约76行以后的几行，给出的标签检索中没有M，所以问题是：1、“M-”对于BMES的标签方式是不是必须的？代码里是否是漏写？2、真实标签是“O”，代码中是“S-”，这是否影响最后结果？
Hi @jiesutd   我最近在用latticeLSTM训练医疗数据集来标注，有如下问题：  1）现在的模型batch size固定为1， 那么mask还起作用吗？  2） 我看到了main文件里调用的是bilstmcrf模型，然后bilstmcrf里面调用了bilstm和crf两个模型，bilstm里面调用了latticelstm，所以整个project是实现了latticeLSTM一个模型还是实现了包括bilstmcrf，latticelstm等各种模型？我用默认配置运行run_main.sh，得到的是latticelstm模型的结果？  3）我如果在医疗数据上运行latticeLSTM，利用目前提供的gigaword_chn.all.a2b.uni.ite50.vec以及ctb.50d.vec可行吗  
用自己已经标注过的语料做了训练，保存了模型到磁盘。在测试阶段，重新加载模型，然后执行后报错。  报错显示维度不匹配。  错误信息如下：     但是训练和测试的时候，网络结构并没有改动，怎么会出现维度失配？还是其他原因？
"你好，我尝试用训练好的模型对一些外部的文本进行命名实体识别。     我发现加载的文本不能是普通的文字序列，需要处理为  char lable 一行这样的格式。  而且句子之间需要有空行。（句子长度是否有限制？）        **然后我发现结果基本没有识别出我的句子中的实体**。   我用的训练数据集即为ResumeNER数据集，不知效果较差是否和这个训练数据集有关。      运行命令：        python main.py --status decode --raw ./data/bioes2.txt --savedset ./data/saved_model.dset --    loadmodel ./data/saved_models/saved_model.35.model --output ./data/res.out          **bioes2.txt为需要命名实体识别的文本内容**    **res.out 为命名实体识别的结果**    **bioes2.txt的内容如下：**  东 B-LOC  光 E-LOC  铁 E-LOC  佛 E-LOC  寺 E-LOC  位 O  于 O  沧 B-LOC  州 E-LOC  市 E-LOC  东 B-LOC  光 E-LOC  县 E-LOC  县 O  城 O  内 O  ， O  是 O  沧 B-LOC  州 E-LOC  最 O  著 O  名 O  的 O  佛 B-LOC  教 E-LOC  寺 E-LOC  院 O  ， O  已 O  有 O  千 O  年 O  历 O  史 O  ， O  在 O  沧 B-LOC  州 E-LOC  当 O  地 O  自 O  古 O  就 O  有 O  “ O  沧 B-LOC  州 E-LOC  狮 B-LOC  子 E-LOC  景 E-LOC  州 E-LOC  塔 E-LOC  ， O  东 B-LOC  光 E-LOC  县 E-LOC  的 O  铁 B-LOC  菩 E-LOC  萨 E-LOC  ” O  的 O  说 O  法 O  ， O  很 O  多 O  当 O  地 O  人 O  拜 O  佛 O  祈 O  福 O  都 O  会 O  选 O  择 O  这 O  里 O  。 O  铁 B-LOC  佛 E-LOC  寺 E-LOC  始 O  建 O  于 O  宋 O  代 O  ， O  后 O  曾 O  经 O  被 O  毁 O  ， O  寺 O  内 O  的 O  古 O  迹 O  和 O  古 O  铁 O  佛 O  早 O  已 O  不 O  存 O  ， O  如 O  今 O  的 O  铁 B-LOC  佛 E-LOC  寺 E-LOC  是 O  九 O  十 O  年 O  代 O  时 O  重 O  新 O  修 O  建 O  的 O  。 O  但 O  修 O  建 O  后 O  的 O  寺 O  院 O  庄 O  严 O  大 O  气 O  ， O  而 O  且 O  修 O  建 O  时 O  也 O  产 O  生 O  了 O  很 O  多 O  神 O  话 O  传 O  说 O  ， O  使 O  得 O  如 O  今 O  的 O  铁 B-LOC  佛 E-LOC  寺 E-LOC  依 O  然 O  香 O  火 O  旺 O  盛 O  。 O  在 O  铁 B-LOC  佛 E-LOC  寺 E-LOC  内 O  游 O  玩 O  时 O  ， O  可 O  以 O  着 O  重 O  观 O  看 O  寺 O  内 O  的 O  巨 O  大 O  铁 O  佛 O  ， O  铁 O  佛 O  高 O  约 O  8 O  米 O  多 O  ， O  非 O  常 O  壮 O  观 O  。 O  寺 O  内 O  另 O  有 O  多 O  座 O  佛 B-LOC  殿 E-LOC  ， O  都 O  可 O  以 O  一 O  一 O  参 O  观 O  。 O  另 O  外 O  还 O  有 O  京 B-LOC  剧 O  名 O  旦 O  荀 O  慧 O  生 O  的 O  纪 B-LOC  念 E-LOC  馆 E-LOC  ， O  可 O  以 O  进 O  入 O  了 O  解 O  一 O  下 O  。 O  在 O  寺 O  内 O  上 O  香 O  、 O  磕 O  头 O  时 O  ， O  一 O  般 O  会 O  被 O  要 O  求 O  给 O  一 O  点 O  香 O  火 O  钱 O  ， O  每 O  人 O  1 O  0 O  - O  2 O  0 O  元 O  左 O  右 O  即 O  可 O  。 O  "" B-LOC  沧 E-LOC  州 E-LOC  民 O  谣 O  ： O  “ O  一 O  文 O  一 O  武 O  ， O  一 O  国 O  宝 O  ， O  一 O  人 O  祖 O  。 O  ” O  文 O  者 O  ， O  是 O  一 O  代 O  文 O  宗 O  纪 O  晓 O  岚 O  ， O  武 O  者 O  ， O  是 O  沧 B-LOC  州 E-LOC  乃 O  驰 O  名 O  中 B-LOC  外 O  的 O  武 O  术 O  之 O  乡 O  ， O  国 O  宝 O  指 B-LOC  沧 E-LOC  州 E-LOC  铁 B-LOC  狮 E-LOC  ， O  人 O  祖 O  即 O  盘 O  古 O  ， O  盘 B-LOC  古 E-LOC  遗 E-LOC  址 E-LOC  就 O  在 O  今 B-LOC  沧 E-LOC  州 E-LOC  市 E-LOC  所 O  属 O  的 O  青 B-LOC  县 E-LOC  境 O  内 O  。 O  青 B-LOC  县 E-LOC  城 O  南 O  6 O  公 O  里 O  有 O  村 O  曰 O  “ O  大 O  盘 O  古 O  ” O  ， O  村 O  西 O  有 O  座 O  盘 O  古 O  庙 O  。 O    **res.out 的结果如下：**  东 O  光 O  铁 O  佛 O  寺 O  位 O  于 O  沧 O  州 O  市 O  东 O  光 O  县 O  县 O  城 O  内 O  ， O    是 O  沧 O  州 O  最 O  著 O  名 O  的 O  佛 B-ORG  教 M-ORG  寺 M-ORG  院 E-ORG  ， O    已 O  有 O  千 O  年 O  历 O  史 O  ， O    在 O  沧 O  州 O  当 O  地 O  自 O  古 O  就 O  有 O  “ O  沧 O  州 O  狮 O  子 O  景 O  州 O  塔 O  ， O    东 O  光 O  县 O  的 O  铁 O  菩 O  萨 O  ” O  的 O  说 O  法 O  ， O  很 O  多 O  当 O  地 O  人 O  拜 O  佛 O  祈 O  福 O  都 O  会 O  选 O  择 O  这 O  里 O  。 O    铁 O  佛 O  寺 O  始 O  建 O  于 O  宋 O  代 O  ， O    后 O  曾 O  经 O  被 O  毁 O  ， O    寺 O  内 O  的 O  古 O  迹 O  和 O  古 O  铁 O  佛 O  早 O  已 O  不 O  存 O  ， O    如 O  今 O  的 O  铁 O  佛 O  寺 O  是 O  九 O  十 O  年 O  代 O  时 O  重 O  新 O  修 O  建 O  的 O  。 O    但 O  修 O  建 O  后 O  的 O  寺 O  院 O  庄 O  严 O  大 O  气 O  ， O    而 O  且 O  修 O  建 O  时 O  也 O  产 O  生 O  了 O  很 O  多 O  神 O  话 O  传 O  说 O  ， O    使 O  得 O  如 O  今 O  的 O  铁 O  佛 O  寺 O  依 O  然 O  香 O  火 O  旺 O  盛 O  。 O    在 O  铁 O  佛 O  寺 O  内 O  游 O  玩 O  时 O  ， O    可 O  以 O  着 O  重 O  观 O  看 O  寺 O  内 O  的 O  巨 O  大 O  铁 O  佛 O  ， O    铁 O  佛 O  高 O  约 O  0 O  米 O  多 O  ， O  非 O  常 O  壮 O  观 O  。 O    寺 O  内 O  另 O  有 O  多 O  座 O  佛 O  殿 O  ， O  都 O  可 O  以 O  一 O  一 O  参 O  观 O  。 O    另 O  外 O  还 O  有 O  京 O  剧 O  名 O  旦 O  荀 O  慧 O  生 O  的 O  纪 O  念 O  馆 O  ， O    可 O  以 O  进 O  入 O  了 O  解 O  一 O  下 O  。 O    在 O  寺 O  内 O  上 O  香 O  、 O  磕 O  头 O  时 O  ， O    一 O  般 O  会 O  被 O  要 O  求 O  给 O  一 O  点 O  香 O  火 O  钱 O  ， O    每 O  人 O  0 O  0 O  - O  0 O  0 O  元 O  左 O  右 O  即 O  可 O  。 O    "" O  沧 O  州 O  民 O  谣 O  ： O  “ O  一 O  文 O  一 O  武 O  ， O  一 O  国 O  宝 O  ， O  一 O  人 O  祖 O  。 O  ” O  文 O  者 O  ， O  是 O  一 O  代 O  文 O  宗 O  纪 O  晓 O  岚 O  ， O  武 B-TITLE  者 E-TITLE  ， O  是 O  沧 O  州 O  乃 O  驰 O  名 O  中 O  外 O  的 O  武 O  术 O  之 O  乡 O  ， O    国 O  宝 O  指 O  沧 O  州 O  铁 O  狮 O  ， O    人 O  祖 O  即 O  盘 O  古 O  ， O  盘 O  古 O  遗 O  址 O  就 O  在 O  今 O  沧 O  州 O  市 O  所 O  属 O  的 O  青 O  县 O  境 O  内 O  。 O    青 O  县 O  城 O  南 O  0 O  公 O  里 O  有 O  村 O  曰 O  “ O  大 O  盘 O  古 O  ” O  ， O  村 O  西 O  有 O  座 O  盘 O  古 O  庙 O  。 O        "
"(1)We can feed three kinds of parameter:""train""，""test"" and ""decode"" to the main.py. In ""train"" step you have use ""dev"" set to choose best mode and save it. It seems that you use the ""test"" data to print the model'performance each iteration. Am I right? When status=""test"",you also use the ""dev"" data and ""test"" data to show the model'performance, but your have used  them during trainning stage. Is that OK？  (2)In the main.py you mention ""raw"" data when status argument is ""decode"".Where to get the ""raw"" data?"
"run the demon-run_demo.sh,it seems request some more data in ""onto4ner.cn"" directory.So where to get   demo.train.char, demo.dev.char and demo.test.char?"
"Hello, I am trying to reproduce your work on OntoNotes 4. Could you please provide some code or scripts for preprocessing that dataset? I mean, to split it into train/ dev/ test set, and to transform the original format in OntoNotes to CoNLL format (BMES).    I have downloaded OntoNotes 4 from LDC using my license, and tried to split that dataset according to the paper *Named Entity Recognition with Bilingual Constraints*, as mentioned in your ACL18 paper. However, some statistics are not consistent with the results shown in your paper. It will help a lot if you could provide the code for preprocessing. Thanks!  "
"> Traceback (most recent call last):    File ""/home/rui/workspace/lattice-lstm/LatticeLSTM-master/main.py"", line 459, in        train(data, save_model_dir, seg)    File ""/home/rui/workspace/lattice-lstm/LatticeLSTM-master/main.py"", line 286, in train      batch_charlen, batch_charrecover, batch_label, mask)    File ""/home/rui/workspace/lattice-lstm/LatticeLSTM-master/model/bilstmcrf.py"", line 32, in neg_log_likelihood_loss      scores, tag_seq = self.crf._viterbi_decode(outs, mask)    File ""/home/rui/workspace/lattice-lstm/LatticeLSTM-master/model/crf.py"", line 159, in _viterbi_decode      partition_history = torch.cat(partition_history,0).view(seq_len, batch_size,-1).transpose(1,0).contiguous() ## (batch_size, seq_len. tag_size)  RuntimeError: invalid argument 0: Tensors must have same number of dimensions: got 2 and 3 at /pytorch/torch/lib/THC/generic/THCTensorMath.cu:102      在对partition_history执行cat操作时，输入的tensor list维度不一致。    partition_history中，第一个tensor是 [batch_size, tag_size, 1]：       而在for中，torch.max返回的partition形状为 [batch_size,tag_size]，与第一个tensor维度不一致，导致cat操作失败       请问如何修改        "
"In my experiment,  char embeddings and word embeddings are gigaword_chn.all.a2b.uni.ite50.vec and  ctb.50d.vec respectively,  while  bichar_emb is set to None.  Other parameters take the default value in the code. Currently, 80 epoches on a nividia 1080 Ti GPU has been run, the test result on the msra test dataset did not reach the result in the paper, and the best result is acc: 0.9891, p: 0.9331, r: 0.9093, f: 0.9210.  Where did I do wrong?    In addition, if  char embeddings trained on  Chinese Wikipedia (bigger than gigaword, and the embeddings contain 16115 words, 100 dimensions) are used instead of gigaword_chn.all.a2b.uni.ite50.vec(11327 words, 50  dimensions),  the difference of test results between Bi-LSTM+CRF based on char + softword and LatticeLSTM (also using the same  char embeddings trained on  Chinese Wikipedia) is small. Is the big difference in the paper because of the use of a weaker char embedding?  "
"In the readme,  you mentioned that the pretrained character and word embeddings are the same with the embeddings in the baseline of RichWordSegmentor, i.e., character and word embeddings are gigaword_chn.all.a2b.uni.ite50.vec and ctb.50d.vec respectively. These seems not be mentioned in the paper.  Are the experimental  results of latticeLSTM in the paper obtained using these two embeddings？    In the paper,  you mentioned that the word embeddings is pretrained using word2vec (Mikolov et al., 2013) over automatically segmented Chinese Giga-Word.  Dose this word embedding is only used  in those baseline methods?"
"I try to train a model on msra data using a nvidia 1080 Ti,  and it takes about 120 seconds on 500 sentences. It is acceptable on small data set, but if the data set is larger, for instance, 5 times bigger than msra, the training time is too long.    Is there any way to speed up the training speed?"
"!   Hi, run_main.sh by using ResumeNER, but error, I  rarely use Pytorch .And I alone run torch.Tensor([1]*seqlen) is success. So I need your help!!!"
"Hi,   I see  It achieves 93.18% F1-value on MSRA dataset, which is the state-of-the-art result on Chinese NER task.  But, I try to find by google, I can't find this MSRA dataset. Please ask where I can find this dataset.    "
下面这两的向量维度不是50，一个是15，一个是 55， 是这个文件本来的问题，还是我下载过程中传输出错了（没有百度网盘会员，下的真慢）？    森悄 -0.420138 -0.189634 0.346326 -0.235297 -0.389551 -0.588 1.164976 -0.610863 0.073047 0.531165 -3.343037 -0.666090 2.384061 0.129748 -1.972636    系v 0.108717 -0.042028 -2.452340 -0.387857 1.953125 0.230040 2.203831 3.083842 0.400699 -0.449208 1.321026 -2.430978 1.369693 0.100625 -1.246027 -0.846308 -2.649471 0.168484 0.593922 -0.481574 0.546810 -2.844704 -0.956998 -2.017416 1.072134 -1.407300 -0.145390 -0.086188 -0.896394 2.064528 1.660699 0.500353 0.773185 -2.036687 3.072354 0.667415 -0.520374 -1.668948 0.729110 0.385540 -0.868025 0.600913 1.883432 3.111219 -1.039192 1.274076 1.103154 3.524141 -0.77819 -2.084318 -1.281501 -2.526086 -2.124930 -0.793325 -0.496073  
"Hi, I noticed that this repo does not contain the evaluation script, which makes it hard for tuning the model on dev set. Can you release the official evaluation script? Thank you very much."
Hi!    When will evaluation be open?    Thanks! 
"Hello,  first of all, thank you for this nice tool!  As stated in the Important notes, the tagger of Owoputi et al. performs better than the shipped tagger.   I was wondering now what is the proper way to combine the twpipe with the tagger of Owoputi?  "
"Hi,    I've tried installing but keep getting stuck on how to actually run the parser. I've done the following:         But after running all this, I can't find `bin/twpipe` (the `bin` directory is empty). Did I miss a step in the installation?"
I got these errors when updating submodule.    
"  when I run ‘make’ after 'cmake .. -DEIGEN3_INCLUDE_DIR=/home/czy/workplace/twpipe2/eigen', there are some failure:  `  Scanning dependencies of target twpipe_tokenizer       there maybe some error in twpipe_tokenizer， and how to solve the problem?    thank you for your help."
"Hi, during training:     I see the following output metrics for the dev set:     (these are the metrics at the last save point)  However, when I later try standalone inference on the dev set with the same model, I get consistent metrics for the POS tagger but a lower LAS for the parser:     Thanks for any help."
"Hi, apologies if this is already documented somewhere.  I'm trying to include the training set `en_ewt-ud-train.conllu` from   but run into an error:     due to the following line in that training set     It looks like it's due to the underscore in the HEAD field.  Is the pipeline expected to be compatible with null nodes / enhanced dependencies?  Thanks.  "
Would it be possible to add a license? Thanks!
"Hi, I'm trying to re-train your system over the  .  Is there a set of instructions for the right way to do so?  Can the datasets be fed in directly? I was able to build the code, but am getting an illegal conllu error:     It looks like it's complaining about the commented lines `# tweet_id = feb_jul_16.1463316480` in the dataset.  I suspect I'm missing other inputs (e.g. embedding files) at this point as well.    Thanks for any help.  (Alternatively, if you have a pre-trained model that could be shared, that would be great as well.)  "
"deer  how to solve it？in the rl.py  autograd.backward(           [critic_loss] + losses,           [torch.ones(1).to(critic_loss.device)]*(1+len(losses)),      )  raise that problem"
"Hello Chen,     Thanks for providing this code. It seems really helpful for my current research.     However, I am having issues with making this code work. I have setup the environment as suggested but still able to load the pre-trained models (both extractive and abstractive models).   1. Extractive Model gives error in line `assert ext_meta['net'] == 'ml_rnn_extractor'`. Looking at the `meta.json` file, `net:rnn-ext_abs_rl`. I am not sure why is this discrepancy.   2. For abstractive model, I face `RuntimeError: CUDNN_STATUS_EXECUTION_FAILED` error in line `self._net = abstractor.to(self._device)`.  I am not sure how to solve this error. I made sure that CUDA is available. Also, it is not the OOM memory as suggested in some of the pages since the GPU memory never exceeds 1 GB.     It would be great if you could help me out.     Thanks,   Naman"
"Hi developer,    Based on your model, the abstractor should be able to generate summaries, then together with ground truth summaries, the reward is calculated and fed to extractor.    So how could we get these generated summaries in each train step?    I do appreciate!"
"Hi @ChenRocks , can you please check this issue?  I tried to train your model with my own dataset in Vietnamese. It's OK at step 1 (train_word2vec.py) and step 2 (make_extraction_labels.py)  But when I start training the abstractor, your model return this error  !     I tried to reproduce that the same step with CNN/DailyMail dataset and the result is the same, it still stuck at step 3 with the same error message as above.  !     Can you give me some suggestion to overcome that error?   "
"I am trying to run it using windows anaconda environment.  i have install perl.it is running.  Getting these error.  Can anyone please help me.  <img width=""960"" alt=""error"" src=""   "
"I have been banging my head against a wall to get this working with my own dataset. I have successfully trained the word2vec model and the extraction labels but run into the too many values to unpack issue.     !     The contents of ""sample"" are an array of sentences from randomly concatenated 'highlights' which I assume is the 'target' portion but have not been able to confirm.      **possible related issues:**    - **made change in first line of main from**  `with open(join(DATA_DIR,'vocab_cnt.pkl'), 'rb') as f` to `join(DATA_DIR,'train', 'vocab_cnt.pkl')` because after training there is no single vocab only one in each 'test' 'train' and 'val' folders (**I think this is the main issue here, what should the data directory be??**)  - ""Warning: METEOR is not configured"" while making extraction labels  - word2vec model trained in <4 seconds on 800,000 words  - differing versions (below)      **python 3.6.10  pytorch 1.5.0  cytoolz 0.10.1**      "
None
"In   you split articles and abstracts into sentences to compute the labels for extraction. However `t.split()` splits into words, and not sentences. Is this a bug? If not, how does it work?"
"In scatter_add function ""source"" parameter is not working , ""src"" should be used instead of ""source""."
BUCKET_SIZE means batch_size?
"`for action, p, r, b in zip(indices, probs, reward, baseline):          advantage = r - b          avg_advantage += advantage          losses.append(-p.log_prob(action)                        * (advantage/len(indices))) # divide by T*B`    I have a question about this piece of code.  If I didn't get it wrong, the variable b here is tensor with gradient enabled, so optimizing tensors in losses will actually both optimize reward by changing policy weights and minimizing the advantage by maximizing baseline. I can't understand why the baseline is optimized here, because as far as I know, the baseline should only be optimized during the training of the critic.   Actually I used this training function in a different summarization task, and I found that the avg_advantage is always dropping.   Thank you very much."
 . There is an ERROR when the target summary has one sentence only.
"when I modify the number of layers in LSTM and train the train_full_rl.py something wrong;  Start training  Traceback (most recent call last):    File ""train_full_rl.py"", line 231, in        train(args)    File ""train_full_rl.py"", line 186, in train      trainer.train()    File ""/home/zhangxiaoyi/pyworkspace/bytecup8/fast_abs_rl/training.py"", line 211, in train      log_dict = self._pipeline.train_step()    File ""/home/zhangxiaoyi/pyworkspace/bytecup8/fast_abs_rl/rl.py"", line 193, in train_step      self._stop_reward_fn, self._stop_coeff    File ""/home/zhangxiaoyi/pyworkspace/bytecup8/fast_abs_rl/rl.py"", line 60, in a2c_train_step      (inds, ms), bs = agent(raw_arts)    File ""/home/zhangxiaoyi/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__      result = self.forward(*input, **kwargs)    File ""/home/zhangxiaoyi/pyworkspace/bytecup8/fast_abs_rl/model/rl.py"", line 221, in forward      outputs = self._ext(enc_art)    File ""/home/zhangxiaoyi/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__      result = self.forward(*input, **kwargs)    File ""/home/zhangxiaoyi/pyworkspace/bytecup8/fast_abs_rl/model/rl.py"", line 130, in forward      self._hop_v, self._hop_wq)    File ""/home/zhangxiaoyi/pyworkspace/bytecup8/fast_abs_rl/model/rl.py"", line 74, in attention      PtrExtractorRL.attention_score(attention, query, v, w), dim=-1)    File ""/home/zhangxiaoyi/pyworkspace/bytecup8/fast_abs_rl/model/rl.py"", line 66, in attention_score      sum_ = attention + torch.mm(query, w)  RuntimeError: The size of tensor a (13) must match the size of tensor b (3) at non-singleton dimension 0"
"hello  dear Chen! i would like to know if i have interrupted my training, can i restore my model from the previous  model  checkpoint?"
"hi, i think there is a bug in model/rl.py class PtrScorer.forward function,  where the lstm states (h, c) never update in for loop.    "
"Hi,    I'm unable to launch it without CUDA. I should be able to use my CPU only...     "
None
"when i run  train_extractor_ml.py I encountered this problem , as the error mentioned , it seems that prepro_fn_extract function need one more argument ""batch"" . But there is no such argument pass through .  "
"I was wondering the exact reason why you chose A2C for RL training.    For example, why not using self-critical policy gradient as described in   (Section 3.2). From my inexperienced point of view, it seems better as you don't have to bother training a Critic.    ---    **So why choosing A2C specifically ?**    * Did you compare this algorithm with other alternatives ? If yes what are the reason to choose A2C ?  * Or did you simply tried it and since it worked and is widely used you decided to use it too ?"
"Hi! I am trying to summarize some text files (not from the cnn daily mail dataset) using your decode_full_model.py. When the number of files is not very big, i.e. around 1000, it was working perfectly fine. However, because I have in total around 1 million files to summarize, the decoding process got stuck at the 1320th file. I tried to restart the decode multiple times and each time it was stuck at the same file. You can see the outputs from the screenshot below.  !     I am wondering what could cause it. I did not modify the code but instead pre process my files so that they have the same structures as the one suggested on the website. "
"Hello,thank you for sharing!      when I use dilation convolution to extract features,The following problem has occurred:    RuntimeError: Expected tensor for argument #1 'input' to have the same dimension as tensor for 'result'; but 4 does not equal 3 (while checking arguments for cudnn_convolution)      and I print the emb_input.size in the /model/extract.py line 27, I found the torch is ([60, 6, 128]) when the problem occured.I guess because the value of the second dimension is too small, it leads to an error. In other normal cases, the value of the second dimension is not so small.      can you tell me the meaning of the value of the second dimension?If it is not for this reason, do you know why?      thank you !    "
"In your paper you mentioned an interesting statistic, the abstractiveness of the model. You compute an abstractiveness score (See et al.,2017) as the ratio of novel n-grams in the generated summary that are not present in the input document.    Can you share the code that is used to re-produce this experiment?    Thanks in advance,    Nick"
"Hi Chen,    I have tried to follow your instructions exactly, but the following happens when I run the pretrained model:     The command I used was:    `python3 decode_full_model.py --path=/home/arnav-gulati/Documents/other_models/fast_abs_rl-master --model_dir=/home/arnav-gulati/Documents/other_models/fast_abs_rl-master/pretrained/acl --beam=5 --test  `    and I followed your instructions on exporting the data path with:  `export DATA=/home/arnav-gulati/Documents/cnn-dailymail-master/finished_files/  `    I am not sure what causes the following line:   `TypeError: scatter_add() missing 1 required positional arguments: ""src""`    Do you have any advice on what I should do? I believe I have installed all the dependencies, but just in case you want to see them, here they are:    `boto            2.49.0     boto3           1.9.171    botocore        1.12.171   bz2file         0.98       certifi         2019.6.16  chardet         3.0.4      cytoolz         0.9.0.1    docutils        0.14       futures         3.2.0      gensim          3.7.3      idna            2.8        jmespath        0.9.4      numpy           1.16.4     pip             19.1.1     protobuf        3.8.0      pyrouge         0.1.3      python-dateutil 2.8.0      requests        2.22.0     s3transfer      0.2.1      scipy           1.2.2      setuptools      41.0.1     six             1.12.0     smart-open      1.8.4      tensorboardX    1.7        toolz           0.9.0      torch           0.4.0      urllib3         1.25.3     wheel           0.33.4 `      Any help would be appreciated!"
"Hey Chen,    I have a question about the DataLoader in the files where the extractor, abstractor and RL are trained, more specifically, about this line of code:      val_loader = DataLoader(          MatchDataset('val'), batch_size=BUCKET_SIZE,          shuffle=False, num_workers=4 if cuda and not debug else 0,          collate_fn=coll_fn      )    I was wondering why pin_memory is not set to TRUE? I have read that this can improve performance when training on Nvidia GPU's. Also, would you advise to set num_workers higher when using a high-end GPU (Tesla V100 16GB) and 8 cores CPU?    Thanks!    - Nick"
"Hey,  Thanks for this great work, I just want to know if it is easily possible to force the model to predict exactly n abstract lines, so all tested articles will have the same number of summary lines."
"Since the preprocessing assumes that the model is going to be run on CNN or Daily Mail. Can this be run on other datasets with minimal changes?    Specifically, is it possible to use pre-trained models on other datasets with minimal changes to the code, or would there have to be too many changes?"
"When I limit the number of sentences in the summary to 1, for example, the `forward` function in `PtrExtractorRL` in `/model/rl.py` is called.   The for loop `for _ in range(n_step):` is run once (`n_step=1`) and the output is returned.   Yet in `decode_full_model.py` the last element of the extracted output is removed as EOE:   ` ext = extractor(raw_art_sents)[:-1] # exclude EOE`.    So in this case the only sentence being extracted is discarded as EOE.     One solution I can think of is to append an EOE in the `forward` function of `PtrExtractorRL`. Specifically, insert the following lines just before the return call:   "
"Hey Chen,    Recently, a very interesting model called BERT (  came out and has shown to achieve novel results on several NLP tasks. Do you think the results of your model can be improved by using their embedding model instead of word2vec?    I think it is an interesting idea, but I think I would need to make many many changes to the current architecture of your model.    Kind regards,    Nick"
"Hi, I tried to reproduce the results of rnn-ext + RL (without abstractor).  First, I pre-train an extractor using train_extractor_ml and construct the proxy training labels using ROUGE-L F1 score. Then, I run the train_full_rl.py and I do not specify the path of abstractor. After I finish the training, the results are significantly lower than the values reported in your paper. I just wonder if it is incorrect to use ROUGE-L F1 score to construct the proxy labels for rnn-ext, or you used other reward functions for it? Thank you so much for your help!"
How can i use this code to train and inference my model in distributed environment of spark cluster which has multiple cpu nodes.
"I notice two unusual places of codes, and was puzzled by that.    (1) in `class StackedLSTMCells` in rnn.py  line 102   @property     def bidirectional(self):          return self._cells[0].bidirectional    #  ==> LSTMCell has no bidirectional?    I suppose `self._cells` stores `nn.LSTMCell`, which has no `bidirectional` attribute?  Do I understand this correctly?    (2) In `class _CopyLinear` in copy_summ.py    inside `__init__` function, I notice a line:  `self.regiser_module(None, '_b')`.  Do you mean `register_module`?    If I set `bias=False`, I will get an error either way.  Could you comment on this?    Thank you!"
"I read your paper again, and couldn't find the reason why you are using the recall of ROUGE-L score to make the pseudo-labels. Specifically :    **Why ROUGE-L ? (and not ROUGE-1 for example)**    **Why precision ?**"
"Hi,    Thanks for sharing the excellent work and high-quality code!  I notice in the attention mechanism for `LSTMPointerNet`, you didn't use `self._attn_wm` in `attention_score` function.  I guess the result is already good without it.  Did you decide to drop `self._attn_wm` because it does not help much?  Thanks for the comments."
"Hi Chen,      I read your paper, which is very interesting.  I have several questions; could you please help me figure them out ?    1. The extractor extracts salient sentences (d_1,d_2,...d_n) from the original documents. For each salient sentence, the abstractor function (g) generate a summary sentence; all the summary sentences are concatenated to form the entire summary.      I am wondering if my understanding is correct.     2. In the paragraph above section 2.1, I am wondering if this is a typo?  <img width=""397"" alt=""屏幕快照 2019-04-03 下午4 58 06"" src=""   S_i should be set of summary sentences in y_i while D_i is the set of document sentences in x_i        Thanks"
"I understand that EOE token allow the agent to stop in RL training.    But why not training also the extractor (only) to extract EOE token ?     ---    I'm trying to add another token into your architecture, and follow your design by adding it only at RL.    However results are not satisfying. I'm wondering if I should train the extractor too, with this supplementary token.    What do you think ?"
RuntimeError: CUDNN_STATUS_EXECUTION_FAILED  !   ubuntu 16.0.4  pytorch 0.4.0  python 3.6.5  cuda 9.0  ##########  what should I do..
"I would like to apply this great code to other datasets, but I met several obstacles :    1. The article is not sentence-separated  2. The summary is a single sentence, representing several sentences of the article    ---    To overcome these obstacles, I tried following approach :    1. Use a sentence tokenizer (SpaCy) to process the dataset before training.   2. As mentioned by #4, extract K sentences and concatenate them into a single sentence to the abstractor.    ---    But it's not a good approach.     First of all, the sentence tokenizer is not perfect and tokenized sentences might be messed up.    About the dataset with a single sentence, most of the time this sentence is very abstractive. Therefore, creating the pseudo-labels is difficult. I believe in this case the ROUGE-L matching might not give the best corresponding sentence.  And if the pseudo-labels are wrong...    ---    So my question is : **do you have any lead of general idea that I can implement to improve the existing code for other datasets ?**"
"Hey,  Thank you very much for this great work,  I am having a problem training extractor model on my own data,  I am using pytorch 0.4.0 with Cuda 10.0 on Ubuntu 18.04  I hope you can help me with this issue.  Thank you very much in advanced.    this is the error traceback:    Traceback (most recent call last):    File ""/home/me/fast_abs_rl-master/model/extract.py"", line 33, in forward      for conv in self._convs], dim=1)    File ""/home/me/fast_abs_rl-master/model/extract.py"", line 33, in        for conv in self._convs], dim=1)    File ""/home/me/.local/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__      result = self.forward(*input, **kwargs)    File ""/home/me/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py"", line 176, in forward      self.padding, self.dilation, self.groups)  RuntimeError: sizes must be non-negative    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""train_extractor_ml.py"", line 237, in        main(args)    File ""train_extractor_ml.py"", line 179, in main      trainer.train()    File ""/home/me/fast_abs_rl-master/training.py"", line 212, in train      log_dict = self._pipeline.train_step()    File ""/home/me/fast_abs_rl-master/training.py"", line 97, in train_step      net_out = self._net(*fw_args)    File ""/home/me/site-packages/torch/nn/modules/module.py"", line 477, in __call__      result = self.forward(*input, **kwargs)    File ""/home/me/fast_abs_rl-master/model/extract.py"", line 287, in forward      enc_out = self._encode(article_sents, sent_nums)    File ""/home/me/fast_abs_rl-master/model/extract.py"", line 309, in _encode      for art_sent in article_sents]    File ""/home/me/fast_abs_rl-master/model/extract.py"", line 309, in        for art_sent in article_sents]    File ""/home/me/.local/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__      result = self.forward(*input, **kwargs)    File ""/home/me/fast_abs_rl-master/model/extract.py"", line 40, in forward      for conv in self._convs])    File ""/home/me/fast_abs_rl-master/model/extract.py"", line 40, in        for conv in self._convs])    File ""/home/me/.local/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__      result = self.forward(*input, **kwargs)    File ""/home/me/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py"", line 176, in forward      self.padding, self.dilation, self.groups)  RuntimeError: sizes must be non-negative"
"From the results of your paper :     !     For both abstractive and extractive approach, it seems that :    **`ff-ext` > `rnn-ext`**    ---    Why not comparing `ff-ext + RL` and `ff-ext + RL + rerank` with other approaches as well ? "
"Hello, I can train the model, I just get a label of extractor, but I can't find how to get the first phrase extrator summay .Could you tell me how to fix it?"
"Hi,    With your repo, I want to create own model to do summarization article to the title in my data.  How u think, when I have done this with your repo, the result had been similar?    Plus, how I should prepare my data? "
"Hi,  I go to step train abstractor and extractor using ML objectives. I want to know the location of path to abstractor and extractor to run."
"Hi,  I noticed that on evaluation part of abstractor, you used ground-truth target to make prediction (not token of argmax prediction of the previous step). Even when we don't let the gradient backpropagate through this dev set, we should act like we don't really see this dataset, do we ?  I observed that the dev score is very close to training score by this bias (though training & dev are not that identical ?). This also leads to a quicker drop in learning rate & perhaps to a suboptimal ?    Thanks,  Hoa"
"Hi, is it any way to make easy inferences from texts? say I have an article and I want to see what is the summary for the model, without any evaluation.   Thanks for your work!"
"Hey,  Thank you very much for this great work.  i am having this error when I train my abstractor model:  AttributeError: 'float' object has no attribute 'item'.  any ideas why ?  Thanks in advanced "
     I am trying to implement the model on chinese dataset. I succeeded to train the abstractive model. But when i try to train the extractive model. It just throw out this error. I have no idea what's going on. Does any one have any idea?   
"**Where should I create val/test folder ?**     I am trying to evaluate from pretrained model. As per the instructions after downloaing and detokenising the dataset I get,    - Prretrained model folder  - */stories folder  - *_stories_tokenized folder      ishandutta2007@MacBook-Pro:~/Documents/Projects/fast_abs_rl$ `export DATA=../cnn-dailymail/cnn/stories`    ishandutta2007@MacBook-Pro:~/Documents/Projects/fast_abs_rl$ `python decode_full_model.py --path=../cnn-dailymail/cnn_stories_tokenized  --model_dir=./pretrained/acl  --beam=5 --val`    > loading checkpoint ckpt-0-0...  > loading checkpoint ckpt-42.508549-0...  > Traceback (most recent call last):  >   File ""decode_full_model.py"", line 169, in    >     args.max_dec_word, args.cuda)  >   File ""decode_full_model.py"", line 49, in decode  >     dataset = DecodeDataset(split)  >   File ""/Users/ishandutta2007/Documents/Projects/fast_abs_rl/decoding.py"", line 30, in __init__  >     super().__init__(split, DATASET_DIR)  >   File ""/Users/ishandutta2007/Documents/Projects/fast_abs_rl/data/data.py"", line 14, in __init__  >     self._n_data = _count_data(self._data_path)  >   File ""/Users/ishandutta2007/Documents/Projects/fast_abs_rl/data/data.py"", line 29, in _count_data  >     names = os.listdir(path)  > FileNotFoundError: [Errno 2] No such file or directory: '../cnn-dailymail/cnn/stories/val'"
"Hi,  when I run the train_abstractor.py, I got the following error:    > Traceback (most recent call last):    File ""train_abstractor.py"", line 220, in        main(args)    File ""train_abstractor.py"", line 166, in main      trainer.train()    File ""/data/comp/fast_abs_rl/training.py"", line 211, in train      log_dict = self._pipeline.train_step()    File ""/data/comp/fast_abs_rl/training.py"", line 107, in train_step      log_dict.update(self._grad_fn())    File ""/data/comp/fast_abs_rl/training.py"", line 20, in f      grad_norm = grad_norm.item()  AttributeError: 'float' object has no attribute 'item'    How to solve this problem? Thanks!"
"Hi,    I am new to pytorch, and was trying to use your model for summarizing some text. I have been trying to run training with news data, and then added only a single new file, in both cases, train_full_rl fails in rl.py file with below error, I tried to switch to GPU and still get same problem, I was wondering if you have seen this error before. When I was using GPU, it would show error that input and target size does not match, that is why I am printing out sizes (torch.Size([700])):      {'net_args': {'extractor': {'net_args': {'conv_hidden': 100, 'emb_dim': 128, 'bi                                                                                                                                                             directional': True, 'vocab_size': 30004, 'lstm_layer': 1, 'lstm_hidden': 256}, '                                                                                                                                                             traing_params': {'batch_size': 32, 'lr_decay': 0.5, 'optimizer': ['adam', {'lr':                                                                                                                                                              0.001}], 'clip_grad_norm': 2}, 'net': 'ml_rnn_extractor'}, 'abstractor': {'net_                                                                                                                                                             args': {'emb_dim': 128, 'bidirectional': True, 'vocab_size': 30004, 'n_hidden':                                                                                                                                                              256, 'n_layer': 1}, 'traing_params': {'batch_size': 32, 'lr_decay': 0.5, 'optimi                                                                                                                                                             zer': ['adam', {'lr': 0.001}], 'clip_grad_norm': 2.0}, 'net': 'base_abstractor'}                                                                                                                                                             }, 'train_params': {'gamma': 0.95, 'lr_decay': 0.5, 'stop_reward': 'rouge-1', 'c                                                                                                                                                             lip_grad_norm': 2, 'batch_size': 32, 'reward': 'rouge-l', 'optimizer': ('adam',                                                                                                                                                              {'lr': 0.0001}), 'stop_coeff': 1.0}, 'net': 'rnn-ext_abs_rl'}  Start training  torch.Size([700])  torch.Size([700])  Traceback (most recent call last):    File ""Geneva_ABS.py"", line 266, in        main(args)    File ""Geneva_ABS.py"", line 224, in main      train_full_rl().train(path=output_path+'/model/', abs_dir=output_path+'/abst                                                                                                                                                             ractor/', ext_dir = output_path+'/extractor/')    File ""/home/train_full_rl.py"", line 195                                                                                                                                                             , in train      trainer.train()    File ""/home/training.py"", line 211, in                                                                                                                                                              train      log_dict = self._pipeline.train_step()    File ""/home/rl.py"", line 177, in train_                                                                                                                                                             step      self._stop_reward_fn, self._stop_coeff    File ""/home/rl.py"", line 108, in a2c_tr                                                                                                                                                             ain_step      [torch.ones(1).to(critic_loss.device)]*(1+len(losses))    File ""/home/venv/lib/python3.5/site-packages/torch/autograd/__in                                                                                                                                                             it__.py"", line 90, in backward      allow_unreachable=True)  # allow_unreachable flag  RuntimeError: invalid gradient at index 0 - expected shape [] but got [1]  "
"   Hi, I found in batcher.py there exist one line that remove the last id of the target, but I am very puzzle why do this, can you kindly explain it for me?"
Thanks for the great work!   I'm just wondering if we can utilize the results form the extractor independently.   
"Traceback (most recent call last):    File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\multiprocessing\context.py"", line 190, in get_context      ctx = _concrete_contexts[method]  KeyError: 'forkserver'    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""D:/lab/2018match/fast_abs_rl-master/fast_abs_rl-master/train_extractor_ml.py"", line 237, in        main(args)    File ""D:/lab/2018match/fast_abs_rl-master/fast_abs_rl-master/train_extractor_ml.py"", line 131, in main      args.cuda, args.debug)    File ""D:/lab/2018match/fast_abs_rl-master/fast_abs_rl-master/train_extractor_ml.py"", line 71, in build_batchers      single_run=False, fork=not debug)    File ""D:\lab\2018match\fast_abs_rl-master\fast_abs_rl-master\data\batcher.py"", line 216, in __init__      ctx = mp.get_context('forkserver')    File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\multiprocessing\context.py"", line 238, in get_context      return super().get_context(method)    File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\multiprocessing\context.py"", line 192, in get_context      raise ValueError('cannot find context for %r' % method)  ValueError: cannot find context for 'forkserver'  "
"`WARNING: Exploding Gradients 8048.54  WARNING: Exploding Gradients 219.81  WARNING: Exploding Gradients 360.95  WARNING: Exploding Gradients 233.41  Traceback (most recent call last):    File ""train_full_rl.py"", line 228, in        train(args)    File ""train_full_rl.py"", line 182, in train      trainer.train()    File ""/data/xiaqiang/nmt/fast_abs_rl/training.py"", line 211, in train      log_dict = self._pipeline.train_step()    File ""/data/xiaqiang/nmt/fast_abs_rl/rl.py"", line 174, in train_step      self._stop_reward_fn, self._stop_coeff    File ""/data/xiaqiang/nmt/fast_abs_rl/rl.py"", line 101, in a2c_train_step      critic_loss = F.mse_loss(baseline, reward)    File ""/data/xiaqiang/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py"", line 1716, in mse_loss      return _pointwise_loss(lambda a, b: (a - b) ** 2, torch._C._nn.mse_loss, input, target, reduction)    File ""/data/xiaqiang/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py"", line 1674, in _pointwise_loss      return lambd_optimized(input, target, reduction)  RuntimeError: input and target shapes do not match: input [204], target [200] at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/THCUNN/generic/MSECriterion.cu:12  `  "
"Hi,  Thanks for the great work. Could you share the training steps that you used for Extractor/Abstractor ML training and the joint model RL training? Basically when did you stop training each of these models?"
"   Excuse me, I found that you only pretrained a word2vec word embedding on the train set，not included val set. Have any deep meaning? I am confused about this.Thanks！"
None
"Line156 of train_abstractor.py:  if args.cuda:          net = net.cuda()  pipeline = BasicPipeline(meta['net'], net,                     train_batcher, val_batcher, args.batch, val_fn,                     criterion, optimizer, grad_fn)    Since there is no cuda, net is not-defined. Therefore, the train_step function of BasicPipeline in training.py will produce a Segmentation fault.    "
"model/util.py has two following lines that requires GPU.   LineNum 59: order = torch.LongTensor(order).to(sequence_emb.get_device())  LineNum 73: order = torch.LongTensor(order).to(lstm_states[0].get_device())    However, my server has no GPU. When I train my own model using   python train_abstractor.py --no-cuda --path=mypath/abstractor --w2v=mypath/word2vec.128d.226k.bin  I will get the error:   fast_abs_rl/model/util.py"", line 73, in reorder_lstm_states      order = torch.LongTensor(order).to(lstm_states[0].get_device())  RuntimeError: _th_get_device is not implemented for type torch.FloatTensor    I just check the doc of pytorch. It says that only GPU tensor has the method get_device().   ""For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides. For CPU tensors, an error is thrown."" as ""RuntimeError: get_device is not implemented for type torch.FloatTensor""     Does the code must need a GPU? Thank you."
   I assume this is  intended:       if mode == 'p':              &nbsp;&nbsp; &nbsp;&nbsp;score = precision          **elif** mode == 'r':              &nbsp;&nbsp; &nbsp;&nbsp; score = recall          else:              &nbsp;&nbsp;  &nbsp;&nbsp;score = f_score
"(train the full RL model)  I am trying to run python train_full_rl.py --path=[path/to/save/model] --abs_dir=[path/to/abstractor/model] --ext_dir=[path/to/extractor/model]  My GPU has 12GB memory.  NVIDIA-SMI 387.26 , Driver Version: 387.26    0  Tesla K40m .   Could you comment on how much GPU RAM does training this needs in the README?  Thank you!   I met such error:    loading checkpoint ckpt-1.482692-6000...  loading checkpoint ckpt-3.515528-3000...  loading checkpoint ckpt-1.482692-6000...  start training with the following hyper-parameters:  {'net': 'rnn-ext_abs_rl', 'net_args': {'abstractor': {'net': 'base_abstractor', 'net_args': {'vocab_size': 30004, 'emb_dim': 128, 'n_hidden': 256, 'bidirectional': True, 'n_layer': 1}, 'traing_params': {'optimizer': ['adam', {'lr': 0.001}], 'clip_grad_norm': 2.0, 'batch_size': 32, 'lr_decay': 0.5}}, 'extractor': {'net': 'ml_rnn_extractor', 'net_args': {'vocab_size': 30004, 'emb_dim': 128, 'conv_hidden': 100, 'lstm_hidden': 256, 'lstm_layer': 1, 'bidirectional': True}, 'traing_params': {'optimizer': ['adam', {'lr': 0.001}], 'clip_grad_norm': 2.0, 'batch_size': 32, 'lr_decay': 0.5}}}, 'train_params': {'optimizer': ('adam', {'lr': 0.0001}), 'clip_grad_norm': 2.0, 'batch_size': 32, 'lr_decay': 0.5, 'gamma': 0.95, 'reward': 'rouge-l', 'stop_coeff': 1.0, 'stop_reward': 'rouge-1'}}  Start training  WARNING: Exploding Gradients 1829948160.00  WARNING: Exploding Gradients 110035.23  THCudaCheck FAIL file=/pytorch/aten/src/THC/generic/THCStorage.cu line=58 error=2 : out of memory  Traceback (most recent call last):    File ""train_full_rl.py"", line 228, in        train(args)    File ""train_full_rl.py"", line 182, in train      trainer.train()    File ""/home/project/fast_abs_rl/fast_abs_rl/training.py"", line 211, in train      log_dict = self._pipeline.train_step()    File ""/home/project/fast_abs_rl/fast_abs_rl/rl.py"", line 173, in train_step      self._stop_reward_fn, self._stop_coeff    File ""/home/project/fast_abs_rl/fast_abs_rl/rl.py"", line 64, in a2c_train_step      summaries = abstractor(ext_sents)    File ""/home/project/fast_abs_rl/fast_abs_rl/decoding.py"", line 94, in __call__      decs, attns = self._net.batch_decode(*dec_args)    File ""/home/project/fast_abs_rl/fast_abs_rl/model/copy_summ.py"", line 63, in batch_decode      attention, init_dec_states = self.encode(article, art_lens)    File ""/home/project/fast_abs_rl/fast_abs_rl/model/summ.py"", line 81, in encode      init_enc_states, self._embedding    File ""/home/project/fast_abs_rl/fast_abs_rl/model/rnn.py"", line 41, in lstm_encoder      lstm_out = reorder_sequence(lstm_out, reorder_ind, lstm.batch_first)    File ""/home/project/fast_abs_rl/fast_abs_rl/model/util.py"", line 58, in reorder_sequence      sorted_ = sequence_emb.index_select(index=order, dim=batch_dim)  RuntimeError: cuda runtime error (2) : out of memory at /pytorch/aten/src/THC/generic/THCStorage.cu:58      "
"Hi,   I followed the institution to install and set pyrouge and ROUGE1.5.5,  when I evaluate the full model by executing the ""eval_full_model.py"" script, the --meteor flag works well, but the --rouge flag will output errors like:    Cannot open exception db file for reading: /home/zqj/Workspace/Tools/pyrouge-master/tools/ROUGE-1.5.5/data/WordNet-2.0.exc.db  Traceback (most recent call last):    File ""eval_full_model.py"", line 53, in        main(args)    File ""eval_full_model.py"", line 26, in main      output = eval_rouge(dec_pattern, dec_dir, ref_pattern, ref_dir)    File ""/home/zqj/Workspace/fast_abs_rl/evaluate.py"", line 40, in eval_rouge      output = sp.check_output(cmd.split(' '), universal_newlines=True)    File ""/usr/lib/python3.5/subprocess.py"", line 626, in check_output      **kwargs).stdout    File ""/usr/lib/python3.5/subprocess.py"", line 708, in run      output=stdout, stderr=stderr)  subprocess.CalledProcessError: Command '['/home/zqj/Workspace/Tools/pyrouge-master/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl', '-e', '/home/zqj/Workspace/Tools/pyrouge-master/tools/ROUGE-1.5.5/data', '-c', '95', '-r', '1000', '-n', '2', '-m', '-a', '/tmp/tmpf5fnmfg3/settings.xml']' returned non-zero exit status 255    My OS  is: Ubuntu 16.04.  Perl version: perl 5, version 22.    My I ask how to solve this is issue?"
"L127-L128 has no effect:      I assume this is intended:`seq.append(art_sent[max(range(len(art_sent)), key=lambda j: attn[j].item())])`."
"Hello,    Could you help me understand why beam search (e.g. `BeamAbstractor`) is not used during training (either abstractor training or RL training)?    Thanks,    Felix"
"Dear Chen     I train this model in othe dataset of  single sentence abstract. but  in the process of implementation, i find that the model(abstractor) build an channel by single sentence to single abstract.     If i try to intelligently select K sentence by rouge in extractor stage, than i flatten this sentence to train in abstractor and full_rl, do you think  this is feasible？    Does the model of fast_abs_rl  apply to  single sentence abstract? I would be very grateful  for your reply and advices."
"Hello,    Here in the CNN model, it seems the dropout layer is placed before the convolutional layer as opposed to after the max pooling layer as per Kim 2014:      Is this intentional or a bug? If it's intentional, what is the rationale behind it?    Thanks,    Felix"
"hi, I want to know how do you divide a document into some sentences? In your code, I found that you treated each word as a sentence. Is it right?"
I read the source code and found that the data format is JSON. How it is specifically constructed? I want to use my own data. What do I need to do?
"Hi,    In the data download link (  it seems the unlabeled data is not provided? Can we also have the unlabeled reviews?"
Hi.    Do you have any plan to release the original annotated dataset for this paper?    Thanks. 
"Hello,    Can you please share the human annotated data?    Thank you,  George."
"     The variable `model_path` was initialized to point the path of the pretrained VHRED weights (the _model.npz file), but it was never used and the `load()` method of `Model` was never called. Is it a bug? "
"Hi, I have tried to use this code to evaluate the dialog models. However, when I apply Adem on the instances in the paper ""Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses"", I find that the output results are different from the results shown in the paper. Here are some examples of my experiments:  context:      Is there something wrong with my input format? Could you please help me figure out why there is such a big difference between scores in my experiment and scores in the paper? Thanks!"
None
"Hi - I was hoping this repo would be suitable for replicating some of the paper's experiments, but it appears the code here is currently incomplete -  the data loading portions including `data.abstract.corpus` are dependencies, but are not currently included. Is this correct, or is there something I'm missing?"
"Hi, could you provide the evaluation code to reproduce the results of the paper?    "
"I run your code with python kgcvae_swda.py on cpu, and get the output:    Avg recall BLEU 0.358250, avg precision BLEU 0.124279 and F1 0.184540 (only 1 reference response. Not final result)  Avg recall BLEU 0.358250, avg precision BLEU 0.124279 and F1 0.184540 (only 1 reference response. Not final result)    I used python3.6 and tensorflow 1.12 cpu version, and made neccessary changes to run your code in my environment. But it seems something went wrong.    Also I ran your code on python2 and tensorflow 1.3 cpu version, but got the same result. Can you give some tips to reproduce your results? Thanks."
If Glove word embeddings is assigned before tf.global_variables_initializer(). The embedding will be randomly initialized. It will not use Glove word embeddings. 
"Hi, could you provide the evaluation code to reproduce the results of the paper?"
"Hi, how can I ignore unk tokens while decoding? It probably just means setting the weight to zero.     So I want to do something like this:       How can I do this in your code?    Thanks."
"Hello, I am a member of the PaddlePaddle NLP team. Can we use your data in our VAE example ? We will note the source of your data in the README file. Thank you so much. "
"     Dear Dr. Zhao. I'm Sangjun Koo from POSTECH  (You may have heard of POSTECH from Dr. Kyusong Lee, one of my former advisor.)    Thanks for releasing your code for further research.  Would you mind me asking some question about your code?    In reimplementing your code into PyTorch, I found that some anomaly which could be a bug(although it is not serious). The line should be  np.argmax(sample_das[r_id], axis=1)[bid] instead of np.argmax(sample_das[r_id], axis=1)[0] since this may cause to print only the first predicted DA element in the batch.    Any comments would be welcome.  "
"Hi,    I want to use the test dataset with multiple references(which you uploaded at   to get the evaluation result you presented in the paper.   But I'm pretty confused because the format of this one is quite different from the one with single reference, for it's in lack of some information, such as topic.  Could you please tell me how to get the evaluation result with multiple references with this test dataset which seems to be incomplete?    Looking forward to your reply. ：）"
"in your paper, you have said: 【Also, to compare the diversity introduced by the stochasticity in the proposed latent variable versus the softmax of RNN at each decoding step, we generate N responses from the baseline by sampling from the softmax. For CVAE/kgCVAE, we sample N times from the latent z and only use greedy decoders so that the randomness comes entirely from the latent variable z.】    the tradictional beam search with size B have two step, first for each beam, generate top-B words from the vocab-softmax,  then generate top-B beams from the B*B candidate sequences using the average probability.  Is the sampling in the above figure means two multinomial step for the inner vocabulary softmax and the outer average probability?  And is the inner sampling with replacement or without replacement?  Is the outer sampling with replacement or without replacement?"
Why do you calculate the the bow loss similar to a sequence (tile bow logits to max_out_len and compare it to labels) instead of forming a single one hot vector out of the labels and compare it to the predicted bow vector?
"Changing the cell type to LSTM throws an error (with default parameters and code):    `ValueError: Shape must be rank 2 but is rank 3 for 'model/concat' (op: 'ConcatV2') with input shapes: [?,30], [?,4], [?,4], [2,?,600], [].`"
"Hi,    When I set:       I get the following error:    `ValueError: Trying to share variable model/contextRNN/rnn/multi_rnn_cell/cell_0/gru_cell/gates/kernel, but specified shape (300, 300) and found shape (302, 300).`"
"Hi,    What modifications in the code are needed to train on a dataset without dialogue acts, metadata, and features?"
"Hi,    I  recently running your code,  and get this after training with glove,   I find it is confusing that the evaluation has no direct connection with results in your paper.  Could you give a more detailed explain  or  more consist evaluation?    <img width=""825"" alt=""2017-11-16 10 14 26"" src=""     "
"Tensorflow 0.12.1    Traceback (most recent call last):    File ""kgcvae_swda.py"", line 13, in        from models.cvae import KgRnnCVAE    File ""/home/ubuntu/NeuralDialog-CVAE/models/cvae.py"", line 8, in        import tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl as rnn_cell  ImportError: No module named core_rnn_cell_impl    Which tensorflow version works fine with it?"
"Hi! I was trying to run the code, but couldn't access the IMDB dataset. Would you please help me open the access? Thnx!"
"Hi,     I am interested in this work and want to run the code myself. However, when I run    `python session_supervised_dec.py`  or    `python session_supervised_enc.py`    I will get     `KeyError: 'NT(TOP)'`    The only change I made is setting the default value of `embedding_file` to `None`, and I get the oracle with     `get_oracle.py wsj/train wsj/train > train.oracle`    I am not quite sure where I am doing wrongly. Any help will be appreciated!"
在您论文的4.4 章节，您说您only select the sentences expressing this relation.因为句子的标签都是按包给的，可能我们无法直接获取到句子的标签。
"When preprocessing data with shell 'script scripts/extract-deepbank-sdp.sh', I meet some question about PyDelphin, such as ""keyerror: e3"" with data ""DMRS"".  Because I just want to deal with ""EDS"" data, so I remove ""dmrs"" type from the shell script.  But, with 'scripts/preprocess.sh', I also meet a lot of question.  Here, I want to ensure that, is this code can be applied only to 'eds' type  without 'dmrs'.  If not, PyDelphin has any requirement about version or other issues?"
None
"for kk, vv in worddicts[ii].iteritems():  AttributeError: 'list' object has no attribute 'iteritems'    how to resolve this ?"
'/data/lisatmp3/chokyun/europarl/europarl-v7.fr-en.en.tok.tree'  how to make .Tree I can't find everywhere I use parallel corpus kindly guide me 
"Hi Cheng, nice work! I read your paper and deeply impressed by your work. I wonder if you can provide data/ folder (including the train/test/valid and general_predicate files) so that we can set up the system and run? Thanks in advance!"
None
Hi @wanghm92   1) Would like to understand how singlish_posTagger.model is used to train the dependency parser.    2) Where to obtain more of the Singlish dataset?
"In the notebook, You probably just need to add something like         after loading the data and use these variable."
"I want to train the model on another dataset, how should I process the data to generate the .pkl files?"
hi 您好    如何把豆瓣开源的对话数据应用到这个multiTurnResponseSelection上面呢？  十分感谢    Thanks  weizhen
None
你好，数据集onedrive下载链接失效了，请问可以回复一下吗？
None
"Hi, in your script **PreProcess.py**, if we use the _DoubanConversaionCorpus_ as you released, where's the coressponding file **word2vec.model** when we call the class **WordVecs**?"
希望README文件能够详细一点，尤其是对于使用到的数据的描述。
您好，阅读您的文献的时候发现了您开源的多轮对话语聊，请问能不能把训练集和验证集的语聊发我一下？(tanglifutlf@126.com) 非常感谢！
"> We release Douban Conversation Corpus, comprising a training data set, a development set and a test set    I can only see the test set (test.txt) and a small sample of the training set. Where is the rest of the training (and validation) set?"
"dataset = r""../ubuntu_data.mul.100d.fullw2v.train"""
"Hi， 我clone下来的代码跑挂了，没弄懂什么原因：    MultiTurnResponseSelection/src/CNN.py:233: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.    self.output =theano.tensor.signal.pool.pool_2d(input=conv_out_tanh, ds=self.poolsize, ignore_border=True,mode=""max"")  Traceback (most recent call last):    File ""SMN_Last.py"", line 422, in        ,hidden_size=100,word_embedding_size=100)    File ""SMN_Last.py"", line 334, in train      train_model = theano.function([index], cost,updates=grad_updates, givens=dic,on_unused_input='ignore')    File ""/usr/lib64/python2.7/site-packages/theano/compile/function.py"", line 326, in function      output_keys=output_keys)    File ""/usr/lib64/python2.7/site-packages/theano/compile/pfunc.py"", line 449, in pfunc      no_default_updates=no_default_updates)    File ""/usr/lib64/python2.7/site-packages/theano/compile/pfunc.py"", line 208, in rebuild_collect_shared      raise TypeError(err_msg, err_sug)  TypeError: ('An update must have the same type as the original shared variable (shared_var= , shared_var.type=TensorType(float32, matrix), update_val=Elemwise{add,no_inplace}.0, update_val.type=TensorType(float64, matrix)).', 'If the difference is related to the broadcast pattern, you can call the tensor.unbroadcast(var, axis_to_unbroadcast[, ...]) function to remove broadcastable dimensions.')    这个是theano的版本问题么？"
None
可以麻烦您重新传一份吗
None
"我训练了2个epoch，loss=0.334, R10_1=0.646, R2_1=0.895，而且奇怪的是后两个指标在训练过程中一直不变。大家训练的loss能下降到多少呢？performance可以达到论文中的数值吗？  我用的tensorflow版本的。"
为什么添加矩阵A，而不是像M1一样直接相乘呢？
谢谢！  @MarkWuNLP 
"Thank you for sharing the code for your paper. I was wondering how many epochs you trained the model on Ubuntu dialogue corpus for? Was training done on a single GPU? If so, how long would a single epoch take on average?"
how is the training file you shared supposed to be read? shall I parse it myself? or have you used a library to store it? I'm talking about this file:  
您好！感谢您对数据集的公开！关于这份数据集，请问是否有未去除标点符号的版本？
onedrive上的URL 失效了，不知道能不能解决一下。感谢
"How did you tokenize the raw Douban corpus? I'm trying to test my model with new data, I need to tokenize them firstly."
If I'm correct the I obtain a vocabulary size larger than 600.000 distinct words from the Ubuntu dataset. That number seems very big. Do you limit the vocabulary size for the model?
在tensorflow版本中，actions从来没有进入到模型训练这一环节。总是把true_utt给response的placeholder。
"I am having an issue with SMN_Dynamic     I'm calling train with 200 for the hidden size and embedding:   `  train(datasets,wordvecs.W,batch_size=200,max_l=max_word_per_utterence  ,hidden_size=200,word_embedding_size=200,model_name='SMN_Dynamic.bin')  `  Can you help please ?  "
"Hi, Yu,    I followed the instructions in   to run  SMN_Last.py. But it does not work. As you suggested, I modified the parameter word_embedding_size=200 hidden_size=200 in SMN_Last.py.    The following is the error message. Do you know the reason for this? Thanks!    trainning data 949999 val data 50001  image shape (200, 2, 50, 50)  filter shape (8, 2, 3, 3)  /mnt/scratch/lyang/working/PycharmProjects/NLPIRNNMatchZooQA/src-match-zoo-lyang-dev/matchzoo/conqa/smn_yuwu_acl17/src/CNN.py:233: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.    self.output =theano.tensor.signal.pool.pool_2d(input=conv_out_tanh, ds=self.poolsize, ignore_border=True,mode=""max"")  (200, 50)  [[ -7.18495249e-03   1.15389853e-02  -4.65663650e-03 ...,   1.29073645e-02      5.36817317e-03  -3.73283600e-03]   [ -7.56547710e-04   2.52639169e-03   2.14198747e-03 ...,   1.87613393e-03      3.13413644e-03  -1.80455989e-03]   [  1.70666858e-02  -1.36987258e-02   2.04090084e-02 ...,   6.40176979e-03      2.26856302e-02  -3.21805640e-02]   ...,    [  1.94880138e-02   1.08157883e-03   7.03368021e-05 ...,   1.80350402e-02     -3.28865572e-03  -1.50815454e-02]   [  5.96401096e-03  -9.91006641e-04   4.77976783e-03 ...,   2.32200795e-02      3.23144545e-02  -8.65365822e-03]   [ -6.93387402e-04   3.27511476e-03   2.07571058e-03 ...,   1.42423584e-03      3.12527304e-03   8.35354059e-04]]  Traceback (most recent call last):    File ""SMN_Last.py"", line 422, in        ,hidden_size=200,word_embedding_size=200)    File ""SMN_Last.py"", line 334, in train      train_model = theano.function([index], cost,updates=grad_updates, givens=dic,on_unused_input='ignore')    File ""/usr/lib/python2.7/site-packages/theano/compile/function.py"", line 326, in function      output_keys=output_keys)    File ""/usr/lib/python2.7/site-packages/theano/compile/pfunc.py"", line 449, in pfunc      no_default_updates=no_default_updates)    File ""/usr/lib/python2.7/site-packages/theano/compile/pfunc.py"", line 208, in rebuild_collect_shared      raise TypeError(err_msg, err_sug)  TypeError: ('An update must have the same type as the original shared variable (shared_var= , shared_var.type=TensorType(float32, matrix), update_val=Elemwise{add,no_inplace}.0, update_val.type=TensorType(float64, matrix)).', 'If the difference is related to the broadcast pattern, you can call the tensor.unbroadcast(var, axis_to_unbroadcast[, ...]) function to remove broadcastable dimensions.')  "
您好，按照您在readme里面写的，修改path后运行PreProcess.py提示错误，找不到word2vec.model，请问在PreProcess.py中用到的word2vec.model，以及其他文件：train.topic、mergedic2.txt等文件在哪里呢？谢谢！
"Hi Wu,    We tried to carefully replicate your work in tensorflow and the performances are awesome, however, our model is still 3-4% lower compared to you reported in your paper.     We suspect the reason is that the word embedding for initialization we used is different to yours. Thus I wonder if you could provide the train/test binary file that you used in your code?    Best wishes,  Xijuan"
"it's a good job, you meant that you have released the douban corpus in the paper, but i couldn't find the training data in this project."
     多谢多谢@MarkWuNLP
您所了解的情况  多谢  多谢  多谢  @MarkWuNLP 
这里的     谢谢谢谢@MarkWuNLP   
@MarkWuNLP 谢谢谢谢
如何我没有理解错的话，Ubuntu Dialogue Corpus和豆瓣数据是 一个帖子题目 加 该帖子的N条回复，  为什么前面可以当作context 以及 最后一条回复可以当作response？  十分谢谢谢谢@MarkWuNLP 
你们的工作对我启发很大，在github上只看到了豆瓣数据的train sample，请问一下完整的数据会发布在哪里，何时发布哦？
您好，全部的豆瓣训练语料能对外公开么，谢谢~
response candidate r 我理解应该是固定的一句话，现在是把r这整句话一起输进SMN，    如果换成r的id会怎样？    @MarkWuNLP 谢谢！
@MarkWuNLP 谢谢  感觉训练数据不应该是一问一答或者多问一答的样子的么？
"Hello,    Thanks for your great research. I've added a SWAP operation for handling non-projective trees to your arc-standard transition system. Now, it can also produce oracles with swap operation. the code is here:      By updating transition.py and parserstate.py files, oracles for Arc-standard with SWAP operation ca be produced.    "
"Hi   I want to use your code to convert parse trees to dependency ones, my data is in the format of one line per parse, I see the codes does not work for this format, how can I convert them to the format acceptable for the codes?  thanks"
"Thanks for you paper and code.    But I'm confused with some code.    In `src/model.py`    function `get_score()` -> function `inner_one_step()`    `update_gate = T.exp(T.dot(ugW[:ln+nhiddens,an+ln-nhiddens:an+ln+ln],com)+ugb[an+ln-nhiddens:an+ln+ln]).reshape((len+1,nhiddens))`    and in `src/tools.py`    function `get_word()`    `update_gate = np.exp(np.dot(ugW[:ln+ndims,an+ln-ndims:an+ln+ln],com)+ugb[an+ln-ndims:an+ln+ln]).reshape((len+1,ndims))`    I think they should be         and           Since the code     represent a vector     where ei_j derived from the jth elem of ith character.  So the reshape((nhiddens, len+1)) reshape the vector to     which may supposed to be       In order to see the difference, I revised the code and ran it on CPU. But The memory occupation raised from 3GB(epoch 1) to 30GB(epoch 8). And it took 8000s per epoch.    The original code took 7000s per epoch and 2.6GB.    Did I misunderstand something?    Thanks!      "
"当我用dynet运行代码时，出现错误        word = dy.sum_cols(dy.cmult(update_gate,dy.reshape(comb,(self.options['ndims'],wlen+1))))  AttributeError: 'module' object has no attribute 'sum_cols'  "
"I'm trying to train the model with my own corpus, but it failed with this error:  !   !   I highlight the important lines, which apparently some matrices' dimension is not compatible to do matrix multiple: `numpy.dot(A,x)`。But all I did is just replace the corpus with my own one, really don't know why this is happening. Is there any requirement for corpus or whatever other reasons that i can not figure out."
形如这样的结果:   \  x  e  4                              \  x  b  8   这是啥啊......
" I have all the necessary installation done. After running ""ruby ./pull-dependencies core"". Only fig folder is created no lib folder is created.     !     So when I try to run ""ant core"" Get the following error. Please suggest what is the issue.    Buildfile: C:\Users\1392287\Documents\sempre\build.xml    init:    BUILD FAILED  C:\Users\1392287\Documents\sempre\build.xml:18: Execute failed: java.io.IOException: Cannot run program "".\scripts\extract-module-classes.rb"" (in directory ""C:\Users\1392287\Documents\sempre""): CreateProcess error=193, %1 is not a valid Win32 application          at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)          at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)          at java.base/java.lang.Runtime.exec(Runtime.java:615)          at org.apache.tools.ant.taskdefs.launcher.Java13CommandLauncher.exec(Java13CommandLauncher.java:58)          at org.apache.tools.ant.taskdefs.Execute.launch(Execute.java:424)          at org.apache.tools.ant.taskdefs.Execute.execute(Execute.java:438)          at org.apache.tools.ant.taskdefs.ExecTask.runExecute(ExecTask.java:660)          at org.apache.tools.ant.taskdefs.ExecTask.runExec(ExecTask.java:699)          at org.apache.tools.ant.taskdefs.ExecTask.execute(ExecTask.java:527)          at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:299)          at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)          at java.base/java.lang.reflect.Method.invoke(Method.java:577)          at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:99)          at org.apache.tools.ant.Task.perform(Task.java:350)          at org.apache.tools.ant.Target.execute(Target.java:449)          at org.apache.tools.ant.Target.performTasks(Target.java:470)          at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1401)          at org.apache.tools.ant.Project.executeTarget(Project.java:1374)          at org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)          at org.apache.tools.ant.Project.executeTargets(Project.java:1264)          at org.apache.tools.ant.Main.runBuild(Main.java:818)          at org.apache.tools.ant.Main.startAnt(Main.java:223)          at org.apache.tools.ant.launch.Launcher.run(Launcher.java:284)          at org.apache.tools.ant.launch.Launcher.main(Launcher.java:101)  Caused by: java.io.IOException: CreateProcess error=193, %1 is not a valid Win32 application          at java.base/java.lang.ProcessImpl.create(Native Method)          at java.base/java.lang.ProcessImpl. (ProcessImpl.java:494)          at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:159)          at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)          ... 23 more    Total time: 0 seconds"
"Hey, so I tried building Sempre with Java 17 for   project but I am having trouble. Firstly, I am using Java 17 and IntelliJ Idea as the IDE. Starting from, building I did the dependencies pull (`./pull-dependencies ...`) and they installed perfectly. Altho, once I start building regex for semparse I run into multiple errors of     but since they are not used anywhere I simply solved it by commenting it. And once I again run it, I face an error of `ClassNotFoundException`.       At first, I thought this might be a Java 17 issue but I guess Java is backward compatible and could work with java 1.8? (correct me if I am wrong?) .   ## Steps to reproduce  1. Clone         This might throw an error of `file not found` its because sempre couldn't finish up the subprocess. But for forwards steps it produces the necessary file. For more logs you can try:       This is the command that throws an error.  Hoping to find a way out Thanks in advance"
"Various papers report denotation accuracy on Overnight, which requires executing the logical form on a database. (The Overnight paper specifies that ""We created a database for each domain by randomly generating facts using entities and properties in the domain(with type-checking)."")  However, it is unclear how to easily calculate denotation accuracy from the given data.  Could you please point me in the right direction?    EDIT: I found SimpleWorld"
"I would like to know how my natural language entity matches the resulting derivation. For example, using the `emnlp2013.grammar` for a freebase question     produces     I would like to visualize the derivation tree and see how the final formula is generated from the NL sentence step by step.    Is there an easy way to do that in the current version? "
"Hello,    I am using  , which uses SEMPRE and I am trying to run inference in some generated CSV files.     I already generated the CSV files using PlotQA, now I just need a way to ""ask some questions"" to these few tables. I am using SEMPRE Tables (Semantic Parsing on Semi-Structured Tables). My question here is how can I do this in a simples way? I could not find commands to run simple questions, just to run training modes.    I hope you guys can help me out. Thanks in advance."
"Hi,  I'm trying to get some deeper insight in the code of sempre.  For the moment, I'm looking at the simple Java logical forms  and their execution by the class JavaExecutor-  The class CallFormula features the method ""forEach"". I can't see  its exact purpose. Intuitively, I think it manages the recursive   decomposition of a more complex CallFormula, and I guess the   Boolean returned by func says if the base case of the recursion is  reached. But again, I don't get exactly how. Also it seems that this foreach  method is not even used in the JavaExecutor's methods execute and   processFormula, that apparently are responsible for the execution. Can someone help me to understand the logic behind all this please? Thanks!"
"I get this error when running `./run @mode=genovernight-wrapper`     here is the full output when running that command:      Could you please take a look into this?     Best,  Arian"
"Exception in thread ""main"" java.lang.NumberFormatException: For input string: ""tables.lambdadcs.LambdaDCSExecutor""   at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)   at java.lang.Integer.parseInt(Integer.java:580)   at java.lang.Integer.parseInt(Integer.java:615)   at fig.basic.OptInfo.interpretValue(OptionsParser.java:155)   at fig.basic.OptInfo.set(OptionsParser.java:344)   at fig.basic.OptionsParser.parse(OptionsParser.java:771)   at fig.exec.Execution.init(Execution.java:212)   at fig.exec.Execution.runWithObjArray(Execution.java:330)   at fig.exec.Execution.run(Execution.java:325)   at edu.stanford.nlp.sempre.Main.main(Main.java:50)    How can I solve the problem ？I look forward to hearing from you"
How to test the samples after training the model using following command?    ./run @mode=overnight @domain=recipes
"I have completely installed the setup and tried parsing utterances such as california, the golden state by following the tutorial. But now we tried using emnlp2013 grammar file by using the command:  ./run @mode=simple-freebase-nocache @sparqlserver=localhost:3001 -Grammar.inPaths freebase/data/emnlp2013.grammar. But I am not able to get the logical forms. Do we need to include a lexicon file as well as they did in the tutorial. If yes, where do we find one? Else please suggest something to help me move forward.    Thanks in advance.  "
"Hi,  I guess for doing so I have to add dependencies to the gradle build file,  somehow like this:  dependencies {  implementation (group: 'com.github.percyliang', name: 'sempre', version: '2.4')  }  but when trying to build, I get an error message saying sempre couldn't be found.  Should I use another version number? Any other idea what is going wrong?  jitpack.io repository (for accessing GitHub) is already added.    Thanks for attention,  Stefan  "
Why wasn't a programming language like python/ java used to specify rules?    Instead of (rule $Expr ($PHRASE) (NumberFn))         Instead of (rule $Operator (plus) (ConstantFn (lambda y (lambda x (call + (var x) (var y))))))         Instead of (rule $Partial ($Operator $Expr) (JoinFn forward))        
"This is probably due to SUTime not loading properly.         From   it looks like the possible solutions are:  - Don't use Java 9, 10, or 11  - Add `--add-modules java.xml.bind` to the command line (does not work on Java 11)  - Disable SUTime."
"It would be great if possible solutions to the tutorial exercises were included. I'm aware that there are likely many possible solutions to a given exercise, but It would be helpful to have at least one correct solution when working through the tutorial.    In my case specifically,  I haven't been able to follow the hints for exercise 2.1 to produce rules that can parse utterances into the category $Expr, since it isn't clear to me how to reference and apply the $FUNCTION type. My best attempts, which do not seem to work, are the following:     "
"Hi,  I have gone through some closed issues, it says there can be problems when working on windows. I wanted to know if it is recommended to work on windows? Thanks."
"Hi,    I download the full copy of freebase with the method in pull-dependencies, and try to use the freebase to do question answering task on WebQuestionsSP, which is an parsed version of Webquestions (a part of project sempre). However, when I try to execute the queries in WebQuestionsSP, only ~5% of the queries have returned result. Not sure if it is the problem of interacting Freebase and Virtuoso, or other problem (at least no error thrown) I'd appreciate a lot if you could help us with this.    Here is the link of WebQuestionsSP:      BTW, the virtuoso I'm using is not strictly under your tutorial, it is when I tried to setup the freebase provided by Siva (which is around 50GB) after extracting out.    Thank you very much!"
"Hi, I'm hoping to use ContextFn similarly to the anaphora rule for 'he' in the documentation, but pulling out a non-leaf logical form that apparently has children of nonuniform depth.    The context exchange looks like this:        (exchange (exchange harriet (call edu.cmu.lia.semparse.ReferenceQuery.generate       (string OBJECT) (string pk=32)) (string OBJECT[pk=32])))    & I would take either a solution for          (string OBJECT[pk=32])    or for         (call edu.cmu.lia.semparse.ReferenceQuery.generate (string OBJECT) (string pk=32))    The documentation suggests a rule like this would work:        (rule $CATEGORY (he) (ContextFn (depth 0) (type fb:type.text)))    ...but, alas, no luck. I've also tried depth 1 and depth 2, with no success. Some digging and print statements suggest that my first preference (`(string ...`) is never investigated, and my second preference (`(call ...`) can never be selected because it has two leaf children of depth 1 and four leaf children of depth 2.    What's the intended behavior of the depth restriction -- are higher-level nodes meant to be completely symmetrical in depth, or would it be alright to replace that `currentLevel == depth` with `currentLevel <= depth`?    "
I am a bit confused on how to use the trained model (e.g. overnight/basketball) interactively.    Is the following command correct?  `./run @mode=simple -languageAnalyzer corenlp.CoreNLPAnalyzer -Grammar.inPaths overnight/basketball.grammar -Builder.inParamsPath state/execs/20.exec/params -Builder.parser FloatingParser`    If i try the utterance: _what players made less than three assists over a season_    I get 0 candidates      I would be nice to provide some example `./run` calls for overnight  
"Hi,  The SEMPRE can be tested in an interactive prompt or on a web interface, but I want to receive a natural-language question and get the SPARQL answer in a Java project. Is there any toturial about how to utilize SEMPRE by writing a Java program?"
"Hi!    I'm attempting to write a CCG grammar using SEMPRE and I observed that the result `formula` produced by joining a binary and a unary with `JoinFn` is not fully reduced when the binary is a `LambdaFormula`. After reading `JoinFn.doJoin`, from my understanding, this happens because the reduction is done using `Formulas.lambdaApply`, which would only apply once, instead of the full `Formulas.betaReduce`, which would reduce all nested lambdas. I am trying to understand the reasoning behind this design decision, could someone provide me some insight on this?      Here's an example, given the following rules  `  (rule $noun (noun) (lambda f ((var f) (string noun))))  `  `(rule $adj (adj) (lambda x (some_adj (var x))))  `  `(rule $ROOT ($noun $adj) (JoinFn forward betaReduce))  `    If we try to parse `""noun adj""`, it would do lambdaApply once and parse to   `((lambda x (some_adj (var x))) (string noun))`, rather than do a full betaReduction which would apply the inner lambda again and get `(some_adj (string noun))`.    Again, I would be grateful if someone could explain here the reasoning behind using `lambdaApply` instead of `betaReduction` in `JoinFn.doJoin`.     Cheers,  Brian  "
"I am a newbee in this research field, and before I dive into this toolkit, could anyone tell me whether it supports Chinese? So that I can write my rules to parse Chinese uttrance into the logic presentations. My sincere thanks!"
I tried to do the ./pull-dependencies fullfreebase-vdb in the tutorial. However the script segmentation fault at the last %1....
"We have semantic parsing methods like NL2SQL, NL2SparQL.  And we have other methods like  . I guess we can also  transform the table into graph to use this path-finding method.    How do you think the two way?  Thank you.  @percyliang @sidaw @ppasupat @samginn @yonatansito "
"I'm following the steps that are presented in the TUTORIAL. Although I can successfully start the virtuoso, I can not add the small graph to the database.   I'm getting the following error when I run 'freebase/scripts/virtuoso add freebase/data/tutorial.ttl 3001':    freebase/scripts/virtuoso:229: command not found: fig/bin/chunk -file freebase/data/tutorial.ttl -headerNumLines 1 -chunkSize 100m -printNumChunks  freebase/scripts/virtuoso:18:in `Integer': invalid value for Integer: """" (ArgumentError)   from freebase/scripts/virtuoso:18:in `parseInt'   from freebase/scripts/virtuoso:130:in `add'   from freebase/scripts/virtuoso:223    Is there anyone that faced the same problem? or any ideas on how to solve it?    OS: Ubuntu 16.04  Ruby:1.8.7    Thanks in advance for any assistance."
get below error when I try to run overnight commands for geo880    Unknown option: 'overnightderivationpruningcomputer.usepredicatedict'; -help for usage
"I am not sure about the purpose of processed-grammar that gets created in state/execs/#.exec directory but when I use in below command, The grammar files in context is for publications domain that comes with overnight samples.  I get few errors given below    command  java -cp libsempre/*:lib/* -ea edu.stanford.nlp.sempre.Main -Main.interactive -interactive false -server true -Grammar.inPaths state/execs/1.exec/processed-grammar    errors  ERROR: java.lang.RuntimeException: Error on (rule $Intermediate1 (that $VP) (SelectFn 0)): java.util.MissingFormatArgumentException: Format specifier '%s':    debugging above error reveals bug in Grammar.java file on line number 259.    after I fix it I get bellow error     **ERROR: java.lang.RuntimeException: Found cycle of unaries involving $Intermediate3:**    not sure how to resolve it.  "
"I am trying to train sempre on a new domain and i get below error    ERROR: Caused by java.io.IOException: Unable to open ""lib/data/overnight/research.word_alignments.berkeley"" as class path, filename or URL:  edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:485)    Apparently the  .word_alignments.berkeley file is missing and hence the issue.    Can you please guide me on how to generate the given word alignment file along with other .ppdb file?  Thanks.  "
"i have some problems, after checking out your code carefully.  First, i found the weights in params are mostly related to your training table. So, if i have a new table on test set. These weights can not be used to compute score of derivation, or you abstract these weights during test? Is that right?  Second, your extend macro rules of an  example when the derivation is ROOT. In your paper, you said you extended macro rules on every subtree. But the ROOT may not be a subtree which is referred on your paper.  "
"when i debug ""tables"" some errors occur:  Unknow option:""tableknowledgegraph.basecsvdir"""
how to generate semantic-cprune.jar
"Hi, I am conducting research based on the  .    However, I found that there are several cases that sempre can't solve.     How the sempre solve the long dependent predicates? For example,     In this case, the alignment should be:     and the parsing rules:     Thus the logic form could be `(film.film.director join Christopher_Nolan) intersect (film.film.actor join Tom_Hanks)`.   The paper proposed that you used a common bottom-up parser. Assuming I use a CYK parser, `film.film.director` and `Christopher_Nolan` are non-consecutive fragments so they can't be assembled together as in this case.     Second case,      The logic form could be `(film.film.director join Christopher_Nolan) intersect (film.film.director join Tom_Hanks)`.   However, the `and` won't be aligned to 'film.film.director' and the `film.film.director` won't be joined with two entities. So how to produce such logic form?    Maybe there is something that I missed when reading this paper. Have you already proposed methods that can deal with such cases?    Thank you in advance for resolving my issue."
"I want to replicate the result in the paper Imitation Learning of Agenda-Based Semantic Parsers about the webquestions dataset. I find parameters have been trained and uploaded on the codalab.So I load the parameters in the ./run command and use the webquestions. However, i can't get the same result in the 3rd training process listed in the log file.  Actually, what I get is much different.  What's wrong with me?  Is there anyone who have successfully get the result?  Thanks very much."
I want to print the Sparql expression when I type in the question.  So how can I get it?  Thank you very very much.
"Hi,   I want to apply sempre (table version) to other language such as Korean.  But, I think sempre only work on English.  Because, when I run this model on Korean table and Korean question, it was not work.  Is it possible that applying sempre to Korean?  or is there any other way (such as replace to Korean semantic pareser) to apply Korean?    Thank you."
"I'm trying to query freebase (I pulled full freebase) but I'm not getting any answers. The strange thing is that if for example I run:    `./run @mode=query @sparqlserver=localhost:3001 -formula 'fb:engineering.engine.energy_source'`    I get:    `(list (name fb:engineering.engine.energy_source ""Energy source""))`    and also with:    `./run @mode=query @sparqlserver=localhost:3001 -formula 'fb:en.internal_combustion_engine'`    I get:    `(list (name fb:en.internal_combustion_engine ""Internal combustion engine""))`    So both entities are there but when I try to run the question from free917:    `./run @mode=query @sparqlserver=localhost:3001 -formula '(fb:engineering.engine.energy_source fb:en.internal_combustion_engine)'`    I get an empty list."
"Hello,    I'm following the installation instructions and everything went fine until the ""Virtuoso graph database"" part. When I run """" I get:    `configure: error: The current version of Virtuoso Open Source Edition (Column Store) can only be build on 64bit platforms`    My system is Ubuntu 16.04 , i686 (32 bits). Is there any quick fix besides upgrading to a 64-bit edition (which I know I should ASAP). Thanks in advance!"
"I am new to sempre and I am trying really hard to understand its functionality and use it for my project. From what I understand, I have to write some base case rules to start, is there a way to generate those base case rules automatically? Also, is there a tutorial online that goes step by step from raw data to rules? The existing one is a little too high level and hard to follow for me. "
"Hi, I followed the setup and I'm running my own installation of Freebase on my server. Simple queries seem to work fine, such as `(execute (count (fb:type.object.type fb:location.citytown)))`. However, queries such as `(execute (and (fb:type.object.type fb:location.citytown) (fb:location.location.containedby fb:en.california)))` timed out. I tried it again, and the result is cached so it immediately returned TIMEOUT.       > \> (execute (count (fb:type.object.type fb:location.citytown)))    SparqlExecutor.execute: (count (fb:type.object.type fb:location.citytown))    (list (number 409037))  > \> (execute (and (fb:type.object.type fb:location.citytown) (fb:location.location.containedby fb:en.california)))    TIMEOUT    Is that normal to observe this behavior when loading the whole freebase database? Moreover, free give me this:         It doesn't seems to load the whole thing into memory. What is the problem?"
"I am trying to run     It seems that the lucene index is missing it complained `sempre/lib/lucene/4.4/free917` was missing, I created the folder hoping it was just expecting the folder to exist and it would write files to that directory. It seems that is also not the case. I now get:   "
"I have installed `sempre` and `virtuoso`, and I have successfully go through the **Setting up your own Virtuoso graph database** section. (However, I skipped the **Setting up a copy of Freebase** section). I have problems when I try the first example in the **lambda DCS** section. After starting the virtuoso database successfully (`freebase/scripts/virtuoso start tutorial.vdb 3001`), I tried this command:  `./run @mode=simple-freebase @sparqlserver=localhost:3001`  Then I have error messages like this:     which should start a command prompt according to the **tutorial**. Any suggestions to fix this? "
This is not an issue but i was wondering if someone has tried generating the factoid answers and then doing some natural language generation to convert them into natural language.   Can we inflect the asked question (eg. What is your name?) into a statement (eg. My name is ____ ) and then fill it with the factoid answer ? I would really appreciate some pointers to papers or implementations. 
"Hi I am doing the tutorial but I can't seem to get pass getting ""cities in california"" to work.. I increased the logs and saw that it had the following:     I feel like at this point it should follow the rule:     as it has a $Binary and $Set. However it never seems to be able to create the set from the `JoinFn forward` so it never achieves getting to $ROOT. I don't know what else to do as I am just learning however it might be useful to point out the type for containedby is:     rather than `(-> fb:type.any fb:type.any)` as it is in the tutorial. The complete output for `cities in california` is:    "
"I understood that that @mode ""genovernight"" can be used to dump set of (z,c) - (logical form, canonical utterances) and by paraphrasing we can get (z,c,x) - (logical form, canonical utterance, paraphrase utterances). This set of (z,c,x) can be divided into two files:  .paraphrases.train.examples and  .paraphrases.test.examples.  But there are more inputs needed to train the semantic parser and those are following:  a.  .train.superlatives.example - superlative training (and test file too) b.  .phrase_alignments - phrase alignment file c.  .word_alignments.berkeley - word alignment file d.  -ppdb.txt - ppdb model  How I can generate these files for my domain? Details about steps to produce these files will be really helpful. "
How can I re-construct the experiment done in the paper(Imitation Learning of Agenda-Based Semantic Parsers (TACL 2015))?  Is it ./run @mode=freebase?  +) Do I need to download any dependencies or source code? 
"Hi, I am trying to retrain on emnlp 2013 by running the following command   ./run @mode=freebase @domain=webquestions @train=1 @sparqlserver=localhost:3001 @cacheserver=local  But, I get the following error   Problem processing: freebase edu.stanford.nlp.sempre.freebase.BinaryLexicon Exception in thread ""main"" java.lang.RuntimeException: java.lang.ClassNotFoundException: edu.stanford.nlp.sempre.freebase.BinaryLexicon     at edu.stanford.nlp.sempre.Master.getOptionsParser(Master.java:443)     at edu.stanford.nlp.sempre.Main.main(Main.java:38) Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.sempre.freebase.BinaryLexicon     at java.net.URLClassLoader.findClass(URLClassLoader.java:381)     at java.lang.ClassLoader.loadClass(ClassLoader.java:424)     at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)     at java.lang.ClassLoader.loadClass(ClassLoader.java:357)     at java.lang.Class.forName0(Native Method)     at java.lang.Class.forName(Class.java:264)     at edu.stanford.nlp.sempre.Master.getOptionsParser(Master.java:437)     ... 1 more Command failed: fig/bin/qcreate java -ea -Dmodules=core,freebase -Xms8G -Xmx10G -cp libsempre/_:lib/_ edu.stanford.nlp.sempre.Main -execDir _OUTPATH_ -overwriteExecDir -addToView 0 -SparqlExecutor.endpointUrl   -FeatureExtractor.featureDomains basicStats alignmentScores entityFeatures context skipPos joinPos wordSim lexAlign tokenMatch rule opCount constant denotation whType span derivRank lemmaAndBinaries -Builder.executor freebase.SparqlExecutor -Builder.valueEvaluator freebase.FreebaseValueEvaluator -LanguageAnalyzer.languageAnalyzer corenlp.CoreNLPAnalyzer -LexiconFn.lexiconClassName edu.stanford.nlp.sempre.fbalignment.lexicons.Lexicon -BinaryLexicon.binaryLexiconFilesPath lib/fb_data/7/binaryInfoStringAndAlignment.txt -BinaryLexicon.keyToSortBy Intersection_size_typed -UnaryLexicon.unaryLexiconFilePath lib/fb_data/7/unaryInfoStringAndAlignment.txt -EntityLexicon.entityPopularityPath lib/fb_data/7/entityPopularity.txt -TypeInference.typeLookup freebase.FreebaseTypeLookup -FreebaseSearch.cachePath /u/nlp/data/semparse/scr/cache/fbsearch/1.cache -Dataset.inPaths train,lib/data/webquestions/dataset_11/webquestions.examples.train.json -Dataset.trainFrac 0.8 -Dataset.devFrac 0.2 -Grammar.inPaths freebase/data/emnlp2013.grammar -Parser.beamSize 200 -Lexicon.cachePath LexiconFn.cache -SparqlExecutor.cachePath SparqlExecutor.cache -FreebaseSearch.cachePath FreebaseSearch.cache -EntityLexicon.inexactMatchIndex lib/lucene/4.4/inexact/ -LexiconFn.maxEntityEntries 10 -Grammar.tags webquestions bridge join inject inexact -Learner.maxTrainIters 3 -BridgeFn.useBinaryPredicateFeatures true -BridgeFn.filterBadDomain true -Dataset.splitRandom 1  +) I read through forum and figured it usually takes 3+ days to train on webquestions. Is there any way to make the process faster?   Thank you! "
"How do you generate the following files given a new set of recipe phrases so I can train a new system: recipes-ppdb.txt, recipes.phrase_alignments, recipes.word_alignments.berkeley "
"line 841 of run script contains: sel(:gen, rlwrap, l()), when running ./run @mode=genovernight @gen=1 @domain=recipes @pooldir=1  -execDir genovernight.out/recipes errors with: Value 1 (for key :gen) is invalid; possible values are [0]  comment out line 841 and everything seems to run. "
"Hi,     I am trying to build a QA system for my project and would like to attach a custom database with SEMPRE but SEMPRE seems to have tight coupling with Freebase and working on Windows environment, it is difficult to set up the whole project and test the flow. Can SEMPRE be used with a custom Knowledge Graph, if yes, it would be great if you point the flow and direction.  Regards, Shrey "
"The following lambda DCS formula `(argmin 1 5 (rdf:type foaf:Person) (lambda x (count (foaf:knows (var x)))))` (aka ""who has the most friends?"") is translated by sempre to `SELECT DISTINCT ?x1 ?x2 WHERE { ?x1 rdf:type foaf:Person . { SELECT DISTINCT (count(DISTINCT ?x3) AS ?x1) ?x2 WHERE { ?x3 foaf:knows ?x2 } } . FILTER (?x1 != ?x2) } ORDER BY ?x2 LIMIT 5`  This is invalid because ?x2 is not grouped. Proper sparql would be `SELECT DISTINCT ?x1 ?x2 WHERE { ?x1 rdf:type foaf:Person . { SELECT DISTINCT (count(DISTINCT ?x3) AS ?x1) ?x2 WHERE { ?x3 foaf:knows ?x2 } GROUP BY ?x2 } . FILTER (?x1 != ?x2) } ORDER BY ?x2 LIMIT 5` "
"Hi,  How results for parasempre 1.0 are reproduced? I'm not sure how to interpret evaluation stats? For example in following stats, correct shows the % of accurate answers?  Evaluation stats for parsing_iter=1.test {             correct = 1/  > /1 (4)             oracle = 1/  > /1 (4)             partCorrect = 1/  > /1 (4)             partOracle = 1/  > /1 (4)             numCandidates = 146/  > /2000 (4)             parsedNumCandidates = 146/  > /2000 (4)             exec-cached = 1/  > /1 (3236)           } "
"The tutorial is pretty helpful. One problem I run into is that, in section 4, the command     returns an empty list instead of three items.   Digging into the code, I think the problem is sempre/freebase/scripts/virtuoso line 154,      I think the 'sed' command is trying to replace the namespace abbreviation of entity name if it is the object of the triple, for example it is trying to change `fb:en.california` to ` ` (an extra space before '>'). Because of this, triples with entities as objects are not added correctly to the database. So the query above returns an empty list.   Adding the space into the regular expression fixed the problem for me.     Just hope to confirm if my understanding and fix is correct. Thanks! "
"Hi,  Is there any quick ways to test the trained acl2014 system on my own picked webquestion testing dataset? Because there is no ""test"" mode in parasempre. Thanks! "
"I tried to run parasempre with following command  sudo ./parasempre @mode=train @sparqlserver=localhost:1111 @domain=free917 @cacheserver=local My virtuoso database is online on 1111.   It seemed to be working fine until it stopped at execution on following error. SparqlExecutor.execute: (!fb:music.artist.genre fb:en.dave_grohl) { ERROR: Server exception: java.net.SocketException: Unexpected end of file from server ERROR: java.lang.RuntimeException: java.lang.RuntimeException: java.net.SocketException: Unexpected end of file from server: fig.basic.Parallelizer.process(Unknown Source) edu.stanford.nlp.sempre.paraphrase.ParaphraseLearner.processParsingExamples(ParaphraseLearner.java:114) edu.stanford.nlp.sempre.paraphrase.ParaphraseLearner.learn(ParaphraseLearner.java:74) edu.stanford.nlp.sempre.paraphrase.ParaphraseLearner.learn(ParaphraseLearner.java:55) edu.stanford.nlp.sempre.paraphrase.ParaphraseMain.run(ParaphraseMain.java:36) fig.exec.Execution.runWithObjArray(Unknown Source) fig.exec.Execution.run(Unknown Source)  **On virtuoso server I received this error 'Malformed data received from IP [127.0.0.1] : Bad incoming tag 71. Disconnecting the client'**  It seems that virtuoso server is not SPARQL query, is there anything else I need to setup?? "
"So, in the tutorial, it says about installing Freebase. I cannot use up 60 GB of my disk space for the Freebase snapshot. Is there an alternative way? "
"Hi, Liang I think there is a bug in sempre1.0. you may try to fix it and see a significant accuracy improvement.  Location: src/edu/stanford/nlp/sempre/paraphrase/VectorSpaceModel.java in function computeSimilarity, line 133.   where you compute the similarity of  two sentence by dot_product their sentences's vectors. but the sentence's vector is simply the mean of all word's vector. (see function computeUtteranceVec() ) your algorithm:      sentence's vector = mean(sum(word's vector))  I think you forgot to normalize the sentence vector which should meet the condition: || sentence's vector|| === 1  sentence's similarity = sqrt {sentence1 \* sentence2 / ||sentence1\* sentence2 || } where  ||vector of sentence1 || ==1 ||vector of sentence1 || ==1       } "
java.lang.IndexOutOfBoundsException: fromIndex = -1     at java.util.ArrayList.subListRangeCheck(ArrayList.java:1002)     at java.util.ArrayList.subList(ArrayList.java:996)     at edu.stanford.nlp.sempre.Example.phraseString(Example.java:112)     at edu.stanford.nlp.sempre.Server$ExchangeState.makeDerivationHelper(Server.java:291)     at edu.stanford.nlp.sempre.Server$ExchangeState.makeDerivation(Server.java:272)     at edu.stanford.nlp.sempre.Server$ExchangeState.makeDetails(Server.java:259)     at edu.stanford.nlp.sempre.Server$ExchangeState.handleQuery(Server.java:644)     at edu.stanford.nlp.sempre.Server$ExchangeState. (Server.java:133)     at edu.stanford.nlp.sempre.Server$Handler.handle(Server.java:60)     at com.sun.net.      at sun.net.      at com.sun.net.      at sun.net.      at com.sun.net.      at sun.net.      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)     at java.lang.Thread.run(Thread.java:745) 
"Hi   would you please tell me for testing Paraphrase in WebQuestion,is this input is ok for Ryby runner: -> without cache and powerful posTagger    "
"When trying to access the dependencies, I get a 403 content forbidden. Are the files hosted elsewhere? "
"Hello,   I tried to understand and use the generation of overnight but got the following error.   ./run @mode=genovernight @gen=1 @domain=basketball /usr/bin/rlwrap /usr/bin/rlwrap Key :pooldir is not in environment {:gen=>1, :domain=>""basketball"", :mode=>""genovernight""}  Has somebody encountered the same problem pelase ? What does the error message mean ? Does this generation generate more than the canonical sentences in traning data normally ?  Thank you "
"Hello,  When I try to run this query: ./run @mode=query @sparqlserver=localhost:3001 -formula '(fb:location.location.containedby fb:en.california)'  I get the following error: Error: Could not find or load main class edu.stanford.nlp.sempre.freebase.SparqlExecutor Command failed: java -ea -cp libsempre/_:lib/_ edu.stanford.nlp.sempre.freebase.SparqlExecutor -SparqlExecutor.endpointUrl   -formula (fb:location.location.containedby fb:en.california)  And when I try to run simple-freebase mode: ./run @mode=simple-freebase @sparqlserver=localhost:3001  I get this error: Problem processing: freebase edu.stanford.nlp.sempre.freebase.BinaryLexicon Exception in thread ""main"" java.lang.RuntimeException: java.lang.ClassNotFoundException: edu.stanford.nlp.sempre.freebase.BinaryLexicon     at edu.stanford.nlp.sempre.Master.getOptionsParser(Master.java:443)     at edu.stanford.nlp.sempre.Main.main(Main.java:38) Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.sempre.freebase.BinaryLexicon     at java.net.URLClassLoader.findClass(URLClassLoader.java:381)     at java.lang.ClassLoader.loadClass(ClassLoader.java:424)     at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)     at java.lang.ClassLoader.loadClass(ClassLoader.java:357)     at java.lang.Class.forName0(Native Method)     at java.lang.Class.forName(Class.java:264)     at edu.stanford.nlp.sempre.Master.getOptionsParser(Master.java:437)     ... 1 more Command failed: java -Dmodules=core,freebase -cp libsempre/_:lib/_ -ea edu.stanford.nlp.sempre.Main -executor freebase.SparqlExecutor -SparqlExecutor.endpointUrl   -FreebaseSearch.cachePath FreebaseSearch.cache -EntityLexicon.mid2idPath freebase.cloudapp.net:4000:freebase-rdf-2013-06-09-00-00.canonical-id-map.gz -TypeInference.typeLookup freebase.FreebaseTypeLookup -FreebaseTypeLookup.entityTypesPath freebase.cloudapp.net:4000:freebase-rdf-2013-06-09-00-00.canonicalized.en-types.gz -EntityLexicon.maxEntries 2 -FeatureExtractor.featureDomains rule -Parser.coarsePrune -JoinFn.typeInference -UnaryLexicon.unaryLexiconFilePath /dev/null -BinaryLexicon.binaryLexiconFilesPath /dev/null -Grammar.inPaths freebase/data/demo1.grammar -SparqlExecutor.returnTable -Main.interactive  These errors persist despite running ./pull-dependencies freebase make freebase  Any thoughts? osmode "
Looks like Sempre is very tightly coupled with Freebase.  What if i don't want to use Freebase schema now that freebase is nearing end of its life     Any plan to decouple the tight integration with Freebase in the new future so that this can used with any other datasource ? 
Thanks in advance! 
"hello I ve been trying to reinstall my system,to use ""v1.0-fix"" but it gives me this problem (I ve installed everything like before,what said in Quickstart)  > Execution directory: state/execs/0.exec > java -ea -Xmx15g -cp classes:lib/lucene-core-4.4.0.jar:lib/joda-time.jar:lib/testng-6.8.5.jar:lib/fig.jar:lib/jackson-annotations-2.2.0.jar:lib/guava-14.0.1.jar:lib/jollyday.jar:lib/jackson-core-2.2.0.jar:lib/jcommander-1.30.jar:lib/stanford-corenlp-3.2.0-models.jar:lib/lucene-queryparser-4.4.0.jar:lib/jackson-databind-2.2.0.jar:lib/stanford-corenlp-3.2.0.jar:lib/stanford-corenlp-caseless-2013-06-07-models.jar:lib/lucene-analyzers-common-4.4.0.jar edu.stanford.nlp.sempre.Main -execDir state/execs/0.exec -overwriteExecDir -addToView 0 -LexiconFn.lexiconClassName edu.stanford.nlp.sempre.fbalignment.lexicons.Lexicon -SparqlExecutor.endpointUrl   -Builder.executor SparqlExecutor -BinaryLexicon.binaryLexiconFilesPath lib/fb_data/6/binaryInfoStringAndAlignment.txt -BinaryLexicon.keyToSortBy Intersection_size_typed -BinaryLexicon.useOnlyJaccard false -UnaryLexicon.unaryLexiconFilePath lib/fb_data/6/unaryInfoStringAndAlignment.txt -Dataset.inPaths train,lib/data/webquestions/dataset_11/webquestions.examples.train.json -Dataset.trainFrac 0.8 -Dataset.devFrac 0.2 -Grammar.inPaths data/emnlp2013.inexactent.grammar -Grammar.tags webquestions bridge join -BeamParser.beamSize 200 -FeatureExtractor.featureDomains basicStats alignmentScores tokensDistance context skipPos joinPos wordSim tokenMatch opCount constant denotation whType lexAlign -Dataset.splitRandom 1 -Lexicon.cachePath LexiconFn.cache -SparqlExecutor.cachePath SparqlExecutor.cache -EntityLexicon.inexactMatchIndex lib/lucene/4.4/inexact/ -EntityLexicon.maxEntries 50 -Learner.partialReward true -Learner.maxTrainIters 4 -Learner.maxTrainIters 0 -Dataset.inPaths test:testinput -Builder.inParamsPath lib/models/15.exec/params -Grammar.inPaths lib/models/15.exec/grammar -Dataset.readLispTreeFormat true > Picked up JAVA_TOOL_OPTIONS: -javaagent:/usr/share/java/jayatanaag.jar  > Error: Could not find or load main class edu.stanford.nlp.sempre.Main > Command failed: fig/bin/qcreate java -ea -Xmx15g -cp classes:lib/lucene-core-4.4.0.jar:lib/joda-time.jar:lib/testng-6.8.5.jar:lib/fig.jar:lib/jackson-annotations-2.2.0.jar:lib/guava-14.0.1.jar:lib/jollyday.jar:lib/jackson-core-2.2.0.jar:lib/jcommander-1.30.jar:lib/stanford-corenlp-3.2.0-models.jar:lib/lucene-queryparser-4.4.0.jar:lib/jackson-databind-2.2.0.jar:lib/stanford-corenlp-3.2.0.jar:lib/stanford-corenlp-caseless-2013-06-07-models.jar:lib/lucene-analyzers-common-4.4.0.jar edu.stanford.nlp.sempre.Main -execDir _OUTPATH_ -overwriteExecDir -addToView 0 -LexiconFn.lexiconClassName edu.stanford.nlp.sempre.fbalignment.lexicons.Lexicon -SparqlExecutor.endpointUrl   -Builder.executor SparqlExecutor -BinaryLexicon.binaryLexiconFilesPath lib/fb_data/6/binaryInfoStringAndAlignment.txt -BinaryLexicon.keyToSortBy Intersection_size_typed -BinaryLexicon.useOnlyJaccard false -UnaryLexicon.unaryLexiconFilePath lib/fb_data/6/unaryInfoStringAndAlignment.txt -Dataset.inPaths train,lib/data/webquestions/dataset_11/webquestions.examples.train.json -Dataset.trainFrac 0.8 -Dataset.devFrac 0.2 -Grammar.inPaths data/emnlp2013.inexactent.grammar -Grammar.tags webquestions bridge join -BeamParser.beamSize 200 -FeatureExtractor.featureDomains basicStats alignmentScores tokensDistance context skipPos joinPos wordSim tokenMatch opCount constant denotation whType lexAlign -Dataset.splitRandom 1 -Lexicon.cachePath LexiconFn.cache -SparqlExecutor.cachePath SparqlExecutor.cache -EntityLexicon.inexactMatchIndex lib/lucene/4.4/inexact/ -EntityLexicon.maxEntries 50 -Learner.partialReward true -Learner.maxTrainIters 4 -Learner.maxTrainIters 0 -Dataset.inPaths test:testinput -Builder.inParamsPath lib/models/15.exec/params -Grammar.inPaths lib/models/15.exec/grammar -Dataset.readLispTreeFormat true "
"Hi, I would like to use SEMPRE for semantic parsing on my own database/graph which is not related to Freebase. I tried to change the tutorial.ttl, tutorial.grammar and tutorial.lexicon files to fit a toy example of my graph but run into several errors.  The changes I tried are removing anything related to freebase such as the line:   @prefix fb: <  (could you explain the meaning of this line?) and special prefixes ""fb:"",""en."" etc from the tutorial.ttl file.  What changes should I be making instead?  Thanks! "
"Hi, What are the differences between the four modes: simple, simple-freebase, simple-sparql, query? Could you please give an example of a command in each mode? Thanks.  "
"Hello!  In some of my data, I need to convert the year, spoken out in words, into numbers. For example  ""two thousand fourteen"" --> 2014  ""nineteen eighty nine"" --> 1989  Looks like DataFn.java can only deal with number inputs.  I found this Stanford code    It seems to contain the things I want, but I don't know which function inside NumberNormalizer.java should I call?   I guess you guys must be familiar with the Stanford packages and code, if there could be any hint, that would be very helpful. Thanks so much! "
"Hi, i have installed sempre on Ubuntu OS. i have another windows based PC and i have installed virtuoso with freebase graph data fully uploaded.  kindly guide me how can i connect sempre to another pc virtuoso with freebase fully uploaded on that. "
"Hi,          I failed in running both Sempre and Parasempre on Free917 dataset:   In Parasempre,       my command:             ./parasempre @mode=train  @sparqlserver=localhost:3093  @domain=free917 @cacheserver=local  @data=1       error: ( I have changed the parameter ""-Xmx10g"" to ""-Xmx25g"" in parasempre.rb, but no help )           ERROR: java.lang.RuntimeException: java.lang.OutOfMemoryError: GC overhead limit exceeded: fig.basic.Parallelizer.process(Unknown Source) edu.stanford.nlp.sempre.paraphrase.ParaphraseLearner.processParsingExamples(ParaphraseLearner.java:114) edu.stanford.nlp.sempre.paraphrase.ParaphraseLearner.learn(ParaphraseLearner.java:74) edu.stanford.nlp.sempre.paraphrase.ParaphraseLearner.learn(ParaphraseLearner.java:55) edu.stanford.nlp.sempre.paraphrase.ParaphraseMain.run(ParaphraseMain.java:36) fig.exec.Execution.runWithObjArray(Unknown Source) fig.exec.Execution.run(Unknown Source) edu.stanford.nlp.sempre.paraphrase.ParaphraseMain.main(ParaphraseMain.java:90)             ERROR: Caused by java.lang.OutOfMemoryError: GC overhead limit exceeded:   In Sempre,       my command:             ./sempre @mode=train @sparqlserver=localhost:3093  @domain=free917 @cacheserver=local @data=1       error: ( occured in the 2nd trainning iteration )               BeamParser.pruneCell [$BaseSet 1:10[position does colby armstrong play on the toronto maple]]: 932871 entries               BeamParser.pruneCell [$BaseSet 0:10[what position does colby armstrong play on the toronto maple]]: 1041159 entries               ERROR: XML: <sparql xmlns=""                ERROR: XML: <sparql xmlns=""                ERROR: XML: <sparql xmlns=""   Thanks in advance! "
"I see this issue is already open.    My command: ./run @mode=freebase @domain=webquestions @sparqlserver=localhost:3093 @cacheserver=remote  Error message: undefined local variable or method `agendaExperiments'  So, my question - Is there a workaround? I wanted to run the system with the most recent algorithm.  Thanks. "
"Hi,  I'd like to benchmark sempre and parasempre with a new set of ground-truth questions to evaluate the accuracy and runtime of the systems. I'm using sempre 1.0 so that I can reproduce the results in the papers. My question is concerned with the parameters to run the systems, and it consists of two parts: 1. If I test the systems with pre-trained models (e.g., the model trained on WebQuestions), what's the most appropriate configuration for each system in order for them to achieve their best performance (in both accuracy and runtime). 2. If I split the new question set into a training set and a test set, what's the best configuration for each system then for both training and testing?   Thanks in advance for taking care of the issue, and feel free to let me know if there is any confusion in the question. "
"Hello,  Thank you very much for sharing with us this great project.  I have some difficulties with installation process.  When I run 'make core' if fails with a number of error messages  cd src/edu/stanford/nlp/sempre && ant compile Buildfile: .../sempre/src/edu/stanford/nlp/sempre/build.xml  compile:     [javac] Compiling 119 source files to .../sempre/classes     [javac] warning: Supported source version 'RELEASE_6' from annotation processor 'org.antlr.v4.runtime.misc.NullUsageProcessor' less than -source '1.8'     [javac] .../sempre/src/edu/stanford/nlp/sempre/AbstractReinforcementParserState.java:4: error: package fig.basic does not exist     [javac] import fig.basic.LogInfo;     [javac]                 ^     [javac] .../sempre/src/edu/stanford/nlp/sempre/AbstractReinforcementParserState.java:5: error: package fig.basic does not exist     [javac] import fig.basic.MapUtils;     [javac]                 ^     [javac] .../sempre/src/edu/stanford/nlp/sempre/AbstractReinforcementParserState.java:6: error: package fig.basic does not exist     [javac] import fig.basic.StopWatchSet;     [javac]                 ^  First I tried to solve the problem with  'fig'. I've added all possible 'CLASSPATH' values.   'export CLASSPATH=$CLASSPATH:$FIG_HOME/fig.jar'  But it doesn't help. I don't understand what's the problem. Thank you.  java-1.8.0-openjdk-1.8.0.45-30.b13.el7_1.x86_64 Apache Ant(TM) version 1.9.4 compiled on April 29 2014 ruby 2.0.0p598 (2014-11-13) [x86_64-linux] "
"Hi folks  I have been tried to retrain the EMNLP 2013(in webQuestions db). It took for 4 days to read 453 samples of 3084.(but you said 3 days for complete) I have core i7,16G ram,ubuntu 15.04  Would you please tell me,where my problem is? I think I dont use of all my computer.so How can I? "
"Hi ,  After obtaining core and running make, I get Error: Could not find or load main class src.edu.stanford.nlp.sempre.Main after running java -Xmx3g -cp classes:lib/\* src.edu.stanford.nlp.sempre.Main -executor JavaExecutor -interactive.   Did I miss out anything?  Cheers, Sky "
"Hi, Liang I used Sempre1.0 paraphrase for test, and I only used the alignment and VSM features, no paraphrase features was adoped, such as ""Denotation Features"", ""Formula Features"", ""Wh- type "" ""NER""  When I train the model I find that the generated param file was too small , So I tried to comment these code, in Params.java , function: private void clipUpdate(String f, double update) comments weights.remove(f);     if(currWeight*(currWeight+update)<0.0)  {       //weights.remove(f);     }  The result is: the generated param file was large(2.6M), and the precision was improved significantly!!  So ,do I over fit the model when I do this?  are there any other negative effect I didn't noticed? "
"Hi all,  Using Sempre 1.0, I used the interact mode as per the quickstart tutorial.  For every logical form I execute, I get an empty (list) as the answer.  (execute fb:en.california)   SparqlExecutor.execute: fb:en.california   (list) (execute (fb:location.country.capital fb:en.france)   (list)  The same happens for all the sparql execution of derivations during training.  predvalue is just empty (list) as below:  utterance=what is the capital of france?    targetValue=(description Paris) predValue=(list)    predFormula=(!fb:location.country.capital fb:en.france) "
I'm using Sempre 2.0 to train the webquestions dataset and running the below command:  ./run @mode=freebase @domain=webquestions @train=1 @sparqlserver=localhost:3001 @cacheserver=local  But I'm getting the below error: ./run:268:in `block in  ': undefined local variable or method`agendaExperiments' for main:Object (NameError)     from /home/sempre/fig/lib/execrunner.rb:130:in `call'     from /home/sempre/fig/lib/execrunner.rb:130:in`getRuns'     from /home/sempre/fig/lib/execrunner.rb:96:in `getRuns'     from /home/sempre/fig/lib/execrunner.rb:96:in`getRuns'     from /home/sempre/fig/lib/execrunner.rb:130:in `getRuns'     from /home/sempre/fig/lib/execrunner.rb:205:in`execute'     from /home/sempre/fig/lib/execrunner.rb:215:in `run!'     from ./run:426:in` '  I cannot find a definition for agendaExperiments in run. Can someone please help with this! 
"Hi, I am trying to train SEMPRE using the webquestions dataset. But I cannot find the command for it. Some old issue posted mentions this as follows:  ./parasempre @mode=train @domain=webquestions @sparqlserver=localhost:3093 @cacheserver=local -ParaphraseLearner.inferOnly -ParaphraseDataset.parsingInPaths test,  -ParaphraseLearner.maxTrainIters 1  However, I cannot find this mode  @mode=train  while running the sempre. Can someone guide me regarding how the command should be? "
"Hi, Liang I didn't find any demo on how to train a json-format dateset in the package ""edu.stanford.nlp.sempre.test"". I would like to train a new model from the start with the dataset ""free917.train.examples.canonicalized.json"" and evalualte the accuracy with ""free917.test.examples.canonicalized.json"".  how can I do this, can you supply the command line to me?  "
What does (string !type) means in some of the grammar rules?
"Hi I'm using ruby 2.2, ant 1.8.2, java 8.  I followed following steps  1) rm -rf fig/ lib/  2) ruby pull_dependencies core  3) ant core  4) ruby run @mode=genovernight @gen=1 @domain=publications  But this gives me error ""Error: Could not find or load main class edu.stanford.nlp.sempre.overnight.GenerationMain""    So to fix that I run the following    5)ruby pull_dependencies overnight  6) ant overnight  But 6) gives me javac errors.  Any help will be highly appreciated. I'm a novice in java and such things fly above my brain.  @ppasupat "
"I have a text file of predicted canonical forms from which I want to compute the denotation accuracy. The canonical forms in this file are predicted by a simplification model that simplifies general utterances into a fixed canonical form so that they can then be parsed by the grammar rules. So now, I need to run the fixed grammar rules to parse the predicted canonical utterances and then execute the logical forms to get the denotation accuracy. What commands should I run for this?"
"To reproduce the problem, I was trying to run on a limited grammar rules on the corenlp mode       with the following simplified version of emnlp2013 grammar:       The exact rules for `$Entity` will give me the followinig error:                   while the inexact rules will give me the followinig error:                       **Questions**  1. What should be in `lib/lucene/4.4/inexact`? Can I generate a segment file using lucene?  2. I see that there is a line in the `class Options` that does not initialize the path for `""exact""`, and there is no other place this value has been set. Is that intentional? Or is that a bug needs to be fixed?   "
"When execute     and then       I encountered an parser error, followed by an message saying one of the folder `lucene/` does not exist, which should have been downloaded in the dependency:                      Indeed, I found the following error silently slipped away in the `pull-dependency` log:                 1. Shall we fetch the `lucene` directly from the Apache archive?   2. It seems there should be a folder `lucene/inexact` when we specify the `inexact` mode. Is it created at runtime (by sempre)? "
"Hi,  shouldn't the last value of row 1 be generalized  ` `  instead of  ` `?    (Either that or I'm getting the principle wrong)    thanks"
"Hello,  the tutorial says     > fb:en.california is the canonical Freebase ID (always beginning with the prefix fb:)    Do I understand correctly that this prefix is part of the SPARQL syntax referring to Freebase (and not part of the Freebase syntax itself)?    thanks  "
"The `fig` commit `8d35254` changes the behavior of `execrunner.rb`, which breaks the run scripts."
"Hi all,  I was using sempre as part of a bigger project fine until today. Updating the dependencies with `./pull-dependencies core interactive` makes the run script in the `interactive` directory crash.  The bug is reproducible with the latest master with the following steps:         The last command is expected to show the sempre REPL, where it fails with an exception: `1 arguments required for Server.port, but got 0`.    Full output:   "
"Hi,  I am getting the sparql expressions in the log file. But is there any way to get them in a text file in an organised manner.    This is the command that I am using:  ./run @mode=simple-freebase-nocache @sparqlserver=localhost:3001 -Grammar.inPaths dbpedia/dbpedia.grammar -SimpleLexicon.inPaths dbpedia/output.lexicon -SparqlExecutor.verbose 10  -FeatureExtractor.featureDomains rule opCount constant whType lemmaAndBinaries denotation lexAlign joinPos skipPos -Dataset.inPaths test:dbpedia/que.json -Learner.maxTrainIters 0 -exec.execDir dbpedia/out/  Please help me to get the sparql expression for each top predicted candidate in an output file just like how we get top predicted derivation for each utterance"
"Hi,  I have a lexicon file which contains different mapping for a same lexeme, for example,  lexeme: Obama formula: fb:en.Obama  lexeme: Obama formula: fb:en.Obama(city)  How to assign different probabilities to the predicted derivations while training.  This command doesn't seem to work:    ./run @mode=simple -Grammar.inPaths data/tutorial-freebase.grammar -SimpleLexicon.inPaths data/tutorial-freebase.lexicon -FeatureExtractor.featureDomains rule -Dataset.inPaths train:data/tutorial-freebase.examples -Learner.maxTrainIters 3    Even after training, equal probabilities are assigned to both the derivations.  In the tutorial they allowed disambiguation in grammar file, but my problem is disambiguation in the lexeme file. Do we have any command for that? Or any modification that needs to be done in the lexeme file, like assign any feature to the lexeme?  Thanks in advance!!"
"Hello,    When I try to run this command:  ./run @mode=simple-freebase-nocache @sparqlserver=localhost:3001  and write (execute fb:en.california) in the prompt, I am getting an empty list.  Output-   SparqlExecutor.execute: fb:en.california     (list)  I have pulled all the dependencies. Please help me solve the issue. Thanks!"
"Here is the compile error output when ant freebase is used . Please resolve . Thanks in advance  Buildfile: /home/ritik/rashi/build.xml    init:       [exec] Wrote modules with 328 files to module-classes.txt: cache core corenlp cprune freebase geo880 interactive overnight tables    core:       [echo] Compiling semparse: core      [javac] Compiling 134 source files to /home/ritik/rashi/classes      [javac] Note: Some input files use unchecked or unsafe operations.      [javac] Note: Recompile with -Xlint:unchecked for details.        [jar] Building jar: /home/ritik/rashi/libsempre/sempre-core.jar    cache:       [echo] Compiling semparse: cache      [mkdir] Created dir: /home/ritik/rashi/classes/cache      [javac] Compiling 8 source files to /home/ritik/rashi/classes/cache        [jar] Building jar: /home/ritik/rashi/libsempre/sempre-cache.jar    freebase:       [echo] Compiling semparse: freebase      [mkdir] Created dir: /home/ritik/rashi/classes/freebase      [javac] Compiling 55 source files to /home/ritik/rashi/classes/freebase      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/BinaryLexicon.java:4: error: package edu.stanford.nlp.io does not exist      [javac] import edu.stanford.nlp.io.IOUtils;      [javac]                           ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/FbFormulasInfo.java:4: error: package edu.stanford.nlp.stats does not exist      [javac] import edu.stanford.nlp.stats.ClassicCounter;      [javac]                              ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/FbFormulasInfo.java:5: error: package edu.stanford.nlp.stats does not exist      [javac] import edu.stanford.nlp.stats.Counter;      [javac]                              ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/lexicons/LexicalEntry.java:9: error: package edu.stanford.nlp.stats does not exist      [javac] import edu.stanford.nlp.stats.ClassicCounter;      [javac]                              ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/lexicons/LexicalEntry.java:10: error: package edu.stanford.nlp.stats does not exist      [javac] import edu.stanford.nlp.stats.Counter;      [javac]                              ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/lexicons/LexicalEntry.java:11: error: package edu.stanford.nlp.util does not exist      [javac] import edu.stanford.nlp.util.StringUtils;      [javac]                             ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/TextToTextMatcher.java:5: error: package edu.stanford.nlp.stats does not exist      [javac] import edu.stanford.nlp.stats.ClassicCounter;      [javac]                              ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/TextToTextMatcher.java:6: error: package edu.stanford.nlp.stats does not exist      [javac] import edu.stanford.nlp.stats.Counter;      [javac]                              ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/TextToTextMatcher.java:7: error: package edu.stanford.nlp.stats does not exist      [javac] import edu.stanford.nlp.stats.Counters;      [javac]                              ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/Stemmer.java:3: error: package org.tartarus.snowball.ext does not exist      [javac] import org.tartarus.snowball.ext.PorterStemmer;      [javac]                                 ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/TextToTextMatcher.java:126: error: cannot find symbol      [javac]   private void addFeaturesToVector(Counter  features, String prefix, FeatureVector vector) {      [javac]                                    ^      [javac]   symbol:   class Counter      [javac]   location: class TextToTextMatcher      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/EntityLexicon.java:3: error: package edu.stanford.nlp.io does not exist      [javac] import edu.stanford.nlp.io.IOUtils;      [javac]                           ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/EntityLexicon.java:14: error: package edu.stanford.nlp.stats does not exist      [javac] import edu.stanford.nlp.stats.Counter;      [javac]                              ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/EntityLexicon.java:15: error: package edu.stanford.nlp.util does not exist      [javac] import edu.stanford.nlp.util.ArrayUtils;      [javac]                             ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/EntityLexicon.java:16: error: package edu.stanford.nlp.util does not exist      [javac] import edu.stanford.nlp.util.StringUtils;      [javac]                             ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/EntityLexicon.java:19: error: package org.apache.lucene.document does not exist      [javac] import org.apache.lucene.document.Document;      [javac]                                  ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/EntityLexicon.java:20: error: package org.apache.lucene.queryparser.classic does not exist      [javac] import org.apache.lucene.queryparser.classic.ParseException;      [javac]                                             ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntitySearcher.java:5: error: package org.apache.lucene.analysis.core does not exist      [javac] import org.apache.lucene.analysis.core.KeywordAnalyzer;      [javac]                                       ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntitySearcher.java:6: error: package org.apache.lucene.analysis.standard does not exist      [javac] import org.apache.lucene.analysis.standard.StandardAnalyzer;      [javac]                                           ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntitySearcher.java:7: error: package org.apache.lucene.document does not exist      [javac] import org.apache.lucene.document.Document;      [javac]                                  ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntitySearcher.java:8: error: package org.apache.lucene.index does not exist      [javac] import org.apache.lucene.index.DirectoryReader;      [javac]                               ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntitySearcher.java:9: error: package org.apache.lucene.index does not exist      [javac] import org.apache.lucene.index.IndexReader;      [javac]                               ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntitySearcher.java:10: error: package org.apache.lucene.queryparser.classic does not exist      [javac] import org.apache.lucene.queryparser.classic.ParseException;      [javac]                                             ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntitySearcher.java:11: error: package org.apache.lucene.queryparser.classic does not exist      [javac] import org.apache.lucene.queryparser.classic.QueryParser;      [javac]                                             ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntitySearcher.java:12: error: package org.apache.lucene.search does not exist      [javac] import org.apache.lucene.search.IndexSearcher;      [javac]                                ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntitySearcher.java:13: error: package org.apache.lucene.search does not exist      [javac] import org.apache.lucene.search.Query;      [javac]                                ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntitySearcher.java:14: error: package org.apache.lucene.search does not exist      [javac] import org.apache.lucene.search.ScoreDoc;      [javac]                                ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntitySearcher.java:15: error: package org.apache.lucene.store does not exist      [javac] import org.apache.lucene.store.SimpleFSDirectory;      [javac]                               ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntitySearcher.java:16: error: package org.apache.lucene.util does not exist      [javac] import org.apache.lucene.util.Version;      [javac]                              ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/EntityLexicon.java:64: error: cannot find symbol      [javac]   public List  lookupEntries(String query, SearchStrategy strategy) throws ParseException, IOException {      [javac]                                                                                               ^      [javac]   symbol:   class ParseException      [javac]   location: class EntityLexicon      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/EntityLexicon.java:122: error: cannot find symbol      [javac]   public List  lookupEntries(FbEntitySearcher searcher, String textDesc) throws ParseException, IOException {      [javac]                                                                                                    ^      [javac]   symbol:   class ParseException      [javac]   location: class EntityLexicon      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntitySearcher.java:28: error: cannot find symbol      [javac]   private final QueryParser queryParser;      [javac]                 ^      [javac]   symbol:   class QueryParser      [javac]   location: class FbEntitySearcher      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntitySearcher.java:29: error: cannot find symbol      [javac]   private final IndexSearcher indexSearcher;      [javac]                 ^      [javac]   symbol:   class IndexSearcher      [javac]   location: class FbEntitySearcher      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntitySearcher.java:53: error: cannot find symbol      [javac]   public synchronized List  searchDocs(String question) throws IOException, ParseException {      [javac]                            ^      [javac]   symbol:   class Document      [javac]   location: class FbEntitySearcher      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntitySearcher.java:53: error: cannot find symbol      [javac]   public synchronized List  searchDocs(String question) throws IOException, ParseException {      [javac]                                                                                      ^      [javac]   symbol:   class ParseException      [javac]   location: class FbEntitySearcher      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntitySearcher.java:69: error: cannot find symbol      [javac]   private ScoreDoc[] getHits(String question) throws IOException, ParseException {      [javac]           ^      [javac]   symbol:   class ScoreDoc      [javac]   location: class FbEntitySearcher      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntitySearcher.java:69: error: cannot find symbol      [javac]   private ScoreDoc[] getHits(String question) throws IOException, ParseException {      [javac]                                                                   ^      [javac]   symbol:   class ParseException      [javac]   location: class FbEntitySearcher      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntitySearcher.java:75: error: cannot find symbol      [javac]   public static void main(String[] args) throws IOException, ParseException {      [javac]                                                              ^      [javac]   symbol:   class ParseException      [javac]   location: class FbEntitySearcher      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/lexicons/LexicalEntry.java:187: error: cannot find symbol      [javac]     public Counter  entityFeatures;      [javac]            ^      [javac]   symbol:   class Counter      [javac]   location: class EntityLexicalEntry      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/lexicons/LexicalEntry.java:191: error: cannot find symbol      [javac]                               Counter  entityFeatures) {      [javac]                               ^      [javac]   symbol:   class Counter      [javac]   location: class EntityLexicalEntry      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/FbFormulasInfo.java:352: error: cannot find symbol      [javac]     private Counter  fCounter;      [javac]             ^      [javac]   symbol:   class Counter      [javac]   location: class FbFormulasInfo.FormulaByCounterComparator      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/FbFormulasInfo.java:354: error: cannot find symbol      [javac]     public FormulaByCounterComparator(Counter  fCounter) {      [javac]                                       ^      [javac]   symbol:   class Counter      [javac]   location: class FbFormulasInfo.FormulaByCounterComparator      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/Free917Converter.java:3: error: package edu.stanford.nlp.io does not exist      [javac] import edu.stanford.nlp.io.IOUtils;      [javac]                           ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/Free917Converter.java:9: error: package edu.stanford.nlp.stats does not exist      [javac] import edu.stanford.nlp.stats.ClassicCounter;      [javac]                              ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/Free917Converter.java:10: error: package edu.stanford.nlp.stats does not exist      [javac] import edu.stanford.nlp.stats.Counter;      [javac]                              ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/Free917Converter.java:11: error: package edu.stanford.nlp.util does not exist      [javac] import edu.stanford.nlp.util.StringUtils;      [javac]                             ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/Free917Converter.java:30: error: cannot find symbol      [javac]   private Counter  argnumCounter = new ClassicCounter ();      [javac]           ^      [javac]   symbol:   class Counter      [javac]   location: class Free917Converter      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/Lexicon.java:10: error: package org.apache.lucene.queryparser.classic does not exist      [javac] import org.apache.lucene.queryparser.classic.ParseException;      [javac]                                             ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/UnaryLexicon.java:4: error: package edu.stanford.nlp.io does not exist      [javac] import edu.stanford.nlp.io.IOUtils;      [javac]                           ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/Lexicon.java:62: error: cannot find symbol      [javac]   public List  lookupEntities(String query, EntityLexicon.SearchStrategy strategy) throws IOException, ParseException {      [javac]                                                                                                                               ^      [javac]   symbol:   class ParseException      [javac]   location: class Lexicon      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/LexiconFn.java:8: error: package org.apache.lucene.queryparser.classic does not exist      [javac] import org.apache.lucene.queryparser.classic.ParseException;      [javac]                                             ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntityIndexer.java:3: error: package edu.stanford.nlp.io does not exist      [javac] import edu.stanford.nlp.io.IOUtils;      [javac]                           ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntityIndexer.java:5: error: package org.apache.lucene.analysis.core does not exist      [javac] import org.apache.lucene.analysis.core.KeywordAnalyzer;      [javac]                                       ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntityIndexer.java:6: error: package org.apache.lucene.analysis.standard does not exist      [javac] import org.apache.lucene.analysis.standard.StandardAnalyzer;      [javac]                                           ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntityIndexer.java:7: error: package org.apache.lucene.document does not exist      [javac] import org.apache.lucene.document.*;      [javac] ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntityIndexer.java:8: error: package org.apache.lucene.index does not exist      [javac] import org.apache.lucene.index.IndexWriter;      [javac]                               ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntityIndexer.java:9: error: package org.apache.lucene.index does not exist      [javac] import org.apache.lucene.index.IndexWriterConfig;      [javac]                               ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntityIndexer.java:10: error: package org.apache.lucene.index.IndexWriterConfig does not exist      [javac] import org.apache.lucene.index.IndexWriterConfig.OpenMode;      [javac]                                                 ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntityIndexer.java:11: error: package org.apache.lucene.store does not exist      [javac] import org.apache.lucene.store.SimpleFSDirectory;      [javac]                               ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntityIndexer.java:12: error: package org.apache.lucene.util does not exist      [javac] import org.apache.lucene.util.Version;      [javac]                              ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/index/FbEntityIndexer.java:20: error: cannot find symbol      [javac]   private final IndexWriter indexer;      [javac]                 ^      [javac]   symbol:   class IndexWriter      [javac]   location: class FbEntityIndexer      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/lexicons/LexicalEntry.java:251: error: cannot find symbol      [javac]     static Counter  counterFromLispTree(LispTree tree) {      [javac]            ^      [javac]   symbol:   class Counter      [javac]   location: class LexicalEntrySerializer      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/lexicons/LexicalEntry.java:257: error: cannot find symbol      [javac]     static LispTree counterToLispTree(Counter  counter) {      [javac]                                       ^      [javac]   symbol:   class Counter      [javac]   location: class LexicalEntrySerializer      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/lexicons/TokenLevelMatchFeatures.java:4: error: package edu.stanford.nlp.stats does not exist      [javac] import edu.stanford.nlp.stats.ClassicCounter;      [javac]                              ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/lexicons/TokenLevelMatchFeatures.java:5: error: package edu.stanford.nlp.stats does not exist      [javac] import edu.stanford.nlp.stats.Counter;      [javac]                              ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/lexicons/TokenLevelMatchFeatures.java:36: error: cannot find symbol      [javac]   public static Counter  extractFeatures(String query, String answer) {      [javac]                 ^      [javac]   symbol:   class Counter      [javac]   location: class TokenLevelMatchFeatures      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/lexicons/TokenLevelMatchFeatures.java:181: error: cannot find symbol      [javac]   public static Counter  extractTokenMatchFeatures(List  source, List  target, boolean strict) {      [javac]                 ^      [javac]   symbol:   class Counter      [javac]   location: class TokenLevelMatchFeatures      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/lexicons/normalizers/BinaryNormalizer.java:3: error: package edu.stanford.nlp.util does not exist      [javac] import edu.stanford.nlp.util.ArrayUtils;      [javac]                             ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/lexicons/normalizers/PrepDropNormalizer.java:3: error: package edu.stanford.nlp.util does not exist      [javac] import edu.stanford.nlp.util.ArrayUtils;      [javac]                             ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/test/TokenMatchTest.java:4: error: package edu.stanford.nlp.stats does not exist      [javac] import edu.stanford.nlp.stats.Counter;      [javac]                              ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/utils/FileUtils.java:6: error: package edu.stanford.nlp.io does not exist      [javac] import edu.stanford.nlp.io.IOUtils;      [javac]                           ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/utils/FileUtils.java:7: error: package edu.stanford.nlp.objectbank does not exist      [javac] import edu.stanford.nlp.objectbank.ObjectBank;      [javac]                                   ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/utils/FileUtils.java:12: error: package edu.stanford.nlp.stats does not exist      [javac] import edu.stanford.nlp.stats.ClassicCounter;      [javac]                              ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/utils/FileUtils.java:13: error: package edu.stanford.nlp.stats does not exist      [javac] import edu.stanford.nlp.stats.Counter;      [javac]                              ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/utils/FileUtils.java:14: error: package edu.stanford.nlp.util does not exist      [javac] import edu.stanford.nlp.util.StringUtils;      [javac]                             ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/utils/FileUtils.java:180: error: cannot find symbol      [javac]   public static Counter  loadStringCounter(String filename) {      [javac]                 ^      [javac]   symbol:   class Counter      [javac]   location: class FileUtils      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/utils/FreebaseUtils.java:5: error: package edu.stanford.nlp.io does not exist      [javac] import edu.stanford.nlp.io.IOUtils;      [javac]                           ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/utils/LinkedExtractionFileUtils.java:3: error: package edu.stanford.nlp.io does not exist      [javac] import edu.stanford.nlp.io.IOUtils;      [javac]                           ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/utils/MathUtils.java:3: error: package edu.stanford.nlp.stats does not exist      [javac] import edu.stanford.nlp.stats.ClassicCounter;      [javac]                              ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/utils/MathUtils.java:4: error: package edu.stanford.nlp.stats does not exist      [javac] import edu.stanford.nlp.stats.Counter;      [javac]                              ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/utils/MathUtils.java:5: error: package edu.stanford.nlp.stats does not exist      [javac] import edu.stanford.nlp.stats.Counters;      [javac]                              ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/utils/MathUtils.java:19: error: cannot find symbol      [javac]   public static   double generalizedJensenShannonDivergence(Counter  c1, Counter  c2) {      [javac]                                                               ^      [javac]   symbol:   class Counter      [javac]   location: class MathUtils      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/utils/MathUtils.java:19: error: cannot find symbol      [javac]   public static   double generalizedJensenShannonDivergence(Counter  c1, Counter  c2) {      [javac]                                                                              ^      [javac]   symbol:   class Counter      [javac]   location: class MathUtils      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/utils/MathUtils.java:46: error: cannot find symbol      [javac]   public static Counter  prefixCounterKeys(Counter  counter, String prefix) {      [javac]                                                   ^      [javac]   symbol:   class Counter      [javac]   location: class MathUtils      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/utils/MathUtils.java:46: error: cannot find symbol      [javac]   public static Counter  prefixCounterKeys(Counter  counter, String prefix) {      [javac]                 ^      [javac]   symbol:   class Counter      [javac]   location: class MathUtils      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/utils/SemparseLogTools.java:3: error: package edu.stanford.nlp.io does not exist      [javac] import edu.stanford.nlp.io.IOUtils;      [javac]                           ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/utils/WordNet.java:28: error: package edu.stanford.nlp.io does not exist      [javac] import edu.stanford.nlp.io.RuntimeIOException;      [javac]                           ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/utils/WordNet.java:29: error: package edu.stanford.nlp.objectbank does not exist      [javac] import edu.stanford.nlp.objectbank.ObjectBank;      [javac]                                   ^      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/FbFormulasInfo.java:343: error: cannot find symbol      [javac]     Counter  counter = new ClassicCounter ();      [javac]     ^      [javac]   symbol:   class Counter      [javac]   location: class FbFormulasInfo      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/FbFormulasInfo.java:343: error: cannot find symbol      [javac]     Counter  counter = new ClassicCounter ();      [javac]                                    ^      [javac]   symbol:   class ClassicCounter      [javac]   location: class FbFormulasInfo      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/lexicons/LexicalEntry.java:69: error: cannot find symbol      [javac]       int currDistance = StringUtils.editDistance(query, description.toLowerCase());      [javac]                          ^      [javac]   symbol:   variable StringUtils      [javac]   location: class LexicalEntry      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/lexicons/LexicalEntry.java:252: error: cannot find symbol      [javac]       Counter  counter = new ClassicCounter ();      [javac]       ^      [javac]   symbol:   class Counter      [javac]   location: class LexicalEntrySerializer      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/lexicons/LexicalEntry.java:252: error: cannot find symbol      [javac]       Counter  counter = new ClassicCounter ();      [javac]                                     ^      [javac]   symbol:   class ClassicCounter      [javac]   location: class LexicalEntrySerializer      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/lexicons/LexicalEntry.java:317: error: cannot find symbol      [javac]         Counter  tokenEditDistanceFeatures = counterFromLispTree(tree.child(i++));      [javac]         ^      [javac]   symbol:   class Counter      [javac]   location: class LexicalEntrySerializer      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/TextToTextMatcher.java:82: error: cannot find symbol      [javac]     Counter  tokenFeatures = new ClassicCounter ();      [javac]     ^      [javac]   symbol:   class Counter      [javac]   location: class TextToTextMatcher      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/TextToTextMatcher.java:82: error: cannot find symbol      [javac]     Counter  tokenFeatures = new ClassicCounter ();      [javac]                                         ^      [javac]   symbol:   class ClassicCounter      [javac]   location: class TextToTextMatcher      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/TextToTextMatcher.java:83: error: cannot find symbol      [javac]     Counter  stemFeatures = new ClassicCounter ();      [javac]     ^      [javac]   symbol:   class Counter      [javac]   location: class TextToTextMatcher      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/TextToTextMatcher.java:83: error: cannot find symbol      [javac]     Counter  stemFeatures = new ClassicCounter ();      [javac]                                        ^      [javac]   symbol:   class ClassicCounter      [javac]   location: class TextToTextMatcher      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/TextToTextMatcher.java:90: error: cannot find symbol      [javac]       Counters.maxInPlace(tokenFeatures, TokenLevelMatchFeatures.extractTokenMatchFeatures(exampleTokens, fbDescTokens, true));      [javac]       ^      [javac]   symbol:   variable Counters      [javac]   location: class TextToTextMatcher      [javac] /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/TextToTextMatcher.java:91: error: cannot find symbol      [javac]       Counters.maxInPlace(tokenFeatures, TokenLevelMatchFeatures.extractTokenMatchFeatures(exampleLemmas, fbDescTokens, true));      [javac]       ^      [javac]   symbol:   variable Counters      [javac]   location: class TextToTextMatcher      [javac] Note: /home/ritik/rashi/src/edu/stanford/nlp/sempre/freebase/LambdaCalculusConverter.java uses unchecked or unsafe operations.      [javac] Note: Recompile with -Xlint:unchecked for details.      [javac] 100 errors    BUILD FAILED  /home/ritik/rashi/build.xml:65: Compile failed; see the compiler error output for details.    Total time: 2 seconds  "
"Hello, I'm new to Sempre, and I want to try and construct a grammar for robotic tasks. I'd like to add additional functions such as     move  turn  rotate  pause  stop  continue    in addition to the basic arithmetic formula made available to us. I was wondering, what is the best way to go about doing that?     Thank you!"
"After cloning it, I am not able to execute the command ""./pull-dependencies core"" it shows an error as ""."" is not recognized as an internal or external command. I am using java 8 and ant version 1.10.5. I even tried to change to .\ but got the same error. Also I tried to add a semicolon at the end of PATH variable in the Environment variable but of no use."
"Hi, I'm attempting to write a category to match utterances like ""at 5"" ""at 12"" but not ""at 37"" ""at four thousand""; i.e. ""at $n"" where 1<= n <= 24. I'm having trouble finding a representation that SEMPRE will accept. Will this require writing a custom module?    Here are several approaches that don't work:        # shared base grammar      (rule $ROOT ($Expr) (IdentityFn))      (rule $NUMBER_STRING ($PHRASE) (FilterNerSpanFn NUMBER))            # Attempt 1: this passes the interpreter, but fails at parse time with ""Method lessThanEq       # not found [...] with arguments [(NumberFn (string 5)), 24] having types       # [class edu.stanford.nlp.sempre.JoinFormula, class java.lang.Integer];""      (rule $at1 (at) (ConstantFn (lambda x (call if (call <= (NumberFn (var x)) (number 24)) (var x) (ConstantFn null)))))      (rule $Expr ($at1 $NUMBER_STRING) (JoinFn forward))            # Attempt 2: this passes the interpreter, but fails at parse time for numbers over 24 with       # ""Unhandled object: (ConstantFn null)""      (rule $at2a (at) (ConstantFn (lambda x (call if (call <= (var x) (number 24)) (var x) (ConstantFn null)))))      (rule $at2b ($NUMBER_STRING) (NumberFn))      (rule $Expr ($at2a $at2b) (JoinFn forward))            # Attempt 3: this doesn't pass the interpreter on $at3a, with ""Invalid value: (var x)""      (rule $at3a (at) (ConstantFn (lambda x (call select (list (var x)) (lambda y (call <= (var y) (number 24)))))))      (rule $at3b ($NUMBER_STRING) (NumberFn))      (rule $Expr ($at3a $at3b) (JoinFn forward))"
"There seems to be an issue with the  . In `extractSubpartsRecursive` helper function of `extractSubparts`, there is an issue with the first case of `AggregateFormula`. The `extractSupbpartsRecursive` is called again with the same `aggFormula`, causing infinite recursion. I think it should be called with `aggFormula.child`."
"This is to inform an issue that I found with   code. It compiles fine but raises the following error while running:      Exception in thread ""main"" java.lang.RuntimeException: java.lang.ClassNotFoundException: edu.stanford.nlp.sempre.cprune.CPruneDerivInfo    However, it works if I also compile,   `ant cprune`    Could you correct the build.xml or add this to the   instructions, so that it will be helpful for others who may try to run it."
"For my question answering project , I need to extract certain entities like name, location, organisation , etc and create a query out of it . The recognition part can be done by training a custom NER using Stanford's corenlp. The doubt I have is if my intent is narrow , then is it required to write a grammar and create the semantics  ? Can't I just create the semantics for a question by just checking if it has so and so entities and directly create one. For example , let's say my query is ""who knows John Doe from New York"" , then I can just create a semantic by just recognising the named entities . More precisely can't I train a classifier on annotated questions and then use it for classifying my utterance . Suppose I have 2 different types of questions , one regarding people and one regarding  company. Now I can train a classifier to classify between these 2 classes and then use that to create a query.    Is creating a grammar overkill here ?"
I can only get the parser to take interactive input. When I point stdin to a file the run script takes it as options. Output redirects ok. How can I get it to read from a file in the same way as it reads in the interactive prompt?
"After we restart sempre interactive mode, score which it learned last time cannot be kept.  Do we have any way to do this? thanks alot!"
"I ran the code and got results in a form like this:    (derivation (formula (min ((reverse fb:cell.cell.number) ((reverse fb:row.row.gf) (argmin (number 1) (number 1) (fb:type.object.type fb:type.row) (reverse (lambda r ((reverse fb:cell.cell.number) ((reverse fb:row.row.pk) (var r)))))))))) (""1 values"" (number 0)) (type fb:type.number))    I have a table and would like to know the step-by-step result of the query above.    For example, from the query above, I would like to know which cells are used as conditions (i.e. cells in column pk where the value is the minimum), which cells are selected (i.e. corresponding gf cells), and finally which cell is the final answer (i.e. the minimum among the gf cells).    I can read code, but it is hard for me to sift out which part of the enormous project should I scan to do this task. Is there a quick code to do this? If not, which class/es and method/s should I look at?    Thanks!"
I am getting following errors when I execute ant core :  !       
"I tried running theovernight samples for 2 domains - publication and geo880 and i get below error in both cases.    geo880  ======    ParserState.pruneCell $NP2:7: maxCellSize = 7440 entries (not pruned yet)                  **ERROR: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0:  java.util.ArrayList.rangeCheck(ArrayList.java:657)**  java.util.ArrayList.get(ArrayList.java:433)  edu.stanford.nlp.sempre.overnight.OvernightFeatureComputer.extractRootFeatures(OvernightFeatureComputer.java:185)  edu.stanford.nlp.sempre.overnight.OvernightFeatureComputer.extractLocal(OvernightFeatureComputer.java:70)  edu.stanford.nlp.sempre.FeatureExtractor.extractLocal(FeatureExtractor.java:71)  edu.stanford.nlp.sempre.ParserState.featurizeAndScoreDerivation(ParserState.java:74)  edu.stanford.nlp.sempre.FloatingParserState.addToChart(FloatingParser.java:208)  edu.stanford.nlp.sempre.FloatingParserState.applyRuleActual(FloatingParser.java:266)  edu.stanford.nlp.sempre.FloatingParserState.applyRule(FloatingParser.java:222)  edu.stanford.nlp.sempre.FloatingParserState.applyFloatingRule(FloatingParser.java:277)  edu.stanford.nlp.sempre.FloatingParserState.buildFloating(FloatingParser.java:460)  edu.stanford.nlp.sempre.FloatingParserState.access$500(FloatingParser.java:170)  edu.stanford.nlp.sempre.FloatingParserState$DerivationBuilder.run(FloatingParser.java:507)  edu.stanford.nlp.sempre.FloatingParserState.buildDerivations(FloatingParser.java:543)  edu.stanford.nlp.sempre.FloatingParserState.infer(FloatingParser.java:572)  edu.stanford.nlp.sempre.Parser.parse(Parser.java:170)  edu.stanford.nlp.sempre.Learner.parseExample(Learner.java:288)  edu.stanford.nlp.sempre.Learner.processExamples(Learner.java:199)  edu.stanford.nlp.sempre.Learner.learn(Learner.java:125)  edu.stanford.nlp.sempre.Learner.learn(Learner.java:90)  edu.stanford.nlp.sempre.Main.run(Main.java:27)  fig.exec.Execution.runWithObjArray(Execution.java:337)  fig.exec.Execution.run(Execution.java:325)  edu.stanford.nlp.sempre.Main.main(Main.java:50)    publications  ==========    Loading ppdb model {                    ParaphraseUtils.loadPhraseTable: number of entries=2149                  }                  **ERROR: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0:  java.util.ArrayList.rangeCheck(ArrayList.java:657)**  java.util.ArrayList.get(ArrayList.java:433)  edu.stanford.nlp.sempre.overnight.OvernightFeatureComputer.extractRootFeatures(OvernightFeatureComputer.java:185)  edu.stanford.nlp.sempre.overnight.OvernightFeatureComputer.extractLocal(OvernightFeatureComputer.java:70)  edu.stanford.nlp.sempre.FeatureExtractor.extractLocal(FeatureExtractor.java:71)  edu.stanford.nlp.sempre.ParserState.featurizeAndScoreDerivation(ParserState.java:74)  edu.stanford.nlp.sempre.FloatingParserState.addToChart(FloatingParser.java:208)  edu.stanford.nlp.sempre.FloatingParserState.applyRuleActual(FloatingParser.java:266)  edu.stanford.nlp.sempre.FloatingParserState.applyRule(FloatingParser.java:222)  edu.stanford.nlp.sempre.FloatingParserState.applyFloatingRule(FloatingParser.java:277)  edu.stanford.nlp.sempre.FloatingParserState.buildFloating(FloatingParser.java:460)  edu.stanford.nlp.sempre.FloatingParserState.access$500(FloatingParser.java:170)  edu.stanford.nlp.sempre.FloatingParserState$DerivationBuilder.run(FloatingParser.java:507)  edu.stanford.nlp.sempre.FloatingParserState.buildDerivations(FloatingParser.java:543)  edu.stanford.nlp.sempre.FloatingParserState.infer(FloatingParser.java:572)  edu.stanford.nlp.sempre.Parser.parse(Parser.java:170)  edu.stanford.nlp.sempre.Learner.parseExample(Learner.java:288)  edu.stanford.nlp.sempre.Learner.processExamples(Learner.java:199)  edu.stanford.nlp.sempre.Learner.learn(Learner.java:125)  edu.stanford.nlp.sempre.Learner.learn(Learner.java:90)  edu.stanford.nlp.sempre.Main.run(Main.java:27)  fig.exec.Execution.runWithObjArray(Execution.java:337)  fig.exec.Execution.run(Execution.java:325)  edu.stanford.nlp.sempre.Main.main(Main.java:50)    Please advice how can i go about debugging this.    Thanks."
"I am not able to figure out how i resolve this error - ""Unknown option: 'overnightderivationpruningcomputer.applyhardconstraints';""    I am trying to run subset of features for ablation studies  command  ./run @mode=overnight @domain=publications -OvernightFeatureComputer.featureDomains match skip-bigram root lf simpleworld    please advice."
"if i have a utterance, how to automatically generate logical form?"
"When I run in genovernight mode, SEMPRE doesn't actually produce any canonical utterances. If I understand the log output correctly, it seems that it doesn't know about the general grammar. Or at least, it doesn't know about `$BinaryOp` or `$Rel0NP`. Here is a representative log file, along with the output grammar. Let me know if you have any ideas! Thanks.        "
"Hello,    As I followed the installation process, I discovered some issues.    1. `zip` is must also be pre-installed.    2. `#!/usr/bin/ruby` doesn't work with ruby environment manager such as        - Shebang had better change into `#!/usr/bin/env ruby` to follow user's $PATH which makes scripts more portable.  "
"I started up sempre with CoreNLP exactly as it states in the tutorial, and other CoreNLP functions (like the ""twenty-five million plus forty-two"" example, and the FilterSpanLengthFn rules) are working just fine, but when I try adding any of the FilterSpanPOSTag rules from the documentation, I get this error:        java.lang.RuntimeException: Error on (rule $ProperNoun ($TOKEN) (FilterSpanPosTag token NNP NNPS)): java.lang.RuntimeException: java.lang.ClassNotFoundException: edu.stanford.nlp.sempre.FilterSpanPosTag    I just pulled so I have the latest version. "
"all what i asked in class **ParserState.java,**  1、**what's the meaning of member variable  ""**customExpectedCounts**""? custom distribution is what？**  (gloss = ""Use a custom distribution for computing expected counts"")  public CustomExpectedCount customExpectedCounts =CustomExpectedCount.NONE;  2、 about function **computeExpectedCounts(),**the output para **_counts_** is gradient in current example?  /**     * Fill |counts| with the gradient with respect to the derivations     * according to a standard exponential family model over a finite set of derivations.     * Assume that everything has been executed, and compatibility has been computed.     */  public static void computeExpectedCounts(List  derivations, Map  counts)     **_""standard exponential family "" means what? can you give some explanations or papers about it?_**    3、funciton **_computeExpectedCounts()_**,i pass params _**CustomExpectedCount.Top**_, then i know how to fill array _trueScores_ and _predScores_,but **_incr_** variable is do what?  ""  double incr = trueScores[i] - predScores[i];  if (incr == 0)    continue;  deriv.incrementAllFeatureVector(incr, counts);  """
"hi：  the question i ask is about  class Params.java ;    there is a member variable   public String l1Reg;  when set it to  Params.opts.l1Reg=""lazy"", then the function lazyL1Update() do some process,what's the intention of lazyl1update do?  can you give some explanation?  thanks!"
"Pred@0000: (derivation (formula ((reverse fb:row.row.name) (argmax (number 1) (number 1) (fb:type.object.type fb:type.row) (reverse (lambda r ((reverse fb:cell.cell.number) ((reverse fb:row.row.run) (var r)))))))) (""**5 values**"" (name fb:cell_name.denard_span ""**Denard Span**"")) (type (union fb:column.name fb:type.cell))) [score=41.757, prob=0.735, comp=0]    This is my test result  (before i trained using my training set)    I think that ""5 values""  means five cells are duplicated.  But, this model only answer for one answer among duplicated answer.    How can i get output for all 5 duplicate cell value? (not only 1 value (""Denard Span""))  I want to just get duplicated answers for output.    Thank you."
"I am running into the following error:    > add 1 to 3    Parser.parse: parse {      FloatingParser.infer()Exception in thread ""Thread-2"" java.lang.ArrayIndexOutOfBoundsException: -1          at java.util.ArrayList.elementData(ArrayList.java:418)          at java.util.ArrayList.get(ArrayList.java:431)          at edu.stanford.nlp.sempre.LanguageInfo.getNormalizedNerSpan(LanguageInfo.java:130)          at edu.stanford.nlp.sempre.NumberFn$1.check(NumberFn.java:81)          at edu.stanford.nlp.sempre.NumberFn$1.createDerivation(NumberFn.java:45)          at edu.stanford.nlp.sempre.SingleDerivationStream.hasNext(SingleDerivationStream.java:17)          at edu.stanford.nlp.sempre.FloatingParserState.applyRuleActual(FloatingParser.java:239)          at edu.stanford.nlp.sempre.FloatingParserState.applyRule(FloatingParser.java:206)          at edu.stanford.nlp.sempre.FloatingParserState.applyFloatingRule(FloatingParser.java:261)          at edu.stanford.nlp.sempre.FloatingParserState.buildFloating(FloatingParser.java:429)          at edu.stanford.nlp.sempre.FloatingParserState.access$000(FloatingParser.java:154)          at edu.stanford.nlp.sempre.FloatingParserState$1.run(FloatingParser.java:480)          at java.lang.Thread.run(Thread.java:748)   {        Parser.ensureExecuted       }    }    Parser.setEvaluation: 0 candidates     Example: add 1 to 3 {      Tokens: [add, 1, to, 3]      Lemmatized tokens: [add, 1, to, 3]      POS tags: [UNK, CD, UNK, CD]      NER tags: [UNK, NUMBER, UNK, NUMBER]      NER values: [UNK, 1, UNK, 3]      Dependency children: []    }    with the following grammar file nlp.grammar.simple:    (def @add edu.cmu.lia.semparse.TestFunctions.add)    (rule $NUM ($PHRASE) (NumberFn) )  (rule $ADD ($NUM $NUM)          ( lambda x ( lambda y ( call @add ( var x ) ( var y ) ))) )  (rule $ROOT ($ADD) (IdentityFn))    and the following command-line:    java -cp libsempre/*:lib/*:out/production/sempre/edu/cmu/lia/semparse -ea edu.stanford.nlp.sempre.Main -Main.interactive -Grammar.inPaths instructable/nlp.grammar.simple -Builder.parser FloatingParser -useSizeInsteadOfDepth -FloatingParser.maxDepth 15    Am I missing something obvious?  "
"I use sempre table version, so i run this model such like './run @mode=tables @data=u-1 @feat=all @train=1'  But training model takes long time, so i want to save training model and test a new question on pre-trained model.  Is it possible? If it is possible, could you tell me how to do it?  Thanks."
"Add make, gcc to the  pre-requisites, or suggest to run `sudo apt install build-essential`  On a ubuntu server distribution this is not standard installed.  Kind regards"
"#hi:  I read the code about “Parsing on Semi-Structured Tables”. In a grammar file the original rule is     **_ori rule_** :     (rule $Operator (how many) (ConstantFn (lambda x (count (var x)))))    after rule analysis, this rule turns into two rules behond                                               **_changedrule1_**       $Intermediate1      how@many         (ConstantFn null)  **_changedrule2_**       $Operator             $Intermediate1   (ConstantFn (lambda x (count (var x))))    >>but  in the FLoatingParser.java ,in the funciton buildFloating() ,you do some check about the changed rules,about rules rhs ,this means **_changedrule1_** is omitted .why this process? can you give me some explanation ？      String rhs1 = rule.rhs.get(0);        String rhs2 = rule.rhs.get(1);        if (!Rule.isCat(rhs1) || !Rule.isCat(rhs2))          throw new RuntimeException(""Floating rules with > 1 arguments cannot have tokens on the RHS: "" + rule);                        "
"I am trying to Generating canonical utterances by running the given steps. However, I am getting the following error:           I tried with gen value as 0, I get `Key :pooldir is not in environment ` error.  After which I tried with value of pooldir as 0 and got `Key :parse is not in environment` error.     Any help!!  "
"I am using the freebase mode with a custom ontology loosely based on the freebase schema. I have no problem with creating my own object types but properties are not working properly (the type system thinks they are unary instead of binary i.e. the type appears as ""fb:common.topic""). I looked into the freebase schema.ttl for how properties are created and replicated that in my own schema but I get the impression that when loading my custom ttl into Virtuoso the properties are not recognized properly (or perhaps it's a problem with the REPL I don't know).     So the question is:  How to create new properties? do I need to edit the freebase schema.ttl directly, do I need to modify the files under the fb_data/7 directory? Or is there a better way of doing this without freebase whatsoever?    Thank you,  Laurent"
"Hi, my goal is to run @mode=tables with learned weights to get interactive answers to custom new questions. First, I tried what's in the tables mode Tutorial:   `./run @mode=tables -interactive`   however, it would give me all the same probabilities and pred choices for any of questions in tested table 204-590       or the other time is `(number n)`, say `(number 10)`  where `n` is the correct no of rows of that table (tested on different sizes).    My steps:  - built the sempre repo (`./pull-dependencies core corenlp tables tables-data`, etc., compile the source)  - `./run @mode=tables @data=u-1 @feat=all @train=1 -maxex train,100 dev,100`  this gave me exec directory with params (weights) and other files located in `sempre/state/execs/{n}.exec`, let's say n=26  - load params from previously trained weights from that Exec `state/execs/{n}.exec` folder and run interact mode. The resulting answers for the question as above are the same.     Other modes are working as expected. I managed to run @mode=simple-freebase-nocache as written in the tutorial and get good answers. However, I understand this is different from WebQuestions knowledge base, cannot compare. Please guide me through the process of running trained model for QA, plus whether I have to train it on u-1, u-2, etc. or test splits for the best results?"
"When I was training parasempre using free917, the following error occurs:  ERROR: Caused by java.lang.OutOfMemoryError: GC overhead limit exceeded  How could I solve it? "
"If I only want to run sempre 1.0, where could I download related files and how to install it? "
"I followed exactly the steps on the `README.md` page, but it didn't work for me on Ubuntu 16.04 when I tried to install `virtuoso`. Here are the error messages:     Here is the configuration summary after I run ./configure --prefix=$PWD/install:     And my packages version:    "
"I'm using `ubuntu 16.04` to compile the code, but I failed when I tried `ant core`. Here is the compile error output. The version information: `ruby 2.3.1p112 (2016-04-26) [x86_64-linux-gnu]`, `openjdk version ""1.8.0_91""`, `Apache Ant(TM) version 1.9.6 compiled on July 8 2015`.     "
"I am using mac 10.11 and I can not get through this step. The issue is:     If I use in to replace `cat /proc/meminfo | grep MemFree | awk '{print $2}'`, the issue will be:     Can this step be done on mac os?  "
"Hello Everyone,  How can I handle synonyms while parsing the utterance into logical form?   For example,  if the knowledge graph has a relation named ""Donald-sex-Male"" and the utterance asks for the ""What is the gender of Donald?"", how can I answer this? I would need to map the word ""gender"" to ""sex"" in some way.  Do we have the support for wordnet/synonyms dictionary in the framework? "
"pull-dependency corenlp stopped working today. So I added the following links stanford-corenlp-full-2015-12-09.zip -> /u/nlp/data/semparse/resources/stanford-corenlp-full- stanford-corenlp-caseless-2015-04-20-models.jar -> /u/nlp/data/semparse/resources/stanford-corenlp-caseless-2015-04-20-models.jar to  /u/apache/htdocs/static/software/sempre/dependencies-2.0/u/nlp/data/semparse/resources  But perhaps there is a reason for their removal, in which case let me know? "
"When I take a look at the log produced by ./run @mode=tables @data=u-1 @feat=all @train=0,   I see two parts of derivations, true@ and pred@.  How is each of them produced and what does each of them mean?  +) How is each derivation scored? (Based on what?)  Thank you! "
"Do we have to train every single time we want to test in ""tables mode""? Or is there a way to test without training (using training data from last time?) If so, how can I do it?  Thank you. "
"Hello~ :) I have some difficulties with installation. When I run ""ant core"", there are a number of error messages.   ubuntu@ip-172-31-27-159:~/sempre$ ant core Buildfile: /home/ubuntu/sempre/build.xml  init:      [exec] Wrote modules with 248 files to module-classes.txt: cache core corenlp freebase overnight tables     [mkdir] Created dir: /home/ubuntu/sempre/classes     [mkdir] Created dir: /home/ubuntu/sempre/libsempre  core:      [echo] Compiling semparse: core     [javac] Compiling 124 source files to /home/ubuntu/sempre/classes     [javac] **/home/ubuntu/sempre/src/edu/stanford/nlp/sempre/AbstractReinforcementParserState.java:3: error: package com.google.common.collect does not exist**     [javac] import com.google.common.collect.Sets;     [javac]                                 ^     [javac] **/home/ubuntu/sempre/src/edu/stanford/nlp/sempre/AbstractReinforcementParserState.java:4: error: package fig.basic does not exist**     [javac] import fig.basic.LogInfo;     [javac]                 ^     [javac] **/home/ubuntu/sempre/src/edu/stanford/nlp/sempre/AbstractReinforcementParserState.java:5: error: package fig.basic does not exist**     [javac] import fig.basic.MapUtils;  How can this be fixed? "
"After installing and even using pull-dependencies to get corenlp, I get this error when trying to run in interactive mode:     How can this be fixed? "
"Hi, Liang I  met a problem when I tried to update the params with sempre1.0 let x be sentence, let y be the class (1) original formula: score(x,y) = sum(feature(x,y)) =  w1 + w2 + ..... wn,  W was the params       feature was added to indicatorFeatures        (score, W takes effect when calculate gradient)      with SGD, the model can be trained to an accuracy 88% (small data).  (2) new formula: score = sum(feature(x,y)) =  w1 \* 0.1 + w2 \* 0.1 + ..... wn \* 0.1      feature was added to generalFeatures       (score, W and 0.1 takes effect when calculate gradient)      with a fixed 0.1 factor, the model accuracy should be the same,right?      but they are not, the new accuracy is 76%, Why?  (3) Ｉmade a different try,  sum(feature(x,y)) =  w1 + w2 + ..... wn,  score = 0.1\* sum(feature(x,y))        W and score was for SGD trainning.         (score, W takes effect when calculate gradient)       feature was added to indicatorFeatures      guess what, the accuracy  returned to 88% again.  Summary: it seems that if I add any factor to SGD trainning process ,the accuracy drops a lot. the accuracy was infected by -Params.l1RegCoeff 0.00562341325.  I guess the problem was caused by L1 reg, It seems to be designed with fixed factor = 1.  do you have any idea how to deal with this issue? by the way how do you get -Params.l1RegCoeff 0.00562341325, i want to generate a new one for my model. Thanks ,hope you can reproduce this issue. "
"Hello!  I'm struggling with getting the correct command to train and save parameters. Using the command below,  the output file for the parameters is always empty. (output is in for example: runOutputs/params.6 )  This is the command I used:  ./run @mode=simple -Grammar.inPaths data/grammar.sempre.v1.3 -FeatureExtractor.featureDomains rule -Dataset.inPaths train:data/withGaze.trueGrammar.examples -Learner.maxTrainIters 3 -interactive false  -Parser.verbose 5 -Parser.coarsePrune true -exec.execDir runOutputs/ -Learner.updateWeights true -Params.initWeightsRandomly true   Note: I am using my own grammar and a toy training example.  It ran and parse correctly, but just did not produce the parameters (the weights).   I guess I’m missing some necessary arguments in the command, but I could not find it in the tutorial.  Do you know what it's missing?   Thank you so much! "
"According to the tutorial, I am running: ./run @mode=simple-freebase @sparqlserver=localhost:3001 -Grammar.inPaths freebase/data/tutorial-freebase.grammar -SimpleLexicon.inPaths freebase/data/tutorial-freebase.lexicon  I get an error message:  [asherst@skinsbegins sempre]$ ./run @mode=simple-freebase @sparqlserver=localhost:3001 -Grammar.inPaths freebase/data/tutorial-freebase.grammar -SimpleLexicon.inPaths freebase/data/tutorial-freebase.lexicon /usr/local/bin/rlwrap /usr/local/bin/rlwrap main() {   Grammar.read {     SimpleLexicon.read(freebase/data/tutorial-freebase.lexicon)java.lang.RuntimeException: java.io.FileNotFoundException: freebase/data/tutorial-freebase.lexicon (No such file or directory)     at edu.stanford.nlp.sempre.SimpleLexicon.read(SimpleLexicon.java:115)     at edu.stanford.nlp.sempre.SimpleLexicon. (SimpleLexicon.java:59)     at edu.stanford.nlp.sempre.SimpleLexicon.getSingleton(SimpleLexicon.java:53)     at edu.stanford.nlp.sempre.SimpleLexiconFn. (SimpleLexiconFn.java:30)     at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)     at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)     at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)     at java.lang.reflect.Constructor.newInstance(Constructor.java:422)     at java.lang.Class.newInstance(Class.java:442)     at fig.basic.Utils.newInstanceHard(Utils.java:268)     at edu.stanford.nlp.sempre.Grammar.parseSemanticFn(Grammar.java:467)     at edu.stanford.nlp.sempre.Grammar.interpretRule(Grammar.java:255)     at edu.stanford.nlp.sempre.Grammar.interpret(Grammar.java:154)     at edu.stanford.nlp.sempre.Grammar.readOnePath(Grammar.java:130)     at edu.stanford.nlp.sempre.Grammar.read(Grammar.java:61)     at edu.stanford.nlp.sempre.Grammar.read(Grammar.java:53)     at edu.stanford.nlp.sempre.Builder.buildUnspecified(Builder.java:45)     at edu.stanford.nlp.sempre.Builder.build(Builder.java:38)     at edu.stanford.nlp.sempre.Main.run(Main.java:17)     at fig.exec.Execution.runWithObjArray(Execution.java:337)     at fig.exec.Execution.run(Execution.java:325)     at edu.stanford.nlp.sempre.Main.main(Main.java:38) Caused by: java.io.FileNotFoundException: freebase/data/tutorial-freebase.lexicon (No such file or directory)     at java.io.FileInputStream.open0(Native Method)     at java.io.FileInputStream.open(FileInputStream.java:195)     at java.io.FileInputStream. (FileInputStream.java:138)     at fig.basic.IOUtils.openIn(IOUtils.java:121)     at fig.basic.IOUtils.openIn(IOUtils.java:119)     at edu.stanford.nlp.sempre.SimpleLexicon.read(SimpleLexicon.java:68)     ... 21 more  {       ERROR: java.lang.RuntimeException: Error on (rule $Unary ($PHRASE) (SimpleLexiconFn (type fb:type.any))): java.lang.RuntimeException: java.io.FileNotFoundException: freebase/data/tutorial-freebase.lexicon (No such file or directory): edu.stanford.nlp.sempre.Grammar.interpret(Grammar.java:176) edu.stanford.nlp.sempre.Grammar.readOnePath(Grammar.java:130) edu.stanford.nlp.sempre.Grammar.read(Grammar.java:61) edu.stanford.nlp.sempre.Grammar.read(Grammar.java:53) edu.stanford.nlp.sempre.Builder.buildUnspecified(Builder.java:45) edu.stanford.nlp.sempre.Builder.build(Builder.java:38) edu.stanford.nlp.sempre.Main.run(Main.java:17) fig.exec.Execution.runWithObjArray(Execution.java:337) fig.exec.Execution.run(Execution.java:325) edu.stanford.nlp.sempre.Main.main(Main.java:38) 1 errors, 0 warnings     } Command failed: rlwrap java -Dmodules=core,freebase -cp libsempre/_:lib/_ -ea edu.stanford.nlp.sempre.Main -executor freebase.SparqlExecutor -SparqlExecutor.endpointUrl   -FreebaseSearch.cachePath FreebaseSearch.cache -EntityLexicon.mid2idPath freebase.cloudapp.net:4000:freebase-rdf-2013-06-09-00-00.canonical-id-map.gz -TypeInference.typeLookup freebase.FreebaseTypeLookup -FreebaseTypeLookup.entityTypesPath freebase.cloudapp.net:4000:freebase-rdf-2013-06-09-00-00.canonicalized.en-types.gz -EntityLexicon.maxEntries 2 -FeatureExtractor.featureDomains rule -Parser.coarsePrune -JoinFn.typeInference -UnaryLexicon.unaryLexiconFilePath /dev/null -BinaryLexicon.binaryLexiconFilesPath /dev/null -Grammar.inPaths freebase/data/demo1.grammar -SparqlExecutor.returnTable -Main.interactive -Grammar.inPaths freebase/data/tutorial-freebase.grammar -SimpleLexicon.inPaths freebase/data/tutorial-freebase.lexicon "
"Thanks for putting your software online! I get these errors from javac:   [javac] Compiling 71 source files to /home/tcassidy/sempre/classes     [javac] /home/tcassidy/sempre/src/edu/stanford/nlp/sempre/ParserAgenda.java:25: error: cannot find symbol     [javac] class ListParserAgenda implements ParserAgenda  {     [javac]                                                ^     [javac]   symbol: class PrioritizedDerivationStream     [javac] /home/tcassidy/sempre/src/edu/stanford/nlp/sempre/ParserAgenda.java:27: error: cannot find symbol     [javac]   private List  agenda = new ArrayList ();     [javac]                ^     [javac]   symbol:   class PrioritizedDerivationStream     [javac]   location: class ListParserAgenda     [javac] /home/tcassidy/sempre/src/edu/stanford/nlp/sempre/ParserAgenda.java:35: error: cannot find symbol     [javac]   public boolean add(PrioritizedDerivationStream item, double priority) {  etc...  [javac] Note: Some input files use unchecked or unsafe operations.     [javac] Note: Recompile with -Xlint:unchecked for details.     [javac] 17 errors "
"I tried to pull fullfreebase-vdb, as described in the tutorial, but it gave a file not found error:  $  ./pull-dependencies fullfreebase-vdb ===== Downloading fullfreebase-vdb: Freebase (Virtuoso database) /u/nlp/data/semparse/scr/freebase/state/execs/93.exec/vdb.tar.bz2 --2015-06-24 02:10:39--    Resolving nlp.stanford.edu... 171.64.67.140 Connecting to nlp.stanford.edu|171.64.67.140|:80... connected. HTTP request sent, awaiting response... 404 Not Found 2015-06-24 02:10:40 ERROR 404: Not Found. "
"Hello!   I'm using the SEMPRE 2.0 to build a semantic parser, and I just started with a toy grammar. But I'm confused about the behavior of the parser, could you provide some hints? So here the grammar: ------------------------------------------------------------------(case1)     (rule $Word ($TOKEN) (IdentityFn))     (rule $ShopName ($Word)  (lambda x (call + (string ShopName:) (var x))))     (rule $ShopName ($ShopName $Word) (ConcatFn "" :""))     (rule $ROOT ($ShopName) (IdentityFn))  The input sentence: ""sushi bar"" What I expect is:     ShopName :sushi :bar What I got is:           null:bar -------------------------------------------------------------------(case2) So if the grammar is as follows:     (rule $Word ($TOKEN) (IdentityFn))     (rule $ShopName ($Word)  (IdentityFn))     (rule $ShopName ($ShopName $Word) (ConcatFn "" :""))     (rule $ROOT ($ShopName) (IdentityFn)) And with the input sentence ""sushi bar"" I got ""sushi bar"". (Correct) -------------------------------------------------------------------(case3) Also, if the grammar is as follows:     (rule $Word ($TOKEN) (IdentityFn))     (rule $ShopName ($Word)  (lambda x (call + (string ShopName:) (var x))))     (rule $ROOT ($ShopName) (IdentityFn)) And the input is ""bar"" I got     ""ShopName:bar""   , which is correct  So is there anything wrong with the lambda part? What does it produce a null value in case1, while it is correct in case3?  Shouldn't the executed output from the lambda function be passed on to the root node? And this is in interactive mode ./run @mode=simple.   If there any other command that I need to specify?   Thanks for the help! "
"Below is my command line terminal _make core_ cd src/edu/stanford/nlp/sempre && ant compile Buildfile: /home/rupela/sempre/src/edu/stanford/nlp/sempre/build.xml  compile:     [javac] Compiling 119 source files to /home/rupela/sempre/classes     [javac] /home/rupela/sempre/src/edu/stanford/nlp/sempre/test/GrammarValidityTest.java:28: error: illegal start of expression     [javac]         Files.walk(Paths.get(dataPath)).forEach(filePath -> {     [javac]                                                           ^     [javac] /home/rupela/sempre/src/edu/stanford/nlp/sempre/test/GrammarValidityTest.java:28: error: illegal start of expression     [javac]         Files.walk(Paths.get(dataPath)).forEach(filePath -> {     [javac]                                                             ^     [javac] /home/rupela/sempre/src/edu/stanford/nlp/sempre/test/GrammarValidityTest.java:28: error: ';' expected     [javac]         Files.walk(Paths.get(dataPath)).forEach(filePath -> {     [javac]                                                              ^     [javac] /home/rupela/sempre/src/edu/stanford/nlp/sempre/test/GrammarValidityTest.java:38: error: 'catch' without 'try'     [javac]           catch (Exception ex) {     [javac]           ^     [javac] /home/rupela/sempre/src/edu/stanford/nlp/sempre/test/GrammarValidityTest.java:38: error: ')' expected     [javac]           catch (Exception ex) {     [javac]                           ^     [javac] /home/rupela/sempre/src/edu/stanford/nlp/sempre/test/GrammarValidityTest.java:38: error: not a statement     [javac]           catch (Exception ex) {     [javac]                 ^     [javac] /home/rupela/sempre/src/edu/stanford/nlp/sempre/test/GrammarValidityTest.java:38: error: ';' expected     [javac]           catch (Exception ex) {     [javac]                              ^     [javac] /home/rupela/sempre/src/edu/stanford/nlp/sempre/test/GrammarValidityTest.java:41: error: illegal start of expression     [javac]         });     [javac]          ^     [javac] /home/rupela/sempre/src/edu/stanford/nlp/sempre/test/GrammarValidityTest.java:25: error: 'try' without 'catch', 'finally' or resource declarations     [javac]     try {     [javac]     ^     [javac] /home/rupela/sempre/src/edu/stanford/nlp/sempre/test/GrammarValidityTest.java:53: error: illegal start of type     [javac]     catch (Exception ex) {     [javac]     ^     [javac] /home/rupela/sempre/src/edu/stanford/nlp/sempre/test/GrammarValidityTest.java:57: error: class, interface, or enum expected     [javac] }     [javac] ^     [javac] 11 errors  BUILD FAILED ................. **java -version** returns as _below_- java version ""1.8.0_45"" Java(TM) SE Runtime Environment (build 1.8.0_45-b14) Java HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode)  Any workaround would be of great help "
"Hello  I just started sempre 2.0,based of steps Easy Setup in readme file.  > Picked up JAVA_TOOL_OPTIONS: -javaagent:/usr/share/java/jayatanaag.jar  > Exception in thread ""main"" java.lang.RuntimeException: java.io.FileNotFoundException: module-classes.txt (No such file or directory) >     at fig.basic.IOUtils.readLinesHard(IOUtils.java:479) >     at edu.stanford.nlp.sempre.Master.getOptionsParser(Master.java:412) >     at edu.stanford.nlp.sempre.Main.main(Main.java:38) > Caused by: java.io.FileNotFoundException: module-classes.txt (No such file or directory) >     at java.io.FileInputStream.open0(Native Method) >     at java.io.FileInputStream.open(FileInputStream.java:195) >     at java.io.FileInputStream. (FileInputStream.java:138) >     at fig.basic.IOUtils.openIn(IOUtils.java:121) >     at fig.basic.IOUtils.openIn(IOUtils.java:119) >     at fig.basic.IOUtils.readLines(IOUtils.java:398) >     at fig.basic.IOUtils.readLinesHard(IOUtils.java:478) >     ... 2 more > Command failed: java -cp libsempre/_:lib/_ -ea edu.stanford.nlp.sempre.Main -interactive  this error appears after - ./run @mode=simple  "
"Hi Percy,  I'm having issues with the run modes query and simple freebase.  For the command ./run @mode=query @sparqlserver=localhost:3001 -formula '(fb:location.location.containedby fb:en.california)' in the tutorial, I get: Error: Could not find or load main class edu.stanford.nlp.sempre.freebase.SparqlExecutor Command failed: java -ea -cp libsempre/_:lib/_ edu.stanford.nlp.sempre.freebase.SparqlExecutor -SparqlExecutor.endpointUrl   -formula (fb:location.location.containedby fb:en.california)  Anf for ./run @mode=simple-freebase @sparqlserver=localhost:3001, having started the Virtuoso database, I get: /usr/local/bin/rlwrap Problem processing: freebase edu.stanford.nlp.sempre.freebase.BinaryLexicon Exception in thread ""main"" java.lang.RuntimeException: java.lang.ClassNotFoundException: edu.stanford.nlp.sempre.freebase.BinaryLexicon     at edu.stanford.nlp.sempre.Master.getOptionsParser(Master.java:434)     at edu.stanford.nlp.sempre.Main.main(Main.java:38) Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.sempre.freebase.BinaryLexicon     at java.net.URLClassLoader.findClass(URLClassLoader.java:381)     at java.lang.ClassLoader.loadClass(ClassLoader.java:424)     at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)     at java.lang.ClassLoader.loadClass(ClassLoader.java:357)     at java.lang.Class.forName0(Native Method)     at java.lang.Class.forName(Class.java:264)     at edu.stanford.nlp.sempre.Master.getOptionsParser(Master.java:428)     ... 1 more Command failed: rlwrap java -Dmodules=core,freebase -cp libsempre/_:lib/_ -ea edu.stanford.nlp.sempre.Main -executor freebase.SparqlExecutor -SparqlExecutor.endpointUrl   -FreebaseSearch.cachePath FreebaseSearch.cache -EntityLexicon.mid2idPath freebase.cloudapp.net:4000:freebase-rdf-2013-06-09-00-00.canonical-id-map.gz -TypeInference.typeLookup freebase.FreebaseTypeLookup -FreebaseTypeLookup.entityTypesPath freebase.cloudapp.net:4000:freebase-rdf-2013-06-09-00-00.canonicalized.en-types.gz -EntityLexicon.maxEntries 2 -FeatureExtractor.featureDomains rule -Parser.coarsePrune -JoinFn.typeInference -UnaryLexicon.unaryLexiconFilePath /dev/null -BinaryLexicon.binaryLexiconFilesPath /dev/null -Grammar.inPaths freebase/data/demo1.grammar -SparqlExecutor.returnTable -Main.interactive  Thanks for all the help :)  Cheers, Sky "
"Hi,  I'm hosting sempre on an EC2 instance and having followed the requirements and all the installation instructions, I'm stuck at ./run @mode=simple.   I get this printout from the command: /usr/local/bin/rlwrap Exception in thread ""main"" java.lang.RuntimeException: java.io.FileNotFoundException: module-classes.txt (No such file or directory)     at fig.basic.IOUtils.readLinesHard(IOUtils.java:479)     at edu.stanford.nlp.sempre.Master.getOptionsParser(Master.java:412)     at edu.stanford.nlp.sempre.Main.main(Main.java:38) Caused by: java.io.FileNotFoundException: module-classes.txt (No such file or directory)     at java.io.FileInputStream.open0(Native Method)     at java.io.FileInputStream.open(FileInputStream.java:195)     at java.io.FileInputStream. (FileInputStream.java:138)     at fig.basic.IOUtils.openIn(IOUtils.java:121)     at fig.basic.IOUtils.openIn(IOUtils.java:119)     at fig.basic.IOUtils.readLines(IOUtils.java:398)     at fig.basic.IOUtils.readLinesHard(IOUtils.java:478)     ... 2 more Command failed: rlwrap java -cp libsempre/_:lib/_ -ea edu.stanford.nlp.sempre.Main -interactive  Thanks, Sky "
"Hi   I have read your paper over 50 times,but I dont understand a part.... this picture shows what I m understanding about 'Train phase' of model Do you please tell me,what is my mistake?? please please please  !   if it is like above,but what does ""alignment""  means in ACL2014?  > We also generate canonical utterances using > an alignment lexicon, released by Berant et al. > (2013), which maps text phrases to Freebase binary predicates  it takes the mind to there is ""a lexicon which is made offline"" that maps phrases to fb predicates. "
"Hi   for training Parasempre in Free917 I used this command:  > ./parasempre  @mode=train \ >               @sparqlserver=localhost:3093 \ >               @domain=free917 \ >               @cacheserver=local  you said 'This should about an hour to complete' but my system is in this part for 8 hours:  >   Lexicon() { >     Loading lexicon file lib/fb_data/6/unaryInfoStringAndAlignment.txt { >       Number of entries: 10780 >     } [6.4s, cum. 6.5s] >     Loading lexicon file lib/fb_data/6/binaryInfoStringAndAlignment.txt { >       Number of entries: 18801 >     } [2.0s, cum. 8.6s] >   } [8.6s, cum. 44s] >   Using cache LexiconFn.cache (242 entries) >   Loading phrase table { >     ParaphraseUtils.loadPhraseTable: number of entries=1264159 >   } [12s, cum. 1m2s] >   TransUtis.loadDerivations: number of entries=13443 > WordNet.loadWordRelation(der): skipping self-loop on tiercel#n#1 > WordNet.loadWordRelation(der): skipping self-loop on tiercel#n#1 > WordNet.loadWordRelation(der): skipping self-loop on unicycle#n#1 > WordNet.loadWordRelation(der): skipping self-loop on township#n#1 > WordNet.loadWordRelation(der): skipping self-loop on township#n#1 > WordNet.loadWordRelation(der): skipping self-loop on isle#n#1 > WordNet.loadWordRelation(der): skipping self-loop on isle#n#1 > WordNet.loadWordRelation(der): skipping self-loop on lapidary#n#2 > WordNet.loadWordRelation(der): skipping self-loop on lapidary#n#2 > WordNet.loadWordRelation(der): skipping self-loop on lapidary#n#1 > WordNet.loadWordRelation(der): skipping self-loop on lapidary#n#1 > WordNet.loadWordRelation(der): skipping self-loop on stripper#n#3 > WordNet.loadWordRelation(der): skipping self-loop on stripper#n#3 > WordNet.loadWordRelation(der): skipping self-loop on interrelation#n#1 > WordNet.loadWordRelation(der): skipping self-loop on interrelation#n#1 > WordNet.loadWordRelation(der): skipping self-loop on bondage#n#2 > WordNet.loadWordRelation(der): skipping self-loop on bondage#n#2 > WordNet.loadWordRelation(der): skipping self-loop on bondage#n#2 > WordNet.loadWordRelation(der): skipping self-loop on bondage#n#2 >   Learner.learn() { >     Iteration 0/10 { >       Learn from parsing dataset { >         Processing parsing_iter=0.train: 512 examples { >           Examples {  Could you please tell me why?  "
Hello all  I am really confused in something When I run this command that is said in _Quickstart.md_:  >    sometime _SparqlExecutor.execute_ output comes in my log and sometimes it dosent come!!!!!!!  > SparqlExecutor.execute: (fb:user.moritzstefaner.maceproject.lesson.interactivity_level fb:m.0sn7v5c)  >               SparqlExecutor.execute: (fb:user.dsp13.default_domain.user_group.interesting_topics fb:en.three_plus)  >               SparqlExecutor.execute: (fb:user.dsp13.default_domain.user_group.interesting_topics fb:m.0sn7v5c)  >               SparqlExecutor.execute: (fb:user.avic.assertion_modeling_kit.freebase_proposition.input_2 fb:m.0smftlj)  >               SparqlExecutor.execute: (fb:user.moritzstefaner.maceproject.lesson.interactivity_level fb:m.0smftlj)  >               SparqlExecutor.execute: (fb:user.avic.assertion_modeling_kit.freebase_proposition.input_2 fb:en.three_plus)  >               SparqlExecutor.execute:  I always use same command! 
"Hi, what alignment tool did you use? giza++?  fast_align? and I noticed that, in /lib/paralex/phrase-table.counts.txt, the align format is:          phrase1    phrase2    cooccurrenceCount     phrase1Count     phrase2Count It is quite weird that cooccurrenceCount could be larger than phrase1Count or phrase2Count, Why? e.g          unix and linux ?       linux and unix ?        52.0        9.0    7.0          in zucchini               in zuccinus              14.0        11.0       4.0 "
"Hi Liang, Can you supply me the files to generate lucene files(lucene/4.4/free917) My email: uwittygit@hotmail.com Thanks a lot "
"HI Liang, I used Sempre1.0 Paraphrase for test. Do you have any strategy to reduce the size of generated formulas, It seems that it produced all possible formulas for a given entity.  like ""what currency does China use ?"" , there are 6000 possible formulas and 12000 possible canonical questions, which were too large. I noticed that you used ""lib/lucene/4.4/inexact"" to align entities, but there were no alignment for unary or binary rules in FormulaRetriever.java. How can I generate formula with these rules to scale down the size. "
"With Freebase shutting down, is the plan to work with WikiData? Or something else?  Perhaps just a more generic way to import your own Lexicon?  "
"Hi Liang, I do read the suggested papers, and was in a chaos: The paper ""Bringing machine learning and compositional semantics together"" says: ""We use [u] for the translation of syntactic expression u into its semantic representation"".  So ,Does Sempre realize the translation from syntactic tree to semantic tree?  This is very important to me, thanks. "
"Hello, I am getting a 403 error when obtaining the Freebase type file.  This issue occurs when running `./pull-dependencies fullfreebase-types`  I can download the rest of the files. The file with the permission problem is located here:   "
Add wget to dep list. I didn't have it on my machine by default. Max os 10.9.5 
Can anyone explain the differences semantic or high level goals between SEMPRE and SPF[1]  They both seem to use a CCG and Lambda calculus to convert natural language to a logical form.  I know SPF has a ML layer for getting to the solution while hiding some of the parsing internals.   Perhaps they projects target sets of use-cases and I simply overlooked it.  [1]   
"Hi, I encountered a problem in evaluation step. Suppose the test parsing example is (example (utterance ""what is the capital of france?"") (targetValues (description ""Paris""))), I output its target value, which is ""(description Paris)"". However, for the top derivation of this example, the target value is ""(list (name fb:en.paris Paris))"". The answer is correct, but when evaluated, it's judged as negative answer, because these two target values are not the same in string level, the compatibility score is 0 (computed at Parser.java, line 246). I think that I made a mistake somewhere, is there anyone who encountered this problem before, and tell me how to do? Thanks in advance. "
"Hello I have moved from step to step of QUICKSTART but in  ""Install the Database"" for comment ""make"" This error is appeared: Can u please told me how can I pass it?  make: **\* No targets specified and no makefile found.  Stop. "
"Hello  Would you please told me,what does ""indicator features"" means in this part? {Each derivation d is the result of applying some number of intersection, join, and bridging operations. To control this number, we define indicator features on each of these counts. }  Tnx "
"The Quickstart.md says to checkout the v7.0.0 tag for virtuoso, but this version of virtuoso does not compile under Ubuntu 14.04, because of an upgrade of bison that is not compatible any more with virtuoso v7.0.0. However, it looks like you can rather checkout the 042f142 commit, which has a patch to fix this problem with Ubuntu 14.04. I've just tried, and it looks like it's working fine with sempre... "
"I'm struggling to use this for Chinese sentence analysis. Processes up to parser is ok now. But I'm stuck at the LexiconFn process.  The LexiconFn looks up entities in the ""lib/lucene/4.4"" indexes, which, I guess, was produced by the fbalignment.index.FbEntityIndexer class. A ""datadump file"" is mentioned in the class but I cannot find it in the project repository.   Can I have a look at what the ""datadump file"" look like? Maybe I can create a Chinese version of that file and index it.  Thanks "
"Hi, I want to run parasempre on new questions with a pre-trained model. Is there a way to do this?  The QuickStart guide mentions a way to do this for sempre - by setting -Learner.maxTrainIters to 0. I tried doing something similar with parasempre, but the system still seems to proceed with training (the iterations begin with ""parsing_iter=0.train""). The command I'm using is     The newdomain method in the parasempre script mirrors the webquestions method, the only change being the test questions path pointing to a new set of questions.  "
"The link to ""an introduction by Lappin"" is broken.  "
"I want to output parameters of a derivation with JSON format and read that log by my code. Fortunately, SEMPRE includes many JSON properties used jackson, like this:     However, I cannot find how to use this options. I think you want this system to be able to output some parameters with JSON, because now this system uses fig to manage logs but these logs are not readable by some programs.  Would you mind telling me how to write the annotated parameters with JSON to some file? "
"Hello,  I execute the following command:  ./sempre @mode=interact          @domain=webquestions          @sparqlserver=localhost:8890/sparql          @cacheserver=local @load=15 @executeTopOnly=0 < Lundi_test.txt   However, inside Master class, the runInteractivePrompt() method will always read the line as null, I put the details in:     Do you know where is the problem, please? "
"Hi, I am trying your system on a different set of queries. I am using the following set of commands. Could you please verify if this is correct?  Train     Test     where 19.exec is the execution directory of the previous (training) run. I have edited the sempre script to add a new method     Likewise, I have also edited selectdomain to add a line for supporting newdomain     Is this the correct way of training (and testing) on a new dataset?  Thanks and Regards, Mandar "
"While going through the tutorial again, I found that, after adding only the rules     as described in the tutorial, entering ""cities in California"" gives the singular derivation     with probability one, rather than     as we would expect. Is this because I haven't trained model yet? "
"Shouldn't the line 111 in scripts/virtuoso (  be: run ""#{$virtuosoPath}/install/bin/virtuoso-t +configfile #{configPath} +wait"" ? "
" I have never used a library for RDF, so maybe I misunderstand something.  SEMPRE depends on Virtuoso for interface to Freebase, but using it through HTTP protocol (not a java library). If you use some library for Virtuoso (such as Jena, Sesame), I think it can be more faster and also can parallel processing. Why you don't use Sesame or some library for Virtuoso? I want to know whether it is technical problem and the cause. "
"Hi all, For those of you using the emnlp2013 models -  We have released a new model that should be better and updated QUICKSTART.md and the sempre run script accordingly (new model directory is lib/models/15.exec).  If you are interested in running our parser on Freebase as in our EMNLP paper you should     Let us know of any issues. "
"I'm having some trouble doing the exercises right not. I've read through the paper that relates to lambda-DCS (  and managed to get `city with the largest area` and  `top 5 cities by area`, but I can't figure out how to get the other phrases that involve more complicated qualifiers, such as `country with the most number of rivers`.  For example, I can get the number of rivers in a given country  `(execute (count (and (fb:type.object.type fb:geography.river (fb:location.location.containedby fb:en.france))))`  and the largest country by area  `(execute (argmax 1 1 (fb:type.object.type fb:location.citytown) fb:location.location.area))`  but I can't figure out how to string these two together to get what I need. Is there another tutorial on lambda-DCS I can look at? "
"1. The command to get the largest city:    (execute (argmax 1 1 (fb:type.object.type fb:location.citytown) !fb:location.location.area))  shouldn't have an exclamation mark. 1. There's a bug in SparqlExecutor.java, in the else block found on lines 510 - 518 (which handles the special (argmax/min 1 1)) case, causing the return value to be seemingly random objects rather than the desired superlative. 2. Trying to add any of the lucene rules results in the following error:    java.lang.RuntimeException: Error on (rule $Entity ($PHRASE) (LexiconFn entity allowInexact)): org.apache.lucene.store.NoSuchDirectoryException: directory '/home/yushiw/sempre/lib/lucene/4.4/inexact' does not exist "
"When trying to add tutorial.ttl to my virtuoso server, I got the following error:  RUNNING: echo ""DB.DBA.TTLP_MT (file_to_string_output ('/Users/yushiwang/Documents/stanford13-14/pliang/virtuoso-3001-0.ttl'), '', '  128);"" | scripts/../virtuoso-opensource/install/bin/isql localhost:13001 2>&1 | tee virtuoso-3001-0.out Connected to OpenLink Virtuoso Driver: 06.01.3127 OpenLink Virtuoso ODBC Driver OpenLink Interactive SQL (Virtuoso), version 0.9849b. Type HELP; for help and EXIT; to exit. SQL>  **\* Error 08S01: [Virtuoso Driver]CL065: Lost connection to server at line 1 of Top-Level: DB.DBA.TTLP_MT (file_to_string_output ('/Users/yushiwang/Documents/stanford13-14/pliang/virtuoso-3001-0.ttl'), '', '  128) RUNNING: cat virtuoso-3001-0.out >> virtuoso-3001.log [Fri Jan 03 09:50:07 -0800 2014] FAILED  After getting this error, my server also gets stopped. Anybody know what the issue is here? "
"Hi,  I'm interested in how to extract Freebase's entities for GeoQuery dataset, and I found following codes in `sempre/sempre`      However, there is not files `data/geo.types` and `data/geo.properties`. Is it missing?  Thanks! "
"Thanks for sharing this great project.  I am having some issues when I following the tutorial.  1. I can not find the file ""data/tutorial.ttl"" in any place. Is it missing? 2. What is the symbol ""!"" means in ""(execute (argmax 1 1 (fb:type.object.type fb:location.citytown) !fb:location.location.area))"". And I cannot understanding the logical forms of Superlative((argmax |rank| |count| |u| |b|)), why did you use rank and count, can you give me some more detailed explanations?  Thanks, Shizhu "
"I've executed commands following QUICKSTART.md, but its output log doesn't include answers.  This gist is a log ran as interactive mode, which loaded a model and parameters from lib/models/2174.exec and executing the question ""what is the capital of france?"" (at 63 line). But there is not the answer or candidates of answer in the log.    This is properly act and it is a correct log? "
"Hi,  I didn't know where to put my message, because I don't know if it is an issue or just my stupidity. I need some help with SEMPRE.  I successfully build it and ran the tests with options ""-excludegroups xfail,sparql,emnlp2013"". I then treid giving it some questions to parse followed QUICKSTART.md, but only this command is failed:     Probably I succesfully install Virtuoso but something is wrong. I tried 7.0.0 and the latest version of Virtuoso  I'm very happy with SEMPRE; I want to parse some sentences with Freebase and analyze its results. "
Is there a command or script to run where I can take a list of questions (e.g. one per line) and get a list of scored answer sets back?  Example input:     Example output:     The details of how the answers are represented doesn't matter.  
"(I figured I'd post here instead of emailing in case others have the same problem.)  Hey guys, I'm trying to train the WebQuestions system from your EMNLP paper. I've downloaded the freebase files and started the virtuoso server, but when I run      I get the error `./fig/lib/myutils.rb:389:in`exec': No such file or directory - fig/bin/qcreate (Errno::ENOENT)`. Full output is here:    I can't find qcreate anywhere in the project. Any idea what's going on? "
"Hi,  Thanks for making this great project open source.   I am having some issues downloading the geofreebase vdb and ttl files. I see that the download_dependencies script was recently changed and the geofreebase download was removed while 2 downloads of the form geofreebase_vdb and geofreebase_ttl were added.   The problem is that when issuing ./download_dependencies geofreebase_ttl the download script tries to find a file called release-geofreebase_ttl.files that does not exist in the main directory of the project. The old release-geofreebase.files file is still there. I tried to manually download the files pointed by this file by they seem to have been taken off line.   Is it still possible to find the geofreebase ttl and vdb files as well as the full freebase ones ?  Thanks, Dimitris "
"Hi, Chen,    Thanks for your sharing code. If I'm making it correct, this is the source code for paper ""Compressing Neural Language Models by Sparse Word Representations"".   I'm not quite clear about how to solve the sparse coding problem, i.e.: min_x L(x) + R1(x) + R2(x), so I want to check the code for details. But I can't find any corresponding part in your released code.   Could you please give any help? Many thanks."
"Hey, is it possible to upload the weights for the trained model. This will be very helpful.  Thank you."
见       ，望后人乘凉
丹琦老师，我是国内的工程师，最近学习MRC，想跑一下相关模型，发现您提供的两个数据集链接失效了，是否有新的仓库链接提供？感谢！
"Hi, is there other alternative links to the preprocessed RC datasets. Since both of the links are dead"
"Hey, thanks for the code, very cool work.  After training the model, how can I test the trained model.    Thanks."
"Hi there, why the code is not available？I'm reading your paper and I'm interested in your code. Thanks. "
请问下大神  @tuzhaopeng  多谢多谢
"Hi,     I was trying to train a model using the following command on a custom dataset:  `python train.py --state german-data.py `  I tried this with theano version 0.8.2 and 0.9 as well.  I faced the following error:     Would appreciate any help.    Thanks!"
"你好，    我在运行THEANO_FLAGS='floatX=float32,device=gpu1'  python sample.py --beam-search --state search_state.pkl search_model.npz --beam-size 5 --source test/ctb.s --trans test/ctb.t    一直有这样的错，不知道是什么样的原因  theano.compile.function_module.UnusedInputError: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute t    <img width=""1132"" alt=""2017-03-28 11 51 31"" src="" "
你好：    我在使用的时候出现了“AttributeError: 'module' object has no attribute 'prototype_search_with_coverage_state'” 这个问题？    请教一下，这是什么原因呢。    !   
I am using the code with a modified objective function and I am observing a memory issue when using the thin stack version.    I am using the modified version of Theano. The GPU memory consumption starts off low and gradually increases till it eventually hits the limit and the code crashes.     I am only observing this issue with the thin stack version. The fat stack version runs fine.     Any help/hints where to look at would be helpful.     
"Loading ../snli_1.0/snli_1.0_train.jsonl Loading ../snli_1.0/snli_1.0_dev.jsonl [1] In open vocabulary mode. Using loaded embeddings without fine-tuning. [1] Constructing vocabulary... [1] Found 36675 word types. Traceback (most recent call last):   File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main     ""__main__"", fname, loader, pkg_name)   File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code     exec code in run_globals   File ""/home/jsc0606/Desktop/muse/spinn/python/spinn/models/fat_classifier.py"", line 823, in       run(only_forward=FLAGS.expanded_eval_only_mode)   File ""/home/jsc0606/Desktop/muse/spinn/python/spinn/models/fat_classifier.py"", line 513, in run     sentence_pair_data=data_manager.SENTENCE_PAIR_DATA)   File ""/home/jsc0606/Desktop/muse/spinn/python/spinn/util/data.py"", line 230, in BuildVocabulary     embedding_path, types_in_data, CORE_VOCABULARY)   File ""/home/jsc0606/Desktop/muse/spinn/python/spinn/util/data.py"", line 245, in BuildVocabularyForASCIIEmbeddingFile     with open(path, 'r') as f: IOError: [Errno 2] No such file or directory: '../glove/glove.840B.300d.txt' "
"Hi,  Most script I found are for re-producing the paper's output. Would you please explain how can I use it to calculate the final representation for given sentence?  Thanks "
Hi I have a clang error on my Mac OS 10.11.6 when running one of the tests. It works on Ubuntu 14.04.  I've installed the custom Theano.     This is the clang version.     I ran one of the tests `python -m spinn.tests.test_plain_rnn`. This gives me the following error (long so sorted and uniq-ed). Thanks    
"Hi,   Would you please provide the specific command to run Spinn with the checkpoint data on SNLI-like input string.  Thanks "
"i use python3 embeds/bert.prep.py  ,then it generate a folder named bert. Then i run the command ""python3 embeds/bert.py"".Unfortunately, it output a sentence""please provide embeddings , conl file and port"". But the folder bert contain five files.( bert_config  ,bert_model.ckpt.index, bert_model.ckpt.data,  bert_model.ckpt.meta, vocab)Which one i should input?"
‘the option of running a CRF has been added’---- how to run the crf version by the  common?
"Hello, and thank you for making this tagger available!    I tried running the tagger with the `--raw` and `--output` options, using an input file with one sentence per line and with space-separated tokens. But it seems that after prediction, once the `output_preds` function is called, the `pred_tags` for each sequence remains empty, and the output file stores only newline characters.    My current workaround is simply to reformat the input file to have one line per token and add my own dummy tags before parsing. But as far as I can see, the tagger successfully parses raw inputs anyway, and correctly stores the `words` and `tags` per sequence. This leads me to believe that the difference lies in the `predict` function behavior."
The link in the readme for downloading the polyglot embeddings does not work any more.
"When running bilty.py and loading trained model, the following error occurs:     It looks like line 113 in bilty.py ` tagger = load(args)` should be `tagger = load(args.model)` - is this correct?    "
to fix: need to reload tagger from model_path (if patience is used)
"code contains some old stuff from bilty, fails when using --save. Reported by @hectormartinez    ` File ""src/simplebilty.py"", line 78, in main      save(tagger, args)    File ""src/simplebilty.py"", line 135, in save      ""tasks_ids"": nntagger.tasks_ids,  AttributeError: 'SimpleBiltyTagger' object has no attribute 'tasks_ids'`"
"Hi,    I am applying a self-trained model to Twitter messages which might contain unicode emojis. The lstm tagger seems to have problems with those emojis.     for instance:       I can help myself with substituting those cases with a text constant before I call the tagger but I wonder if this problem is known?     I am using `python3` on a `linux` machine.    Best,"
"> Hi Barbara,  >   > I tried playing with your bi-LSTM tagger. [..]  However, after loading the source files successfully following  > your example, it throws an error:  > Traceback (most recent call last):  >   File ""src/bilty.py"", line 567, in    >     main()  >   File ""src/bilty.py"", line 86, in main  >     tagger.fit(args.train, args.iters, args.trainer, dev=args.dev)  >   File ""src/bilty.py"", line 222, in fit  >     self.predictors, self.char_rnn, self.wembeds, self.cembeds =  > self.build_computation_graph(num_words, num_chars)  >   File ""src/bilty.py"", line 261, in build_computation_graph  >     wembeds = self.model.add_lookup_parameters(""lookup_wembeds"", (num_words,  > self.in_dim))  > TypeError: add_lookup_parameters() takes exactly one argument (2 given)      Dynet's library has changed. Although the documentation moved, the source code of the tagger hasn't. Working on it."
"In a short test, I tried the sentences below to ascertain the accuracy of PathLSTM PropBank/NomBank for verb/nun sense. As for find out and go, the results seem puzzling and I wonder if there is something wrong with my setup, as these verbs should be find.03 and go.02 respectively:         I used the following models:   "
"Hi,     Its a great effort towards SRL Task 👍 I was looking for getting dependency path embeddings instead of end-to-end SRL pipeline. Could you please share or guide in achieving sample code which illustrates generation of dependency path embeddings with some sample text input. (may be dig into intermediate steps as well)    It would be great learning more about dependency path embeddings from you. Looking forward from you !!"
"I got the error above in two cases: when there are empty lines in the input file (so I got rid of them), and again immediately after getting `ERROR: sentence length mismatches token number in Stanford annotation`, maybe it has something to do with one of the words in that sentence being ""voila"" with an accented letter ""a"".    Is there a flag I can pass so that the pipeline will silently ignore such errors? On the same note, I've 23M sentences to label - do you think it's better to split them to N files and run N processes for `parse_fn.sh`, or I should stick to my current 1 file with 23M sentences?    Thanks!"
"Hey again, @microth  :)  I'm sorry to keep raising issues for problems this trivial, but its really hard to find help related to this package. Its just my deficiency in understanding  certain subjects I'm hoping you can help me with.    Uh, I was successful in using the complete pipeline on a sentence, and the result was this    ****************************************************************************    !     *****************************************************************************    I just wanted to know how I could get the FrameNet kind of semantic annotations, on the input text, like shown below. (Taken from the SEMAFOR Demo page.)    ******************************************************************************    !     *******************************************************************************    Like, 'born' has a Being_Born semantic frame to it, and I'd like to have my input text annotated as shown in the second picture. Is it possible? If you don't mind, could you guide me on how I could achieve this? Below are the arguments I passed    *******************************************************************************    `CompletePipelineCMDLineOptions options = new CompletePipelineCMDLineOptions();    String[] arss = {""eng"", ""-lemma"", ""/Users/vishnumohan/Desktop/LTh/PathLSTM-pre-illinois-built/src/main/java/se/lth/cs/srl/models/CoNLL2009-ST-English-ALL.anna-3.3.lemmatizer.model"",      ""-tagger"", ""/Users/vishnumohan/Desktop/LTh/PathLSTM-pre-illinois-built/src/main/java/se/lth/cs/srl/models/CoNLL2009-ST-English-ALL.anna-3.3.postagger.model"",      ""-parser"", ""/Users/vishnumohan/Desktop/LTh/PathLSTM-pre-illinois-built/src/main/java/se/lth/cs/srl/models/CoNLL2009-ST-English-ALL.anna-3.3.parser.model"",      ""-srl"", ""/Users/vishnumohan/Desktop/LTh/PathLSTM-pre-illinois-built/srl-FN17.model"",      ""-framenet"", ""/Users/vishnumohan/Desktop/LTh/fndata-1.7"",      ""-tokenize"", ""-reranker"", ""-externalNNs"", ""-test"", ""/Users/vishnumohan/Desktop/LTh/PathLSTM-pre-illinois-built/src/main/java/se/lth/cs/srl/tesen.txt""};    options.parseCmdLineArgs(arss);`    *******************************************************************************    After reading through the docs, I also saw people mentioning a `srl-ICCG16-eng.model` file. Could you provide me with a link, for the same?    Best Regards,  Vishnu "
"Hey, @microth   I've recently started using the PathLSTM, for an application which requires srl, but the sheer size of the model file ( srl-ACL2016-eng.model || 2.7G ), throws me an insufficient heap space - Out of memory error    *******************************************************************************************    Loading pipeline from   C:\Users\Vyso\Downloads\NLP\SRL\SEMAFOR\absSemafor\LTH\wttv\PathLSTM-pre-  illinois-built\srl-ACL2016-eng.model  Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space  at java.lang.reflect.Array.newInstance(Array.java:75)  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1883)  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1919)  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2231)  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2155)  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2013)  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2231)  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2155)  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2013)  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2231)  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2155)  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2013)  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2231)  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2155)  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2013)  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1919)  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2231)  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2155)  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2013)  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2231)  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2155)  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2013)  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)    Process finished with exit code 1    *******************************************************************************************  So naturally, I changed the heap space of both the JVM on my system, and my IDE, where I could change the vmoptions, as follows. (It was -Xms128m and -Xmx512m, by default).  *********************************************************************************************  custom IntelliJ IDEA VM options    -Xms2048m  -Xmx4000m  -XX:ReservedCodeCacheSize=240m  -XX:+UseConcMarkSweepGC  -XX:SoftRefLRUPolicyMSPerMB=50  -ea  -Dsun.io.useCanonCaches=false  -Djava.net.preferIPv4Stack=true  -XX:+HeapDumpOnOutOfMemoryError  -XX:-OmitStackTraceInFastThrow    *************************************************************************************************  But even after assigning around 4G as Max Heap space, I get the error. Funny thing is, in the memory management toolbar of my IDE, I can see that the code uses a max of just 500m, during runtime, so I really don't know how this heap space error is still getting thrown.    Could you tell me if this is unusual an error with this algorithm, or if you've seen it before, too?  Also, should I add all the mentioned dependencies just for the parse class to function? Is there a probability that this error gets thrown because of incomplete dependency additions?    Maybe it's just a beginner level mistake from my side, but I've been trying to get out out of this problem for quite a few days now, and I'd really appreciate it if you could instruct me on how I could get rid of this error.     Thank You,  Vishnu"
"Hello Mike,  I have trouble with parse method when model is loaded, it throws null pointer wxception in LibLinearModel.java on line 43, here is my stack trace:         Thanks in advance,  Daniel"
"Hello,    I didn't really understand how to use the `parse.sh` script for FrameNet SRL. I've all the models and libraries, including BISTparser and NLP4J, and FrameNet 1.5 (downloaded using NLTK).    From here, I'm pretty lost. How to retrain BISTparser and NLP4J using the 10-fold jackknifing, to recreate your results? Do I even have to do that?    Sorry if it's a newbie question, thank your for your time!"
"Hello,    I downloaded all dependencies and tried to `mvn compile`, but getting the above error.    A simple grep showed that 45 source files are importing this non-existing package. I tried switching to master branch, just to be sure, but no luck.    Thank you for your time :)"
"I tried directly using ""pathlstm.jar"" as I was unable to compile through ""mvn compile"". I am getting an error. Can you please tell me if I am doing something wrong as soon as possible.      java -Xmx40g -cp libs/anna-3.3.jar:target/pathlstm.jar se.lth.cs.srl.CompletePipeline eng -lemma models/CoNLL2009-ST-English-ALL.anna-3.3.lemmatizer.model -tagger models/CoNLL2009-ST-English-ALL.anna-3.3.postagger.model -parser models/CoNLL2009-ST-English-ALL.anna-3.3.parser.model -srl models/srl-ACL2016-eng.model -tokenize -reranker -externalNNs -test sample.txt    54.21.744  is2.data.ParametersFloat 121:read ->        read parameters 134217727 not zero 296071  54.21.763  is2.data.Cluster 113:  ->              Read cluster with 0 words   54.21.764  is2.lemmatizer.Lemmatizer 192:readModel ->  Loading data finished.   54.21.764  is2.lemmatizer.Lemmatizer 194:readModel ->  number of params  134217727  54.21.765  is2.lemmatizer.Lemmatizer 195:readModel ->  number of classes 92  54.26.6    is2.data.ParametersFloat 121:read ->        read parameters 134217727 not zero 1613201  54.26.6    is2.data.Cluster 113:  ->              Read cluster with 0 words   54.26.7    is2.tag.Lexicon 103:  ->               Read lexicon with 0 words   54.26.7    is2.tag.Tagger 141:readModel ->             Loading data finished.   54.26.55   is2.parser.Parser 188:readModel ->          Reading data started  54.26.102  is2.data.Cluster 113:  ->              Read cluster with 0 words   54.31.336  is2.parser.ParametersFloat 101:read ->      read parameters 134217727 not zero 19957525  54.31.336  is2.parser.Parser 201:readModel ->          parsing -- li size 134217727  54.31.354  is2.parser.Parser 211:readModel ->          Stacking false  54.31.355  is2.parser.Extractor 56:initStat ->         mult  (d4)   Used parser   class is2.parser.Parser  Creation date 2012.11.02 14:33:53  Training data CoNLL2009-ST-English-ALL.txt.crossannotated  Iterations    10 Used sentences 10000000  Cluster       null  54.31.361  is2.parser.Parser 240:readModel ->          Reading data finnished  54.31.363  is2.parser.Extractor 56:initStat ->         mult  (d4)   Loading pipeline from models/srl-ACL2016-eng.model  Loading reranker from models/srl-ACL2016-eng.model  Writing corpus to out.txt...  Exception in thread ""main"" java.lang.Error: Unresolved compilation problems:    PTBTokenizer cannot be resolved to a type   Word cannot be resolved to a type   PTBTokenizer cannot be resolved   Word cannot be resolved to a type     at se.lth.cs.srl.preprocessor.tokenization.StanfordPTBTokenizer.tokenizeplus(StanfordPTBTokenizer.java:35)   at se.lth.cs.srl.preprocessor.Preprocessor.tokenizeplus(Preprocessor.java:37)   at se.lth.cs.srl.CompletePipeline.parse(CompletePipeline.java:73)   at se.lth.cs.srl.CompletePipeline.parseNonSegmentedLineByLine(CompletePipeline.java:165)   at se.lth.cs.srl.CompletePipeline.main(CompletePipeline.java:138)"
"Hello @microth ,  I have tried running PathLSTM but when running it seems some classes are missing, could you instruct me any further?          I used the pre Illinois built branch because the main branch gives me:       Best regards,  Daniel"
Would anything break if I use version 3.7.0 for Stanford dependency? 
"I tried running the `scripts/parse.sh` script using the FrameNet model (`srl-ICCG16-eng.model`), but I got the error: java.io.InvalidObjectException: enum constant PathEmbeddingacN_FNET_seed3 does not exist in class uk.ac.ed.inf.srl.features.FeatureName. The other model (`srl-ACL2016-eng.model`) works fine, but the FrameNet model does not. I have `SRL_MODEL=models/srl-ICCG16-eng.model` in `scripts/parse.sh` and the FrameNet data in `models/fndata-1.5/`. I'm not sure where I should specify where the FrameNet data is though, or if I need to do that at all. Could the problem be that I didn't add something to the classpath? I haven't added anything because the other model worked fine without it.    Here is the stacktrace:    scripts/parse.sh tests/testParse1In.txt   54.53.188  is2.data.ParametersFloat 121:read ->        read parameters 134217727 not zero 296071  54.53.196  is2.data.Cluster 113:  ->              Read cluster with 0 words   54.53.196  is2.lemmatizer.Lemmatizer 192:readModel ->  Loading data finished.   54.53.197  is2.lemmatizer.Lemmatizer 194:readModel ->  number of params  134217727  54.53.197  is2.lemmatizer.Lemmatizer 195:readModel ->  number of classes 92  54.59.358  is2.data.ParametersFloat 121:read ->        read parameters 134217727 not zero 1613201  54.59.359  is2.data.Cluster 113:  ->              Read cluster with 0 words   54.59.362  is2.tag.Lexicon 103:  ->               Read lexicon with 0 words   54.59.364  is2.tag.Tagger 141:readModel ->             Loading data finished.   54.59.391  is2.parser.Parser 188:readModel ->          Reading data started  54.59.431  is2.data.Cluster 113:  ->              Read cluster with 0 words   55.6.812   is2.parser.ParametersFloat 101:read ->      read parameters 134217727 not zero 19957525  55.6.814   is2.parser.Parser 201:readModel ->          parsing -- li size 134217727  55.6.826   is2.parser.Parser 211:readModel ->          Stacking false  55.6.827   is2.parser.Extractor 56:initStat ->         mult  (d4)   Used parser   class is2.parser.Parser  Creation date 2012.11.02 14:33:53  Training data CoNLL2009-ST-English-ALL.txt.crossannotated  Iterations    10 Used sentences 10000000  Cluster       null  55.6.830   is2.parser.Parser 240:readModel ->          Reading data finnished  55.6.831   is2.parser.Extractor 56:initStat ->         mult  (d4)   Loading pipeline from models/srl-ICCG16-eng.model  java.io.InvalidObjectException: enum constant PathEmbeddingacN_FNET_seed3 does not exist in class uk.ac.ed.inf.srl.features.FeatureName   at java.io.ObjectInputStream.readEnum(ObjectInputStream.java:1746)   at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)   at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)   at java.util.HashMap.readObject(HashMap.java:1394)   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   at java.lang.reflect.Method.invoke(Method.java:497)   at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)   at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1900)   at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)   at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)   at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)   at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)   at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)   at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)   at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)   at se.lth.cs.srl.pipeline.Pipeline.fromZipFile(Pipeline.java:192)   at se.lth.cs.srl.pipeline.Pipeline.fromZipFile(Pipeline.java:226)   at se.lth.cs.srl.pipeline.Reranker. (Reranker.java:63)   at se.lth.cs.srl.CompletePipeline.getCompletePipeline(CompletePipeline.java:52)   at se.lth.cs.srl.CompletePipeline.main(CompletePipeline.java:122)  Caused by: java.lang.IllegalArgumentException: No enum constant uk.ac.ed.inf.srl.features.FeatureName.PathEmbeddingacN_FNET_seed3   at java.lang.Enum.valueOf(Enum.java:238)   at java.io.ObjectInputStream.readEnum(ObjectInputStream.java:1743)   ... 21 more  Writing corpus to out.txt...  Exception in thread ""main"" java.lang.NullPointerException   at se.lth.cs.srl.pipeline.Reranker.parse(Reranker.java:96)   at se.lth.cs.srl.SemanticRoleLabeler.parseSentence(SemanticRoleLabeler.java:12)   at se.lth.cs.srl.CompletePipeline.parseX(CompletePipeline.java:93)   at se.lth.cs.srl.CompletePipeline.parse(CompletePipeline.java:73)   at se.lth.cs.srl.CompletePipeline.parseNonSegmentedLineByLine(CompletePipeline.java:165)   at se.lth.cs.srl.CompletePipeline.main(CompletePipeline.java:138)"
"Hi, Micheal     For example, the result of sentence ""My cat is sitting on my book."" label ""cat"" and ""book"" to be cat.01 and book.01. But cat and book are nouns in this sentence (which is correctly identified in the POS column).     Shouldn't each predicate be a VB*?   I am quite confused.    Bill"
"Hi, Michael.     We are wondering if you can tell us how to know where the last term of a argument is.   For example, ""My room contains a book, a dog and a cat.""    In the demo web site, we can see that ""a book, a dog and a cat"" is the A1 of contain.01.   But in the table below (and out.txt) we cannot know that the cat is the last word of A1.   Should I change the code or something?     Thank you very much.  "
Where can i download the models from as stated below?    LEMMA_MODEL=models/lemma-eng.model  POS_MODEL=models/tagger-eng.model  PARSER_MODEL=models/parse-eng.model  SRL_MODEL=models/srl-ACL2016-eng.model
"Hey microth,    After fixing the previous  , I had a new issue.    My command :   `java -Xmx40g -cp ""libs/anna-3.3.jar:target/classes"" se.lth.cs.srl.CompletePipeline eng -lemma models/CoNLL2009-ST-English-ALL.anna-3.3.lemmatizer.model -tagger models/CoNLL2009-ST-English-ALL.anna-3.3.postagger.model -parser models/CoNLL2009-ST-English-ALL.anna-3.3.parser.model -srl models/srl-ACL2016-eng.model -tokenize -reranker -externalNNs -test models/text_to_parse.txt`    The output :      Concerning the downloaded files, stanford-corenlp-full-2016-10-31 is unzipped into the libs/ sub-directory.    Best,  Julien"
"Hi,   I'm working on a Ubuntu 16:04.    I created the two subdirectories libs/ and models/, where I put the librairies and models.  When I execute :  `.../PathLSTM$ ./scripts/parse.sh   `  the following error appears:  `Could not find or load main class se.lth.cs.srl.CompletePipeline`    Any idea to fix it?     Best wishes for the new year,  Julien"
"If you please, could you provide a downloadable reference for the other model files referenced in scripts.parse.sh?  Specifically, LEMMA_MODEL, POS_MODEL, and PARSER_MODEL.   "
"By running `./quick-start-cbt-ne.sh`, what might be the issue?    systems Ubuntu 16.04 with python and theano of        "
"Hi, I want to make a Keras version of Attention Sum Reader.  But after Attention layer, how to get the predict answer?  In theano, user  theano.scan, but in tensorflow or keras, what should I do? Thanks a lot!              def sum_prob_of_word(word_ix, sentence_ixs, sentence_attention_probs):                  word_ixs_in_sentence = tt.eq(sentence_ixs,word_ix).nonzero()[0]                  return sentence_attention_probs[word_ixs_in_sentence].sum()                def sum_probs_single_sentence(candidate_indices_i, sentence_ixs_t, sentence_attention_probs_t):                  result, updates = theano.scan(                      fn=sum_prob_of_word,                      sequences=[candidate_indices_i],                      non_sequences=[sentence_ixs_t, sentence_attention_probs_t])                  return result                def sum_probs_batch(candidate_indices_bt,sentence_ixs_bt, sentence_attention_probs_bt):                  result, updates = theano.scan(                      fn=sum_probs_single_sentence,                      sequences=[candidate_indices_bt, sentence_ixs_bt, sentence_attention_probs_bt],                      non_sequences=None)                  return result                # Sum the attention of each candidate word across the whole context document              y_hat = sum_probs_batch(candidates_bi, context_bt, mem_attention_bt)"
"     I think no model was found to fuse together, but am not sure what exactly is the problem...  Can you help me solve this? Any help will be much appreciated!"
"After loading data and when it starts to train, it gets wrong.  I tried the older version of block.Is it something wrong with the evironment?    ERROR:blocks.main_loop:Error occured during training.    Blocks will attempt to run on_error extensions, potentially saving data, before exiting and reraising the error. Note that the usual after_training extensions will not be run. The original error will be re-raised and also stored in the training log. Press CTRL + C to halt Blocks immediately.  Traceback (most recent call last):  File ""/home/wh/eclipseworkspace/Theano/asreader-master/asreader/text_comprehension/as_reader.py"", line 170, in  exp.execute()  "
"I tried running a the algorithm with the cnn dataset    specifically i run    `  python ./text_comprehension/as_reader.py \   -b 32 \   --train ~/cnn_like_train_dataset.txt \   --valid ~/cnn_like_valid_dataset.txt \   --test  ~/cnn_like_test_dataset.txt \   --save_path ~/asreader_data/model.blocks.pkl \   --output_dir ~/asreader_data/ \   --dataset_type cnn \   --no_html \   -ehd 64 \   -sed 64     `    Unfortunately training takes forever   the training file is ~20000000 lines long      The program also printed this :     `Blocks tried to match the sources  (['candidates_mask', 'question_mask', 'question', 'context_mask', 'candidates', 'context', 'answer'])  of the training dataset to the names of the Theano variables  (['answer', 'context_mask', 'context', 'question_mask', 'question', 'candidates']),  but failed to do so.  If you want to train on a subset of the sources that your dataset provides, pass  the `sources` keyword argument to its constructor. Or pass on_unused_sources='warn' or  on_unused_sources='ignore' to the GradientDescent algorithm.    `    I don't really know if this is a problem but ""candidates_mask"" is missing from the second set.      "
"I add ""--load_model"" in command to load trained model, but failed.  The error information is like this:    Loading model test_output_cnn/cnn_model.blocks.pkl ...    File ""text_comprehension/as_reader.py"", line 174, in        exp.execute()    File ""/home/qigan/asreader-master/asreader/text_comprehension/text_comprehension_base.py"", line 320, in execute      main_loop = load(model_file)  # load function from blocks.serialization    File ""/home/qigan/.local/lib/python2.7/site-packages/blocks/serialization.py"", line 261, in load      return p.load()    File ""/home/qigan/anaconda2/lib/python2.7/pickle.py"", line 864, in load      dispatch     File ""/home/qigan/anaconda2/lib/python2.7/pickle.py"", line 1139, in load_reduce      value = func(*args)    File ""/home/qigan/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.py"", line 1028, in _constructor_Function      (container.data == x)  AssertionError    I have struggled for days on it.   Hopefully you can help me with it. Thanks!  "
"rzai@rzai00:~/prj/asreader/asreader$ bash quick-start-cbt-ne.sh  2>&1 > tee yknote---quick-start-cbt-ne.sh---log  Using gpu device 0: GeForce GTX 1080 (CNMeM is disabled, cuDNN 5005)  Traceback (most recent call last):    File ""text_comprehension/as_reader.py"", line 166, in        exp.execute()    File ""/home/rzai/prj/asreader/asreader/text_comprehension/text_comprehension_base.py"", line 386, in execute      **self.args.evaluate_every_n)    File ""/home/rzai/prj/asreader/asreader/text_comprehension/monitoring.py"", line 79, in __init__      self._evaluator = MemoryDatasetEvaluator([context_attention, context,y_hat,y,candidates, candidates_mask, context_mask,x_mask,x], updates)    File ""/home/rzai/prj/asreader/asreader/text_comprehension/monitoring.py"", line 233, in __init__      self.theano_buffer = AggregationBuffer(theano_variables)    File ""/usr/local/lib/python2.7/dist-packages/blocks/monitoring/evaluators.py"", line 127, in __init__      _validate_variable_names(variables)    File ""/usr/local/lib/python2.7/dist-packages/blocks/monitoring/evaluators.py"", line 23, in _validate_variable_names      raise ValueError('Variables must have names: {}'.format(none_names))  ValueError: Variables must have names: [Elemwise{second,no_inplace}.0]  Using gpu device 0: GeForce GTX 1080 (CNMeM is disabled, cuDNN 5005)        Traceback (most recent call last):    File ""text_comprehension/as_reader.py"", line 166, in        exp.execute()    File ""/home/rzai/prj/asreader/asreader/text_comprehension/text_comprehension_base.py"", line 386, in execute      **self.args.evaluate_every_n)    File ""/home/rzai/prj/asreader/asreader/text_comprehension/monitoring.py"", line 79, in __init__      self._evaluator = MemoryDatasetEvaluator([context_attention, context,y_hat,y,candidates, candidates_mask, context_mask,x_mask,x], updates)    File ""/home/rzai/prj/asreader/asreader/text_comprehension/monitoring.py"", line 233, in __init__      self.theano_buffer = AggregationBuffer(theano_variables)    File ""/usr/local/lib/python2.7/dist-packages/blocks/monitoring/evaluators.py"", line 127, in __init__      _validate_variable_names(variables)    File ""/usr/local/lib/python2.7/dist-packages/blocks/monitoring/evaluators.py"", line 23, in _validate_variable_names      raise ValueError('Variables must have names: {}'.format(none_names))  ValueError: Variables must have names: [Elemwise{second,no_inplace}.0]  Traceback (most recent call last):    File ""text_comprehension/eval/copyBestPredictions.py"", line 124, in        print bestValModel['params']  KeyError: 'params'  /home/rzai/.local/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.    warnings.warn(""Mean of empty slice."", RuntimeWarning)  /home/rzai/.local/lib/python2.7/site-packages/numpy/core/_methods.py:70: RuntimeWarning: invalid value encountered in double_scalars    ret = ret.dtype.type(ret / rcount)  Traceback (most recent call last):    File ""text_comprehension/eval/fusion.py"", line 394, in        result = fuse_predictions(prediction_files)    File ""text_comprehension/eval/fusion.py"", line 158, in fuse_predictions      ensemble_accuracy = accuracy(numpy.mean(all_preds, 0))    File ""text_comprehension/eval/fusion.py"", line 57, in accuracy      for row in probas:  TypeError: 'numpy.float64' object is not iterable  rzai@rzai00:~/prj/asreader/asreader$   "
"Currently the accuracy is being reported for the test file, how can I modify the network to produce the answer given something like:     1 Sentence  2 Sentence   ...  ..  21 This is the question XXXXX ? Word|Word|Word|Answer|Word    The network should predict that Answer is the target answer. "
"Could you explain what exactly is the problem ?     i used  a cnn dataset_type    the full error is     `ERROR:blocks.main_loop:Error occured during training.    Blocks will attempt to run `on_error` extensions, potentially saving data, before exiting and reraising the error. Note that the usual `after_training` extensions will *not* be run. The original error will be re-raised and also stored in the training log. Press CTRL + C to halt Blocks immediately.  Traceback (most recent call last):    File ""./text_comprehension/as_reader.py"", line 166, in        exp.execute()    File ""/home/uname/ASReader/asreader/asreader/text_comprehension/text_comprehension_base.py"", line 413, in execute      use_own_validation=True)    File ""/home/uname/ASReader/asreader/asreader/learning_experiment_base.py"", line 370, in train      main_loop.run()    File ""/home/uname/.local/lib/python2.7/site-packages/blocks/main_loop.py"", line 197, in run      reraise_as(e)    File ""/home/uname/.local/lib/python2.7/site-packages/blocks/utils/__init__.py"", line 258, in reraise_as      six.reraise(type(new_exc), new_exc, orig_exc_traceback)    File ""/home/uname/.local/lib/python2.7/site-packages/blocks/main_loop.py"", line 183, in run      while self._run_epoch():    File ""/home/uname/.local/lib/python2.7/site-packages/blocks/main_loop.py"", line 230, in _run_epoch      self._run_extensions('before_epoch')    File ""/home/uname/.local/lib/python2.7/site-packages/blocks/main_loop.py"", line 263, in _run_extensions      extension.dispatch(CallbackName(method_name), *args)    File ""/home/uname/.local/lib/python2.7/site-packages/blocks/extensions/__init__.py"", line 338, in dispatch      self.do(callback_invoked, *(from_main_loop + tuple(arguments)))    File ""/media/large_space_1/uname/ASReader/asreader/asreader/text_comprehension/monitoring.py"", line 137, in do      context_strs = get_str_masked(self.context,i)    File ""/media/large_space_1/uname/ASReader/asreader/asreader/text_comprehension/monitoring.py"", line 132, in get_str_masked      return ints_to_words(word_ixs[:true_length])  TypeError: slice indices must be integers or None or have an __index__ method    Original exception:          TypeError: slice indices must be integers or None or have an __index__ method`"
I'm passing a subset of the training set in place of the original training set and I get the following attribute error from Theano.        
I'm running the ./quick-start-cbt-ne.sh and I get the following error:          I have the latest bricks as was suggested in #1     There are no predictions or logs being produced. Could you tell me what I'm wrong?    Thank you!  
"I run the ./quick-start-cnn.sh script, it can output predictions and logs, but there is no html file that visualize the attention.  how to set the argument?  Thanks a lot "
"Hi rkadlec,  I would like to initialize the lookup table with pre-trained embeddings such as Word2Vec or GloVe in the code.   I tried to figure out how to load other word vector model into blocks LookupTable but there is little information about it.     Do you have any idea for this problem?  FYI, I'm using gensim Word2Vec tool to load pre-trained model and trying to initialize lookup table with it.  Thanks in advance! "
"hi, rkadlec,  i run the  quick-start-cbt-ne.sh  ,but i  get the error, what's the problem , thank you in advance!  Computing new vocabulary for file ../data/CBTest/data/cbtest_NE_train.txt. Processed line 100000 Processed line 200000 Processed line 300000 Processed line 400000 Processed line 500000 Processed line 600000 Processed line 700000 Processed line 800000 Processed line 900000 Processed line 1000000 Processed line 1100000 Processed line 1200000 Processed line 1300000 Processed line 1400000 Processed line 1500000 Processed line 1600000 Processed line 1700000 Processed line 1800000 Processed line 1900000 Processed line 2000000 Processed line 2100000 Processed line 2200000 Processed line 2300000 STATISTICS Total words: 53825761 Total distinct words: 60278 STATISTICS Total words: 942535 Total distinct words: 10720 Added 920 new words from file ../data/CBTest/data/cbtest_NE_valid_2000ex.txt to previous vocabulary. STATISTICS Total words: 1213447 Total distinct words: 13277 Added 1311 new words from file ../data/CBTest/data/cbtest_NE_test_2500ex.txt to previous vocabulary.   File ""text_comprehension/as_reader.py"", line 166, in       exp.execute()   File ""/data1/arvinjin/text/test_as/asreader-master/asreader/text_comprehension/text_comprehension_base.py"", line 386, in execute     **self.args.evaluate_every_n)   File ""/data1/arvinjin/text/test_as/asreader-master/asreader/text_comprehension/monitoring.py"", line 79, in **init**     self._evaluator = MemoryDatasetEvaluator([context_attention, context,y_hat,y,candidates, candidates_mask, context_mask,x_mask,x], updates)   File ""/data1/arvinjin/text/test_as/asreader-master/asreader/text_comprehension/monitoring.py"", line 233, in __init__     self.theano_buffer = AggregationBuffer(theano_variables)   File ""/usr/lib/python2.7/site-packages/blocks-0.2.0-py2.7.egg/blocks/monitoring/evaluators.py"", line 127, in **init**     _validate_variable_names(variables)   File ""/usr/lib/python2.7/site-packages/blocks-0.2.0-py2.7.egg/blocks/monitoring/evaluators.py"", line 23, in _validate_variable_names     raise ValueError('Variables must have names: {}'.format(none_names)) ValueError: Variables must have names: [Elemwise{second,no_inplace}.0] "
"I am running with a very big file: about 150M lines, disk size 60GB, --num-workers 10, and then :  'vocab += pickle.load(f)' in learn_bpe.py  will report error: EOFError: Ran out of input.    tested on windows 10 os. I assume the 'tmp = tempfile.NamedTemporaryFile' introduce this?  anyone has such experience?    thx"
"Hi,    I am running this sockeye tutorial:      I encountered a problem while translating a sentence. This is the command I entered:  `echo ""ich weiss nicht"" | python -m apply_bpe -c bpe.codes --vocabulary bpe.vocab.de --vocabulary-threshold 50 | python -m sockeye.translate -m de-bar-base-model 2>/dev/null | sed -r ""s/@@( |$)//g""`    And this is the error I received:  `/home/jovyan/sockeye/subword-nmt/apply_bpe.py:398: DeprecationWarning: this script's location has moved to /home/jovyan/sockeye/subword-nmt/subword_nmt. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'    warnings.warn(  /home/jovyan/sockeye/subword-nmt/apply_bpe.py:420: ResourceWarning: unclosed file      args.codes = codecs.open(args.codes.name, encoding='utf-8')  ResourceWarning: Enable tracemalloc to get the object allocation traceback  /home/jovyan/sockeye/subword-nmt/apply_bpe.py:426: ResourceWarning: unclosed file      args.vocabulary = codecs.open(args.vocabulary.name, encoding='utf-8')  ResourceWarning: Enable tracemalloc to get the object allocation traceback  Traceback (most recent call last):    File ""/opt/conda/lib/python3.9/runpy.py"", line 197, in _run_module_as_main      return _run_code(code, main_globals, None,    File ""/opt/conda/lib/python3.9/runpy.py"", line 87, in _run_code      exec(code, run_globals)    File ""/home/jovyan/sockeye/subword-nmt/apply_bpe.py"", line 450, in        args.output.write(bpe.process_line(line, args.dropout))  BrokenPipeError: [Errno 32] Broken pipe  Exception ignored in:  ' encoding='utf-8'>  BrokenPipeError: [Errno 32] Broken pipe`    I understand that this is originating from a BrokenPipeError in apply_bpe.py.  Is there any way to counter this? Thank you!"
"In learn_bpe.py, the function prune_stats has code as follow:   for item,freq in list(stats.items()):          if freq < threshold:              del stats[item]              if freq < 0:                  big_stats[item] += freq              else:                  big_stats[item] = freq  I want to ask why the freq can bellow zero? This conditional judgment is for what?"
"When BPE is applied to machine translation, it can be encoded using a vocabulary.txt.  But when in the prediction stage, how do convert the numbers to text?  Thank you."
"Hi,    When running `apply_bpe.py` to segment given texts with the generated vocabulary I get the following error:         The exact command lines I used:       I added the vocab and train file I'm trying to segment:       A similar issue was reported here   , but it doesn't seem to solve the error in my case.    "
"I build the bpe vocabulary by doing the source and target vocabularies, in my case, English and tokenized Chinese(with THULAC). Applying bpe works for both English and Chinese data when their size is 1000000 lines, but it stops working after that number gets larger. I tried manipulating the bpe vocabulary size of 32000, 64000, etc. as well but didn't work. Anyone know a fix? Thanks!  "
"Hi, I have some problem in how to avoid '\t' being split into '@@ \t@@ '    I have tried `glossaries` after reading readme.me like this  `python subword-nmt/apply_bpe.py --glossaries \t,\n -c \    wmt17_en_de/code  wmt17_en_de/bpe.nocommer`    but it seems doesn't work, hope get some help from you : )"
"Hi, thanks for the great work! After reading the code about the vocabulary filtering strategy. I have a question and I want to know is there a possible solution.    Suppose I have a `En-De` dataset, after using `learn-bpe` on the concatenation of both datasets, I got a `code.bpe` file, with which I can segment both dataset.     But if in `En` dataset, there accidentally mix with a Chinese Word like `欲言又止` only appearing once. Even if I use `vocabulary-threshold` in `apply-bpe`, it will still be tokenized as `欲@@ 言@@ 又@@ 止`. And will be included in the vocabulary generated by `nematus/data/build_dictionary.py` which is definitely not what I want. What I want is `欲言又止`-->` `.    So I'm wondering is it more reasonable to add a `threshold` option in `build_dictionary.py` as well ? Thanks so much."
"Hi, I am confused about the usage of `subword-nmt learn-joint-bpe-and-vocab`. What is the edge case using joint-bpe ? Since all words will be `debpe` at test time. Why this could produce unknown words ? Thanks for answering."
Don't know how to use the `subword-nmt learn-joint-bpe-and-vocab` and what it can do.    `subword-nmt learn-joint-bpe-and-vocab -h` is not enough.    Example please.
"Hello Rico Sennrich,     When reading your paper ""Neural Machine Translation of Rare Words with Subword Units"", I found that you said, ""The final symbol vocabulary size is equal to the size of the initial vocabulary, plus the number of merge operations"". When I reviewed your given toy example, I found that if I set the number of merge operations to 1, there is a problem: 1) if the initial vocabulary points out the symbol vocabulary, the size of symbol vocabulary after merging is equal to the initial vocabulary (In the first merge operation, 'es' has the highest frequency and BPE merges 'e' and 's'. When we recount symbol vocabulary, 's' is removed and 'es' is added, so vocabulary keeps constant); 2) if the initial vocabulary points out word vocabulary, the size of symbol vocabulary is not equal to 5 (4 words plus 1 merge operation).    This problem confuses me these days, so could you use your toy example to tell me how to calculate the vocabulary size?    Thank you very much!!!"
"After training the model I wanted to do the translation in the section translation   but I only receive  `/opt/conda/envs/rapids/bin/python3: No module named apply_bpe`,    How to fix this problem?"
"I'm working on this Sockeye tutorial:      After running the preprocessing command, I have been getting this output for over 30 minutes.         Nothing has been changed yet. According to   it's just a debug tool. Is it normal, that preprocessing the WMT17 data took so long?    I want to make sure that the subword-nmt is working properly.  "
"Hello，  I am wondering is there any possibility to recover the code file of bpe when I only have bped-file.  with such bped-file, I can get the vocab by `split.(' ')`, and remove the @ by `re.sub(r'(@@ )|(@@ ?$)`, but i don't know how to encoder new raw text,thanks so much!"
"Hi,     I'm trying to implement BPE dropout using the tecnique you mention in the README, by creating an augmented training dataset  by concatenating the original training (5K sentences) dataset multiple times, and then applying BPE dropout on this. I'm just wondering do I have to apply the ""learn BPE"" method on the concatenated dataset or does it suffice to learn BPE on the original 5K dataset, and then to simply apply BPE with the dropout probability on the concatenated dataset using the vocabulary learned on the original dataset? "
"Hello,  I would like to use your learn_joint_bpe_and_vocab.py function to train a BPE tokenizer for Japanese.  The problem is, since Japanese's kanji script, similarly to Mandarin, doesn't separate words and writes everything in one long sequence.  So I was wondering if this feature of Japanese might affect the training of the BPE model, or does it support it somehow.  Thank you!"
"Traceback (most recent call last):    File ""subword-nmt/apply_bpe.py"", line 1, in        subword_nmt/apply_bpe.py  NameError: name 'subword_nmt' is not defined  "
I am trying to understand the format for the input file.  What I have is a new sentence on each line.  Am I suppose to have a \  at the end of every word in the file?
"Hello, thank you for your work!    I have a confusion about how to use BPE. For example, I follow your tutorial and generate corresponding train data, validation data, and test data. Then I use   pair (batch_size=64) to train my model. But in some papers, e.g. ""Attention is all you need"", they use Tokens (batch_size=2048) to train their model. Could you tell me how to achieve it and the difference between these two ways?"
Is it possible to add more tokens to the vocab and model after the pretraining of e.g. mBART or mRASP?
"Hello,    I am trying to generate a vocab.bpe file from a large corpus in Dutch to use for GPT-2 encoding.  I use the following command in subword-nmt/subword_nmt :   python learn_bpe.py -o ./vocab.bpe -i corpus --symbols 50000  Then after a while ""Killed"" is printed on the terminal  How should I proceed ?"
"Hello,    I am trying to replicate _Revisiting Low-Resource Neural Machine Translation: - A Case Study, paper_. I am not quite sure with the term BPE vocabulary in relation with subword-nmt    The paper mentioned BPE operations were 30000 which I assumed correlates with _bpe_operations_ parameters in subword_NMT.  How about **BPE vocabulary**, does this correlate with _--vocabulary-threshold_ parameters in subword_NMT ?    Thank you"
"Hello,    What sed -r 's/(@@ )|(@@ ?$)//g' means?    Do you have a python code for this, from code, not from command line if that applies to command line?    Thanks"
"Given that BPE dropout introduces randomness in the result of `apply-bpe`, it would be great if there was a `--seed` command-line argument that made it reproducible."
"I'm trying to use the new Facebook BlenderBot. This includes ParlAI and also subword-nmt.  When I attempt to run the examples I get an error indicating that subword-nmt needs to be installed.     Links to ParlAI:            Link to BlenderBot:       The result I'm seeing:  ilab@ilab-jupyterhub:~/ParlAI$ pip install subword-nmt  Collecting subword-nmt    Using cached    Installing collected packages: subword-nmt  Successfully installed subword-nmt-0.3.7    ilab@ilab-jupyterhub:~/ParlAI$ sudo python3 parlai/scripts/safe_interactive.py -t blended_skill_talk -mf zoo:blender/blender_90M/model  [ warning: overriding opt['task'] to blended_skill_talk (previously: internal:blended_skill_talk,wizard_of_wikipedia,convai2,empathetic_dialogues )]  [ warning: overriding opt['model_file'] to /home/ilab/ParlAI/data/models/blender/blender_90M/model (previously: /checkpoint/edinan/20200210/baseline_BST_retnref/lr=7.5e-06_attention-dropout=0.0_relu-dropout=0.0/model )]  Dictionary: loading dictionary from /home/ilab/ParlAI/data/models/blender/blender_90M/model.dict  [ num words =  54944 ]  Traceback (most recent call last):    File ""parlai/scripts/safe_interactive.py"", line 85, in        safe_interactive(parser.parse_args(print_args=False), print_parser=parser)    File ""parlai/scripts/safe_interactive.py"", line 59, in safe_interactive      agent = create_agent(opt, requireModelExists=True)    File ""/home/ilab/ParlAI/parlai/core/agents.py"", line 406, in create_agent      model = create_agent_from_opt_file(opt)    File ""/home/ilab/ParlAI/parlai/core/agents.py"", line 361, in create_agent_from_opt_file      return model_class(new_opt)    File ""/home/ilab/ParlAI/parlai/core/torch_generator_agent.py"", line 422, in __init__      super().__init__(opt, shared)    File ""/home/ilab/ParlAI/parlai/core/torch_agent.py"", line 710, in __init__      self.dict = self.build_dictionary()    File ""/home/ilab/ParlAI/parlai/core/torch_agent.py"", line 793, in build_dictionary      d = self.dictionary_class()(self.opt)    File ""/home/ilab/ParlAI/parlai/core/dict.py"", line 303, in __init__      self.bpe = bpe_factory(opt, shared)    File ""/home/ilab/ParlAI/parlai/utils/bpe.py"", line 79, in bpe_factory      bpe_helper = SubwordBPEHelper(opt, shared)    File ""/home/ilab/ParlAI/parlai/utils/bpe.py"", line 285, in __init__      ""Please run \""pip install 'git+   RuntimeError: Please run ""pip install 'git+   ilab@ilab-jupyterhub:~/ParlAI$    "
"I am training a NMT system where I need to apply BPE online as I train(don't mind my approach). I have my BPE model, and the sentences to encode.   Going through the documentation   , on installing `subword NMT` with `pip install subword-nmt`, there are no inline python functions to use. Or am I wrong? Are there inline python functions such as `applybpe(bpemodel,....)` that I can use?"
"I ran the fairseq-interactive from fairseq  . The environment is Windows 10 18363, Powershell 5.1.18362.628. Python version is 3.7.5.    The interactive executable failed to launch with       Further investigation showed that `arg_parse` in `apply_bpe::create_parser` use ""cp1252"" encoding if not specified in `argparse.FileType()` under my environment. Adding `encoding='utf-8'` to the function in the line of `--codes` temporarily fix the problem. But this is very hacky."
"Hello, I have a use case that needs the old style bpe algorithm that has the   as a separate space. How do I use that version?"
"Every line of the output file of learn.py is consisted of two subword units(for instance, `o f `)    I am a little bit confused by these two units, does that mean `o` and ` f ` can be merged into `of ` when applying BPE?    Thanks for answering!"
"Hello,    I want to train vocabulary on the custom text corpora and lately to add this vocabulary to pre-trained BERT vocabulary.  The thing is that pre-trained vocabulary has its intra-word boundary marker ## standing at the beginning of the continuing subword, for example:  * bird  -> [bi, ##rd ]    At the same time, when I train new vocabulary, I get a word tokenised  as :  * bird -> [bi##, rd]    Unfortunately, I could not find any way how to possible do it with subword-nmt. May be I am missing something?  "
"Hi  I am using BPE for language Hindi, is there something I need to change in scripts?  or it will run correctly as it is?  Thank you in advance."
"Hi, thanks for your great work!    I am a beginner in NLP, I am confused that after translation I get the prediction results,  but how to recover it? i.e.,  from    ""how long do we t@@ o@@ l@@ er@@ at@@ e it?""    to    ""how long are we going to tolerate it?""    I saw there are some issues have the same question but they were closed without an answer. Maybe this is a silly question. Thanks for your patience!  "
"Hi, If I use joint bpe; for example:   Training with joint bpe model and set the bpe vocab size is 64000;and in network vocabulary:  1.for source vocab 35000 + target vocab 35000 (slightly larger than 32000+32000)  2.for source vocab 70000 + target vocab 70000 (slightly larger than 64000+64000)    so which is right? how to set the size of bpe size and network vocabulary size?  "
"My corpus isn't a natural language, but rather JavaScript code. I am running:       But then my `jsbpe.txt` file has a ton of `@@`. I don't think it's really splitting into the encodings. Am I misunderstanding how to use this?  "
"I have a large corpus, around 40GB of text. I install subword-nmt via pip and try to make the dictionary with subword-nmt command line and it takes forever to finish. I just wonder whether there any solution for that situation?"
"I'm trying to use the package programatically. I'm doing         Is that correct? Also, I'm not sure of the `vocabulary_threshold`, since I do not see any default value. Is there any one?    Thank you."
None
"Hello, I am getting this error if I try to preprocess german data? I am curious if this works only for english?"
"Hi, Rico:  This may be a newbie question, so apologies in advance.  An undergrad student of mine and I are putting together a Python script that does a bunch of things. It learns a BPE and applies it to some files (Python is a nice way for things to work both in her Windows laptop and in my GNU/Linux machines).   I am currently launching (""subword-nmt apply-bpe"" and ""subword-nmt learn-bpe"") via subprocesses.  Is there a way to import this functionality into a Python script?  I have seen that from a Python shell I can type ""from subword_nmt import learn_bpe, apply_bpe"", but then if I try something like   infile=open(""test.tok.true.es"")  outfile=open(""/tmp/zzz"",""w"")  learn_bpe(infile,outfile,10000)  I get a ""'module' object is not callable"" type error.  Thanks a million in advance."
"I understand that by removing the `@@ ` symbols I get back to the input text, but how can I identify the smallest subunits in the processed text?    If for example I have `di@@ rect`, How can I figure out the smallest subunits, as I understand it, it could be {`di`, `rect`}, {`d`, `i`, `rect`}, {`d`, `i`, `re`, `ct`} and so on, since I don'nt know which part of `di` and which part of `rect` belongs to the subunit, and which part is unknown to the tokenizer.   How do I know what part of a word which contains is part of the binary pair, and which part is the rest of the word?    I'm sorry, if I just got the overall concept wrong, but I can't figure this out. "
"I trained two vocabularies with about 900M Chinese-English materials, and then coded two data sets (900M training set and 500K test set) with these two Chinese-English vocabularies.    The training set can get normal results, but there are many @ in the test set.    Before using subword-nmt for bpe, I had participled Chinese.    The corresponding instructions are as follows:    > python learn_joint_bpe_and_vocab.py --input data/train.en data/train.zh -s 32000 -o data/bpe32k --write-vocabulary data/vocab.en data/vocab.zh  python apply_bpe.py --vocabulary data/vocab.en --vocabulary-threshold 50 -c data/bpe32k   data/corpus.32k.en  python apply_bpe.py --vocabulary data/vocab.zh --vocabulary-threshold 50 -c data/bpe32k   data/corpus.32k.zh  python apply_bpe.py --vocabulary data/vocab.zh --vocabulary-threshold 50 -c data/bpe32k   data/aval_bpe_enzh.zh  python apply_bpe.py --vocabulary data/vocab.en --vocabulary-threshold 50 -c data/bpe32k   data/aval_bpe_enzh.en    The result is shown in the figure.    !   !     I don't know where the problem is. Please help me to answer it. Thank you very much.      "
None
"Hi Reco,    You mentioned the vocabulary size is the number of characters + # bpe_operations. Does that mean the network is based on characters and bpes, not (words and bpes)? For example, for the following text which is segmented using bpes, what's the vocabulary size?    e.g.,  low lo@@ w@@ e@@ r    If we count characters and bpes: 7  Or if we count words and bpes: 5"
"My train text data is in Chinese. and it reports UnicodeEncodeError when using `subword-nmt learn-bpe`command with verbose mode on, while it works fine with learn_bpe.py script.  Then I check the code and find out the reason.     It seems that command line `subwor-nmt learn-bpe` doesn't run the above codes, then `sys.stderr` used by verbose mode (see below)  would be the default system stderr，which encodes unicode with ""ascii"" encoding.      "
"Hi there, I want to add some special tokens into my corpus, such as  , and keep it fixed. But apply bpe will tokenize it as ` `.   Is there a way to skip some special tokens?  Thanks."
"Hi, can you please guide me how to do post processing on the output files generated by nematus of code generaton and code documentation?  @rsennrich     Thank you :)  "
"Hi,    I have been using this code for sometime but lately I've been getting this issue.     `no pair has frequency >= 2. Stopping`    What's strange about this is that it works with some input files whereas it fails with the above message for some files (although these are exactly in the same format as required)    What exactly is the problem? Also, any ideas about how to fix this? I have already tried the solution suggested in #29 but that didn't help much.    thanks!"
"Hello,  Could you please tell me, given a corpus, how do we decide on the number of merge operations? Should we use the same number of operations on both the source and the target sides?    Thank you."
"In case, if I want to remove a generated BPE dictionary in a text file (supposing that I am not provided the original training/dev/test data). Can I do it?   "
"Hi, I have been using `apply_bpe` from October 2016. I tested a recent copy of `apply_bpe` and the number of segments are significantly lower than before. I am using exactly the same settings and codes as before. Has something changed significantly in the algorithm for applying the codes that may be causing this issue?    "
"Below a feature suggestion path.    The number of operations specified with -s leads to very different vocabulary sizes, due to the number of unique characters to start with. A value of 49500 creates small vocabularies for language that use Latin alphabet, but easily 70000 or so for Chinese. So, it would be good to subtract the number of unique characters from the number of symbols being generated.     +++ b/subword_nmt/learn_bpe.py  @@ -56,6 +56,9 @@ def create_parser(subparsers=None):       parser.add_argument('--dict-input', action=""store_true"",           help=""If set, input file is interpreted as a dictionary where each line contains a word-count pair"")       parser.add_argument(  +        '--total-symbols', '-t', action=""store_true"",  +        help=""subtract number of characters from the symbols to be generated"")  +    parser.add_argument(           '--verbose', '-v', action=""store_true"",           help=""verbose mode."")    @@ -197,7 +200,7 @@ def prune_stats(stats, big_stats, threshold):                   big_stats[item] = freq      -def learn_bpe(infile, outfile, num_symbols, min_frequency=2, verbose=False, is_dict=False):  +def learn_bpe(infile, outfile, num_symbols, min_frequency=2, verbose=False, is_dict=False, total_symbols=False):       """"""Learn num_symbols BPE operations from vocabulary, and write to outfile.       """"""    @@ -211,6 +214,16 @@ def learn_bpe(infile, outfile, num_symbols, min_frequency=2, verbose=False, is_d         stats, indices = get_pair_statistics(sorted_vocab)       big_stats = copy.deepcopy(stats)  +  +    if total_symbols:  +        uniq_char = defaultdict(int)  +        for word in vocab:  +            prev_char = word[0]  +            for char in word[1:]:  +                uniq_char[char] += 1  +        print('Number of characters: {0}'.format(len(uniq_char)))  +        num_symbols -= len(uniq_char)  +       # threshold is inspired by Zipfian assumption, but should only affect speed       threshold = max(stats.values()) / 10       for i in range(num_symbols):  @@ -270,4 +283,4 @@ if __name__ == '__main__':       if args.output.name != ' ':           args.output = codecs.open(args.output.name, 'w', encoding='utf-8')    -    learn_bpe(args.input, args.output, args.symbols, args.min_frequency, args.verbose, is_dict=args.dict_input)  +    learn_bpe(args.input, args.output, args.symbols, args.min_frequency, args.verbose, is_dict=args.dict_input, total_symbols=args.total_symbols)"
Can this tool  be used to do Chinese word segmentation?  Thank you very much!
"Installed with pip and python3 venv, calling the subword-nmt modules throws this error.    `subword-nmt learn-bpe -s 3000 < input.txt`         I think we have to change the import to .learn_bpe "
Have you considered packaging this up as a module and submitting to the  ? It's not too time-consuming to do and would make it very easy to include the code in downstream projects (including incorporating it as an explicit dependency).
"Hi @rsennrich ,    Can you please shed some light on how to use the glossaries field?   What file format? How should it be structured, etc?    Thanks,  mzeid"
"Hi,   it seems `apply_bpe.py` duplicates empty lines, minimal example:         and twice as many with the script.         Can you reproduce this?"
"When training with `learn_bpe.py` and then trying to use the generated file, I get the following error:          This error message is not very useful as it does not indicate on what line the error is."
"When I use the following command for BPE operation：  ./apply_bpe.py -c bpe.train.mn        bpe = BPE(args.codes, args.merges, args.separator, vocabulary, args.glossaries)    File ""./apply_bpe.py"", line 45, in __init__      self.bpe_codes_reverse = dict([(pair[0] + pair[1], pair) for pair,i in self.bpe_codes.items()])    File ""./apply_bpe.py"", line 45, in        self.bpe_codes_reverse = dict([(pair[0] + pair[1], pair) for pair,i in self.bpe_codes.items()])  IndexError: tuple index out of range  I would like to ask what is the reason for this.  Looking forward to your advice or answers.  Best regards,    yapingzhao  "
"  Hi, i want to know why do you use byte pair encoding for code generation task to further sub-tokenise it ? can't we proceed further directly to nematus without byte-pair encoding?  "
"In the code generation we have to give declation+docstring as x.train and body as y.train. So, i am following your code and i did byte pair encoding as i am new to this field i really want to show you my work so that you can tell me whether i am going in a right direction or not.      As directed .. this is my vocab file for declaration and description which i got after using the shell command    <img width=""641"" alt=""screen shot 2018-04-10 at 17 15 03"" src=""     this is the vocab file of body   <img width=""420"" alt=""screen shot 2018-04-10 at 17 17 55"" src=""     this is the declaration_description.bpe file  <img width=""632"" alt=""screen shot 2018-04-10 at 17 18 56"" src=""       this is the body.bpe file  <img width=""627"" alt=""screen shot 2018-04-10 at 17 19 48"" src=""       so , can u please guide me whether i am going in the right direction or not.  Thank you so much in advance 👍   "
"I'm trying to replicate English-German benchmarking using this link:  . When I closely look at result file it has duplicates like 'Flank@@' 'flank@@', 'Check-In' 'check-in'. I removed these duplicates by sorting and filtering but somehow my bleu scores get messed up (test and dev bleu scores show 0.0). I'm thinking it because I modified my vocabulary file it's reading it incorrectly. Attached is the vocab file.      "
"There are several sentences in WMT dataset that used to work fine, but currently break, I was able to narrow down the problem    1) Double space and special caracters:       2) Empty string:           Both of those worked with older versions of subword-nmt, e.g. with this one: 8247488"
"To reproduce:       I know that having EOL before EOF is a good thing and everyone better do it, but no one is protected from it."
I am running into a test case failure on a fresh clone of the repo. It's only a method signature error but I just wanted to confirm whether the test case failure to safe to disregard ?    Steps to reproduce.   
" python -m learn_joint_bpe_and_vocab --input corpus.en corpus.ch -s 30000 -o bpe.codes --write-vocabulary bpe.vocab.en bpe.vocab.ch  After running the above command, the following hints appear：no pair has frequency >= 2. Stopping。  I don't understand what this message means. I hope to give it a answer, thank you.  "
Hi @rsennrich! this is quite an interesting work! I'm interested in applying this subword units (BPE) approach for translation from Arabic to English. I see that this approach seems to work only for `romanized` text encodings. Are there any ways to apply it for _Arabic_ text?     My idea is to generate these subword units for _Arabic and English_ and train **Ar-En NMT** system using  . I know the scripts work for English but I'm not sure about how to achieve the same subword unit generation for `Arabic`.
"not sure it's highly relevant, but it may be of help if someone runs into this again. when running `apply_bpe.py` on a specific file with 1.9 million lines, the script outputs 4 extra lines. after a few hours of investigation it boils down to the `codecs` module: `for line in args.input:` produces the extra 4 lines. replacing `codecs` with `io` solves the issue.    unfortunately the extra lines are at random positions in the middle and once i worked around this i didn't spend extra time on finding where they come from and what may be anomalous in the data to trigger this.    i'm happy provide the offending file on request."
"Is it suitable for any language? I want use it in Chinese, is it correct ?"
"I think there is a serious bug in `apply_bpe.py` resulting from having a mutable default parameter (`cache={}`) in the `encode` function (     I guess it's fine if you use the script as a command-line tool, but if I use multiple instances of the `BPE` class directly, I observe the following behavior:     During the first `segment` call the `cache` dictionary is modified during `encode`, and the same instance of `cache` is then used in the second call of `encode`, essentially overwriting the codes loaded from file.    If I make the following simple modification     I get the expected behavior:       See also  "
"I have recently found out   post by google about the transliteration models implemented with `FST` encoder/decoder used on the Gboard keyboard. I'm not sure which is the difference between `BPE` presented   and the FST used by Google. Any hint, thank you?"
"Hi,    I read your paper of BPE, that's a great work. I just wondering where is the result of newstest2014, seems I couldn't find the reported BLEU score. Am I missing anything?    Thanks."
README has a small typo. Flag --write-vocabulary is listed as --write_vocabulary for sample command to learn joint bpe
"I'm not sure whether this is helpful but I wrote some comments on the `update_pair_statistics` function:      @rsennrich if you find it's useful. I'll create a PR for merging otherwise, I'll leave it in my fork so that I don't get stunned for 10-15 mins every time I revisit the function ;P"
"@rsennrich What's the motivation for change in the ` ` addition in the ""tuplization"" of the word at   ?    Previous, it was:         Now it's:     "
None
"For this fake corpus   `when engage what`  Its character vocabulary size is 7 (`e a h w n g t `).   Lean BPE by two num_operations, and apply it with the two generated codes (`wh` and `en`), we get:  `wh@@ en en@@ g@@ a@@ g@@ e wh@@ a@@ t`  The final vocabulary size is 7 (`a@@ wh@@ g@@ e t en en@@` ), not 9.   Do I calculate it wrong?    In my opinion, the equation `Final vocabulary size = character vocabulary + num_operations` based on the assumption that every merge operation generates one new token.   But in this case, the merge operation of `e` and `n`, generates two token `en` and `en@@` in the encoded text, and this phenomenon is totally unpredictable. To make sure there is no unknown word, the final vocabulary size should be 18 ??   (`e a h w n g t wh en e@@ a@@ h@@ w@@ n@@ g@@ t@@ wh@@ en@@ `)  I am really confused !   How to generate the final vocabulary, and how to control its size exactly ?    "
"Hi,    An MT system can generate hypotheses **ending** with a subword token which are not cleaned by the given sed pattern `s/\@\@ //g`:   "
I'm using the latest github clone from this repository and I've noticed the following (Note that I removed the shebang line from `learn_bpe.py` so that it does not default to `/usr/bin/python`):       I also attached the `input.en` file (as input.en.txt)         
"Hi Rico,    Let say that I want to have the final vocabulary size of the input corpus is V_e. Would it be possible using the current implementation?    For example, Google wordpieces is trained to select ""D"" subwords that maximize the language-model likelihood of the training data.    Thank you!"
"Hi, Would it make sense to add a minimal frequency threshold instead of a maximal number of symbols? That way we could make sure that the symbols are being seens at least N times during NMT training.  "
"./learn_bpe.py -s {num_operations}   {codes_file}  what is num_operations and how to set it?  I think it maybe the number of tokens in the final vocabulary, so I executed the above command using 30000 for 'num_operations' parameter on a machine with 32GB of memory. After two days, it's still running while takes all the available memory (32GB) + 15GB of swap space. Is it normal?! "
"Hi Rico  First, thanks a lot for releasing your code!  I have a few questions w.r.t. to the implementation/usage 1. I tried to learn a model/codes  on the french europarl corpus and encode the same text using the learned model. When I use  5000 or 10000 symbols i get a vocabulary size of 5464 and 10435 respectively on the encoded text. (checked using the script `get_vocab` on the encoded text. The trained models/code files do contain 5000 or 10000 lines as expected). I this intended behaviour or am I doing something wrong? 2. Whats the best way to process the encoded text downstream of `apply_bpe.py`. Should i just encoded as i would normally with a space separated tokenized text - Specifically I'm thinking how you go about extracting the vocabulary, do you get that from the model/codes file or extract that from the encoded text?   BR  Casper Sønderby "
"pair 1920: \pccsrv \ -> \pccsrv\ (frequency 2798) Traceback (most recent call last):   File ""./learn_bpe.py"", line 201, in       changes = replace_pair(most_frequent, sorted_vocab, indices)   File ""./learn_bpe.py"", line 145, in replace_pair     new_word = pattern.sub(pair_str, new_word)   File ""/usr/lib/python2.7/re.py"", line 273, in _subx     template = _compile_repl(template, pattern)   File ""/usr/lib/python2.7/re.py"", line 260, in _compile_repl     raise error, v # invalid expression sre_constants.error: bogus escape (end of line) "
"Running the command:     leads to this error:     This is due to the fact, that the reference contains no 6-gram. Changing line 96 to     fixed the problem for me, but I am not sure if this is the right way to do it.  Anyway thank you for sharing your implementation of chrF! "
"In this part of the R-NaD Code, `rho` is set to inf       This contradicts the value of 1 in the readme. Is this intentional?"
"Hi, I met few problems when I try to compile with librtorch.  I can use this Open Spiel without libtorch correctly, but when I:  1. Turn on 'BUILD_WITH_LIBTORCH' and 'BUILD_WITH_LIBNOP'  2. run ./install.sh # download libtorch  3. run ./open_spiel/script/build_and_run_tests.sh  And I met these errors:       And I think the problem might be related to linking with ABSL?  Please help me solving this problem, big thank you!"
"It seems that the notation you use in Quoridor is not one that I have encountered.  The rows and columns are ordered fine (rows 1-9, columns a-i), but the starting player is at the bottom (starting at e9).  Neither in Glendenning's Notation nor in Modern Algebraic Notation this is the case.  I would be happy if you will consider changing the board orientation.  It is just that when looking at games and strategies for quoridor I can't match write it into the board because of this difference.  Thanks!"
"I might have to edit this later to better illustrate what I mean.    !     This is the formula for the estimate of the state value and Q-value used in DeepNash    !   And here are these estimates used in an update calculation.    Briefly, we can see in the picture the update is basically the logit of an action times the q-value of that action.  However the code used in the policy update is very different              adv_pi = q_vr - torch.sum(pi * q_vr, dim=-1, keepdim=True)            adv_pi = is_c * adv_pi  # importance sampling correction            adv_pi = torch.clip(adv_pi, min=-clip, max=clip)            adv_pi = adv_pi.detach()          logits = logit_pi - torch.mean(logit_pi * legal_actions, dim=-1, keepdim=True)            threshold_center = torch.zeros_like(logits)            nerd_loss = torch.sum(                legal_actions * apply_force_with_threshold(logits, adv_pi, threshold, threshold_center), axis=-1            )            nerd_loss = -renormalize(nerd_loss, valid * (player_ids == k))            loss_pi_list.append(nerd_loss)      Here we can see the gradient is in the direction of the logit times the *advantage*. Additionally, we are also translating the logits so they have mean 0. This was not mentioned in the paper.  More importantly I see this `eta_reg_entropy` term in the code, which seems to be the reward shaping term:  `eta_reg_entropy = -eta * torch.sum(merged_policy * merged_log_policy, dim=-1) * torch.squeeze(player_others, dim=-1)`    !   However this is not quite the same as the update in the paper. In the code, we seem to have this extra `merged_policy` (pi) term in the multiplication of eta.    I've been putting this question off for a while, so sorry if this seems hasty    "
"The EFG parser should not depend on the presence or uniqueness of action labels.    Gambit's   states  > it is encouraged for users to assign nonempty text labels to objects if the game is going to be viewed in the graphical interface.    The current EFG parser uses labels to identify actions, and fails in cases where the labels are empty or even non-unique.    Consider this game with a value of 4:         With no labels, OpenSpiel thinks the value is 8; with labels, it correctly returns 4. Gambit returns 4 in both instances.       Test code in `pygambit` for reference.   "
"Is laser_tag not available for this game? When I run example --game=laser_tag, the following message appears:  Creating game..    Starting new game...  Initial state:  State:  .......  .......  ..*.*..  .**.**.  ..*.*..  .......  .......  Orientations: 1 1  Chance Node  player -1  sampled outcome: (spawned at location #2)  State:   .......  .......  ..*.*..  .**.**.  ..*.*..  .......  B......  Orientations: 1 1  Chance Node  player -1  sampled outcome: (spawned at location #1)  State:   ......A  .......  ..*.*..  .**.**.  ..*.*..  .......  B......  Orientations: 1 1    player -2  Spiel Fatal Error: InformationStateTensorShape unimplemented.  When I run pyspiel.load_game(""laser_tag"",{""players"": 2}) I get the following error:  pyspiel.SpielError: Unknown parameter 'players'. Available parameters are: grid, horizon, zero_sum"
"Hi,    I find it interesting to model prices from the point of view of marketing in an entrepreneurial setting. This is relevant because the product is sold on the basis of the entrepreneur's characteristics too along with the product and consumer economics.     So, the environment comprises of consumers that adopt the new product (Bass, 1969 (  Adoption is driven by their innovativeness which is relative in that an individual has either more or less of it than others in a social system (Rogers & Shoemaker, 1971).    For B2B products, early adopters are interested in adopting the product early and using it in their supply chain. Innovators are interested in trying the product and take risks and can also buy products that eventually do not succeed in the marketplace. And then there is a segment of customers that are price conscious.    Do you want me to take a deeper look into this game between the entrepreneur and the environment. This may be important to understand from the technology point of view as IP creation may be involved and the game may have serious cascading implications for the entire supply chain."
"Hi,   I am planning to contribute a JAX implementation of   if no one currently works on it and if it is still appreciated (I noticed it was mentioned in your call for contributions list).    I just wanted to check before starting to implement it :slightly_smiling_face: "
"Hi there, suppose I have a pytorch model, and I want to use it to generate trajectories for reinforcement learning, for example REINFORCE. The code in python should be like     but it seems like a bit slow in python, so i'm wondering if I can use pybind and libtorch to generate trajectories in parallel and get buffers return to python. But I can't find an example, that is, is it possible to pass a network to libtorch and get tensors which need gradients back to python?    Also, I don't know whether it will be faster or should I write this totally in C++?    Any idea for this?"
"Is it possible to release some well-trained models for the games supported by the open spiel framework such as connect four, breakthrough, and othello?"
"As the question says, my code tries to compare two different game states. When is `state1 == state2`?    From my limited knowledge of pybind11, I can see that equality for games is determined from this code snippet:      However, I cannot find anything similar for states. Any help will be appreciated!"
"Hi all,     I'm starting a thread to collect requests for new games and plans for upcoming games. We will keep this updated as new games get released, and as requests come in (please feel free to comment on this thread to add a request).     Games we would like to add to OpenSpiel:    - Capture-the-Flag on a gridworld (or other team adversarial games)  -    -   (see    -    -    - Dominoes  -  : (PR submitted!    -    - Latent Tic-Tac-Toe (see description in  )  -    -   (PR submitted:    -    - Pursuit-evasion game played on a graph or grid  -    -  )    Implementing one of these games is a great way to get involved with contributing to OpenSpiel. If you would like to volunteer to implement one, please contact us and let us know as we might already be working on them.     Games that have been added since posting this thread:  -  ) (Thanks @Jazeem. PR now merged:    - Checkers (thanks @Jazeem! PR now merged:    - Euchre (thanks @jhtschultz! Added in    - Mancala (Kalah) (thanks @Jazeem! see    -   (thanks @acforvs! PR merged:    - Phantom Go (thanks @Syor! PR now merged:      With these additions, OpenSpiel now has 95 games. Can we make up to 100 in 2022?  "
"I found Openspiel super useful in my research. I am wondering if the API is friendly to any of the popular multiagent RL frameworks(e.g. rllib, stablebaselines3, tianshou) so that we can use different algorithms/develop custom algorithms directly from the RL frameworks? "
"Hello, first off thanks for creating this amazing group of environments for board games! It is only place where I have found an implementation of ReconBuildChess besides APL's own implementation.    I've been trying to utilize the RBC environment and here are some issues/pain points that I have encountered and thought I'd share:    Note: I am using the RBC environment through a gym like wrapper created by the   team      So far my main goal has been to recreate the random, attacker, and trout bots that APL provides located  .    is my progress. Which I have been able to recreate with some slight issues and workarounds.    Ultimately, I'd like to have the same API for creating bots as APL does so that a class that defines an APL bot can be used one to one for a ray/rllib policy with their open_spiel wrapper. Right now I have a rough skeleton on where the APL bot functions go in the ray/rllib pipeline, but don't separate them into their own functions.    Basically, be able to train an RBC agent using a rllib with the open_spiel environment and then deploy it on the APL rbc servers.    Anyway here are some issues I have had concerning open_spiel's rbc implementation.    ## 1) Not all sense actions are encoded.       From what I understand of the above code, since a sense grid is 3x3, it make less sense for an agent to sense at the edges of the board, i.e. ranks 1 and 8 and files a and h. First off, this is different from APL's implementation as they do allow you to sense any location. Secondly, this is currently not implemented properly as the `cached_legal_actions_` is simply resized. this gives the first (inner_size x inner_size 6x6) 36 actions, instead of the 36 actions corresponding to the inner board. Effectively, allowing the agent to sense file a, but not files g or h.    I believe the proper way to fix this is to allow the agent to sense anywhere on the game board, even on the edges, for APL consistency's sake, but honestly just fixing the logic for the `cached_legal_actions_` should be good enough.    Related commit:      ## 2) Return the previous taken_action in ObservationString/ObservationTensor    RBC is an uncertain game and the requested action may not be the action that is taken due to the board state. This information is required for agents to properly keep track of the game state and is information that APL's RBC provides. See 6i) for more info on how I incorrectly workaround this issue currently using state.history. But essentially, the previous taken_action should be provided to the agents via both the ObservationString and the ObservationTensor. See APL's   for more info.    Maybe this information exists, but I have not been able to extract it from ObservationString.    Current rationale:         I believe this should be changed to provide the information explicitly in the ObservationString/ObservationTensor.  This could also help speed up learning, as this important doesn't have to be extracted.    ## 3) Return your captured piece and location in ObservationString/ObservationTensor    Previously related issue:      APL's RBC will return the your captured piece and location for you to handle any internal logic based on the opponents move. This allows you to update say your internal board representation based on pieces you have lost.  From what I can tell, in open_spiel, you are told that your piece is captured, but not which one or where.         This looks to be missing functionality.  See   for more info.    Current rationale:     I believe this should be changed to provide the information explicitly in the ObservationString/ObservationTensor.    This could also help speed up learning, as this important doesn't have to be extracted.    ## 4) Return the previous sense action in ObservationString/ObservationTensor    This is similar to (2): return the previous taken_action. The ObservationString contains the current observed board and does contain the newly sensed information when calling state.observation_string() during the move phase. However, the open_spiel env should return which grids were requested to sense so that the agent can parse the ObservationString to determine the latest sense results. See:   for what APL provides the agents.    ## 5) Pawn promotion to queen by default    Certain moves that are illegal will still have an effect on the board and this case is not captured in open_spiel's implementation. Particularly, say my pawn is at d7 and the move my algorithm choose is d7e8 to capture an opponent piece. Based on the legal moves (for both open_spiel and APL), d7e8 does not exist, but d7e8q does. (with the queen promotion). APL's RBC will convert my d7e8 to d7e8q but open_spiel's rbc will throw an error saying d7e8 does not exist.      -------------------------------------------------------------------------------------------------------------------------------------------------------------------------    Note: The qualms below are only valid since (2)-(4) are not currently properly implemented. This has made try to use state.action_to_string() to attempt to obtain the missing information in uci form, which made me realize the limitations that the current `action_to_string()` method has. (6) may not/is not worth the time if (2)-(4) are properly handled and the information is provided in ObservationString.    ## 6) Cannot call `action_to_string()` and `string_to_action()` with the different action spaces (sense action space, move action space).          This one might be a bit harder to explain, but essentially when creating an agent that uses the uci action encoding (for example an agent based on stockfish) then moving between the open_spiel action representation and the uci representation is important. Moreover, it is important to be able to use the `action_to_string` method on the input space of move actions while I am currently in sense mode and vice versa. Here are the following cases where it is particularly needed.    i) During an agent's sense phase, the agent would like to   of its last move that it took. This means that (See (2)) the open_spiel env should provide the agent the taken_move (note which is different from the requested move as rbc is an uncertain game). This taken_move should be provided in the `ObservationString`, but since it is not I actually use the `state.history()` to obtain the last requested_move by the player (which is in open_spiel move action space). For me to convert this into something that my agent can utilize (say to keep track of its internal board representation) then I need to use `action_to_string(taken_action)`. This will fail since `action_to_string` is being called during the sense phase and not the move phase. To handle this, I actually keep a deep copy of a state which is in sense phase, and a copy of a state which is in move phase, so that I can call `move_state.action_to_string(taken_action)` during the sense phase to get the proper string of the corresponding previous move action.    See the following for how I handle this in detail:       Note: I am assuming that the previous requested action was the taken action which is not the case (See: (2))    There are two other places (See: (3)/(4)) where I utilized this action_to_string, but they are sort of needed because of the lack of functionality elsewhere. Similarly, in case i) I would not be resorted to using `state.action_to_string` if taken_move was provided to me in the ObservationString.    -------------------------------------------------------------------------------------------------------------------------------------------------------------------------    cc'ing as original implementer and avid user  @michalsustr and @VitamintK    if you got here then have a cookie: :cookie:"
"I'm trying to build open spiel on windows 10. I've followed the installation instructions for windows, but I'm getting an error importing pyspiel when running example.py.    $ python example.py  Traceback (most recent call last):    File ""C:\Users\henry.lei\Documents\Projects\open_spiel\open_spiel\python\examples\example.py"", line 26, in        from open_spiel.python import games  # pylint: disable=unused-import    File ""C:\Users\henry.lei\Documents\Projects\open_spiel\open_spiel\python\games\__init__.py"", line 29, in        from open_spiel.python.games import dynamic_routing    File ""C:\Users\henry.lei\Documents\Projects\open_spiel\open_spiel\python\games\dynamic_routing.py"", line 50, in        from open_spiel.python.observation import IIGObserverForPublicInfoGame    File ""C:\Users\henry.lei\Documents\Projects\open_spiel\open_spiel\python\observation.py"", line 56, in        import pyspiel  ModuleNotFoundError: No module named 'pyspiel'      My PYTHONPATH contains the root openspiel directory, as well as out/build/x64/python:    C:\Users\henry.lei\Documents\Projects\open_spiel\open_spiel\out\build\x64\python;C:\Users\henry.lei\Documents\Projects\open_spiel\open_spiel\out\build\x64;C:\Users\henry.lei\Documents\Projects\open_spiel;  "
"I'm on Windows 10, Python 3.8.2 (but the same issue is happening on another machine with Windows 11, Python 3.7.3).    Running `py -m pip install open-spiel==1.0.2` in a Windows Powershell terminal (both with and without administrator privileges) results in a `legacy-install-failure` error preceded by a `subprocess-exited-with-error` error. The full output is dumped   (note that Pastebin doesn't format this properly in the first window, so scroll down to the raw input at the bottom of the page).    One small locale footnote: `[WinError 2] Het systeem kan het opgegeven bestand niet vinden` translates to `[WinError 2] The system cannot find the file specified`.    I just upgraded from Windows 7 recently, exactly because the OpenSpiel installation required CMake and clang++. Now that I have those (as evidenced by the output), different errors arise. Is OpenSpiel's pip install not meant to work on Windows, perhaps?  "
"     Saving a pickle file of a universal poker solver and then loading it gives an error. I've played around with removing or editing some of the parameters here from the string used to load the game (e.g. removing `GAMEDEF` and `limit`, adding a space before `GAMEDEF`) but it still gives an exception.    a hopefully reproducible example   "
The AlphaZero documentation states that the checkpoint are compatible between the C++ and Python versions. Does this still hold true for LibTorch AlphaZero? I could not find a way to use those checkpoints in python.
"Hi, I am trying to create a good Ai for the backgammon game, as there are no examples for it. I tried pure PPO, but it did not give very good results. I was thinking to apply the AlphaZero algorithm, but realized it is only for deterministic games. Is there are inherent reason for this? I see that it currently does not handle ChanceNode's correctly but that could be changed rather easily right?       I would appreciate any feedback in this area!"
"I was interested in implementing and submitting a PR for these two games:    * No Thanks! — VERY simple mechanics.  * Mamono Sweeper — a basic extension of Minesweeper, where instead of each ""bomb"" being a simple binary yes/no, each ""bomb"" has a value from 1 to 9, and you ""level up"" over time and are able to clear increasingly more powerful ""bombs"". Even though this seems like a radical departure from Minesweeper, it still plays mostly the same.    However, both are proprietary (e.g. No Thanks! is published in English by Z-Man Games). Are these types of games welcomed to be submitted via a PR?"
"First of all, I want to thank the developers for this awesome project! It's simple, clean yet powerful. I really enjoyed playing with it.    I'm currently studying at the University of Alberta under the supervision of Prof. Martin Mueller. My primary research focus is learning/planning with a model, and general game playing. MuZero was a big step in this direction and I would like to implement an open-source version of it as a foundation of my project. I'm aware of other open-source implementations but I would like to have a more efficient and robust implementation. (This is partially why I opened #592, since utilizing cloud computation power well is a must-have.) There's also #135 but unfortunately no follow-up.    My plan is to implement MuZero in a separate repo first using both the C++ and Python interfaces of OpenSpiel. I'll use C++ for search (MuZero flavored MCTS) and Python for everything else. I'll use JAX for neural-net-related things (I heard that's what you are using in DeepMind for MuZero). If the project works, we can integrate the project into OpenSpiel at some point in the future.     Here are some questions of mine:  1. What are the caveats of writing a MuZero flavored MCTS in C++ similar to OpenSpiel's own MCTS?  2. I presume since the interfaces of the games are quite unified, the algorithm should work on all the games without too much tweaking (work in terms of running without errors, but not necessarily performing well on the task). Is this correct?  3. What do you do to display/visualize information/metrics? Right now I'm just reading console logs.    It would be great if the developers can help me with these questions and any other tips regarding the project would be greatly appreciated! 😃 "
"See the loss function in the repo:       `      v_target_, has_played, policy_target_ = v_trace(            v_target,            ts.env.valid,            ts.env.player_id,            ts.actor.policy,            policy_pprocessed,`    `policy_pprocessed = self.config.finetune(pi, ts.env.legal, learner_steps)`    `pi, v, log_pi, logit = rollout(params, ts.env)`    It seems to be using params, but the paper says params_target:    !     !     "
"state.returns() at line 955 of rnad.py is always [0, 0] because the terminal state is changed in the following code.     is this intentional?"
"In the code below, `v_target` is initialised from the target net         Then, in the vtrace calculation, the `v_target` is overridden by the first vtrace call         is this intentional?"
"   I think the total loss should be   `total_loss = policy_loss - entroy_loss`   instead of  ` total_loss = tf.add(            total_loss, entropy_loss, name=""total_loss_with_entropy"")`  @lanctot @elkhrt "
"According to my testing thus far, it seems that not all games have the same method of accessing player observations, and there are four separate ways to get the current game state:         Without having to do a bunch of try/excepts, is there a place that documents which games have which methods implemented, or is there a possibility that I'm not seeing a section of documentation that pertains to this?    Just some code example of what I'm doing to find out what's implemented:     "
"Hi there, I'm trying to use Openspiel in a project where all games share a common API with PettingZoo, and due to the architecture of the system, the stepping of  the environment looks something like so (modified from the  ):    P/S: This bug only occurs on `backgammon` AFAICT.         For some reason, when trying to get legal_actions after executing any and all possible chance nodes, the following stack trace is raised:         Following this trace leads me to this section of the code in `/open_spiel/open_spiel/games/backgammon.cc`:         I have further tried browsing around the code to see what that `SPIEL_CHECK_GE` actually does, but cannot seem to find its definition.    So my question is, what is that error and why is it not possible to get legal actions after executing chance nodes?"
I am looking for a zero-sum or fully cooperative multistep simultaneous move games (SMG) that are used as standard benchmarks for evaluation (especially where I can control the size of game)? I am also looking for a way to use and visualize SMG as imperfect information extensive form game. I am assuming I will have to use `convert_to_turn_based` method to convert the SMG to imperfect information extensive form game but I am not sure how to visualize it?
Trying to load sample games in the Gambit EFG format from Gambit's documentation results in a thrown exception.    Here are their sample games       Here is the affected code       Which throws   
"I have the following problem when installing open_spiel from source：  [ 54%] Built target bridge_double_dummy_solver  /home/gaozh/pipline/dependencies/open_spiel/open_spiel/games/phantom_go.cc:70:15: error: no viable constructor or deduction guide for deduction of template arguments of 'vector'    std::vector points(placement.begin(), placement.begin() + num_handicap);                ^  /usr/bin/../lib/gcc/x86_64-linux-gnu/7.5.0/../../../../include/c++/7.5.0/bits/stl_vector.h:295:7: note: candidate function not viable: no known conversion from        'std::array ::iterator' (aka 'unsigned short *') to 'unsigned long' for 1st argument; dereference the argument with *        vector(size_type __n, const value_type& __value,        ^  /usr/bin/../lib/gcc/x86_64-linux-gnu/7.5.0/../../../../include/c++/7.5.0/bits/stl_vector.h:283:7: note: candidate template ignored: couldn't infer template argument '_Tp'        vector(size_type __n, const allocator_type& __a = allocator_type())        ^  /usr/bin/../lib/gcc/x86_64-linux-gnu/7.5.0/../../../../include/c++/7.5.0/bits/stl_vector.h:411:2: note: candidate template ignored: couldn't infer template argument '_Tp'          vector(_InputIterator __first, _InputIterator __last,          ^  /usr/bin/../lib/gcc/x86_64-linux-gnu/7.5.0/../../../../include/c++/7.5.0/bits/stl_vector.h:348:7: note: candidate template ignored: could not match 'vector ' against        'unsigned short *'        vector(const vector& __x, const allocator_type& __a)        ^  /usr/bin/../lib/gcc/x86_64-linux-gnu/7.5.0/../../../../include/c++/7.5.0/bits/stl_vector.h:358:7: note: candidate template ignored: could not match 'vector ' against        'unsigned short *'        vector(vector&& __rv, const allocator_type& __m)        ^  /usr/bin/../lib/gcc/x86_64-linux-gnu/7.5.0/../../../../include/c++/7.5.0/bits/stl_vector.h:383:7: note: candidate template ignored: could not match        'initializer_list ' against 'unsigned short *'        vector(initializer_list  __l,        ^  /usr/bin/../lib/gcc/x86_64-linux-gnu/7.5.0/../../../../include/c++/7.5.0/bits/stl_vector.h:216:11: note: candidate function template not viable: requires 1 argument, but 2 were        provided      class vector : protected _Vector_base             ^  /usr/bin/../lib/gcc/x86_64-linux-gnu/7.5.0/../../../../include/c++/7.5.0/bits/stl_vector.h:259:7: note: candidate function template not viable: requires 0 arguments, but 2 were        provided        vector()        ^  /usr/bin/../lib/gcc/x86_64-linux-gnu/7.5.0/../../../../include/c++/7.5.0/bits/stl_vector.h:270:7: note: candidate function template not viable: requires single argument '__a', but 2        arguments were provided        vector(const allocator_type& __a) _GLIBCXX_NOEXCEPT        ^  /usr/bin/../lib/gcc/x86_64-linux-gnu/7.5.0/../../../../include/c++/7.5.0/bits/stl_vector.h:326:7: note: candidate function template not viable: requires single argument '__x', but 2        arguments were provided        vector(const vector& __x)        ^  /usr/bin/../lib/gcc/x86_64-linux-gnu/7.5.0/../../../../include/c++/7.5.0/bits/stl_vector.h:344:7: note: candidate function template not viable: requires single argument '__x', but 2        arguments were provided        vector(vector&& __x) noexcept        ^  1 error generated.  games/CMakeFiles/games.dir/build.make:963: recipe for target 'games/CMakeFiles/games.dir/phantom_go.cc.o' failed  make[2]: *** [games/CMakeFiles/games.dir/phantom_go.cc.o] Error 1  make[2]: *** Waiting for unfinished jobs....  [ 54%] Built target bots  [ 54%] Built target game_transforms  [ 54%] Built target tests  [ 54%] Built target algorithms  CMakeFiles/Makefile2:13472: recipe for target 'games/CMakeFiles/games.dir/all' failed  make[1]: *** [games/CMakeFiles/games.dir/all] Error 2  Makefile:111: recipe for target 'all' failed  make: *** [all] Error 2    I tried installing from scratch and still get this problem. I also tried looking for answers in past questions, but unfortunately there are no such questions. Please help me to solve this problem.  this is my system info：  Linux ipdc 5.4.0-120-generic #136~18.04.1-Ubuntu SMP Fri Jun 10 18:00:44 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux  this is clang version:  clang version 10.0.0-4ubuntu1~18.04.2"
"As the title says, I'm wondering which game is the simplest (as in smallest, fastest to train on) for 2 players and perfect information. It looks like it should be Pathfinding but I am unsure how large the grid is and whether I can reduce the rows/columns. I'm not sure if multiagent Pathfinding is zero-sum or cooperative but that matters less."
"Referencing #933, I am trying to build open_spiel as a shared library in Windows in MS Visual Studio 2019, but get the following error:    `Severity Code Description Project File Line Suppression State  Error LNK2019 unresolved external symbol ""void __cdecl open_spiel::LogUsage(void)"" (?LogUsage@open_spiel@@YAXXZ) referenced in function ""class std::shared_ptr  __cdecl open_spiel::LoadGame(class std::map ,class std::allocator  >,class open_spiel::GameParameter,struct std::less ,class std::allocator  > >,class std::allocator ,class std::allocator  > const ,class open_spiel::GameParameter> > >)"" (?LoadGame@open_spiel@@YA?AV?$shared_ptr@$$CBVGame@open_spiel@@@std@@V?$map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@VGameParameter@open_spiel@@U?$less@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@VGameParameter@open_spiel@@@std@@@2@@3@@Z) C:\Users\user\apps\spiel_win\open_spiel\open_spiel\out\build\x64-Debug\open_spiel C:\Users\user\apps\spiel_win\open_spiel\open_spiel\out\build\x64-Debug\spiel.cc.obj 1   `    The build produces an open_spiel.dll file, but when I add that file as a shared library in my project, I get this error:    `Severity Code Description Project File Line Suppression State  Error LNK1107 invalid or corrupt file: cannot read at 0x370 some_project C:\some_project\dependencies\open_spiel\lib\open_spiel.dll 1`    I can build open_spiel without the shared library option without any problems.      Any suggestions on how to get the shared library to build correctly?  "
I have run into a bug while building in Windows Visual Studio Code. I have followed the instructions here:   . The CMake is successful but run into the following errors when attempting to Build All:          Any help would be appreciated. Thank you! 
"  only provides this on linux and macos, so are there any methods to do this on windows? or it only supports linux and macos?"
"  (which is being called by my code via  ) does not work when policy probabilities are JAX abstract tracer values:       This is due to the `if p > self._cut_threshold` clauses, which require `p` to be concrete. Is there a recommended workaround? If not, perhaps a `threshold` flag could be added to disable the threshold check completely."
  Done~     Best
"Hi,guys, i was wondering whether it is possible to accelerate the convergence process in Neural Replicator Dynamics. In this paper,  the accumulated values **y** t  at time t is approximated by Euler discretization. From my knowledge, the second order approximation is available and is it possible to use to accelerate the convergence process?    The analysis is as follows:    The Adams-Bashforth formula is as follows (second order approximation):    y k+1  = y k  + h* ( 3/2 * f(t k , y k ) - 1/2 * f(t k-1 , y k-1  ))  dy/dt = f(t,y)    From the neural replicator dynamics paper, the NeuRD's policy update formula comes from minimizing the Logit error, and the Logit estimates are given by the Euler discretization.  We know that the Euler discretization is a first-order method, and the Adams-Bashforth method is a second-order method. The Adams-Bashforth method satisfies the root condition and its local truncation error is second order, converges according to the Dahlquist equivalence theorem, and the global error is also second order with higher accuracy than the Euler discretization method. So We can choose  a larger learning step size. And the update applied to NeuRD means that the critic networks, not only at time t, but also at the previous time t-1, give different advantage function estimates. Does this mean that policy will converge faster?     Any suggestion is appreciated.    "
"Hello. In the current implementation of psro_v2, right before it is going to do best response, it will first select a subset of strategies from the current strategy pool and then decided how to do BR using these selected strategies:       However here is one thing that is unclear to me. When we are considering non-marginal/joint strategy, according to line 310-313 it seems that currently_used_policies should be a list of joint strategies, while current_indexes should be a ""num_player""-sized list each recording a list of policy indexes for each player. This design looks strange to me.     And I doubled checked in psro_v2/strategy_selector.py, there were non-marginal selector implemented. For example,       However the interface is not consistent with line 293 in psro_v2.py. Does that mean we need to write a customized strategy selector based on these? If so, is there a easy way we get used_indexes? Thanks."
"Hi guys,  I am trying to implement the neural replicator dynamics algorithm in the paper ""Neural Replicator Dynamics:  Multiagent Learning via Hedging Policy Gradients"", and the link of the article is      The pseudo code is:  !     I have a few questions, and any suggestions are appreciated  (1) The critic network outputs q value, not v value. Is it correct?  Is there any benefits for using q value? Because the critic network outputs v value in A2C,so i think v value is more reasonable.    (2) The algorithm first sample some trajectories and then do the policy evaluation, which seems like monte carlo sampling. Is it right?  It uses TD error in A2C to do the update. Is it a good idea to use TD error to do the policy evaluation?    (3) A2C and neural RD is very similar, and they both have a critic network and a policy network. There is only one difference, and the last softmax layer is cut off in the neural RD. If i am right, can i just cut off the last softmax layer in A2C and then train the rest part of the network?       "
"I have been using OpenSpiel for my project and I found the string representation of the states are helpful but do have some limitations. I have been building visualization tools to use with OpenSpiel and I think it would be a nice contribution for OpenSpiel users, either as a part of OpenSpiel, or a standalone project.    ## Key Features  ### Render an image based on observation tensor  (Here, observation tensor means `np.array(state.observation_tensor()).reshape(game.observation_tensor_shape())`, an image in `HWC` format.)    In #678 I described the need of rendering human-readable game states from observation tensors, and rendering them as images is more human-friendly than ever (their visual representations were designed for human eyes at the first place). This way, the training pipeline does't have to store OpenSpiel game states. Instead, it can store observations as tensors, put them in a replay buffer and train the network with them, but still being human-readable.     ### Render image differences and continuations  I found tracing the changes between two string representations very difficult. There are two solutions that I found effective. One is to render trajectories (a stacked observation tensor in `BHWC` format) as GIFs, and the other one is to render the pixel differences between two rendered images of two consecutive states. I found using arrow keys to play the GIF back and forth gives me a much better idea of what's actually happening in the game.    ## Example  My project (related #595) currently works well on Breakthrough, so I use it here as an example. This image is rendered based on the observation tensor of OpenSpiel's game state, the faint pink color indicates the difference from last state image.  !     I also used these images to render the search tree using learned model (MuZero), and I found analyzing search trees with visually rich representation is easier than string representation.  ! "
"     Hi,  The docstring states the critic learning rate is 0.001, but it really defaults to 0.01.  It has the same issue in PPO, etc., I guess it's related to some copy-pasting or something similar :)    Thanks"
"Hello all,     OpenSpiel v1.2 has been released!    Several new games and algorithms have been added, improvements and bug fixes, and documentation updates (including a core API reference).     Details here:  "
"Hello all,    We've added a new core API reference:  -    -      It's mostly intended for Python users but also has links to jump to the C++ function definitions too.    Please let us know if this is helpful, if it's missing anything, or if you have any other suggestions."
"In C++, game and state can be created by  `std::string game_name = ""bridge"";  open_spiel::GameParameters params;  std::shared_ptr  game = open_spiel::LoadGame(game_name, params);  std::unique_ptr  state = game->NewInitialState();`    But the state only has methods which was written in spiel.h class State, How do I create a bridge state which has special methods for bridge game, like CurrentPhase()?    I've tried like this  `open_spiel::bridge::BridgeGame bridge_game = open_spiel::bridge::BridgeGame(params);  open_spiel::bridge::BridgeState bridge_state = open_spiel::bridge::BridgeState(game, false, false, false);`    But the bridge_state cannot give observation_tensor,it needs   `Player player, absl::Span  values`  However the state can correctly work with ObservationTensor().  Are there any examples to show how to use BridgeGame and BridgeState?  "
"I'm installing open_spiel on windows using vs2019, and it gave me errors.    Cannot open include file: 'pybind11_abseil/absl_casters.h': No such file or directory     I guess it's due to pybind11_abseil is not installed.  "
"Hi,    As the Nim game and AlphaZero algorithms are implemented in the OpenSpiel, I wonder if it is possible to train an AlphaZero agent on Nim game using the library. Is there an instruction page on how to do it? "
"I get an error `bad_weak_ptr` when I try to use python games with C++ CFR. I'm 99% sure this used to work, but sometime over the past half-year it stopped working.    Tested on both Mac OS and Ubuntu with a fresh install:     "
"     On a fresh computer, `$OPEN_SPIEL_BUILD_WITH_ACPC` and `$OPEN_SPIEL_BUILD_WITH_HANABI` default to ""ON"" which means that if not manually turned to ""OFF"", the installation errors.    Just wondering if they should both default to ""OFF"" to make the initial installation experience smoother?"
"Hi,    I was installing the open_spiel library by building from source and following the instructions  . Everything was fine until I run the last command, which raised the error: `ImportError: /home/bei/PycharmProjects/open_spiel/build/python/pyspiel.so: undefined symbol: _ZNSt15__exception_ptr13exception_ptr10_M_releaseEv  `. I have tried multiple times with different options, but unfortunately none of them worked.     1. ./install.sh  2. virtualenv -p python3 venv  3. source venv/bin/activate  4. curl   -o get-pip.py  5. python3 get-pip.py  6. pip3 install --upgrade pip  7. pip3 install --upgrade setuptools testresources  8. pip3 install -r requirements.txt  9. ./open_spiel/scripts/build_and_run_tests.sh"
Nim game is on the game list at:  . But it is not available in `pyspiel.registered_games()`. 
"I am attempting to use a Monte Carlo Tree Search (MCTS) algorithm in   to determine the policy of an agent, and I want to use an Evaluator that evaluates a game state using the policy of some other agent. Specifically, I would have something similar to  , but instead of randomly choosing an action at each step of the rollout, I would select an action depending on a given agent's policy. How would I implement this given that an Agent take TimeSteps but an Evaluator takes State? Should I just load a new game environment for each rollout to only use TimeStep in the Evaluator?         **General Note:** I've noticed that OpenSpiel seems to have two ways to represent a game state: (1) **pyspiel.State**, which can be used for a simple manipulation of gameplay as seen in  , and (2) **rl_environment.TimeStep**, which is used for interacting with an Environment as seen in   (as well as many other game examples where agents are trained). There are some minute differences between the two (e.g. TimeStep holds RL-specific info such as the rewards given to agents so far), but they both share the common purpose of holding info about the current game state, and can be used to progress a game step by step (again, see examples referenced above). Is there a general way to convert between these two types of objects?"
"I get this error with CMake 3.16.3    > CMake 3.17 or higher is required.  You are running version 3.16.3    In the installation guide, it still says 3.12     "
"Hi, I have just read the   paper and I am quite interested in the NeuRD algorithm. But it seems the sample-based version is not implemented in the OpenSpiel repository, so I am trying to do it myself.    I find there are some discussions about it in #148 and #316, so I just follow the instructions in #316 and add a **PyTorch** version. I choose to use the **regular policy entropy** instead of the q-value entropy used in   because I still have some questions about it and decide to leave it for a future task.    To compare the results between different algorithms, I run each algorithm 5 times using the OpenSpiel PyTorch code and reproduce the results in   (except NFSP). The hyperparameters are from #155. The final performances appear to be basically the same as in the paper, except for the RPG, which is **worse**.    Due to the different entropy types used, I need to do more hyperparameter searches before I finally confirm the performance of my implementation of NeuRD. I will update the results once I have done that.    !   "
"I attempted to train a AlphaZero model for the Othello game and followed the instruction. The running script is as below:  python3 alpha_zero.py --game othello --nn_model mlp  --actors 10 --path othello/    However, after running the program for 6 steps, the program throw an exception as below:    Traceback (most recent call last):    File ""alpha_zero.py"", line 98, in        app.run(main)    File ""/home/zxc5262/.local/lib/python3.8/site-packages/absl/app.py"", line 312, in run      _run_main(main, args)    File ""/home/zxc5262/.local/lib/python3.8/site-packages/absl/app.py"", line 258, in _run_main      sys.exit(main(argv))    File ""alpha_zero.py"", line 93, in main      alpha_zero.alpha_zero(config)    File ""/home/zxc5262/.local/lib/python3.8/site-packages/open_spiel/python/algorithms/alpha_zero/alpha_zero.py"", line 545, in alpha_zero      learner(game=game, config=config, actors=actors,  # pylint: disable=missing-kwoa    File ""/home/zxc5262/.local/lib/python3.8/site-packages/open_spiel/python/algorithms/alpha_zero/alpha_zero.py"", line 171, in _watcher      return fn(config=config, logger=logger, **kwargs)    File ""/home/zxc5262/.local/lib/python3.8/site-packages/open_spiel/python/algorithms/alpha_zero/alpha_zero.py"", line 425, in learner      num_trajectories, num_states = collect_trajectories()    File ""/home/zxc5262/.local/lib/python3.8/site-packages/open_spiel/python/algorithms/alpha_zero/alpha_zero.py"", line 372, in collect_trajectories      game_lengths_hist.add(len(trajectory.states))    File ""/home/zxc5262/.local/lib/python3.8/site-packages/open_spiel/python/utils/stats.py"", line 108, in add      self._counts[bucket_id] += 1  IndexError: list index out of range    The problem remained to exist when I tried different actors like 40.     It looks like the length of the trajectory collected by the actors is beyond the limit (8*8). But the grid for the othello game is actually 8 cols * 8 rows. Any suggestion on fixing this problem will be very helpful.    "
"Hello, one minor question about the current implementation of NFSP. According to the  , data generated by either SL or RL policy are all stored in M_{RL} for learning RL best response. However according to the current implementation:         It stores data in M_{RL} only when it is the SL network is acting. Is it because DQN is an off-policy algorithm so there is no difference here...?"
I get the following error message while running ./install.sh         This can be fixed by adding the following at line 117 in 'install.sh'        
"It looks like ExtensiveToMatrixGame defaults to creating a payoff matrix of all pure strategies but that is far from desirable now. Especially with the new sparsification techniques, is there any option to convert the extensive form game (kuhn poker for example) into its sparse matrix form based on sequences? "
"In open_spiel/python/example/deep_cfr, the tabular method is usually used to solve the exploitability or NashConv of kuhn poker.       """"""      average_policy = policy.tabular_policy_from_callable(game, deep_cfr_solver.action_probabilities)      conv = exploitability.nash_conv(game, average_policy)      """"""  Can we find a good way to calculate the exploitability when the scale of game is large?  Thank you for your reply!"
"Hi,    I was wondering - is there a plan to include Diplomacy in OpenSpiel?  Anthony et al. wrote a (beautiful) paper about SBR which cites OpenSpiel - and I was not sure if it means that it was implemented using the framework.    The thing is, I can probably ""easily"" (nothing is easy but kind of) extend MILA's game engine (  or maybe dipcc (SearchBot's) for OpenSpiel (as I partially implemented state encoding, etc. from MILA's engine based on MILA's encoding, to infer using available models) but:    1. It's Python (at least MILA's), not CPP - therefore slower.  2. I am not sure about the copyright (it's possible that they don't allow it anyway).  3. It has ""heavy"" dependencies currently.    So I guess my main question would be - does it worth thinking about/contracting the creators of the game engine, or are you planning to integrate your version? Are there additional copyright issues that you are aware of regarding that game?    Thanks!"
"Hello, I have a question regarding the current AZ implementation. It looks that it uses the average return from MCTS simulation as the value targets:     The loss of the value network is defined as:     Where I suppose `batch.value=root.total_reward / root.explore_count`  However if I am not mistaken in the original paper the value targets should be the actual outcomes during an AZ training episode. Is there any difference between these two kind of value targets?"
"Hello,   (and Kudos for the amazing work!)  I'm struggling since a couple of days to have a working version of alphazero libtorch version (python version works fine, but I guess it'll takes ages to train a model)     During compilation phase (/build_and_run_tests.sh), I get compilation errors on alpha_zero_torch_example or  torch_integration_test  like those :     > /usr/bin/ld: vpnet.cc:(.text+0x1508): undefined reference to `absl::lts_20211102::StrCat(absl::lts_20211102::AlphaNum const&, a  bsl::lts_20211102::AlphaNum const&)'  clang: error: linker command failed with exit code 1 (use -v to see invocation)  make[2]: *** [libtorch/CMakeFiles/torch_integration_test.dir/build.make:530: libtorch/torch_integration_test] Error 1    Some context info:     - container (using Docker) : nvcr.io/nvidia/tensorflow:21.06-tf2-py3   (basically I modified the provided Dockerfile.base to include nvidia/cuda/cudnn stuff)  - python : 3.8.5  - libtorch : 1.11.0+cu113  - cuda : cuda_11.3.r11.3/compiler.29920130_0    So my question is : would you have a reference configuration that enable usage of alphazero libtorch version ? (os|libtorch version | cuda ...)?    Many thanks in advance    Julien   "
"     Use of designated initializers is a C++20 feature and the supported version of C++ is C++17. This led to a compile error when building on Windows 10. I'm not an expert on C++ so not sure how this should best be rewritten, but I was able to make open_spiel compile by commenting out these lines (but presumably breaking the Mean Field Game).    @lanctot "
"import random  import pyspiel  import numpy as np    game = pyspiel.load_game(""kuhn_poker"")  state = game.new_initial_state()  while not state.is_terminal():    legal_actions = state.legal_actions()    if state.is_chance_node():      # Sample a chance event outcome.      outcomes_with_probs = state.chance_outcomes()      action_list, prob_list = zip(*outcomes_with_probs)      action = np.random.choice(action_list, p=prob_list)      state.apply_action(action)    else:      action = legal_actions[0]      state.apply_action(action)  -----------------------------------------------------------------------------  I don't know how to enter pyspiel.state and calling procedure of state. So it is impossible to step_into state.is_ terminal(), state. chance_ outcomes()... when I debug the code. I hope you can solve my questions，thanks.  "
module 'pyspiel' has no attribute 'Gameparameter'
"so in this for loop       Is there a particular reason that it is written as a for loop? Can't it be just checking whether my_infostate_str is in infostate_action_map, and if it is then directly extract the corresponding action from the map and add the new (state, prob) to the queue?"
I am running Leduc Poker and I am having a hard time figuring out the format for the Information State Tensor. Where can I find more information on how the following Information State String is translated to the corresponding Information State Tensor?   
"I'm plotting 2x2 and 3x3 dynamics besides each other. You'd expect the top and bottom edges of the square 2x2 plot to be reasonably close to the level of the triangular 3x3 plot's base and tip, but the 3x3 plot seems shrunk down. I've looked in many places, and there doesn't seem to be a way in matplotlib to scale up just one of the subplots manually.    On the topic of scaling in 3x3 plots: I've been trying to make it so that the dynamics arrows show smaller magnitude not by retracting their tail into a fixed-size arrow head, but instead scale down the arrows as a whole (see e.g. figure 4 in   Is there an easy way to cause this behaviour?    Here's an MWE and its result:     ! "
"I want to implement a multiplayer and multicard kuhn_poker game, just like 4 players with 6 cards(not 5).  So I prepare to use the python version kuhn_poker as my prototype. But now I don't know how to use it. Whatever I change in the open_spiel/python/games/kuhn_poker.py,  it can't work. So how can I see my code wrriten in open_spiel/python/games/kuhn_poker.py work? Thank you very much! "
"I'm trying to run leduc_nfsl.py on my windows machine but its failing with the errors like:    OP_REQUIRES failed at save_restore_v2_ops.cc:110 : Not found: Failed to create a NewWriteableFile: /tmp/nfsp_test\q_network_pid0.data-00000-of-00001.tempstate15867241677518851320 : The system cannot find the path specified.    The full error trace:     python leduc_nfsp.py  WARNING:tensorflow:From C:\Users\henry.lei\Miniconda3\envs\openspeilEnv\lib\site-packages\tensorflow\python\compat\v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.  Instructions for updating:  non-resource variables are not supported in the long term  I0511 04:38:56.422645  5896 leduc_nfsp.py:110] Loading leduc_poker  I0511 04:38:56.422645  5896 rl_environment.py:184] Using game settings: {'players': 2}  2022-05-11 04:38:56.429413: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2  To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.  2022-05-11 04:38:56.430551: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.  2022-05-11 04:38:56.465064: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.  I0511 04:40:10.324198  5896 leduc_nfsp.py:156] Losses: [(0.59765416, 15.002959), (0.56038153, 26.946913)]  I0511 04:40:20.520366  5896 leduc_nfsp.py:164] [10000] NashConv 4.503853030514542  2022-05-11 04:40:20.529199: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at save_restore_v2_ops.cc:110 : Not found: Failed to create a NewWriteableFile: /tmp/nfsp_test\q_network_pid0.data-00000-of-00001.tempstate15867241677518851320 : The system cannot find the path specified.  ; No such process  Traceback (most recent call last):    File ""C:\Users\henry.lei\Miniconda3\envs\openspeilEnv\lib\site-packages\tensorflow\python\client\session.py"", line 1375, in _do_call      return fn(*args)    File ""C:\Users\henry.lei\Miniconda3\envs\openspeilEnv\lib\site-packages\tensorflow\python\client\session.py"", line 1359, in _run_fn      return self._call_tf_sessionrun(options, feed_dict, fetch_list,    File ""C:\Users\henry.lei\Miniconda3\envs\openspeilEnv\lib\site-packages\tensorflow\python\client\session.py"", line 1451, in _call_tf_sessionrun      return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,  tensorflow.python.framework.errors_impl.NotFoundError: Failed to create a NewWriteableFile: /tmp/nfsp_test\q_network_pid0.data-00000-of-00001.tempstate15867241677518851320 : The system cannot find the path specified.  ; No such process           [[{{node save_2/SaveV2}}]]    During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""C:\Users\henry.lei\Miniconda3\envs\openspeilEnv\lib\site-packages\tensorflow\python\training\saver.py"", line 1188, in save      model_checkpoint_path = sess.run(    File ""C:\Users\henry.lei\Miniconda3\envs\openspeilEnv\lib\site-packages\tensorflow\python\client\session.py"", line 967, in run      result = self._run(None, fetches, feed_dict, options_ptr,    File ""C:\Users\henry.lei\Miniconda3\envs\openspeilEnv\lib\site-packages\tensorflow\python\client\session.py"", line 1190, in _run      results = self._do_run(handle, final_targets, final_fetches,    File ""C:\Users\henry.lei\Miniconda3\envs\openspeilEnv\lib\site-packages\tensorflow\python\client\session.py"", line 1368, in _do_run      return self._do_call(_run_fn, feeds, fetches, targets, options,    File ""C:\Users\henry.lei\Miniconda3\envs\openspeilEnv\lib\site-packages\tensorflow\python\client\session.py"", line 1394, in _do_call      raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter  tensorflow.python.framework.errors_impl.NotFoundError: Failed to create a NewWriteableFile: /tmp/nfsp_test\q_network_pid0.data-00000-of-00001.tempstate15867241677518851320 : The system cannot find the path specified.  ; No such process           [[node save_2/SaveV2 (defined at \Documents\Projects\open_spiel\open_spiel\python\algorithms\nfsp.py:122) ]]    Errors may have originated from an input operation.  Input Source operations connected to node save_2/SaveV2:   mlp/weights_1 (defined at \Documents\Projects\open_spiel\open_spiel\python\simple_nets.py:48)   mlp/bias_1 (defined at \Documents\Projects\open_spiel\open_spiel\python\simple_nets.py:52)    Original stack trace for 'save_2/SaveV2':    File ""\Documents\Projects\open_spiel\open_spiel\python\examples\leduc_nfsp.py"", line 186, in        app.run(main)    File ""\Miniconda3\envs\openspeilEnv\lib\site-packages\absl\app.py"", line 312, in run      _run_main(main, args)    File ""\Miniconda3\envs\openspeilEnv\lib\site-packages\absl\app.py"", line 258, in _run_main      sys.exit(main(argv))    File ""\Documents\Projects\open_spiel\open_spiel\python\examples\leduc_nfsp.py"", line 140, in main      agents = [    File ""\Documents\Projects\open_spiel\open_spiel\python\examples\leduc_nfsp.py"", line 141, in        nfsp.NFSP(sess, idx, info_state_size, num_actions, hidden_layers_sizes,    File ""\Documents\Projects\open_spiel\open_spiel\python\algorithms\nfsp.py"", line 122, in __init__      (""q_network"", tf.train.Saver(self._rl_agent._q_network.variables)),    File ""\Miniconda3\envs\openspeilEnv\lib\site-packages\tensorflow\python\training\saver.py"", line 836, in __init__      self.build()    File ""\Miniconda3\envs\openspeilEnv\lib\site-packages\tensorflow\python\training\saver.py"", line 848, in build      self._build(self._filename, build_save=True, build_restore=True)    File ""\Miniconda3\envs\openspeilEnv\lib\site-packages\tensorflow\python\training\saver.py"", line 876, in _build      self.saver_def = self._builder._build_internal(  # pylint: disable=protected-access    File ""\Miniconda3\envs\openspeilEnv\lib\site-packages\tensorflow\python\training\saver.py"", line 513, in _build_internal      save_tensor = self._AddSaveOps(filename_tensor, saveables)    File ""\Miniconda3\envs\openspeilEnv\lib\site-packages\tensorflow\python\training\saver.py"", line 205, in _AddSaveOps      save = self.save_op(filename_tensor, saveables)    File ""\Miniconda3\envs\openspeilEnv\lib\site-packages\tensorflow\python\training\saver.py"", line 120, in save_op      return io_ops.save_v2(filename_tensor, tensor_names, tensor_slices,    File ""\Miniconda3\envs\openspeilEnv\lib\site-packages\tensorflow\python\ops\gen_io_ops.py"", line 1701, in save_v2      _, _, _op, _outputs = _op_def_library._apply_op_helper(    File ""\Miniconda3\envs\openspeilEnv\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 748, in _apply_op_helper      op = g._create_op_internal(op_type_name, inputs, dtypes=None,    File ""\Miniconda3\envs\openspeilEnv\lib\site-packages\tensorflow\python\framework\ops.py"", line 3561, in _create_op_internal      ret = Operation(    File ""\Miniconda3\envs\openspeilEnv\lib\site-packages\tensorflow\python\framework\ops.py"", line 2045, in __init__      self._traceback = tf_stack.extract_stack_for_node(self._c_op)      During handling of the above exception, another exception occurred:    Traceback (most recent call last):    File ""C:\Users\henry.lei\Documents\Projects\open_spiel\open_spiel\python\examples\leduc_nfsp.py"", line 186, in        app.run(main)    File ""C:\Users\henry.lei\Miniconda3\envs\openspeilEnv\lib\site-packages\absl\app.py"", line 312, in run      _run_main(main, args)    File ""C:\Users\henry.lei\Miniconda3\envs\openspeilEnv\lib\site-packages\absl\app.py"", line 258, in _run_main      sys.exit(main(argv))    File ""C:\Users\henry.lei\Documents\Projects\open_spiel\open_spiel\python\examples\leduc_nfsp.py"", line 170, in main      agent.save(FLAGS.checkpoint_dir)    File ""C:\Users\henry.lei\Documents\Projects\open_spiel\open_spiel\python\algorithms\nfsp.py"", line 295, in save      path = saver.save(    File ""C:\Users\henry.lei\Miniconda3\envs\openspeilEnv\lib\site-packages\tensorflow\python\training\saver.py"", line 1207, in save      raise exc  ValueError: Parent directory of /tmp/nfsp_test\q_network_pid0 doesn't exist, can't save.    Any idea what might be happening?  "
"Building from source from master (haven't done it in a while, so I could definitely be doing something wrong?)    Get the error:         What I did:  - pull latest master  -  ./install.sh  - pip install -r requirements.txt  - ./open_spiel/scripts/build_and_run_tests.sh    and that's when the test fails.    Anyone else run into this before?  All other tests pass just fine."
It looks like the code here is optimizing the policy but clipping the gradients for the value head     
Changed in commit:      Faulty line:       Solution: Move usage_logging.{h.cc} from utils to core (or remove LogUsage from spiel.cc)
"Python 3.6 has reached its end-of-life in December 2021.     This coming week, we will be merging   into master which upgrades many dependency versions and drops support for Python 3.6. The minimum version of Python supported will be Python 3.7.     OpenSpiel might still still work in Python 3.6 but may stop working at any time. If you need Python 3.6 or Ubuntu 18.04 support, please use  .    "
"Hello there. I am currently working on pathfinding games and found the implementation here really helpful!    Some minor questions about the implementation. According to the reference   there maybe several mechanics that are missing: (1) when an agent hits an obstacle, actually there could be a probability to pass through the block. (2) when two agents contest they may also incur a penalty (3) two agents may share the same destination. According to my understanding, they are not currently supported?"
"I'm a little confused on how to implement NSFP for simultaneous games. In the examples, NSFPpolicies's actionProbabilities method relies on state.currentPlayer(). But in simultaneous games (in my game at least), the curr_player_ is always kSimultaneousID. How exactly does the right obs/rewards get routed to the right NSFPpolicy? Do I have to define special behavior in the game to make sure that things happen in the right order?"
"I don't understand the purpose of the Observer class in making a game. Isn't there already a function InformationStateTensor(player, values) to generate observations? Also, what exactly should go into values?"
"I am trying to use the C++ shared library in a CMAKE project. I am trying to do something simple which is basically to be able to run a game by also include another library, such as libtorch. I understand libtorch is already built into the current alphazero lbtorch example but I am trying to use it on its own.    I understand that in the C++ useage instructions that you can use a shared library for open_spiel and this is what I am trying to do but via a CMAKE project. I see that there is a way to compile a file for open_spiel in the shared_library_example with clang++ but I want to make this via CMAKE so I can also incorporate libtorch or other libraries in my project.    I am new to using C++ libraries having done most things in python.    My current errors are that when I try to include the shared library and include the headers, etc. when I run something as simple as this: there is the error I am getting in my CMAKE build.    tester.cpp:     my CmakeList.txt file:         I have open_spiel repository downloaded into my $HOME/open_spiel directory      The error I am getting is in tester.cpp is:  undefined reference to `open_spiel::RegisteredGames()'    I am using CLION and it recognizes the open_spiel namespace and I can thus use Intellisense so it looks like to me that the shared library is being recognized, but it isn't!    I have tried also adding this before the add_executable:  `add_library(open_spiel SHARED ${HOME}/open_spiel/build/libopen_spiel.so)`    and then this after add_executable:  `target_link_libraries(tester open_spiel)`    hoping this would help.    I also tried adding this:  `set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS} -v -I${HOME}/ -I${HOME}/open_spiel/open_spiel/abseil-cpp -L${HOME}/open_spiel/build -lopen_spiel -std=c++17"")`    to basically recreate the C++ clang++ compile code and still I get the same error of undefined reference.    I try to include the library and have access to the headers     I cannot figure it out though. I have read that there is a need for a Open_spielConfig.Cmake (or some sort of config.cmake) file to be able to use find_package() in a CMAKE file, howver there doesn't seem to be any kind of Config.Cmake file getting generated.      I have spent hours on this and feel it is a simple thing I am missing out but just can't seem to figure it out. I am hoping someone can help me with this.    Thank you.    "
"Dear authors,    I noticed there might be an indexing issue when using tabular policy for mean field games. In the example code below, I first created a MFG and a uniform policy that is transformed to a tabular policy using `to_tabular()`. For a tabular policy, we can use `policy.state_lookup` (a dictionary) to look up the index of the state by inputting the state string (keys). However, the keys of `policy.state_lookup` are `dict_keys(['0', '0, 0, 2', '1', '0, 1, 2', '2', '0, 2, 2', '3', '0, 0, 0', '4', '0, 0, 1'])`. I think the correct keys should be `['0, 0', '0, 2', '1, 0', '1, 2', '2, 0', ...]`.  This is because in MFGs a state string `'0, 0'` represents the state 0 at time step 0. The `policy.state_lookup` seems to group these numbers (keys) in a wrong way. Any thoughts on this would be very helpful. Thanks.         "
"I've been trying to write an OpenSpiel example that illustrates how to plot replicator dynamics. However, even with the minimal code below, nothing seems to show up. Perhaps I'm missing something, perhaps something is broken.   "
I have a local build that I'm testing out and I'm getting errors for single player games.    This is the error I get when trying to run the python package via rl_envionment.Environment('blackjack'):         I also found that I get these errors during playthroughs when I'm running build_and_run_tests.sh       
Hardware: Macbook Pro M1 Max  OS: macOS 12.3  Python version: 3.10  Using a fresh conda environment on miniforge    Here's the error message I get:           Below is the output for test 135:             
"Hello, I want to implement the Xiangqi game. I'm referring the Chess game implementation as an example for Xiangqi. For chess, there is chess folder which has files like chess_board, chess-common. I want to create the same xiangqi folder with same files (my programming skill is not so good, so i'm just mimicking Chess). Will the files be recognized by the compiler when I run `build_and_run_tests.sh`? How can I make sure that my implementation will be merged into open_spiel? Thanks!"
"Hi OpenSpiel team!    I'm working on understanding how OpenSpiel works, and I've been implementing the card game   (in Python) to learn about the game and state objects OpenSpiel uses.    I've gotten my implementation to a point that I believe the game logic is correct: I can sample random actions manually and rollout an entire game trajectory, but when I try testing my game implementation with the MCTS bot, I get an error from `open_spiel/python/algorithms/mcts.py:69`         It seems that when doing a lookahead rollout, the MCTS bot reaches a point where it tries to sample actions, but there are no legal actions to be found - which *should* mean the game has terminated, but apparently, it hasn't. I'm guessing I've missed something small in my implementation of the Python state interface, but I can't see my mistake.    My code   - I would greatly appreciate if anyone more familiar with the project had time to take a peek and let me know if you can see the problem with my game implementation.    If I can get this working, I'd be happy to contribute this implementation as an unofficial demo of the python API.    Thank you,"
"OpenSpiel 1.1.0 has been released.    Many new games, algorithms, and features. Some highlights:    - Windows support  - Rust API  - ADIDAS Nash equilibrium solver  - AlphaZero support for chance nodes  - Mean-field games: several new algorithms and functionality    Full details:   "
"Dear authors,    Does anyone get the following failure information while testing MFGs?        I found two errors lead to the test failure.  (1)     (2)       I also noticed that when test the routing game, a bunch of playthrough information is printed out. For example,        Thanks in advance!  "
"I'm trying to build OpenSpiel on windows.    I followed the instructions  , cloning 5eaf401f7d08d68285fe214ab5cf5a741807ea6e.    CMake runs properly, but I get 8 syntax errors, all from the file `morpion_solitaire.cc`. It seems this file uses some fancy C++ notation I (or VS at least) am not familiar with - using `...` in a `case` block. My complete build log is too long for a GitHub issue comment, but I've paste-binned it here:      The relevant build log snippets are;         My VS info is as follows;     "
"Hi OpenSpiel team!    I'm excited about using OpenSpiel as a tool when prototyping my own board game designs.    > TLDR; when should I use `ChanceMode::kExplicitStochastic` or `ChanceMode::kSampledStochastic` for for a stochastic game?    I'm attempting to define my first (non-deterministic) game, and I'm struggling to understand the role of the `ChanceMode` enum.    `spiel.h:67`  ;         And I can also see some information about chance nodes in the docstring for `ApplyAction()` at  ;         ---    From these descriptions, it seems like you can use either `kExplicit` or `kSampeld` - it's just a matter of how your implementation code works. Is my understanding correct? Are there stochastic games that can only be implemented with one or the other?    I can see that very few games used the sampled mode;              But don't know enough about these games to tell why they might chose the sampled mode rather than the explicit mode.    I can't find anything about `ChanceMode` in the OpenSpiel paper, and I couldn't find any other documentation to answer this question - perhaps a documentation section could be added for this."
"Do we have NFSP implementation using a CNN rather than a linear network? I can't seem to locate it, I would appreciate it if someone could direct me (if it exists)."
"Forenote: I previously wrote the `stones_n_gems` game.    ## Problem   The `stones_n_gems` game uses a string containing commas to represent the map being used, which clashes with how the `GameParametersFromString` function parses serialized Games.    ## Description / Explanation  The `stones_n_gems` game allows for various input maps to play. Maps are used in the game initialization process through the GameParameter `grid`  string, which is a string containing newlines/commas to represent each item in the underlying map. I've been trying to use the game in python code which pickles the game/state, and have been getting errors about `Unknown parameter` during the deserializing process.    I've looked into how the pyspiel state python objects are pickled, and it appears that both the game/state are pickled. I've narrowed down the issue to what I believe is the `GameParametersFromString` function.  For this particular game, it receives as input the following (note that the commas/newlines are part of the string):     During the inner loop which reads each game parameter, I see the following when debug printing `game_string.substr(equals + 1, i - equals - 1)`:     etc.    ## Solution  I believe the solution to this is to change the commas in the `grid` parameter to be another symbol (say `|` as an example). This is an easy fix on my end, but will break compatibility for anyone who has existing code which uses this game.    Let me know if I should go ahead with a PR on this.  "
"I've been working on using the new LP tools to solve some games, including poker subgames and was wondering if anyone had any ideas for optimizing infostate trees and their memory usage.     It seems like there are lots of calls to `.Clone` or `.Child`, specifically with subtree rebalancing and updating the leaf nodes. It doesn't seem like `std::vector > corresponding_states_` is used for anything at the moment either.     There are a number of other things that seem like they could be cut directly for my own work, but I was wondering if there was an approach more inline with open spiel as a whole that might help.    Thanks in advance for the help!"
"I just learned that Quoridor can also be played with 3 or 4 players. According to the documentation this is also supported in open_spiel.    However, the number of players is fixed in the header.         Either the documentation or the implementation should be updated accordingly.    Looking forward to hearing from you."
"Following along with the suggestions on card abstraction  , I removed suits from the ACPC poker implementation.    This greatly reduced the number of information sets needed for the toy game I was training. However, despite there only being ~2,000 information sets, when I run `cfr.CFRSolver(custom_universal_poker_game)` the solver object still takes up 12GB ram!?! I'm at a loss for how to debug this or to reduce the memory usage as I had thought reducing the number of information sets was the way to go for reducing memory usage.        EDIT: My current best guess at a solution is to update `ChanceOutcomes` and `LegalActions` in such a way that they will also reflect the suit-less card abstraction."
"I'm training nfsp agents on hearts using `leduc_nfsp.py`.  However, I was forced to disable the nash_conv computation, since it appears to be trying to compute the complete set of possible information states in the game which blowing up my memory/cpu.    Is there a sampling based implementation that could efficiently calculate approximate nash_conv for games with large game trees?  or is there another exploitability metric I should be using instead?"
"Attempted to build with XINXIN support (export BUILD_WITH_XINXIN=""ON"").  Cmake fails, it appears there are some missing files that are not checked into the git?         `find . -name Algorithm.cpp` finds nothing."
"Do we have any low-depth high breadth games implemented in the library?    I was thinking of something like   for two-player.     I have a clean implementation I probably attempt to convert to open_spiel sometime soon, wanted to see if we have anything in this area, as I see the research on this type of game is not that wide-variety."
"Hi @lanctot , there's a little question. I'm wondering how to exchange the private cards in leduc poker?  I want to use duplicate tables to remove some of the randomness of the cards.  However  the definition of the class _state_  is  unclear, where the _state_  show as follows:  `game = pyspiel.load_game(""leduc_poker"")  state=game.new_initial_state()  `  when exchange the private cards, I could not assignment a collection  hand of saved  cards to  the corresponding  variable in _state_.  Please instruct, thanks!"
"I'm wondering what would be the best way to parallelize the env steps? For gym envs, we have the SubProcessEnv wrapper, is there anything equivalent in open_spiel? I would like to avoid passing policy networks to subprocess. Please instruct, thanks!"
"Quick, and possibly silly, question regarding libtorch AZ at test-time (as in alpha_zero_torch_game_example).    alpha_zero_torch_game_example is running AZ as an MCTSBot with AZ's value network -- i.e. it inits an MCTSBot with the VPNetEvaluator instead of the standard evaluator. So it seems that (i) AZ is forced to use mcts at test-time (which requires at least max_simulations=2) and (ii) AZ uses the same number of mcts simulations (max_simulations) as its opponent.    Getting around number (ii) is pretty straightforward --> adding another flag separating, say, max_az_simulations from max_opponent_simulations, but I'm not quite sure about (i). So far, I've just been running AZ with max_simulations=2, but I'm not sure if this is equivalent to not using mcts at all. But I can confirm that changing max_simulations at test-time changes AZ's performance (especially from an early checkpoint where AZ is playing sub-optimally).    Any feedback/insight would be much appreciated!"
"E.g., we might want a 10x10 board with the win conditions being          (up to any number of 90 degree rotations.).    PS: If you’re too busy, would a PR of this feature be accepted?"
thanks!
" After pip install open_spiel ,there is an error:  `RuntimeError: A C++ compiler that supports c++17 must be installed to build the following extensions: pyspiel. We recommend: Clang version >= 7.0.0.`  thanks."
"Hi @lanctot , first of all this is amazing collaborative effort! Thanks for this library.  I tried training DQN with Solitaire in Pytorch by tweaking the code   but the rewards are too high that my Q-values becomes NaN. Is there any way to resolve this?   Is rescaling the rewards a good option? If yes, how should I rescale them.    Any help would be really appreciated!  Thanks"
"Hi, I am interested in implementing NeuRD in my actor-critic RL model. I think the version in the NeuRD paper samples actions only for the critic and not for the actor and so I wanted to know how can I implement it for the sampled version? Can I just include 1/pi in the update like you have explained before the equation (5) of the paper?"
"Hi, I follow the tutorial on installing from source on ubuntu 18.04. All tests passed except          After installation and setting the  ,   works, but   fails with    Any idea on debugging this issue? Thanks"
"Hi there, I'm interested in using OpenSpiel to play games implemented in other languages. An example: I have a game implemented in C# - I don't want to reimplement it in C++/Python, and I don't want to implement OpenSpiel algorithms in C#.    Is there an existing way to get OpenSpiel communicating with games/agents running in other processes? I couldn't find anything. I'm happy to have a go at implementing something, but thought I should ask first.    My current thinking is a text-based interface over sockets. Hopefully this will be simple and portable."
"Hi, I would like to run the simultaneous games as turn based game by following the code in the  :     But i got        because neither   nor   is true for      Is it possible to run simultaneous game as turn_based_games?  "
"@milecdav     Basic simulation tests are not running on the Restricted Nash Response game. Once the crashing bug is fixed, the error will be:         See BasicRNRTests in `restricted_nash_response_test.cc` and a similar test in `python/games_sim_test`. Both are currently disabled."
I just upgraded my desktop to Ubuntu 21.10.     It seems like there is a problem building on master (see below). I'm looking into it.      
"When attempting to install latest master open_spiel, I ran into the following failed test:       I followed the steps  > ./install.sh  > pip3 install -r requirements.txt  > ./open_spiel/scripts/build_and_run_tests.sh    I have numpy installed on the `python` I'm using, as well as my system's python, so I'm not sure why this is happening.    I can disable the test to move forward, but I'm filing this issue here in case anyone else experiences this or knows how to fix it.    cc: @michalsustr "
"Is there any way to convert `policy.Policy` object to `pyspiel.Policy` object? I am trying to use something like the `AveragePolicy` implemented on MCCFR, for `pyspiel.TabularBestResponseMDP()`, which requires a game and a pyspiel policy object.      "
"when I run:  game = 'leduc_poker'  env_configs = {""players"": num_players}  env = rl_environment.Environment(game, **env_configs)    one error rasised:   File ""/mnt/d/ubuntu/venv/lib/python3.8/site-packages/open_spiel/python/rl_environment.py"", line 318, in step      self._state.apply_action(actions[0])  pyspiel.SpielError: /tmp/pip-req-build-muvq_yxe/open_spiel/games/leduc_poker.cc:392 num_raises_ < kMaxRaises  num_raises_ = 2, kMaxRaises = 2    I think is that you use actions[0] to execute, not legal_actions[0]. And not only in leduc_poker, in many other environments,  the error will also rasise."
"example:         results include both `Raxd2` and `Rxd2`, but should instead include `Raxd2` and `Rdxd2`, since `Rxd2` is ambiguous.    (the board is      )    This is due to   using only (pseudo)-legal moves to disambiguate, of which Raxd2 is not one, so it thinks disambiguation is not necessary.      Sorry to raise yet another issue for reconchess! 😛 I can make the fix for this one, but I'm not sure how to do it: pass an extra argument to `ToSAN` that's a vector of all moves to disambiguate between?    Or is the better fix to just switch ActionToString to use `ToLAN` instead?  This might be the better choice, since some `SAN` concepts fall apart in recon chess anyways: e.g. an action could be either `Qd2` or `Qxd2`, but with LAN the action only has one valid representation."
"(spinning out the potential issues I brought up in  #695 regarding RBC observation strings/tensors into its own thread)    1. The RBC rules say ""If a player captures a piece, she is informed that she made a capture (but she is not informed about what she captured)"" but from skimming the observation string code in open_spiel, I couldn't find this in there.  2. Captures do seem to be part of the observation tensor already, but it seems to always be set to `true`. Isn't `state.move_captured_` always true?  It's set by `move_captured_ = Board().at(move.from).color != Board().at(move.to).color`, and since the color of the destination square is either the opponent or empty, it will always be unequal to the piece's color, no?  If true, this could be fixed by using `OppColor` instead of `!=`  3. `illegal_move_attempted_` is in the public info tensor, but it doesn't seem like it should be common knowledge if my opponent attempted an illegal move (if things like failed pawn captures or forward moves are considered illegal moves)  4. It looks like the observation string (DarkFEN) includes the half-move clock, which informs the other player whether the previous player made a pawn move or not:     But this doesn't seem to be an issue in the observation tensor."
"The rules for recon blind chess   state that ""all rules associated with stalemates or automatic draw conditions are eliminated.""    The OpenSpiel implementation doesn't currently match:         Should this be changed, or does the implementation intentionally differ?"
The policy network seems to be created using the advantage network parameters and the policy network layer parameters appear to be unused.      
"Hi all,     We are now building the bdist_wheel for OpenSpiel!     It would be helpful for us to get a sample from different testing environment before we deploy live. If you would like to help, on a Linux x86_64 or MacOS x86_64 machine could you:    1. `python3 -m pip uninstall open_spiel`  2. Remove or temporarily move any other installation (i.e. from source) so pyspiel.so is not picked up by `PYTHONPATH`. You can check to be sure by trying to start Python and importing pyspiel; it should fail.  3. Download    4. Uncompress the zip file, will create a subdir `wheelhouse/`  5. Install the specific file for your python version and OS (see below).  6. Try starting python and `import pyspiel`. Maybe try creating a game and a state. (This step is so short because if there is any failure, I expect that it would show up at load-up time.)  7. Report back the result of step 6 (_failure or success!_ ... even if it's just to say ""it works!"") If failure, please post the specific failure. Please feel free use this thread to post your results. Also tell us which wheel you installed.  8. **Bonus**. Record how long the install takes by prepending the install using the `time` command. Current time to beat is is ~2.243~ ~0.816~ 0.709 seconds! Current max is 5.262 seconds.    The wheels are built for the python version (cp36 - cp39 for Python 3.6 to 3.9). You can find that info when you start the python interpreter. For example, to install on Mac OS using Python 3.8, run:         Thanks!    @elkhrt @jblespiau @finbarrtimbers @jhtschultz @tewalds @dhennes @perolat @michalsustr @Asugawara @StochasticEntropy @TheoCabannes @findmyway @christianjans @calebho @ssokota       "
"DDS colors are spades = 0,hearts = 1,diamonds = 2, clubs =3  bridge.h :  enum Denomination { kClubs = 0, kDiamonds, kHearts, kSpades, kNoTrump };    It's nothing wrong with it, until you try to calculate par result "
Is there a method implemented in openSpiel for saving a trained tabular qlearner and restoring it for later use? The existence of a lambda seems to stop it from pickling at the moment
"Hi authors,    When installing openspiel (build from source), I met the following issue.    [ 93%] Building CXX object python/CMakeFiles/pyspiel.dir/pybind11/algorithms_trajectories.cc.o  In file included from /Users/wangyzh/PycharmProjects/open_spiel/open_spiel/python/pybind11/algorithms_corr_dist.cc:15:  In file included from /Users/wangyzh/PycharmProjects/open_spiel/open_spiel/../open_spiel/python/pybind11/algorithms_corr_dist.h:18:  In ./open_spiel/python/pybind11/pybind11.h:29:10: fatal error:         'pybind11/include/pybind11/smart_holder.h' file not found  #include ""pybind11/include/pybind11/smart_holder.h""           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    I found that smart_holder.h only exists in the smart_holder branch of pybind11 rather than the master branch. So it is possible that the smart_holder branch is not installed by default.    Thanks.  "
"The   uses `TARGET_OBJECTS:human_bot`, but this target does not exist. This shows up if `OPEN_SPIEL_BUILD_WITH_LIBTORCH` is enabled."
@vofak     Upon import of   I added a basic test that implements a simple UCI chess program that takes random moves. Unfortunately it seems to hang and not work. So I have marked the bot as experimental and opening this issue to discuss it.
"@michalsustr     We are seeing memory issues with RBC, in the first test where there are two successive pass moves. You can reproduce if you compile in debug mode and run rbc_test under valgrind.    I think it is caused by some checks or functions which should not apply in the case of pass moves.  Also, `illegal_move_attempted_` was being immediately set back to false after being set to true; that does not seem right.     To the best of my understanding, I changed the `DoApplyAction` to the following:         .. but now it is causing a crash on the first step of the random simulations in ChessBoard::IsBreachingMove:       "
"It would be helpful if some games implement `ObservationToString`, a function that pretty prints an observation tensor.    Here are two scenarios:   - The user has no direct control over the environment interaction process (e.g., using libraries like Acme that wraps open_spiel) and only sees observations as inputs.   - The observations are stored somewhere (e.g., in a replay buffer) while the full game states are not stored.    In both cases, it would be helpful for the user to visualize what's going on by printing observation tensors."
"Is this correct?  Phantom Tic-Tac-Toe's `InformationStateString` gives `absl::StrAppend(&str, history_.size(), ""\n"");` even when obstype is `reveal-nothing` instead of `reveal-numturns`."
"Dear authors,    It seems to me that the code for nashconv only works when the initial distribution is uniform. For other initial distributions, this definition, summing up the deviation payoffs over all states, may be improper since states may associate with different probabilities at the beginning. I just want to note this since the current available MFG games in Openspiel seem to start from the uniform distribution. Thanks.     "
"From  :  > Python games are significantly slower than C++, but it may still be suitable  for prototyping or for small games.  >   > It is possible to run C++ algorithms on Python implemented games, This is likely  to have good performance if the algorithm simply extracts a game tree and then  works with that (e.g. CFR algorithms). It is likely to be poor if the algorithm  relies on processing and updating states as it goes, e.g. MCTS.    My interpretation of this is that if CFR extracts a game tree and works with that, then any slowness in `state.apply_action()` or `state.clone()` won't matter at all once we're done extracting the game tree, which makes sense and sounds great!    My question is: do the openspiel implementations of CFR actually ""simply extract a game tree and then  work with that""?  From skimming the source of `open_spiel/algorithms/cfr.cc`, it looks like each call to `EvaluateAndUpdatePolicy()` traverses the game tree with the recursive `ComputeCounterFactualRegretForActionProbs()`, which looks like it calls `state.Child(action)`.  So each call to `EvaluateAndUpdatePolicy()` calls `state.Child(action)` for each (state, action) pair in the game.    Is the comment wrong, did I misinterpret the comment, or did I misunderstand the CFR code?"
"Unlike `matching_pennies_3p` and `blotto`, `coordinated_mp` does not inherit from NormalFormGame/State nor MatrixGame/State. @michalsustr, is there a reason for this? It might be good to change them to make them consistent with the rest (there are some free helpful functions that get inherited)."
"Is there a way to run the game logic on GPUs? Currently the games all run on the CPU because of the C++ bindings as far as I know. This results in a lot of data movement from GPU memory to CPU memory and back for an algorithm that trains better on a GPU.     I'm wondering if it would be possible to implement a game in such a way that all the data can be kept on the GPU.     I'm thinking maybe JAX might be a good library to use for that, but I'm not sure if it's designed for these use cases where there would be a lot of control flows.     Just mentioning @mattjj here, because you never know, he might know ...    If we wrote all the ifs as jax where clauses, could we even vmap it?  And would that be efficient for a GPU?"
"Hello,    I was wondering if there is an easy way to replay certain chance outcomes switched for the players in order to be able to better evaluate the performance of differently trained agents. I would be especially interested in doing so for the game leduc poker, so the goal would be to play a game and in the following game the cards of two agents are switched.     "
"When I am running the OpenSpiel tests on my computer some are failing due to `RuntimeError: Tried to call pure virtual function ""Game::new_initial_state""` and `AttributeError: 'pyspiel.State' object has no attribute '__dict__'`    Here are my logs:        When trying to debug the errors, I found that rolling back GitHub to   solve the issue.    I have a dependency error when rolling back to      I also have the tests failing (for the same reasons) when rolling back GitHub to      Here is the result of `conda list`:     "
Is there a plan to include the AIVAT variance reduction implentation in open spiel?  
"The title is pretty straightforward, was looking for a DQN example with PyTorch, and the notebook ` research_dqn_pytorch.ipynb ` in fact uses TensorFlow. i didn't see a file in `open_spiel/python/examples` either that met this criteria. do you have any suggestions? Or could this be modified to used pytorch?"
"In python games, @elkhr recently changed `Clone` in the C++ layer to copy the attributes of the python object instead of replaying all actions in the history, which is great!:      It does through deepcopying all attributes       As someone who has no idea how pybind works, is this because simply doing      doesn't work?    If the latter is possible, I propose switching to that, since we'd be able to define our own cloning behavior by overriding `__deepcopy__(self)`.  For example, if our game state keeps an internal attribute that's a list, or a python object, we can override deepcopy to make a shallow copy of that attribute sometimes or always.    If it's not possible, that's fine, and I'll work around by either overriding `clone` at the python level or by rewriting my state to pull any nonmutating attributes out."
"when I was installing openspiel，some errors raise：  ERROR: Command errored out with exit status 1: /home/mlj/venv/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-wfyikrao/open-spiel_27b15f01be044bb49384b9b6a08727a8/setup.py'""'""'; __file__='""'""'/tmp/pip-install-wfyikrao/open-spiel_27b15f01be044bb49384b9b6a08727a8/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /tmp/pip-record-wqmxrk1f/install-record.txt --single-version-externally-managed --compile --install-headers /home/mlj/venv/include/site/python3.8/open-spiel Check the logs for full command output."
"I ran the code of the pytorch implementation of Deep CFR on Leduc poker. However, the current implementation cannot converge and the results sometimes become even worse along with the training, as below:        The tf version works well. Similar to the one in NFSP, I found that the tf version uses a different initialization of the networks. So I adapted the Sonnet initialization in NFSP to Deep CFR. The results are as follows:       The later one is similar to the tf implementation (sometimes better):         So I think the current implementation of Deep CFR with Pytorch should be updated. Below is my implementation.           (The code also includes an ugly implementation of reinitialization. More elegant way may be used.)    The main function is as follows (from the codebase)   "
"Hi,     I am trying to understand how the symmetric game works in PSRO solver. When I am running the PSRO kuhn/leduc experiments from   if I set the symmetric to true in the PSRO solver, the computed exploitabilities keep increasing rather than converge to 0 as the game progress. I am wondering why is this, thank you. "
"When trying to run MFG fictitious play on game with a terminal reward model, I run into the following error:  - In a iteration ficticious play create a BestResponse    - In best response (MFG) initialization the initialization finishes on    - In evaluate, when   then rewards is    - In case the game has a terminal reward model, then it is checked in    - Therefore, the reward function is called on a chance node and it is checked that the game is not on a chance node which creates a SPIEL CHECK ERROR."
"I am writing this issue to ask for a feature request on mean field game. Feel free to close the issue if this is not the right place to discuss about the feature request.    I would like to compare how a policy learnt on a mean field game will perform on the corresponding N-player game.  More specifically, I have implemented a N-player dynamic routing and its corresponding mean field routing game (very similar to  ) in open spiel with python (in a  ).    I can learn a policy to solve the N-player game using the open spiel algorithms (NFSP, CFR, MCCFR) and I can learn a policy to solve the mean field game (NFSP).  I would like to test how the mean field policy performs in the N-player game. To reach this goal, I should call the `mean_field_policy` on a N-player game state with `action_probabilities(state, player)`. However the N-player game state is not a mean field game and the call to `action_probabilities ` might fail.    I am thinking about the ways to implement a call to the policy on a N-player game. I wonder if this is something that people consider implementing already.    My idea for the implementation is the following:  - add a method `convert_state_to_mean_field_state` in the N-player game that takes as input a `player_id` and returns a `mean_field_game_state`.  - add a method `convert_mfg_action_to_N_player_action` that takes as input a mean field game action and returns an action that can be applied to a N player game.  - create a policy called `DerivedNPlayerPolicyFromMeanFieldPolicy` that has the mean field policy as an attribute and implements `action_probabilities` which takes as argument a N-player game `state` and a `player_id` by:      - calling `convert_state_to_mean_field_state` with the state and the player id in order to get a `mfg_state`      - calling `mean_field_policy.action_probabilities(mfg_state, pyspiel.PlayerId.DEFAULT_PLAYER_ID)` to get a list of actions with their probabilities      - then call `convert_mfg_action_to_N_player_action` for each action to get a list of action that can be applied to the N-player game    This is something that I can probably do in my own game, without changing anything in open spiel. However, I wonder if you have some feedbacks, ideas on things that can be done within open spiel. It seems that this is quite usual thing to apply mean field policy on the corresponding N player game. I am quite happy to help if you want me to implement something directly in OpenSpiel with possible `NotImplementedError`.        "
"I hope this is the right place to ask questions:     I trained the nfsp agents for leduc poker provided in the example files and for testing purposes I saved the agents at half of the total iterations as well. Now I wanted to restore both the half trained agents and the fully trained agents and test them against eachother to see how much they have improved in the last half of the iterations. However while restoring only the fully trained or only the half trained agents works fine, when I try to load the fully trained and its respective half trained agent I get an error:     !       I was wondering why that is? Because as I said, loading only the fully trained agents or the half trained agents works fine on its own. Should I also post my code of restoring the agents?"
"Hello,    I have implemented the Kingdom Builder board game from Queen Games (there a maybe reserved rights) in python3.    The repo is here:      I have written this game with the goal to train an alpha zero agent to create a really powerful player, but I failed. =(  I would like to ask smarter guys then me, to build a advanced player. So I could learn why I failed that bad...    **The game:**    - non deterministic (random terrain cards)  - some hidden information (terrain cards of opponents)   - multiplayer (2 - 5 players)  - hexagon grid board (20x20) (complex CNN)  - board could be build with 2 * 8 * 4 = 48 different combinations  - random rule cards for each new game - changes strategy drastically   - there is a immediately ""gold"" reward per move, but following this reward will miss the long term strategy and in fact this ""nice feature"" is useless    **History of my tries:**  I have forked a multiplayer alpha zero implementation for the agent and made a complete code mess with my tries (sorry for that):      I have changed the implementation a lot to fit my needs:  - allow hidden information (states) to be saved and transferred to the stateless game class  - add a modified version of rays rllib MCTS with recursive traversing because the original MCTS implementation made a memory and cpu mess, because of my new game interface  - rework the multiplayer value loss function: The loss is only calculated over players current in the game. My hope was that one agent could play strong with different numbers of opponents. I think mathematically there should be no problem and this would be a nice ""investigation""  - rework the overall loss function to weight the value head more then the policy head - because the 4802 logits large policy head has never learned anything  - add chance nodes for the terrain cards  - rework observation variable to bypass the CNN and add some extra information to the CNN following FF directly (save some CNN layers) - add some models for this tests  - rework the CNN hexgon interpretation so that each node in each row has the same neighbors    - add tensorboard support for some training data parameters      I have trained only on my nvidia laptop GPU (GTX 1660 TI). Could be the missing processing power the bottleneck?    Would be great to get some assessments for more advance people.    I know this topic is not a real issue, but if there is no copy right issue this game would be a really good learning environment. Also this games makes a lot of fun to play :)     Thanks    Sebastian  "
"CentOS 7.8.2003 (Core)  cmake version 3.18.4  clang version 10.0.1   Python 3.8.8    I have successfully built/run OpenSpiel + libtorch on macOS and Ubuntu, but am running into issues when building with CentOS. It is a lab cluster (more compute needed for complex games), so cuda is also installed in a non-standard location. I already fixed a few issues installing libtorch from source because of this (namely,  ) but am now stumped on some linker errors during the torch tests---or really anytime find_package(Torch REQUIRED) is used. I know this is really more of a libtorch issue, but I am curious if anyone else has solved this while building OpenSpiel, because the solution from the libtorch forums seems to be to use a different compiler (see solutions   and  ). Of course, clang is required for OpenSpiel, so this is not an option in my case.    Anyways, here is the error:       Any advice is much appreciated!"
"In   if I try to run `./open_spiel/scripts/generate_new_playthrough.sh python_mfg_crowd_modelling` then I got the following error:     I am willing to help debugging this issue, but it might be easier with some help. I will open a PR to follow the issue."
"Hello, I am sorry to contact @lanctot in this way, but I cannot find any personal email or anything.   I am a researcher focus on the CFR. Recently I am working on implementing the ARMAC.  In this paper, I have some problems, anyone could help me on this?    First one:   <img width=""812"" alt=""Screen Shot 2021-05-24 at 10 12 31 PM"" src=""     Second one:  According to this article, the W is mapping state S to actions' accumulative regrets conditioned on some value to avoid Importance sampling. But this value is used to predict advantage value based on the history h and action a. And we all know that the state s is not equal to history h. So this is right?  <img width=""390"" alt=""Screen Shot 2021-05-24 at 10 21 49 PM"" src=""     If anyone could help me with this problem, I could not be possible to more thankful. "
None
"     I was creating a thin wrapper over `open_spiel.python.rl_environment.Environment` to interface with `dm_env.Environment`. However, I notice that the `observation_spec` of OpenSpiel doesn't provide information such as `dtype`, `minimum`, and `maximum`. Can I assume that OpenSpiel observations are always a list of python `float`s that are bounded between `0` and `1`?      EDIT:  I found that such a wrapper already exists in  , so I no longer need to solve this problem myself."
"When installing openspiel on windows, I followed the steps listed in documentation. Afterwards, 36 tests failed. It seems the issue was that some (many?) requirements were missing. Eg, numpy didn't get installed. The file requirements.txt only contained three items, so it could have something to do with that."
"Hi everyone,     I wanted to use the following line to start a game between the az-trained model and me.     python3 open_spiel/python/examples/mcts.py --game=connect_four --player1=az --player2=human --az_path=/mnt/d/Windows_to_Ubuntu/connect_four/checkpoint-240.pt    However, I ran into the following issue. I am using Ubuntu 20.04 LTS on my Windows 10. It passed 100% of the tests after I installed everything based on the instructions for Windows users. Also, I installed libtorch based on my colleague's suggestion. Unfortunately, this is still an issue. I hope someone could give me some suggestions. Thank you for your time and help in advance.     2021-05-18 19:01:49.871135: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory  2021-05-18 19:01:49.873442: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.  Traceback (most recent call last):    File ""open_spiel/python/examples/mcts.py"", line 223, in        app.run(main)    File ""/home/fengzhihao/.local/lib/python3.8/site-packages/absl/app.py"", line 303, in run      _run_main(main, args)    File ""/home/fengzhihao/.local/lib/python3.8/site-packages/absl/app.py"", line 251, in _run_main      sys.exit(main(argv))    File ""open_spiel/python/examples/mcts.py"", line 197, in main      _init_bot(FLAGS.player1, game, 0),    File ""open_spiel/python/examples/mcts.py"", line 96, in _init_bot      model = az_model.Model.from_checkpoint(FLAGS.az_path)    File ""/home/fengzhihao/open_spiel/open_spiel/python/algorithms/alpha_zero/model.py"", line 202, in from_checkpoint      model = cls.from_graph(checkpoint, path)    File ""/home/fengzhihao/open_spiel/open_spiel/python/algorithms/alpha_zero/model.py"", line 215, in from_graph      saver = tf.train.import_meta_graph(metagraph)    File ""/home/fengzhihao/.local/lib/python3.8/site-packages/tensorflow/python/training/saver.py"", line 1465, in import_meta_graph      return _import_meta_graph_with_return_elements(meta_graph_or_file,    File ""/home/fengzhihao/.local/lib/python3.8/site-packages/tensorflow/python/training/saver.py"", line 1481, in _import_meta_graph_with_return_elements      meta_graph_def = meta_graph.read_meta_graph_file(meta_graph_or_file)    File ""/home/fengzhihao/.local/lib/python3.8/site-packages/tensorflow/python/framework/meta_graph.py"", line 643, in read_meta_graph_file      text_format.Merge(file_content.decode(""utf-8""), meta_graph_def)  UnicodeDecodeError: 'utf-8' codec can't decode byte 0x90 in position 70: invalid start byte"
"Hi @findmyway, seems like there are still some problems with the Julia test so I've disabled it again. See:      I _think_ currently the line with the copy of the libstdc++.so is only triggered when julia is installed manually. On Github Actions, Julia 1.6.1 is installed in /usr/local/julia. So I wonder if there's a similar patch for this issue in that case?"
Happened twice today.     Here's an example:      @perolat can you take a look?
I am also one researcher focused on the EFG. I have read the preprint version of that paper. Is there any possibility that Lanctoc can support me with the implementation of this algorithm?
"Since OpenSpiel's development is quite healthy (actively developed and many tests), I assumed that I could use docker smoothly. This is the promise of docker, after all. However, this was not the case. Here are some observations of mine after trying to build and run docker stuff with OpenSpiel. If I missed some important information somewhere, please let me know!    ### documentation inaccurate  The command in the documentation doesn't work. It has a syntax error (missing build path, usually the current directory `.`).         ### building an image from `Dockerfile.base` doesn't work out-of-the-box  I tried building docker images using `Dockerfile.base` on both macOS and Linux, and both them needed extra care. More specifically, I had to insert `pip3 install tensorflow` somewhere to make it work, otherwise, `python_policy_gradient_test` would fail.     ### no related tests  AFAIK, there are no docker-based tests running in github actions. I think the Dockerfile is outdated now but since there are no related tests I don't know what broke it.    ### proposed solution  I think OpenSpiel should build docker images using github actions and host the built images using github packages.  . This way, users could pull docker images and run OpenSpiel inside of it directly without going through the hassle of building it."
"In case a player has no possible actions, the documentation is not clear.    ApplyActions states that the player should have the action kInvalidAction:    Generate_playthrough.py seems to work when the player has kInvalidAction as action.    LegalActions seems to say that the player should have a empty list:    spiel.cc returns an error in case kInvalidAction is used    This makes CFR not working on a game I am developing in my own fork:      Can you help me? Is what I am saying true?  Can CFR works with games where some players do not have valid actions?"
"Using matplotlib >= 3.2 the line from the example here:      fails with          Using matplotlib 3.1.3, it works just fine.    "
"Hey,    I am trying to compile the code and I have the following error:       My computer is using MacOS BigSur 11.3. With cmake version 3.15.3 and Apple clang version 11.0.3 (clang-1103.0.32.59). I am running the code from a conda environment (with conda 4.8.3).  I get this error when downloading the git and running  . I also get this error when running   after running   (and the same with --vertualenv=true).    Thank you a lot for your help!  "
"Hi all,    Just a quick announcement to let everyone know, in preparation for the next release, we will be moving to support Python 3.9 as of next week's sync. Here is the PR change:       This is a big change because TF  = 2.5.0rc2.)    Hence, if you are using TF with OpenSpiel on master, you will need to move to TF 2.5.0rc2. If this will be an immediate problem for you, I suggest forking now and rebasing/merging after the release.    Thanks!  "
"This feature was talked about before with @selfsim and @SwiftGodDev.    I have been working on it a little bit and should hopefully have something soon, but I just thought I would make an issue to ensure that it is not forgotten about."
"When I use the docker in windows to build open_spiel with the command  ""docker build --target python-slim -t openspiel -f Dockerfile.base --rm .""  The docker Error shows:  ------  ` => ERROR [base 11/20] RUN ./install.sh                                                                            0.2s   > [base 11/20] RUN ./install.sh:  #21 0.222 /usr/bin/env: 'bash\r': No such file or directory  executor failed running [/bin/sh -c ./install.sh]: exit code: 127`  ------  I'm not familar to Docker, can you help me?"
"Should `__str__()` have perfect recall? I ask because it looks like for `tic tac toe`, `battleship`, and `oshi_zumo`, the `__str__()` does not have perfect recall, e.g.:       I think this is causing issues with the way that the BR caches `.value()`     Here's a demo I ran (can supply the policy action_probability_array to repro `my_policy` if anyone is interested):       I believe the expected outcome is that they should have the same value, e.g.:       An alternative fix could be to memoize `best_response.BestResponsePolicy.value` by `history_str()` instead of `__str__()`."
"Hello again,    I have found an issue with the ""state.spiel_move_to_checker_moves"" function in backgammon.    The third item in the ""checker_moves"" array, (which is the return of ""state.spiel_move_to_checker_moves""), flags if an opponent's checker was 'hit' i.e. placed on the bar.    This boolean is always displaying false even when an opponent is hit. I have attached an image."
   This prints:     Is this an intended behavior?
"Hello everyone,    We are planning a release in the coming months, so I am starting a thread to discuss it and give updates.    I am currently looking into how we can get fully into PyPI, as a bdist_wheel so you don’t have to compile from source when installing via pip. Luckily we mostly already have what is necessary, and a great project like   will help automate this for various platforms and Python versions.    I have hit a few snags, mostly in an attempt to support Python 3.9, which comes as default in MacOS 10.15 (and Ubuntu 21.04). It seems like TF 2.5 has Python 3.9 support, and I was able to compile OpenSpiel and run most tests under Python 3.9 with TF 2.5, which recently issued its first release candidate. The rest seem minor and fixable, so I don’t anticipate any major problems.    Anyway it seems like a good opportunity to line up with the TF/Ubuntu releases and official Python 3.9 support, so it won’t be any earlier than May.. maybe more like June.  "
I successfully built openspiel with `BUILD_WITH_LIBTORCH=ON`. I have cuda 11.2 installed on my system. But when I start training it does not seem to utilize gpu.
"Just a minor issue I noticed -- maybe it's even intended.    When making an observer for public info and no private info for Leduc, even if the observer is supposed to have perfect recall, the tensor immediately before and after a chance node seems to be exactly the same.       outputs       but I'd expect them to differ, or if calling `set_from()` on chance nodes is unintended, then I'd expect an error to be raised."
"`Game` has a map of params called  , while `GameType` has also a map of params called   which also uses default values.    I find this very confusing. For example, a game may specify which parameters it uses, but then if you call MyGame::GetParameters(), you do not get the defaulted values, **unless** the game uses the `T Game::ParameterValue` method call (and to add to the confusion, that call can also provide _another_ defaulted value). I stumbled upon this in universal_poker.    I'd like to suggest that there should be only the (immutable) GameType.parameter_specification. The Game receives a set of parameters and makes a union of them and GameType.parameter_specification in the constructor, and saves it into its internal immutable map of game_parameters_. These will be then accessible to DerivedGame and can use ParameterValue in their constructors as to avoid string lookups in the param map, and these saved members should be typically marked as const.    WDYT?"
"Hi, here is a crazy idea I was thinking about for a while and I wanted to ask your opinion about it.     In a number of games I encountered things like ""write comma/space separated values to encode arrays"", game-transformations that take nested expressions, aliasing of common game strings (for goofspiel/poker), etc.   Additionally I think it would be pretty cool to have some new powerful game transformations in place where you could just specify things like ""fix this player as a chance node"", ""factorize actions in this game to have a maximum branching factor"", ""create a specific subgame"",  ""after each action of the player, add a chance node in this specific way"", and so on, and possibly compose any of them together as well. Then you can simply reuse the same algorithms on the game as you would normally, without having to modify the algorithms!    IMHO all of this starts to look like a domain specific language over the possible games, with a C++ backend. These features can be added incrementally as they evolve, leading to hard-to-compose / hard-to-maintain code, or alternatively, they could be designed up-front. There is arguably no better language for DSLs than Lisp, and   . Some functions would be directly binded to C++, and there would a small number of predefined functions / macros.    Is it too crazy of an idea? :)    "
"Dear @lanctot,    Thanks to you and your team for developing such good repo. Will you share the code of the PSRO's paper: _A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning_ in this repo as good learning examples for other researchers?    Thank you for your attention."
"I can invoke `ToJointTabularPolicy` with `pyspiel.to_joint_tabular_policy` but I can't figure out how to invoke `ToTabularPolicy` ( ).  (My end goal is to turn a C++ CFR policy into a C++ tabular policy, to turn it into a python tabular policy, so if there's a better way to do that then I can do that instead.)    "
"Hi all,     This is a spin-off thread from a conversation in the Amazons PR:      @selfsim was asking about doing RL in C++. Amazingly, we have very few RL algorithms implemented in C++. We have AlphaZero that uses the TF and PyTorch C++ interfaces, but we only two traditional ones are Q-learning and Sarsa which were recently added by @satyaki3794. I've been discussing some basic RL and algorithms with Satyaki as well, who is now interested in understanding DQN, so I thought I would suggest it as a potential collaboration with @selfsim.    So I thought.. why not also have a C++ implementation of DQN, just like we have C++ AlphaZero? I think it would be quite a fun and unique addition to the library. I don't know of any pure C++ implementations, could make a nice reference.     I think it might be possible to almost directly port the PyTorch DQN to C++ using the PyTorch C++ API. I suggest PyTorch only because it seems to be easier to install and use the C++ interface; we've had a number of unresolved issues with connecting to TF from C++, compiling it from source is non-trivial.     Anyway, I'm tagging also @Asugawara, who contributed several of our PyTorch algorithms, and @jhtschultz who might also be interested, and @christianjans who contributed the PyTorch C++ AlphaZero. I'm curious what you all think about this. Some part of me just wants to see it happen because we _can_ do it. And, go on.. just admit it.. it'd be pretty neat..  ;-)   "
"Hello all,    Today we released our   API, see here:      It is currently quite the bare minimum: just the main API, no algorithms or metrics, nothing beyond simple access to the core API and the games. It should be fairly easy to add more access to some of the C++ code as we do in Python and Julia APIs. It'd be great to start adding some basic algorithms. We have a few basic ones internally that we need to clean up first, that we'll add in the coming months. But e.g. it'd be wonderful to have a simple, clean parallel MCTS that uses goroutines. And maybe eventually hook up to the Tensorflow Go API or  .    I have very limited experience with Go, so I'm not really sure how to best structure the development of this API over time nor how Go packages/distributes/maintains libraries (with C++ dependencies) like this. If anybody has more Go experience please feel free to comment here. Pull requests are also very welcome :)    Huge thanks to Julien Perolat (@perolat), Thomas Degris (@ThomasDegris), and Audrunas Gruslys for help putting this together.    Enjoy!    "
"Ubuntu 18.04  cmake version 3.18.4  clang version 10.0.0-4ubuntu1~18.04.2   bazel 2.0.0  Python 3.8.8    Build succeeds when following standard installation instructions (i.e. BUILD_WITH_TENSORFLOW_CC=OFF). Following instructions from alphazero.md and   I installed @mrdaliri 's open_spiel branch of tensorflow_cc and followed installation instructions. Tensorflow_cc built/installed fine, as did the usage examples. After running: `BUILD_WITH_TENSORFLOW_CC=ON ./install.sh` and `CXX=/usr/bin/clang++ BUILD_WITH_TENSORFLOW_CC=ON ./open_spiel/scripts/build_and_run_tests.shc`, I noticed that tf_trajectories_example, vpnet_test, and alpha_zero_example_test were each failing with the same error:     tf_trajectories_example:  >   1/189 Test  #94: tf_trajectories_example ...........................Child aborted***Exception:   0.75 sec  2021-03-17 23:21:02.238356: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA  2021-03-17 23:21:02.357149: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3600000000 Hz  2021-03-17 23:21:02.357492: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x24a7d20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:  2021-03-17 23:21:02.357505: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version  2021-03-17 23:21:02.499277: E tensorflow/core/framework/op_segment.cc:54] Create kernel failed: Invalid argument: NodeDef mentions attr 'allowed_devices' not in Op  resource:resource; attr=container:string,default=""""; attr=shared_name:string,default=""""; attr=dtype:type; attr=shape:shape; is_stateful=true>; NodeDef: {{node beta1_power}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).  2021-03-17 23:21:02.508800: F /.../open_spiel/open_spiel/contrib/tf_trajectories.cc:114] Non-OK-status: tf_session_->Run({}, {}, {""init_all_vars_op""}, nullptr) status: Invalid argument: NodeDef mentions attr 'allowed_devices' not in Op  resource:resource; attr=container:string,default=""""; attr=shared_name:string,default=""""; attr=dtype:type; attr=shape:shape; is_stateful=true>; NodeDef: {{node beta1_power}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).    [[beta1_power]]    vpnet_test:  > 2021-03-17 23:21:12.672911: E tensorflow/core/framework/op_segment.cc:54] Create kernel failed: Invalid argument: NodeDef mentions attr 'allowed_devices' not in Op  resource:resource; attr=container:string,default=""""; attr=shared_name:string,default=""""; attr=dtype:type; attr=shape:shape; is_stateful=true>; NodeDef: {{node beta1_power}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).  2021-03-17 23:21:12.697392: F /.../open_spiel/open_spiel/algorithms/alpha_zero/vpnet.cc:92] Non-OK-status: tf_session_->Run({}, {}, {""init_all_vars_op""}, nullptr) status: Invalid argument: NodeDef mentions attr 'allowed_devices' not in Op  resource:resource; attr=container:string,default=""""; attr=shared_name:string,default=""""; attr=dtype:type; attr=shape:shape; is_stateful=true>; NodeDef: {{node beta1_power}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).    [[beta1_power]]    alpha_zero_example_test:  > 2021-03-17 23:21:45.369748: E tensorflow/core/framework/op_segment.cc:54] Create kernel failed: Invalid argument: NodeDef mentions attr 'allowed_devices' not in Op  resource:resource; attr=container:string,default=""""; attr=shared_name:string,default=""""; attr=dtype:type; attr=shape:shape; is_stateful=true>; NodeDef: {{node beta1_power}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).  2021-03-17 23:21:45.371901: F /.../open_spiel/open_spiel/algorithms/alpha_zero/vpnet.cc:92] Non-OK-status: tf_session_->Run({}, {}, {""init_all_vars_op""}, nullptr) status: Invalid argument: NodeDef mentions attr 'allowed_devices' not in Op  resource:resource; attr=container:string,default=""""; attr=shared_name:string,default=""""; attr=dtype:type; attr=shape:shape; is_stateful=true>; NodeDef: {{node beta1_power}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).    [[beta1_power]]    After searching around a little bit, it looks like this is usually caused by a version mismatch between tf and the tf model server, but I haven't seen any dependency on tf model server for open_spiel or tensorflow_cc. I have a feeling there is a simple/key piece I am missing here. Have you seen this error before?"
"Hi,    We have been trying to get the PyTorch agents provided by OpenSpiel on our GPU.  Normally you can do this with `.to(""cuda:0"")` on an nn.Module. However, the agents are a subclass of `rl_agent.AbstractAgent`which does not provide such a method."
"`    else:        other_player = state.current_player()        _, strategy = self._sample_action_from_advantage(state, other_player)        # Recompute distribution for numerical errors.        probs = np.array(strategy)        probs /= probs.sum()        sampled_action = np.random.choice(range(self._num_actions), p=probs)        self._strategy_memories.add(            StrategyMemory(                state.information_state_tensor(other_player), self._iteration,                strategy))        return self._traverse_game_tree(state.child(sampled_action), player)`  In the code line314-325, when the current player is not the updating player, the function _tarverse_game_tree will return the child node's utility directly which is not coincided with the original definition for CFR.    To be more specific, these code's defintion for CFV and CFR are:  !     But the original definition for these are:  !     We can see that these equations differ one factor which is:  !       I cannot find any modification in the origin paper  . So these modifications are okay? Will it changes the boundary for the accumulative regret?"
"Hi,    I saw the public state API in the repository with kuhn poker available.   I wonder if there is any work in progress to add ReBeL algorithm.   Thanks.  "
I tried calling C implementation of `mcts_search` trough Python:       and  got the following error:   
"Not a huge issue, but I figured I'd log it here since I noticed:    in Kuhn poker and universal poker, picking an illegal action for a deal does not raise an error (i.e., dealing the same card twice).    in Leduc poker, doing so raises a `SpielError`.     Seems like they should probably all exhibit the Leduc poker behavior?          "
"I have built openspiel from source, but I have trouble testing it.    - running `open_spiel/scripts/global_variables.sh` I get python 50% errors caused by missing module `absl`. Despite the fact that I installed `absl` trough pip3 (and I can import it in python3 repl)  - running `open_spiel/scripts/global_variables.sh` with `BUILD_WITH_JULIA=ON` and `test_only=julia_test` I get `make: *** No rule to make target 'spieljl'.  Stop.`    I have up to date pip, python 3.8 and julia 1.6.  "
"Currently sim-move games have to be transformed to sequential games in order to compute best response (and exploitability). This adds overhead and if I remember well it doesn't work too well with observations / observer API.    The way BR works it essentially constructs the entire history tree along with infostates for the players, keeps track of the decision states and is making non-trivial recursion and updates for the BR calculation, in order to make the response computation consistent with the decision space of the responding player.     Infostate trees do all of the above points (construct the entire game with infostates, keep track of decision states, etc.). Additionally they work with sim-move games out of the box. The BR calculation is very simple, it is just a greedy recursive traversal of the infostate tree for the responding player.    I would like to propose swapping the implementation of BR that uses history tree with infostate trees. Let me know if you agree, I can make a PR which implements this. Additionally, this would make the history tree obsolete, as it is used only by BR within the repo, and is therefore a candidate for removal.    I raised this somewhere before, but only now got time to take a look at it. Tagging @finbarrtimbers and @lanctot   "
"I installed openspiel with `] add OpenSpiel` on julia 1.5.3 and copy pasted code from one of your test:         If I run this code I get:     - Is it possible to fix this error?   - I am also getting a little bit lost in the types. Is there some way to get more helpful type signatures? For example by truncating the prefixes `CxxWrap.CxxWrapCore`.  - Finally, how can I pass custom `child_selection_fn` in MCTSBot? I assume using ordinary Julia function won't work.    ----  Btw I am trying to rewrite my Python project in Julia to improve performance. But the Julia API is much more rough than the Python one (complex types, no comments, harder to discover methods for given type). Are there some tricks / tooling to list methods, simplify types or inspect comments / jump to C++ source definition? I would be very thankful for any advice on this."
"Could we move bridge dummy solver as an optional dependency, i.e. `BUILD_WITH_BRIDGE_DUMMY_SOLVER`? I don't know how crucial it is for bridge game evaluation -- is it just a bot? It takes a while to compile and it is probably not used by that many people. I can make a PR that implements this, if it is ok to move it out of the core."
**Background:** I am using C++ External Sampling MCCFR with a custom Python game (Liar's Dice) as per the example at `mccfr_cpp_example.py`. PolicyBot is used to evaluate the model against a simple hard-coded Policy.    **Problem:** It seems that action probabilities dict returned from `solver.average_policy()` are shuffled which causes the PolicyBot to improperly choose an action.    I was able to fix this locally by using policy keys as legal actions:     
"I am looking for a way to paralellize python tests, especially for `parameterized.TestCase`s, as currently only the test files themselves are paralellized. Is there some simple way to do this? I looked at flags for abseil py with no luck. I found   in abseil, but from looking at it I have an impression it only sets env flags and does not run in parallel."
"Shell is not my kind of thing but I believe this code in install.sh is not doing what it's supposed to:     The   is supposed to have a non-zero value if git is not installed, because then   returns a non-zero value. But because of the  , the script exits instead of setting the variable and installing git.  I propose this:       Could anyone check if I'm right?"
"I am in the process of attempting to implement Risk in openspiel (with and without fog of war), and have run into a bit of a roadblock regarding the game with fog of war.  The problem is that when it is not an agent's turn (i.e. they cannot act), they should make and store observations as they contain meaningful information about the state of the game. Each turn has multiple steps so agents can take many actions before their turn ends. One idea I have had is, after every action is taken, to allow the agents whose turn it is not( in the human understanding of the game) to make an observation, but only take an action that ends their pseudo-turn and so on until the game is back in the hands of the agent whose turn it truly is. This feels a bit silly. Any alternative suggestions about this, or advice more generally, would be greatly appreciated."
"From the doc, it says that:         However, the `maximizing_player_id` is not considered here.          "
"Hi,    Is there any implementation of approximate exploitability, like the one mentioned in the paper:    It seems that in the example code, the full game tree traversal is needed to calculate exploitability.   Please let me know if I misunderstood anything.  Thanks."
"I generated playthroughs with `./open_spiel/scripts/generate_new_playthrough.sh ""dark_chess(board_size=4)""`  (soon to be merged)    The resulting file did not include observation tensors. When I ran `./scripts/regenerate_playthroughs.sh` it was updated with the observation tensors (and stays unchanged on further calls). Maybe the `generate_new_playthrough` does not work well with Observer API?    If you want, I can take a closer look for more details."
"I’ve looked through several of the python MARL algorithms(rcfr, deep cfr, nfsp, and psro) and it seems that they all use information states. Are there any MARL algorithms that can use observation tensors instead of information states? "
"I cloned this repo and build a docker image.  Then, I ran one example game which is my favorite ""backgammon"".    `python3 example.py --game=backgammon`    I noticed that it doesn't play doubles correctly.  For instance, in the following situation, X can bear off four checkers instead of two.     "
"For details see,  "
"This is a request for advice, because I can't figure this out from inspecting the code.    I see that the intended format for parameterising a game is     `game = MyGame({""myparam"": pyspiel.GameParameter(2)})`    or    `game = pyspiel.load_game(""game_short_name(myparam=2)"")`    My first question: how are we intended to then invoke the parameter value? I have tried    `self._myparam=params['myparam']`    but this gives me a GameParameter object, when I want an integer    My second question is more of a comment - I can't get the load_game method using the syntax above to work with an actual parameter. Fails with the message      `Traceback (most recent call last):    File "" "", line 1, in    pyspiel.SpielError: Unknown parameter 'myparam'. Available parameters are:  `      I do have a default for the specific parameter set when creating the class, as in:         Thanks in advance"
When trying to install from pip and running the test.         matplotlib's version is 3.2.2.
"On MacOS 11.2.1, XCode 12.4, using Python 3.8.2:         @tewalds, any idea why it would fail in this case?"
"Hi,  I installed everything in a separate virtual environment. Please see the attached screenshot.  !   "
Error thrown when trying to declare the GameInfo of a custom game
"Hello,      I am trying to install open_spiel from source (after several failed attempts using pip directly).   Ubuntu 20.04.2 LTS    cmake version 3.16.3    clang version 10.0.0-4ubuntu1   Target: x86_64-pc-linux-gnu  Thread model: posix  InstalledDir: /usr/bin    Everything runs fine per the installation instructions until   ./open_spiel/scripts/build_and_run_tests.sh         This is a bit beyond my skill set to try and solve. I was wondering if anyone had an idea on how to fix this.     Thanks!"
"Specifically, `pyspiel.SpielError: GetStatePolicy(const std::string&) unimplemented.` when running `mccfr_cpp_example.py` when replacing pyspiel.exploitability with pyspiel.nash_conv.    This is because NashConv requires computing the on-policy values, but the policy may not have every info state in the table due to Monte Carlo sampling, so we need to expose the NashConv overload with the extra boolean that specifies calling the State-based GetStatePolicy.    I'll fix it in the coming weeks."
"Is there a reason that the   expose a method to get a callable policy (`callable_avg_policy()`) but not a way to get a   object?    I think we could manually subclass from `Policy` and define `self.action_probabilities(state)` as `cfr_solver.callable_avg_policy(state)`, but is there a reason this isn't built into the MCCFR class already?"
"Call for game suggestions!    As announced on the AAAI-21  , we'd like to organize a hidden information games competition: in the spirit of now-dead ACPC.  The intention is for the competition to be aligned with the workshop so that results are announced at the workshop.     The web site is here:   . Let us know what kind of games you'd be interested in competing in by filling the form it currently directs to.    Tagging a couple of people who may be interested. If you know anybody who might be interested, please direct them to the form above. @jhtschultz @noambrown @inejc @elkhrt @flostim @solinas @michalsustr @lifrordi @nathansttt"
"Hi @findmyway, we had to move our CI away from Travis and to Github Actions this week. The problem is that the Julia tests are now failing again, see here:      The problem is similar to   except the command I used before no longer works because I copied the library to the $HOME/packages/julias/julia-1.3.1. See here:      Now the machines have Julia installed in /usr/local/julia1.5.3, and this might generally happen on the user side. So we'll probably need to do a sudo to copy the file. I think we can do this in our install script as long as we echo something like why we requiring sudo... but even if that works, it does not feel like an elegant solution. Do you have a better idea about how to fix this?    "
"Same with python tests in noxfile.py  session.run(        ""ctest"", f""-j{4*os.cpu_count()}"", ""--output-on-failure"", external=True)    With 12 cores/24 threads for the 1920x it cannot fit that many tests at once.  "
"Build looks fine on Ubuntu 18.04 after changing script to use CXX=clang++-10 and commenting out the if test in build_and_run_tests.sh    Some GPU tests fail due to out of memory as several running at once with   MAKE_NUM_PROCS=$(${NPROC})  let TEST_NUM_PROCS=4*${MAKE_NUM_PROCS}    Changed that to just  let TEST_NUM_PROCS=1  and re-ran the failed tests individually by changing  ArgsLibAddArg test_only string ""python_deep_cfr_tf2_test"" 'Builds and runs the specified test only (use ""all"" to run all tests)'  and they all passed.    Perhaps some GPU memory fencing would be helpful.  2080ti has 11GB VRAM.  The tests seem to individually max that out, so only one at a time can run."
"Acme is a research framework for reinforcement learning:    -    -      It now supports  , thanks to @jhtschultz! Thanks also to help and support from the Acme team (@fastturtle @mwhoffman @bshahr).    I'm quite happy to see this cross-project collaboration driven by external contributions. The hope is that this will facilitate large-scale RL in games.    Enjoy!"
"I'm looking to create agents for complicated boardgames such as  ,  ,   and other resource management type games. A prominent feature of most of these games is a complicated turn based system. In Puerto Rico & New Frontiers there is an order to the players that can change, and the active player picks a shared phase where then again the turn order specifies in which order players can play out this phase. In Clans of Caledonia a player can keep playing until they pass or run out of options.    I'm just exploring whether OpenSpiel is a good fit for my use case so I'd like to know:    * Can OpenSpiel games deal with dynamic turn orders?  * Is support for Python games coming soon? (see #480, #479) I think there are a lot of people playing around with ML in Python that don't know C++ but could still contribute an endless variety of games."
"Hi,   I want to build from source in MacOS. Here's what I put in the command line       Following the messages   and        Here's the error message:       Some information of my machine:       I'ver referred to the issues from   and  , but still no luck.     Anyone has idea to help me out? "
"Wikipedia mentions that in the game of Liar's Dice    >  common bidding variants, given a previous bid of an arbitrary quantity and face value, include:  > - the player may bid a higher quantity of any particular face, or the same quantity of a higher face (allowing a player to ""re-assert"" a face value they believe prevalent if another player increased the face value on their bid);  > - the player may bid a higher quantity of the same face, or any particular quantity of a higher face (allowing a player to ""reset"" the quantity);  > - the player may bid a higher quantity of the same face or the same quantity of a higher face (the most restrictive; a reduction in either face value or quantity is usually not allowed).    However, after comparing differing results on our internal implementation of Liar's Dice and OpenSpiel's, I now think that OpenSpiel implements a different bidding variant, which can be summarized as    > the player may bid the same quantity of a higher face, or a higher quantity of any face (allowing a player to ""reset"" the face).    I think that can be seen in the implementation of the game, here:         By encoding bids with the bid quantity occupying the higher bits, you end up concluding that a bid is higher if the quantity is higher (no matter the face), or the quantity is the same but the face is the same.    I think this is potentially problematic because face 6 is considered the ""catch-all"" face. So, if a player does not bid face 6, under the implemented rules the opponent should bid the same quantity of face 6, and be guaranteed to be safe. This ruins the strategy of the game.    Given that the class-level documentation of the game simply points to Wikipedia without comments, I think that if we wanted to stick to the currently implemented bidding rules, we should at least document this quirk. Alternatively, I'm very happy to fix the implementation to align the bidding rules to what Wikipedia says."
"A quick question that's been confounding me: how do I interpret the `player_improvements` returned by `nash_conv` in  ?    From the OpenSpiel paper: ""NashConv [represents] in total, how much each player gains by deviating to their best response (unilaterally)"".    However, in my testing, I've sometimes gotten negative values for one or both players in `player_improvements`, e.g. `[-0.01648912 -0.00547954]`.  Am I doing something wrong (very possible, since I'm using a custom game written in python), or is this an expected outcome?    (Secondary question: is there a better place to ask for help than here in github issues?)"
"For eg.       The output will be       This problem annoys me when I was just implementing the vanilla CFR. I found that when I created the Node of Information Set with legal_actions [0,1,2]. After that, I use recursion to update the regret, but found that the legal_actions returns me [1,2]."
I think there may be a small bug in the exp descent example. It looks like the hidden layers constructed in the for loop aren't included in the resulting model (line 71 initialises the input tensor which is passed to the linear output layer on line 77)     
"Hi openspiel team,  I am an undergraduate student interested in intelligence and RL. I am planning on training various RL agents on my implementation of the ""Game of Amazons"", a close relative of Chess. Is this game currently in development at openspiel? If not, would you be interested in my contribution?   Cheers"
I trained a model for chess. I need to use it in a gui(Arena). Is there anyway to do this?
"(this may also be an issue with other agents - mcts is the one that I've tried)    The situation: I experimentally modified mcts.py from python/examples to test it on the Python version of tic_tac_toe    imported tic_tac_toe.py and loaded the game directly rather than with pyspiel.load_game    code fails with error message:    >     bot.inform_action(state, current_player, action)  > TypeError: inform_action(): incompatible function arguments. The following argument types are supported:  >     1. (self: pyspiel.Bot, arg0: pyspiel.State, arg1: int, arg2: int) -> None  >     This appears to be fixable by making TicTacToeState explicitly inherit from pyspiel.State, but I then had to hack the clone function also to copy all the class attributes individually rather than by using copy.deepcopy()    Since the TicTacToe code explicitly says ""not inheriting from pyspiel.State because of problems with pickle"" I assume that this is a non-workable solution in general - I hope there's a better one."
"Apologies if this isn't the right place for requests.    I understand that the preferred method for adding new games to open_spiel is to code them in c++    However, is it possible to add instructions to the documentation for adding and playing a new python game, similar to what already exists for a new c++ game, including a rundown of what features of the package can and can't be accessed by new python games?    For instance, it isn't clear to someone unfamiliar to the code (ie, me) whether there's a mechanism for including python games among the list of registered games so that they can be loaded by pyspiel.load_game, and the python implementation of tic tac toe currently available can't be added to the python integration tests, because it's not in the registered games list"
"Hi, I noticed that all warnings are disabled:         Is this intentional? "
"I'm having some trouble understanding the info_state for the first price sealed bid auction info state. I'm using the following code snippet:         which returns:         Since there are 3 players, shouldn't the action be a 3 length vector? Or is there some other more accurate interpretation? Additionally, what does the actual info state itself represent, as I couldn't find clear documentation for this or infer from the C++ implementation. Specifically, I was also wondering this for the `pig` game (as well as how to interpret the info_state for that game as well).    Additionally, in a previous issue I was told that there were other ways of retrieving state from a game; in this case is there a better one?"
"Is there any documentation explaining what each of the 11 indices in the state space of Kuh Poker represents? I understand the gist of the game, but I need to know what the indices mean -- in order to compare my final models against the Nash equilibrium strategies. "
"Whenever I try to get the observation of a terminal state, it fails with the error:  `pyspiel.SpielError: /open_spiel/spiel.cc:699 player >= 0`  `player = -4, 0 = 0`    It looks like the problem is that the player is -4, which doesn't really make sense in a 2-player game.  I've tried with both Chess and TicTacToe, but both have the same result."
"I'm interested in running a custom algorithm on the Markov Soccer game, and wanted to use the `rl_environment` API provided, something along the lines of the following code:         However, I get this as output, and I'm not quite sure how to interpret this `'info_state'` element, as each player has a 120 element observation list, when the board is only 5x4. Could I get some clarification (or direction to documentation) how this 'info_state' parameter should be interpreted correctly? For reference, the output is the following:     "
"Is there a way to change the perspective of a state.  For example, in Chess, is it possible to make the white player black and vice versa?"
The leduc_poker's history returns all the histories. Is there any method to part of the history visible only by himself?
"Thanks for your attention!   As far as I know, the application scope of exploitability in this code was limited to imperfect information game, is there any solution to calculate the exploitability of an perfect information game?"
"Hi, I'm interested in implementing the `InformationStateTensor` function for the Tarok game but I think it would also be useful to have general recommendations for info state/observation tensors written down somewhere to also benefit others.    I see three possible approaches for the specific task above: (1) one-hot encode all the stages of the game, (2) have a dense representation of the info state, and (3) a combination of the two. It seems to me that (2) would result in faster convergence but it also seems implausible as the dimensionality would explode pretty quickly so I'm leaning more towards the dense, e.g.:         Do you have any suggestions for the encoding? Is my intuition above the dense representation correct (it's less expressive in a way)? How many dimensions are too many if I were to choose dummy encoding (I realize this is game/algorithm dependent, is there a rule of thumb suggestion that you have observed in the past)?"
Thanks for your attention!  when I run the following code to build from source:  `./open_spiel/scripts/build_and_run_tests.sh`    the following error ocurred:  `CMakeFiles/Makefile2:1022: recipe for target 'CMakeFiles/open_spiel_core.dir/all' failed  make[1]: *** [CMakeFiles/open_spiel_core.dir/all] Error 2  make[1]: *** ....  [ 16%] Linking CXX static library libabsl_malloc_internal.a  [ 16%] Built target absl_malloc_internal  [ 17%] Linking CXX static library libabsl_strings.a  [ 17%] Built target absl_strings  Makefile:113: recipe for target 'all' failed  make: *** [all] Error 2  Exiting virtualenv  `  
"Hi there, thanks for this awesome framework. I am using it for my third year project on a backgammon RL agent.    Please can you help me understand the encoding of the agent actions for backgammon. I can't seem to understand it.  !       Thanks  Robert McVickers"
"Hello,     I am currently writing my master thesis about (scalable) multi-agent systems for large/complex games with hidden information. Such a large game I want to try is Stratego/Barrage Stratego.     My question is: would it be wise with the current state of open_spiel to try to include Stratego/Barrage Stratego or is the game simply not feasible/out of scale?    If it is possible, do you have any recommendations (@michalsustr maybe?)    I know that is not an issue with the current state and more a question about a possible extension. Hope it is okay to open an issue for this.     "
"ASAN reports the following memory leak on the bridge test:     `    Unfortunately the llvm symbolizer seems unable to figure out the intermediate frames, so here is `valgrind`'s output:        Valgrind's output                      It's not clear to me if the issue is in `bridge.cc` in open_spiel (for example, not freeing memory when it should), or upstream in the double_dummy_solver. It could also be that both `valgrind` and ASAN reported false positives, though I'm not sure how likely that is. "
"_This is more of announcement than an issue, so please feel free to close it. I think the folks who work on this repository might be interested, and couldn't think of a better way of communicating. Apologies if this is considered inappropriate._    I've recently created a Vanilla CFR solution for Setback, using a hand-crafted game abstraction. I'm a .NET developer, so the code is written in F#, rather than using open_spiel, but I think it might still be of interest to this group for two reasons:    * The abstract game has a move tree that fits on a single commodity PC (albeit one with at least 16GB of RAM). I think Setback is a great game to study because it has much of the complexity of Bridge, but a much smaller move tree. (IMHO, it's way more fun than Bridge as well.)    * It bootstraps itself by first applying CFR to solve each deal as a standalone game (without regard to score), then applies CFR again using the resulting model to create a stronger, score-aware bidding solution. I believe the result is close to a Nash equilibrium for the overall game. This approach could potentially be applicable to Bridge as well.    The repository is  . Let me know if you have any further interest. Perhaps someone might even want to port the solution to open_spiel. Thanks."
"In      Isn't this wrong?    For example   recursively builds the tree, but it doesn't stop on ""histories"" (better called a world state in this case!) that were already found. It also overwrites the previously stored pointers.    Additionally, `HistoryNode / HistoryTree` seems to be written inefficiently, there are TODOs left. It's quite an old code that doesn't reflect updates to the library I think. As I will be using this code, I can send a backwards-compatible updates?"
Is there a plan to include the ARMAC implentation in open spiel?  
"Hi. My project has changed a bit, I am now targeting tensorflow. I tried to go from your Dockerfile and simply switch out the second stage baseimage and wound up with this:         It seems to build fine, besides tensorboard, which isn't strictly necessary. However when I start the container and try to import pyspiel, I get a missing GLIBC 2.29 error and a segfault when I try to import tensorflow. The tensorflow:latest-gpu image by itself works fine.         Does anyone have a working Python3.6+ docker image supporting TensorflowGPU(nvidia) and openspiel?"
"When I develop code of algorithms and I switch between branches, often many files need to be recompiled due to cache invalidation. This motivated me to try to make a little benchmark and isolating ""core"" games (see below) from all the other games. This makes approximately 2x speed-up in compilation times on my machine (`make all -j 7`).          I expect the gap to grow with more games added to the library. Do you think it would be a valuable change for OpenSpiel to make a distinction between core games and the remaining ones? The important distinction is that core games will be the only ones for which we can always write _algorithm_ tests, for example exploitability calculation of a strategy should be done only on core games, as they will be always compiled by default.    It could work by adding a global variable     which would be ON by default and people who need to recompile the library often can turn it off.    List of the core games, taken mostly  :   "
"Hi, I am trying to setup OpenSpiel on a cluster on which I do not have sudo access. I am following the instructions as given in the   setup for OpenSpiel. While running  , I am getting the following error:    CMake Error: Could not find CMAKE_ROOT !!!  CMake has most likely not been installed correctly.  Modules directory not found in  path/to/env/miniconda3/share/cmake-3.5  CMake Error: Error executing cmake::LoadCache(). Aborting.    Following is the output when I run `cmake --version`:    CMake Error: Could not find CMAKE_ROOT !!!  CMake has most likely not been installed correctly.  Modules directory not found in  path/to/env/miniconda3/share/cmake-3.5  cmake version 3.5.1    Please let me know how to resolve this error. Thank you in advance!"
"Is there a way or already planned development that makes it possible to distinguish between private and public information in games, as outlined, e.g., in   ?    An even more fine-grained model would be ideal for a game like Hanabi, where there is partially shared, but not completely public, information."
"OpenSpiel's default installation using documentation doesn't install the Hanabi game. Running `open_spiel/scripts/regenerate_playthroughs.sh` fails without Hanabi installed. After a quick discussion with @lanctot in the pull request linked below, the goal is to skip  Hanabi in `regenerate_playthroughs.sh` if you choose to install and build OpenSpiel without Hanabi.    _Originally posted in      Console output of the error:   "
"I came across this paper   and I believe it would be nice to have this version of mccfr implemented. I think that's a technique which can boost converge speed, especially for deep games where extensive sampling is slow and outcome sampling has high variances (particularly in the deepest nodes of the tree).     Could it be a good idea to implement it? Any recommendation for that?"
"Is there currently any way to parallelize algorithms such as MCCFR? Currently I run into race conditions if I try to run, for example, the OutcomeSamplingMCCFRSolver RunIterations function from several threads, but I don't know if I'm missing a way to do this currently or if multi threaded support is not implemented yet."
"Hi, there are TODO comments in the policy_gradient.py file who indicate that legal actions still need to be accounted for when computing the losses to update both the critic as well as the actor network.  I am not quite sure if this is even necessary, and if it is, how to do it myself. Would it work if I left the critic update as is, and for the actor update added masked legal actions to all the logits (-infinity when invalid, 0 when valid)?    Any info is much appreciated :)   Edit: adding the mask to the logits seems to work for RPG at least, learning slowly but steady."
I am trying to install basing from other docker containers that provide PyTorch/Tensorflow-GPU respectively. However during build I get this error:         Dockerfile:   
"This is a minor bug.  When I build using arguments build_only, it displays an error at the end.    $./open_spiel/scripts/build_and_run_tests.sh --build_only=true  ...  [ 99%] Built target logger_test  [100%] Built target file_test  [100%] Built target tensor_view_test  ./open_spiel/scripts/build_and_run_tests.sh: line 226: ARG_build_only: command not found  *** Skipping runing tests as build_only is    Exiting virtualenv  $    This is the offending line:  echo -e ""\033[32m*** Skipping runing tests as build_only is $(ARG_build_only) \e[0m""    It should be  echo -e ""\033[32m*** Skipping runing tests as build_only is ${ARG_build_only} \e[0m""    "
"Hi,    I am new to using open_spiel and new to using bridge bots. I have two questions.    [1] I am trying to get a sense of what bride bots I can connect to this framework in order to evaluate my trained model's performance. I understand that this framework supports any program supporting the BlueChip network protocol, but what are some popular bots that support this protocol that I can use? Besides WBridge5 are there any other popular bots that would support this protocol?     [2] Where do I download the WBridge5 bot? I see there test python code for using the bot, but I am having trouble finding where to download the bot and instructions for setting the bot up.          "
"@solinas Seems like there are still some problems with Oh Hell test.     Log here:      (Only failed 1/6 of the tests. We fixed the seed, right?)"
"Steps to repro:  1. Download the repository  2. Run `export BUILD_WITH_ACPC=""ON""`  3. Run `./install.sh`  4. Run `./open_spiel/scripts/build_and_run_tests.sh`    The 4th step yields `CMake generate step failed. Build file cannot be regenerated correctly.` Along with lots of error messages along the lines of       I'm running Mac OS and python 3.7.9.    Any help would be much appreciated!"
"Hi all,     Just a quick heads-up that one of our big priorities is the planning another release, followed by finally getting OpenSpiel into PyPI so it can be installed quickly via pip (the team knows well by now that I love one-line installs!)    I am hoping the release will include binary distributions in shared object form (for MacOS and Linux x86_64), which will mean being able to use OpenSpiel as a library using only its headers via dynamic linking. This will save on compilation time and make it easier to use as a library rather than a framework.    Once this release is done, we'd like to get OpenSpiel into PyPI. This one is a bit trickier since the python API dynamically loads C++-compiled code as a Python extension. We will need to be sure we can support a wide enough set of platforms and python versions in order to make it widely accessible, and a good testing framework to ensure that it's stable. Still, I think it will be worth the effort. If anybody has experience doing this, please by all means let us know.     Ideally the pair would arrive as a Christmas gift, but I think it is slightly optimistic to have both done by then. I think getting the release out by then is attainable... PyPI might take a bit longer, but we'll try for both.  Looking forward to closing this especially tough year with some good news, and will keep this thread for updates and discussion. Stay tuned!"
"During fresh install of openspiel, pip install of requirements fails on MacOS.    $ python3 -m pip install .  Processing /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel  Requirement already satisfied: pip>=20.0.2 in ./venv/lib/python3.8/site-packages (from pyspiel==0.0.1rc2) (20.2.3)  Collecting absl-py==0.9.0    Using cached absl-py-0.9.0.tar.gz (104 kB)  ERROR: Could not find a version that satisfies the requirement tensorflow==2.2.1 (from pyspiel==0.0.1rc2) (from versions: 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0)  ERROR: No matching distribution found for tensorflow==2.2.1 (from pyspiel==0.0.1rc2)      $ pip3 --version  pip 20.2.3 from ~/OrbitalVelocity/ai/open_spiel/venv/lib/python3.8/site-packages/pip (python 3.8)"
"In a fresh new install, I get an error on make. This is on MacOS.    $ make -j$(nproc)  [  0%] Built target absl_spinlock_wait  [  0%] Built target absl_log_severity  [  0%] Built target absl_exponential_biased  [  0%] Building CXX object CMakeFiles/open_spiel_core.dir/fog/observation_history.cc.o  [  0%] Building CXX object CMakeFiles/open_spiel_core.dir/game_parameters.cc.o  [  0%] Building CXX object CMakeFiles/open_spiel_core.dir/matrix_game.cc.o  [  1%] Built target absl_dynamic_annotations  [  1%] Building CXX object CMakeFiles/open_spiel_core.dir/observer.cc.o  In file included from /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/game_parameters.cc:15:  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:133:7: error: explicit specialization of 'value' in class scope    int value() const {        ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:137:10: error: explicit specialization of 'value' in class scope    double value() const {           ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:141:22: error: explicit specialization of 'value' in class scope    const std::string& value() const {                       ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:145:15: error: explicit specialization of 'value' in class scope    std::string value() const {                ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:149:8: error: explicit specialization of 'value' in class scope    bool value() const {         ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:153:47: error: explicit specialization of 'value' in class scope    const std::map & value() const {                                                ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:157:40: error: explicit specialization of 'value' in class scope    std::map  value() const {                                         ^  In file included from /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/fog/observation_history.cc:15:  In file included from /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/fog/observation_history.h:24:  In file included from /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/spiel.h:36:  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:133:7: error: explicit specialization of 'value' in class scope    int value() const {        ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:137:10: error: explicit specialization of 'value' in class scope    double value() const {           ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:141:22: error: explicit specialization of 'value' in class scope    const std::string& value() const {                       ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:145:15: error: explicit specialization of 'value' in class scope    std::string value() const {                ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:149:8: error: explicit specialization of 'value' in class scope    bool value() const {         ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:153:47: error: explicit specialization of 'value' in class scope    const std::map & value() const {                                                ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:157:40: error: explicit specialization of 'value' in class scope    std::map  value() const {                                         ^  In file included from /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/matrix_game.cc:15:  In file included from /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/matrix_game.h:25:  In file included from /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/normal_form_game.h:24:  In file included from /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/simultaneous_move_game.h:22:  In file included from /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/spiel.h:36:  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:133:7: error: explicit specialization of 'value' in class scope    int value() const {        ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:137:10: error: explicit specialization of 'value' in class scope    double value() const {           ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:141:22: error: explicit specialization of 'value' in class scope    const std::string& value() const {                       ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:145:15: error: explicit specialization of 'value' in class scope    std::string value() const {                ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:149:8: error: explicit specialization of 'value' in class scope    bool value() const {         ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:153:47: error: explicit specialization of 'value' in class scope    const std::map & value() const {                                                ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:157:40: error: explicit specialization of 'value' in class scope    std::map  value() const {                                         ^  In file included from /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/observer.cc:22:  In file included from /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/spiel.h:36:  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:133:7: error: explicit specialization of 'value' in class scope    int value() const {        ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:137:10: error: explicit specialization of 'value' in class scope    double value() const {           ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:141:22: error: explicit specialization of 'value' in class scope    const std::string& value() const {                       ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:145:15: error: explicit specialization of 'value' in class scope    std::string value() const {                ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:149:8: error: explicit specialization of 'value' in class scope    bool value() const {         ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:153:47: error: explicit specialization of 'value' in class scope    const std::map & value() const {                                                ^  /Users/swetha/Desktop/OrbitalVelocity/ai/open_spiel/open_spiel/../open_spiel/game_parameters.h:157:40: error: explicit specialization of 'value' in class scope    std::map  value() const {                                         ^  7 errors generated.  make[2]: *** [CMakeFiles/open_spiel_core.dir/game_parameters.cc.o] Error 1  make[2]: *** Waiting for unfinished jobs....  [  3%] Built target absl_time_zone  [  3%] Built target absl_int128  [  4%] Built target absl_civil_time  Scanning dependencies of target absl_leak_check_disable  [  4%] Building CXX object abseil-cpp/absl/debugging/CMakeFiles/absl_leak_check_disable.dir/leak_check_disable.cc.o  [  4%] Linking CXX static library libabsl_leak_check_disable.a  7 errors generated.  make[2]: *** [CMakeFiles/open_spiel_core.dir/fog/observation_history.cc.o] Error 1  7 errors generated.  make[2]: *** [CMakeFiles/open_spiel_core.dir/matrix_game.cc.o] Error 1  7 errors generated.  Scanning dependencies of target absl_leak_check  make[2]: *** [CMakeFiles/open_spiel_core.dir/observer.cc.o] Error 1  make[1]: *** [CMakeFiles/open_spiel_core.dir/all] Error 2  make[1]: *** Waiting for unfinished jobs....  Scanning dependencies of target absl_city  [  4%] Built target absl_leak_check_disable  [  4%] Building CXX object abseil-cpp/absl/debugging/CMakeFiles/absl_leak_check.dir/leak_check.cc.o  [  4%] Building CXX object abseil-cpp/absl/hash/CMakeFiles/absl_city.dir/internal/city.cc.o  [  5%] Linking CXX static library libabsl_leak_check.a  [  5%] Built target absl_leak_check  [  5%] Linking CXX static library libabsl_city.a  [  5%] Built target absl_city  make: *** [all] Error 2  "
"Just a heads up that this is a known issue. We plan to look into it soon (fix it, or disable it from CMake tests).  "
"See discussion in       Basically, we would like it if someone copied the implementation of   but made two changes:     1) Use `State::FullHistory` to check for equality, rather than `State::ToString`  2) Return a `std::vector >` rather than a `std::map >`."
Can someone tell me what the problem is? I am using clang-8 as default and gcc/g++ 7.5.  !   Thank you
"Hit as part of #376 .    +@lanctot +@michalsustr +@finbarrtimbers    The implementation of `GetAllStates` expects that `state->ToString()` be unique for states, which is not true. For example, in the Battleship game, `ToString()` returns an ascii art representation of the player boards. In Go, my understanding is that `ToString()` renders the Go board (**edit** see also   discussion). Neither of these mappings are injective: different orders of moves can result in the same boards, and therefore the same `ToString()`.         This results in the method failing to explore the full game tree. Since the method is performing a DFS on a tree, it is impossible to hit the same state twice, so I am not sure what the comment `add only if not already present` was supposed to catch. I suggest the following:  - If we want to keep the map, we should have histories as key.  - There is no need to keep an expensive map: a vector of (unique) pointers to states would be more efficient and simplify the code even further. I'm not sure how breaking a change that would be though?   "
"Hi @andrschl, @michalsustr, @gabrfarina,     I've recently been working on LP-solving from C++, using  . I have a simple example that solves a matrix game (not yet released). It works like a charm internally, but I am having trouble externally. I managed to get it to compile and link against the binary distribution but it crashes when I run it, and in so doing I discovered the wonderful world of ABI incompatibilities between C++ standards, so that's not an option. When I try building from source I get a bunch of errors with cmake (see below: OR-Tools's CMake setup is marked as experimental, so this could just be something that is not supported yet).    I will include the code in the next update including my attempt to get it to work with CMake if you're curious. But, this has led me to consider other LP solvers. OR-Tools is very appealing because it checks off all the boxes: works internally and externally, and problems can be solved using some of the large commercial solvers. But it a lot of dependencies, so building from scratch could take some time and long-term could be hard to maintain. I've used GLPK in the past, and it worked quite well, but building the LP code on GLPK would mean it can't transfer seamlessly to e.g. CPLEX or Gurobi.    So I'm a bit torn.. I still want to try to get OR-Tools working, especially because they support a CMake build (!) which is great for us, but what I know for sure is that I can't spend much more time on this at the moment. I'm wondering what you have used in the past to do your LP solving, and was it from C++? Any recommendations on where to go from here?     Here's the promised CMake error message:         "
"Hi @solinas ,    Seems like the Oh Hell test occasionally has trouble on Travis, printing out a lot of output and then the job fails. Here's an example:   . It fills Travis's output buffer.    I wonder if it's hitting a case where there is a never-ending episode? If we're not using a fixed seed in the test then we should change that, because it seems to be non-deterministic.    Also, if there are sequences that could result in no terminal, maybe we should end the episode after a fixed number of turns? (I'm not saying you didn't do this -- haven't looked at the code)    Any ideas?"
"I was wondering if you planned to develop the external sampling version of MCCFR in Python.     I noticed that the outcome sampling version is already in place (`outcome_sampling_mccfr.py`) and that it is possible to call the external sampling version by using the C++ version (`mccfr_cpp_example.py`), but for me it would be useful to also have the direct implementation in Python.    Are you open to contributions for this task?    "
"I am trying to open the examples with a vscode debugger under Linux to get a better understanding what is happening in there.    I could only figure out one way to change and build those which is to run open_spiel/scripts/build_and_run_tests.sh again after editing. When I change BUILD_TYPE in CMakeList.txt to Debug I get those symbols supposedly. I do not get vscode to find the correct source file for that executable or something either way the graphical debugger does not work.    I get the debugger to work with a simple 5 line test source file that I edited and compiled myself in the examples directory.    How can I build a source file that uses the open_spiel libraries? I tried this:    g++ cfr_example.cc -I../ -I../../ -I../abseil-cpp/ -std=c++17 -g    I get this:  In file included from ../../open_spiel/spiel.h:36,                   from ../../open_spiel/policy.h:25,                   from ../../open_spiel/games/universal_poker.h:28,                   from main.cc:4:  ../../open_spiel/game_parameters.h:132:13: error: explicit specialization in non-namespace scope ‘class open_spiel::GameParameter’    132 |   template          |             ^  ../../open_spiel/game_parameters.h:136:13: error: explicit specialization in non-namespace scope ‘class open_spiel::GameParameter’    136 |   template    ...    Sorry if this is very basic. The code is incredibly well commented and very accessible. I spent two days trying to figure out this building stuff though.      "
"In UniversalPokerGame, there are a bunch of problems with the action encoding. We've hardcoded the assumption that the bet size is a multiple of the big blind, which is wrong. As such, we've disabled it currently.    There's a stub implementation right now that uses `std::iota`; however, it doesn't work, as it will suggest illegal actions that cause the underlying ACPC game to fail.  "
Hope asking this via issue is ok as it is only very implicitly related to the implementation in OpenSpiel. I'm reading    -    -  
"Currently `Bot` has functions like `std::pair  StepWithPolicy(const State& state)` which will block until the bot computes and returns its policy. For the purposes of, I don't know, timed competitions :) it would be nice to have a method that can update the policy continuously in a separate thread:       Initially the policy is uniformly random and the `Bot` can change it by writing to the pointer. There will be need to figure out how to do it in thread-safe way.    What do you think?    "
"Hit as part of implementing the Battleship game (#376).    `DeserializeGame` fails to deserialize games that have (at least) one double game parameter whose value corresponds to an integer. While it's easy to trigger this failure by hand, it is important to note that it will also be triggered automatically by `TestSerializeDeserialize`, which automatically tries to serialize and then deserialize a game --- in fact, that's how I hit the issue.    The issue is due to the fact that the parser for game parameters (snippet below) is *too eager* to match integers: any sequence composed uniquely of characters ""+-0123456789"" is immediately parsed as an integer.       Unfortunately, it is not clear to me how to fix this issue:  * Asking that all double values always include either 'e' or a period '.' is very non-user-friendly, and would still require changes in the serialization code, which currently serializes a double like ""2.0"" to just ""2"". I think that imposing a restriction on how doubles are serialized is going to impose unnecessary burden to users.  * On the other hand, it seems like the parsing of game parameters is not done taking into account the desired resulting types. The type of the game parameters is determined lexically at game deserialization time, and the deserialized parameters are them matched against the expected schema by the `ValidateParams` method.    I can see two ways to fix the issue:  1. Either have `DeserializeGame` and `GameParameterFromString` have access to information about the schema of game parameters they should parse. This seems the cleanest solution, but would require some heavy reworking of some internal pipelines from what I can see.  2. Have `ValidateParams` include some failure recovery strategy that will promote integers to doubles instead of failing. This has the advantage that the fix would be contained to a single method. But it seems very hacky: for example, if a string parameter has the misfortune of being the string `""true""`, the parameter would currently be parsed as `bool`, even if the schema says that it should be string.    I think as a first step towards solving this, the maintainers should agree on what solution strategy to adopt. I am happy to help with whatever you might determine is best.    This issue is currently a blocker for the implementation of the Battleship game, which includes a double game parameter whose value will often be integer --- as is 2.0 by default."
"While implementing the Battleship game (#376) I noticed that the following assertion fails, contrary to what I was expecting:         On the other hand, this passes:       I think this is a serious papercut, due to quirks in the C++ ISO standard (see §13.3.3.1.1 and especially  ).  I'm happy to open a pull request addressing this. "
"Hello OpenSpiel users,     Are you working on RL & Games? You might be interested in the AAAI-RLG workshop. Here is a Call for Participation below, maybe you can consider submitting your work to our workshop :)      **Submission Deadline: November 9, 2020**    Website:       Games provide an abstract and formal model of environments in which multiple agents interact: each player has a well-defined goal and rules to describe the effects of interactions among the players. The first achievements in playing these games at super-human level were attained with methods that relied on and exploited domain expertise that was designed manually (e.g. chess, checkers). In recent years, we have seen examples of general approaches that learn to play these games via self-play reinforcement learning (RL), as first demonstrated in Backgammon. While progress has been impressive, we believe we have just scratched the surface of what is capable, and much work remains to be done in order to truly understand the algorithms and learning processes within these environments.    The main objective of the workshop is to bring researchers together to discuss ideas, preliminary results, and ongoing research in the field of reinforcement in games.    We invite participants to submit papers on the 9th of November, based on but not limited to, the following topics:     - RL in various formalisms: one-shot games, turn-based, and Markov games, partially-observable games, continuous games, cooperative games  - Deep RL in games  - Combining search and RL in games  - Inverse RL in games  - Foundations, theory, and game-theoretic algorithms for RL  - Opponent modeling  - Analyses of learning dynamics in games  - Evolutionary methods for RL in games  - RL in games without the rules  - Monte Carlo tree search  - Online learning in games.    ### Format of workshop    RLG is a 1 full-day workshop. It will start a 60 minute mini-tutorial covering a brief tutorial and basics of RL in games, 2-3 invited talks by prominent contributors to the field, paper presentations, a poster session, and will close with a discussion panel.    ### Submission requirements    Papers must be between 4-8 pages in the AAAI submission format, with the eighth page containing only references. Papers will be submitted electronically using Easychair. Accepted papers will not be archival, and we explicitly allow papers that are concurrently submitted to, currently under review at, or recently accepted in other conferences / venues.    Please submit your paper using the submission link on the web site.    Workshop Chair: Martin Schmid (DeepMind)    Workshop committee: Marc Lanctot (DeepMind), Julien Perolat (DeepMind), Martin Schmid (DeepMind)  "
"Dear DeepMind team,    I'm working on exercises from   book and to make things more fun and learn something new I implemented the games and algorithms in OpenSpiel `GameProtocol`.    I hit a two small bumps along the road, so I figured it would be nice to fix the ""virtual potholes"" and pay the community back.    1)   struct doesn't have a _public_ initializer. Swift by default synthesizes a member-wise `init(...)` for every struct but the access level is only _internal_. This internal initializer is called by all Swift games implemented here in the repo.       The problem happens when you try to implement a game outside of the `OpenSpiel` module in a separate package that only depends on `OpenSpiel` to import the `GameProtocol`. Not having the initializer makes implementing the `GameProtocol` by third party packages more complicated.       I don't think this is intentional. I think there should be a public member-wise constructor but I'm not 100% sure.    2) Building with latest (0.11)   toolchain on macOS fails with the following error:           I   and it looks like the S4TF toolchain indeed requires at least macOS 10.13. Maybe there is a way to enforce this on the command line but the most fool-proof fix seems to be adding `platforms` section to the `Package.swift` manifest. This will make plain `swift build` work again.           The change sounds simple enough but it opens a whole can of worms - Swift version upgrade. The problem is that the `platforms` argument was introduced in Swift 5. So making the updated`Package.swift` work requires lifting the minimal Swift tools version requirement.       The decision whether this is okay or not is way above my pay grade of an anonymous volunteer 😉. I've noticed that there's already a `v0.1.0` release. So maybe people that use older Swift versions could still use `v0.1.0`.  And version `v0.2.0` could be Swift 5? I think not having the ability to build with the S4TF toolchain is a big enough bummer to warrant an upgrade.    3) Would you be interested in having games from the exercises in   merged here? I think it could be of some use when one develops and tests a new algorithm. The exercises have well tested solutions so bugs have a better chance of being spotted.    ---    I already implemented fixes to 1. and 2. in  . Here's   for easier review.    I'll be happy to create a PR with the fixes but I figured it would be better to ask and discuss it first.    Cheers,      Vojta"
"It would be great if we could have a bot API similar to the one for games, such as        We could also add the following method to the `Bot` interface:       The bots right now are disorganized -- Roshambo is in `bots/`, but xinxin is illogically in `games/hearts/`. I can put it together more nicely. This will also provide a structure for adding other external bots in the future, like Stockfish. What do you think?"
"Hi, I was trying to install open_spiel in my server. And when I do the step of ""ctest -j$(nproc)"", it said ""4% tests passed, 159 tests failed out of 166"". I am using python3.6.9."
"Hi, I have followed the instructions to copy aa existed game to a new game but when I verified it before modifying, the error message said the newgame i copied is not registered."
"Solitaire was recently merged, but was not added to the games list. It should be added to the games list."
"Hello,  When I follow the Installation Instructions, I faced the following error in ""make"" period.    Env: Ubuntu 18.04.4 LTS   Python 3.7.7  GNU 7.3.0  Clang 9.0    Error:  /anaconda/envs/py37_default/x86_64-conda_cos6-linux-gnu/sysroot/usr/lib/librt.so: undefined reference to `__vdso_clock_gettime@GLIBC_PRIVATE'  clang: error: linker command failed with exit code 1 (use -v to see invocation)  examples/CMakeFiles/matrix_example.dir/build.make:154: recipe for target 'examples/matrix_example' failed  make[2]: *** [examples/matrix_example] Error 1  CMakeFiles/Makefile2:5161: recipe for target 'examples/CMakeFiles/matrix_example.dir/all' failed  make[1]: *** [examples/CMakeFiles/matrix_example.dir/all] Error 2  make[1]: *** Waiting for unfinished jobs....      I tried to add “set(CMAKE_EXE_LINKER_FLAGS ""${CMAKE_EXE_LINKER_FLAGS} -lrt -lpthread -lresolv"")” to CMakeLists.txt, and add ""-lrt -lpthread -lresolv"" to link.txt file manually. But Both of them failed.    I would appreciate it if you could reply."
"The Observer's WriteTensor implementations contain boilerplate code for encoding one-hot vector representations. Instead of looping arrays and setting indices to 1 at some places, `DimensionedSpan` could have new methods to accept, check and appropriately copy vectors from which these observations are made. Wha do you think?"
"At some point in the future I'd like to make a neat interface to render `Observer`'s tensor output. While we know the shape sizes, I thought it would be neat to optionally label the individual dimensions, so it is cleared what each represents. What do you think?"
"The observations in Goofspiel are player relative, i.e. the requested player's observation will be at index 0, and the remaining players will be ""modulo wrapped around"", as in change:    Add ObservationString, ObservationTensor, and ObservationTensorShape to Goofspiel and make both tensors observing player-relative  PiperOrigin-RevId: 283363694 Change-Id: I205363c9d641a574e2018cb9312672c8b15e8d9f    I can't think of a justification for this. What is the reason?"
"Now that we have an `Observer` and all `State::*String/Tensor` methods are forwarded to `Observer,` custom `State::*TensorShape` seem redundant. @elkhrt is the plan to get rid of them as well? I think it could be implemented in `Game` as a call to `Observation::tensor_info()` (why the snake_case btw?), something like this:       where `ComputeShape` will return flat shape if there are multiple tensors, and specific shape if there is only one tensor."
"FYI, I'm going to implement Base to public state API transformation."
FYI I'm currently implementing public observations for Goofspiel.
"Hi @elkhrt , what is the use case for `GameParameters params` in `Game::MakeObserver`?      AFAIK it is currently not used, and these parameters should be accessible via Game::GetParameters(). I think this implies the observer would fill tensors for a different game than is actually played."
"Hello, as a small side-kick I made  . I finally had a free weekend when I could put final things together. Enjoy!    It is based on another open source project which uses LGPL-3, so it cannot be integrated into OpenSpiel. However, if you feel like putting a link to it, I would be happy. :) In the docs there is a mention of publishing pyspiel as a pip package. What is the status on this, since there is an 0.1.0 version out there?"
"Hi everyone!    I followed the installation instructions on Windows.  My current specifics are Windows 10 Version 2004 Build 19041.450, running WSL 2 and then Ubuntu 20.04.1 LTS    In the installtion instructions I skipped the part of updating make, because it said it's only necessary with Ubuntu Version 18.04 and I already have 20.04.  Besides omitting this step I followed all instructions. My errors arise when I type the 'make -j$' command.  The output is:    In file included from /home/seresheim/open_spiel/open_spiel/../open_spiel/spiel.h:36,                   from /home/seresheim/open_spiel/open_spiel/spiel.cc:15:  /home/seresheim/open_spiel/open_spiel/../open_spiel/game_parameters.h:116:13: error: explicit specialization in non-namespace scope ‘class open_spiel::GameParameter’    116 |   template          |                        ^  /home/seresheim/open_spiel/open_spiel/../open_spiel/game_parameters.h:120:13: error: explicit specialization in non-namespace scope ‘class open_spiel::GameParameter’    120 |   template          |                        ^  /home/seresheim/open_spiel/open_spiel/../open_spiel/game_parameters.h:121:10: error: ‘double open_spiel::GameParameter::value() const’ cannot be overloaded with ‘int open_spiel::GameParameter::value() const’    121 |   double value() const {        |          ^~~~~  /home/seresheim/open_spiel/open_spiel/../open_spiel/game_parameters.h:117:7: note: previous declaration ‘int open_spiel::GameParameter::value() const’    117 |   int value() const {        |            ^~~~~    This is just part of the output, but it repeats for several other files and functions.  (On the console '>' and 'value()' is highlighted in red)    As far as I understand, this has something to do with c++, but I'm not that fluent in it.    Can somebody guide mit in the right direction on how to fix this?    Kind regards"
how to compile it as static lib?
"Hello，  When I follow the detailed steps, some errors occured. When I run 'make -j$', the following error occurred.    My environment：  ubuntu 16.04.12  gcc 5.4.0  python 3.6.5    Error message:  In file included from /home/wuqi/wt/open_spiel/open_spiel/../open_spiel/game_parameters.h:23:0,                   from /home/wuqi/wt/open_spiel/open_spiel/../open_spiel/spiel.h:36,                   from /home/wuqi/wt/open_spiel/open_spiel/../open_spiel/policy.h:25,                   from /home/wuqi/wt/open_spiel/open_spiel/../open_spiel/spiel_bots.h:22,                   from /home/wuqi/wt/open_spiel/open_spiel/spiel_bots.cc:15:  /home/wuqi/wt/open_spiel/open_spiel/../open_spiel/spiel_utils.h: In instantiation of ‘void open_spiel::internal::SpielStrOut(Out&, const T&, Args&& ...) [with Out = std::__cxx11::basic_ostringstream ; T = int; Args = {const char (&)[2], const char (&)[57], const char (&)[28], const char (&)[4], std::vector  >&, const char (&)[32], std::vector  >&}]’:  /home/wuqi/wt/open_spiel/open_spiel/../open_spiel/spiel_utils.h:119:14:   recursively required from ‘void open_spiel::internal::SpielStrOut(Out&, const T&, Args&& ...) [with Out = std::__cxx11::basic_ostringstream ; T = char [2]; Args = {int, const char (&)[2], const char (&)[57], const char (&)[28], const char (&)[4], std::vector  >&, const char (&)[32], std::vector  >&}]’  /home/wuqi/wt/open_spiel/open_spiel/../open_spiel/spiel_utils.h:119:14:   required from ‘void open_spiel::internal::SpielStrOut(Out&, const T&, Args&& ...) [with Out = std::__cxx11::basic_ostringstream ; T = char [50]; Args = {const char (&)[2], int, const char (&)[2], const char (&)[57], const char (&)[28], const char (&)[4], std::vector  >&, const char (&)[32], std::vector  >&}]’  /home/wuqi/wt/open_spiel/open_spiel/../open_spiel/spiel_utils.h:133:14:   required from ‘std::__cxx11::string open_spiel::internal::SpielStrCat(Args&& ...) [with Args = {const char (&)[50], const char (&)[2], int, const char (&)[2], const char (&)[57], const char (&)[28], const char (&)[4], std::vector  >&, const char (&)[32], std::vector  >&}; std::__cxx11::string = std::__cxx11::basic_string ]’  /home/wuqi/wt/open_spiel/open_spiel/spiel_bots.cc:98:7:   required from here  /home/wuqi/wt/open_spiel/open_spiel/../open_spiel/spiel_utils.h:118:7: error: ambiguous overload for ‘operator ’ and ‘const int’)     out  & std::operator &, unsigned char) [with _Traits = std::char_traits ]       operator & __out, unsigned char __c)       ^  /usr/include/c++/5/ostream:514:5: note: candidate: std::basic_ostream & std::operator &, signed char) [with _Traits = std::char_traits ]       operator & __out, signed char __c)       ^  /usr/include/c++/5/ostream:508:5: note: candidate: std::basic_ostream & std::operator &, char) [with _Traits = std::char_traits ]       operator & __out, char __c)       ^  /usr/include/c++/5/ostream:502:5: note: candidate: std::basic_ostream & std::operator &, char) [with _CharT = char; _Traits = std::char_traits ]       operator & __out, char __c)       ^  In file included from /home/wuqi/wt/open_spiel/open_spiel/../open_spiel/game_parameters.h:23:0,                   from /home/wuqi/wt/open_spiel/open_spiel/../open_spiel/spiel.h:36,                   from /home/wuqi/wt/open_spiel/open_spiel/spiel.cc:15:  /home/wuqi/wt/open_spiel/open_spiel/../open_spiel/spiel_utils.h:117:6: error: ‘void open_spiel::internal::SpielStrOut(Out&, const T&, Args&& ...) [with Out = std::__cxx11::basic_ostringstream ; T = char [2]; Args = {const char (&)[24], const char (&)[13], const char (&)[4], open_spiel::DeserializeGameAndState(const string&)::Section&, const char (&)[14], open_spiel::DeserializeGameAndState(const string&)::Section&}]’, declared using local type ‘open_spiel::DeserializeGameAndState(const string&)::Section’, is used but never defined [-fpermissive]   void SpielStrOut(Out& out, const T& arg1, Args&&... args) {        ^  /home/wuqi/wt/open_spiel/open_spiel/../open_spiel/spiel_utils.h:117:6: error: ‘void open_spiel::internal::SpielStrOut(Out&, const T&, Args&& ...) [with Out = std::__cxx11::basic_ostringstream ; T = char [2]; Args = {const char (&)[21], const char (&)[13], const char (&)[4], open_spiel::DeserializeGameAndState(const string&)::Section&, const char (&)[11], open_spiel::DeserializeGameAndState(const string&)::Section&}]’, declared using local type ‘open_spiel::DeserializeGameAndState(const string&)::Section’, is used but never defined [-fpermissive]  [ 31%] Linking CXX static library libabsl_random_seed_sequences.a  [ 31%] Built target absl_random_seed_sequences  At global scope:  cc1plus: warning: unrecognized command line option ‘-Wno-everything’  CMakeFiles/open_spiel_core.dir/build.make:146: recipe for target 'CMakeFiles/open_spiel_core.dir/policy.cc.o' failed  make[2]: *** [CMakeFiles/open_spiel_core.dir/policy.cc.o] Error 1  cc1plus: warning: unrecognized command line option ‘-Wno-everything’  At global scope:  cc1plus: warning: unrecognized command line option ‘-Wno-everything’  CMakeFiles/open_spiel_core.dir/build.make:172: recipe for target 'CMakeFiles/open_spiel_core.dir/spiel.cc.o' failed  make[2]: *** [CMakeFiles/open_spiel_core.dir/spiel.cc.o] Error 1  CMakeFiles/open_spiel_core.dir/build.make:185: recipe for target 'CMakeFiles/open_spiel_core.dir/spiel_bots.cc.o' failed  make[2]: *** [CMakeFiles/open_spiel_core.dir/spiel_bots.cc.o] Error 1  CMakeFiles/Makefile2:971: recipe for target 'CMakeFiles/open_spiel_core.dir/all' failed  make[1]: *** [CMakeFiles/open_spiel_core.dir/all] Error 2  make[1]: *** Waiting for unfinished jobs....  [ 31%] Linking CXX static library libabsl_flags_config.a  [ 31%] Built target absl_flags_config  [ 31%] Linking CXX static library libabsl_raw_hash_set.a  [ 31%] Built target absl_raw_hash_set  Makefile:113: recipe for target 'all' failed  make: *** [all] Error 2    I would appreciate it if you could reply.    "
"Hi there,    I am trying to set up Open Spiel on a cluster in which I don't have sudo access following the installation instructions specified in      The first step requires running `install.sh` which tries to install the external dependencies specified in   through `apt-get` in line 212.    I am wondering how can I find alternative ways to install the specific version of these dependencies in a conda environment. Thank you so much in advance for your help.      "
Host OS:    - macOS Catalina 10.15.6  - Docker 19.03.12    Run command:       Error message:   
"Hello,    The ""Build Open Spiel"" sub-section inside the Colab fails since some time with the below error:            Do you have suggestions on how to fix this? "
"I've been trying to perform a parameter sweep for AlphaZero in Connect Four and added some loops to open_spiel/python/examples/alpha_zero.py to do so. However, once AlphaZero is finished training with the first set of parameters, the program stops. I've spent today debugging and it seems like the actor and evaluator processes aren't joining properly. I get the following logging:    learner exiting  actor-0 exiting  .  .  .  evaluator-0 exiting    However, when I add a print statement at the end of alpha_zero() in open_spiel/python/algorithms/alpha_zero/alpha_zero.py:         ""Joined"" is not printed.    I read the multiprocessing documentation:   and found the following:    > As mentioned above, if a child process has put items on a queue (and it has not used JoinableQueue.cancel_join_thread), then that process will not terminate until all buffered items have been flushed to the pipe. This means that if you try joining that process you may get a deadlock unless you are sure that all items which have been put on the queue have been consumed. Similarly, if the child process is non-daemonic then the parent process may hang on exit when it tries to join all its non-daemonic children.    With this in mind, I tried emptying q_in and q_out in spawn.py using get() before join() was called and also tried using Queue.cancel_join_thread() but wasn't able to fix the problem. Do you have any suggestions on how I can run my parameter sweep without running into this Process joining issue?"
"I was working with `open_spiel/open_spiel/python/algorithms/neurd.py` and was trying implement thresholding part in my version of Gradient Update. However it seems like the Remark in the NRD paper about the gradient clipping and it's implementation in OpenSpiel don’t quite match up.  In particular the latter just checks whether the logit is already too large and then zeros out the gradient if it is, where the former does a more complicated check (which maybe requires an additional forward pass? since it used the `θ' = θ + η∇z(θ) and calculates z(θ') ∈ [−β, β)` ) about whether the updated parameters would have resulted in a logic * regret that is too large.      There seem to be several differences between the two – whether to check for threshold before / after taking the update into account and whether z = logit or z = logit*regret, and why the paper describes this as bound the logit-gap when each logit seems to be clipped in isolation.  It also isn’t clear whether the code implementation matches what you described about this for the paper.  "
For example:  In file `open_spiel/python/examples/tic_tac_toe_dqn_vs_tabular.py`  I would like to save trained agents:      How to save dqn_agent?
"This might be a minor issue. I found in  , the L2-projection operator is implemented as        I think this is not quite aligned with the  , where the result is supposed to be the closest point in L2 distance on the simplex to updated_strategy. Just by replacing values less than gamma with gamma and renormalizing it doesn't feel right.    Here is my implementation based on  :     In practice I doubt it may differ a little in terms of performance, but hope it is helpful."
"Dear authors,    We met an installing issue when building Openspiel from source. We first created an environment and then run the following commands in the Openspiel directory.    mkdir build  cd build  CC=/sw/arcts/centos7/gcc/9.2.0/bin/gcc CXX=/sw/arcts/centos7/gcc/9.2.0/bin/g++ cmake -DPython_TARGET_VERSION=3.6 -DCMAKE_CXX_COMPILER=/sw/arcts/centos7/gcc/9.2.0/bin/g++ ../open_spiel  make    The version and position of gcc and g++ are specified (cmake/3.13.2 and gcc/9.2.0). We are working on CentOS. When we ""make"", we met the following error:    Scanning dependencies of target open_spiel_core  [ 1%] Building CXX object CMakeFiles/open_spiel_core.dir/canonical_game_strings.cc.o  [ 1%] Building CXX object CMakeFiles/open_spiel_core.dir/fog/observation_history.cc.o  In file included from /home/wangyzh/open_spiel/open_spiel/fog/observation_history.cc:15:  /home/wangyzh/open_spiel/open_spiel/../open_spiel/fog/observation_history.h:48:10: error: declaration of ‘open_spiel::Action open_spiel::ActionOrObs::Action() const’ changes meaning of ‘Action’ [-fpermissive]    48 |  Action Action() const {     |     ^~~~~~  In file included from /home/wangyzh/open_spiel/open_spiel/../open_spiel/spiel_globals.h:18,           from /home/wangyzh/open_spiel/open_spiel/../open_spiel/fog/observation_history.h:23,           from /home/wangyzh/open_spiel/open_spiel/fog/observation_history.cc:15:  /home/wangyzh/open_spiel/open_spiel/../open_spiel/spiel_utils.h:112:7: note: ‘Action’ declared here as ‘using Action = int64_t’   112 | using Action = int64_t;     |    ^~~~~~  cc1plus: warning: unrecognized command line option ‘-Wno-everything’  make[2]: *** [CMakeFiles/open_spiel_core.dir/fog/observation_history.cc.o] Error 1  make[1]: *** [CMakeFiles/open_spiel_core.dir/all] Error 2  make: *** [all] Error 2     We skipped compiling the fog directory by removing fog files from CMakeLists.txt under the open_spiel directory. The ""make"" succeeded but we noticed that the pyspiel was not installed and tests files related to python packages were not run. So we were wondering how to fix this. Thank you so much!"
"What should I do about `REGISTER_SPIEL_GAME` and `GameType` in parameterised games, where different `GameParameter` selections should lead to different values for some of the `GameType` variables?    For context: I'm thinking here of (finally) continuing the  . Ludii has hundreds of games implemented. My thinking was, OpenSpiel would probably just have a single ""LudiiGame"" implementation, with at least one mandatory `GameParameter` of type string that would contain the name of the specific game to be loaded from Ludii (e.g. `Tic-Tac-Toe.lud` to load Ludii's version of Tic-Tac-Toe). Many games in Ludii themselves also have parameters to tweak aspects like board size, player number, sometimes rules, etc., which could all be further customised through additional `GameParameters` from OpenSpiel... but I'm mainly worried just about the first and biggest one here; the game name.    If we only have a single LudiiGame, I assume that I can only register it once through `REGISTER_SPIEL_GAME`, and provide only one `GameType`. Here I could specify only a single `GameType::Dynamics` value, a single `GameType::ChanceMode` value, etc. So which should I pick here? I assume the most ""general"" ones, which would mean that OpenSpiel thinks that even Ludii's implementation of Tic-Tac-Toe is a stochastic, simultaneous-move game with imperfect information for 16 players. How bad would this be? Would there be a better solution?"
"hello, im trying to understand how to modify an existing game, but from the spiel.h i cant figure out how many functions works and the requirement for the different gameType (whats is needed for a kSimultaneous vs kSequential for example).  could you provide a simple guide at least to explain in detail how to create a game?      from spiel.h:       // Which type of information state representations are supported?    // The information state is a perfect-recall state-of-the-game from the    // perspective of one player.    bool provides_information_state_string;    bool provides_information_state_tensor;    the comments above do not explain in detail whats the meaning of each xString and xTensor, but it gives only a general explanation of the ""provides_information_"""
"Hi @yarncraft, we now support Ubuntu 20.04. Would you like to add it back to the Dockerfile and run the tests to ensure that it works?"
"I get a build error on my machine, I updated cmake and clang thinking they might be the issue, but apparently not.    Any idea what might be causing this?        "
"Hi @findmyway I am having a problem in the Julia tests on Travis that I cannot reproduce locally, details here:      Any ideas?     "
"As far as I can tell, in most of the cases where neural nets are used as part of a learning algorithm, the structure of the neural network is basically specified by the algorithm class, with only basic tunable knobs (e.g.   for policy gradient algorithms).    The inputs and outputs are assumed to be fixed size unstructured vectors (i.e. every layer is just fully connected).  This makes sense for simple games like kuhn poker, but for more complex games, it can be important for the networks to be structured in a way that encodes the observations and action space in a way that is tailored to the structure of the game (for an extreme example, the  ).    For a one-off project, I could imagine just ripping out the neural network code of the policy gradient algorithm, and replacing it with my own tensorflow code that only works for my specific game.  However, this does not seem like a scalable way to implement things.  Has there been any thought into making algorithms *parameterizeable* over the neural network structure?      p.s. Another way of looking at it is that the observation_tensor destroys all structure in the observations, and the action representation as a discrete list destroys all structure of the action space.  Is there any way we could put some awareness of the action/observation space structure into Open Spiel?"
Running on #8c8e0863 I get segmentation fault: (clang-9 compiler)   
"For example, in the game `kuhn_poker`, the `LegalActions` of the first chance node is `[0,1,2]`, while the `LegalActionsMask` fallbacks to the default implementation and returns `[1,1]`. Is this expected? (If so, does it mean that I should always rely on `LegalActions` instead of `LegalActiosnMask` for chance nodes?)"
"Strictly speaking, I believe stateful game objects are currently not serialized correctly. For example, `NegotiationGame` which holds an internal RNG loses the state of the RNG after serialization and deserialization (using `ToString` and `LoadGame`). I guess this issue is mostly relevant for sampled stochastic implementations, e.g. I have the same problem in      While there is the ability to override `Serialize` and `Deserialize` for state objects, the same should probably be true for game objects. The default implementation would assume stateless game objects and would simply be identical to the current implementation.    The issue occurs in cases where the `rng_seed` parameter is set by a user and then the game and state objects are de/serialized. While the state object is deserialized correctly, a `NewInitialState` call on the deserialized game object, would essentially result in the same sequence of RNG samples and thus the same sequence of new initial states (compared to prior to serialization)."
"The PEP8 style guide is available here:   It is possible to use auto formatters, e.g.   is a good choice."
"Hi all!    In the course of a small project we are currently trying to find some nice algorithms for goofspiel. However, from what I've seen so far, algorithms that would need a LP-solver for the simultaneous case such as (star-) minimax, MCTS (  value iteration, and Q-learning are currently not implemented. Since we don't want to end up with pure strategies, converting the game to a imperfect information sequential game does also not seem to make sense for those algorithms.    So shall we just focus on policy gradient methods in this case? Or does anyone have some experience with goofspiel (or similar simultaneous games) and knows which algorithms generally work well?    Thanks in advance for your suggestions and sorry if this is the wrong place for such a question.    Best,  Andreas"
"In  , function `sample_random_tensor_index(probabilities_of_index_tensor)` fails when running psro_v2_example.py with --rectified=True flags or any case where sample_joint. The functions could be modified into the following:    `        def sample_random_tensor_index(probabilities_of_index_tensor, shape=None):        shape = probabilities_of_index_tensor.shape if not shape else shape        reshaped_probas = probabilities_of_index_tensor.reshape(-1)        num_strats = len(reshaped_probas)        chosen_index = random_choice(num_strats, reshaped_probas)        return np.unravel_index(chosen_index, shape)        def sample_strategy_joint(total_policies, probabilities_of_playing_policies):        """"""Samples strategies given joint probabilities.        Uses independent sampling if probs_are_marginal, and joint sampling otherwise.        Args:          total_policies: A list, each element a list of each player's policies.          probabilities_of_playing_policies: This is a list of play probabilities of          the joint policies specified by total_policies.        Returns:          sampled_policies: A list specifying a single sampled joint strategy.        """"""        shape = tuple([len(ele) for ele in total_policies])        sampled_index = sample_random_tensor_index(probabilities_of_playing_policies, shape)        sampled_policies = []        for player in range(len(sampled_index)):          ind = sampled_index[player]          sampled_policies.append(total_policies[player][ind])        return sampled_policies  `    here an extra shape is passed into `sample_random_tensor_index` because `probabilities_of_index_tensor` as a joint is a one-dimensional vector and its shape could not be used for unraveling."
"Dear authors.    I have been reading the code of DeepCFR and was wondering why a reservoir sampling is not chosen?       According to the following paper, the performance with a reservoir sampling is better than one with a sliding window memory.     From this point of view, I think it is better to choose a reservoir sampling memory."
"Hello everyone,     Starting today until July 3rd, there will be concentrated effort to add functionality to OpenSpiel. During this time, we will be updating github every day (possibly even many times per day).    We will be more available than usual to answer questions that you may have about OpenSpiel. Please use this thread.    Among the list of planned additions:    - Public states sub-API (for implementing algorithms such as DeepStack)  - Card games and RL (Hearts, Skat, Bridge, Tarok)  - More games! Clobber, Emergent Communication, Emerald Mines / Boulderdash, Hyper-backgammon)  - Add bidding to Skat  - Get C++ AlphaZero working externally  - AlphaZero in Hex; Hex & AlphaZero improvements  - Initial investigation in convergence of CFR in n-player and general-sum games    We will also try to run some algorithms and might report a few results."
"Hi all!    I'm currently trying to figure out the best way to implement the game ""Qwinto"". The complete rules can be found here:      In short, it's a turn-based game. During each turn **one** player rolls one to three dice up to two times and then **all** players enter the result into an empty field in their personal score card. In the next turn the next player rolls the dice and so on.    I'm especially looking for recommendations/guidelines how to implement the fact that in the first two steps of a turn a single player is making decisions -- namely how many dice to roll and whether to roll the dice a second time -- whereas in the last step all players individually need to decide where to fill in the dice result in their score card.    I started with an implementation of a simultaneous move game. For all but the ""current"" player (the one who rolls the dice) I return an invalid action for the first two steps of the turn. For the last step all players get actions assigned depending on the state of their score card.  Does that make sense or is there a better alternative? I was thinking about converting it into a sequential move game in which the `CurrentPlayer()` is staying the same during the first to steps and only when the dice result is handled _every_ player is considered.  I noticed that sequential games seem to have better support in terms of algorithm (though I haven't yet looked at many). That's why I'm currently leaning towards converting the game type.    Also, as you might have noticed already, there is a slight confusion about the term ""turn"". In the game description each turn consists of multiple steps. But, correct me if I'm wrong, that is not possible in ``open_spiel``. During each iteration, i.e. checking for termination, and getting and applying actions, only one decision can be made. So in my current implementation each step is represented as an iteration.  Is there any prior or assumption what should be done/how the state should change during one of those iteration? For instance, Qwinto can only ever terminate at the end of a turn (step 3, after the players have filled in the cards) but not after either of the first to steps in a turn. So with my current implementation termination is checked too often. Not an issue in itself but makes me wonder if my design decision is good or if there is a better alternative.    Suggestions and recommendation are welcome. Let me know if any of the points above is not clear and I'll try to explain it better."
"Dear Authors, I am working on universal_poker using open_spiel (python) and was wondering if there is a standard way to add information abstraction and action abstraction to these games and then use the preexisting algorithms on the abstracted game?    If there isn't a standard way, what would be your suggestion to do this? I was currently thinking of subclassing the state class and changing the definition of the infomation_state_string  and information_state_tensor methods. Is this the best way?     Thanks a lot for this wonderful platform and your help!"
"Hi all,    This is a post just to let you know that we plan to make some changes in the coming month(s) in order to support TF2.2 and Python 3.8, with the aim of finally fixing and closing      I expect the changes will mostly be minor, like adding `disable_v2_behavior` in several places, but at least one I consider to be a major change: removing our dependency on Sonnet (until we can properly add it back). This is necessary because it does not play nice with TF1 backwards compatibility, which is necessary if we want to upgrade to have OpenSpiel fully working on Ubuntu 20.04 in the short-term (see details in the thread above). Luckily most of the uses of Sonnet are simple so I hope this will not be disastrous.    We will not ""fully"" support TF2 in the sense that most of our implementations will still be TF1 running under `disable_v2_behavior`, so we can migrate them over time. Also, we will not force Python 3.8 nor fully test it until Travis-CI supports Ubuntu 20.04 officially.    Just in case, before undergoing any of these changes, we will tag a release later this week for anybody who wants to depend on a stable release pre-changes.    Thanks!  Marc  "
"This was happening yesterday and still today, anyone else having problems? This usually happens when an outside library upgrades / changes.     "
"Hi,    I am trying to run Kuhn poker playthrough using different policies (NSFP and CFR from examples), but I could not find a way to save these. Pickling doesn't seem to work. Is there some other method to save/load these?    Thanks"
"While compiling the `pyspiel` target, I get following warnings:         Compiled with g++-7 (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0    Not too important I guess, pyspiel works just fine, I just thought I'd mention it since there are no other warnings generated anywhere else."
"Playing random games with Swift environments is generally faster than the combination of Python/C++. But Breakthrough is significantly slower for some reason. Times below are in μs:  |  | **Swift** | **Python** |  | :---         |     :---:      |          ---: |  | TicTacToe  | 87     | 255    |  | Breakthrough     | 9340      | 2506      |  | Kuhn Poker     | 15       | 149      |    These numbers were generated on a 2019 15' Macbook Pro, and the Swift implementation was built with `-O` and `Whole Module` optimization flags:     and      "
"Hi I have been reading through laser_tag.cc, and open_spiel/open_spiel/integration_tests/playthroughs/laser_tag(horizon=20).txt.     Some of the agents' behaviors are strange. Here is one example where agent A fails to step right even when it is still within the grid even after doing so. There exist numerous similar problems in the afore-mentioned txt file. Would you kindly look into that?   "
"When trying to run the tic-tac-toe example got an out of GPU memory error (CUDA_ERROR_OUT_OF_MEMORY).  Full log output:    Hardware: RTX 2070 Super with 8GB of GPU memory.  Using Tensorflow 2, Python 3.6.2. Pip list output:      What's the minimun GPU memory requirement to try the Alpha Zero tic-tac-toe example? Might the out of memory related error be related to using TF2?    In order to use TF2 I just modifed `requirements.txt`, git diff output is:   "
"During our project I encountered several methods in the Python code that commented on the possibility of multithreading that specific part. A good example is the following for loop in the PSRO algorithm:      The implementation of CFR also lends itself to distributed programming when we observe the recursive nature of the utility updates:         In order to drastically improve performance (when run on CPU) it is possible to improve this by using a distributed task scheduler such as Dask. This can specifically be realised with     -    -           Such an approach allows for computation across multiple workers which can drastically improve training time in practice. At the moment I am not able to implement this feature, due to my exams, but is open for anyone who would give it a shot."
"Hi,    I'm trying to run `tic_tac_toe_alpha_zero.py` but it fails with an **ZeroDivisionError: division by zero** exception. Here is the stack trace:     > ===================== Exception caught =====================  > Traceback (most recent call last):  >   File ""\~/openspiel/open_spiel/python/algorithms/alpha_zero/alpha_zero.py"", line 166, in _watcher  >     return fn(config=config, logger=logger, **kwargs)  >   File ""\~/openspiel/open_spiel/python/algorithms/alpha_zero/alpha_zero.py"", line 452, in learner  >     ""results"": [sum(e.data) / len(e) for e in evals],  >   File ""\~/openspiel/open_spiel/python/algorithms/alpha_zero/alpha_zero.py"", line 452, in    >     ""results"": [sum(e.data) / len(e) for e in evals],  > ZeroDivisionError: division by zero  >     I tried it on a Mac and Ubuntu 18.04 machine, both failed with same error."
"     If the .efg file counts infosets separately per player, this will incorrectly label a perfect information game as imperfect information. Confirmed it with a test on tic tac toe."
"Hi,    When running tests with Nox, test #141 (`run_python_test`) fails with the following `ModuleNotFoundError` exception.    Any suggestions? I thought it could be wrong package name, since `open_spiel` is installed as `pyspiel` so replaced `open_spiel.utils.run_python_test_file` with `pyspiel.utils.run_python_test_file` but it didn't solve the issue.      "
"Hi,    I tried to run nox tests within my Python 3.7 virtual environment, but it requires python **3.6**:       I found that the `python` argument has been added in a893c45b to mitigate build problems with TravisCI. However, it breaks things when running nox inside a venv.    I think if we remove python version specifier, it uses parent environment version and works perfectly. (parent env in case of TravisCI will be Python 3.6, and in a local venv, such as mine, it will be Python 3.7)    I'm relatively new to this repository and still looking around to understand how things work. If there is a specific reason for using Python 3.6 only and not newer versions, I believe it should be noted in the README file.  "
"Hi @findmyway , seems that as of a few days ago the julia test is failing (oddly, only the test fails... it still compiles fine). This was the original error I got         Then when I removed julia and all the packages, starting from scratch (re-installing Julia via `./install.sh`) I get a different error:         Did something change with the packages recently? Can you reproduce this?    "
"Dear authors, I am currently trying C++ version AlphaZero and I have been trying to link TensorFlow with open_spiel for some time. I have tried to compile  TensorFlow c++ libs, but linking them from open_spiel just failed. The reported problems are about Absl lib and Eigen lib.  I also tried to link with the prebuilt TensorFlow c_api lib. It works without linking errors. However, the program may be stuck somehow. It seems the problem is caused by absl::Mutex.  I gauss it is because open_spiel and TensorFlow depend on different versions of Absl, like below.    > open_spiel -----> Absl-lts_2020_02_25  > └─--------------> TensorFlow -----> Absl_some_old_version    What do you think? Could you please give me some advice?"
can I use open spiel in a unity3d environment?
Hey there my name is Julian Bokelmann and I am a computer science student at Heinrich-Heine-Universität in Duesseldorf Germany. I want to integrate The Settlers of Catan (Catan for short) into OpenSpiel as part of my bachelor thesis. I plan to use an existing implementation of Catan and first port it over into C++ and then into OpenSpiel. When I have a working game in OpenSpiel I want to test and evaluate different reinforcement learning algorithms.
"It seems that the some packages on colab servers have been upgraded (e.g. sonnet) causing some backward-compatibility issues with the Python learning algorithms.  (See some discussion here:      We know about it, but we're not sure when we will be able to fix this. If anybody works out what to fix, a PR would be greatly appreciated! "
I'm trying to get deepcfr to work in colab:       I get the following error:    /usr/local/open_spiel/open_spiel/python/algorithms/deep_cfr.py in reinitialize_advantage_networks(self)      241   def reinitialize_advantage_networks(self):      242     for p in range(self._num_players):  --> 243       for key in self._advantage_networks       245     AttributeError: 'MLP' object has no attribute 'initializers'  
"I simply concatenated ""install_open_spiel.ipynb"" and ""test_universal_poker.ipynb"" from open_spiel/open_spiel/colabs/ in Google Colab.   However I get the error:  ""RuntimeError: Unknown game 'universal_poker'. Available games are:"""
"I want to be able to train a model on the Chess game environment using different Algorithms to compare and test performance as part of my research and experimentation for my extended essay.  I need some assistance and guidance as to how (and if) I can implement the PSRO_rn model (playing against itself in a competitive multi-agent scenario) and the A2C playing against a random opponent, to train these models, and save the models as well for later use. "
"I test CFR on the Leduc poker. I count the number of visited states during CFR updating process. The print_freq = 10.    On my own Mac, it costs around 200 seconds to visit 1e6 states. It seems to be slow. Is there any method that we can accelerate CFR?    Thanks~"
Would you be willing to support checkers? I think that it's a really interesting game for RL because of how jumping impacts the action space.
"Hi,  I was wondering if there were any plans for an implementation of PBR to be released anytime soon?  Thanks!"
"First of all, thanks for the repo!  I am trying to use NFSP to run some simple imperfect-information games implemented in the repo, however, I found that the networks and losses in NFSP/DQN didn't define on cuda(gpu) devices. Are there any methods to run NFSP on gpu device?  Thanks a lot~"
"When I compare our implementation of AlphaZero with the pseudocode of paper, I find two different, maybe it will have a slight influence on the performance.  The first is the Dirichlet noise:  - In the pseudocode of AlphaZero, the noise is added before MCTS simulations, so it will have the same noise in different simulations.     - In our MCTS,  different noise is added in every simulations.              different noise between simulations may have better exploration, but I don't know whether it will make it unstable?      (Edit after review: I find the sample batch of the games in pseudocode is sampled with the probability according to the length of the game, the second one is just different in implementation detail.)  The second one is the sample batch for training.   - In pseudocode of AlphaZero, games are saved as the sample, when sampling from replay buffer, it sample num_batch of game, and random choice a game_pos in every game.     - In our implementation, we save every game position as the sample, and we just random sample from game position        In the implementation of pseudocode, sampling from games will make it fair for the games which have different length, and our implementation is fair for every data point.    Maybe the different implementation can all work well, but I think we should have careful consideration or it may also need to have some experiments.    @sbodenstein @lanctot         "
"Hi,    I'm trying to do some research on psro (v2) and have been trying to work through the example to understand the code. However, there seems to me to be an issue with     and similarly with the policy gradient responder. Using the example code yields the following error when running this part of the function     `__init__() takes 3 positional arguments but 6 were given` which I currently havent found an obvious fix for.    Please let me know if I am making an obvious mistake here, thanks."
"I followed the steps 1, 2 and 3 at   without complications.    But when I enter julia console and type `using OpenSpiel_jll` I get following error:     "
"Hello,    I tried to use Bazel to build Openspiel. It succeeds. But when I tried to use Bazel to build Openspiel in a Tensorflow folder, it failed.    I put open_spiel folder in Tensorflow folder. I used Tensorflow's WORKSPACE. It gives following errors(seemed that compiling passed, while linking failed with errors on absl). I tried removing .bazelrc file in Tensorflow source folder, and Bazel build passed again. Could anyone help? Thanks a lot.    Environment: WSL Ubuntu 18.04; Openspiel latest version; Bazel 1.2.1; Tensorflow: using source codes; Python 3.6.    Linking of rule '//tensorflow/open_spiel/games:go_test' failed (Exit 1)  bazel-out/k8-opt/bin/tensorflow/open_spiel/tests/_objs/basic_tests/basic_tests.o:basic_tests.cc:function std::__cxx11::basic_string , std::allocator  > absl::StrCat , std::allocator  > > const&, absl::BitGenRef): error: undefined reference to 'absl::strings_internal::CatPieces '  bazel-out/k8-opt/bin/tensorflow/open_spiel/_objs/spiel/spiel.o:spiel.cc:function open_spiel::Game::DeserializeState(std::__cxx11::basic_string , std::allocator  > const&) const: error: undefined reference to 'absl::ByChar::Find(std::basic_string_view  >, unsigned long) const'  bazel-out/k8-opt/bin/tensorflow/open_spiel/_objs/spiel/spiel.o:spiel.cc:function open_spiel::Game::DeserializeState(std::__cxx11::basic_string , std::allocator  > const&) const: error: undefined reference to 'absl::ByChar::Find(std::basic_string_view  >, unsigned long) const'  bazel-out/k8-opt/bin/tensorflow/open_spiel/_objs/spiel/spiel.o:spiel.cc:function open_spiel::GameRegisterer::CreateByName(std::__cxx11::basic_string , std::allocator  > const&, std::map , std::allocator  >, open_spiel::GameParameter, std::less , std::allocator  > >, std::allocator , std::allocator  > const, open_spiel::GameParameter> > > const&): error: undefined reference to 'absl::strings_internal::CatPieces '  bazel-out/k8-opt/bin/tensorflow/open_spiel/_objs/spiel/spiel.o:spiel.cc:function open_spiel::DeserializeGameAndState(std::__cxx11::basic_string , std::allocator  > const&): error: undefined reference to 'absl::ByChar::Find(std::basic_string_view  >, unsigned long) const'  bazel-out/k8-opt/bin/tensorflow/open_spiel/_objs/spiel/spiel.o:spiel.cc:function open_spiel::DeserializeGameAndState(std::__cxx11::basic_string , std::allocator  > const&): error: undefined reference to 'absl::ByChar::Find(std::basic_string_view  >, unsigned long) const'    Best regards,  Lancelot"
"Hi,  I'm having an issue building the open spiel environment this morning that wasn't an issue yesterday, happens at 63% of build. Here's the error:      Happy to give more info if needed  "
"In the documentation   we read in several places that games are written in C++    >  Game implementations are in C++ and wrapped in Python    >   Games are written in C++    This is incorrect and may lead to people giving up on using this.    If you open python/games/tic_tac_toe.py you read what I suppose to be really correct    >Games are strongly encouraged to be written in C++ for efficiency and so tha tthey can be available from all APIs. Writing Python-only games means that they will be slow and only be accessible from Python algorithms.  > Nonetheless, there are many cases where this is desirable (or required),....  "
"I followed the installation steps for windows subsystem (I have ubuntu 18.04 WSL).  But when i run `pip3 install --upgrade -r requirements.txt # Install Python dependencies`,   I get the following error:   "
how to set environment seed so that I can achieve the exact the same results every time.
"Hi all,     We recently discovered OpenSpiel has installation problems on Ubuntu 20.04; there is a failure when trying to install tensorflow from pip.     A quick investigation reveals that TF (both version 1.15 that we use now and version 2) requires a Python version <= 3.7 but Ubuntu 20.04 comes with Python 3.8.     We don't currently have a solution, but will be looking into it. Any tips or advice on how to fix this are welcome!"
"I found dockerfile is added in yesterday commit: #158 , while it was placed in open_spiel/open_spiel/Dockerfile.  In the document, installing via docker   bash  docker build -t openspiel .  docker run openspiel python3 python/examples/matrix_game_example.py     RUN ./install.sh    RUN pip3 install --upgrade setuptools testresources   RUN pip3 install --upgrade -r requirements.txt  RUN pip3 install --upgrade cmake    Output:  Step 3/11 : RUN ./install.sh   ---> Running in 06c1f93031b3  /bin/sh: 1: ./install.sh: not found ( failed in this step)  ```  Whether the Dockerfile should be placed in the top-level of the repo, just like in this  ?            "
"Hi @findmyway ,    We recently had to change `MCTSBot` to use a `shared_ptr `. However, when I passed it directly via the constructor, I got a strange problem with the Julia API test that I couldn't explain (see below).     The only thing that fixed it was to change the constructor to take a `const std::shared_ptr &` instead, which is a bit weird: see      Do you have any idea why the error occurs below and how we can fix it? Here's the error when we try to use `std::shared_ptr ` directly (and changing the signature of MCTSBot in the wrapper `spieljl.cc`):     "
"I've been working on putting OpenSpiel in Docker container (since it always greatly boosts framework adoption). So far I can get the tests to run, but there are still some tests prone to failure. Is there anyone else with some Docker experience that spots the problem in the Dockerfile.    Dockerfile Reference:      Thanks in advance!    <img width=""1751"" alt=""Tests"" src=""         "
"Dear Authors,    I am trying to reproduce the results of   based on pytorch. The idea in this paper is very interesting. Before my implementation, I run the experiments on two-players kuhn and leduc based on the release codes:   and  .    To see the long-term convergence, I set num_episodes=1e7 and use the default settings for other hyperparameters,  the exploitability of different methods after 1e7 episode are listed as follows:            a2c, qpg, rpg, rmpg, nfsp  kuhn:  0.19757, 0.19307, 0.19336, 0.12419, 0.01980  leduc: 1.05118, 0.60193, 0.67432, 1.22162, 0.63754    It seems that the results are worse than the results reported in Figure 2. I wonder whether it's due to the high well-known variance in deep rl, the fine-tuning hyperparameters or some other tricks I have missed.    Best,  Hui      "
"when I execute CXX=clang++ cmake -DPython_TARGET_VERSION=3.6 -DCMAKE_CXX_COMPILER=${CXX} ../open_spiel ,is it normal to print the following info?    I notice that   ""CMake Error at games/CMakeLists.txt:112 (add_library):    Cannot find source file:        bridge/double_dummy_solver/include/dll.h  ""  does it have an effect on the next step? how can I handle it?    BUILD_WITH_HANABI not set. Defaults to OFF  BUILD_WITH_HANABI: OFF   BUILD_WITH_ACPC not set. Defaults to OFF  BUILD_WITH_ACPC: OFF   BUILD_WITH_JULIA not set. Defaults to OFF  BUILD_WITH_JULIA: OFF   -- Configuring done  You have changed variables that require your cache to be deleted.  Configure will be re-run and you may have to reset some variables.  The following variables have changed:  CMAKE_CXX_COMPILER=   CMAKE_CXX_COMPILER=     -- The C compiler identification is GNU 5.4.0  -- The CXX compiler identification is Clang 8.0.1  -- Check for working C compiler: /usr/bin/cc  -- Check for working C compiler: /usr/bin/cc -- works  -- Detecting C compiler ABI info  -- Detecting C compiler ABI info - done  -- Detecting C compile features  -- Detecting C compile features - done  -- Check for working CXX compiler: /usr/local/bin/clang++  -- Check for working CXX compiler: /usr/local/bin/clang++ -- works  -- Detecting CXX compiler ABI info  -- Detecting CXX compiler ABI info - done  -- Detecting CXX compile features  -- Detecting CXX compile features - done  BUILD_WITH_HANABI: OFF   BUILD_WITH_ACPC: OFF   BUILD_WITH_JULIA: OFF   -- Looking for pthread.h  -- Looking for pthread.h - found  -- Looking for pthread_create  -- Looking for pthread_create - not found  -- Looking for pthread_create in pthreads  -- Looking for pthread_create in pthreads - not found  -- Looking for pthread_create in pthread  -- Looking for pthread_create in pthread - found  -- Found Threads: TRUE    -- Found Python: /home/chenxli123/anaconda3/lib/libpython3.7m.so (found version ""3.7.1"") found components:  Development   -- Configuring done  CMake Error at games/CMakeLists.txt:112 (add_library):    Cannot find source file:        bridge/double_dummy_solver/include/dll.h      Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm    .hpp .hxx .in .txx      -- Generating done  -- Build files have been written to: /home/chenxli123/open_spiel/build  "
"hello,  I has some problems when execute make -j12  [ 17%] Built target absl_bad_any_cast_impl  [ 18%] Building CXX object abseil-cpp/absl/base/CMakeFiles/absl_malloc_internal.dir/internal/low_level_alloc.cc.o  [ 18%] Building CXX object abseil-cpp/absl/strings/CMakeFiles/absl_strings.dir/charconv.cc.o  [ 18%] Building CXX object abseil-cpp/absl/strings/CMakeFiles/absl_strings.dir/ascii.cc.o  [ 18%] Building CXX object abseil-cpp/absl/strings/CMakeFiles/absl_strings.dir/escaping.cc.o  [ 19%] Building CXX object abseil-cpp/absl/strings/CMakeFiles/absl_cord.dir/cord.cc.o  [ 19%] Linking CXX static library libabsl_demangle_internal.a  [ 19%] Built target absl_demangle_internal  [ 20%] Building CXX object abseil-cpp/absl/strings/CMakeFiles/absl_strings.dir/internal/charconv_bigint.cc.o  In file included from /home/cxl/open_spiel/open_spiel/simultaneous_move_game.cc:15:  In file included from /home/cxl/open_spiel/open_spiel/../open_spiel/simultaneous_move_game.h:22:  /home/cxl/open_spiel/open_spiel/../open_spiel/spiel.h:24:10: fatal error: 'optional' file not found  #include             ^~~~~~~~~~  In file included from /home/cxl/open_spiel/open_spiel/spiel.cc:15:  /home/cxl/open_spiel/open_spiel/../open_spiel/spiel.h:24:10: fatal error: 'optional' file not found  #include             ^~~~~~~~~~  [ 20%] Building CXX object abseil-cpp/absl/strings/CMakeFiles/absl_strings.dir/internal/charconv_parse.cc.o  [ 20%] Linking CXX static library libabsl_malloc_internal.a  [ 20%] Built target absl_malloc_internal  In file included from /home/cxl/open_spiel/open_spiel/matrix_game.cc:15:  In file included from /home/cxl/open_spiel/open_spiel/../open_spiel/matrix_game.h:25:  In file included from /home/cxl/open_spiel/open_spiel/../open_spiel/normal_form_game.h:24:  In file included from /home/cxl/open_spiel/open_spiel/../open_spiel/simultaneous_move_game.h:22:  /home/cxl/open_spiel/open_spiel/../open_spiel/spiel.h:24:10: fatal error: 'optional' file not found  #include             ^~~~~~~~~~  [ 20%] Building CXX object abseil-cpp/absl/strings/CMakeFiles/absl_strings.dir/internal/memutil.cc.o  In file included from /home/cxl/open_spiel/open_spiel/policy.cc:15:  In file included from /home/cxl/open_spiel/open_spiel/../open_spiel/policy.h:24:  /home/cxl/open_spiel/open_spiel/../open_spiel/spiel.h:24:10: fatal error: 'optional' file not found  #include             ^~~~~~~~~~  In file included from /home/cxl/open_spiel/open_spiel/tensor_game.cc:15:  In file included from /home/cxl/open_spiel/open_spiel/../open_spiel/tensor_game.h:26:  In file included from /home/cxl/open_spiel/open_spiel/../open_spiel/normal_form_game.h:24:  In file included from /home/cxl/open_spiel/open_spiel/../open_spiel/simultaneous_move_game.h:22:  /home/cxl/open_spiel/open_spiel/../open_spiel/spiel.h:24:10: fatal error: 'optional' file not found  #include             ^~~~~~~~~~  In file included from /home/cxl/open_spiel/open_spiel/spiel_bots.cc:15:  In file included from /home/cxl/open_spiel/open_spiel/../open_spiel/spiel_bots.h:22:  In file included from /home/cxl/open_spiel/open_spiel/../open_spiel/policy.h:24:  /home/cxl/open_spiel/open_spiel/../open_spiel/spiel.h:24:10: fatal error: 'optional' file not found  #include             ^~~~~~~~~~  [ 20%] Building CXX object abseil-cpp/absl/debugging/CMakeFiles/absl_symbolize.dir/symbolize.cc.o  [ 20%] Building CXX object abseil-cpp/absl/synchronization/CMakeFiles/absl_graphcycles_internal.dir/internal/graphcycles.cc.o  [ 20%] Building CXX object abseil-cpp/absl/strings/CMakeFiles/absl_strings.dir/match.cc.o  1 error generated.  CMakeFiles/open_spiel_core.dir/build.make:127: recipe for target 'CMakeFiles/open_spiel_core.dir/simultaneous_move_game.cc.o' failed  make[2]: *** [CMakeFiles/open_spiel_core.dir/simultaneous_move_game.cc.o] Error 1  make[2]: *** Waiting for unfinished jobs....  [ 21%] Building CXX object abseil-cpp/absl/strings/CMakeFiles/absl_strings.dir/numbers.cc.o  1 error generated.  CMakeFiles/open_spiel_core.dir/build.make:75: recipe for target 'CMakeFiles/open_spiel_core.dir/spiel.cc.o' failed  make[2]: *** [CMakeFiles/open_spiel_core.dir/spiel.cc.o] Error 1  [ 21%] Building CXX object abseil-cpp/absl/strings/CMakeFiles/absl_strings.dir/str_cat.cc.o  1 error generated.  CMakeFiles/open_spiel_core.dir/build.make:101: recipe for target 'CMakeFiles/open_spiel_core.dir/matrix_game.cc.o' failed  make[2]: *** [CMakeFiles/open_spiel_core.dir/matrix_game.cc.o] Error 1  [ 21%] Building CXX object abseil-cpp/absl/strings/CMakeFiles/absl_strings.dir/str_replace.cc.o  1 error generated.  CMakeFiles/open_spiel_core.dir/build.make:114: recipe for target 'CMakeFiles/open_spiel_core.dir/policy.cc.o' failed  make[2]: *** [CMakeFiles/open_spiel_core.dir/policy.cc.o] Error 1  [ 21%] Building CXX object abseil-cpp/absl/strings/CMakeFiles/absl_strings.dir/str_split.cc.o  1 error generated.  CMakeFiles/open_spiel_core.dir/build.make:153: recipe for target 'CMakeFiles/open_spiel_core.dir/tensor_game.cc.o' failed  make[2]: *** [CMakeFiles/open_spiel_core.dir/tensor_game.cc.o] Error 1  [ 22%] Building CXX object abseil-cpp/absl/strings/CMakeFiles/absl_strings.dir/string_view.cc.o  1 error generated.  CMakeFiles/open_spiel_core.dir/build.make:88: recipe for target 'CMakeFiles/open_spiel_core.dir/spiel_bots.cc.o' failed  make[2]: *** [CMakeFiles/open_spiel_core.dir/spiel_bots.cc.o] Error 1  CMakeFiles/Makefile2:72: recipe for target 'CMakeFiles/open_spiel_core.dir/all' failed  make[1]: *** [CMakeFiles/open_spiel_core.dir/all] Error 2  make[1]: *** Waiting for unfinished jobs....  [ 22%] Building CXX object abseil-cpp/absl/strings/CMakeFiles/absl_strings.dir/substitute.cc.o  [ 22%] Linking CXX static library libabsl_cord.a  [ 22%] Built target absl_cord  [ 22%] Linking CXX static library libabsl_graphcycles_internal.a  [ 22%] Built target absl_graphcycles_internal  [ 23%] Linking CXX static library libabsl_symbolize.a  [ 23%] Built target absl_symbolize  [ 23%] Linking CXX static library libabsl_strings.a  [ 23%] Built target absl_strings  Makefile:94: recipe for target 'all' failed  make: *** [all] Error 2    could you please help me how to fix it ??? thanks!"
FYI.
I can't seem to find any support for persisting the state of C++ CFR solver implementations (apologize if it exists and this issue is redundant). I did find pickling support for game and state objects though.    I believe it would be useful to be able to specify where and how often checkpoints (from which self-play can continue at some later time) are saved to disk. I can think of two reasons:  - this would make training more robust (e.g. when it takes longer times)  - it would allow for usage of interruptible cloud VMs (reduce the cost of training significantly)    I haven't spent much time on this but maybe there are other solvers where this would be useful?
"I have been reading ""Neural Replicator Dynamics"" (NRD) and  ""Actor Critic Policy Optimization in Partially Observable Multiagent Environments"" (ACPO) and I am trying to replicate some of the results in the NRD paper. From what I can tell, the implementation of NRD on here is different than the setup used in the paper, which uses what appears to be more similar to how the ACPO paper is done (and the kuhn_policy gradient code on here).    Correct me if I am wrong, but the QPG policy update rule from ACPO corresponds to the Softmax Policy Gradient (SPG) mentioned in NRD? In which case, this would be the one to modify with the ""one-line-fix"" from NRD? I have been attempting to incorporate this change but so far have been unsuccessful. Is the exact setup used in the NRD paper somewhere and I am missing it, or can anyone shed any light on where exactly to modify QPG so that it becomes the new update rule from NRD i.e. ""bypasses the gradient through the softmax?""    Thanks in advance"
"Thanks for open-sourcing this project, great initiative! I'm a bit confused about how to encode the actions when implementing a rather distinctive sequential game that consists of multiple phases. Each phase is made up of multiple neighboring states within the game tree and has its own set of distinct actions. We can think of them as different phases because actions in one phase can never be legal in any other phase.    For example, the first phase could be multiple players choosing a single card out of maximum `N` possible cards, each turn, until none of the cards are left to choose from. The second phase could be the same players playing the selected cards, discarding them in turns, until all players have empty hands. The first phase thus has `N` distinct actions and the second phase has a different set of `N` distinct actions.    Do actions returned by `State::LegalActions()` need to be encoded uniquely across all different phases, e.g. `[0, N-1]` in phase 1 and `[N, 2N-1]` in phase two, or can I use codes `[0, N-1]` for both phases? Do actions need to be encoded uniquely across arbitrary states at all or is this only a requirement for states within the same information set?"
"I noticed that there is ""See open_spiel/python/examples/nfsp.py for an usage example."" in open_spiel/python/algorithms/nfsp.py.   But I can't find this example.  Is there an example of using nfsp, or how can I quickly create one based on the existing content?"
"Dear Authors,    Does open spiel support vector cfr, that is, performing cfr on public tree?    I am running cfr_example.py on leduc_poker to test the performance of open spiel.  It seems that even for this toy game, it will need a lot of time to finish 1k iterations? I plan to evaluate one multi-agent reinforcement learning algorithm on large-scale imperfect information games, so that the running time will be very an key issue.    BTW, the default setting is simultaneous updates or alternating updates?   "
"Hi,    Thanks for all the hard work developing & maintaining this great library.     I'm having a bit trouble building the project with the latest commits. I'm running:  - Ubuntu 18.04  - Clang 9.0.0  - cmake 3.16.3    To reproduce, after following the installation instructions in the readme,  `./open_spiel/scripts/build_and_run_tests.sh` errors with        I've narrowed the issue down to commit 71ded47c2866f5adfc6cb434ac675204f1372b05, the previous commit builds fine for me.     From my Googling around I think it might be something to do with `filesystem` directive not being available but am unsure what my next steps are?     Thanks in advance for any help,  Dave  "
"Dear authors,    I am trying to find some examples of applying DRL as an oracle to the generalized PSRO framework. However, I found that in examples related to PSRO (e.g generalized PSRO and RNR) only an evolutionary-strategy oracle is being used for simplification. So I am wondering if there is an example in which PSRO is combined with DRL as experiments in the corresponding papers. Moreover, I notice that there are implemented actor-critic and DQN in the library. Is there a simple way to combine them with PSRO? Thank you."
"Hi all,   I am trying to install the open_spiel according to the published paper. It gives me an error in the last step which is :   $cmake -DPython_TARGET_VERSION=3.6 -DCMAKE_CXX_COMPILER=clang++ ./open_spiel    And the error is :   -- Could NOT find Python3 (missing: Python3_LIBRARIES Python3_INCLUDE_DIRS Development) (found version ""3.8.0"")    When I ran ctest -j12 it gave me error on almost all python related tests:    56% tests passed, 51 tests failed out of 117    Total Test time (real) = 192.82 sec    The following tests FAILED:    63 - python_api_test (Failed)    64 - python_playthrough_test (Failed)    65 - python_action_value_test (Failed)    66 - python_action_value_vs_best_response_test (Failed)    67 - python_best_response_test (Failed)    68 - python_cfr_br_test (Failed)    69 - python_cfr_test (Failed)    70 - python_deep_cfr_test (Failed)    71 - python_discounted_cfr_test (Failed)    72 - python_dqn_test (Failed)    73 - python_eva_test (Failed)    74 - python_evaluate_bots_test (Failed)    75 - python_expected_game_score_test (Failed)    76 - python_exploitability_descent_test (Failed)    77 - python_exploitability_test (Failed)    78 - python_fictitious_play_test (Failed)    79 - python_generate_playthrough_test (Failed)    80 - python_get_all_states_test (Failed)    82 - python_lp_solver_test (Failed)    84 - python_mcts_test (Failed)    85 - python_minimax_test (Failed)    86 - python_neurd_test (Failed)    87 - python_nfsp_test (Failed)    88 - python_outcome_sampling_mccfr_test (Failed)    89 - python_policy_aggregator_test (Failed)    90 - python_policy_gradient_test (Failed)    92 - python_generalized_psro_test (Failed)    93 - python_rectified_nash_response_test (Failed)    94 - python_random_agent_test (Failed)    95 - python_rcfr_test (Failed)    96 - python_sequence_form_lp_test (Failed)    97 - python_value_iteration_test (Failed)    98 - python_bluechip_bridge_uncontested_bidding_test (Failed)    99 - python_uniform_random_test (Failed)   100 - python_alpharank_test (Failed)   101 - python_alpharank_visualizer_test (Failed)   102 - python_dynamics_test (Failed)   103 - python_heuristic_payoff_table_test (Failed)   104 - python_utils_test (Failed)   105 - python_visualization_test (Failed)   106 - python_catch_test (Failed)   107 - python_cliff_walking_test (Failed)   108 - python_data_test (Failed)   109 - python_tic_tac_toe_test (Failed)   110 - python_bot_test (Failed)   111 - python_games_sim_test (Failed)   112 - python_matrix_game_utils_test (Failed)   113 - python_policy_test (Failed)   114 - python_pyspiel_test (Failed)   115 - python_rl_environment_test (Failed)   116 - python_tensor_game_utils_test (Failed)      Do you how can I fix the -- Could not find Python3 problem? My python 3 version is:  $ python3 --version  Python 3.6.9    Any help is appreciated."
"Dear Authors,    I am trying to evaluate my new algorithm on this open-spiel. However, when I try to install this package,   after I execute the installation command below, some error occurs:  `./open_spiel/scripts/build_and_run_tests.sh`    error:    5 errors generated.  CMakeFiles/open_spiel_core.dir/build.make:101: recipe for target 'CMakeFiles/open_spiel_core.dir/matrix_game.cc.o' failed  make[2]: *** [CMakeFiles/open_spiel_core.dir/matrix_game.cc.o] Error 1  CMakeFiles/Makefile2:72: recipe for target 'CMakeFiles/open_spiel_core.dir/all' failed  make[1]: *** [CMakeFiles/open_spiel_core.dir/all] Error 2  make[1]: *** Waiting for unfinished jobs....  [ 15%] Building CXX object abseil-cpp/absl/base/CMakeFiles/absl_base.dir/internal/sysinfo.cc.o  [ 15%] Building CXX object abseil-cpp/absl/base/CMakeFiles/absl_base.dir/internal/thread_identity.cc.o  [ 15%] Building CXX object abseil-cpp/absl/base/CMakeFiles/absl_base.dir/internal/unscaledcycleclock.cc.o  [ 16%] Linking CXX static library libabsl_absl_debugging_internal.a  [ 16%] Built target absl_debugging_internal  [ 17%] Linking CXX static library libabsl_absl_base.a  [ 17%] Built target absl_base  Makefile:94: recipe for target 'all' failed  make: *** [all] Error 2      How to handle this issue.    Best,  Hui    "
"I am updating with `pip3 install --upgrade -r requirements.txt`, but I get this message:       I have currently 1.14.0 installed, on Windows (WSL).    How can I solve this?"
Running python rl_environment_test.py (python/tests) shows a failed test:     
"A natural extension of AlphaZero is  . The pseudocode implementation is very similar to that of AlphaZero, so it would be interesting to extend the AlphaZero implementation #134 to also support MuZero.    Is there any interest in having a MuZero implementation in OpenSpiel? "
"Although the travis build runs successfully, there are some errors with the build on Mac. See       I guess that is the main reason why the build on Mac hit  , while on Ubuntu it hit  .    One possible solution may be  "
"Hi there,  don't know if this is the right place to ask this but yet I don't have any other Contact Details so I'm trying it this way.    Yesterday I got an idea about an algorithm for solving imperfect games. Basically it it a cross between deep_ctr and exploitability decent. The main idea behind it is to train a deep Policy network directly with exploitability decent. I made the implementation yesterday and well here it is:         This works very well for Kuhn poker and the exploitability reaches near nash equilibrum (<0.001). But for leduc poker convergence is very slow and gets stuck at approx. 1.3 . Do you have any tips how to improve my algorithm that I can contribute it to open_spiel.    Best regards  Dennis      "
"I am trying to train a2c agent for the full contract bridge environment by modifying breakthrough_dpn.py. While running, the following error happens:      File ""/Users/sunshuo/Desktop/open_spiel/open_spiel/python/algorithms/policy_gradient.py"", line 261, in _act      action = np.random.choice(len(probs), p=probs)    File ""mtrand.pyx"", line 1144, in mtrand.RandomState.choice  ValueError: probabilities contain NaN    It happens when the policy_probs of all legal_actions is 0, which means sum(probs)=0  #       probs = np.zeros(self._num_actions)      probs[legal_actions] = policy_probs[0][legal_actions]      probs /= sum(probs)      action = np.random.choice(len(probs), p=probs)      return action, probs    print out:  legal_actions:  [52, 54]  probs[legal_actions]:  [0. 0.]  sum 0, Nan will happen  Actions 52,54 should correspond to PASS and REDOUBLE. I got exactly the same error in my own implementation. I am confused how the prediction value of neural networks could be exactly 0 (I assume it will be very tiny value but not 0). This error does not happen when I use DQN agent, the internal reason for this is not clear for me also."
"I am interested in applying, and contributing to, this framework and have some questions and remarks!    My background: I am a data scientist, app developer and a boardgame enthusiast. As a hobby project, last year I have  implemented a web-app, an online version of a multiplayer strategic boardgame, and now I am think about developing an AI for it, so players can choose to play against AI bots instead of against other players. I might implement more games in the future as well. I guess this framework is a great starting point for me.     Some information on the game. The game is named Ponzi Scheme (more info on:  ). Some key characteristics:  economic trading game, hidden actions, a few chance events, three or more players.    The implementation of the game is in Python (on the the server side). I think it won't be too difficult to adapt the implementation to the openspiel API. Most important change will be handling the chance events, i.e. adding a chance player instead of handling chance events as game state changes as a result of player actions.     First question: is there an interest that I add this game to this github? If so, I am happy to do so. I have understood the games here are implemented both in Python in C++. Is a C++ implementation mandatory? Also happy to do that, but it will take time.    I am planning to try different algorithms but maybe someone can give some advice. I am familiar with some algorithms like MCTS, minmax, DQN. But several others in the list here are rather new to me. Which algorithms are most suited for my type of game (i.e. 3+ players, hidden info).    The feature I am most concerned about is the hidden information. I see there are several games with hidden information implemented here, but as far as I can tell most of them deal with information which is hidden at the start of a playthrough (like dealing cards face down), and is possibly revealed during the playthrough. In my game, some actions are hidden to some players (players may make secret trades with each other; the amount of money involved is hidden to players not involved in the trade). So during the whole playthrough there is basically more and more information hidden. In the games listed here, I think only phantom tic-tac-toe has hidden actions, but maybe I have overlooked one or the other game with that characteristic. Anyway, which algorithms are suitable for games with hidden actions?    "
"Running into a problem including OpenSpiel in a Swift XCode 11 project (obviously using the S4TF compiler toolchain): I want to use it in my own project via `import OpenSpiel`, but if I try to include it as a dependency ('Swift Packages' -> 'Add Package Dependency'), it won't compile as it complains that 'Packages are not supported using the legacy build system'. And without using the legacy build system, S4TF doesn't work.     Has anyone managed to get this to work?     @taliesinb, @saeta"
"Unfortunately we cannot import a fix to the current NeuRD (see ( see   ) implementation, see   for details.    I'm adding this issiue a reminder to implement a sample-based NeuRD  along with the rest of the policy gradient variants in `policy_gradients.py`.    (If anybody wants to do this, please let us know. Otherwise, I'll add it eventually.)  "
Discovered when enabling Travis tests on MacOS:  
"Dear Authors,    Just want to ask whether there are some games and benchmark algorithms to support the research on  cooperative MARL.    Thanks.  Hui"
"I've got a question on the mean episode rewards returned whilst running the DQN algorithm (and probably other algorithms) when playing against random bots, e.g.:    [1000] Mean episode rewards [0.542 0.336]    As far as I can tell the DQN algorithm implements two agents, which play against each other, and then both agents are tested against the random bot. The first score above is the performance of agent1, whilst the second score is agent2.    I let this run for 400 episodes of Connect4 and got the following results:    Agent 1  --  Max : 0.986  Min : 0.542  Average : 0.8883675  StdDev : 0.0811125295114757      Agent 2  --  Max : 0.9  Min : 0.174  Average : 0.72085  StdDev : 0.110491594702945      There's quite a difference between these, and Agent 1 has a better score than Agent 2 in 95% of episodes.    Am I reading this right, and is this because Agent 1 is always playing first or is there another reason for this behavior?"
"I've been running experiments with lots of different agents being empirically evaluated in all possible matchups (all possible pairs of agents in 2-player games, all possible triples in 3-player games, etc.), and storing the outcomes of all played games in CSV files.    I'm trying use OpenSpiel's alpha-rank implementation to evaluate these agents, and figured the   function would be the most convenient approach for converting my CSV data into payoff tables. So I structured my CSV file as described in the comments of that function: a dataframe of two columns, ""agents"" and ""scores"", where each of those columns contains a tuple of size n per row for n-player games. Every row corresponds to a single game played between n agents.    I put the code I'm using for this here:      The comments at the top of the gist also contain a link to an example CSV file with results for a 3-player game. It's about 16MB, if a smaller CSV file would be easier I can prepare one.    For 2-player games, my code appears to work fine and the results I'm getting out of alpha-rank also seem to be reasonable. But with this kind of data for 3-player games, the `alpharank.compute()` call crashes with the following stack-trace:         Is my CSV file formatted incorrectly? Or am I calling the alpharank code incorrectly? Or is it an issue in the alpharank implementation itself?    I was particularly unsure about line 54 in my gist:         I wrap the output from `heuristic_payoff_table.from_match_results(df, all_agents)` in a single-element list because the subsequent calls seem to expect lists as inputs, but it does seem strange to have to do this to me so I suspect this might be where I'm doing something wrong?"
Apologies if this was answered in the documentation and I missed it but it looks like there are algorithms implemented in both python and c++.  For contributors who are looking to add some algorithms is there any preference for python vs c++ implementations? Or maybe it is better to add both? Or is it a case-by-case sort of thing? Thanks.
"Currently, there is only a simple oracle Evolutionary Strategy implemented in  .    To reproduce the results in   (Reactor),   (PBR), etc, more oracles (e.g., deep reinforcement learning) are required to expand a new policy over mixed meta policies.    Let me know if I have missed somethings/oracles that already implemented in the project."
What is the advantage of regret matching against exp3 as policy improvement methods?  What will happen if we replace RM in all CFR variants by Hedge?
"I am the creator of a new board game called ""Orbital Velocity"" (to be released in Dec 2019). It is a simple card game to help students learn, and in this finally learn MachineLearning.    I wonder if I could use OpenSpiel to build RL platform. I can contribute code related to that code. From what I understand, the existing OpenSpiel API is sufficient to build my game.    What is the best way to contribute my game to this repository.  "
I noticed that there is a c++ API to transform normal form games to turn-based one.  But how to do it in python.  Is there an example to do it?  E.g. doing CFR on matrix game.
"Hi,    This might be more of an RL theory question but I came across some interesting behavior after making slight adjustments to `open_spiel/python/examples/breakthrough_dqn.py`. I wanted to see how well the two DQN agents played against each other during evaluation, so I edited `eval_against_random_bots` so that they only faced each other instead of random agents (by commenting out lines 56, 57). I'm relatively new to RL so it was difficult for me to explain the output beyond handwaving:       In other words, one agent is winning all 1000 games during each evaluation.  Is this behavior expected? I was wondering if this is expected greedy behavior since `is_evaluation=True`. If so, can anyone provide a rigorous explanation for this? "
"Hi,    I'm doing an experiment that involves training a slightly modified version of `open_spiel/python/algorithms/dqn.py` from scratch against an unmodified trained version of `open_spiel/python/algorithms/dqn.py`. I'm not adept at Tensorflow so I guessed that I would need two different tf.Sessions (one for each agent) so that the untrained agent does not have the same weights as the trained agent. To restore the trained agent, I used        The rest follows the same logic as `open_spiel/python/examples/breakthrough.py`    However, I get a list of errors such as the following:       Any assistance on having an untrained agent train against a trained agent would be appreciated.    "
"Thanks for implementing the uncontested bidding for bridge, I'm curious if it is in your plans to implement the full game (both the bidding and the card playing)?  "
"Hello. When I try to run the Build_and_run_tests.sh file, it could Link and built all the target,but there is a Failed in ""Performing Test CMAKE_HAVE_LIBC_PTHREAD, Looking for pthread_create in pthreads - not found"".And when Test all the target tests, some test occor the massage,"" ImportError: /XXX/Deepmind/open_spiel/open_spiel/build/python/pyspiel.so: undefined symbol: PyThread_tss_set "". Below is the building message:    + CXX=g++  + NPROC=nproc  + [[ linux-gnu == \d\a\r\w\i\n* ]]  ++ nproc  + MAKE_NUM_PROCS=56  + let 'TEST_NUM_PROCS=4*56'  ++ python3 -c 'import sys; print(sys.version.split("" "")[0])'  + PYVERSION=3.6.7  + BUILD_DIR=build  + mkdir -p build  + cd build  + echo 'Building and testing in /XXX/Deepmind/open_spiel/open_spiel/build using '\''python'\'' (version 3.6.7).'  Building and testing in /XXX/Deepmind/open_spiel/open_spiel/build using 'python' (version 3.6.7).  + cmake -DPython_TARGET_VERSION=3.6.7 -DCMAKE_CXX_COMPILER=g++ ../  -- The C compiler identification is GNU 4.8.5  -- The CXX compiler identification is GNU 7.3.0  -- Check for working C compiler: /usr/bin/cc  -- Check for working C compiler: /usr/bin/cc -- works  -- Detecting C compiler ABI info  -- Detecting C compiler ABI info - done  -- Detecting C compile features  -- Detecting C compile features - done  -- Check for working CXX compiler: /public/software/gcc-7.3/bin/g++  -- Check for working CXX compiler: /public/software/gcc-7.3/bin/g++ -- works  -- Detecting CXX compiler ABI info  -- Detecting CXX compiler ABI info - done  -- Detecting CXX compile features  -- Detecting CXX compile features - done  -- Looking for pthread.h  -- Looking for pthread.h - found  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed  -- Looking for pthread_create in pthreads  -- Looking for pthread_create in pthreads - not found  -- Looking for pthread_create in pthread  -- Looking for pthread_create in pthread - found  -- Found Threads: TRUE  -- Found Python3: /***/software/anconda3/lib/libpython3.7m.so (found version ""3.7.1"") found components:  Development  -- Configuring done  -- Generating done  What should I do for this problem?"
"I'm playing around with a basic DQN working with backgammon, and having got this working with a FF network, would like to try this using a convolutional network instead. So far I've:    - created a copy of the dqn.py algorithm called dqn_conv.py, which implements a simple conv network using Sonnet  - created a version of breakthrough_dqn.py file, which points at backgammon instead, and the above dqn_conv algorithm instead of dqn    this currently fails (which I'd expect it to) as the backgammon state is output as (None, 198) of rank 2 which doesn't match the input I need for the conv net.    I'm presuming that I need to modify BackgammonState::InformationStateAsNormalizedVector in backgammon.cc to output the right tensor, but any advice on doing this in the right way would be appreciated.     I'm currently thinking I should add a game option which defines which output format the game returns, from the current one (default), perhaps a 1D conv format (as backgammon is essentially a 1D game) and a 2D conv format.    Does this make sense, and any issues with doing this?"
"Hi,    I'm trying to speed up the training of the openspiel rl agents. I've tried both python's multiprocessing package and   but in both cases I get the same error:   `TypeError: can't pickle pyspiel.Game objects`    If there is no way around this, can anyone suggest other methods of parallel/distributed training of agents playing openspiel games?    Thank you."
This would enable us to detect a breakage on that platform. it seems there are quite a few users of Mac among OpenSpiel users
"I am getting below error message in make of hyperscan...I'm trying to build Hyperscan latest version as of today.  cmake verison : 3.15.3     Error Details  Scanning dependencies of target hsbench  [ 95%] Building CXX object tools/hsbench/CMakeFiles/hsbench.dir/data_corpus.cpp.o  [ 95%] Building CXX object tools/hsbench/CMakeFiles/hsbench.dir/engine.cpp.o  [ 95%] Building CXX object tools/hsbench/CMakeFiles/hsbench.dir/engine_hyperscan.cpp.o  [ 96%] Building CXX object tools/hsbench/CMakeFiles/hsbench.dir/heapstats.cpp.o  [ 96%] Building CXX object tools/hsbench/CMakeFiles/hsbench.dir/huge.cpp.o  [ 96%] Building CXX object tools/hsbench/CMakeFiles/hsbench.dir/main.cpp.o  [ 96%] Building CXX object tools/hsbench/CMakeFiles/hsbench.dir/sqldb.cpp.o  [ 97%] Linking CXX executable ../../bin/hsbench  CMakeFiles/hsbench.dir/engine_hyperscan.cpp.o: In function `buildEngineHyperscan(std::map , std::allocator  > > const&, ScanMode, std::string const&, std::string const&, ue2::Grey const&)':  /root/NAEEM/hyperscan/tools/hsbench/engine_hyperscan.cpp:427: undefined reference to `ue2::hs_compile_lit_multi_int(char const* const*, unsigned int const*, unsigned int const*, hs_expr_ext const* const*, unsigned long const*, unsigned int, unsigned int, hs_platform_info const*, hs_database**, hs_compile_error**, ue2::Grey const&)'  /root/NAEEM/**hyperscan/tools/hsbench/engine_hyperscan.cpp:434: undefined reference to `ue2::hs_compile_multi_int(char const* const*, unsigned int const*, unsigned int const*, hs_expr_ext const* const*, unsigned int, unsigned int, hs_platform_info const*, hs_database**, hs_compile_error**, ue2::Grey const&)'  collect2: error: ld returned 1 exit status**  make[2]: *** [bin/hsbench] Error 1  make[1]: *** [tools/hsbench/CMakeFiles/hsbench.dir/all] Error 2  make: *** [all] Error 2  [root@howler hyperscan]# ^C  [root@howler hyperscan]#  "
"Are there any plans to implement mcts solver into mcts? I see in code there is just simple back-propagation of the values, regardless wether they come from interior node or terminal node. Mcts solver is when we encounter terminal node, we can apply something like inf/-inf value to the node, but also back-propagate the terminals as far as we can, by using logic that if terminal node is a win for a player, then it's obviously loss for its parent (opponent). Similarly if all siblings are loss, then it's a win for the parent and again loss for grandparent."
"For game like goofspiel, there could be two ways to determine the utility of terminal nodes.   1. the final win, draw, lose   2. the difference of chance card value wins by each player    It seems the repository only support the first way now. Will it be helpful to add the second way for game like this?"
"I try to implement discounted cfr by writing a subclass of _CFRSolver class. I overwrite the _compute_counterfactual_regret_for_player function. Since linear CFR is just a special case of discounted cfr, so it is also implemented. I am not sure if I implement it in a nice way.    I test it by running on goofspiel4, it gets decent results. Will the implementation be helpful? Should I pull it?"
"I want to run cfr_example.py on different games other than Kuhn poker. I changed the game name to ""goofspiel"". Then I got a running error like this:    RuntimeError: /Desktop/open_spiel/open_spiel/games/goofspiel.cc:313 player >= 0  player = -2, 0 = 0     I don't know how to fix it. What mistake do I make?  "
"Looking for some guidance - keen to implement the backgammon PubEval bot for benchmarking backgammon game performance. The original code is in C++ and would rather implement in this if possible, but all the existing bots are written in Python - is it possible to write a C++ bot, or is there a smarter way of doing this?    I can convert the code to Python if necessary, but just want to know if there are other options first."
"As pointed out in #51 domains can factorize their actions if there's too many of them per node.     Would you like to impose a limit on that, for example having at most 256 actions per node? The reason why this would practically useful is because pretty much all the algorithms work with some notion of saving a value per action."
"Thanks a lot for open sourcing the repository. It's quite helpful.    I see there are many variants of CFR algorithm implemented. I would like to ask if there are someone already working on the VR-MCCFR and if the discounted regret minimisation algorithm also on the list to be done.    If I want to try this two tasks, are there some suggestions to make the code more suitable for the repository?  "
"Hello,  I was checking policy gradient algorithm on HLE.  Firstly, I think there is something wrong with how Hanabi handles rewards (or I am missing something). I ran this simple code (similar to PG for poker example) and looked how agents save rewards. I made discount =  0, so `agent._dataset['returns']` is a list of immediate rewards.       This is an example of weird output.     P0 get 0pt, then P1 gets 1pt and then P0 loses. As I understand, P0 should have rewards [1, -1] which is correct. However P1 should have [0] since he scored a point and right after P0 lost it.    **Player0**  Fireworks: R0 Y0  --> Fireworks: R0 Y0      **Player 1**  Fireworks: R0 Y0  --> Fireworks: R0 Y1      **Player 0**  Fireworks: R0 Y1  --> lost     **Agent 0 rewards history:**  [1.0, -1.0]  **Agent 1 rewards history:**  [-1.0]    Second question is concerning self-play. I would like to run PG agent in a self-play there it plays with itself (not similar agent.) All examples I found in open_spiel have several copies of agents playing with each other and learning separately.  As I understand, using one agent to play with itself in current implementation will not work since episode experience of different players will mix up."
"I'd like to take a look at implementation of doubling cube and match logic for Backgammon.    The doubling cube is critical to backgammon strategy, and previous Backgammon value networks such as TDGammon have not implemented this within the neural network - it's been traditionally bolted on via a separate doubling algorithm. I think this has the potential to be an area where this framework could offer significant improvements over previous works.    The way I would see it working would be:    - Players with an available double would have a move choice of offering the double or rolling the dice.   - If double is offered, opponent would have the move choices of accept the double or resign.  - If the double is accepted the doubling dice would then be owned by the opposing player.    For this to work, the backgammon State will need to include:    - Match Player 0 score  - Match Player 1 score  - Match SetPoints (the number of points a player needs to get to)  - Match isCrawford (is the Crawford rule applicable?)  - Doubling cube value  - Doubling cube owner (0/1 for players, -1 for no owner)    Would also need to have a way of passing the following options to backgammon.cc :     - UseDoublingCube  - MatchSetPoints (1 for single game)  - UseCrawfordRule    Would be interested in views on this - probably won't be looking at this for a few weeks (assuming you're happy to support this), but keen to get initial thoughts down.  "
"I just came across Gamut library:       I thought it might be an interesting addition for generating normal-form games and wanted to let you know about it, in case you didn't. It's written in Java, so maybe it would be possible to write a ""bridge"" for it? Maybe it can be added to the list of call for contributions?  "
"environment:  system: mac  compiler: gcc     I have seccessfully build the test until ""make -j$(nproc)"", and have change the command line to ""make -j$(sysctl -n hw.ncpu)"",since mac not identify nproc.  and the this happen:    Scanning dependencies of target open_spiel_core  [  1%] Building CXX object abseil-cpp/absl/base/CMakeFiles/absl_spinlock_wait.dir/internal/spinlock_wait.cc.o  [  1%] Building CXX object abseil-cpp/absl/base/CMakeFiles/absl_dynamic_annotations.dir/dynamic_annotations.cc.o  [  2%] Building CXX object abseil-cpp/absl/base/CMakeFiles/absl_log_severity.dir/log_severity.cc.o  [  2%] Building CXX object CMakeFiles/open_spiel_core.dir/game_parameters.cc.o  clangclang: clangclang: : : error: argument unused during compilation: '-undefined dynamic_lookup' [-Werror,-Wunused-command-line-argument]errorerror: :  argument unused during compilation: '-undefined dynamic_lookup' [-Werror,-Wunused-command-line-argument]argument unused during compilation: '-undefined dynamic_lookup' [-Werror,-Wunused-command-line-argument]    error: argument unused during compilation: '-undefined dynamic_lookup' [-Werror,-Wunused-command-line-argument]  make[2]: *** [CMakeFiles/open_spiel_core.dir/game_parameters.cc.o] Error 1  make[1]: make[2]: *** [CMakeFiles/open_spiel_core.dir/all] Error 2  make[1]: *** Waiting for unfinished jobs....  *** [abseil-cpp/absl/base/CMakeFiles/absl_log_severity.dir/log_severity.cc.o] Error 1  make[2]: *** [abseil-cpp/absl/base/CMakeFiles/absl_spinlock_wait.dir/internal/spinlock_wait.cc.o] Error 1  make[2]: *** [abseil-cpp/absl/base/CMakeFiles/absl_dynamic_annotations.dir/dynamic_annotations.cc.o] Error 1  make[1]: *** [abseil-cpp/absl/base/CMakeFiles/absl_log_severity.dir/all] Error 2  make[1]: *** [abseil-cpp/absl/base/CMakeFiles/absl_spinlock_wait.dir/all] Error 2  make[1]: *** [abseil-cpp/absl/base/CMakeFiles/absl_dynamic_annotations.dir/all] Error 2  make: *** [all] Error 2        when I use python to import pyspiel, this happen:  ModuleNotFoundError: No module named 'pyspiel'    please help!"
"So it just freezes forever at the final make step (install.sh, venv, etc. run without a glitch):  [ 97%] Built target matrix_games_test  [ 97%] Built target turn_based_simultaneous_game_test  [ 98%] Built target misere_test  [ 99%] Built target spiel_test  [100%] Building CXX object python/CMakeFiles/pyspiel.dir/pybind11/pyspiel.cc.o"
It would be great if we could use this repo with Hanabi. Probably adding some kind of wrapper would  be enough.
"Have had a first stab at running OpenSpiel within the Windows Subsystem for Linux (WSL) version 1 on Windows 10, and its been partially successful - 54% of tests passed. Not suggesting this as a long term solution for Windows support, but may hep some in the short term.    First of all installed WSL as per the instructions here:      The highest version of SUSE support was 18.04, so had to upgrade cmake via the following commands:     wget     tar -xvzf cmake-3.12.4.tar.gz  cd cmake-3.12.4/  ./configure  make  sudo make install  sudo update-alternatives --install /usr/bin/cmake cmake /usr/local/bin/cmake 1 --force    After that, the install instructions seemed to work fine. Looks like the tests failing are the Python ones - anyone got any suggestions on resolving this?    54% tests passed, 43 tests failed out of 94    Total Test time (real) = 130.80 sec    The following tests FAILED:           49 - python_api_test (Failed)           50 - python_playthrough_test (Failed)           51 - python_action_value_vs_best_response_test (Failed)           52 - python_best_response_test (Failed)           53 - python_cfr_test (Failed)           54 - python_deep_cfr_test (Failed)           55 - python_dqn_test (Failed)           56 - python_eva_test (Failed)           57 - python_evaluate_bots_test (Failed)           58 - python_expected_game_score_test (Failed)           59 - python_exploitability_descent_test (Failed)           60 - python_exploitability_test (Failed)           61 - python_fictitious_play_test (Failed)           62 - python_generate_playthrough_test (Failed)           64 - python_rl_losses_test (Failed)           65 - python_lp_solver_test (Failed)           66 - python_mcts_test (Failed)           68 - python_nfsp_test (Failed)           69 - python_outcome_sampling_mccfr_test (Failed)           70 - python_policy_gradient_test (Failed)           71 - python_projected_replicator_dynamics_test (Failed)           72 - python_generalized_psro_test (Failed)           73 - python_rectified_nash_response_test (Failed)           74 - python_random_agent_test (Failed)           75 - python_rcfr_test (Failed)           76 - python_sequence_form_lp_test (Failed)           78 - python_bluechip_bridge_wrapper_test (Failed)           79 - python_uniform_random_test (Failed)           80 - python_alpharank_test (Failed)           81 - python_alpharank_visualizer_test (Failed)           82 - python_dynamics_test (Failed)           83 - python_heuristic_payoff_table_test (Failed)           84 - python_utils_test (Failed)           85 - python_visualization_test (Failed)           86 - python_catch_test (Failed)           87 - python_cliff_walking_test (Failed)           88 - python_data_test (Failed)           89 - python_bot_test (Failed)           90 - python_games_sim_test (Failed)           91 - python_matrix_game_utils_test (Failed)           92 - python_policy_test (Failed)           93 - python_pyspiel_test (Failed)           94 - python_rl_environment_test (Failed)  Errors while running CTest  jfadmin@QJF-SURFACEBOOK:~/open_spiel/build$    "
"First of all a huge thank you for making this framework available, I think this is going to be a fantastic resource for anyone wanting to learn modern best practices for reinforcement learning.    I'm just getting my head around what this framework can do, but have noticed an issue with the notation within the Backgammon implementation - this doesn't follow the normal standards so may cause confusion. Currently it shows the starting point and the number of spaces moved, as opposed to the starting point / ending point.  For example from the start state, the following is displayed after a roll of 6-1:    chose action: 621 (23-6 23-1)    This should actually be:  chose action: 621 (23-17 23-22)    not sure if this has been done deliberately, but I'm happy to fix this if not - just wanted to check first.    I'd also be keen to implement the pubeval code as an agent / bot - this is a reference backgammon agent created by the mighty Gerard Tesauro which is useful for being able to test the performance of a backgammon bot.    Finally, would be keen to get involved with the Windows port - is there a vision for the toolset that this will use?"
"I am not familiar with game classification.    For example, I want to add tetris.  But I don't know if Tetris applies to the classification described in  . Is there any information that can be used to identify it?    Thank you."
Is this a list of supported games?    > CMakeLists.txt    I wanted to see a simpler list if possible.    Thank you.
"In games with large branching factor (such as stratego): `std::vector  LegalActions` will simply run out of memory, even on a smaller 6x6 variant (12!^2).    Do you think it would make sense to support them, and if yes, how?   I suggest implementing methods such as `unsigned long LegalActionsCount` and `LegalActionAt(unsigned long index)`"
"I think it would be nice to have a general test suite for all games which makes these kinds of generic tests, like action-infoset consistency, zero-sumness if the game claims so, etc., it would be beneficial for game developers. I didn't notice such a thing in the lib. The kinds of tests we've done in GTLib are:  - isDomainZeroSum  - domainMaxUtility  - domainMaxDepth  - isNumPlayersCountActionsConsistentInState  - isActionGenerationAndAOHConsistent    It's not much, but catches bunch of bugs fast."
"I'm trying to rewrite OOS (  and specifically information state targeting.    Part of it is to write a function which takes two ActionObservation sequences and returns if they are compatible (it's possible to get from the current infoset to the target infoset).    In OpenSpiel, neither the `InformationState` / `InformationStateAsNormalizedVector` provide ""incremental"" information: it's a domain-dependent representation of the whole AO sequence.   `Observation` or `ObservationAsNormalizedVector` do these ""incremental"" observations: however how is the observation of ""no observation"" encoded in phantom games? Empty vector?    Also, do you think it would make sense to store the ActionObservation for each player in State (similarly to how history_ is stored)? It is not possible to do a currentState->parentState lookup, so to retrieve the whole AO for arbitrary state you'd have to make targetted tree traversal from the root to this state.     I'm asking this specifically in the context of OOS, where you will be given a target infoset (or State, in the Bot API), but you might not know what it's ActionObservations are (if you didn't visit this infoset before and cached it). The result is that you cannot target this infoset efficiently.  "
"     The behavioral policy is being added to the cumulative policy when the player's sequence probability (the probability that the player plays to the current information state and plays action `aidx`) should be added instead. The probability that the node was sampled will also have to be taken into account by dividing by that probability.    It doesn't look like there's a trivial fix for this because the reach and sampling probabilities aren't being passed down the tree, but adding and keeping track of these values would make the fix easy."
For similar reasons why there is `typedef int64_t Action` I'd like to suggest to use `typedef int Player` as well.
"The Step method     or respectively `EvaluateBots` does not offer a way to specify for how many iterations or for what time the bots are allowed to update their strategies, so it's not possible for example to compare the performance of two Monte Carlo bots which both receive 5s of time per move.    If there's interest, I can submit a PR (I've developed something similar in GTLib)"
"The API currently has a method      I believe this should be rather      as the bots should not get access to current state of the world, but only to their possible perception of it.     The current API also allows bots to ""cheat"", to see where they are **exactly** in the game, or to ask for observations of their opponent.    I understand changing this has some drawbacks - for example in perfect information (PI) games it's unnecessary (as num. of histories in information state is always 1) and it would require the authors somehow derive what state they are in. For such games, a workaround could be to derive all the actions from information state string -- player can always know his actions, and opponent's actions are received player's observations, since they are public in perfect info games. There can be a helper function for that in PI games.     In imp. info games this derivation is a **crucial component** of the algorithms themselves, as they have to ""figure out"" what could have happened in the game so that they've received such observations."
The documentation `docs/developer_guide.md` seems outdated - there is no `new_game.h` and the registration seems to be done via `REGISTER_SPIEL_GAME` macro.
"I just wanted to make sure I understand some things:    `Action` is an int64_t, but it is not guaranteed to have them indexed from `i=0...n-1`, i.e. `State.LegalActions.at(i) == i` does not always hold. (Btw this could be mentioned in comment for `using Action = int64_t;` in `spiel_utils.h`)    However, does it hold that `x.LegalActions.at(i) == y.LegalActions.at(i)` for all states `x,y` within the same infoset `I` and all action indices `i`? Or in other words, are actions of histories within infoset consistent?    If yes, is there a test for that which checks it for all games (maybe up to certain depth of the tree)?    Thanks for great work!    "
I saw that a games wrapper for the Ludii General Game System was something that you were looking to add in contributing.md.  It looks like Ludii is written in Java.  Was the idea to be able to call the Ludii jar from C++ in order to interact with it?
there seems to be no way to access the go board using the python API
"I want to run open_spiel/python/examples/playthrough.py for an richer example generating a playthrough and printing all available information. (MacOS, Python 3.7.3)    The error message is:  ModuleNotFoundError: No module named 'open_spiel'    The snapshot is as the follows:  !     How can I import open_spiel.python.algorithms when I run the python program?  Thanks~    @lanctot         "
"Hello,    I tried to use example.py to play a tic_tac_toe game using command `python3 example.py --players=1`. Here's the error I ran into:  `Traceback (most recent call last):        File ""example.py"", line 111, in              app.run(main)        File ""/open_spiel/venv/lib/python3.7/site-packages/absl/app.py"", line 299, in run            _run_main(main, args)        File ""/open_spiel/venv/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main            sys.exit(main(argv))        File ""example.py"", line 46, in main            {""players"": pyspiel.GameParameter(FLAGS,players)})  RuntimeError: Unknown parameter players`  The example.py works well if I do not pass any parameter to it. Could anyone tells me how to solve this problem? Thanks!"
"Hello,    according to the   referenced in connection to the ""Cooperative Box-Pushing"" game, the agents are supposed to receive observations that correspond to what they currently see in a given time step:    > After every time step each agent gets one out of 5 possible observations deterministically describing the situation of the environment in front of the agent: empty field,wall, other agent, small box, large box.    However, the game as implemented in OpenSpiel appears to return something else. In `time_step.observations[""info_state""]`, each agent receives a field of 704 binary values which seems to correspond to the global state (704 = 64 cells x 11 possible cell states).    Also, the `GameType` flag `provides_information_state` is set to `true`, while `provides_observation` is set to false which also goes contrary to what I would expect after reading the paper.    Is this an oversight or is there a purpose behind this?  "
"I have successful run the following steps on MacOS:  1) ./install.sh   2)   virtualenv -p python3 venv  source venv/bin/activate  pip3 install -r requirements.txt  3) ./open_spiel/scripts/build_and_run_tests.sh    But when I try to run my first example, like:  examples/example --game=tic_tac_toe    I can not find the ""example"" program in the  ""examples"" directory:  !     Anyone knows what happens here? Thanks!"
"First of all, thanks for making this framework open source.    I’m investigating the possibility of making a (simplified) alphaZero implementation using openspiel, and I was looking for some implementation ideas, especially since you already mention this in the contributors guide.    Please note: I am not sure if I will have the time to make the code up to openspiel standards. Also I might not very closely follow the alphazero pseudo-code. Thus, I am uncertain sure whether this effort will eventually result in a pull request. I still think some pointers would be very helpful, since others might be going to work on similar algorithms.    Implementation wise, it seems most logical to me to create an _rl_agent_ implementation called alphaZero. When taking a _step_ however; the agent will perform a MCTS. To do this, a complete game state will have to be reconstructed. The easiest way to do this would be to pass the current environment state as an argument to the _step_() function of the _rl_agent_ and then creating a game with this _state_ internally in the _rl_agent_. This feels  hacky to me: instead of using the _time_step_ argument, which was seemingly designed to provide all available information to the agent, you are feeding it extra with the full game state (of course, in a perfect information game this would be available already anyways).     What would be your perspective on this topic?  Of course, any other design advice on the implementation would be very welcome as well.         **tl;dr:** How to do state reconstruction in rl_agent? Do you have design advice on the implementation of an alphaZero-like algorithm?"
"In README under   when we click on CONTRIBUTING.md under ""Roadmap and Call for Contributions"" it says Not Found.    Apparently, the link address is incorrect. I'm willing to open a PR to fix this. "
"It seems that the terminal state detection of connect_four does not detect all winning states.   See the following example for a minimal reproduction:       results in:       This should be a winning state for x and thus print True, right? Or do I just understand the functionalities of state and is_terminal() incorrectly?"
"When I use ""make -j$(nproc)"" to make, some errors display as the follows:    In file included from /Users/user/documents/deeplearning/open_spiel/open_spiel/abseil-cpp/absl/random/internal/randen_slow.cc:21:  /Users/user/documents/deeplearning/open_spiel/open_spiel/abseil-cpp/absl/random/internal/platform.h:165:48: error:         'TARGET_OS_IPHONE_SIMULATOR' is not defined, evaluates to 0 [-Werror,-Wundef]  #if defined(__APPLE__) && (TARGET_OS_IPHONE || TARGET_OS_IPHONE_SIMULATOR)      [  8%] Built target absl_leak_check_disable  In file included from /Users/user/documents/deeplearning/open_spiel/open_spiel/abseil-cpp/absl/random/internal/randen_hwaes.cc:26:  /Users/user/documents/deeplearning/open_spiel/open_spiel/abseil-cpp/absl/random/internal/platform.h:165:48: error:         'TARGET_OS_IPHONE_SIMULATOR' is not defined, evaluates to 0 [-Werror,-Wundef]  #if defined(__APPLE__) && (TARGET_OS_IPHONE || TARGET_OS_IPHONE_SIMULATOR)  ...    Please give some help. Thanks a lot."
"I read   and saw that one of them was adding examples and supports for PyTorch. I wanted to contribute, but was wondering if you had particular algorithms or examples in mind to start with.    If there is no particular preference, I would like to start with DQN, imitating the style of the existing TensorFlow example in  .    Thank you! "
I am following the instructions from   and I am getting an error in step 3 when running     The output is      I am running this is Ubuntu 16.04 and using the virtualenv specified in the previous steps
# Output of the test:       # My setup:            
Need to include `#!/bin/bash` in the first line.
"Hi All    Firstly, thanks for this fantastic repo!    Looking at the `python/algorithms/mcts.py` function `SearchNode.child_value` it seems UCB is implemented as follows:         In the usual UCB formula (  or  ) the second term contains `log(parentN) / childN`, while above is `log(childN)/childN`.    I tested both formulas on different implementation of MCTS and both work, but `log(childN)/childN` seems to degrade performance.    There seems to be equivalent issue in C++ version in `mcts.cc`    Is this a potential bug, or am I missing something?"
None
None
"while trying to run `install.sh`, errors pump up for some reason finally ends up manually install dependencies in that .sh file    env:  Ubuntu 19.04  python 3.6    error:       "
"The Problem is solved.Thanks to rejuvyesh.    (lost) hyh@amax2080-1:~/Downloads/open_spiel-master/build$ CXX=g++ cmake   -DPython_TARGET_VERSION=3.6 -DCMAKE_CXX_COMPILER=${CXX} ../open_spiel  -- The C compiler identification is GNU 5.4.0  -- The CXX compiler identification is GNU 5.4.0  -- Check for working C compiler: /usr/bin/cc  -- Check for working C compiler: /usr/bin/cc -- works  -- Detecting C compiler ABI info  -- Detecting C compiler ABI info - done  -- Detecting C compile features  -- Detecting C compile features - done  -- Check for working CXX compiler: /usr/bin/g++  -- Check for working CXX compiler: /usr/bin/g++ -- works  -- Detecting CXX compiler ABI info  -- Detecting CXX compiler ABI info - done  -- Detecting CXX compile features  -- Detecting CXX compile features - done  CMake Error at CMakeLists.txt:45 (add_subdirectory):    add_subdirectory given source ""abseil-cpp"" which is not an existing    directory.      -- Found Python3: /home/hyh/anaconda3/lib/libpython3.7m.so (found version ""3.7.0"") found components:  Development   -- Configuring incomplete, errors occurred!  See also ""/home/hyh/Downloads/open_spiel-master/build/CMakeFiles/CMakeOutput.log"".  "
"**environment:**  system: Ubuntu 18.04.3 LST  cmake: 3.15.2  compiler: gcc (Ubuntu 6.5.0-2ubuntu1~18.04) 6.5.0 20181026    I am unable to build and run tests (the following was run within the activated virtual environment, as specified in the installation instructions):    ~/github/open_spiel/build$ CXX=g++ cmake -DPython_TARGET_VERSION=3.6 -DCMAKE_CXX_COMPILER=${CXX} ../open_spiel  -- The C compiler identification is GNU 7.4.0  -- The CXX compiler identification is GNU 6.5.0  -- Check for working C compiler: /usr/bin/cc  -- Check for working C compiler: /usr/bin/cc -- works  -- Detecting C compiler ABI info  -- Detecting C compiler ABI info - done  -- Detecting C compile features  -- Detecting C compile features - done  -- Check for working CXX compiler: /usr/local/cuda-9.0/bin/g++  -- Check for working CXX compiler: /usr/local/cuda-9.0/bin/g++ -- works  -- Detecting CXX compiler ABI info  -- Detecting CXX compiler ABI info - done  -- Detecting CXX compile features  -- Detecting CXX compile features - done  -- Looking for pthread.h  -- Looking for pthread.h - found  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed  -- Looking for pthread_create in pthreads  -- Looking for pthread_create in pthreads - not found  -- Looking for pthread_create in pthread  -- Looking for pthread_create in pthread - found  -- Found Threads: TRUE    -- Found Python3: /usr/lib/x86_64-linux-gnu/libpython3.6m.so (found version ""3.6.8"") found components:  Development   -- Configuring done  You have changed variables that require your cache to be deleted.  Configure will be re-run and you may have to reset some variables.  The following variables have changed:  CMAKE_CXX_COMPILER= /usr/local/cuda-9.0/bin/g++    -- The C compiler identification is GNU 7.4.0  -- The CXX compiler identification is GNU 6.5.0  -- Check for working C compiler: /usr/bin/cc  -- Check for working C compiler: /usr/bin/cc -- works  -- Detecting C compiler ABI info  -- Detecting C compiler ABI info - done  -- Detecting C compile features  -- Detecting C compile features - done  -- Check for working CXX compiler: /usr/local/cuda-9.0/bin/g++  -- Check for working CXX compiler: /usr/local/cuda-9.0/bin/g++ -- works  -- Detecting CXX compiler ABI info  -- Detecting CXX compiler ABI info - done  -- Detecting CXX compile features  -- Detecting CXX compile features - done  -- Looking for pthread.h  -- Looking for pthread.h - found  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed  -- Looking for pthread_create in pthreads  -- Looking for pthread_create in pthreads - not found  -- Looking for pthread_create in pthread  -- Looking for pthread_create in pthread - found  -- Found Threads: TRUE    CMake Error at python/CMakeLists.txt:1 (if):    if given arguments:        ""STREQUAL"" """"      Unknown arguments specified      -- Configuring incomplete, errors occurred!  See also ""/home/skif/github/open_spiel/build/CMakeFiles/CMakeOutput.log"".  See also ""/home/skif/github/open_spiel/build/CMakeFiles/CMakeError.log"".  You have changed variables that require your cache to be deleted.  Configure will be re-run and you may have to reset some variables.  The following variables have changed:  CMAKE_CXX_COMPILER= /usr/local/cuda-9.0/bin/g++    CMAKE_HAVE_LIBC_PTHREAD is causing an issue - I am not entirely sure how to resolve it.    Any suggestions would be most welcome. Apologies in advance if it's a trivial issue"
"Thanks for open sourcing open_spiel. I love the aim for completeness in this space!    One thing that I noticed is missing is games with continuous action-spaces and related algorithms (such as Deterministic Policy Gradient). Are there any plans to add support for those?    I'm especially interested in MARL and equilibrium learning in auction games.  As an example, I looked at the included implementation of the discretized first-price sealed-bid auction with integer valuations and actions. On the one hand, continuous-actions are necessary when studying emergent MARL-behavior: While a discrete implementation is pretty straight-forward for symmetric-uniform valuation-distributions, it become a lot less meaningful in settings with asymmetric or non-uniform (e.g. Gaussian) priors, where equilibria are nonlinear.  On the other hand, more involved auction games, that are being studied in economics, simply become all but intractable in a discrete implementation: Take, e.g. combinatorial FPSB auctions, where bundles of multiple items are sold to bidders at the same time. Even in a continuous-action implementation, the representation size of the action space already grows exponentially in the number of items; but in a discrete implementation it grows double-exponentially.    I'm sure there's many other use cases, e.g. in optimal control."
FYI
I'm getting into an infinite loop when running `cmake` per the installation guide:     
Running (patched -- see #2) `./open_spiel/scripts/build_and_run_tests.sh` per install instructions results in the following:     
     Apparently `nproc` alias is not expanded in the `build_and_run_tests.sh` script in the `$(nproc)` expression. My fix:       
"I've looked over the code and the arxiv paper but I didn't see a mention of public states. What is the status on these? Are you planning to support them in the library? If yes, are you considering also introducing ""shared"" states between players? And how about the FOG formalism?    We are developing a game theoretic library at CTU as well. It seems like wasteful redundant effort to make two libraries, however for the algorithms we develop public trees / infoset trees are needed.    I'm very happy to open a discussion on this topic! :) It would save us tremendous engineering efforts if we do not have to duplicate code like this. "
